{"title": "Winning Amazon KDD Cup'24", "authors": ["Chris Deotte", "Ivan Sorokin", "Ahmet Erdem", "Benedikt Schifferer", "Gilberto Titericz Jr", "Simon Jegou"], "abstract": "This paper describes the winning solution of all 5 tasks for the Amazon KDD Cup 2024 Multi Task Online Shopping Challenge for LLMs. The challenge was to build a useful assistant, answering questions in the domain of online shopping. The competition contained 57 diverse tasks, covering 5 different task types (e.g. multiple choice) and across 4 different tracks (e.g. multi-lingual). Our solution is a single model per track. We fine-tune Qwen2-72B-Instruct on our own training dataset. As the competition released only 96 example questions, we developed our own training dataset by processing multiple public datasets or using Large Language Models for data augmentation and synthetic data generation. We apply wise-ft to account for distribution shifts and ensemble multiple LoRA adapters in one model. We employed Logits Processors to constrain the model output on relevant tokens for the tasks. AWQ 4-bit Quantization and vLLM are used during inference to predict the test dataset in the time constraints of 20 to 140 minutes depending on the track. Our solution achieved the first place in each individual track and is the first place overall of Amazon's KDD Cup 2024.", "sections": [{"title": "Introduction", "content": "The capabilities of Large Language Models (LLMs) have significantly improved in the last years and they have become popular due to their easiness to use. Users can interact with the systems in natural language. The LLMs excel on a variety of tasks, such as general reasoning, math questions, coding, etc. Many systems are getting updated by adding a LLMs to make them easier to use and/or providing more functionality. (Online) shopping is a large domain with billions of users and high economic output. The Amazon KDD Cup 2024 [2] is designed to evaluate LLMs to be a useful shopping assistant."}, {"title": "Amazon KDDCup 2024: Multi Task Online Shopping Challenge for LLMs", "content": "Amazon hosted the KDD Cup 2024 for Multi Task Online Shopping Challenge for LLMs [2]. They developed a test dataset, called ShopBench, containing 20,000 questions across 57 tasks. A development dataset of 96 question of only 18 different tasks were shared with the participants for the competition. The KDD Cup 2024 is designed as a code competition. Participants submit model weights with code which will be evaluated on infrastructure provided by Amazon. Participants have no access to the test dataset and can build solutions based on the 96 development questions. They receive only the scores on the full ShopBench dataset via the leaderboard. A submission is evaluated on 4x NVIDIA T4 GPUs with each 16 GB GPU memory within a runtime limit (see Table 1).\nParticipants have to address following challenges:\n(1) No training dataset: Participants have access to only 96 examples.\n(2) Hidden tasks: The development dataset contains only 18 out of 57 tasks. Therefore, the solution has to generalize to the unknown tasks.\n(3) Time and compute constrains: Solutions have to run within a runtime limit on 4x NVIDIA T4 GPUs with each 16 GB memory (see Table 1)\nThe competition contains 5 tracks:\n\u2022 Shopping Concept Understanding: Understanding shopping concepts (e.g. brands, product lines, attributes, etc.)\n\u2022 Shopping Knowledge Reasoning: Reasoning ability about products or product attributes (e.g. total amount in a product pack, are two products compliments or substitutes, etc.)\n\u2022 User Behavior Alignment: Understanding user behavior in online shopping (e.g. implicit information by user click stream\n\u2022 Multi-lingual Abilities: Shopping concept understanding and user behavior alignment across different languages\n\u2022 Overall: A final track which combines all 4 tracks\nThe evaluation dataset is based on 5 task types:\n\u2022 Multiple Choice: Only one correct answer. Evaluation metric is accuracy.\n\u2022 Ranking: Input contains multiple candidates and the model should provide an ordered list. Evaluation metric is nDCG.\n\u2022 Named Entity Recognition (NER): Extract pieces of text given an entity type. Evaluation metric is Micro-F1.\n\u2022 Retrieval: Select candidates from a list which satisfy the requirements. Evaluation metric is Hit@3\n\u2022 Generation: There are a diverse set of generation tasks depending on the task (e.g. translation). Evaluation metrics are ROUGE-L, BLEU or cosine similarity of sentence embedding.\nA track can contain one or multiple task types. The final score of a track is calculated by averaging across all questions because each evaluation metric is between 0-1. The overall challenge score is determined by the sum of position per track.\nFor every question, the requirement is to generate text, which is parsed by Amazon's evaluation script. The solution has to follow the prompt instructions (e.g. return 3 candidates IDs separated by a comma). If the evaluation script is not able to parse the generated text, then the score will be 0 for this question.\nThe competition was organized in 2 phases. The organizer shared that Phase 2 contains harder samples and tasks than Phase 1. They increased the compute resources from 2x NVIDIA T4s to 4x NVIDIA T4s for phase 2."}, {"title": "Training Dataset", "content": "Amazon shared multiple of eCommerce datasets with participants, which are related to the ShopBench dataset, but do not have the same structure. We created our training dataset by processing multiple datasets to have a similar structure as the 18 tasks from Shop-Bench development dataset. In addition, we developed new tasks. Finally, we augmented the dataset by prompting LLaMa3-70B-Instruct [3] and GPT-4 [11] for more diversity or infer missing information (e.g. product type, category). A detailed overview can be found in Appendix A."}, {"title": "Real Datasets", "content": "We utilized multiple data sources, including non e-commerce datasets such as MMLU and Alpaca-Cleaned. Samples from these datasets were transformed into the instruction prompts.\nAmazon-M2 [9] - A multi-lingual Amazon session dataset with rich meta-data used for KDD Cup 2023.\nAmazon Reviews 2023 [8] - A large scale Amazon Review Dataset with rich features and over 500M reviews across 33 categories."}, {"title": "Synthetic Datasets", "content": "To further improve diversity of dataset we utilized the synthetic data generation (SDG) pipelines. In general, we used three different methods.\nWe prompt LLM to construct the tasks specific prompts from the seed data. For example, we rephrase the original tasks from NingLab ECInstruct dataset. These tasks include various information about the product (title, description, attributes) and we combine all of them into one prompt. Before constructing a task specific prompt, we extract the correct labels from the seed data using LLM. For example, we extract the product type, categories or attributes first and then construct the question. We used GPT-4 to generate the instructions with different word-ings, and then used it to construct MC tasks from ESCI-data dataset. The correct answer was randomly selected from the E entries, the remaining options were selected from the entries with S/C/I labels"}, {"title": "Model", "content": "We explored both zero shot LLM models and fine-tuned LLM models. Our final winning solution achieving our best model accuracy is fine-tuned. When using zero shot with an instruction tuned LLM, we found it helpful to use both the system role and user role when formatting prompts. Designing better prompts improved the zero shot model's performance.\nWhen fine-tuning, we found that the prompt was not as important because the model is fine-tuned to exhibit a certain behavior given whatever prompt we choose to train with.\nOne technique of our fine-tuned models used is to include an instruction to the model identifying which of the 5 task types the model is solving. Then during inference, we used a heuristic rule classifier which determined question task type and included this is the system role's instruction prompt. Specifically, we used the following template."}, {"title": "Fine-Tuning Qwen2", "content": "We fine-tuned Qwen/Qwen2-72B-Instruct [18] on our developed training dataset using 8x NVIDIA A100 with each 80GB GPU memory. Training on 500k examples takes around 24 hours. We used the library axolotl 1 and bitsandbytes 2 with QLoRA [5] with 4-bit quantization and bfloat16 [4]. The library applies Multipack (Sample Packing) 3, concatenating multiple sequences into one batch to increase training throughput. We train the LLM in a supervised fine-tuning strategy. The loss is calculated only on the answer tokens. A common technique in large language model training is to apply Reinforcement Learning from Human Feedback (RLHF) [12]. Our hypothesis is that supervised fine-tuning is sufficient for the competition. Many answers are a single number or a list of numbers, which have an exact solutions and does not require human preferences between multiple possible answers."}, {"title": "Ensemble Adapters", "content": "Our five track solutions are created from 4 fine-tuned LoRA adapters. We call them v7, v8, v7b, and v9b. First for tracks 1,3,5 we merged v8 to base model Qwen2-72B with 56% weight. For tracks 2,4 we merged v7 to base with 100%. Version 7 adapter was trained with 417k samples whereas v8 was trained with 462k. \nNext we trained two more LoRA adapters named v7b and v9b using two different new subsets of 152k and 40k samples respectively."}, {"title": "Wise-ft", "content": "To take into account the distribution shift between the evaluation data from the ShopBench dataset and the collected training data (see Section 3), we used wise-ft [17].\nWise-ft interpolates between the weights \\(W_{base}\\) of a base model and the weights \\(W_{ft}\\) of a fine-tuned model using the following formula:\n\\(W_{wise} = (1 \u2212 \u03b1) * W_{base} + \u03b1 * W_{ft}\\)\nwhere \\(\u03b1 \u2208 [0, 1]\\). This approach effectively balances the trade-off between the zero-shot capabilities of the base model (\u03b1 = 0) and the task-specific performance of the fine-tuned model (\u03b1 = 1).\nFor LoRA, as \\(W_{ft} = W_{base} + W_A W_B\\), we can rewrite the formula as:\n\\(W_{wise} = W_{base} + \u03b1 * W_A W_B\\)\nWe implemented wise-ft by rescaling the ensembled adapter weights \\(W_A\\) and \\(W_B\\) by a factor \\(\\sqrt{\u03b1}\\), so that \\((\\sqrt{\u03b1} * W_A) (\\sqrt{\u03b1} * W_B) = \u03b1 * W_A W_B\\). For each track, we optimized \u03b1 based on leaderboard results and obtained significant improvements for Track 1, Track 3 and Track 5"}, {"title": "Logits Processors", "content": "We employed a variety of logits processors to generate outputs in specific formats. For multiple choice, ranking, and retrieval questions, we constrained our models to produce only digits and commas. For NER tasks, we enhanced the logits of the prompt tokens, encouraging the model to cite directly from the prompt. These logits processors were particularly useful in Phase 1 when we utilized less powerful models. These constrains also were useful in case when the training dataset includes only few task types. For example, you can finetune a model for MC tasks only and successfully apply it for Retrival or Reranking tasks. However, their importance diminished in Phase 2 as we transitioned to larger models that more effectively followed instructions."}, {"title": "Quantization / vLLM", "content": "KDD Cup 2024 was a code competition meaning that we must submit code plus model weights to be run on the host's pre-defined compute resources. Each participant could submit (to each track) a GitLab repository of maximize size 100GB to be executed on 4x NVIDIA T4 GPU each with 16 GB GPU memory within a time constraint.\nThe Qwen2-72B model is about 150GB at fp16. Therefore in order to fit this into disk and memory size constraints, we used 4bit quantization which reduced its size to 40GB.\nQuantization plus using the library vLLM [10] accelerated our inference which allowed our model to answer all the questions within the time limit. Tracks 1-5 had 6102, 1896, 2373, 1349, and 11720 questions to be answered in 70, 20, 30, 20, 140 minutes respectively.\nWe improved AWQ quantization accuracy by calibrating with the 96 development questions. We compared AWQ versus GPTQ quantization and found both to be about equal in speed and accuracy.\nAWQ quantization for Qwen2-72B takes about 1.5 hours on 1xA100 GPU to process. In order for the AWQ quantized Qwen2-72B to work with vLLM, we needed to pad the unquantized model with zeros to change the shape of the weights before quantization."}, {"title": "Results", "content": "Our quantized, fine-tuned Qwen2-72B model achieves the highest score on each individual track (T1 - T4) and overall track T5 with a significant lead of 0.007 to 0.026 to the 2nd place (Table 3). As we placed 1st in each individual track, our final score is 5, the sum of our positions, which is the highest possible score.\nEach submission for the individual track is based on the key concepts of fine-tuning a Qwen2-72B model on our developed training dataset and optionally, ensemble multiple versions and/or apply wise-tf. The submission might differ slightly in the fine-tuning time, exact amount of training dataset and ensemble combination.\nWe provide an ablation study in Table 4, 5 and 6. Some values are missing in the tables due to failed submissions and the successful submissions were sufficient to decide the next experiments. First, table 4 compares different base models without being fine-tuned. We observe that Qwen2-72B has the highest score except of for Track 2, followed by LLaMa3-70B is 2nd place except of Track"}, {"title": "Conclusion", "content": "The KDD Cup 2024 was a great competition with a diverse set of tasks to evaluate Large Language Models capabilities in the domain of online shopping. The code competition design ensured a fair comparison of solutions. Our team solution is a single models with multiple optimization methods, which scored the 1st place on each track. It was essential to fine-tune a base model with an additional training dataset. The lack of an official training dataset was compensated by processing multiple public datasets and prompting Large Language Models. We ensembled multiple LoRA adapater, applied wise-ft for distribution shift and constrained the model output with a Logits Processors. We optimized inference with 4-bit quantization and vLLM to run a 72 billion parameters model on 4x NVIDIA T4 with each 16 GB GPU memory in the time constrain. In addition, we share multiple experiments as an ablation study."}, {"title": "Details on Training Dataset", "content": "In Table 7, we provide an overview of the different datasets we generated. We share which dataset was used as a source input. We describe which task the resulting dataset is most similar to (column Task), the size and if a LLM was used. Finally, we provide additional explanation for our own ideas."}]}