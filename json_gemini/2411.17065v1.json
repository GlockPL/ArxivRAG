{"title": "Creative Agents: Simulating the Systems Model of Creativity with Generative Agents", "authors": ["Naomi Imasato", "Kazuki Miyazawa", "Takayuki Nagai", "Takato Horii"], "abstract": "With the growing popularity of generative AI for images, video, and music, we witnessed models rapidly improve in quality and performance. However, not much attention is paid towards enabling AI's ability to \"be creative\". In this study, we implemented and simulated the systems model of creativity (proposed by Csikszentmihalyi) using virtual agents utilizing large language models (LLMs) and text prompts. For comparison, the simulations were conducted with the \"virtual artists\" being: 1)isolated and 2)placed in a multi-agent system. Both scenarios were compared by analyzing the variations and overall \"creativity\" in the generated artifacts (measured via a user study and LLM). Our results suggest that the generative agents may perform better in the framework of the systems model of creativity.", "sections": [{"title": "1 Introduction", "content": "As the use of artificial intelligence (AI) models and solutions is becoming common, researchers and AI enthusiasts have been working on maximally utilizing the definition of \"Intelligence\" in AI. There is an increasing interest in exploring a machine capability for solving problems that are not well-defined, which requires being creative.\nThe definition of creativity varies across different sources of literature, but two factors that are consistently referred to are novelty and value, both of which are difficult to define in a vacuum, as novelty refers to something being presented for the first time, and value refers to the importance or relevance that it holds [Ama96, RJ12]. The complexity in defining novelty and value lies in the fact that both imply a collective meaning. To determine novelty, the relative creations must be known, as well as to whom they were presented to. In practice, the public relies on the information and commentaries propagated by specialists. For example, if an art curator indicates that a certain painting is credibly unique and has a high market value, we are compelled to believe it. Artists seeking to create new artwork may also rely on the implications of this aforementioned curator to remain updated with the current standards of creativity, thus have a higher likelihood of producing a successful artwork. This dynamic was described in the work of Csikszentmihalyi. He proposed a systems model of creativity [Csi15] in which the society is partitioned into three main groups. Each part plays a role in determining and shaping the standards for what is considered \"novel\" and \"valuable\"; namely, \"creative\"."}, {"title": "2 Related works", "content": null}, {"title": "2.1 Creativity", "content": "A fundamental problem in the field of computational creativity is defining creativity. Researchers have used different approaches to define creativity. Multiple studies by Boden are often used as references to define creativity [Bod04, Bod09]. Boden often defined a hypothetical space of ideas, the process of forming new ideas in this space, and whether these new ideas indicate a transformation in the space of concern. Despite its popularity, Boden's definition poses a major obstacle for computational implementation, as it requires high levels of abstraction to represent all the \"ideas\" in a space. Conversely, Csikszentmihalyi designed a systems model of creativity, which proposes that creativity is product of social interactions and transforms overtime as part of societal change [Csi15]. Unlike Boden's definition, Csikszentmihalyi's approach considers the process of making something \u201ccreative\u201d as a product of the interactions between multiple parts, which can be more easily achieved with the current technology.\nWe based our study on the definition of Csikszentmihalyi's systems model of creativity; therefore, in this study, we will not focus on determining the creativity in a single individual, rather observing the impact of social interactions on \"creativity\" in a virtual environment with multiple generative agents.\nThe systems model of creativity features individuals who create artifacts (i.e. artists), the community to whom the artifact is being displayed, and the context in which the artifact was created (e.g. location, historical events, significant pieces of art, recent trends and political climate). Each part of the system that does not live in a vacuum is constantly affecting the others and vice-versa: the artist does not create art without inspiration or motivation from the community or context they are in, the community cannot exist without multiple individuals contributing to the domain with new artifacts, and the domain cannot change without the creations of the artists or the judgment of the community. All the variables form a cycle in which each part of the system contributes to the system itself."}, {"title": "2.2 Creativity and AI", "content": "Researchers in the field of computational creativity have attempted to develop a computer program capable of being creative, such as the Joke Analysis and Production Engine (Jape) [Bin96]. It was developed at a time where AI was at its early stages of development, and was designed to create puns. Humor heavily relies on creativity, considering its surprising aspect that is found by many. The author used exploratory programming, which indicates that the program was used to explore and test ideas. They designed a formal model for punning riddles based on patterns observed in the structures and mechanisms of the indicated riddles; this formal model was then used to implement Jape. The riddles that were used as a base for the analysis were obtained from children's books that contained a collection of jokes. Despite the author claiming that there were \"a huge number of riddles to choose from\", which may have been the case at that time, three books is relatively limited for the current standards of a dataset. To implement the model, they stored the information regarding the phonological, semantic, syntactic and surface forms (appearance, spelling) of the words, as well as the relationship between these lexical items. These were then applied to templates, where the relationships were properly placed in the form of punning riddles. Although this is a robust model for generating punning riddles, which is better than merely picking words to fill a riddle template, the use of these templates restricts and limits the possibilities of outputs from this model. Moreover, significantly more effort is required to prepare the program for creating new riddles compared to that when using the modern generative models.\nGenerative adversarial networks (GANS) [GPAM+14] were once a popular choice for computational creativity owing to the latent space created during training (alluding to Boden's hypothetical \"space of ideas\"). GANs are formed by using a generator and discriminator model, where both parts are trained to outdo the other (hence, \"adversarial\"); DesIGN is an example of this model [SEB+18]. The authors proposed StyleGAN, an adapted GAN model where the generator is conditioned by the texture and the shape of the articles of clothing while being \"encouraged\" to generate more novel samples, as opposed to replicating the samples observed in training, as that performed by the classic GAN. This \"encouragement\" was achieved by adding two loss functions that \"confuse\" the discriminator in the GAN. However, GANs are notoriously difficult to train, as both the discriminator and generator should ideally perform sufficiently; however, achieving a balanced training process is challenging.\nTransformer-based models have recently become a staple of generative models [Vas17, HVU+18, RWC+19]. Considering the development of transformers, researchers have proposed a method for controlled text generation where the user can specify \"tags\" or \"keywords\" to manipulate the probability distribution of the next word to be sampled by an autoregressive model (GPT-2). For example, the Plug and Play Language Model (PPLM) [DML+19], successfully controlled the generation of text sequences, where a significant portion of the outputs followed the \"tag\" or \"keyword\" specified without compromising the output coherence or grammatical accuracy. Another noteworthy experiment conducted by the authors utilized more than one \"tag\" or \"keyword\" for the controlled text generation where the control terms were unlikely pairs. Although the authors did not assess the creative ability of this method, they shared some noteworthy results obtained by combining different themes and concepts. Compared to Jape, the model imposed significantly milder constraints and limits, and the outputs were based on a large volume of written information sourced from the Web. The limit is based on the ability of the model to tie two or more concepts together when they are significantly different from one another. We successfully used this controlled generation method in music generation [IMDN23], where the autoregressive model was trained to generate sequences of MIDI events instead of text. Control was achieved using a part of the PPLM method, where we used an auxiliary discriminator model to classify the data into two or more categories. In this case, the emotion perceived from a piece of music was used as the main factor (categorized according to Russell's model of affect). Analogous to the original publication of PPLM, using unlikely pairs of controls and prompts can lead to noteworthy (or creative) outputs.\nAssessing or measuring the creativity of an artifact is one of the hardest (if not the hardest) problems in this field. As humans, we visualize or hear a certain type of art or music, respectively, and almost immediately make judgments based on the first impressions and our preferences. If desired, we also seek for context, debate and further discussions with other people regarding whether the art or music of concern can be considered creative. However, if asked to define measurements or objective values to determine the creativity of an artifact, the reply is significantly less trivial and more complex than determining whether something or someone is creative. In an experiment, the authors of Jape considered 122 children who read jokes (generated by Jape and humans) and non-jokes (sensible and nonsensible ones). The texts generated by Jape performed almost as well as the human-made jokes compared to the non-joke texts [Bin96]. To assess the creativity and funniness of a punning riddle, the authors resorted to the decisions made by a group of individuals who participated in their experiment; as the judgment of whether something is creative or funny is highly subjective, this is a significantly common practice among the different works in the field. Certain objective measurements can be considered, such as the choice of words for poetry, color palettes for images, among others. However these metrics do not determine creativity; in fact, many artifacts can have good values for these metrics and remain to be considered non-creative, similar to one with poor values in the same metrics being considered highly creative. In this regard, the quality of computationally generated artifacts can be easily measured with objective metrics; however measuring their creativity is significantly more complex given the subjectivity of the matter."}, {"title": "2.3 Human-likeness in AI models", "content": "Modelling human behavior to better understand the mechanisms and dynamics in our society is a major goal of AI research. In this regard, large language models (LLMs) integrated in chat bots, such as GPT-4 integrated in ChatGPT [O+24], are among the most notable developments in the field. The quality of outputs obtained by ChatGPT users is significantly high, leading to discussions regarding Artificial General Intelligence (AGI) and ethics of the training and usage of these AI models and systems.\nA recent study used LLMs to simulate individual agents in a small community, where each agent was defined by text excerpts used to prompt text generation for their actions and utterances [POC+23]. The authors also conducted a study to assess the human-likeness of the agents, as well as the generation of actions and phrases by LLMs that appeared natural to most participants. Additionally, when an agent was initialized with a plan to host an event for the community, the agent invited other agents to the event, who did the same and invited other agents.\nAnother study used a similar principle but maintained the LLM requests to a minimum, thus required a lower demand of resources [KNK+23]. The authors also used different \"situations\" where the agents would have specific problems to solve to evaluate their performance. These studies indicate that LLM-based agents can fulfill their roles in a human-like manner and successfully coordinate among themselves as a community.\nAs LLMs were successfully used in simulating \"individuals\" in both of the aforementioned studies, we hypothesized that LLM-based agents can be used to run simulations of the Csikszentmihalyi's systems model of creativity [Csi15]."}, {"title": "3 Method", "content": "In this study, we designed, implemented and tested a simplified simulation of the systems model of creativity, as proposed by Csikszentmihalyi [Csi15]. In this section, we describe the overall structure of our system and the decisions made during its implementation."}, {"title": "3.1 The Systems Model of Creativity", "content": "According to Csikszentmihalyi [Csi15], creativity does not originate from an object or person in isolation; rather, \"it is the product of three main shaping forces: a set of social institutions, or field, that selects from the variations produced by individuals those that are worth preserving; a stable cultural domain that will preserve and transmit the selected new ideas or forms to the following generations; and finally the individual, who brings about some change in the domain, a change that the field, will consider to be creative. (...) Creativity is a phenomenon that results from the interaction between these three systems.\". Based on this description, three essential elements must be considered in a systems model of creativity: the creative individual (or simply individual), field and domain. Note, at the end of the aforementioned passage, the author refers to each part as a \u201csystem\", as each part is highly complex on its own.\nThe creative individual would not be the same if it was not for the context they are in. This includes (but is not limited to) the political, historical, and cultural contexts they are surround by. Although people have their own individuality, personality, and experiences, their surroundings play a major role in shaping their opinions and values. Despite using text, describing each detail of an individual and \"modeling\" them would be practically impossible. A single individual is difficult to model precisely. Modeling the field and domain is an even more complicated task, as they are formed and affected by multiple agents, each with their own complexities."}, {"title": "3.2 Our design", "content": "Given that the system of creativity in the real world is significantly complex, and precisely reproducing it in a virtual setting is impossible, we decided to simplify the model originally proposed in a previous reference [Csi15].\nWe considered three fundamental subsystems: artist (individual), field, and domain. Each part operates using a combination of generative models and text prompts. We further describe each subsystem as follows. An overview of our system is shown in Figure 1, and the pseudo-code for our simulation can be found in Algorithm Block 1.\nWe used Gemini Pro version 1.5 [G+24] for text generation and multimodal text generation tasks. We refer to Gemini as the LLM in the sections below because theoretically, any LLM can be used for this system. Stable Diffusion version 1.5 [RBL+22] was used to visually express the agent creations and generate images from text prompts."}, {"title": "3.2.1 Artist", "content": "Considering a simple description, the artist is responsible for creating art and contributing to the domain with new pieces of art (contributing novelty). Despite having the same common goal of creating art, each artist has their own individuality and creates art in their own way (examples of these differences affecting our system can be found in Appendix 9).\nAnalogous to a study that simulated agents in a village [POC+23], we also described the agent artist in text form, which was used to prompt text generation. The artist agent was initialized with a \"core description\", which was manually expressed. For example, we can write the main motives and inspirations of the agent. An individual usually has their own set of \"principles\" that define them and are rarely changed. These aspects were expressed for the \"core description\" of our agent. However, regardless of these principles, certain (less deep) aspects of an individual can change over-time as they interact with their surroundings. To describe these subtle changes that may occur overtime, we maintained a log of \"additional text descriptions\" that were generated after the agent \"reflected\" on the feedback their art received.\nWe expected a high rate of \"change\", as the artists in our experiments were defined as young art students who did not possess artistic talent but wished to become successful artists. The artist agents were defined such that they would be more susceptible to \"accepting\" advice in the feedback.\nLastly, the artist agent was also equipped with a text-to-image model that would generate the \"piece of art\" that it intends to create."}, {"title": "3.2.2 Field", "content": "The field is represented by individuals, such as experts and scholars, who have a certain degree of authority in the area of interest. The individuals in the field act as a type of filter (\u201cgatekeeper\u201d), where they are responsible for selecting the pieces of art that are significant to theirs and the domain's current status. Their decisions help update and shape the trends and creative climate while performing \"quality control\", which ultimately affects the domain.\nTo simulate this mechanism in our system, we implemented each field agent with its individual description (similar to an artist's implementation). To \"assess\" and \"evaluate\" the artifacts generated by the artists, a multimodal LLM that can generate text from an input that is comprised of both text and image was used. Each field agent in the system generated a critique for each new artifact generated during the time-step in question."}, {"title": "3.2.3 Domain", "content": "All the agents in a creative system coexist and share a space, which is not only a physical space, but also a historical, political, and cultural context, among other variables. The combination of all the different \"circumstances\" as well as the time and place, is referred to as the domain. In the same manner that individuals transform their surrounding environment, the surroundings can also cause changes within the said individuals. Examples of the domain impact on the agents are found in Appendix 9.\nTo implement the domain in our system, we prepared a base text description that was used throughout the simulation, in which we provided the necessary (or known) information regarding the domain we intended to simulate. In addition, to better focus on the artistic aspect of the domain, we initialize the domain with a list of \"significant\" paintings and their descriptions to use as reference as the most significant paintings at time-step t = 0. For simplicity and convenience, we initialized the list with popular paintings that are currently regarded as significant. This list was maintained throughout the simulation and at each time-step, it was updated with the newly generated artifacts as well as the text prompt used to generate them (their descriptions). The paintings in the list were ranked according to the impressions obtained from the field agent(s). The ranking mechanics are detailed in the next section.\nEach time the domain needed to be referenced, the top 3 most significant artworks were picked from the ranking and three keywords were obtained from each of their descriptions (nine keywords total). The keywords were selected using an LLM, where the text prompt explicitly instructed the model to provide keywords that can describe the painting without referencing the names of famous artists or paintings. In the early stages of our experiments, we found that the names of famous paintings and artists heavily influenced both the text and image generation."}, {"title": "3.2.4 The system dynamics", "content": "At time-step t = 0, all parts of the system were initialized with their respective base descriptions that were manually written. The domain was also initialized with a collection of real paintings along with their descriptions. These paintings were considered the most significant artworks at the beginning of the simulation.\nOnce all parts are initialized, the artist agents \"create\" their paintings. The process starts by building a text prompt from the base descriptions of the artist and domain. The prompt is constructed as follows:\n[domain descrip.] + [artist descrip.] + ''This young art student just finished his latest painting.\nProvide a brief but detailed description of what is depicted in the canvas.''\nAfter feeding this prompt to the LLM, we generated a text that described the latest artwork of the agent (\"art prompt\"). The final step of the \"creation\" was obtaining an image of the artwork created by each artist agent, which was achieved by feeding the art prompt into a text-to-image generative model.\nThe art prompts and their respective generated images were then passed to the next step, with the field agents (referred to as \"critics\" hereafter). To evaluate the new artwork, we used a multimodal LLM, as it can generate text from an input comprising both text and images. The text input was built as follows:\n[domain descrip.] + [critic descrip.] + ''The student made this painting. This is how the student described his artwork: + [art prompt] + ''Was the student able to convey his intentions? Do you think this painting is creative? Briefly explain why.''\nPerforming the aforementioned enables, the LLM to generate what each critic is more likely to \"say\" regarding each new artwork based on their description. The final product of this step is a collection of critiques obtained from all the critics in this system.\nWhen adding the newly \"created\" artworks to the domain ranking, we considered the critiques that each artifact received. If the artifact received a positive critique, the artwork earned a \"significance point\"; however, if the critique was negative, the artwork did not earn any points. The sentiment assessment of the critiques was performed by an LLM pretrained for sentiment analysis (using DistilBERT-base-uncased finetuned SST-2, made available as the default model for sentiment analysis on HuggingFace's library) [SDCW19]. All artworks added at initially start with one significance point because they are meant to represent the most significant artworks at the very beginning of the simulation. Additionally, at each iteration, critics go over the list of all the artworks to reconsider the significance of the artwork. Similar to the newly added artifacts, the artworks that already exist in the history of this simulation are awarded one significance point if deemed \"significant\" by the critic, and no points otherwise. The sum of the points awarded at each time-step was maintained along with the ranking. We also applied a decay to these \"significance scores\", where the sum was reduced by half every d time-steps. The decay was enforced thus the considerations made at the earlier time-steps were not as valuable as those more recent. This mechanism was utilized to grossly emulate the effects of changes in the trends, where one style can lose its appreciation over time as another style replaces it. Thus, the more recent considerations of the critics have a better chance of employing changes in the domain.\nThe critiques were also returned to the artists, where each agent underwent a process of \"self-reflection\", which occurred via another text generation that was prompted with the following:\n[domain descrip.] + ''The art student made a piece of art that was described as:'' + [art prompt] + ''The art teacher made the following comment about this artwork:'' + [critique] + [artist descrip.] ''How do you react to this feedback? Briefly describe what actions you will take next.''\nThe output was considered as the products of the artist's self-reflections and added to the [artist descrip.] as additional information (the \"additional text description\" previously indicated). This \"self-reflection\" step was performed at the end of each iteration, which yielded a new piece of text describing this agent; usage of all the text was desired to describe the agent. To prevent reaching the limit of tokens, we considered all the additional text descriptions and summarized them with an LLM, more specifically, a fine-tuned version of t5-small [RSR+20].\nThese steps were then repeated n times, where the descriptions of the artists and the ranking of significant artworks underwent changes according to the text generated at each time step."}, {"title": "4 Experiments", "content": "This section presents the simulations that were performed in our experiments. To validate the usage of this framework to simulate the emergence of creativity in a social setting, we ran the \"creative cycle\" in two situations: isolated artist and artist in system.\nSimulations of the artist in system were performed as described in the previous section. Simulations for isolated artists were performed such that the artist agent was aware of the domain at the beginning of the simulation but did not interact with any part of the system. The \"creation\" process occurred as described in the previous section; however, the critic did not propagate the feedback, indicating that both the artist and domain remain unchanged from the beginning to the end of the simulation. In the isolated case, the generated critiques were not used in the system itself but were retained for comparison with the critiques generated in system.\nBoth conditions were run in four separate simulations, with each run consisting of 15 iterations. Each"}, {"title": "5 Results", "content": "Herein, we demonstrate the results obtained from the simulations described in the previous section. Furthermore, the noteworthy artifacts that were generated are presented, and the results of the simulations are compared with isolated artist and with artist in system."}, {"title": "5.1 Variety", "content": "Over all the simulations, we generated 120 art prompts for the artists in isolation and 120 art prompts for the artists in system. To measure the variety of content in the art prompts in each group, we used Sentence Transformers [RG19]. The similarity was measured from 0.0 to 1.0. The average similarity from the art prompts generated by the artists in isolation was 0.7892\u00b10.06, whereas that generated by the artists in system was 0.7638\u00b10.07. Figure 3 suggests subtle differences in the similarity scores when observing the art prompts generated by one artist at a time. Despite the similarity values being on the higher end (red shades) in both cases overall, the similarity values were relatively lower (blue hues) at certain time-steps for artists in system.\nAmong the 8 artist agents that \"created\" artwork in isolation, one of the artists was noteworthy because for 14 of the 15 iterations, it generated \"art prompts\" that were nearly the same theme. At nearly all time-steps, the artist used a dragon as the main subject of the painting. The second artist in the same simulation (and all other artist agents in other simulations) did not repeat themes in the same manner. Unfortunately, we were unable to further investigate this, as we cannot control the randomness of the LLM, thus making it impossible to reproduce. Despite being unable to implicate this occurrence with the fact that the artist was not \"creating\" artwork in a system, it was noteworthy to highlight."}, {"title": "5.2 User study", "content": "A user study was conducted to collect subjective assessments from 100 participants. The questionnaire was built using SurveyMonkey2 and distributed to participants using Prolific\u00b3. This study aimed to verify which artifacts were considered \"more creative\" by the participants.\nWe allocated all 16 agents (8 isolated artists and 8 artists in system) in 4 different questionnaires. Each"}, {"title": "5.2.1 Demographic information", "content": "At the end of the questionnaire, we asked the participants to respond to a few demographic questions. Most questions included options for those who did not want to provide a response.\nThe age group ranging from 21-29 years (64 individuals) was the most representative from our pool of participants, followed by 30-39 (22 individuals), 40-49 (7 individuals), 50-59 (4 individuals) and 18-20 (3 individuals). Furthermore, the number of participants were identified as follows: 45 females, 53 males, 1 agender, and 1 gender-fluid.\nAs all the instructions and captions in our study were in English, we asked the participants to refrain from completing the questionnaire if they were not fluent in English. We chose not to geographically restrict the target of our study, as we prioritized the diversity of our participants. Among the participants, 44 reported that they resided in Europe, 26 in Africa, 25 in the Americas, 3 in Asia, and 2 in Oceania.\nThe participants were asked if they were familiar with machine learning and reported the following: 34 claimed to have limited knowledge, 24 claimed they were familiar with it and knew how it generally works, 12 claimed they were familiar with it and had experience using or implementing it, 19 claimed to have heard of it but did not know how it works, and 11 indicated that they have never heard of it. We also asked the participants how often they used AI tools and applications and reported the following: 12 claimed not using it at all, 23 used it very rarely, 38 used it occasionally, and 27 used it often."}, {"title": "5.2.2 Analysis", "content": "Situation 1, in which the questions were aimed to compare the artifacts generated by the same agent at different time-steps, is considered here first. Based on the questions, we expected to find that artists in system would tend to receive more votes for artifacts generated at later time-steps (after receiving multiple critiques from the mentor). For the artist in isolation, we expected the votes to be more evenly distributed across earlier and later time-steps. A total of 1200 votes were obtained, among which for isolated artists, the artifacts generated at the 1st, 5th, 10th and 15th time-steps received 286, 242, 351, and 321 votes, respectively. For the artists in system, the artifacts received 278, 242, 390, and 290 votes for the 1st, 5th, 10th and 15th time-steps, respectively. The distribution of the votes is shown in Figure 6a. In both cases, the participants appeared to have a preference for the artifacts generated in later time-steps when asked which \"artwork\" they considered more creative. Moreover, the artifacts that received the most votes were generated at the 10th time-step, followed by those generated at the 15th (both representing the later time-steps), 1st, and 5th time-steps, respectively. At first, the differences between artists in isolation and those in system when comparing the artifacts generated by a given artist agent.\nSecond, Situation 2 describes the questions that compare the artifacts generated by different agents at the same time-step, where the agents being compared were always an isolated artist and an artist in system. In this case, the artifacts generated by the isolated artists demonstrated 376 votes (97, 96, 78, and 105 for 1st, 5th, 10th, and 15th time-steps, respectively), whereas those generated by artists in system had a total of 424 votes (103, 104, 122, and 95 for the 1st, 5th, 10th, and 15th time-steps, respectively). Figure 6b clearly shows the distribution of these votes across the different time-steps. While the votes for the isolated artists were nearly split in half between the earlier (1st and 5th) and later (10th and 15th) time-steps, the votes for artists in system were slightly more skewed towards the latter half of the simulation.\nThe fact that we used separate models to generate art prompts and images is a limitation of our study. Consequently, images do not always reflect the text in art prompts. Because our system was mostly focused on the social aspect of creativity and how different parts of a society change as they interact, we decided to prioritize the generated text over the images. To help us investigate this perspective in the user study, we asked the participants whether the captions under the images affected their decisions, obtaining the following answers: 18, 42, 25, 9, and 6 participants said \"not at all\", \"a little\", \"a moderate amount\", \"a lot\" and \"a great deal\", respectively. For a better understanding of the answers provided by the participants who considered the captions, we filtered those who answered \"a moderate amount\", \"a lot\" and \"a great deal\" when asked if the captions influenced their decisions.\nConsidering the responses given in Situation 1 with the filtered population, the isolated artists received 119, 103, 134, and 124 votes for the artifacts generated at the 1st, 5th, 10th, and 15th time-steps, respectively. Comparatively, the artifacts generated by artists in system received 99, 103, 158, and 120 votes for the 1st, 5th, 10th, and 15th time-steps, respectively (Figure 7a). Note the artifacts generated by the artists in system in the second half of the simulation (time-steps 10 and 15) received a considerably larger number of votes than the artifacts generated in the earlier time-steps. This \"preference\" was not as prominent for the artifacts generated by the isolated artists, suggesting that the system framework encouraged the artist agent to \"evolve\" and \"improve\" creatively. We conducted a Barnard exact test to verify our hypothesis of whether the artifacts generated by the agent in the system in the second half of the simulation would be significantly greater than those generated by the isolated agent. However, the difference was not statistically significant (p-value = 0.1 > 0.05).\nIn Situation 2, the isolated artists received a total of 171 votes (45, 44, 37, and 45 for the 1st, 5th, 10th, and 15th time-steps, respectively), and the artists in system received a total of 149 votes (35, 36, 43, and 35 for the 1st, 5th, 10th, and 15th time-steps, respectively). Overall, the artifacts generated by the artists in system received less votes than those generated by the isolated artists; however, the distribution of votes (Figure 7b) continued to support that the agents in system were apparently \"improving\" overtime."}, {"title": "5.3 Grading with LLM", "content": "We used Gemini 1.5 Flash to score each artifact on its creativity. For each artist agent, we patched all images next to one another, where the first and last artifacts were placed on leftmost and rightmost sides of the image, respectively. For the text prompt, we enumerated all the corresponding art prompts in the order of generation. The text prompts were as follows:"}, {"title": "6 Discussion", "content": "The currently implemented system is a simplified version of the systems model of creativity. We chose to grossly simplify our initial implementation to enable a better understanding of the evolvement of the agents and domain overtime and to more easily identify any implementation mistakes. In the early stages of our study, we made the conscious choice of maintaining a minimalistic system design, enabling an easier navigation of the simulations. Despite its simple design, we believe the results suggest the following: the systems model of creativity can benefit \"computational creativity\".\nAnother issue we had to overcome in the implementation was the inability to use the same generative model for both text and image generation, as access to these models via API is limited to those who have been granted special permission by major AI companies. Therefore, we used Gemini for text generation and Stable Diffusion for image generation, as it is easily accessible through Hugging Face's Transformers library. Although the current version of Stable Diffusion may be robust, it had certain limitations when generating images, especially from long text prompts. The art prompts generated in this study were often very descriptive of the pieces the agents were \u201ccreating\u201d, as this is what we tasked the LLM in our prompts. However, many details were omitted from the generated image. Depending on the subject descriptions of the \"creations\" in the art prompts, Stable Diffusion occasionally omitted the main subject in the image. We anticipated certain limitations in the text-to-image model, especially given that it can be used locally; therefore, we excluded the participants who indicated that the text did not (or did little to) influence their decisions in the user study.\nIn this study, the artist agents were defined as young novices in arts by design to encourage more changes via the mentor feedback. Consequently, we expected that the artist agents in the system perform creatively better than the isolated artist agents. However, an artist (or any individual) is significantly more complex than that and can be defined in several ways. In future studies, we would like to examine the impact of adding more variations in the agents on contributing creative outputs. Furthermore, considering that the agents were fairly restricted in the current implementation, we intend to implement the ability for all agents to directly interact with one another in future studies, thus anticipating a more evident effect of social dynamics and trends."}, {"title": "7 Conclusion", "content": "Despite the simplified design, the artists in system apparently improved their \"creations\" overtime"}]}