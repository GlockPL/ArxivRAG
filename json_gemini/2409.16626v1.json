{"title": "Ascend HiFloat8 Format for Deep Learning", "authors": ["Yuanyong Luo", "Zhongxing Zhang", "Richard Wu", "Hu Liu", "Ying Jin", "Kai Zheng", "Minmin Wang", "Zhanying He", "Guipeng Hu", "Luyao Chen", "Tianchi Hu", "Junsong Wang", "Minqi Chen", "Mikhaylov Dmitry", "Korviakov Vladimir", "Bobrin Maxim", "Yuhao Hu", "Guanfu Chen", "Zeyi Huang"], "abstract": "This preliminary white paper proposes a novel 8-bit floating-point data format HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered precision. For normal value encoding, it provides 7 exponents with 3-bit mantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit mantissa. For denormal or subnormal value encoding, it extends the dynamic range by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades). Meanwhile, HiF8 encodes all the special values except that positive zero and negative zero are represented by only one bit-pattern. Thanks to the better balance between precision and dynamic range, HiF8 can be simultaneously used in both forward and backward passes of AI training. In this paper, we will describe the definition and rounding methods of HiF8, as well as the tentative training and inference solutions. To demonstrate the efficacy of HiF8 format, massive simulation results on various neural networks, including traditional neural networks and large language models (LLMs), will also be presented.", "sections": [{"title": "1 Introduction", "content": "In 2020, several of our top chip architects foresaw that as Moore's Law [1] slows down, low-precision training and inference would be an important way to reduce the computing power and mitigate the memory wall [2] for AI hardware. Thus in 2021, HiSilicon launched the HiFloat project, aiming to study and develop novel low-precision data formats for our AI products. Subsequently, this project attracted many researchers from other departments to join. In this paper, we will officially disclose some of our research achievements on Float8.\nFirst, let's take a brief look at history. Generally, the development of AI data formats can be divided into the following four phases based on the time for commercial use:\nPhase 1 (1959 - 2006): FP64. The concept of AI was first proposed in 1956. Then, several theoretical prototypes and directions were established and developed, including backpropagation algorithm [3], convolutional neural network (CNN) [4], and long short-term memory [5]. During this period, CPUs were the major AI hardware, and the mainstream data format was the 64-bit double precision floating-point format FP64 defined in IEEE Standard for Floating-Point Arithmetic [6] in 1985. Prior to this, some predecessors of FP64 were in use.\nPhase 2 (2006 - now): FP32. In 2006, FP32 was used for the first time to train CNNs on GPUs and achieved a four-fold performance improvement compared with FP64 training on CPUs [7]. The bit width of FP32 is only half that of FP64. Therefore, a chip that uses the FP32 format can store more data and integrate more multiply-accumulate (MAC) units than one that uses FP64. The single instruction, multiple data (SIMD) computing mode enables GPUs to have a significantly higher degree of computing parallelism than CPUs [8]. Finally, AlexNet [9] which ran on two GPUs won the championship of ILSVRC 2012, making FP32 the mainstream data format for deep learning training from 2012 to 2017.\nPhase 3 (2017 - now): Float16 mixed precision. In 2016, Google proposed a novel Float16 data format in its TensorFlow white paper [10]. This format was equivalent to FP32 without the last 16 bits and was used as the input data format of multiplication, whereas FP32 was used for accumulation in the general matrix-matrix multiplication (GEMM). This format was later named Brain Floating Point 16 (BF16) [11] and deployed on Tensor Processing Unit (TPU) [12] V2 and V3 . In 2017, Google's AlphaGo [13] powered by TPU V2 defeated the world's number one Go player Ke Jie in all three games, causing a global sensation. Unlike Google, NVIDIA and Huawei adopt the IEEE 754 half-precision data format [14]. NVIDIA released the V100 GPU [15] in 2017 and Huawei shipped the Ascend NPU [16] in 2019. Both the GPU and NPU support the FP16 and FP32 mixed precision training strategy, with backward global loss-scaling to prevent excessive zero-valued activation gradients caused by narrow dynamic range of FP16 [17]. Since 2017, the Float16 mixed precision training solution, mainly consisting of BF16 and FP16, has become the main choices for deep learning. However, FP32 is still used for some networks that require high precision.\nPhase 4 (2022 - now): Float8 mixed precision. In 2018, IBM radically cut off the last 8 bits of FP16 to obtain a Float8 data format with 1 sign bit, 5 exponent bits, and 2 mantissa bits (E5M2) [18]. However, on MobileNetV2 and Transformer networks, the accuracy of E5M2 training decreased dramatically. Then in 2019, IBM further proposed the 8-bit hybrid FP8 (HFP8) training solution, with E4M3 (a Float8 format that has 1 sign bit, 4 exponent bits, and 3 mantissa bits) as the weights and activations format and E5M2 as the activation gradients format [19]. Thus, when calculating gradients in the backward pass, GEMM that supports mixed Float8 inputs is needed, and HFP8 is therefore called FP9 in hardware implementation [20]. In 2022, NVIDIA deployed HFP8 (renamed as FP8) mixed precision solution with twice the computing power of FP16 on its new H100 GPU, and shipped by the end of the year [21]. Subsequently, Intel, ARM, AMD, and Quanlcomm et al., announced that they would also support this solution [22, 23].\nFrom a brief historical review, we can see that low-precision training [24] has always been an important direction to improve Al performance. And the commercial use of Float8 mixed precision has begun. However, it is also important to note that the trade-off between precision and dynamic range is challenging in the evolution from Float16 to Float8. For Float16, one fixed field-width format (either BF16 or FP16), could be elegant to cover all GEMM inputs, and works well for almost all scenarios. But this failed in Float8 mixed precision. In addtion to the research for fixed field-width formats, Posit [25] is a decent exploration for tapered precision data type, because it matches the centralized characteristic of AI data distribution during training and inference. Unfortunately, Posit16 failed in the competition for Float16 mixed precision, due to its larger hardware cost than BF16 and FP16, and insignificant precision improvement for training [26]. And Posit8 also failed in the competition for Float8 mixed precision, because its encoding method cannot balance the precision and dynamic range very well to meet the training requirement [27, 28].\nInspired by FP16, Posit, and HFP8/FP8, this paper proposes a novel 8-bit floating point format HiF8 for deep learning, which features the better balance between precision and dynamic range compared with the existing Float8 formats, and can be simultaneously used in both forward and backward passes for AI training. A large number of simulation results would be presented to illustrate the advantages of HiF8 in this paper."}, {"title": "2 HiFloat8", "content": "This section first describes the definition of the novel 8-bit floating-point data format HiF8, including the support for special values. Then, some consideration and design issues for HiF8 will be explained."}, {"title": "2.1 Novel Data Format", "content": "We propose a new general-purpose floating-point enconding and decoding method for data expression, for which the field width, dynamic range, and significand precision can be scaled based on scenario requirements. This paper focuses on the 8-bit floating-point instance for deep learning usage. On the basis of the IEEE 754 [14], HiF8 defines an additional dot field. Therefore, HiF8 consists of the four fields as listed in Table 1: a sign field, a dot field, an exponent field, and a mantissa field.\nThe following describes each field in detail:\n\u2022 Sign Filed: 1 bit, determining the sign of the HiF8 number, which is the sign of the significand as well. By default, 1 indicates the negative sign and 0 indicates the positive sign.\n\u2022 Dot Field: 2 to 4 bits, used to code the five D values (0 to 4) and the sign of DenorMaL (DML). D value explicitly indicates the number of bits occupied by the exponent field, and implies the number of bits occupied by the mantissa field. DML sign specifies that the HiF8 number has no exponent field, and needs to be parsed by denormal equation (2). Otherwise, NorMaL (NML) equation (1) should be used. The dot field is coded using unconventional prefix codes, that is, the 4-bit width is used for coding small value 0 and DML sign, the 3-bit width is used for coding medium value 1, whereas the 2-bit width is used for coding large values 2, 3, and 4.\n\u2022 Exponent Field: D bits (an implicit leading magnitude bit with value 1 unstored), where D is equal to the coded value of the dot field and $D \\in \\{0,1,2,3, 4\\}$. Different from the offset-binary method used in the IEEE 754, the exponent field of HiF8 uses sign-magnitude code to represent values. But the most significant bit (MSB) of the magnitude is fixed to 1. Mark the sign of exponent as $S_e$. By default, 1 means negative sign and 0 means positive sign. Then denote the complete sign-maginitude exponent as $E_i$ in binary mode, we have:\n$E_i = \\{S_e, Mag[1 : end] \\} = \\{S_e, 1, Mag[2 : end]\\}$\nSince the fixed MSB of the magnitude can be hidden, $E_i$ is simplified as $E_m$ in the memory format:\n$E_m = \\{S_e, Mag[2: end]\\}$\nThe number of bits in $E_m$ equals the value of D. When D is zero, $E_m$ dose not occupy any bit width, indicating that the exponent value equals zero.\n\u2022 Mantissa Field: 1 to 3 bits (an implicit leading significand bit with value 1 unstored), coded by unsigned integer. For the normal number of HiF8, mantissa represents the fractional bits (to the right of the binary point) in the significand. For the denormal value of HiF8, mantissa represents the extended exponents in a biased form.\nSo far, we have introduced the four fields for HiF8. Table 3 summarizes the corresponding code-value mapping details. In the memory format, dot field is stored as outlined in Table 2. Exponent field is stored as Em with an implicit bit. Em can be further interpreted as $E_i$ in binary mode, and E in decimal mode.\nDenote the sign as S, and the mantissa as M. For the normal number, HiF8 should be interpreted as:\n$X = (-1)^S \\times 2^E \\times 1.M$                                                                   (1)\nIn the normal equation (1), 2 bit-patterns with the largest absolute value ($2^{15} \\times 1.5$) should be interpreted specially for Infinities. For the denormal number, HiF8 should be interpreted as:\n$X = (-1)^S \\times 2^{M-2^3} \\times 1.0$               (2)\nIn the denormal equation (2), $M \\in [1, 7]$, which offers 7 addtional exponent values of [-22, -16]. 2 bit-patterns with M = 0 should be interpreted specially for Zero and NaN (Not a Number)."}, {"title": "2.2 Consideration and Design", "content": "Before the launch of the HiFloat project, we investigated lots of literatures on low-percision training. Finally, three data formats were selected as the references. The first is the FP16 defined by the IEEE 754 [14], with the global backward loss-scaling method [29], this data type trains well for almost all neural networks. The second is the HFP8 [19] proposed by IBM, which is the first 8-bit floating point format for commercial use [21]. The third is the Posit [25] invented by John Gustafson, which features the tapered precision that matches the centralized characteristic of AI data distribution. In the following, we will explain several design considerations for HiF8 and show how those references guide our design.\n\u2022 Dot field\nConsideration: To avoid multi-float8 formats like HFP8 for deep learning, tapered precision is a promissing direction to explore. Posit realizes the tapered precision by adopting a variable-length field called regime to encode two base values of the exponents. However, the regime field using unary coding is not convenient and flexible enough to manipulate the significant bits distribution over the exponent to better match the training requirements.\nDesign: Therefore, HiF8 adopts the flexible prefix code to form a novel dot field, which directly indicates the width of exponent and the sign of denormal. Moreover, to smooth the precision variation and avoid the mantissa width jumping down by more than 1 bit, we use large width to code small numbers and small width to code large numbers. One can obtain an in-depth analysis from Table 1 and Table 2.\n\u2022 Exponent field\nConsideration: To avoid coding redundancy, we must ensure that the exponent values pointed by each value of the dot field do not overlap with each other. Coding methods like the offset-bianry used in FP16 and the special one used in Posit, cannot achieve this goal. Fortunately, there are other three signed number reprentations for us to consider: sign-magnitude, one's complement, and two's complement.\nDesign: Inspired by the implicit bit design of the mantissa field in FP16, HiF8 chooses the sign-magnitude coding for the exponent field, where one implicit bit is fixed to 1 to avoid repetitive representation of the exponent values. As shown in Table 3, HiF8 becomes non-redundant in data expression.\n\u2022 Denormal Mode\nConsideration: Without the denormal design, HiF8 would have a 4-bit mantissa when the exponent is equal to zero, but would only support 31 exponent values from -15 to 15. Currently, the activation gradients of LLMs require a higher dynamic range of the data format [30]. While from the practice of HFP8 or FP8, we can conclude that 3-bit mantissa is sufficient for training tasks. Thus a better balance is possible to extend the dynamic range of HiF8.\nDesign: We reduce the mantissa width from 4 bits to 3 bits when the exponent is equal to zero. Then the resulting free coding spaces are directly used to expand the exponent range, as formulated by the denormal euqation (2) and depicted in Fig. 1. In this way, the binades that HiF8 can cover increase from 31 to 38, very close to the 40 of FP16.\nFrom the above analyses, we can see that HiF8 stands on the shoulders of the three data formats, absorbs their advantages, and finally achieves a better balance for deep learning at the 8-bit limit. Compared with FP8, HiF8 takes both precision and dynamic range into account, and is capable of replacing two formats of FP8 with only one format. Compared with Posit(8, 2) [27,28], HiF8 has 31 exponents with mantissa no less than 1 bit, while Posit(8, 2) only has 24 exponents with mantissa no less than 1 bit. And compared with FP16, HiF8 has almost the same dynamic range, which is much better than FP8-E4M3 and FP8-E5M2."}, {"title": "3 Rounding Methods", "content": "In Float8 mixed precision training and inference, high-precision floating-point formats such as FP32 need to be converted into low-precision format Float8, and then input to GEMM, during which rounding is involved. As the precision of Float8 is relatively lower than BF16 and FP16, the rounding method is extremely sensitive to the convergence and accuracy of neural network training. After the theoretical analysis and a large number of experiments, we conculde that HiF8 will support two rounding methds: rounding half to away (from zero), and hybrid rounding. To covert high-precision data to HiF8, we use only rounding half to away in the forward pass, and rounding half to away or hybrid rounding in the backward pass. In addition, to meet the requirements of certain AI algorithms, HiF8 also provides two options: saturation to boundary upon overflow, and NaN saturation to zero. The following describes the rounding methods during the conversion from high-preciosn formats to HiF8."}, {"title": "3.1 Rounding Half", "content": "Rounding half (rounding to nearest) yields an error of 0.5 ulp (unit of least precision), and can be generally classfied into rounding half to even (TE) and rounding half to away (TA) [14]. Technically, if the MSB of the discarded bits is 1 and the other discarded bits are all 0, TE would ensure that the LSB (least significant bit) of the rounded number is even by carrying or not. Otherwise, both TE and TA would carry as long as the MSB of the discarded bits is 1. Although TA features easier hardware implementation, TE is used by default in most papers and commercial products, because it maximizes the unbiasedness [22]. In fact, the occurrence probability of the TE special case is extremely low during the conversion from high-precision formats to HiF8. For example, if HiF8 reserves 3-bit mantissa, the probability is only $2^{-20}$ during the conversion from FP32 to HiF8.\nThe biggest challenge of Float8 in AI is its limited data resolution capability. The analysis result shows that the data resolution capability of TA is slightly higher than that of TE. Consider a TE special case of three 3-bit numbers with consecutive integer bits: 00.1, 01.1, and 10.1. TE is rounded as TE(00.1) = 00, TE(01.1) = 10, TE(10.1) = 10, giving two different results. TA is rounded as TA(00.1) = 01, TA(01.1) = 10, TA(10.1) = 11, giving three different results. Therefore, in the TE special case, TA enables a higher data resolution capability of Float8 than TE. The simulation experiments of HiF8 also evidence that TA produces slightly higher training accuracy than TE. For example, the TA-based training accuracies of ResNet50 [31] and MobileNet_V2 [32] are 0.06% and 0.11% higher than the TE-based training accuracies on average.\nSince TA features simpler hardware implementation and higher training accuracy, it is therefore supported during the conversion from high-precision formats (including FP32, FP16, and BF16) to HiF8."}, {"title": "3.2 Hybrid Rounding", "content": "Large-scale HiF8 mixed precision training experiments show that global TA rounding works well for almost all neural networks. But for YoLo-V3-Tiny [33], some segments of the loss curve crashed. As a result, the final accuracy was 1.67% lower than the FP32 baseline. After extensive research and many experiments, in addition to the global TA rounding method, we propose a second HiF8 rounding method for training, which combines TA rounding for the forward pass and hybrid rounding for the backward pass. This makes the training accuracy of the YoLo-V3-Tiny close to the baseline value. Therefore, HiF8 supports both TA rounding and hybrid rounding (HR). In fact, HR is essentially an optimized version of standard stochastic rounding [34], which is easier to implement in circuits and has slightly better training accuracy.\nThe error of stochastic rounding (SR) is 1 ulp. Compared with TA, SR has a significant advantage when data is processed in batches. Specifically, in SR, a uniformly distributed random number needs to be randomly generated and used as threshold T, (T\u2208 [0, 1)). All bits to be discarded are regarded as fractional bits and marked as F, (F \u2208 [0, 1)). If F > T, 1 is added to the reserved bits K, otherwise 0 is added to the reserved bits K. As threshold T is uniformly distributed, the expected value after SR is expressed as follows:\n$(K + 1) \\times F + K \\times (1 - F) = K + F$\nApparently, SR can maximize the invariance of the overall mean value during the rounding of batch data. However, deep learning needs to generate a large number of uniformly distributed random numbers in parallel, both software and hardware implementations of SR hit a performance bottleneck [35].\nTo tackle the bottleneck, we first come up with a simplified SR hardware solution. Theoretical analysis and experiments show that the lower mantissa bits of floating-point numbers obey uniform distribution. Therefore, for the FP32 source data, we set 14 LSBs of the source format as the threshold T14, and 14 MSBs of the discarded bits as the fractional bits F14. However, for FP16 and BF16 source data, the discarded bits are not wide enough to be divided into reasonable and weakly relevant threshold and fraction. To solve this dilemma, we combine a fixed 1 and the LSB of the source format into a special 2-bit threshold T2, and set the 2-bit MSBs of the discarded bits as the fractional bits F2. As illustrated in Fig. 2, we have designed a 14-bit SR (SR14) for FP32 and a 2-bit SR (SR2) for FP16 and BF16, without generating the random numbers by complex algorithms. SR14 is very similar to standard SR, with the same 1 ulp rounding error. SR2 is weakly stochastic with only 2 thresholds of 0.25 and 0.75, but has a small rounding error of 0.75 ulp. So far, by comparing F14/F2 with T14/T2, simplified SR can be done in hardware.\nHiF8 training experiments show that the effect of simplified SR is quite close to that of standard SR, but there is still a very small gap. In fact, simplified SR achieves better mean invariance than TA, but incurs greater rounding error. Meanwhile, from Fig. 1 and the centralized characteristic of AI data, we can make the following logical inference. Most of the data is within the high-precision range of HiF8, in which TA rounding will cause only a small change in the average value, becuase the rounding direction is relatively balanced for large data volume. However, a small amount of data (especially large values) is within the low-precision range of HiF8, in which TA rounding may incur a large change in the average value, because the rounding direction can be unbalanced for small data volume. Through such analysis, we propose a hybrid rounding method as follows.\n$Y = \\{\\begin{array}{ll}TA Rounding & if |E|< 4,\\\\Simplified SR & if |E|\\geq 4.\\end{array}$              (3)\nIn the HR rounding equation (3), E is the exponent value of the source data format. Experiments show that HR yields slightly better training accuray than the standard SR with low hardware cost. Especailly, for YoLo-V3-Tiny training, the baseline accuracy of FP16 mixed precsion is 16.63%. When using global TA rounding, the training accuracy of HiF8 is 14.96%, 1.67% lower than the baseline. When using TA rounding for the forward pass and standard SR for the backward pass, the training accuracy of HiF8 is 16.43%, 0.20% lower than the baseline. When using TA rounding for the forward pass and hybrid rounding for the backward pass, the training accuracy of HiF8 is 16.69%, slightly better than the baseline.\nThus, in addition to the TA rounding, the proposed hybrid rounding is also supported during the conversion from high-precision formats (including FP32, FP16, and BF16) to HiF8. Note that there is no need to use hybrid rounding in the forward pass, because the distribution of activations and weights is relatively more concentrated than the distribution of gradients in the backward pass. At the same time, for the vast majority of neural networks, training with TA and HR makes very little difference, and so far we have only found one neural network that definitely needs HR."}, {"title": "4 Experiments on Traditional Neural Networks", "content": "Currently, no hardware platform is available to support HiF8 data type and complete the computation process. Thus both training and inference experiments of traditional neural networks and LLMs, were performed with simulated HiF8 format. Specifically, using the rounding methods described above, the tensor values were converted from high-precision formats to only those that could be represented in HiF8. Hardware platforms, including Huawei Ascend NPUs [16,36] and NVDIA GPUs [15,37], and software framework PyTorch [38], were utilized to conduct the HiF8 training and inference experiments.\nSince traditional neural networks and LLMs have been identified to have varying degrees of data dispersion, we handle them differently. In this section, only empirical results for the traditional neural networks are presented."}, {"title": "4.1 Training with Backward Loss-Scaling", "content": "As mentioned earlier, the dynamic range of HiF8 is almost the same as that of FP16, so for the traditional neural network, we inherit the training method of FP16 mixed precision. Specifically, only the GEMM inputs, including activation, weight, and activation gradient tensors, are changed from FP16 to HiF8 (excluding the last fully-connected layer). The others such as non-linearities or normalizations, remain the same as the FP16 mixed precision training. Most importantly, backward global loss-scaling is enabled to avoid excessive zero-valued gradients, which is the core trick to make both FP16 and HiF8 training effective [17]. As for the conversion from high-precision formats to HiF8, only TA rounding is used in the forward pass, and either TA rounding or hybrid rounding is used in the backward pass.\nWe trained FP16 baseline results and HiF8 results with the same model architectures, weight initializations (non-fixed random seeds), and optimizer hyper-parameters, and compared them. To verify the training accuracy of HiF8 for the traditional nerural networks, we chose two main application directions for simulation experiments: computer vision and natural language processing (NLP). The subdivided application scenarios of computer vision include classification, detection, and segmentation. In this paper, the most widely used typical networks based on the CNN and Transformer structures are selected for the experiments, including ResNet series [31], ResNeXt [39], VGG [40], MobileNet [32], Inception [41], EfficientNet [42], DenseNet [43], ViT series [44], YoLo series [33], and DeepLab [45]. For NLP, the mainstream Transformer models [46, 47] are used."}, {"title": "4.2 Inference with Per-Tensor Scaling", "content": "The model trained by HiF8 can be directly used for inference. Therefore, this section only focuses on the process of converting high-precision trained model into HiF8 inference model. To reflect the capability of HiF8, two inference results of post-training quantization (PTQ) will be presented. First, we directly cast high-precision activation and weight tensors of all layers into HiF8 format without calibration, and compare its accuray with the original model. Second, we evaluate HiF8 calibration of models trained in FP32 or FP16. Unlike the int8 and FP8 calibration, which usually use per-tensor scaling for activations and per-channel scaling for weights [22, 48], in this paper, we only perform the per-tensor scaling for both activations and weights of all layers.\nAs shown in Algorithm 1, we restrict the value of each tensor's scaling factor to an integer power of two. In this way, there are only a limited number of proper choices for scaling factors near $2^0$, and the scaling operations involve only addition and subtraction of exponents, no multiplication. To find two suitable scaling factors for a tensor multiplication inputs, we search for several combinations of scaling factors and choose the result that minimizes the MSE (mean squared error) of the output tensor."}, {"title": "5 Experiments on Large Language Models", "content": "LLMs exhibit some unique features that differ from the traditional neural networks. On the training side, the gradients distribution is more dispersed. Thus larger dynamic range or special technique, is required to avoid too much data becoming zero during data type conversion [52]. In terms of inference, outliers play an important role in validation accuracy. Therefore, dedicated method such as SmoothQuant [53], is possible needed to reduce the quantization error of outliers. In this section, to apply HiF8 on LLMs, we proposed and tried three training methods, each with different costs and coverage. We also evaluated three inference methods, including direct-cast, per-tensor scaling, and SmoothQuant. Note that for LLMs, only TA rounding was used for HiF8."}, {"title": "5.1 Training", "content": "First, we inroduce the HiF8 training methods for LLMs in our experiments:\n\u2022 Backward Loss-Scaling (BLS)\nThis method is inherited from FP16 mixed-precision training [17]. More details can be found in Section 4.1.\n\u2022 Adaptive Loss-Scaling (ALS)\nThis is an optimal configuration for the backward loss-scaling. In the current mixed-precision training, scale window (growth_interval) is an input constant throughout the training task 2, and is usually set to 1000 or 2000 for the traditional neural networks. In some LLMs, the maginitude of gradients changes rapidly in the early iterations. At this moment, large scale window cannot catch up with the change in gradients, resulting in a failure of convergence. However, a scale window that is too small can significantly degrade the performance of training.\nTo address this dilemma, we propose the adaptive loss-scaling. Specifically, we first define an incremental list of scale window, such as {1, 20, 50, 100, 200, 500, 1000}. Then the inital scale value and scale window are set to $2^{32}$ and 20. During training, if the scale value increases three times, the scale window will increase once as per the list order. If the scale value decreases three times in a row, scale window will decrease once as per the list order. The proposed ALS can be applied to both FP16 and HiF8 to improve the stability of training. Please refer to the website for more information 3.\n\u2022 Per-Tensor Scaling (PTS)\nAlthough training with BLS and ALS can be successfully carried out in HiF8 for a number of LLMs, there are cases where per-tensor scaling is needed to improve accuracy and mitigate the difficulty of hyper-parameters tuning. To be specific, we define a scale factor for each GEMM input tensor, including activation, weight, and activation gradient. The initial scale factors are all set to 1. Then every 10 iterations, we compute the maximun absolute value (Amax) of each tensor, and update the corresponding scale factor by a certain algorithm. To avoid additional rounding errors, the scale factors are restricted to integer powers of 2. While in each iteration, whether the scale factors are updated or not, they will scale the corresponding tensors to a better range that can be represented by HiF8. Finally, GEMM outputs need to be descaled to restore the correct results. Note that scale and descale operations are all carried out in high-precision formats, such as FP32, BF16, and FP16 (with BLS or ALS).\nActually, the PTS we used for HiF8 is very similar to the transformer engine for FP8 [52]. However, because HiF8 has much larger dynamic range than FP8 (especially E4M3), we do not need to compute Amax in all iterations, thereby greatly reducing the occupation of vector resources."}, {"title": "5.2 Inference", "content": "The LLM trained by HiF8 can be directly used for inference. Thus we only consider the HiF8 PTQ of LLMs trained in higher precision. In addition to the PTQ methods of direct-cast and per-tensor scaling used in Section 4.2, we further evaluated the SmoothQuant [53] calibration method for LLMs, due to the influence of outliers. First, to objectively reflect the inference ability of HiF8, we chose the quantization-tolerant LLaMA [55], and the quantization-sensitive OPT [58] as the experimental LLMs."}, {"title": "6 Conclusion and Outlook", "content": "In this preliminary white paper, we propose a novel 8-bit floating-point format HiFloat8, consisting of sign, dot, exponent, and mantissa fields. By deeply exploring the match between format and data distribution, HiF8 achieves a much better balance between precision and daynamic range than the existing 8-bit formats. Experments on a large number of traditional neural networks and LLMs, demonstrate that as a single format, HiF8 works well in both training and inference. In the future, we will disclose another research achievement of HiFloat project: HiFloat below 8-bit, as well as its training and inference capabilities."}]}