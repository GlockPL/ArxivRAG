{"title": "Flow as the Cross-Domain Manipulation Interface", "authors": ["Mengda Xu", "Zhenjia Xu", "Yinghao Xu", "Cheng Chi", "Gordon Wetzstein", "Manuela Veloso", "Shuran Song"], "abstract": "We present Im2Flow2Act, a scalable learning framework that enables robots to acquire manipulation skills from diverse data sources. The key idea behind Im2Flow2Act is to use object flow as the manipulation interface, bridging domain gaps between different embodiments (i.e., human and robot) and training environments (i.e., real-world and simulated). Im2Flow2Act comprises two components: a flow generation network and a flow-conditioned policy. The flow generation network, trained on human demonstration videos, generates object flow from the initial scene image, conditioned on the task description. The flow-conditioned policy, trained on simulated robot play data, maps the generated object flow to robot actions to realize the desired object movements. By using flow as input, this policy can be directly deployed in the real world with a minimal sim-to-real gap. By leveraging real-world human videos and simulated robot play data, we bypass the challenges of teleoperating physical robots in the real world, resulting in a scalable system for diverse tasks. We demonstrate Im2Flow2Act's capabilities in a variety of real-world tasks, including the manipulation of rigid, articulated, and deformable objects.", "sections": [{"title": "1 Introduction", "content": "A key step for scaling up robot learning is giving robots the ability to learn from various data sources beyond expensive real-world robot data [1, 2, 3, 4, 5]. Prior work has approached this problem from two main directions: learning from cross-embodiment videos [6, 7, 8, 9, 10, 11, 12] and simulated robot data. While both approaches have shown promising progress, they each face significant challenges. Learning from cross-embodiment data is hampered by the large embodiment gap, while simulated data struggles with a significant sim-to-real gap and the complexity to build task-specific environment, especially for contact-rich manipulation tasks.\nIn this paper, we introduce Im2Flow2Act, a novel framework for efficiently teaching robots manipulation skills from diverse data sources. Our key idea is to use object flow-the exclusive motion of the manipulated object, excluding any background or embodiment movement\u2014as a unifying interface to connect these data sources and achieve one-shot generalization for new skills in the real world. Our framework (Fig. 1) consists of two main components: a flow generation network (Fig. 1-a) trained on cross-embodiment demonstration videos and a flow-conditioned policy (Fig. 1-b) trained on embodiment-specific cross-environment play data. During inference, the flow generation model first generates the task-specific flow based on the initial visual observation and task description (Fig. 1-c). Conditioned on this generated flow, our task-agnostic policy generates actions to execute the task. Below, we outline the benefits of using object flow as a unifying interface:\n\u2022 Expressive task descriptions: Object flow not only captures changes in object pose but also accounts for articulations and deformations. Its versatility enables it to represent a wide range of objects and tasks, including rigid, articulated, and deformable objects. In Im2Flow2Act, we leverage this versatility by training a single manipulation policy for diverse tasks.\n\u2022 Embodiment agnostic: Object flow describes the change in the state of an object caused by an action rather than the action itself, making it independent of the agent's embodiment. Compared to prior work [13] utilizing uniform grid flows that also track the embodiment motion, our method uses object flow, making our framework more effective for cross-embodiment learning.\n\u2022 Minimal sim-to-real gap: Compared to image-based representations, flow focuses on motion rather than appearance, reducing the sim2real gap and aiding generalization. Unlike prior works utilizing flow for manipulation that rely on heuristic policy [14], realworld teleoperation data [13, 15], or both [16], our closed-loop policy is entirely learned from simulated robot play data.\n\u2022 Interpretable: Unlike vector-based state representations, flow-based representation has intuitive physical meaning. This makes it a good interface for human-robot interaction. For example, a user can easily understand and select a flow when multiple flows exist for a task.\nTo utilize object flow as an unifying interface for learning from diverse data sources, we design our system into two components:\n\u2022 Flow generation network: The goal of the flow generation network (Fig. 1-a) is to learn high-level task planning through cross-embodiment videos, including those of different types of robots and human demonstrations. We develop a language-conditioned flow generation network built on top of the video generation model Animatediff [17]. Compared to prior approaches [13, 14, 16] that train on the original flow space, we leverage the autoencoder (AE) from Stable Diffusion [18] to first compress the flow into latent space, and then train the model on that compressed representation, making our training process more efficient. This high-level planning component generates the task flow based on initial visual observation and task description.\n\u2022 Flow-conditioned policy: The goal of the flow-conditioned imitation learning policy (Fig. 1-b) is to achieve the flows generated by the flow generation network, focusing on low-level execution. The policy learns entirely from simulated data to build the mapping between actions and flows. Unlike most sim-to-real work that requires building task-specific simulation, our policy learns entirely from play data, which is easier to collect. Moreover, the diverse and random nature of play data compels the policy to grasp the relationships between actions and their effects on objects. Since flow represents motion, a common concept across both real-world and simulated environments, our policy can be seamlessly deployed in realworld settings.\nTogether, the flow generation network and the flow-conditioned policy form a cohesive system. Our final system, Im2Flow2Act, provides a scalable framework for acquiring robot manipulation skills by bridging cross-embodiment demonstration video and cost-effective simulated robot play data via object flows as an interface. We achieve an average success rate of 81% across four tasks, including those involving rigid, articulated, and deformable objects."}, {"title": "2 Related Work", "content": "Flow-based manipulation. Robot policies leveraging flows have demonstrated promising results in manipulating articulated objects [19, 20] and tools [21]. However, these approaches are limited to specific tasks. With recent advances in point tracking algorithms in computer vision, flows can be estimated in a video sequence through learning-based methods. Vecerik et al. [15] achieved few-shot learning through tracking flows but required explicit pose estimation and was still limited to rigid objects. To make flow applicable to more general manipulation tasks, Wen et al. [13] learned a flow-conditioned behavior cloning policy that can be applied to both articulated and deformable objects. However, it still required collecting in-domain robot data through teleportation in the real world, which is costly and challenging to scale up. To ease the data challenge, Yuan et al. [14] proposed a heuristic flow-based control policy but required manual positioning of the robot gripper on the object, making the overall system less autonomous. More recently, Bharadhwaj et al. [16] propose to learn a residual policy on top of the heuristic-based policy but still require collecting additional in-domain robot data. In Im2Flow2Act, we train a data-efficient and fully autonomous flow-conditioned imitation learning policy from task-less simulation play datasets which does not require costly in-domain robot data collection and can be transferred to realworld in one-shot.\nLearning from cross-embodiment data. A large body of work [10, 22, 23, 24, 25] has studied leveraging cross-embodiment data to learn robotic policies. Prior works explore different directions, including visual pretraining [8, 26, 27], learning reward functions [7, 28, 29, 30], extracting affordances [31, 9, 32, 33], hand pose detection [6, 34, 35], and domain translation [36, 37, 38, 39, 40, 41, 42]. As most of cross-embodiment data lacks explicit action or the action is challenging to transfer due to large embodiment gaps, many prior works still require collecting in-domain robot data [43, 41, 12, 11, 44] to mitigate the large embodiment gap. More recently, a number of works [45, 13, 14] have learned video generation or flow generation models through cross-embodiment data. Although showing promising results, these generation models still require costly in-domain robot data to finetune or large computational resources. In Im2Flow2Act, we propose to learn a flow generation model that only predicts the flow of objects but not the embodiment, allowing us to seamlessly learn from human videos and do not require any in-domain robot data. Furthermore, we formulate the object flow into a sequence of structured flow images, which enables us to leverage pretrained image generation models [18] to achieve more efficient training.\nSim2Real Transfer. To overcome the data collection challenge in the real world, a desired approach is to train a robot policy in simulation and deploy it in the real world. Due to the sim-to-real gap, various techniques have been employed to bridge this gap, including but not limited to using depth observation [46, 47, 48, 49], domain randomization [50, 51, 52, 53, 54, 55], knowledge distillation [56, 57], and system identification [58]. Learning a sim-to-real policy typically requires task-specific"}, {"title": "3 Method", "content": "We aim to build a framework that leverages object flow as a unified interface for robots to acquire real-world manipulation skills using both cross-environment and cross-embodiment data. Our framework (See Fig. 2) comprises two components: an open-looped flow generation network which run once at the beginning of the task and a closed-loop flow-conditioned imitation learning policy act adaptively based on the current state. During inference, the flow generation network first generates a complete task flow conditioned on a set of initial object key points at the beginning of the task, and then the policy executes the actions to reproduce the generated flow. The final system is capable of one-shot generalization for new skills and novel objects in the real world. Our key idea is to use object flows as a general and expressive interface to bridge:\nCross-Embodiment data: Transferring actions between different embodiments is often hindered by the embodiment gap. However, object flow encapsulates transferable task knowledge independent of embodiment-specific actions. To exploit this, we develop a flow generation network focused solely on extracting object flows from cross-embodiment videos.\nCross-Environment data: In this work, cross-environment majorly refers to simulation and real-world environments. Visual inconsistencies, such as differences in scene backgrounds and object textures, pose challenges in transferring learned policies from simulation to real environments. Conversely, object flow captures motion dynamics\u2014changes in object pose, articulation, and deformation\u2014consistent across both simulated and real settings, providing an ideal interface for policy learning. Our flow-conditioned imitation learning policy is learned entirely from simulated play data. The randomness in play data actions fosters an understanding of the relationship between actions and resulting object flows, further reducing the need for task-specific simulation environments."}, {"title": "3.1 Flow Generation Network", "content": "For the flow generation network, our main idea is to form permutation invariant object flows into a sequence of structured rectangular flow images. This allows us to better utilize pretrained image generation models and video generation architectures for more efficient training. Specifically, we obtain the bounding box of the object of interest through Grounding DINO [59] in the initial RGB frame and then perform uniform sampling within this bounding box, resulting in an rectangular flow \\(F_o \\in \\mathbb{R}^{3\\times H\\times W}\\), with three channels at a spatial resolution of \\(H \\times W\\). The first two channels represent the u, v coordinates of object keypoints in the image space, while the third channel represents their visibility. We then leverage point tracking methods TAPIR [60] on the videos to get the rectangular flow sequence \\(F_{1:T} \\in \\mathbb{R}^{3\\times T\\times H\\times W}\\) with T steps in the temporal space. With this flow representation, we utilize the existing diffusion video generation architecture AnimateDiff [17] as our flow generator network, conditioning it on an initial RGB frame and a task description to generate object flow. The generated flow is then processed by motion filters to preserve only the"}, {"title": "Compressing Flow into Latent Space.", "content": "High-precision flow is critical for enabling robots to achieve fine-grained control over objects. However, generating object flow at high resolution introduces significant computational complexity and results in inefficient generation speed. Inspired by previous work [18], we propose to compress the object flow into a compact latent space with a lower spatial dimension, followed by training the generative model within this latent space. StableDiffusion (SD) compresses input images using an autoencoder (AE) proposed in VQGAN [61], resulting in a well-distributed latent space by training on a vast amount of RGB images. In this work, we explore the idea of using AE to encode the flow. To leverage this well-structured latent space, we fix the encoder \\(E_{\\theta}(\\cdot)\\) from the AE and slightly finetune the pretrained decoder \\(D_{\\theta}(\\cdot)\\) to better adapt it to the flow images. The latents \\(z_{1:T} \\in \\mathbb{R}^{C \\times T \\times H_r \\times W_r}\\) for input flow can be obtained by \\(z_{1:T} = E_{\\theta}(F_i) \\,\\, i \\in [1,T]\\), where \\(H_r\\) and \\(W_r\\) denote the spatial dimensions, which are downsampled by a factor of 8 compared to the input flow spatial dimensions H and W."}, {"title": "Video Diffusion Models for Flow Generation.", "content": "As we share the same latent space as StableDiffusion (SD), we can utilize the pretrained SD to provide a strong content prior for flow generation. Therefore, we choose to inflate the SD network along the temporal axis for flow generation rather than training a model from scratch. We then insert the motion module layer into SD proposed by Animatediff [17] to model the temporal dynamics for flow generation. The motion module performs self-attention on each spatial feature along the temporal dimension to incorporate temporal information The SD model with the motion module, learns to capture the temporal variations in keypoints' location (u, v) in the image space, which constitute the motion dynamics in the object flow sequence. During the training, we train the motion module layer from scratch but only insert LORA (Low-Rank Adaptation) layers [62] into the SD model."}, {"title": "Condition Injection.", "content": "The text condition are passed into the CLIP [63] text encoder to obtain text embedding. We also input the initial frame and the initial object keypoints \\(F_0\\) into the model to ensure the generation process considers the spatial relationship between objects and the scene. We use the CLIP image encoder to encode the initial frame, yielding a \\(P_2\\) embedding for each patch from the last ViT [64] layer. The initial keypoints \\(F_0\\) are encoded using fixed 2D sinusoidal positional encoding. All conditions are injected into the denoising process through cross-attention."}, {"title": "3.2 Flow-Conditioned Imitation Learning Policy", "content": "Our imitation learning policy \\(p(a_t | F_{0:T}, s_t, p_t)\\) takes the object flow \\(F_{0:T}\\) for a complete task (i.\u0435., task flow), the current state representation \\(s_t\\), and the robot proprioception \\(p_t\\) as input. The output is a sequence of actions \\(\\{a_t,..., a_{t+L}\\}\\) of length L, starting from the current time t, denoted as \\(a_t\\). The robot action \\(a_t\\) includes a 6-DOF end-effector position in Cartesian space and a 1-DOF gripper open/close state. The state representation \\(s_t\\) is formed by keypoints location at current frame denoted as \\(f_t\\) containing N keypoints' locations \\(\\{(u, v)\\}_{i=1}^N\\) in the image space at time t and the 3D coordinates of N keypoints \\(x_0 \\in \\mathbb{R}^{N\\times 3}\\) at the initial frame.\nDuring inference, the task flow F is generated once by the flow generation network at the beginning of the task, as described in Sec. 3.1. We leverage an online point tracking algorithm to obtain the \\(f_t\\) during robot manipulation for the same set of keypoints. The policy consists following components: a state encoder \\(\\phi\\) takes the N keypoints encapsulate in the \\(f_t\\) and their corresponding initial 3D coordinates \\(x_0\\) as input to generate state representation \\(s_t\\), a temporal alignment module \\(\\psi\\) which compares task flow and current keypoints location \\(f_t\\) to infer remaining task flow, and finally, a diffusion action head [65] to output the robot action.\nTraining data: We briefly describe how we construct the training data and some decision choice (See supplementary for more details). For an episode with time duration of T', we run the point tracking algorithm to get location of keypoints on object being manipulated at each step, i.e, \\(f_{0:T'}\\). We can get a dataset \\(\\mathcal{D}\\) in the form of trajectories \\(\\mathcal{T}_k = \\{(\\rho_0, f_0, a_0), \\dots, (\\rho_{T'}, f_{T'}, a_{T'})\\}\\). During the"}, {"title": "State Encoder:", "content": "The state encoder \\(\\phi(f_t, x_0)\\) is a transformer-based [66] network that processes all N keypoints in the \\(f_t\\) and the corresponding initial 3D coordinates \\(x_0\\) to output a current tate representation \\(s_t\\), i.e., \\(s_t = \\phi(f_t, x_0)\\). For each keypoint, its location (u, v) at time t is first encoded using 2D sinusoidal positional encoding and the initial 3D coordinate \\(x_{0,n}\\) is passed through a linear projection head. They are then concatenated together to form a unique descriptor \\(e\\) for each point. The descriptors for all N points are passed into the state encoder, and we use a CLS token [67] to summarize the current state. Since the points are permutation invariant, we do not add any positional embedding and only add a learnable embedding for the CLS token."}, {"title": "Temporal Alignment:", "content": "During inference, the policy should locate the current task progress and be conditioned on the remaining task flow, rather than the complete task flow, to predict precise actions. To this end, we incorporate a temporal alignment module \\(\\psi\\) to predict the remaining task flow given the current keypoints location and the complete task flow. The idea is similar to the skill alignment in XSkill [44] but we extend it into more complex flow domain.\nInstead of predicting the raw remaining task flow, we conduct it in the latent space \\(\\mathcal{Z}\\) to improve training efficiency. We construct a transformer-based temporal alignment model \\(\\psi(F_{0:T}, s_t, p_t)\\) to predict the latent representation of the remaining task flow \\(z_t\\) based on the complete task flow \\(F_{0:T}\\), the current state \\(s_t\\), and the robot proprioception \\(p_t\\). For the alignment module supervision \\(z_t\\), we use another transformer encoder \\(\\xi\\) to encode the ground truth remaining task flow \\(f_{t:T'}\\) into the latent space which is accessible in the training dataset D. We use L2 loss between \\(z_t\\) and \\(z_t\\), i.e., \\(||z_t - \\hat{z}_t||^2\\) to supervise the alignment learning."}, {"title": "Diffusion Action Head:", "content": "During inference, the policy can directly condition on \\(\\hat{z}_t\\), i.e., \\(p(a_t | \\hat{z}_t, s_t, p_t)\\) to predict future actions. During training, the policy is conditioned on \\(z_t\\) such that the encoder \\(\\xi\\) gets supervision. Although the whole system can be trained in an end-to-end fashion, we detach the \\(\\hat{z}_t\\) when computing alignment loss, i.e., \\(||\\hat{z}_{detach} - z_t||^2\\) for more stable training."}, {"title": "4 Evaluation", "content": "We demonstrate Im2Flow2Act's capability across 4 tasks ranging from rigid, articulate, and deformable objects, including pick-and-place, pouring, open drawer, and folding cloth. The task description can be seen in the leftmost column in Fig. 4."}, {"title": "4.1 Experiment Details", "content": "Training data: We collect robot play data in simulation and human hand demonstration in the realworld. (i.) In simulation, we collect play data across rigid, articulated, and deformable objects using a UR5e robot through a set of predefined random heuristic actions for exploration. The details for each exploration strategy can be found in the supplementary material. We trained one imitation learning policy with all the play data. (ii.) In realworld, we collect 100 human hand demonstrations for each task. We mixed the data together and used it to train the flow generation model.\nComparisons. We compare our method with the following baselines:\n\u2022 Grid Flow: Similar to ATM [13], we replace the initial object keypoints with a set of uniformly sampled keypoints over the whole image.\n\u2022 Heuristic: A heuristic action policy selects object contact points and applies a pose estimation method, such as RANSAC, on the future object flow (requiring 3D flow) to infer robot actions, similar to General Flow and Track2Act [14, 16]. This baseline examines the necessity of having a learning-based policy for translating flow into actions.\n\u2022 No alignment: To ablate our alignment module design, we removed the alignment model \\(\\psi\\) and the policy condition on the complete task flow."}, {"title": "4.2 Key Findings", "content": "Flows can effectively bridge different data source: Im2Flow2Act achieves best performance among all baseline in both simulation and realworld, as shown in both Tab 1 and Tab 2. Further, the performance only drops 15% on average in realworld compare to in simulation. As shown in Fig. 4, our learned policy can largely follow the generated flow to complete the desired tasks across rigid, articulated and deformable objects. This suggests that object flow is a good interface to connect the both cross-embodiment and cross-environment data sources.\nLearning-based policy is necessary for translating flow to actions: To map the generated flow to robot actions, we used an learned policy and alignment network. In contrast, prior works [16, 14] use heuristic-based policy to compute robot action from the flow. When compared to them, we"}, {"title": "4.3 Limitation and Future Work", "content": "Our system uses 2D flow as the manipulation interface. This design allows us to leverage large-scale 2D videos; however, it is intrinsically ambiguous for representing 3D actions. For example, a major failure case for pouring tasks occurs when the robot's actions are not precise enough along the z-axis of the camera coordinates. Additionally, our framework assumes that object dynamics are consistent between simulation and realworld, i.e., the same robot action should lead to the same object flow. However, simulating deformable objects is a challenging task. Therefore, we observe a significant drop in success rates in the cloth folding task when deploy policy to realworld. In our current system, we assume the camera view point in simulation is calibrated with respect to the testing environment, and action is visible from the camera view (note, viewpoints for flow generation is not constrained). This limitation can be potentially addressed with 3D flow [14, 68]. Moreover, by using flow as action abstraction the algorithm also ignores some action details. Hence, it may not capture sufficient information for some dexterous tasks such as in-hand manipulation."}, {"title": "5 Conclusion", "content": "We introduce Im2Flow2Act, a scalable learning framework that enables robots to acquire diverse manipulation skills from cost-effective cross-domain data using object flow as a unifying interface. Our final system demonstrates strong real-world manipulation capability by outperforming all baselines across various types of manipulation tasks. Our work paves the way for future research to scale up robotic manipulation skills from diverse data sources."}, {"title": "A Data Collection", "content": "In Im2Flow2Act, we collect two types of data: a simulated robot play dataset and real-world human demonstration videos. Note that we do not collect any real-world robot data. Below, we detail the data collection process."}, {"title": "A.1 Simulated Play Data", "content": "We use MuJoCo as our simulation engine and have constructed three types of play environments featuring rigid, articulated, and deformable objects. An UR5e robot explores these environments using a set of predefined random heuristic actions.\nRigid: In the rigid play environment, the robot can interact with five different objects, including a toy car, a shoe, a mini-lamp, a frying pan, and a mug. At the beginning of each episode, one object is randomly selected and placed on the table. The robot first picks up the object and starts executing 6DoF random trajectories. A random trajectory is constructed as a 3D cubic B\u00e9zier curve with four key waypoints: the start point \\(p_0\\), two control points \\(p_1\\) and \\(p_2\\), and the end point \\(p_3\\). The start point"}, {"title": "A.2 Real-world Human Demonstration Video:", "content": "We collect in-domain human demonstration videos for four tasks: pick & place, pouring, opening drawers, and folding cloth. We use a RealSense camera to record the demonstrations at 30 FPS. The descriptions below also serve as detailed task descriptions. In Im2Flow2Act, we use the human videos to provide the system with task information. We define the robot base as the origin of the world coordinate system and use the Right-Handed Coordinate System.\n\u2022 Pick & Place: Pick up a green cup and place it into a red plate. The red plate is randomly placed on one side of the table, while the cup is placed randomly in the middle of the table.\n\u2022 Pouring: Pick up a green cup and hover it over a red bowl, then rotate the cup towards the bowl. The initial placement of the cup and the bowl is the same as in the pick & place task.\n\u2022 Drawer Opening: Fully open a drawer that is randomly placed on one side of the table. The pose of the drawer, including its position and orientation, can vary. The drawer is not rigidly attached to the table.\n\u2022 Cloth Folding: Fold a 33 cm x 33 cm cloth from one corner (either lower left or lower right) to the diagonal. The cloth is randomly placed in the middle of the table."}, {"title": "B Ablation study", "content": "We conducted an ablation study in simulation to evaluate the impact of using pretrained StableDiffusion (SD) versus training it from scratch on the flow generation model. In this ablation study, we still use pretrained AE (auto-encoder) from the SD but trained the U-Net from scratch instead of incorporating LoRA layers. To ensure a fair comparison, we deploy the same flow-conditioned"}, {"title": "C Experimental Details", "content": "We use a UR5e robot equipped with a WSG-50 gripper. During inference, the UR5e receives end-effector space positional commands from a 2.5 Hz policy. We limit the end-effector's speed to less than 0.2 m/s and restrict its position to at least 1 cm above the table for safe execution. A RealSense D415 depth camera is mounted at the table to capture policy observations, including the initial depth"}, {"title": "C.2 Real-World Evaluation Protocol", "content": "In this section, we describe the details of the evaluation protocol in the real world.\n1) Initial State: When evaluating our system, we do not match the background scene setup to those recorded in the human demonstrations such that we can also test the generalization capability of flow generation network. For the initial position of objects, they are placed with roughly the same distribution as in the human video demonstrations.\n2) Success Metric: Below are the details of the real-world success metric for all four tasks:\n\u2022 Pick & Place: The robot needs to pick up the cup and place it into the red plate. We do not require the mug to be centered on the plate. We consider one episode successful if: i. the robot can steadily place the mug onto the red plate; ii. the plate does not move more than 5 cm on the table during the robot's manipulation process.\n\u2022 Pouring: The robot needs to pick up the cup, hover it near the bowl, and execute pouring actions. We consider one episode successful if: i. the robot demonstrates the complete pouring behavior by rotating the cup more than 30 degrees; ii. at least half of the cup overlaps with the red bowl on the x-axis in terms of world coordinates."}, {"title": "C.3 Evaluation Procedure", "content": "Im2Flow2Act with/without alignment: For each task, we first record the initial frame with different initial states for 20 evaluation episodes. We then query Grounding DINO on the object of interest to obtain the bounding boxes in all initial frames. Using the bounding box, initial frame, and the task description, we generate the object flow (task flow) for all episodes and store them as a buffer on the disk. With all ingredients for policy inference set, we start policy evaluation for each episode by manually matching the initial states to be close to pixel-perfect within the mounted RealSense camera and load the corresponding generated flow from the previously saved flow buffer.\nHeuristic-based policy: To obtain ground truth future point clouds for the objects, we first record human demonstrations for 20 evaluation episodes. We store both RGB and depth images during this process. For each evaluation episode, we manually match the initial states to be close to pixel-perfect and obtain the open-loop action sequence by estimating object pose transformations between the initial frame and future frames for each time step in human demonstrations. We use the same motion filters in Im2Flow2Act to ensure fair evaluation. Furthermore, we provide the maximum available points (without downsampling) from the task flow. We manually check the transformed point cloud by overlapping the transformed initial frame point cloud and future frame point cloud to ensure the transformation is largely correct under the noisy conditions of the real-world depth camera."}, {"title": "D Training Details", "content": "For the rectangular flow image, we set the spatial resolution to \\(H = W = 32\\) and \\(T = 32\\), generating flow for 1024 keypoints over 32 steps. We finetune the decoder from StableDiffusion for 400 epochs with a learning rate of 5e 5. To obtain these keypoints, we uniformly sample them from the bounding box provided by Grounding DINO. For training AnimateDiff, we insert the LORA (Low-Rank Adaptation) with a rank of 128 into the Unet from StableDiffusion and train the motion module layer from scratch with learning rate of \\(1 \\times 10^{-4}\\) for 4000 epochs using AdamW [69] optimizer with weight deacy \\(1 \\times 10^{-2}\\), betas (0.9, 0.999) and epsilon \\(1 \\times 10^{-8}\\). We load the pretrained (openai/clip-vit-large-patch14) weights from CLIP [63] to process the initial frame and freeze them during the entire training. Zero-initialized linear layers are used to process the patch embedding and the initial keypoints embedding before passing the conditions into the cross-attention layers."}, {"title": "D.2 Flow-Conditioned Imitation Learning Policy", "content": "Training Data Format: A training sample consists of \\((\\rho_t, f_t, a_t, F_{0:T})\\), where \\(\\rho_t\\) is the proprioception data, \\(a_t\\) is a sequence of actions \\(a_t,..., a_{t+L}\\) of length L, and \\(f_t\\) contains the locations (u, v) of \\(N = 128\\) object keypoints in the image space at time t. We set the object flow (i.e., task flow) horizon \\(T = 32\\), which matches the output of the flow generation network. The action sequence length is set to 16. The N keypoints are randomly selected from all available keypoints for every training sample during the training process. To construct the task flow \\(F_{0:T}\\), we randomly select T"}, {"title": "E Inference Details", "content": "In this section, we describe the details of the inference process, which includes Grounding DINO, motion filters, and online point tracking."}, {"title": "E.1 Grounding DINO", "content": "For each task, we begin by using Grounding DINO to identify the object of interest. We manually provide the keyword to the model; however, this process could potentially be automated using a large language model to find the desired object in the task description. Specifically, we employ the grounding-dino-base model to extract the object's bounding box. The keywords used for the pick & place and pouring tasks are \u201cgreen cup\". For drawer opening, the keyword is \"yellow drawer\", and for cloth folding, it is \"checker cloth\". The input images are processed at a resolution of 480x640."}, {"title": "E.2 Motion Filters", "content": "We use motion filters to process the object flow (i.e., task flow) generated from the flow generation model. As explained in the main paper, the initial keypoints are constructed by uniformly sampling within the bounding box. This approach inevitably yields keypoints that are not on the object, specifically, keypoints that fall on the background. To address this, we deploy several filters simultaneously to remove these background keypoints. Additionally, we implement depth filters to eliminate keypoints that lack depth data from noisy real-world depth image.\nMoving Filter: In the training set, keypoints sampled on the background remain static in the image space, as only the object is moving. Therefore, we deploy a moving filter during inference time to remove keypoints whose movement in the image space (256x256) is below a certain threshold. We find that this filter effectively eliminates most background keypoints. In real-world experiments, we set the threshold as 20 for pick & place, pouring, and drawer opening tasks, and as 10 for cloth folding.\nSAM Filter: To further remove points after applying the moving filter, we deploy the Segment Anything Model (SAM) [70]. Specifically, we first resize the initial frame to 256x256 and pass it"}]}