{"title": "Flow as the Cross-Domain Manipulation Interface", "authors": ["Mengda Xu", "Zhenjia Xu", "Yinghao Xu", "Cheng Chi", "Gordon Wetzstein", "Manuela Veloso", "Shuran Song"], "abstract": "We present Im2Flow2Act, a scalable learning framework that enables\nrobots to acquire manipulation skills from diverse data sources. The key idea\nbehind Im2Flow2Act is to use object flow as the manipulation interface, bridg-\ning domain gaps between different embodiments (i.e., human and robot) and\ntraining environments (i.e., real-world and simulated). Im2Flow2Act comprises\ntwo components: a flow generation network and a flow-conditioned policy. The\nflow generation network, trained on human demonstration videos, generates ob-\nject flow from the initial scene image, conditioned on the task description. The\nflow-conditioned policy, trained on simulated robot play data, maps the generated\nobject flow to robot actions to realize the desired object movements. By using\nflow as input, this policy can be directly deployed in the real world with a minimal\nsim-to-real gap. By leveraging real-world human videos and simulated robot play\ndata, we bypass the challenges of teleoperating physical robots in the real world,\nresulting in a scalable system for diverse tasks. We demonstrate Im2Flow2Act's\ncapabilities in a variety of real-world tasks, including the manipulation of rigid,\narticulated, and deformable objects.", "sections": [{"title": "1 Introduction", "content": "A key step for scaling up robot learning is giving robots the ability to learn from various data sources\nbeyond expensive real-world robot data [1, 2, 3, 4, 5]. Prior work has approached this problem from\ntwo main directions: learning from cross-embodiment videos [6, 7, 8, 9, 10, 11, 12] and simulated"}, {"title": "2 Related Work", "content": "Flow-based manipulation. Robot policies leveraging flows have demonstrated promising results\nin manipulating articulated objects [19, 20] and tools [21]. However, these approaches are limited\nto specific tasks. With recent advances in point tracking algorithms in computer vision, flows can\nbe estimated in a video sequence through learning-based methods. Vecerik et al. [15] achieved few-shot learning through tracking flows but required explicit pose estimation and was still limited to\nrigid objects. To make flow applicable to more general manipulation tasks, Wen et al. [13] learned\na flow-conditioned behavior cloning policy that can be applied to both articulated and deformable\nobjects. However, it still required collecting in-domain robot data through teleportation in the real\nworld, which is costly and challenging to scale up. To ease the data challenge, Yuan et al. [14]\nproposed a heuristic flow-based control policy but required manual positioning of the robot gripper\non the object, making the overall system less autonomous. More recently, Bharadhwaj et al. [16]\npropose to learn a residual policy on top of the heuristic-based policy but still require collecting\nadditional in-domain robot data. In Im2Flow2Act, we train a data-efficient and fully autonomous\nflow-conditioned imitation learning policy from task-less simulation play datasets which does not\nrequire costly in-domain robot data collection and can be transferred to realworld in one-shot.\nLearning from cross-embodiment data. A large body of work [10, 22, 23, 24, 25] has stud-\nied leveraging cross-embodiment data to learn robotic policies. Prior works explore different di-\nrections, including visual pretraining [8, 26, 27], learning reward functions [7, 28, 29, 30], ex-\ntracting affordances [31, 9, 32, 33], hand pose detection [6, 34, 35], and domain translation\n[36, 37, 38, 39, 40, 41, 42]. As most of cross-embodiment data lacks explicit action or the action\nis challenging to transfer due to large embodiment gaps, many prior works still require collecting\nin-domain robot data [43, 41, 12, 11, 44] to mitigate the large embodiment gap. More recently,\na number of works [45, 13, 14] have learned video generation or flow generation models through\ncross-embodiment data. Although showing promising results, these generation models still require\ncostly in-domain robot data to finetune or large computational resources. In Im2Flow2Act, we pro-\npose to learn a flow generation model that only predicts the flow of objects but not the embodiment,\nallowing us to seamlessly learn from human videos and do not require any in-domain robot data.\nFurthermore, we formulate the object flow into a sequence of structured flow images, which enables\nus to leverage pretrained image generation models [18] to achieve more efficient training.\nSim2Real Transfer. To overcome the data collection challenge in the real world, a desired approach\nis to train a robot policy in simulation and deploy it in the real world. Due to the sim-to-real gap,\nvarious techniques have been employed to bridge this gap, including but not limited to using depth\nobservation [46, 47, 48, 49], domain randomization [50, 51, 52, 53, 54, 55], knowledge distillation\n[56, 57], and system identification [58]. Learning a sim-to-real policy typically requires task-specific"}, {"title": "3 Method", "content": "We aim to build a framework that leverages object flow as a unified interface for robots to ac-\nquire real-world manipulation skills using both cross-environment and cross-embodiment data. Our\nframework (See Fig. 2) comprises two components: an open-looped flow generation network which\nrun once at the beginning of the task and a closed-loop flow-conditioned imitation learning policy\nact adaptively based on the current state. During inference, the flow generation network first gen-\nerates a complete task flow conditioned on a set of initial object key points at the beginning of the\ntask, and then the policy executes the actions to reproduce the generated flow. The final system is\ncapable of one-shot generalization for new skills and novel objects in the real world. Our key idea\nis to use object flows as a general and expressive interface to bridge:\nCross-Embodiment data: Transferring actions between different embodiments is often hindered by\nthe embodiment gap. However, object flow encapsulates transferable task knowledge independent of\nembodiment-specific actions. To exploit this, we develop a flow generation network focused solely\non extracting object flows from cross-embodiment videos.\nCross-Environment data: In this work, cross-environment majorly refers to simulation and real-\nworld environments. Visual inconsistencies, such as differences in scene backgrounds and object\ntextures, pose challenges in transferring learned policies from simulation to real environments.\nConversely, object flow captures motion dynamics\u2014changes in object pose, articulation, and defor-\nmation-consistent across both simulated and real settings, providing an ideal interface for policy\nlearning. Our flow-conditioned imitation learning policy is learned entirely from simulated play\ndata. The randomness in play data actions fosters an understanding of the relationship between ac-\ntions and resulting object flows, further reducing the need for task-specific simulation environments."}, {"title": "3.1 Flow Generation Network", "content": "For the flow generation network, our main idea is to form permutation invariant object flows into\na sequence of structured rectangular flow images. This allows us to better utilize pretrained image\ngeneration models and video generation architectures for more efficient training. Specifically, we\nobtain the bounding box of the object of interest through Grounding DINO [59] in the initial RGB\nframe and then perform uniform sampling within this bounding box, resulting in an rectangular flow\n$F_o \\in \\mathbb{R}^{3\\times H \\times W}$, with three channels at a spatial resolution of $H \\times W$. The first two channels\nrepresent the u, v coordinates of object keypoints in the image space, while the third channel rep-\nresents their visibility. We then leverage point tracking methods TAPIR [60] on the videos to get\nthe rectangular flow sequence $F_{1:T} \\in \\mathbb{R}^{3 \\times T \\times H \\times W}$ with T steps in the temporal space. With this\nflow representation, we utilize the existing diffusion video generation architecture AnimateDiff [17]\nas our flow generator network, conditioning it on an initial RGB frame and a task description to\ngenerate object flow. The generated flow is then processed by motion filters to preserve only the"}, {"title": "3.2 Flow-Conditioned Imitation Learning Policy", "content": "Our imitation learning policy $p(a_t | F_{0:T}, s_t, p_t)$ takes the object flow $F_{0:T}$ for a complete task (i.\u0435.,\ntask flow), the current state representation $s_t$, and the robot proprioception $p_t$ as input. The output\nis a sequence of actions ${a_t,..., a_{t+L}}$ of length L, starting from the current time t, denoted as\n$a_t$. The robot action $a_t$ includes a 6-DOF end-effector position in Cartesian space and a 1-DOF\ngripper open/close state. The state representation $s_t$ is formed by keypoints location at current\nframe denoted as $f_t$ containing N keypoints' locations ${(u, v)^i}_{i=1}^N$ in the image space at time t\nand the 3D coordinates of N keypoints $x_0 \\in \\mathbb{R}^{N\\times3}$ at the initial frame.\nDuring inference, the task flow F is generated once by the flow generation network at the beginning\nof the task, as described in Sec. 3.1. We leverage an online point tracking algorithm to obtain the $f_t$\nduring robot manipulation for the same set of keypoints. The policy consists following components:\na state encoder $\\phi$ takes the N keypoints encapsulate in the $f_t$ and their corresponding initial 3D\ncoordinates $x_0$ as input to generate state representation $s_t$, a temporal alignment module $\\psi$ which\ncompares task flow and current keypoints location $f_t$ to infer remaining task flow, and finally, a\ndiffusion action head [65] to output the robot action.\nTraining data: We briefly describe how we construct the training data and some decision choice\n(See supplementary for more details). For an episode with time duration of T', we run the point\ntracking algorithm to get location of keypoints on object being manipulated at each step, i.e, $f_{0:T'}$.\nWe can get a dataset $\\mathbb{D}$ in the form of trajectories $T_k = {(\\rho_0, f_0, a_0), \\dots, (\\rho_{T'}, f_{T'}, a_{T'})}$. During the"}, {"title": "4 Evaluation", "content": "We demonstrate Im2Flow2Act's capability across 4 tasks ranging from rigid, articulate, and de-\nformable objects, including pick-and-place, pouring, open drawer, and folding cloth. The task de-\nscription can be seen in the leftmost column in Fig. 4.\n4.1 Experiment Details\nTraining data: We collect robot play data in simulation and human hand demonstration in the\nrealworld. (i.) In simulation, we collect play data across rigid, articulated, and deformable objects\nusing a UR5e robot through a set of predefined random heuristic actions for exploration. The details\nfor each exploration strategy can be found in the supplementary material. We trained one imitation\nlearning policy with all the play data. (ii.) In realworld, we collect 100 human hand demonstrations\nfor each task. We mixed the data together and used it to train the flow generation model.\nComparisons. We compare our method with the following baselines:\n\u2022 Grid Flow: Similar to ATM [13], we replace the initial object keypoints with a set of uniformly\nsampled keypoints over the whole image.\n\u2022 Heuristic: A heuristic action policy selects object contact points and applies a pose estimation\nmethod, such as RANSAC, on the future object flow (requiring 3D flow) to infer robot actions,\nsimilar to General Flow and Track2Act [14, 16]. This baseline examines the necessity of having\na learning-based policy for translating flow into actions.\n\u2022 No alignment: To ablate our alignment module design, we removed the alignment model $\\psi$ and\nthe policy condition on the complete task flow."}, {"title": "4.2 Key Findings", "content": "Flows can effectively bridge different data\nsource: Im2Flow2Act achieves best perfor-\nmance among all baseline in both simulation\nand realworld, as shown in both Tab 1 and Tab\n2. Further, the performance only drops 15%\non average in realworld compare to in simula-\ntion. As shown in Fig. 4, our learned policy\ncan largely follow the generated flow to com-\nplete the desired tasks across rigid, articulated and deformable objects. This suggests that object\nflow is a good interface to connect the both cross-embodiment and cross-environment data sources.\nLearning-based policy is necessary for translating flow to actions: To map the generated flow to\nrobot actions, we used an learned policy and alignment network. In contrast, prior works [16, 14]\nuse heuristic-based policy to compute robot action from the flow. When compared to them, we"}, {"title": "4.3 Limitation and Future Work", "content": "Our system uses 2D flow as the manipulation interface. This design allows us to leverage large-\nscale 2D videos; however, it is intrinsically ambiguous for representing 3D actions. For example,\na major failure case for pouring tasks occurs when the robot's actions are not precise enough along\nthe z-axis of the camera coordinates. Additionally, our framework assumes that object dynamics\nare consistent between simulation and realworld, i.e., the same robot action should lead to the same\nobject flow. However, simulating deformable objects is a challenging task. Therefore, we observe\na significant drop in success rates in the cloth folding task when deploy policy to realworld. In\nour current system, we assume the camera view point in simulation is calibrated with respect to the\ntesting environment, and action is visible from the camera view (note, viewpoints for flow generation\nis not constrained). This limitation can be potentially addressed with 3D flow [14, 68]. Moreover,\nby using flow as action abstraction the algorithm also ignores some action details. Hence, it may not\ncapture sufficient information for some dexterous tasks such as in-hand manipulation."}, {"title": "5 Conclusion", "content": "We introduce Im2Flow2Act, a scalable learning framework that enables robots to acquire diverse\nmanipulation skills from cost-effective cross-domain data using object flow as a unifying interface.\nOur final system demonstrates strong real-world manipulation capability by outperforming all base-\nlines across various types of manipulation tasks. Our work paves the way for future research to scale\nup robotic manipulation skills from diverse data sources."}, {"title": "A Data Collection", "content": "In Im2Flow2Act, we collect two types of data: a simulated robot play dataset and real-world human\ndemonstration videos. Note that we do not collect any real-world robot data. Below, we detail the\ndata collection process.\nA.1 Simulated Play Data\nWe use MuJoCo as our simulation engine and have constructed three types of play environments\nfeaturing rigid, articulated, and deformable objects. An UR5e robot explores these environments\nusing a set of predefined random heuristic actions.\nRigid: In the rigid play environment, the robot can interact with five different objects, including a\ntoy car, a shoe, a mini-lamp, a frying pan, and a mug. At the beginning of each episode, one object\nis randomly selected and placed on the table. The robot first picks up the object and starts executing\n6DoF random trajectories. A random trajectory is constructed as a 3D cubic B\u00e9zier curve with four\nkey waypoints: the start point $p_0$, two control points $p_1$ and $p_2$, and the end point $p_3$. The start point"}, {"title": "A.2 Real-world Human Demonstration Video:", "content": "We collect in-domain human demonstration videos for four tasks: pick & place, pouring, opening\ndrawers, and folding cloth. We use a RealSense camera to record the demonstrations at 30 FPS.\nThe descriptions below also serve as detailed task descriptions. In Im2Flow2Act, we use the human\nvideos to provide the system with task information. We define the robot base as the origin of the\nworld coordinate system and use the Right-Handed Coordinate System.\n\u2022 Pick & Place: Pick up a green cup and place it into a red plate. The red plate is randomly placed\non one side of the table, while the cup is placed randomly in the middle of the table.\n\u2022 Pouring: Pick up a green cup and hover it over a red bowl, then rotate the cup towards the bowl.\nThe initial placement of the cup and the bowl is the same as in the pick & place task.\n\u2022 Drawer Opening: Fully open a drawer that is randomly placed on one side of the table. The\npose of the drawer, including its position and orientation, can vary. The drawer is not rigidly\nattached to the table.\n\u2022 Cloth Folding: Fold a 33 cm x 33 cm cloth from one corner (either lower left or lower right) to\nthe diagonal. The cloth is randomly placed in the middle of the table."}, {"title": "B Ablation study", "content": "We conducted an ablation study in simulation to evaluate the impact of using pretrained StableD-\niffusion (SD) versus training it from scratch on the flow generation model. In this ablation study,\nwe still use pretrained AE (auto-encoder) from the SD but trained the U-Net from scratch instead\nof incorporating LoRA layers. To ensure a fair comparison, we deploy the same flow-conditioned"}, {"title": "C Experimental Details", "content": "C.1 Real-World Setup\nWe use a UR5e robot equipped with a WSG-50 gripper. During inference, the UR5e receives end-\neffector space positional commands from a 2.5 Hz policy. We limit the end-effector's speed to less\nthan 0.2 m/s and restrict its position to at least 1 cm above the table for safe execution. A RealSense\nD415 depth camera is mounted at the table to capture policy observations, including the initial depth"}, {"title": "C.2 Real-World Evaluation Protocol", "content": "In this section, we describe the details of the evaluation protocol in the real world.\n1) Initial State: When evaluating our system, we do not match the background scene setup to those\nrecorded in the human demonstrations such that we can also test the generalization capability of\nflow generation network. For the initial position of objects, they are placed with roughly the same\ndistribution as in the human video demonstrations.\n2) Success Metric: Below are the details of the real-world success metric for all four tasks:\n\u2022 Pick & Place: The robot needs to pick up the cup and place it into the red plate. We do not\nrequire the mug to be centered on the plate. We consider one episode successful if: i. the robot\ncan steadily place the mug onto the red plate; ii. the plate does not move more than 5 cm on the\ntable during the robot's manipulation process.\n\u2022 Pouring: The robot needs to pick up the cup, hover it near the bowl, and execute pouring actions.\nWe consider one episode successful if: i. the robot demonstrates the complete pouring behavior\nby rotating the cup more than 30 degrees; ii. at least half of the cup overlaps with the red bowl\non the x-axis in terms of world coordinates."}, {"title": "C.3 Evaluation Procedure", "content": "Im2Flow2Act with/without alignment: For each task, we first record the initial frame with differ-\nent initial states for 20 evaluation episodes. We then query Grounding DINO on the object of interest\nto obtain the bounding boxes in all initial frames. Using the bounding box, initial frame, and the\ntask description, we generate the object flow (task flow) for all episodes and store them as a buffer\non the disk. With all ingredients for policy inference set, we start policy evaluation for each episode\nby manually matching the initial states to be close to pixel-perfect within the mounted RealSense\ncamera and load the corresponding generated flow from the previously saved flow buffer.\nHeuristic-based policy: To obtain ground truth future point clouds for the objects, we first record\nhuman demonstrations for 20 evaluation episodes. We store both RGB and depth images during\nthis process. For each evaluation episode, we manually match the initial states to be close to pixel-\nperfect and obtain the open-loop action sequence by estimating object pose transformations between\nthe initial frame and future frames for each time step in human demonstrations. We use the same\nmotion filters in Im2Flow2Act to ensure fair evaluation. Furthermore, we provide the maximum\navailable points (without downsampling) from the task flow. We manually check the transformed\npoint cloud by overlapping the transformed initial frame point cloud and future frame point cloud\nto ensure the transformation is largely correct under the noisy conditions of the real-world depth\ncamera."}, {"title": "D Training Details", "content": "D.1 Flow Generation Network\nFor the rectangular flow image, we set the spatial resolution to $H = W = 32$ and $T = 32$,\ngenerating flow for 1024 keypoints over 32 steps. We finetune the decoder from StableDiffusion\nfor 400 epochs with a learning rate of $5e-5$. To obtain these keypoints, we uniformly sample\nthem from the bounding box provided by Grounding DINO. For training AnimateDiff, we insert the\nLORA (Low-Rank Adaptation) with a rank of 128 into the Unet from StableDiffusion and train the\nmotion module layer from scratch with learning rate of $1 \\times 10^{-4}$ for 4000 epochs using AdamW\n[69] optimizer with weight deacy $1 \\times 10^{-2}$, betas (0.9, 0.999) and epsilon $1 \\times 10^{-8}$. We load the\npretrained (openai/clip-vit-large-patch14) weights from CLIP [63] to process the initial frame and\nfreeze them during the entire training. Zero-initialized linear layers are used to process the patch\nembedding and the initial keypoints embedding before passing the conditions into the cross-attention\nlayers."}, {"title": "D.2 Flow-Conditioned Imitation Learning Policy", "content": "Training Data Format: A training sample consists of $(p_t, f_t, a_t, F_{0:T})$, where $p_t$ is the proprioc\u0435\u0440-\ntion data, $a_t$ is a sequence of actions $a_t, ..., a_{t+L}$ of length L, and $f_t$ contains the locations (u, v)\nof $N = 128$ object keypoints in the image space at time t. We set the object flow (i.e., task flow)\nhorizon T = 32, which matches the output of the flow generation network. The action sequence\nlength is set to 16. The N keypoints are randomly selected from all available keypoints for every\ntraining sample during the training process. To construct the task flow $F_{0:T}$, we randomly select T"}, {"title": "E Inference Details", "content": "In this section, we describe the details of the inference process, which includes Grounding DINO,\nmotion filters, and online point tracking.\nE.1 Grounding DINO\nFor each task, we begin by using Grounding DINO to identify the object of interest. We manually\nprovide the keyword to the model; however, this process could potentially be automated using a\nlarge language model to find the desired object in the task description. Specifically, we employ the\ngrounding-dino-base model to extract the object's bounding box. The keywords used for the pick &\nplace and pouring tasks are \u201cgreen cup\u201d. For drawer opening, the keyword is \u201cyellow drawer\u201d, and\nfor cloth folding, it is \u201cchecker cloth\u201d. The input images are processed at a resolution of 480x640.\nE.2 Motion Filters\nWe use motion filters to process the object flow (i.e., task flow) generated from the flow gener-\nation model. As explained in the main paper, the initial keypoints are constructed by uniformly\nsampling within the bounding box. This approach inevitably yields keypoints that are not on the\nobject, specifically, keypoints that fall on the background. To address this, we deploy several filters\nsimultaneously to remove these background keypoints. Additionally, we implement depth filters to\neliminate keypoints that lack depth data from noisy real-world depth image.\nMoving Filter: In the training set, keypoints sampled on the background remain static in the image\nspace, as only the object is moving. Therefore, we deploy a moving filter during inference time to\nremove keypoints whose movement in the image space (256x256) is below a certain threshold. We\nfind that this filter effectively eliminates most background keypoints. In real-world experiments, we\nset the threshold as 20 for pick & place, pouring, and drawer opening tasks, and as 10 for cloth\nfolding.\nSAM Filter: To further remove points after applying the moving filter, we deploy the Segment\nAnything Model (SAM) [70]. Specifically, we first resize the initial frame to 256x256 and pass it"}]}