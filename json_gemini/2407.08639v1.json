{"title": "B-DPO: Direct Preference Optimization with Dynamic B", "authors": ["Junkang Wu", "Yuexiang Xie", "Zhengyi Yang", "Jiancan Wu", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Xiangnan He"], "abstract": "Direct Preference Optimization (DPO) has emerged as a compelling approach for training Large Language Models (LLMs) to adhere to human preferences. However, the performance of DPO is sensitive to the fine-tuning of its trade-off parameter \u03b2, as well as to the quality of the preference data. We analyze the impact of \u1e9e and data quality on DPO, uncovering that optimal \u1e9e values vary with the informativeness of pairwise data. Addressing the limitations of static \u1e9e values, we introduce a novel framework that dynamically calibrates \u1e9e at the batch level, informed by data quality considerations. Additionally, our method incorporates B-guided data filtering to safeguard against the influence of outliers. Through empirical evaluation, we demonstrate that our dynamic \u1e9e adjustment technique significantly improves DPO's performance across a range of models and datasets, offering a more robust and adaptable training paradigm for aligning LLMs with human feedback. The code is available at https://github.com/junkangwu/beta-DPO.", "sections": [{"title": "Introduction", "content": "The alignment of Large Language Models (LLMs) with human feedback, as explored in works like GPT-4 and LLaMA-2 [22, 31, 8], has marked a significant advancement in generating responses that are more helpful, factual, and ethical [23]. Among the various alignment strategies, Reinforcement Learning from Human Feedback (RLHF) [23] is a notable method that refines LLMs using the Proximal Policy Optimization (PPO) algorithm [29]. This approach employs a KL divergence penalty to ensure minimal deviation of the model from its original configuration, ensuring the retention of its initial characteristics while improving alignment.\nDespite the effectiveness, RLHF's instability and computational requirements often limit its practical applications, prompting the exploration of alternatives like Direct Preference Optimization (DPO) [27]. DPO circumvents the reinforcement learning loop by exploiting the inherent connection between reward functions and optimal policies, thereby simplifying the policy model training. It encourages the model to favor the response that aligns with human preferences (yw) over the dispreferred (y\u03b9), implying DPO's sensitivity to the quality of pairwise data. The balance between maintaining the original reference model (\u03c0ref) and incorporating new preferences (\u03c0\u03b8) is controlled by a B hyperparameter, whose lower values advocate for aggressive updates and higher values support more"}, {"title": "Related Work", "content": "Reinforcement Learning from Human Feedback. Despite RLHF's effectiveness in aligning language models (LMs) with human values [9, 2, 31, 23], its complexity and resource demands have spurred the exploration of alternatives. RAFT [10] selects optimal training samples via an existing reward model, whereas RRHF [37] employs a simpler ranking loss, retaining PPO's efficiency. Diverse from these, DPO [27] directly optimizes LMs using a preference-based loss function, showcasing enhanced training stability in comparison to traditional RLHF. Innovatively, SLiC-HF [39] and KTO [11] devise loss functions rooted in human decision-making, focusing on preference calibration and utility optimization, respectively. Further, RSO [19] and ORPO [15] introduce efficient preference modeling and optimization, with ORPO uniquely combining supervised fine-tuning and preference alignment. These advancements reflect the ongoing shift towards more efficient, nuanced RL methods.\nData Quality in LLM's Alignment. Recent studies have increasingly recognized the significance of data quality in the alignment of LLMs. For example, LIMA [40] leverages heuristics such as post scores, response lengths, formatting, and topics to manually craft 1000 high-quality datasets from StackExchange, wikiHow, and Reddit for superficial alignment. In a similar vein, Bai et al. [3] prioritize data points based on user engagement levels for dataset assembly. Rejection Sampling (RS) and Best-of-N (BoN) techniques, as evidenced in the works of Nakano et al. [21] and Gao et al. [12], involve selecting the optimal candidate from N generated possibilities through the application of a reward model. To enhance preference optimization, RSO [19] uses statistical weightings to differentiate outcomes from an optimal policy and a base SFT policy. Besides, fDPO [20] employs a Reward Model to filter out low-quality data, effectively addressing dataset quality concerns."}, {"title": "Preliminaries", "content": "Given a text sequence (commonly referred to as a prompt) x, a sequence y = [y1, y2,... yN] is generated as a response to the prompt x. An autoregressive language model \u03c0, when provided with the prompt x, can generate the response sequence y following the probability decomposition:\n$$\u03c0(y|x) = \\prod_{t=1}^{N} \u03c0(y_t|x, y_{<t}),$$"}, {"title": "Method", "content": "In this section, we investigate the critical connection between the parameter \u03b2 and the quality of pairwise data in optimizing DPO. We present empirical evidence demonstrating the effect of \u03b2 settings on DPO performance across datasets of varying quality. Our proposed method, \u03b2-DPO, introduces dynamic calibration of \u03b2 and a data filtering mechanism tailored to improve DPO's effectiveness across diverse data conditions."}, {"title": "Motivation: The Impact of Pairwise Data Quality on \u1e9e Selection", "content": "Scrutinizing Equation (4), we argue that DPO's effectiveness critically hinges on two factors: the choice of \u03b2 and the quality of pairwise data. Here, we conduct experiments to demonstrate the influence of variations in \u03b2 and data quality on DPO, pivotal for its effective real-world application.\nDatasets. We utilize the Anthropic HH dataset [2] for our experimental analysis, which contains approximately 170,000 dialogues between humans and an automated assistant. In this dataset, a human inquiry, denoted as x, is paired with two responses (yw, y\u03b9), where yw represents the response favored by the human annotator, while y\u03b9 is the alternate response. Notably, the alternate response y\u03b9 retains informational value, making this dataset high-quality with minimal discrepancies between the response pairs, which we classify as a low gap dataset. To further explore the impact of data quality on DPO, we construct a synthetic dataset, referred to as the high gap dataset. This dataset differs from the low gap dataset by introducing a greater disparity between responses. Specifically, the alternative response y\u03b9 is generated by a Supervised FineTuned (SFT) Pythia-2.8B model, while the preferred response yw remains consistent with the original dataset. We also combine the two datasets in equal proportion to create a mixed gap dataset, with each contributing 50%, to incorporate the characteristics of both the low gap and high gap datasets.\nModels and Metrics. Our study evaluates various model sizes, specifically Pythia-410M, Pythia-1.4B, and Pythia-2.8B [5], to ensure a comprehensive assessment. Following the established protocol in DPO [27], each model iteration undergoes a single epoch with a batch size of 64. This setup provides a uniform basis for evaluation across different models. We adopt the evaluation strategy from DPO [27] to calculate the win rate, a metric that measures how often the GPT-4 model prefers a response generated by our models over the default chosen response on the subset of the test dataset.\nFindings: (1) The optimal value of \u03b2 varies with data quality, reflecting divergent perfor- mance patterns across datasets. In Figure 2, we present the win rate results across three levels of pairwise data gap, each evaluated under varying \u03b2 parameters. As can be observed, with low gap pairwise data, a smaller \u03b2 value is preferable for optimizing performance. This is likely"}, {"title": "Method: Dynamic \u1e9e Calibration in DPO", "content": "Through our empirical analysis, we highlight the sensitivity of DPO to \u03b2 selections and the frequent occurrence of outliers. Hence, determining the optimal \u03b2 value requires careful consideration of the quality of pairwise data while also addressing the influence of outliers. This prompts the question: what criteria define a superior choice of \u03b2? In response, we propose the following guiding principles:\nPrinciple 1: The optimal \u03b2 value should be responsive to pairwise data's quality.\nPrinciple 2: The selection of \u03b2 value should minimize the influence of outliers."}, {"title": "Dynamic \u1e9e Calibration at Batch-Level", "content": "We begin by introducing the concept termed \u2018individual reward discrepancy', which represents the difference between the rewards of winning and losing for each triplet, serving as a measurement for pairwise data quality. Formally, for a triplet (x(i),yw(i),y\u03b9(i)) \u2208 D, the individual reward discrepancy is defined as Mi = r(yw(i); x(i)) \u2212r(y\u03b9(i); x(i)). Motivated by our guiding principles, a straightforward approach is to assign a distinct \u03b2 to each triplet, allowing each \u03b2 to serve as a parameter tailored to its respective triplet. This instance-level dynamic \u03b2 adaption can be formulated as follows:\n$$\u03b2_i = \u03b2_0 + \u03b1(M_i \u2212 M_0)\u03b2_0 = [1 + \u03b1(M_i \u2212 M_0)]\u03b2_0,$$"}, {"title": "\u00df-Guided Data Filtering", "content": "To mitigate the adverse impact of outliers on the \u03b2 selection process, we introduce a \u03b2-guided data filtering mechanism. Informed by 3\u03c3 confidence criterion [26], this strategy employs a probabilistic model to assess the significance of each triplet (x(i), yw(i), y\u03b9(i)) based on its individual reward discrepancy Mi, which is defined as:\n$$p(M_i) = \\frac{1}{\\sqrt{2\u03c0\u03c3}} exp( \u2212\\frac{(M_i \u2212 M_0)^2}{2\u03c3^2} )$$"}, {"title": "Discussion with Previous Studies", "content": "Relations to Data Selection. An increasing volume of works [19, 20, 40, 25, 36] have underscored the impact of data quality on the performance of LLM's alignment. A common practice among these efforts involves employing a so-called \u201cgold model\u201d for data selection. This approach, however, introduces significant computational demands and the choice of the gold model directly influences the resultant system's performance. The focus of this work, it should be noted, is not to propose a superior strategy for data selection. Instead, we aim to enhance adaptability to the quality of data by dynamically adjusting the \u03b2 parameter. This adjustment facilitates improved \u03b2 estimation by selecting data based on the reward. Moreover, Section 5.2 illustrates the compatibility of dynamic B adjustment with other data selection methodologies.\nRelations to Recent Temperature Schemes. Dynamic temperature frameworks have been introduced in the realm of contrastive learning, motivated by various objectives, such as addressing out-of- distribution tasks [38] or accommodating long-tail data distributions [17]. The work most closely related to ours, MACL [16], has indeed proposed an alignment-adaptive strategy; however, its primary aim was to navigate the uniformity-tolerance dilemma. Hence, the integration of dynamic temperature mechanisms with LLM's alignment remains an underexplored area against this backdrop."}, {"title": "Experiments", "content": "In this section, we commence by conducting an empirical evaluation of \u03b2-DPO on two specific tasks: dialogue generation and summarization. Subsequently, we analyze the various adaptations of the proposed method \u03b2-DPO. Concluding this section, we underscore the imperative need for batch-level dynamic \u03b2 calibration, highlighting its significance in the context of our study."}, {"title": "Empirical Evaluation of \u03b2-DPO on Dialogue Generation and Summarization", "content": "Datasets and Setup. Our experiments are conducted on the Anthropic HH dataset [2] and Reddit TL;DR summarization dataset [32]. The training configuration follows from Rafailov et al. [27]. The goals of these experiments are to study: 1) How \u03b2-DPO performs on single-turn dialogue generation and summarization tasks; 2) How the sampling temperature affects the performance of B-DPO; 3) How B-DPO works with different model sizes. For detailed experimental settings, please refer to Appendix A.1.\nBaselines. In our comparison, we examine the performance of \u03b2-DPO relative to its counterparts: the standard DPO, DPO implemented with a dynamic \u03b2 yet devoid of B-guided data filtering, and DPO complemented by data filtering with \u03b2 fixed at 0.1."}, {"title": "Adaptations of B-DPO", "content": "In this section, our inquiry is twofold: first, we aim to understand the performance of B-DPO when applied across various filtering strategies; second, we examine its efficacy across different adaptations of the DPO framework. In terms of filtering strategies, prevailing studies [25, 36] in the domain largely employ a gradient-based approach. We propose to extend this methodology into three distinct scenarios. This involves arranging the gradients of pairwise data within a batch and consequently: (1) Excluding the top 20% of samples, hereby referred to as Filter Head, (2) Excluding the bottom 20% of samples, hereby referred to as Filter Tail, (3) Excluding both the top and bottom 10% of samples, a method we denote as Filter Tail & Head. For a fair comparison, we maintain the amount of data excluded at 20% for the above strategies. Second, we integrate three variants of DPO into our analysis: the IPO [1], a novel approach that facilitates learning directly from preferences without the need for the Bradley-Terry (BT) model. Additionally, we consider the KTO [11], which focuses on discerning whether a preference is desirable or undesirable and SPPO [35], which approximates the Nash equilibrium. For detailed settings, we refer the reader to the supplementary material.\nSelective filtering of the top 20% of samples markedly enhances model performance. This approach, detailed in Figure 5 (Left), not only surpasses other filtering strategies but also suggests that these samples, which exhibit the smallest discrepancies between positive and negative pairs, are particularly prone to flipped noise. By excluding them, the model's learning efficacy is appreciably improved.\nDynamic \u1e9e adapts to and improves upon existing filtering strategies. Figure 5 (Left) corroborates our stance that a static \u1e9e proves insufficient within the DPO framework. We contend that the applica- tion of our dynamic \u1e9e-DPO could markedly reshape the DPO field by fostering the development of advanced filtering techniques.\nDynamic \u1e9e Enhancement across DPO Variants. We introduce dynamic \u1e9e-DPO, a novel strategy enhancing DPO and its variants: IPO, KTO, and SPPO in Figure 5 (Middle). Our results on the Anthropic HH dataset demonstrate that while IPO initially leads in performance, the integration of"}, {"title": "Necessity of Batch-Level Dynamic \u1e9e Calibration", "content": "In this section, we aim to underscore the pivotal role of batch-level tuning in calibrating the parameter \u03b2. To this end, we compare the performance of our \u03b2-DPO algorithm under two distinct regimes: one employing batch-level dynamic \u03b2 calibration, and the other utilizing instance-level dynamics. To emulate the diverse data disparity scenarios encountered in practical applications, we adopt the methodology outlined in Section 4.1, meticulously blending datasets characterized by both low gap and high gap attributes at varying ratios.\nBatch-level calibration surpasses both instance-level and population-level approaches. The results presented in Table 2 illustrate that batch-level dynamic \u03b2 calibration yields superior perfor- mance compared to instance-level dynamics and the baseline population-level approach (referred to as vanilla DPO) across a range of mixture ratios. This improvement can be credited to the batch-level calibration's ability to adjust to the varying data quality present within a batch, thus refining the model's learning process. Conversely, instance-level dynamics can provoke excessively vigorous model updates, precipitating a decline in performance particularly noticeable at a mixture ratio of 40%, a scenario in which outliers exert a significant negative impact.\nInstance-level calibration magnifies the impact of outliers. As demonstrated in Figure 5 (Right), instance-level calibration can inadvertently widen the range of reward discrepancy distribution. This broadened range suggests that instance-level calibration might be leading to excessively high or low \u03b2 values for the model. Such disparities in \u03b2 values consequently cause disproportionate update rates for certain samples, further intensifying the extremities in the distribution."}, {"title": "Conclusion and Future Work", "content": "This paper introduces \u03b2-DPO, a novel framework designed to optimize DPO by dynamically ad- justing the \u03b2 parameter in response to the variability in the informativeness of pairwise data. Our approach, which incorporates B-guided data filtering and batch-level dynamic \u03b2 calibration, has demonstrated significant improvements in DPO's performance across a range of models and datasets."}, {"title": "Experiment", "content": "B-DPO Implementation Details and Hyperparameters\nB-DPO is relatively straightfoward to implement; The full algorithm is summarized in Algorithm 1."}, {"title": "B-Direct Preference Optimization", "content": "Require: Preference dataset D, batch size b, constraint coefficient \u03b20, selection ratio \u03c1, scaling factor \u03b1, and learning rate \u03b7.\n1: Initialize model \u03c0\u03b80 with supervised finetuning on D.\n2: while not converged do\n3: Sample a batch B = {(x(i), yw(i), y\u03b9(i))}bi=1 from D.\n4: Compute the individual reward discrepancy Mi = r(yw(i); x(i)) \u2212 r(y\u03b9(i); x(i)).\n5: Update the threshold M0 and \u03c3 using Equations (7) and (9).\n6: Sample b \u00d7 \u03c1 samples without replacement based on p(Mi) in Equation (8).\n7: Compute the batch-level \u03b2 using Equation (5).\n8: Compute the loss using the Equation (4).\n9: Compute the gradient and update the model \u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b7\u2207\u03b8l(\u03b8t\u22121, B).\n10: end while\n11: return Final model \u03c0\u03b8."}, {"title": "GPT-4 prompts for computing summarization and dialogue win rates", "content": "A fundamental element of our experimental framework involves the assessment of win rates facilitated by GPT-4. In this segment, we delineate the prompts employed to ascertain win rates pertinent to our summarization and dialogue-oriented investigations. For the entirety of our experimental endeavors, we utilize GPT-4. It is important to note that the sequence in which summaries or responses are presented is randomized for each evaluation.\nSummarization GPT-4 win rate prompt.\nWhich of the following summaries does a better job of summarizing the most \\ important points in the given forum post?\nDialogue GPT-4 win rate prompt.\nFor the following query to a chatbot, which response is more helpful?"}, {"title": "Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}