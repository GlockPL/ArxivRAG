{"title": "B-DPO: Direct Preference Optimization\nwith Dynamic B", "authors": ["Junkang Wu", "Yuexiang Xie", "Zhengyi Yang", "Jiancan Wu", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Xiangnan He"], "abstract": "Direct Preference Optimization (DPO) has emerged as a compelling approach for\ntraining Large Language Models (LLMs) to adhere to human preferences. However,\nthe performance of DPO is sensitive to the fine-tuning of its trade-off parameter \u03b2,\nas well as to the quality of the preference data. We analyze the impact of \u1e9e and data\nquality on DPO, uncovering that optimal \u1e9e values vary with the informativeness\nof pairwise data. Addressing the limitations of static \u1e9e values, we introduce a\nnovel framework that dynamically calibrates \u1e9e at the batch level, informed by\ndata quality considerations. Additionally, our method incorporates B-guided data\nfiltering to safeguard against the influence of outliers. Through empirical evaluation,\nwe demonstrate that our dynamic \u1e9e adjustment technique significantly improves\nDPO's performance across a range of models and datasets, offering a more robust\nand adaptable training paradigm for aligning LLMs with human feedback.", "sections": [{"title": "Introduction", "content": "The alignment of Large Language Models (LLMs) with human feedback, as explored in works like\nGPT-4 and LLaMA-2 [22, 31, 8], has marked a significant advancement in generating responses that\nare more helpful, factual, and ethical [23]. Among the various alignment strategies, Reinforcement\nLearning from Human Feedback (RLHF) [23] is a notable method that refines LLMs using the\nProximal Policy Optimization (PPO) algorithm [29]. This approach employs a KL divergence penalty\nto ensure minimal deviation of the model from its original configuration, ensuring the retention of its\ninitial characteristics while improving alignment.\nDespite the effectiveness, RLHF's instability and computational requirements often limit its practical\napplications, prompting the exploration of alternatives like Direct Preference Optimization (DPO)\n[27]. DPO circumvents the reinforcement learning loop by exploiting the inherent connection between\nreward functions and optimal policies, thereby simplifying the policy model training. It encourages\nthe model to favor the response that aligns with human preferences (yw) over the dispreferred\n(yi), implying DPO's sensitivity to the quality of pairwise data. The balance between maintaining\nthe original reference model (\u03c0ref) and incorporating new preferences (\u03c0\u03b8) is controlled by a B\nhyperparameter, whose lower values advocate for aggressive updates and higher values support more"}, {"title": "Related Work", "content": "Reinforcement Learning from Human Feedback. Despite RLHF's effectiveness in aligning\nlanguage models (LMs) with human values [9, 2, 31, 23], its complexity and resource demands have\nspurred the exploration of alternatives. RAFT [10] selects optimal training samples via an existing\nreward model, whereas RRHF [37] employs a simpler ranking loss, retaining PPO's efficiency.\nDiverse from these, DPO [27] directly optimizes LMs using a preference-based loss function,\nshowcasing enhanced training stability in comparison to traditional RLHF. Innovatively, SLiC-HF\n[39] and KTO [11] devise loss functions rooted in human decision-making, focusing on preference\ncalibration and utility optimization, respectively. Further, RSO [19] and ORPO [15] introduce efficient\npreference modeling and optimization, with ORPO uniquely combining supervised fine-tuning and\npreference alignment. These advancements reflect the ongoing shift towards more efficient, nuanced\nRL methods.\nData Quality in LLM's Alignment. Recent studies have increasingly recognized the significance\nof data quality in the alignment of LLMs. For example, LIMA [40] leverages heuristics such as\npost scores, response lengths, formatting, and topics to manually craft 1000 high-quality datasets\nfrom StackExchange, wikiHow, and Reddit for superficial alignment. In a similar vein, Bai et al.\n[3] prioritize data points based on user engagement levels for dataset assembly. Rejection Sampling\n(RS) and Best-of-N (BoN) techniques, as evidenced in the works of Nakano et al. [21] and Gao et al.\n[12], involve selecting the optimal candidate from N generated possibilities through the application\nof a reward model. To enhance preference optimization, RSO [19] uses statistical weightings to\ndifferentiate outcomes from an optimal policy and a base SFT policy. Besides, fDPO [20] employs a\nReward Model to filter out low-quality data, effectively addressing dataset quality concerns."}, {"title": "Preliminaries", "content": "Given a text sequence (commonly referred to as a prompt) x, a sequence y = [Y1, Y2,... YN] is\ngenerated as a response to the prompt x. An autoregressive language model \u03c0, when provided with\nthe prompt x, can generate the response sequence y following the probability decomposition:\n\u03c0(y|x) = \u03a0\u03c0(Yix, y<t),\nwhere y<t denotes the preceding tokens in the response sequence. Now, given a preference dataset\nD = {(x(t), y),y())}M\u2081, wherein each triplet consists of a prompt x with two responses yw \u2208 \u03a3*\nand y\u03b9 \u2208 \u03a3*, with \u2211* representing the alphabet, a preference oracle either a human annotator or\na language model \u2014 provides preference feedback o(yw \u203a y\u0131|x) \u2208 {0, 1}, indicating whether yw\nis preferred over y\u0131. We denote P(yw > y\u0131|x) = E[0(yw > y\u0131|x)] the probability of yw \u201cwinning\nthe duel", "as": "nP(yw > y\u0131x) =\nexp(r(yw; x))\nexp(r(yw; x)) + exp(r(y\u0131; x))\n= \u03c3(r(yw; x) \u2013 r(y\u0131; x)),"}, {"title": "Method", "content": "In this section, we investigate the critical connection between the parameter \u1e9e and the quality of\npairwise data in optimizing DPO. We present empirical evidence demonstrating the effect of \u1e9e settings\non DPO performance across datasets of varying quality. Our proposed method, \u03b2-DPO, introduces\ndynamic calibration of \u1e9e and a data filtering mechanism tailored to improve DPO's effectiveness\nacross diverse data conditions.\n4.1 Motivation: The Impact of Pairwise Data Quality on \u1e9e Selection\nScrutinizing Equation (4), we argue that DPO's effectiveness critically hinges on two factors: the\nchoice of \u1e9e and the quality of pairwise data. Here, we conduct experiments to demonstrate the\ninfluence of variations in \u1e9e and data quality on DPO, pivotal for its effective real-world application.\nDatasets. We utilize the Anthropic HH dataset [2] for our experimental analysis, which contains\napproximately 170,000 dialogues between humans and an automated assistant. In this dataset, a\nhuman inquiry, denoted as x, is paired with two responses (yw, y\u0131), where yw represents the response\nfavored by the human annotator, while yi is the alternate response. Notably, the alternate response y\u0131\nretains informational value, making this dataset high-quality with minimal discrepancies between\nthe response pairs, which we classify as a low gap dataset. To further explore the impact of data\nquality on DPO, we construct a synthetic dataset, referred to as the high gap dataset. This dataset\ndiffers from the low gap dataset by introducing a greater disparity between responses. Specifically,\nthe alternative response y\u012b is generated by a Supervised FineTuned (SFT) Pythia-2.8B model, while\nthe preferred response yw remains consistent with the original dataset. We also combine the two\ndatasets in equal proportion to create a mixed gap dataset, with each contributing 50%, to incorporate\nthe characteristics of both the low gap and high gap datasets.\nModels and Metrics. Our study evaluates various model sizes, specifically Pythia-410M, Pythia-\n1.4B, and Pythia-2.8B [5], to ensure a comprehensive assessment. Following the established protocol\nin DPO [27], each model iteration undergoes a single epoch with a batch size of 64. This setup\nprovides a uniform basis for evaluation across different models. We adopt the evaluation strategy\nfrom DPO [27] to calculate the win rate, a metric that measures how often the GPT-4 model prefers a\nresponse generated by our models over the default chosen response on the subset of the test dataset.\nFindings: (1) The optimal value of \u1e9e varies with data quality, reflecting divergent perfor-\nmance patterns across datasets. In Figure 2, we present the win rate results across three levels\nof pairwise data gap, each evaluated under varying \u1e9e parameters. As can be observed, with low\ngap pairwise data, a smaller \u1e9e value is preferable for optimizing performance. This is likely"}, {"title": "Method: Dynamic \u03b2 Calibration in DPO", "content": "because the informative content of such data allows a lower \u1e9e to facilitate more substantial up-\ndates, thereby enhancing alignment accuracy. Conversely, for high gap pairwise data, maintain-\ning a low \u1e9e may lead to overfitting, which significantly undermines the alignment process. The\nmixed gap dataset a combination of both low gap and high gap datasets exhibits a more\nnuanced performance pattern, suggesting the necessity for a dynamic \u1e9e calibration strategy to\nadapt to varying data quality. Consequently, adhering to a fixed \u1e9e value, i.e., configuring \u1e9e at the\npopulation level, might be inadequate for the dynamic and varied nature of real-world datasets.\n(2) The dataset exhibits notable outliers. In Figure 3, utilizing\nthe Pythia-2.8B model, we evaluate the data quality by examining\nthe distribution of reward discrepancy for each triplet (which we\nwill define as \u201cindividual reward discrepancy\" later) within the HH\ndataset's training samples. The tails of the density plot extend beyond\nthe highlighted percentiles, suggesting the existence of data samples\nwith significantly higher or lower reward discrepancies. Notably,\ncases with significantly higher rewards for positive samples over\nnegative ones suggest low informational value, as these discrepancies\nlikely do not contribute meaningfully to the model's learning process.\nWhereas the opposite cases hint at potential labeling errors. Both\ncases deviate from an expected rational distribution range and are\nthus classified as outliers. For further details on outliers, kindly refer\nto Appendix A.2.\n4.2 Method: Dynamic \u1e9e Calibration in DPO\nThrough our empirical analysis, we highlight the sensitivity of DPO to \u1e9e selections and the frequent\noccurrence of outliers. Hence, determining the optimal \u1e9e value requires careful consideration of the\nquality of pairwise data while also addressing the influence of outliers. This prompts the question:\nwhat criteria define a superior choice of \u1e9e? In response, we propose the following guiding principles:\nPrinciple 1: The optimal \u1e9e value should be responsive to pairwise data's quality.\nPrinciple 2: The selection of \u1e9e value should minimize the influence of outliers.\n4.2.1 Dynamic \u1e9e Calibration at Batch-Level\nWe begin by introducing the concept termed \u2018individual reward discrepancy', which represents the\ndifference between the rewards of winning and losing for each triplet, serving as a measurement for\npairwise data quality. Formally, for a triplet (x(i),y),y()) (x(t), yw, y \u2208 D, the individual reward discrepancy\nis defined as Mi = r(y); x(i)) -r(y(i); x(i)). Motivated by our guiding principles, a straightforward\napproach is to assign a distinct \u1e9e to each triplet, allowing each \u1e9e to serve as a parameter tailored to\nits respective triplet. This instance-level dynamic \u1e9e adaption can be formulated as follows:\n\u03b2\u03af = \u03b2\u03bf + \u03b1(\u039c\u2081 \u2013 Mo)\u03b2\u03bf = [1 +\u03b1(\u039c\u03af \u2013 \u039c\u03bf)]\u03b2\u03bf,\nwhere Bo represents the benchmark hyperparameter intrinsic to DPO, typically set to 0.1. The term\nMo denotes a predetermined threshold, and the coefficient a is a scaling factor within the interval\""}, {"title": "Method: B-Guided Data Filtering", "content": "[0, 1] that adjusts the influence of Mi on \u1e9ei. Specifically, when a = 0, \u03b2\u2081 remains constant at Bo,\nthus maintaining the standard DPO framework without modification.\nEquation (5) illustrates that Bi increases monotonically with Mi, allowing the model to adjust\nthe \u1e9e value based on the running reward differential between paired samples. Nevertheless, such\ninstance-level adjustments may introduce instabilities during training. Prior studies have shown that\na minibatch approach can help avoid saddle points or local minima [13], as well as mitigate the\nimpact of noise [28, 6]. Drawing inspiration from these benefits, we propose a batch-level dynamic\nestimation methodology for \u03b2:\nBbatch = [1 + a(Ei~batch [Mi] - \u039c\u03bf)]\u03b2\u03bf.\nIn practical applications, the threshold Mo can be estimated by employing the global mean of Mi\nwith a moving average updating scheme [18]:\nMomMo + (1-m)Ei~batch [Mi],\nwhere m\u2208 [0,1) is a momentum coefficient. Such a batch-level calibration method introduces\nonly one new parameter, a, to control the scale of \u1e9e adjustment. The calculation of Ei\u223cbatch[Mi] is\nstraightforward within DPO procedures, thereby incurring no additional computational overhead.\n4.2.2 B-Guided Data Filtering\nTo mitigate the adverse impact of outliers on the \u1e9e selection process, we introduce a B-guided data\nfiltering mechanism. Informed by 30 confidence criterion [26], this strategy employs a probabilistic\nmodel to assess the significance of each triplet (x(t), y),y()) based on its individual reward\ndiscrepancy M\u2081, which is defined as:\np(Mi) =\n1\n\u221a2\u03c0\u03c3 exp\n(Mi - Mo)2\n202\nwhere Mo and o represent the mean and standard deviation of Mi across the training dataset,\nrespectively. Similar to the updating scheme of Mo in Equation (7), we dynamically estimate the\nvalue of o using the moving average method:\n\u03c3 \u2190 m\u03c3 + (1 \u2212 m)\u221aVi~batch[Mi].\nThis probabilistic weighting discerns the relative importance of each sample, guiding the selection of\n|batch \u00d7 p samples (without replacement) based on their calculated probabilities p(Mi). Here, p\ndenotes the selection ratio, defaulting to 0.8, a choice validated by preliminary experiments aimed at\noptimizing training efficiency and model accuracy.\nThis process is iterated for each training batch, ensuring that the training data is continuously updated\nto reflect the most informative samples. The introduction of the B-guided data filtering strategy is\ninstrumental in fortifying the model against outliers, thereby facilitating the accurate estimation of\nthe \u1e9e value.\nHighlights: We underline the following key features of our proposed \u1e9e-DPO framework:\n\u2022 Simplicity: \u03b2-DPO is extremely straightforward and quick to implement. It merely involves a\ndynamic \u1e9e adjustment at the batch level and a B-guided data filtering mechanism, both of which\nare predicated upon the reward discrepancy denoted by Mi.\n\u2022 Efficiency: Unlike other methodologies [19, 20, 25, 33] that necessitate an additional gold model\nfor data filtering, our method leverages the running reward discrepancy M\u2081 within the DPO\nframework. Moreover, our empirical studies indicate that \u1e9e-DPO exhibits insensitivity to the\nhyperparameters p. A default setting of p = 0.8 typically yields satisfactory performance.\n\u2022 Model-agnostic: As a variant of the traditional DPO, the proposed B-DPO can function as a\nplug-and-play module. It allows straightforward integration of future enhancements and extensions\nwithin the DPO framework. Our empirical investigations corroborate this assertion."}, {"title": "Discussion with Previous Studies", "content": "Relations to Data Selection. An increasing volume of works [19, 20, 40, 25, 36] have underscored\nthe impact of data quality on the performance of LLM's alignment. A common practice among these\nefforts involves employing a so-called \u201cgold model\u201d for data selection. This approach, however,\nintroduces significant computational demands and the choice of the gold model directly influences\nthe resultant system's performance. The focus of this work, it should be noted, is not to propose a\nsuperior strategy for data selection. Instead, we aim to enhance adaptability to the quality of data\nby dynamically adjusting the \u1e9e parameter. This adjustment facilitates improved \u1e9e estimation by\nselecting data based on the reward. Moreover, Section 5.2 illustrates the compatibility of dynamic B\nadjustment with other data selection methodologies.\nRelations to Recent Temperature Schemes. Dynamic temperature frameworks have been introduced\nin the realm of contrastive learning, motivated by various objectives, such as addressing out-of-\ndistribution tasks [38] or accommodating long-tail data distributions [17]. The work most closely\nrelated to ours, MACL [16], has indeed proposed an alignment-adaptive strategy; however, its primary\naim was to navigate the uniformity-tolerance dilemma. Hence, the integration of dynamic temperature\nmechanisms with LLM's alignment remains an underexplored area against this backdrop."}, {"title": "Experiments", "content": "In this section, we commence by conducting an empirical evaluation of \u03b2-DPO on two specific tasks:\ndialogue generation and summarization. Subsequently, we analyze the various adaptations of the\nproposed method \u1e9e-DPO. Concluding this section, we underscore the imperative need for batch-level\ndynamic \u1e9e calibration, highlighting its significance in the context of our study.\n5.1 Empirical Evaluation of \u03b2-DPO on Dialogue Generation and Summarization\nDatasets and Setup. Our experiments are conducted on the Anthropic HH dataset [2] and Reddit\nTL;DR summarization dataset [32]. The training configuration follows from Rafailov et al. [27]. The\ngoals of these experiments are to study: 1) How \u1e9e-DPO performs on single-turn dialogue generation\nand summarization tasks; 2) How the sampling temperature affects the performance of B-DPO; 3)\nHow B-DPO works with different model sizes. For detailed experimental settings, please refer to\nAppendix A.1.\nBaselines. In our comparison, we examine the performance of \u03b2-DPO relative to its counterparts:\nthe standard DPO, DPO implemented with a dynamic \u1e9e yet devoid of B-guided data filtering, and\nDPO complemented by data filtering with \u1e9e fixed at 0.1."}, {"title": "Adaptations of B-DPO", "content": "Table 1. We observe that \u1e9e-DPO consistently outperforms DPO, DPO with dynamic \u1e9e, and DPO\nwith data filtering across all model sizes. We observe that in a smaller model, the improvement of\ndata filtering is more significant, while in a larger model, the improvement of dynamic \u1e9e is more\nsignificant. We attribute this to the fact that the larger model has more capacity to learn the optimal\npolicy, while the smaller model needs more help from the data filtering.\n5.2 Adaptations of B-DPO\nIn this section, our inquiry is twofold: first, we aim to understand the performance of B-DPO when\napplied across various filtering strategies; second, we examine its efficacy across different adaptations\nof the DPO framework. In terms of filtering strategies, prevailing studies [25, 36] in the domain\nlargely employ a gradient-based approach. We propose to extend this methodology into three distinct\nscenarios. This involves arranging the gradients of pairwise data within a batch and consequently:\n(1) Excluding the top 20% of samples, hereby referred to as Filter Head, (2) Excluding the bottom\n20% of samples, hereby referred to as Filter Tail, (3) Excluding both the top and bottom 10% of\nsamples, a method we denote as Filter Tail & Head. For a fair comparison, we maintain the amount\nof data excluded at 20% for the above strategies. Second, we integrate three variants of DPO into our\nanalysis: the IPO [1], a novel approach that facilitates learning directly from preferences without the\nneed for the Bradley-Terry (BT) model. Additionally, we consider the KTO [11], which focuses on\ndiscerning whether a preference is desirable or undesirable and SPPO [35], which approximates the\nNash equilibrium. For detailed settings, we refer the reader to the supplementary material.\nSelective filtering of the top 20% of samples markedly enhances model performance. This\napproach, detailed in Figure 5 (Left), not only surpasses other filtering strategies but also suggests\nthat these samples, which exhibit the smallest discrepancies between positive and negative pairs, are\nparticularly prone to flipped noise. By excluding them, the model's learning efficacy is appreciably\nimproved.\nDynamic \u1e9e adapts to and improves upon existing filtering strategies. Figure 5 (Left) corroborates\nour stance that a static \u1e9e proves insufficient within the DPO framework. We contend that the applica-\ntion of our dynamic \u1e9e-DPO could markedly reshape the DPO field by fostering the development of\nadvanced filtering techniques.\nDynamic \u1e9e Enhancement across DPO Variants. We introduce dynamic \u1e9e-DPO, a novel strategy\nenhancing DPO and its variants: IPO, KTO, and SPPO in Figure 5 (Middle). Our results on the\nAnthropic HH dataset demonstrate that while IPO initially leads in performance, the integration of"}, {"title": "Necessity of Batch-Level Dynamic \u03b2 Calibration", "content": "Mixture Ratio Table 2. This\nunderscores dynamic \u1e9e-DPO's capability to significantly enhance model training through adaptable\nimprovements, solidifying its value in advancing language models via human feedback.\n5.3 Necessity of Batch-Level Dynamic \u1e9e Calibration\nIn this section, we aim to underscore the pivotal role of batch-level tuning in calibrating the parameter\n\u03b2. To this end, we compare the performance of our \u1e9e-DPO algorithm under two distinct regimes:\none employing batch-level dynamic \u1e9e calibration, and the other utilizing instance-level dynamics.\nTo emulate the diverse data disparity scenarios encountered in practical applications, we adopt the\nmethodology outlined in Section 4.1, meticulously blending datasets characterized by both low gap\nand high gap attributes at varying ratios.\nBatch-level calibration surpasses both instance-level and population-level approaches. The\nresults presented in Table 2 illustrate that batch-level dynamic \u1e9e calibration yields superior perfor-\nmance compared to instance-level dynamics and the baseline population-level approach (referred to\nas vanilla DPO) across a range of mixture ratios. This improvement can be credited to the batch-level\ncalibration's ability to adjust to the varying data quality present within a batch, thus refining the\nmodel's learning process. Conversely, instance-level dynamics can provoke excessively vigorous\nmodel updates, precipitating a decline in performance particularly noticeable at a mixture ratio of\n40%, a scenario in which outliers exert a significant negative impact.\nInstance-level calibration magnifies the impact of outliers. As demonstrated in Figure 5 (Right),\ninstance-level calibration can inadvertently widen the range of reward discrepancy distribution. This\nbroadened range suggests that instance-level calibration might be leading to excessively high or low\n\u1e9e values for the model. Such disparities in \u1e9e values consequently cause disproportionate update rates\nfor certain samples, further intensifying the extremities in the distribution."}, {"title": "Conclusion and Future Work", "content": "This paper introduces \u1e9e-DPO, a novel framework designed to optimize DPO by dynamically ad-\njusting the \u1e9e parameter in response to the variability in the informativeness of pairwise data. Our\napproach, which incorporates B-guided data filtering and batch-level dynamic \u1e9e calibration, has\ndemonstrated significant improvements in DPO's performance across a range of models and datasets."}, {"title": "Experiment", "content": "A.1 B-DPO Implementation Details and Hyperparameters\nB-DPO is relatively straightfoward to implement; The full algorithm is summarized in Algorithm 1.\nAlgorithm 1 B-Direct Preference Optimization\nRequire: Preference dataset D, batch size b, constraint coefficient Bo, selection ratio p, scaling factor\na, and learning rate \u03b7.\n1: Initialize model \u03c0\u03b8\u03bf with supervised finetuning on D.\n2: while not converged do\n3: Sample a batch B = {(x(t), y),y())}=1 from D.\n4: Compute the individual reward discrepancy Mi = r(y); x(i)) \u2013 r(y(); x(i)).\n5: Update the threshold Mo and \u03c3 using Equations (7) and (9).\n6: Sample b \u00d7 p samples without replacement based on p(Mi) in Equation (8).\n7: Compute the batch-level \u1e9e using Equation (5).\n8: Compute the loss using the Equation (4).\n9: Compute the gradient and update the model 0+ \u2190 Ot\u22121 \u2013 n\u2207ol(0t\u22121, B).\n10: end while\n11: return Final model \u03c0\u03b8.\nUnless noted otherwise, we use a \u1e9e = 0.1, batch size of 64, m = 0.9 to ensure the stability of the\nglobal Mi estimation, p = 0.8 to filter 20% uninformative samples, and the Adam optimizer with\na learning rate 5e 7 by default. We carried out all computational tasks on a suite of four 80GB\nA100 GPUs. For the Pythia-410M model, we use a batch size of 128, while the rest of the parameters\nremain the same.\nIn examining the arena of single-turn dialogue, our experimental framework leverages Pythia-\n2.8, Pythia-1.4b, and Pythia-410M for empirical analysis using the Anthropic-HH dataset. Given\nthe absence of a pre-existing Supervised Fine-Tuning (SFT) model tailored for this dataset, we\nfine-tune an accessible language model exclusively with preferred completions to sculpt our SFT\nmodel. Turning our focus to the domain of summarization, we employed the Reddit TL;DR\nsummarization dataset, enriching our research with human preferences as documented by the study\n[30]. Our methodology here incorporates an SFT model meticulously fine-tuned on expert-composed\nsummaries of forum posts 3, operating within the TRLX framework for Reinforcement Learning\nfrom Human Feedback (RLHF), as introduced by Havrilla et al. [14]. The human preference dataset\nwas gathered by Stiennon et al. [30] on samples from a different, but similarly-trained, SFT model."}, {"title": "Mixture of low gap and high gap", "content": "A.2 Mixture of low gap and high gap\nIn our previous discussion, we identified a low gap dataset, constituted by pairs of responses generated\nfrom the HH dataset. Given that these responses originate from the same model, we can infer that they\nrepresent semantically similar answers, hence the designation low gap. Concurrently, we maintain the\npositive samples constant while selecting negative samples generated by a Pythia 2.8B model. The\nsignificant performance disparity between the models results in a considerable variation in the quality\nof these negative samples, which we refer to as the high gap. We mix these two types of data in\nvarying proportions to mimic the distribution of data in real-world scenarios. We label this approach\nthe \"mixture experiment.\" To facilitate a better comparison of the distributions across different ratios,\nwe proceed to illustrate our findings with the following graph:\nFigure 6 clearly illustrates that as the mixture ratio increases that is, the proportion of high gap data\nrises the dispersion of the overall dataset broadens. Conversely, a decrease in the mixture ratio,\ncorresponding to an elevated presence of low gap data, results in a more concentrated distribution of\nreward discrepancy."}, {"title": "Hyperparameter Sensitivity", "content": "Figure 7, where varying model sizes exhibit distinct\nsensitivities to the parameter p. Within the context of the Pythia-2.8B model, a p value of 0.3\nyields optimal performance, whereas for the Pythia-410M model, a p value of 0.5 is superior. This\ncan be posited to suggest that smaller models may require more stringent data filtering to perform\noptimally, whereas larger models possess the increased capacity necessary for learning the most"}, {"title": "GPT-4 prompts for computing summarization and dialogue win rates", "content": "effective strategies. This insight provides a significant directive for future research: the value of p\nshould be fine-tuned according to the specific circumstances of the application.\nA.4 The ablation study w.r.t. Mo\nIn this work, we employed a moving average updating scheme [18] for the updating of Mo. In order\nto investigate the superiority of this configuration, we also conducted a comparative experiment\ninvolving hyperparameter settings. Specifically, Mo was treated as a constant in the training process,\nwhile all other settings remained unchanged. The experimental results are as follows:\nA fundamental element of our experimental framework involves the assessment of win rates facilitated\nby GPT-4. In this segment, we delineate the prompts employed to ascertain win rates pertinent to our\nsummarization and dialogue-oriented investigations. For the entirety of our experimental endeavors,\nwe utilize GPT-4. It is important to note that the sequence in which summaries or responses are\npresented is randomized for each evaluation.\nSummarization GPT-4 win rate prompt.\nWhich of the following summaries does a better job of summarizing the most \\\nimportant points in the given forum post?\nPost:\n<post>\nSummary A:\n<Summary A>\nSummary B:\n<Summary B>\nFIRST provide a one-sentence comparison of the two summaries, explaining which \\\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\nchoice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: <\"A\" or \"B\">\nDialogue GPT-4 win rate prompt.\nFor the following query to a chatbot, which response is more helpful?\nQuery: <the user query>\nResponse A:\n<either the test method or baseline>\nResponse B:"}, {"title": "Broader Impacts", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none of which we feel must be specifically highlighted\nhere."}]}