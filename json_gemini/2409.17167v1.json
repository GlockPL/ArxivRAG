{"title": "StressPrompt: Does Stress Impact Large Language Models\nand Human Performance Similarly?", "authors": ["Guobin Shen", "Dongcheng Zhao", "Aorigele Bao", "Xiang He", "Yiting Dong", "Yi Zeng"], "abstract": "Human beings often experience stress, which can significantly influence their performance. This study explores whether Large Language Models (LLMs) exhibit stress responses similar to those of humans and whether their performance fluctuates under different stress-inducing prompts. To investigate this, we developed a novel set of prompts, termed StressPrompt, designed to induce varying levels of stress. These prompts were derived from established psychological frameworks and carefully calibrated based on ratings from human participants. We then applied these prompts to several LLMs to assess their responses across a range of tasks, including instruction-following, complex reasoning, and emotional intelligence. The findings suggest that LLMs, like humans, perform optimally under moderate stress, consistent with the Yerkes-Dodson law. Notably, their performance declines under both low and high-stress conditions. Our analysis further revealed that these StressPrompts significantly alter the internal states of LLMs, leading to changes in their neural representations that mirror human responses to stress. This research provides critical insights into the operational robustness and flexibility of LLMs, demonstrating the importance of designing AI systems capable of maintaining high performance in real-world scenarios where stress is prevalent, such as in customer service, healthcare, and emergency response contexts. Moreover, this study contributes to the broader A\u0399 research community by offering a new perspective on how LLMs handle different scenarios and their similarities to human cognition.", "sections": [{"title": "Introduction", "content": "The advent of Large Language Models (LLMs) has\nmarkedly transformed the field of artificial intelligence,\nushering in unprecedented advancements in natural lan-\nguage processing, decision-making, and cognitive simula-\ntion. These Transformer-based architectures (Vaswani et al.\n2017) have consistently demonstrated capabilities that not\nonly rival but often surpass human performance in a variety\nof cognitive tasks (Radford et al. 2019; Kojima et al. 2022).\nResearch has highlighted the exceptional ability of LLMs to\nengage in deep reasoning, tackle complex problem-solving,\nand generate sophisticated text, achieving outstanding re-\nsults across numerous benchmarks (Hendrycks et al. 2021a;"}, {"title": "Related Works", "content": "In recent years, the exploration of how Large Language\nModels (LLMs) think and behave has garnered significant\nattention (Hutson 2024). LLMs have achieved remarkable\nadvancements across various domains, including natural\nlanguage understanding (Hendrycks et al. 2021a), mathe-\nmatical proficiency (Hendrycks et al. 2021b), coding capa-\nbilities (Chen et al. 2021), and medical knowledge (Sing-\nhal et al. 2023), often surpassing traditional artificial intel-\nligence models. Benchmark studies, such as Paech (2023)\nwith the EQ-Bench, have evaluated the emotional intelli-\ngence of these models, revealing that LLMs can compre-\nhend and even be enhanced by emotional stimuli (Wang\net al. 2023). Furthermore, Strachan et al. (2024) have com-\npared LLMs and humans in higher-order theory of mind\ntasks, demonstrating LLMs' capacity to understand and pre-\ndict mental states. Despite these advances, existing studies\noften lack a quantitative analysis of LLMs' internal state\nchanges across different scenarios. Our research addresses\nthis gap by focusing on stress\u2014a prevalent psychological\nphenomenon-to investigate the performance of LLMs un-\nder stress conditions. We analyze their internal states to ex-\nplore the similarities and differences between LLMs and\nhuman behavior, contributing to a deeper understanding of"}, {"title": "StressPrompt Construction", "content": "To systematically investigate the impact of stress on LLM\nperformance, we developed a dataset named StressPrompt,\ngrounded in established psychological theories. The objec-\ntive was to design prompts that elicit varying levels of stress,\nthereby enabling the evaluation of LLMs under different\nstress conditions.\nAs illustrated in Figure 3, the prompts were developed\nbased on four key psychological frameworks, each offering\na distinct perspective on stress and cognitive performance:\nStress and Coping Theory: This theory focuses on how\nindividuals appraise and cope with stressors. We developed\nprompts to simulate varying levels of perceived threat and\nchallenge, as well as the coping strategies employed, to pro-\nvide insight into the dynamic interaction between stress ap-\npraisal and cognitive functioning.\nJob Demand-Control Model: This model suggests that\njob stress is influenced by the balance between job demands\nand the control or autonomy an individual has over their\nwork tasks. We designed prompts to simulate scenarios with\nvarying job demands and levels of control, allowing us to\nstudy their effects on stress and cognitive performance.\nConservation of Resources Theory: This theory posits\nthat stress occurs when there is a threat to, loss of, or insuf-\nficient gain of resources necessary to achieve one's goals.\nUsing this framework, we created prompts that explore the\ndynamics of resource gain, loss, and protection in the con-\ntext of stress, highlighting how these factors influence cog-\nnitive performance.\nEffort-Reward Imbalance Model: According to this\nmodel, stress arises from an imbalance between the efforts\nan individual puts into their work and the rewards they re-\nceive. We crafted prompts to examine scenarios where this\nbalance is either maintained or disrupted, assessing its im-\npact on stress levels and task performance.\nWe constructed a total of 100 prompts for this study, col-\nlectively referred to as StressPrompt. After finalizing the\nprompts, we conducted an annotation process with 20 offline\nparticipants. Each participant rated the stress induced by all\n100 prompts on a scale from 1 to 10, where 1 represented"}, {"title": "StressPrompt Evaluation", "content": "To systematically assess the performance of LLMs under\nvarying stress conditions, we designed a comprehensive ex-\nperimental framework utilizing the StressPrompt dataset.\nThis framework introduces different levels of stress via sys-\ntem prompts, specifically targeting instruction-tuned LLMs,\nwith the aim of simulating a range of stress conditions and\nevaluating their impact on LLM performance, as illustrated\nin Figure 2.\nWe constructed ten distinct sets of prompts, each cor-\nresponding to a specific stress level $S_i$ where $i \\in\n\\{1,2,..., 10\\}$. Each set $S_i = \\{s\\}_{j=1}^{N_i}$ contains prompts $s$\nthat induce a specific stress level $i$.\nFor each task $T$, consisting of multiple question-answer\npairs {q, a}, and each stress level set $S_i$, we evaluated the\nperformance of the LLM f by conditioning the model on the\nprompts in $S_i$. Let $\\hat{a}, h = f(q | s)$ represent the LLM's out-\nput $\\hat{a}$ and hidden states h given a question q and a prompt s.\nWe systematically varied s to cover all stress levels i across\nall tasks T. The performance for each task T under each\nstress level i was quantified using task-specific evaluation\nmetrics.\nThe performance of the model f on task T under stress\nlevel i is given by:\n$P(f,T, S_i) = \\frac{1}{N_i} \\sum_{s \\in S_i} \\sum_{(q_k,a_k) \\in T} Metric(a_k, \\hat{a_k})$  (1)\nIn Eq. 1, the Metric represents the evaluation metric spe-\ncific to the task T, ak is the ground truth answer, $\\hat{a_k}$ is the\npredicted answer, and $N_i$ is the number of prompts in $S_i$.\nThis evaluation framework allows for a systematic anal-\nysis of the impact of varying stress levels on LLM perfor-\nmance across diverse tasks. By examining performance vari-\nations under different stress conditions, we can gain valu-\nable insights into the effects of stress on LLMs. These find-\nings not only deepen our understanding of LLM behavior\nbut also enable us to draw meaningful parallels with human\nstress responses."}, {"title": "StressPrompt Analysis", "content": "To further investigate how stress impacts the internal states\nof LLMs, we developed a Stress Scanner using techniques\ninspired by Representation Engineering (RepE) (Zou et al.\n2023). The Stress Scanner examines how different stress\nprompts from the StressPrompt dataset affect the hidden\nstates of LLMs across various layers and token positions.\nWe collected hidden states h from the LLMs when\nexposed to the full range of stress prompts $S =\n\\{S_1, S_2,..., S_{10}\\}$. By analyzing these hidden states, we\naimed to identify significant changes in neural processing\npatterns induced by varying stress levels.\nFor each stress prompt $s \\in S$, we collected the hidden\nstates h from the LLM at various layers and token positions.\nFormally, let H(Si) represent the set of hidden states col-\nlected for stress level $S_i$:\n$H(S_i) = \\{h = f(s) | s \\in S_i\\}$   (2)\nTo quantify the impact of stress on the hidden states, we\napplied Principal Component Analysis (PCA) to the col-\nlected hidden states. We defined the stress vector v as the\nfirst principal component that captures the maximum vari-\nance between the low-stress and high-stress conditions:\n$v_i = PCA (H(S_i) | i \\in \\{1, . . ., 10\\})_1$ (3)\nUsing the stress vector v, we projected the hidden states\nonto v to obtain a stress score for each hidden state, reflect-\ning the degree of stress induced by the prompt. For a given\nhidden state h, the stress score \u03c3 was computed as:\n$\\sigma = \\hat{h} \\cdot \\upsilon $ (4)"}, {"title": "Experiments", "content": "We evaluated the performance of several instruction-\ntuned LLMs under varying stress conditions using the\nStressPrompts dataset. The models tested included Llama-3-\n8B-Instruct, Llama-3.1-8B-Instruct, Llama-3-70B-Instruct\n(AI@Meta 2024), Phi-3-mini-4k-Instruct (Abdin et al.\n2024), Qwen2-72B-Instruct, Qwen2-7B-Instruct (Yang et al.\n2024), and Mistral-7B-Instruct-v0.3 (Jiang et al. 2023). The\ngeneration temperature was set to 0, and specific dialogue\ntokens were used to ensure consistency.\nWe utilized a range of benchmarks that assessed emo-\ntional intelligence, bias detection, instruction following, rea-\nsoning, and mathematical problem-solving. The datasets\nemployed in these evaluations included IFEval (Zhou et al.\n2023), BBH (Suzgun et al. 2022), MATH (Hendrycks\net al. 2021b), GPQA (Rein et al. 2023), MuSR (Sprague\net al. 2023), MMLU-P (Wang et al. 2024b), EQ-\nBench (Paech 2023), MMLU (Hendrycks et al. 2021a),\nTruthfulQA (Lin, Hilton, and Evans 2021), and Toxi-\nGen (Hartvigsen et al. 2022). The evaluations were con-\nducted using the lm_eval (Gao et al. 2023) framework with\ndefault settings. Baseline prompts used for comparison were\nyou are a helpful assistant and let's think step by step.\nAll evaluations were performed on NVIDIA A100 GPUs."}, {"title": "Analysis Under Varying Stress Levels", "content": "The experimental results summarized in Table 1 illustrate\nthe effects of varying stress levels induced by StressPrompts\non the performance of different language models across mul-\ntiple tasks. Our analysis focuses on the impact of stress on\nseveral dimensions, including task performance, model sen-\nsitivity, and general trends observed.\nIn most tasks, moderate stress levels enhance perfor-\nmance, while high stress levels lead to declines, consistent\nwith the Yerkes-Dodson law. This suggests that moderate\nstress stimulates cognitive engagement, whereas excessive\nstress overwhelms the system and impairs function.\nComplex reasoning and problem-solving tasks, such as\nMuSR and MATH, exhibit significant performance varia-\ntions under different stress levels. These tasks benefit from\nmoderate stress but experience marked declines under high\nstress. For example, Llama-3-8B-Instruct's performance on\nMATH improves from 0.04 at stress level 1 to 2.93 at\nstress level 6, demonstrating the positive impact of moder-\nate stress on problem-solving abilities. Similarly, multitask\nunderstanding tasks follow this trend, with moderate stress\nlevels enhancing performance. The impact of stress is partic-\nlarly pronounced in professional-level tasks like MMLU-\nPRO, where tasks with higher cognitive loads show greater\nbenefits from moderate stress. These findings underscore the\nunique advantage of StressPrompt in addressing complex\nreasoning and problem-solving challenges. By fine-tuning\nstress levels, StressPrompt can effectively enhance LLMs\u2019\nperformance in tasks requiring high cognitive load, aligning\nLLM performance with human-like responses under stress.\nDifferent large models exhibit varying sensitivity to\nstress, with a similar trend observed across multiple mod-\nels. For instance, Llama-3-8B-Instruct shows substantial im-\nprovement in several tasks under moderate stress, while\nmodels like Mistral-7B-Instruct-v0.3 display more grad-\nual performance changes. This indicates that model archi-\ntecture and training specifics play a crucial role in how\nstress affects performance. While some models, such as\nQwen2-7B-Instruct and Phi-3-mini-4k-Instruct, exhibit rel-\native smaller fluctuations in performance under different\nstress levels, they are still influenced by stress. These dif-\nferences may be attributed to varying strategies and prefer-\nences during fine-tuning. Overall, while the impact of stress\non model performance is evident, the extent and nature of\nthese changes vary depending on the model's training ap-\nproach."}, {"title": "Impact of Stress on Emotional Intelligence, Bias,\nand Hallucination", "content": "As depicted in Figure 8, the effects of varying stress lev-\nels on LLM performance across three datasets-EQ-Bench\nfor emotional intelligence, ToxiGen for bias detection, and\nTruthfulQA for susceptibility to hallucination-reveal nu-\nanced patterns. For emotional intelligence, models exhibit\nimproved performance under moderate stress, with declines\nat both low and high stress extremes. This suggests that a\nbalanced level of arousal enhances cognitive engagement\nwithout overwhelming the model.\nIn contrast, increased stress levels correlate with declining\nperformance in bias detection, indicating that higher stress\nexacerbates biases. This finding is critical for applications\nrequiring unbiased decision-making, such as content mod-\neration. Regarding hallucination susceptibility, stress has\nminimal impact, with performance remaining stable across\nstress levels. This suggests that hallucinations are driven\nmore by intrinsic model factors rather than by stress-induced\narousal.\nThese findings underscore the importance of tailoring\nstress levels to optimize LLM performance, particularly in\ntasks demanding high emotional intelligence and fairness.\nBy understanding how stress affects different cognitive and\nsocial competencies, we can better align LLMs with human-\nlike responses, enhancing their utility in diverse applica-\ntions."}, {"title": "Visualization of the Effect of Stress on Neural\nActivity", "content": "To gain insights into how LLMs respond to different stress\nlevels, we visualized their neural activity. As shown in Fig-\nure 5, the neural activity of the last token when inputting\nStressPrompt effectively reflects the induced stress. We con-\nducted an experiment using T-SNE to visualize the neural\nactivities of LLMs across various layers, as depicted in Fig-\nure 7. The results indicate that initial layers are unable to\ndistinguish between stress levels, whereas deeper layers can\nclassify prompts into low-stress and high-stress categories,\nindicating a higher sensitivity to stress in these layers.\nFurthermore, we performed a stress scan on the last token\nof all prompts, illustrated in the heatmap in Figure 9. This\nvisualization captures neural activity across all layers for\nvarious stress levels, revealing significant changes in deeper"}, {"title": "Conclusion", "content": "In this study, we constructed a dataset named StressPrompt\nto induce varying levels of stress in LLMs. Our analysis\nshows that stress significantly affects the internal states of\nLLMs, with deeper layers exhibiting higher sensitivity to\nstress levels. Moderate stress can enhance performance in\ntasks such as instruction following, reasoning, and emo-"}, {"title": "Appendix", "content": "We evaluated the performance of several instruction-tuned\nlarge language models (LLMs) under varying stress condi-\ntions. The models evaluated include Llama-3-8B-Instruct,\nLlama-3.1-8B-Instruct, Llama-3-70B-Instruct (AI@Meta\n2024), Phi-3-mini-4k-Instruct (Abdin et al. 2024), Qwen2-\n72B-Instruct, Qwen2-7B-Instruct (Yang et al. 2024), and\nMistral-7B-Instruct-v0.3 (Jiang et al. 2023). To ensure re-\nproducibility, the generation temperature was set to 0 dur-\ning evaluations. Each model was configured with its specific\ndialogue tokens to clearly define conversational roles and\ninstructions. StressPrompts were introduced as system in-\nstructions, with all other settings kept consistent to ensure\nfair comparisons.\nWe considered a diverse set of benchmarks to evaluate dif-\nferent dimensions of LLM capabilities, including emotional\nintelligence, bias detection, instruction following, reason-\ning ability, and mathematical problem-solving. The datasets\nused are: IFEval (Zhou et al. 2023), which evaluates a\nmodel's ability to follow explicit instructions; BBH (Big\nBench Hard) (Suzgun et al. 2022), a set of 23 challeng-\ning tasks from the BigBench dataset; MATH (Hendrycks\net al. 2021b), comprising high-school level competition\nproblems; GPQA (Graduate-Level Google-Proof Q&A\nBenchmark) (Rein et al. 2023), a dataset with challeng-\ning questions crafted by PhD-level experts; MuSR (Mul-\ntistep Soft Reasoning) (Sprague et al. 2023), featuring\ncomplex algorithmically generated problems; MMLU-P\n(Massive Multitask Language Understanding - Profes-\nsional) (Wang et al. 2024b), a refined version of the MMLU\ndataset; EQ-Bench (Paech 2023), designed to evaluate emo-\ntional intelligence in LLMs; MMLU (Hendrycks et al.\n2021a), a test covering 57 diverse tasks; TruthfulQA (Lin,\nHilton, and Evans 2021), a benchmark for measuring the\ntruthfulness of generated answers; and ToxiGen (Hartvigsen\net al. 2022), a large-scale dataset for adversarial and implicit\nhate speech detection.\nThe first five datasets constitute the new generation of the\nOpen LLM Leaderboard\u00b9, while the latter datasets provide\nadditional perspectives on emotional intelligence, bias de-\ntection, hallucinations, and other critical aspects of model\nperformance.\nWe utilized the lm_eval (Gao et al. 2023) framework\nfor all evaluations, maintaining default settings to ensure re-\nproducibility and broad applicability. This framework facili-\ntates consistent and standardized evaluations across different\nmodels and datasets. To intuitively compare the impact of\ndifferent prompts on the performance of LLMs, we set two\nbaselines: the default prompt 'you are a helpful assistant'\nand the chain-of-thought (CoT) prompting 'let's think step\nby step'. Each of the StressPrompts was applied individu-\nally, and the LLMs' responses were recorded and evaluated\nbased on the specified metrics.\nAll evaluations were performed on NVIDIA A100 GPUs,\nwith a maximum batch size of 16 and adaptive execution to\noptimize performance. This comprehensive setup allowed us\nto systematically assess the impact of stress levels on LLM\nperformance and draw meaningful insights into their robust-\nness and resilience under different conditions.\nTo classify the stress levels of the prompts in our\nStressPrompt dataset, we recruited human participants of-\nfline. A total of 20 participants were recruited and compen-\nsated at a rate that meets or exceeds the minimum hourly\nwage in our region. All participants provided informed con-\nsent prior to participating in the study, being informed about\nthe purpose of the study, the procedures involved, and their\nrights as participants, including the right to withdraw at any\ntime without any penalty.\nParticipants were provided with a set of 100 prompts, each\ndesigned to reflect different levels of stress based on estab-\nlished psychological frameworks. All participants rated the\nstress induced by each of the 100 prompts on a scale from\n1 (minimal stress) to 10 (maximum stress). To ensure con-\nsistency and reliability in the ratings, the final stress level\nfor each prompt was determined by averaging the ratings\nacross all participants. All data collected from participants\nwas anonymized to protect their privacy, with no personally\nidentifiable information stored or shared. The dataset will be\nprovided as an appendix for transparency and reproducibil-\nity. The distribution of stress levels across the prompts is\nshown in Figure 4."}]}