{"title": "Deepfake Detection of Singing Voices With Whisper Encodings", "authors": ["Falguni Sharma", "Priyanka Gupta"], "abstract": "The deepfake generation of singing vocals is a concerning issue for artists in the music industry. In this work, we propose a singing voice deepfake detection (SVDD) system, which uses noise-variant encodings of open-AI's Whisper model. As counter-intuitive as it may sound, even though the Whisper model is known to be noise-robust, the encodings are rich in non-speech information, and are noise-variant. This leads us to evaluate Whisper encodings as feature representations for the SVDD task. Therefore, in this work, the SVDD task is performed on vocals and mixtures, and the performance is evaluated in %EER over varying Whisper model sizes and two classifiers-CNN and ResNet34, under different testing conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "The growing improvements in generative models have considerably enhanced automatic human voice generation, with nearly natural-parity performances. However, such an artificial audio data generation has paved the way for forgeries, popularly known as deepfakes [1], [2]. Particularly in the music industry, unauthorized deepfake copies that closely resemble well-known vocalists are a growing source of concern for musicians. These reproductions pose a direct threat to the commercial worth and intellectual property rights of the original artists. Some of the singing voice synthesis models such as VISinger [3], DiffSinger [4], MidiVoices [5], and SinTechSVS [6] have the capability to synthesize convincing levels of natural sounding vocals, thereby mimicking vocalists well. Therefore, the effective detection of deepfake-generated music calls for attention.\nAs opposed to speech spoof detection, singing voice deep-fake detection (SVDD), presents a unique set of issues that are not observed in speech. For example, the pitch and duration of phonemes in a singing voice are significantly affected by the melody. Moreover, the wider range of timbre and voicing by artists in a musical context is not observed in speech. Additionally, extensive editing in singing voices is done to mix with musical instrumental accompaniments. These contrasts with speech pose the question of whether the countermeasures designed for speech can be directly applied to SVDD. In this context, research in Singing Voice Deepfake Detection (SVDD) has come up recently, with the proposition of the Singfake dataset [7]. This was followed by the SVDD 2024 challenge, having CtrSVDD and WildSVDD tracks, with its evaluation labels yet to be released. Recently, SingGraph has been proposed as a state-of-the-art model for singing voice deepfake detection, integrating music and linguistic analysis with domain-specific augmentation techniques. Our work proposes a different approach, focusing on non-invariant features without using data augmentation techniques, to address the challenges in this domain.\nWe propose an end-to-end model for SVDD which utilizes the encoder representations from the pre-trained Web-scale Supervised Pretraining for Speech Recognition (WSPR) model also referred to as Whisper [8]. Whisper is incredibly resilient to real-world background sounds, like music. However counter-intuitively, unlike the other Automatic Speech Recognition (ASR) systems, its audio representation is not noise-invariant [9]. Therefore, we use these noise-variant encodings from the encoder output of the Whisper model, for the SVDD task. In particular, this paper has the following contributions:\n\u2022 End-to-end Whisper encoding-based SVDD system is proposed.\n\u2022 Noise-variance in encodings is used as a discriminative cue to classify the bonafide and deepfake voices, in pure vocals, as well as in the case of mixtures.\n\u2022 Performance evaluation is done on four variants (W(Tiny), W(Small), W(Base), and W(Medium)) of Whisper.\n\u2022 Experiments on vocals and mixtures are also done to investigate the effect of instrumental accompaniments in the songs.\n\u2022 Experimental analysis w.r.t. various testing conditions is done to observe the effect of seen/unseen singers, different communication codecs, languages, and musical contexts."}, {"title": "II. PROPOSED WORK", "content": "Whisper is an extensively trained ASR model, with training done on 680K hours of data collected from the Internet with diverse environments and recording setups. It is based on encoder-decoder Transformer architecture, primarily given by [10] and it is trained on a substantial and varied supervised dataset and focuses on zero-shot transfer. In this work, we"}, {"title": "A. Whisper Encoder", "content": "propose a SVDD system, which uses Whisper encodings along discriminative feature/cue to distinguish between bonafide and\nwith ResNet34 in the pipeline, as shown in Figure 1. deepfake singing voices. In this work, the Singfake dataset\nThe encoder of the Whisper model from which the encod- has been used which is divided into two scenarios - vocals\nings are extracted takes input audio, which is pre-processed (containing only the singer's voice), and mixtures (the singer's\nto mel-spectrogram. The corresponding mel-spectrogram is voice is accompanied by background music). In context with\nprocessed through two 1-D convolutional layers to extract this, Figure 2 shows the spectrographic comparison of bonafide\nfeatures. Additionally, sinusoidal positional embeddings are vs. deepfake singing voices for both vocals and mixtures. In\nthen added for temporal context. The variant of the Whisper\nmodels are with respect to the size of its encoder. In particular,\ntiny model incorporates 4 blocks, the base model uses 6, the\nsmall model employs 12, and the medium model utilizes 24\nblocks and according to that it gives a vector output of fixed\ndimensions in its last hidden state. Each block consists of a\nmulti-head self-attention mechanism,\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V$                                                                                                                                                                                                                                                              (1)\nwhere $Q \\in R^{T \\times d_k}$, keys $K \\in R^{T \\times d_k}$, and values $V \\in R^{T \\times d_v}$,\nwhere T is the sequence length, and $d_k$ and $d_v$ are the\nhidden dimensionality for queries/keys and values respectively.\nFollowing this is a position-wise fully connected feed-forward\nnetwork. The output of each sub-layer is LayerNorm(x +\nSublayer(x)), where Sublayer(x) is the function implemented\nby the sub-layer itself."}, {"title": "B. Noise-Variant Encodings from Noise Robust Whisper", "content": "There have been various studies that focus on noise-invariant representations of ASR [11]\u2013[13]. Researchers often specify noise invariance as an explicit inductive bias for robust ASR since it is widely accepted that a robust ASR model's representation should be noise-invariant [11]\u2013[14]. For Whisper however, a counter-intuitive finding is that Whisper's audio representation is not noise-invariant; rather, it encodes rich information about non-speech background sounds. This indicates that the Whisper model encodes the type of noise and subsequently detects speech conditioned on it, instead of learning a noise-invariant representation [9]. Since the training data of Whisper is very large (680K hours), it consists of diverse settings of environments, languages, and speakers, and has noisy labels. This differentiates the Whisper model from the other ASR models. As opposed to learning a representation that is independent of noise, it encodes the background sound first and then transcribes text based on the type of noise [9].\nWe leverage Whisper's noise-conditioned encodings to capture the noise introduced in deepfakes and use it as the"}, {"title": "III. EXPERIMENTAL SETUP", "content": "For this study, the Singfake dataset [7] is used. It offers 28.93 hours of bonafide (real songs) and 29.40 hours of deepfake generated songs, with partitioning details as shown in Table I. Furthermore, the Singfake dataset has two subsets-vocals, and mixtures. The vocals subset consists of separated singing vocals from the background instrument accompaniments. For vocal separation, Demucs [15] was used, and each song was separated into 6 8 clips of an average of"}, {"title": "A. Dataset", "content": "particular, as indicated by the regions marked in green circles\nin Figure 2, in the case of vocals, the end of a non-vocal region\n(silent region), has a gradual fading in bonafide utterance\n(Figure 2 (i)). However, in the deepfaked singing voice, a\nvery distinct and sharp transition from the silence region to the\nsinging region can be observed (Figure 2 (ii)). Furthermore, in\nthe case of mixtures, the formants of the singer's voice dissolve\ninto the background instrument frequencies, as observed by\nthe region marked in green in Figure 2 (iii). However, in the\ncase of deepfaked mixture, the formants appear very distinct\nfrom the background instrumentals, as observed by the region\nmarked in Figure 2 (iv)."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This subsection shows the experimental results of the\nproposed Whisper-encodings-based-SVDD system, of varying\nsizes- tiny, base, small, and medium, denoted as W(Tiny),\nW(Base), W(Small) and W(Med.), respectively. To observe the\neffectiveness of the proposed method, additional experiments\nw.r.t. non-Whisper standard feature sets MFCC, and CQCC\nwere performed. The experimental results corresponding to\nLFCC are taken from [7].\nFigure 3 and Figure 4 show the training, testing\n(average of the performances of all the 4 test cases), and\nvalidation performances on classifiers ResNet34 and CNN,\nrespectively, for the case when only vocals from the dataset\nare considered. It can be observed that for both the classifiers,\nthe proposed approach of extracting Whisper encodings signif-\nicantly outperforms all the standard feature sets. In particular,\nwith ResNet34 (as shown in Figure 3), encodings from the\nmedium-sized Whisper model (denoted as W (Med.)) achieve\nan average EER of 4.86%."}, {"title": "A. Whisper Encodings vs. Standard Representations", "content": "1) Vocals:"}, {"title": "2) Mixtures:", "content": "For the case of mixtures (i.e., singing voices\nthat have background instrumental accompaniments), Figure\n5 and Figure 6, show the comparative performances on the\ntraining, average testing, and validation sets, using classifiers\nResNet34 and CNN, respectively. It can be observed that for\nmixtures as well, for both the classifiers, the encoding from\nthe Whisper-medium model significantly outperforms all the"}, {"title": "B. Analysis on Testing Conditions", "content": "We now analyze our findings for different test cases (T01\nto T04). Given that in the previous subsection IV-A, the\nResNet34 classifier showed the best performance on both vo-\ncals and mixtures, we now consider ResNet34 at the classifier\nend for the rest of the experiments in this paper."}, {"title": "1) Vocals:", "content": "Figure 7 shows the experimental results obtained\non various testing conditions (T01 to To4) such as seen/unseen\nsingers & languages, communication codecs, and musical con-\ntexts. It is observed that for all the systems, T01 is the easiest,\nand T04 is the hardest to detect. In particular, on the T01\ncase (seen singers, but unseen songs), the best performance\nof 1.09% EER is achieved by the proposed system based on\nW(Med.) with ResNet34 as the classifier."}, {"title": "2) Mixtures:", "content": "It can be observed from Figure 8, that the\nperformances on mixtures follow the same trend as for the\ncase of vocals. Additionally, for all the values of the EERs\nfor all the representations, the vocals are found to be easier\nto detect than deepfakes in the mixture. Overall, the proposed\nWhisper encodings-based system is observed to outperform all\nthe remaining systems, with remarkable differences in EER\nvalues, for both vocals and mixtures. However, with unseen language and musical style, T04 is the most difficult to detect,\nindicating the limitation of the models."}, {"title": "C. Comparison with Existing Systems", "content": "We compare the performance of our proposed model-\nWhisper(Med.) with ResNet34, against the best-performing\nmodel (Wav2vec2 + AASIST) on Singfake [7]. It is to be\nmentioned that the Singfake paper did not provide results for\nthe validation dataset. The comparison of both the vocals and\nthe mixture highlights that the Whisper(medium) based model\nconsistently performed better across all training and testing\nconditions, especially in T04, with an absolute difference of\n28.94% and 24.52, in vocals and mixtures, respectively."}, {"title": "V. CONCLUSIONS AND FUTURE WORKS", "content": "This work proposed noise-variant encodings from the Whis-\nper model as representations to be used for SVDD. It was\nobserved that the encodings from the Whisper (medium)\nmodel with ResNet34 classifier achieved the best performances\nthroughout all the testing scenarios of the Singfake dataset.\nOur findings showed that amongst the non-whisper represen-\ntations, LFCC showed the best performance.\nHowever, our system's robustness under T04 conditions,\nwhich involve unseen languages, remains a limitation. Al-\nthough our results surpass those of the baseline paper, there\nis room for improvement in handling such diverse linguistic\nconditions. In the future, the effectiveness of the proposed\nSVDD system can also be improved by incorporating data\naugmentation."}]}