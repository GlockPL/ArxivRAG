{"title": "Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints", "authors": ["Xiaoshan Lin", "Roberto Tron"], "abstract": "This work addresses the problem of multi-robot coordination under unknown robot transition models, ensuring that tasks specified by Time Window Temporal Logic are satisfied with user-defined probability thresholds. We present a bi-level framework that integrates (i) high-level task allocation, where tasks are assigned based on the robots' estimated task completion probabilities and expected rewards, and (ii) low-level distributed policy learning and execution, where robots independently optimize auxiliary rewards while fulfilling their assigned tasks. To handle uncertainty in robot dynamics, our approach leverages real-time task execution data to iteratively refine expected task completion probabilities and rewards, enabling adaptive task allocation without explicit robot transition models. We theoretically validate the proposed algorithm, demonstrating that the task assignments meet the desired probability thresholds with high confidence. Finally, we demonstrate the effectiveness of our framework through comprehensive simulations.", "sections": [{"title": "1 INTRODUCTION", "content": "Efficient and high-quality task allocation plays a critical role in cooperative multi-robot applications, such as on-demand ridesharing and delivery [11], assembly lines [13], and warehouse logistics [42]. Recently, multi-robot task allocation with temporal constraints has attracted growing attention due to its significance in time-sensitive applications. For instance, [32] formulates a distributed constraint optimization problem to allocate tasks with time windows and ordering requirements to heterogeneous robots. [5] considers uncertainty in completing time-windowed tasks and combines high-level task allocation with low-level scheduling to minimize task incompletion. [26] addresses tasks with precedence constraints, proposing an algorithm that ensures the probability of task failure remains below a user-specified threshold under uncertainties in robot traits. Research in this area typically considers tasks that can be executed independently.\nMulti-robot systems often involve more complicated scenarios where tasks are logically and temporally correlated. For instance, in warehouse logistics, robots need to coordinate to retrieve items from multiple storage locations based on orders, inventory, and replacement options, then assemble and deliver them within a set time frame, with each step depending on the successful completion of the previous one. Temporal logic (TL) provides a rigorous framework for specifying such temporal order and dependencies between tasks.\nDifferent methods have been proposed to coordinate robots to satisfy a global temporal logic formula, including graph search on product automata [28], sampling-based methods [14, 21], and optimization-based approaches [2, 15, 41]. Other works assign local temporal logic tasks to individual robots, employing methods such as receding horizon planning [34], path-finding on product graph [8], and integer programming-based planning [31]. Nevertheless, these studies do not address uncertainty, which is crucial for time-sensitive tasks. For instance, environmental factors like traffic jams can disrupt task execution within the required time window.\nSome prior studies [1, 3, 10, 12] employ reinforcement learning (RL) to maximize the probability of satisfying temporal logic constraints under unknown transition dynamics, primarily focusing on single-robot systems. Multi-agent reinforcement learning (MARL) has been explored for joint policy learning with temporal logic objectives [9, 38, 43]. Additionally, semi-decentralized methods [30], deep RL [23], and model-based RL [20] have been explored to improve learning efficiency. However, these MARL approaches either do not explicitly address system model uncertainty or assume full system knowledge, limiting their applicability in uncertain environments.\nOverall, there are research gaps in existing multi-robot task allocation, planning, and learning approaches, as they either assume the tasks are independent, neglect the uncertainty in system models, or unrealistically presume full knowledge of uncertainties. Additionally, prior studies often aim to maximize robustness degree [23, 38], minimize incomplete tasks [5], or optimize some cost functions [14, 21]. While these approaches are effective, they often lack quantifiable guarantees on task satisfaction.\nMotivated by these gaps, this paper addresses multi-robot task allocation to ensure a specified probability of satisfying Time Window Temporal Logic tasks under unknown transition dynamics. Additionally, a secondary objective is to maximize auxiliary reward functions for each robot based on user preferences. Fig. 1 illustrates an example where robots must transport materials within specific time windows with high probability while also performing auxiliary tasks such as identifying and monitoring traffic-prone intersections or returning to stations for future assignments.\nOur proposed formulation, which incorporates primary tasks and auxiliary objectives, has broader applicability in various scenarios, such as: (i) Search-and-rescue: Robots deliver supplies within time-critical windows while minimizing energy use or maximizing coverage to identify other areas needing help. (ii) Environmental monitoring: Drones monitor areas during specific intervals, such as wildlife peaks, while collecting additional data or conserving power for longer missions.\nIn summary, this work makes the following contributions:"}, {"title": "2 PRELIMINARIES", "content": "Let \\( \\Sigma \\) be a finite set. We denote the power set of \\( \\Sigma \\) by \\( 2^{\\Sigma} \\). A finite or infinite sequence of elements from \\( \\Sigma \\) is called a word over \\( \\Sigma \\). In this context, \\( \\Sigma \\) is also called an alphabet. Let \\( k, i, j \\in \\mathbb{Z}_{\\geq 0} \\) with \\( i \\leq j \\). The \\( k \\)-th element of a word \\( \\sigma \\) is denoted by \\( \\sigma_{k} \\), and the subword \\( \\sigma_{i, ..., j} \\) is denoted by \\( \\sigma_{i,j} \\). We denote the set of atomic propositions by AP.\n2.1 Time Window Temporal Logic\nTime Window Temporal Logic (TWTL) [37] is a language for expressing time-bounded specifications. A TWTL formula is defined over a set of atomic propositions AP as follows:\n\\( \\phi ::= H_{d}s \\mid H_{d}\\neg s \\mid \\neg \\phi_{1} \\mid \\phi_{1} \\wedge \\phi_{2} \\mid \\phi_{1} \\vee \\phi_{2} \\mid \\phi_{1}\\cdot\\phi_{2} \\mid [\\phi_{1}]_{[a,b]} \\).\nHere, \\( s \\) represents either the constant \"true\" or an atomic proposition in AP; \\( \\phi_{1} \\) and \\( \\phi_{2} \\) are TWTL formulas. The hold operator \\( H_{d}s \\), with \\( d \\in \\mathbb{Z}_{\\geq 0} \\), specifies that \\( s \\in \\text{AP} \\) should hold for \\( d \\) time units.\nThe negation operator \\( \\neg \\phi_{1} \\) specifies \"do not satisfy the formula\u201d. The conjunction operator \\( \\phi_{1} \\wedge \\phi_{2} \\) and disjunction operator \\( \\phi_{1} \\vee \\phi_{2} \\) specify \"satisfy both formulas\", and \"satisfy at least one formula\", respectively. The concatenation operator \\( \\phi_{1}\\cdot\\phi_{2} \\) specifies that \\( \\phi_{1} \\) must be satisfied first, and \\( \\phi_{2} \\) must be satisfied immediately after. The within operator \\( [\\phi]_{[a,b]} \\), where \\( a, b \\in \\mathbb{Z}_{\\geq 0} \\) and \\( a \\leq b \\), restricts the satisfaction of \\( \\phi \\) to the time window \\( [a, b] \\). The time bound of a TWTL formula \\( \\phi \\), denoted as \\( ||(\\phi)|| \\), represents the maximum time allowed to satisfy \\( \\phi \\). For a formal definition of the TWTL semantics and the time bound, we refer readers to [37].\nDefinition 2.1. (Deterministic Finite-State Automaton) A deterministic finite-state automaton (DFA) is a tuple \\( A = (Q, q_{0}, \\Sigma, \\delta, F) \\), where \\( Q \\) is a finite set of states, \\( q_{0} \\) is the initial state, \\( \\Sigma = 2^{\\text{AP}} \\) is the input alphabet, \\( \\delta : Q \\times \\Sigma \\rightarrow Q \\) is the transition function, and \\( F \\) is the set of accepting states.\nA finite input word \\( \\sigma = \\sigma_{0}, \\sigma_{1}, ..., \\sigma_{T} \\) over the alphabet \\( 2^{\\text{AP}} \\) generates a trajectory \\( q = q_{0}, q_{1},..., q_{T+1} \\) on the DFA, where \\( q_{0} \\) is the initial state of the DFA and \\( q_{k+1} = \\delta(q_{k}, \\sigma_{k}) \\) for all \\( k \\geq 0 \\). A finite input word \\( \\sigma \\) over \\( \\Sigma \\) is accepted by a DFA if the corresponding trajectory \\( q \\) ends in an accepting state of the DFA.\nA TWTL formula \\( \\phi \\) can be translated into a DFA that either accepts or rejects an input word [37]. An input word \\( \\sigma \\) is said to satisfy the corresponding TWTL formula \\( \\phi \\) if it is accepted by the DFA, denoted as \\( \\sigma \\models \\phi \\). For example, the TWTL formula \\( \\phi = [H_{1}P]_{(1,2)} \\cdot [H_{0}D]_{(0,2)} \\), specifying a pickup (P) and hold for one time step within time window [1, 2] followed by a delivery (D) within two time steps, is translated into a DFA shown in Fig. 2a. An input word \\( \\sigma = \\{P\\}, \\{P\\}, \\{P\\}, \\{D\\} \\) satisfies the TWTL formula, as it generates a trajectory \\( q = q_{0}, q_{1}, q_{2}, q_{3}, q_{6} \\) that ends in the accepting state \\( q_{6} \\). This TWTL formula can also be translated into a temporally relaxed DFA (see Fig. 2b), which is more compact and facilitates more efficient computation and construction.\n2.2 Markov Decision Process\nDefinition 2.2. (Labeled MDP) A labeled MDP is a tuple \\( M = (S, A, \\Delta, R, l) \\), where \\( S \\) represents the state space, and \\( A \\) denotes the set of actions. The probabilistic transition function is given by \\( \\Delta : S \\times A \\times S \\rightarrow [0, 1] \\), and the reward function is defined as"}, {"title": "3 PROBLEM FORMULATION", "content": "Given a multi-robot system \\( \\{N\\} \\) and a set of tasks \\( \\{K + 1\\} \\), the objective is to find an optimal policy for each MDP in \\( \\{M\\} \\) to ensure that the team output word \\( \\{\\sigma_{jT, jT+T}\\} \\) satisfies each TWTL formula \\( \\phi_{k} \\) with at least probability \\( P_{k} \\) while maximizing the sum of rewards. With unknown MDP transitions, this can be formulated as a constrained RL problem, with the probabilistic satisfaction of TWTL tasks encoded as constraints, and reward maximization as the objective. This problem presents multiple challenges, including coordinating robots under uncertain dynamics, ensuring probabilistic satisfaction of TWTL tasks, and maximizing individual rewards.\nWe propose a bi-level solution to address these challenges hierarchically. At the high level, we solve for multi-robot task allocation to ensure desired probabilistic satisfaction of the TWTL tasks. Specifically, we consider task allocation \\( \\Pi: \\{N\\} \\times \\{K+1\\} \\rightarrow [0, 1] \\), where \\( \\Pi(i, k) \\) represents the probability of robot \\( r_{i} \\) being assigned to task \\( t_{k} \\). At the low level, each robot independently finds its own policy \\( \\pi^{i} \\), to satisfy its assigned task while simultaneously maximizing its individual reward. We formally define the problem as below.\nProblem 1. Given a set of robots \\( \\{N\\} \\), a set of TWTL tasks \\( \\{K+1\\} \\), a set of MDP \\( \\{M\\} \\) subject to Assumption 1, and a discount factor \\( \\gamma \\),"}, {"title": "4 PROPOSED SOLUTION", "content": "Our approach consists of a high-level coordinator for task allocation and a low-level task execution mechanism. The high-level coordinator considers each robot's probability of satisfying TWTL tasks from its current state and the expected rewards associated with specific tasks. It computes task assignments that maximize the total expected reward while ensuring that each task is completed with the desired probability threshold. At the low level, each robot maintains K + 1 policies: K stationary policies, one for each TWTL task, and an additional policy learned over time to maximize individual rewards when not assigned to a TWTL task. Upon receiving an assignment from the high-level coordinator, each robot follows the corresponding policy for its designated task.\n4.1 High-level Multi-robot Task Assignment\nBefore each episode, each robot provides the coordinator with its \\( P_{e}^{i,k}(\\pi^{i}) \\), the expected probability of satisfying TWTL formula \\( \\phi_{k} \\), and \\( V_{i,k}(\\pi^{i}) \\), the expected reward under its current policy. Here, \\( p^{i} \\) denotes the current state of the robot. For simplicity, we refer to these quantities as \\( P_{e}^{i,k} \\) and \\( V_{i,k} \\) in the following sections.\nFor now, we assume that \\( P_{e}^{i,k} \\) and \\( V_{i,k} \\) are known to the robots. The process of obtaining these values will be discussed in later sections. Given a set of tasks \\( \\{K + 1\\} \\), the high-level task assignment for robot \\( r_{i} \\in \\{N\\} \\) is represented by the probability distribution \\( \\{P_{i,k\\}}_{k=1,...,K+1} \\), where \\( \\sum_{k=1}^{K+1} P_{i,k} = 1 \\). For \\( k = 1, 2, ..., K \\), \\( P_{i,k} \\) denotes the probability of robot \\( r_{i} \\) being assigned to TWTL task \\( t_{k} \\), while \\( P_{i,K+1} \\) denotes the probability that robot \\( r_{i} \\) prioritizes maximizing its reward function instead of executing a TWTL task. Following the multi-robot task allocation taxonomy proposed by [7], our task allocation problem falls under ST-SR-IA, i.e. single-task robots (ST) execute single-robot tasks (SR), with instantaneous assignments (IA). Note that although the stochastic task allocation allows multiple robots to be assigned to the same task, we assume they execute tasks in parallel rather than collaboratively. Therefore, tasks are classified as single-robot rather than multi-robot. The\nhigh-level task allocation is determined by solving the following optimization problem.\n\\(\n\\begin{aligned}\n\\mathop{\\arg \\max}\\limits_{\\Pi} & \\sum_{i=1}^{N} \\sum_{k=1}^{K+1} P_{i,k} V_{i,k}  \\\\\n\\text{subject to} & 1 - \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot P_{e}^{i,k}) \\geq P_{k} \\quad \\forall k = 1, 2, ..., K \\\\\n& \\sum_{k=1}^{K+1} P_{i,k}= 1 \\quad \\forall i = 1, 2, ..., N \\\\\n& P_{i,k} \\geq 0 \\quad \\forall i = 1, 2, ..., N, k = 1, 2, ..., K +1\n\\end{aligned}\n\\)\nIn problem (3), the objective function (3a) aims to maximize the sum of the expected rewards across all agents. Constraint (3b) ensures that the probability of at least one robot satisfying task \\( t_{k} \\) is not less than its desired probability \\( P_{k} \\). Constraints (3c) and (3d) ensure that the task assignment \\( \\{P_{i,k\\}} \\) define a valid probability distribution over tasks for each robot. Although the assignments are stochastic, each robot selects and executes only one task at a time, sampled from the task assignment. Unlike deterministic solutions commonly used in multi-agent task allocation, our approach allows robots to optimize auxiliary rewards in addition to satisfying TWTL tasks. Notably, deterministic task allocation is a special case of our framework, occurring when \\( P_{i,k} \\) are either 1 or 0.\nThe probability \\( P_{k} \\) in (3b) is unknown due to the lack of knowledge about the transition probabilities in the MDPs. One solution is to substitute \\( P_{k} \\) with its lower bound. The key question is whether solving problem (3) using lower bounds of \\( P_{k} \\) still ensures satisfaction of the original constraints. To answer this, we present the following proposition.\nPROPOSITION 4.1. Let \\( [P_{e}^{i,k}] \\) be an arbitrary lower bound for \\( P_{e}^{i,k} \\), that is, \\( 0 \\leq [P_{e}^{i,k}] \\leq P_{e}^{i,k} \\leq 1 \\). If a set of probabilities \\( \\{P_{i,k\\}} \\) satisfies\n\\( 1 - \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot [P_{e}^{i,k}]) \\geq P_{k} \\quad \\forall k = 1, 2, ..., K, \\)\nthen \\( \\{P_{i,k\\}} \\) also satisfies constraint (3b).\nPROOF. Since \\( P_{e}^{i,k} \\geq [P_{e}^{i,k}] \\quad \\forall i, k \\), it follows that:\n\\( \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot P_{e}^{i,k}) \\leq \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot [P_{e}^{i,k}]). \\)\n\\( 1 - \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot P_{e}^{i,k}) \\geq 1 - \\prod_{i=1}^{N} (1 - P_{i,k} \\cdot [P_{e}^{i,k}]). \\)\nThis proposition guarantees that replacing \\( P_{k} \\) with its lower bound in (3) preserves the original task satisfaction constraints (3b).\n4.2 Low-level Single-Agent Task Execution\nUpon receiving its task assignment \\( \\{P_{i,k\\}}_{k=1,2,...,K+1} \\) from the high-level coordinator, robot \\( r_{i} \\) samples from this probability distribution to determine which task it will execute in the upcoming episode. The robot's policy during the episode depends on whether the selected task \\( t_{k} \\) is a TWTL task (\\( 1 \\leq k \\leq K \\)), or the auxiliary task focused on maximizing the robot's individual reward (\\( k = K + 1 \\))."}, {"title": "5 SIMULATION", "content": "To validate the proposed algorithm, we conduct simulations based on the pickup and delivery example illustrated in Fig. 1, incorporating time window requirements. The environment depicted in Fig. 3b is used across all experiments. In Fig. 3b, the colored cells represent stations (S1, S2) where robots can idle, warehouses (W1, W2) for storing resources, resource processing units (P1, P2), and an operation site (O) where processed resources are delivered. The black zones indicate restricted areas that robots must avoid, while the blue zone represents water, which ground robots are prohibited from entering. The gray zone represents an area of interest requiring aerial monitoring, which is unknown to the robots. The arrows show two one-way bridges over the water."}]}