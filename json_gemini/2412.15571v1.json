{"title": "Continual Learning Using a Kernel-Based Method Over Foundation Models", "authors": ["Saleh Momeni", "Sahisnu Mazumder", "Bing Liu"], "abstract": "Continual learning (CL) learns a sequence of tasks incrementally. This paper studies the challenging CL setting of class-incremental learning (CIL). CIL has two key challenges: catastrophic forgetting (CF) and inter-task class separation (ICS). Despite numerous proposed methods, these issues remain persistent obstacles. This paper proposes a novel CIL method, called Kernel Linear Discriminant Analysis (KLDA), that can effectively avoid CF and ICS problems. It leverages only the powerful features learned in a foundation model (FM). However, directly using these features proves suboptimal. To address this, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random Fourier Features (RFF) to enhance the feature representations from the FM, leading to improved performance. When a new task arrives, KLDA computes only the mean for each class in the task and updates a shared covariance matrix for all learned classes based on the kernelized features. Classification is performed using Linear Discriminant Analysis. Our empirical evaluation using text and image classification datasets demonstrates that KLDA significantly outperforms baselines. Remarkably, without relying on replay data, KLDA achieves accuracy comparable to joint training of all classes, which is considered the upper bound for CIL performance. The KLDA code is available at https://github.com/salehmomeni/klda.", "sections": [{"title": "Introduction", "content": "Continual learning (CL) is a machine learning paradigm that incrementally learns a sequence of tasks, enabling models to adapt to new data while retaining knowledge from previously learned tasks (Chen and Liu 2018; De Lange et al. 2021). This paper specifically addresses class-incremental learning (CIL), a setting where each task introduces a distinct set of classes, and the objective is to train a single unified model capable of recognizing all classes encountered across tasks. A key feature of CIL is that no task identification information is provided during testing, meaning the model must distinguish between classes from different tasks without being informed which task a test sample belongs to.\nThere are also two other main CL settings: task- incremental learning (TIL), which has the task information provided at test time, and domain-incremental learning (DIL), which learns tasks from different domains with the"}, {"title": "Background", "content": "same set of classes. For more details about TIL and DIL, please refer to (Van de Ven and Tolias 2019).\nCIL has two major challenges: (1) catastrophic forgetting (CF), which occurs when learning new tasks causes the model's performance on earlier tasks to deteriorate due to parameter updates (McCloskey and Cohen 1989); (2) inter- task class separation (ICS) (Kim et al. 2022), which arises because, without access to previous task data while learning a new task, it is difficult to learn decision boundaries between the new and old classes. Many existing CIL methods have been proposed (Ke and Liu 2022; Wang et al. 2024). However, due to the two challenges, they still have significant performance deterioration as more tasks are learned.\nThis paper proposes a novel technique using foundation models (FMs) that addresses both challenges and can achieve an accuracy level comparable to the upper bound of CIL - the accuracy obtained by joint training on all classes/- tasks together \u2013 without using any replay data.\nThe proposed method is called Kernel Linear Discriminant Analysis (KLDA). In KLDA, the FM is frozen, meaning no updates are made to its parameters to avoid CF. KLDA does not use trainable adapters (Houlsby et al. 2019) or prompts (Wang et al. 2022b) either, as they are also susceptible to CF or ICS. It uses only the latent features of the input samples extracted from the FM. However, directly using these features is suboptimal, as the classes in the original feature space may not be easily separable. To address this, we employ kernel functions, which enhance feature representation by implicitly mapping the data to a higher- dimensional space where the features become more linearly separable. As computing the traditional kernel matrix is impractical for CIL due to its huge size and incremental learning nature, we approximate it using Random Fourier Features (RFF) (Rahimi and Recht 2007), making it feasible for CIL. When a new task arrives, KLDA uses the kernal-ized features to compute the class mean of each class and updates a shared covariance matrix for all classes learned thus far. The class mean and the shared covariance matrix define a Gaussian distribution for each class, which Linear Discriminant Analysis (LDA) (Izenman 2013) then employs to optimize class separability.\nIn summary, this paper makes three key contributions:"}, {"title": "Class-incremental Learning", "content": "In CIL, a model is trained on a sequence of tasks {$T_1, T_2, ..., T_t$}, where each task $T_t$ introduces a disjoint set of classes with its associated training data $D_t = {(x_i, y_i)}_{i=1}^{N_t}$. The goal is to develop a unified model F: X \u2192 Y capable of classifying instances from any of the classes encountered across the T tasks.\nBecause in learning a new task $T_t$, the data from previous tasks $T_1,..., T_{t-1}$ is not accessible, CIL faces two key challenges CF and ICS as mentioned earlier. During inference, the task identity is unknown, and the model must predict the correct class label from all the classes encountered so far."}, {"title": "Class-prototypes for Continual Learning", "content": "Fine-tuning a foundation model for CIL often leads to catastrophic forgetting. Instead, leveraging the latent features to incrementally accumulate class-prototypes (CPs) while keeping the FM frozen can result in more accurate classification, as we will demonstrate in the experiments section.\nA simple yet effective method is the Nearest Class Mean (NCM) classifier, where the prototype for a class is the mean of the feature vectors extracted from the FM for all training samples of the class. For simplicity, from this point on, we will use x to denote the feature vector extracted from the FM for an input. The class prototype $\u03bc_m$ for a class m is then computed as:\n$\u03bc_m = \\frac{1}{N_m} \\sum_{i=1}^{n_m} x_i$\nwhere $n_m$ is the number of samples for class m. This mean vector can be computed incrementally for each class and does not cause CF as it doesn't involve training.\nDuring inference, a test sample is classified by finding the class mean with the highest cosine similarity:\n$\u0177 = arg max_m \\frac{x_{test}^T \u03bc_m}{||x_{test}|| || \u03bc_m||}$"}, {"title": "Proposed Method: KLDA", "content": "This straightforward method surprisingly outperforms several more complex prompt-based or fine-tuning-based CIL overcome this limitation is to use kernel functions, which implicitly map the input data (features from the FM in our case) into a higher-dimensional space where the data becomes more linearly separable. In this high-dimensional space, the model can learn a linear decision boundary that corresponds to a non-linear boundary in the original space.\nMathematically, if we have an input space X and a mapping \u03c6 : X \u2192 V, where V is a potentially infinite- dimensional feature space, a kernel function K (xi, xj) computes the inner product in this space without explicitly performing the transformation:\n$K(x_i, x_j) = (\u03c6(x_i), \u03c6(x_j))_V$\nOne of the most commonly used kernels is the Radial Basis Function (RBF), which is defined as:\n$K(x_i, x_j) = exp(-\\frac{||x_i - x_j||^2}{2\u03c3^2})$\nThe RBF kernel corresponds to an inner product in an infinite-dimensional space, making it highly effective for capturing complex patterns in data.\u00b9 Using this kernel trick involves computing the kernel matrix K, where Kij = K(xi, xj). This results in an N \u00d7 N matrix, which is computationally prohibitive and impractical in CL. Instead, we approximate the mapping \u03c6(x) using Random Fourier Features and work directly with these finite-dimensional features to avoid computing or storing K."}, {"title": "Kernel Functions and Non-linear Transformations", "content": "baselines (Zhou et al. 2024), which are susceptible to CF. This suggests that FMs provide robust, generalizable repre- sentations suitable for downstream tasks.\nTo enhance NCM, higher-order statistics can be incorporated. A well-known approach is Quadratic Discriminant Analysis (QDA) (Hastie 2009), which assumes that the features of class m follow a multivariate Gaussian distribution $N(\u03bc_m, \u03a3_m)$. The likelihood P(x | y = m) can be computed from the Gaussian distribution, and the predicted class is determined by Bayes' rule as the one that maximizes the posterior probability P(y | x). However, this method is un- suitable for continual learning because it requires storing the covariance matrix Em for every class, which becomes pro- hibitively large as more classes are introduced in CIL.\nLinear Discriminant Analysis (LDA) simplifies this by as- suming that all classes share the same covariance matrix \u03a3. LDA is well-suited for CIL, as it requires storing only the mean vector for each class and a single shared covariance matrix, ensuring that the number of parameters does not grow significantly as the number of classes increases. The shared covariance matrix is computed as follows:\n$\u03a3 = \\frac{1}{M} \\sum_{m=1}^{M} \\frac{1}{N_m} \\sum_{i=1}^{n_m} (x_{m,i} \u2013 \u03bc_m)(x_{m,i} \u2013 \u03bc_m)^T$\nwhere M is the number of classes seen so far, and N is the total number of samples, i.e., N = \u2211M=1 nm. This shared covariance matrix can be updated incrementally in the CIL process when a new task arrives, by first computing the mean for each class before updating \u03a3.\nUnder the shared covariance assumption, the log- posterior for LDA can be written as:\n$log P(y = m | x) = x^T\u03a3^{-1}\u03bc_m - \\frac{1}{2}\u03bc_m^T\u03a3^{-1}\u03bc_m + constant$\nHere, the \"constant\" term refers to P(x) in Bayes' rule, which is the same for all classes. This approach improves class separation by accounting for the distribution of the data in a class, addressing some limitations of the NCM method.\nFrom this equation, we can define the weight vector wm and bias term bm for each class m as follows:\n$W_m = \u03a3^{-1}\u03bc_m$\n$b_m = -\\frac{1}{2}\u03bc_m^T\u03a3^{-1}\u03bc_m$\nThis shows that LDA assumes a linear decision boundary, which may not be optimal for features from the FM."}, {"title": "Approximating the Kernel with Random Fourier Features", "content": "Linear models often struggle when data is not linearly separable in its original feature space. A powerful approach to Random Fourier Features (Rahimi and Recht 2007) provides an efficient way to approximate the kernel function by leveraging Bochner's theorem (Rudin 2017), which states that any continuous, shift-invariant kernel can be represented as the Fourier transform of a non-negative measure:\n$K(x_i, x_j) = \\int p(\u03c9)e^{i\u03c9^T (x_i\u2212x_j)} dw = E_\u03c9 [e^{i\u03c9^T (x_i-x_j)}]$\nHere, \u03c9 is the frequency in the Fourier domain, and p(\u03c9) is the probability density function associated with \u03c9. Given that both the kernel K(xi, xj) and the distribution p(\u03c9) are real, the integral can be simplified. The complex exponential eiw (xi-xj) can be expressed in terms of its real part using Euler's formula. Therefore, we can obtain a real-valued mapping zw that satisfies the condition E[zw(xi)zw(x)] = K(xi, xj) by setting:\n$z_w(x) = \\sqrt{2} cos(\u03c9x + \u03b2)$\nwhere \u03c9 ~ p(\u03c9), \u03b2 ~ Uniform(0, 2\u03c0). For the RBF kernel, the Fourier transform p(w) is a Gaussian distribution (Rahimi and Recht 2007). We now have a simple and efficient algorithm to estimate the kerneled features by pooling D independent pairs \u03c9, \u03b2 from these distributions and estimating the expectation. Therefore, we can define the random"}, {"title": "Efficiency and Runtime Analysis", "content": "feature map as:\n$z(x) = \\sqrt{\\frac{2}{D}} [cos(\u03c9_1^T x + \u03b2_1),..., cos(\u03c9_D^T x + \u03b2_D)]^T$\nwhere w is drawn from $N(0,\u03c3^{-2}I)$ and \u03b2 from Uniform(0, 2\u03c0). As the number of pooled pairs D increases, the approximation of the kernel function improves because more Monte Carlo samples are used to estimate the expectation. The dot product of these random features approximates the original kernel function:\n$z(x_i)^Tz(x_j) \u2248 K(x_i, x_j)$\nThus, z represents an approximation of \u03c6. We can now convert the input x into random features z(x) and apply linear methods. This approximation enables us to avoid directly computing the kernel matrix, making it feasible to apply in continual learning settings while preserving the benefits of the kernel transformation.\nKLDA is highly efficient as it does not update the FM parameters or compute gradients for training. Instead, it simply computes class means and the covariance matrix. On"}, {"title": "Algorithm 1: KLDA Training", "content": "Classification with KLDA\nTraining: The training process of KLDA is outlined in Algorithm 1. We first apply RFF to the original feature vector x \u2208 Rd, transforming it into z \u2208 RD. With each new class, the mean \u00b5m is calculated, and the shared covariance matrix \u2211 is updated incrementally.\nPrediction: We compute the linear coefficients wm (Eq. 5) and bias bm (Eq. 6) based on the mean vectors and the shared covariance. These parameters are aggregated into a weight matrix W = [w_m]_{m=1}^M and a bias vector b = [b_m]_{m=1}^M. The classification function in the transformed space is:\n$F(x) = z(x)W + b$\nHere, F(x) provides the score for each class, and the predicted class is the one with the highest score value.\nWe also introduce an ensemble approach, KLDA-E, to further enhance performance. It leverages multiple KLDA models, each initialized with distinct frequency matrices and phase vectors (Algorithm 1). During inference, the scores are calculated for each model and then transformed into probabilities via softmax. The final prediction is made by averaging the probabilities of all models and selecting the class with the highest average probability:\n$\u0177 = arg max_m \\frac{1}{E} \\sum_{e=1}^E Pe(y = m | x)$"}, {"title": "Experimental Setup", "content": "datasets to evaluate our proposed method. However, our\nTheoretical Justification: It has been shown theoretically that good within-task prediction (WP) and effective out-of-distribution (OOD) detection for the tasks learned so far are necessary and sufficient conditions for good CIL (Kim et al. 2022, 2023). In our approach, since each class is represented as a Gaussian distribution, effectively, each task has only one class. Then, WP is always correct and the Gaussian distribution serves as an OOD detector for each class.\nThis section outlines the datasets, baselines, implementation details, and evaluation metrics employed in our experiments. primary focus is text classification, as language foundation models (LFMs) are more mature. For our main experiments, we use the following four text classification datasets:\n\u2022 CLINC: It has 150 classes of dialogue intents from many different application domains (Larson et al. 2019). We used the train/test split of 10,000/750 samples, and the classes were randomly divided into 10 disjoint tasks.\n\u2022 Banking: It has 77 classes of dialogue intents in the banking domain (Casanueva et al. 2020). We used a 10,000/1,000 train/test split and divided the classes into 7 disjoint tasks.\n\u2022 DBpedia: A text classification dataset of Wikipedia articles with 70 classes (Liu et al. 2021b). We used a train/test split of 10,000/1,000 samples and divided the classes into 7 disjoint tasks.\n\u2022 HWU: Another dialogue intent classification dataset featuring 20 domains with 64 classes (Auer et al. 2007). We used a train/test split of 9,000/1,000 samples and partitioned the classes into 8 disjoint tasks.\nWe follow the typical CIL protocol. The classes in each dataset are randomly shuffled and assigned to the tasks. We perform multiple runs with random shuffles to account for the variability due to different task splits.\nImage Datasets: We also evaluate KLDA using four image classification datasets: CIFAR10 and CIFAR100 with 10 and 100 classes respectively (Krizhevsky, Hinton et al. 2009), TinyImageNet with 200 classes (Le and Yang 2015), and Stanford Cars with 196 classes (Yang et al. 2015), applying their official train/test splits. The comparison is made against the CIL upper bound, which jointly trains on all tasks simultaneously. Since KLDA supports the incremental addition of CPs, task splits are not required for this evaluation."}, {"title": "Implementation Details", "content": "For the main experiments, we use BART-base (Lewis et al. 2019), which consists of a 6-layer encoder-decoder architecture with a 768-dimensional hidden state. This LFM was chosen because most of our fine-tuning baselines use a generative objective or require generating pseudo-replay data during training. Additionally, the same FM is also used in the state-of-the-art VAG system (Shao et al. 2023).\nTo demonstrate the versatility of KLDA, we also evaluate it with multiple other LFMs, including paraphrase-MiniLM- L3 (Reimers and Gurevych 2019) (3 layers, 384 dimensions), BERT-base (Kenton and Toutanova 2019) (12 layers, 768 dimensions), RoBERTa-large (Liu et al. 2019) (24 layers, 1024 dimensions), T5-3b (Raffel et al. 2020) (24 layers, 1024 dimensions), and Mistral-7b (Jiang et al. 2023) (32 layers, 4096 dimensions).\nFor vision foundation models (VLMs), we use the DINOv2-small (12 layers, 384 dimensions) and DINOv2- base (12 layers, 768 dimensions) models (Oquab et al. 2023). We chose DINOv2 because it is a self-supervised model, unlike commonly used ViT (Dosovitskiy et al. 2020) and ResNet (He et al. 2016) models, which are trained in a supervised manner on ImageNet-21k or ImageNet-1k (Deng et al. 2009). This supervised training can lead to information leakage due to class overlap with datasets used in CIL.\nLAMOL and VAG were executed using their official codes and configurations. For the remaining fine-tuning baselines, we used implementations from (Shao et al. 2023) repository. The class-prototype-based baselines were implemented using our own code, adhering to the same update rules applied in KLDA to ensure consistency in comparison.\nThe Joint Fine-tuning method, representing the upper bound, is trained for 50 epochs with a batch size of 128, using the Adam optimizer with a learning rate of le-3 for the classifier head and 1e-4 for the FM parameters. Additionally, we experimented with various configurations, including different learning rates, batch sizes, and epoch numbers, to ensure the models were thoroughly trained and optimized.\nFor our ensemble approach KLDA-E, we use a set of 5 models. KLDA has two hyperparameters itself: the transformation dimension D and the RFF \u03c3. Given the CIL setup, where tasks are learned incrementally, the system does not see all tasks at the same time, and validation sets are not typically available. Therefore, it is hard to optimize the parameters for all tasks. Through empirical testing, we found that setting D to 5000 offers a balanced trade-off between memory usage and performance. The o parameter is also empirically determined within range [10-2, 10-6] for each FM. We will show the results of different parameter settings later.\nOur implementation is built using PyTorch, with all pre- trained models sourced from the Hugging Face Transformers library. All experiments were conducted on a single NVIDIA A100 GPU with 80GB of VRAM."}, {"title": "Evaluation Metric", "content": "Baselines\nFor text classification, we compare our approach against multiple baselines, categorized into fine-tuning based methods, class-prototype methods, and joint training.\nWe measure classification accuracy after all tasks have been processed, referred to as Last or Final Accuracy like (Shao et al. 2023). Each experiment is repeated three times with different random seeds, and the average accuracy is reported to ensure reliable results. We also discuss the efficiency and the memory requirement of the proposed method."}, {"title": "Fine-tuning Based Baselines", "content": "\u2022 Vanilla: It sequentially fine-tunes the model on each task with no mechanism to mitigate forgetting.\n\u2022 EWC: A regularization-based method that uses a penalty to preserve important parameters from previous tasks to mitigate forgetting (Kirkpatrick et al. 2017).\n\u2022 KD: It uses knowledge distillation to help the model retain information from old tasks by learning from softened output probabilities of previous versions of itself (Hinton, Vinyals, and Dean 2015).\n\u2022 L2P: Freezes the model and learns trainable prompts to guide inference, adapting to new tasks without altering the base model (Wang et al. 2022b).\n\u2022 LAMOL: Employs pseudo-replay by generating pseudo- examples of previous tasks to mix with new task data, maintaining past performance while learning new tasks (Sun, Ho, and Lee 2019).\n\u2022 VAG: It leverages vocabulary sparsity to mask the probability of unused tokens when training on a task, mitigating forgetting via label generation rather than the traditional classification objective. (Shao et al. 2023)."}, {"title": "Class-prototype Based Baselines", "content": "\u2022 NCM: It maintains a mean feature vector for each class, added incrementally. Classification is based on the distance to the mean vectors.\n\u2022 LDA: This utilizes the original feature space without our RBF kernel extension."}, {"title": "Joint Training Baseline", "content": "\u2022 Joint Fine-tuning: Fine-tuning the FM by updating its parameters and a classifier head added on top of the latent features, training on all classes simultaneously as a single task. This approach is regarded as the upper-bound performance of CIL."}, {"title": "Experiment Results", "content": "This section evaluates KLDA in terms of accuracy, memory usage, and efficiency across multiple text classification baselines. Additionally, KLDA's performance is compared to the Joint upper bound on image classification datasets, along with an analysis of the impact of hyperparameters on its performance."}, {"title": "Comparison with Baselines", "content": "prompt-based methods (L2P), and pseudo-replay methods (LAMOL, VAG). Despite specialized mechanisms for mitigating CF, these methods still exhibit significant forgetting, with even the best-performing method, VAG, falling short of the accuracy achieved by the simple NCM method.\nNCM, while effective, significantly underperforms compared to Joint, indicating that merely accumulating a mean feature vector for each class is insufficient to fully leverage the information in the FM representations. LDA improves class separation by incorporating a shared covariance and KLDA improves this by leveraging the kernel. The addition of the ensemble approach further enhances accuracy.\nKLDA-E consistently matches the accuracy of the Joint Fine-tuning upper bound, even surpassing it on 3 out of the 4 datasets, and achieving nearly identical results on the fourth (DBpedia). Notably, even KLDA alone performs on par with Joint Fine-tuning. This shows that the features of FMs are well-suited for accurate CIL, and the key lies in how to utilize these features appropriately, which is achieved by the proposed method.\nMemory Usage Comparison\nWe compare the methods in terms of memory usage. The Joint Fine-tuning only adds a classifier head on top of the FM features, introducing approximately 0.1M additional parameters for typical values of M = 150 classes and d = 768 hidden dimensions. Fine-tuning baselines, particularly those required to operate in the generation mode, significantly increase memory usage. For instance, an LM head required for text generation adds approximately 38.5M parameters for a vocabulary of 50,265 tokens, although this number does not increase with the number of classes.\nCP methods are more memory-efficient as they only require storing the class prototypes. NCM requires M \u00d7 d parameters for the mean vectors, similar to the classifier head of the Joint model. LDA adds an d x d covariance matrix, increasing the parameter count by approximately 0.6M. KLDA introduces D \u00d7 (d+1) fixed parameters for the RFF transformation. With D set to 5000, this adds around 3.8M parameters. KLDA also scales the parameters required for CPs by a factor of D/d, leading to an additional 0.75M pa-"}, {"title": "Generalizability Across Different LMs", "content": "Table 1 compares the performance of KLDA with various baselines, including regularization techniques (EWC, KD), rameters. The covariance matrix for KLDA is D \u00d7 D, resulting in an additional 25M parameters. In total, KLDA's memory footprint is approximately 29.5M parameters. This memory requirement is still significantly lower than the LM head needed for text generation alone. Our ensemble method utilizes 5 models, resulting in a 5x increase in memory usage, which remains within a reasonable limit. For reference, the BART-base used in our main experiments has 139.5M parameters. We highlight that a large portion of KLDA's parameters are associated with the fixed RFF transformation and the shared covariance matrix, which do not increase as more classes are added to the CIL process.\nWe conducted experiments on text classification datasets with five other LMs of varying sizes. The results, shown in Table 2, indicate that KLDA consistently achieves performance comparable to Joint Fine-tuning across all datasets, regardless of the LM used. This highlights the robustness of our approach for CIL."}, {"title": "Evaluation on Image Datasets", "content": "We now evaluate KLDA on image classification datasets, comparing its performance against the Joint upper bound. We do not use other baselines here as their accuracy values are significantly below the upper bound (Lin et al. 2024). The results are presented in Table 3. KLDA achieves performance on par with the Joint on two datasets and performs competitively on the other two (CIFAR100 and Tiny ImageNet) with only a slight gap. This gap suggests that the cur-"}, {"title": "Conclusion", "content": "rent VFMs, unlike their language counterparts, still lack sufficiently expressive and generalized features for more complex datasets.\nKLDA is highly efficient as it does not update the FM parameters or compute gradients for training. Instead, it simply computes class means and the covariance matrix. On"}, {"title": "Analysis of Hyperparameters", "content": "We measure classification accuracy after all tasks have been processed, referred to as Last or Final Accuracy like (Shao et al. 2023). Each experiment is repeated three times with different random seeds, and the average accuracy is reported to ensure reliable results. We also discuss the efficiency and the memory requirement of the proposed method."}, {"title": "Limitations", "content": "KLDA's performance depends on two key hyperparameters: transform dimension D and kernel scale \u03c3. Figure 1 shows how these hyperparameters affect accuracy across different text classification datasets. D controls the balance between the memory usage and the accuracy of kernel approximation. We found that setting D to 5000 provides a good balance, offering sufficient accuracy without excessive memory usage. o affects the scale of the RBF kernel and thus influences the separation of the transformed features. We fixed \u03c3 for all the datasets after determining it for each FM. KLDA performs well across all datasets with this parameter setting. This indicates that KLDA can learn various tasks incrementally without adjustments to its configuration.\nClass-incremental learning is perhaps the most challenging setting of continual learning as it faces two major CF and ICS problems. KLDA avoids both challenges by utilizing a fixed foundation model and relying solely on its latent features while enhancing them with the Radial Basis Function kernel. Since computing the full kernel matrix is infeasible in CIL, we employed Random Fourier Features, an approximation method that enables kernel transformation while allowing incremental updates to our classification model. This is achieved by maintaining class means and a shared covariance matrix, allowing for better class separation through Linear Discriminant Analysis. Our experiments show that KLDA significantly outperforms existing baselines and, more importantly, achieves accuracy levels on par with the upper bound accuracy of joint training.\nKLDA assumes that the foundation model contains sufficient features for CIL tasks in the target domain. If the FM features are not well-suited to a specific domain, the accuracy of our method may suffer. A standard approach to address this is to fine-tune or adapt the general-purpose FM using a large domain-specific corpus before applying it to CIL."}]}