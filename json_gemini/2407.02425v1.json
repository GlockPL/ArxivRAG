{"title": "Reinforcement Learning and Machine ethics: a systematic review", "authors": ["Ajay Vishwanath", "Louise A. Dennis", "Marija Slavkovik"], "abstract": "Machine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some systematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses reinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we have witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics specifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic review aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine ethics landscape.", "sections": [{"title": "1 Introduction", "content": "Machine ethics \"is concerned with the behaviour of machines towards human users and other machines\" [8]. This includes studying how to implement machine reasoning in a way that includes the consideration of ethical factors [9, 75]. Since a 2006 AAAI Workshop on Machine Ethics [8], there has been a rapid expansion in proposed systems and formalisms for performing machine ethics (e.g, [73, 28, 19, 17, 40, 30]) as evident from surveys of the field [44, 71, 77].\nReviews of machine ethics, at present, cover the period up to 2020. Currently, we observe a notable trend in reinforcement learning, specifically focusing on the ethical behavior of reinforcement learning agents.\nIn reinforcement learning (RL) [68] an artificial agent makes decisions in an environment and receives rewards and punishments as outcomes of those decisions. The goal, typically of the learning process, is to find a policy on how to make the decisions so that an optimal reward is attained. Reinforcement learning, and specifically deep reinforcement learning that combines neural networks and reinforcement learning, has been of particular recent interest since its successful application in training artificial agents to play human level computer games [42]. As reinforcement learning is about training an artificial agent how to make decisions, it is very intuitive as to why ethical decision-making would receive attention in the reinforcement learning community.\nEarly papers on the reinforcement learning and machine ethics topic, such as [11], gained recognition, along with subsequent work by researchers [13, 34, 43, 56, 45, 70, 74]. There is a gap in the understanding of the state of the art in machine learning if that understanding does not include the most recent work on reinforcement learning. Our goal is to bridge this gap by conducting a literature review on reinforcement learning in the context of machine ethics.\nFollowing a keywords search in the usual repositories of artificial intelligence research articles we found 605 potential papers which we have narrowed down to 62 papers that were on topic and which were carefully analysed. We analysed the papers based on the RL paradigm they deploy, how they modify which RL \"component\" to achieve ethical behaviour, and which ethical theory they directly or indirectly implement as well as what kind of implementation examples the papers use. Beyond this we observe and discuss the trends in this body of work and draw recommendation for future research in this field.\nThis article contributes to both the disciplines of reinforcement learning and machine ethics by establishing a clear state of the art of the intersection of these two fields and giving future work a good anchor to position and grow from existing experience. Furthermore, we highlight challenges and possible pitfalls of reinforcement learning and machine ethics, thus ensuring a responsible and sustainable development of what at present appears to be a trendy and growing research field."}, {"title": "Background and related work", "content": "In this section, we discuss reinforcement learning and the various algorithms employed in the papers we review. This is followed by a discussion on previous works in machine ethics and relevant gaps."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning (RL) is a machine learning paradigm in which an agent navigates an environment with states \\(s \\in S\\), actions \\(a \\in A\\),\nnext states \\(s' \\in S'\\) and rewards \\(r \\in R\\). Such an agent learns through trial and error an optimum behavior which maximizes a numerical reward\nfunction [68]. This optimum behavior is denoted by \\(\\pi^*\\), or the optimum policy. Markov decision processes (MDP) are a mathematically\nidealized form of the RL problem. In an MDP, the probability of each possible value of the next state \\(S'\\) and reward \\(R\\) depends on the\nimmediately preceding state and action, \\(S\\) and \\(A\\), completely characterize the environment and that these states, actions and rewards are finite.\nA method to solve MDPs is by using value function approximation, where a numerical value is calculated for being in a state \\(V(s)\\) and\ncommitting an action in a given state \\(Q(s, a)\\). As the agent visits different states in the environment, a value table is updated based on the\nreward obtained. This method is often referred to as Q-learning, where a Q table is updated and based on these Q-values an optimal policy is\ndetermined. However, an issue with such an approach is that in an environment with a very high state space, such as the game of chess, it can\nbe very expensive to compute a Q-table.\nIn high state-space scenario, a neural network is often used to encode the state space and output the Q-value for being in that state. This\nparadigm of using a neural network in the Q-learning algorithm is referred to as Deep Q-learning. There are, however, limitations with using\nvalue function approximation, since there is an exploration-exploitation conundrum. These networks are said to be more exploratory in nature\nand often fail to converge to an optimum. To overcome this issue, actor-critic methods are used where two neural networks, the actor and the\ncritic are used to predict action probabilities and Q-values, respectively, instead of only calculating Q-values. Methods such as PPO, DDPG\netc are popular actor-critic methods.\nThere are other types of RL, such inverse RL, multi-objective RL and co-operative RL. In inverse RL (IRL) [1], the agent learns the\nreward functions in an environment, given the policy. This is useful in applications such as robotics, where the robot can \"watch\" a human\ndemonstrate a task (optimal policy) and then the robot can imitate the same, while learning a reward function in its environment. Next, multi-\nobjective RL (MORL), an agent is trained to optimize conflicting objective functions, demonstrated in works by Rodriguez et al. [59] and\nservral others discussed in this review. Finally, co-operative inverse reinforcement learning, is a variant of IRL in a multi-agent scenario.\nHowever, unlike classical IRL, where the human is the basis for learning, the other agents in the system often perform active teaching, learning\nand communicative actions [33]."}, {"title": "Systematic reviews in machine ethics", "content": "We here briefly summarise the findings of the three reviews in machine ethics, their findings and methodology.\nTolmeijer et al. [71] focused on the implementations of machine ethics specifically and found 49 papers from Web of Science, Scopus, ACM\nDigital Library, Wiley Online Library, ScienceDirect, AAAI Publications, Springer Link, and IEEE Xplore. These papers were then analysed\nbased on types of ethical theory they implemented, the nontechnical aspects when implementing those theories, and technological details.\nNallur [44] considers the landscape of machine ethics implementations focusing on the different problems, techniques, methodologies and\nevaluation methodologies that are used. He did not do a systematic review, but rather a more qualitative analysis of selected prominent works.\nYu et al [77] survey advances in techniques for incorporating ethics into AI. They consider publications in \"leading AI research conferences\nincluding AAAI, AAMAS, ECAI and IJCAI, as well as articles from well-known journals\". They organise the papers into four categories:\nExploring Ethical Dilemmas, Individual Ethical Decision Frameworks, Collective Ethical Decision Frameworks; and 4. Ethics in Human-AI\nInteractions. There is of course significant overlap among the papers included in all three surveys.\nWe here follow to a large degree the approach of Tolmeijer et al. [71] in terms of where we search for papers. This is because Tolmeijer et\nal. [71] is the only systematic review in machine ethics. Understanding the state of the art in machine ethics is not without challenges. The\npapers published in this area are sent to a wide spectrum of venues. Machine ethics is also not a very large area of research and it is not very\nprecisely positioned in the context of the AI alignment and AI Ethics initiatives. Therefore, it has to be accepted that any systematic review of\nthe field is likely to not be exhaustive, even if we limit ourselves to a specific domain or methodology such as reinforcement learning."}, {"title": "2 Survey methodology", "content": "In this section, we describe the strategy used to find papers related to machine ethics and reinforcement learning, following which we discuss\nthe process used to review the found articles."}, {"title": "Search strategy", "content": "Our review on implementations in machine ethics focuses on RL, hence we mandated this terminology while querying the databases. Since\nRL is to be used towards machine ethics implementations, we used alternative search terms such as artificial morality [5], machine morality\nand robot ethics [41], computational ethics [10], roboethics [26], artificial moral agents [22], and value alignment [59]. These terms have been\nused interchangeably to communicate the embedding of ethical theories into machines/computers/artificial intelligence agents. We show the\nquery used below:\n\"reinforcement learning\" AND (\"machine ethics\" OR \"artificial morality\" OR \"machine morality\" OR \"computational ethics\" OR\n\"roboethics\" OR \"robot ethics\" OR \"artificial moral agents\" OR \"value alignment\")\nThe query was used to search the following databases"}, {"title": "The analysis", "content": "In this section, we discuss the taxonomy and categorizations used to organize our survey. Since the focus is on RL, we closely examined\nits components such as the reward, policy, and value function. Additionally, we considered the specific RL paradigms used, allowing us to\nobserve trends within the research community. Furthermore, we explore ethical theories and implementation examples, which constitute the\ncore of machine ethics implementations. Finally, we categorize trends in machine ethics for RL based on contribution type, whether they are\nargumentative, empirical, mathematical, or a combination."}, {"title": "Component of RL Modified", "content": "There are several aspects that affect RL algorithms and their performance on tasks. For instance, the challenge of designing a good reward\nfunction to correctly incentivize agents can make or break the agent's performance. Other aspects such as policy functions, value functions,\netc., also play a part. In terms of developing RL algorithms which make more ethical choices, several researchers modify either one or more\nof these components to achieve the desired result. We categorized the research articles based on which component of RL was modified and we\npresent our findings"}, {"title": "RL paradigm", "content": "Similar to observing the component of the RL algorithm modified, it was also interesting to categorize and analyse the RL paradigm adopted\nto bring about ethical behavior. By paradigm, we refer to the type of RL used based on which the relevant component is modified. We briefly\nintroduced some of the RL paradigms in Section 1, such as inverse RL, multi-objective RL, constrained RL, etc. Each of these paradigms has\nvariants and sub-variants, but these variations within paradigms largely function in a similar fashion. In Table 3, we specify the paradigms and\nthe corresponding research works."}, {"title": "Ethical Theory", "content": "A crucial aspect to explore in this systematic literature review is the ethical theories used by the researchers in their respective works. We\nreviewed each article to try to understand its underlying ethical proclivities. Based on this examination, we found that a majority does not\nexplicitly state their ethics specifications and due to this, it was challenging to distinguish these manuscripts based on moral philosophy.\nHowever, to help researchers better understand the ethical underpinnings behind these machine ethics papers, we focus on the ethical theory\nthat influences the RL algorithm the most, rather than all the theories at play. For example, if researchers codified social norms and then\noptimized a utility function as is done in RL, then we consider such implementations as deontological. On the other hand, if a paper does\nnot explicitly mention deontological codification based on rules or norms, then it is considered consequentialist (which also includes variants\nof utilitarianism). We also found several other researchers using human values, where they optimized metrics based on human evaluation. In\nTable 4, we summarize the ethics distinctions."}, {"title": "Implementation Example", "content": "A wide variety of settings were used for experimental evaluation of systems. Very few of these settings were shared across teams of researchers\nrevealing a lack of comparative benchmarking sets. As a result, ethical frameworks were generally compared against baselines in which there\nwas no attempt to represent ethics, rather than alternative frameworks or theories. A broad categorisation of these examples can be seen in\nTable 5.\nThe only family of examples that appeared to have real traction in the literature were examples involving the equitable sharing of resources\nin a multi-agent setting - in which the metrics used were the ability of the community to support all its members. These scenarios ranged\nfrom abstract Grid Worlds representing a foraging games with various opportunities to share and/or steal resources from other agents, to more\nrealistic settings based on, for instance, water distribution among several communities [35].\nAnother common family of examples were abstract representations of forbidden behaviour - often as a Grid World with forbidden areas\n[4] has a variant on this that involved the agent learning to play Tetris taking into account human preferences about colour placement. In\nmany cases it was difficult to see these examples as specifically about ethical behaviour, as opposed to more general examples around safe\nreinforcement learning, learning preferences, or constrained learning.\nBeyond these two distinct families of examples, there was a varied range of examples which involved motion planning in an environment\nthat contained conflicting values or values conflicting with goals. For instance two papers [45, 46] required an agent to learn to play vegan\npac-man where it was deemed unethical to eat the ghosts. Another set of papers all by the same team [55, 58, 57, 59] involved a \u201cpublic civility\ngame\" in which agents need to reach a goal which is obstructed by rubbish and they must learn to divert to place the rubbish in a bin, rather\nthan throwing it at the other agents involved.\nTwo examples involved moderately sophisticated simulations of smart grids [23] and financial markets [21].\nThe remaining cases were ad-hoc examples some with clear ethical relevance - e.g., around patient consent to treatment [38]. However, in\nsome cases, particularly in systems where the focus was primarily on learning human norms, rather than a more specialised focus on values,\nwe saw examples that seemed more about learning preferences than anything with particular ethical force \u2013 for instance [29] focuses on robot\nchef learning to cook meals that its owner will like."}, {"title": "Contribution type", "content": "We found several differences in claimed types of contributions in our literature review. Some were position papers, which were argumentative\nin nature, theorizing on how RL could be used in machine ethics research. Others were purely empirical contributions, either by referring to a\npreviously proposed theory on a new problem, or improving upon existing results. The remaining ones were a combination of argumentative,\nempirical and mathematical proofs. It was interesting to explore these contribution types mainly to understand the maturity of the field: RL in\nmachine ethics. The contributions are summarized"}, {"title": "Discussion", "content": "We here discuss the trends we observed in the surveyed papers and draw out some recommendations for future research based on our observa- \ntions."}, {"title": "Recent increase of contributions in RL", "content": "Based on our survey, we found a steady increase in the use of RL in machine ethics"}, {"title": "No moral theories", "content": "Wallach [75] categorises approaches to the problem of machine ethics as either \"top-down\" or \"bottom up\". Broadly, top down approaches\nseek to operationalise some ethical theory from Philosophy and apply this to a decision faced by the machine. Bottom-up approaches seek to\nlearn ethical behaviour from data."}, {"title": "Paradigm trends", "content": "A significant number of approaches we encountered have encoded the ethics in some fashion within the reward or objective function, within\nthese we observed three trends:\nEthical RL is Multi-Objective RL In many cases the ethics signal was implemented as a reward signal in addition to the other (non-ethical)\nrewards such as maximising profit or efficiency. This frames ethical RL as a form of multi-objective RL.\nEthics as Constraints In other cases the ethics were represented as constraints. Ethical behaviour did not confer some reward upon the agent,\nalthough in some cases unethical behaviour incurred a penalty. This frames ethical RL as a form of constrained RL. In some cases there was\nan additional requirement that behaviour remain ethical during training in which case ethical RL becomes a form of Safe RL.\nMulti-agent RL Another significant theme, highlighted by the number of implementation examples based around equitable distribution of\nresources, is framing ethical ethical behaviour in terms of maximising group benefit in a multi-agent context.\nWe would argue that it is a valuable contribution to the field to point out that much by way of ethical behaviour can be achieved by appropriate\napplication of techniques from Constrained Multi-Objective RL and Safe RL, but that going forward researchers should take this point as\nestablished and, if working in these areas, focus their attention on how ethics can be represented in terms of these multiple objectives and\nconstraints."}, {"title": "Human factors: whose ethics?", "content": "Based on our analysis of ethics specifications and moral philosophy, we found that several researchers have included a human factor to\nimplement ethical behavior in RL agents. It is either the developer, the user, an expert or an adversary (Table 7), whose ethics an RL agent\nmust try and emulate. Clearly, there is no real agreement on which ethical theory we, as humanity fall back on. We use some version of\nutilitarianism, social contracts, virtue ethics, deontology and other ethical theories interchangeably."}, {"title": "Summary", "content": "We here present and analyze the results of a systematic review of articles that use reinforcement learning for machine ethics or consider\nthe problem of machine ethics in reinforcement learning. The increased volume of papers in machine ethics RL in recent years points to a\npromising area of research. The main challenges for the RL in machine ethics community going forward is to organise itself around a taxonomy\nthat will help authors position their efforts against existing work.\nThe secondary challenge is to avoid \"anecdotal ethics\". RL in machine ethics most of the time uses bottom-up and sometimes hybrid\napproaches (in the sense of [75]) to achieve artificial moral agency, to avoid developing methods around the moral sensitivities and judgments\nof its developers. The secondary challenge is thus to establish a firmer connection with the fields of moral philosophy and moral psychology\n[16]. The systematic evaluation and bench-marking of the abilities of reinforcement learning ethical agents remains an open problem."}]}