{"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning", "authors": ["Qitao Tan", "Jun Liu", "Zheng Zhan", "Caiwen Ding", "Yanzhi Wang", "Jin Lu", "Geng Yuan"], "abstract": "Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose Divergence-driven Zeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning ROBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning.", "sections": [{"title": "1. Introduction", "content": "Fine-tuning pre-trained large language models (LLMs) with backpropagation demonstrates superior performance for many natural language processing tasks. However, the extensive parameterization imposes a substantial memory burden, limiting their practicality for widespread downstream applications. In line with the neural scaling laws, next-generation LLMs continue to increase in parameter count. Specifically, model sizes are expanding at a rate of 410x every two years, dramatically outpacing the scaling of DRAM bandwidth (1.4\u00d7 every two years) and DRAM capacity (2x every two years). This disparity leads to the memory wall challenge, which becomes even more severe when deploying LLMs on memory-limited devices.\nRecently, zeroth-order (ZO) optimization has emerged as a promising memory-efficient training paradigm for LLM fine-tuning, attracting significant attention. By relying solely on forward passes (i.e., inference) to estimate gradients and update model parameters, ZO bypasses the need for backward propagation and significantly reduces extensive storage requirements for activations, gradients, and optimizer states. As reported, fine-tuning LLMs via ZO optimization reduces up to 12x memory cost. Nevertheless, ZO optimization still exhibits a gap in convergence speed and accuracy compared to the conventional first-order (FO) method (i.e., compute gradient via backpropagation). As shown in Table 1, one can observe that the FO method substantially outperforms ZO method in both accuracy and GPU hours. Though ZO method achieves higher throughput due to its computational simplicity, it requires more than 10x iterations for convergence, dramatically increasing GPU hours. Previous studies typically attribute this gap to the fact that ZO optimization leverages random perturbation for gradient estimation, and thus results in unavoidable estimation error, but without further exploration of other underlying causes.\nTo bridge this gap, we begin by examining the distinct update patterns shown by ZO and FO methods during LLM fine-tuning. Our analysis reveals a substantial difference in their layer-wise update magnitudes. Specifically, ZO method relies on high-dimensional random search and tends to apply uniform-magnitude updates without consid-"}, {"title": "2. Preliminaries and Pattern Analysis", "content": "ering layer-wise individual characteristics. In contrast, FO method benefits from fine-grained gradient estimation and applies diverse-magnitude updates precisely scaled to the layer-wise individual optimization needs. Motivated by these observations, we are interested in investigating: if we can also provide ZO with diverse-magnitude updates, effectively achieving training acceleration and accuracy improvement.\nDrawing on these insights, we propose Divergence-driven Zeroth-Order optimization (DiZO). DiZO conducts divergence-driven layer adaptation by incorporating projections, enabling layer-wise adaptive updates that closely resemble FO approaches. Notably, the projections can be optimized without gradients, ensuring that DiZO retains the appealing backpropagation-free features. Moreover, we validate DiZO on a variety of tasks, including classification and generation, using several LLMs such as ROBERTa-large, the OPT series, and the Llama series. Experimental results show that DiZO substantially decreases training iterations for convergence while maintaining throughput, cutting training GPU hours by up to 48% on diverse datasets. Furthermore, our method can be seamlessly integrated with parameter-efficient tuning techniques like low-rank adaptation for additional speedups. DiZO also consistently outperforms the representative ZO baselines and, in some cases, surpasses memory-intensive FO fine-tuning.\nThe summary of our contributions is as follows:\n\u2022 We introduce a novel layer-wise divergence analysis to uncover the fundamental differences in the updating patterns of FO and ZO methods.\n\u2022 We introduce DiZO, a novel ZO method using divergence-driven layer adaptation, achieving a learning capacity closely resembling FO while maintaining the throughput benefit of ZO optimization.\n\u2022 DiZO consistently exceeds existing baselines in both accuracy and GPU hours, and it can be seamlessly integrated with LoRA for additional benefits. These advantages hold across diverse tasks and LLM archi-\ntectures."}, {"title": "2.1. Revisiting Zeroth-order Optimization", "content": "Recently, ZO optimization has gained significant attention in machine learning. Unlike conventional FO optimization, which calculates gradients via backpropagation, ZO optimization estimates gradients using only objective oracles via finite differences. This property can be leveraged for LLM fine-tuning to alleviate the extensive memory costs. Specifically, as ZO only needs two forward passes to obtain the estimated gradients, it avoids computing and storing the most memory-consuming information needed in the conventional FO training, i.e., activations in the forward process, gradients in the backward process, and the optimizer state.\nThe core idea of ZO optimization is to estimate gradients by applying random perturbations to the weights and computing differences in the objective. For a mini-batch of data B, sampled from a labeled dataset D = {xi, yi}i=1q, a model with parameters \u03b8 \u2208 Rd, where d represents the dimension of the parameter space, and the corresponding loss function L(\u03b8; B). The gradient is estimated as follows:\n$\\nabla L(\\theta; B)=\\frac{1}{q} \\sum_{i=1}^{q} \\frac{L\\left(\\theta+\\epsilon u_{i}; B\\right)-L\\left(\\theta-\\epsilon u_{i}; B\\right)}{2 \\epsilon} u_{i}$   (1)\nwhere ui is a random vector with the same dimension as the model weights and is typically drawn from standard Gaussian distribution N(0, I), or from Gaussian sampling over a unit sphere, q is the number of objective queries, and \u03f5 > 0 is a small perturbation scalar for smoothing.\nGiven the learning rate \u03b7 and the mini-batch data Bt at t-th iteration, once the estimated gradient \u2207L(\u03b8; Bt) is obtained, then ZO-SGD updates the parameters with the following rule:\n$\\theta_{t+1}=\\theta_{t}-\\eta \\nabla L(\\theta_{t}; B_{t})$  (2)"}, {"title": "2.2. Layer-wise Divergence Analysis", "content": "Drawing insight from the update formula of ZO optimization, we notice that ZO method applies uniform-magnitude updates across layers, e.g., the L2-norm of the updates is about the same for all layers in one iteration (see Appendix E for proof). This fact may be the root of the inferior performance of ZO optimization. To measure how the divergence of update magnitude affects the convergence speed and accuracy, we investigate the training dynamics of ZO and FO methods, respectively."}, {"title": "Analysis indicator.", "content": "To quantify the effect of updates, we adapt the layer-wise L2-norm distance gap between the weights of the pre-trained and the fine-tuned model as an indicator. The layer-wise L2-norm distance gap is defined as:\n$\\|\\Delta\\theta_{t}^{(l)}\\|_{2}=\\|\\theta_{t}^{(l)}-\\theta_{0}^{(l)}\\|_{2}$  (3)\nwhere t is t-th fine-tuning iteration, l is l-th layer of the model, and \u03b80(l) indicates the weights of l-th layer of pre-trained model."}, {"title": "Analysis result.", "content": "Figure 1 compares the training dynamics of FO and ZO methods. Regardless of whether ZO or FO is used, the divergence of distance gap among layers grows during training, i.e., the line of distance gap gradually 'bends'. This pattern implies that different layers benefit from maintaining diverse distance gaps with the pre-trained model. However, FO and ZO diverge in how the distance gap divergence is accumulated. FO employs fine-grained gradient estimations, resulting in diverse-magnitude updates (FO updates in Figure 1). Therefore, it can rapidly reach the desired layer-wise distance gap in only a few iterations. In contrast, ZO relies on random search in high-dimensional parameter space and generates uniform-magnitude updates (ZO updates in Figure 1), resulting in thousands more iterations required for accumulating a meaningful layer-wise distance gap.\nWith the above findings, we suspect the inferior performance of ZO stems from its inability to deliver layer-wise adaptive updates, a challenge that arises from its reliance on random perturbations for gradient estimation."}, {"title": "3. Methodology", "content": "We find that ZO applies uniform-magnitude updates for all layers, which could be the root of its inferior performance in accuracy and convergence speed. Consequently, we introduce a variant of ZO optimization which performs divergence-driven layer adaptation, thereby providing diverse-magnitude updates to enhance the overall learning capacity."}, {"title": "3.1. Design of the Divergence-driven Layer Adaptation", "content": "To provide layer-wise adaptive updates for ZO optimization, we apply projections to the updates of different layers, generating updates with diverse magnitudes. The pseudocode for the proposed method is shown in Algorithm 1.\nSpecifically, We treat training iteration as a two-step process that iteratively updates the weights and the projection factor. Our approach involves two key steps performed in an alternating manner. First, we perform vanilla ZO optimization as defined in Eq. (2). Second, we identify the ideal projections for the weights and apply them, generating the projected weights. Formally, we define the ideal projection learning as solving the following minimization problem:\n$\\min_{\\Upsilon_{t}} L\\left(\\theta_{0}+\\frac{\\Upsilon_{t}}{\\|\\Delta \\theta_{t}\\|} \\Delta \\theta_{t}; B_{t}\\right)$ (4)\nwhere Yt = {\u03b3l}l=1L is a projection vector at t-th iteration, and L is the number of layers. While searching for the ideal projection, we freeze the model weights and use the same mini-batch data Bt that is employed for the main ZO weight fine-tuning.\nAfter finding the ideal projection for the t-th ZO step, we project the weights as:\n$\\theta_{t}=\\theta_{0}+\\frac{\\Upsilon_{t}}{\\|\\Delta \\theta\\|} \\Delta \\theta$ (5)\nwhere we get the new \u03b8t after projection, and then we use the projected one for the following fine-tuning. When the value of \u03b3l is larger than ||\u2206\u03b8t||, enlarges the distance gap between the fine-tuned model and the pre-trained model, and vice versa."}, {"title": "3.2. How to Learn the Projection?", "content": "Although promising, finding the ideal projection (defined in Eq. (4)) remains challenging due to the high complexity of the objective. A straightforward solution is to also perform backpropagation for gradient computation and optimize the projection accordingly (FO-based method). For example, we use Adam optimizer to directly update Yt. The results are shown in Table 2, one can observe that it significantly reduces 67.7% of the iteration and 58.5% of the training GPU hours, and increases by 3.4% in accuracy. These results underscore the effectiveness of incorporating our proposed divergence-driven layer adaptation.\nHowever, searching projection with the FO method makes DiZO only partially gradient-free. Specifically, while the model weights are updated via ZO, the per-layer projection parameter fe is updated via FO, which still requires the backward pass and storing memory-intensive activation. The only memory saving, compared to the vanilla FO fine-tuning, is the optimizer state. As a result, relying on FO to"}, {"title": "3.3. Projection Learning by Zeroth-order Optimization", "content": "Our major goal is to find the ideal projection for adaptive updates while avoiding memory-intensive backpropagation. One potential promising solution is to also utilize the ZO method to update the projection. We estimate the gradient and update the projection as:\n$\\nabla_{\\Upsilon_{t}} L(\\Upsilon_{t}; \\theta_{t})=\\frac{L\\left(\\Upsilon_{t}+e \\mathbf{u}; \\theta_{t}\\right)-L\\left(\\Upsilon_{t}-e \\mathbf{u}; \\theta_{t}\\right)}{2 \\epsilon}$ (6)\n$\\Upsilon_{t, j+1}=\\Upsilon_{t, j}-\\eta \\nabla L(\\gamma_{t}; \\theta_{t})$ (7)\nwhere u \u2208 RL is a random vector from N(0, I), L is the objective defined in Eq. (4).\nHowever, naively applying vanilla ZO optimization for the sub-optimization (projection learning) results in unsatisfactory enhancement. More critically, it can lead to sub-optimization failure and undermine the main fine-tuning process (see Appendix C.2 for results). Two primary issues contribute to the failure. First, the values of projections are not only related to \u03b3t but also the distance gap ||\u2206\u03b8t||. Ignoring the distance gap when searching for projections causes uninformative optimization and yields sub-optimal solutions. Second, because the projection is derived through noisy ZO optimization over only a few iterations, there is a risk that the projection magnitude becomes inappropriately small or large. A small projection drives the fine-tuned model too close to the pre-trained model, nullifying many previous updates, while a large projection applies overly ag-"}, {"title": "4. Discussion and Overhead Analysis", "content": "gressive weight updates, destabilizing the training process.\nTo address the above issues, two strategies are devised.\nRe-initialization. To introduce the distance gap ||\u2206\u03b8t || into the projection learning process, the initial value Yt,0 is reset to || \u2206\u03b8t || each time the projection is optimized. This means that, initially, the projection magnitude = 1. If pro- || \u0394\u03b8t jection updates are not performed, DiZO reverts to standard\nZO optimization.\nProjection clipping. To prevent drastic weight changes and maintain training stability, we introduce projection clipping. Specifically, given a clipping range \u03c4 > 0, if the projec- tion magnitude = , it is clipped to\nremain within this interval. This prevents aggressive model adjustments that could destabilize training.\nWith the above two strategies, we enhance the learning process of projection, more analysis can be found in Appendix C.2. We also provide a Pytorch-style implementation, please refer to Appendix F for details."}, {"title": "4. Discussion and Overhead Analysis", "content": "We have some discussion on our method and analyze the computational overhead here and elaborate further later."}, {"title": "Would adjusting the learning rate be equally effective?", "content": "As discussed in Section 2.2, our main objective is to provide ZO optimization with diverse-magnitude updates. A seemingly straightforward alternative is to assign different learning rates to each layer. However, in practice, this approach yields results that are similar to or even worse than vanilla ZO in terms of accuracy and GPU hours. We attribute this to the noisy gradient estimation of one single ZO step, which is likely to yield imprecise update directions. Therefore, using unrefined layer-wise learning rates can intensify this noise and further destabilize the optimization process. In contrast, DiZO enables the awareness of the pre-trained model during fine-tuning (see Eq. 5), robustifies the training process, and mitigates the noise introduced by ZO's random perturbations. More results and analysis are shown in Appendix C.3."}, {"title": "Memory utilization.", "content": "Our method requires additional memory as it involves storing the pre-trained model and calculating the weight distance gap with the fine-tuned model, which can become costly when scaling to large LLMs. However, in DiZO, we find that projecting only the weight updates of the Query and Value layers in the attention module, instead of updating all layers, not only reduces memory requirements but also delivers better performance. As a result, we only need to store the weights of these two types of layers from the pre-trained model, accounting for approximately 16.7% of the parameters in OPT-2.7B, which is a manageable over-head. Similarly, LoRA also focuses on"}, {"title": "Computational overhead.", "content": "Our method introduces extra computational cost, as the projection is learned alongside the main optimization (fine-tuning). However, we observe that performing projection learning intermittently, only once every few training iterations, does not compromise performance and significantly reduces the added overhead. This strategy reduces the computational burden while maintaining efficiency, allowing DiZO to achieve throughput comparable to vanilla ZO fine-tuning. Additionally, the reduced frequency of projection updates ensures that DiZO remains scalable for larger models and datasets. Please refer to Section 5.4 and Appendix D.1 for more details on computational overhead."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Settings", "content": "Models and datasets. We evaluate DiZO with various models, including medium-sized masked models and large-sized autoregressive models with different size, including OPT-2.7B, OPT-6.7B, Llama3-3B, and Llama3-8B. The total parameter size is ranging from 355M to 8B. Both classification and generation tasks are included. More details on datasets are shown in Appendix B.1.\nBaseline. We mainly compare with two ZO works, memory-efficient ZO optimization (MeZO) and Hessian-informed ZO optimization (HiZOO). MeZO is a fundamental and representative work in ZO LLM fine-tuning but suffers from slow convergence speed. HiZOO is a recently proposed ZO acceleration for LLM fine-tuning, which leverages the estimated second-order information to speed up. In addition, we also incorporate the parameter-efficient fine-tuning (PEFT) technique LORA, applying it on top of FO fine-tuning, MeZO, and HiZOO.\nEvaluation. For training and evaluation, we follow previous works. We study few-shot and many-shot settings on RoBERTa-large, randomly sampling k samples per class for training and validation, and 1000 samples for testing. For RoBERTa models, we evaluate k = 16 and k = 512. For OPT and LLaMA, we sample 1000, 500, and 1000 samples for training, validation, and testing. All experiments are conducted on NVIDIA A100 and A6000 GPUs."}, {"title": "5.2. Medium-sized masked language models", "content": "We conduct experiments on RoBERTa-large across three types of datasets and compare DiZO with two ZO baselines. We also explore PEFT by integrating LoRA. Table 3 presents the results, while Figure 2 shows the trajectory of training loss curves, indicating the convergence speed of DiZO and MeZO. Our key findings are as follows:\nDiZO greatly increases the convergence speed over MeZO. By using divergence-driven layer adaptation, the loss curve of DiZO decreases much faster, cutting the required iterations by over 50% on SST-2, MNLI, and RTE. In addition, DiZO improves accuracy by 1.7%, 3.6%, and 8.5% on these three datasets, respectively.\nDIZO outperforms MeZO and achieves results on par with full fine-tuning. From Table 3, DiZO consistently surpasses MeZO on all six datasets. Notably, on SST-2 and RTE datasets, DiZO even shows better performance than FO full-parameter fine-tuning, increasing by 0.3% and 1.5%, respectively.\nDIZO is effective for both full-parameter fine-tuning and PEFT. Although DiZO applies projections based on the distance with the pre-trained model, while such prior knowledge does not exist for the decomposed weights of LORA, it still delivers some gains."}, {"title": "5.3. Large autoregressive language models", "content": "To assess the broader applicability of DiZO, we run experiments on the OPT and Llama series autoregressive LLMs covering both classification and generation tasks. The overall results are summarized in Table 4, Table 5, and Figure 3 for OPT-2.7B, OPT-6.7B, and Llama series, respectively. We also compare the convergence speeds of DiZO and MeZO on OPT-2.7B across multiple datasets in Figure 4. Below, we highlight the key observations from these experiments.\nDiZO dramatically reduces the training GPU hours compared with the representative baseline MeZO. By incorporating divergence-driven layer adaptation, DiZO quickly establishes meaningful divergence across layers, whereas MeZO requires many more iterations to achieve the desired layer-wise divergence. As shown in Table 4, DiZO converges with far fewer iterations across nine datasets, resulting in up to a 48% reduction in training GPU hours. Moreover, unlike HiZOO, which reduces the number of iterations needed but slows the throughput of MeZO by more than 1.5x due to Hessian estimation, DiZO keeps its throughput nearly on par with MeZO. This efficiency is achieved because the additional projection learning procedure needs only two forward passes and is performed intermittently.\nDiZO outperforms baselines in both standard and parameter-efficient settings. From Table 4, DiZO surpasses MeZO and HiZOO with or without the LoRA, achieving results comparable to FO methods. Across seven classi-"}, {"title": "5.4. Memory and Speed Analysis", "content": "In this section, we examine the memory utilization and convergence speed of DiZO in comparison with both ZO baselines and FO fine-tuning approaches (with and without LoRA). Table 6 presents the results of fine-tuning OPT-2.7B on the RTE dataset, more results are shown in Appendix D.1.\nFrom the memory perspective, DiZO maintains the advantage of avoiding backpropagation, getting rid of the storage of memory-intensive data, and reducing memory usage by about 90% compared to FO fine-tuning. As explained in Section 4, the additional memory requirement of DiZO stems from storing a portion of the pre-trained weights, including the weight of the Query and Value, amounting to only 16.7% of the total parameters. In contrast, HiZOO needs to store Hessian information for all layers, with memory usage proportional to the size of the parameters.\nFrom the perspective of convergence speed, DiZO greatly reduces the required iterations while maintaining throughput"}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel layer-wise divergence analysis to reveal the distinct update pattern between FO and ZO methods. Building on these insights, we present DiZO, an enhanced ZO method using divergence-driven layer adaptation to resemble the learning capacity of the FO method. DiZO achieves significant training acceleration and superior performance across diverse tasks and architectures. Moreover, our method can be seamlessly integrated with PEFT techniques like LoRA for additional speedup.\nFor future work, we plan to explore DiZO in other fields, particularly for fine-tuning large pre-trained vision models."}, {"title": "7. Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Related Work", "content": ""}, {"title": "A.1. Fine-tuning of Pre-trained Models", "content": "Fine-tuning a pre-trained model offers a powerful way to reuse learned representations and reduce training costs compared to building models from scratch, often achieving superior performance. Initially successful in NLP with models like BERT, ROBERTa, and GPT, fine-tuning has also shown promise in vision tasks such as CLIP and SWAG. Recent parameter-efficient fine-tuning (PEFT), including LoRA, and prefix tuning further minimize resource needs by updating only a small subset of parameters, preserving most of the pre-trained weights and ensuring valuable knowledge is retained."}, {"title": "A.2. Zeroth-order Optimization and Acceleration", "content": "ZO optimization emerges as an attractive technique that optimizes the model without backpropagation. Unlike most frequently used FO optimization which directly obtains and leverages the gradient for optimization, the zeroth-order method utilizes objective function value oracle only, estimating the gradient by finite differences. ZO method has a wide range of applications in machine learning fields, including adversarial attack and defense , machine learning explainability, reinforcement learning, and on-chip training. Recently, the ZO method has been proposed to be leveraged on LLM fine-tuning to address the significant memory usage. Malladi et al. (2023) proposed MeZO, first scaling ZO optimization to fine-tuning parameter-intensive LLMs, greatly reducing memory utilization. On top of MeZO, Zhao et al. (2024) proposed HiZOO, leveraging the estimated Hessian information for better learning capacity, but reducing the throughput of MeZO to some extent.\nZO optimization, although it significantly saves memory, converges more slowly than FO methods due to higher variance from random search. Liu et al. (2018) introduced ZO-SVRG by incorporating variance reduction techniques . Shu et al. (2023) proposed using a Gaussian process to model objective function queries, thereby reducing query complexity and allowing more frequent queries to lower gradient variance. Sener & Koltun (2020) performed random search on a learned low-dimensional manifold, reducing the number of needed objective queries. However, existing ZO accelerators face two main challenges when adapting to ZO fine-tuning for LLMs. First, these approaches were typically designed for smaller-scale tasks involving fewer parameters and less data, and cannot be directly extended to large-scale LLMs. Second, many prior methods focus on improving query efficiency, whereas recent work has shown that a single query can suffice for LLM fine-tuning. How to effectively accelerate ZO optimization on large model fine-tuning remains a problem."}, {"title": "B. Experiment Settings and Analysis", "content": ""}, {"title": "B.1. Datasets and Evaluation", "content": "For the ROBERTa-large model, we use the following classification datasets: SST-2, SST-5, SNLI, TREC, MNLI, and RTE. Following previous studies, we cap the test set size at 1000 samples. Two training settings are considered: k = 16 and k = 512, where we randomly select 16 or 512 samples per class for both training and validation.\nFor the OPT and Llama series models, we use the SuperGLUE benchmark which includes RTE CB, BoolQ , WIC , WSC, and MultiRC. We also include SST-2 and two question answering datasets, SQUAD and DROP. For each of these datasets, we randomly sample 1000 instances for training, 500 for validation, and 1000 for testing."}, {"title": "B.2. Implementation of Baselines", "content": "Memory-efficient ZO (MeZO) MEZO serves as a fundamental baseline for fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization. By resampling perturbations with a fixed random seed, MeZO"}, {"title": "B.3. Hyperparameter Setting", "content": "We use the hyperparameters in Table 7 for experiments on RoBERTa-large, OPT-series, and Llama-series models. Specifically, the choice of clip range did not significantly impact the performance. The selection of the projection update cycle and scalar for projection affects the performance somewhat. Generally, for datasets that need larger iterations for convergence, or for these harder datasets, DiZO prefers a larger update cycle, while for those less complicated datasets, DiZO benefits from a smaller update cycle."}, {"title": "C. Closer look at DiZO", "content": ""}, {"title": "C.1. Ablation for Projection Layers Selection", "content": "Instead of applying projections to all layers, which would require storing the entire pre-trained model, we focus only on projecting the weights of the Query and Value in the attention modules. As shown in Table 8, this strategy achieves the best trade-off between the overall performance and extra storage requirements, does not reduce the performance and only 16.7% of the parameters of the pre-trained model are needed to store. A Similar strategy has also been adopted in LoRA."}, {"title": "C.2. Ablation for Strategies in ZO Projection Learning", "content": "As discussed in Section 3.3, we introduce two strategies, re-initialization (Re-init) and projection clipping (Clipping), to enhance projection learning and improve the stability of fine-tuning. The ablation results for these strategies, along with the"}, {"title": "C.3. Does Other Alternative Strategies for Layer-wise Divergence Work?", "content": "As discussed in Section 2.2, our objective is to enhance layer-wise divergence in ZO optimization. Naturally, with consideration of this objective, one may raise two questions regarding the projection strategy we adopt: 1) Can we perform layer-wise projections on the learning rate? 2) When updating weight by projection at t-th iteration, why do we use the weights of pre-trained model \u03b80 as the base of the update (shown in Eq.5) instead of the weights from the (t - 1)-th iteration, \u03b8t\u22121?"}, {"title": "D. More Experiment Results", "content": ""}, {"title": "D.1. Memory and Speed Analysis", "content": "We present the memory and speed results for OPT-2.7B on the SST-2 and SQUAD datasets in Table 10 and Table 11, respectively. DiZO significantly reduces the number of required iterations while maintaining throughput comparable to MeZO, leading to substantially fewer training GPU hours. In contrast, HiZOO achieves only modest iteration savings and further reduces the throughput of MeZO by approximately 1.5\u00d7 due to its reliance on second-order information estimation."}, {"title": "D.2. Llama Experiments", "content": "To demonstrate the broader applicability of DiZO, we conducted experiments on the Llama-series models. The results for Llama3-3B and Llama3-8B are presented in Table 12 and Table 13, respectively. DiZO consistently outperforms MeZO across both the 3B and 8B Llama models.\nHowever, we observed that ZO LORA performs poorly with Llama models (including DiZO, MeZO and HiZOO). The loss value remains stagnant, and the resulting accuracy is comparable to or even worse than zero-shot results. We leave it to future work to investigate why ZO LORA fails with Llama models. We suspect that this limitation may be related to the Group Query Attention (GQA) mechanism employed in Llama3."}, {"title": "E. Proof", "content": "We consider a neural network with L layers (or parameter blocks) and wish to estimate the gradient of some loss function L(\u03b8; B) with respect to all parameters \u03b8. We use a two-point finite-difference (zero-order) method with directions drawn from an isotropic distribution. We show below why the expected norm-squared of the resulting gradient estimator is identical (or follows the same dimension-based law) for each layer/block.\nConsider the l-th layer. Its estimator is\n$\\nabla_{\\theta^{(l)}} L=\\frac{1}{q} \\sum_{i=1}^{q} \\frac{L(\\theta+\\epsilon u_{i})-L(\\theta-\\epsilon u_{i})}{2 \\epsilon} u_{i}^{(l)}$,\nwhere \u2206i is the same scalar for all blocks. We want\n$E[\\|\\nabla_{\\theta^{(l)}} L\\|^{2}].$\nNote that:\n1. Ai does not depend on l; it is a single scalar for each direction i.\n2. u(l)i is the sub-vector of ui associated to the l-th block.\n3. uis drawn from an isotropic distribution in Rd, meaning each coordinate has zero mean, unit variance, and there is no cross-correlation between different coordinates. Thus, different blocks ul(l) and u(lm) (for l \u2260 m) are uncorrelated, and each block uuhas an identity covariance in its own subspace Rde .\nHence, when we expand\n$\\left\\|\\nabla_{\\theta^{(l)}} L\\right\\|^{2}=\\frac{1}{q^{2}} \\sum_{i=1}^{q} \\Delta_{i}^{(l)}$\nthe expectation w.r.t. {ui} depends on l only through the dimension de, not through any other distributional asymmetry. If de are the same for all l, then the second moment is literally the same across all blocks. If de differ, the dependence is only a (known) function of de.\nIn short, isotropy ensures that\n$E[\\|\\nabla_{\\theta^{(l)}} L\\|^{2}]$ is the same functional form of $\\|\\nabla_{\\theta^{(l)}} L\\|^{2}$ for each layer l.\nTherefore, in the simplest scenario where de are all the same, each layer gets the same second-moment behavior for its gradient estimator."}, {"title": "F. Implementation", "content": "The following is an implementation of our \u201cZO projection learning\" in PyTorch.\n\n\n    def ZO Projection Learning(theta_t", "x)": "n        \"\"\"\n        Perform Zeroth-order Projection Learning.\n        Args:\n            theta_t: Current model parameters to be fine-tuned.\n            theta_0: Pre-trained model parameters (anchor).\n            Gammas: Projection parameters need to be optimized.\n            delta: Smoothing parameter.\n            eta: Learning rate for projection gradient descent.\n            tau: Clipping factor for projection bounds.\n            x: Input data for the forward pass.\n        \"\"\"\n        # Calculate the L2 norm of the distance gap\n        norms = {"}, "n        name: torch.norm(param.data - anchor.data)\n        for (name, param), anchor in zip(theta_t.named_parameters(), theta_0.parameters())\n\n        # Initialize the projection values\n        for name, gamma in Gammas.named_parameters():\n            gamma.data = norms[name"], "range(max_iters)": "n            # Step 1: Perturb and apply projection", "2": "Reverse and apply projection", "3": "Reset projection and compute gradient\n            Gammas = PerturbGamma(Gammas, delta) # Reset projection\n            grad = (loss1 -"}