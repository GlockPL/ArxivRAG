{"title": "MERGE\u00b3: Efficient Evolutionary Merging on Consumer-grade GPUs", "authors": ["Tommaso Mencattini", "Adrian Robert Minut", "Donato Crisostomi", "Andrea Santilli", "Emanuele Rodol\u00e0"], "abstract": "Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE\u00b3, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50\u00d7 while preserving performance. MERGE\u00b3 achieves this by Extracting a reduced dataset for evaluation, Estimating model abilities using Item Response Theory (IRT), and Evolving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library\u00b9, democratizing high-quality model merging.", "sections": [{"title": "1. Introduction", "content": "Model merging has become a powerful and accessible approach for developing new state-of-the-art models without the need for cluster-grade computing typically required for large model training (Yang et al., 2024a). Its key advantage lies in performing the merging process post-hoc directly in the parameters of pre-existing models (endpoint models), eliminating the need for training and significantly reducing the demand for expensive computational resources.\nThis approach has significantly broadened access to the field, with ML practitioners producing competitive models out of existing ones on standard consumer GPUs\u00b2(Ilharco et al., 2022). However, although computationally inexpensive, most of the existing approaches are quite rudimentary, require ad-hoc choices, and are usually based on ungrounded trial-and-error strategies for selecting the merge coefficients, which ultimately limits their downstream performance (Yadav et al., 2023; Yu et al., 2024). On the other hand, recent work has shown that evolutionary merging can produce models of unprecedented quality by automating the hyperparameter search for merging coefficients (Akiba et al., 2025). While this technique can incorporate any standard merging method, such models are absent from public leaderboards likely due to a mismatch between the high computational demands of evolutionary merging and single-GPU setups typical of merging practitioners. Indeed this computational cost is significantly high: computing the fitness function requires generating and evaluating answers for each dataset element, for each candidate in every evolutionary step. As shown in Figure 1, the fitness computation alone in the 1,000-trial evolutionary merge from Akiba et al. (2025) requires approximately 4 \u00d7 106 TFLOPs, with the full algorithm demanding largely over a month of continuous computation if run on a single NVIDIA 4090 (\u00a7C.3.2). This renders evolutionary merging effectively out of reach for consumer hardware, risking to exclude the very user base it was meant to empower.\nIn this paper, we address this challenge by introducing MERGE\u00b3, an evolutionary merging framework that runs on a single consumer GPU with competitive results (see"}, {"title": "2. Related Work", "content": "Model Merging has emerged as an efficient alternative to ensembling by integrating existing models without any additional training. One set of methods identifies neuron permutations that align the models into a shared optimization basin, allowing them to be merged through straightforward averaging (Ainsworth et al., 2022; Jordan et al., 2023; Stoica et al.; Pe\u00f1a et al., 2023; Crisostomi et al., 2025). Closer to our work, multi-task model merging focuses on the case where a single pre-trained model is fine-tuned for different tasks (Ilharco et al., 2022; Yadav et al., 2023; Yu et al., 2024; Matena & Raffel; Wortsman et al., 2022; Davari & Belilovsky, 2025; Wang et al., 2024; Zhou et al., 2024; Gargiulo et al., 2025). In this direction, several works address task interference by pruning or selectively combining parameters\u2014e.g., TIES-merging (Yadav et al., 2023), Model Breadcrumbs (Davari & Belilovsky, 2025), and DARE Merging (Yu et al., 2024)\u2014or by optimizing merge coefficients (Yang et al.), introducing task-specific modules (Yang et al., 2024b), and disentangling weights (Ortiz-Jimenez et al., 2024).\nEvolutionary Algorithms. Evolutionary Algorithms are black-box optimization algorithms operating on a population of potential solutions by evolving them through generations with operators such as selection, mutation, recombination, and crossover (B\u00e4ck & Schwefel, 1993; P\u00e9trowski & Ben-Hamida, 2017; Dasgupta & Michalewicz, 1997). Recent applications include neural architecture"}, {"title": "3. MERGE\u00b3", "content": "Our method MERGE\u00b3 speeds up evolutionary model merging by reducing the computational cost of fitness evaluation. It achieves this by shrinking the fitness evaluation dataset and using IRT-based performance estimators to maintain full-dataset accuracy from subset evaluations. Figure 2 shows an overview of our method (algorithm 1)."}, {"title": "3.1. Extract & Estimate", "content": "Evaluating the fitness function involves generating and assessing answers for each data sample, repeated across all models in the population at every evolutionary step. Given the computational demands of evolutionary algorithms and LLMs, this process is highly intensive. To mitigate this, we reduce the dataset D to a smaller subset D \u2282 D with |D| \u00ab |D|. After exploring various subsampling strategies, we found uniform random sampling as effective as more complex methods (see appendix C.1) and adopted it for simplicity. Since dataset reduction is not our main focus, we leave further optimizations for future work.\nReducing the dataset speeds up evaluation but does not guarantee identical results \u2013 particularly when the subset is significantly smaller, as in our case. To bridge this gap, we build an IRT-based estimator that adjusts for this discrepancy, effectively estimating performance to reflect fulldataset results (Lord et al., 1968; Polo et al., 2024).\nIRT model. We first define an estimator to assess each endpoint model's inherent abilities, derived from the latents of a Bayesian network. This ensures that merging preserves individual model strengths. In the Evolve step (\u00a73.2), the estimated latent abilities are fed to a performance estimator to compute the final fitness.\nTo estimate LLM abilities, we build on Polo et al. (2024), who applied IRT to evaluate LLM performance; however, while they used IRT for benchmarking, we extend it to estimate inherent abilities relevant for model merging, and explicitly use them to guide merging in the Evolve step.\nIn IRT, latent variables (\u03b3) represent a model's underlying abilities, while manifest variables (Y) indicate response correctness. The framework models the probability of a correct response based on model abilities and item characteristics (e.g., difficulty).\nIRT defines this probability as:\n$P(Y_{im} = 1 | \\gamma_m, a_i, \\beta_i) = \\frac{1}{1+ exp(-a_i \\gamma_m + \\beta_i)}$ (1)\nHere, \u03b3m \u2208 Rd represents model m's latent abilities, ai \u2208 Rd defines the ability dimensions needed to answer example i, and \u03b2i denotes its difficulty. A model is more likely to answer correctly when its abilities (\u03b3m) align with the example's required traits (ai) and less likely when the difficulty (\u03b2i) is higher. Yim is a binary variable indicating whether model m correctly predicts example i (1 if correct, 0 otherwise).\nCrucially, this approach estimates a model's likelihood of answering correctly without directly analyzing the example's content, relying solely on the estimated IRT parameters (\u03b3m, ai, \u03b2i).\nFitting. We use variational inference to efficiently estimate both example-specific (ai, Bi) and model-specific (\u03b3m) parameters within a hierarchical Bayesian model (Lalor & Rodriguez, 2023), initialized as detailed in appendix B.1. Following Polo et al. (2024), we estimate ai"}, {"title": "3.2. Evolve: Performance Estimator", "content": "The performance estimator, a key part of the Evolve step, efficiently approximates the fitness function, which measures the merged model's accuracy. Since fitness evaluation runs repeatedly during evolution (once per model per iteration), reducing its computational cost is crucial. Instead of evaluating the full dataset, the estimator predicts performance using only the endpoint models' abilities and the reduced dataset from previous steps, significantly accelerating the process.\nWe introduce two novel performance estimators for merging: MP-IRT and GMP-IRT. Since model merging linearly combines weights, we assume the merged model's ability is also a linear combination of the endpoint abilities. This makes our approach far more efficient, estimating only the interpolation coefficients (\u03bb\u2081) instead of recomputing the full ability vector y of the merged model from scratch (as done in P-IRT and GP-IRT (Polo et al., 2024)).\nAssumption 1 (Linear Combination of Latent Abilities). Let {mo,m1,...,mn} be endpoint models with latent ability vectors Yi. If a new model m is formed as a linear combination of their parameters, its ability vector Ym can be expressed as:\n$Y_m = \\sum_{i=1}^{n} \\lambda_i \\gamma_i = [\\gamma_1,..., \\gamma_n] \\lambda$ (2)\nwhere x = (1, ..., An) are the interpolation coefficients.\nThis assumption allows us to compute the multidimensional IRT model (Eq. 1) for model merging as a linear combination of the individual models' abilities:\n$P_{im} = P(Y_{im} = 1 | \\lambda_1 \\gamma_1 + \\lambda_2 \\gamma_2, a_i, \\beta_i) = \\frac{1}{1 + exp(-a_i (\\lambda_1 \\gamma_1 + \\lambda_2 \\gamma_2) + \\beta_i)}$ (3)\nSince the endpoint models' latent abilities Yi were pre-estimated over the full dataset D in the Estimate step, we only need the subset D to estimate the interpolation coefficients i via MLE.\nPerformance Estimators. To estimate the accuracy of the merged model m using only the reduced dataset D and Pim, we define the merged performance-IRT (MP-IRT) estimator as:\n$\\hat{Z}_{m}^{mp-IRT} = \\frac{\\tau}{\\vert D \\vert} \\sum_{i \\in D} Y_{im} + \\frac{1-\\tau}{\\vert D \\backslash D \\vert} \\sum_{i \\in D \\backslash D} \\hat{P}_{im}$ (4)\nwhere \u03c4 = |D|/|D| downweights smaller subsets that may be noisier. In practice, we are considering the observed correctness for the data points we have access to, while pim predictions are used for the rest, enabling accurate performance estimation across all examples despite evaluating only a subset, where pim = P(Yim = 1 | 1171 + 1212, di, \u03b2i) is the distribution defined by plugging into eq. (3) the parameter found via MLE.\nAlthough designed for model merging, 2mp-IRT inherits certain limitations of P-IRT (Polo et al., 2024), such as non-uniform weighting and imperfect IRT fits. To mitigate these, we define a generalized estimator that interpolates between 2mp-IRT and the observed correctness on D:\n$\\hat{Z}_{m}^{gmp-IRT} = c \\sum_{i \\in D} w_i Y_{im} + (1 - c) \\hat{Z}_{m}^{mp-IRT}$ (5)\nwhere c is a heuristic scalar chosen as in Polo et al. (2024) and wi are uniform per-sample weights.\nAlthough model merging can sometimes degrade performance due to weight interference\u2014suggesting non-linear ability interactions- our assumption is empirically supported as we are interested only in evolved models that show a positive performance gain. As validated in our experiments (\u00a74.1), our custom estimators, designed around this assumption, outperform standard IRT estimators."}, {"title": "3.3. Evolve: Evolutionary Search", "content": "The final step of our algorithm frames model merging as a multi-objective optimization problem. Each merging objective F(m, D\u2081) represents the performance of the merged model m on task i. In practice, we select a multi-objective evolutionary algorithm (e.g., NSGA-II (Deb et al., 2002)) and a merging strategy (e.g., TIES (Yadav et al., 2023)), aiming to optimize the corresponding Pareto front, formally defined as:\n$PF(0) = {\\theta \\epsilon \\Theta : \\nexists \\theta_j \\epsilon \\Theta s.t. \\theta_j \\succ \\theta_i}$"}, {"title": "4. Experiments", "content": "In this section, we evaluate MERGE\u00b3, demonstrating its effectiveness in evolutionary model merging on consumer-grade GPUs. We first validate the proposed ability and performance estimators, assessing their accuracy in approximating full-dataset evaluations. Next, we examine crosslingual transfer, where MERGE\u00b3 enables efficient merging of multilingual models, improving mathematical reasoning across languages. Finally, we evaluate its ability to synthesize multilingual models, surpassing individual fine-tuned baselines while remaining computationally efficient. All the merging experiments were performed with our custommade library Mergenetic (see Appendix A)."}, {"title": "4.1. Validating Estimators", "content": "In this section, we empirically validate our mergedperformance estimators by comparing them against standard P-IRT and GP-IRT estimators (Polo et al., 2024) across five benchmark datasets: GSM8K (Cobbe et al., 2021), Winogrande (Sakaguchi et al., 2021), TruthfulQA (Lin et al., 2022), Hellaswag (Zellers et al., 2019), and ARC (Clark et al., 2018). Due to space limitations, additional results are provided in Appendix C.\nAbility Estimators. To validate our ability estimators we compare their inferred latent ability vectors to the reference \"ground-truth\" vectors \u0393. Specifically, we measure the cosine similarity and the Euclidean distance from the ground-truth \u0393 both for y{mp,gmp}-IRT, estimated with our merged-performance IRT approaches, and y{P,gp}-IRT, estimated with the P-IRT and GP-IRT estimators (Polo et al., 2024). Here, \u0413m is computed by fitting the IRT model (as in section 3.1) to each merged model m using its entire set of responses on the full dataset D. Incorporating all available data, Im serves as our best proxy for the model's true ability. Conversely, both y{mp,gmp}-IRT and y{P,gp}-IRT are estimated using only a smaller subset D \u2282 D of size n."}, {"title": "4.2. Cross-Lingual Transfer of Mathematical Skills", "content": "To assess the transfer of mathematical reasoning from English to other languages, we merge an English mathspecialized model with a Mistral-7B (Jiang et al., 2023) fine-tuned on each target language, then evaluate on the corresponding GSM8K translations (Cobbe et al., 2021). Appendix B.3 provides details on the specific models used for merging. Following Akiba et al. (2025), we label an answer correct only if it is both accurate and written in the target language. We benchmark our approach against three commonly used merging baselines Task Arithmetic (Ilharco et al., 2022), TIES (Yadav et al., 2023) and DARE (Yu et al., 2024). Following standard practice in the merging community, we apply either TIES and DARE jointly or SLERP (Shoemake, 1985).\nAs shown in figure 5, merging a language-specific finetuning with a math-specialized model consistently surpasses both endpoint models by 10-20% in accuracy on the translated GSM8K. In contrast, standard baselines often yield sub-optimal merges, performing worse than the endpoints themselves. This highlights the importance of optimized merging coefficients and motivates our evolutionary framework. To rule out the possibility that gains arise merely from exploiting a small, in-distribution"}, {"title": "4.3. Evolving a Multilingual model", "content": "We next combine individually fine-tuned models for {IT, EN, DE, NL} into a single multilingual model. Appendix B.3 provides details on the specific models used for each language. As shown in table 1, the resulting merged model surpasses each language-specific variant by up to 19% in accuracy on the ARC-Challenge dataset (Clark et al., 2018). Even more notably, it outperforms all its constituent endpoints, demonstrating a clear positive transfer of knowledge across languages. Beyond the clear accuracy boosts in each language, a few key insights stand out. First, the largest improvement occurs for Dutch (from 50% to 69%), suggesting that merging particularly benefits languages where the baseline performance is lower. Second, even English, which starts from the highest baseline, still gains by 4%, indicating that positive transfer is not limited to low-resource or weaker endpoints. Finally, the fact that the merged model outperforms all individual finetunings (rather than landing between them) points to a genuine cross-lingual synergy, wherein knowledge from each language-specific model collectively strengthens the multilingual result."}, {"title": "5. Theoretical Analysis", "content": "In this section, we provide theoretical guarantees for our performance estimator, demonstrating that their estimated accuracy is a reliable approximation of full-dataset accuracy. We provide formal guarantees for its performance, analyze its stability under dataset reduction, and explain why it remains a robust proxy for the true fitness of the merged models. This analysis not only solidifies the estimator's theoretical foundation but also offers practical insights into its behavior in finite-data and asymptotic regimes.\nThe section is structured as follows: first (\u00a75.1), we derive a correlation between the accuracy of the performance estimator and the quality of the minimum found by solving an optimization problem using that performance estimator as objective function; second (\u00a75.2), we study the asymptotic properties of the performance estimator as the dataset size approaches infinity, formalizing it as an unbiased estimator; and finally (\u00a75.3), we demonstrate that our performance estimator behaves in expectation within a e-bound of the accuracy on the true optimum dataset. The proofs for all the theorems and propositions presented below are outlined in appendix D."}, {"title": "5.1. Part I: e-Stable Estimators and e-Optimality Preservation", "content": "We first consider a performance metric F(\u03b8; D) for \u03b8\u2208 CR"}, {"content": ", where D is a dataset. If we choose a smaller subset D \u2282 D to approximate this metric, denoted F(0; D), we wish to control the loss in optimality incurred by replacing F(0; D) with F(0; D).\nDefinition 1 (e-Stability.). Given two datasets D and D, we say F(\u00b7; D) is e-stable with respect to F(\u00b7; D) if, for all \u03b8\u0395\u0398,\n$|F(\\theta; D) \u2013 F(\\theta; \\hat{D})| \\leq \\epsilon$\nUnder this condition, minimizing F(\u00b7; D) yields an objective value within e of minimizing F(\u00b7; D). Formally:\nTheorem 2 (e-Optimality Preservation). Let D be a dataset, let D \u2282 D be a subset, and let F(\u00b7; D) be e-stable with respect to F(\u00b7; D), with a fixed \u0454 > 0. Define\n$\\theta^* = argmin_{ \\theta \\in \\Theta} F(\\theta; D)$ and $\\hat{\\theta} = argmin_{\\theta \\in \\Theta} F(\\theta; \\hat{D})$\nThen\n$|F(\\theta^*; D) \u2013 F(\\hat{\\theta}; D)| \\leq \\epsilon$\nThus, e-stability ensures that any global minimizer on D achieves an objective value on D no worse than e from the true global optimum. Nevertheless, uniformly bounding |F(0; D) \u2013 F(0; D)| for all 6 may be too strong in practice. For this reason, we introduce:"}, {"title": "5.2. Part II: Theoretical Guarantees for MP-IRT", "content": "We now apply these ideas to our proposed MP-IRT estimator (cf. \u00a73.1). We first show that MP-IRT is asymptotically unbiased, and then combine this fact with Theorem 4 to argue that MP-IRT-based minimizers remain close to those that minimize the full-dataset performance measure.\nAsymptotic unbiasedness. The following proposition establishes that, as D grows, \u017dmp-IRT converges in probability to the true performance Z. Its proof relies on classical limit arguments for unbiased estimators.\nProposition 5 (Asymptotic unbiasedness of MP-IRT). Assume: (i) ^ \u2192 \u5165 in probability as |\u00ce| \u2192 \u221e, (ii) for each i\u2208 I, the true values \u03b1\u03af, \u03b2\u03af, 01, 02 are known, with SupieI ||ai||2 \u2264 c for a fixed c, (iii) linear inheritance of abilities (cf. Assumption 1) holds. Then, for all j, l,\n$E[\\hat{Z}_{jl} | Y_{iol},..., Y_{ikl}] - E[Z_{jl} | Y_{iol},..., Y_{ial}] \\rightarrow 0$\nin probability as |\u00ce| \u2192 \u221e. Thus, for sufficiently large subsets D, the discrepancy between \u017dm and Zm can be made arbitrarily small with high probability."}, {"title": "5.3. Part III: performance preservation via MP-IRT", "content": "We now conclude that MP-IRT preserves near-optimality when we train on a suitably large D \u2282 D. Since Proposition 5 asserts that 2 approximates Z well for large |D|, it follows (under mild conditions) that MP-IRT remains e-stable in expectation. Hence, Theorem 4 shows that minimizing 2 on D yields, on average, a solution within e of the full-dataset optimum."}, {"title": "6. Conclusions", "content": "We introduced MERGE\u00b3, an evolutionary merging framework that makes high-quality model merging feasible on a single consumer GPU. By combining a subset-based approach with IRT-driven performance estimation, MERGE\u00b3 reduces merging costs by up to fifty-fold compared to prior methods without sacrificing the quality of the merged model. Our experiments demonstrate successful cross-lingual transfer in mathematics (e.g., from English to Japanese), as well as the synthesis of new multilingual models that outperform each of their language-specific endpoints. Overall, MERGE\u00b3 expands the practical reach of evolutionary merging, allowing everyday practitioners to benefit from advanced multi-task and multilingual model compositions at a fraction of the usual computational cost."}, {"title": "Impact Statement", "content": "The introduction of MERGE\u00b3 provides an efficient and accessible method for evolutionary model merging on consumer-grade GPUs. By combining dataset reduction techniques and Item Response Theory (IRT)-based performance estimations, MERGE\u00b3 significantly lowers computational requirements while maintaining competitive performance. This enables researchers and developers to synthesize high-quality multilingual and cross-lingual models without requiring cluster-scale hardware.\nThe open-source release of MERGE\u00b3 aims to make evolutionary model merging widely accessible, fostering further innovation in resource-constrained environments. With applications in multilingual NLP and low-resource language modeling, MERGE\u00b3 addresses practical challenges in the field, offering an efficient solution for creating state-of-theart models on standard hardware."}, {"title": "A. Mergenetic", "content": "Each experiment was run using a library developed specifically for this paper, which will be released as open-source software, called Mergenetic. This library allows for defining a merging problem as either a single-objective or multi-objective optimization problem, where one only needs to specify the merging method,a fitness function, and select the hyperparameters for a chosen evolutionary algorithm.\nThe implementation relies on Mergekit (Goddard et al., 2024) for merging the models, Pymoo (Blank & Deb, 2020) for optimizing the objective function through evolutionary algorithms, and Lm-Evaluation-Harness (Gao et al., 2024) for implementing some of the fitness functions. In table 2 we outline the supported merging methods, while in table 3 we outline the currently available evolutionary algorithms.\nWe believe this library is a significant contribution as it facilitates evolutionary model merging and aligns well with the paper's approach, which aims to reduce computational burden. It can be a valuable tool for the community and for users interested in cross-lingual transfer or creating multilingual models for target low-resource languages."}, {"title": "B. Additional Details", "content": "This section provides additional implementation and experimental details that were not included in the main paper."}, {"title": "B.1. IRT Fitting Details", "content": "As previously stated, we used the implementation from Polo et al. (2024) and adopted their configuration settings. Specifically, we used Ym ~ \u039d(\u03bc1\u03b1, 1/uIa), Ai ~ \u039d(\u03bc\u03b11\u03b1, 1/4aId), and \u1e9ei ~ \u039d(\u03bc\u03b2,1/u\u03b2). Following Polo et al. (2024), we also applied (hyper)priors to the prior parameters using the software for fitting hierarchical Bayesian models (Lalor & Rodriguez, 2023): \u03bc\u03b7 ~ \u039d(0,10), \u03ba\u03b3 ~ \u0393(1,1), \u03bc\u03b1 ~ \u039d(0,10), \u03ba\u03b1 ~ \u0393(1,1), \u03bc\u03b2 ~ N(0,10), and up ~ \u0393(1,1). For both the model and example-specific parameters Ym, ai, and Bi, we take their point estimates as the means of their respective variational distributions. The y model dimensionality is set to 15 following the parameter choice suggested by Polo et al. (2024)."}, {"title": "B.2. MERGE\u00b3 Algorithm", "content": "Below we outline the pseudo-code for the end-to-end MERGE\u00b3 algorithm:"}, {"title": "B.3. Experimental Details", "content": "Models. One key assumption of model merging is that the endpoint models lie within the same basin (Ilharco et al., 2022). This means that merging arbitrary models is not feasible; rather, all models involved must be fine-tuned versions of the same base model. To satisfy this requirement, we selected several fine-tuned models from the Hugging Face Hub that originated from the same base model. Specifically, we focused on models fine-tuned starting from Mistral-7B (Jiang et al., 2023), following common best practices in the community (Akiba et al., 2025). Table 4 lists all the models used in our experiments, along with their corresponding names on the Hugging Face Hub. A total of 27 models were considered for our experiments.\nIn the rest of the section, we provide further details for reproducing the experiments in section 4.2 and section 4.3 of the main paper."}, {"title": "B.3.1. CROSS-LINGUAL TRANSFER", "content": "In the cross-lingual transfer evolutionary merging, we evolved four merged models with mathematical capabilities in different languages: Japanese, Romanian, German, and Dutch. In each of these experiments, we deployed an ad-hoc genetic algorithm for single-objective optimization. We employed the Simulated Binary Crossover (Deb et al., 2007) operator to generate offspring solutions by combining parent solutions. To maintain diversity and explore the search space, we applied Polynomial Mutation (Deb et al., 2007), which introduces small perturbations to offspring solutions and enhances the algorithm's ability to escape local optima. This combination of SBX and PM effectively balances exploration and exploitation, facilitating efficient convergence toward optimal solutions.\nFurthermore, guided by empirical tests, we decided to deploy SLERP to evolve solutions for the Romanian and Dutch problems, while we used a combination of TIES and DARE for the Japanese and the German ones. We deployed four different sizes of the fitness datasets for Japanese, namely 20, 30, 50, and 100, in order to obtain a more detailed analysis of the method for comparison with the work of (Akiba et al., 2025). On the other hand, we kept the fitness dataset size fixed to 20 for all other aforementioned experiments. The fitness dataset was extracted from the test set of GSM8K, and we used the remaining, nonoverlapping samples as test set for evaluating the model."}, {"title": "B.3.2. MULTI-LINGUAL TRANSFER", "content": "In this experiment, we tackle the ARC dataset in multiple languages (Italian, Dutch, German, and English)\u00b3 (Thellmann et al., 2024) using a multi-objective evolutionary merging procedure based this time on NSGA-II (Deb et al., 2002). We configure the population size to 25 and the number of evolutionary iterations to 7. We deployed a combination of TIES and DARE as merging strategy. As in previous settings, both the fitness function and the test metrics operate by extracting the final model-generated choice via a regex, but this time they look for an instance from the set {A, B, C, D} rather than a number. On top of this, we employed a dataset composed by 20 datapoints for each language from the relative translation of ARC to compute the fitness, and we extracted the test set as for the previous experiments. Furthermore, unlike the single-objective approach described earlier, here we explicitly optimize multiple objectives simultaneously. This time, the employed models are Mistral-Ita-7b, GEITje-7B-ultra, leo-mistral-hessianai-7b, and the base model Mistral-7B-v0.1."}, {"title": "B.3.3. ABILITY AND PERFORMANCE ESTIMATOR", "content": "In these experiments (reported in section 4.1 and section 4.1) we used the test set of the standard version of GSM8K, HellaSwag, ARC, Winogrande, and TruthfulQA. Furthermore, we used 6 different models to test the different performance of the ability and performance estimator: SOLAR-tail-10.7B-Merge-v1.0, FuseChat-7B-Slerp, NeuralPipe-7B-slerp, T3Q-Merge-Mistral7B, FuseChat-7B-TA, and supermario-slerp-v3. These models were chosen as already available on the Open LLM Leaderboard."}, {"title": "C. Additional Experiments", "content": "We report here additional experiments and analyses."}, {"title": "C.1. Extract Step", "content": "In the extract step outlined in section 3.1, random sampling has been proposed as the main method to subsample the dataset D \u2282 D. While we explored various dataset subsampling strategies, we ultimately opted for uniform random sampling, as our experiments showed that more complex approaches offered no significant advantage over this simpler method. In this section we report some of the experiments behind this decision and the two alternative"}, {"title": "C.1.1. IRT CLUSTERING", "content": "Given a dataset D and the parameter of a fitted IRT model a and \u1e9e, one can define a low-dimensional embedding of each datapoint i \u2208 D by E\u2081 = [ai||Bi]. Therefore, IRTclustering obtains a representative subset by first obtaining a clustering over this embedding space through K-Means, and then choosing the points closest to the centroids as representative samples."}, {"title": "C.1.2. REPRESENTATION CLUSTERING", "content": "Let {m}}=1 be the set of endpoint models, and let D = {x}1 be our full dataset. For each sample xi, we first encode it into a high-dimensional vector by concatenating model-specific embeddings. Concretely, we compute:\n$E_{ij} = \\frac{1}{T_i} \\sum_{t=1}^{T} E_{i,j,t} \\in \\mathbb{R}^d,$\nwhere Ei,j,t is the embedding of the t-th token of sample xi under model mj, and Ti is the number of tokens in xi. We form the concatenated representation:\n$E_i = [E_{i,1}||E_{i,2}||\u00b7\u00b7 || E_{i,M}] \\in \\mathbb{R}^{M\\cdot d}.$\nSince Ei can be very high-dimensional, we apply Principal Component Analysis (PCA) to project E\u00bf onto a lower-"}, {"title": "C.1.3. EXPERIMENTS", "content": "To compare the performance of the Sample Extractors, we followed a procedure similar to that described in section 4.1, computing the absolute estimation error for each extractor. For random sampling, the accuracy estimator was obtained via uniform averaging, whereas for IRT and RC it was obtained via weighted averaging. We evaluated the estimator in two different settings: (1) merging a math model with a language-tuned model (similar to the crosslingual setting of section 4.2) for several languages (Italian, German, Romanian, Dutch) and testing the extractor on the corresponding translations of GSM8K (see fig. 7), and (2) merging several math models and testing the extractor on the English version of GSM8K (see fig. 8).\nFocusing on fig. 7, we see that performance variability is somewhat higher (larger error bars) due to different language-specific datasets. Even so, Random sampling never falls behind IRT or RC, especially for small sample sizes. By the time the subset size reaches 50 or more examples, all three methods converge to comparable accuracyerror levels, underscoring the robustness of Random sampling. Instead, in fig. 8, the trends are broadly similar for"}, {"title": "C.2. Estimation step", "content": "C.2.1. ADDITIONAL EXPERIMENT FOR ABILITY ESTIMATOR\nWe report in fig. 9 the Euclidean distance between the estimated and ground-truth ability vectors across different sample sizes. The results are consistent with the case n = 10, 20 seen in fig. 4, with our estimated ability vector being significantly closer to the ground-truth one compared to the ability vector estimated by pIRT and gp-IRT. Similarly, we report the corresponding cosine similarity in fig. 10, confirming much higher similarity in our case."}, {"title": "C.3. Evolve Step", "content": "C.3.1. ADDITIONAL EXPERIMENT FOR MULTILINGUAL EVOLUTION: SELF-MERGING\nIn this section", "hypotheses": "nNull Hypothesis (H0). The improvements seen in the merged models are due to the model fitting itself on the prompt template, rather than any cross-lingual knowledge exchange.\nAlternative Hypothesis (H1). The improvements arise from actual cross-lingual knowledge transfer\u2014specifically, from the mathematical model to the linguistic model\u2014and are not merely the result of fitting the prompt template.\nTo evaluate these hypotheses, we propose a self-merging procedure. Concretely, we take the linguistic model and merge it with itself using the standard MERGE\u00b3 methodology outlined in algorithm 1. Under Ho, if the improvements are solely due to the prompt template, merging the model with itself should lead to performance gains (i.e., the merged model would still \"fit\" the template). Conversely, under H1, if cross-lingual knowledge transfer is responsible"}]}