{"title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "authors": ["Qiuming Zhao", "Chao Zhang", "Guangzhi Sun", "Mingxing Xu", "Thomas Fang Zheng"], "abstract": "Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LORS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications\u00b9.", "sections": [{"title": "1 Introduction", "content": "Language diversity poses a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition (ASR) (Prabhavalkar et al., 2023) and speech translation (ST) (Xu et al., 2023). With over 7,000 languages spoken worldwide, developing robust S2T systems that generalise across varied linguistic structures remains a fundamental research goal (Liu and Niehues, 2024; Cheng et al., 2023; Sun et al., 2023; Saif et al., 2024; Wang et al., 2021; Le et al., 2021). The advent of end-to-end (E2E) models (Chan et al., 2016; Gulati et al., 2020; Barrault et al., 2023) has marked a paradigm shift in S2T tasks, enabling direct mapping from speech to text across multiple languages within a unified framework. A prominent example is Whisper (Radford et al., 2023), an advanced multi-lingual speech model trained on a large-scale, diverse dataset covering multiple languages and tasks. Despite these advances, existing multi-lingual models still encounter significant challenges in scalability, efficiency, and performance trade-offs.\nTo address these challenges, multi-lingual training strategies (Saif et al., 2024; Xiao et al., 2021; Bai et al., 2018) have been adopted, aiming to enhance model generalisation across languages. These approaches typically rely on joint optimisation of diverse S2T tasks across multiple languages, leveraging shared representations to improve performance. Nevertheless, multi-lingual training is subject to inherent limitations, including substantial training costs, complex model configurations, and limited access to training data across multiple languages and tasks. Moreover, when handling new languages, the training methods typically require training from scratch.\nTo mitigate these issues, this paper proposes to use model merging (Ilharco et al., 2023; Yang et al., 2024a; Khan et al., 2024) to integrate models trained on different languages or tasks while maintaining performance and reducing computational overhead. Model merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues"}, {"title": "2 Related Work", "content": "Multi-lingual speech models inherently face a trade-off between knowledge sharing and negative interference. Early studies adopted hand-picked sub-network sharing strategies, such as language-specific decoders (Dong et al., 2015), attention heads (Zhu et al., 2020), and layer norm/linear transformation (Zhang et al., 2020). Recent research has shifted toward approaches such as mixture-of-experts (Kwon and Chung, 2023; Wang et al., 2023), adapters (Le et al., 2021; Kannan et al., 2019), and pruning (Lu et al., 2022; Lai et al., 2021). To enhance multi-lingual representation learning, language tokens (Johnson et al., 2017), embeddings (Di Gangi et al., 2019) or output factorisations (Zhang et al., 2023a) are introduced to encode language identity, helping the model distinguish between languages.\nThe more effective approach is to adopt multi-lingual training strategies, such as multi-objective optimisation (Saif et al., 2024; Zhang et al., 2022), adversarial learning (Xiao et al., 2021), meta learning (Hsu et al., 2020), and reinforcement learning (Bai et al., 2018). Moreover, large-scale pre-training by leveraging massive amounts of multi-lingual and multi-task data enables models to learn robust and transferable representations across languages, e.g. Whisper (Radford et al., 2023), SeamlessM4T (Barrault et al., 2023), and AudioPaLM (Rubenstein et al., 2023). LoRS-Merging, as an efficient post-training method proposed in this paper, further advances multi-lingual ASR and ST based on pre-trained speech models."}, {"title": "2.2 Model Merging", "content": "Model merging (Yang et al., 2024a; Khan et al., 2024) is an efficient post-training technique that integrates knowledge from models trained on different domains. One stream of research focuses on the loss landscape geometry (Khan et al., 2024) and studies the linear mode connectivity (LMC) (Frankle et al., 2020; Draxler et al., 2018) property that demonstrates the existence of a linearly connected path between local minima within the same loss basin. Many studies (Nagarajan and Kolter, 2019; Izmailov et al., 2018; Frankle et al., 2020) indicate that if two neural networks share part of their optimisation trajectory, such as different finetuned models from the same pretrained model, they typically satisfy LMC, allowing interpolation without sacrificing accuracy and forming the basis of our model merging method. For local minima in different loss basins, inspired by the permutation invariance (Entezari et al., 2021) of neural networks, neuron alignment techniques (Ainsworth et al., 2023; Singh and Jaggi, 2020; Tatro et al., 2020) can be used to place them into the same basin, thereby reducing merging loss.\nAnother stream considers the model spaces, including activation spaces and weight spaces. Research on activation spaces seeks to align the output representations or loss of the merged model with those of each single model as closely as possible (Yang et al., 2024b; Wei et al., 2025; Xiong et al., 2024). Studies based on weight spaces aim to remove redundant parameters or localise effective parameters to resolve task interference. TIES-Merging (Yadav et al., 2024) and DARE (Yu et al., 2024) perform magnitude or random pruning on each single model to significantly remove redundant parameters. TALL-masks (Wang et al., 2024) and Localise-and-Stitch (He et al., 2024) optimise binary masks to localise sparse and effective task-specific parameters. In contrast, LoRS-Merging explores weight space merging by considering not only the detailed parameter redundancy as well as maintaining the effective structure of the weight space via low-rank pruning."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": ""}, {"title": "3.1.1 Task Arithmetic", "content": "Among diverse model merging methods, Task Arithmetic (TA) (Ilharco et al., 2023) has become a fundamental technique in this field due to its simplicity and effectiveness. TA introduces the concept of \"task vector\", defined as the delta parameter derived by subtracting pretrained weights from finetuned weights. By performing simple arithmetic operations on task vectors, TA enables task learning, forgetting, and analogising.\nAssume that $\\theta = {W_l}_{l=1}^L$ represents the parameters of the model, where $W_l$ is the weight of $l$-th layer, and $L$ is the total number of layers. Given a pretrained model $\\theta_0$ and a model $\\theta_i$ finetuned on task $t_i$, the task vector is computed as $\\tau_i = \\theta_i - \\theta_0$. Multiple task vectors can be summed to form a multi-task model, expressed as $\\theta_{merged} = \\theta_0 + \\lambda \\Sigma_{i=1}^n \\tau_i$, where $\\lambda$ is a scaling coefficient for the task vectors."}, {"title": "3.1.2 Pruning", "content": "Given that neural networks are typically over-parameterised and exhibit high redundancy, a considerable number of neurons or connections can be pruned without affecting accuracy (LeCun et al., 1989). In model merging, pruning methods can reduce redundant parameters to mitigate task interference, thereby improving the merging performance.\nMagnitude Pruning (MP) is an unstructured pruning method that prunes connections based on the magnitude of parameters as a measure of importance. Specifically, MP prunes the parameters according to a specific ratio $p$, as follows.\n$\\Delta_{ij} = \\begin{cases} 1 & \\text{if } w_{ij} \\in \\text{top } p\\% \\\\ 0 & o.w. \\end{cases}$ (1)\n$W_{pruned} = M \\odot W$ (2)\nwhere $W, M \\in \\mathbb{R}^{d\\times k}$, and $\\odot$ denotes the element-wise multiplication. However, MP only focuses on the redundancy at the parameter level, overlooking the crucial structural information, which may lead to the disruption of the weight structure.\nSingular Value Pruning (SVP) is a structured pruning method that removes smaller singular values and their corresponding singular vectors. In particular, SVP retains only the top $r$ singular values while discarding the others.\n$W = U\\Sigma V^T$ (3)\n$W_{pruned} = U_r\\Sigma_r V_r^T$ (4)\nwhere $U \\in \\mathbb{R}^{d \\times d}$ and $V \\in \\mathbb{R}^{k \\times k}$ are the left and right singular vector matrices of $W$, and $U_r, V_r$ denote their first $r$ columns. Although SVP preserves a compact weight structure, its coarse pruning granularity makes it challenging to reduce redundancy at a fine-grained parameter level."}, {"title": "3.2 Model Merging for Speech Models", "content": "The model merging process for speech model on S2T tasks with LoRS-Merging as an example is shown in Fig. 1, which comprises four steps. In step 1, a suitable pre-trained speech model is selected. In step 2, for each target language and target task combination, e.g. Catalan ASR, the pre-trained model is finetuned with the task-language-specific data and the delta weight is obtained. In step 3, weight pruning is applied to remove redundant and conflicting delta parameters. In step 4, task arithmetic is applied to combine pruned delta weights into each single merged matrix and hence obtain the merged model.\nModel merging allows new language or task knowledge to be integrated into the model in a flexible post-training manner. When a new set of data for a specific language is obtained, model merging incorporates such knowledge by fine-tuning with the new data alone with data-specific configuration, which also releases the burden of requiring other data to avoid catastrophic forgetting. This benefit is thoroughly demonstrated in our experiments."}, {"title": "3.3 Low-Rank and Sparse Model Merging", "content": "The weights of neural networks contain information on both structure and details. Structural information is coherent, compact, and coarse-grained, whereas detail information is incoherent, scattered, and fine-grained. Both structural and detail information include effective and redundant parts. To reduce redundant parts in both the structure and detail aspects of the weights while retaining effective parts, the LoRS-Merging method is introduced as shown in detail in Fig. 2, which exploits the combination of low-rank structure by SVP and sparsity by MP. SVP performs coarse-grained pruning at the structure level, while MP enables fine-grained pruning at the detail level.\nIn the implementation, we approximate the original weights as the sum of a low-rank component and a sparse component, where the low-rank component captures the compact structure, and the sparse component captures the scattered details, as shown in Eqn. (5).\n$W \\approx L + S$ (5)\nwhere $L$ represents the low-rank component, and $S$ represents the sparse component. Specifically, $L$ is the low-rank matrix obtained by retaining the top $r$ singular values and their corresponding singular vectors from $W$:\n$L = U\\Sigma V^T$ (6)\nand $S$ is the sparse matrix obtained by performing MP on the residual of $W$ and $L$:\n$S = M \\odot (W - L)$ (7)\nTo simplify the description, we refer to this entire process as $\\text{LoRS}(\\cdot)$. In this manner, SVP decouples the structure and details of the weight, preserving a compact structure while allowing fine-grained MP to remove redundant parts in the details.\nFor each model finetuned on single specific language or task data, we apply $\\text{LoRS}(\\cdot)$ to its task vector as a preprocessing step to reduce language or task interference in model merging. A multi-lingual or multi-task model can be achieved through simple merging, expressed as:\n$\\Theta_{merged} = \\Theta_0 + \\lambda \\sum_{i=1}^n \\text{LoRS}(\\tau_i)$ (8)"}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 Data", "content": "CoVoST-2 (Wang et al., 2020a) is a large-scale multi-lingual ST corpus based on Common Voice. It covers translations from English into 15 languages and from 21 languages into English, with a total of 2,880 hours of speech from 78k speakers. We selected 5 high-resource languages and 5 low-resource languages as two language sets to investigate their ASR tasks and the from X to English ST tasks. The high-resource language set includes Catalan (ca), German (de), Spanish (es), French (fr), and Italian (it), while the low-resource language set includes Indonesian (id), Dutch (nl), Portuguese (pt), Russian (ru), and Swedish (sv). Due to the more abundant data in the high-resource language set, our main experimental results are obtained on the high-resource language set, while the low-resource language set serves as an auxiliary validation set. To balance the amount of data across different languages, we fixed the duration of traning data for each language, with 5 hours for the high-resource language set and 1 hour for the low-resource language set. The dev and test sets of both language sets are 1 hour in duration."}, {"title": "4.2 Model and Training Specifications", "content": "Whisper (Radford et al., 2023) is a general-purpose multi-lingual ASR and ST model, a Transformer-based model trained on 680k hours of diverse audio. We chose the small version as the foundation model for the experiments because it achieves a good balance between performance and cost. It has 244 million parameters, with the encoder and decoder each consisting of 12 Transformer blocks. The weight matrices of the attention layers are all 768 by 768, and the MLP layers are 768 by 3072.\nFor each language-specific or task-specific finetuned model, we use a different, optimal learning rate for each during training, and these models are subsequently used for model merging. Finetuning involves updating all parameters. We choose Adam as the optimiser, set the batch size to 8, the accumulation iterations to 4, and train for 10 epochs. We also select the proportions of low-rank parameters retained by SVP from {1%, 2%, 3%, 5%} and sparse parameters retained by MP from {10%, 20%, 40%, 60%}. The beam size for decoding is set to 20 across all languages and tasks. We use Sclite and SacreBLEU tools to score the ASR and ST results, respectively. See Appendix A for more details on hyper-parameter settings. Our experiments are performed on a single RTX 4090 GPU where training on one language and one task with 5 hours of speech data requires 1 hour."}, {"title": "4.3 Baseline and Merging Methods", "content": "We use Multi-lingual and multi-task training as the baseline for comparison with model merging methods, where training is conducted on data mixed from both multi-lingual and multi-task sets. To ensure a fair comparison, the same amount of training data is used from each language and each task. Note that for 5 different languages with both ASR and ST tasks, multi-lingual and multi-task training is performed on 10 times more data and hence 10 times more computational resources.\nIn addition to LoRS-Merging, we investigate the following model merging methods:\nWeight Averaging (WA) merges multiple single models by and unweighted averaging of their weights, $\\Theta_{merged} = \\frac{1}{n} \\sum_{i=1}^n \\Theta_i$.\nTask Arithmetic (TA) uses a scaling factor to weight multiple task vectors estimated on a small development set, $\\Theta_{merged} = \\Theta_0 + \\lambda \\sum_{i=1}^n \\Gamma_i \\tau_i$.\nMP-Merging performs fine-grained magnitude pruning on task vectors to reduce redundancy at the detail level, $\\Theta_{merged} = \\Theta_0 + \\lambda \\sum_{i=1}^n MP(\\tau_i)$.\nSVP-Merging performs coarse-grained singular value pruning on task vectors to reduce redundancy at the structure level, $\\Theta_{merged} = \\Theta_0 + \\lambda \\sum_{i=1}^n SVP(\\tau_i)$. (see Section 3.1.2).\nMoreover, we compare methods against the performance of fine-tuning on each language-task combination. This is the topline of all merging methods since the model is completely adapted to a specific language for a specific task with optimised configurations and without any language conflicts."}, {"title": "5 Evaluation Results and Analysis", "content": ""}, {"title": "5.1 Multi-Lingual Model Merging", "content": "First, we investigate the merging of finetuned models for different languages on the same task, which corresponds to multi-lingual single-task learning.\nLanguage knowledge interference yields imbalanced improvements: Table 1 shows the multi-lingual results of the ASR task with the high-resource language set. On average, multi-lingual training slightly improves the pretrained model but significantly underperforms the finetuned models and merging methods. This may be due to negative interference between the knowledge of different languages, leading to gradient conflicts during training (Wang et al., 2020b). From a per-language perspective, it is observed that ca and fr achieve the largest improvements during fine-tuning while still showing significant improvements in multi-lingual training, whereas languages with smaller improvements during finetuning exhibit a substantial performance drop in multi-lingual training, even worse than the pretrained model. This indicates a strong language conflict in multi-lingual training, with ca and fr dominating. Additionally, we observe that the optimal learning rates for finetuned models vary significantly across languages (see Appendix A), while the unified learning rate configuration required by multi-lingual training prevents each language from reaching its optimal performance.\nModel merging mitigates language conflicts:\nIn contrast, model merging methods show obvious improvements across almost all languages, demonstrating reduced conflict and better stability. Among model merging methods, TA outperforms WA due to its flexible scaling factor. Both MP-Merging and SVP-Merging further improve the performance of TA by reducing redundancy, and MP-Merging slightly outperforms SVP-Merging due to its finer-grained pruning. Combining the advantages of SVP and MP, LORS-Merging achieves the best performance.\nTable 2 provides the multi-lingual results on ST task with the high-resource language set. The main conclusion is consistent with the ASR task: model merging methods still significantly outperform multi-lingual training, with LoRS-Merging achieving the best performance."}, {"title": "5.2 Multi-Task Model Merging", "content": "Next, we merge finetuned models for different tasks (ASR and ST) with the same language which corresponds to multi-task single-language learning.\nASR and ST tasks for the same language can mutually benefit from each other: Table 3 presents the multi-task results with the high-resource language set. In general, multi-task training performs similarly to finetuned models on ASR but is a lot worse on ST. This is likely due to the substantial differences in optimal hyper-parameter configurations between the two tasks. Model merging methods clearly outperform finetuned models, which not only demonstrates their effectiveness but also shows the mutual benefits between ASR and ST. In terms of performance gains, the improvement in ASR is greater than in ST. We attribute this to the fact that ASR is inherently simpler than ST"}, {"title": "5.3 Multi-Lingual Multi-Task Model Merging", "content": "Then, we investigate the merging of finetuned models for both different languages and tasks, which correspond to multi-lingual (ML) and multi-task (MT) learning. Specifically, we explore 4 different training and merging settings:\nML and MT training: Fine-tuning on all languages and both tasks jointly.\nML and MT merging: Fine-tuning on each language for each task separately and merging all.\nMT training and ML merging: Fine-tuning both tasks jointly for each language, and merging models from different languages.\nML training and MT merging: Fine-tuning on all languages jointly for each task, and merging models from different tasks.\nTable 4 displays the multi-lingual and multi-task results with the high-resource language set. Multi-lingual and multi-task training shows little improvement over the pretrained model, due to negative interference during training and the use of a unified training configuration for all languages and tasks. Nevertheless, the performance of multi-lingual and multi-task merging is on par with that of finetuned models, further underscoring the superiority of model merging. ML training followed by MT merging achieves the best performance, even significantly outperforming finetuned models. Although we did not observe the same phenomenon"}, {"title": "5.4 Effect of Numbers of Languages", "content": "To further demonstrate the robustness of LoRS-Merging to language selection, experiments are performed using different numbers of languages. Figure 3 shows the average performance across all languages and all training runs with possible combinations of 2, 3, 4 or 5 languages.\nLoRS-Merging improvements are consistent across different numbers of languages: As the number of languages increases, the performance of both TA and LoRS-Merging degrades due to negative interference between languages. LoRS-Merging consistently outperforms TA in both ASR and ST tasks, and even surpasses the finetuned models in the ASR task. This is likely due to"}, {"title": "5.5 Effect of Language Data Scale", "content": "We then demonstrate the robustness of merging methods to different training data sizes for both tasks. Fig. 4 shows the WER (top) and BLEU (bottom) scores for ASR and ST at different data scales, respectively. As the data scale increases, the performance of multi-lingual training does not always improve. This may be because the pretrained model already performs well, and the significant language interference and conflict in multi-lingual training hinder the effective improvement of multi-language performance. Furthermore, the performance loss of model merging increases with data scale, compared to finetuned models. It can be explained by the fact that larger training data tends to increase the divergence in the optimisation trajectories of different finetuned models, resulting in"}, {"title": "5.6 Analysis of Model Redundancy", "content": "Furthermore, we justify the necessity of SVP and MP to remove model redundancy by showing the model performance against the pruning ratio of finetuned models for ASR as shown in Fig. 5. As shown, both SVP and MP significantly improve the performance of finetuned models, indicating the presence of substantial redundancy in the structure and details of the finetuned models, respectively. The model performance reaches the best at a high pruning level, indicating that the redundancy is particularly large for ASR. We observed a much smaller redundancy in ST, which also explains the observation that LoRS-Merging achieves more salient improvement on ASR than ST. Moreover, redundancy increases with training data, possibly due to the accumulation of gradient noise during training. MP achieves greater performance gains than SVP, indicating more redundancy at the detail level, which is better addressed by fine-grained MP."}, {"title": "6 Conclusion", "content": "This paper explores model merging for multi-lingual ASR and ST on pre-trained speech models and proposes the LoRS-Merging approach. LoRS-Merging combines low-rank and sparse pruning to retain essential structures, eliminate redundant parameters and mitigate language and task interference. Experiments across 10 languages show that LoRS-Merging effectively alleviates language interference and significantly outperforms multi-lingual multi-task training baselines."}, {"title": "7 Limitations", "content": "There are three main limitations of this work. First, as a common limitation of all model merging methods, the same model structure is required across all tasks and languages. This is less of a concern under the current trend of using the same Transformer structure, but methods need to be developed in the future to accommodate subtle structural differences. Second, reasonably-sized training sets are required for each language, and low-resource languages may suffer from reduced improvements. Third, this work mainly explores the two most popular S2T tasks. Other possible tasks can be explored in future work, including spoken language understanding and speaker adaptation."}, {"title": "A Hyper-Parameter Details", "content": "The detailed hyper-parameter settings for each language are shown in Table 5 for ASR and Table 6 for ST, respectively."}, {"title": "B Results of Low-Resource Language Set", "content": "The results of the low-resource language set are shown in this section. Specifically, Table 7 and 8 show the multi-lingual single task training and merging for ASR and ST respectively."}, {"title": "C Detailed Results on Multi-task merging", "content": "Detailed per-language results of Table 3 are shown in Table 11."}]}