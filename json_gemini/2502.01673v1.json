{"title": "Multilingual State Space Models for Structured Question Answering in Indic Languages", "authors": ["Arpita Vats", "Rahul Raja", "Mrinal Mathur", "Vinija Jain", "Aman Chadha"], "abstract": "The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA). To address these challenges, this paper explores the application of State Space Models (SSMs) to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMS to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. Furthermore, we propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.", "sections": [{"title": "1. Introduction", "content": "In recent years, the field of Natural Language Processing (NLP) has witnessed significant advancements with the development of models capable of handling complex linguistic structures and long-range dependencies. State Space Models (SSMs) (Goel et al., 2022) have emerged as a promising alternative to traditional architectures like Transformers (Vaswani et al., 2017), particularly in tasks requiring efficient long-sequence modeling. Models such as Mamba (Gu & Dao, 2024) and its variants have demonstrated the ability to manage long-context language modeling with constant memory usage, addressing some limitations inherent in Transformer-based models (Nguyen et al., 2024) (Bourdois, 2022). Despite these advancements, the development of question-answering (QA) systems for low-resource languages, particularly Indic languages, remains a challenging endeavor. India has a population of 1.4 billion and is home to 122 languages and 270 mother tongues. Indian languages fall in the minority as far as NLP models are concerned. One of the Indian languages\u2014Hindi\u2014is spoken by 577.8 million people worldwide (Sabane et al., 2024). Likewise, Marathi, another Indian language, is ranked 11th on the list of most spoken languages. However, the scarcity of annotated datasets and neural models has hindered progress in building effective QA systems for these languages. Recent efforts have been made to address this gap by introducing large-scale QA datasets for Indic languages, such as the Indic QA dataset (Singh et al., 2024), which provides a substantial resource for developing systems tailored to the linguistic nuances of Indic languages. This paper explores the various SSM models, to build efficient and contextually aware QA systems for Indic languages. SSMs are particularly suited for this task due to their ability to model both long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. The main contributions of this paper are:\n\u2022 This work represents the first application of SSMs to question-answering tasks in Indic languages, setting a foundational benchmark for future research in this domain.\n\u2022 We evaluate multiple SSM architectures on diverse datasets representing various Indic languages, conducting a detailed comparative analysis to highlight their strengths and limitations.\n\u2022 Through extensive experiments, we demonstrate how SSMs effectively capture the linguistic subtleties of"}, {"title": "2. Related Works", "content": "The field of QA and NLP has seen rapid advancements, particularly with the introduction of models leveraging state-space representations and multi-modal learning. This section reviews key advancements related to SSMs, Indic language processing, and multi-modal approaches to contextual understanding."}, {"title": "2.1. Advances in State Space Models", "content": "Traditional SSMs, such as the Kalman Filter (Kalman, 1960) and Hidden Markov Models (HMMs) (Rabiner & Juang, 1986), have been widely used for decades. While effective for many tasks, these models struggle with long-range dependencies and high-dimensional data (Bourdois, 2022). To overcome these limitations, SSMs have emerged, combining the foundational principles of traditional SSMs with the expressive power of deep learning (Sarrof et al., 2024). These approaches allow for scalability and adaptability in modern machine learning applications (Goel et al., 2022).SSMs have emerged as a robust alternative to transformers in sequence modeling, excelling particularly in areas where transformers face inherent limitations (Patro et al., 2024). Mathematically, an SSM can be represented as follows:\n$X_{t+1} = Ax_t + Bu_t + W_t, W_t \\sim N(0,Q)$ (1)\nThe Equation (1) (Gu et al., 2022) describes a framework where the state vector $x_t$ at time $t$ represents latent variables encapsulating the system's underlying dynamics. The state evolution from t to t + 1 is governed by the state transition matrix A, while the input matrix B determines the effect of the control input $u_t$ on the state transitions. The control input $u_t$ serves as an external influence, allowing modifications to the state dynamics at time t. Additionally, process noise $w_t$ is modeled as a Gaussian random variable with zero mean (N(0, Q)) and covariance matrix Q, capturing uncertainties and randomness in the state transitions. The covariance matrix Q quantifies the variability of the process noise $w_t$, ensuring the model accounts for inherent stochasticity.\nTransformers rely on self-attention mechanisms to capture relationships across sequences (Vaswani et al., 2017). While effective, their quadratic complexity with respect to sequence length leads to significant computational overhead and memory usage for long contexts (Taha, 2025). In contrast, SSMs are designed with a focus on efficiency and scalability, leveraging state transitions and compact latent representations to model temporal dependencies (Shakhadri et al., 2025). This design enables SSMs to process se-"}, {"title": "2.2. Indic Language Question Answering", "content": "Indic languages, with their diverse scripts, complex grammar, and linguistic variations, present unique challenges for QA systems (Dani & Sathe, 2024). Features such as free word order, inflectional patterns, and compound words complicate NLP tasks. While initiatives like the IndicQA dataset (Sabane et al., 2024) have advanced question answering in these languages, the scarcity of annotated datasets and neural models remains a significant challenge. Techniques such as transfer learning and domain adaptation have shown promise, with multilingual models fine-tuned on task-specific data achieving notable improvements in Hindi and Marathi QA tasks (Jin et al., 2022).\nAdopting the SQUAD format (Rajpurkar et al., 2016), a widely used benchmark in QA research, facilitates the creation of structured datasets for Indic languages. This format includes context passages, questions, and corresponding answers with exact character positions, ensuring consistency and compatibility across QA frameworks. Translating and adapting the SQUAD format for Indic languages has proven effective, as demonstrated by the performance improvements of the IndicQA dataset when aligned with this structure (Singh et al., 2024).\nTo address data scarcity, data augmentation techniques such as back-translation have been used to generate synthetic SQUAD-style datasets (Khan et al., 2024). Fine-tuning multilingual pre-trained models like XLM-R (Conneau et al., 2020) and mT5 (Xue et al., 2021) on such datasets has further improved performance in Indic QA tasks. The structured nature of the SQUAD format allows for robust evaluation of fact-based, reasoning-based, and opinion-based questions, making it particularly suitable for Indic languages (Upadhyay et al., 2024). Aligning Indic-language datasets with this format enhances model performance and facilitates better comparisons across multilingual QA research."}, {"title": "3. Methodology", "content": "Our methodology focuses on developing a question-answering framework tailored for Indic languages by leveraging the capabilities of SSMs. Specifically, we experimented with various models, including Mamba, Mamba-2,Falcon Mamba, Jamba, Zamba, Samba, and Hymba, to address the linguistic diversity and complexity of Indic languages. Each model was evaluated for its ability to process questions, answers, and context efficiently while preserving the grammatical and semantic nuances of the respective languages. The overall framework for this system is illustrated in Figure 1."}, {"title": "3.1. Dataset Preprocessing", "content": "Dataset preprocessing is a critical step in preparing the data for the SSM-based QA model. The preprocessing pipeline involves tokenization, vocabulary construction, encoding as mentioned in Section 3.1.1, and data transformation as mentioned in Section 3.1.2."}, {"title": "3.1.1. Tokenization", "content": "For our data preprocessing pipeline, we employed Indic-NLPtokenizer (Kunchukuttan, 2020), which is specifically designed to address the unique grammatical structures and linguistic characteristics of Indic languages. This tokenizer is particularly effective for handling the complex and diverse scripts of languages such as Hindi and Marathi. By maintaining consistent tokenization across the context (context about the question), question, and answer in our dataset, it enabled uniformity and alignment throughout the preprocessing pipeline. The tokenizer was trained on our corpus to capture language-specific patterns and generate a custom vocabulary tailored to Indic languages. This vocabulary included frequently occurring tokens and special tokens such as padding (<pad>), unknown tokens (<unk>), start-of-sentence (<sos>), and end-of-sentence (eos) markers, which are essential for enabling sequence alignment, handling missing or unknown words, and defining sentence boundaries during both training and inference. These functionalities ensure that the model can process variable-length inputs, manage incomplete data, and correctly interpret input sequences. After constructing the vocabulary, we performed encoding, where the starting position (answer_start) of the answer, representing the character index of the answer within the context, was mapped to its corresponding tokenized representation. This process generated the start (token_start) and end (token_end) token positions, which define the answer span within the tokenized sequence. These fields are essential for enabling the model to accurately learn and predict answer spans, ensuring precise alignment between the original text and its tokenized format for effective training and evaluation. The overall pipeline is illustrated in Figure 2."}, {"title": "3.1.2. Dataset Transformation", "content": "After tokenization, the dataset was transformed into a structured format to maintain consistency across all samples. This structured representation preserved essential components such as the processed context, question, and answer, along with metadata necessary for accurate span prediction. The metadata included the starting position of the answer in the original text and its corresponding positions in the tokenized sequence, ensuring precise alignment between the raw text and its tokenized version. To handle variable-length sequences, padding was applied using a designated"}, {"title": "3.2. Training", "content": "After data preprocessing, the next step was training. Initially, we evaluated the model's performance without any fine-tuning, using only tokenization mentioned in Section 3.1.1. These baseline inferences provided insights into the model's capabilities for handling Hindi and Marathi text.\nSubsequently, we fine-tuned the SSM model for question answering in Indic languages, we designed a training pipeline to enhance the performance for question answering in Indic languages. The fine-tuning process employed a resource-efficient strategy using low-rank adaptation (LoRA) (Hu et al., 2021). LoRA employs a strategy of updating only the parameters in the projection layers of attention modules and the embeddings layer, leaving the rest of the model unchanged. By focusing on these key components, the fine-"}, {"title": "3.3. Models", "content": "Mamba: Mamba is a linear-time sequence model designed to efficiently handle long sequences by incorporating input-dependent parameterization through a selective propagation mechanism. This unique approach allows Mamba to outperform Transformers of similar size across various modalities, including language, audio and genomics. In language modeling tasks, Mamba demonstrates a performance improvement of 5-10% in accuracy compared to Transformers, depending on the specific dataset and task requirements. Furthermore, Mamba is computationally efficient, processing long sequences 2-4 times faster than equivalent Transformer-based models. These advantages make Mamba particularly effective for tasks involving extensive text data, such as question"}, {"title": "3.4. Prompting Strategies for Indic Language based SSMS", "content": "SSMs like Mamba and Mamba-2 require careful prompt engineering to maximize the question-answering capabilities in low-resource languages such as Hindi and Marathi. Also because of their lexicals, the tokens are not of constant length. Our approach combines the architectural strengths of SSMs with linguistic consideration for Devanagari script based languages.\nThe time-invariant nature of SSMs enables effective zero-shot prompting through structured template design. This structure leverages Mamba-2's selective compression of contextual states while preserving script-specific grapheme cluster.\nFor Few-shot prompting Ye & Durrett (2022), The model maintains separate state trajectories for example patterns through its recurrent SSM block."}, {"title": "3.5. Evaluation Metrics", "content": "To comprehensively evaluate model performance in question-answering tasks, we employed multiple metrics capturing different aspects of accuracy and relevance. EM provides a strict precision measure by evaluating the percentage of predictions that exactly match the ground truth, making it ideal for extractive QA tasks (Maalouly, 2022). F1 Score balances precision and recall, rewarding partial matches to better reflect token-level similarity, which is especially useful for paraphrased responses (Powers, 2020). To assess deeper semantic relationships, BERTScore (Zhang et al., 2020) leverages contextual embeddings from a pre-trained model, capturing meaning beyond exact word matches. ROUGE evaluates n-gram recall and overlap, ensuring that key phrases from the reference answer are retained even if the phrasing differs (Lin, 2004). Lastly, BLEU quantifies n-gram precision while incorporating a brevity penalty to prevent overly short predictions, making it particularly effective for assessing fluency and word-level accuracy in extractive QA settings (Papineni et al., 2002). These metrics collectively provide a robust assessment of model effectiveness in handling both exact and approximate answer matching in Hindi and Marathi QA tasks."}, {"title": "4. Experiments", "content": "In our experimentation, we fine-tuned several models-Falcon Mamba, Mamba, Mamba-2, Jamba, Zamba, Samba, and Hymba-on a Hindi and Marathi SQuAD-style question-answering dataset specifically designed for these languages. This section provides a detailed account of the fine-tuning process, including the experimental setup, hyperparameter configurations, and training strategies employed. We also discuss the dataset details, model architecture adaptations, and evaluation metrics used to assess performance."}, {"title": "4.1. Dataset", "content": "We utilized the IndicQA dataset, which was constructed by aggregating multiple open-source datasets to develop a SQuAD-style resource for Hindi and Marathi. This dataset includes a significant component of approximately 28,000"}, {"title": "4.2. Implementation Details", "content": "The fine-tuning process for the models used task-specific configurations to optimize performance and efficiency. LORA was the primary method for fine-tuning across all models, with a rank of 8, a scaling factor (a) of 32 to control the impact of low-rank updates on the model's original weights, and a dropout rate of 0.1. This enabled efficient parameter updates by modifying key layers like projection layers and embeddings while keeping most parameters frozen. The models were trained on Hindi and Marathi question-answer pairs, formatted in a conversational template with system, user, and assistant messages, and tokenized to align inputs and outputs with a maximum sequence length of 2048 tokens.\nFor Mamba and Mamba-2, the fine-tuning configuration included a learning rate of 2 \u00d7 10-4, a batch size of 4 (with gradient accumulation for an effective batch size of 32), 3 epochs, 100 warmup steps, and mixed precision (FP16). The Falcon Mamba model was fine-tuned with start and end predictors, cross-entropy loss, and the Adam optimizer, using a learning rate of 1 \u00d7 10-4, a batch size of 4, and 10 epochs. The Jamba model employed a LoRA configuration with an increased rank of 16 to enhance capacity and processed sequences with a context window of up to 256K tokens. The Zamba model was optimized for long sequences, using a maximum sequence length of 4096 tokens. The Samba model focused on computational efficiency, handling sequences up to 256K tokens with a batch size of 8 (effective batch size of 64 with gradient accumulation), excelling in modeling complex conversational patterns. Finally, the Hymba model incorporated learnable meta tokens and partial sliding window attention, trained with a batch size of 4 and a learning rate of 3 \u00d7 10-4. Evaluations for"}, {"title": "4.3. Results", "content": "This section evaluates model performance on Hindi and Marathi datasets, comparing results before and after fine-tuning. Additionally, Figure 8 includes sample model responses to highlight qualitative differences in predictions."}, {"title": "4.3.1. Results without Fine-Tuning", "content": "Table 3 presents model performance before fine-tuning, showing variations across different architectures. Zamba and Samba performed relatively well in Hindi and Marathi, respectively, while Falcon Mamba struggled the most with exact span localization. Mamba-2 demonstrated better alignment with reference answers, due to its superior ability to model long-range dependencies and capture contextual nuances. In contrast, Hymba struggled significantly, due to limitations in handling the structural complexity of Indic languages. Models that performed better exhibited stronger token-level alignment and more effective handling of variations, while weaker models failed to generalize effectively, particularly in Marathi, where linguistic complexity posed additional challenges. Another key observation is that models generally performed better on Hindi than Marathi, which can be attributed to differences in dataset size, linguistic complexity, and pre-training exposure. Hindi, being more widely studied, benefits from larger corpora and more refined tokenization strategies, whereas Marathi's syntactic variations pose additional challenges. Models that struggled, such as Falcon Mamba and Hymba, lacked the ability to effectively process inflectional diversity and complex word formations, leading to lower performance. This highlights the necessity of incorporating language-specific optimizations and better pre-training strategies to improve generalization in underrepresented Indic languages."}, {"title": "4.3.2. Results with Fine-Tuning", "content": "Fine-tuning significantly enhanced model performance across all evaluation metrics, demonstrating the importance of task-specific adaptation for Indic languages. The most improvements were observed in Mamba-2, which exhibited superior alignment with reference answers, improved span localization, and increased robustness in handling complex linguistic structures. These gains highlight the ability of fine-tuned models to better capture long-range dependencies and semantic nuances in Hindi and Marathi question-answering tasks. One key observation is that fine-tuning helped narrow the performance gap between Hindi and Marathi, particularly in models that struggled before adaptation. While Marathi remained more challenging, models showed increased accuracy in capturing token overlaps and generating more contextually relevant responses. Additionally, models that initially performed poorly, such as Falcon Mamba and Hymba, saw only marginal improvements, suggesting fundamental architectural limitations that fine-tuning alone could not overcome. Beyond numerical improvements, fine-tuning also contributed to better lexical and semantic alignment. Models demonstrated enhanced fluency in response generation, with improvements in phrase structure and word selection, reducing inconsistencies seen in non-fine-tuned outputs. However, challenges persisted for certain models, particularly in maintaining coherence when processing long-form text in Marathi. These findings emphasize the necessity of fine-tuning pre-trained models on domain-specific datasets to optimize their performance for low-resource languages. Models results after fine-tuning is provided in Table 4, illustrating the extent of improvement across different architectures."}, {"title": "5. Conclusion", "content": "This paper explores the development of efficient question-answering systems for Indic languages. By applying SSMs, this research demonstrates advancements in capturing the linguistic nuances of Hindi and Marathi. Among the models evaluated, Mamba-2 consistently delivered superior performance, showcasing its ability to handle both short-term and long-term dependencies effectively. Fine-tuning proved critical in improving model accuracy, highlighting the importance of adapting pre-trained architectures to meet the specific needs of low-resource languages. The integration of robust preprocessing techniques, such as tokenization tailored for Indic scripts, further contributed to the models' success. These findings lay a strong foundation for scalable, multilingual QA systems and underscore the potential of SSMs in addressing language-specific challenges. Future work will focus on expanding dataset diversity by including additional dialects of Indic languages and developing a unified model for commonly used dialects."}, {"title": "Impact Statement", "content": "This research addresses key challenges in developing efficient and context-aware QA systems for Indic languages by leveraging SSMs. It introduces a scalable framework tailored to the complex morphology, syntax, and contextual nuances of these languages. The study advances NLP for low-resource languages, setting a benchmark for multilingual scenarios through SSM-based architectures. By focusing on widely spoken languages like Hindi and Marathi, it enhances accessibility and inclusivity, enabling AI systems to serve underrepresented linguistic communities. The research has real-world applications across sectors such as education, healthcare, governance, and customer support, improving service delivery through localized QA systems. Its emphasis on computational efficiency ensures that these models can be deployed in resource-constrained environments, extending their impact to underserved regions. This work lays a strong foundation for future Indic language QA research and provides a scalable blueprint for adapting similar approaches to other low-resource and linguistically diverse languages worldwide."}, {"title": "A. Dataset Statistics", "content": "The datasets used in this study encompass diverse Indic languages, primarily focusing on Hindi and Marathi. The datasets are structured in a SQuAD-style format, comprising context paragraphs, questions, and corresponding answers. This standardized format ensures consistency across different QA tasks and facilitates effective model training and evaluation.\nThe scatter plot in Figure 5 illustrates the relationship between question length and answer length across the Hindi (blue) and Marathi (red) datasets. The visualization highlights a clear positive correlation between the lengths of questions and their corresponding answers, with longer questions often leading to longer answers. Notably, the Marathi dataset shows slightly shorter average lengths compared to the Hindi dataset, reflecting language-specific characteristics. This analysis provides insights into the structural patterns within the dataset, aiding in understanding the data distribution and informing preprocessing decisions for the SSM-based QA model.\nThe scatter plot in Figure 6 illustrates the relationship between the starting position of answers within the context and the total context length for the Hindi (blue) and Marathi (red) datasets. The visualization shows that as the context length increases, the starting positions of answers become more varied, indicating that longer contexts are more likely to contain answers at different positions. This trend is consistent across both datasets, although the Marathi dataset tends to have shorter contexts on average compared to Hindi. This analysis highlights the diversity in answer positions relative to context length, which is crucial for designing models capable of handling varied input structures in question-answering tasks.\nThe correlation heatmaps provide insights into the relationships between key features in the Hindi and Marathi datasets, including context length, question length, answer length, and answer start position. As shown, context length exhibits a strong positive correlation with answer start position (0.48 for Hindi and 0.53 for Marathi), indicating that longer contexts tend to have answers starting further into the text. This relationship is more pronounced in the Marathi dataset. Additionally, the correlations between question length and answer length are negligible, suggesting that the lengths of questions and answers are largely independent. These observations reflect the structural differences in data organization and provide useful information for designing models that can effectively capture these patterns. The heatmaps highlight the similarities and differences in feature interactions between the two languages, guiding preprocessing and feature selection for downstream tasks."}, {"title": "B. Results", "content": "The results, as shown in Figure 9, illustrate the performance metrics for the Hindi and Marathi datasets, comparing outcomes before and after fine-tuning the model. Metrics such as BLEU, ROUGE, F1, Exact Match, and BERT scores are presented for both languages, highlighting the impact of fine-tuning on model performance. The left panel represent the scores without fine-tuning, while the right panels show the improved results after fine-tuning. Across both datasets, fine-tuning consistently improves all evaluation metrics, with notable gains in Exact Match and F1 scores, indicating better alignment between predicted and ground-truth answers. The BERT scores also demonstrate significant improvements, reflecting enhanced semantic understanding. These results emphasize the importance of fine-tuning for adapting the model to the nuances of Hindi and Marathi languages in the QA task."}]}