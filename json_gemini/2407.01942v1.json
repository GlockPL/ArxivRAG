{"title": "Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness", "authors": ["Khyathi Raghavi Chandu", "Linjie Li", "Anas Awadalla", "Ximing Lu", "Jae Sung Park", "Jack Hessel", "Lijuan Wang", "Yejin Choi"], "abstract": "The ability to acknowledge the inevitable uncertainty in their knowledge and reasoning is a prerequisite for AI systems to be truly truthful and reliable. In this paper, we present a taxonomy of uncertainty specific to vision-language AI systems, distinguishing between epistemic uncertainty (arising from a lack of in-formation) and aleatoric uncertainty (due to inherent unpredictability), and further explore finer categories within. Based on this taxonomy, we synthesize a benchmark dataset, CERTAINLYUNCERTAIN, featuring 178K visual question answering (VQA) samples as contrastive pairs. This is achieved by 1) inpainting images to make previously answerable questions into unanswerable ones; and 2) using image captions to prompt large language models for both answerable and unanswerable questions. Additionally, we introduce a new metric confidence-weighted accuracy, that is well correlated with both accuracy and calibration error, to address the short-comings of existing metrics. Despite the recent rapid progress in vision-language models (VLMs), evaluations on our benchmark show that they perform poorly in uncertain scenarios. Further experiments demonstrate that supervised fine-tuning with CERTAINLYUNCERTAIN enhances the performance of VLMs, and reduces the calibration error. These improvements extend beyond our benchmark to existing refusal-oriented datasets and show positive results on reducing hallucinations, while maintaining performance on standard VQA benchmarks. Our work underscores the importance of addressing uncertainty in vision-language AI systems to improve their reliability and trustworthiness in real-world applications.", "sections": [{"title": "1 Introduction", "content": "An AI system with intellectual integrity must know when to admit \"I don't know\", which, in turn, requires a sharp awareness of its own limitations of knowledge and reasoning, as well as the inherent uncertainty around the external world [1, 2, 3, 4, 5, 6]. However, current vision-language models [7, 8, 9] do not exhibit such sufficiently sharp awareness of its own mistakes, which lead to overly-confident, uncalibrated predictions [10] and hallucinations [11, 12]. This is only as expected however, given that the predominant training recipe does not typically encourage the models to express uncertainty or acknowledge when they do not know the answer. Rather, they are incentivized to make predictions regardless of their confidence level. Moreover, existing benchmarks mostly focus on scenarios where clear and definitive answers are available [13, 14], leaving a notable gap as the models are not adequately exposed to explicitly uncertain training instances.\nMotivated by these, we introduce CERTAINLYUNCERTAIN, a dataset of approximately 178K visual question answering (VQA) instances that encompass diverse types of uncertainties."}, {"title": "2 CERTAINLY UNCERTAIN", "content": "To train models to properly admit \"I don't know\", it is crucial to construct a large-scale dataset that covers a diverse range of uncertain situations. This is challenging, as most internet data focus on"}, {"title": "2.1 Taxonomy of Uncertainty Awareness", "content": "Depending on whether it is due to contextual inexpressiveness or genuine incapability to answer, we broadly categorize multimodal uncertainty into 2 types, epistemic and aleatoric uncertainty.\nEpistemic Uncertainty refers to the uncertainty in a model's predictions that arises from a lack of knowledge or complete information about the system being modeled. It is due to the model's limited understanding or insufficient data, which can be reduced by gathering more information, improving the quality of data, or enhancing the model itself. This type of uncertainty highlights areas where the model's predictions may be less reliable due to the lack of sufficient evidence to make accurate inferences. We further categorize the awareness of epistemic uncertainty into 3 finegrained types:\n\u2022 Knowledge awareness means understanding that some questions require information or common sense that is not shown in the image. For example, you might need specialized knowledge or up-to-date information from outside sources. Knowing when this extra information is needed helps avoid wrong answers.\n\u2022 Complexity awareness is recognizing when a question is difficult because it involves many parts or is hard to understand. This difficulty can come from how the question is asked or from the effort needed to understand the context and details of the question.\n\u2022 Extraneous awareness refers to the ability to identify and disregard elements within an image that are not relevant to the question at hand. This involves recognizing objects, attributes, or aspects that, while present in the image, do not contribute to answering the question.\nAleatoric Uncertainty is the inherent unpredictability in a system or process that cannot be reduced or eliminated. It arises from the fundamental randomness or chaotic nature of the task itself. For example, predicting the outcome of a coin toss involves intrinsic uncertainty because the result is inherently probabilistic and cannot be determined with certainty in advance. Similarly, we define 2 sub-categories under aleatoric uncertainty:\n\u2022 Temporal awareness means understanding that we may not always have access to all relevant data required to predict specific outcomes with absolute certainty, especially when it involves reasoning about time. This includes events in the past or future that cannot be inferred from the image alone with absolute certainty. Recognizing the limitations of temporal reasoning helps manage expectations about the accuracy of predictions involving time-related aspects.\n\u2022 Ambiguity awareness involves recognizing situations, objects, or individuals that can be under-stood, interpreted, or perceived in more than one way. Ambiguity introduces uncertainty and a lack of clarity, leading to multiple possible interpretations. While ambiguity can encourage exploration of different meanings or perspectives, it can also cause confusion. It is essential to be aware of the levels of certainty in ambiguous scenarios to avoid misinterpretation and errors."}, {"title": "2.2 Dataset Creation", "content": "Based on the aforementioned taxonomy, we construct CERTAINLYUNCERTAIN, comprising con-trastive VQA pairs for each category described above. The statistics of our dataset are summarized in Table 1. The contrastive instances in CERTAINLY UNCERTAIN are derived from two sources: images and captions. For sourcing from images, the same question that is answerable for the original image is rendered unanswerable for the perturbed image. For sourcing from captions, we prompt GPT-4 [15] to generate both an answerable and an unanswerable question based on the same caption. Below, we describe the dataset creation pipeline in detail.\nSourcing from captions. We use detailed paragraph captions to prompt questions for each category of uncertainty. Each prompt includes a definition of the category along with examples of answerable and unanswerable questions and their answers. The captions are sourced from DOCCI [20] which"}, {"title": "2.3 Evaluation Metrics", "content": "Standard metrics. We report model performance on CERTAINLY UNCERTAIN with standard metrics, including accuracy and F1.\nFor accuracy, we use LAVE [25] with Mistral-7B [26] as the evaluator, comparing ground truth and predictions to assign scores of 0, 0.5, or 1. To adapt LAVE to unanswerable settings, we introduce a dual-stage judging mechanism. This approach is more reliable because refusals or IDK responses can be expressed in various ways, such as simply stating IDK, asking a follow-up question, or offering a reasonable guess. The first stage is IDK normalization, where we use LAVE to determine if either the prediction or ground truth (GT) is IDK and normalize the answer to IDK. For refusal-based benchmarks, since the unanswerability of the question is annotated, we directly rely on the ground truth label for GT answers. The second stage is to award accuracy. If either the prediction or GT is normalized to IDK, we compare the strings. Otherwise, we award the standard LAVE score. Formally, the $LAVE_{idk}$ score is defined as\n$LAVE_{idk} =\n\\begin{cases}\n1  \\text{ if LAVE(pred == IDK) or LAVE(GT == IDK)}\\\\\nLAVE(GT, pred) \\text{ else }\n\\end{cases}$\nIn addition, we report $F1_{idk}$ which is the F1 score only on the unanswerable questions.\nConfidence-weighted accuracy. Current evaluation metrics have significant limitations in compre-hensively assessing both the accuracy and the confidence of model predictions. Accuracy metrics, which score binarily, fail to consider model confidence as they ignore the probability estimates asso-ciated with predictions. Conversely, metrics like Expected Calibration Error (ECE), which measures"}, {"title": "3 Experiments", "content": "3.1 Experimental Details\nWe conduct experiments with the instruction-tuned models including variants of LLaVA [27] - 7B, 13B, 34B[28], and Qwen-VL [29], as well as evaluating the performance of GPT-4V on our CERTAINLY UNCERTAIN benchmark.\nIn addition to direct evaluation, we investigate 3 training strategies: supervised finetuning, R-tuning, and preference optimization, with our data and compare them to the base model. As an additional baseline, we implement a naive selective prediction approach, marking predictions as IDK when the prediction probability falls below a threshold. For supervised finetuning, we assess the effectiveness of our data by comparing finetuning with CERTAINLYUNCERTAIN against LLaVA and LRV [30] instruction-tuning datasets. For R-tuning we follow [18] to re-annotate ground truth answers that are incorrectly predicted by the base model to reflect IDK, and use this re-annotated refusal data for supervised fine-tuning. For preference optimization, we directly adopt the two answers to the contrastive VQA pairs as the answer choices, and perform DPO [19]."}, {"title": "3.2 Evaluation Benchmarks", "content": "To demonstrate the effectiveness of our data, we additionally evaluate the models trained with our data on other benchmarks, which we detail below.\nRefusal-based benchmarks: UNK-VQA [16] contains about 10K instances of answerable and unanswerable questions constructed from manipulating the VQA v2 instances using question pertur-bation and image perturbation. We deliberately discard the ambiguous category from UNK-VQA as the ambiguity here was defined as having multiple plausible answers and simply listing all of them should be correct instead of saying IDK. The \"absurd\" category of the TDIUC [24] data containing ~ 366K questions is constructed by compiling a list of objects that are missing from a given image and then identifying questions from the rest of TDIUC that inquire about these absent objects. In our experiemnts, we randomly sample 5K instances from each dataset for evaluation.\nHallucination-based benchmarks: MMHal-Bench [31] contains 96 questions curated based on the expert observations in 8 hallucination categories such as object attribute, adversarial object, counting etc., Upon establishing the severity of object hallucinations, [12] introduce POPE with ~ 9K instances that samples objects randomly, adversarially, and based on popularity to check for their presence binarily. To comprehensively study types of hallucinations, [11] introduce AMBER for existence, attribute, and relation hallucinations and AMBER-based evaluation metrics.\nStandard benchmarks: While mitigating hallucination and learning to refuse is important, the goal is also to not hurt model performance on standard datasets. Therefore, we conduct evaluations on standard datasets VQAv2 [14] and VizWiz [32] validation splits."}, {"title": "3.3 Results and Discussion", "content": "We extensively evaluate the performance of GPT-4V, LLaVA and Qwen-VL models on CERTAIN-LYUNCERTAIN. As shown in Table 3, we observe that these models including GPT-4V (despite the questions generated with it) perform poorly on our benchmark. It is also worth noting that all"}, {"title": "4 Related Work", "content": "Abstention. Early studies in abstention primarily focused on the notion of confidence estimation in predictions, allowing to abstain when uncertain [34, 35, 36]. Recent works used selective prediction approaches to particularly improve reliability under domain shift [37] and with adversarial inputs [38]. Another promising direction involves extracting additional evidences by iteratively accumulating context [39, 40, 41, 42, 43], rephrasing underspecified questions [44], probing through code [45, 46, 47]. Unlike our work, these approaches aim to reduce the risk of incorrect predictions despite having definitive answers, without addressing epistemic or aleatoric uncertainties.\nHallucinations. Models tend to over-confidently hallucinate in uncertain scenarios [48, 49, 50]. There are two primary techniques for hallucination detection [51] \u2013 at token-level [52, 53, 54, 55] and sentence-level [56, 57, 58, 59]. We aim to reduce the confidence of hallucinatory responses at the answer-level. Similar to extracting additional evidence, hallucination mitigation strategies use retrieval-based approaches [60, 54, 61], which condition outputs on factual data by using external knowledge sources, particularly helps increase reliability on the extraneous category.\nEvaluation. Standard accuracy or generation metrcics such as BLEU are insufficient to evaluate the confidence of open-ended answer generation. To assess the semantic possibilities the LAVE metric [25] was introduced to fully or partially score the predicted answer based on their overlap with the ground truth. Expected Calibration Error (ECE) measures the accuracy of probability estimates in representing true correctness likelihood. More recent approaches also rely on object detection [62, 63, 64, 65, 66] or entailment (Faithscore) [67] to measure hallucinations. However, none of these metrics directly indicate the confidence of the model predictions. In this work, we build upon LAVE accuracy by introducing confidence-weighted accuracy, which better correlates with ECE.\nDatasets. Most standard multimodal benchmarks focus on clear, definitive answers or partial hallucinations [68, 69] for discriminative [64, 70, 71] or generative tasks [30, 67]. In contrast, CERTAINLY UNCERTAIN targets scenarios where being underconfident or responding with IDK is the correct response. Similar concurrent efforts for text-only benchmarks [59, 72, 73] are widely explored. Generating counterfactual instruction text data [74] is the closest equivalent of LRV data [30] which includes positive (or negative) instructions about objects or attributes present (or absent) in the image. We also use the MMInstruction [75] with preference annotations for helpfulness, faithfulness and ethical considerations. Finally, we generate model-dependent refusal datasets automatically which is explored by [18] to adapt to multimodal R-tuning. Our experiments show that these datasets are insufficient for benchmarking or improving multimodal epistemic and aleatoric awareness."}, {"title": "5 Conclusions and Future Work", "content": "Acknowledging uncertainty in responses and appropriately responding with \u201cI don't know\u201d (IDK) is crucial for the reliability and trustworthiness of VLMs. In this work, we introduce a new taxonomy specifically designed to handle epistemic and aleatoric uncertainty in multimodal systems. Based on this taxonomy, we present a new benchmarking dataset, CERTAINLYUNCERTAIN, and demonstrate that current VLMs lack self-awareness of these uncertainties. Empirical results show that fine-tuning with our data leads to performance gains, particularly on our held-out test set, existing refusal-based benchmarks, and some hallucination-based benchmarks, all while maintaining performance on standard benchmarks. Additionally, we propose a new confidence-weighted accuracy metric that combines predictive performance with the confidence of the prediction, showing strong correlations"}, {"title": "A Limitations", "content": "While our CERTAINLY UNCERTAIN covers various categories of multimodal uncertainty, and showed improvements over the base model when finetuned with it, there are potential limitations to be acknowledged. Though our synthetic data is rigorously quality-checked, it is possible that the synthetic generation pipeline may not capture all the nuances of real-world uncertain scenarios. Additionally, the most effective way to improve model performance on our benchmark currently is SFT with LoRA, which is more resource-intensive compared to techniques such as selective prediction that makes decisions based on the prediction probabilities during inference. Moreover, providing a reasonable or best guess based on existing knowledge can be more suitable than either answering or abstaining, which we leave as future work."}, {"title": "B Broader Impact", "content": "Current models are incentivized to predict definitive answers even in uncertain scenarios. This can lead to outputs with unwarranted confidence, which is particularly problematic in high-stakes applications such as medical diagnosis or financial forecasting. This tendency can result in misleading information and erroneous decisions. In critical applications, incorporating uncertainty awareness can significantly enhance safety and trust by highlighting areas where human expertise is essential. Our proposed taxonomy and data creation pipeline can be adapted to various scenarios, provided domain-specific inpainting techniques are available. Additionally, when models are trained with CERTAINLY UNCERTAIN, it can facilitate more efficient resource allocation, as models can identify when additional data or analysis is required, ultimately leading to more robust and trustworthy models. Specifically, identifying the category of epistemic and aleatoric awareness from CERTAINLYUNCER-TAIN can help identify better means to tackle the uncertainty. Finally, our confidence-weighted metric allows for comprehensive performance evaluation across a wide range of domains, encompassing both unimodal and multimodal scenarios."}, {"title": "C Samples visualizing CERTAINLYUNCERTAIN benchmark", "content": "We visualize some samples from each fine-grained category of the epistemic and aleatoric awareness. For the category of extraneous, our data is made of samples where the answer differs for the same question when the image is perturbed. For the rest of the categories, the dataset contains samples where the same image is paired with answerable and unanswerable questions.\nFigure 6 shows the category of knowledge awareness; as we can see the unanswerable questions ask about information that is hard to identify from the context of the image and requires additional knowledge. Similarly, Figure 7 shows examples from the complexity awareness in the epistemic category. The unanswerable questions are too tedious to arrive at an answer while the answerable questions still require some efforts, such as counting but is not laborious to answer."}, {"title": "D Samples visualizing predictions and confidence-weighted metric", "content": "Our proposed confidence-weighted accuracy takes into account the prediction probability and the correctness of the predicted answer to give a holistic score. Figure 11 presents the visualization of model predictions and the corresponding $LAVE_{idk}$ accuracy, Ppred and confidence-weighted accuracy. We show that the proposed confidence-weighted accuracy gives less score for a correct answer with lower confidence, and penalizes more for an incorrect answer with higher confidence. In addition, our visualization shows that Qwen-VL-Chat [29] is able to say equivalents of \u201cI don't know\" more confidently from (a) and (b), after continued finetuning on our data with SFT-LORA.\nExamples (a) and (b) show cases where the base model is less confident for a correct answer. Our metric gives a partial score for the correctness owing to the prediction probability. After finetuning, as the prediction probability of the correct answer increases, our confidence-weighted accuracy increases accordingly. In case (c), the base model predicts an incorrect answer with high confidence. Our metric penalizes this more heavily with a high negative score. After finetuning, the prediction is rectified and the scores are adjusted accordingly. In the case of (d), the base model predicts the incorrect answer but with low confidence. Our metric still gives a negative score but penalizes less compared to (c). The cases of (c) and (d) differentiate answering incorrectly with high and low probabilities respectively.\nMoreover, after finetuning with our CERTAINLYUNCERTAIN, we see the corrected predictions with relatively higher probabilities for correctness, which are reflected in our confidence-weighted metric score. These probabilities of the model predictions are not reflected in the $LAVE_{idk}$ accuracy."}, {"title": "E Additional Results", "content": "LLaVA with LoRA-SFT. We include results with LoRA-SFT on LLaVA-v1.5-7b in Table 6, which show consistent performance improvement when trained with our data.\nComparing 7B to 13B models. We conduct experiments to study the performance of a larger model across different uncertainty awareness categories. These results are presented in Table 7.\nWe observe consistent performance improvements over LLaVA-1.5-7B-LoRA and LLaVA-1.5-13B-LoRA [27] with the augmentation of CERTAINLYUNCERTAIN during the instruction-tuning phase. When instruction-tuned with only our data (i.e., Ours-only), compared to the results on the 7B-LORA model, a larger model 13B-LORA only marginally improves on confidence-weighted accuracy and"}, {"title": "F Implementation Details", "content": "For Thresholding baselines, we perform grid search among (0.1, 0.2, ...0.9) and (0.91, 0.92, ...0.99) to decide the optimal threshold for each split. The latter range is included, as we observe that the models are often over-confident in their own predictions.\nFor SFT/Instruction-tuning with LoRA, we follow the instructions provided by Qwen-VL and LLaVA official implementations, with exactly the same setting of learning rate and LoRA configurations. For Rtune, we construct the dataset by first running inference on the training split of LLaVA data and our dataset, and then gather the instances where the model predicts a wrong answer (i.e., receives a LAVE accuracy of 0). With the constructed dataset, we tune Qwen-VL with the same training configuration as SFT. For DPO, we follow the implementations of Silkie [75].\nAll experiments are conducted with V100s on Microsoft Azure [77], adopting mixed-precision training with DeepSpeed [78] stage 3. To match the batch size suggested in official implementations, we train the models on 64 V100s for 1 epoch with a batch size of 2 per GPU.\nFor evaluation on Vizwiz, we first use LAVE refusal prompt to judge whether the prediction is IDK. If so, we convert the answer to \"unanswerable\" and use the standard VQA-based VizWiz evaluation."}, {"title": "G Additional Details on Data Creation", "content": "G.1 More Details on sourcing from image\nThe masks of salient objects are generated by Grounded-SAM [21] with box_threshold of 0.3 and text_threshold of 0.25. The mask is dilated with kernel size 20 and then input to LaMa inpainting model [22] to remove the object.\nFor VQA images, we use GPT-4 to first identify the salient objects given the question-answer pairs, which will use as text queries to Grounded-SAM.\nFor GQA images, we identify objects in the scene graphs that is associated with a question as the salient object. Then we traverse the scene graphs to find all other objects with the same label. Since GQA also offers groundtruth bounding box (bbox) annotations, we use the mask generated by Grounded-SAM from GT bbox, following by inpainting to remove all such objects. In this way, the same question becomes unanswerable for the perturbed image, and we replace the answer with IDK answers by randomly sample from (1) \u201cI don't know.\u201d; (2) \u201cI don't see any [Object].\u201d; (3) \u201cThere is no [Object] in the image.", "I can't see any [Object].": "nG.2 Prompts for Data Creation\nHere are the prompts for generating data in epistemic and aletoric subcategories with GPT-4 or GPT-4V.\nKnowledge (Epistemic Awareness)\nYou are given a descriptive caption of an image. Generate a knowledge based answerable and an unanswerable question from the cation. An unanswerable question requires external knowledge or commonsense that is not explicitly absent in the image to answer the question. An answerable question requires commonsense knowledge not present in the image pixels but can be answered from the context. Make the unanswerable and answerable questions as similar to each other as possible yet one is answerable and the other is unanswerable. Here are some examples:\nCaption: In the center of the image, a vibrant blue lunch tray holds four containers, each brimming with a variety of food items. The containers, two in pink and two in yellow, are arranged in a 2x2 grid. In the top left pink container, a slice of bread rests, lightly spread with butter and sprinkled with a handful of almonds. The bread is cut into a rectangle, and the almonds are scattered across its buttery surface. Adjacent to it in the top right corner, another pink container houses a mix of fruit. Sliced apples with their fresh white interiors exposed share the space with juicy chunks of pineapple. The colors of the apple slices and pineapple chunks contrast beautifully against the pink container. Below these, in the bottom left corner of the tray, a yellow container holds a single meatball alongside some broccoli. The meatball, round and browned, sits next to the vibrant green broccoli florets. Finally, in the bottom right yellow container, there's a sweet treat - a chocolate chip cookie. The golden-brown cookie is dotted with chocolate chips, their dark color standing out against the cookie's lighter surface. The arrangement of these containers on the blue tray creates a visually appealing and balanced meal, with each component neatly separated yet part of a cohesive whole.\nUnanswerable Q: How many calories in this meal?\nAnswer: Unanswerable\nAnswerable Q: Which cuisine is the meal?\nA: English meal\nCaption: This image captures a fascinating scene in a dense jungle. Two majestic, gray ele-phants are the main subjects of the photo. They are carrying people on their backs, who are seated in wooden seats and wearing helmets for safety. The elephants are walking in a line, one following the other, on a path that cuts through the lush greenery of the jungle. The photo is taken from a higher vantage point, providing a bird's eye view of the elephants and their verdant surroundings. The dense foliage and towering trees of the jungle envelop the path, creating a sense of adventure and exploration.\nUnanswerable Question: What are the relationships between the people on the elephants?\nAnswer: Unanswerable\nAnswerable Question: Who are the people on the back of the elephants?\nAnswer: Most likely tourists\nKeep in mind that you should make your question more natural, meaning that the question is plausible to be asked by a human.\nPlease generate an unanswerable question and an answerable question for the given caption, in the following format:\nQ1: <Unanswerable question>\nA1: <answer to Q1>\nQ2: <Answerable question>\nA2: <answer to Q2>\nDO NOT ask about anything that is difficult to observe or learn even with external knowl-edge, such as the exact time, exact location, the exact thought of someone, or the conversation or the topic of conversation between people. If you can only come up with such a question, put \"Not a good question\" for A1.\nComplex (Epistemic Awareness)\nYou are given a caption of an image. Generate unanswerable questions that asks about an existing object in the image, but is too complex even for humans to answer. The unanswerable question should be extremely difficult in framing or tedious to infer the answer. The answerable question should have a convoluted framing but should have an accurate and direct answer.\nHere are some examples:\nCaption: This image captures a serene moment in a zoo enclosure, where two majestic gi-raffes are seen in their natural behavior. The giraffes, adorned in their distinctive brown and white patterns, stand tall against the backdrop of lush green trees. On the left, one giraffe is actively engaged in a meal, its long neck extended towards the tree as it munches on the verdant leaves. Its companion on the right stands leisurely next to a tree trunk, perhaps taking a break from its own leafy feast. The enclosure they inhabit is grassy and spacious, providing them with ample room to roam and forage. The trees dotting the enclosure not only offer a source of food but also create a naturalistic habitat for these towering creatures. In summary, this image is a snapshot of life in a zoo, showcasing the grace and beauty of giraffes in an environment designed to mimic their wild habitats.\nUnanswerable Question: How many tree leaves are seen in the image?\nAnswer: Unanswerable\nAnswerable Question: How many animal legs are present?\nAnswer: 8 legs of 2 girraffes\nCaption: This image captures a fascinating scene in a dense jungle. Two majestic, gray ele-phants are the main subjects of the photo. They are carrying people on their backs, who are seated in wooden seats and wearing helmets for safety. The elephants are walking in a line, one following the other, on a path that cuts through the lush greenery of the jungle. The photo is taken from a higher vantage point, providing a bird's eye view of the elephants and their verdant surroundings. The dense foliage and towering trees of the jungle envelop the path, creating a sense of adventure and exploration.\nUnanswerable question: What are the interactions of the individuals on the elephants' backs with the environment?\nAnswer: Unanswerable\nAnswerable question: A couple of living beings are carrying another couple of living beings. What are the latter living beings?\nAnswer: Humans\nIMPORTANT: COMPLEXITY OF THE QUESTION SHOULD BE ONLY AND ONLY BASED ON DIFFICULTY TO ANSWER OR FRAMING OF THE QUESTION. THEY SHOULD NOT REQUIRE ADDITIONAL INFORMATION.\nPlease generate an unanswerable question and an answerable question for the given caption, in the following format:\nQ1: <unanswerable question>\nA1: <answer to Q1>\nQ2: <answerable question>\nA2: <answer to Q2>\nFor the extraneous category, we first identify the noun phrases that are most relevant to the answer, so that the absence of this object would make it difficult to answer the question. We then mask out the object using Grounded-SAM and inpaint the mask to obtain a perturbed image. Following this, we provide the original and the perturbed image and prompt GPT-4V to generate a question that is answerable for only one of the images.\nIdentification of salient objects for extraneous (Epistemic Awareness)\nYou are given a question and an answer based on an image. Return the most relevant object in the image that the question is asking about.\nThere are some policies to follow:\n1. The most relevant object should be the one that when removed from the image, the question would become unanswerable. Here are some examples:\n\"question\": \"What is the color of the car?\", \"answer\": \"red\"\nRelevant object: red car\n- \"question\": \"What objects are reflected?\", \"answer\": \"trees\" Relevant object: trees\n- \"question\": \"What brand of bike can you see?\", \"answer\": \"yamaha\"\nRelevant object: yamaha bike\n- \"question\": \"What is stopping the animals from running away?\", \"answer\": \"wall\"\nRelevant object: wall\n2. Remember that are limitations in removing object from the image. If the question is re-garding the overall presentation of the image, it is impossible to masking out the whole image, so the answer should be na. For example,\n- \"question\": \"Is this picture taken during the day or night?\", \"answer\": \"day\"\nRelevant object: na\n- \"question\": \"Is this a house kitchen or a restaurant kitchen?\", \"answer\": \"restaurant\"\nRelevant object: na Don't over do it for policy 2, for example,\n- \"question\": \"Is the rider a child or an adult?\", \"answer\": \"adult\" Relevant object: adult rider\n3. Imagine that even after masking the most relevant object, the question can still be answered, then the answer should be na. For example,\n- \"question\": \"What is the woman standing on?\", \u201canswer\u201d: \u201cfloor\"\nRelevant object: na\nReasoning: we can still reason that she is standing on the floor, given the rest of the context of the image\n- \"question\": \"What is the person standing on?\", \u201canswer\u201d: \"ski\"\nRelevant object: na\nReasoning: we can still reason that he or she is standing on snow, given the rest of the context of the image\n4. In the case that there are rich descriptions about the object mentioned in the question, the answer should be the most relevant object that is mentioned in the question, and please try keep the decription intact. For example,\n- \"question\": \"What does the sign on the door on the bottom right say?\", \"answer\": \"caution\"\nRelevant object: the caution sign on the door on the bottom right\n\"question\": \"What stuffed animal is the child in the red jacket holding?\u201d, \u201canswer\": \"teddy bear\"\nRelevant object: teddy bear that the child in the red jacket is holding\n5. When the question can be answered, regardless of what is in the image\n- \"question\": \"Glasses assist in helping what organ?\", \"answer\": \"eyes\u201d\nRelevant object: na\n6. For questions that are general, please evaluate how often there might be multiple objects belonging to the same category appearing in a scene, and return the most plausible answer. For example,\n- \"question\": \"What food is presented?\", \"answer\": \"sandwich\" Relevant object: \"food\"\n- \"question\": \"What is being eaten?\u201d, \u201canswer\": \"sandwich\" Relevant object: \"food\"\nPrompt to generate Extraneous category (Epistemic Awareness)\nYou are given a pair of very similar images. In image 2, there is a specific object that is missing or changed from image 1. Generate a question that is answerable for image 1 while not answerable for image 2.\nThere are a few rules to follow for each question:\n1. The question should be answerable for image 1, that is there is a definitive answer to the question, just by looking at image 1.\n2. The question should not be answerable for image 2. \"Not answerable\" means, just by looking at image 2, the answer would be something like \u201cI don't know\u201d, \u201cI don't see SOMETHING\" or \"Nothing\". For example,\n- If the question is \u201cWhat color is the car?\u201d, and there is no car in image 2, the answer should be \u201cI don't see a car\".\n- If the question is \"What is on the man's head\", and there is nothing on the man's head in image 2, the answer should be \"Nothing\".\n- If the question is asking about something that cannot be seen clearly in image 2, the answer should be \"I don't know\".\n- Try not to ask questions about the presence of an object, but rather about the properties of the object. For example, instead of asking \"Is there a car in the image?\", ask \"What color is the car?\". Instead of asking \"How many people are there?", "What is the person wearing?\".\n3. The question should be relevant to the content of each image alone, even without seeing the other image.\nThe response should be formatted as": "n- Q: <question>\n- A1: <answer for image 1>\n- A2: <answer for image 2", "I don't know\", \"I don't see xxx\" or \"Nothing\". Try not to refer to the answer for image 1>\nAmbiguous (Aleotoric Awareness)\nYou are given a caption of an image. Generate unanswerable questions that asks about an existing object in the caption, but is ambiguous.\nDEFINITION": "Ambiguity refers to a situation or statement that can be understood or interpreted in multiple ways. It often involves uncertainty or lack of clarity", "examples": "nCaption: This image captures a serene moment in a zoo enclosure, where two majestic gi-raffes are seen in their natural behavior. The giraffes, adorned in their distinctive brown and white patterns, stand tall against the backdrop of lush green trees. On the left, one giraffe"}]}