{"title": "Provably Efficient Action-Manipulation Attack\nAgainst Continuous Reinforcement Learning", "authors": ["Zhi Luo", "Xiyuan Yang", "Pan Zhou", "Di Wang"], "abstract": "Manipulating the interaction trajectories between the intelligent agent and the\nenvironment can control the agent's training and behavior, exposing the potential\nvulnerabilities of reinforcement learning (RL). For example, in Cyber-Physical Sys-\ntems (CPS) controlled by RL, the attacker can manipulate the actions of the adopted\nRL to other actions during the training phase, which will lead to bad consequences.\nExisting work has studied action-manipulation attacks in tabular settings, where the\nstates and actions are discrete. As seen in many up-and-coming RL applications,\nsuch as autonomous driving, continuous action space is widely accepted, however,\nits action-manipulation attacks have not been thoroughly investigated yet. In this\npaper, we consider this crucial problem in both white-box and black-box scenarios.\nSpecifically, utilizing the knowledge derived exclusively from trajectories, we\npropose a black-box attack algorithm named LCBT, which uses the Monte Carlo\ntree search method for efficient action searching and manipulation. Additionally,\nwe demonstrate that for an agent whose dynamic regret is sub-linearly related to the\ntotal number of steps, LCBT can teach the agent to converge to target policies with\nonly sublinear attack cost, i.e., $O (R(T) + MH^{3}KE \\log(MT)) (0 < E < 1)$,\nwhere H is the number of steps per episode, K is the total number of episodes,\nT = KH is the total number of steps, M is the number of subspaces divided in\nthe state space, and R(T) is the bound of the RL algorithm's regret. We conduct\nour proposed attack methods on three aggressive algorithms: DDPG, PPO, and\nTD3 in continuous settings, which show a promising attack performance.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) aims to maximize the accumulated rewards through the interaction\nbetween the agent and the environment. With the development of RL, it has demonstrated outstanding\nperformance in areas like robot control [34, 7], autonomous system [22, 12], healthcare [25, 36],\nfinancial trading [8, 1] and etc. Nowadays, to accommodate more application scenarios, new\nalgorithms such as DDPG [15] have been developed by researchers, extending the applicability of\nRL from discrete action space to continuous action space. However, the inherent vulnerability of\nRL might be exploited by attackers in this new setting [11]. Therefore, studying RL's potential\nweaknesses is essential for constructing a secure RL system.\nApart from tampering with observations [5, 37, 35, 24, 2, 16], rewards [20, 38, 19, 10, 21, 32], or\nthe environment [33, 29, 9, 3], in this paper, we focus on attacks on the action space. For example,\nin Cyber-Physical Systems (CPS), during the training process, by tampering with the action signals\noutput by the RL-based controller, the subsequent states and rewards are affected, thereby influencing"}, {"title": "2 Related Work", "content": "The work [17] explored action-manipulation attacks against RL in a discrete action space and provided\nan attack cost bound, but it is not applicable in a continuous setting. [14] and [27] formalized the attack\nas an adversarial optimization problem, and used projected gradient descent (PGD) to manipulate\nactions. However, these two works require prior knowledge such as the model or specific algorithm\nused by the agent. The attacks in [13] take place at test-time, where the trained agent and the\nenvironment are viewed as a new environment, and another adversarial DRL agent is trained to\nmanipulate actions, but it can not change the policy of the agent itself and requires query permissions"}, {"title": "3 Preliminaries and Problem Formulation", "content": "Notations and Preliminaries: This paper considers a finite-horizon MDP over continuous do-\nmain. Such an MDP can be defined as a 6-tuple $M = (S, A, H, P, R, \\mu)$, where S and A\nare respectively bounded continuous state and action spaces, H denotes the number of steps\nper episode, P is the transition kernel, $R^{h}: S \\times A \\rightarrow [0,1]$ represents the reward function\nat step hand $\\mu$ is the initial state distribution. Define K as the total number of episodes and\n$T = KH$ as the total number of steps. For each episode $k \\in [K]$, a trajectory $\\gamma \\sim \\pi$ gener-\nated by policy $\\pi$ is a sequence $\\{s_{1},a_{1},r_{1}, s_{2}, a_{2}, r_{2}, ..., s_{H}, a_{H},r_{H},s_{H+1}\\}$. A policy $\\pi$ can be\nevaluated by the expected rewards. Formally, we define $V_{\\pi}^{h}(s) = E[\\sum_{h'=h}^{H}r_{h'}|s_{h} = s, \\gamma \\sim \\pi]$ and\n$Q_{\\pi}^{h}(s,a) = E[\\sum_{h'=h}^{H}r_{h'}|s_{h} = s, a_{h} = a, \\gamma \\sim \\pi]$. For notation simplicity, we denote $V_{\\pi}^{H+1} = 0$,\n$Q_{\\pi}^{H+1} = 0$ and $P^{h}V_{\\pi}^{h+1}(s,a) = E_{s'\\sim P^{h}(.\\vert s,a)}[V_{\\pi}^{h+1}(s')]$. A deterministic policy is a policy that\nmaps each state to a particular action. For a deterministic policy $\\pi$, we use $\\pi^{h}(s)$ to denote the action\na which satisfies $\\pi^{h}(a\\vert s) = 1$.\nWe assume that H is finite, and because $r_{h} \\in [0, 1]$, the Q-values and V-values are bounded. The\npurpose of RL is to find the optimal policy $\\pi^{*}$, which gives the optimal value function $V_{h}^{*}(s) =$\n$V^{*}(s) = \\sup_{\\pi} V_{\\pi}^{h}(s)$ and $Q_{h}^{*}(s, a) = Q^{*}(s,a) = \\sup_{\\pi} Q_{\\pi}^{h}(s, a)$ for all $s \\in S$, $a \\in A$ and\n$h \\in [H]$. To assess an RL algorithm's performance over K episodes, we quantify a metric known as\nregret, which is defined as: $Regret(K) = \\sum_{k=1}^{K}[V_{\\pi^{*}}^{1}(s_{1}^{k})-V_{\\pi_{k}}^{1}(s_{1}^{k})]$.\nThreat Model Description: The attacker aims for the agent to learn a deterministic target policy\n$\\pi^{\\dagger}$. However, in a continuous action space that contains infinitely many policies, it is difficult for the\nagent to learn a specific policy exactly. In this study, the attacker expects the agent to learn a policy\nthat approaches the target policy. The attacker measures the similarity of policies based on the degree\nof closeness of actions produced in the same state. Therefore, the attacker sets a dissimilarity radius\nof $r_{a}$, which requires that the dissimilarity between the actions produced by the agent's learned policy\nand the target policy does not exceed $r_{a}$ w.r.t a distance function $l$ employed by the attacker (e.g.\nEuclidean distance). Define $A^{h}(s) = \\{a : l(a, \\pi^{\\dagger}(s)) < r_{a}\\}$ as the target action space for state s.\nIntuitively, $A^{h}(s)$ is the set of acceptable malicious actions for the adversary. According to the target\naction space, we define the target policy set as $\\Pi^{\\dagger} = \\{\\pi : \\pi^{h}(s) \\in A^{h}(s), \\forall s \\in S, h \\in [H]\\}$, which\nrepresents the set of acceptable malicious policies. We describe the threat model in this paper from\nthe perspective of attacker's knowledge, capability, and goal."}, {"title": "4 Attack Strategy and Analysis", "content": "4.1 White-box Attack\nIn the white-box setting, the attacker has comprehensive knowledge of the MDP $\\mathcal{M}$. Therefore, the\nattacker can compute the Q and V-values of any policy, and the action $a_{\\pi^{0}}^{h}(s)$ is known to the attacker.\nIn order to make the agent learn the policies in $\\Pi^{\\dagger}$, the attacker can mislead the agent into believing\n$\\pi^{0}$ is the optimal policy. We now introduce an effective oracle attack strategy. Specifically, at the\nstep h, if the action selected by the agent is within the target action space, i.e., $a_{h} \\in A^{h}(s_{h})$, the\nattacker does not launch an attack, i.e., $\\tilde{a}_{h} = a_{h}$. Otherwise, the attacker launches an attack and\nsets $\\tilde{a}_{h} = a_{\\pi^{0}}^{h}(s_{h})$ as the worst action. In earlier $K_{0}$ episodes, the attacker does not carry out any\nattacks, and $K_{0}$ can be set to 0, or a number of order $o(K)$. Under non-attack conditions, there is\ncontinuity between the Q-values of actions in $A^{h}(s_{h})$ and those of other actions (the oracle attack\ncan greatly disrupt this continuity). Therefore, an appropriate $K_{0}$ value can enhance the hit rate of\nthe RL algorithm for the target action space during the initial exploration phase, which is beneficial\nfor learning the policies within $\\Pi^{\\dagger}$ in the later stages. This becomes particularly evident when the\nproportion of the target action space $A^{h}(s_{h})$ in the total action space A is quite small. The detailed\nprocedure of the oracle attack algorithm is delineated in Appendix D. Then, we have:\nLemma 1. * If $\\Delta_{min} > 0$ and the attacker follows the oracle attack scheme, then from the agent's\nperspective, $\\pi^{0}$ is the optimal policy.\nThe detailed proof is included in Appendix H. With the condition $\\Delta_{min} > 0$, the upper bounds of $|\\mathcal{T}|$\nand $|\\alpha|$ under the oracle attack can be obtained. The detailed proof is included in Appendix I.\nTheorem 1. In the white-box setting, with a probability at least $1 - \\delta_{2}$, the oracle attack will force\nthe agent to learn the policies in $\\Pi^{\\dagger}$ with $|\\mathcal{T}|$ and $|\\alpha|$ bounded by\n$|\\mathcal{T}| \\le \\frac{Regret(K) + 2H^{2} \\sqrt{\\ln(1/\\delta_{2})} \\cdot Regret(K)}{\\Delta_{min}}$,\nand $|\\alpha| < |\\mathcal{T}| + HK."}, {"title": "4.2 Black-box Attack", "content": "In the black-box attack scenario, the attacker has no knowledge of the underlying MDP process and\nthe algorithm used by the agent but only knows the interaction information: $s_{h}, a_{h}$, and $r_{h}$. Without\nthe knowledge of $a_{\\pi^{0}}^{h}(s)$, the attacker can not determine $\\tilde{a}_{h}$ and fails to attack.\nTo counteract this issue, in our proposed attack, the attacker can employ a technique that methodically\nidentifies and approximates $a_{\\pi^{0}}^{h}(s)$. In particular, an infinite Q-value tree structure is developed\nto encapsulate the action space A, facilitating cluster-level analysis thereby reducing the scale of\nactions being analyzed. Meanwhile, we use important sampling to calculate the Q-values of $\\pi^{0}$ and\nuse Hoeffding's inequality to establish the LCB of the Q-values. The attack algorithm has been"}, {"title": "5 Numerical Results", "content": "In this section, we conduct the oracle attack and the LCBT attack on three aggressive continuous RL\nalgorithms, including DDPG, PPO, and TD3, in three different environments. The experiments are run\non a machine with NVIDIA Quadro RTX 5000. Environment 1 involves a one-dimensional continuous\ncontrol problem with $s \\in [-1,1]$ and $a \\in [-1,1]$ where a slider moves on a rail, receiving positive\nrewards proportional to the move distance, with a negative reward when it falls off. Environment 2\ndescribes a two-dimensional continuous control problem with $s \\in [0, 8]^{2}$ and $a \\in [-1,1]^{2}$ where a\nvehicle moves on a two-dimensional plane, receiving rewards negatively proportional to the distance\nfrom the central point for every step. Environment 3 is a five-dimensional version of Environment\n2 with $s \\in [0,8]^{5}$ and $a \\in [-1,1]^{5}$. A detailed description of the environments can be found in\nAppendix F. The target policy $\\pi^{\\dagger}$ is trained by constraining the movement range of both the slider\nand vehicle. In Appendix G, we present the experimental results of Environment 3.\nTraining Phase: In Fig. 2 (a, b) and Fig. 3 (a, b), the target policy reward (- - -) is the average reward\nachieved by the $\\pi^{\\dagger}$ over $10^{3}$ episodes. The convergence of rewards obtained by the agent under three\ndifferent conditions (attack-free, LCBT attack, and oracle attack) is represented by the remaining\nthree curves. Experimental results demonstrate that the reward of the trained policy converges to the\nreward of the target policy under the oracle attack and the LCBT attack. In Fig. 2 (c) and Fig. 3\n(c), the cost is sublinear to time steps in both the oracle attack and the LCBT attack, which is in line\nwith our theoretical expectations. Meanwhile, as the attack progresses, after some exploration, the\neffectiveness of LCBT will also approach that of the oracle attack. The experimental results reveal"}, {"title": "6 Conclusion", "content": "In this study, we investigated the action-manipulation attack to disrupt RL in continuous state and\naction spaces. We modeled the attack under continuous settings and introduced the concepts of the\ntarget action space and the target policy set. Based on the knowledge level of the attacker, we studied\nwhite-box and black-box attacks, and provided theoretical guarantees for the efficiency of the attacks."}, {"title": "A Notation Table", "content": null}, {"title": "B Limitations and Broader Impacts", "content": "Limitations: The black-box attack method proposed in this paper depends on the binary tree to divide\nthe action space, which might result in a slight performance loss in a high-dimensional environment."}, {"title": "C Related works", "content": "We elucidate the work related to the security of RL from the perspectives of different attack methods.\nObservation manipulation: [5] employs observation manipulation to cause agent misbehavior only\nat specific target states. [37], [35], and [24] minimize the total rewards of the intelligent agent using\nthe observation manipulation method. Moreover, [2] utilizes the method of observation manipulation\nto achieve policy induction attacks. [16] aims at minimizing the agent's reward or luring the agent to\na designated target state through observation manipulation.\nReward manipulation: [20], [38], [19] and [10] employ reward manipulation to train the agent to\nlearn a specified policy, while [21] focuses on minimizing the total rewards of the agent using this\nattack approach. In the study by [32], reward tampering attacks were implemented in multi-agent RL,\nthus inducing multiple agents to follow the target policy.\nEnvironment manipulation: [33] train agents to use a target policy through environment manipula-\ntion. [29] and [9] minimize the total rewards of the agent through environment manipulation. In [3],\nthe environment is manipulated to cause a controller failure in autonomous driving."}, {"title": "D The Oracle Attack Algorithm", "content": null}, {"title": "E Assumption", "content": "Assumption 1. We assume that there exist constants $v_{1}, 0 < \\rho < 1, L_{s}$ and $\\delta_{s}$ such that:\n(a) $diam_{a}(P_{D,I}) \\le v_{1}\\rho^{D}$ for all node $(D, I)$,\n(b) $L_{s}\\delta_{s} < \\frac{\\Delta_{min}}{2}$, and for all $S_{m}, \\forall m \\in [1, M]: diam_{s}(S_{m}) \\le L_{s}\\delta_{s}$,\n(c) $\\forall a, a' \\in A, s \\in S, h \\in [1, H], |Q^{h}(s, a) - Q^{h}(s, a')| \\le l_{a}(a, a')$,\n(d) $\\forall S_{i}, \\forall s, s' \\in S_{i}, a \\in A, h \\in [1, H], |Q^{h}(s, a) - Q^{h}(s', a)| \\le l_{s}(s, s')$."}, {"title": "F Experimental Settings", "content": "Environment 1. The objective of this environment, as depicted in Fig. 4, is to control the slider to\nslide on the rod and maximize the reward within $H = 10$ steps. Specifically, for any $h \\in [H]$, the\nstate space $s \\in [-1,1]$ represents the current position of the slider on the rod, where 0 represents\nthe center point of the rod. The action space $a \\in [-1,1]$ represents the base sliding distance, with\nnegative values signifying leftward movement and positive values signifying rightward movement.\nThe final sliding distance $d = a * 2$, and a reward of $|a|$ is obtained for each step taken. If the slider\nfalls off the rod (i.e., the slider's position coordinate is outside of [-1,1]), the round of the game ends\nimmediately and a reward of -1 is received. The slider's initial position $s_{1}$ is arbitrarily set between\n-0.7 and 0.7. Our target policy $\\pi^{\\dagger}$ for the slider is to move only within the interval $[-0.7, 0.7]$. We\ngenerate the target policy model by training and obtaining the optimal policy within the limited\ninterval $[-0.7, 0.7]$ through augmentation of the environment constraints. The optimal policy is then\nemployed to act as the target policy. Clearly, $\\pi^{\\dagger}$ is neither a globally optimal policy nor the worst\npolicy.\nEnvironment 2. As depicted in Fig. 5, the objective of this environment is to control a vehicle to\nmove on a two-dimensional plane surrounded by a boundary and obtain maximum reward within\n$H = 10$ steps. For any $h \\in [H]$, the state space of Environment 2 includes the current two-\ndimensional position of the vehicle, denoted as $s = (s_{1}, s_{2}) \\in [0, 8]^{2}$. The action space, denoted as\n$a = (a_{1}, a_{2}) \\in [-1,1]^{2}$, denotes the distance of the vehicle's movement. The vehicle's next position\nis $(s_{1} + a_{1}, s_{2} + a_{2})$ and the reward gained is negatively linearly proportional to the distance $d$\nbetween the vehicle and the center point (the yellow dot at [4, 4]). The closer the distance, the higher\nthe reward. The initial position $s_{1}$ of the vehicle is at any point outside the red circle (d > 1). Our\ntarget policy $\\pi^{\\dagger}$ for the vehicle is to move only outside the red circle. Similarly, we train and obtain\nthe optimal policy that only moves outside the red circle by increasing the environmental constraints,\nwhich serves as the target policy.\nEnvironment 3. Environment 3 is a five-dimensional version of environment 2, where the goal\nremains the same: to control an object to move as close as possible to the center point within\n$H = 10$ steps to obtain maximum reward. The state space is denoted as $s = (s_{1}, s_{2}, s_{3}, s_{4}, s_{5}) \\in$\n$[0,8]^{5}$, the action space as $a = (a_{1}, a_{2}, a_{3}, a_{4}, a_{5}) \\in [-1,1]^{5}$, and the next state is given by\n$(s_{1} + a_{1}, s_{2} + a_{2}, s_{3} + a_{3}, s_{4} + a_{4}, s_{5} + a_{5})$. The center point is located at [4,4,4,4,4]. The\nattacker's target policy $\\pi^{\\dagger}$ is defined as getting as close to the center point as possible without entering\nthe range less than 1 unit distance from the center point."}, {"title": "G Additional Numerical Results", "content": "In these three tables, the similarity calculation formula we adopt is as follows:\n$sim = \\frac{\\sum (I \\{l(a_{1}, a_{2}) \\le r_{a}\\})}{\\textit{steps}}$"}, {"title": "H The Proof of Lemma 1", "content": "In the action-manipulation settings, the attacker sits between the agent and the environment. We\ncan regard the combination of the attacker and the environment as a new environment. For the new\nenvironment, we represent the Q-value and V-value as $\\hat{Q}$ and $\\hat{V}$. Because the attacker does not\nlaunch an attack when the agent chooses an action within the target action space, we have\n$\\hat{V}^{h}(s) = V^{h}(s) \\geq \\hat{V}_{\\pi^{0}}^{h}(s) = V_{\\pi^{0}}^{h}(s), \\forall s \\in S, h \\in [1, H], \\pi \\in \\Pi^{\\dagger}$.\nSuppose from the agent's perspective, at the step h + 1, $\\pi^{0}$ is the optimal policy. At step h, the learner\ntakes the action $a_{h} \\notin A^{h}(s)$, and the attacker manipulates it to $a_{\\pi^{0}}^{h}(s)$. According to the Bellman"}, {"title": "J Proof of Lemma 2", "content": "Lemma 2. In the LCBT algorithm, the following confidence bound:\n$|Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I}) - \\mathbb{E}[Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})]| \\leq \\frac{H-h+1}{\\sqrt{2T_{D,I}^{B,h}(k)}} \\sqrt{\\ln(\\frac{2Mk\\sum_{h'=1}^{H}|T^{h'}|}{\\delta_{1}})} \\qquad(14)$\nholds for $\\forall h \\in [H]$, $S_{m}(s \\in S_{m}, m \\in [M])$, $(D,I) \\in \\mathcal{T}^{h}, T_{D,I}^{B,h}(k) \\in [1,k]$ with a probability at\nleast $1 - \\delta_{1}$.\nThe lemma offers a confidence bound for $Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})$. According to the Algorithm 2, the\nattacker traverses the tree with smaller B-values, which means $Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})$ will converge to\n$Q^{h}(s_{k}^{h}, a_{\\pi^{0}}^{h}(s_{k}^{h}))$.\nProof: Firstly, we will transform Q-value into a non-recursive form, i.e.,\n$Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})$\n$= (1 - \\frac{1}{T_{D,I}^{B,h}(k)}) Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I}) + \\frac{1}{T_{D,I}^{B,h}(k)} (r^{h} + G_{h+1:H+1}^{k} * P_{h+1:H+1}^{k})$\n$= (1 - \\frac{1}{T_{D,I}^{B,h}(k)}) (1 - \\frac{1}{T_{D,I}^{B,h}(b,I(k)) - 1}) Q_{D,I}^{B,h}(s_{k^{b,I(k)}}^{h}, a_{D,I})$\n$+ \\frac{1}{T_{D,I}^{B,h}(k)} (r^{h} + G_{h+1:H+1}^{k} * P_{h+1:H+1}^{k})$\n$+ \\frac{1}{T_{D,I}^{B,h}(k)} (1 - \\frac{1}{T_{D,I}^{B,h}(b,I(k)) - 1}) (r^{h} + G_{h+1:H+1}^{k^{b,I(k)}} * P_{h+1:H+1}^{k^{b,I(k)}})$\n$+\\ldots + \\frac{1}{T_{D,I}^{B,h}(k)} (r_{1}^{h} + G_{h+1:H+1}^{1} * P_{h+1:H+1}^{1})$\n$\\stackrel{\\text{k}}{=} \\frac{1}{T_{D,I}^{B,h}(k)} \\sum_{i=1} \\mathbb{I}\\{s_{k^{i}}^{h} \\in S_{i(s^{h})}, a^{h} = a_{D,I}\\}(r_{i}^{h} + G_{h+1:H+1}^{i} * P_{h+1:H+1}^{i})$\nThen we have\n$\\mathbb{E} [Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})]$\n$= \\mathbb{E}[\\frac{1}{T_{D,I}^{B,h}(k)} \\sum_{i=1} \\mathbb{I}\\{s_{k^{i}}^{h} \\in S_{i(s^{h})}, a^{h} = a_{D,I}\\}(r_{i}^{h} + G_{h+1:H+1}^{i} * P_{h+1:H+1}^{i})]$\n$= \\frac{1}{T_{D,I}^{B,h}(k)} \\sum_{i=1}^{k} \\mathbb{I}\\{s_{k^{i}}^{h} \\in S_{i(s^{h})}, a^{h} = a_{D,I}\\} \\mathbb{E}[r_{i}^{h} + G_{h+1:H+1}^{i} * P_{h+1:H+1}^{i}] \\qquad(15)$\n$= Q^{h}(s^{k,h}, a_{D,I}).$\nIt should be noted that for $\\mathbb{E}[Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})]$, there exists a state $s^{k,h} \\in S_{i(s^{h})}$ such that\n$Q^{h}(s^{k,h}, a_{D,I}) = \\mathbb{E}[Q_{D,I}^{B,h}(s^{k,h}, a_{D,I})]$ holds due to the smoothness of Q-function (Assumption 1\n(d)).\nDefine the event:\n$\\xi_{k} = \\{\\forall h \\in [H], m \\in [M], (D, I) \\in \\mathcal{T}^{h}, T_{D,I}^{B,h}(k) \\in [1,k],$\n$|Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I}) - \\mathbb{E} [Q_{D,I}^{B,h}(s^{k,h}, a_{D,I})]| \\leq \\beta^{h}(T_{D,I}^{B,h}(k), \\delta_{1}) \\}$,\nwhere $\\beta$-function is calculated by\n$\\beta^{h}(N, \\delta) = \\frac{H-h+1}{\\sqrt{2N}} \\sqrt{\\ln(\\frac{2Mk\\sum_{h'=1}^{H}|T^{h'}|}{\\delta})} \\qquad(16)$."}, {"title": "K Proof of Lemma 3", "content": "Lemma 3. In the LCBT algorithm, based on the lemma 2, there exists\n$|\\mathbb{E}[Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})] - Q^{h}(s_{k}^{h}, a_{\\pi^{0}}^{h}(s_{k}^{h}))| \\leq 3\\beta^{h}(T_{D,I}^{B,h}(k), \\delta) + L_{s}\\delta_{s}. \\qquad(17)$\nThe Lemma gives the relationship between $\\mathbb{E}[Q_{D,I}^{B,h}(s_{k}^{h}, a_{D,I})]$ and $Q^{h}(s_{k}^{h}, a_{\\pi^{0}}^{h}(s_{k}^{h}))$.\nProof: From the traverse function, along the path $\\mathcal{P}^{h}$, we have\n$B_{D,I}^{B,h}(k) = max\\{\\underline{L}_{D,I}^{B,h}(k), min_{j\\in \\{2I-1,2I\\}} B_{D',J}^{B,h}(k)\\} \\qquad(18)$\n$\\geq min_{j\\in \\{2I-1,2I\\}} B_{D',J}^{B,h}(k)$\n$= B_{D+1,I'}^{B,h}(k) ((D + 1, I') \\in \\mathcal{P}^{h})$.\nWe make an assumption that at the step h in episode k, the attacker launches the attack and chooses a\nnode $(D, I)$ along the path $\\mathcal{P}^{h}$, then we have\n$B_{D,I}^{B,h}(k) \\geq B_{D',I'}^{B,h}(k) > \\underline{L}_{D',I'}^{B,h}(k) (D' < D, (D', I') \\in \\mathcal{P}^{h})$.\n(19)\nBecause the root node includes $a_{\\pi^{0}}^{h}(s_{k}^{h})$, so in the path $\\mathcal{P}^{h}$, except $(D, I)$, there must exist a node\n$(D_{min}, I_{min})(D_{min} < D)$ containing action $a_{\\pi^{0}}^{h}(s_{k}^{h})$. So we have\n$B_{D_{min},I_{min}}^{B,h}(k) \\geq B_{D,I}^{B,h}(k) \\geq \\underline{L}_{D,I}^{B,h}(k), \\qquad(20)$\nestablished. We now show that for any node $(D_{m}, I_{m})$ such that $a_{\\pi^{0}}^{h}(s_{k}^{h}) \\in P_{D_{m},I_{m}}$, then\n$Q^{h}(s_{k}^{h}, a_{\\pi^{0}}^{h}(s_{k}^{h}))$ is a valid upper bound of $\\underline{L}_{D_{m}, I_{m}}^{B,h}(k)$. From the definition of $\\underline{L}_{D,I}^{B,h}(k)$, we can"}, {"title": "L Proof of Theorem 2", "content": "Since the attacker will not launch the attack when the agent chooses an action satisfying $a \\in A^{h}(s^{k})$"}]}