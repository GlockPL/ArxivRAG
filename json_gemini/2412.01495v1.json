{"title": "Adversarial Attacks on Hyperbolic Networks", "authors": ["Max van Spengler", "Jan Zah\u00e1lka", "Pascal Mettes"], "abstract": "As hyperbolic deep learning grows in popularity, so does the need for adversarial robustness in the context of such a non-Euclidean geometry. To this end, this paper proposes hyperbolic alternatives to the commonly used FGM and PGD adversarial attacks. Through interpretable synthetic benchmarks and experiments on existing datasets, we show how the existing and newly proposed attacks differ. Moreover, we investigate the differences in adversarial robustness between Euclidean and fully hyperbolic networks. We find that these networks suffer from different types of vulnerabilities and that the newly proposed hyperbolic attacks cannot address these differences. Therefore, we conclude that the shifts in adversarial robustness are due to the models learning distinct patterns resulting from their different geometries.", "sections": [{"title": "1 Introduction", "content": "Non-Euclidean deep learning is a rapidly growing field where various inductive biases are instilled into our models through the use of geometric techniques [8]. For example, many works focus on challenging the usual choice of Euclidean space as the foundation for our models by replacing it with various non-Euclidean geometries, such as hyperspherical [37,50] or hyperbolic [26,63] spaces. Such geometries allow us to shed new light on deep representation learning. Each comes with its own benefits and drawbacks, highlighting the importance of judiciously choosing which space to learn in.\nHyperbolic geometry specifically has shown to significantly improve the representational capacity of models when dealing with hierarchically structured data [57]. In many areas of machine learning, from natural language [13,68,88] and graph learning [12, 16, 48,79,84] to computer vision [20, 28, 38,64], hierarchical structures are ubiquitous. Consequently, many existing architectures benefit from incorporating hyperbolic geometry into some or all of their layers. As the importance of hyperbolic deep learning grows, so does the need for adversarial robustness of models adopting it. Early evidence [64] has shown that adversarial attacks designed for Euclidean models have degraded performance against hyperbolic models, underlining the need for adversarial attacks that operate in the same geometry in which the networks are trained."}, {"title": "2 Related work", "content": "As applications of Artificial Intelligence are swiftly permeating everyday life, we are becoming increasingly susceptible to malign adversaries through, often unknown, weaknesses in our models. This has led to a surge in interest for universally applicable adversarial attacks that can be used to pinpoint and quantify any such underlying vulnerabilities [30,67]. In the absence of known powerful attacks, models can neither be evaluated for adversarial robustness nor protected against malicious intent [30,76].\nOver the years, several well performing universal adversarial attack frameworks have been proposed for various modalities, such as AutoAttack [15] for images and TextAttack for text [55]. However, existing adversarial attacks are designed against networks built in Euclidean space and their performance for other geometries cannot be guaranteed. In fact, models with different geometries can be more robust to these universal attacks [64], suggesting that the existing methods are inappropriate for non-Euclidean settings. We therefore require universal adversarial attacks that are geometry-agnostic in nature. The aim of this paper is to make a first step towards generalized attacks and to provide some insights into their behaviour when applied to Euclidean and hyperbolic networks.\nIn this paper, we propose Riemannian generalizations of the FGM [30] and PGD [52] adversarial attacks, with a specific application for hyperbolic networks. First, we use the proposed attacks on synthetic datasets to showcase the differences between the geometry-informed attacks and existing Euclidean attacks. Second, we investigate standard and our Riemannian adversarial attacks on identically trained Euclidean and hyperbolic ResNets to understand their effect and the differences in behaviour. With the introduced geometry-agnostic adversarial attacks and the empirical results, we hope to make a first step towards adversarially robust geometric deep learning and to shine some light on the potential difficulties that will have to be solved along the way."}, {"title": "2.1 Adversarial attacks", "content": "Adversarial attacks change a model's output by adding subtle adversarial perturbations to the input data [30,67]. The most common type of adversarial perturbation is noise restricted in terms of a norm: usually l\u221e [30, 42, 52], but l\u00ba [75] and l\u00b2 [10] norm attacks are viable as well. Beyond adversarial noise, adversarial patches [34,70], frequency perturbations [22], camouflage [21,35], shadow [46, 86], rain [83], or even 3D meshes [81] may be used as adversarial perturbations. Adversarial attacks can be categorized into white and black-box. White box attacks have full access to the model and leverage the model's gradient to optimize the adversarial perturbation. The most widely used general attacks are the fast gradient sign method (FGSM) [30] and projected gradient descent (PGD) [42,52]. Black box attacks optimize perturbations solely based on the model output, either on output probabilities [2] or hard labels [36]. Overall, there is a wide variety of adversarial attacks, and they boast a high attack"}, {"title": "2.2 Hyperbolic learning", "content": "The field of hyperbolic learning forms a recent and fast-growing research direction in deep learning [54, 60, 87], focusing on addressing known limitations of using Euclidean geometry as the geometric basis, including but not limited to hierarchical modelling, low-dimensional learning, and robustness. Early work showed that embedding hierarchies in hyperbolic space is vastly superior to Euclidean space [25,44,57], allowing for hierarchical embeddings with minimal distortion [62]. The results on hyperbolic embeddings of hierarchies sparked great interest in deep learning with hyperbolic embeddings or hyperbolic layers. As a natural extension to hierarchies, a wide range of works investigated hyperbolic graph learning [12,16,48,79,84], with applications in molecule classification [85], recommendation [65, 71, 82], and knowledge graph reasoning [72]. Hyperbolic learning has furthermore been shown to be an effective geometric network basis for text embeddings [68], speaker identification [45], single-cell embeddings [40], traditional classifiers [14,19], reinforcement learning [11] and more.\nWithin computer vision, early works showed the latent hyperbolic nature of visual datasets [38] and the potential of hierarchical classification with hyperbolic embeddings [18, 49, 51]. Visual learning with hyperbolic embeddings has since shown to improve a wide range of problems in computer vision, such as semantic segmentation [23,29,43,73,74], fine-grained classification [20,28], few-shot learning [27,31], video and temporal learning [24, 47, 66], vision-language learning [17, 41, 61], and generative learning [7,33, 53, 56]. Based on these advances, recent works have made the step towards fully hyperbolic visual representation learning, where all network layers are embedded in hyperbolic space [4,64]. Fully hyperbolic convolutional networks have shown to improve out-of-distribution robustness and increased adversarial robustness. Such evaluations were however performed using conventional adversarial attacks, which assume a Euclidean"}, {"title": "3 Background", "content": "This paper will use the Poincar\u00e9 ball model of hyperbolic space. Here, we provide a short overview of the formulas that will be of importance in later sections. For a more thorough treatment of hyperbolic space and the different isometric models that are used to study it, the reader is referred to [1,9]. The Poincar\u00e9 ball model is defined as the Riemannian manifold (D\u207f, gc), where -c is the constant negative curvature of the corresponding hyperbolic space. The manifold is defined as\nDn = {x \u2208 Rn: ||x||\u00b2 < 1/c}  (1)\nand the Riemannian metric as\n gc = \u03bb\u00b2In, \u03bb\u00b2 =  2/(1-c||x||\u00b2)\nwhere In denotes the n-dimensional identity matrix.\nThere exists no binary operation on this manifold that could be used to turn it into a vector space. However, we can use the non-commutative M\u00f6bius addition operation to get a similar but slightly weaker gyrovector structure [69]\nx\u2295cy= ((1 + 2c(x, y) + c||y||\u00b2)x + (1 \u2212 c||x||\u00b2)y) / (1 + 2c(x, y) + c2||x||2||y||2)\nThis operation can be used to effectively compute the hyperbolic distance between x, y \u2208 Dn as\ndc(x, y) = 2/\u221ac tanh-(\u221ac|| - x \u2295cy||).  (4)\nAn important concept in Riemannian geometry is the tangent space TxM, which can intuitively be considered as the collection of tangent lines to the manifold M at the point x \u2208 M. Vectors in this tangent space are called tangent vectors and they represent possible directions and velocities with which one could travel away from the point x. Given a tangent vector v at a point x \u2208 M, the exponential map at x can be used to determine where one would end up when traveling along the manifold for 1 unit of time in the direction of this tangent vector with a velocity determined by the norm of the vector. For Dn, these exponential maps can be computed as [26]\nexp(v) = x \u2295c (tanh (\u221ac||v||/2/\u221ac||v|| ))."}, {"title": "4 Hyperbolic attacks", "content": "We consider the setting where we are attacking a classification model \u03c6 : M \u2192 RC, with M an n-dimensional Riemannian manifold on which the model input is defined, C the number of output classes, and where \u03c6 is parameterized by parameters \u03b8. Here, M will usually be either Rn with the Euclidean distance or Dn as defined in Section 3. We note however that our proposed adversarial attacks work for arbitrary M as long as the exponential map can be computed efficiently. We will use x \u2208 M and y \u2208 {1,...,C} to denote the model input and ground truth label, respectively. The model \u03c6 outputs predictions \u1ef9 as\n\u1ef9 = arg max \u03c6i(x).\ni\u2208 {1,...,C}  (9)\nSuppose that for some input x the model \u03c6 correctly outputs the true label \u1ef9 = y. Then, the objective of an adversarial attack is to generate an adversarial sample x through a slight perturbation of the original input x such that the model \u03c6 will no longer correctly predict the true label y for x. Usually the perturbation is constrained in terms of some metric d : M \u00d7 M \u2192 R, so\nd(x, x) \u2264 \u03f5,  (10)\nwhere \u03f5 > 0 is the maximal perturbation size. For example, in the case where M = R\", this constraint is typically given in terms of the LP metrics. Following the notation of [52], the subset of M that satisfies equation 10 is denoted by S.\""}, {"title": "4.1 Hyperbolic FGM", "content": "The fast gradient method (FGM) is an efficient attack defined for Euclidean inputs which generates adversarial samples through a single update [30]. Given some input x \u2208 Rn, an adversarial sample is obtained through computing\nx = x + \u03b1\u2207xJ(\u03b8, x, y),  (11)\nwhere J is some objective function (typically the same as the one used for training the model \u03c6) and where \u03b1 > 0 is the magnitude of the attack. The value of \u03b1 can be chosen such that x \u2208 S. This process is essentially just typical gradient ascent on some objective function that is assumed to be inversely related to the model's performance. Therefore, inspired by [6], this process can be generalized to an arbitrary Riemannian manifold as long as the exponential maps can be computed efficiently. More precisely, we can define Riemannian FGM with x \u2208 M as\nx = expx (\u03b1\u2207xJ(\u03b8, x, y)),  (12)\nwhere expx: TM \u2192 M is the exponential map at x. Note that this method is indeed equivalent to the original FGM when applied to Euclidean space, since the exponential maps for Euclidean space can be defined as\nexpx(v) = x + v. (13)\nHyperbolic FGM on the Poincar\u00e9 ball can then be defined as a special case of Riemannian FGM, where the exponential maps are defined as in Equation 5. It should be mentioned here that a commonly used attack is the fast gradient sign method (FGSM), which uses the sign of the gradient instead of the gradient itself in equation 11. This method is often applied in the setting where the constraint in equation 10 is given in terms of the L\u221e metric. Under this constraint, the FGM and FGSM updates usually result in identical adversarial samples. When using for example the L2 norm, it makes more sense to use the full information given by the gradient, which is why we have chosen to use FGM instead of FGSM."}, {"title": "4.2 Hyperbolic PGD", "content": "The projected gradient descent (PGD) attack is essentially a multi-step version of FGM where, after each step, the output is projected back onto S [42, 52]. More specifically, each PGD update is computed as\nxt+1 = \u03c0S(x\u1d57 + \u03b1\u2207xJ(\u03b8, x, y)),  (14)\nwhere x\u2070 = x \u2208 R\u207f, where \u03b1 > 0 is the step size and where \u03c0S : Rn \u2192 S is some projection from R\u207f onto S. This projection onto S is usually taken to be the shortest path projection, so\n\u03c0S(y) = arg min d(y, z), (15)\nz\u2208S"}, {"title": "4.3 Objective functions", "content": "There are several interesting objective functions that can be considered for computing the gradients used by the attacks. The most commonly used [30, 42, 52] objective function is the usual cross entropy loss, given by\nJCE(\u03b8, x, y) = \u2212 log  (exp(\u03c6y (x))) / (\u2211i=1 exp(\u03c6i(x)))\nAs this loss has been used to train the model, perturbing the input to increase this loss can be expected to efficiently degrade model performance.\nUsing cross entropy loss, however, may not always lead to optimal attacks, as demonstrated for instance by [15]. Under the assumption that the model is correct for the original input, there are several other interesting options where the perturbations are aimed at directly changing specific logits. For example, we can use the negative value of the maximal logit\nJNFL = max \u03c6i(x),  (18)\ni\nwhich will push the model away from the correct label. Alternatively, we can target the second largest logit\nJSL = max \u03c6i(x), (19)\ni\u2260arg maxj \u03c6j(x)\nwhich steers the model towards the second most probable class. Lastly, the smallest logit can be used as the objective function\nJLL = min \u03c6i(x). (20)\nThis pushes the model towards predicting the least likely class."}, {"title": "5 Synthetic example", "content": "To gain some insights into the behaviour of the hyperbolic attacks compared to the original attacks we perform a small synthetic classification experiment with data on the Poincar\u00e9 disk with curvature -1, so where c = 1."}, {"title": "5.1 Dataset", "content": "We generate data by sampling from 4 different wrapped normal distributions [56] with covariance matrices \u03a3i \u2208 R2\u00d72 and means \u03bci \u2208 D\u00b2 for i \u2208 {1,...,4}. Sampling from such a distribution works as follows:\n1. Sample points \u03bej from a multivariate normal distribution N(0, \u03a3i) for j =\n1,..., Ni, where Ni is the number of points for the i-th class.\n2. Interpret the sampled points as tangent vectors at the origin of the disk, so\n\u03bej\u2208 ToD2.\n3. Use parallel transport to transport these tangent vectors to the tangent space\nat \u03bci, so compute Po\u2192\u03bci(\u03bej) \u2208 T\u03bciD2.\n4. Map the tangent vectors to the disk using the exponential map at \u03bci, i.e.\nxj = exp\u03bci (Po\u2192\u03bci (\u03bej)).\nIn our experiments we choose the \u03bci to be evenly spaced points along a circle in the Poincar\u00e9 disk with the origin as its center and with radius r = 1.5 with respect to the hyperbolic distance. We set each covariance matrix to \u03a3i = \u03c3\u00b2 I2, where I2 is the 2-dimensional identity matrix and with \u03c3\u00b2 = 0.25. The generated training dataset and test dataset each contain 10K samples for each class."}, {"title": "5.2 Model", "content": "We train a simple hyperbolic multinomial logistic regression model [26, 63] to perform the classification. The parameters of this model define 4 hyperplanes, one for each class, and the output logits are computed as the signed distances of the input to each of these hyperplanes. For the training setup we use Riemannian SGD [6] with a learning rate of 5 * 10-5 and a batch size of 4096 for 100 epochs. The model achieves an accuracy of 86.7% on the test data."}, {"title": "5.3 Attack configuration", "content": "We attack the resulting model with the proposed hyperbolic FGM. The constraint on the perturbation size in equation 10 is defined in terms of the hyperbolic distance as defined in equation 4. The step size is set to\n\u03b1 = \u03f5 / (1 + \u03f5||\u2207xJ(\u03b8, x, y)||) (21)\nsuch that dc(x,x) = \u03f5. For the objective function we try each of the options discussed in Section 4 to see their effects on the resulting attack.\nFor comparison, the model is attacked naively with the original FGM as well. Note that this attack ignores the geometry of the manifold, which makes it particularly inappropriate for this kind of data. Moreover, the choice of projection \u03c0S is less obvious in this case since the perturbation is not along a geodesic, meaning that orthogonal projection onto S is generally not equivalent to picking a suitable step size. To stay close to the true nature of this attack, we opt to pick \u03b1 such that dc(x,x) = \u03f5 here as well, which is equivalent to projection along a Euclidean straight line. We use Newton's method to find the value of \u03b1 that satisfies this condition."}, {"title": "5.4 Results", "content": "Figure 2 shows several example adversarial samples that were generated with the objective functions from Section 4 and with either Riemannian FGM or, naively, with the original FGM. For each of the samples shown, the model predicts a different label for the two adversarial samples generated with either attack. When using JCE we find that the gradient generally points somewhat in the direction of the hyperplane corresponding to the current prediction, while still being quite sensitive to the sample's position with respect to the other hyperplanes. On the other hand, when using JNFL the gradient points nearly exactly in the direction of the geodesic that forms the shortest path to the hyperplane of the correct class. Similarly, JSL and JLL lead to gradients that point in the direction of the geodesic forming the shortest path to the hyperplanes corresponding to the targeted logits.\nFuthermore, we oberserve that for gradients pointing inward towards the origin, the hyperbolic attack seems to be more potent, while for other gradients the Euclidean attack seems stronger. As the hyperbolic attack generates adversarial samples along geodesics of the Poincar\u00e9 ball, samples generated with this"}, {"title": "6 Hyperbolic models", "content": "In this section, we will take a look at the differences in adversarial robustness between Poincar\u00e9 [64] and Euclidean ResNets [32]. Moreover, we will apply the proposed hyperbolic attacks to the Poincar\u00e9 ResNets to see if accounting for the geometry resolves these differences in adversarial performance. A Poincar\u00e9 ResNet is a fully hyperbolic version of the original ResNet, where each of the layers appearing in the network is replaced by a fully hyperbolic alternative, based on [26,63]. To apply these networks to images, the image is first mapped to hyperbolic space by considering each pixel vector to be a tangent vector at the origin of D\u00b3 and using the exponential map corresponding to this tangent space. This operation can be seen as a preprocessing step. The resulting network has been found to be more adversarially robust than its Euclidean counterpart [64]."}, {"title": "6.1 Datasets and models", "content": "The experiments in this section are performed on the CIFAR-10 and CIFAR-100 datasets. On each dataset, we train a Poincar\u00e9 ResNet [64] and a Euclidean ResNet [32] each with a depth of 20 or 32 layers and channel widths 8, 16 and 32. For the Poincar\u00e9 ResNets we use the Riemannian Adam [5] optimizer with a learning rate of 1*10-3 and a weight-decay of 1*10-4. For the Euclidean ResNets we use the Adam optimizer with the same learning rate and weight-decay. In each setting we train for 500 epochs. For the Poincar\u00e9 ResNets the curvature"}, {"title": "6.2 Poincar\u00e9 ResNets versus Euclidean ResNets", "content": "We attack each of the ResNet models with the original FGM and PGD attacks. Note that this is possible for Poincar\u00e9 ResNets since we can backpropagate through the exponential map that is applied as a preprocessing step. The results of this attack for FGM on CIFAR-10 are shown in Figure 4a. The results for CIFAR-100 show a very similar pattern, so these have been left out of here for brevity. For PGD we set the step size equal to 0.5\u03f5 and T = 10. We found that this gives nearly identical results as FGM, so for clarity we will only show the results of FGM. The results for FGM show that the Poincar\u00e9 and Euclidean ResNet-20 perform similarly against the attack. However, the Poincar\u00e9 ResNet-32 appears notably more robust than its Euclidean counterpart.\nFor additional insights into the weaknesses of these models, we visualize the comparative misclassification matrix Mcomp for CIFAR-10 in Figure 5a. The value at position (i, j) in this matrix represents the difference between the Poincar\u00e9 ResNet and the Euclidean ResNet in proportion of misclassified images with true label determined by the row number i that was misclassified as the label determined by j, so"}, {"title": "6.3 Hyperbolic attacks versus Euclidean attacks", "content": "One possible explanation for the difference in behaviour between these models is the fact that we attacked the hyperbolic model without accounting for the geometry. To see if this is the case we compare the behaviour of the Poincar\u00e9 ResNets when attacked with the proposed hyperbolic FGM versus the behaviour of the Euclidean ResNets attacked with the original FGM. In order to apply the hyperbolic FGM, we have to apply it to the preprocessed image, where each pixel vector has been mapped to D\u00b3. However, the constraint is still given with respect to the original input image. To account for this, we can choose the step size \u03b1 such that\n||x - x||2 = || log0 (expX(\u03b1\u2207xJ(\u03b8, x, y))) - X||2 = \u03f5, (25)\nwhere x represents the generated adversarial sample in the image space. This value of \u03b1 can be found using Newton's method.\nThe results of the comparison are shown in Figures 4b and 5b. While there are some slight differences, the results are very similar to the results in Figures 4a and 5a. This is likely due to the fact that the perturbations are on such a small scale that the hyperbolic space is approximately Euclidean at this level.\nThis shows that accounting for the geometry when performing the attack does not negate the discrepancy in behaviour of the two types of models. Somehow, the different geometries cause the models to learn different patterns, each with its own weaknesses and strengths."}, {"title": "7 Conclusion", "content": "In this paper we proposed hyperbolic variants of the frequently used FGM and PGD adversarial attacks. The goal of these attacks is to improve adversarial performance by accounting for the geometry of the network. In the toy example of Section 5 we have seen that these attacks help in certain cases, but not all. Furthermore, in Section 6 we have seen that existing attacks perform differently versus Euclidean and hyperbolic networks, with hyperbolic networks being slightly more robust. Accounting for the geometry by applying the proposed hyperbolic attacks to the hyperbolic models did not remove any of the observed differences, demonstrating that the choice of geometry leads a model to learn different patterns with different strengths and weaknesses. As non-Euclidean geometry becomes increasingly pertinent to deep learning, future research into the resulting vulnerabilities and their origins will be of critical importance for ensuring adversarial robustness."}]}