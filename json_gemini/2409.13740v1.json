{"title": "LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCIENTIFIC KNOWLEDGE", "authors": ["Michael D. Skarlinski", "James D. Braza", "Manvitha Ponnapati", "Sam Cox", "Michaela Hinks", "Samuel G. Rodriques", "Jon M. Laurent", "Michael J. Hammerling", "Andrew D. White"], "abstract": "Language models are known to \u201challucinate\u201d incorrect information, and it is unclear if they are\nsufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI\ncomparison methodology to evaluate language model agents on real-world literature search tasks\ncovering information retrieval, summarization, and contradiction detection tasks. We show that\nPaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds\nsubject matter expert performance on three realistic literature research tasks without any restrictions\non humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-\nstyle summaries of scientific topics that are significantly more accurate than existing, human-written\nWikipedia articles. We also introduce a hard benchmark for scientific literature research called\nLitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we\napply PaperQA2 to identify contradictions within the scientific literature, an important scientific\ntask that is challenging for humans. PaperQA2 identifies 2.34\u00b11.99 (mean \u00b1 SD, N = 93 papers)\ncontradictions per paper in a random subset of biology papers, of which 70% are validated by human\nexperts. These results demonstrate that language model agents are now capable of exceeding domain\nexperts across meaningful tasks on scientific literature.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have the potential to assist scientists with retrieving, synthesizing, and summarizing\nthe literature 1,2,3, but still have several limitations for use in research tasks. Firstly, factuality is essential in scientific\nresearch, and LLMs hallucinate, confidently stating information that is not grounded in any existing source or evidence.\nSecondly, science requires extreme attention to detail, and LLMs can overlook or misuse details when faced with\nchallenging reasoning problems. Finally, benchmarks for retrieval and reasoning across the scientific literature today\nare underdeveloped. They do not consider the entire literature, but instead are restricted to abstracts6, retrieval on a\nfixed corpus7, or simply provide the relevant paper directly. These benchmarks are not suitable as performance proxies\nfor real scientific research tasks, and, more importantly, often lack a direct comparison to human performance. Thus, it\nremains unclear whether language models and agents are suitable for use in scientific research."}, {"title": "Answering scientific questions", "content": "To evaluate AI systems on retrieval over the scientific literature, we first generated LitQA2, 11 a set of 248 multiple choice\nquestions with answers that require retrieval from scientific literature (Figure 2A). LitQA2 questions are designed to\nhave answers that appear in the main body of a paper, but not in the abstract, and ideally appear only once in the set of\nall scientific literature. These constraints enable us to evaluate response accuracy by matching the system's cited source"}, {"title": "Performance analysis of PaperQA2", "content": "We varied the parameters of PaperQA2 to understand which are responsible for its accuracy (Figure 2C). We created\na non-agentic version (No Agent) which had a hard-coded sequence of actions (paper search, gather evidence, then\ngenerate answer). The non-agentic system had significantly lower accuracy (t(3.7) = 3.41, p = 0.015), validating the\nchoice of using an agent. We attribute the performance difference to the agent's better recall because it can return to and"}, {"title": "Summarizing scientific topics", "content": "To evaluate PaperQA2 on summarization, we engineered a system called WikiCrow, which generates cited Wikipedia-\nstyle articles about human protein-coding genes by combining several PaperQA2 calls on topics such as the structure,\nfunction, interactions, and clinical significance of the gene (Figure 3A). There has been previous work on unconstrained\ndocument summarization, where the document must be found and then summarized, 26 and even writing Wikipedia-style\narticles with RAG\u00b2. These studies have not compared directly against Wikipedia with human evaluation. Instead, they\nused either LLMs to judge or compared ROGUE (text overlap) against ground-truth summaries. Here, we measure\ndirectly against human-generated Wikipedia with subject mater expert grading.\nWe used WikiCrow to generate 240 articles on genes that already have non-stub Wikipedia articles to have matched\ncomparisons. WikiCrow articles averaged 1219.0 \u00b1 275.0 words (mean \u00b1 SD, N = 240), longer than the corresponding\nWikipedia articles (889.6 \u00b1 715.3 words). The average article was generated in 491.5 \u00b1 324.0 seconds, and had\nan average cost of $4.48 \u00b1 $1.02 per article (including costs for search and LLM APIs). We compared WikiCrow\nand Wikipedia on 375 statements sampled from the 240 paired articles. Statements were selected using cues from\ndocument formatting (Section 8.3). The initial article sampling excluded any Wikipedia articles that were \"stubs\"\nor incomplete articles. Statements were then shuffled and given, blinded, to human experts, who graded statements\naccording to whether they were (1) cited and supported; (2) missing a citation; or (3) cited and unsupported. We\nfound that WikiCrow had significantly fewer \"cited and unsupported\" statements than the paired Wikipedia articles"}, {"title": "Detecting contradictions in the literature", "content": "Because PaperQA2 can explore scientific literature at much higher throughput than human scientists, we reasoned that\nwe could deploy it to systematically identify contradictions and inconsistencies in the literature at scale. Contradiction\ndetection is a \u201cone versus many\" problem, which in principle involves comparing claims or statements in one paper\nwith all other claims or statements in the literature. At scale, contradiction detection becomes a \u201cmany versus many\"\nproblem and loses feasibility for humans. Thus, we leveraged PaperQA2 to build a system called ContraCrow that\nautomatically detects contradictions in the literature (Figure 4A).\nContradiction detection is also known as claim verification or colloquially as \u201cfact checking\u201d27. This task has been\nstudied for over a decade, especially in the context of claims in the news or the internet27,28. Although originally\nrestricted to context and a claim, the setting extended to be unconstrained29 and recent work tries to work at the scale of\nthe internet 30. Some claim verification work has also focused on scientific claims 31,32. The main novelty of this work is\nin detecting contradictions without a restricted corpus and evaluating with human experts, not against a benchmark.\nContraCrow first extracts claims from a provided paper using a series of LLM completion calls (similar to Schlichtkrull\net al. (2024) 30), and then feeds those claims into PaperQA2 with a contradiction detection prompt. This prompt\ninstructs the system to evaluate whether there are any contradictions in literature to the provided claim, providing both\nan answer and a choice from an 11-point Likert scale (Figure 4B, Methods Section 8.4). Utilizing a Likert Scale allows\nthe system to give more reliable and interpretable scores when providing rank33.\nTo evaluate ContraCrow, we first derived a contradiction detection benchmark, ContraDetect, from LitQA2, as detailed\nin Section 8.4.2. Briefly, we converted half of the question-answer pairs in LitQA2 into declarative, incorrect statements\nthat are contradicted by the papers referenced in the corresponding LitQA2 question. (For example, a question like\n\"what color is grass?\" would become \u201cgrass is purple.\u201d) We converted the other half of the LitQA2 questions into\ndeclarative, correct statements that are supported by the corresponding papers (\"what color is grass\u201d becomes \u201cgrass is\ngreen.\u201d).\nWe then evaluated ContraCrow on its ability to detect the contradictions in ContraDetect. By transforming the Likert\nscale output into integers, we were able to tune the detection threshold and obtain an ROC curve with an AUC of 0.842\n(Figure 4C). Setting a threshold of 8 (contradiction), ContraCrow achieved 73% accuracy, 88% precision, and a false\npositive rate of only 7%. To further evaluate precision and to demonstrate ContraCrow's ability to handle \u201cHas anybody\never done x?\u201d questions, we then ran ContraCrow on 42 \u201cno-evidence statements\" that have never to our knowledge\nbeen reported on (detailed in Section 8.4.2). These claims were hand-generated by the authors within their fields of\nexpertise, and ContraCrow correctly chose lack of evidence (5) as its response 98% of the time, indicating its ability to\ndistinguish between real contradictions and lack of support.\nWe then applied ContraCrow to a set of 93 biology-related papers randomly selected from our database, identifying an\naverage of 35.16 \u00b1 21.72 (mean \u00b1 SD, N = 93) claims per paper. Of the 3,180 claims analyzed over the 93 papers,\n6.85% were deemed by ContraCrow to be contradicted by the literature, with 2.89%, 3.77%, and 0.19% assigned scores\n8, 9, and 10, respectively (Figure 4D). Setting a Likert scale threshold of 8, we detected an average of 2.34 \u00b1 1.99\ncontradictions per paper (mean \u00b1 SD) (Figure 4E). As an example, one contradiction detected by our system concerned\nthe prognostic implications of LEF1 expression in colorectal carcinomas. The source paper, Kriegl et al. (2010) 34, finds\nusing immunohistochemistry on a tissue microarray that LEF-1 correlates positively with longer overall survival. By\ncontrast, a study published the following year also using tissue microarrays found LEF1 overexpression in colorectal\ncancer correlates negatively with longer overall survival and is also correlated with liver metastasis 35, results that have\nbeen supported by other studies as well36,37,38, thus explicitly contradicting the original statement.\nTo evaluate the validity of the contradictions detected this way, expert human annotators evaluated 50 claims that were\nassigned ContraCrow scores of 8 and 50 claims that were assigned scores of 9 or 10, considering all evidence and\nreasoning cited by the model. Annotators agreed with ContraCrow's findings on 70% of evaluated claims, or 1.64\""}, {"title": "Conclusions", "content": "We developed a methodology to compare or validate AI systems against human performance in realistic tasks for\nscientific research. PaperQA2 outperforms human experts on answering questions across all scientific literature;\nproduces summaries that are, on average, more factual than Wikipedia summaries; and can be deployed to identify\ncontradictions in scientific literature at scale. The contradiction work, in particular, attests to the potential of systems\nsuch as PaperQA2 for science: notably, one human expert who performed this task, who was also tested on a large\nbattery of other benchmarks, reported without solicitation that the contradiction detection task was the hardest task\nthey were asked to perform. Although PaperQA2 is expensive compared to lower accuracy commercial systems, it is\ninexpensive in absolute terms, costing $1 to $3 per query. Scaling up PaperQA2 and other literature-enabled agents like\nWikiCrow and ContraCrow empowers us to take advantage of the latent knowledge in literature at much greater scale\nthan is possible today."}, {"title": "Data Availability", "content": "The code necessary to replicate these results or modify the algorithm for further research is included on Github via\npaperqa. Data including all evaluator responses, contradiction detection claims, litQA questions, and WikiCrow\ncandidate statements are available in the supplementary materials. All generated WikiCrow articles for this study are\navailable in a public Google Cloud bucket here: https://storage.googleapis.com/fh-public/wikicrow2/."}, {"title": "8 Methods", "content": "All reported figures and data in this work were built on the open source PaperQA package, available on GitHub at\npaperqa. While the core PaperQA repository provides the basic algorithms used, it does not include the Grobid parsing\ncode, access to non-local full-text literature searches, or the citation traversal tool. The open source version of PaperQA\nutilizes LangChain\u00b9 for its agentic and state update operations. The full configuration objects for all experiments run in\nthis paper are included for further customization.\nNote that while paperqa gives the ability to recreate this work, the experiments reported in this paper were performed\nusing a more featureful HTTP server that takes advantage of bespoke infrastructure at the authors' institution. This\ninfrastructure includes features such as user authentication, MongoDB request caching, Redis object caching and\nglobal-load balancing, several PostgreSQL DBs with associated ORM code, cost-monitoring modules, time-profiling\nmodules, configuration storage and run orchestration (Dagster\u00b2 and kubernetes\u00b3), cloud bucket storage for PDFs, a CI\npipeline with semi-automated deployments, and infrastructure code for deploying auto-scaling instances in the cloud.\nNone of these features affect performance on a per-query basis, but provide increased scalability, measurability, and\npersistence. To run our same server infrastructure, users would need to provision all of these assets and configure the\ndeplyments themselves. paperqa should serve allow usage and customization sufficient for most research purposes,\nand should be sufficient to reproduce the results reported here.\nEven within paperqa, the \"Paper Search\" tool is limited by access to full text repositories of scientific papers, often\nbound by licensing agreements. The included implementation only works from local files accessible to each user. Our\nimplementation starts with a full or partial-text keyword search, where the keywords have been specified by the agent\nwhen selecting the paper search tool. The ranked results returned from these services are then matched to a user's\nexisting paper repository or can be retrieved on-the-fly if open-access or partner links exist for these works. These\nmatching papers are parsed, and pulled into our agent state for usage with other tools. Note that the search services will\nhave access to a larger corpus of works than is available to us via our repository and accessible link traversal, in these\ncases the system will simply skip these papers and they are not used. A stub of the paper search tool is implemented\nin paperqa with directions for users to implement their own retrieval since it will be limited to their own access to\nfull-text papers.\nAblations and configurations for workflows like WikiCrow are exposed in paperqa as nested configuration objects. All\nexperiments performed in this work correspond to included configuration objects. Here we highlight the configuration\nvariable descriptions corresponding to the salient features tested in this work, though all variable names are available in\nthe included files.\n\u2022 query: The main query task asked of the PaperQA agent, i.e. a LitQA question or a directive to write an\narticle.\n\u2022 11m: The LLM used in the generate answer tool can be a valid Anthropic, OpenAI, or Gemini model identifier.\nThis parameter was varied for the model experiments in Figure 2.\n\u2022 agent_llm: The LLM used for the agent orchestration, in this work, it was always fixed to\ngpt-4-turbo-2024-04-09."}, {"title": "8.1 PaperQA Implementation and Parameters", "content": "\u2022 summary_l1m: The LLM used for the RCS step in the gather evidence tool, must be a valid Anthropic,\nOpenAI, or Gemini model identifier. This parameter was varied for the model ablations in Figure 2.\n\u2022 prompts: A PromptCollection object from PaperQA4, which allows for specification of prompts in each\ntool, as well as features like turning off RCS (via prompts.skip_summarization). The No RCS Model\nablation used this input as well as the WikiCrow prompts.\n\u2022 max_sources: The number of top ranked sources to be included in the generate answer tool, in Figure 1A,\nthe 'filter top summaries\u201d cutoff. This parameter was 5 in the top-performing Answer cutoff @ 5, but 15 for all\nother experiments.\n\u2022 consider_sources: The top-k cutoff, i.e. the number of chunks that will be used in the RCS step. This\nparameter was set to 30 by default in LitQA experiments, save for the Top-k Rank @ X experiment where it\nwas set to X. Additionally, for our WikiCrow prompts this parameter was set to 25.\n\u2022 agent_tools: An ordered list of tool names that will be used by the agent, including gather_evidence,\npaper_search, generate_answer, and citations_traversal. This always included all four tools except\nfor the No Cit. Trav. and No Agent runs where citations_traversal was excluded.\n\u2022 docs_index_mmr_lambda: A pre-gather evidence MMR lambda parameter which can be used to pre-filter\nsimilar papers by name before gathering evidence. This was set to 0.9 for our WikiCrow run to promote\ndiversity of sources, but 1.0 for LitQA experiments.\n\u2022 parsing_configuration.ordered_parser_preferences: A list of the parsing algorithm to use, either\npaperqa_default (PyMuPDF) or grobid. paperqa_default was the default for each ablation, and\ngrobid was used for WikiCrow generation. This parameter was also varied in the experiments shown in\nFigure 6.\n\u2022 parsing_configuration.chunksize: the chunk size (in characters) to be used when chunking parsed\ndocuments. This parameter was varied in the experiments shown in Figure 6.\n\u2022 parsing_configuration.overlap: the overlap (in characters) that will be common between sequential\nchunks. This was fixed at 750 for this work.\n\u2022 parsing_configuration.chunking_algorithm:\nthe algorithm used to chunk documents,\nsimple_overlap simply uses a sliding window with overlap, and sections uses semantic pars-\ning by section (i.e. one chunk per section where possible), if sections need to be broken into\nmultiple chunks the system will automatically handle this. sections is only supported via\nparsing_configuration.ordered_parser_preferences=grobid. This parameter was varied in\nthe experiments shown in Figure 6, and in our WikiCrow generation.\n\u2022 temperature: temperature used for the LLM in the generate answer tool. This was set to 0 for all runs in this\nwork.\n\u2022 summary_temperature: temperature used for the LLM in the gather evidence tool's RCS step, this was set\nto 0 for all runs in this work."}, {"title": "8.1.1 Tool implementations", "content": "PaperQA2's agentic tools were implemented as in PaperQA4. Our agent was prompted with the following message to\nguide tool usage:\nAnswer question: {question}. Search for papers, gather evidence, collect papers cited in evidence\nthen re-gather evidence, and answer. Gathering evidence will do nothing if you have not done a new\nsearch or collected new papers. If you do not have enough evidence to generate a good answer, you\ncan:\nSearch for more papers (preferred)\nCollect papers cited by previous evidence (preferred)\nGather more evidence using a different phrase\nIf you search for more papers or collect new papers cited by previous evidence, remember to gather\nevidence again. Once you have five or more pieces of evidence from multiple sources, or you have\ntried a few times, call {gen_answer_tool_name} tool. The {gen_answer_tool_name} tool output is\nvisible to the user, so you do not need to restate the answer and can simply terminate if the answer\nlooks sufficient. The current status of evidence/papers/cost is {status}"}, {"title": "Paper Search Tool", "content": "The paper search tool uses an initial keyword search, generated by the agent in the context of the user query. The agent\nis prompted as follows:\nA search query in this format: [query], [start year]-[end year]. You may include years as the last word\nin the query, e.g. 'machine learning 2020' or 'machine learning 2010-2020'. The current year is\n{get_year()}. The query portion can be a specific phrase, complete sentence, or general keywords, e.g.\n'machine learning for immunology'.\nOur initial search relies on services like Semantic Scholar, where candidate lists (default of 12) of relevant papers\nare generated then parsed. When parsed, the papers are first turned into text using either Grobid or PyMuPDF, then\nsplit into chunksize character sized pieces. If the sections parsing is used, then section chunks are split on header\nmetadata provided by Grobid. An embedding vector is generated for each chunk using a hybrid implementation\nwhich concatenates a dense and sparse, keyword based embedding model. For the experiments included in this study,\nOpenAI's text-embedding-large-3 was used. It was concatenated with a normalized 256 dimension vector which\nused modulus-encoding to extract a hot-encoded keyword from the tokenization integers provided by OpenAI's\ntiktoken library. These text chunks are put into a document context which is accessible by the agent for further\nmanipulation with tools. The PaperQA entrypoint for these functions can be found on github."}, {"title": "Gather Evidence Tool", "content": "As detailed in github for PaperQA, the Gather Evidence tool begins with a top-k vector ranking step, using the\nembedding vectors created in the Paper Search tool. The user query is embedded with the same algorithm, and cosine\nsimilarity is used to match all document chunks in the agent's context with the user query. The top-k chunks are then\nselected for the RCS step.\nThe reranking and contextual summarization step most differentiates PaperQA's implementation relative to other\nRAG technologies. The tool's prior step, an top-k vector retrieval ranking, is a widely implemented7,8 approach to\nidentify relevant documents, however, the RCS second step, is unique to PaperQA (to the authors' knowledge). While\nperformance improvements with both deep reranking (or LLM) models and map-reduced summarizations 9,10,11,12 are\nwell documented, combining the reranking operation with a contextual summary provides novel benefits.\nThe step is implemented by mapping an LLM completion across each top-k chunk (system prompt):\nProvide a summary of the relevant information that could help answer the question based on the\nexcerpt. The excerpt may be irrelevant. Do not directly answer the question - only summarize relevant\ninformation. Respond with the following JSON format: {{ \"summary\": \"...\",\n\"relevance_score\": \"...\" }} where \"summary\" is relevant information from text - {summary_length}\nwords and \"relevance_score\" is the relevance of \"summary\" to answer the question (integer out of 10)\nWhere each chunk is injected as follows:\nExcerpt from citation - {text} Query: {question}\nAfter completion, each JSON object is parsed and the passages are re-ranked according to the new relevance scores.\nWhen running with WikiCrow, gene names are also prompted to be extracted as additional JOSN keys, these are kept\nand injected in the final answering context. Advantages of the RCS step are as follows: 1. Token usage efficiency\nis vastly improved, a contextual summary will be 200-400 tokens compared with our standard document's chunk\nsize of 2,250 tokens. This allows for a significantly more accessible document corpus for injection into PaperQA's\nanswering context window. Furthermore, we see no decrease in summarization efficacy, using LitQA performance\nas a proxy, across document chunk sizes from 750-3,000 tokens. 2. As a new feature in this work, the LLM can be\nprompted to provide its summary in a structured JSON or XML format to simplify its downstream data extraction.\nIn addition to a relevance score used for reranking, this structure can include metadata (such as a gene name) which\nwill be retained through the PaperQA workflow. This is used to reduce hallucination and confusion in the final"}, {"title": "Generate Answer Tool", "content": "This tool answers questions by taking a subset of the top ranked sources (from the RCS ranking), and injects them into\na final context for answering. The default in this study was to inject 15 contextual summaries, but we saw maximal\naccuracy with 5 at the cost of precision. LLMs were prompted to answer as follows:\nAnswer the question below with the context.\nContext: {context} Question: {question}\nWrite an answer based on the context. If the context provides insufficient information and the question\ncannot be directly answered, reply \"I cannot answer.\" For each part of your answer, indicate which\nsources most support it via citation keys at the end of sentences, like (Example2012Example pages\n3-4). Only cite from the context and only use the valid keys. Write in the style of a Wikipedia article,\nwith concise sentences and coherent paragraphs. The context comes from a variety of sources and\nis only a summary, so there may inaccuracies or ambiguities. If quotes are present and relevant,\nuse them in the answer. This answer will go directly onto Wikipedia, so do not add any extraneous\ninformation.\nAnswer ({answer_length}):\nWhere contexts are injected by the generate answer code before output is returned to the agent."}, {"title": "Citation Traversal Tool", "content": "Atop the PaperQA4 tools, we created an additional tool to traverse one degree of citations, both forward in time (\u201cfuture\nciters\") and backwards in time (\u201cpast references\"). This tool enables a fine-grained search around paper(s) containing\nrelevant information. The traversal originates from any paper containing a highly-scored contextual summary (RCS\nscore 0-10), and our minimum score threshold was eight (inclusive). The papers corresponding to highly-scored\nsummaries are referred to as Dprev in Algorithm 1."}, {"title": "8.2 LitQA", "content": "LitQA questions were generated manually by a combination of the authors as well as contracted human experts (see14).\nAll human annotators were compensated, informed that their evaluations were being used in research of human-level\nperformance, and consented to the use of their annotations and participation. Question authors were instructed to\nidentify recent papers (published within the last 36 months), and develop a multiple-choice question that requires\ncontext within the main text of the paper to answer and is not answerable by the abstract or title alone. They were\nfurther advised that the question should require some amount of reasoning within the paper context, and not be a\ndirect quote or statement from the paper. Distractors were instructed to be reasonable within the context, using either\nother information in the paper (e.g. other genes being discussed) or based on inherent or other knowledge. We also\nperiodically tested question drafts with ChatGPT 3.5 or 4 (with logging disabled) to ensure questions were not easily\nanswerable by models already, or to help design effective distractors by asking to provide plausible answers. Question"}, {"title": "8.2.1 Question Construction and Human Evaluation", "content": "drafts were also often searched against Google Scholar to ensure that it was not trivial to find the exact statement\nnecessary to answer the question. This was an effective aid in revising question wording.\nFor contracted question authors or anyone new to drafting, the first 10 or so questions produced were carefully reviewed\nby one or more of the authors for quality, after which they were either asked to rework them, they were added to corpus,\nor they were removed from consideration. This feedback (referred to as calibration) was usually enough to ensure\nquality question generation going forward from contractors, though questions were reviewed by authors on an ongoing\nbasis prior to merging into the main corpus. Contractors were compensated via multiple structures during different\nphases of generation as we iterated on effective strategies. Initially, they were paid on an hourly basis at $50 per hour.\nAt later phases, they were paid on a completion basis such that they were paid per accepted question, which equated to\nvariable hourly equivalents always $50 or more per hour. Using this methodology, LitQA2 was built up from LitQA (47\nquestions) in two stages of releases, first 100 questions (147), then an additional 101 questions, adding to the original\nsubset to make 248 total questions.\nHuman evaluators were assigned questions from the LitQAv2 corpus in rounds of \"quizzes\" composed of 20-40\nquestions each. They were allocated up to a week to complete the quiz, but were given no other time constraints. They\nwere also allowed to use tools such as internet search or journal collection search provided via their institutions. They\nwere asked to explicitly refrain from using AI-based tools such as ChatGPT or Claude, though we did not have any\nmethod of enforcement of this request. In order to encourage completion as well as high performance, we attempted to\ndesign an incentive scheme that separately promoted both: In brief, we paid a base dollar amount (from $3-6) to each\nquestion for completion. We then added a bonus to each question that was based on overall performance:\n\u2022 80% or more overall score = bonus equal to base question amount for each correct question.\n\u2022 60% - 80% overall score = bonus equal to half the base question amount for each correct question.\n\u2022 Less than 60% overall score = bonus equal to $1 for each correct question.\nWe additionally paid a completion bonus for the entire quiz in some instances, amounting to $150. On average,\nevaluators were paid between $50 to $100 per hour based on self-estimation of time spent and total compensation.\nQuizzes were sometimes combined with similar evaluation quizzes for other evaluation benchmark categories we are\ndeveloping.\nFor the human LitQA2 performance reported in this work, 2 rounds of quizzes (20 questions each) were given to 8\nevaluators. In total, evaluators answered provided 160 unique answers across 147 unique questions.\nA third round of quizzes was attempted with another set of 160 questions given to evaluators, with questions which\noverlapped from both the initial set of 147 questions and the remaining 101 questions. However, this quiz round had\nseveral outlier scores (>90%) in quizzes on the same subsets of questions which had been given previously to a much\nlower average (66.3%). Upon investigating, we found that a portion of the initial 147 LitQA2 questions that were\navailable on GitHub had been parsed by google-indexed data aggregators, causing it to trivially return results with a\nGoogle search from evaluators. Interestingly, this did not impact PaperQA2 performance, because PaperQA2 does not\nuse Google websearch. Thus, we chose to exclude data from this third set of quizzes as it was no longer a valid human\ncomparison. As reported in Table 2, we don't see significant PaperQA2 accuracy differences between the tranches of\nLitQA2 releases, so we concluded that the initial two rounds of quizzes are still a valid human baseline for comparison."}, {"title": "8.2.2 PaperQA measurement", "content": "When measuring LitQA2 performance, paperqa function calls were performed for each LitQA question. LitQA answer\norder was randomized for each call, and, for all configurations shown, 3 full runs of all LitQA questions were performed.\n3 runs were necessary to control for the inherent noise in LLM inference, even at 0 temperature (where all tests were\nperformed). LitQA2 was automatically evaluated using an evaluation LLM call (GPT-4-0613), which extracted the\nletter answer from PaperQA2's output. It was prompted to extract as follows:\nExtract the single letter answer from the following question and answer {prompt_output(\u2018QA')}\nSingle Letter Answer:\nThe extraction was then parsed and graded against the LitQA2 benchmarks correct ideal answer. All questions were\ngraded as \"Unsure\" if the \u201cInsufficient information to answer this question\u201d option was selected, except for the LitQA2\nquestions where the ideal answer is \u201cnull\u201d, then the \u201cInsufficient information\" was graded as \"Correct\".\nLitQA2 was developed in two stages of 100 then 101 questions, adding to the original subset of LitQA (47) questions\nto make 248 questions. This gave us an opportunity to evaluate our system before and after adding the set of 101"}, {"title": "8.3 WikiCrow", "content": "WikiCrow statements were generated using a linked set of 5 different queries, 4 to PaperQA2, and 1 to a frontier\nLLM model, GPT-4-Turbo. As detailed in Figure 3A, each PaperQA2 query created a different article section. The\nPaperQA2 prompts and settings used for these queries can be found in our paperqa repository. The primary PaperQA2\nparameter differentiation between WikiCrow and LitQA2 runs was the usage of Grobid for document parsing, as well\nas the modification of the RCS step to extract a gene name that is the focus of each paper chunk being summarized.\nThey were designed to reduce hallucinations and accurately read data from tables.\nWe compared WikiCrow and Wikipedia statements starting from a sample of 300 genes which were sampled from\nfrom a complete list of all protein encoding human genes17 (n=19,255) after being filtered for only those having valid\n(non-stub) Wikipedia articles (n=3,639). Among the 300 articles, Wikipedia statements were sampled, first from the\npool of 300 genes, then among each article's <p> HTML sections with more than 25 characters. 500 total Wikipedia\nstatements were sampled, which resulted in 240 unique genes. Matching WikiCrow articles were generated for each of\nthe 240 genes for comparison. WikiCrow statements were broken at either a reference list (notated with parenthesis and\nPaperQA2 document names) or at a paragraph break, most closely matching Wikipedia's HTML structure. Each of the\n500 Wikipedia statements were randomly matched with a WikiCrow statement from the same gene article to ensure that\nevaluators were given a similar gene-distribution of statements between WikiCrow and Wikipedia articles.\nReferences were extracted from the Wikipedia statements using the <sup> link, and matched to the correct DOI or\nhyperlink in the article references. References were extracted from the WikiCrow statements by parsing the parenthetical\ninsertions (via regular expressions) and matching the DOI to the document name given by the model. In this way,\neach statement was associated with a list of DOI links to the sources listed. Efforts were then made to structure each\nstatement similarly, removing any HTML from the Wikipedia statements, and removing all inserted references into the\nWikiCrow statements. All references were replaced with random numbers between 1 and 30, but kept consistent if\nlisted several times in the same statement. References were then uniformly reformatted as [x"}]}