{"title": "FOUNDATION MODELS FOR ELECTROCARDIOGRAMS", "authors": ["Junho Song", "Jong-Hwan Jang", "Byeong Tak Lee", "DongGyun Hong", "Joon-myoung Kwon", "Yong-Yeon Jo"], "abstract": "Foundation models, enhanced by self-supervised learning (SSL) techniques, represent a cutting-edge frontier in biomedical signal analysis, particularly for electrocardiograms (ECGs), crucial for cardiac health monitoring and diagnosis. This study conducts a comprehensive analysis of foundation models for ECGs by employing and refining innovative SSL methodologies\u2014namely, generative and con-trastive learning on a vast dataset of over 1.1 million ECG samples. By customizing these methods to align with the intricate characteristics of ECG signals, our research has successfully developed foundation models that significantly elevate the precision and reliability of cardiac diagnostics. These models are adept at representing the complex, subtle nuances of ECG data, thus markedly enhancing diagnostic capabilities. The results underscore the substantial potential of SSL-enhanced foundation models in clinical settings and pave the way for extensive future investigations into their scalable applications across a broader spectrum of medical diagnostics. This work sets a benchmark in the ECG field, demonstrating the profound impact of tailored, data-driven model training on the efficacy and accuracy of medical diagnostics.", "sections": [{"title": "1 Introduction", "content": "Electrocardiograms (ECGs) are essential bio-signals that record the heart's electrical activity, providing critical information about heart rhythm, strength, timing, and beat rate. These signals play a pivotal role in diagnosing heart diseases, detecting abnormalities that may indicate conditions such as myocardial infarction, arrhythmias, and other cardiac disorders. Analyzing ECGs is crucial for effective diagnosis and treatment, making advancements in this field highly impactful.\nDeep Neural Networks (DNNs) have emerged as powerful tools for analyzing ECG data, capable of performing various tasks such as prediction, classification, and denoising. However, two significant challenges hinder their utilization. Firstly, ECG data is sensitive medical information that requires strict privacy protection, leading to a relative scarcity of available data. Secondly, the low incidence of certain heart diseases results in insufficient curated ECG datasets with annotated medical labels to effectively train DNNs.\nResearchers have adopted foundation models with Self-Supervised Learning (SSL) to address these challenges, showing great promise in various fields, including natural language processing (Devlin et al., 2018; Achiam et al., 2023), computer vision (Chen et al., 2020; He et al., 2022), and speech recognition (Baevski et al., 2020). In the context of ECG analysis, foundation models can generalize from unlabeled data and be fine-tuned with minimal labeled data, making them highly flexible and efficient.\nFoundation models with SSL offer several key benefits:\n\u2022 Reduced Need for Labeled Data: SSL requires only unlabeled data to train foundation models, learning robust representations without the need for extensive labeled datasets.\n\u2022 Enhanced Accuracy and Efficiency: These models can be fine-tuned for specific downstream tasks, resulting in improved accuracy and faster convergence compared to training models from scratch.\n\u2022 Resource Efficiency: Since foundation models are pre-trained, fine-tuning them requires fewer computational resources, making the process more efficient.\nThe success of foundation models hinges on effective pre-training, which relies heavily on SSL methods. Two main streams of SSL are widely adopted: Contrastive Learning (CL) and Generative Learning (GL). CL enhances the model's ability to distinguish between similar and dissimilar instances by bringing positive pairs closer and pushing negative pairs apart in the representation space. GL, on the other hand, focuses on reconstructing data by masking parts of the input, thereby improving the model's ability to predict missing information (Lai et al., 2023; Wang et al., 2024; Chien et al., 2022; Na et al., 2024).\nWhile promising, the application of foundation models for ECGs is still in its early stages. There is a need for extensive investigation into various aspects, such as representation schemes, SSL methods, and practical applications, to fully realize their potential. Our study aims to guide the development of foundation models for ECGs using a large-scale dataset of over 1.1 million samples. We investigate the models from three perspectives:\n\u2022 Representation scheme: Analyzing the architectural design based on the Vision Transformer (ViT (Dosovitskiy et al., 2020)) to customize it for the unique characteristics of ECG data.\n\u2022 SSL methods: Evaluating different SSL methodologies, including contrastive, generative, and hybrid ap-proaches, to determine the most effective strategy for ECG data representation.\n\u2022 Practical applications: Validating the efficacy of these models in real-world scenarios characterized by low incidence rates and restricted data availability.\nOur contributions include:\n\u2022 Large-Scale Pre-Training: To the best of our knowledge, this is the first work using a large-scale dataset to pre-train foundation models on ECG data.\n\u2022 Hybrid SSL Framework: Introducing a novel combination of SSL techniques for foundation models of ECGs.\n\u2022 Guidance for Foundation Models: Providing extensive insights through the comprehensive analysis into the performance of ECG foundation models using various SSL methods.\nThe findings of this study are expected to significantly enhance the application of SSL-enhanced foundation models in ECG classification tasks, providing a robust framework for future research and practical implementations in clinical settings."}, {"title": "2 Related Work", "content": "The advent of foundation models has revolutionized various fields by leveraging large-scale, unlabeled datasets to produce versatile and robust data representations. These representations enable models to adapt efficiently to specific downstream tasks with minimal reliance on labeled data, significantly enhancing performance and efficiency. In the realm of bio-signal analysis, particularly ECGs, the application of foundation models enhanced by Self-Supervised Learning (SSL) techniques has shown immense promise (Abbaspourazad et al., 2024; Yue et al., 2022)."}, {"title": "2.1 Advanced Self-Supervised Learning Techniques for Foundation Models", "content": "Foundation models utilize advanced SSL techniques during their pre-training phase to learn meaningful data repre-sentations without direct supervision. These techniques are crucial for enabling versatile applications in subsequent task-specific training:\n\u2022 Contrastive Learning (CL): CL improves a model's ability to form embeddings by bringing similar instances closer in the embedding space while pushing dissimilar instances apart. This is typically achieved using a contrastive loss function, such as InfoNCE, which helps the model develop rich, semantically meaningful representations from unlabeled data. CL has been successfully applied across various domains, enhancing model performance on downstream tasks by improving the quality of learned representations (Chen et al., 2020).\n\u2022 Generative Learning (GL): GL techniques, such as Masked AutoEncoders (MAE), challenge models to reconstruct obscured segments of input data. This process strengthens the model's capacity to intuit and regenerate missing data components; fostering a deeper understanding of complex data patterns. GL has been particularly effective in applications like image and natural language processing, where it aids in denoising and anomaly detection (He et al., 2022; Dosovitskiy et al., 2020).\n\u2022 Hybrid Learning (HL): HL combines the strengths of both CL and GL, leveraging the discriminative power of CL and the reconstructive ability of GL. This synergistic approach has shown promising results in domains such as computer vision and holds significant potential for ECG analysis and other medical tasks. By balancing the advantages of both methods, HL aims to create robust and flexible data representations (Huang et al., 2023)."}, {"title": "2.2 Pioneering Applications of SSL in Bio-Signal Analysis", "content": "SSL methodologies have been increasingly adapted for bio-signal analysis, particularly in ECG, demonstrating substantial improvements in model accuracy and generalizability:\n\u2022 Diverse Approaches to ECG Analysis: SSL has been instrumental in developing deep multi-task learning frameworks that significantly enhance emotion recognition from ECG signals, achieving state-of-the-art classification results. These frameworks illustrate the ability of SSL to capture complex physiological patterns inherent in ECG data (Sarkar and Etemad, 2020).\n\u2022 Innovative ECG Classification Techniques: Techniques such as U-ResNet-based SSL have shown meaning-ful improvements in ECG classification accuracy, underscoring the utility of SSL in clinical diagnostics. These methods leverage SSL pre-training to refine models, leading to better performance compared to traditional supervised approaches (Gedon et al., 2021).\n\u2022 Addressing Data Challenges: SSL methods have been employed to overcome challenges like label imbalance and data scarcity in ECG datasets. Self-supervised pre-training has demonstrated superior performance in handling complex bio-signal datasets, highlighting the effectiveness of SSL in managing the inherent challenges of ECG data (Liu et al., 2021).\n\u2022 Lead-agnostic Learning Models: Developing SSL methods that are agnostic to ECG leads and capable of learning both local and global features has shown improved adaptability and performance on downstream tasks. These approaches enhance the flexibility and generalizability of SSL in medical applications, making them valuable for diverse clinical scenarios (Oh et al., 2022).\n\u2022 Non-contrastive Learning for ECG: The introduction of non-contrastive SSL approaches, which do not rely on data augmentation or negative pairs, offers a novel pathway for SSL application in bio-signal processing. These methods provide an alternative to traditional contrastive techniques, simplifying the training process and potentially improving model performance (Atienza et al., 2023)."}, {"title": "2.3 Research Goals", "content": "Despite the promising results and growing adoption of SSL for bio-signal generalization, such as ECG, EEG, and PPG, there remains a significant gap in comprehensive implementation strategies for applying foundation models using diverse SSL frameworks in real-world healthcare settings. The existing literature often revolves around closed datasets and controlled experimental conditions, which may not accurately replicate the complexities encountered in practical environments (Na et al., 2024, Abbaspourazad et al., 2024, Liu et al., 2024).\nOur research aims to bridge these gaps by providing detailed and empirically supported guidance on pre-training foundation models. We meticulously evaluate these models across a broad spectrum of architectural designs and SSL methodologies, enhancing their efficacy for bio-signal applications such as ECG. By extending our investigations to include a wide range of real-world applications, we seek to refine these foundation models to better suit clinical needs, thereby advancing their readiness for broader healthcare applications and facilitating significant technological advancements in medical practice."}, {"title": "3 Study Procedure", "content": "Our study's methodology unfolds through a meticulously structured sequence of steps, aiming to provide a comprehen-sive evaluation of foundation models for ECG classification. The process includes data preparation, model training, thorough evaluation, and analysis, ensuring a robust framework for developing and assessing the performance of these models. The entire procedure of our study is illustrated in Figure 1."}, {"title": "3.1 Data preparation", "content": "We utilized two types of datasets: unlabeled and labeled. The unlabeled dataset, presented in Table 1, was compiled from five public repositories, MIMIC (Johnson et al., 2016), CODE15 (Ribeiro et al., 2021), BIOBANK (Sudlow et al., 2015), SAMI (Ribeiro et al., 2021), and IKEM (Sej\u00e1k et al., 2023), to ensure demographic fairness by including data collected from various continents. This dataset encompasses a total of 1,291,868 ECG samples from 442,736 distinct patients, aiding in mitigating potential biases and enhancing the generalizability of the study results.\nFor the classification tasks, we employed labeled datasets, specifically utilizing ECGs from the PTB-XL dataset outlined in Table 2. This dataset includes super-classes: MI (Myocardial Infarction), STTC (ST/T change), CD (Conduction Disturbance), and HYP (Hypertrophy). These super-classes were chosen to reflect the diverse morphologies, features, and rhythmic patterns inherent in ECG data, providing a robust framework for assessing the efficacy of our foundation models. The dataset comprises a total of 21,265 ECG samples collected from 8,000 patients.\nBefore analysis, all data underwent a standardization process to account for inherent variances in sample rates, measuring durations, and MV-units among different datasets. To standardize our comparisons and maintain consistency across analyses, we adjusted the sample rate to 250 Hz, the measurement duration to 10 seconds, and the MV-units to 1.0, rendering each ECG signal in our dataset to consist of a 2500 length per lead."}, {"title": "3.2 Model Training", "content": "We adopt widely recognized Self-Supervised Learning (SSL) methodologies to pre-train our foundation models, specifically focusing on Generative Learning (GL), Contrastive Learning (CL), and Hybrid Learning (HL). These models incorporate the Vision Transformer (ViT) (Dosovitskiy et al., 2020) as the primary architectural backbone,modified with a one-dimensional convolution projection layer specifically tailored for embedding ECG signals (Woo et al., 2023). The comprehensive architecture of our foundation models, which effectively combines these innovative self-supervised learning strategies."}, {"title": "3.2.1 Foundation models", "content": "Figure 2 illustrates the overall structure: the preprocessor divides ECG signals into patches, the encoder extracts features and understands global contextual relationships, and the decoder reconstructs the original ECG signals from the embeddings. By integrating these self-supervised learning methodologies, especially the pioneering HL approach, the foundation models effectively capture complex patterns in ECG data, significantly improving their utility in various downstream tasks, such as classification and prediction, and setting a new benchmark in the field.\nIn the training of foundation models for ECG analysis, we employ three primary self-supervised learning (SSL) methodologies: Contrastive Learning (CL), Generative Learning (GL), and Hybrid Learning (HL), each with a specific focus and procedure. As depicted in Figure 2, the preprocessor divides the input ECG signals into smaller patches using a one-dimensional convolution projection layer.\nFor CL, these patches are augmented to create different views, forming positive and negative pairs. The encoder, consisting of 1D-Convolutional layers and a Transformer block, processes these patches to extract features and generate embeddings. The contrastive learning objective is to bring the embeddings of positive pairs closer while pushing the embeddings of negative pairs apart, achieved using the contrastive loss function (InfoNCE loss):\n$L_{CONT} = -log \\frac{exp (z_i \\cdot z_j /\\tau)}{\\sum_{k=1}^N exp (z_i \\cdot z_k /\\tau)}$       (1)\nwhere $z_i$ and $z_j$ are the embeddings of the positive pair, $\\tau$ is a temperature parameter, and $N$ is the number of negative samples. During training, the encoder weights are updated to minimize this loss, enhancing the model's ability to distinguish between similar and dissimilar ECG signals.\nFor GL, the preprocessor randomly masks a portion of the patches, creating incomplete ECG inputs. The encoder processes the unmasked patches, and the resulting embeddings are input into the transformer decoder, which aims to reconstruct the original ECG signals. The generative learning objective is to minimize the reconstruction loss, quantified using the mean absolute error (MAE):\n$L_{RECON} = \\frac{1}{N} \\sum_{i=1}^N |x_i - y_i|$   (2)\nwhere $x_i$ is the original signal and $y_i$ is the reconstructed signal. The encoder and decoder weights are updated to minimize this loss during training, improving the model's ability to predict missing information."}, {"title": "3.2.2 Downstream models", "content": "After pre-training the foundation models using SSL, we employed two downstream learning strategies to assess their performance: linear probing and fine-tuning.\nLinear Probing: Linear probing involves freezing the weights of the pre-trained foundation model and adding a linear classifier on top of its output layer. This linear classifier is trained using the labeled ECG dataset, evaluating the quality of the learned representations without updating the pre-trained model's weights. This approach provides insights into how well the foundation model's pre-trained features can be used for classification tasks.\nFine-Tuning: Fine-tuning involves updating the weights of the entire model, including both the pre-trained foundation model and the newly added linear classifier. This step allows the model to adapt more specifically to the classification tasks using the labeled dataset. Fine-tuning typically results in better performance than linear probing, as it adjusts the pre-trained features to better fit the specific characteristics of the labeled data."}, {"title": "3.3 Evaluation", "content": "The evaluation of these downstream models is conducted from two critical viewpoints: optimization and applicability.\nOptimization: We scrutinize the parameters utilized during the pre-training phase of the foundation models, focusing on their architectural designs and SSL methodologies, to determine their impact on the representation of ECG data. The evaluation includes: Impact of Patch Size, Influence of Block Depth, Role of Embedding Size.\nApplicability: We assess the practical effectiveness of these models in ECG classification tasks, considering variables such as data availability and case ratios. This involves validating the efficacy of these models in real-world scenarios characterized by low incidence rates and restricted data availability."}, {"title": "3.4 Analysis", "content": ""}, {"title": "3.4.1 Research Questions", "content": "Our research endeavors to elucidate the potential of self-supervised learning (SSL) in the context of electrocardiogram (ECG) modeling through foundation models. It systematically addresses several pivotal research questions designed to deepen our understanding of model development and integration within practical settings:\nRQ1. What architectural designs of foundation models are most effective for capturing the nuanced semantics of ECG?\nThe foundation model is pivotal in generalizing electrocardiogram (ECG) data effectively. Determining the most suitable architectural designs to capture diverse ECG signal features\u2014including beat rate, pqrst values, morphological attributes, and temporal correlations\u2014is critical. We conducted an extensive grid search to investigate the impact of key parameters such as patch size, depth, and embedding size on data representation. This search involved testing patch sizes of 250, 125, and 60; depths of 2, 4, and 8; and embedding sizes of 256, 512, and 1024, with embedding sizes set to half that of the encoders. This methodical exploration aims to identify the architectural designs that most accurately capture the complex nuances of ECG signals.\nRQ2. Which are the most adept at accurately encoding the complex representations of ECGs among the SSL methods?\nSelf-supervised learning (SSL) techniques fundamentally influence the way foundation models represent and processdata. Recent studies suggest that the effectiveness of models trained via generative learning (GL) and contrastive learning (CL) varies depending on their application in specific downstream tasks such as classification and prediction (Dubois et al., 2023). Our research examines how different SSL methods impact the representation of ECG data when tailored to these tasks. We assessed the effectiveness of each model using four distinct metrics: approximation error, representation usability error, probe generalization error, and encoder generalization error, as outlined in the risk decomposition approach (Dubois et al., 2023). Detailed methodologies for these metrics are further elaborated in Section B.1.\nRQ3. How can foundation models be effectively implemented within clinical environments to optimize diagnostic processes?\nThis aspect of our study addresses the deployment of foundation models in scenarios characterized by limited data and scarce labeling resources. We evaluated the performance of the optimal architectural designs identified previously under various conditions of data scarcity. Specifically, we simulated scenarios with case ratio percentages of 1%, 2%, and 5%, and data usage percentages of 10%, 25%, and 50%. The objective is to determine the adaptability and efficacy of these models in diverse real-world situations, assessing their performance across different levels of data availability and their potential reliability and utility in resource-constrained environments.\nThese questions guide our exploration into the practical applications and theoretical implications of SSL techniques in medical informatics, particularly within the realm of cardiac health diagnostics. The findings from our study are expected to provide valuable insights into the development and deployment of foundation models for ECG analysis, with significant implications for clinical practice and future research directions."}, {"title": "4 Experimental Results", "content": "In this section, we provide a summary of the experimental results. Due to paper length limitations, the complete results are included in Appendix C."}, {"title": "4.1 Dataset", "content": "For our study, the data was utilized in two primary stages: the foundation model training and the downstream model training and testing.\nWe used all available unlabeled data for training the foundation model. This extensive use of unlabeled data allowed the foundation model to learn robust and generalized representations of ECG signals through self-supervised learning techniques.\nFor the downstream tasks, we utilized labeled data. The labeled dataset was split into two parts: 80% of the data was used for training and validation, while the remaining 20% was reserved for the test. It is important to emphasize that the test set was strictly not used during any training step of both the foundation model and the downstream model. This separation ensures that the performance evaluation of the downstream models reflects the foundation models' ability to generalize to unseen data, thereby providing an unbiased assessment of the foundation models' effectiveness."}, {"title": "4.2 Environment", "content": "The experiments were conducted using a high-performance computing infrastructure equipped with a parallel distributed architecture, including 16 NVIDIA A100 GPUs. This setup facilitated the efficient handling of the extensive datasets and computationally intensive processes required for our study. To ensure consistent training conditions across all phases, we adopted a uniform set of optimization parameters. Throughout the pre-training, linear probing, and fine-tuning stages, we applied a fixed learning rate of 1 \u00d7 10-4 and a weight decay of 1 \u00d7 10\u20135. These parameters were selected to optimize convergence and minimize the risk of overfitting.\nThe initial stage involved pre-training 81 distinct foundation models (all combinations of 3 patch sizes, 3 block depth, 3 embedding sizes, and 3 SSL methodologies) on a large dataset of 1.1 million unlabeled electrocardiograms (ECGs). This extensive pre-training was conducted over approximately one week for each, aiming to develop robust data representations that form the basis for later supervised learning tasks.\nFollowing SSL pre-training, the top layers of each model were replaced with new linear classifiers, while the underlying base layers retained their pre-trained weights. During this phase, training was restricted to these top layers using a subset of labeled data to assess the effectiveness of the SSL-derived features under the established optimization parameters. This process, known as linear probing, evaluates the quality of the learned representations without updating the pre-trained model's weights."}, {"title": "4.3 Overall Performance", "content": ""}, {"title": "4.3.1 Foundation Model Loss", "content": "The results regarding the loss of foundation models, categorized by the learning methods of Contrastive Learning (CL), Generative Learning (GL), and Hybrid Learning (HL), are summarized in Table 3. Detailed evaluations of the loss variations across different structure settings for these models were extensively analyzed in Appendix C, with the complete results of the evaluation presented in Tables 1, 2, and 3.\nThe findings illustrated that the structure settings significantly influenced the efficiency of foundation models under different learning frameworks. CL-based models registered the lowest loss with a patch size of 250, suggesting that larger patch sizes enhanced the effectiveness of contrastive mechanisms by encapsulating more comprehensive information cues. Conversely, models based on GL and HL recorded minimal loss at a smaller patch size of 60, indicating a preference for more granular data segmentation in these learning approaches.\nAdditionally, block depth emerged as a crucial factor affecting model performance, with a depth of 2 blocks proving to be most effective across all models. This highlighted an optimal balance between model complexity and computational manageability. For embedding sizes, CL-based models demonstrated optimal performance with an embedding size of 512, sufficient for capturing essential features without excessive complexity. In contrast, GL and HL models exhibited the best loss metrics at a higher embedding size of 1024, implying a need for more detailed feature extraction to achieve effective learning of ECG representation.\nThese findings underscored the intricate dependencies between model architectural parameters and their operational efficacy within different SSL paradigms, enriching the understanding of how specific architectural designs could be optimized to enhance the performance of SSL-based foundation models in various applications."}, {"title": "4.3.2 Linear Probing Performance", "content": "Table 4 provided a comprehensive comparison of the linear probing performance with foundation models trained using Contrastive Learning (CL), Generative Learning (GL), and Hybrid Learning (HL) against a baseline model initialized with random encoder weights. The evaluation criteria included AUROC and AUPRC across four downstream tasks: MI, STTC, CD, and HYP.\nPerformance of the AUROC revealed that the Hybrid Learning (HL) method consistently outperformed the other approaches, achieving the highest AUROC for MI (0.8318) and CD (0.8411). This indicated HL's superior ability to distinguish between classes in these tasks. Generative Learning (GL) demonstrated the highest AUROC for STTC (0.8458), while Contrastive Learning (CL) led in the HYP task (0.8467). When examining AUPRC, which evaluated the precision-recall trade-off, HL again excelled with the highest AUROC for MI (0.616) and CD (0.635), underscoring its robustness in these predictive tasks. GL, on the other hand, showed the highest AUPRC for STTC (0.6442).\nOverall, the performance criteria indicated that Hybrid Learning (HL) achieved the highest score (5.559), suggesting it was the most balanced and effective method among the three evaluated. Contrastive Learning (CL) followed with a score of 5.4628, while Generative Learning (GL) scored 5.1559, both outperforming the baseline of 4.8713. This comprehensive evaluation highlighted the efficacy of Hybrid Learning (HL) in achieving superior performance across various metrics, thereby making it a promising approach for foundation model training in self-supervised learning contexts."}, {"title": "4.3.3 Fine-tuning Performance", "content": "Table 5 illustrates the performance for fine-tuned models based on the various foundation models, with a supervised learning (SL) model serving as the baseline. Among the fine-tuned models, Hybrid Learning (HL) demonstrated the highest AUROC for MI (0.9448) and CD (0.9449), reflecting its superior ability to distinguish between classes in these tasks. Contrastive Learning (CL) achieved the highest AUROC for STTC (0.9426), while also showing strong performance in other tasks. In terms of AUPRC, which evaluates the precision-recall trade-off, HL again excelled with the highest AUPRC for MI (0.8809) and CD (0.8841), further underscoring its robustness in these predictive tasks.\nThe superior performance of the SSL-based models compared to the SL baseline can be attributed to the inherent advantages of self-supervised learning. SSL-based models leverage vast amounts of unlabeled data to pre-train models, capturing a broad range of patterns and features that might not be as effectively learned through limited labeled data in SL. This extensive pre-training enables foundation models to develop more robust and generalizable representations, which translate to improved performance when fine-tuned on specific downstream tasks. Additionally, SSL approaches like HL and CL facilitate better utilization of the data structure and intrinsic relationships, further enhancing model performance beyond what is achievable with end-to-end supervised learning alone.\nAnalyzing the task-specific characteristics, each performance of fine-tuned models with the various foundation models reveals insights into their strengths. For Myocardial Infarction (MI) detection, the highest AUROC and AUPRC of fine-tuned model with the HL-based foundation model indicate its effectiveness in identifying subtle variations in heart signals, likely due to its balanced pre-training approach that captures a wide range of features. For ST/T Changes (STTC) detection, the highest AUROC of fine-tuned model with the CL-based foundation model suggests that its contrastive approach, which emphasizes differences between samples, is particularly suited for identifying specific changes in ECG waveforms. In the case of Conduction Disturbance (CD), the highest AUROC and AUPRC of fine-tuned model with the HL-based foundation model highlights its capability to generalize well from pre-training, capturing necessary patterns indicative of conduction disturbances. For Hypertrophy (HYP), detecting structural changes in the heart muscle higher AUROC of fine-tuned model with the CL-based foundation model indicates that contrastive learning is effective in distinguishing features associated with hypertrophy, possibly due to its focus on capturing diverse structural representations.\nOverall, the performance criteria suggest that Hybrid Learning (HL) is the most balanced and effective SSL method, achieving the highest overall score. Contrastive Learning (CL) and Generative Learning (GL) also performed well, surpassing the baseline model, but HL's ability to consistently achieve high scores across multiple tasks highlights its potential as a robust approach for foundation model training in self-supervised learning contexts. The superior"}, {"title": "4.4 Impact of pre-training strategies on AUROC", "content": "To answer the resaerch question, we aim to validate the linear probing and fine-tuning processes on foundation models utilizing different learning methodologies\u2014contrastive learning (CL), generative learning (GL), and a hybrid approach combining both (HL). The four binary classification tasks\u2014Myocardial Infarction (MI), ST/T Changes (STTC), Conduction Disturbance (CD), and Hypertrophy (HYP)\u2014serve as benchmarks to assess the efficacy of these learning techniques under varying parameter settings, such as patch size, embedding size, and block depth."}, {"title": "4.4.1 Patch Size", "content": "Foundation Model Loss: As shown in both Figure 3a and 4a, the loss of CL-based foundation model did not show a consistent trend with patch sizes (e.g., FM Loss showed v-shaped trend with 2-block depth and 256-embedding size, and opposite with 4-block depth and 512-embedding size). The loss of GL-based foundation model showed increasing trend, indicating less learning efficacy in Figure 3b and 4b. In last, the loss of HL-based foundation model also increased with larger patch sizes but less than that of GL-based as depicted in Figure 3c and 4c.\nLinear Probing Performance: As shown in Figure 3a, for the linear probing results based on the CL-based foundation model, AUROC for MI generally decreased with larger patch sizes, with a noticeable drop at mid-range patch sizes (e.g., MI AUROC dropped significantly around the 125 patch size). CD and HYP showed slight decreases with larger patchsizes but were more stable compared to MI, while STTC remained generally stable with slight fluctuations. As shown in Figure 3b, for the linear probing results based on the GL-based foundation model, MI and CD showed improvement with larger patch sizes but with some fluctuations (e.g., AUROC for MI improved from 0.62 to 0.67 as patch size increased with 4-block depth and 512-embedding size), while STTC and HYP showed slight decreases with larger patch sizes. As shown in Figure 3c, for the linear probing results based on the HL-based foundation model demonstrated more stable AUROC for all tasks with slight decreases with larger patch sizes.\nFine-Tuning Performance: In Figure 4a, for the fine-tuning results based on the CL-based foundation model, high and stable AUROC for MI, STTC, CD, and HYP were observed, with slight improvements as patch sizes increased (e.g., MI AUROC remained above 0.93 and slightly improved). As shown in Figure 4b, for the fine-tuning results based on the GL-based foundation model, AUROC remained high and stable across different patch sizes for all tasks, with slight improvements as patch sizes increased (e.g., AUROC for HYP improved on all parameter variations). As depicted in Figure 4c, the HL-based foundation model showed stable AUROC for STTC and CD with slight improvements as patch sizes increased. While the AUROC of MI and HYP were decreased slightly as patch sizes increased. However, the fine-tuning results based on the HL-based foundation model achieved the highest AUROC for MI, STTC, CD, and HYP, further improving with larger patch sizes and FM Loss decreased, indicating effective learning."}, {"title": "4.4.2 Block Depth", "content": "Foundation Model Loss: As shown in both Figure 5 and 6, the loss of CL-, GL-, and HL-based foundation model increased with deeper block depth, indicating worse learning (e.g., CL-based FM Loss increased from 524 to 804 with 125-patch size and 256-embedding dims, GL-based FM Loss increased 12.6 to 14.3, and HL-based FM Loss increased from 363 to 368 as block depth increased).\nLinear Probing Performance: As shown in Figure 5a, for the linear probing results based on the CL-based foundation model, AUROC for MI, STTC, CD, and HYP generally showed decreases with deeper block depth (e.g., AUROC for CD decreased from 0.72 to 0.69 with deeper block depth). The FM Loss increased with deeper block depth. The linear probing results based on the GL-based foundation model in Figure 5b demonstrated the slight increasing trends, with AUROC showing variability but generally improving with deeper block depth. The linear probing results based on the HL-based foundation model in Figure 5c exhibited better performance compared to other linear probing methods, with AUROC improving slightly with deeper block depth.\nFine-Tuning Performance: For the fine-tuning results based on the CL-based foundation model in Figure 6a, high AUROC for all tasks were observed, slightly improving with deeper block depth (e.g., AUROC for MI increased from 0.93 to 0.94). As shown in Figure 6b, for the fine-tuning results based on the GL-based foundation model, AUROC remained stable across different block depths, with slight improvements. As shown in Figure 6c, the fine-tuning results based on the HL-based foundation model achieved the highest AUROC for all tasks except for HYP, further improving with deeper block depth, while AUROC of HYP slightly decreased with 125-patch size and 256-embedding size."}, {"title": "4.4.3 Embedding Size", "content": "Foundation Model Loss: As depicted in Figure 7 and 8", "Performance": "As shown in Figure 7a"}, {"Performance": "For the fine-tuning results based on the CL-based foundation model in Figure 8a, AUROC for all tasks were slightly decreased with larger embeddings (e.g., AUROC for HYP decreased in all parameter settings). As shown in Figure 8b, for the fine-tuning results based on the GL-based foundation model, AUROC remained stable across different embedding sizes for all tasks, with slight improvements as embedding sizes increased. The fine-tuning results based on the HL-based foundation model in Figure 8c showed slight improving with larger embedding sizes, further achieving the highest AUROC for all tasks"}]}