{"title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views", "authors": ["Liang Zhao", "Zehan Bao", "Yi Xie", "Hong Chen", "Yaohui Chen", "Weifu Li"], "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field, achieving both panoptic and interactive segmentation of 3D scenes. However, existing methodologies often overlook the critical need for reconstructing specified targets with complex structures from sparse views. To address this issue, we introduce TSGaussian, a novel framework that combines semantic constraints with depth priors to avoid geometry degradation in challenging novel view synthesis tasks. Our approach prioritizes computational resources on designated targets while minimizing background allocation. Bounding boxes from YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask predictions, ensuring semantic accuracy and cost efficiency. TSGaussian effectively clusters 3D gaussians by introducing a compact identity encoding for each Gaussian ellipsoid and incorporating 3D spatial consistency regularization. Leveraging these modules, we propose a pruning strategy to effectively reduce redundancy in 3D gaussians. Extensive experiments demonstrate that TSGaussian outperforms state-of-the-art methods on three standard datasets and a new challenging dataset we collected, achieving superior results in novel view synthesis of specific objects. Code is available at: https://github.com/leon2000-ai/TSGaussian.", "sections": [{"title": "Introduction", "content": "Learning 3D representations from 2D images has long been a fundamental objective in computer vision, underpinning a wide range of applications such as augmented reality (Dai et al. 2020), robotics (Lu et al. 2024a), and autonomous navigation (Jin et al. 2024). Recent advances in 3D Gaussian Splatting (3DGS) have improved this field. These advancements allow for more effective reconstruction of 3D scenes (Kerbl et al. 2023). However, existing methods typically struggle to maintain semantic consistency and avoid geometric degradation when isolating specific targets in cluttered environments (Jain, Tancik, and Abbeel 2021; Yu et al. 2021a). Moreover, optimizing computational resources while preserving the quality of the reconstructed scene remains a significant challenge.\n2D semantic segmentation typically requires lower annotation costs compared to 3D methods. Especially, the Segment Anything Model (SAM) has been proven to achieve competitive or even superior zero-shot performance in scene understanding compared to previous supervised models (Cen et al. 2023). As a result, the 2D semantic segmentation of SAM can be employed to enhance the capture of 3D details in 3DGS. For example, GaussianEditors performs semantic tracking based on SAM's semantic segmentation, enabling more precise and efficient editing control (Chen et al. 2024). Gaussian Grouping assigns semantic attributes to each Gaussian primitive based on 2D semantic segmentation (Ye et al. 2025). These methods demonstrate promise in panoramic reconstruction and interactive semantics but face challenges with complex geometries and cluttered scenes (Wang, Zhao, and Petzold 2024). Existing methods often conduct interactive or panoramic segmentation post-reconstruction, resulting in considerable computational redundancy when focusing on specific objects.\nFor 3D reconstruction of specific objects, the input image sequence may be relatively sparse due to weather dependency, high capture costs, and time constraints (Wang et al. 2023). Although effective for forward-facing scenes in sparse views, they struggle to maintain geometric integrity in omnidirectional reconstruction (Niemeyer et al. 2022). Additionally, few algorithms introduce semantic constraints for reconstructing targeted objects from sparse views. Addressing these limitations requires developing approaches that effectively balance semantic constraints with depth regularization (Li et al. 2024b). To this end, we propose an innovative framework (illustrated in Fig. 1) that facilitates a cost-effective transition from 2D semantic labels to 3D semantic understanding, with a specific emphasis on the rapid reconstruction of targeted objects. Our approach is engineered to ensure robustness even when faced with sparse views. This paper primarily contributes the following:\n\u2022 We employ YOLOv9 for scene comprehension, utilizing it as a prompt for SAM to achieve cost-effective 2D semantic segmentation, which guides the training of 3D semantic encoding.\n\u2022 We design a semantic operator to identify unnecessary Gaussians during target reconstruction and implement a pruning strategy to minimize redundant computations.\n\u2022 We integrate monocular depth estimation as a prior and utilize the depth estimation loss to enhance the robustness of 3DGS in sparse views."}, {"title": "Related Work", "content": "The advancement of 3DGS has emerged as a crucial technique for novel view synthesis. A growing body of research has introduced significant enhancements to 3DGS, refining its methodologies and broadening its scope of application. To mitigate artifacts during the reconstruction process, Mip-splatting incorporates a smoothing filter that regulates the size of Gaussian primitives by controlling the maximum sampling frequency (Yu et al. 2024). VDGS further advances the field by proposing using a neural network, similar to NeRF, to replace spherical harmonics in the original 3DGS (Malarz et al. 2023). This innovation improves colour and opacity attributes, yielding more realistic rendering effects. LightGaussian accelerates the rendering speed by identifying Gaussian primitives with minimal contribution to scene reconstruction (Fan et al. 2023). It employs a pruning and restoration process that effectively reduces redundancy in Gaussian counts while preserving the visual quality. Despite the significant progress (Lu et al. 2024b; Morgenstern et al. 2025), existing methods struggle to achieve high-quality reconstruction for specific semantic targets in complex environments. Therefore, exploring automated 3D reconstruction methods for specific targets remains a major challenge.\nUnderstanding and tracking the 3D scene are vital to achieve 3D reconstruction for specific targets (Takmaz et al. 2023). However, the complexity of 3D shapes poses a challenge in maintaining semantic consistency. Mainstream methods typically integrate 2D mask predictions from SAM with 3D spatial consistency constraints to embed semantic features into 3D scene representations (Zhang et al. 2023), enabling effective segmentation, understanding, and editing of scenes. Geometry-Preserving Neural Radiance Fields (GP-NeRF) further integrates the Transformer architecture to jointly aggregate radiance and semantic embedding information, enhancing the discrimination and quality of the semantic field while maintaining geometric consistency (Li"}, {"title": "Method", "content": "Building on recent advancements in 3DGS, we develop an algorithm that extends high-performing 2D scene understanding techniques to the 3D domain in sparse views. As illustrated in Fig. 1, our approach focuses on constructing a 3D scene representation for specific targets by utilizing semantic and depth constraints. The proposed TSGaussian offers the following technical highlights: 1) 2D detection, semantic segmentation, and 3D reconstruction of specific target objects; 2) Separation of reconstructed 3D objects based on their semantic identities, enabling distinct handling of different semantic components; 3) High-quality 3D reconstruction and rendering in sparse-view scenarios without compromising reconstruction quality."}, {"title": "Consistent Targeted Semantic Segmentation", "content": "In order to obtain accurate semantic information, we first integrate YOLOv9 and SAM to generate more accurate 2D semantic masks. Subsequently, a zero-shot tracker is employed to align semantic identities across different views.\nWe deploy the YOLOv9 model with pre-trained weights to identify targets within the multi-view image collection, generating bounding boxes for each image and thus determining the total number of targets in the 3D scene. This method requires only low-cost annotations and fine-tuning during training, making it scalable to complex targets within custom datasets. To obtain the corresponding 2D masks for each target, we use these bounding boxes as prompts for SAM, which automatically generates 2D masks for each image. As shown in Fig. 2(a), our approach captures the semantic masks that focus exclusively on the specified targets, which is superior to the coarse and panoramic semantics obtained using SAM alone.\nAs shown in Fig. 2(b), targets with identical semantics may be assigned different identity documents (IDs) across multiple views. To ensure the consistency, a well-trained zero-shot tracker is employed to correlate the IDs (Cheng et al. 2023). This approach helps associate masks with the same identity across different views and assigns a unique ID to each 2D mask within the 3D scene."}, {"title": "Semantic Constraints for 2D-to-3D", "content": "To ensure that the results of 2D identity tracking effectively supervise the 3DGS rendering process, Identity Encoding is utilized to assign semantic attributes to the Gaussians. This is followed by a Semantic Rendering module that projects the Gaussians back into 2D semantic masks for calculating the 2D identity loss. Additionally, a 3D semantic regularization loss is introduced to enhance consistency. To further improve the efficiency of the reconstruction process, we implement a Semantic-Driven Gaussians Control and Pruning strategy, which adaptively clones, splits, and prunes the"}, {"title": "Gaussians based on their semantic attributes.", "content": "We introduce an identity encoding mechanism for each Gaussian function, where the identity code is a learnable, highly compact vector that assigns a unique instance ID to each target object. To optimize these identity codes, a differentiable Gaussian renderer is employed, allowing the identity code to become a learnable attribute. This approach enables end-to-end training using optimization algorithms like gradient descent. To achieve the semantic rendered masks, we have employed a neural-point-based $a'$-rendering technique, where $a'$ denotes the influence weight assessed for each pixel (Kopanas et al. 2022, 2021). The identity feature $E_{id}$ of the rendered 2D mask for each pixel is obtained by a weighted sum of the identity codes as the following:\n$E_{id} = \\sum_{i \\in N} e_i \\alpha_i \\prod_{j=1}^{i-1}(1-\\alpha_j)$, (1)\nwhere the $e_i$ is a 16-dimensional identity encoding for each Gaussian. The identity feature $E_{id}$ can be transformed into identity classification through a linear layer followed by a softmax layer as $softmax(f(E_{id}))$. For simplicity, this will be denoted as $F(E_{id})$ in the following text.\nThe grouping loss consists of 2D identity loss and 3D regularization loss. The 2D identity loss $L_{2d}$ is calculated as:\n$L_{2d} = H(P,F(E_{id}))$, (2)\nwhere H represents the cross-entropy loss and P denotes the correct identity classification. To ensure that the identity encodings of the top K-nearest 3D Gaussians are closely matched in terms of their feature distances, the 3D regularization loss for m sampling points is formalized as follows:\n$L_{3d} = \\frac{1}{mK} \\sum_{j=1}^m \\sum_{i=1}^K F(e_j) log(\\frac{F(e_i)}{\\sum_{i=1}^K F(e_i)})$, (3)\nThe grouping loss $L_{id}$ is ultimately computed as follows:\n$L_{id} = \\lambda_{2d}L_{2d} + \\lambda_{3d}L_{3d}$. (4)\nThe 3DGS technique employs adaptive control to dynamically adjust the Gaussian density, transitioning from a sparse to a denser configuration. However, this process relies solely on view-space position gradients and does not account for semantic constraints, which may result in the generation and accumulation of semantically incorrect Gaussians. To address this issue, we implement a control mechanism based on Gaussian semantic attributes during the densification process. First, we determine the region of interest (ROI) in 3D space according to the semantic attributes of the Gaussians. View-space position gradients are then computed only for Gaussians within this ROI. Gaussians in under-reconstructed areas are cloned, while those in over-reconstructed regions are split. Additionally, since the semantic attributes of Gaussians may change during the densification iterations, we apply pruning to remove Gaussians that no longer belong to the ROI, thereby reducing the accumulation of error. A floating mask is introduced for each training view to leverage the explicit representation of the 3D Gaussian distribution, eliminating incorrect semantic artifacts."}, {"title": "Multi-Scale Depth Regularization", "content": "As illustrated in Fig. 3, insufficient attention to depth errors also leads to artifacts in the context of sparse views. To mitigate this issue, we incorporate a monocular depth estimator as an additional spatial geometry prior to generate depth maps for each input view (Ranftl et al. 2020). To avoid overfitting on the target depth map, a multi-scale depth regularization loss including a soft-hard depth loss and a global-local depth loss is introduced to learn the shape parameters {\u03bc, s, q, a} of 3D Gaussians and enhance sensitivity to depth errors.\nThe soft-hard depth loss specifically focuses on the opacity a and center \u03bc, as these parameters represent the object's spatial occupancy and location, respectively. During the depth regularization process, the scale parameter s and the rotation parameter q are kept fixed to prevent overfitting. A hard depth map $D_{hard}$ is rendered, primarily composed of the nearest Gaussians along the rays emanating from the camera center and passing through each pixel. In this process, only the center \u03bc remains as an optimizable parameter, with Gaussian center regularization encouraging the hard depth to align with the monocular depth estimation. The hard depth loss over the target objects T is calculated as:\n$L_{hard}(T) = L_2(D_{hard}(T), D(T))$, (5)\nwhere D represents the output of the depth estimator. Similarly, we fix the Gaussian center \u03bc, render a soft depth map"}, {"title": null, "content": "$D_{soft}$, and use depth regularization to adjust the opacity a. The soft depth loss for this process is as follows:\n$L_{soft}(T) = L_2(D_{soft}(T), D(T))$. (6)\nThen the soft-hard depth loss is formulated by:\n$L_{SH} = R_{hard} + R_{soft}$. (7)\nThe global-local depth loss is employed to finely correct minor errors in depth estimation. To achieve this, the predicted depth map and the depth estimator's output are divided into smaller patches, which are then normalized on a local scale to have a mean value of 0 and a standard deviation close to 1. The normalized result is denoted as $D^{LN}$. We utilize the global standard deviation of the depth map in place of the standard deviation of the local blocks to obtain $D^{GN}$. Then the global-local depth loss $L_{GL}$ is defined as:\n$L_{GL} = L_2(D^{GN}, D^{GN}) + \\gamma L_2(D^{LN}, D^{LN})$. (8)\nThus, the multi-scale depth loss is formulated by:\n$L_D = \\lambda_{SH}L_{SH} + \\lambda_{GL}L_{GL}$. (9)\nFor color reconstruction, we combine an L1 reconstruction loss with a D-SSIM measure to ensure the structural similarity between the rendered image and the actual image:\n$L_{color} = \\lambda L_1(\\hat{I}, I) + \\lambda_{DSSIM} d_{DSSIM}(\\hat{I}, I)$. (10)\nThe total loss function for training is formulated by:\n$L = L_{color} + \\lambda_{Id} L_{Id} + \\lambda_{D} L_{D}$. (11)"}, {"title": "Experiments and Analysis", "content": "Unlike panoramic reconstruction, this research focuses on scenes dedicated to specific targets for the evaluation. Notable scenes include \"garden\u201d from Mip-NeRF 360 (Barron et al. 2022), \"bouquet\" from LERF Datasets (Kerr"}, {"title": null, "content": "of a specific object, we uniformly extracted one-third of the original views to ensure completeness and divided these views evenly into training and test sets. Inspired by previous sparse view setting, the camera poses are assumed to be known through calibration or other methods. The number of views is set as 10 in the \"bear\" scene, and is 30 for the larger \"bouquet\" and \"garden\u201d scenes. Note that the output bounding boxes of YOLOv9 model serve as the prompts for the SAM to generate high-quality masks. The pretrained YOLOv9 on the COCO dataset is directly utilized to detect targets for the public scenes. In our own dataset, a low-cost box annotation method using 12 videos is employed to fine-tune the pretrained YOLOv9 model, thereby enhancing its ability to detect citrus targets. Furthermore, to address the recognition challenges of YOLOv9 in target frames, we also use the output bounding boxes from adjacent frames as the detection output, ensuring the integrity of the mask.\nWe compare the proposed method with the original 3D GS (Kerbl et al. 2023), as well as its variants including Gaussian Grouping (Ye et al. 2025), DNGaussian (Li et al. 2024b), and SparseGS (Xiong et al. 2023). To ensure a fair comparison, all compared algorithms use the same semantic masks of specific objects.\nThe naive COLMAP is utilized to obtain the camera poses (Schonberger and Frahm 2016). The semantic masks are employed based on SAM with prompts by YOLOv9 and tracking was performed with DEVA to ensure cross-view identity consistency for each scene (Cheng et al. 2023). We randomly initialize 10,000 points as the initial gaussian. The model was trained for 10,000 iterations using an NVIDIA A6000 GPU."}, {"title": "Comparison Results", "content": "The quantitative and qualitative analyses are shown in Table 1 and Fig. 4, respectively. We found that other models often suffer from overfitting when reconstructing specific targets from sparse views. On public dataset, we outperform all baselines in terms of SSIM, LPIPS, and PSNR. In the \"bear\" scene, our PSNR exceeds that of 3DGS by 6.28. In the \"bouquet\" scene, our SSIM is 0.12 higher than that of 3DGS. Across various scenes, our method consistently achieves lower LPIPS values. In qualitative analysis, our method consistently produces clear outputs across all scenes and accurately recovers the geometric structure. A key feature of our approach is its ability to leverage depth"}, {"title": "Ablation and Analysis", "content": "As indicated by the red arrow in Fig. 6, the absence of depth regularization will lead"}, {"title": "Conclusion", "content": "In this study, we have proposed a novel approach that integrates depth regularization and semantic constraints to enhance the performance of 3D Gaussian Splatting. To minimize the impact of background noise, we optimize the Gaussian distribution by leveraging the SAM-based semantic segmentation with prompts from YOLOv9. Despite potential inaccuracies in 2D semantic information, our framework can achieve robust recognition in complex environments by utilizing the consistency across 3D views. Furthermore, we have introduced Multi-Scale Depth Regularization to reduce data acquisition costs and minimize redundant information, effectively mitigating artifacts during the reconstruction process. This method proves effective for studying target objects from sparse views, with the pruning of Gaussian primitives for specific targets. The experiments highlight the importance of integrating semantic and depth information in 3D reconstruction tasks, paving the way for future advancements in this field and expanding the applicability of our approach across diverse scenarios."}, {"title": "Limitations and Future Work", "content": "We strongly believe that 3DGS with semantic constraints and depth regularization holds significant potential for enhancing the quality of target-specific reconstruction and minimizing artifacts. By focusing on the precise reconstruction of specific targets, this approach not only establishes a robust foundation for subsequent 3D model applications but also contributes to the broader advancement of the field. Our next goal is to improve the combination of semantic and depth data to make the Gaussian reconstruction method more efficient and effective."}]}