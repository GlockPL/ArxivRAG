{"title": "ARADICE: Benchmarks for Dialectal and Cultural Capabilities in LLMs", "authors": ["Basel Mousi", "Nadir Durrani", "Fatema Ahmad", "Md. Arid Hasan", "Maram Hasanain", "Tameem Kabbani", "Fahim Dalvi", "Shammur Absar Chowdhury", "Firoj Alam"], "abstract": "Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes \u224845K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We will release the dialectal translation models and benchmarks curated in this study.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have consistently pushed the boundaries of Natural Language Processing (NLP), achieving state-of-the-art performance on a wide range of tasks. These models have excelled in areas such as machine translation, summarization, sentiment analysis, and even more complex applications like legal document analysis and creative writing (OpenAI, 2023; Touvron et al., 2023; Bubeck et al., 2023). Their remarkable ability to extract, reason, and generalize knowledge is fueled by training on vast amounts of data covering diverse topics and domains.\nHowever, the success of these models is heavily skewed towards languages with abundant resources, such as English (Bang et al., 2023; Ahuja et al., 2023). Low-resource languages, including Arabic and its various dialects, are significantly underrepresented in the datasets used to train these models. This disparity poses a substantial challenge, as LLMs require extensive and diverse data to perform effectively. Consequently, speakers of low-resource languages are at a disadvantage, unable to fully benefit from the advancements in NLP technologies. Recent efforts have been made to train Arabic LLMs (Sengupta et al., 2023) and to adapt multilingual models for Arabic (Huang et al., 2024; Touvron et al., 2023), but these models are predominantly tailored to MSA, leaving them less effective in handling dialectal Arabic (DA). To formalize this observation, we conduct a systematic study to benchmark Arabic and multilingual models in their performance on Dialectal Arabic. More specifically, we aim to answer the following questions:\n\u2022 Can LLMs effectively perform basic NLP tasks in dialects? (Understanding and Generation)\n\u2022 Can LLMs demonstrate reasoning, comprehension, and handle knowledge and misinformation in dialects? (Cognitive Abilities)\n\u2022 Are they aware of Arabic cultural knowledge? (Cultural Understanding)\nTo this end, we compiled a comprehensive suite of both existing and newly developed benchmarks to assess the capabilities of these models. We primarily evaluate their fundamental NLP abilities in encoding dialects through dialect identification (e.g. El-Haj et al. (2018)) and dialectal machine translation (e.g. Abdul-Mageed et al. (2023)) tasks. To assess broader cognitive abilities, we include benchmarks that gauge World Knowledge (ArabicMMLU (Koto et al., 2024)), Common Sense"}, {"title": "2 Datasets", "content": "First, we select a number of datasets that have been used for standard LLM benchmarking, including Arabic. The capabilities assessed on these datasets include understanding and generation, world knowledge, commonsense reasoning, reading comprehension, misinformation, and cultural understanding, as listed in Figure 1. We selected the datasets with a focus on diversity and linguistic compatibility. These datasets include tasks involving generation and multiple-choice questions. The datasets used in this study include: (i) four existing Arabic datasets for understanding and generation: Arabic Dialects Dataset (ADD) (El-Haj et al., 2018), ADI (Abdelali et al., 2024), and QADI (Abdelali et al., 2021), along with a dialectal response generation dataset (Naous et al., 2022), and MADAR (Bouamor et al., 2018b); (ii) seven datasets translated into MSA and dialects (Levantine and Egyptian), which include ArabicMMLU (Koto et al., 2024) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), OBQA (Mihaylov et al., 2018), Winogrande (Sakaguchi et al., 2021), Belebele (Bandarkar et al., 2024), and TruthfulQA (Lin et al., 2022); and (iii) AraDiCE-Culture, an in-house developed regional Arabic cultural understanding dataset. A detailed description of each dataset is in Appendix A.\nCultural Dataset: We curated a dataset of 180 culturally specific questions by hiring native annotators from the Gulf, Egypt, and the Levant regions to generate seed questions centered on cultural and country-specific themes. After reviewing all submitted questions, we selected those with varying answers across regions. To verify these differences, we appended a country name to each question and used Google Search to examine the top 5 search results. Only questions with distinct answers across countries were retained, resulting in a final set of 30 unique questions. These questions were then translated into the dialects of six countries within the targeted three regions (Gulf, Levant, and Egypt) using GPT-4. The questions span various categories, including public holidays, food, geography, and religion. For gold-standard answers, we followed the NativeQA framework (Arid Hasan et al., 2024). Annotators reviewed both the questions and the top 5 search results, combining web data with their cultural knowledge to provide the most accurate answers. Detailed guidelines and sample question-answer pairs are available in Appendices G, A.3 and Table 20."}, {"title": "3 Machine Translation and Post-Editing", "content": "MT is crucial for developing resources for low-resource languages with multiple dialects, like Arabic, which has MSA for formal use and diverse dialects like Egy and Lev for everyday communication. These dialects differ significantly from MSA and from each other, presenting unique challenges for NLP tasks.\nFor MSA, several studies have released datasets, including MMLU, HellaSwag, and Arc-Challenge by Okapi (Lai et al., 2023), as well as MMLU by AceGPT (Huang et al., 2024). Due to inconsistencies in the number of samples between the original datasets, we refrained from adopting them directly. Instead, we translated these datasets into MSA using Google Translate. Currently, no publicly available translation systems support direct translation between Arabic dialects or between English and specific Arabic dialects; most MT systems focus on MSA-English translation. To address this gap, we trained models to translate from MSA to various Arabic dialects. Translated datasets were manually post-edited for fluency and accuracy. Details of our dialectal MT and post-editing framework are provided below."}, {"title": "3.1 Dialectal MT", "content": "We develop MT models translating between MSA and two major dialects: Egy and Lev Arabic. By translating MSA benchmarks like ArabicMMLU, we create resources for evaluating LLMs' understanding of dialectal Arabic.\nData: For training our translation systems, we utilized several datasets, including the MADAR (Bouamor et al., 2018a), UFAL (Krubi\u0144ski et al., 2023), LDC (Mubarak, 2018), ArzEn-MultiGenre (Hamed et al., 2023), and the SADID (Abid, 2020). Please see Appendix B.1 for more details."}, {"title": "3.2 Post-Editing of MT (PEMT)", "content": "The translated datasets were manually post-edited to ensure fluency and adequacy. We post-edited the majority of the test sets (see Table 1), with the exception of BoolQ and TruthfulQA. For these two datasets, we chose specific samples for post-editing based on the following criteria: (i) shorter text length to minimize post-editing effort, (ii) cultural and religious compatibility with the Arab region, and (iii) linguistic compatibility with Arabic. We excluded language-specific samples, such as those asking for the origin of an English word. In Table 1, we provide statistics of the datasets along with their translation and post-editing status.\nGuidelines To assist the translators and maintain dataset integrity, we provided two sets of guidelines for each dataset (except Arabic MMLU). One set focused on post-editing translated samples in MSA, while the other targeted the same process for dialects. Each dataset's guidelines included: (i) details on the task components (e.g., one question and four answers), (ii) general instructions for correcting errors and improving fluency and adequacy, and (iii) specific instructions for handling unique cases within the datasets. Detailed instructions for the guidelines are provided in the Appendix F.\nTeam and Setup The translation team comprised 32 native speakers fluent in Lev, Egy, and MSA, with educational backgrounds ranging from Bachelor's to Master's degrees, and ages between 21 and 53. Many were professional translators. They received specific guidelines and training tailored to each dataset and dialect to ensure translation quality. To manage the post-editing workload efficiently, we assigned one translator per item, which helped minimize costs and time. A random sample of post-edited texts was reviewed by the expert translators for quality assurance. The process was streamlined using an in-house annotation platform and 17 dedicated projects, resulting in the post-editing of \u224845K items. A third-party company handled the hiring and competitive compensation of translators."}, {"title": "4 Experimental Setup", "content": "Models: For the LLMs benchmarking experiments we used open models, such as Llama-3-8B-Instruct (Touvron et al., 2023), Mistral-7B-Instruct (Jiang et al., 2023), AceGPT-13B-chat (Huang et al., 2024) and Jais-13b-chat (Sengupta et al., 2023). A random model was used as a baseline to evaluate the relative performance of these LLMs. We have chosen to use only open models to reduce the computational budget.\nPrompt: In our experiments, we employed zero-shot learning to observe performance differences across dialects. Since few-shot learning is known to enhance performance, we did not conduct experiments with it in order to simplify the experimental setups and reduce computational costs. We used"}, {"title": "5 Results", "content": "We evaluate the models' ability to encode and generate dialectal Arabic, focusing on tasks like dialect identification, dialect generation, and MT."}, {"title": "5.1 Understanding and Generation", "content": "We evaluate the models' ability to encode and generate dialectal Arabic, focusing on tasks like dialect identification, dialect generation, and MT."}, {"title": "5.1.1 Dialect Identification", "content": "Figure 2 show that all LLMs struggle to distinguish between dialects, especially compared to SOTA models (Hassan et al., 2021). Performance varies across datasets: Llama 3 excels on QADI, while Jais outperforms it on ADI and ADD, likely due to differences in the data\u2014QADI uses tweets, ADI has speech transcripts, and ADD includes Arabic commentaries. Our error analysis (Figure 4) shows Llama 3 confuse Gulf with Lev, Mistral confuses MSA with Lev/Gulf, Jais often confuse Egy with Gulf, while AceGPT mistakes MSA for Gulf. The models tend to fall back on MSA knowledge when"}, {"title": "5.1.2 Dialect Generation", "content": "We assessed the models' ability to generate responses in dialectal Arabic. Initially, we used a dialectal response generation task (Naous et al., 2022), where models were prompted in a specific dialect and asked to generate a response. However, qualitative analysis revealed that models often explained the prompt instead of generating a dialectal response (see Appendix C.2 for examples). To address this, we simplified the task: models were given multiple response options, with only one in the correct dialect, and asked to select the best response. Accuracy scores for this MCQ response selection task are shown in Figure 3. Overall, models performed better on the Gulf dialect, with Arabic LLMs like Jais and AceGPT outperforming Llama 3 and Mistral across all dialects."}, {"title": "5.1.3 Machine Translation", "content": "To assess the models' capacity to encode and generate dialectal knowledge, we use Dialect-to-English, MSA and English, MSA-to-Dialect tasks. The former evaluates the ability to encode dialects, while the latter assesses generation capabilities. Results averaged across multi-dialectal MADAR test sets are shown in Table 2. Several observations emerge: Dialect-to-English translation scores are significantly higher across all models and dialects compared to translations into dialects. This suggests that models excel at understanding and encoding dialects more than generating them. While the models benefit from their similarity to MSA in encoding, they struggle with generating dialectal text due to differing vocabulary and data sparsity. This highlights limitations in producing fluent and accurate dialectal language.\nAceGPT consistently scores higher in BLEU across all dialects, showing superior translation performance. Jais also performs well compared to Llama 3 and Mistral, which score lower. However, all models show significantly reduced BLEU scores for translating into dialects, complicating reliable performance comparisons in this direction."}, {"title": "5.2 Cognitive Abilities", "content": "We now assess the dialectal Arabic abilities in LLMs through world knowledge, reading comprehension, commonsense reasoning, and misinformation tasks, providing a thorough evaluation of their"}, {"title": "5.2.1 World Knowledge", "content": "The overall results in Figure 5 show that AceGPT and Jais excel across the board on both the EGY and LEV ArabicMMLU benchmarks proposed in this work, and the original MSA ArabicMMLU. This suggests that these Arabic-centric models are well-suited not only for MSA but also for dialectal variations. In contrast, Llama 3 and Mistral, which are not trained specifically on Arabic, struggle significantly more with EGY and LEV compared to their performance on ArabicMMLU. This highlights the effectiveness of the proposed dialectal benchmarks in revealing performance gaps in general models. These results demonstrate the need for models trained on dialectal data to handle the complexity of regional variations. Detailed individual results are provided in the Appendix D.1."}, {"title": "5.2.2 Commonsense Reasoning", "content": "Our evaluation across the PIQA, OBQA, and Winogrande (See Figure 6) reveals several key insights into model performance with respect to MSA and various Arabic dialects, and how these compare to English. Jais consistently outperforms other models across MSA and dialects, including Lev"}, {"title": "5.2.3 Reading Comprehension", "content": "Figure 8 show our results on reading comprehension tasks. The results across BoolQ and Belebele provide insights into how different models handle reading comprehension in MSA and dialects compared to English. For the BoolQ task, we observe that while Llama 3 performs exceptionally well in English, achieving 0.85 accuracy, its performance drops noticeably when applied to MSA (0.74) and even further in Lev (0.71) and Egy (0.73). This indicates a significant challenge for general models when transitioning from English to dialectal Arabic, despite the rich knowledge base Llama has. Interestingly, Mistral and Jais show a similar drop in performance across dialects, with Jais maintaining relatively higher accuracy in MSA, likely due to its Arabic-centric training. However, AceGPT stands out with the highest MSA score (0.77) and remains competitive across dialects, suggesting that it better adapts to the linguistic variations within Arabic.\nFor the Belebele dataset, the performance trends are similar. AceGPT and Jais lead the pack in MSA and dialects, with Jais achieving the highest MSA score (0.57) and performing equally well in Lev and Egy (0.49). This further demonstrates the ability of Arabic-centric models to leverage dialect similarities and perform well across diverse Arabic varieties. Llama and Mistral, while strong in general tasks, show a clear performance gap"}, {"title": "5.2.4 Misinformation", "content": "The results for the TruthfulQA task across Lev, Egy, and MSA show that model performances are often close to random, especially for Lev and Egy. Mistral performs best overall, particularly in Lev, but its scores are only slightly above random. We leave a detailed exploration of this for the future."}, {"title": "5.3 Cultural Understanding", "content": "The results on the cultural understanding task (MCQ version) (Figure 9), indicate that Jais is the most culturally aligned model, followed closely by AceGPT, both showing superior awareness of the Egyptian culture. One possible justification is that the Egyptian population (and consequently its online presence) is significantly larger than the other two regions. Llama 3 and Mistral generally show performances close to the random baseline, suggesting limited awareness of Arabic culture.\nIn a qualitative analysis of the models' responses (when prompting the models in a generation setup), Llama 3 frequently generated fictional entities (e.g., names of people or holidays) and lacked geographical and historical knowledge. For example, when asked about the highest mountain in Jordan, it incorrectly named a mountain in Saudi Arabia. Jais and AceGPT performed better overall, though AceGPT had some issues following instructions as accurately as Jais. Further discussion and example models' responses can be found in the Appendix E."}, {"title": "6 Related Work", "content": "There has been considerable work on benchmarking language models in Arabic, with prior research often including Arabic in multilingual benchmarks like XGLUE (Liang et al., 2020), XTREME (Hu et al., 2020), XTREME-R (Ruder et al., 2021),"}, {"title": "7 Conclusions", "content": "We develop benchmarks for dialectal Arabic using machine translation and human post-editing. Our benchmarks evaluate various NLP tasks, including understanding, generation, and cultural awareness across Gulf, Lev, and Egy dialects. Our results show that while Arabic-centric models like Jais and AceGPT perform better in dialectal contexts, they still face challenges compared to MSA and English. Performance varies by dialect and task, highlighting the need for more specialized training for effective handling of regional linguistic and cultural nuances. We release our dialectal benchmarks and models to support future research and advancements in NLP for low-resource languages. We will release the dialectal translation models and benchmarks developed in this study to support further research in NLP for low-resource languages."}, {"title": "8 Limitations", "content": "\u2022 Post-editing machine translation outputs is a tedious process, and the translator's choice of words can significantly impact translation quality. We provided clear instructions, conducted thorough training, and reviewed random samples to offer feedback. However, due to the size and diversity of the datasets, we could not review all datasets comprehensively.\n\u2022 Another issue to highlight is that most datasets, except ArabicMMLU, are adapted from English and thus are influenced by Western culture. While we made efforts in annotation guidelines and post-editing to address these cultural biases, the subjective nature of sensitivity means that some samples may still be considered sensitive by different individuals or communities.\n\u2022 While our study primarily focuses on Arabic dialects, it is limited in its coverage of the diverse dialects spoken across the Arab region. We mainly addressed Levantine, Gulf and Egyptian Arabic, but left out dialects such as Maghrebi and Sudanese. Future work should aim to fill these gaps by expanding coverage to a broader range of dialects, providing a more comprehensive evaluation of Arabic language models.\n\u2022 Due to resource limitations, we only evaluated our benchmarks using models up to 13B parameters. As a one-off experiment, we tested the ArabicMMLU task with a larger Jais 30B, model. We found (See results in Appendix D.1, Figure 13) Jais 30B to perform similarly to Jais 13B indicating that larger models do not necessarily show significant improvements in this case. Due to hardware limitations, we could not run Jais 70B models, but it would be interesting to compare the higher-scale Jais and Llama models to see if their increased scale can compensate for the lack of dialect-specific training."}, {"title": "Ethical Consideration", "content": "We do not anticipate any ethical issues in this study. We extended publicly available datasets through the PEMT process. Additionally, our culturally aware"}, {"title": "Appendix", "content": "A Details of the Dataset"}, {"title": "A.1 Understanding and Generation", "content": "To assess whether LLMs can effectively understand and communicate in various Arabic dialects, we focus on tasks such as dialect identification, dialectal generation and machine translation to and from dialects. These tasks are crucial for evaluating a model's ability to comprehend and generate dialectal content. We have selected datasets specifically designed for these tasks, as illustrated in Figure 1, to validate the models' proficiency in handling dialectal information."}, {"title": "A.2 Cognitive Abilities", "content": "To further evaluate the cognitive abilities of LLMs in understanding and communicating across different Arabic dialects, we curated a set of challenging tasks that assess the models' knowledge, reasoning, comprehension, and ability to handle misinformation. These tasks are designed to test whether the models can accurately interpret and generate responses in dialectal Arabic, a critical skill for real-world applications. The datasets were translated and post-edited from Modern Standard Arabic (MSA) and English into Egyptian and Levantine dialects to form a comprehensive benchmark that captures the linguistic diversity and complexity of Arabic dialects. Notably, the ArabicMMLU dataset (Koto et al., 2024) is already provided in MSA, so only dialectal translations were generated for this dataset. Below, we outline the datasets used, with further details on our translation and post-editing process provided in Section 3."}, {"title": "B Machine Translation", "content": "B.1 Data\nWe used the dataset listed below to develop the machine translation system. We provide a brief description of each dataset."}, {"title": "F Detailed PEMT Guideline", "content": "The purpose of PEMT (Brockmann et al., 2022) is to refine and improve the output generated by MT systems to ensure accuracy, fluency (i.e. they reflect the nuances of how the dialect is spoken), adequacy (i.e. they maintain the semantic meaning"}, {"title": "G Annotation Guideline for AraDiCE-Culture Dataset", "content": "For this annotation task, the following two information is provided to annotators.\n1. A question written in a dialect (e.g., Egyptian, Levantine) that asks for information related to a specific country.\n2. A list of a maximum 5 URLs of web pages that potentially have an answer to that question. Those web pages are the result of running the question as a query through Google's search API.\nThe annotation task is to identify the answer to the given question from the provided web pages. Therefore, the task is to visit the web pages through the links. The following guidelines should be followed when identifying the answer:\n1. Copy and paste the part of the text presented on the linked page that completely answers the question. This could be a few words, a long paragraph, or a short snippet.\n2. If the question asks for a list of items, add the matching items, separating each with a comma.\n3. Ensure that the text fully and accurately answers the question.\n4. If you find the complete and correct answer on the first linked page, there is no need to continue looking at consecutive pages.\n5. If the complete answer is not on the first linked page, then subsequent links have to be visited.\n6. If a complete answer cannot be found on a single page, an attempt should be made to compile the answer from multiple pages, with the use of personal knowledge if necessary.\n7. The answer should be general enough to cover the specified country. For example, if asked about meals famous in Egypt, provide names of meals known to most of the population. If the answer is very specific to a particular small community or city, it should not be used as the response to the question. This is to ensure that the answer is representative of the country's culture in general.\n8. The answer should be concise and to the point. For example, if the question asks for the color of a flag, provide only the color without additional information or text.\n9. If no answer can be found in the provided web pages:"}, {"title": "H Challenges in PEMT", "content": "Translation is a complex process in itself. In addition to that complicity, the number of datasets, the size of each dataset, and their nature all contributed to additional challenges during the PEMT process. Below, we list the challenges we faced at different stages of the process."}, {"title": "H.1 Challenges in the Pre-PEMT Process", "content": "H.1.1 Preparing Guidelines\nBefore starting the PEMT, we developed detailed guidelines to support the post-editing efforts. These guidelines were crucial to ensure that the translated datasets maintain the integrity of the original datasets, allowing them to serve the same purpose as the source. This step required significant time and effort to:\n1. Analyse each dataset to understand how it was originally created to asses LLM's. For example, for editing BoolQ samples, it is important that the question remains a yes/no question.\n2. Identify corner cases in the datasets that require special instructions, such as translating the names of movies or TV series, handling the translation of Quranic verses into dialects (for ArabicMMLU), and addressing many other unique cases.\n3. Determine whether post-editing MSA samples require a different set of guidelines compared to dialects. For MSA, we prioritized correcting grammatical errors. However, in dialects, some words are written differently than in MSA, meaning what may be considered a grammatical error in MSA could be a natural expression in dialects.\nH.1.2 The Size of the Datasets\nIn an attempt to reduce the effort and the time needed to finish post editing all the dataset, we opted to select only 1K samples from BoolQ. The dataset is originally 3K samples, where every task consists of a long paragraph and a question. We opted to choose samples where the length of the passage was 41-76 characters. Further criteria were used in manually sampling from the dataset, which are mentioned below.\nH.1.3 Addressing Cultural Mismatch\nDuring our analysis of the datasets, we noticed that some samples in Winogrande conflicted with the religion and culture of the Arab region. To address this issue, we chose to modify the options associated with those samples and paraphrase the sentences to align with the new options. Below is an example:\nExample\nOriginal Sentence: Logan ended the one-side relationship they were having with Robert, which made_feel relieved.\nOriginal Options: (i) Logan, (ii) Robert\nEdited Sentence: Logan ended the one-side relationship they were having with Elizabeth, which made_ feel relieved.\nEdited Options: Logan, Elizabeth\nH.1.4 Addressing Culture, Language, and Country Specific Samples\nIn all the datasets that were post-edited, except for ArabicMMLU, we consistently observed that the samples were designed with a focus on Western culture. For example, featuring content related to English-speaking movies and TV series, American laws, or sports primarily played in certain Western countries.\nH.1.5 Hiring Native Speakers\nFor post editing process, we hired native speakers of specific dialects. This is important to ensure a good quality post-editing. Finding such native speakers was also a challenge for us.\nH.2 Challenges during the Post-Editing Process\nAs mentioned earlier, the datasets varied in format, size, and complexity, making it difficult to accurately estimate the post-editing time. Our initial estimates did not align well with the actual time required. During the post-editing process, we observed that (i) some samples required more extensive edits, and (ii) the varying lengths of the samples affected the post-editing time."}]}