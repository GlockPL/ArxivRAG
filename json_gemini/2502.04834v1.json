{"title": "Lightweight Operations for Visual Speech Recognition", "authors": ["Iason Ioannis Panagos", "Giorgos Sfikas", "Christophoros Nikou"], "abstract": "Visual speech recognition (VSR), which decodes spoken words from video data, offers significant benefits, particularly when audio is unavailable. However, the high dimensionality of video data leads to prohibitive computational costs that demand powerful hardware, limiting VSR deployment on resource-constrained devices. This work addresses this limitation by developing lightweight VSR architectures. Leveraging efficient operation design paradigms, we create compact yet powerful models with reduced resource requirements and minimal accuracy loss. We train and evaluate our models on a large-scale public dataset for recognition of words from video sequences, demonstrating their effectiveness for practical applications. We also conduct an extensive array of ablative experiments to thoroughly analyze the size and complexity of each model. Code and trained models will be made publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual Speech Recognition (VSR) is a computer vision problem which aims to decode speech present in visual media of one or more speakers without the presence of sound. Alongside Audio Speech Recognition (ASR) which is its counterpart regarding audio-only media, such as recordings, it is a subset of the more general problem of Speech Recognition (SR). Applications of SR in everyday life can be found in several domains, notably in the medical assistance, where such a system can be utilized to provide assistance to patients that are speech-impaired or otherwise have communication difficulties. In the same spirit, various instances of existing accessibility platforms (e.g., mobile devices and interfaces) can benefit from the addition of a SR system, improving the everyday lives of many individuals.\nAutomatic SR systems have been adopted by the entertainment industry as well, where they have been employed to automatically generate captions of video segments, short clips and even entire full-length movies, or to produce transcriptions for older, silent films, by relying only the video stream. A striking example is the \"automatic captions\" feature provided by various video hosting platforms, such as YouTube, where an ASR unit generates captions for some videos using only the audio track. Employing such systems for the purpose of creating media transcriptions can save significant time and effort, when compared to using human lip readers or speech transcribers, as automatic systems have now surpassed humans for this task in both speed and accuracy, streamlining the overall process by reducing the associated costs.\nRecognizing speech units using only an audio signal is considered to be a less-demanding task, due to the lower dimensions of the audio stream (one-dimensional sequence), when compared with the spatio-temporal aspects associated with a video (three-dimensional sequence). Additionally, the amount and variety (e.g., languages) of publicly available audio data far surpasses that of video, allowing for a wider adoption and deployment of ASR systems. As a result of these factors, VSR models have been adapted to more specialized use-cases or in a secondary, auxiliary capacity to assist existing ASR units. However, in cases where applications of ASR are rendered ineffective by a significant amount of noise (e.g., crowded environments, multiple speakers) or in media where the audio track does not exist, such as in silent video recordings and films, employing a VSR system remains the only option.\nIn contrast to ASR, relying on visual cues to recognize speech is a more challenging process which involves more sophisticated and powerful architectures in order to produce meaningful results. An important distinction between the two modalities (audio and video) lies in the form of data to be processed, since a video contains spatio-temporal data of higher dimensionality compared to an audio stream and is therefore more demanding on computing resources. In addition, visual ambiguities between words that are produced from similar or identical mouth movements can cause erroneous results. A typical example includes the plural version of a word where the added suffix is hard to distinguish using only the visual information, while the added audio cue at the end of the word greatly contributes in successfully predicting the correct word. Distinguishing between such words demands powerful models with sufficient representation capabilities and as a result, VSR-specific systems rely on deep, large-sized models in terms of parameters in order achieve high performance. These networks also suffer from higher latency and prohibitive computational costs, factors which hinder their applicability in practical scenarios or applications where speed of operation is critical (e.g., embedded devices).\nA few of the aforementioned use cases of SR which are met in real-life scenarios typically demand on-line data processing and acceptable performance (e.g., low runtime) in order to be useful for the end-user. In this article, our goal is developing lightweight and compact architectures for visual speech recognition of words in order to enable such applications. To that end, we design deep neural networks by utilizing cost-effective network components that take advantage of operations with low computational overhead. Our models benefit from low sizes in terms of required parameters as well as reduced computational complexity, making them ideal for various practical applications. We conduct an extensive experimental analysis"}, {"title": "II. RELATED WORK", "content": "The task of visual speech recognition has been under active research for several decades. In order to tackle the problem, the paradigm typically followed by the literature splits it into simpler sub-problems and involves a series of steps. Initially, a spatial processing step aims to extract high-dimensional feature representations from the input. Subsequently, a sequential modeling step interprets the temporal inter-relations between the feature representations of each time-step of the sequence. Finally, a classification step aims to correctly predict the spoken word depicted in the frames.\nEarlier works commonly employed simple image transform techniques such as Principal Component Analysis or Discrete Cosine Transform for visual feature extraction from lip area images, while Support Vector Machines or Hidden Markov Models were used as classifiers [1]. Their vocabularies consisted of few words or single digits and as a result their applicability in real-life scenarios was rather limited. Furthermore, the available hardware at the time was insufficient to handle the non-trivial computational overhead, constraining the deployment and application of such methods. With recent progress in both of these domains, research efforts have been increased and powerful models achieving impressive results have been developed.\nDue to the remarkable advances in machine learning research of the last decade, the commonly-followed approach employs deep neural networks for the two initial steps, and a single densely-connected layer for the latter, since these archi-tectures have demonstrated high performance on such tasks. For visual feature extraction, Convolutional Neural Networks (CNN) have been established as the primary model of choice due to their ability to extract strong representations when trained on sufficient amounts of data. For the second step, the models used for sequential processing of the extracted features have been predominantly based on recurrent architectures, such as Long Short-Term Networks (LSTM) [2]\u2013[4] and Gated Recurrent Units (GRU) [5]\u2013[7]. The RNNs that are employed are typically set as bi-directional where they also process the reverse of the sequence, and subsequently their outputs are fused by concatenation.\nMore recently, variants of Temporal Convolution Networks (TCN) [8] have been proposed as alternatives to recurrent neural networks for sequential tasks, offering higher perfor-mance. Such architectures are gradually replacing recurrent ones due to their favorable characteristics regarding training stability and model simplicity [9]. An approach that utilizes several convolutions with different kernel sizes in each block of the standard TCN architecture is introduced in [10]. This model leverages the different effective receptive fields of the convolution kernels to increase its representation capabilities by incorporating more features across the time domain and has been adopted by several recent works (e.g., [11]\u2013[13]). A more complex model building upon the multi-kernel approach is proposed in [14], where dense connections are added in the architecture. In this way, more features are utilized per stage, increasing the model's depth and expressiveness at the cost of its size and required calculations.\nFor single word recognition, while a few works utilize only the visual stream (e.g., [2], [10]), others propose methods that utilize both streams in a complementary fashion to boost performance (e.g., [5], [7]). Typically, modality fusion mech-anisms of various complexities are employed to seamlessly integrate information from the video and audio streams. For instance, while a simple concatenation operation is used in [5], [7] propose a hybrid fusion network that utilizes features from both audio and video modalities with a decision fusion mechanism to predict the final word.\nThe majority of published works focuses on improving word recognition accuracy without considering the associated computational overhead that is a consequence of using sizable models that integrate several, oftentimes complex, components in their architectures. Consequently, the proposed models cannot be utilized in a resource-restricted environment due to the significant hardware requirements. In comparison, research aimed at lowering model complexity and improving efficiency has not received as much attention and remains at an early stage, with fewer works appearing recently.\nModels intended for applications by low-power hardware such as mobile devices were proposed in [15], where the authors design low cost networks by following lightweight convolutional neural network principles. More specifically, a compact spatio-temporal module is introduced in order to improve the performance of video recognition. It is combined with an architecture for visual feature extraction that is designed to be efficient by combining blocks with residual connections and depth-wise convolutions. A scaling factor controls the balance between performance and accuracy by adjusting the network size. To further improve the running speed and lower memory-intensity of the proposed models, a simple sequential network based on temporal convolutions is adopted for modeling the extracted features of the entire sequence.\nIn order to reduce the computational complexity of the entire visual speech recognition process, [16] propose changing each component used for feature extraction and sequence modeling. Using the model from [10] as a baseline, a lightweight convolutional neural network is used for visual feature extraction reducing the hardware requirements in terms of parameters and processing operations. Then, to further reduce the overall model's overhead, a lightweight block is introduced to the TCN architecture, replacing its standard operations with the commonplace depthwise-separable design paradigm of lightweight CNNs (e.g., [17]\u2013[19]). Finally, to recover some of the accuracy lost due to the drop in network capacity, a form of knowledge distillation is used to train the models.\nA large study benchmarking several deep learning architectures for extraction as well as sequence modeling was performed in [20]. The authors train and evaluate an extensive selection of recently-proposed models on a variety of publicly available datasets for both English and Chinese languages. The architectures used in their experiments cover a wide range of networks for feature extraction as well as sequence modeling, such as convolutional, vision transformers and temporal convolution networks.\nA more recent approach [21], [22] involves applying a parameter sharing technique to compress the components of VSR systems leading to more compact models without compromising accuracy. More specifically, the convolutional layers employed by both components in the VSR pipeline (i.e., feature extractor and sequential model) are replaced equivalent layers following a formulation that exploits a sum of Kronecker products to enable parameter sharing, greatly reducing the required size of each layer. The models achieve significant reductions in size and parameters for a minor performance penalty, which becomes more pronounced for higher rates of compression."}, {"title": "III. METHOD", "content": "The architecture of our proposed model follows the two-step design paradigm outlined in Section II, and its general structure is depicted in Figure 1. We experiment with efficient components to design lightweight speech recognition models with affordable computation demands. For both feature extraction and sequence modeling, we employ Ghost modules (Section III-B), greatly reducing network overhead, while we also propose a Partial Temporal Block (Section III-C) to develop ultra-lightweight TCN-based architectures suitable for scenarios with very-low-powered hardware. Using these components, our models can be deployed in several applications due to their low resource requirements."}, {"title": "B. Ghost Module", "content": "Ghost modules were proposed in [23] as a component that takes advantage of \u201ccheap operations\" to reduce its computation cost compared to a typical convolution layer. A ghost module achieves low computational overhead in two steps. First, a regular 1 \u00d7 1 convolution generates a set of feature maps from the input. A fixed ratio determines the number of channels in the generated feature maps, controlling the resource savings of the component. Typically, the ratio is set to 0.5 meaning that channels in the produced feature maps equal half of the input volume's channels.\nA \"cheap operation\u201d uses these intermediate feature maps to produce an additional set with the same channel size. The role of the cheap operation can be undertaken by any lightweight function, in the Ghost module, a depth-wise convolution with a kernel size of 3 \u00d7 3 is used. This convolution operates on each filter and processes the spatial information it contains, while preserving the amount of channels. Finally, the two distinct feature maps are concatenated along the channel dimension, meaning that the output volume matches the input's channels. Compared to the standard convolution operation, this formulation reduces the total amount of computation required since the initial 1 \u00d7 1 convolution generates feature map with fewer channels, and the depth-wise operation which is much cheaper computationally, is also applied on this volume, rather than the whole input. By preserving the original output size of a convolution layer, a Ghost module can act as a drop-in replacement for that layer to reduce computational overhead in a network architecture. The operations of the Ghost module can be summarized as:\n$X_1 = ReLU(BatchNorm(Conv_{1\\times1}(X)))$\n$X_2 = ReLU(BatchNorm(DWConv_{3\\times3}(X_1)))$\n$Out = concatenate([X_1, X_2]), (1)$\nwhere X refers to the input volume and DWConv to the depth-wise convolution.\nA drawback related to the representation capabilities of the Ghost module arises from the fact that the initial 1 \u00d7 1 convolution reduces the feature map channel dimensionality (to half) in order to keep the costs of the module low. Subsequently, the second (3 \u00d7 3 depth-wise) convolution operates on a subset of the input feature map and might miss some spatial relationships that would otherwise be captured by operating on the full input volume. Since half of the final feature map in the output of the Ghost module is produced from the 1 \u00d7 1 convolution without any spatial interaction between the pixels, the performance of the module is constrained. To alleviate this weakness, the authors of [24] propose an"}, {"title": "C. Partial Temporal Block", "content": "Reducing the size of the input feature map and operating on the resulting tensor is an effective approach to reduce the computational overhead of a network component, that has been followed by several works (e.g., [17], [18]). Within a network block, using the initial layer to reduce the channel dimension of an input volume, and applying the subsequent layers in the produced, smaller output, allows controlling the amount of calculations and enables the development of lightweight network components with low operating costs. An additional operation, typically the final one in a block, restores the channel dimension to match that of the input, usually in order to facilitate a residual connection. This design is commonly known as a bottleneck, since the intermediate feature maps have a lower number of channels.\nA similar approach [25]-[27] splits the input feature map across the channel dimension in two parts according to a fixed ratio, and applies two separate branches, one in each part. The operations in either branch can have any form, for instance, in [27] a regular convolution followed by two point-wise layers is applied on one branch, while the second branch leaves the input unchanged. To form the output, the results of each separate branch are merged along the channel dimension via concatenation.\nInspired by the practicality and results of methods following this paradigm (e.g., [25], [27]), we design the Partial Temporal Block, which follows the same principle. Our block allows for a wide network design flexibility as it can be tailored to each specific application constraints (e.g., hardware capabilities, dataset availability and size), and can even be part of a search space, in order to obtain the most optimal setup, depending on the problem. For an input volume X, the operations of the partial block can be summarized as:\n$X_1 = AveragePool(X)$\n$X_2 = BatchNorm(Conv_{1\\times1}(X_1))$\n$X_3 = BatchNorm(Conv_{1\\times5}(X_2))$\n$X_4 = Batch Norm(Conv_{5\\times1}(X_3))$\n$X_5 = Sigmoid(X_4). (2)$\nAdding DFC attention to the Ghost module incurs an increase in parameter size due to the additional convolutions but only a slightly higher computation cost in FLOPs. However, in practice, when tested on mobile devices, it achieves better performance at the same latency [24].\nwhere the channel split operation divides the input in two parts along the channel dimension according to a fixed ratio, F and G can be any type of operation, including sequences of layers, and the final concatenation merges the output of each branch in the channel dimension. A skip connection with the input is also added to facilitate easier training of deep architectures. The general block architecture is depicted in Figure 3a(a).\nAs a baseline, we employ the standard Temporal Convolution layer [9] as the core of our block in one branch. This layer uses a sequence of 1D causal convolutions with batch normalization and non-linear activation functions, repeated twice. The other branch uses no operations, this way the computational overhead of the block is greatly reduced. The overall architecture is comprised of four stages, where each stage is one Partial Temporal Block with increasing dilation rate for the non-point-wise convolutions. This way, the entire network is very lightweight in terms of hardware requirements (see Section IV-F).\nFurthermore, following the designs of [25] and [27], we construct two other highly efficient (also four-stage) TCN-based networks that require few parameters and have very low computational overhead in terms of FLOPs. Their operations as used within our proposed block are depicted in Figure 3 (b) and (c). We note that, for the ShuffleNet [25] block design, a channel mixing operation is added at the very end (after concatenation and addition), while for the FasterNet [27] block design, the MLP network is applied after concatenation of the branches and before adding the input via the skip connection.\n$X_{1}, X_{2} = Channel\\_split(X)$\n$X_{3} = F(X_{1})$\n$X_{4} = G(X_{2})$\n$X_{c} = concatenate([X_{3}, X_{4}])$\n$X_{out} = X_{c} + X, (3)$"}, {"title": "IV. EXPERIMENTS", "content": "Networks and experiments are implemented using the Py Torch framework2. All models are trained from randomly initialized weights on the LRW training set. An initial learning rate of 0.01 with a cosine annealing schedule is used. To prevent over-fitting, weights are decayed by 0.01 and dropout on the TCN layers for all models is set to 0.2. We train for a total of 80 epochs with Stochastic Gradient Descent, using a batch size of 32, without any warming up period. During training, spatial cropping flipping are randomly applied, as well as MixUp [30] and variable length augmentation [10]. After each epoch, the model is validated and the best performing checkpoints are saved."}, {"title": "V. CONCLUSION", "content": "In this work, we proposed taking advantage of low-cost components to develop lightweight architectures for practical visual speech recognition (VSR) applications. Using the recently proposed Ghost modules where an amount of the channels within are calculated with cost-efficient operations, we developed low-resource models for VSR of isolated words. We replaced the standard convolution operations with Ghost modules in the visual extraction and sequence modeling networks creating compact and efficient alternatives that showcase significantly lowered computational resource requirements. Their reduced overhead enables a multitude of applications in several scenarios where speed of operation is critical and hardware resources are constrained. Evaluation on the largest single word speech recognition dataset showed that our models outperform other lightweight architectures while demanding fewer computational resources measured in FLOPs. Simultaneously, the achieved accuracy of the models is very competitive with other architectures that are much larger in terms of model size and complexity. Moreover, we proposed a general component called \"Partial Temporal block\" for building ultra-lightweight sequential models intended for devices with very limited hardware capabilities, such as IoT and edge devices. This block splits the computation path in two branches and can be customized to fit each use case according to the task and resources at hand.\nFuture work includes addressing the weaknesses outlined in this work, i.e., architectural tuning to take advantage of the DFC module and the larger kernel sizes. We also intend to expand our proposed partial block's capabilities by exploring automated techniques for optimal operation selection, as well as introducing other efficient channel attention methods to increase performance. Finally, specialized training strategies exploiting the latest augmentation and weight averaging approaches are also planned."}, {"title": "A. Dataset & Preprocessing", "content": "This work uses the Lip Reading in the Wild\u00b9 (LRW) dataset [28] for model training and evaluation. LRW features a rich vocabulary of 500 distinct words spoken from a variety of more than 1000 speakers in short segments recorded from public television programs and therefore exhibits variations in the backgrounds as well as the speakers. While the scene background generally varies depending on the program, lighting conditions are adequate and the speakers are clearly visible. Multiple angles of persons speaking are also present, adding to the complexity of the dataset.\nThe LRW dataset is split into three subsets (train, validation and test) without overlapping segments, and each sequence spans 29 frames at a fixed frame rate of 25 FPS. A single word utterance occurs at the middle of each video sequence. The total length of the dataset's segments amounts to 173 hours. Details about the dataset splits are shown in Table I.\nTo prepare the raw data for training, we employ a simple procedure that is typically used by previous works in the literature (e.g., [10], [29]). First, landmarks are computed using a face alignment network after the face of the speaker"}, {"title": "B. Training Setup", "content": "Networks and experiments are implemented using the Py Torch framework2. All models are trained from randomly initialized weights on the LRW training set. An initial learning rate of 0.01 with a cosine annealing schedule is used. To prevent over-fitting, weights are decayed by 0.01 and dropout on the TCN layers for all models is set to 0.2. We train for a total of 80 epochs with Stochastic Gradient Descent, using a batch size of 32, without any warming up period. During training, spatial cropping flipping are randomly applied, as well as MixUp [30] and variable length augmentation [10]. After each epoch, the model is validated and the best performing checkpoints are saved."}, {"title": "C. Results & Discussion", "content": "Our proposed models are evaluated in the LRW test set and in Table II we provide a comparison with other lightweight models from the literature. The metric used to evaluate the methods is word accuracy, measured as a percentage of correct word predictions. We also include size and model complexity measurements, more specifically, the amounts of total network parameters and Floating Point OPerations (FLOPs), as these values are useful when gauging the overall practicality of the methods when considering several applications. More detailed, per-model overviews are provided in Tables VI and VII. All measurements are obtained using torchinfo3.\nOur experimental evaluation showcases that utilizing the Ghost modules on each component of the architecture (feature extraction or sequence model), bestows a noticeable improvement in computation requirements, since the cheap operations in the Ghost module are much more lightweight compared to the regular convolutions in the original networks. In addition, we also gain significant savings in model sizes by lowering parameter counts leading to more compact final models, allowing for applications in a broader range of devices as network size is essential for energy savings due to memory access costs.\nSimultaneously, a minor accuracy drop occurs, arguably due to the reduced representation capabilities of the Ghost module, which is a drawback also mentioned in [24]. Nevertheless, the residual convolutional network [33] equipped with Ghost modules still performs rather well, being highly competitive with larger networks, and it surpasses other works while being more lightweight in terms of both size and computation. Notably, employing GhostV2 modules in the residual architecture causes an increase in model parameters, due to the design of DFC attention which uses two additional convolution layers (see Equation 2 in Section III-B). When using this module, since we replace both standard convolutions within each residual block in the original network, the total number of parameters increases. However, this added amount is offset when combined with a TCN variant that also uses the Ghost module as its building block, so the overall parameter count drops and is still lower than the original network.\nInterestingly, the GhostV2 module that includes the DFC attention provides a minor accuracy improvement only in cases where both components utilize Ghost modules, indicating that the DFC attention is better utilized in a more resource constrained network and this component can be used to recover some performance in such applications. This added accuracy can also be explained by the larger network size, for example, when using the MS-TCN [10] with Ghost modules, employing the GhostV2 module in the feature extraction network surpasses the performance of the original Ghost module (87.39% vs 86.67% accuracy), but requires an additional 11.05 million parameters and 1.82 GFLOPs. We note a similar observation when using the densely-connected (DC-TCN) [14] sequence"}, {"title": "D. Partial TCNs", "content": "We evaluate the ultra-lightweight Temporal Convolution Network variants on LRW when using our proposed Partial Temporal Block as their core component. As mentioned previously (see Subsection III-C), we employ three architectures from the literature within our block. The results are shown in Table III."}, {"title": "E. Ablation Studies", "content": "We perform an ablation analysis experimenting with the ratio used in the partial temporal block within the TCN-based sequence models, see Table IV. This parameter controls the balance between the channels of each computation branch when splitting the input feature map (as shown in Figure 3a). In this experiment, we use the FasterNet [27] formulation (Figure 3c) as the core of our Partial Temporal Block, since it outperforms the other two methods. In this setup, one branch has no calculations, and therefore, no overhead, meaning that the ratio effectively controls the amount of calculations per block; a higher ratio provides more channels to the branch with the resource-intensive computations, increasing overall performance at the cost of resources and vice-versa. For feature extraction, we employ two CNNS: the standard 18-layer residual model [33] and a lightweight version with Ghost modules. We train all models with the procedure mentioned in Subsection IV-B.\nUsing a higher ratio, as one would expect, leads to greater overall recognition accuracy, since, after splitting, the branch that performs calculations receives a larger volume and operates on a higher percentage of the input, exploiting information from more channels. This is accompanied by a slightly higher FLOP and parameter count of the TCN-based models, which is not significant, especially when using the Ghost module, which significantly shrinks the overall costs. Switching the ratio from 0.25 to 0.75 only adds 0.05 GFLOPs and 1.6 million parameters while raising accuracy by 1.82% and up to 4.18%, depending on feature extraction model. The higher ratio (0.75) allows the CNN with Ghost modules to achieve large accuracy gains, surpassing several networks that are much more expensive.\nWe also perform an additional experiment, where we increase the kernel size of the convolutions in each block, in order to provide the network with a larger effective receptive field and tabulate the results in Table V. For this experiment, we evaluate the Temporal and ShuffleNet [25] architectures in our block and set the ratio to 0.75 as it offers the best performance for a negligible impact in computation overhead. Same as before, we keep the previous training settings.\nGenerally, using a larger kernel size improves recognition accuracy while slightly raising the overhead due to the amount of calculations required by the larger kernel. We note however, that this does not apply to all cases, for instance, when using the ShuffleNet block, a larger kernel size than 5 (e.g., 7,9) does not improve accuracy and in fact, hampers performance when the residual network with Ghost modules is used. For a more clear overview of the complexity that each component adds to the overall measurements, the reader is referred to Section IV-F.\nAs for the TCN using the Temporal block, it scales better with a larger kernel size, improving its performance, compared to the ShuffleNet block, however, this network's FLOPS and parameters increase at a much higher rate since it uses regular convolutions. The same diminishing effect in accuracy gains is noticed for the largest kernel sizes. We believe this result is caused by the the dilation amount used in the deeper layers of the TCN architecture, which causes the larger kernels to miss information from their input. Similar to the results shown in"}, {"title": "F. Parameter Analysis", "content": "In addition, we gather all measurements related to network size and complexity for all proposed architectures in this work and provide the results in Tables VI and VII, showcasing the efficiency gained by using Ghost modules and our proposed partial block when designing lightweight networks."}, {"title": "G. Limitations", "content": "A current drawback of the DFC attention block lies in its design which exploits two convolutions in two directions (vertical and horizontal). This prevents its exploitation by the temporal networks which utilize 1D convolutions, and for this reason in our models, its use is limited in the residual convolutional architecture which serves as a feature extractor. Also, in Table II is shown that this module does not bring improvements in all cases where it is used, for example, when the sequence model does not employ Ghost modules. A possible explanation is that the DFC module was originally designed for images of higher dimensions (224 \u00d7 224) and its use is sub-optimal in that architecture due to the fact that the 3D convolution and pooling block at the beginning of the overall model reduce the spatial dimensions of the feature map. The additional down-sampling (see Section III-B, Equation 2) performed by the DFC attention module of the (already low-dimension) feature map removes much of the information contained and hinders the module's ability to exploit it. We believe that removing the pooling operations could possibly improve the overall performance, slightly increasing the computational complexity, and plan on investigating this in the future."}]}