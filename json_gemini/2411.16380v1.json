{"title": "Privacy-Preserving Federated Foundation Model for Generalist Ultrasound Artificial Intelligence", "authors": ["Yuncheng Jiang", "Chun-Mei Feng", "Jinke Ren", "Jun Wei", "Zixun Zhang", "Yiwen Hu", "Yunbi Liu", "Rui Sun", "Xuemei Tang", "Juan Du", "Xiang Wan", "Yong Xu", "Bo Du", "Xin Gao", "Guangyu Wang", "Shaohua Zhou", "Shuguang Cui", "Rick Siow Mong Goh", "Yong Liu", "Zhen Li"], "abstract": "Ultrasound imaging is widely used in clinical diagnosis due to its non-invasive nature and real-time capabilities. However, conventional ultrasound diagnostics face several limitations, including high dependence on physician expertise and suboptimal image quality, which complicates interpretation and increases the likelihood of diagnostic errors. Artificial intelligence (AI) has emerged as a promising solution to enhance clinical diagnosis, particularly in detecting abnormalities across various biomedical imaging modalities. Nonetheless, current Al models for ultrasound imaging face critical challenges. First, these models often require large volumes of labeled medical data, raising concerns over patient privacy breaches. Second, most existing models are task-specific, which restricts their broader clinical utility. To overcome these challenges, we present UltraFedFM, an innovative privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively pre-trained using federated learning across 16 distributed medical institutions in 9 countries, leveraging a dataset of over 1 million ultrasound images covering 19 organs and 10 ultrasound modalities. This extensive and diverse data, combined with a secure training framework, enables UltraFedFM to exhibit strong generalization and diagnostic capabilities. It achieves an average area under the receiver operating characteristic curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level ultrasonographers (4-8 years of experience) and matches the performance of expert-level sonographers (10+ years of experience) in the joint diagnosis of 8 common systemic diseases. These findings indicate that UltraFedFM can significantly enhance clinical diagnostics while safeguarding patient privacy, marking a significant advancement in Al-driven ultrasound imaging for future clinical applications.", "sections": [{"title": "Introduction", "content": "Ultrasound is becoming increasingly important in clinical practice worldwide. It offers significant advantages over magnetic resonance imaging (MRI) and computed tomography (CT), including freedom from radiation, non-invasive nature, and cost-effectiveness. Thus, it is widely adopted as the primary imaging method for monitoring fetal growth during pregnancy\u00b9 diagnosing internal organ pathology, and assisting in surgical decision-making2. However, ultrasound-based diagnosis relies heavily on the clinician's experience, while factors like noise and artifacts in the images can compromise quality and hinder the clinician's assessment of pathological regions, increasing the risk of missed or incorrect diagnoses3,4. Recent efforts have turned to artificial intelligence (AI) technologies to enhance diagnostic accuracy5\u20137. Despite notable successes, existing AI-based ultrasound models typically focus on very specific medical scenarios and require large amounts of high-quality labeled data, which restricts their scalability and generalizability across diverse medical applications.\nOver the past two years, foundational models (FMs) have attracted much attention due to their generality and high performance. In the medical field, many efforts8,9 have leveraged unlabeled ultrasound data to pre-train FMs and fine-tuned them for specific tasks using labeled data. However, existing ultrasound foundational models (USFMs) face three key challenges: (1) Data privacy. Ultrasound data are distributed across multiple medical institutions and cannot be shared due to privacy regulations (e.g., GDPR10), restricting the volume of data available for pre-training; (2) Limited modality. Many USFMs are designed for particular ultrasound imaging modalities (e.g., echocardiograms), limiting their applicability to other imaging modalities and reducing their versatility; (3) Imbalanced data distribution. Existing USFMs often face an imbalance caused by the long-tailed distribution of the organ/lesion types represented in the dataset (e.g., 91% breast ultrasound in 3M-US), leading to a biased performance in diagnosing uncommon conditions. These challenges highlight the need for new solutions that simultaneously address data privacy, scalability, and generalizability across various ultrasound imaging modalities and clinical scenarios.\nIn this work, we introduce UltraFedFM, a novel ultrasound foundation model pre-trained collaboratively by multiple medical institutions without exposing and aggregating all the data together. Specifically, we utilize a federated learning framework with one server and 16 clients from 9 countries, collectively possessing 1,015,754 unlabeled ultrasound images (Fig. 1a). These images cover 19 systemic organs and 10 ultrasound imaging modalities (Fig. 2a-b), providing extensive and diverse representation for pre-training. The development of UltraFedFM consists of two stages: (1) Federated pre-training, in which the multiple clients collaboratively pre-train a shared model in a distributed, self-supervised manner. Throughout the pre-training process, the server periodically aggregates the local model parameters from each client without accessing their private data (Fig. 1b); (2) Downstream fine-tuning, where the pre-trained FM is fine-tuned using specific data to adapt to various clinical tasks, such as disease screening and diagnosis, sub-classification of disease phenotypes (e.g., tumor infiltration depth and type classification), prenatal maternal-fetal health analysis, and critical lesion identification and segmentation (Fig. 1c).\nUltraFedFM is adapted to various ultrasound imaging modalities, modes, qualities, and clinical tasks. To accommodate the diverse features of different modalities, we propose a dynamic ultrasound image masking approach based on the specific texture features of organs and lesions. Additionally, we incorporate a random image corruption branch within the masked image modeling process to handle low-quality images commonly encountered in real-world scenarios. Furthermore, we use simple yet effective image transformations to generate simulated ultrasound images, aiming to address the uneven distribution of scan patterns in the pre-training dataset (see Method for details).\nWe conduct extensive experiments to evaluate the performance of UltraFedFM. To provide a fair and comprehensive evaluation, we collect and curate the largest ultrasound evaluation benchmark, covering the two most common ultrasound clinician tasks (i.e., disease diagnosis and lesion segmentation) with 11 sub-tasks from 19 ultrasound datasets. Several fully-supervised methods and a state-of-the-art USFM are utilized for comparison. Experimental results demonstrate that UltraFedFM outperforms all baselines, achieving an average area under the curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM outperforms ultrasonographer clinicians with intermediate levels (e.g., 4-8 years of clinical experience) and achieves comparable performance to high-level (e.g., more than 10 years of clinical experience) ultrasonographers in the joint diagnosis of 8 common systemic diseases. Moreover, we perform a thorough investigation to demonstrate the generalizability of UltraFedFM to new modalities, its stability against input perturbations, and its interoperability. With these capabilities, UltraFedFM provides a reliable model for clinical tasks, making it a pioneering solution for advancing ultrasound AI across institutions, regions, and clinical tasks."}, {"title": "Results", "content": "UltraFedFM enables systemic disease diagnosis\nUltraFedFM aims to serve as a comprehensive FM in ultrasound imaging. To assess its effectiveness for disease diagnosis, 6 publicly available datasets and 2 private datasets (see Table 2) are utilized, covering 8 kinds of organs (i.e., pancreas, gallbladder, liver, lung, colorectum, breast, heart, and fetal organs, see Fig. 10) and 6 ultrasound imaging modalities (i.e., abdominal, lung,\ntask-specific classifiers utilized in other FMs. To demonstrate this, we constructed an organ-agnostic dataset by combining eight distinct datasets from different organs and fine-tune UltraFedFM to recognize eight types of malignant tumors. It is observed that UltraFedFM accurately identifies most categories without requiring a separate classifier for each organ and the predicted scores of UltraFedFM concentrate in higher confidence intervals (Fig. 3b). Fig. 3c illustrates the receiver operating characteristic (ROC) curves among eight different diseases, showing that UltraFedFM achieves superior efficiency in organ-agnostic disease diagnosis. More quantitative results for UltraFedFM, including accuracy, F1-score, and ROC, are provided in Supplementary Fig. 11, Fig. 12, and Fig. 13.\nUltraFedFM is comparable with ultrasonographers\nTo evaluate the reliability of UltraFedFM's generalist intelligence from the clinical practice perspective, we compare it with ultrasonographers having different clinical levels. Seven ultrasonographers participated in this study, in which two of them are intermediate-level (clinicians A, B: 4-8 years of clinical experience) and five of them are high-level (clinicians C-G: more than 10 years of clinical experience). A total of 80 ultrasound images containing 8 systemic malignant diseases were tested. As shown in Fig. 3d and Table 5, UltraFedFM outperforms the ultrasonographers with intermediate-level and achieves comparable performance with high-level ultrasonographers. More specifically, while some specific organ diseases are easy for ultrasonographers to diagnose (e.g., average accuracy: 0.800 for breast and 0.871 for kidney), their diagnostic capabilities are limited when multiple ultrasound diseases are jointly diagnosed (e.g., average accuracy: 0.314 for gallbladder). In contrast, UltraFedFM can provide a consistent and accurate diagnosis of different ultrasound organ diseases (average accuracy: 0.900 for breast, 1.000 for kidney, and 0.800 for gallbladder. These results reveal that the UltraFedFM has the potential to serve as a reliable general intelligence to assist clinicians in the diagnostic process.\nUltraFedFM facilitates organ and lesion segmentation\nOrgan and lesion segmentation for ultrasound images are crucial for clinical decision-making. To assess UltraFedFM's segmentation accuracy across different ultrasound imaging modalities, we evaluated it on four binary segmentation datasets (nerve12, muscle13, heart14 and thyroid15\u201318) and one multi-class segmentation dataset (pubic symphysis-fetal head19). UltraFedFM consistently achieves high segmentation accuracy, successfully managing targets with diverse shapes and structures. In the binary segmentation task (Fig. 4a), UltraFedFM achieves the highest average dice similarity coefficient (DSC) score of 0.857 across the three binary segmentation datasets, significantly outperforming all other baselines (p < 0.005). In particular, USFM achieves a DSC score of 0.828, which is much lower than that of UltraFedFM (p = 0.002).\nThe multi-class segmentation task involves two steps beginning by segmenting the pubic symphysis and fetal head, followed by measuring the angle between them. In this task, UltraFedFM achieves a DSC score of 0.842, significantly outperforming the second-best method (USFM) with a DSC score of 0.910 (p = 0.004). Additionally, UltraFedFM excels in measuring the angle of progression (AoP), with a mean absolute error of 8.80, outperforming all baselines by a significant margin (p < 0.005) (see Fig. 4a). Similar to the classification settings, we also evaluated UltraFedFM's effectiveness in scenarios with limited labeled data (see line charts in Fig. 4a). Notably, even with 20% of the fine-tuning data, UltraFedFM still achieves an average DSC score of 0.772, outperforming the supervised method and USFM by 14.0% and 2.3%, respectively. To further assess UltraFedFM's generalization capability, we compiled an organ-agnostic segmentation dataset comprising five types of lesions. As shown in Fig. 4b, UltraFedFM demonstrates superior performance in locating and segmenting these lesions using a single unified segmentation model.\nIn addition to evaluating UltraFedFM on datasets with similar distributions, we conducted cross-institutional evaluations by fine-tuning the model on one institutional dataset and testing it on others. UltraFedFM consistently outperforms other baselines in all cross-sets (p < 0.01), demonstrating superior stability and balanced generalization (see Fig. 4c). To enhance UltraFedFM's ability to recognize feature pairs from different scanning modes (i.e., linear-array and convex-array), we generated synthetic ultrasound imaging data via Polar-Cartesian transformation. Ablation studies were performed to quantify the impact of these synthetic images on UltraFedFM's learning outcomes. Fig. 4d illustrates the performance of UltraFedFM fine-tuned with different ratios of real to synthetic data. We observed that adding synthetic data generally enhances performance. This highlights the importance of balancing real and synthetic data, where the optimal performance is achieved at a 1:1 ratio.\nThe impact of SSL strategies on model performance\nTo validate the effectiveness of our modified masked autoencoder (MAE) strategy in UltraFedFM, we compare it with several baseline self-supervised learning (SSL) strategies, including vanilla MAE11, SimCLR20, SwAV21, DINO22, and MoCo23. Fig. 5a-b shows that UltraFedFM with the modified MAE significantly outperforms all other baselines (p < 0.001) in both disease diagnosis and lesion segmentation tasks. Specifically, UltraFedFM achieves the highest average AUROC of 0.926 across eight classification tasks and an average DSC score of 0.878 across four segmentation tasks. In contrast, the vanilla MAE achieves the second-best performance, with an average AUROC of 0.884 and an average DSC score of 0.839. These results suggest that MAE-based approaches are more effective for ultrasound imaging than contrastive learning-based methods."}, {"title": "The stability of UltraFedFM's predictions", "content": "This success may be attributed to MAE's ability to learn robust feature representations in images where structures can vary significantly across patients or organs. Clinically, this translates to more accurate diagnostic predictions, especially for complex cases involving subtle lesions or challenging anatomical regions.\nThe stability of model predictions is essential for ensuring reliable clinical decision-making, particularly in ultrasound-based diagnostics where inconsistencies can lead to misdiagnosis. To this end, we quantitatively compared the prediction stability of UltraFedFM with the baseline USFM8 under two settings: organ-specific (Fig. 5c) and organ-agnostic (Fig. 5d). USFM shows a broader distribution (mean \u00b5 = 0.808 and standard deviation \u03c3 = 0.174). In contrast, UltraFedFM's predictions concentrate in a high DSC range (mean \u00b5 = 0.857 and standard deviation \u03c3 = 0.103). Moreover, stability is paramount when dealing with organs that exhibit significant inter-patient variability, such as the liver or kidneys. Thus, we further introduce random noise to simulate real-world ultrasound imaging perturbations, such as tissue movement, operator variability, or imaging artifacts. We compared the test results under varying levels of noise. Despite these disturbances, UltraFedFM maintained highly correlated test scores (Fig. 5e-f), demonstrating its robustness and reliability in clinical environments where imaging conditions can be unpredictable."}, {"title": "UltraFedFM generalizes to new medical scenarios", "content": "Beyond learning ability, a crucial metric to evaluate the practicality of FMs in real-life scenarios is the generalization ability. To assess this, we selected two medical institutions not involved in the pre-training stage (high-frequency skin ultrasound imaging dataset and kidney disease ultrasound imaging dataset). This evaluation aims to determine how well the model performs on unseen ultrasound imaging modalities and organs, both key challenges in ultrasound diagnostics. As shown in Fig. 6. UltraFedFM consistently demonstrates superior generalization across different modalities (Fig. 6a-b), achieving an average AUROC of 0.925, significantly outperforming all other baselines (p < 0.01). Fig. 6c shows UltraFedFM's strong generalization ability to new organs and related diseases, with an AUROC of 0.878 and an average precision (AP) of 0.980. Fig. 6d illustrates that UltraFedFM achieves an AUROC of 97.1% and an AP of 0.910, despite the textural and color differences of high-frequency ultrasound imaging from conventional methods. Such generalization is essential for real-world applications where clinicians frequently encounter new organs or modalities that the training data may not be easily accessed."}, {"title": "Model interpretation", "content": "UltraFedFM's performance on downstream tasks is determined by its feature extraction capabilities. To gain a deep understanding of how UltraFedFM interprets ultrasound images, we qualitatively analyzed the internal mechanisms of the pre-text task of UltraFedFM during pre-training and how UltraFedFM made task-specific decisions on downstream tasks.\nDuring pre-training, the pre-text task enables the model to learn ultrasound-specific context across various ultrasound imaging modalities. As shown in Fig. 6e, UltraFedFM accurately reconstructs images even when large portions are masked. This process allows UltraFedFM to effectively capture the critical anatomical details, such as muscle textures, organ tissues, and lesion structures, which contribute to the performance in downstream tasks. Fig. 6f shows the two ultrasound scanning modes used in the pre-training process, where we applied a scanning mode-aware transformation (SMAT) to balance the dataset. This prevents UltraFedFM from easily overfitting to one scanning mode during fine-tuning, which is essential since ultrasound imaging varies widely depending on the equipment and operator techniques used, and models need to adapt across these variations.\nFor downstream lesion segmentation tasks, Fig. 6g and Fig. 6h visualize UltraFedFM's precise localization of salient lesion areas and target boundaries. Clinically, the model's ability to focus on salient areas while excluding irrelevant background interference (Fig. 6h) enhances its accuracy in detecting complex lesion structures, which is essential for diagnosing diseases with subtle or overlapping symptoms.\nFig. 7 illustrates the embedding feature space of different classes in the fine-tuned model. UltraFedFM demonstrates superior class discrimination, with different classes clearly separated in high-dimensional space, resulting in more precise classification boundaries. This ability is crucial in clinical applications where precise differentiation between pathological and non-pathological tissue can impact treatment decisions. In contrast, the baseline supervised model exhibits weaker differentiation and less distinct classification results. Fig. 8a further shows how UltraFedFM effectively recognizes specific patterns and targets via the attention mechanism. For example, in pancreas, liver, breast, and gallbladder imaging, the model focuses on the center and surrounding areas of tumor lesions. In colorectal and lung imaging, it targets high-density textured regions while ignoring irrelevant hollow regions such as intestines and alveoli. For fetal ultrasound imaging, UltraFedFM focuses on the solid parts of the fetus and excludes irrelevant regions such as the uterus and muscles. In addition, we visualized the evolution of the attention map during pre-training (see Fig. 8b). As pre-training progresses, the local model increasingly focuses on meaningful regions, thereby enhancing the effectiveness of the global model."}, {"title": "Discussion", "content": "With the growing demand for public health solutions, there is an urgent need to develop AI-based foundation models for wide application to real-world clinical scenarios. In this work, targeting the most widely used ultrasound data, we are the first to propose a comprehensive privacy-preserving ultrasound foundation model (USFM) using federated learning, namely UltraFedFM. By eliminating privacy concerns through decentralized pre-training, UltraFedFM leverages large-scale global datasets, enhancing its generalization capabilities. Regarding the above extensive experimental results, UltraFedFM demonstrates excellent performance, favorable generalization and robustness, and good adaptability to fine-tuning data. Specifically, it can handle different clinical tasks, such as diagnosing diseases, segmenting interest of regions (i.e., pathological tissues or organs), and analyzing spatial relationships of fetal organs, making it versatile for a wide range of medical applications. Moreover, UltraFedFM can be fine-tuned in a modality-agnostic manner to enable a single decoder to diagnose multiple diseases present in different modalities. Even with limited fine-tuning data, it consistently outperforms other baseline methods in both accuracy and stability. Across various ultrasound modalities and clinical tasks, UltraFedFM performs with judgment capabilities comparable to human clinicians.\nThe relations of UltraFedFM with previous achievements. Ultrasound imaging, a widely used clinical diagnostic tool, is renowned for its convenience and accuracy. Previous research in ultrasound diagnostics primarily focused on deep learning models trained on specific ultrasound modalities, targeting the diagnosis or segmentation of disease types within fixed imaging contexts. For example, Antropova et al.24 developed a method that utilized pre-trained CNNs to extract and aggregate features, which were then combined with hand-crafted features from CADx for breast cancer diagnosis. Similarly, Basu et al.7 investigated multi-scale and second-order pooling architectures to address false textures in gallbladder ultrasound, achieving precise localization and detection of malignant gallbladder tumors. Jiang et al.25 introduced a sparse computation and temporal fusion architecture designed for the accurate and real-time segmentation of colorectal cancer lesions. These studies have significantly advanced the application of AI in various ultrasound modalities, leading to improvements in automatic ultrasound image analysis, including lesion segmentation, disease diagnosis, and treatment planning. These successes are primarily attributed to the synergistic effects of data, models, and algorithms, specifically the collection and thorough annotation of datasets for specific organs or diseases, as well as specially designed network structures and training methods. However, a critical bottleneck limiting the advancement of medical imaging algorithms is the limited availability of fully annotated medical data.\nMedical image annotation demands the expertise of trained physicians, and the segmentation tasks in particular requires substantial time and effort. Traditional methods were often trained on only hundreds or thousands of samples, significantly impacting the models' stability and generalizability in real-world applications. Meanwhile, with the evolution of ultrasound imaging technology, its application to an increasing number of organs and diseases presents challenges for models trained on single-organ or single-disease data, making it difficult to meet expanding clinical demands. There is a growing interest in developing a label-efficient ultrasound model that can be generalized across various tasks and organs, enabling rapid adaptation and deployment in clinical practice.\nThis led to the introduction of the foundation model (FM) based on self-supervised learning (SSL), which is capable of learning universal features independent of organs and diseases from unlabeled data. Prior to UltraFedFM, several studies explored FMs in medical imaging26\u201330, covering various modalities such as ophthalmic images, endoscopy, and CT scans. In the field of ultrasound imaging, Christensen et al. proposed an FM specifically for cardiac ultrasound, pre-trained on over one million echocardiogram videos for diagnosing various heart diseases. Jiao et al.8 compiled and organized over two million ultrasound images across 12 different categories to establish a general USFM. However, these foundation models either focus on developing and applying to a single ultrasound modality, which is limited in actual clinical deployment; or require centralized collection and processing of multi-center large-scale data. On the one hand, this approach requires expensive servers to store and process data, and on the other hand, the circulation of data inevitably involves the disclosure of patient information, especially rare disease data, which hinders the development of universal models.\nThe advantage of UltraFedFM. While UltraFedFM is not the first FM developed for ultrasound imaging, it is the first to integrate privacy protection during the model development process. Previous methods highlighted that, in most cases, private data held by different institutions is not shared, and public data must be anonymized to protect patient privacy, making it challenging to utilize a vast amount of available medical data. In this study, federated learning was used to pre-train the USFM. UltraFedFM was pre-trained on over 1 million ultrasound images from 16 independent institutions worldwide, covering 19 different organs and 10 ultrasound modalities. The model's efficacy was validated on data from more than 12 different types of organs and lesions. UltraFedFM utilizes the most comprehensive pre-training dataset, including emerging ultrasound modalities like contrast-enhanced ultrasound (CEUS), high-frequency ultrasound (HFUS), and endorectal ultrasound (ERUS). Recognizing the different scanning modes produced by various ultrasound probes, an image generation technique based on scan mode conversion was innovatively proposed. This technique augments ultrasound images without the need for additional generative models, effectively doubling the pre-training dataset to 2 million images with balanced representation across scanning modes.\nThrough careful selection of pre-training image quantity and the design of pre-training algorithms specific to ultrasound imaging, it has been demonstrated for the first time that a distributed pre-training foundation model can achieve comparable performance to centralized foundation models in overall performance across multiple downstream tasks. In ultrasound disease diagnosis tasks, UltraFedFM achieved an average AUROC of 0.927 across eight organs, significantly (p < 0.001) surpassing USFM's 0.894 AUROC. In ultrasound lesion segmentation tasks, UltraFedFM achieved an average DSC score of 0.876, significantly (p < 0.001) exceeding USFM's 0.858.\nThe security of UltraFedFM during the pre-training process and its accuracy across various ultrasound imaging applications facilitate the clinical translation of AI technology. Previously, only large institutions with efficient data management workflows could develop FM from vast private medical datasets. This study shows that using a federated learning framework enables the global medical community to leverage diverse and abundant medical data resources to pre-train comprehensive and general FM, and even small-scale data clients with uncommon organs (e.g., Uterus, testis) or modalities (e.g., contrast-enhanced ultrasound, high-frequency ultrasound) can effectively participate in training without worrying about privacy issues, accelerating AI advancement in the medical field. Currently, UltraFedFM's diagnostic and image analysis capabilities have reached levels comparable to those of professional ultrasound physicians in many areas. This can significantly alleviate the pressure on outpatient services and large-scale ultrasound disease screening in regions with high medical demand and limited medical resources, substantially enhancing the work efficiency of ultrasound physicians.\nThe current limitations of UltraFedFM. While systematically evaluating the advantages of UltraFedFM in ultrasound imaging analysis, this study still has several limitations and unresolved challenges. Firstly, during the federated pre-training process, the quantity and variety of ultrasound image data vary across institutions, resulting in non-independent and identically distributed (non-IID) data. This poses challenges to the aggregation process of models from different institutions. Although a data volume-based balancing method was incorporated to adjust each institution's contribution to the global model during pre-training, the non-IID problem persists and may worsen with more institutions participating in the pre-training. Secondly, while redundancy in the pre-training dataset was reduced, public dataset collection conditions still led to numerous images originating from the same ultrasound video or patient, limiting the diversity of features that the model can learn. It is speculated that by extending UltraFedFM's pre-training framework to collaborate with more global medical institutions and using a more balanced and diverse pre-training dataset, the model's accuracy and generalizability could be further enhanced. Lastly, although UltraFedFM effectively addresses critical clinical diagnostic and segmentation tasks and has been validated across 12 different types of organs and diseases, it does not utilize the vast amount of textual diagnostic reports available in ultrasound examinations. Integrating multimodal features from both text and images could further improve the foundational model's accuracy in various clinical tasks and enable the development of additional clinically useful tasks like question-answering and diagnostic report generation.\nThe significance and application of UltraFedFM. In conclusion, this study provided a robust and reliable framework for developing comprehensive USFMs. It has been demonstrated that high-performance and stable models can be pre-trained without risking privacy leakage. This breakthrough can significantly advance the development of USFMs and potentially lead to the emergence of more powerful general-purpose medical AI. UltraFedFM has already shown excellent performance in various ultrasound clinical tasks and holds promise for further expansion. It has the potential to replace traditional ultrasound Al diagnostic models and play a crucial role in clinical decision support. The theoretical contributions of this research lie in validating the efficacy of federated learning for pre-training medical FMs, inspiring further academic advancements in the field. Practically, the implementation of UltraFedFM can enhance diagnostic accuracy, streamline clinical workflows, and improve patient outcomes. Additionally, these findings offer valuable insights for policy-making, particularly in the areas of data privacy and the integration of AI in healthcare. By ensuring data privacy and leveraging federated learning, the collective power of global medical data can be harnessed to drive innovations in medical diagnostics and treatment planning, ultimately transforming the healthcare delivery landscape."}]}