{"title": "IFORMER: INTEGRATING CONVNET AND TRANSFORMER FOR MOBILE APPLICATION", "authors": ["Chuanyang Zheng"], "abstract": "We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, i.e., ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios. Code and models are available at: https://github.com/ChuanyangZheng/iFormer.", "sections": [{"title": "INTRODUCTION", "content": "Building lightweight neural networks facilitates real-time analysis of images and videos captured by mobile applications such as smartphones. This not only enhances privacy protection and security by processing data locally on the device but also improves overall user experience. Through the decades, convolutional neural networks (CNNs) have emerged as the primary choice for balancing latency and performance on resource-constrained mobile devices. However, a significant limitation of CNNs is their reliance on a local sliding window mechanism, which imposes crucial inductive biases that may hinder modeling flexibility. Recently, the soaring development of vision transformers (ViTs) has begun to dominate various computer vision tasks, including image classification , object detection, and semantic segmentation. The core mechanism underlying ViTs is self-attention, which dynamically learns interactions between all image patches. This enables the model to focus on important regions adaptively and capture more global features. Nevertheless, deploying ViTs on mobile devices with limited resources poses significant challenges. On the one hand, the quadratic computational complexity of attention renders them unsuitable for large feature maps, which are common in the early stages of vision networks. On the other hand, the multi-head mechanism requires reshaping operations, leading to increased memory usage.\nMany research efforts are devoted to combining the advantages of both CNNs and ViTs in designing lightweight networks while mitigating inefficient operations in mobile applications. Some studies revisit the architectural designs of lightweight CNNs from a ViT perspective and incorporate key components that contribute to the performance of ViTs into CNNs. Although these pure lightweight CNNs show improved performance compared to previous mobile networks, they still lag behind the powerful self-attention in ViTs. Another line of works proposes innovative attention mechanisms to address the limitation of standard attention and blend convolutions to achieve a better balance between latency and performance. These attention mechanisms either reduce the number of queries and keys, limit the attention span , or adopt linear attention , which may compromise performance to some extent.\nIn this work, we present the iFormer, a herd of lightweight models that integrates the strengths of both CNNs and ViTs, achieving a state-of-the-art balance between latency and accuracy. Specifically, we employ a hierarchical architecture consisting of four stages. In the earlier, high-resolution stages, we utilize fast convolution to extract local representations. To construct the convolutional block, we start with a \"modern\" ConvNeXt , which incorporates a series of design decisions inspired by ViTs. Then we progressively \u201clighten\u201d the ConvNeXt to create a streamlined lightweight network, optimizing it for real-time mobile latency on an iPhone 13, in contrast to the FLOPs and parameters used in prior works. This results in a fast convolutional architecture with strong performance. To further enhance the dynamic properties and its ability to model long-range contexts, we incorporate self-attention in the later low-resolution stages. However, direct implementation of standard multi-head self-attention (MHA) brings notable memory overheads and slows down inference speed on mobile devices. We identify that the increased latency stems primarily from the reshaping operations in MHA. More analyses reveal that multiple attention heads behave similarly. Therefore, we propose a simple yet effective single-head modulation self-attention (SHMA), which significantly minimizes memory costs while preserving strong performance. In detail, SHMA learns spatial context interactions through optimized self-attention. Concurrently, a parallel feature extraction branch is employed to capture informative features. Finally, we fuse the outputs of these two branches to facilitate a more flexible and dynamic exchange of information, compensating for the slight performance degradation of the single-head attention when compared to M\u041d\u0410.\nBenefiting from the fast local representation capacity of convolution and the efficient global modeling proficiency of the proposed SHMA, iFormer outperforms existing pure lightweight CNNs and hybrid networks across multiple visual recognition tasks, including image classification, object detection, instance segmentation, and semantic segmentation. For instance, in the context of image classification, iFormer-M achieves a Top-1 accuracy of 80.4% with only 1.10 ms on an iPhone 13 without advanced training strategies such as knowledge distillation or reparameterization. Notably, our model obtains a 0.5% improvement in Top-1 accuracy compared to the recent MNV4-Conv-M , while being 1.4\u00d7 faster than FastViT-SA12 with similar accuracy. These results demonstrate the effectiveness of the proposed network in capturing both local and global feature representations."}, {"title": "RELATED WORK", "content": "EFFICIENT CONVOLUTIONAL NETWORKS\nIn the past 2010s, computer vision was dominated by CNNs, and so were efficient networks. The first remarkable breakthrough in mobile CNNs is MobileNets , which hatches the concept of decomposing standard convolution into depthwise and pointwise counterparts. Subsequently, MobileNetV2 introduces an inverted residual bottleneck block to push the state-of-the-art for mobile models. Numerous studies have aimed to accelerate CNNs via various approaches, such as channel shuffle in ShuffleNet and cheap linear transformations in GhostNet. Meanwhile, Neural architecture search"}, {"title": "EFFICIENT VISION TRANSFORMERS", "content": "The success of Vision Transformer offers a compelling demonstration of the potential to apply transformer to computer vision tasks. Following this, ViT and its numerous variants sweep across various scenarios. However, the quadratic complexity of self-attention behind ViTs poses significant challenges for efficiency. The following researches seek to boost ViT efficiency through efficient attention mechanisms, model compression, knowledge distillation, and token reduction. Recent studies further introduce ViTs into mobile applications. One mainstream of work combines efficient convolution and ViT to create lightweight hybrid networks. MobileViT directly integrates MobileNetv2 blocks and ViT blocks, while Mobile-Former features a parallel design of MobileNet and ViT with a two-way bridge connecting the two. To further accelerate inference, some approaches replace the standard attention with efficient variants within the hybrid networks. These include reducing the number of delegate tokens for computing attention , employing channel attention , substituting projection in attention with efficient ghost modules , and utilizing linear attention mechanisms. Besides manual designs, EfficientFormer and MobileNetV4 search for efficient architectures in a unified space encompassing both convolution operators and transformer operators. Another stream of work focuses on efficient attention mechanisms and directly employs them throughout the entire network. For example, CMT takes advantage of depth-wise convolution to downsample key and value to reduce computation. GhostNetV2 applies two fully connected layers along the horizontal and vertical directions to compute attention, a decoupled version of MLP-Mixer. Recently, SHViT observes computational redundancy in the multi-head attention module and proposes to apply sing-head attention. In contrast to these existing approaches, we introduce a novel efficient attention module without sacrificing informative interactions, thereby maintaining strong representational capacity. Regarding attention design, ours is a bit similar to SHViT but is considerably superior as shown in Table 18 in the supplementary material. The key difference lies in the novel modulation attention. In addition, we explore efficient attention mechanisms in an on-device environment while SHViT focuses on general-purpose GPUs, fundamentally different hardware."}, {"title": "METHOD", "content": "We present the overall architecture of our iFormer in Fig. 4, which offers a Pareto-optimal accuracy-latency trade-off on mobile applications. Our exploration towards a streamlined lightweight network unfolds as follows: 1) establishing the baseline and measure metric in Sec. 3.1. 2) exploring acceleration techniques consisting of macro and micro designs in Sec. 3.2. 3) injecting global attention in Sec. 3.3. Finally, we create a new family of efficient hybrid vision transformers tailored for mobile applications in Sec. 3.3. A detailed trajectory illustrating the evolution from a general hierarchical CNN to a fast hybrid vision transformer is depicted in Fig. 2."}, {"title": "PREPARING CONVNEXT", "content": "Our goal is to create an efficient multiscale network, where spatial dimensions of intermediate representations shrink as inference proceeds. In this hierarchical architecture, early network layers have larger spatial dimensions and fewer channels (e.g. 56\u00d756\u00d748), which renders them memory-bound. Highly optimized convolution is more appropriate for these layers. Guided by this principle, we choose a pure convolutional network as our base architecture, specifically ConvNeXt which absorbed several key components from ViTs and competes favorably against ViTs. We gradually \"lighten\" the network to achieve a more favorable balance between latency and accuracy. For speed metric, we utilize on-device latency, measured on an actual iPhone 13 and compiled by Core ML Tools (CoreML), rather than FLOPs and parameter counts in previous methods, which are not well correlated with latency. Regarding performance, we follow the training recipe in ConvNeXt while removing the layer scale to align prior methods for a fair comparison. Please refer to Sec. B in the supplementary material for more details. To initiate our study, we systematically scale down the ConvNeXt by reducing the number of blocks and the width. This results in a lightweight model with a latency of 1.07 ms and a Top-1 accuracy of 74.9%, serving as our initial baseline."}, {"title": "LIGHTENING BASELINE", "content": "Seeing Better with Early Convolutions Following ViTs, ConvNeXt adopts an aggressive \"patchify\" strategy as the stem cell, specifically by splitting the input image into a series of non-overlapping patches via a 4x4 non-overlapping convolutional layer. However, some studies indicate that an early convolutional stem can increase optimization stability and facilitate faster model convergence. Moreover, compared to general models, lightweight models typically have fewer parameters and a reduced capacity. An aggressive non-overlapping layer may lead to the premature loss of rich information. Consequently, we opt to replace the non-overlapping \u201cpatchify\" stem with a stack of overlapping convolutional layers, as shown in Fig. 4. This modification elevates the top-1 accuracy to 76.7% with a neglectable increase in latency of 0.1 ms.\nNormalization An obvious difference between ConvNeXt and previous CNNs is the normalization layer. ConvNeXt utilizes Layer Normalization (LN) commonly used in Natural Language Processing (NLP), whereas the latter uses Batch Normalization (BN) . Albeit its superior performance, LN requires on-the-fly statistics calculation in inference along with"}, {"title": "SINGLE-HEAD MODULATION ATTENTION", "content": "Single-Head vs. Multi-Head ViTs typically apply MHA, which projects the queries, keys, and values multiple times with different learnable linear projections and performs multiple attention functions simultaneously. In practice, the multi-head mechanism requires the reshaping of feature maps first, causing large memory access and transfer costs. This can seriously impact inference latency, especially on resource-constrained mobile devices. To investigate this issue, we substitute the last half of the convolutional blocks in the third stage and all blocks in the last stage with standard ViT blocks, as depicted in Fig. 4. We refer to this hybrid network as the MHA baseline. Next, we build another network by substituting the MHA with Single-Head self-Attention (SHA), referring to it as the SHA baseline. The comparison is shown in Table 1. The SHA baseline shows a 1.25\u00d7 acceleration over its MHA counterpart on the iPhone 13. This verifies that additional reshaping operations in MHA incur significant memory access costs, leading to a considerable decline in inference speed.\nThis naturally calls for optimizing MHA. Recent methods primarily focus on downsampling the query or the key, which may hurt global attention capacity. Instead, we aim to reduce the redundant reshaping of MHA while preserving all token-to-token interactions. Previous works indicate that a single attention head can approach the performance of multiple heads in general plain transformer models, such as DeiT. \u03a4\u03bf investigate this on the mobile application, we analyze the average cosine similarity of multiple heads within the same layer of the aforementioned MHA baseline, which is a hierarchical lightweight net-work, and present our findings in Fig. 3. We clearly see that the average cosine similarity reaches 50% and even 75% in the final layer. Furthermore, the SHA baseline, as shown in Table 1, exhibits only a negligible accuracy drop of 0.1%. These suggest that SHA achieves a more favorable balance between accuracy and latency, obtaining an accuracy of 79.8% with a latency of 1.12 ms.\nModulation Attention We further introduce a novel modulation attention to boost performance and strengthen flexibility in modeling, as illustrated in Fig. 4. Formally, we start from the abstracted modulation mechanism , similar to the gate mechanism Assume we are given an input feature map $x \\in \\mathbb{R}^{C\\times H\\times W}$ where $C$, $H$, and $W$ denote the channels, height, and width of the feature map. The modulated output can be written as follows:\n$x_o = f(x) \\cdot ctx(x)$,\nwhere $f(\\cdot)$ denotes the feature mapping branch and $ctx(\\cdot)$ is the context modeling branch. The output $x_o$ is the fused features from both branches via efficient element-wise multiplication. The key idea of our approach is to modulate the feature using SHA instead of relying on convolutional layers, as seen in previous works. Since SHA captures global interactions through self-attention, it excels in extracting rich contextual information and better controlling the flow of information. This process can be expressed as follows:\n$ctx(x) = SHA(W_Q x, W_K x, W_V x)$,\nwhere $W_Q$, $W_K$, $W_V$ are the project weights for query, key, and value, respectively. For simplicity, we omit the bias term. To minimize inference costs, we utilize a single projection layer in the feature mapping branch. To enhance expressivity and improve optimization stability, we apply individual nonlinear activation functions to both branches, as follows:\n$x_o = \\sigma(W_M x) \\cdot \\sigma(ctx(x))$,\nwhere $\\sigma$ is the sigmoid function and $W_M$ denotes the feature projection weight. We also experiment with various activation functions for modulation in Sec. 5 and observe that the sigmoid works rather well. Finally, the output from the modulation attention is projected in a manner as standard attention.\nEquipped with Single-Head Modulation Attention (SHMA), our model improves the accuracy to 80.4% with an intermediate latency of 1.15 ms. This performance notably surpasses that of the recent MobileNetV4, which achieves an accuracy of 79.9%."}, {"title": "EXPERIMENTS", "content": "IMAGE CLASSIFICATION\nSettings. We first evaluate our models on classification on ImageNet-1K . \u03a4\u03bf ensure a fair comparison with prior studies, we follow the previous training recipe and train all models for 300 epochs with a standard image size of 224x224. Please refer to Sec. B in the supplementary material for details. Besides Top-1 validation accuracy, we also report the latency measured on an iPhone 13 with models compiled by Core ML Tools (CoreML) under a batch size of 1, as done in . It's worth highlighting that we do not apply any advanced strategies such as distillation and reparameterization."}, {"title": "OBJECT DETECTION AND INSTANCE SEGMENTATION", "content": "To validate the effectiveness of iFormer on downstream tasks, we train Mask R-CNN with iFormer as the backbone for 12 epochs (1x), using the MMDetection toolkit. We also report backbone latency measured at a resolution of 512\u00d7512 on an iPhone 13. The results are presented in Table 5. In comparison to lightweight models, iFormer-M surpasses FastViT-SA12 by +1.9%/+2.0% in Apbox /APmask while running 1.32\u00d7 faster. iFormer-L also obtains +0.1%/+0.6% in Apbox /Apmask than EfficientMod-S, which utilizes a convolutional modulation mechanism to learn dynamics similar to self-attention. Notably, EfficientMod-S operates 3.7\u00d7 slower when processing high-resolution input, underscoring that the proposed novel attention mechanism is more suitable for mobile networks. Meanwhile, when compared to general networks that are not optimized for mobile applications, iFormer demonstrates significant advantages. For instance, iFormer-L exceeds the performance of ConvNeXt-T with improvements of +1.2%/+1.4% in"}, {"title": "SEMANTIC SEGMENTATION", "content": "We conduct experiments on the ADE20K using the Semantic FPN , based on the MMSegmentation toolkit . Thanks to its efficient attention design, iFormer outperforms all competing methods in mIoU with similar and much lower latency. For example, iFormer-L surpasses FastViT-SA24 by +3.5% in mIoU with a 1.36\u00d7 faster inference speed. In addition, iFormer-M demonstrates superior mIoU compared to general networks, which typically exhibit substantially greater latency when processing higher-resolution inputs on mobile devices. Although PVTv2-B utilizes downsampled attention, it still requires 27 ms for latency. Similarly, Swin-T involves intensive operations in window partitioning, making it less suitable for mobile applications. Running at 6.6 ms, iFormer-L achieves +2.0% better mIoU than PVTv2-B1 and +3.0% better than Swin-T. These results suggest that the proposed attention mechanism offers significant benefits for tasks requiring the perception of fine-grained details."}, {"title": "ABLATION STUDIES", "content": "Activation Function Here we explore whether an activation function without an upper bound can enhance the SHMA by allowing neurons to express arbitrarily large values. We compare the widely used Sigmoid Linear Unit (SiLU) with the sigmoid function and present the results in Table 6. Directly re-placing the activation function in"}, {"title": "Choice of Conv v.s. ViT Blcoks", "content": "In Section 3.3, we replace the convolutional blocks in Stages 3 and 4 with the proposed SHMA block. We provide further ablation studies on the choice of ratio for the ViT blocks. Specifically, We choose the model after enlarging the kernel size as a starting point, then we progressively replace the convolutional blocks in Stages 3 and 4. We do not modify Stages 1 and 2 as their larger spatial dimensions would considerably increase the memory requirements for the self-attention mechanism."}, {"title": "Scaling to Larger Model", "content": "Although iFormer is designed for mobile-device applications, the combination of fast local representation capacity of convolution and the efficient global modeling proficiency of the proposed SHMA enables its scalability for a broader range of applications. To demonstrate the scalability of iFormer, we developed a larger model named iFormer-H with 99M parameters and trained it for 300 epochs following the same strategy outlined in Section B. It is important to note that we add drop path and layer scale, which are commonly used in the training of larger models."}, {"title": "CONCLUSION", "content": "This work proposes iFormer, which integrates highly optimized convolutional operations for the early layers alongside a novel and efficient single-head modulation attention for the later layers. iFormer achieves SOTA Pareto-front in terms of Top-1 accuracy and mobile latency. We also validate the effectiveness of iFormer on downstream dense prediction tasks, including COCO object detection, instance segmentation, and ADE20K semantic segmentation. These inspiring results highlight the potential for mobile applications. We hope iFormer can facilitate the application of artificial intelligence on more mobile devices. In future work, we will seek to alleviate inference bottlenecks sociated with high-resolution images. Meanwhile, we plan to optimize iFormer for more hardware platforms, such as Android devices and NVIDIA Jetson Nano."}, {"title": "APPENDIX", "content": ""}, {"title": "EXPERIMENTAL SETTINGS", "content": "IMAGE CLASSIFICATION\nWe mainly follow the training recipe of ConvNeXt, while removing stochastic depth, layer scale, and exponential moving average to ensure a fair comparison with prior works. The models are trained for 300 epochs on 8 NVIDIA GPUs with a total batch size of 4096. We employ the same learning rate across all models. It is possible to further improve performance by adjusting the learning rates for different model variants, which we will explore in the future.\nFor distillation, we use the RegNetY-16GF model as the teacher model and apply a hard distillation loss, following the approach of DeiT . During inference, the average output of the classification head and the distillation head is used as the final output."}, {"title": "OBJECT DETECTION AND SEMANTIC SEGMENTATION", "content": "For object detection experiments, we train MaskR-CNN models on the COCO 2017 dataset for 12 epochs using standard training settings from the MMDetection toolkit.\nFor semantic segmentation experiments, we train Semantic FPN models on the ADE20K dataset for 40,000 iterations using standard training settings from the MMSegmentation toolkit. The input images are cropped to a resolution of 512\u00d7512 during training.\nFor backbone latency, we keep the same input size as training (i.e., 512\u00d7512) and measure the mobile latency on an iPhone 13 compiled by Core ML Tools."}, {"title": "MORE ABLATION STUDIES", "content": "Different Ways for Reducing Latency Here we provide a comparison of different methods for reducing latency, contrasting them with the approach discussed in Sec. 3.3. Specifically, we reduce the baseline latency to similar latency by directly removing blocks, cutting down FFN expansion width,"}, {"title": "RELATION TO SHVIT", "content": "We clarify the difference between SHA in iFormer and its counterpart in SHViT from the following two aspects: First, in terms of motivation, iFormer explores efficient attention mechanisms specifically tailored for the on-device environment, whereas SHViT is geared towards general-purpose GPUs, which may exhibit different hardware characteristics. Second, in terms of methodology, as shown in Fig. 5, we utilize single-head attention with more channels (R is set to 2.), while SHViT employs fewer than 1/4 of channels for attention. The reduced number of channels can result in a lower rank of the attention matrix, potentially degrading its expressiveness. Additionally, the split and concatenate operations in SHViT introduce extra runtime."}, {"title": "ARCHITECTURE DETAILS", "content": "In Table 15, we show the different architecture configurations of the iFormer model variants.\nFIFORMER FOR HIGHER RESOLUTION\nSelf-attention exhibits quadratic complexity with respect to the number of tokens, i.e., the resolution of the input image. This issue is exacerbated in dense prediction tasks, which usually require high-"}]}