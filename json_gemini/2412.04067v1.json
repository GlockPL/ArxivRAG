{"title": "Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning", "authors": ["Amnon Bleich", "Antje Linnemann", "Bj\u00f6rn H. Diem", "Tim OF Conrad"], "abstract": "Recent advances in deep learning and natural language generation have significantly improved image captioning, enabling automated, human-like descriptions for visual content. In this work, we apply these captioning techniques to generate clinician-like interpretations of ECG data. This study leverages existing ECG datasets accompanied by free-text reports authored by healthcare professionals (HCPs) as training data. These reports, while often inconsistent, provide a valuable foundation for automated learning. We introduce an encoder-decoder-based method that uses these reports to train models to generate detailed descriptions of ECG episodes. This represents a significant advancement in ECG analysis automation, with potential applications in zero-shot classification and automated clinical decision support.\nThe model is tested on various datasets, including both 1- and 12-lead ECGs. It significantly outperforms the state-of-the-art reference model by Qiu et al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the reference model. Furthermore, several key design choices are discussed, providing a comprehensive overview of current challenges and innovations in this domain.", "sections": [{"title": "I. INTRODUCTION", "content": "CARDIOVASCULAR diseases remain a leading cause of morbidity and mortality worldwide, highlighting the importance of effective and timely diagnostic methods. Traditional ECG analysis, while indispensable, often requires manual interpretation by trained physicians, which can be time-consuming and subject to human error. At the same time, advances in AI-driven text generation algorithms present a novel opportunity to take advantage of existing ECG data sets for which physician-written free-text comments are available. These models have the potential to generate informative text about ECG episodes, which could offer decision support to clinicians during diagnostic tasks and thus reduce cognitive load on medical professionals and potentially improve the speed and accuracy of diagnosis.\nSignificant progress in encoder-decoder architectures, particularly in image captioning, has demonstrated their capacity for generating coherent descriptions [1], [2]. While these methods have succeeded in domains such as image processing, their application to medical data, specifically electrocardiography (ECG), remains underexplored [3]. One major challenge is the scarcity of purpose-built, high-quality labeled datasets for ECG analysis, as generating such datasets requires the involvement of highly trained individuals, making the process resource-intensive [4]. However, a promising alternative lies in leveraging existing ECG records paired with free-text comments, which are often generated as a byproduct of routine clinical practices [5]. Although these free-text comments were not originally created for training machine learning models, they represent a rich source of data that can be adapted to train encoder-decoder architectures, capable of generating concise textual summaries of ECG episodes, as demonstrated in this work.\nThis study introduces an innovative approach to text generation designed for ECG analysis, validated on the publicly available PTB-XL dataset [6]. In addition to achieving state-of-the-art performance, a key contribution of this work is the establishment of a reproducible benchmark for future studies, as our work is, to the best of our knowledge, first of its kind to be tested on the official test subset of the PTB-XL dataset, ensuring both transparency and comparability. Furthermore, we present a case-study on a more challenging and larger dataset of single-lead, variate, and subcutaneous ECG (SECG) signals obtained from implantable cardiac monitors (ICMs) [4]. Unlike purpose-built datasets, this dataset comprises labels and free-text reports generated as a byproduct of routine clinical and administrative processes. These reports, often created to fulfill procedural requirements, have not been cross-checked for accuracy, introducing additional noise and complexity that make the dataset more difficult to leverage for automated learning. This case study highlights the robustness and adaptability of our method to diverse and less curated real-world data sources.\nThe method introduced in this study employs an encoder-decoder architecture, utilizing a ResNet-based encoder [7] paired with either an LSTM [8] or Transformer [9] decoder. In the following sections, we provide an overview of related work to contextualize this study within the broader scientific landscape and facilitate comparison with existing methods. We then present a comprehensive description of our proposed model, detailing its architecture, tokenization techniques, data preprocessing procedures, and the hyperparameter optimization and training strategies employed. This is followed by a description of the datasets used, along with the specific preprocessing steps applied. Next, we outline the experiments conducted to evaluate the performance of our model and analyze the results obtained. Finally, we discuss the key findings and suggest directions for future research."}, {"title": "II. RELATED WORK", "content": "Various methods have been proposed in recent years for the automatic diagnosis of ECG data (e.g., [10]\u2013[12]) and other feature-based heart data (e.g., [13]). Stracina et al. [14] provide a comprehensive review of advancements and possibilities in ECG analysis since the first successful recording of the electrical activity of the human heart in 1887, including recent developments leveraging deep learning techniques.\nA key takeaway from these reviews is that the majority of existing ECG analysis methods focus primarily on the classification of ECG episodes, such as identifying atrial fibrillation (AF) or other abnormal heart rhythms. In contrast, the field of automated report generation for ECG data remains largely unexplored. Meanwhile, significant advancements in the task of generating textual descriptions from visual data (commonly referred to as image captioning) have been achieved through the development of sophisticated machine learning models and language models in particular. Seminal works in this domain include works by Vinyals et al. [1], and subsequently Xu et al. [2], which introduced an encoder-decoder architecture utilizing ResNet [7] as the encoder and a Long Short-Term Memory (LSTM) network [8] as the decoder to successfully generate descriptive text for images. This framework has since been extended to various domains, including medical signal processing. Pang et al. [3] conducted a survey of recent technologies in medical report generation, primarily addressing medical imaging, such as X-ray screenings, demonstrating the versatility and potential of such architectures. However this work offers limited insight into one-dimensional signals like ECG or EEG. This highlights a critical gap in the application of LLMs to medical data, emphasizing the need for further exploration in this area.\nGiven the substantial progress in automated data captioning and the relative scarcity of work in applying these techniques to ECG data, this area represents a significant research opportunity. Building on the foundational work of Vinyals et al."}, {"title": "III. A NEW METHOD FOR ECG CAPTION GENERATION: ARCHITECTURE AND TRAINING OPTIMIZATION", "content": "This section introduces a novel approach for generating text descriptions of ECG data through encoder-decoder architectures, adapted from successful image captioning frameworks [1], [2]. The method leverages a ResNet-based encoder for embedding ECG signals and incorporates Transformer or LSTM decoders to produce descriptive, clinically relevant reports.\nThe entire encoder-decoder architecture is visualized in Fig. 2."}, {"title": "A. Encoder for ECG Embedding", "content": "To create an effective embedding for ECG signals, we employed a modified 34-layer ResNet architecture tailored for 1D inputs. The standard classification layer was removed and replaced with an average pooling layer, allowing for adjustable output sizes to preserve temporal information across 512 output channels. This adaptation enables the retention of temporal relationships, which together with attention mechanism in the decoder, can be utilized for ECG interpretation. The output embedding from this encoder forms the input to the decoding stage."}, {"title": "B. Decoder Architectures for Generating ECG Description", "content": "The decoder's task is to generate meaningful, contextual descriptions from the ECG embeddings. We tested two architectures for this purpose: a Transformer-based decoder and an LSTM-based decoder, described in detail below."}, {"title": "1) Transformer Decoder:", "content": "The Transformer decoder [9] receives a combined input of ECG embeddings and tokenized report sequences, leveraging self-attention to dynamically focus on different parts of the ECG signal during token prediction. The concatenation is formalized as follows:\n$$X = [f_1, f_2, ..., f_k, e_1, e_2, ..., e_T]$$\nwhere $e_t$ denotes the embedding of the t-th token in the report, and $f_k$ represents the features of the k-th ECG segment provided by the encoder. K is set by the user and determines the extent of downsampling of the ECG signal.\nUsing the combined input X, the transformer calculates the context vector $c_t$ at each time step t:\n$$c_t = \\sum_{i=1}^{K+T} a_{t,i}x_i$$\nWhere $x_i$ represents each element in X which can be either an ECG-segment feature vector $f_k$ or a token embedding $e_t$. The attention weights $a_{t,i}$ are calculated following the standard Transformer attention mechanism described in [9]."}, {"title": "2) LSTM Decoder:", "content": "The LSTM decoder [8] generates one word at a time, conditioned on a context vector, the previous LSTM hidden state, and the last predicted token (or ground truth token during training). The attention mechanism dynamically weights different segments of the encoded ECG signal, creating a context vector $c_t$ at each time step t:\n$$c_t = \\sum_{k=1}^{K} a_{t,k}f_k$$\nwhere $a_{t,k}$ is the attention weight at time step t for temporal segment k, defined by:\n$$a_{t,k} = \\frac{exp(w^T(W_1h_{t-1} + W_2f_k)+)}{\\sum_{j=1}^{K} exp(w^T(W_1h_{t-1} + W_2f_j)+)}$$\nHere, w, $W_1$ and $W_2$ are learnable weights within linear layers, and $h_{t-1}$ is the previous hidden state.\nUltimately, both the transformer and LSTM based models are trained using cross-entropy loss."}, {"title": "Attention:", "content": "In the realm of natural language processing (NLP), attention mechanisms have been shown to enhance the recognition of objects in images, as demonstrated by Ba et al. [18]. These mechanisms, which focus on relevant parts of the data, are hypothesized to enable the model to dynamically focus on relevant portions of the ECG signal and thus improve the quality of generated descriptions.\nWhile self-attention is natively integrated into the Transformer's structure and can be adjusted to attend to different segments of the ECG (refer to equation 2), it is externally added in our LSTM-based model. In this case, in each prediction step t we use an at vector, sized to match the size of each channel output by the encoder (240 in our best-performing model, which represents temporal encoding of the ECG signal), and perform weighted aggregation of it. This results in a single weighted value for each encoder output channel, conditioned on the networks previous state $h_{t-1}$. Refer to equation 4 for details about the calculation of $a_t$. By applying weighted aggregation to the channels of the downsampled ECG signal using weights that account for both the ECG signal and the current network state, the model is able to attend to different parts of the ECG when predicting each word in the generated report. An example to this attention driven prediction process is visualized in Fig. 1. Finally, as in [2], doubly stochastic attention regularization is applied to prevent overfitting. This encourages the model to distribute its attention more uniformly, avoiding a tendency to over-focus on specific time steps and ensuring diverse attention across the signal."}, {"title": "C. Training Optimization and Enhancement Techniques", "content": "Our training pipeline incorporates several optimization strategies to improve model performance and ensure reliable generation of ECG descriptions. Key techniques include encoder pre-training and targeted hyperparameter tuning."}, {"title": "Encoder Pre-Training:", "content": "In one configuration, we pre-trained the encoder using rhythm class labels as a preliminary task to strengthen ECG embeddings before fine-tuning on free-text reports. This pre-training phase was intended to leverage rhythm labels for additional contextual learning; Moreover, as described in Section IV-A the ICM dataset includes approximately 738K episodes with rhythm class labels but no accompanying reports, enabling us to tap into this large dataset, which would otherwise not be utilized. Pre-training resulted in an F-score of 0.69 on the PTB-XL dataset (6 classes) and 0.49 on the ICM dataset (11 classes)."}, {"title": "Hyperparameters Tuning:", "content": "Extensive hyperparameter tuning was conducted to optimize model performance and stability. Key parameters were adjusted as follows:\nEncoder architecture: We tested both ResNet-18 and ResNet-34, aiming to optimize both model performance and computational efficiency.\nDecoder architecture: We varied the layer depth for both LSTM and Transformer decoders, from a single layer up to 20 layers.\nLearning rates: Separate learning rates were optimized for the encoder and decoder.\nStopping Criteria: We explored various early stopping criteria such as cross-entropy loss (which was also used for back-propagation), BLEU-1, BLEU-4 and METEOR scores.\nAdditional Parameters: We also tuned various parameters, including embedding sizes, batch sizes, teacher-forcing probabilities [19], normalization techniques (at batch, dataset, and episode levels), and top-k sampling. These adjustments contributed to enhancing model robustness and performance consistency.\nThis comprehensive tuning process involved multiple repetitions of experiments to reduce the impact of random variations, resulting in a stable and reliable evaluation of model performance. The best-performing parameter configurations are as follows:\nEncoder: A learning rate of 4 * 10-4 was optimal, with a stem kernel of 9 in the first convolutional layer and subsequent ResNet stage kernel sizes of 9, 7, 7, and 5, respectively. Each of the 512 output channels was set to an output size of 240.\nTransformer: The best configuration used 12 layers and 8 attention heads, a learning rate of 1 * 10-4, and an ECG input size of K = 1 (as described in Section III-B, subsection Transformer Decoder).\nLSTM: We used a single LSTM layer with a learning rate of 4 * 10\u20134, a teacher forcing probability of 1 during training (disabled during validation and testing), and an attention layer of size 512.\nFor both our LSTM and Transformer based models we used batch size of 32, no normalization, cross-entropy as the loss function and Adam optimizer. The token embedding size was set to 512, the dropout rate to 0.5 and the learning rate decay factor to 0.8 every 8 epochs without improvement, with the METEOR score (see section V-D under METEOR) as the target metric. Top-k sampling of 1 was used for deterministic token selection."}, {"title": "D. Complexity & Runtime", "content": "The model was trained on a single node equipped with 8 CPU cores, 62.5 GB of memory, and one NVIDIA A100 GPU (80 GB memory). The runtime was estimated based on training both the encoder and decoder from scratch (without pre-training) on the PTB-XL dataset, using the optimal hyperparameters described in section III.\nEncoder: The ResNet-34 architecture contains 13.76M parameters.\nDecoder: The Transformer decoder, with 38.36M parameters, has an estimated runtime of approximately 7 hours and 10 minutes on the specified hardware for training the entire model, including the encoder (without encoder pre-training). By comparison, the LSTM decoder with 5.52M parameters-completes training in roughly 1 hour and 10 minutes (in both cases, runtime depends on the number of epochs before early stopping).\nWhile the best-performing Transformer model uses 12 layers, reducing this to 8 layers provides a runtime-optimized alternative with minimal impact on performance. This modification reduces the parameter count to 25.75M and shortens the runtime accordingly. Similarly, using ResNet-18 instead of ResNet-34 reduces the encoder's parameter count to 6.93M without substantially affecting accuracy.\nNote: Both the Transformer and the LSTM training are not optimized in terms of runtime and, presumably a relatively minor effort could decrease it significantly. Furthermore, using a pre-trained encoder reduces the runtime by about 10% (not including the time for pre-training, which, if included, results in a longer training time overall)."}, {"title": "IV. DATA & PREPROCESSING", "content": "In this section, we introduce the datasets used for training and evaluating our ECG caption generation model. We outline the main characteristics of each dataset, followed by the preprocessing techniques employed to ensure compatibility with our model architecture. These steps are critical for enhancing model performance and ensuring robust, generalizable results."}, {"title": "A. Data", "content": "The success of our model relies on access to annotated ECG data that captures a wide range of cardiac conditions and data quality.\nTo achieve this, we selected two complementary datasets: the publicly available PTB-XL dataset, which provides clinical-grade, multi-lead ECG recordings, and the proprietary dataset provided by BIOTRONIK SE & Co KG, comprising single-lead sECG data from implantable cardiac monitor (ICM) devices. This combination allows us to evaluate the model's adaptability to varying ECG formats, sampling rates, and recording environments, thus ensuring its relevance for both clinical and personal health monitoring applications."}, {"title": "1) PTB-XL:", "content": "The PTB-XL dataset [6], [20] comprises 21,801 10-second ECG episodes, with two optional sampling frequencies: 500 Hz (resulting in 5000 data points per ECG record) and 100 Hz (1000 data points per record). These episodes were collected from 18,869 unique patients, giving a unique patient proportion of 0.87 - indicating that 87% of the total episode count corresponds to unique individuals, while the remaining episodes are contributed by patients with multiple recordings. Each episode is annotated with a rhythm label and a corresponding report (for details see [6]).\nThe dataset includes 9839 unique reports, where uniqueness is determined by ignoring minor differences such as whitespace, case sensitivity, and punctuation. This results in a unique report proportion of 0.45, meaning that 45% of the total reports are distinct, with the remainder being repeated across multiple episodes. Using this dataset served multiple purposes in our research. Firstly, it enabled us to test our model on a publicly available dataset, ensuring the reproducibility of our results. Additionally, it allowed us to assess the model's performance on 12-lead, 500 or 100 Hz, 10-second ECG records and evaluate how it manages a relatively small sample size. Furthermore, we used this dataset for comparability with the reference model, which was also tested on PTB-XL [15] (on 100Hz)."}, {"title": "2) Implantable Cardiac Monitor (ICM):", "content": "The ICM dataset contains records from Implantable Cardiac Monitors (ICMS) BIOMONITOR III and BIOMONITOR IIIm [21], which consists of 60-second sECG episodes recorded at a sampling frequency of 128 Hz, resulting in 7680 data points per sECG record. The report and rhythm class (a single label selected by HCPs out of given list) data were assembled as a byproduct of routine medical procedures conducted worldwide. The anonymized data underwent additional filtering to ensure that all personal information was removed from the manually entered reports. To avoid translation issues only reports in English were used. The final report dataset (Part 2 below) comprised data from 1033 clinics. The dataset is divided into two parts:\nPart 1: 737,999 episodes, each labeled with a rhythm class provided by an HCP. These labels are a byproduct of routine medical procedures and were not cross-checked for accuracy and are therefore potentially inaccurate.\nPart 2: 206,768 episodes, where each episode includes both a rhythm class and a free-text report: a sentence written by an HCP as part of routine medical procedures, not necessarily intended as an ECG caption for learning or training purposes. These episodes come from 6687 implanted devices. Among these 200k episodes, there are 38,652 unique reports, (here too, ignoring minor differences like whitespace, case sensitivity, and punctuation), resulting in a unique report proportion of 0.19."}, {"title": "B. Data Pre-Processing", "content": "Consistent data preprocessing ensures compatibility with our model architecture and optimizes performance by standardizing data across both datasets. Steps such as train-validation-test splits, abbreviation unification, and tokenization were carefully designed to maintain data integrity while enhancing comparability."}, {"title": "a) Episode Deduplication::", "content": "In the ICM dataset, we applied an episode deduplication process. An episode was considered duplicated if it originated from the same implanted device, had the same report, rhythm class, and recording date. During deduplication, one episode from each group of duplicates was randomly selected, and the others were removed from the dataset."}, {"title": "b) Train/Validation/Test Splits::", "content": "To prevent data leakage and enhance model generalizability, we applied device-level splits for training, validation, and testing. This approach ensures that each patient's data is confined to one split, reducing the risk of overfitting and supporting a more meaningful evaluation of the model's performance.\nPTB-XL: We tested two splitting approaches for this dataset. The first approach used the official splits provided by PhysioNet, which maintain patient exclusivity across splits and mitigate data leakage risks. These splits allocate 80% of data for training, 10% for validation, and 10% for testing, as further detailed in [6]. The second approach, used for comparability with the reference model [15], involved a random split into training, validation, and testing sets with proportions of 64%, 16%, and 20%, respectively. Results from both approaches are presented in Section VI, Tables II and III.\nICM: For this dataset, splits were applied at the implanted device level to ensure no overlap between the training, validation, and test sets, preventing the model from leveraging morphological similarities across episodes from the same device. After deduplication, data was divided into training (80%), validation (10%), and test (10%) sets."}, {"title": "c) Abbreviations & Translations::", "content": "Due to medical terminology variations in the datasets, an abbreviation unification process was applied to reduce prediction errors. Additionally, the OPUS-MT model [22], [23] was used to translate German reports from the PTB-XL dataset into English, allowing for consistent application of abbreviation standardization and enhancing model compatibility. The abbreviations and their unified form are presented in Table I."}, {"title": "C. Report Tokenization", "content": "Report tokenization was conducted by splitting comments based on non-letter characters (such as spaces, commas, slashes, etc.). These splitting characters were tokenized and retained within the text, with the exception of spaces. Spaces were not tokenized but assumed to occur between every two tokens to avoid skewing evaluation metrics (e.g., METEOR or BLEU-score) due to the abundance of spaces, which would otherwise inflate scores and reduce the relative significance of other tokens.\nThis process yielded an initial vocabulary size of 5,304 for the ICM dataset and 2,282 for the translated PTB-XL dataset, or 7,015 and 2,455, respectively, without applying the abbreviation script. As abbreviations only apply to English, they were not used on the non-translated PTB-XL reports. Filtering out words occurring fewer than twice further reduced the ICM and PTB-XL datasets vocabularies to 3,194 and 1,383 tokens, respectively. To ensure comparability across datasets, we truncated each vocabulary, retaining only the 1,024 most frequent tokens. Note that after vocabulary truncation, the least frequent word of the PTB-XL has a frequency of 3 and that of the ICM dataset a frequency of 14. Therefore, in the presented case filtering tokens based on their frequency deemed redundant.\nEach report was prefixed with a start token and suffixed with an end token, with padding tokens added to a maximum report length of 300 tokens (although in practice, no reports exceeded this length). A special token was assigned for unknown tokens: words appearing only once or outside the top 1,024 most common tokens.\nThe Byte-Pair-Encoding (BPE) method [24] was also tested, but it performed suboptimally compared to word tokenization, likely due to the specialized terminology in these datasets.\nIn summary, the PTB-XL and ICM datasets provide diverse ECG data well-suited for training and evaluation of our caption generation model. By carefully preprocessing the data, including patient/device-level splits, translation and abbreviation unification, we enhanced our model's ability to generate precise, contextually relevant ECG descriptions."}, {"title": "V. EXPERIMENTS", "content": "In this section, we summarize extensive evaluations of our model across multiple design configurations (Section V-A) and its comparison with a state-of-the-art method (see Section V-B). Additionally, we present the results of our case study (Section VI-C) and a sanity check conducted to confirm that the model's performance is attributable to its ability to learn ECG morphology (Section VI-D)."}, {"title": "A. Key Experiments", "content": "Throughout our experimentation, we addressed several key research questions by running a series of targeted experiments. Table II, summarizes the performance of our model on the publicly available PTB-XL dataset across the following main variations:\nDecoder Architecture (LSTM vs. Transformer): As the choice of decoder architecture is fundamental to our model, we tested all configurations with both LSTM and Transformer decoders, allowing us to compare overall performance between the two.\nEncoder Depth (ResNet18 vs. ResNet34): To determine whether the depth of the ResNet encoder affects model performance, we experimented with both ResNet18 and ResNet34 encoders.\nEffect of Encoder Pre-Training: To quantitatively evaluate whether pre-training the encoder on rhythm class labels improves model performance, we compared results with and without encoder pre-training. In our case-study, as discussed in Section IV-A, such pre-training also leverages an additional 738K episodes.\nTranslation to English: Since our study primarily uses English as the target language, we conducted all experiments with text translated to English. To maintain reproducibility, however, we also ran an experiment without translation or abbreviation unification, testing the model on raw text.\nAbbreviation Unification vs. Raw Text: To test the impact of the abbreviation unification process described in Section IV-B, we compared results on such standardized text against results on the raw text. As the abbreviation unification applies exclusively to English, we included translation for this comparison as well."}, {"title": "B. Comparison with the Reference Model", "content": "To evaluate the effectiveness of our approach, we benchmarked its performance against the reference model proposed by Qiu et al. [15]. This model employs a ResNet-based encoder for feature extraction from ECG signals, which are then aligned with pretrained embeddings from large language models (LLMs) such as GPT-2 and BERT. Additionally, Qiu et al. introduced an Optimal Transport (OT)-based objective complementing the standard cross-entropy loss, to enhance alignment between ECG embeddings and language embeddings. Their method has achieved strong results in ECG disease classification and report generation tasks, particularly on the PTB-XL dataset, making it a relevant benchmark for comparison.\nHowever, Qiu et al.'s use of random train/validation/test splits raises concerns regarding patient data overlap, potentially inflating results due to shared patient data across splits. Such overlap is problematic for generalizability, a critical factor for clinical applications. To address this, we adopt more rigorous splitting methods in our experiments, including the official PTB-XL splits that ensure patient exclusivity across sets, thereby enhancing reliability and comparability.\nA detailed comparison between the reference model and our model's performance is provided in Section VI, particularly in Table III."}, {"title": "C. Experimental Setup", "content": "Our primary experiments were conducted on the PTB-XL dataset, which serves as a comprehensive benchmark in this domain. To ensure reproducibility and generalizability, we used the train/validation/test splits recommended by the PTB-XL authors. These fixed splits enhance reliability as they are not random, and the validation and test datasets are considered gold standards, comprising reports manually reviewed by professionals [6].\nTo facilitate a direct comparison with the reference model, which is evaluated on random splits, we also tested our best-performing models on comparable random splits, applying translation but excluding abbreviation unification to align with the reference approach. Results from this experiment are presented in Table III."}, {"title": "D. Evaluation", "content": "Evaluating generated text is a challenging task, as even humans may struggle to assess the similarity in meaning between two sentences. We used three key metrics suited for such evaluation:\nBLEU [25] measures the precision of N-grams (1- to 4-grams) between generated and reference texts, applying a brevity penalty for shorter outputs to account for recall. This metric is widely used for its simplicity, but it emphasizes precision over recall.\nMETEOR [26] improves on BLEU by incorporating both precision and recall, offering a more balanced assessment. It also includes word-order sensitivity and explicit word-matching (exact, stemmed, and synonym-based), making it suitable for tasks where accurate phrasing is critical.\nROUGE [27], originally designed to emphasize recall (R), is particularly useful for assessing the completeness of information, as is often required in summarization tasks. However, beyond Recall, ROUGE also includes Precision (P) and F1-score (F), providing a more comprehensive evaluation of generated text.\nAnother metric worth noting is MRScore [28] (preprint), which utilizes large language models (LLMs) such as GPT to evaluate generated reports based on human-like criteria. While it has not yet gained widespread adoption in clinical text generation, it represents a promising direction for incorporating LLM-based assessments in the future.\nMETEOR was chosen as our primary metric due to its ability to evaluate semantic similarity more effectively than BLEU or ROUGE. BLEU's reliance on exact matches makes it less suitable for tasks like clinical text generation, where paraphrasing or synonym usage is common. ROUGE, while including precision and F1-score, is primarily recall-focused, which can bias evaluations towards longer outputs. METEOR, on the other hand, balances precision and recall while incorporating linguistic features like stemming and synonyms, making it better suited for capturing subtle nuances in clinical reports."}, {"title": "VI. RESULTS", "content": ""}, {"title": "A. Key experiments", "content": "The results of our main experiments, as shown in Table II, indicate that our model outperforms the current state-of-the-art reference method across all metrics. This improvement is achieved even under a more rigorous setup, using patient-exclusive test sets, compared to the random splits used in the reference model.\nOur findings suggest that the optimal configuration employs a non-pretrained ResNet34 as the encoder and an LSTM as the decoder. However, performance across other configurations did not differ significantly. Notably, the performance gap between ResNet34 and the more compact ResNet18 encoder was minimal, suggesting that ResNet18 is a practical alternative when hardware or time constraints are considerations. This setup, combined with an LSTM decoder and bypassing time-intensive translation steps, still yielded comparable results.\nMoreover, pre-training the encoder on episode rhythm labels did not improve model performance; in fact, it appeared to slightly diminish predictive accuracy. Translating the reports to English, however, resulted in a noticeable METEOR score boost from 50.72% for the non-translated version to 55.01% for the translated version in the top-performing architecture. Abbreviation unification had a minor yet positive impact (about 0.5%) on the scores for translated reports.\nFig. 3 shows the progression of the METEOR score over training epochs for key experiments. The plot suggests the absence of overfitting and reveals that the LSTM-based model achieves accuracy comparable to the Transformer-based model in nearly half the epochs."}, {"title": "B. Experiment with the Reference Model Setup", "content": "To ensure a fair comparison, we conducted an additional experiment replicating the setup used by the reference model (detailed in Section V-C). The results, presented in Table III, demonstrate that both our LSTM-based and Transformer-based models outperform the reference model, with the LSTM-based model showing a slight advantage over the Transformer-based approach."}, {"title": "C. Case Study - Single lead, subcutaneous ECG", "content": "In our case-study, we evaluated performance on single-lead, subcutaneous ECG data with reports from ICM devices (See Section IV-A). The model's performance on this dataset, though lower than on PTB-XL, still surpassed current state-of-the-art benchmarks, demonstrating the model's robustness in challenging data scenarios. Notably, the Transformer and LSTM-based models performed similarly, with the Transformer showing a slight edge, likely due to minor random variations.\nNotably, despite the 738K episodes containing only rhythm labels and thus available only for pre-training (alongside ~200K episodes with labels as well as reports which are used for both pre-training and training), pre-training the encoder did not enhance performance on this dataset.\nThe comparatively lower BLEU-4 score for the ICM dataset may be due to the brevity of the reports (median length of 4 tokens and an average of 5.67), reducing 4-gram match probability between generated and reference texts. This is supported by the relatively higher BLEU-1 and BLEU-2 scores, indicating more accurate matching in shorter text segments.\nLastly, abbreviation unification markedly improved performance on this dataset. While its effect was minimal on PTB-XL, it led to a pronounced increase (from 15.57% to 32.59%) on the ICM dataset. This is likely because a significant portion (31.2%) of the PTB-XL dataset is automatically generated, which may result in a higher degree of uniformity in terms and language. In contrast, the ICM dataset exhibits greater variability in terminology, an inconsistency mitigated effectively by abbreviation unification."}, {"title": "D. Sanity Check", "content": "To ensure our models were generating text based on ECG data rather than relying on common text patterns, we performed a sanity check. Here, we replaced the ECG input with a uniform vector in which all entries are set to 1, effectively removing ECG information. A successful sanity check would result in a substantial drop in performance, confirming that the model's output is informed by ECG morphology. As expected, this experiment led to prediction of uniform comments of optimal length comprising the most common words in the corpus.\nThe sanity check was conducted both on the PTB-XL and the ICM datasets, post-abbreviation unification. The results on the ICM dataset demonstrate an 8.52% drop in METEOR score, from 32.59% in the regular experiment to 24.07% in the sanity check and a drop of 2.11% in BLEU4 score, from 10.61% in the regular experiment to 8.5% in the sanity check, reflecting 26% and 19% reduction in these scores respectively. While the relatively high performance in the sanity check suggests some dependency on recurring tokens, the observed drop highlights the model's ability to incorporate meaningful ECG data, even in the relatively challenging ICM dataset.\nAs expected, the sanity check performed on the official splits of the PTB-XL dataset resulted in a more pronounced performance drop across various metrics. For instance, BLEU4 decreased from 0.35 in the original experiment to 0.08 in the sanity check, reflecting a 77% reduction. METEOR score dropped from 0.56 to 0.31"}]}