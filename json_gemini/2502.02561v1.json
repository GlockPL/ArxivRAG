{"title": "Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents", "authors": ["Shayan Kiyani", "George Pappas", "Aaron Roth", "Hamed Hassani"], "abstract": "A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions in ways that can usefully inform downstream action. This interface between prediction uncertainty and decision-making is especially important in risk-sensitive domains, such as medicine. In this paper, we develop decision-theoretic foundations that connect uncertainty quantification using prediction sets with risk-averse decision-making. Specifically, we answer three fundamental questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers? We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk. (2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions? We show that a simple max-min decision policy is optimal for risk-averse decision makers. Finally, (3) How can we derive prediction sets that are optimal for such decision makers? We provide an exact characterization in the population regime and a distribution free finite-sample construction. Answering these questions naturally leads to an algorithm, Risk-Averse Calibration (RAC), which follows a provably optimal design for deriving action policies from predictions. RAC is designed to be both practical-capable of leveraging the quality of predictions in a black-box manner to enhance downstream utility and safe adhering to a user-defined risk threshold and optimizing the corresponding risk quantile of the user's downstream utility. Finally, we experimentally demonstrate the significant advantages of RAC in applications such as medical diagnosis and recommendation systems. Specifically, we show that RAC achieves a substantially improved trade-off between safety and utility, offering higher utility compared to existing methods while maintaining the safety guarantee.", "sections": [{"title": "1 Introduction", "content": "Predictions are frequently used to inform actions. For example, in clinical medicine, patient data are used to predict diagnoses and outcomes when choosing treatments. In high-stakes cases where an incorrect treatment decision could lead to serious complications or death it is crucial not to rely solely on a model's predictions. Instead, decisions must account for the uncertainty in these predictions, opting for more conservative interventions when that uncertainty makes the potential outcomes (e.g., complications, side effects) highly variable. Connecting uncertain predictions to actionable, principled decisions is a significant challenge in safety-critical domains, including medical diagnosis, finance, robotics, and control, and requires balancing safety with utility. At one extreme, we could ensure safety by avoiding any action, essentially ignoring predictions and thus failing to be of practical use. At the other extreme, we could aggressively exploit predictions attempting to maximize expected utility, trading off high (expected) upside gains for significant downside risk at the cost of realizing poor outcomes with significant probability. Balancing this trade-off requires optimally integrating prediction into decision making in a way that is risk-sensitive. To this end, we focus on the following question:\nWhat is the optimal interface between prediction and action that allows for navigating the trade-off between safety and utility in high stakes applications?\nThe optimal design of an action policy hinges on optimal uncertainty quantification. Among the various methods for quantifying uncertainty, a widely adopted approach-driven by advances in conformal"}, {"title": "2 Fundamentals of Risk Averse Decision Making", "content": "In this section, we will formalize the central objective of a risk averse decision maker. We are given a space of features (or covariates, contexts) X, a set of labels Y, and a set of possible actions (or decisions) A. We assume that the pair (x, y) \u2208 X \u00d7 Y, i.e., the feature and the label, is generated according to a fixed but unknown distribution D. Upon observing x \u2208 X, the decision maker will have to take an action a \u2208 A. Importantly, the decision maker does not observe the true label y. However, the utility of the decision maker will depend on both the chosen action a and the label y, and is captured by a given utility function u(\u00b7, \u00b7) : A \u00d7 Y \u2192 R+.\nIn this paper, we will focus on risk-averse decision making; I.e., the decision maker would like to choose its actions such that the obtained utility is guaranteed to be large enough with high probability over the randomness of the label. In other words, risk-averse behavior refers to a preference for actions that minimize the likelihood of low-utility outcomes, even if it means overlooking actions with potentially higher but uncertain utilities. To be more precise, consider a pre-specified risk tolerance threshold \u03b1. For a given x \u2208 X, from the viewpoint of the risk-averse decision maker each action a \u2208 A has value equal to: v_\u03b1(a; x) := quantile_\u03b1[u(a, Y) | X = x]^1, where Y is distributed according to p(y|x). This objective is a standard risk measure and is known in the financial risk literature as the Value at Risk (VaR) (see e.g. (Duffie and Pan, 1997)). The value at risk represents the largest value such that, if action a is taken when facing x, then the obtained utility is guaranteed to be at least v_\u03b1(a; x) with probability 1 \u2212 \u03b1. Consequently, the risk averse decision maker should choose the action that has the largest quantile value v_\u03b1(a; x), and the resulting best (risk-averse) utility will become:\nv_\u03b1(x) = max v_\u03b1 (\u03b1; x) := max quantile_\u03b1 [u(a, Y) | X = x], \u2200x \u2208 X.        (2)\n\u03b1\u0395\u0391\n\u03b1\u0395\u0391\nThe above risk-averse utility should be contrasted with the best expected utility max_a E[u(a, Y)|X = x]. The latter leads to actions that maximize the average utility whereas the former aims to maximize the worst-case utility that can happen with probability 1 \u2212 \u03b1. Hence the former will be more risk averse at the cost of becoming more conservative.\nMarginal Version. The quantity in (2) is a point-wise or conditional quantity; i.e. to find the best action according to (2) the decision maker requires access to the conditional distribution p(y|x). In practice, such distributions are unknown and are often intractable when only a finite sample of the distribution is available. An analogous situation arises in conformal prediction (CP), where obtaining fully-conditional coverage guarantees is known to be impossible from a finite sample of data (Vovk, 2012; Foygel Barber et al., 2019). Consequently, conformal prediction focuses on relaxed marginal (or \"group conditional\", which still marginalize over part of the distribution (Bastani et al., 2022; Jung et al., 2023)) coverage guarantees which are statistically tractable.\nBy analogy, we will now introduce the marginal version of (2). First we rewrite the objective. For a given x \u2208 X, the value v_\u03b1(x) in (2) can be equivalently written as follows\nMaximize \u03bd\n\u03b1,\u03bd\nsubject to Pr[u(a, Y) > \u03bd | X = x] \u2265 1 \u2212 \u03b1.\nLet us examine the constraint in the above optimization more carefully. We are looking for action-value pairs (\u03b1,\u03bd) such that we are guaranteed with probability at least 1 \u2212 \u03b1 that, when taking action a, the resulting utility is at least \u03bd. Of course, to maximize utility, we should maximize over the choice of the action a and the value \u03bd which results in the above optimization. Now, the risk-averse constraint in the above optimization has the following marginal counterpart:\nPr[u(a(X), Y) \u2265 \u03bd(X)] \u2265 1 \u2013 \u03b1, (3)\nwhere the function \u03b1(\u00b7) : X \u2192 A is a decision-policy that\u00b2 maps features to actions such that it guarantees average utility according to the function \u03bd(\u00b7) : X \u2192 R with probability at least 1 \u2212 \u03b1, marginalized over X. Now, rather than optimizing over a single value for \u03b1 and \u03bd for each x separately, we jointly optimize over policies \u03b1(\u00b7) and value functions \u03bd(\u00b7)3 which map X to actions and values respectively. This results in the following marginal version of the decision maker's optimization problem:\nRisk Averse Decision Policy Optimization (RA-DPO):\nmaximize Ex[\u03bd(X)],\n\u03b1(.), \u03bd(\u00b7)\nsubject to Pr[u(a(X), Y) \u2265 \u03bd(X)] \u2265 1 \u2212 \u03b1,\nRemark 2.1. Despite our primary focus on the marginal formulation of risk-averse optimization, the core arguments and methodologies presented in this work naturally extend to the more advanced setting of risk-averse optimization with group conditional validity. Specifically, consider a collection of arbitrary, pre-specified and potentially intersecting groups G1, G2,\u2026, Gm \u2286 X. The marginal constraint in RA-DPO can be generalized to a group-conditional form as follows:\nPr[u(a(X), Y) \u2265 v(X) | X \u2208 gi] \u2265 1-a, \u2200i\u2208 [1,\u2026\u2026,m].\nThese types of conditional constraints are known as group-conditional validity in the conformal prediction literature (see (Jung et al., 2021; Bastani et al., 2022; Jung et al., 2023; Gibbs et al., 2023)). This extension allows for a more fine-grained control over risk across different subpopulations or scenarios, which is crucial in applications where group-specific guarantees are important. Notably, all the theoretical results developed in section 2.3 and proposition 3.1 can be systematically generalized to accommodate these group-conditional constraints."}, {"title": "2.2 A Prediction Set Perspective", "content": "Recall that in our setting the (feature, label) pair is generated according to a distribution. The decision maker only observes the feature x based on which it will choose its action a. However, the realized utility will depend on both the action a and the label y. The decision maker does not observe the label,"}, {"title": "2.3 An Equivalent Formulation Through Prediction Sets", "content": "In the previous section we argued that if the decision maker's knowledge about the label is only based on the prediction sets, then the (minimax) optimal policy aRA and its associated value vRA are given in (5). Hence, assuming that the decision maker is playing aRA, the prediction sets C(x) should be designed to maximize the resulting utility of the decision maker while ensuring marginal coverage; I.e., the prediction sets C(x) should be designed according to the following optimization:"}, {"title": "3 The Optimal Prediction Sets", "content": "In this section, we characterize the optimal solution (i.e., prediction sets) for RA-CPO 2.3 in terms of the conditional distribution p(y | x). Before that, let us summarize what we have done so far and give an overview of the steps we are going to take in this section. We proved in the previous section that RA-DPO (2.1) and RA-CPO (2.3) are equivalent. To solve the RA-CPO we will first introduce a reparametrization as in (10), which we then solve using techniques from duality theory.\nWe now focus on three fundamental objects (denoted by bold symbols) that describe the optimal sets:\n1. A function \u03b8(x, t) \u2208 R, which represents the optimal (risk-averse) utility achievable under a conditional coverage assignment t \u2208 [0,1] for a covariate x \u2208 X.\n2. A function a(x,t) \u2208 A, which gives the action that attains the corresponding risk-averse utility level.\n3. A function t*(x) \u2208 [0,1] that optimally assigns conditional coverage probabilities for x \u2208 X, ensuring a total coverage of 1 \u2212 \u03b1. Moreover, as we will show, t*(x) can be characterized via a single scalar \u03b2 and a function g(x, \u03b2), which is derived from the conditional distribution and the utility values.\nWe begin by introducing the main notion that we use to relate optimal utility to coverage. We define the functions \u03b8 : X \u00d7 [0, 1] \u2192 R and a : X \u00d7 [0, 1] \u2192 A as follows:\n\u03b8(x, t) = max quantile\u2081\u2212t [u(a, Y) | X = x], a(x, t) = arg max quantile\u2081\u2212t [u(a, Y) | X = x] (8)\na\u2208A\na\u2208A\nIn words, given a feature x \u2208 X and a probability coverage value t \u2208 [0, 1], \u03b8(x, t) is computed as follows (see also Figure 2): For each action a, we first find the (1-t)-quantile of the random variable u(a, Y) with Y being distributed according to p(y|x). This quantile value is the largest utility achievable with"}, {"title": "4 The Main Algorithm: Risk Averse Calibration (RAC)", "content": "So far, we have shown that RA-DPO (2.1), the primary problem that a risk averse agent cares about, is equivalent to RA-CPO (2.3), an optimization problem over prediction sets. In Proposition 3.1 and Theorem 3.2, we derived the structure of the optimal prediction sets for the RA-CPO problem. These sets are defined by the following functions: (i) \u03b8(x, t) given in eq. (8), which fundamentally relates coverage to utility; (ii) a(x, t) given in eq. (8), which is the corresponding action that provides the desired utility; and (iii) the assignment function g(x, \u03b2) introduced in eq. (12). These quantities are defined based on the true conditional distribution which is often unknown in practice.\nIn this section, we consider the finite-sample setting in which we assume access to a set of calibration samples {(Xi, Yi)}_i=1^n, as well as a predictive model, f : X \u2192 \u0394y, which assigns to each x \u2208 X a |Y|- dimensional probability vector. Here, the output of f for an input x, denoted by fx, should be thought of as the approximate probabilities that a forecaster provides for the labels given the input x. For example,"}, {"title": "5 Experiments", "content": "In this section, given a pre-trained model, f(\u00b7), which assigns probability fx (y) to input-label pair (x, y), we compare RAC with two groups of baselines:\nCalibration + Best-Response. We calibrate the model on the calibration data using a strength- ened version of decision calibration (Zhao et al., 2021), specifically the variant from (Noarov et al., 2023), which provides swap regret bounds. We then apply the best-response policy: best-response(x) = arg maxa\u2208A Ey~fz(y) [u(a, y)]. The primary purpose of implementing this baseline is to highlight the consequences of fully trusting the predictive model. While we expect the best-response policy over a calibrated model to achieve higher average utility at test time (see the Section 1), it is likely to make more frequent critical mistakes compared to our method.\nConformal Prediction + Max-Min. We construct (1 \u2013 \u03b1)-valid prediction sets using split conformal prediction with three different scoring rules. The decision policy then applies the max-min rule from Section 2: aRA (C(x)) = arg maxa\u2208 A miny\u2208C(x) u(a, y), which we proved is the optimal strategy when deciding based on prediction sets in Section 2. The three scores are:\n\u2022 score-1 (Sadinle et al., 2019): 1 \u2013 fx(y),\n\u2022 score-2 (Romano et al., 2020): \u03a3fx(y')\ny': fx (y')>fx(y) ,\n\u2022 score-3 (Cortes-Gomez et al., 2024): a greedy scoring rule tailored to the max-min policy.\nBy varying \u03b1, we can control the degree of conservativeness, trading off average utility against the avoidance of catastrophic errors. We compare in terms of safety and utility using the following metrics:\n\u2022 (a) Average realized max-min value: The test-time mean of the worst-case utility across the prediction sets (i.e., the average of vRA in (5)).\n\u2022 (b) Fraction of critical mistakes: For samples with a critical ground-truth label, we report the fraction of cases in which each method chooses the worst action in test-time.\n\u2022 (c) Average realized utility: The empirical mean of the realized utilities across all test samples.\n\u2022 (d) Realized miscoverage: The fraction of test samples for which the true label is not in the prediction set."}, {"title": "5.1 Medical Diagnosis", "content": "In this experiment, we explore decision making in medical diagnosis and treatment as a risk-sensitive application. Specifically, we use the COVID-19 Radiography Database (Chowdhury et al., 2020; Rahman et al., 2021), which contains chest X-ray images of four categories: Normal, Pneumonia, COVID-19, and Lung Opacity. We randomly split the data into training (70%), calibration (10%), and test (20%) sets. In this experiment, we use the Inception_v3 architecture (Szegedy et al., 2015, 2016), developed by google, a convolutional neural network known for its inception modules that employ multiple filter sizes in parallel. We initialize the model with ImageNet-pretrained weights and then fine-tune it on our dataset, meaning we retrain the higher layers while preserving much of the earlier-layer feature representations.\nTo capture clinical priorities, we employ the utility matrix in Table 1, which maps each true condition (row) to a set of actions (column). Although we use the specific matrix below, our setup can accommodate any alternative choice of the utilities. All the baselines then will be calibrated to connect model's predictions to these four actions.\nAfter training, we vary the nominal miscoverage parameter \u03b1 during calibration to study its impact on performance. As shown in Figure 3(a), our method achieves the best trade-off curve among baselines, providing higher worst-case utilities for every nominal \u03b1. Equivalently, it offers stronger utility certificates"}, {"title": "5.2 Recommender Systems", "content": "We next consider a risk-sensitive recommendation scenario using the MovieLens dataset. Each data point is a user-movie pair (x = (user features, movie features), y), where the label y \u2208 {1,2,3,4,5} is the user's rating. We split the data into training (80%), calibration (10%), and test (10%), and train a neural network classifier f (details in the Appendix) to estimate the probability distribution fy(x).\nAt test time, the policy must decide whether to recommend or not recommend a movie. We use the utility function in Table 2: if a movie with true rating y is recommended, the utility is y \u2212 3, while not recommending yields 0.\nWe vary the nominal miscoverage \u03b1 during calibration and measure performance on test data. As shown in Figure 3(a), our method achieves the best trade-off among baselines, offering stronger utility certificates (worst-case utility) at all \u03b1 levels. Figure 3(c) also shows that our approach outperforms other CP-based methods in average utility."}, {"title": "6 Discussion and Future Work", "content": "In this paper, we establish the decision-theoretic foundations of conformal prediction, demonstrating that valid prediction sets serve as a sufficient statistic for risk-averse agents seeking to optimize their value at risk. Building upon this framework, we developed an algorithmic interface to connect the predictions of any black-box predictive model to actions with marginal, distribution-free safety guarantees.\nWhile this paper primarily focuses on marginal safety guarantees, we recognize that in many practical scenarios, marginal guarantees alone may not suffice. Specifically, three types of conditional safety guarantees are often desirable: group-conditional safety (i.e., safety conditioned on certain characteristics of the covariate x), label-conditional safety (i.e., safety conditioned on the true label y), and action- conditional safety (i.e., safety conditioned on the action a taken by the decision maker). Although we leave these aspects as avenues for future exploration, we believe that the majority of our findings can be systematically extended to these more complex cases.\nFurthermore, while in many applications it is feasible to define a utility function that captures the preferences of the decision maker, in some contexts this may prove challenging. In such cases, we could rely on estimation of the utility or express preferences in a relative manner-indicating that one action is preferred over another in a given context. These considerations point to two promising directions for future work: first, examining the robustness of Risk-Averse Calibration under utility mis-specification, and second, developing algorithmic frameworks that require only access to preference functions, rather than explicit quantitative utility functions. Alternately, we could explore uncertainty quantification that is simultaneously useful for many downstream decision-makers with different utility functions. This approach does not require knowledge of the utility function of the specific downstream decision maker that we are interested in. Noarov et al. (2023); Roth and Shi (2024) show that this is possible for expectation maximizing decision makers using refinements of (decision) calibration \u2014 is the same possible for risk averse decision makers?"}, {"title": "7 Acknowledgments", "content": "This work was supported by the NSF Institute for CORE Emerging Methods in Data Science (EnCORE) and NSF grant FAI-2147212. The authors wish to thank John Cherian, Natalie Collina, and Bruce D. Lee for helpful discussions."}, {"title": "A Proofs", "content": "We prove that the risk-averse decision rule\naRA(C(x)) := arg max min u(a, y)\na\u2208A y\u2208C(x)\nsolves the minimax problem in (6).\nPart 1: Upper bound for any arbitrary policy. Let \u03c0(\u00b7) : 2Y \u2192 A be any policy, and let C(.) be a fixed set function satisfying\nPr [Y\u2208 C(X)] \u2265 1-\u03b1.\n(X,Y)\u223cP\nWe construct a \"worst-case\" distribution in \u03a9 for \u03c0.\nPick any x \u2208 X for which C(x) != 0. Define a distribution p\u2217(x, y) by\np\u2217(X = x) = 1, p\u2217(Y = y | X = x) = { 1 for some y \u2208 arg minz\u2208C(x) u(\u03c0(C(x)), z),\n 0 otherwise.\nUnder p\u2217, we have Y \u2208 C(X) almost surely (since C(x) is nonempty and we place all mass on a label in C(x)). Hence p\u2217 \u2208 \u03a9 because the marginal coverage constraint\nPr [Y\u2208 C(X)] = 1 \u2265 1-\u03b1\n(X,Y)\u223cp\u2217\nis satisfied. But under this distribution, the utility of \u03c0(C(x)) is forced to be\nmin u(\u03c0(C(x)), y),\ny\u2208C(x)\nsince Y is chosen (with probability 1) to be the worst-case label within C(x). Thus, for this specific x, no matter how we choose \u3160, its achievable value is at most miny\u2208C(x) u(\u03c0(C(x)), y). Also,\nmin u(\u03c0(C(x)), y) \u2264 max min u(a, y),\ny\u2208C(x)\na\u2208A y\u2208C(x)\nBecause x was arbitrary (among those with C(x) != (\u00d8), repeating the same argument for each such a yields\ninf \u03bd\u2217(\u03c0,\u03c1) < inf max min u(a,y).\n\u03c1\u2208\u03a9 x: C(x)\u22600 a\u2208A y\u2208C(x)\nIn other words, any policy cannot achieve a value larger than the above infimum for the inner mini- mization in (6).\nPart 2: Achievability by the max min policy. Next, we show that the policy\n\u03c0\u2217(C(x)) = arg max min u(a, y)\na\u2208A y\u2208C(x)\nmatches the upper bound from Part 1 and is thus minimax optimal. Consider any \u03c1 \u2208 \u03a9.\nDefine\nv(x) := max min u(a, y).\na\u2208A y\u2208C(x)\nFor those x \u2208 X such that C(x) is empty put v(x) = maxa\u2208a maxyey u(a, y). We claim that with probability at least 1 \u2013 \u03b1, the policy aRA(C(x)) achieves a utility at least v(x). Indeed, on the event {Y \u2208 C(X)} (which has probability at least 1 - \u03b1 by assumption), it holds that\nu(ARA (C(X)), Y) \u2265 min u(ARA (C(X)),y) = v(X)."}, {"title": "A.1 Proof of Proposition 2.2", "content": null}, {"title": "A.2 Proof of Theorem 2.3", "content": "We give a constructive proof by showing how from each solution of RA-DPO we can construct a feasible solution of RA-CPO without losing any utility, and vice versa. By applying this to the optimal solutions of both problems, we obtain the result of the theorem.\n(I) From RA-DPO to RA-CPO. Suppose we have an feasible solution (a(\u00b7), v(\u00b7)) to the RA-DPO problem. Consider a pair (a(\u00b7), v(\u00b7)) such that a : X \u2192 A and v : X \u2192 [0, Umax]. Here, we have Umax = maxa maxy u(a, y), and as mentioned in Section 2.1, since v is a utility certificate its value at any x should be less than Umax. Since (a, v) is a feasible solution of RA-DPO, it satisfies the following:\nPrx,y [u(a(X), Y) \u2265 v(X)] \u2265 1 \u2212 \u03b1.\nDefine a prediction set\nC(x) = {y | u(a(x),y) \u2265 v(x)}. (15)\nIn words, C(x) is the set of labels y for which the utility u(a(x), y) is at least v(x). By definition, we have\nPr[Y \u2208 C(X) | X = x] = Pr[u(a(X), Y) \u2265 v(X) | X = x].\nAs a result, we have\nPr[Y \u2208 C(X)] = Ex [Pr[Y \u2208 C(X) | X]]\n= Ex [Pr[u(a(X),Y) \u2265 v(X) | X]]\n= Pr[u(a(X), Y) \u2265 v(X)]\n> 1 - \u03b1.\nHence, C(\u00b7) satisfies the marginal coverage constraint of RA-CPO.\nNext, we will improve the prediction sets C to new prediction sets C\u0303 which satisfy the marginal guarantee but can potentially have larger value under the objective of RA-CPO. The basic idea is to consider points x \u2208 X such that C(x) is empty and augment an additional element to those empty sets. Recall that we defined Umax := maxa\u2208A maxy\u2208y u(a, y). Hence, there exists at least one (action, label) pair, which we call (amax, Ymax) such that Umax = u(amax, Ymax). Now, let us define\nXempty = {x \u2208 X : C(x) = 0},\nwhere \u2205 denotes the empty set. We now update C(\u00b7) to C\u0303(\u00b7) as follows:\n- if x \u2208 Xempty : C\u0303(x) = {ymax},\n- if x \u2209 Xempty: C\u0303(x) = C(x)."}, {"title": "A.3 Proof of Proposition 3.1", "content": "Proof of Proposition 3.1. Fix any instance x \u2208 X and a coverage value t \u2208 [0,1]. Recall from (8) that\n\u03b8(x,t) = max quantile1_t [u(a, Y) | X = x], a(x,t) = arg max quantile1_t [u(a, Y) | X = x].\na\u2208A\na\u2208A\nWe want to show that among all sets C with Pr[Y \u2208 C | X = x] > t, the set\nC(x,t) = { y \u2208 Y : u(a(x,t),y) \u2265 \u03b8(x,t)}\nmaximizes the risk-averse utility vRA(C) = maxa\u2208a miny\u2208c u(a, y), and the maximum value is \u03b8(x, t)."}, {"title": "A.4 Proof of Theorem 3.2", "content": "We start from the reparametrization of RA-CPO given in (10):\nmaximize Ex [\u03b8(X, t(X))]\nt:X\u2192[0,1]\nsubject to: Ex [t(X)] \u2265 1 \u2212 \u03b1.\n(Reparametrization of RA-CPO)\nWe will further reparametrize this optimization problem and find equivalent relaxations. To do so, let us define\np(x,t) = 1[t < t(x)]. (16)\nAlso, we will need to consider the derivative of the function \u03b8(x, t) in terms of its second argument t. Since the function \u03b8 can be discontinuous, we will have to consider its generalized derivative (i.e. consider delta functions). More precisely, let \u03b8'(x, .) : R \u2192 R\u2217 where R\u2217 is the space of functionals on R, such that \u03b8'(x, .) is the generalized derivative of \u03b8(x, .). In other words, for any real values a and b,\n\u03b8'(x, t)dt = \u03b8(x, b) \u2013 \u03b8(x, a).\nWe can just think of \u03b8'(x,t) as the derivative \u03b8(x,t). We can then rewrite the objective of our optimization problem as\nEx [\u03b8(X, t)] = Umax + Ex p(x, t)\u03b8' (x, t)dt,\nt=0"}, {"title": "A.5 Proof of Theorem 4.1", "content": "We have:\nPr[Ytest \u2208 CRAC (Xtest)] @ Pr [Ytest \u2208 \u0108(Xtest; \u03b2Ytest)]\n= E[1[Ytest \u2208 \u0108(Xtest; \u03b2Ytest)]]\nE\n\u2248 E [n[t + n1(1Y (X) \u2208 \u0108(Xi; \u03b2Y ))] + [Ytest \u2208 \u0108(Xtest; \u03b2Ytest)])]\n\u2200Y\n@ 1 - \u03b1,\nwhere, (a) comes form the definition of the prediction set. (b) comes from the fact that\n{(X1,Y1, \u03b2Ytest), (Xn, Yn, \u03b2Ytest), (Xtest, Ytest, \u03b2Ytest)}\nare exchangeable, which is due to the fact that (i) the exchangeability of the original (n + 1) pairs {(Xi, Yi)} \u222a {(Xtest, Ytest)}, and (ii) the symmetric way in which Algorithm 1 assigns \u03b2y to each y \u2208 V. Finally, (c) follows from the definition of \u03b2Ytest."}, {"title": "BUtility function for medical experiment", "content": "Our results and findings in the medical experiment of section 5.1, can be reproduced with any other reasonable design of utility function. The goal of that experiment is not to capture a precise characteri- zation of difficulties and consequences in medical decision making but rather to pinpoint the advantages of a risk averse calibration approach in sensitive tasks like medical decision making. Of course, in real world scenarios, a more comprehensive approach is needed to define a principled utility function that captures the interests of all the involving parties. That being said, for the sake of proof of concept, we designed a utility matrix using the ChatGPT ol model by OpenAI. The following is an AI generated text justifying the proposed utility matrix.\nClinical Justification of the Utility Matrix\nThe utility matrix presented in Table 3 reflects the balance of benefits and harms associated with different medical actions for each true clinical condition. Each utility value is determined based on standard clinical guidelines and evidence-based practices, ensuring that the chosen actions optimize patient outcomes while minimizing potential risks.\nNormal (No Disease)\n\u2022 No Action = 10\nFor a patient who is truly healthy, no intervention is optimal as it avoids unnecessary costs, side effects, and patient anxiety. Unwarranted use of antibiotics or quarantine measures can lead to adverse effects and resource wastage (NIHCE, 2015).\n\u2022 Antibiotics = 2\nPrescribing antibiotics to a healthy individual can contribute to antimicrobial resistance and cause side effects without any clinical benefit (CDC, 2022).\n\u2022 Quarantine = 2\nQuarantining a healthy person imposes unnecessary social and psychological burdens without pro- viding any medical advantage (CDC, 2020).\n\u2022 Testing = 4\nWhile testing can confirm the absence of disease, routine testing in healthy individuals is often not cost-effective and may lead to unnecessary follow-up procedures (of Radiology, 2023).\nPneumonia"}]}