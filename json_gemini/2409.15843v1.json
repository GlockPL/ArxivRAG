{"title": "FROM PASSIVE WATCHING TO ACTIVE LEARNING: EMPOWERING PROACTIVE PARTICIPATION IN DIGITAL CLASSROOMS WITH AI VIDEO ASSISTANT", "authors": ["Anna Bodonhelyi", "Enkeleda Thaqi", "S\u00fcleyman \u00d6zdel", "Efe Bozkir", "Enkelejda Kasneci"], "abstract": "In online education, innovative tools are crucial for enhancing learning outcomes. SAM (Study with AI Mentor) is an advanced platform that integrates educational videos with a context-aware chat interface powered by large language models. SAM encourages students to ask questions and explore unclear concepts in real-time, offering personalized, context-specific assistance, including explanations of formulas, slides, and images. In a crowdsourced user study involving 140 participants, SAM was evaluated through pre- and post-knowledge tests, comparing a group using SAM with a control group. The results demonstrated that SAM users achieved greater knowledge gains, with a 96.8% answer accuracy. Participants also provided positive feedback on SAM's usability and effectiveness. SAM's proactive approach to learning not only enhances learning outcomes but also empowers students to take full ownership of their educational experience, representing a promising future direction for online learning tools.", "sections": [{"title": "1 Introduction", "content": "Chatbots have increasingly become valuable tools in educational settings, offering significant enhancements to the learning experience and outcomes Hobert and Meyer von Wolff [2019], Kuhail et al. [2023]. Research consistently shows that these AI-driven tools can increase student engagement, motivation, and knowledge retention by providing personalized support and immediate feedback Pappagallo [2024], Ghayoomi [2023], Tapalova and Zhiyenbayeva [2022], Benotti et al. [2017], Kuhail et al. [2023]. Chatbots have been effectively used to deliver course materials, assist with"}, {"title": "2 Related Work", "content": "Artificial intelligence (AI) is significantly transforming education by enabling personalized learning experiences, enhancing student engagement, and optimizing administrative tasks Pratama et al. [2023], Benotti et al. [2017]. AI technologies, such as machine learning and natural language processing, allow for tailored educational approaches that cater to individual learning styles and paces, potentially leading to improved student outcomes Harry and Sayudin [2023]. This section highlights key advancements relevant to SAM, examining online learning tools, chatbots, virtual"}, {"title": "2.1 Overview of Online Learning Tools", "content": "Online learning tools have revolutionized education, providing flexible and accessible options for students and educators worldwide. These digital platforms and software applications facilitate remote learning, content creation, student engagement, and assessment in virtual environments Al-Fraihat et al. [2020], Martin and Bolliger [2018]. They also have a crucial role in modern education, especially in the context of engineering disciplines Sivapalan et al. [2016], science education Rutten et al. [2012], language learning Gonz\u00e1lez-Lloret [2020], and overall digital transformation of education Beaumont [2018]. These tools, ranging from digital collaboration platforms like Zoom Minhas et al. [2021], Gunawan et al. [2021], Microsoft Teams Rojabi [2020], and Google Classroom Beaumont [2018] to interactive video-based learning and simulation tools Gordillo et al. [2022], have been shown to enhance student engagement, motivation, and learning outcomes in various educational settings Vermeulen and Volman [2024]. By incorporating features like learner-centered approaches, fast feedback mechanisms, and modern technologies, online learning tools not only improve the quality of education but also make the learning process more efficient for students Pokotylo [2023]. Additionally, the use of various tools like gamification, simulation, and virtual reality has been experimented with to enhance knowledge dissemination and learning experiences Sinha [2024]. As technology continues to evolve, further research is needed to explore the full potential of these tools in diverse educational contexts and student populations.\nOne of the most notable trends in online education is the rise of Massive Open Online Courses (MOOCs), which offer high-quality courses for mass delivery through the Internet Dabbagh et al. [2016], Ferreira [2016]. Initially coined in 2008 Fini [2009], Liyanagunawardena [2015], MOOCs have gained popularity globally, attracting learners interested in experiencing top-tier university teaching Liyanagunawardena [2015]. These courses have disrupted conventional educational models by offering free, accessible, and diverse learning opportunities to a wide audience, challenging the economic and pedagogical paradigms of traditional on-campus universities Ferreira [2016]. With over 11,000 MOOCs available worldwide as of 2022 Gupta and Gupta [2022], these courses have revolutionized education delivery, reaching thousands of students and prompting discussions on their impact on international communication and learning models. Platforms such as Coursera Ayoub et al. [2020], edX Sanchez-Gordon et al. [2016], and Udacity Anyatasia et al. [2020] have expanded access to education by offering courses from top universities and institutions to a global audience Tsironis et al. [2016], often for free or at a low cost. MOOCs cover many subjects and provide learners with flexibility and accessibility, accommodating diverse schedules and learning paces. These platforms also incorporate interactive elements such as peer assessments, discussion forums, and real-time feedback to enhance the learning experience.\nIn addition to MOOCs, video-based learning platforms have also gained popularity by offering benefits such as enhanced student engagement and motivation, flexibility and accessibility for learners, and support for teacher professional development Sabli\u0107 et al. [2021]. Additionally, using video-based student support platforms, like ORBITS Shehata et al. [2023] predictive engine, has shown positive results in enhancing student learning experiences through artificial intelligence techniques. Platforms such as Panopto Getenet et al. [2022] and Kaltura Govender and Khoza [2022] provide comprehensive video hosting and streaming services, which include features like interactive quizzes and analytics to track learner engagement and progress. These tools are designed to cater to various learning styles by incorporating visual and auditory elements, making complex subjects more accessible and engaging for students Sheridan and Campbell [2024]. While these platforms offer valuable features, SAM goes a step further by providing real-time, personalized support during learning sessions, enabling students to ask questions and receive immediate, context- specific feedback. This level of interactivity helps bridge knowledge gaps more effectively than traditional video-based platforms, enhancing the overall learning experience."}, {"title": "2.2 Chatbots and Virtual Tutors in Learning", "content": "Chatbots are increasingly recognized for their transformative potential in education, leveraging artificial intelligence to enhance learning experiences. They offer personalized, 24/7 support, which can significantly improve student engagement and motivation, particularly in e-learning environments where traditional interaction may be limited Pappagallo [2024], Ghayoomi [2023], Tapalova and Zhiyenbayeva [2022], Benotti et al. [2017], Kuhail et al. [2023]. They can enhance learning by providing course materials Cunningham-Nelson et al. [2019], assignments and practice questions Sinha et al. [2020], while also engaging students individually Hobert and Meyer von Wolff [2019] or facilitating group learning activities Tegos et al. [2014] Kuhail et al. [2023]. Chatbots usually engage learners by utilizing text, speech, visuals, touch, and gestures to support them in different educational activities Kuhail et al. [2023]. Research indicates that chatbots can foster higher-order cognitive skills and reduce dropout rates by adapting to individual student needs Pappagallo [2024]. A recent study Kuhail et al. [2023] indicates that many previous works"}, {"title": "2.3 Engagement Through Questions", "content": "Learner engagement, which refers to the investment of students' cognitive and emotional effort in completing a learning task Lam et al. [2012], has been shown to positively influence key educational outcomes such as academic performance, persistence, satisfaction, and a sense of community Wang and Degol [2014] Halverson and Graham [2019]. In online settings, engagement often encompasses not only traditional indicators like contributing to discussions and asking questions Zeegers and Elliott [2019], B\u00fchler et al. [2023], but also involves consistent participation in virtual activities such as live chats Hew et al. [2023], quizzes Raes et al. [2020], and collaborative tasks Kim et al. [2015]. In online learning, where distractions can be more common B\u00fchler et al. [2024], engagement becomes crucial to ensure students remain focused, driving better learning outcomes. Effective online engagement also involves leveraging digital tools that promote interaction and personalized learning experiences. AI-driven approaches, such as those used for automating classroom observation protocols, can provide teachers with specific feedback on their practices, streamlining the assessment process while ensuring consistent and valuable insights Hou et al. [2024].\nProactive behaviors, such as students actively asking questions, serve as key indicators of engagement in online learning environments Zeegers and Elliott [2019], B\u00fchler et al. [2023], Sedova et al. [2019]. When students take the initiative to ask about course content, it reflects a high level of cognitive engagement, pointing to that they are not passively absorbing information but critically processing it. Questioning is an essential component of active learning, as it demonstrates curiosity and a desire to deepen understanding, driving students to connect new knowledge with prior learning B\u00f6heim et al. [2020a,b].\nSAM's design directly supports student engagement by facilitating question-asking during learning. By offering real-time, context-aware responses, SAM prompts students to interact with the material actively. This feature aligns with research that shows questioning is an indicator of engagement and a significant factor in improving learning outcomes Zeegers and Elliott [2019], B\u00fchler et al. [2023], Sedova et al. [2019]. Through its integration with educational videos and the ability to provide immediate feedback, SAM helps to create an interactive learning environment where students feel empowered to ask questions, thus deepening their understanding and retention of the material."}, {"title": "3 Functionalities and Working Principles of SAM", "content": "SAM (Study with AI Mentor) is an innovative tool designed to enhance educational experiences by integrating AI-driven assistance with video-based learning. SAM operates as an advanced video-watching platform, enabling users to view YouTube lectures while interacting with an AI mentor to pose questions related to the content (Figure 2). The integrated AI mentor is powered by GPT-40 OpenAI [2024], a highly advanced language model that offers sophisticated natural language processing capabilities. By leveraging a large language model (LLM), SAM provides real-time, contextually accurate assistance, enabling students to receive immediate responses to their inquiries. This interaction encourages students to actively engage with the material, asking questions whenever they encounter challenging concepts during the lecture. The AI mentor not only supports the instant clarification of doubts but also promotes a deeper understanding of complex topics by delivering explanations tailored to the learner's needs. This dynamic and personalized learning experience ultimately helps students grasp the subject matter more effectively, enhancing their overall educational journey.\nThe user experience with SAM begins when a lecture link from YouTube is provided. Users may also upload corresponding slides, which enriches the AI mentor's ability to deliver accurate and context-aware responses by leveraging both the video transcript and the additional slide information. To achieve highly accurate answers and only answer lecture-related questions, the following prompt was used:\nYou are roleplaying as an assistant teacher helping students understand their lecture\ncontent. Answer their questions based on the video, video transcripts, and slides. If\na question is about a lecture unrelated topic, respond with \"Please focus on the lecture\nmaterial.\" If you don't know the answer, just say that you don't know. Do not attempt\nto fabricate an answer.\nThen, the video transcript, lecture slides, chat history, and user questions were also sent to the language model."}, {"title": "4 User Study", "content": "To explore how proactive behavior can be encouraged in online learning environments, SAM was designed to provide learners with personalized, real-time support while maintaining their full control over the learning process. This approach aims to investigate the impact of SAM in fostering active participation and deeper engagement during independent learning. The user study for evaluating SAM's effectiveness and usability was conducted in two stages.\nInitially, we piloted the platform among participants of the Introduction to Deep Learning course at the Technical University of Munich. This pilot study focused on the 12th lecture \u00b9 of the course with a specific focus on graph neural networks, generative adversarial networks, and reinforcement learning, allowing us to gather preliminary feedback on SAM's usability and measure the participants' knowledge gain. We chose this lecture because neural networks and machine learning are increasingly popular topics that resonate with a wide audience, making them ideal for engaging participants. The course at the university had hundreds of students enrolled, providing a substantial pool of potential participants. Participation in this pilot was entirely voluntary, providing valuable insights to refine SAM before broader testing.\nFollowing the pilot, we launched the main study on the Prolific platform, where a diverse group of participants was recruited to watch a carefully selected lecture on neural networks. This lecture is titled \"Introduction to Neural Networks\" and is openly available on YouTube as part of the previously mentioned Introduction to Deep Learning"}, {"title": "4.1 User Study Phase 1: Pre-Knowledge tests", "content": "In the first phase of the study, participants were asked to complete both a pre-knowledge test (Appendix A.3.5) and a pre-test (Appendix A.3.6). The pre-knowledge test, which was only included in the main study, consisting of six questions with only one correct answer each, was designed to assess and categorize participants based on their existing knowledge of neural networks. Additionally, participants answered a question regarding their self-reported previous knowledge of neural networks, which allowed us to compare this subjective assessment with the objective results of the pre-knowledge test. The pre-test also contained six questions but differed in that each question had two correct answers in the main study, and one or more correct answers in the pilot study. Participants were informed that one or more answers could be correct, which helped measure their initial understanding without bias. The pre-test questions were categorized into three key areas: the basics of neural networks, the structure and training of neural networks, and activation functions for the main study, with two questions per category. Following these assessments, participants were provided with a brief text on linear regression in the main study to ensure they had the necessary background knowledge before moving on to the video-watching phase of the study (Appendix A.2). As participants in the pilot study has more background knowledge, no summary from previous lectures were provided."}, {"title": "4.2 User Study Phase 2: Lecture watching", "content": "In the second phase of the study, participants engaged in a lecture-watching activity, with different lectures being used for the pilot and main study, both of which are available on YouTube. For the pilot study, participants watched a lecture about deep learning, while participants watched an introductory video about neural networks (NNs) in the main study. To evaluate the effectiveness of a context-aware AI assistant, we randomly assigned participants to one of two groups: a control group that watched the video without an integrated AI mentor, and a test group that engaged with the lecture using SAM."}, {"title": "4.3 User Study Phase 3", "content": "In the third phase of the study, participants were asked to complete a post-test (Appendix A.3.7), provide feedback on SAM's usability, and share their demographic information. The post-test comprised six questions, mirroring the structure of the pre-test, with two questions per topic across the three categories. Each question had two correct answers, and both the control and test groups were required to complete this test in the main study. Participants were informed that one or more answers could be correct, similar to the pre-test questionnaire. This assessment was designed to gauge any improvements in the participants' understanding of the material after watching the lecture, with a specific focus on whether the AI-driven mentoring of SAM facilitated enhanced learning outcomes.\nKey aspects of the feedback questionnaire included the response time of SAM, the quality and reliability of its answers, and whether their comprehension of the three neural network topics improved after interacting with SAM. Afterward, demographic data were collected from both groups, covering age, gender, and employment status. The questionnaire also probed participants' prior experience with neural networks, machine learning models, and intelligent agents such as Alexa or Siri, providing a comprehensive overview of their background knowledge and familiarity with AI technologies."}, {"title": "4.4 Participant Recruitment", "content": "In our main user study evaluating SAM, we recruited participants from the Prolific platform, splitting them into two groups-one utilizing an AI mentor during video watching (test group) and the other simply watching the video (control group). We chose Prolific due to its reputation for providing high-quality data Peer et al. [2022]. Our sample was gender- balanced, including participants who were 18 or older, located in Germany and fluent in English. After recruitment, participants were directed to Qualtrics for the study, and upon completion, they returned to Prolific for compensation, which was set at a rate of \u20ac12.41/hour. Given the time difference between interacting with the AI mentor and watching the video uninterrupted, the study duration varied between 42 and 45 minutes. All data was collected anonymously,"}, {"title": "4.5 Experimental Design", "content": "For the purpose of our user study, SAM was adapted to simplify the user experience. The selected YouTube video and corresponding slides were uploaded into the platform. Participants were only required to click a 'Watch Lecture' button to initiate the session, streamlining the process and focusing on the interaction with SAM.\nParticipants were divided into two groups: a test group and a control group. The test group had full access to SAM, meaning they could watch the video and interact with the AI mentor to ask questions at any time during the lecture (Figure 4b). In contrast, the control group also used SAM's video-watching interface, but the chat functionality was disabled (Figure 4a). To measure participants' attentiveness in both groups, an attention check was implemented after 20 minutes of video playback. A window popped up asking, \"Would you like to continue to watch the video?\" with a button labeled \"Continue.\" The time taken to click this button was recorded as an indicator of attentiveness. This setup (Figure 4) enabled us to compare the learning outcomes between participants who had access to the AI mentor and those who did not, providing insights into the impact of interactive AI support on the learning process."}, {"title": "4.6 Analysis", "content": "We evaluate the collected results through a comprehensive analysis of multiple aspects, including knowledge gain, user satisfaction, and answer accuracy. By examining these key metrics, we aim to assess the overall effectiveness of SAM in enhancing the learning experience. We assessed participants' overall performance by assigning points to each question based on the number of correct answers it contained. Participants earned points for correctly selecting valid answers and for refraining from selecting incorrect ones. We initially analyzed the total points achieved by participants in both the pre- and post-test phases. Additionally, we examined the number of incorrect selections made, providing insight into participants' accuracy and comprehension before and after using SAM. After the pilot study, a pre-knowledge test was included because self-reported assessments of prior knowledge, rated on a 5-point scale from \"no knowledge\" to \"expert knowledge,\" can be subjective and inconsistent. To obtain a more accurate measure of participants' understanding of neural networks, we included six objective pre-knowledge questions in the main study. These questions provided a standardized basis for grouping participants according to their actual level of expertise, ensuring a more reliable analysis of the results. Participants who scored 1-2 points on the pre-knowledge test were categorized as having no prior knowledge, while those who scored 3-4 points were considered to have basic knowledge, and those who scored 5-6 points were classified as having intermediate knowledge.\nIn the pilot study, the maximum available points varied based on the number of correct answers per question across six pre-test and post-test questions, whereas in the main study, each of the six questions in both the pre-test and post-test featured four answer options with two correct answers each, standardizing the point distribution. The difficulty level of the pre- and post-test questions in the main study was evaluated by 10 unrelated machine learning experts, who rated the questions on a scale of 1 to 3, with 1 standing for easy and 3 standing for difficult. The average difficulty level for both the pre- and post-tests was 1.65, indicating that the questions were optimized for participants with little to no prior knowledge of the field, while still aiming to challenge their understanding. In both the pilot and main studies, the pre- and post-test questions varied, but consistently addressed the same topics covered in the lecture.\nTo draw meaningful conclusions from the observed knowledge gains, we conducted a t-test comparing the knowledge gains between the test group, who used SAM, and the control group. This statistical analysis allowed us to assess whether the differences in knowledge gain were significant. The corresponding p-value is reported in the results section (Section 5), providing insight into the effectiveness of SAM in enhancing learning outcomes."}, {"title": "5 Result", "content": "In this section, we present the findings from our study, focusing on the evaluation of SAM's effectiveness as an educational tool. By comparing the performance of participants in the test and control groups, we aim to offer insights into how triggered interactions enhance learning outcomes. Additionally, we explore how these insights can inform the design of future educational applications for more effective learning experiences."}, {"title": "5.1 Measuring performance and knowledge gain", "content": "To provide a clear understanding of the impact SAM had on participants' learning, we first present the detailed pre- and post-test scores. The exact mean values achieved by individuals, as detailed in Table 1, show the pre- and post-test results for both the test and control groups in the pilot and main studies. The results from the main study indicate that participants in the test group made fewer incorrect selections after watching the video with SAM. We further analyzed these findings by calculating knowledge gains to draw meaningful conclusions, ensuring a comprehensive evaluation of SAM's impact on learning outcomes.\nThe statistical analysis of knowledge gains in the main study revealed significant differences between the test and control groups, as indicated by a t-test with a p-value of 0.014. These results, suggest that SAM's AI-driven mentoring offers a measurable advantage in improving learning outcomes (Figure 5a). The evaluation of the achieved knowledge gain results in the pilot and main studies indicates that participants in the test group, who used SAM, demonstrated higher knowledge gains compared to the control group. To better understand the impact of SAM's features, we conducted a deeper analysis by examining the data across various subgroups, such as age and employment status. This involved comparing the performance of younger participants against older ones, as well as analyzing differences between students and full-time workers. Additionally, we looked at the test group in more detail, focusing on the number and nature of questions asked, which helped us identify how different participants engaged with SAM's context-aware capabilities.\nTo assess knowledge gains across different demographic groups, we focused on the main study due to the limited demographic diversity in the pilot study, where most participants were university students under 30 years old. In the analysis of age groups, participants under 28 years old in the test group demonstrated higher knowledge gains compared to their counterparts in the control group, while the opposite was true for participants aged 28-37 (Figure 5b). Given the limited number of participants older than 37, we focused only on the first two age groups to ensure representative data across all four subgroups. This trend suggests that our tool, SAM, which was primarily designed with students in mind, is particularly effective for younger learners, many of whom fall within the student demographic. This could indicate that younger participants, who are likely more accustomed to interactive learning technologies, benefit more from the AI-driven mentoring provided by SAM.\nIn our analysis of the main study, we examined knowledge gains in relation to age and employment status. For employment analysis, we specifically considered groups with at least four participants to ensure meaningful comparisons. The test group showed higher knowledge gains across most categories, including those working part-time, freelancers, the unemployed, and students (Figure 5c). However, the tool's effectiveness was less pronounced among those working full-time, whereas the control group performed better. Namely, participants in the test group with flexible working environments, such as students and part-time workers, achieved averagely a higher knowledge gain (1.8\u00b13.6) compared to the control group (mean of 1.4 \u00b1 3.9). However, full-time workers in the test group showed a lower knowledge gain (mean of 0.1 \u00b1 3.3) compared to their counterparts in the control group (mean of 1.9 \u00b1 2.7). This finding may suggest that students and those with more flexible work schedules can more easily engage with and benefit from SAM's"}, {"title": "5.2 User feedback", "content": "Overall, users in the pilot and main studies reported positive experiences with SAM, indicating that they found the tool helpful and engaging (Table 2). This is reflected in the generally high satisfaction scores and the willingness of participants to use SAM in the future. In general, on a 5-point Likert scale, participants rated high response quality and trust in the accuracy of answers, demonstrating SAM's effectiveness in providing real-time assistance. Only for the question of whether participants refrained from asking questions because of long response time, a 3-point Likert scale was used. In both studies, participants found personalized learning support and feedback to be the most valuable feature of SAM. In the pilot study, this was followed by the ability to request summaries, receive formula explanations, and upload images to inquire about figures and formulas. In the main study, the order of preference shifted slightly, with formula explanations being the second most valued, followed by the ability to upload images for clarification on figures and formulas, and the option to request summaries."}, {"title": "5.3 AI tutor answer analysis", "content": "In both the pilot and main studies, participants were asked to report whether they encountered any incorrect answers provided by SAM. Most participants indicated that they did not find or notice any inaccuracies. This feedback suggests that SAM's responses were generally perceived as accurate and reliable by the users. The detailed responses regarding the perceived accuracy of SAM's answers are summarized in Table 3, further supporting the tool's effectiveness in delivering correct and contextually relevant information.\nSince the lecture material was mostly new to the participants, their responses regarding whether they encountered incorrect answers may not be fully reliable, necessitating a manual evaluation of SAM's provided answers to ensure accuracy. In the main study, we collected all user-generated questions and had them evaluated by two machine learning experts to assess the correctness of the responses provided by SAM. Two experts carefully reviewed each answer, considering not only its general correctness but also its relevance to the specific context of the lecture material. For instance, SAM occasionally provided accurate but contextually inappropriate answers, such as offering a general explanation of matrix indices when the question was specifically related to the indices within a neural network's weight matrix, a concept tied closely to the lecture's content on network structure. Because the majority of the provided answers were correct, resulting in unbalanced labels, Cohen's K cannot not accurately reflect the inter-rater reliability. The experts achieved 95.2% agreement, and after discussion, they found that the LLM provided a factually correct answer in 96.8% of the cases. This high level of accuracy reflects SAM's effectiveness in responding to user queries within the context of the lecture material. A more detailed analysis, focusing on the factual correctness in each category, is provided in Table 4."}, {"title": "6 Discussion and Limitations", "content": "SAM (Study with AI Mentor) is an AI-driven educational tool designed to enhance learning by integrating real-time, context-aware question-answering with video content. A user study involving 140 participants evaluated SAM's effectiveness, focusing on knowledge gain, user satisfaction, and answer accuracy. However, variability in participants' prior knowledge, engagement levels, or motivation could have influenced the results. In our main study, we utilized a video lecture from a university course as the main instructional material. This video, being the third in the course series, may not have been sufficient to bring all participants to a similar baseline understanding of the topic at hand. By providing a summary of the concept to the participants before watching the lecture helped address this issue. This gap is evident in the test group, where many participants sought further explanations (e.g. on linear regression) during the study. Although we instructed participants not to use external resources during the tests, we cannot be entirely certain that this was adhered to. We attempted to mitigate this by monitoring the time participants spent on the study.\nMoreover, because the pilot study was conducted over the course of a semester, we lacked clear insights into the participants' prior knowledge. Since the question, \"Do you have previous knowledge in the following areas: Graph Neural Networks, Generative Models, or Reinforcement Learning?\" relies on self-reporting, we were unable to objectively measure how much they had absorbed from previous lectures or related courses in machine learning and deep learning offered by the same university. Feedback from the pilot study was valuable and informed adjustments for the main study, though some areas for improvement may have remained unaddressed. Another limitation identified was SAM's inconsistent performance in answering questions. In rare cases, SAM responded with \"I don't know,\" even when it had previously provided correct answers to differently phrased questions. There were also some instances where SAM failed to understand the specific context of a question, offering general but irrelevant answers to the current lecture content.\nFurthermore, while SAM was specifically designed with students in mind and demonstrated effectiveness for this subgroup, it did not perform as well for full-time employees. The structure of the tool emphasizes flexibility and interactive engagement, aligns well with the learning habits and needs of students who are accustomed to using digital platforms for their studies. However, full-time employees, who may have different learning preferences or time constraints, did not benefit from SAM as much. This suggests that while SAM is a powerful tool for enhancing student learning, its design might need to be adjusted to better accommodate different user groups, such as full-time professionals who require more tailored educational resources.\nSAM was only tested on a popular topic, \"Introduction to Neural Networks,\" which is well-suited to the technical capabilities of the tool. However, it would be valuable to explore how participants would interact with and utilize SAM in subjects beyond the realm of computer science, such as biology, literature, or other humanities. Testing SAM in these diverse areas could provide insights into its versatility and effectiveness across different disciplines. Based on the positive reception and high number of participants in our study, we suspect that SAM would also prove to be an effective educational tool in these varied subject areas.\nAdditionally, although SAM is flexible in its use, it is currently optimized for videos available on YouTube. This is a limitation given that many educational institutions utilize other platforms, such as Panopto Getenet et al. [2022], for hosting and streaming lecture content. Future iterations of SAM should consider integrating with a broader range of video-watching tools to enhance its applicability and reach within academic environments. Expanding SAM's compatibility with widely used educational platforms would not only increase its utility but also support its adoption across various learning contexts, ensuring that more students and educators can benefit from its capabilities.\nAlthough SAM currently utilizes GPT-40 OpenAI [2024] as its underlying LLM, this model is easily exchangeable, allowing for flexibility in integrating other commercial or open-source LLMs. Our primary focus has been on SAM's long-term impact in fostering personalized, real-time learning support, rather than being tied to a specific model. Investigating the most efficient LLM for SAM's continued development is a future direction for this project."}, {"title": "7 Conclusion", "content": "We introduced SAM, a context aware AI chatbot, offering the possibility to students to ask questions and explore unclear parts of the lecture. User studies with a total of 140 participants revealed its effectiveness as an educational tool focusing on proactive teaching. SAM enhances learning outcomes by encouraging students to take full ownership of their learning experience and fosters a more interactive and engaged learning environment. The real-time, personalized assistance provided by SAM allows learners to delve deeper into complex topics, reinforcing their understanding of the lecture content. The studies demonstrated that the test group using SAM, achieved higher knowledge gains compared to the control group, underscoring the potential of SAM as a future learning tool. The high variance in results motivated a more detailed analysis of demographic groups, which indicated that individuals in flexible working environments, such as students and freelancers, benefited the most, showing the greatest knowledge gains. The study also found that participants who had intermediate-level prior knowledge asked more questions during lecture watching. We also found empirical correlation between the number of questions asked and knowledge gain, highlighting the importance of proactive interaction in enhancing learning outcomes. According to expert raters, SAM maintained a high answer"}, {"title": "A.2 Linear Regression Summary from the Pre-Test Questionnaire", "content": "The following text appeared for each participant before watching the video:\n\"Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The simplest form, simple linear regression, involves a single independent variable and models the relationship with the equation $y = mx + b$, where $y$ is the predicted value, $x$ is the independent variable, $m$ is the slope of the line (indicating the relationship strength and direction between $x$ and $y$), and $b$ is the y-intercept (the value of $y$ when $x$ is zero). In multiple linear regression, the model includes multiple independent variables and extends to\n$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$.                                                                                                 (1)\nThe coefficients $b_i$ are estimated using methods such as Least Squares, which minimizes the sum of the squared differences between the observed values and the values predicted by the model. The goal is to find the best-fitting line that explains the variation in the dependent variable based on the independent variables, enabling predictions of $y$ for given values of $x$.\nLinear regression connects to classification tasks by providing a foundation for understanding how inputs (such as images of cats and dogs) can be used to predict outputs (labels - \"cat\" or \"dog\"). While linear regression predicts continuous values, classification tasks involve predicting discrete class labels. Techniques similar to linear regression, such as logistic regression, extend this concept by applying a transformation to the linear equation to handle binary or multiclass classification. This approach models the probability that a given input belongs to a particular class, enabling the assignment of class labels based on the highest predicted probability.\""}]}