{"title": "JUREE NOT JUDGES: SAFEGUARDING LLM INTERACTIONS WITH SMALL, SPECIALISED ENCODER ENSEMBLES", "authors": ["Dom Nasrabadi"], "abstract": "We introduce JurEE, an ensemble of efficient, encoder-only transformer models designed to strengthen safeguards in AI-User interactions within LLM-based systems. Unlike existing LLM-as-Judge methods, which often struggle with generalization across risk taxonomies and only provide textual outputs, JurEE offers probabilistic risk estimates across a wide range of prevalent risks. Our approach leverages diverse data sources and employs progressive synthetic data generation techniques, including LLM-assisted augmentation, to enhance model robustness and performance. We create an in-house benchmark comprising of other reputable benchmarks such as the OpenAI Moderation Dataset and ToxicChat, where we find JurEE significantly outperforms baseline models, demonstrating superior accuracy, speed, and cost-efficiency. This makes it particularly suitable for applications requiring stringent content moderation, such as customer-facing chatbots. The encoder-ensemble's modular design allows users to set tailored risk thresholds, enhancing its versatility across various safety-related applications. JurEE's collective decision-making process, where each specialized encoder model contributes to the final output, not only improves predictive accuracy but also enhances interpretability. This approach provides a more efficient, performant, and economical alternative to traditional LLMs for large-scale implementations requiring robust content moderation.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across different Natural Language Processing (NLP) tasks, transforming organisational interactions with information and users through applica-tions ranging from internal document retrieval systems to agentic, customer-facing chatbots. This has created a new medium of communication for organisations with their customers. As LLMs become more powerful and widespread, they offer unprecedented opportunities but also present significant challenges, particularly in ensuring safe operations within acceptable risk parameters. Consequently, the need for effective, real-time content moderation in user-AI interactions has become a priority across various sectors, amplifying the demand for robust safety mechanisms in these increasingly integral systems [41].\nWhile LLM-as-Judges offer flexibility by leveraging next-word prediction to assess safety risks [27, 20, 69, 22, 75], this approach inherently lacks the ability to provide probabilistic estimates of risk types. Research indicates that fine-tuned LLMs, when adapted for specific safety tasks, often devolve into narrow classifiers, losing their broader capabilities and becoming inefficient outside their finetune domain [26], while also having several inherent biases [73, 30, 8]. This not only results in suboptimal resource utilisation, where models with billions of parameters are employed for tasks that don't require their full potential, but also creates bottlenecks in real-time applications due to the computational demands. Additionally, reliance on external API calls for LLM-based safety evaluations can drive up operational costs, with some teams spending more on these guardrails than on directly addressing user queries.\nTo address these limitations, we propose an Encoder-Ensemble (JurEE), which we show to be significantly more performant and cheaper (in terms of inference latency and cost) than their decoder counterparts from the transformer model family. Our approach, reminiscent of the adage 'what is old is new again,' revisits the power of encoder-only models for text classification. While recent research has focused heavily on using decoder-based models (LLMs) for this task, we argue that encoder classifiers remain highly effective and significantly more lightweight. This return to efficient, purpose-built models challenges the trend of using increasingly large models for specialized tasks, offering a more practical solution for real-time applications. Our method reframes the challenge of safety moderation as a series of binary classification tasks, which can also be viewed as a multiclass classification problem, offering nuanced, probability-based assessments of risk rather than the binary, discrete outcomes typical of LLM-based judges.\nIn many scenarios, it is more practical to develop smaller, task-focused models that, while not as broadly skilled, are finely tuned to excel in specific domains. Leveraging these small and specialized models can deliver results on par with or exceeding that of much larger models, while being more resource-efficient and easier to deploy. Additionally, we outline how our methods, when combined with synthetic data generation and domain-specific fine-tuning, allow our models to navigate the trade-off between generality and efficiency for real-time applications. This approach not only addresses the current limitations in content moderation but also paves the way for more efficient and effective safety mechanisms in LLM-based systems across various domains."}, {"title": "Related Works", "content": "2.1 LLM Evaluation\nTraditional evaluation metrics such as BLEU [46] ROUGE [35], METEOR [3], SacreBLEU [49] and chrF [48], have been widely used to assess natural language generation (NLG). These metrics rely on n-gram overlap between generated text and reference outputs, focusing on surface-level similarities. However, they have been criticized for their low correlation with human judgments and their inability to fully capture the quality of text, particularly in terms of fluency, coherence, and coverage [54, 72, 67]. As a result, newer model-based metrics like BERTScore [72] and BARTScore [67] have emerged, leveraging pre-trained language models to evaluate semantic similarity and text quality. Despite these advancements, these model-based metrics still face limitations in scope and application, often requiring reference texts and lacking robustness [18, 61].\nBenchmarks such as MMLU [24] and HellaSwag [68], have become the primary method for measuring foundational model capabilities, which are typically designed for multi-choice selection tasks. While these benchmarks measure specific capabilities, they often fall short in assessing the generative abilities of LLMs, particularly in open-ended tasks [26]. LLM-based chat assistants fine-tuned with RLHF are user-preferred, yet benchmarks like MMLU often fail to capture these improvements, revealing a key gap in evaluation methods [73].\nHuman evaluation is considered one of the most reliable methods for assessing the quality of LLM outputs, as it aligns closely with real-world user preferences [37]. However, the process is both time-consuming and financially demanding, often requiring significant resources to gather high-quality human judgments [9]. Additionally, there are challenges related to standardization, diverse evaluation criteria, and data privacy concerns, which complicate the aggregation of existing human evaluations across different studies [61, 45]. While evaluating LLM outputs with human annotation is considered the gold standard - the cost and effort involved in obtaining these remains a significant barrier.\nLLM-as-Judge is an emerging paradigm involving the use of LLMs to evaluate responses of other LLMs [73, 17, 34]. It's shown promising results with comparable accuracy to humans [6, 44, 21], better stability [73] and much lower cost [17]. However, it also comes with limitations which we discuss below in Section 2.4. Despite these drawbacks, the use of LLM-as-Judge continues to grow, driven by their ability to generalize across tasks after undergoing extensive multitask instruction tuning [62, 11].\n2.2 Safety Content Moderation\nWhile moderating digital content has been a challenge for more than a decade, it's taken on greater urgency and complexity with the growing use of LLMs in widely used and scrutinised applications. Two primary approaches to improving safety behaviours in LLMs include Alignment based methods such as RLHF [59, 2],"}, {"title": "Transformer Architectures: Encoders vs Decoders", "content": "Most LLMs are built upon the decoder-only transformer architecture [60, 50], and their recent success is largely due to their ability to scale to tens or even hundreds of billions of parameters [5, 52]. This extreme scale allows them to be highly versatile, with the capability to generalize effectively to novel domains through zero-shot and few-shot learning. However, this versatility comes with significant trade-offs, including high operational costs and substantial computational demands.\nIn contrast, encoder-only models like BERT [15] and DeBERTa [23] are optimized for understanding and analyzing text, making them particularly efficient for targeted tasks such as text classification, named entity recognition, and sentiment analysis. These models typically have fewer parameters, which leads to faster inference and lower computational overhead. While encoder-only models excel in specific, narrow tasks and offer greater adaptability through efficient fine-tuning [76, 63, 26], they are less versatile compared to decoder-only models. However, for domain-specific applications, encoder-only models often provide a more sustainable and cost-effective solution, balancing performance with resource efficiency.\nThe difference in attention mechanisms is reflected in the training objectives of these models. Encoder-only models typically use Masked Language Modeling (MLM), where the objective is to predict masked tokens given their context:\n$L_{MLM} = -E_{(i,m)\\in M}[logp(x_i|x_{\\setminus m})]$\nDecoder-only models, on the other hand, are trained to predict the next token in a sequence:\n$L_{NWP} = -\\sum_{i=1}^{n-1}log p(x_{i+1}|<i)$\nEncoder models benefit from bidirectional attention, allowing them to consider the entire context of an input simultaneously, which is particularly useful for classifying completed texts. This bidirectional understanding, combined with their MLM training objective, often makes encoders preferable for many classification tasks. However, decoder models have gained prominence due to their sample efficiency, using 100% of tokens for training compared to the typical 15% masked tokens in encoder training. This efficiency, along with their versatility in reformulating tasks as text generation problems, has led to the recent trend of scaling decoder models to much larger sizes."}, {"title": "Limitations of LLM-as-Judge", "content": "We examine the limitations of LLM-as-Judge, categorising our analysis into three categories.\nCognitive, Evaluation and Social Biases: LLMs used as judges often exhibit various biases that compromise the reliability of their evaluations.\nPosition Bias: favor responses based on their order in the prompt rather than content quality [75, 73, 30].\nVerbosity/Salience Bias: prefer longer, more verbose responses, even when shorter responses might be clearer or more accurate [73, 26, 30].\nSelf-Enhancement Bias: tend to favor their own generated responses, leading to skewed evaluations [73].\nSuperficial Quality/Beauty Bias: privileging superficial features like formality or verbosity, rather than deeper content quality [26, 8].\nCompassion Fade (Naming): less empathy and different evaluation behavior when identifiable names are used [30].\nBandwagon Effect & Authority Bias: favoring statements from perceived majorities or authorities, regardless of actual evidence [30, 8].\nMisinformation Oversight Bias: overlooking factual errors in content [8].\nReversal Curse: may produce inconsistent rulings due to their inability to generalise causal statements in the reverse direction [4]."}, {"title": "Synthetic Data Generation", "content": "The construction of robust AI systems is fundamentally underpinned by high-quality data [70]. This emphasis on data quality is vital, however, acquiring such high-quality data presents significant challenges, being both costly and time-intensive. The data annotation phase is often laborious and prone to inaccuracies resulting from human involvement. Moreover, the long-tail distribution of labels of interest further complicates the data collection process, especially for content moderation applications [61].\nTo address these challenges, LLMs have emerged as a powerful tool for generating synthetic data [47]. By leveraging these models' language understanding capabilities, synthetic data can be created to meet specific human requirements [74, 16]. This approach aligns with the broader concept of data augmentation, which involves adopting innovative methods to bolster model efficacy by broadening training data diversity without necessitating further data collection efforts. The utilization of synthetic data produced by AI models becomes essential once high-quality human-generated data resources are fully exploited. Furthermore, research into the scaling laws pertinent to LLMs highlights the critical role of data as a renewable resource crucial for the enhancement and advancement of models [29].\nEarlier works experimented with GPT-3, using a small set of human-labelled examples in a few-shot manner to scale their training set for medical dialogue summarisation [10]. This can be extended to instead use the LLM to rephrase each sentence in the training samples into multiple conceptually similar but semantically different samples, enhancing few-shot learning text classification tasks [14, 66]. More recently, researchers utilised a persona-driven data synthesis methodology leveraging diverse perspectives within a large language model, facilitated by a Persona Hub containing 1 billion automatically curated personas from web data [7]. A similar concept was applied to help automate adversarial evaluation by using AI-assisted recipes to define, scope, and prioritize diversity within the application context, followed by a structured LLM-generation process to scale up evaluation priorities [51]. This synthetic data generation capability is particularly valuable in creating diverse and adversarial examples, which are crucial for improving the robustness and generalization of AI systems, especially in content moderation applications."}, {"title": "Building JurEE", "content": "3.1 Problem Setup\nOur primary objective is to classify content received (or generated) by LLM-based chatbot systems according to a predefined risk taxonomy, using the banking domain as an example. Our encoder-ensemble is designed to handle both binary and multiclass, multilabel classification tasks. The binary setting categorizes content as either \"Safe\u201d or \u201cUnsafe.\u201d The \u201cSafe\" category includes content that is typical, expected, and directly relevant to banking operations, such as inquiries about account balances, transaction details, or product information. On the other hand, the multiclass problem breaks down the \"Unsafe\" category into more"}, {"title": "Data Construction", "content": "Our dataset is constructed from three primary sources. The first pillar of the dataset is internally sourced data, collected from various internal chatbot applications within CBA. This data is annotated from Subject Matter Experts (SMEs) and covers a variety of guardrail categories. Obtaining even a small set of real-world data collected for the specific use-case is invaluable as it represents the true distribution of content we expect the model to encounter in production settings. By incorporating real-world interactions, we ensure that our model is grounded in the practical realities of its deployment environment.\nTo augment our dataset and ensure wide coverage, we incorporate externally sourced data from published datasets and papers. These sources span a broad spectrum of domains, including safety, bias, and system attack-related topics. A key aspect of our approach is the careful aggregation and mapping of these external datasets to our own risk taxonomy. For instance, in our classification setup, the \"Harmful\u201d category is expanded to include various subcategories such as toxicity, profanity, crimes, illegal substances, and weapons - each originally labeled under different classes in their respective datasets. Similarly, we create the \u201cSystem Attack\" label, which encompasses prompt injection, model jailbreaking, and other related threats. This comprehensive aggregation allows us to build larger, more representative datasets for each risk category, enhancing the model's ability to generalize across diverse scenarios. We preferred data that was human labelled or followed meticulous labelling procedures over those generated by other LLMs as it would be hard to know how representative the data would be otherwise.\nSynthetic data generation plays a pivotal role in our data construction strategy, enabling us to further diversify and expand our training dataset. We employ multiple synthetic data generation and augmentation techniques, particularly leveraging the other two data pillars as \"seed\" data to create additional synthetic samples. The details of our synthetic data pipeline are discussed below."}, {"title": "Synhetic Data Pipeline", "content": "We employ a variety of generation and augmentation techniques, grounded in recent literature, to systematically expand our initial data pool. This process revolves around leveraging high-quality internal and external data as \"seed\" data, from which we generate a multitude of synthetic examples."}, {"title": "Seed Data Initialisation:", "content": "The process begins with carefully selected seed data drawn from both internal and external real-world datasets. These seed examples serve as the foundation for the synthetic data generation, ensuring that the generated outputs are grounded in realistic scenarios."}, {"title": "Few-shot Prompting and LLM Generation:", "content": "A generation loop is implemented, wherein the LLM is exposed to new combinations of few-shot examples in each iteration. Examples are randomly sampled from the dataset along with their corresponding labels. Specific instructions are provided at various stages to tailor the generation process, focusing on aspects such as:\nCustomer type\nCultural background\nEducational and professional expertise\nGrammatical variations\nSpecificity\nEmotional tone\nOver multiple rounds of prompting, the LLM is guided to reformulate examples according to these factors, imitating different personas and situations, ensuring a wide range of diversity and quality in the synthetic data."}, {"title": "Model Diversity and Uncensored Content Generation:", "content": "A variety of models are utilized, including GPT-40 [43], GPT-3.5 [5], and leading models from Hugging Face's Uncensored General Intelligence leaderboard. For categories labeled as unsafe, where certain models exhibit excessive censorship, uncensored models from open-source platforms are employed. Sampling hyperparameters, such as temperature and repetition penalty, are adjusted to optimize the diversity and quality of the outputs."}, {"title": "Counterfactual Example Generation:", "content": "Counterfactual examples are generated by modifying similar queries to change their label, such as transforming a benign banking-related query into a system attack scenario. This approach enhances the model's ability to distinguish between closely related but differently labeled examples."}, {"title": "Traditional Data Augmentation:", "content": "Conventional data augmentation techniques, including random deletion, insertion, synonym swaps, and backtranslation, are applied to both real seed data and synthetic data. However, these techniques yield mixed results, necessitating manual inspection to ensure optimal quality."}, {"title": "Postprocessing and Synthetic Data Filtering:", "content": "A comprehensive postprocessing and filtering approach is implemented, involving:\n(a) Model-Based Filtration: A round-trip validation method is employed. Generated examples are fed back into a LLM to predict their labels. Examples where the model's prediction matches the original synthetic label are retained as high-quality, consistent data points.\n(b) Distance-Based Metrics: Embeddings are computed for the synthetic data, and Euclidean distances and cosine similarities to the seed data are calculated. This analysis identifies synthetic examples that closely resemble the seed data and detects potential overlaps between different label classes. Outliers exhibiting significant distance from the seed data are investigated or discarded.\n(c) Clustering Techniques: Methods such as t-SNE [58] and UMAP [42] are used to visualize the distribution of different labels in the latent space. This visualization provides insights into how the synthetic examples are grouped and helps identify inconsistencies or misclassifications.\n(d) Manual Annotation: Based on the visualizations, manual annotation is conducted to either confirm existing labels or assign new, more accurate labels where necessary."}, {"title": "Active Learning:", "content": "Using verified seed data, we train the ensemble and score new synthetic examples. We utilise label smoothing and weight decay to identify uncertain predictions. These uncertain examples undergo manual review and label reassessment before being incorporated into the training set."}, {"title": "Continuous Refinement:", "content": "The process of generating synthetic data, applying rigorous postprocessing and filtering, and refining the dataset is repeated multiple times. The goal is to achieve a final dataset where almost every synthetic example has undergone human scrutiny, ensuring that the labels are correct and that the data is of the highest quality for training a robust classifier."}, {"title": "Model & Training Details", "content": "We initialized our models using DeBERTa-v3-base, which has approximately 184 million parameters. DeBERTa was chosen for its size, which allows for efficient performance, low memory usage, and fast inference-critical"}, {"title": "Results", "content": "4.1 Evaluation Data & Baseline Models\nFor evaluation purposes, we set aside 2175 examples covering all 6 label classes. This test set is also stratified so label proportions are similar between the training and test sets, mirroring what we expect to see in production. The choice for number of examples in the test set is also constrained by the fact we need to evaluate our baseline models using their APIs or respective model files which can become expensive with more examples.\nWe report our results in the multiclass classification setting. Further, we report our prompt setup for both settings for the LLM Judges in the appendix for review.\nWe compare JurEE to multiple LLM-Judge models in the domain, including finetuned judges such as ShieldGemma [69] and LlamaGuard [27]. Additionally, we use OpenAI's GPT3.5 and GPT40 in both single and multi-judge settings.\nSingle Judge Baseline: In this baseline, GPT3.5 and GPT40 are employed as a single judge, where a single API call is made with the full risk taxonomy of all label classes provided in the prompt. The input text is included in the prompt, and the model is prompted classify the text into the appropriate label class. This approach leverages the model's ability to process all possible labels in a single pass, minimizing latency but potentially diluting the model's focus on any specific class.\nMulti-Judge Baseline: For this baseline, we used GPT3.5 with separate API calls for each risk taxonomy class label. Each API call is specifically targeted to predict a single label class, and this process is conducted sequentially during our experiments. While this method allows for more targeted predictions, it significantly increases latency. Although this could be optimized by running the API calls in parallel, doing so would still involve increased costs, as making six parallel API calls would be more expensive than a single larger call.\nBinary Classification Evaluation: JurEE can be used in a binary classification setting, where inputs are categorized as either in-scope (banking-related) or out-of-scope (harmful, system attack, off-topic, vulnerable,\ncomplaint). This approach allows us to assess the effectiveness of our method in distinguishing relevant content from a broad spectrum of potentially unsafe or irrelevant inputs. For each input i, we calculate the probability of it being in-scope, $\u0177_{in-scope,i}$, as the probability of the input being banking-related:\n$\u0177_{in-scope,i} = \u0177_{banking-related,i}$\nConversely, the probability of the input being out-of-scope, $Y_{out-of-scope,i}$, is computed as the maximum probability across the five out-of-scope categories:\n$Y_{out-of-scope,i} = max (\u0177_{harmful, \u0177system attack, Y_{off-topic, i, \u00dcvulnerable,i \u00dd complaint,i})$\nMulticlass Classification Evaluation: In addition to the binary setting, we evaluate our method using a multiclass classification framework, where the model is tasked with assigning one of six possible label classes: banking-related, harmful, system attack, off-topic, vulnerable, and complaint. This more granular approach allows us to assess the model's capability to accurately classify inputs across a diverse risk taxonomy. For each input i, the model output probabilities for each class:\n$\u0177_{c,i}$, for $c\u2208$ {banking-related, harmful, system attack, off-topic, vulnerable, complaint}\nThe final predicted class $c^*$ for input i is the one with the highest probability:\n$C^* = arg max_{C} ax \u0177_{c,i}$\n4.2 Main Results\nThe performance of JurEE is evaluated against several baseline models, as summarized in Table 4. The results indicate that JurEE consistently outperforms the baseline models across all evaluated metrics. The GPT3.5 single judge baseline achieves an F1 score of 0.61, with accuracy, recall, and precision scores all around 0.62-0.65. The GPT40 single judge shows a slight improvement, achieving an F1 score of 0.64, indicating a modest enhancement in performance. When using the GPT3.5 multi-judge setup, where separate API calls were made for each risk category, the F1 score increases to 0.64. This suggests that while the targeted, multi-judge approach can provide some performance gains, improvements remain limited by the underlying model's capabilities.\nIn comparison, the JurEE method achieves an F1 score of 0.92, with accuracy, recall, and precision scores of 0.91, 0.92, and 0.91, respectively. These results demonstrate that JurEE significantly improves classification performance over the baselines. The higher recall and precision scores reflect JurEE's enhanced ability to accurately identify positive cases while minimizing false positives, which is particularly important in multi-class classification tasks. The focus on precision and recall in this setting is particularly important: precision allows us to measure how accurately the model identifies relevant content, minimizing the risk of"}, {"title": "Experiments and Ablation Studies", "content": "4.3.1 Training Data Size Regimes\nOur first ablation studies the impact of varying training data sizes on model performance using a DeBERTa base model, trained over 3 epochs. We evaluate four data regimes: 500, 1,000, 10,000, and the full 42,000 examples. The results indicate that performance consistently improves with increased data, with the most significant gains observed between the 1,000 and 10,000 example ranges. Figure 5 highlights the critical role of sufficient data volume in achieving optimal model performance.\n4.3.2 Latency\nTo illustrate the speed of JurEE vs larger LLM judges, we show the computation time for classifying our custom test set. GPT models are used via an API and hence will be fairly consistent across environments, with the caveat that we are implementing the multi-judge setting sequentially but it is possible to do so in parallel. We perform test inference of JurEE using 2 Nvidia V100s with a batch size of 128, but this speed advantage remains large even with less powerful consumer grade GPUs.\n4.3.3 Model Architecture\nIn another comparative study, we evaluate the performance of JurEE when fine-tuned on different variants of DeBERTa pretrained models. This analysis aims to determine how varying model architectures and sizes influence the effectiveness of our method. Generally, we observe the larger models perform better on the test set than smaller models, which could be attributed to the larger model's enhanced capacity to capture complex linguistic patterns and finer-grained details in the data, which smaller models might miss due to their reduced parameter count. Interestingly, the full ensemble method does not outperform the baseline JurEE model (shared embedding layer with different transformer/output heads) which could be due to the fact a single model is sufficient to classify the diversity of our dataset."}, {"title": "Zero vs Few-shot for Baseline", "content": "We examined the impact of zero-shot versus few-shot prompting on the performance of baseline LLM judges. By providing two examples per label category, we observed a moderate improvement in the baselines' performance compared to their zero-shot settings. However, even with this enhancement, the baselines still lagged behind our JurEE method."}, {"title": "Limitations", "content": "Building JurEE with our comprehensive synthetic data pipeline introduces specific challenges. When using LLMs for synthetic data generation, each model's reliance on distinct prompt templates and output formats necessitates robust parsing mechanisms. The sensitivity of prompts to exemplar choice, order, and diversity further complicates this process. Additionally, the combinatorial nature of semantic and syntactic variations in open-ended inputs makes it difficult to achieve exhaustive coverage in synthetic data generation.\nModel selection and weighting within the ensemble add another layer of complexity, as the effectiveness of the approach depends on how well the models complement each other. Data requirements for training are also significant, particularly since each model needs tailored data that aligns with specific categories in the risk taxonomy, which can vary between use cases. Scalability can be an issue, as managing multiple models increases computational demands, however, the use of multiple small encoder models is still significantly faster than larger LLM judges.\nHuman expertise remains crucial, particularly in advanced prompt engineering and long-tail adversarial testing, where nuanced judgment is essential. The approach also faces limitations in addressing emerging attack patterns due to the inherent ambiguity in defining adversarial prompts. Computational demands during synthetic data generation are considerable, and the reliance on a predefined taxonomy limits adaptability to new domains, especially in streaming and multiturn response scenarios. We leave it for future work to engage SMEs for different categories of guardrails such as Privacy, Compliance and Vulnerability so that we can incorporate their collective knowledge into the training data of the model, especially for defining the finer details of delineating between acceptable and unacceptable inputs.\nAdditional challenges stem from synthetic data augmentation and the use of LLMs. These models may misinterpret complex relationships and struggle with multi-subject combinations, leading to potential inaccuracies. The inherent ambiguity in natural language prompts often hinders precise prompt engineering. Furthermore, generating outputs that significantly deviate from the norm remains difficult, particularly for rare or highly fictional subjects. The absence of a universal data augmentation method applicable across diverse tasks and the non-linear impact of data quantity on performance further complicate the landscape. Current task-specific evaluation metrics may not adequately capture the diversity and consistency of augmented data, highlighting the need for more comprehensive assessment methods."}, {"title": "Conclusion & Discussion", "content": "We introduced JurEE, a lightweight encoder-ensemble to enhance the moderation of content within LLM-based systems, with a focus on customer-facing applications. JurEE serves as an auxiliary guardrails component for LLM systems, using textual inputs in the form of user prompts (or responses) and produces probabilistic scores between 0 and 1 indicating if a certain aspect of the risk taxonomy is present.\nOur results demonstrate a performance boost of upto 40% against current state-of-the-art LLMs in this task achieving 92% F1 on our custom test set, as opposed to 55-65% of current LLM judges. We show that our superior performance is achieved through our novel approach to synthetic data augmentation and rigorous data filtering, guided by seed data from real-world internal and external sources.\nOur findings demonstrate that smaller, specialized encoder models can effectively serve as auxiliary guardrails within LLM systems, often surpassing larger models in content generation tasks. This shift towards more efficient models enhances the reliability and scalability of AI systems, particularly in resource-constrained environments where model interpretability is key."}, {"title": "Appendix", "content": "7.1 Comparison of Risk Taxonomy vs others\n7.2 Examples of Generation Prompts\n7.2.1 Instruction aspects covered to simulate diversity\nWe cover 12 instructional aspects to introduce additional diversity and improve the quality/depth of our synthetic examples. These aspects are covered in the table below, and can also be used in combinations of each other to create further distinct examples."}, {"title": "Seed Data Initialisation::", "content": "The process begins with carefully selected seed data drawn from both internal and external real-world datasets. These seed examples serve as the foundation for the synthetic data generation, ensuring that the generated outputs are grounded in realistic scenarios."}, {"title": "Few-shot Prompting and LLM Generation::", "content": "A generation loop is implemented, wherein the LLM is exposed to new combinations of few-shot examples in each iteration. Examples are randomly sampled from the dataset along with their corresponding labels. Specific instructions are provided at various stages to tailor the generation process, focusing on aspects such as:\nCustomer type\nCultural background\nEducational and professional expertise\nGrammatical variations\nSpecificity\nEmotional tone\nOver multiple rounds of prompting, the LLM is guided to reformulate examples according to these factors, imitating different personas and situations, ensuring a wide range of diversity and quality in the synthetic data."}, {"title": "Model Diversity and Uncensored Content Generation::", "content": "A variety of models are utilized, including GPT-40 [43], GPT-3.5 [5], and leading models from Hugging Face's Uncensored General Intelligence leaderboard. For categories labeled as unsafe, where certain models exhibit excessive censorship, uncensored models from open-source platforms are employed. Sampling hyperparameters, such as temperature and repetition penalty, are adjusted to optimize the diversity and quality of the outputs."}, {"title": "Counterfactual Example Generation::", "content": "Counterfactual examples are generated by modifying similar queries to change their label, such as transforming a benign banking-related query into a system attack scenario. This approach enhances the model's ability to distinguish between closely related but differently labeled examples."}, {"title": "Traditional Data Augmentation::", "content": "Conventional data augmentation techniques, including random deletion, insertion, synonym swaps, and backtranslation, are applied to both real seed data and synthetic data. However, these techniques yield mixed results, necessitating manual inspection to ensure optimal quality."}, {"title": "Postprocessing and Synthetic Data Filtering::", "content": "A comprehensive postprocessing and filtering approach is implemented, involving:\n(a) Model-Based Filtration: A round-trip validation method is employed. Generated examples are fed back into a LLM to predict their labels. Examples where the model's prediction matches the original synthetic label are retained as high-quality, consistent data points.\n(b) Distance-Based Metrics: Embeddings are computed for the synthetic data, and Euclidean distances and cosine similarities to the seed data are calculated. This analysis identifies synthetic examples that closely resemble the seed data and detects potential overlaps between different label classes. Outliers exhibiting significant distance from the seed data are investigated or discarded.\n(c) Clustering Techniques: Methods such as t-SNE [58] and UMAP [42] are used to visualize the distribution of different labels in the latent space. This visualization provides insights into how the synthetic examples are grouped and helps identify inconsistencies or misclassifications.\n(d) Manual Annotation: Based on the visualizations, manual annotation is conducted to either confirm existing labels or assign new, more accurate labels where necessary."}, {"title": "Active Learning::", "content": "Using verified seed data, we train the ensemble and score new synthetic examples. We utilise label smoothing and weight decay to identify uncertain predictions. These uncertain examples undergo manual review and label reassessment before being incorporated into the training set."}, {"title": "Continuous Refinement::", "content": "The process of generating synthetic data, applying rigorous postprocessing and filtering, and refining the dataset is repeated multiple times. The goal is to achieve a final dataset where almost every synthetic example has undergone human scrutiny, ensuring that the labels are correct and that the data is of the highest quality for training a robust classifier."}]}