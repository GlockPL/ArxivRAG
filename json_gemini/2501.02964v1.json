{"title": "Socratic Questioning: Learn to Self-guide\nMultimodal Reasoning in the Wild", "authors": ["Wanpeng Hu", "Haodi Liu", "Lin Chen", "Feng Zhou", "Changming Xiao", "Qi Yang", "Changshui Zhang"], "abstract": "Complex visual reasoning remains a key challenge today. Typically, the challenge is\ntackled using methodologies such as Chain of Thought (COT) and visual instruction\ntuning. However, how to organically combine these two methodologies for greater\nsuccess remains unexplored. Also, issues like hallucinations and high training cost\nstill need to be addressed. In this work, we devise an innovative multi-round training\nand reasoning framework suitable for lightweight Multimodal Large Language\nModels (MLLMs). Our self-questioning approach heuristically guides MLLMS\nto focus on visual clues relevant to the target problem, reducing hallucinations\nand enhancing the model's ability to describe fine-grained image details. This\nultimately enables the model to perform well in complex visual reasoning and\nquestion-answering tasks. We have named this framework Socratic Questioning\n(SQ). To facilitate future research, we create a multimodal mini-dataset named\nCapQA, which includes 1k images of fine-grained activities, for visual instruction\ntuning and evaluation, our proposed SQ method leads to a 31.2% improvement\nin the hallucination score. Our extensive experiments on various benchmarks\ndemonstrate SQ's remarkable capabilities in heuristic self-questioning, zero-shot\nvisual reasoning and hallucination mitigation. Our model and code will be publicly\navailable.", "sections": [{"title": "Introduction", "content": "Effective visual reasoning and question answering in complex scenarios are highly valuable, as they\nprovide accurate and in-depth insights that can be crucial in practical applications. Currently, visual\nreasoning and question answering in complex scenes remain a significant challenge. Researchers\nare actively developing models, making training and fine-tuning datasets, and creating evaluation\nbenchmarks to improve performance in this area.\nChain of Thought (COT) and visual instruction tuning are the common methods used to tackle\ncomplicated visual reasoning and question answering tasks. Both methods have developed over\ntime to become effective and mature, but how to organically combine them for complementary\nadvantages remains an area worth exploring. At the same time, both methods face challenges such as\nhallucinations and high training costs.\nSocratic Questioning (SQ): In this paper, we propose an innovative multi-round training and\nreasoning framework compatible with lightweight Multimodal Large Language Models (MLLMs).\nOur method is named Socratic Questioning (SQ): Facing a main problem, SQ uses heuristic,"}, {"title": "Related Work", "content": "2.1 Multimodal Chain of Thought\nMM-CoT[39] first proposed a two-stage reasoning framework where an LLM initially processes\nimage-text data to obtain a rationale, and then the rationale is fed into the LLM to obtain the final\nanswer. Some subsequent works concentrate on the better alignment and fusion of language and\nvision modalities. DPMM-CoT[14] leverages the idea and architecture of T2I stable diffusion\nmodel to flexibly adjust visual feature extraction according to problem prompts. Additionally, some\nresearch focuses on using graph data to encode people, objects, and their mutual relationships. This\napproach aims to capture more fine-grained information from images, thereby enhancing the benefits\nof multimodal CoT from the visual modality and reducing hallucinations. KAM-CoT[28] harnesses\ngraph neural networks to process and encode the Knowledge Graph produced from each image,\nwhile CCoT[5] delicately prompts LLM to generate a scene graph in Json dictionary format from"}, {"title": "Hallucination mitigation", "content": "The work done by Zechen Bai et al[3] is an excellent survey offering a comprehensive, in-depth,\nand systematic introduction and analysis of the causes, evaluation metrics, and current solutions for\nhallucinations in Multimodal Large Language Models (MLLMs). According to[3], hallucinations\ncan originate from data, model, training process and inference process. Our insights highlight a\nparticularly important cause of hallucinations: MLLMs tend to spontaneously ignore visual features.\nCurrent MLLM architectures are highly imbalanced as the language model (LLM), with strong\npriors embedded during massive pretraining, weighs much more than the visual module, suffering\nsignificant information loss while extracting visual features. Also when an MLLM generates tokens\nsequentially in an autoregressive manner during inference, it increasingly focuses on the tokens\nthat have already been generated as the output gets longer and longer, gradually ignoring the input\nprompt, especially the visual information.\nResearchers have come up with many solutions for the issue of \"Visual Ignorance\". LLaVA-1.5[25],\nQwen-vl[2], Internvl[8] and HallE-Switch[38] have shown that increasing the number of parameters\nin the vision encoder and improving image resolution can effectively reduce hallucinations. The\nworks in[15], [17],[33] and [18] enhance the representation of the visual component by integrating\nvisual features extracted from various vision encoders, utilizing visual perception tools such as OCR\ntools and object detectors, and incorporating perceptual information like depth maps and segmentation\nmasks, allowing the visual part to play a bigger role. To enable MLLM training to benefit from\nfeedback in the same way that LLM training does, Silkie[22], HA-DPO[41], LLaVA-RLHF[31]\nand RLHF-V[37] leverage feedback from both AI systems(RLAIF) and humans(RLHF) to train a\nreward model that can identify hallucinations and prefer low-hallucination responses. With regard to\ninference stage, MARINE[40], GCD[11] and HALC[7] adhere to the concept of \"guided decoding,\"\nutilizing grounded visual objects, grounded visual tokens, and even scores that can accurately measure\nthe degree of hallucination to guide the decoding process of MLLMs. These approaches ensure that\nthe generated language is as visually grounded as possible."}, {"title": "Method", "content": "3.1 Architecture\nSQ architecture The network archtecture of SQ is illustrated in Figure 2. In order to reduce memory\nusage, we make the LLM act as a Question Generator, Question Answer and Visual Summarizer\nsimultaneously. As a Question generator, the LLM generates a list of questions seeking valuable\ninformation to help itself correctly interpret the ongoing activity within the given image. As a\nQuestion Answerer, the LLM answers these questions one by one (essentially performing VQA tasks)\nto produce the rationale consisting of the Q&A pairs. As a Visual Summarizer, the LLM provides\nfinal detailed descriptions and summarized captions based on the information encoded in the previous\nrationale. Note that the dashed LLM module named Socratic Questioning on the left denotes the\nroles of Question Generator and Question Answerer, while the undashed LLM module on the right"}, {"title": "Generation of the CapQA Dataset", "content": "4.1 Data Collection\nWe collect data from the Consented Activities of People (CAP) [4] dataset, which comprises video\nclips of daily activities performed by consenting individuals around the world. The CAP dataset\ncontains 1,454,540 clips, categorized into 512 classes of fine-grained activities with labels (like\n\"person opens car door\") encoding subjects' actions and the objects they are interacting with. The\nactivities are fine-grained because they differ in the subtle details of actions and interacting objects,\nalthough they may appear similar overall. We select 20 activities and randomly extract 50 clips from\neach activity. From each clip, we chose one key frame that clearly demonstrates the ongoing activity\nto serve as our final image data with activity label.\n4.2 Designing prompts to automatically generate annotation\nTo further annotate the image data obtained in 2.1, we utilize GPT-4v [1] to automatically generate\nthe annotations including a list of questions, corresponding answers, a detailed description and a\nsummarized caption. Our meticulously designed prompt for annotations acquisitions is shown at 1:\n\u2022 Questions & Answers. To address the ongoing activity depicted in an image, we prompt the\nGPT-4V model to generate relevant questions and provide corresponding answers. We guide\nthe GPT-4v model to refine its questions and validate that its answers are visually well-founded\nso that each QA pair is specific, accurate, and meaningful. We also provide the ground truth\nactivity label (excluded in the final produced annotation) such as \u201cperson opens car door\u201d for\nbetter alignment of annotations to reality. Please note that the ground truth label will only be\nused in data generation.\n\u2022 Detailed Description. Based on the information implied in the sequence of produced Q&A\npairs, the GPT-4v model provides a detailed description that includes the person's appearance\nand actions, the surrounding environment, the attributes and condition of the interacted objects,\nas well as insights into the person's intentions and potential changes in the situation.\n\u2022 Summarized Caption. Although greatly informative, the detailed description contains lots of\nredundant information and even some hallucinations, which increases the risk of misleading\nusers. Therefore, we also prompt the GPT-4v model to condense the detailed description into\na summarized caption, a concise expression retaining the core content most relevant to the\nactivity's theme."}, {"title": "Data Label Format", "content": "To facilitate future fine-tuning, we have decided to organize the acquired annotations in a multi-round\nconversation format, similar to that used in LLaVA [26]. The first round of conversation generates\nthe list of questions and the subsequent rounds consist of Q&A pairs where the questions are taken\nfrom the list in order and answers are given by GPT-4v accordingly. Finally, the last two rounds\nof conversation elicit the detailed description and summarized caption respectively. An example of\nannotation formatted into multi-round conversation is shown at Table 8 in the appendix.\nWe extract key frames from selected activity clips of the CAP dataset, leverage GPT-4v to automati-\ncally annotating image data and finally organize the annotations into a structured conversation format.\nThis approach enhances the granularity, accuracy, depth, and comprehensiveness of our annotations,\nwhile streamlining the annotation process and optimizing data utility for further analysis."}, {"title": "Training", "content": "4.4.1 Training data format\nIn Section 4.3, we depict the label format used for our CapQA dataset 8. Assuming a multi-turn\nconversation {X1, X2,..., XT\u22121, XT} consists of T turns, we denote the human's question in\nthe j-th turn of conversation as Xqj and system(like GPT)'s answer in the j-th turn of conversation as\nXaj .\nQuestions Generation. The first turn [X1, X21] is specifically designed to train the LLM to function\nas a Question Generator. X1q denotes a carefully crafted prompt requesting the questions generation,\nwhile X1a denotes the list of questions generated upon X1q . Again, the questions would guide the\nLLM to capture fine-grained details of human activity so as to correctly interpret the image.\nAnswers Generation.\n(Xq2, Xa2, ..., XqT\u22122, XaT\u22122) are the individual questions contained in the\nlist X1a , while (X2a, X3a, ..., XTa\u22123, XT\u2212a2) are their corresponding answers. Thus, the turns"}, {"title": "Training Procedure", "content": "We train our model using a classical two-stage process: first pretraining, followed by instruction\ntuning.\nStage 1: Pretrain We utilize the LLaVA-CC3M-Pretrain-595K dataset [26], comprised of 595K\nimage-text pairs filtered from CC3M, to pretrain the adapter of SQ. The purpose of this stage is to\nachieve a good alignment between the visual feature space and the token embedding space of LLM.\nThe parameters of both the image encoder and the LLM are frozen throughout the pretraining phase.\nStage 2. Instruction-Tune We fine-tune our SQ model using 666K image-text pairs. It contains\nllava_v1_5_mix665k [25] and CapQA_0.9k dataset introduced elaborately in Sections 4.3 and 4.4.1.\nWe processed the questions from the Conv58k dataset [25], included in the llava_v1_5_mix665k [25],\nthese questions were reorganized to conform to the data format described in Section 4.3. To prevent\noverfitting, during training, we randomly insert the generated question list at any round of the\nconversation. During this phase, the image encoder remains frozen, while the adapters and LLM\n(using LORA [16]) are fine-tuned. We perform instruction-tuning of the LLM on the prediction\ntokens, using the same auto-regressive training objective as LLaVa [26]:\n$\\L p(Xa | Xv, Xq) = \\prod_{i=1}^{L}p_{\\theta}(x_i | Xv, Xq, <i, Xa, <i),$\nWhere L is the token sequence length, Xv stands for the visual input (visual tokens), Xq and Xa\nstand for the tokens of human instructions and system answers, respectively, across all T rounds of\nconversation. Xq,<i and Xa,<i are respectively the human instructions tokens and system answers\ntokens in all turns before the currently predicted token xi ."}, {"title": "Inference", "content": "The inference process is illustrated in the right side of Figure 3. We can choose to employ 1-turn\nor 3-turn inference. Simply, 1-turn inference directly produces the final caption based on the given\nimage, problem statement and context(Inputs). As the right side of Figure 3 shows, 3-turn inference\nfirst prompt the LLM to generate a list of questions based on the Inputs, then make the LLM provide\nvisually grounded answers of these questions and finally let the LLM generate the detailed description\nand summarized caption, where the more concise caption is treated as the final output. Experiments\nshow that 1-turn inference is better suited for straightforward problems while 3-turn inference works\nbetter for complicated problems requiring multi-step reasoning and fine-grained details."}, {"title": "Experiments", "content": "5.1 CapQA\nCapQA, proposed in this paper, is a novel mini\ndataset consisting of 982 images, each asso\nciated with a multi-turn conversations. The\ndataset is divided into a training set and a test\nset, with the training set containing 882 samples\nand 11.9k QA pairs, and the test set containing\n100 samples and 1.4k QA pairs.\nWe designed two evaluation metrics: (A). Hallu\ncination, measuring the degree of hallucination\nin detailed descriptions, with higher scores indi\ncating less hallucination. (B). Questions Quality,\nreflecting model's ability to generate questions,\nwith higher scores reflecting better quality, diversity, and effectiveness. The calculation method for\nthe score can be expressed as follows:\nHalS =$\\frac{HalS_{pred}}{HalS_{gt}}$ *$\\frac{QQS_{pred}}{QQS_{gt}}$ (2)\nIn Eq 2, HalSpred represents the average score of all model predictions reviewed by GPT-4 [29], while\nHalSgt denotes the average score of all labels reviewed by GPT-4 [29]. Similarly, QQSpred is the\naverage score of all model predictions reviewed by GPT-4 [29], and QQSgt is the average score of all\nlabels reviewed by GPT-4 [29]. The prompt used to instruct GPT-4 for scoring is shown in Table 6.\nAs shown in Table 2, our proposed SQ framework leads to a 31.2% improvement in the hallucination\nscore and a significant increase in the question quality score from 31.5 to 92.3. Additionally,\nemploying a 3-turn inference mode, which includes question-answer-caption 3 steps during the\ninference phase, further reduces hallucination by 2.3%. Without increasing computational cost, our\nSQ method effectively reduces the model's hallucination while generating detailed descriptions.\n5.2 POPE\nPOPE [24] is focused on assessing the hallucinations in MLLMs by testing if the MLLMs can\ncorrectly tell the existence of objects in images. It employs different sampling methods to construct\nnegative samples, including random, popular, and adversarial sampling. In the random sampling\nsetting, objects that are not present in the image are chosen randomly. For the popular setting, the\nabsent objects are selected from a pool of the most frequently occurring objects. In the adversarial\nsetting, objects that commonly co-occur but are not present in the image are used as negative\nsamples. We achieved better performance than Woodpecker [36] on the POPE benchmark and\nattained state-of-the-art (SOTA) F1 scores across three different modes.\n5.3 Comparative Experiment\nThe experimental results presented in Table 12 demonstrate the superiority of our proposed method\ncompared to several state-of-the-art (SoTA) methods across six benchmarks. Our method uti\nlizes the Vicuna-7B [9] large language model with 336 resolution, 558K pre-train data, and\n666K(llava_v1_5_mix665k [25] + CapQA_0.9k) fine-tune data. It achieves a Hallucination Rate\n(HalR) of 0.57 and an MMHal Average Score (AvgS) of 2.16, outperforming other methods in\nthese metrics. Notably, our method also excels in the LLaVA-QA90 [26] benchmark with a score\nof 81.3, and shows competitive performance in the LLaVA-Bench (In-the-Wild) [26], \u039c\u039c\u0395 [13],\nScienceQA-IMG [27], and TextVQA [30] benchmarks with scores of 66.8, 1523.4, 68.37, and 58.57,"}, {"title": "Conclusion", "content": "In this work, we introduce the Socratic Questioning (SQ), an flexible, reliable and effective frame\nwork for visual reasoning and question answering that fits well to lightweight Multimodal Large\nLanguage Models (MLLMs). SQ combines Chain of Thought (CoT) reasoning and visual instruction\ntuning through heuristic self-questioning, effectively reducing hallucinations and training costs while\nimproving fine-grained visual detail description and zero-shot reasoning. Our experiments, including\nthose with the new CapQA dataset, demonstrate SQ's effectiveness in reducing hallucinations and\nimproving visual description quality. By efficiently utilizing lightweight MLLMS, SQ provides a\ncost-effective, high-performance solution for complex visual tasks, paving the way for future research\nin multimodal reasoning.\nDiscussion. This work is merely an exploration of heuristic self-questioning, and there are areas\nthat require further improvement. For example, designing a reasonable loss function to constrain the\nmodel to ask more effective questions that benefit the overall task, and enhancing fine-grained visual"}, {"title": "Training Parameters Detail", "content": "A\nPre-training We directly use the pretrained weights of LLaVA-1.5 [25]. You can download from:\nhttps://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md\nInstruct Fine-tuning We conduct instruction fine-tuning training of our model on four NVIDIA\nA800-SXM4-80GB GPUs, which takes approximately 28 hours. The hyperparameters are shown in"}, {"title": "Prompt", "content": "B"}, {"title": "Datasets", "content": "C\nC.1 MMHal\nMMHal is comprised of 96 delicately designed image-question pairs, ranging in 8 question categories\n\u00d7 12 object topics. MMHal concentrates on detecting hallucinations within the LMM responses and\nadopts general, realistic, and open-ended questions to better reflect the response quality in real-world\nuser-LMM interactions. The images are from the validation and test sets of OpenImage to avoid\ndata leakage. The questions, asking LLM to figure out the object attributes, spatial relations, make\ncounting, provide holistic description and etc, are created in an adversarial manner to make LLM\nhallucinates on purpose. As a result, MMHal offers a great assessment on LLM's capability to\nrobustly resist various kinds of hallucinations."}, {"title": "MME", "content": "C.2\nMME, introduced in [13], is a comprehensive MLLM Evaluation benchmark consisting of 1k - 2k\nimages and instruction-answer pairs. MME has four main characters:\n1. MME offers a comprehesive assessment for different aspects of a MLLM's ability including\nperception (coarse-grained and fine-grained object recognition and OCR) and cognition (common\nsense reasoning, numerical calculation, text translation, and code reasoning), up to totally 14 subtasks.\n2. All instruction-answer pairs are manually constructed and great proportion of images are newly\ncollected in order to avoid data leakage.\n3. The instructions are made concise so as to be similar to commonly used ones. The unfair\nadvantage of prompt engineering is avoided.\n4. The answers are simple \"yes\" or \"no\", which is accurate, objective and convenient for quantitative\nanalysis.\nHence, MME is an accurate, objective, fair and comprehensive benchmark for MLLM's visual\nperception and cognition capabilities."}, {"title": "TextVQA", "content": "C.3\nTextVQA dataset, introduced in [30], contains 28408 images on which 45336 questions are asked by\nhuman annotators. The images, selected from the Open Images dataset, belong to the categories that\ntend to contain text e.g. \"billboard\u201d, \u201ctraffic sign\u201d, \u201cwhiteboard\". The questions require reading and\nreasoning about text in the image. Date are organized in the format of question-image pairs where\neach has 10 ground truth answers provided by humans. This benchmark evaluate model's reasoning\nability specialized in Optical Character Recognition (OCR). Our SQ achieve state-of-art performance\nwithout a specialized OCR module."}, {"title": "LLaVA-Bench(In-the-Wild)", "content": "C.4\nLLaVA-Bench(In-the-Wild) is introduced in the work of LLaVa[26] created to evaluate models ability\nto handle challenging tasks and generalize to new domain. It has totally 24 images, each comes with\na manually annotated detailed description, of indoor and outdoor scenes, memes, paintings, sketches,\netc. Authors also provide a list of 60 questions, from which individual questions are properly selected\nto be associated with each image. In this way, LLaVA-Bench(In-the-Wild) works well as a benchmark\nfor visual captioning and question answering that require strong spatial awareness and background\nknowledge."}, {"title": "LLaVA-QA90", "content": "C.5\nLLaVA-QA90 is also introduced in the paper of LLaVa[26]. Authors select 90 images from COCO\nVal-2014 and leverage the data generation pipeline introduced in the paper to annotate them. As a\nresult, each image is associated with a detailed description, a multi-round conversation and a complex\nreasoning Q&A pair. Thus, LLaVA-QA90 serves as a benchmark for evaluating model's capability to\nconduct a long conversation, make a detailed description and solve a complex reasoning problem\nbased on an image."}, {"title": "Examples", "content": "D\nThe conversation in Table 7 is based on Figure 5."}]}