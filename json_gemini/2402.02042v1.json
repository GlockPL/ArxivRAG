{"title": "Learning General Parameterized Policies for Infinite Horizon\nAverage Reward Constrained MDPs via Primal-Dual Policy\nGradient Algorithm", "authors": ["Qinbo Bai", "Washim Uddin Mondal", "Vaneet Aggarwal"], "abstract": "This paper explores the realm of infinite horizon average reward Constrained Markov Decision Pro-\ncesses (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint\nviolation analysis of average reward CMDPs with a general policy parametrization. To address this chal-\nlenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints\nwhile ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demon-\nstrate that our proposed algorithm achieves  \u00d5(T^{3/4})  objective regret and  \u00d5(T^{3/4})  constraint violation\nbounds.", "sections": [{"title": "1 Introduction", "content": "The framework of Reinforcement Learning (RL) is concerned with a class of problems where an agent\nlearns to yield the maximum cumulative reward in an unknown environment via repeated interaction. RL\nfinds applications in diverse areas, such as wireless communication, transportation, and epidemic control\n(Yang et al., 2020; Al-Abbasi et al., 2019; Ling et al., 2023). RL problems are mainly categorized into three se-\ntups: episodic, infinite horizon discounted reward, and infinite horizon average reward. Among them, the\ninfinite horizon average reward setup is particularly significant for real-world applications. It aligns with\nmost of the practical scenarios and captures their long-term goals. Some applications in real life require\nthe learning procedure to respect the boundaries of certain constraints. In an epidemic control setup, for\nexample, vaccination policies must take the supply shortage (budget constraint) into account. Such restric-\ntive decision-making routines are described by constrained Markov Decision Processes (CMDP) (Bai et al.,\n2023b; Agarwal et al., 2022a; Chen et al., 2022). Existing papers on the CMDP utilize either a tabular or a\nlinear MDP structure. This work provides the first algorithm for an infinite horizon average reward CMDP\nwith general parametrization and proves its sub-linear regret and constraint violation bounds.\nThere are two primary ways to solve a CMDP problem in the infinite horizon average reward set-\nting. The first one, known as the model-based approach, involves constructing estimates of the transition\nprobabilities of the underlying CMDP which are subsequently utilized to derive policies (Chen et al., 2022;\nAgarwal et al., 2022b,a). The caveat of this approach is the large memory requirement to store the estimated\nparameters which effectively curtails its applicability to CMDPs with large state spaces. The alternative\nstrategy, known as the model-free approach, either directly estimates the policy function or maintains an\nestimate of the Q function, which is subsequently used for policy generation (Wei et al., 2022). Model-free\nalgorithms typically demand lower memory and computational resources than their model-based counter-\nparts. Although the CMDP has been solved in a model-free manner in the tabular (Wei et al., 2022) and\nlinear (Ghosh et al., 2023) setups, its exploration with the general parameterization is still open and is the\ngoal of this paper.\nGeneral parameterization indexes the policies by finite-dimensional parameters (e.g., via neural net-\nworks) to accommodate large state spaces. The learning is manifested by updating these parameters using\npolicy gradient (PG)-type algorithms. Note that, PG algorithms are primarily studied in the discounted\nreward setup. For example, (Agarwal et al., 2021) characterizes the sample complexities of the PG and the\nNatural PG (NPG) algorithms with softmax and direct parameterization. Similar results for general pa-\nrameterization are obtained by (Liu et al., 2020; Mondal and Aggarwal, 2024). The regret analysis of a PG\nalgorithm with the general parameterization has been recently performed for an infinite horizon average\nreward MDP without constraints (Bai et al., 2024). Similar regret and constraint violation analysis for the\naverage reward CMDP is still missing in the literature. In this paper, we bridge this gap."}, {"title": "2 Related work", "content": "The constrained reinforcement learning problem has been extensively studied both for infinite horizon\ndiscounted reward and episodic MDPs. For example, discounted reward CMDPs have been recently stud-\nied in the tabular setup (Bai et al., 2022), with softmax parameterization (Ding et al., 2020; Xu et al., 2021),\nand with general policy parameterization (Ding et al., 2020; Xu et al., 2021; Bai et al., 2023b). Moreover,\n(Efroni et al., 2020; Qiu et al., 2020; Germano et al., 2023) investigated episodic CMDPs in the tabular set-\nting. Recently, the infinite horizon average reward CMDPs have been investigated in model-based setups"}, {"title": "3 Formulation", "content": "This paper analyzes an infinite-horizon average reward constrained Markov Decision Process (CMDP) de-\nnoted as  M = (S, A, r, c, P, p)  where S is the state space, A is the action space of size A,  r : S \u00d7 A \u2192 [0, 1] \nis the reward function,  c : S \u00d7 A \u2192 [-1,1]  is the constraint cost function\u00b9,  P : S \u00d7 A \u2192 \u2206^{|S|}  is the state\ntransition function where  \u2206^{|S|}  denotes a probability simplex with dimension |S|, and  \u03c1 \u2208 \u2206^{|S|}  is the initial\ndistribution of states. A policy  \u03c0\u2208\u03a0 : S \u2192 \u2206^{A}  maps the current state to an action distribution. The\naverage reward and cost of a policy, \u03c0, is,\n J_{g,\u03c0} \\triangleq \\lim_{T\u2192\u221e} E_{S_0 \\sim \u03c1, \u03c0} \\left[ \\frac{1}{T} \\sum_{t=0}^{T-1} g(s_t, a_t) \\right] (1)\nwhere g = r, c for average reward and cost respectively. The expectation is calculated over the distribution\nof all sampled trajectories  {(st, at)}t=0  where  at ~ \u03c0(st), St+1 ~ P(\u00b7|st, at), \u2200t \u2208 {0,1,\u2026\u2026}.  For notational\nconvenience, we shall drop the dependence on p whenever there is no confusion. Our goal is to maximize\nthe average reward function while ensuring that the average cost is above a given threshold. Without loss\nof generality, we can mathematically represent this problem as follows 2.\n\\max_{\u03c0\u2208\u03a0} J_r  \\text{ s.t. } J_c \u2265 0 (2)\nHowever, the above problem is difficult to handle when the underlying state space, S is large. Therefore,\nwe consider a class of parametrized policies,  {\u03c0\u03b8|\u03b8 \u2208 \u0398}  whose elements are indexed by a d-dimensional\nparameter,  \u03b8\u2208 Rd  where  d \u00ab |S||A|.  Thus, the original problem in Eq (2) can be reformulated as the\nfollowing parameterized problem.\n\\max_{\u03b8\u2208\u0398} J_r^\u03b8  \\text{ s.t. } J_c^\u03b8 \u2265 0 (3)\nIn the rest of this article, we denote  Jg\u03b8 = Jg(\u03b8), g \u2208 {r, c}  for notational convenience. Let,  P^{\u03c0\u03b8} : S \u2192 \u2206^{|S|}\nbe a transition function induced by  \u03c0\u03b8  and defined as,  P^{\u03c0\u03b8}(s, s') = \\sum_{a\u2208A}P(s'|s,a)\u03c0\u03b8(a|s), \u2200s, s'.  If M\nis such that for every policy \u03c0, the induced function,  P^{\u03c0}  is irreducible, and aperiodic, then M is called\nergodic.\nAssumption 1. The CMDP M is ergodic."}, {"title": "4 Proposed Algorithm", "content": "Algorithm 1 Primal-Dual Parameterized Policy Gradient\n1: Input: Episode length H, learning rates \u03b1, \u03b2, initial parameters  \u03b81, \u03bb1,  initial state  s0 ~ \u03c1(\u00b7),\n2:  K = T/H\n3: for  k \u2208 {1,..., K}  do\n4:  Tk \u2190 \u2205\n5:  for  t \u2208 {(k \u2212 1)H,\u2026\u2026\u2026 ,kH \u2212 1}  do\n6:  Execute  at ~ \u03c0\u03b8\u03ba (St)\n7:  Observer(st, at), c(st, at) and st+1\n8:  TkTkU {(st, at)}\n9:  end for\n10:  for  t \u2208 {(k \u2212 1)H,\u2026\u2026 ,kH \u2212 1}  do\n11:  Obtain  AL(St, at)  using Algorithm 2 and Th\n12:  end for\n13:  Compute wk using (18)\n14: Update parameters as\n \u03b8_{k+1} = \u03b8_k + \u03b1\u03c9_k\n \u03bb_{k+1} = P_{[0,3]}[\u03bb_k - \u03b2 \\hat{J_c}(\u03b8_k)] (13)\n15:  where  \\hat{J_c}(\u03b8_k) = \\frac{1}{HN} \\sum_{t=(k-1)H+N}^{kH-1}c(s_t, a_t)\n16: end for\nWe solve (3) via a primal-dual algorithm which is based on the following saddle point optimization.\n\\max_{\u03b8\u2208\u0398} \\min_{\u03bb\u22650} J_L(\u03b8, \u03bb) (14)\nwhere  J_L(\u03b8, \u03bb) \\triangleq J_r(\u03b8) + \u03bbJ_c(\u03b8)\nThe function,  JL(\u00b7, \u00b7),  is called the Lagrange function and \u03bb the Lagrange multiplier. Our algorithm updates\nthe pair  (\u03b8, \u03bb)  following the policy gradient iteration as shown below \u2200k \u2208 {1,\u2026\u2026, K} with an initial point\n(\u03b81, \u03bb1).\n \u03b8_{k+1} = \u03b8_k + \u03b1\u2207_\u03b8J_L (\u03b8_k, \u03bb_k)\n \u03bb_{k+1} = P_{[0,3]} [\u03bb_k \u2013 BJ_c(\u03b8_k)] (15)\nwhere \u03b1 and \u03b2 are learning parameters and  \u03b4  is the Slater parameter introduced in the following assumption.\nFinally, for any set, A, the function PA[\u00b7] denotes projection onto A.\nAssumption 2 (Slater condition). There exists a  \u03b4 \u2208 (0, 1)  and  \\bar{\u03b8} \u2208 \u0398  such that  Jc(\\bar{\u03b8}) \u2265 \u03b4."}, {"title": "5 Global Convergence Analysis", "content": "This section first shows that the sequence  {\u03b8k, Ak}k=1  produced by Algorithm 1 is such that their associ-\nated Lagrange sequence  {JL(\u03b8k, dk)}k=1  converges globally. By expanding the Lagrange function, we then\nexhibit convergence of each of its components  {Jg(\u03b8k,dk)}k=1,g \u2208 {r,c}. This is later used for regret\nand constraint violation analysis. Before delving into the details, we would like to state a few necessary\nassumptions.\nAssumption 3. The score function (stated below) is G-Lipschitz and B-smooth. Specifically, \u22000, 01, 02 \u2208 Rd,\nand \u2200(s, a), the following inequalities hold.\n ||\u2207_\u03b8 \\log \u03c0_\u03b8(a|s) || \u2264 G,\n ||\u2207_\u03b8 \\log \u03c0_{\u03b8_1}(a|s) \u2013 \u2207_\u03b8 \\log \u03c0_{\u03b8_2}(a|s)|| \u2264 B||\u03b8_1 - \u03b8_2||\nRemark 1. The Lipschitz and smoothness properties of the score function are commonly assumed for policy\ngradient analyses (Agarwal et al., 2020; Zhang et al., 2021; Liu et al., 2020). These assumptions hold for\nsimple parameterization classes such as Gaussian policies.\nNote that by combining Assumption 3 with Lemma 2 and using the gradient estimator as given in (18),\none can deduce the following result.\nLemma 3. The following inequality holds \u2200k provided that assumptions 1 and 3 are true.\n E [||w_k \u2013 \u2207_\u03b8 J_L (\u03b8_k, \u03bb_k) ||^2] \u2264 \u00d5 \\left( \\frac{\u03b1 G^2 t_{mix}^2}{\\delta^2 \\sqrt{T}} \\right) (20)\nLemma 3 claims that the gradient estimation error can be bounded as  \u00d5(1/\u221aT) . We will use this result\nlater to prove the global convergence of our algorithm.\nAssumption 4. Let the transferred compatible function approximation error be defined as follows.\nL_{\u03b1\u03c0^*, \u03c0^*} (\u03c9_{\u03b8,\u03bb}, \u03b8, \u03bb) = E_{s \\sim d^{\u03c0^*}} E_{a \\sim \u03c0^*(s)} \\left[ \\left( \\nabla_\u03b8 \\log \u03c0_\u03b8(a|s) \\cdot \u03c9_{\u03b8,\u03bb} - A_\u03bb^{\u03c0^*}(s, a) \\right)^2 \\right] (21)\nwhere  \u03c0^*  is the optimal solution of unparameterized problem in (2) and\n\u03c9_{\u03b8,\u03bb}^\u2217 = \\arg \\min_{\u03c9 \\in R^d} E_{s \\sim d^\u03b8} E_{a \\sim \u03c0_\u03b8(s)} \\left[ \\nabla_\u03b8 \\log \u03c0_\u03b8(a|s) \\cdot \u03c9 - A_\u03bb^{\u03c0_\u03b8}(s, a) \\right]^2. (22)\nWe assume that  L_{\u03b1\u03c0^*, \u03c0^*} (\u03c9_{\u03b8,\u03bb}, \u03b8, \u03bb) \u2264 \u03b5_{bias}  for any \u03bb > 0 and \u03b8 \u2208 Rd where  \u03b5bias  is a positive constant."}, {"title": "6 Regret and Violation Analysis", "content": "In this section, we utilize the convergence analysis in the previous section to bound the expected regret and\nconstraint violation of Algorithm 1. Note that the regret and constraint violation can be decomposed as,\n R_T = \\sum_{t=0}^{T-1} (J^* - r(s_t, a_t)) = H \\sum_{k=1}^{K} (J^* \u2013 J_r(\u03b8_k)) + \\sum_{k=1}^{K} \\sum_{t \\in I_k} (J_r(\u03b8_k) \u2013 r(s_t, a_t))\n Vi_T = \\sum_{t=0}^{T-1} (-c(s_t, a_t)) = H \\sum_{k=1}^{K} (-J_c(\u03b8_k)) + \\sum_{k=1}^{K} \\sum_{t \\in I_k} (J_c(\u03b8_k) - c(s_t, a_t))"}, {"title": "A Proofs for Lemmas in 4", "content": "A.1 Proof of Lemma 1\nSince the first step of the proof works in the same way for functions Jr and Jc, we use the generic notations\nJg, Vg, Qg where g = r, c and derive the following.\n\u2207_\u03b8V_g^{\u03c0_\u03b8}(s) = \u2207_\u03b8 \\bigg(\\sum_a \u03c0_\u03b8(a|s) Q_g^{\u03c0_\u03b8}(s, a) \\bigg) (32)\n= \\sum_a (\u2207_\u03b8 \u03c0_\u03b8(a|s)) Q_g^{\u03c0_\u03b8}(s, a) + \\sum_a \u03c0_\u03b8(a|s) \u2207_\u03b8Q_g^{\u03c0_\u03b8}(s, a)\n \\stackrel{(a)}{=} \\sum_a (\u2207_\u03b8 \u03c0_\u03b8(a|s)) Q_g^{\u03c0_\u03b8}(s, a) + \\sum_a \u03c0_\u03b8(a|s) \u2207_\u03b8 \\bigg( g(s, a) - J_g(\u03b8) + \\sum_{s'} P(s'|s,a)V_g^{\u03c0_\u03b8}(s') \\bigg)\n= \\sum_a (\u2207_\u03b8 \\log \u03c0_\u03b8(a|s)) \u03c0_\u03b8(a|s) Q_g^{\u03c0_\u03b8}(s, a) + \\sum_a \u03c0_\u03b8(a|s) \\bigg( \\sum_{s'} P(s'|s,a)\u2207_\u03b8V_g^{\u03c0_\u03b8}(s') \\bigg) - \u2207_\u03b8J_g(\u03b8)\nwhere the step (a) is a consequence of  \u2207_\u03b8 \\log \u03c0_\u03b8 = \\frac{\u2207_\u03b8\u03c0_\u03b8}{\u03c0_\u03b8}  and the Bellman equation. Multiplying both sides\nby d\u03b8(s), taking a sum over s \u2208 S, and rearranging the terms, we obtain the following.\n\u2207_\u03b8J_g (\u03b8) = \\sum_s d^\u03b8(s)\u2207_\u03b8J(\u03b8)\n= \\sum_s d^\u03b8(s) \\bigg( \\sum_a \u03c0_\u03b8(a|s) (\u2207_\u03b8 \\log \u03c0_\u03b8(a|s)) Q_g^{\u03c0_\u03b8}(s, a) + \\sum_a \u03c0_\u03b8(a|s) \\bigg( \\sum_{s'} P(s'|s,a)\u2207_\u03b8V_g^{\u03c0_\u03b8}(s') \\bigg) \\bigg) - \\sum_s d^\u03b8(s)\u2207_\u03b8V_g^{\u03c0_\u03b8}(s)\n= E_{s \\sim d^\u03b8, a \\sim \u03c0_\u03b8(\u00b7|s)} \\bigg[Q_g^{\u03c0_\u03b8}(s, a) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s) \\bigg] + \\sum_{s'} P(s'|s,a)\u2207_\u03b8V_g^{\u03c0_\u03b8}(s') - E_{s \\sim d^\u03b8} [\u2207_\u03b8V_g^{\u03c0_\u03b8}(s)]\n \\stackrel{(a)}{=} E_{s \\sim d^\u03b8, a \\sim \u03c0_\u03b8(\u00b7|s)} \\bigg[Q_g^{\u03c0_\u03b8}(s, a) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s) \\bigg] + \\sum_{s'} d^\u03b8(s')\u2207_\u03b8V_g^{\u03c0_\u03b8}(s') - E_{s \\sim d^\u03b8} [\u2207_\u03b8V_g^{\u03c0_\u03b8}(s)] = E_{s \\sim d^\u03b8, a \\sim \u03c0_\u03b8(\u00b7|s)} \\bigg[Q_g^{\u03c0_\u03b8}(s, a) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s) \\bigg] (33)\nwhere (a) uses the fact that  d^\u03b8  is a stationary distribution. Note that,\nE_{s \\sim d^{\u03c0_\u03b8}, a \\sim \u03c0_\u03b8(\u00b7|s)} [V_g^{\u03c0_\u03b8}(s) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s)] = \\sum_{s \\in S} V_g^{\u03c0_\u03b8}(s) \u2207_\u03b8 \\sum_{a \\in A} \u03c0_\u03b8(a|s) \u2207_\u03b8 \\log \u03c0_\u03b8(a|s) (34)\n= \\sum_{s \\in S} V_g^{\u03c0_\u03b8}(s) \\sum_{a \\in A} \u2207_\u03b8 \u03c0_\u03b8(a|s)\n= E_{s \\sim d^{\u03c0_\u03b8}} V_g^{\u03c0_\u03b8}(s) \\sum_{a \\in A} \u2207_\u03b8 \u03c0_\u03b8(a|s) = E_{s \\sim d^{\u03c0_\u03b8}} [V_g^{\u03c0_\u03b8}(s)] = 0\nWe can, therefore, replace the function  Q_g^{\u03c0_\u03b8}  in the policy gradient with the advantage function  A_g^{\u03c0_\u03b8} (s, a) =\nQ_g^{\u03c0_\u03b8}(s, a) \u2013 V_g^{\u03c0_\u03b8} (s), \u2200(s, a) \u2208 S \u00d7 A. Thus,\n\u2207_\u03b8J_g(\u03b8) = E_{s \\sim d^\u03b8, a \\sim \u03c0_\u03b8(\u00b7|s)} \\bigg[A_\u03bb^{\u03c0_\u03b8}(s, a) ,\u2207_\u03b8 \\log \u03c0_\u03b8(a|s) \\bigg] (35)\nThe proof is completed using the definitions of JL,\u03bb and AL,\u03bb."}]}