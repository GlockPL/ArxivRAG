{"title": "Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance", "authors": ["Md Fahim Anjum"], "abstract": "Recent advances in image generation, particularly via diffusion models, have led to impressive improvements in image synthesis quality. Despite this, diffusion models are still challenged by model-induced artifacts and limited stability in image fidelity. In this work, we hypothesize that the primary cause of this issue is the improper resampling operation that introduces aliasing in the diffusion model and a careful alias-free resampling dictated by image processing theory can improve the model's performance in image synthesis. We propose the integration of alias-free resampling layers into the UNet architecture of diffusion models without adding extra trainable parameters, thereby maintaining computational efficiency. We then assess whether these theory-driven modifications enhance image quality and rotational equivariance. Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and MNIST-M, reveal consistent gains in image quality, particularly in terms of FID and KID scores. Furthermore, we propose a modified diffusion process that enables user-controlled rotation of generated images without requiring additional training. Our findings highlight the potential of theory-driven enhancements such as alias-free resampling in generative models to improve image quality while maintaining model efficiency and pioneer future research directions to incorporate them into video-generating diffusion models, enabling deeper exploration of the applications of alias-free resampling in generative modeling.", "sections": [{"title": "Introduction", "content": "Recent advancements in generative modeling, particularly in diffusion models [6], have pushed the boundaries of what is possible in high-quality image synthesis. Among these, the Stable Diffusion model has gained prominence for its ability to generate realistic images by iteratively refining noise into coherent visual outputs [4]. Despite its success, there remains a challenge in further enhancing the model's performance, particularly in terms of stability and image fidelity [3].\nIn this paper, we hypothesize that the existing resampling operations (upsampling/downsampling) in the architecture of current diffusion models introduce aliasing which leads to a reduction of image quality. We also propose that proper theory-driven alias-free resampling can improve the model's performance in image synthesis. Improving the performance of image synthesis via alias-free resampling techniques has recently been explored in generative adversarial networks (GANs). Indeed, StyleGAN3 [9], the latest iteration in the StyleGAN series, has demonstrated significant improvements over its predecessors by incorporating carefully designed alias-free resampling layers via anti-aliasing filtering techniques that prevent high-frequency artifacts and improve the overall visual coherence"}, {"title": "Theoretical Foundations", "content": "Alias-free resampling is a critical signal processing technique for mitigating aliasing in 1D signals and 2D images, ensuring that high-frequency details are faithfully represented without introducing unwanted artifacts. For models like diffusion-based architectures, aliasing can occur due to improper downsampling and upsampling operations or nonlinearities like ReLU, causing artifacts that disrupt the generation of high-quality images. Alias-free resampling addresses this by adhering to the Shannon-Nyquist sampling theorem [14], a fundamental theoretical principle that provides the necessary conditions for sampling a signal or image. At its core, this theorem states that if we uniformly sample a signal, the sampling rate has to be at least twice the highest frequency of the signal's bandwidth. Conversely, the bandwidth of a discrete signal has to be half the sampling rate. If the sampled signal contains frequencies beyond this limit, aliasing occurs, meaning that high-frequency components will overlap with lower frequencies, distorting the reconstructed signal. Suppose x(t) is a 1D signal that was uniformly sampled at a rate s to obtain a discrete signal x[n]. Then, the Shannon-Nyquist sampling theorem dictates that the frequency bandwidth of x[n] must lie within the Nyquist limit, which is half the sampling rate, $f_{Nyquist} = s/2$ [14]. Therefore, before resampling a signal, it is critical to apply a low-pass filter (also known as an anti-aliasing filter) with a frequency cutoff up to $f_{Nyquist}$ to stop the aliasing. The principle is identical for 2D signals such as images."}, {"title": "The Role of Alias-Free Resampling in Nonlinear Transformations", "content": "Nonlinear operations like GeLU or ReLU in the continuous domain introduce sudden fluctuations causing arbitrarily high frequencies that cannot be represented in the sampled output and a natural solution is to eliminate the offending high-frequency content by applying an ideal low-pass filter. However, diffusion networks utilize discrete domain data where point-wise nonlinearity is utilized which does not commute with fractional transformations (such as rotation). Therefore, to temporarily approximate a continuous representation, we utilize a proper 2\u00d7 alias-free upsampling, apply the nonlinearity in the higher resolution and finally use a 2\u00d7 alias-free downsampling equipped with low-pass anti-aliasing filter for returning to the original discrete space."}, {"title": "Designing Anti-Aliasing Filters", "content": "As discussed in the previous section, alias-free resampling involves applying a low-pass filter to the signal to ensure that no aliasing occurs (Figure 1). In particular, given a discrete 2D signal x[n1, n2] sampled on a regular grid with spacing 1/s, alias-free resampling applies a low-pass filter with a cutoff frequency $f_c \\leq f_{Nyquist}$ (or $\\omega_c < \\pi/2$ ), ensuring that no frequencies above the Nyquist limit are included in the resampled signal. The 2D circularly symmetric low-pass filter has an impulse response h[n1, n2] given by:\n\n$h[n_1, n_2] = \\frac{\\omega_c}{2\\pi\\sqrt{n_1^2 + n_2^2}} J_1(\\frac{\\omega_c\\sqrt{n_1^2 + n_2^2}}{s})$\n\nwhere $J_1$ is the Bessel function of the first kind and of the first order [13]. At the center point, $n_1 = n_2 = 0$, the limiting value is used: $h[0, 0] = \\omega_c^2/4\\pi$. The impulse response in (1) is also known as the Jinc function, analogous to the sinc function in 1D case, and is defined as:\n\n$jinc(x) = \\frac{J_1(x)}{x}$ \n\nUsing this, the impulse response can be rewritten as:\n\n$h[n_1, n_2] = \\frac{\\omega_c}{2\\pi} jinc(\\omega_c \\rho)$, $ \\rho = \\sqrt{n_1^2 + n_2^2}$ \n\nThis circular low-pass filter ensures that only frequencies below the cutoff $\\omega_c$ are preserved, effectively eliminating aliasing by attenuating higher frequencies (Figure 1). By convolving this filter with the"}, {"title": "Architectural Revisions in Diffusion Model", "content": "We use a classical unconditional diffusion model with standard noising and denoising steps as our baseline where we follow Algorithm 1 from [6] for training. The baseline architecture is based on a UNet encoder-decoder structure with skip connections, where the input image is progressively 2x downsampled to capture high-level features and subsequently 2\u00d7 upsampled to recover fine details.\nThe input first passes through a series of convolutional layers. Conventional downsampling is achieved through max pooling, which selects the maximum value in non-overlapping regions, reducing the spatial resolution by half. Upsampling is performed through conventional bilinear interpolation, which smoothly increases the resolution by averaging the neighboring pixel values. Additionally, an alignment step ensures that the corners of the input and output grids are matched, preserving the spatial consistency across layers. The bottleneck section consists of several convolutional layers. Skip connections pass intermediate features from the downsampling stages to their corresponding upsampling stages, ensuring that important spatial information is retained. Additionally, self-attention layers are applied in both the downsampling and upsampling stages to refine the feature maps by considering the global context. We denote this baseline architecture as Config A (Figure 2)."}, {"title": "Alias-Free Resampling (Config B)", "content": "First, we revise our baseline architecture (Config A) by replacing the up and downsampling layers, which do not guarantee alias-free outputs, with their alias-free versions respectively. In particular, the downsampling layers first apply low-pass anti-aliasing filters to the data as described in (4), and then reduce the sampling rate by uniformly removing interleaving samples (Figure 1). During upsampling, zeros are interleaved with the data to increase the sampling rate, followed by passing the result through a low-pass filter to remove unwanted high-frequency components (Figure 1). These steps are grounded in classical resampling theory from image processing. We denote this modified version as Config. B (Figure 2), which significantly improves the quality of resampling, reducing aliasing artifacts and enhancing output fidelity."}, {"title": "Enhanced Nonlinearities via Alias-Free Resampling (Config C)", "content": "Next, we shift our attention to the nonlinear components of our baseline architecture (Config. A) and revise it by introducing 2\u00d7 alias-free upsampling before the nonlinear ReLU or GeLU operations and 2\u00d7 alias-free downsampling afterward to retain the original sampling rate. These adjustments aim to mitigate aliasing introduced by the nonlinear operations while preserving high-frequency details. Importantly, the existing upsampling and downsampling layers in the network remain unmodified in this configuration (Config. C; Figure 2), as we only inject alias-free resampling layers around the nonlinear operations."}, {"title": "Combining Alias-Free Resampling and Nonlinear Enhancements (Config D)", "content": "Here, we combine Configurations B and C by replacing the upsampling and downsampling layers in Configuration C with their alias-free counterparts from Configuration B. This ensures that both the nonlinear operations and the standard resampling processes in the network are alias-free, effectively reducing artifacts and improving image fidelity across all stages. We denote this revised architecture as Config. D (Figure 2)."}, {"title": "Improving Rotational Consistency", "content": "Lastly, we revise the classical diffusion process (Algorithm 2) to incorporate controlled rotation during image generation. The core idea is to progressively distribute the target rotation over the time steps (Figure 3). At each time step, the image is rotated by a small, constant angle, ensuring that as the diffusion progresses, the image gradually rotates towards a target orientation (Algorithm 3). This modification allows the model to generate images with user-defined rotational transformations while maintaining coherence throughout the generative process.\nIn particular, given the input matrix to be rotated x and the rotation angle $\\phi$, the rotation function can be represented as, Rotate (x, $\\phi$) which performs an affine transformation on x, effectively rotating it around its center by $\\phi$. This process begins by determining the center of the image, which serves as the pivot point for the rotation. The positions of all pixels are then adjusted according to $\\phi$, recalculating their coordinates to reflect the desired rotation. This involves translating the coordinates so that the rotation occurs around the image center and then translating them back to their original location. Note that there are parts of the image that will have to be extrapolated due to the rotation operation. Finally, the rotation is applied at each time step. Therefore, if the desired rotation is $\\Phi$, rotation at time step t is, $\\phi_t = \\Phi/T$."}, {"title": "Configuration Naming Scheme", "content": "In addition to the Alphabets (A-D) we use for denoting our revised UNet architecture, we also use two parameters during the naming of our models. These are the Kaiser $\\beta$ value and whether or not the kernel was normalized. Specifically, the $\\beta$ value is added after the configuration alphabet, indicating"}, {"title": "Experiments", "content": "To train and evaluate our model, we used three benchmark image datasets [11, 12, 5], each pre- processed to ensure consistency in input dimensions and pixel intensity normalization across experiments. In particular, for each dataset, we applied standard normalization across all channels by centering the pixel values around zero with a range of [-1,1]. All images were resized to 32 \u00d7 32 pixels."}, {"title": "CIFAR-10", "content": "CIFAR-10 dataset consists of 60,000 32 \u00d7 32 color images in 10 classes, with 6,000 images per class [11]. We utilized a subset of 10,000 images (test set of the original dataset) from the CIFAR-10 dataset with 10 classes (1,000 images from each class). Each sample is a 3-channel RGB color image with a native resolution of 32 \u00d7 32 pixels."}, {"title": "MNIST", "content": "MNIST dataset of handwritten digit images consists of 60,000 single-channel grayscale samples [12]. We used a subset of the MNIST dataset containing 19,999 samples, which was available in the Google Colab environment. Each image was initially 28 \u00d7 28 pixels which was resized to 32 \u00d7 32 pixels."}, {"title": "MNIST-M", "content": "We also utilized MNIST-M, a variation of MNIST with added background textures [5]. We used 6,000 randomly selected samples from the original 60,000-image set. Each image is a 3-channel RBG color image. The native image dimensions of 28 \u00d7 28 pixels were resized to 32 \u00d7 32 pixels."}, {"title": "Evaluation Metrics", "content": "To benchmark the performance of the generative models, we utilized several metrics: Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and Kernel Inception Distance (KID). IS measures the diversity and quality of generated samples, with higher scores indicating better performance. FID score quantifies the difference between the generated and real data distributions by comparing their feature representations, where lower values correspond to more realistic samples while KID is a variation of FID that uses the squared maximum mean discrepancy between samples, providing an unbiased comparison, with lower values indicating better performance. These metrics were computed across all datasets to provide a comprehensive evaluation of the model's performance."}, {"title": "Standard Image Synthesis Performance", "content": "In this section, we present the quantitative results across the CIFAR-10, MNIST-M, and MNIST datasets, comparing our modified configurations (Config B-D) against the baseline architecture (Config A)."}, {"title": "CIFAR-10", "content": "Configuration D-1N (Alias-free resampling and nonlinear enhancements; Kaiser window $\\beta$ = 1, Normalized kernel) achieved the best overall performance for the CIFAR dataset (Table 2; Figure 4), significantly outperforming the baseline (Config A) with a FID of 90.21, representing an 8.7% improvement over the baseline FID and a KID of 5.54 (7.2% improvement). While its IS of 4.51 is slightly lower than the baseline (4.54), the improvements in FID and KID suggest superior sample quality. Configuration B-0 (Alias-free resampling; No Kaiser window or kernel normalization) also performed well, achieving a FID of 94.23 (4.6% improvement), a KID of 5.44 (8.9% improvement) and the highest IS of 4.71."}, {"title": "MNIST-M", "content": "On the MNIST-M dataset, configuration D-2N yielded the best results, with the lowest FID of 82.46, representing a 3.0% improvement over baseline (Table 2; Supplemental Figure 6) and the lowest KID of 5.35 (14.1% improvement over the baseline KID of 6.23). These results highlight D-2N as the strongest performer in generating high-quality samples, though its IS of 3.99 was only modestly better than the baseline of 3.76. Configuration B-2N also demonstrated competitive performance with the highest IS of 4.14, a FID of 88.05 (2.3% lower than the baseline), while its KID of 5.47 was also superior to the baseline by 12.2%."}, {"title": "MNIST", "content": "For the MNIST dataset, the baseline architecture remained the best in terms of sample quality (Table 2; Supplemental Figure 7). However, configuration B-0 came close, with a FID of 10.23 (6.4% higher than the baseline) and a KID of 0.58. Config B-2N, while achieving the highest IS of 2.00, did not outperform the baseline in terms of FID or KID."}, {"title": "Summary of Best Performances", "content": "In summary, Config D-1N outperformed the baseline in the CIFAR-10 dataset, achieving 8.7% FID and 7.2% KID improvement. For the MNIST-M dataset, Config D-2N delivered the best results with 3.0% FID and a 14.1% KID improvement. These results highlight that just by introducing alias-free resampling into the UNet network, significant improvements in sample quality can be achieved, particularly in terms of FID and KID, which are critical indicators of generative model performance. Note that the reliability of the performance metrics (IS, FID, and KID) on MNIST data is not well-established as the MNIST dataset contains single-channel gray image data while these metrics are designed for RGB images."}, {"title": "Assessing Rotational Equivariance", "content": "We conducted an initial evaluation to assess the rotational equivariance of our modified diffusion process (Algorithm 3) by varying the target rotation angle $\\Phi$ from -$\\pi$/2 to $\\pi$/2 radians and generating images through the modified diffusion process using models trained on the CIFAR-10, MNIST-M, and MNIST datasets. For each dataset, we compared two models: the baseline (Config. A) and our enhanced UNet (Config. D), with the latter theoretically offering superior robustness to rotation. Figure 5 illustrates the promising ability of our modified diffusion process to generate images at specific rotations without any additional training where our enhanced UNet architecture showed more consistent object rotation for various angles. These results indicate that our additional filtering layers used for alias-free resampling reduce the dependency of image details on absolute pixel coordinates, enabling more coherent image rotation. While these results are encouraging, further comprehensive evaluation is required to confirm its effectiveness."}, {"title": "Ablation Studies and Comparative Analysis", "content": "In the ablation study, we analyze the effects of architectural variations (Config B, C, D), the impact of the Kaiser window $\\beta$, and the influence of kernel normalization (N) on model performance across the"}, {"title": "Architecture Comparison (Config B, C, D)", "content": "Across CIFAR-10 and MNIST-M, Config D outperformed Config B and C, particularly in terms of FID and KID, while maintaining competitive IS scores (Table 2). On CIFAR-10, Config D- 1N achieved the lowest FID score, representing an 8.7% FID and a 7.6% KID improvement over Config C-2N. Config D-2N also reduced FID by 6.3% and KID by 20.9% compared to Config C-2N. On MNIST-M, Config D-2N achieved the best results showing a 17.6% FID and a 20.7% KID improvement over Config C-2N. Config B came close with the best KID scores in CIFAR-10 and was particularly superior in MNIST dataset. Indeed on MNIST, Config B-0 achieved the second-best FID and KID while maintaining a superior IS. Overall, Config D provided better FID and KID scores in CIFAR-10 and MNIST-M while Config B achieved the highest IS scores across all datasets with slightly better results in MNIST."}, {"title": "Effect of Kaiser Window", "content": "No Kaiser window ($\\beta$ = 0) resulted in higher IS but poorer FID scores (Table 2). For instance, on CIFAR-10, Config B-0 had the highest IS score, but its FID was worse than Config D-1N by 4.3%. On MNIST-M, Config B-0's IS score was close to the baseline, but its FID was 13.8% worse than Config D-2N. Kaiser window with $\\beta$ = 1 provided a more balanced trade-off. For CIFAR-10, Config D-1N achieved the best FID while maintaining strong IS and KID scores. Compared to Config D-0 (no Kaiser window), $\\beta$ = 1 reduced FID by 7.1% and KID by 14.8%. Finally, Kaiser window with"}, {"title": "Effect of Kernel Normalization", "content": "The introduction of kernel normalization had varying effects on model performance across the datasets. For this, we fixed the Kaiser window $\\beta$ = 1 and observed the effect of kernel normalization across Config B, C, and D. Configurations with normalization generally improved FID and KID. Particularly, on CIFAR, Config D-1N improved IS by 4.40%, FID by 16.51%, and KID by 25.34% compared to Config D-1. Config C-1N also provided a 12.08% increase in IS, a 22.69% improvement in FID, and a 33.49% reduction in KID over Config C-1. On MNIST-M, Config B-1N improved IS by 9.12%, but FID decreased by 7.22%, and KID was marginally worse by 1.76%, showing mixed effects. On the other hand, Config C-1N improved IS by 6.03%, FID by 13.58%, and KID by 18.88% over Config C-1. For Config D-1N, IS increased by 6.98%, while FID improved by 5.36%, and KID saw an 8.38% reduction. Finally, on MNIST, kernel normalization had no significant effect on IS for Config B and C but improved FID by 5.64% and KID by 12.5% for Config B-1N. Config C-1N improved FID by 8.06% and KID by 17.14%. Config D-1N saw no IS change, but FID improved by 11.39%, and KID by 13.39%, indicating consistent improvements in quality."}, {"title": "Discussion and Future Directions", "content": "In this work, we hypothesize that the current resampling operations (upsampling and downsampling) in diffusion model architectures introduce aliasing, which degrades image quality and utilizing theory-driven alias-free resampling can enhance model performance in image synthesis. We proposed architectural modifications of the classical diffusion models by incorporating alias-free resampling into the UNet structure. We demonstrated that our proposed modifications can substantially enhance image quality and model stability without adding complexity or increasing the number of trainable parameters. Our approach aligns with a growing trend in machine learning to leverage domain- specific theories such as signal processing and statistical physics to drive innovation in deep learning architectures [1, 9]. Furthermore, we proposed a modification of the diffusion process that enables user-controlled image rotation without any additional model training. As computational resources continue to be a bottleneck for such generative models, our work offers a promising theory-driven pathway for achieving customizable high performance in generative modeling without significant increases in computational cost. Finally, our work has the potential to benefit other image-based deep learning architectures by enhancing conventional resampling and nonlinear operations with alias-free resampling techniques."}, {"title": "Limitations", "content": "A primary limitation of this study is the absence of large-scale training on full high-resolution datasets, a constraint imposed by limited computational resources. However, as a proof-of-concept, our goal was to demonstrate the benefits of integrating alias-free resampling via theory-driven modifications of diffusion models for image generation, rather than achieving state-of-the-art results through exhaustive training. To this end, our findings suggest that even under our constrained training conditions, the proposed models consistently outperformed the baseline models, indicating the potential for even greater performance enhancements in large-scale settings. Future work will aim to explore this potential through comprehensive training with full datasets."}, {"title": "Future Work", "content": "In this study, we explore the integration of theory-driven alias-free resampling techniques in diffusion models for image generation. Our future works will focus on several avenues to further refine and extend our approach. First, it might interesting to conduct large-scale training on diverse datasets with high-resolution images to fully realize the potential of our proposed alias-free resampling and controlled rotation techniques. Second, we aim to incorporate these techniques into video-generating diffusion models [2, 7] to explore their effectiveness in enhancing temporal stability and image coherence across frames. Another key area for future research is a more rigorous assessment of rotational equivariance of controlled rotation to ensure that model performance remains consistent under various rotational transformations. Finally, in this work we kept the filter cutoff and kernel length fixed. However, in future study we plan to investigate filtering with various kernel lengths and cutoffs across different layers of the UNet to enhance image fidelity and stability during generative tasks."}, {"title": "Conclusion", "content": "In conclusion, this work presents a significant step toward enhancing diffusion models through the integration of alias-free resampling techniques. We hypothesize that the current upsampling and downsampling operations in diffusion model architectures introduce aliasing, which diminishes image quality. We propose modifications to the diffusion model by introducing alias-free resampling within the UNet architecture without adding trainable parameters. Our experimental results across benchmark datasets indicate that these modifications yield consistent quality improvements, particularly in terms of FID and KID, underscoring the effectiveness of theory-driven architectural refinements. Our work not only advances the capabilities of diffusion models but also illustrates the broader potential of incorporating alias-free resampling into other deep learning architectures to achieve efficiency and performance gains. Additionally, we introduce a modified diffusion process with user-controlled rotation, which further demonstrates the potential for more customizable image synthesis. As generative modeling continues to advance, our work offers a pathway for future innovations that provide theory-driven computationally efficient generative architectures."}, {"title": "Data and Code Availability", "content": "The actual datasets utilized in this study can be found via this Dropbox link and the codebase with examples are given in https://mdfahimanjum.github.io/AliasFree-Diffusion-Models-PyTorch/."}]}