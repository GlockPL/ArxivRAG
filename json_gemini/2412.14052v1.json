{"title": "Neural Combinatorial Optimization for Stochastic Flexible Job Shop Scheduling Problems", "authors": ["Igor G. Smit", "Yaoxin Wu", "Pavel Troubil", "Yingqian Zhang", "Wim P.M. Nuijten"], "abstract": "Neural combinatorial optimization (NCO) has gained significant attention due to the potential of deep learning to efficiently solve combinatorial optimization problems. NCO has been widely applied to job shop scheduling problems (JSPs) with the current focus predominantly on deterministic problems. In this paper, we propose a novel attention-based scenario processing module (SPM) to extend NCO methods for solving stochastic JSPs. Our approach explicitly incorporates stochastic information by an attention mechanism that captures the embedding of sampled scenarios (i.e., an approximation of stochasticity). Fed with the embedding, the base neural network is intervened by the attended scenarios, which accordingly learns an effective policy under stochasticity. We also propose a training paradigm that works harmoniously with either the expected makespan or Value-at-Risk objective. Results demonstrate that our approach outperforms existing learning and non-learning methods for the flexible JSP problem with stochastic processing times on a variety of instances. In addition, our approach holds significant generalizability to varied numbers of scenarios and disparate distributions.", "sections": [{"title": "Introduction", "content": "Combinatorial optimization (CO) problems are prevalent in various application areas, posing significant challenges due to their complex nature and computational intractability. One critical CO problem is the flexible job shop scheduling problem (FJSP), which has wide applicability ranging from semiconductor manufacturing (Tamssaouet et al. 2022) to healthcare scheduling (Burdett and Kozan 2018) and aluminum production (Zhang, Yang, and Zhou 2016). Traditional approaches, such as constraint programming (Baptiste, Pape, and Nuijten 2001; Col and Teppan 2022), heuristics (Sels, Gheysen, and Vanhoucke 2012), or meta-heuristics (Rooyani and Defersha 2019) have made great progress in solving these problems. However, these methods generally assume deterministic problems. In practice, oppositely, there is an abundance of uncertainty, leading to vulnerable plans produced by these optimization methods.\nIn recent years, increasing focus has been directed to stochastic optimization methods. However, these methods remain largely underrepresented, especially for more complex optimization problems such as the FJSP. Existing methods solving the stochastic FJSP, such as a simulation-optimization framework (Ghaedy-Heidary et al. 2024) or a meta-heuristic approach including Monte-Carlo sampling (Flores-G\u00f3mez, Borodin, and Dauz\u00e8re-P\u00e9r\u00e8s 2023) add significant computational complexity to already expensive approaches, while also being tailored to specific use-cases and objectives, limiting their potential for adoption.\nA recent branch of optimization technology is constituted by neural combinatorial optimization (NCO) methods, leveraging deep (reinforcement) learning (DRL) to solve CO problems (Bengio, Lodi, and Prouvost 2021). They learn high-quality optimization policies that fit a set of problem instances, eliminating the need for handcrafted expert rules, while also being efficient. Current NCO methods mainly target deterministic optimization, overlooking their practical use in stochastic situations. It is highly beneficial to extend NCO to the stochastic optimization domain.\nSeveral existing works leverage learning-based methods for stochastic routing or scheduling. However, these are generally focused on online or dynamic scenarios. Conversely, in many practical cases, such as manufacturing plants, a one-time schedule is needed in advance. Moreover, existing methods only focus on implicitly learning stochastic dynamics through the Markov decision process (MDP). In stochastic optimization, on the contrary, the assumed distributions can be explicitly considered, allowing for better plans.\nWe address these issues by a novel attention-based scenario processing module (SPM) that extends deterministic NCO architectures to solve stochastic problems. The SPM, which is expressive yet easily adaptable to different base neural networks for different problems, effectively captures the representation of stochastic scenarios to favorably intervene the solution policy. We embed SPM in a novel training paradigm that can address various stochastic objectives and offers high-quality yet fast solution policies, using a sampling strategy commonly found in stochastic optimization methods. We combine SPM with an existing network architecture to form SPM-DAN and solve the stochastic FJSP. Results show our method outperforms existing methods consistently. Our main contributions are summarized as follows:\n1. We propose an attention-based SPM to solve stochastic optimization problems, which is expressive and modular, with a good transferability to different base neural networks.\n2. We put forward an effective DRL training paradigm, which is used to optimize different stochastic objectives.\n3. We apply SPM to solve the stochastic FJSP, which outperforms existing learning and non-learning methods."}, {"title": "Related Work", "content": "NCO for FJSP Graph neural networks (GNNs) for solving a variety of scheduling problems have been developed in recent years (Zhang et al. 2020; Park et al. 2021; Lei et al. 2022; Kwon et al. 2021). We refer to a recent survey (Smit et al. 2025) for a complete outline. Song et al. (2022) first proposed a competitive end-to-end DRL algorithm to construct FJSP solutions. They used a heterogeneous graph and designed a heterogeneous GNN using different GAT (Veli\u010dkovi\u0107 et al. 2018) layers to encode machine and operation nodes. In the follow-up work, Wang et al. (2023) proposed a recent state-of-the-art dual attention network (DAN) that comprises both self- and cross-attention, which achieved superior performance over previous DRL approaches.\nNCO for Dynamic FJSP Multiple papers target the dynamic FJSP (e.g. Zhang et al. 2023; Lei et al. 2023), which involve different sources of uncertainty such as random arrivals, machine availability, or processing times. These papers mainly focus on developing training strategies and MDP formulations to fit the dynamic use cases. The network architectures are not specifically tailored to address uncertainties, which are implicitly handled by approximating the expected reward in MDP trajectories. Similarly, NCO methods for other problems also mainly target deterministic or dynamic cases, in which the realized values become known during solution construction (Schmitt-Ulms et al. 2022; Kwon et al. 2020; Joe and Lau 2020).\nNCO for Stochastic FJSP Similar to us, Infantes et al. (2024) consider a stochastic job shop scheduling problem (JSP) for which a full plan must be created before the realizations become known. They include three features in the state space to describe the assumed triangular distribution, and base the reward function on a single sampled realization for each instance to optimize the expected makespan. While promising, their method is limited to triangular processing time distributions and the expected makespan objective."}, {"title": "Preliminaries and Problem Formulation", "content": "Flexible Job Shop Scheduling Problem The FJSP consists of a pair $(\\mathcal{J}, \\mathcal{M})$ where $\\mathcal{J}$ and $\\mathcal{M}$ are jobs and machines, respectively. A job $J_i \\in \\mathcal{J}$ consists of a set $\\mathcal{O}_i = \\{O_{i1},..., O_{in_i}\\}$ of $n_i$ operations to be performed in order. The total set of operations is $\\mathcal{O} = \\bigcup \\mathcal{O}_i$. Each operation $O_{ij}$ must be processed by a single machine, selected from the set of compatible machines $M_{ij} \\subseteq \\mathcal{M}$. The processing time to perform operation $O_{ij}$ on machine $M_k \\in M_{ij}$ is given by $p > 0$ and each machine can only process one job at a time. A solution to the FJSP is a schedule, which assigns a compatible machine to each operation $O_{ij}$ and determines the order of operations on each machine. The goal is to minimize the makespan $C_{max} = max_{O_{ij}\\in \\mathcal{O}} C_{ij}$, which is the maximum completion time $c_{ij}$ of all operations.\nStochastic Flexible Job Shop Scheduling Problem In the stochastic FJSP, we assume that the processing times are random variables $P_{ij}^k$. The realized processing times $p_{ij}^k \\sim P_{ij}$ follow the probability distributions $P_S$ and only become known after the schedule is created. As a result, the realized $C_{ij}$ and $C_{max}$ are also realizations of the random variables $\\mathcal{C}_{ij}$ and $\\mathcal{C}_{max}$. The goal is to create a schedule that optimizes an objective function $f(C_{max})$, which can be the expected value $\\mathbb{E}(C_{max})$, the Value-at-Risk $VaR_\\alpha (C_{max}) = min\\{C_{max} : \\mathbb{P}(C_{max} \\leq \\bar{C}_{max}) > \\alpha\\}$, or other functions of $C_{max}$. $VaR_\\alpha$ indicates the value for which, with probability $\\alpha$, our makespan is at most this high. As such, it considers the practically relevant robustness to the uncertainty of schedules. We set and keep $\\alpha = 95\\%$ in this paper."}, {"title": "Attention Mechanism", "content": "The attention function aims to map a group of $n$ queries $Q \\in \\mathbb{R}^{n\\times d_q}$ to their advanced representations, using $n$ keys $K\\in \\mathbb{R}^{n_v\\times d_q}$ and $n$ values $V\\in \\mathbb{R}^{n_v\\times d_v}$, such that:\n$Att(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_q}})V$\nwhere $QK^T$ defines the similarity between $Q$ and $K$, based on which weights are computed by the softmax function and then used to obtain a weighted sum of the values in $V$.\nVaswani et al. (2017) extended the attention mechanism by introducing the multi-head attention. By setting $h$ heads, $Q$, $K$, and $V$ are first projected to $h$ matrices, respectively. Then, the attention function is applied to every three matrices in each head. The advanced representations from all heads are concatenated and linearly transformed:\n$MHA(Q, K, V) = concat(head_1, ..., head_h)W^O$\nwhere $head_i = Att(QW_i^Q, KW_i^K, VW_i^V)$. $W_i^Q \\in \\mathbb{R}^{d_q\\times d_i}, W_i^K \\in \\mathbb{R}^{d_q\\times d_i}, and W_i^V \\in \\mathbb{R}^{d_v\\times d_v'}$ are learnable matrices. We follow a typical choice for the dimensions by setting $d_q = d = d_v$ and $d_i = d_i' = d/h$. The multi-head attention layer is often embedded in a block, alongside the normalization, skip-connection, and feed-forward layer. Given input matrices $X \\in \\mathbb{R}^{n\\times d}$ and $Y \\in \\mathbb{R}^{n_v\\times d}$ a multi-head attention block is defined as:\n$MHAB(X, Y) = LayerNorm(Z + FF(Z))$\nwhere $Z = LayerNorm(X + MHA(X, Y, Y))$. $FF$ is a fully-connected feed-forward layer and $LayerNorm$ is the layer normalization operation (Ba, Kiros, and Hinton 2016)."}, {"title": "Methodology", "content": "We take the state-of-the-art FJSP method, i.e., the DAN from (Wang et al. 2023), for an example, and describe how our method SPM-DAN is developed from DAN. Our method is easily applicable to other NCO methods."}, {"title": "Markov Decision Process", "content": "The scheduling process can be considered as a sequential decision-making process of iteratively assigning operations to available machines. At every decision moment $t$, an operation-machine pair $(O_{ij}, M_k)$ is selected such that $O_{ij}$ can be assigned to $M_k$. In the MDP formulation, the agent at each step receives state $s_t$, representing the environment, and takes action $a_t = (O_{ij}, M_k) \\in A(t)$ from the set of eligible actions $A(t)$, which are the possible assignments of the first unscheduled operation in each job to a compatible machine. The environment feeds back reward $r_t$ and new state $s_{t+1}$. The schedule is completed after $|\\mathcal{O}|$ actions.\nState Space The relevant operations $\\mathcal{O}_u(t) \\subseteq \\mathcal{O}$ for the state $s_t$ are all operations except those that already have a successor scheduled on the same machine and, thus, do not directly influence the schedule anymore. The relevant machines $\\mathcal{M}_u(t) \\subseteq \\mathcal{M}$ are all machines on which any of the remaining operations can be scheduled. For the deterministic method proposed in (Wang et al. 2023), the state $s_t = \\{\\mathcal{H}_O, \\mathcal{H}_M, \\mathcal{H}_{OM}\\}$ consists of the operation features $\\mathcal{H}_O = \\{h_{O_{ij}} \\in \\mathbb{R}^{10}|O_{ij} \\in \\mathcal{O}_u(t)\\}$, machine features $\\mathcal{H}_M = \\{h_{M_{k}} \\in \\mathbb{R}^8|M_k \\in \\mathcal{M}_u(t)\\}$, and operation-machine pair features $\\mathcal{H}_{OM} = \\{h_{(O_{ij},M_{k})} \\in \\mathbb{R}^5|(O_{ij}, M_k) \\in A(t)\\}$. These features are dynamic and interdependent. We refer to the original paper for the detailed feature descriptions. We introduce an expansion of this state space, based on sample approximation. Concretely, we sample $n_{scn}$ independent scenarios, each representing a set of realized processing time values $p_{ij}^k \\sim P_{ij}$. For each scenario, we keep track of all the features during the scheduling process. In addition, we maintain the deterministic problem instance. In doing so, the expanded state captures both the deterministic and stochastic problem characteristics. Formally, our state at step $t$ is $S_t = \\{s_{det}, S_{stoch}\\}$, where $s_{det} = \\{\\mathcal{H}^{det}_O, \\mathcal{H}^{det}_M, \\mathcal{H}^{det}_{OM}\\}$ represents the deterministic instance state, and $S_{stoch} = \\{s_l\\}_{1 \\leq l \\leq n_{scn}}$ is the set of states representing the scenarios $s_l = \\{\\mathcal{H}^l_O, \\mathcal{H}^l_M, \\mathcal{H}^l_{OM}\\}$. Note that the sets $\\mathcal{O}_u(t)$, $\\mathcal{M}_u(t)$, and $A(t)$ are the same in the deterministic and sampled scenarios.\nAn alternative to capture stochastic information is representing the distribution by statistical features (Infantes et al. 2024). However, these features are hard to define for complex distributions and may lead to high dimensionality. Moreover, the varied characteristics in the state formulation depend on both the scheduling logic and the processing times, which cannot be accurately described by the statistical features, so valuable information is inevitably lost. Our features of the deterministic instance and scenarios can favorably reflect the influence of stochasticity, and are applicable to different problems, feature definitions, and distributions. Using efficient multiprocessing or parallel GPU computations, the runtime overhead of processing the states of scenarios is negligible, thereby maintaining fast solution construction.\nAction Space The action space $A(t)$ consists of all compatible operation-machine pairs of the first unscheduled operations per job. The same action is applied to the determin-istic instance and all the sampled scenarios."}, {"title": "State Transition", "content": "Upon taking an action, the sets $\\mathcal{O}_u(t)$, $\\mathcal{M}_u(t)$, and $A(t)$ are updated to reflect the new relevant operations, machines, and potential actions. Additionally, the features for $s_{det}$ and all $s_l$ are updated independently, resulting in a new state $s_{t+1}$."}, {"title": "Reward", "content": "We generalize the reward defined in (Zhang et al. 2020) and (Wang et al. 2023). In this deterministic reward function, a lower bound $C(O_{ij}, s_t)$ of the completion time is estimated for each operation $O_{ij}$ at time step $t$. This bound equals the scheduled completion time if the operation has been scheduled. For unscheduled operations, the lower bound is approximated using the recursion $C(O_{ij}, s_t) = C(O_{i(j-1)}, s_t) + min_{k \\in M_{ij}} P_{ij}^k$. Then the reward at step $t$ is $r_t = max_{O_{ij}\\in \\mathcal{O}} C(O_{ij}, s_t) - max_{O_{ij}\\in \\mathcal{O}} C(O_{ij}, s_{t+1})$.\nFor the stochastic reward, we consider $n_{rew}$ scenarios that are independently sampled and do not overlap with the scenarios of the state, to facilitate the generalization. These scenarios require a small amount of computation for tracking their makespan lower bounds since they do not necessitate computing all features. Concretely, let $s_l'$ represent the state of reward scenario $l$, then $C_t = \\{max_{O_{ij}\\in \\mathcal{O}} C(O_{ij}, s_t^l)|1 \\leq l \\leq n_{rew}\\}$ are the makespan lower bounds of all scenarios at time $t$. The reward is computed as $r_t = f(C_t) - f(C_{t+1})$, where $f$ is the objective function, which in our case is the $VaR_{95\\%}$ or mean makespan. However, other functions such as conditional $VaR$ or median are also possible. If the discounting factor is $\\gamma = 1$, the sum of all rewards naturally sums to $\\sum_{t} r_t = f(C_0) - f(C_{|\\mathcal{O}|-1})$, where $C_{|\\mathcal{O}|-1} = \\{C_{max}^l|1 \\leq l \\leq n_{rew}\\}$ are the makespans of all scenarios. Hence, maximizing the cumulative reward corresponds to minimizing the objective."}, {"title": "Network Architecture", "content": "Our neural network consists of the scenario processing modules (SPMs) that take the scenarios $S_{stoch}$ and compute embeddings from them to represent stochasticity. These are combined with the deterministic state $s_{det}$ and inputted to the base neural network. In doing so, we do not need to restructure the base neural network drastically, making our method easily applicable to different GNN architectures. An alternative would be to pass all scenarios through the GNN and do 'post-processing' before the actor and critic layers. However, this is significantly more computationally complex. Figure 1 outlines the proposed SPM and its application to the state-of-the-art DAN for FJSP (Wang et al. 2023).\nScenario Processing Module To efficiently and effectively extract embeddings from the scenario states to reflect the stochasticity, we design the SPM by a multi-head attention mechanism, which is inspired by (Lee et al. 2019). Consider a set $\\mathcal{H}$ of $n_{scn}$ scenario states, which are linearly transformed into $d$-dimensional embeddings $H \\in \\mathbb{R}^{n_{scn}\\times d}$. We expect to extract an embedding that represents the distribution of scenario states. To this end, we capture the interaction between scenarios using the multi-head attention. A straightforward way would be to apply the full self-attention $MHAB(H, H)$. However, self-attention complexity scales quadratically with $n_{scn}$, making it too expensive for a large value $n_{scn}$. Hence, we propose a trainable set of $m$ $d$-dimensional inducing point vectors $I \\in \\mathbb{R}^{m\\times d}$ to compute the cross-attention over $H \\in \\mathbb{R}^{n_{scn}\\times d}$, thereby maintaining a linear complexity with $n_{scn}$ and resulting in the embeddings $J \\in \\mathbb{R}^{m\\times d}$. We then apply the original set $H$ to perform cross-attention with $J$, to get $H' \\in \\mathbb{R}^{n_{scn}\\times d}$, which approximates $MHAB(H, H)$. To extract a single embedding of the stochasticity, we must apply permutation invariant aggregation, for which we use mean aggregation. Formally, our scenario processing module (SPM) can be expressed by:\n$SPM(H) = Avg(MHAB(H, MHAB(I, H))$\nSPM-DAN The DAN in (Wang et al. 2023) comprises $L$ operation message attention blocks (OMBs) and machine message attention blocks (MMBs), followed by the actor and critic network. Each OMB\u1d62 computes embeddings $h^{o}_{O_{ij}}$ of operations $O_{ij} \\in \\mathcal{O}_u$ by an attention mechanism:\n$\\left\\{h^{o}_{O_{ij}}|O_{ij} \\in \\mathcal{O}_u\\right\\} = OMB_l \\left(\\left\\{h^{l-1}_{O_{ij}}|O_{ij} \\in \\mathcal{O}_u\\right\\}\\right)$\nSimilarly, MMB\u1d62 computes embeddings $h^{m}_{M_k}$ for all machines $M_k \\in \\mathcal{M}_u$ through an attention mechanism by involving both the operation and machine features, such that:\n$\\left\\{h^{m}_{M_k}|M_k \\in \\mathcal{M}_u\\right\\} = MMB_l \\left(\\left\\{h^{l-1}_{M_k}|M_k \\in \\mathcal{M}_u\\right\\},\\left\\{h^{l-1}_{O_{ij}}|O_{ij} \\in \\mathcal{O}_u\\right\\}\\right)$\nThe actor network in the DAN is a multilayer perceptron (MLP) that calculates a score $\\mu_{(O_{ij},M_k)}$ for each action $(O_{ij}, M_k) \\in A$:\n$\\mu_{(O_{ij}, M_k)} = MLP \\left([h^{o(L)}_{O_{ij}};h^{m(L)}_{M_k};h^{om(L)}_{(O_{ij}, M_k)} ;\\sum_{O_{ij} \\in \\mathcal{O}_u}h^o_{O_{ij}}; \\sum_{M_k \\in \\mathcal{M}_u}h^m_{M_k}; h^{g(L)}_{(O_{ij}, M_k)}]\\right)$\nwhere $h^{g(L)}_{(O_{ij}, M_k)}$ is a graph embedding. Then, the DAN uses the softmax function over the scores to transform them into a probability distribution $\\pi_\\theta$, from which the action can be sampled. The critic network is an MLP for outputting a scalar value:\n$v = MLP(h^{g(L)})$"}, {"title": "Training Procedure", "content": "Following (Song et al. 2022) and (Wang et al. 2023), we train our policy by an actor-critic-based proximal policy optimization (PPO) algorithm (Schulman et al. 2017), shown in Algorithm 1. We extend the default PPO by keeping track of multiple independent scenarios per problem instance and applying the same actions. We compute the reward and state using these distinct scenarios and combine them into a single transition tuple reflecting all scenarios per problem instance per step. Hence, we maintain many scenarios, accommodating our state space to reflect the stochastic problem, without increasing the PPO batch sizes. The model is trained for $N_{ep}$ episodes. We sample batches of $n_\\beta$ problem instances, with $n_{scn}$ scenarios to compute the state and $n_{rew}$ scenarios for the reward after $N_s$ episodes. We validate on a fixed set of $N_{vali}$ instances every $N_{eval}$ episodes. As a common practice, we sample actions from the distribution $\\pi_\\theta$ for training and infer greedy solutions for validation."}, {"title": "Experiments", "content": "In this section, we numerically evaluate our method on various problem instances and compare the performance with several baseline methods.\nBaselines We use the first-in-first-out (FIFO), most-operations-remaining (MOR), shortest-processing-time (SPT), and most-work-remaining (MWKR) priority dispatching rules (PDRs). In addition, we evaluate the deterministic OR-tools CP-SAT solver (Perron, Didier, and Gay 2023) with the median processing times, using the implementation by (Reijnen et al. 2023). We also extend the CP-SAT formulation to optimize over multiple sampled scenarios simultaneously, which we name CP-stoch. For these methods, we produce a plan using the deterministic information (or the used scenarios for CP-stoch) and evaluate on $n_{rew}$ independent scenarios. We set a 1-hour time limit for the CP-SAT solver. Based on preliminary tests, we set the number of scenarios for CP-stoch to 25 for synthetic 10\u00d75 and 20\u00d75 instances, and 10 for the other. Lastly, we evaluate the deterministic DAN policies on stochastic instances and use DAN with our reward mechanism but without SPM (DAN-stoch).\nDatasets As is common, we use synthetic datasets for training and evaluation. We use SD1 from (Wang et al. 2023). However, SD\u2081 has processing times in the range $\\{1,..., 20\\}$. Since it is common to round to integers, the stochasticity is limited, especially for small numbers (e.g., 1 with 50% standard deviation will mostly be 1). We exclude SD2 from (Wang et al. 2023) as it has an unrealistic assumption of unrelated processing times $p_{ij}^k \\in \\{1, ..., 99\\}$ for an operation $O_{ij}$, drastically limiting the effects of stochasticity (e.g., $p_{ij}^k = 10$ will mostly be favored over $p_{ij}^k = 90$ regardless of the variance). Instead, we propose a more realistic SD3 for which we mimic the instance structure of SD2, but sample processing times using $p_{ij}^k \\sim \\mathcal{U}(1,99)$ for each $O_{ij}$, after which we sample $p_{ij}^k \\sim \\mathcal{U}(0.85p_{ij}^k, 1.15p_{ij}^k)$. We use instance sizes 10\u00d75, 20\u00d75, 15\u00d710, 20\u00d710, 30\u00d710, and 40\u00d710, with $n \\times m$ indicating $n$ jobs and $m$ machines.\nWe also evaluate our method using the mk (Brandimarte 1993), rdata, edata, and vdata instances (Hurink, Jurisch, and Thole 1994). Note, that the mk instances suffer from both limitations of SD1 and SD2.\nCreating Stochastic Instances We assume the processing times of the deterministic instances to be the median value. For each pair $(O_{ij}, M_k)$, we sample $CV \\sim \\mathcal{U}(0.1, 0.5)$ and set the standard deviation $\\sigma_{ij}^k = CV \\times median$, as different machine-operation pairs have various uncertainty levels. There is no agreement on the best probability distributions in the literature. Unless specified otherwise, we assume log-normal distributions like (Caldeira and Gnanavelbabu 2021).\nConfigurations For a fair comparison, we keep the same training configurations $N_{ep} = 1000$, $N_B = 20$, $N_s = 20$, $N_{eval} = 10$ as (Wang et al. 2023). We also keep the same hyperparameters for the OMB and MMB blocks, the critic network, and the PPO algorithm, for which we refer to their paper. We increase the hidden layer size of the actor network to 128 to facilitate the expanded size of the input features $h_{(O_{ij}, M_k)}$. Based on preliminary tests, we use 4 attention heads and dimensions $d = 32$ for the SPMs. We set the number of inducing points $m = 16$. For training, validation, and testing we set $N_{rew} = 1000$ and use $n_{scn} = 100$. For the synthetic data, we test 100 problem instances per instance type. We use an NVIDIA A100 GPU and 18-core Intel Xeon Platinum 8360Y CPU for DRL training and a 32-core AMD Rome 7H12 CPU for the CP-SAT solver. We train policies on SD1 and SD3 for the smallest four instance sizes, which takes between 1 and 14 hours. For inference, we use greedy solution construction and a sampling strategy with 100 samples per instance, conforming to the existing FJSP papers. Unless specified otherwise, we assume the $VaR_{95\\%}$ of the makespan as the objective. We evaluate this metric using $N_{rew} = 1000$ scenarios for each instance. We also report the inference time and the gap to the CP-SAT solutions, calculated as $gap = \\frac{f(C_{max})-f(C_{max})^{CP}}{f(C_{max})^{CP}}$. The reported values are the average for each set of instances."}, {"title": "Synthetic Datasets Results", "content": "Table 1 presents the performance of our methods using test instances generated from the same distributions as the training instances. These results show that our method consistently outperforms all PDRs. In addition, using sampling inference, our model outperforms the deterministic CP-SAT solver on all instance sets. The CP-stoch formulation is only competitive on the 10\u00d75 instances. In larger instances, the additional computational complexity outweighs the increased model accuracy. DAN and DAN-stoch are competitive with our model only on the smallest SD1 instances. For other instance sets, especially larger ones, our method performs considerably better. In our implementation, SPM-DAN adds runtime of roughly 20% for greedy inference and 3 times for sampling (3.14 vs. 2.63 and 10.50 vs. 3.80 seconds for SD3 20\u00d710 per instance), but is still relatively fast and leads to clear performance improvements. For example, greedy inference using SPM-DAN considerably outperforms sampling inference using the baseline models for 20\u00d710 instances, showcasing a better performance-runtime trade-off despite an initial runtime increase. The DAN-stoch models do not consistently outperform DAN, showcasing the value of synergizing our training procedure and SPM.\nTable 2 shows the generalization of our policies to larger instances, using the 20\u00d710 policies on 30\u00d710 and 40\u00d710 instances. We do not report CP-stoch, as memory limits are reached before finding sensible solutions for these instance sizes. In these larger instances, our method consistently and considerably outperforms all baselines, even achieving a 6% improvement over CP-SAT and DAN using greedy sampling. DAN-stoch also beats DAN in these instances but remains far from SPM-DAN performance. These results show that SPM-DAN can scale to new instance sizes without the need for retraining."}, {"title": "Public Datasets Results", "content": "We assess the cross-distribution performance of our method on the public benchmark datasets, shown in Table 3. We report the policy trained on 15\u00d710 SD3 instances, but policies trained on 10\u00d75 SD3, 15\u00d710 SD1, and 10\u00d75 SD\u2081 instances show similar results. We find that our method maintains performance across the benchmark instances. On the mk data, we see no performance improvement of our approach over the default DAN, caused by the previously mentioned instance limitations. On the other datasets, we maintain a clear performance improvement over the baseline neural methods and outperform CP-SAT up to 8% on vdata instances."}, {"title": "Flexibility to Different Objectives", "content": "We train and evaluate policies for optimizing the expected makespan to show that our method works for different objective functions. Table 4 shows the results of these policies for SD3 instances. We see a similar pattern as for the $VaR_{95\\%}$ where there is a limited effect for the smallest instances but our policies outperform the other methods toward larger problem sizes. The overall improvement is slightly less compared to the $VaR_{95\\%}$ objective. This is expected as the expected value and deterministic values tend to be more related to each other than the $VaR_{95\\%}$ and deterministic values as the $VaR_{95\\%}$ is more dependent on the distributional range of processing times. However, the improvements of our method are still considerable and for the larger instances we improve over CP-SAT, as well as the other baselines."}, {"title": "Resilience to Varying Probability Distributions", "content": "An important criterion for stochastic optimization methods is that they perform under different assumed probability distributions. Therefore, we create one instance set with beta distributions (B) similar to (Flores-G\u00f3mez, Borodin, and Dauz\u00e8re-P\u00e9r\u00e8s 2023), one set with a mixture of log-normal and beta distributions (LB), and one set with a mixture of log-normal, beta, and gamma distributions (LBG). The results are shown in Table 5. It is clear that our method maintains performance across all instance sets. We see that DAN-stoch struggles to outperform DAN, as it cannot capture the dynamics that it observes through the rewards. Oppositely, our SPM-DAN model is aware of the types of stochasticity, which helps it distinguish different stochastic states. As a result, SPM-DAN achieves a gap compared to CP-SAT of 6% and improves performance up to 5% over the other models."}, {"title": "Trade-Off Performance and Number of Scenarios", "content": "We assess how $n_{scn}$ affects the performance of SPM-DAN. To this end, we evaluate models trained with 100 (SPM-DAN) and 200 (SPM-DAN-200) scenarios using 50, 100, and 200 scenarios for inference. Table 6 shows that, at the cost of higher (lower) inference times, using a higher (lower) $n_{scn}$ to generate solutions generally improves (decreases) performance. The differences in performance gap are roughly between 0.5% and 1%. However, training using more scenarios does not per se lead to better performance, especially for smaller instances. Namely, SPM-DAN with 200 evaluation scenarios outperforms SPM-DAN-200 in some instance sets. In larger instances, the performance difference between SPM-DAN and SPM-DAN-200 is limited, although SPM-DAN-200 performs slightly better. Hence, computational requirements can be decreased by training with fewer scenarios at the cost of a limited potential performance loss. Then, in inference, $n_{scn}$ can be varied to trade off runtime and performance. The increased runtime of using a higher $n_{scn}$ is also limited by increasing parallelism."}]}