{"title": "ROTATED RUNTIME SMOOTH: TRAINING-FREE ACTIVATION SMOOTHER FOR ACCURATE INT4 INFERENCE", "authors": ["Ke Yi", "Zengke Liu", "Jianwei Zhang", "Chengyuan Li", "Tong Zhang", "Junyang Lin", "Jingren Zhou"], "abstract": "Large language models have demonstrated promising capabilities upon scaling up parameters. However, serving large language models incurs substantial computation and memory movement costs due to their large scale. Quantization methods have been employed to reduce service costs and latency. Nevertheless, outliers in activations hinder the development of INT4 weight-activation quantization. Existing approaches separate outliers and normal values into two matrices or migrate outliers from activations to weights, suffering from high latency or accuracy degradation. Based on observing activations from large language models, outliers can be classified into channel-wise and spike outliers. In this work, we propose Rotated Runtime Smooth (RRS), a plug-and-play activation smoother for quantization, consisting of Runtime Smooth and the Rotation operation. Runtime Smooth (RS) is introduced to eliminate channel-wise outliers by smoothing activations with channel-wise maximums during runtime. The Rotation operation can narrow the gap between spike outliers and normal values, alleviating the effect of victims caused by channel-wise smoothing. The proposed method outperforms the state-of-the-art method in the LLaMA and Qwen families and improves WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models have demonstrated promising capabilities as parameters are scaled up. However, serving large language models is plagued by the high cost of computation and memory movement due to their scale. Consequently, many quantization methods are applied to reduce size and gain throughput improvement. From the service perspective, quantization can be categorized as weight-only quantization and weight-activation quantization. The former can focus on compressing the model's weights and saving costs related to memory movement, which is suitable for the memory-bound decoding stage. The latter quantizes both weight and activation to low bits and utilizes low-bit matrix multiplication kernels to achieve speedup. However, the existence of outliers in activation stretches the quantization range, compressing the effective bits for normal values and thus hindering the development of low-bit weight-activation quantization.\nTo address outliers, previous works such as (Kim et al., 2023; Dettmers et al., 2022) separate outlier and normal values into two matrices. However, the implementation is not hardware-compatible and fails to expedite inference. To achieve acceleration and maintain accuracy under A8W8 quantization, SmoothQuant (Xiao et al., 2023) transfers appropriate outliers from activation to weight offline through channel-wise smoothing scales. Nevertheless, the offline smoothing scales would be ineffective when facing unmatched input, and the outlier sharing scheme makes weight difficult to quantify. The aforementioned reason impedes the implementation of SmoothQuant for A4W4 quantization. QuaRot (Ashkboos et al., 2024) utilizes the property that rotation can suppress outliers; hence, pairwise rotate the activation and weight with equivalent output. The rotated activation and weight tend to spread outliers internally, leading to effectiveness smoothness for A4W4 quantization. However, rotation cannot guarantee a smoother matrix, and a rotated matrix may still exhibit the shape of channel-wise outliers, as depicted in Figure 2. Therefore, deriving a more robust, accurate, and training-free scheme for INT4 inference remains an open challenge.\nBased on the observation, outliers could be categorized into channel-wise outliers and spike outliers, as shown in Figure 1. To handle channel-wise outliers, we propose Runtime Smooth. Firstly, weights and activations are reordered to gather outliers and normal values. Subsequently, we group up activations and smooth activations by dividing group-wise maximums. The later quantized smoothed activation, weight, and group-wise maximums are inputs for the fused GEMM kernel. The entire process incurs minimal overhead compared to the original A4W4 pipeline. However, the existence of spike outliers causes the effect of victims after channel-wise smoothing, as shown in Figure 1. To address both channel-wise outliers and spike outliers, we propose Rotated Runtime Smooth, where we rotate weights and activations following (Ashkboos et al., 2024) and apply Runtime Smooth on rotated activations. The spike outlier is spread along with its token, leading to a smoother token with consistent values. The consistent values are comparatively larger than the normal values, thereby serving as smoothing scales for Runtime Smooth. The genesis of victims is abnormal smoothing scales, and the consistent smoothing scales across channels prevent the existence of victims.\nTo evaluate the proposed method, we conducted experiments on LLaMA families, Qwen families, Mistral, etc. We validate the performance on the WikiText-2 perplexity and Common Sense QA benchmarks. On LLaMA3-70B, Rotated Runtime Smooth can gain perplexity improvement from 57.33 to 6.66 under A4W4 quantization compared with the state-of-the-art. We summarize our contributions as follows:\n\u2022 We comprehensively revisited the activation smoothing method for LLM quantization, concluding the reasons for success or failure under A4W4 quantization.\n\u2022 We propose Runtime Smooth, a plug-and-play component that eliminates channel-wise outliers of activation in runtime without migrating outliers to weights, bringing negligible overhead for INT4 matrix multiplication.\n\u2022 We propose Rotated Runtime Smooth to overcome the spike outliers and enhance the robustness for channel-wise outliers. A comprehensive evaluation validates the effectiveness of the proposed methods, which gain thorough improvement on various models for INT4 inference."}, {"title": "2 PRELIMINARIES", "content": "2.1 QUANTIZATION\nQuantization converts high-precision matrices into discrete elements with scaling factors, achieving a lower bit per element. The process of quantization can be expressed as $X_{INT} = \\lfloor \\frac{X}{\\alpha} \\rfloor$, $\\alpha = \\frac{max(|x|)}{2^{N-1}-1}$, where X represents the floating-point tensor and $\\alpha$ is the scaling factor. The existence of outliers stretches the scaling factor and leaves few effective bits for normal values. Dividing the matrix into groups for quantization can mitigate the effect of outliers. In previous literature, the per-tensor quantization considers the entire matrix as a group; the per-channel quantization assigns different scaling factors to each row, and the sub-channel quantization divides rows into fine-grained groups. Although fine-grained grouping can alleviate accuracy degradation, more scaling factors entail additional computation and storage costs. In this work, we adopt the per-channel scheme following (Xiao et al., 2023; Ashkboos et al., 2024; Liu et al., 2024) for INT4 quantization.\n2.2 CHANNEL-WISE SMOOTHING METHOD\nUnder the assumption that outliers persist in fixed channels of activations, SmoothQuant migrates the outliers by dividing the smoothing scale $s \\in R^{K}$, where k denotes channel dimension. For ensuring equivalence of output, s would be multiplied to weight; the process can be described as $Y = (Xdiag(s)^{-1})(diag(s)W^{T}) = XW^{T}$. The smoothing scales are computed as $s_{j} = max(|x_{j}|)/max(|W_{j}|)^{1-\\alpha}, j = 1,2,..., K$, to fairly share the outliers between weights and activations. Since weights are mostly quantized offline (Frantar et al., 2022), directly multiplying s during runtime would undermine the quantization property. Therefore, s is pre-computed using a calibration set and merged into weights before quantization.\nAlthough SmoothQuant is effective under the A8W8 scheme, it fails for INT4 inference in three respects, as shown in Figure 1. Firstly, the smoothing scales depending on the calibration set are prone to being unmatched with online activations; hence, they cannot smooth outliers. Secondly, the outlier channels of activation are not eliminated but partially migrated to weights, thus leading to failure in low-bit quantization. Thirdly, outliers are not always channel-wise consistent, where spike outliers exist, and normal values are pruned as 'victim' after smoothing."}, {"title": "2.3 ROTATION-BASED METHOD", "content": "A rotation matrix is an orthogonal matrix R satisfied $RR^{T} = 1$ and $|R| = 1$. Quip (Tseng et al., 2024) showed that multiplying a weight matrix on the left and right by an orthogonal matrix can theoretically alleviate outliers, making matrices easier to quantize. QuaRot (Ashkboos et al., 2024), employs a similar technique by multiplying weight or activations by only one rotation matrix, maintaining an equivalent output as depicted in Figure 2 (a). However, multiplying one rotation matrix could not theoretically guarantee a smoother weight or activation. To explain the success of the rotation-based method, we computed the probability that the activation of different models becomes less smooth after rotation, as illustrated in Figure 2 (b). Following previous works, we measure the smoothness as $\\mu = absmax(t)/RMS(t)$, where t denotes one token in activation, and RMS denotes root mean square. Rotating activation from LLMs consistently exhibits a low probability of being less smooth compared with rotating a random matrix. However, having a chance to be less smooth is a potential trouble. On the other hand, activations with channel-wise outliers can be viewed as a collection of vectors with the same direction. From the rotation property, the rotated activation maintains the channel-wise consistency, as shown in Figure 2 (c), leaving space for better smooth."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce Runtime Smooth to eliminate channel-wise outliers (3.1) and how to implement it efficiently with kernel fusion (3.2). To comprehensively eliminate outliers, we propose Rotated Runtime Smooth, which addresses the effect of the victim and sub-smoothness of rotated activation 3.3."}, {"title": "3.1 RUNTIME SMOOTH", "content": "Challenges for the smoothing-based method under INT4 inference are discussed in Section 2.2. One intuitive way to mitigate this challenge is to obtain smoothing scale s in runtime and not merge s into weights. The process can be formulated as:\n$s_{j} = max(|X_{j}), j = 1, 2, ..., K$ (1)\n$\\tilde{X} = Quantize(X/s), \\tilde{W} = Quantize(W),$ (2)\n$Y = \\sum_{j=1}^{K} \\tilde{X_{j}}\\tilde{W_{j}}^{T}s_{j},$ (3)\nwhere $s \\in R^{1\\times K}$ denotes the runtime smoothing scale, $X \\in R^{N \\times K}$ denotes activations, $W \\in R^{M \\times K}$ denotes weights. We conducted an ablation study with LLaMA3-8B and the WikiText-2 dataset to understand better the effect of unmatched scale and outlier shared scheme. As shown in Figure 3, merely applying the runtime smoothing scale could not make A4W4 feasible, whereas Runtime Smooth does, echoing the importance of not migrating outliers to weights. To avoid the effect of quantization error from weight, we further apply the A4W16 setting. The perplexity improvement, from 4e2 to 10.9, validates the effectiveness of adopting the runtime scale."}, {"title": "3.2 RUNTIME SMOOTH WITH KERNEL FUSION", "content": "However, the naive implementation cannot integrated into the GEMM pipeline. A GEMM kernel splits the input matrix into blocks by columns and conducts parallel block computation, where the inconsistent smoothing scale would make the block-wise computation complex. Intuitively, if the smoothing scale is the same within a block, equation 1 can be deduced to $Y = \\sum_{j=1}^{K}X_{j}W_{j}^{T}$. The overhead would be minimized due to fewer multiplication operations. Roughly altering the"}, {"title": "3.3 ROTATED RUNTIME SMOOTH", "content": "In this section, we propose Rotated Runtime Smooth (RRS) to comprehensively eliminate outliers, including both channel-wise and spike outliers. Here, the activations are rotated and subsequently applied Runtime Smooth.\nFor activations with channel-wise outliers, it maintains the channel-wise consistency after rotation due to the rotation property, as shown in Figure 5. Moreover, the rotated activations have a chance to raise the level of outliers with a small probability. Compared with the pure rotation method, RRS comprehensively smooths channel-wise, leading to low quantization error. Compared with Runtime Smooth, RRS is more robust to activations with tiny spikes in the practical scenario.\nAs discussed in Section 2.2, the existence of spike outlier is the bottleneck of Runtime Smooth. Figure 5 shows that the spike outliers are spread within tokens after rotation. Hence, the smoothing scales are more consistent across channels, leaving fewer victims since all elements are divided by such a consistent scale. The process can be described as:\n$R = \\frac{1}{\\sqrt{K}}[C_{i,j}]_{KxK}, t = [\\epsilon, \\cdots, \\epsilon, O_{i}, \\epsilon, \\cdots, \\epsilon],$ (4)\n$t_{rotation} = t \\cdot R = \\frac{1}{\\sqrt{K}}=[C_{i,1}O_{i} + \\epsilon, C_{i,2}O_{i} + \\epsilon, \\cdots, C_{i,K}O_{i} + \\epsilon],$\nsmooth_scale = max(|trotation|) \u2248 $\\frac{1}{\\sqrt{K}}[|O_i|, |O_i|, \\cdots, |O_i|],$ (4)\nwhere the $O$ denotes spike outliers and $\u03b5$ denotes normal values. $S_{i,j} \u2208 {\u22121,+1}$ denotes the elements in Hadamard rotation matrix. In a practical scenario, the normal values $\u03b5$ are relatively small compared with spike outliers $O$; hence, they are omitted in the smoothing scale as shown in Equation 4. The genesis of victims is the abnormal smoothing scale; hence a more consistent smoothing scale brought by RRS frees 'victims'. Here, we only discuss the scenario where there is only one spike in the token, and we conduct a comprehensive analysis of multiple spikes for real scenarios in Section A.1.\nWe apply RRS to Linear Layers in Transformer blocks. We first offline rotate the weight matrix and insert online rotation operation before output and down projectors following previous works (Ashkboos et al., 2024). The rotated weights are quantized offline with GPTQ (Frantar et al., 2022). During inference, we perform Runtime Smooth on the rotated activations of linear layers and apply activation quantization subsequently."}, {"title": "4 EXPERIMENTS", "content": "4.1 SETTINGS\nWe conduct experiments on mainstream LLMs, including LLaMA families (Touvron et al., 2023b) (LLaMA2-13B, LLaMA2-70B, LLaMA3-8B, LLaMA3-70B, LLaMA3.1-8B, LLaMA3.1-70B), Qwen families (Yang et al., 2024) (Qwen1.5-7B, Qwen1.5-14B), Mistral (Jiang et al., 2023) and Mixtral (Jiang et al., 2024). Activation quantization employs per-channel symmetric scheme with round-to-nearest (RTN) strategy. KV cache quantization employs sub-channel symmetric scheme with group size 128 and round-to-nearest (RTN) strategy. In most cases, weight quantization employ per-channel symmetric scheme with GPTQ (Frantar et al., 2022) strategy, except for baseline 'RTN'. We apply standard GPTQ settings by using 128 samples from WikiText-2 with a sequence length of 2048 as the calibration set. We evaluate the performance of the models on WikiText-2 perplexity and zero-shot Common Sense QA benchmarks. The Common Sense QA benchmarks include ARC-e, ARC-c (Clark et al., 2018), BoolQ (Clark et al., 2019), and OBQA (Mihaylov et al., 2018).\n4.2 MAIN RESULT\nRuntime Smooth emphasizes activation smoothing for INT4 inference. The plug-and-play Runtime Smooth operators are employed before activation quantization. We conduct a comparison between Runtime Smooth and SmoothQuant (Xiao et al., 2023), demonstrating promising improvement. For instance LLaMA2-70B: 1e2 -> 6.95, LLaMA3-8B: 8e2 -> 10.47, Qwen1.5-7B: 3e2 ->13.32 under the A4W4KV16 scheme, as shown in Table 1. For certain models like LLaMA3-70B and LLaMA3.1-70B, both Runtime Smooth and SmoothQuant fail for the difficulty of weight quantization. To eliminate the influence of quantization error from weights, we carry out experiments under A4W16KV16 settings. Under the activation-only quantization setting, Runtime Smooth consistently outperforms SmoothQuant and achieves 40x improvement on LLaMA3-8B, validating the effectiveness of Runtime Smooth. Here the group size of the smoothing scale is 1 to observe the upper bound performance.\nTo further narrow the accuracy gap between INT4 inference and full precision inference, we propose Rotated Runtime Smooth. Here, the group size of the smoothing scale is set to 128, which is identical"}, {"title": "4.3 COMPARISON WITH TRAINING-BASED METHOD", "content": "SpinQuant (Liu et al., 2024) suggests that diverse Rotation matrices exhibit variations in their impact of smoothing. Consequently, it substitutes the origin fix Rotation matrix with a trainable Rotation matrix and trains the rotated network. The training process is time-consuming, taking 1.5 hours for a 7B model on one A100 GPU and 12 hours for 70B models on eight A100 GPUs. We re-implement SpinQuant and compare it with our method as shown in Table 3. It is noteworthy that SpinQuant applies asymmetric quantization to activation and KV cache. We experiment with our method under the same settings. The result reveals that the training-based method degrades WikiText-2 perplexity compared to the training-free method. This suggests that the current technique for optimizing the rotation matrix still has room for improvement."}, {"title": "4.4 ABLATION STUDY", "content": "We conduct an ablation study on the group size of runtime smoothing scale for Mistral, Qwen1.5-7B, and LLaMA3.1-8B, as presented in 4. Runtime Smooth can effectively minimize the gap between A4W4 and full precision through a subtle group strategy. However, the accuracy deteriorates as the group size increases. Rotated Runtime Smooth employs the rotation technique to minimize the gap between outliers and normal values, thereby being robust to the coarse group scheme and enabling the implementation of the fused kernel. It should be noted that in Qwen1.5-7B, the size of the input activation for Down_projector is 11008, which does not support a group size of 512."}, {"title": "4.5 EFFICIENCY EVALUATION", "content": "We evaluate the GEMM kernel fused with Runtime Smooth on NVBench (NVIDIA, 2024) with RTX 4070 Ti, as shown in Figure 6. The group size of the smoothing scale is 128, the same as the block size of the GEMM kernel. We implement Per-Channel A4W4 and Sub-Channel A4W4 as baselines. Compared with Per-Channel A4W4, Runtime Smooth fused Kernel brings limited overhead, including the movement of the smoothing scale (shape of [1, K]) and a multiplication between matrix and scalar. Sub-Channel A4W4 brings noticeable overhead, including the movement of group-wise quantization scale (shape of [N, L] and [M, L]) and multiplication between matrices. Hence, across different batch sizes and hidden dimensions, Runtime Smooth fused Kernel brings negligible overhead compared with A4W4 Per-Channel quantization, which is also the setting of QuaRot and SpinQuant."}, {"title": "5 RELATED WORK", "content": "5.1 LARGE LANGUAGE MODELS.\nPre-trained language models have achieved remarkable progress through scaling. Open-source large language models (LLMs) can reach up to 405 billion parameters and offer promising few-shot/zero-shot results. The mainstream LLMs (Yang et al., 2024; Touvron et al., 2023a; DeepSeek-AI et al., 2024; Jiang et al., 2023) continuously provide models with large scales and enhanced capabilities. However, serving large language models for inference becomes costly and challenging as the models expand.\n5.2 MODEL QUANTIZATION.\nQuantization represents an effective approach for reducing model size and expediting inference. From a serving perspective, quantization can be classified into weight-only quantization and weight-activation quantization. Weight-only quantization compression employs low-bit representations for weight matrices, thereby saving memory movement in memory-bound scenarios, specifically the decoding stage. GPTQ (Frantar et al., 2022) used 4-bit to quantize the weight based on the approximate second-order information. SqueezeLLM (Kim et al., 2023) handled outliers through non-uniform quantization and used a sparse format to keep outliers and sensitive weights at high precision. AWQ (Lin et al., 2023) further advanced accuracy by preserving salient weights. QFA (Yi et al., 2024) fine-tune a supernetwork encompassing multiple mixed precision configurations and efficiently offer high-performance sub-networks for diverse scenarios. QuiP (Chee et al., 2024; Tseng et al., 2024) successfully represents weights using 2 bits via an adaptive rounding method. Weight-activation quantization can further accelerate computation by leveraging a low-bit GEMM kernel suitable for compute-bound scenarios, namely the pre-filling stage. SmoothQuant (Xiao et al., 2023) pushes the quantization scheme to A8W8, achieving speed up and maintaining accuracy. Qserve (Lin et al., 2024) further implement the A8W4KV4 and better accelerate. Among weight-activation quantization methods, the existence of outliers presents the most formidable problem, as it can result in substantial drops in accuracy.\n5.3 OUTLIERS CHALLENGE.\nOutliers can expand the quantization range and compress the information intensity for normal values. LLM.int8() (Dettmers et al., 2022) employs mixed INT8/FP16 decomposition to handle activation outliers. Nevertheless, such an implementation results in significant latency overhead and can even be slower than FP16 inference. Subsequent work (Yuan et al., 2023) rearranges the channels to reduce the variance within one quantization group, further enhancing accuracy. Atom (Zhao et al., 2024) integrates the reorder technique and mixed INT4/INT8 precision to maintain accuracy and accelerate compared to the FP16 baseline. In addition to the reorder and mixed-precision techniques, SmoothQuant (Xiao et al., 2023) exchanges outliers between weights and activations to find an optimal point that shares appropriate outliers in weights and activations, achieving A8W8 inference with minimal accuracy degradation. However, the outlier sharing scheme may not be suitable for the extreme A4W4 setting. To further smooth outliers, QuaRot (Ashkboos et al., 2024) pairwise rotates the activation and weight to suppress outliers and maintain output equalization, enabling INT4 inference with well-smoothed activations."}, {"title": "6 CONCLUSION", "content": "This work presents Rotated Runtime Smooth, a plug-and-play runtime activation smoother that facilitates INT4 inference. In Rotated Runtime Smooth, Runtime Smooth effectively eliminates channel-wise outliers. Additionally, rotation operations migrate the negative impact from spike outliers. Rotated Runtime Smooth can be easily implemented with negligible overhead and is generalized across various large language models (LLMs). Through comprehensive elimination of channel-wise and spike outliers, Rotated Runtime Smooth achieves significant enhancement compared to the prior channel-wise smoothing approach and outperforms state-of-the-art methods on diverse models for INT4 inference."}, {"title": "A OUTLIER", "content": "A.1 ACTIVATION WITH MULTIPLE SPIKE OUTLIERS\nThis section further analyzes how tokens with multiple outliers behave after rotation. The token with multiple outliers is defined as:\n$t = [\u2026., O_{i1}, \u2026, O_{i2}, \u2026, O_{il}, \u2026]$, (5)\nwhere $l$ denotes the number of outliers, {$i_1, i_2, \u2026i_l$} denotes the index of outliers. In this work, we set the Hadamard matrix as the rotation matrix $R = \\frac{1}{\\sqrt{K}}[S_{i,j}]_{KxK}$, where $S_{i,j} \u2208 {\u22121,+1}$. The rotated token can be described as:\n$t^{rot} = t \\cdot R$\n[\u2026., i1, \u2026., i2, \u2026., il ]\u00b7(R1,\u2026.Rl,)\n[K\\sum_{d=1}^{l} S_{id,1}O_{id},\u2026,\\sum_{d=1}^{l} S_{id,k}O_{id}]$\n$\\frac{1}{\\sqrt{K}}[\\sum_{d=1}^{l} S_{id,1}O_{id},\u2026,\\sum_{d=1}^{l} S_{id,k}O_{id}]$\n(6)\n(7)\nThe construction of $t^{rot}$ can be viewed as contributions of outliers, where outliers are canceled out by each other or stacked to enlarge. The effect of victims refers to the smoothness of normal tokens after smoothing. The process is defined as:\nx = [1, 1, 1, ..., 1] (8)\n$\\text{scale} = [\\text{absmax}(1, \\frac{1}{\\sqrt{K}} \\sum_{d=1}^{l} S_{id,1}O_{id}), ..., \\text{absmax}(1, \\frac{1}{\\sqrt{K}} \\sum_{d=1}^{l} S_{id,k}O_{id})]$ (9)\nwhere we assume normal tokens are filled up with 1. The effect of victims can be qualified with the equation:\n$X_{smooth} = \\frac{1}{\\text{scale}}, u = \\text{max}(|x_{smooth}|)/RMS(|x_{smooth}|)$ (10)\nTo measure the effect of victims in the actual scenario, we first collect the activations from LLaMA3-8B. For activations from the Down Projector, spike outliers are 1000x larger than the medium value, as shown in Figure 7, where outliers in a channel-wise manner are not overly large.\nTo analyze the effect of smoothing rotated spike outliers, we apply the Monte Carlo approach by generating a token from the Gaussian distribution and inserting spike outliers according to statistics of spike outliers, then rotating, smoothing, and calculating u as shown in Figure 8. Rotated tokens with multiple outliers are up and down across channels due to the effect of offset and stack. On the other hand, we can stack rotated tokens to obtain a consistent large scale across channels. As shown in Figure 8, normal tokens after smoothing are mostly easy to quantify but contrary when two outlier tokens are exhibited in one activation. The reason is that two tokens cannot cover the whole channel, where more stacked tokens can lead to lower u. Notably, the case with only two outlier tokens is rare but could potentially trouble Rotated Runtime Smooth.\nA.2 ANALYSIS EXTENT OF OUTLIER REMOVAL FOR DIFFERENT METHOD\nWe conduct experiments on mainstream models with different outlier smoothing approaches to analyze smoothness integrally rather than simulating with manual spike outliers. Specifically, we collect activations with models evaluated by WikiText-2 and apply different smoothing approaches. To measure the level of outliers; we set \u00b5 = absmax(t)/||t||2, where t denotes one token. Figure 9 shows the specific impact of approaches on outliers on different LLM's components. For QKV_Projector, UP_Projector, and Gate_project, the activations are channel-wise consistent; hence, Runtime Smooth outperforms rotation, where pure rotated activations are sub-smooth. For Down_Projector, the intermediate activations contain spike outliers due to SwiGLU (Shazeer, 2020) functions; hence, Runtime Smooth suffers from the effect of the victim and fails to smooth. Rotated Runtime Smooth solves two kinds of outliers and consistently outperforms other approaches."}]}