{"title": "Graph-Guided Test-Time Adaptation for\nGlaucoma Diagnosis using Fundus Photography", "authors": ["Qian Zeng", "Fan Zhang"], "abstract": "Glaucoma is a leading cause of irreversible blindness world-wide. While deep learning approaches using fundus images have largely improved early diagnosis of glaucoma, variations in images from different devices and locations (known as domain shifts) challenge the use of pre-trained models in real-world settings. To address this, we propose a novel Graph-guided Test-Time Adaptation (GTTA) framework to generalize glaucoma diagnosis models to unseen test environments. GTTA integrates the topological information of fundus images into the model training, enhancing the model's transferability and reducing the risk of learning spurious correlation. During inference, GTTA introduces a novel test-time training objective to make the source-trained classifier progressively adapt to target patterns with reliable class conditional estimation and consistency regularization. Experiments on cross-domain glaucoma diagnosis benchmarks demonstrate the superiority of the overall framework and individual components under different backbone networks.", "sections": [{"title": "Introduction", "content": "Glaucoma, a chronic progressive optic neuropathy due to high intraocular pressure, is a leading factor in irreversible blindness [29]. Early screening and detection play a crucial role in facilitating prompt treatment to prevent continual vision loss [27]. Color fundus photography (CFP) is one of the most viable non-invasive means of examining the retina for glaucoma diagnosis [1]. Recent advances in deep learning have largely automated the diagnosis of glaucoma with CFP images, accelerating the diagnosis process, reducing assessment costs, and lowering the burden on healthcare systems [33]. To date, deep networks, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have greatly enhanced both the accuracy and efficiency of glaucoma diagnosis, showing great potential as alternatives to conventional diagnostic approaches using intraocular pressure measurement and visual field testing [33,6,8,12,10].\nDespite the notable performance on in-distribution data, the distribution shift of CFP images from various imaging devices across different sites presents"}, {"title": "Method", "content": "Fig. 1 provides an overview of the proposed GTTA. During training phase, to learn generalizable deep representations from source site data, we design a Graph-assisted Relation-aware Training (GRT) module that learns the topological relations based on hidden features extracted by backbone models. During inference phase, we introduce a novel TTA solution, namely Test-time Prediction Disambiguation (TPD), to adapt the source-trained decision boundaries to unforeseen target circumstances, mitigating the potential semantic misalignment."}, {"title": "Graph-assisted Relation-aware Training", "content": "Classic deep learning models such as CNNs are incapable of explicitly modeling the high-order relational dependencies of different regions within a CFP image. Vision Transformers (ViTs) [14], which can be seen as spatial attention on a fully-connected graph, mitigate this challenge to some extent but may introduce noisy relations, e.g., connecting two irrelated local regions. To address this issue, we introduce a novel relation graph module that utilizes backbone features to model the topological relationships among various regions. Such relation graphs can flexibly and efficiently model more local and sparse dependency relations.\nGraph construction. Formally, the relation graph is denoted by $G = {V, E}$, where $V$ and $E$ represent graph nodes and edges, respectively. First of all, GRT receives hidden features extracted by the backbone feature extractor and divides them into a set of vectors, where each vector corresponds to a certain region of the input image. (See Sec. 3.2 for the tested backbone networks: for CNN, a vector represents a pixel in the feature map, and for ViT and MLP, it stands for the embedding of a patch.) Then, these vectors are linearly projected into an embedding space of dimension $F$. We further add a learnable 1D positional embedding to these feature vectors, denoted as $\\mathbf{h} = {\\mathbf{h}\\_1,\\mathbf{h}\\_2,..., \\mathbf{h}\\_N}$, $\\mathbf{h}\\_i \\in \\mathbb{R}^F$, where $N$ is the number of the nodes. Regarding the graph edge, instead"}, {"title": null, "content": "of using prior knowledge, such as Euclidean distance and cosine similarity, for determining the node adjacency matrix, we propose a flexible graph structure by using a learnable weight matrix, $W \\in \\mathbb{R}^{F \\times F}$, to discern the relationships between nodes and thereby form the edges, $V = \\mathbf{h}W\\mathbf{h}^T, V \\in \\mathbb{R}^{N \\times N}$.\nGraph learning. To update the node feature based on $G$, we resort the graph self-attention mechanism [32], which takes node features $\\mathbf{h}$ and a learnable edge matrix $V$ as inputs. The nodes updated by the layer can be represented as $\\mathbf{h}^{\\prime} = {\\mathbf{h}\\_1^{\\prime}, \\mathbf{h}\\_2^{\\prime},..., \\mathbf{h}\\_N^{\\prime}}$, $\\mathbf{h}\\_i \\in \\mathbb{R}^{F^{\\prime}}$, where $F^{\\prime}$ is the dimension of the higher-level output features. After aggregating the topological structure, we reduce the number of parameters and eliminate non-essential node representation by employing top-k pooling for node downsampling:\n$$\\mathbf{h}^{\\prime \\prime} = {\\mathbf{h}^{\\prime} \\in \\mathbf{h}^{\\prime} | \\sigma (\\alpha \\mathbf{h}^{\\prime}) \\geq \\alpha\\_k },$$\nwhere $\\mathbf{h}^{\\prime \\prime}$ is the output feature set from top-k pooling, $\\alpha \\in \\mathbb{R}^{F^{\\prime}}$ represents a trainable weight vector, $\\sigma$ denotes an activation function such as the sigmoid function, and $\\sigma (\\alpha \\mathbf{h}^{\\prime})$ is referred to as the score. The value $\\alpha\\_k$ indicates the $k$-th largest score computed across all output features. The filtered features are then flattened and fed into a linear classifier to obtain the GRT's prediction value $p\\_{GRT}$. Finally, after acquiring the GRT prediction value $p\\_{GRT}$ and the backbone prediction value $p\\_B$, the GRT and backbone engage in mutual learning, and the final loss is defined as:\n$$\\mathcal{L}\\_{GRT} = CE(p\\_B, y) + \\lambda \\cdot CE(p\\_{GRT}, y) + (1 - \\lambda) \\cdot CE(p\\_{GRT}, p\\_B),$$\n$$\\hat{c} = \\mathop{\\mathrm{argmax}}\\_{c} p\\_B(c | x) = k$$\nwhere $CE$ denotes the standard cross-entropy loss, $y$ is the ground truth, and $\\lambda$ is a hyperparameter to balance the losses."}, {"title": "Test-Time Prediction Disambiguation", "content": "In practice, test data may not be all pre-prepared but rather continuously added. Thus, the basic idea of TPD is to update the weights of the source model using unlabeled target data in an online manner. The trained source model can be decomposed into a linear classifier $g\\_w$ and a feature extractor $f\\_\\theta$, where $w$ and $\\theta$ denote the parameters.\nMemory bank. First of all, we introduce a memory bank to dynamically store the target data features and logits, denoted by $S\\_t = {s\\_1^t, s\\_2^t,\u2026\u2026\u2026,s\\_K^t }$, where $K$ is the number of classes and $t$ denotes the time step. Following [17], at time $t = 0$, we initialize the memory bank with the L2 norm of the linear classifier's weights. Given an input sample $x$, at time $t$, $S\\_t$ is updated as follows:\n$$s\\_k^t = \\begin{cases} S\\_k^{t-1} \\cup {f\\_{\\theta}(x\\_t)} & \\text{if } \\mathop{\\mathrm{argmax}}\\_c p\\_c = k \\\\ S\\_k^{t-1} & \\text{else,} \\end{cases}$$"}, {"title": null, "content": "where $p\\_k$ denotes the probability of the linear classifier's prediction being class $k$. Compared to training-free strategies [17] that relies on the predictive power of the source-trained model to make prediction for target samples, we directly utilize the constructed $S\\_t$ to memorize reliable samples for model updating with their pseudo labels. By doing so, TPD can alleviate the source-bias and error accumulation problems resulted from the distribution shift between training and test data.\nClass conditional estimation. To update the model parameters, we estimate the class conditionals of unlabeled target data using the memory bank, rather than simply relying on the classifier's output as pseudo labels for self-training. Specifically, we identify the nearest neighbors $\\mathcal{N}(x)$ of the input test data within $S\\_t$:\n$$\\mathcal{N}(x) := {z \\in S | d (f\\_{\\theta}(x), z) \\leq \\eta },$$\nwhere $z$ is the L2-normalized target sample embedding (i.e., $f\\_{\\theta}(x)$), $d$ is the distance function, and $\\eta$ represents the distance between $x$ and the $n$-th closest neighbor. Here, we opt for cosine similarity to compute the distance between $x$ and its neighbors in $S\\_t$. Once identifying the neighbors of the input target data, we utilize a Parallel Linear Module (PLM), denoted as $h\\_{\\phi}(z) = \\sum\\_{i=1}^N h\\_{\\phi\\_i}(z)$ to obtain the centroid of each class in the embedding space of $f\\_{\\theta} h\\_{\\phi\\_i}$, where $h\\_{\\phi\\_i}$ is the $i$-th linear module. The centroid can then be represented as:\n$$\\mathbb{M}\\_i = {\\mu\\_{i,1}, \\mu\\_{i,2},..., \\mu\\_{i,K} }, \\quad \\mu\\_{i,k} = \\frac{1}{|S\\_k|} \\sum\\_{z \\in S\\_k} h\\_{\\phi}(z)$$\nUsing the class centroids computed by the prototype classifier, we can predict the corresponding class conditionals for the neighbors of the target data and obtain the average pseudo label integrated neighbor information:\n$$\\text{proto}(k | z) := \\frac{\\exp (-d (h\\_{\\phi}(z), \\mu\\_{i,k}) /\\tau)}{\\sum\\_c \\exp(-d (h\\_{\\phi}(z), \\mu\\_{i,c}) /\\tau)},$$\n$$\\hat{p}\\_i(k | x) := \\frac{1}{n} \\sum\\_{z \\in \\mathcal{N}(x)} \\mathop{\\mathrm{argmax}}\\_c \\text{proto} (c | z) = k,$$\nwhere $\\tau$ is the softmax temperature coefficient. Similarly, we use the prototype classifier to obtain the logits for the current sample, denoted as $\\text{proto} (k | x)$.\nTest-time training objective. To enhance the robustness of the classifier's prediction, we introduce an Entropy Punishment Divergence to assign larger weight to predictions with lower confidence:\n$$D\\_{EP}(p(x)||\\hat{p}) = \\text{softmax}\\_B (H(x)/\\tau) \\cdot D\\_{KL}(p(x)||\\hat{p}),$$\nwhere $B$ is the batch size, $H(x) = - \\sum\\_{k=1}^K p(k | x) \\log (p(k | x))$, and $D\\_{KL}$ denotes the Kullback-Leibler (KL) divergence. Finally, we fine-tune the linear classifier with the following test-time training objective:\n$$\\mathcal{L}\\_{TTT} = D\\_{EP} [p\\_{\\text{proto}} (x)|||hat{p}\\_i(x)] + \\lambda\\_1 \\cdot D\\_{EP} [g\\_w (f\\_{\\theta}(x)) |||hat{p}\\_i(x)] + \\lambda\\_2 \\cdot D\\_{KL} [g\\_w (f\\_{\\theta}(x)) || p\\_{\\text{proto}}(x)],$$"}, {"title": "Experimental Setup", "content": "Datasets. We evaluate our method on the Standardized Multi-Channel Dataset for Glaucoma (SMDG-19) [20], which is the largest public database of multi-site fundus images with glaucoma. The SMDG-19 comprises 4,817 glaucoma and 7,499 non-glaucoma images from 15 sites. Since the domain gap is small for data from REFUGE1, we exclude them from our benchmark. Additionally, due to the similar image style and the absence of category, we amalgamate CRFO-v4, JSIEC-1000, and LES-AV into a single target domain dataset, referred to as CJL; similarly, DR-HAGIS, HRF, and PAPILA are merged into a single target domain dataset, denoted as DHP. Fig. 2 presents sample CFP images from the different sites, where we can observe notable domain-specific styles characterized by variations in color, brightness, and contrast. Tab. 1 shows the statistics of source and target data. All fundus images are preprocessed involving background cropping, centering, padding for missing areas, and resizing to 512x512 pixels.\nImplementation Details. Our model is implemented using the PyTorch deep learning framework. During training, the batch size is 16, with an initial learning rate of 1 \u00d7 10-4, which is decreased by an order of magnitude every five epochs, and $\\lambda$ in Eq. (2) is set to 0.5. During testing, we freeze the feature extractor and the batch size is 32. $n$ in Eq. (4) is set to 8. The learning rate for the PLM is set to 1 \u00d7 10-3. $\\lambda\\_1$ and $\\lambda\\_2$ in Eq. (8) are all set to 1."}, {"title": "Experimental Results", "content": "We performed two experiments: 1) an ablation study to evaluate the performance of the proposed GRT and TPD, and 2) a comparison with SOTA TTA methods. We implemented three backbone networks: ResNet-18 [15], ViT-B/16 [7], and gMLP-S [23]. For each network, we tested multiple architectures and selected the best-performing one. We employed ImageNet pre-trained backbones, as networks without pretraining underperformed due to domain shift.\nAblation Study. We conducted ablation studies to demonstrate the role of GRT and TPD. As shown in Tab. 2, the combined use of GRT and TPD enhances performance in target domains beyond what is achieved when using either method alone or neither. Compared to the vanilla backbone network, our GTTA framework achieves a 17.34% increase in accuracy on ResNet-18, a 12.40% increase in accuracy on ViT-B/16, and an 11.02% increase in accuracy on gMLP-S. Across the different backbones, ResNet-18 yielded the best performance, as GRT benefits from the deeper hidden features when using ResNet-18.\nComparison with SOTA Methods. As shown in Tab. 3, we compared our TPD with state-of-the-art TTA methods: 1) TentClf [34], fine-tunes the linear classifier through entropy minimization to obtain high-confidence predictions on the target domain; 2) PLClf [21], fine-tunes the linear classifier by high-confidence pseudo labels; 3) MEMO [38] enhances the source model's robustness on the target domain through data augmentation and introduces marginal entropy minimization for fine-tuning the source model. 4) T3A [17] creates support sets to adjust class prototypes for training-free adaptation. 5) TAST [18] leverages neighbor information and fine-tunes an adaptation module to update class prototypes. Here we only performed TTA on the backbone and GRT was frozen. Experiments show the proposed TPD achieves an average improvement of 6.06% in accuracy and 4.85% in AUC across three backbones networks (source-trained with GRT), largely outperforming other TTA methods and exhibiting higher robustness across various domains. These results highlight the unique contribution of our TTA solution, revealing its superior ability to adapt the source classifier to the test environment compared to previous state-of-the-art TTA approaches."}, {"title": null, "content": "Grad-CAM Visualization. The results are displayed in Fig. 3. The third row clearly verifies that the proposed GRT is capable of assisting the backbone network to pay more attention to the optic nerve head area. While the second row demonstrate that the vanilla Resnet-18 is easy to make spurious correlations with the periphery, particularly in images with significant domain shift, such as strong contrast and low brightness. This result further support the effectiveness of GRT as the nerve head area, including the optic cup, optic disc, and retina nerve fibers, are all important diagnostic criteria for glaucoma in clinical practice [29]."}, {"title": "Conclusion", "content": "We propose GTTA for automated glaucoma diagnosis using CFP images. GTTA in-tegrates topological information inherent in CFP images into the source model"}, {"title": null, "content": "training and enables prediction disambiguation through a test-time training objective during inference phase. To the best of our knowledge, this work represents the first application of TTA for automated glaucoma diagnosis. Moreover, our method can be seamlessly extended to other medical scenarios with distribution shifts, enhancing their transferability in unknown environments."}]}