{"title": "Graph-Guided Test-Time Adaptation for\nGlaucoma Diagnosis using Fundus Photography", "authors": ["Qian Zeng", "Fan Zhang"], "abstract": "Glaucoma is a leading cause of irreversible blindness world-\nwide. While deep learning approaches using fundus images have largely\nimproved early diagnosis of glaucoma, variations in images from differ-\nent devices and locations (known as domain shifts) challenge the use\nof pre-trained models in real-world settings. To address this, we pro-\npose a novel Graph-guided Test-Time Adaptation (GTTA) framework to\ngeneralize glaucoma diagnosis models to unseen test environments. GTTA\nintegrates the topological information of fundus images into the model\ntraining, enhancing the model's transferability and reducing the risk of\nlearning spurious correlation. During inference, GTTA introduces a novel\ntest-time training objective to make the source-trained classifier progres-\nsively adapt to target patterns with reliable class conditional estimation\nand consistency regularization. Experiments on cross-domain glaucoma\ndiagnosis benchmarks demonstrate the superiority of the overall frame-\nwork and individual components under different backbone networks.", "sections": [{"title": "Introduction", "content": "Glaucoma, a chronic progressive optic neuropathy due to high intraocular pres-\nsure, is a leading factor in irreversible blindness [29]. Early screening and de-\ntection play a crucial role in facilitating prompt treatment to prevent continual\nvision loss [27]. Color fundus photography (CFP) is one of the most viable non-\ninvasive means of examining the retina for glaucoma diagnosis [1]. Recent ad-\nvances in deep learning have largely automated the diagnosis of glaucoma with\nCFP images, accelerating the diagnosis process, reducing assessment costs, and\nlowering the burden on healthcare systems [33]. To date, deep networks, such as\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs), have\ngreatly enhanced both the accuracy and efficiency of glaucoma diagnosis, show-\ning great potential as alternatives to conventional diagnostic approaches using\nintraocular pressure measurement and visual field testing [33,6,8,12,10].\nDespite the notable performance on in-distribution data, the distribution\nshift of CFP images from various imaging devices across different sites presents"}, {"title": "Method", "content": "Fig. 1 provides an overview of the proposed GTTA. During training phase, to learn\ngeneralizable deep representations from source site data, we design a Graph-\nassisted Relation-aware Training (GRT) module that learns the topological rela-\ntions based on hidden features extracted by backbone models. During inference\nphase, we introduce a novel TTA solution, namely Test-time Prediction Disam-\nbiguation (TPD), to adapt the source-trained decision boundaries to unforeseen\ntarget circumstances, mitigating the potential semantic misalignment."}, {"title": "Graph-assisted Relation-aware Training", "content": "Classic deep learning models such as CNNs are incapable of explicitly modeling\nthe high-order relational dependencies of different regions within a CFP image.\nVision Transformers (ViTs) [14], which can be seen as spatial attention on a\nfully-connected graph, mitigate this challenge to some extent but may introduce\nnoisy relations, e.g., connecting two irrelated local regions. To address this issue,\nwe introduce a novel relation graph module that utilizes backbone features to\nmodel the topological relationships among various regions. Such relation graphs\ncan flexibly and efficiently model more local and sparse dependency relations.\nGraph construction. Formally, the relation graph is denoted by $G =$\n${V, E}$, where $V$ and $E$ represent graph nodes and edges, respectively. First\nof all, GRT receives hidden features extracted by the backbone feature extractor\nand divides them into a set of vectors, where each vector corresponds to a certain\nregion of the input image. (See Sec. 3.2 for the tested backbone networks: for\nCNN, a vector represents a pixel in the feature map, and for ViT and MLP, it\nstands for the embedding of a patch.) Then, these vectors are linearly projected\ninto an embedding space of dimension $F$. We further add a learnable 1D po-\nsitional embedding to these feature vectors, denoted as $h = \\{h_1,h_2,..., h_N\\}$,\n$h_i \\in R^F$, where $N$ is the number of the nodes. Regarding the graph edge, instead"}, {"title": "Graph learning", "content": "To update the node feature based on $G$, we resort the\ngraph self-attention mechanism [32], which takes node features $h$ and a learnable\nedge matrix $V$ as inputs. The nodes updated by the layer can be represented\nas $h' = \\{h'_1, h'_2,..., h'_{N}\\}$, $h'_i \\in R^{F'}$, where $F'$ is the dimension of the higher-\nlevel output features. After aggregating the topological structure, we reduce\nthe number of parameters and eliminate non-essential node representation by\nemploying top-k pooling for node downsampling:\n$h'' = \\{h' \\in h' | \\sigma (\\alpha h') \\ge \\alpha_k\\},$                                                              (1)\nwhere $h''$ is the output feature set from top-k pooling, $\\alpha \\in R^{F'}$ represents a\ntrainable weight vector, $\\sigma$ denotes an activation function such as the sigmoid\nfunction, and $\\sigma (\\alpha h')$ is referred to as the score. The value $\\alpha_k$ indicates the\n$k$-th largest score computed across all output features. The filtered features are\nthen flattened and fed into a linear classifier to obtain the GRT's prediction value\n$p_{GRT}$. Finally, after acquiring the GRT prediction value $p_{GRT}$ and the backbone\nprediction value $p_B$, the GRT and backbone engage in mutual learning, and the\nfinal loss is defined as:\n$L_{GRT} = CE(p_B, y) + \\lambda \\cdot CE(p_{GRT}, y) + (1 - \\lambda) \\cdot CE(p_{GRT}, p_B)$,\n$\\hat{y} = \\underset{c}{\\text{argmax}} \\, p_B (c | x) = k$                                                                                                                                                 (2)\nwhere $CE$ denotes the standard cross-entropy loss, $y$ is the ground truth, and $\\lambda$\nis a hyperparameter to balance the losses."}, {"title": "Test-Time Prediction Disambiguation", "content": "In practice, test data may not be all pre-prepared but rather continuously added.\nThus, the basic idea of TPD is to update the weights of the source model using\nunlabeled target data in an online manner. The trained source model can be\ndecomposed into a linear classifier $g_w$ and a feature extractor $f_{\\theta}$, where $w$ and $\\theta$\ndenote the parameters.\nMemory bank. First of all, we introduce a memory bank to dynamically\nstore the target data features and logits, denoted by $S_t = \\{s^0_t, s^1_t, \\dots, s^K_t\\}$,\nwhere $K$ is the number of classes and $t$ denotes the time step. Following [17],\nat time $t = 0$, we initialize the memory bank with the $L_2$ norm of the linear\nclassifier's weights. Given an input sample $x$, at time $t$, $S_t$ is updated as follows:\n$s^k_t =\\begin{cases}s^{t-1} \\cup \\{ f_{\\theta}(x_t) \\} & \\text{if } \\underset{c}{\\text{argmax}} p_c = k\\\\s^{t-1} & \\text{else} \\end{cases}$                                                                                                                                                    (3)"}, {"title": "Class conditional estimation", "content": "To update the model parameters, we es-\ntimate the class conditionals of unlabeled target data using the memory bank,\nrather than simply relying on the classifier's output as pseudo labels for self-\ntraining. Specifically, we identify the nearest neighbors $N(x)$ of the input test\ndata within $S_t$:\n$N(x) := \\{z \\in S | d (f_{\\theta}(x), z) \\le \\eta\\},$                                                               (4)\nwhere $z$ is the L2-normalized target sample embedding (i.e., $f_{\\theta}(x)$), $d$ is the\ndistance function, and $\\eta_n$ represents the distance between $x$ and the $n$-th closest\nneighbor. Here, we opt for cosine similarity to compute the distance between $x$\nand its neighbors in $S_t$. Once identifying the neighbors of the input target data,\nwe utilize a Parallel Linear Module (PLM), denoted as $h_{\\phi}(z) = \\sum^N_{i=1}h_{\\phi_i} (z)$\nto obtain the centroid of each class in the embedding space of $f_{\\theta} \\circ h_{\\phi_i}$, where\n$h_{\\phi_i}$ is the $i$-th linear module. The centroid can then be represented as:\n$M_i = \\{\\mu^0_i, \\mu^1_i, \\dots, \\mu^K_i \\}, \\,\\, \\,\\, \\mu^k_i = \\frac{1}{|S_k|} \\underset{z \\in S_k}{\\sum} h_{\\phi_i}(z)$                                               (5)\nUsing the class centroids computed by the prototype classifier, we can predict\nthe corresponding class conditionals for the neighbors of the target data and\nobtain the average pseudo label integrated neighbor information:\n$\\begin{aligned}p^{\\text{proto}} (k | z) &:= \\frac{\\exp (-d (h_{\\phi_i}(z), \\mu_{i,k}) /\\tau)}{\\sum_{c} \\exp(-d (h_{\\phi_i}(z), \\mu_{i,c}) /\\tau)},\\\\p_i(k | x) &:= \\frac{1}{n} \\underset{z \\in N(x)}{\\sum} \\underset{c}{\\text{argmax}} p^{\\text{proto}} (c | z) = k , \\end{aligned}$                                                                (6)\nwhere $\\tau$ is the softmax temperature coefficient. Similarly, we use the prototype\nclassifier to obtain the logits for the current sample, denoted as $p^{\\text{proto}} (k | x)$.\nTest-time training objective. To enhance the robustness of the classifier's\nprediction, we introduce an Entropy Punishment Divergence to assign larger\nweight to predictions with lower confidence:\n$D_{EP}(P(X)||\\hat{y}) = \\text{softmax}_B (H(X)/\\tau) \\cdot D_{KL}(P(x)||\\hat{y}),$                                                                                                                                            (7)\nwhere $B$ is the batch size, $H(x) = - \\sum_{k=1}^K P(k | x) \\log (p(k | x))$, and $D_{KL}$\ndenotes the Kullback-Leibler (KL) divergence. Finally, we fine-tune the linear\nclassifier with the following test-time training objective:\n$L_{TTT} = D_{EP} [p^{\\text{proto}} (x)||p_i(x)] + \\lambda_1 \\cdot D_{EP} [g_w (f_{\\theta}(x)) ||p_i(x)] \\\\+\\lambda_2 \\cdot D_{KL} [g_w (f_{\\theta}(x)) ||p^{\\text{proto}}(x)],$                                                      (8)"}, {"title": "Experiments", "content": "We performed two experiments: 1) an ablation study to evaluate the performance\nof the proposed GRT and TPD, and 2) a comparison with SOTA TTA methods.\nWe implemented three backbone networks: ResNet-18 [15], ViT-B/16 [7], and\ngMLP-S [23]. For each network, we tested multiple architectures and selected the\nbest-performing one. We employed ImageNet pre-trained backbones, as networks\nwithout pretraining underperformed due to domain shift.\nAblation Study. We conducted ablation studies to demonstrate the role of\nGRT and TPD. As shown in Tab. 2, the combined use of GRT and TPD en-\nhances performance in target domains beyond what is achieved when using either\nmethod alone or neither. Compared to the vanilla backbone network, our GTTA\nframework achieves a 17.34% increase in accuracy on ResNet-18, a 12.40% in-\ncrease in accuracy on ViT-B/16, and an 11.02% increase in accuracy on gMLP-S.\nAcross the different backbones, ResNet-18 yielded the best performance, as GRT\nbenefits from the deeper hidden features when using ResNet-18.\nComparison with SOTA Methods. As shown in Tab. 3, we compared our\nTPD with state-of-the-art TTA methods: 1) TentClf [34], fine-tunes the linear\nclassifier through entropy minimization to obtain high-confidence predictions\non the target domain; 2) PLClf [21], fine-tunes the linear classifier by high-\nconfidence pseudo labels; 3) MEMO [38] enhances the source model's robustness\non the target domain through data augmentation and introduces marginal en-\ntropy minimization for fine-tuning the source model. 4) T3A [17] creates support\nsets to adjust class prototypes for training-free adaptation. 5) TAST [18] lever-\nages neighbor information and fine-tunes an adaptation module to update class\nprototypes. Here we only performed TTA on the backbone and GRT was frozen.\nExperiments show the proposed TPD achieves an average improvement of 6.06%\nin accuracy and 4.85% in AUC across three backbones networks (source-trained\nwith GRT), largely outperforming other TTA methods and exhibiting higher ro-\nbustness across various domains. These results highlight the unique contribution\nof our TTA solution, revealing its superior ability to adapt the source classifier\nto the test environment compared to previous state-of-the-art TTA approaches."}, {"title": "Conclusion", "content": "We propose GTTA for automated glaucoma diagnosis using CFP images. GTTA in-\ntegates topological information inherent in CFP images into the source model\ntraining and enables prediction disambiguation through a test-time training ob-\njective during inference phase. To the best of our knowledge, this work represents\nthe first application of TTA for automated glaucoma diagnosis. Moreover, our\nmethod can be seamlessly extended to other medical scenarios with distribution\nshifts, enhancing their transferability in unknown environments."}]}