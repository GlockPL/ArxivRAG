{"title": "A Comprehensive Survey on Composed Image Retrieval", "authors": ["Xuemeng Song", "Haoqiang Lin", "Haokun Wen", "Bohan Hou", "Mingzhu Xu", "Liqiang Nie"], "abstract": "Composed Image Retrieval (CIR) is an emerging yet challenging task that allows users to search for target images using a multimodal query, comprising a reference image and a modification text specifying the user's desired changes to the reference image. Given its significant academic and practical value, CIR has become a rapidly growing area of interest in the computer vision and machine learning communities, particularly with the advances in deep learning. To the best of our knowledge, there is currently no comprehensive review of CIR to provide a timely overview of this field. Therefore, we synthesize insights from over 120 publications in top conferences and journals, including ACM TOIS, SIGIR, and CVPR. In particular, we systematically categorize existing supervised CIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive review, we also briefly discuss approaches for tasks closely related to CIR, such as attribute-based CIR and dialog-based CIR. Additionally, we summarize benchmark datasets for evaluation and analyze existing supervised and zero-shot CIR methods by comparing experimental results across multiple datasets. Furthermore, we present promising future directions in this field, offering practical insights for researchers interested in further exploration.", "sections": [{"title": "1 Introduction", "content": "Image retrieval has been a fundamental task in computer vision and database management since the 1970s [36], serving as a cornerstone for various applications, such as face recognition [47], fashion retrieval [182], and person re-identification [92]. Traditional image retrieval systems primarily rely on unimodal queries, using either text or images to convey a user's search intent [33, 120, 121, 125]. However, users often struggle to clearly express their search intent through a single text query or to find the perfect image that accurately represents it. To address these limitations and provide greater flexibility, composed image retrieval (CIR) [154] emerged in 2019, which allows users to express their search intent by a reference image combined with a textual description specifying the desired modifications. By enabling users to utilize more nuanced search queries, CIR offers significant potential to enhance search experiences across domains, such as e-commerce [43] and internet search engines [75, 118, 156, 167].\nThe concept of CIR, which allows users to utilize a multimodal query to express their search intent, can be easily adapted for various real-world retrieval scenarios. For example, the reference image could be replaced with a reference video to enable composed video retrieval, or single-turn CIR could evolve into dialog-based multi-turn image retrieval. Since its introduction in 2019, CIR has garnered increasing research attention due to its potential value across various domains. As illustrated in Figure 1, the number of publications on CIR is increasing rapidly. To summarize the past and current achievements in this rapidly developing field, we present a comprehensive overview of work conducted up to November 2024. Existing studies primarily focus on addressing the following key challenges. 1) Multimodal Query Fusion. In CIR, the modification text and reference image play complementary roles in conveying the user's search intent. The modification text typically specifies changes in certain attributes of the reference image. For instance, given the modification requirement, \"I want the dress to be black and more professional\", only the color and style of the dress in the reference image should be changed, while other attributes of the reference image should be kept unchanged. Due to this nature, how to achieve an effective multimodal fusion for accurately comprehending the multimodal query poses the first challenge. 2) Target Images Matching. The semantic gap between the multimodal query and target images presents a significant challenge due to their heterogeneous representations. Additionally, the brevity of modification texts can lead to ambiguity. For example, the text \"I want to change the dress to longer sleeves and yellow in color\" could have multiple interpretations: the sleeves could change from sleeveless to either short or long, and the color could range from light to dark yellow. Such ambiguity suggests that multiple target images could satisfy the given query. Therefore, bridging this semantic gap and managing the one-to-many query-to-target matching relationship is crucial for accurate query-target matching. 3) Scale of Training Data. Training CIR models typically requires triplets in the form of <reference image, modification text, target image>. For each triplet, the reference-target image pair is often generated using a heuristic strategy, while the modification text is usually annotated by humans. Creating such training samples is both costly and labor-intensive, which significantly restricts the size of benchmark datasets. Consequently, addressing the issue of insufficient training data to improve the model's generalization capabilities remains a significant challenge.\nExisting work in this area can be broadly divided into two main categories: supervised learning-based approaches and zero-shot learning-based approaches. The key distinction between these methods lies in the availability of annotated training triplets. Supervised approaches rely on annotated triplets from the dataset to train the model, while zero-shot approaches leverage large-scale, easily accessible data, such as image-text pairs, for pre-training without requiring annotated triplets for optimization. To facilitate deeper analysis, we establish a fine-grained taxonomy for each category. For supervised CIR approaches, we summarize existing methods based on the four key components of the general framework: feature extraction, image-text fusion, target matching, and data augmentation. For zero-shot composed image retrieval (ZS-CIR) approaches, we classify methods into three groups: textual-inversion-based, pseudo-triplet-based, and training-free. As previously mentioned, the concept of using a composed multimodal query can be adapted for various scenarios. Beyond the primary task of CIR, several related tasks also involve composed queries, such as reference image plus attribute manipulation, sketch plus modification text, and video plus modification text. Since these tasks are closely related to CIR, we include their recent advancements to provide a comprehensive review of the topic. Based on the type of multimodal query, we categorize these related tasks into five groups: attribute-based, sketch-based, remote sensing-based, dialog-based, and video-based.\nIn summary, our main contributions are as follows:\n\u2022 To the best of our knowledge, this paper presents the first comprehensive review of CIR, incorporating over 120 primary studies. It aims to provide a timely and insightful overview to guide future research in this rapidly advancing field.\n\u2022 We systematically organize research findings, technical approaches, benchmarks, and experiments to deepen the understanding of this field. Additionally, we propose an elaborate taxonomy of methods, catering to the diverse needs of readers.\n\u2022 CIR remains an emerging area of research. Based on the surveyed literature, we identify several key research challenges and propose potential future directions, offering forward-looking guidance for researchers in this domain.\nThe remainder of this paper is organized as depicted in Figure 2. Sections 2 and 3 review supervised CIR models and zero-shot CIR models, respectively. Section 4 introduces tasks related to CIR. Section 5 describes the currently available datasets, evaluation metrics used, and experimental results from existing approaches. Finally, we discuss possible future research directions in Section 6 and conclude the work in Section 7."}, {"title": "2 Supervised Composed Image Retrieval", "content": "In this section", "components": "feature extraction", "types": "traditional encoders and vision-language pre-trained (VLP) model-based encoders.\n2.2.1 Traditional Encoder. For textual feature extraction", "30": "and Long Short-Term Memory networks (LSTMs)", "205": "employ BiGRUs as text encoders to process sequences bidirectionally", "194-198": "utilize LSTMs", "148": "a growing number of CIR studies [5", "174": "adopt transformer-based encoders", "39": "and its variants (e.g.", "106": "and DistilBERT [132", "203": "extract image features with pre-trained CNN-based encoders", "86": "GoogleNet [141", "66": "which yield generalizable feature embeddings by being pre-trained on large-scale datasets like ImageNet [38", "147": "is Vision Transformer (ViT) [41", "179": "employ Swin Transformers [109", "208": "adopt CLIP [123", "113": "utilize BLIP [91", "90": "has also been adopted for feature extraction in recent CIR studies [173", "176": "which bridges the modality gap with a lightweight Querying Transformer (Q-Former) and achieves state-of-the-art performance on various vision-language tasks. Collectively", "groups": "explicit combination-based fusion"}, {"types": "transformed image-and-residual combination and content-and-style combination.\nTransformed Image-and-Residual. The key idea of this group of methods is to keep the image feature as the dominant component", "parts": "the transformed reference image feature and the residual feature. This group of methods can typically be expressed as g ([img; txt", "195": "local features [87", "196": "hierarchical features [24", "203": "and decoupled features [17", "200": "have been explored. The function g() is a neural network that derives the parameters applied to the reference image to achieve the modification operation", "154": "VAL [24", "23": "MGF [107", "83": "CLVC-Net [164"}, {"203": "and CRN [179", "txt": ".", "154": "which designs g (.) as a gating function with a Sigmoid activation function to adaptively preserve the unchanged information in the reference image", "what elements to modify\" within the reference image, guided by the modification text; while the latter emphasizes determining \"what to retain\" within the modification text given the reference image. Another group of methods adheres to the formula expressed as res = h ([img; txt": "txt to learn the residual part. Typically", "166": "DWC [70", "175": "and EER [196", "195": "and Ranking-aware [20", "69": "where g (.) = I and SAC [75"}, {"txt": "txt. s (\u2022) denotes the attentional transformation network.\nContent-and-Style. Considering that \"each image can be well characterized by their content and style\" [17", "200": "typically hypothesizes that both the image style and content will be modified in accordance with the modification text. Thereby", "87": "initially devises a content modulator equipped with a disentangled multimodal non-local block for effecting content modifications. Subsequently", "17": "and PCaSM [200", "27": "primarily centers on the style aspect. It introduces an explicit definition of style as the commonality and difference among the local patches of an image. With this concept in mind", "subgroups": "MLP-based", "208": "primarily rely on the multi-layer perceptron (MLP) to fulfill the image-text fusion. For example"}, {"208": "introduces a multi-stage learning framework to progressively acquire the complex knowledge necessary for multimodal image retrieval and employs an MLP-based query adaptive weighting strategy to dynamically balance the influence of image and text. As pioneers in applying CLIP to CIR tasks", "8": "introduce a classic multimodal fusion network", "9": "within this framework to alleviate the domain discrepancy between CLIP's pre-training data and the downstream task data. This combiner network has been directly adopted by many subsequent methods [112", "163": "or refined in later studies [100", "155": ".", "70": "introduces an Editable Modality De-equalizer (EMD). This module employs two modality editors equipped with spatial and word attention mechanisms to refine image and text features", "178": "initially employs an attention mechanism for text and image feature realignment at multiple levels when encoding the reference image and modification text using FashionCLIP [29", "174": "in this branch primarily adopt cross-attention to model the interaction between each word in the modification text and every local region in the reference image", "69": "introduces a localization mask", "126": "as an additional input for image-text fusion", "89": "proposes a multi-stage compositional framework that sequentially modifies the reference image based on textual semantics. At each stage", "51": "introduces a dual-channel matching model comprising a semantic matching module and a visual matching module. The semantic matching module utilizes a gate-based attention mechanism to fuse attributes of the reference image", "176": "leverages the Q-former module of BLIP2 [90", "88": "introduces a cross-attention driven shift encoder based on BLIP's image-grounded text encoder", "207": "in this branch typically feed the concatenation of the encoded reference image feature and the modification text feature into a self-attention-based network", "146": "refines the standard Transformer encoder with an additive self-attention layer", "181": "treats the modification text as an instruction and hence explicitly decomposes the semantic transformation conveyed by the modification text into two steps: degradation and upgradation. Specifically", "210": "first concatenate the encoded reference image features and word tokens from the modification text and then feed the concatenated token sequence into a transformer-based model for image-text fusion. Different from the above methods that adopt a single fusion strategy", "212": "and SDFN [171", "136": "focus on addressing various vision-and-language (V+L) tasks in the fashion domain", "192": "leverages the pretrained Faster R-CNN to extract visual attribute features for each reference/target image. These features are then utilized as vertices to construct a graph", "180": ".", "93": "to filter out redundant attributes", "193": "initially learns cross-modal embedding for the composed query in a geometry-aware way and then rectifies the visual feature under the guidance of the modification text with a multi-head graph attention network [150", "143": "pioneers integrate the Generative Adversarial Networks (GANs) into CIR", "194": "introduces a multi-stage GAN-based structure that embeds a retrieval model within a GAN framework. To learn a discriminative composed query feature", "discriminators": "one targeting global differences between generated and target images"}, {"207": "focuses on developing a unified framework leveraging LLMs and diffusion models to enhance the performance of multimodal retrieval and generation tasks with mutual task reinforcement.\n2.4 Target Matching\nThe target matching module aims to accurately retrieve images that match the given multimodal query. One fundamental technique for target matching in CIR is metric learning", "groups": "basic metric learning", "functions": "the batch-based classification (BBC) [154", "196": ".", "185": ".", "193": ".", "xt(i))": "max[0"}, {"xt(i))": 4, "109": "several studies [24", "174": "have explored hierarchical matching to improve the alignment between the input query and the target image. These methods start by sampling multi-granular visual features from the visual encoder and separately integrating them with the modification text feature. Then", "198": "explore image difference alignment", "89": "adapt the conventional BBC loss for image difference alignment as follows", "185": "adopts the Mean Squared Error (MSE) loss to narrow the distance between image differences and text modifications as follows"}, {"198": "formulates image difference alignment as a modification text generation problem. It inputs the features of reference and target images into an LSTM", "205": "designs multimodal concept alignment", "116": ".", "127": "to supervise multimodal concept alignment. This loss function ensures effective alignment by accounting for the inherent imbalance in the richness of concepts between textual and visual modalities", "follows": "n{Si = sigmod(vrt"}]}