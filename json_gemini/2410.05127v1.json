{"title": "LAST ITERATE CONVERGENCE\nIN MONOTONE MEAN FIELD GAMES", "authors": ["Noboru Isobe", "Kenshi Abe", "Kaito Ariu"], "abstract": "Mean Field Game (MFG) is a framework utilized to model and approximate the\nbehavior of a large number of agents, and the computation of equilibria in MFG\nhas been a subject of interest. Despite the proposal of methods to approximate the\nequilibria, algorithms where the sequence of updated policy converges to equilib-\nrium, specifically those exhibiting last-iterate convergence, have been limited. We\npropose the use of a simple, proximal-point-type algorithm to compute equilibiria\nfor MFGs. Subsequently, we provide the first last-iterate convergence guaran-\ntee under the Lasry-Lions-type monotonicity condition. We further employ the\nMirror Descent algorithm for the regularized MFG to efficiently approximate the\nupdate rules of the proximal point method for MFGs. We demonstrate that the al-\ngorithm can approximate with an accuracy of \\varepsilon after O(log(1/\\varepsilon)) iterations. This\nresearch offers a tractable approach for large-scale and large-population games.", "sections": [{"title": "1 INTRODUCTION", "content": "Mean Field Games (MFGs) provide a simple and powerful framework for approximating the be-\nhavior of large populations of interacting agents. Originally formulated by Lasry & Lions (2007);\nHuang et al. (2006), MFGs model the collective behavior of homogeneous agents in continuous time\nand state settings using Partial Differential Equations (PDEs) (Cardaliaguet & Hadikhanloo, 2017;\nLavigne & Pfeiffer, 2023; Inoue et al., 2023). Subsequently, the formulation of MFGs using Markov\nDecision Processes (Bertsekas & Shreve, 1978; Puterman, 1994) has enabled the study of discrete-\ntime and discrete-state models (Gomes et al., 2010), broadening the applicability of MFGs to Multi-\nAgent Reinforcement Learning (MARL) (Yang et al., 2018). Moreover, it has become possible to\ncapture interactions among heterogeneous agents (Gao & Caines, 2017; Caines & Huang, 2019).\nThe applicability of MFGs to MARL drives research into their\ncomputational aspects. Under fairly general assumptions, the\nproblem of finding an equilibrium in MFGs is known to be\nPPAD-complete (Yardim et al., 2024). Consequently, it would\nbe essential to impose assumptions that allow for the exis-\ntence of algorithms capable of efficiently computing an equi-\nlibrium. One of the assumptions is contractivity (Xie et al.,\n2021; Anahtarci et al., 2023; Yardim et al., 2023). However,\nit is known that many problems are not contractive in prac-\ntice (Cui & Koeppl, 2021). One of the more realistic assump-\ntions is monotonicity (P\u00e9rolat et al., 2022; Zhang et al., 2023;\nYardim & He, 2024), which intuitively implies that as more\nagents converge to a single state, the reward monotonically decreases. Under the monotonicity as-\nsumption, Online Mirror Descent (OMD) has been proposed and widely adopted (P\u00e9rolat et al.,\n2022; Cui & Koeppl, 2022; Lauriere et al., 2022; Fabian et al., 2023). OMD, especially when com-\nbined with function approximation via deep learning, has enabled the application of MFGs to MARL\n(Yang & Wang, 2021; Zhang et al., 2021; Cui et al., 2022)."}, {"title": "2 SETTING AND PRELIMINARY FACT", "content": "2.1 NOTATION\nFor a positive integer N \\in N, [N] := {1,...,N}. For a finite set X, \\Delta(X) := {p \\in\nR^X_{\\geq 0} | \\Sigma_{x\\in X}p(x) = 1}. For a function f: X \\rightarrow R and a probability \\pi \\in \\Delta(X), (f, \\pi) :=\n\\Sigma_{x\\in X} f(x)\\pi(x). For p^0, p^1 \\in \\Delta(X), define the Kullback\\unicode{x2013}Leibler (KL) diver-\ngence D_{KL}(p^0, p^1) := \\Sigma_{x\\in X}p^0(x)\\log (p^0(x)/p^1(x)), and the total variation (TV) distance as\n||p^0 - p^1 || := \\Sigma_{x\\in X} |p^0 (x) \\unicode{x2013} p^1(x)|.\n2.2 MEAN-FIELD GAMES\nConsider a Mean-Field Game (MFG) that is defined through a tuple (S, A, H, P, r, \\mu_1). Here, S is\na finite discrete space of states, A is a finite discrete space of actions, H \\in N_{>2} is a time horizon,\nand P = (P_h)_{h=1}^H is a family of transition kernels P_h: S \\times A \\rightarrow \\Delta(S), that is, if a player with\nstate s_h \\in S takes action a_h \\in A at time h\\in [H], the next state s_{h+1} \\in S will transition\naccording to s_{h+1} \\sim P_h(\\cdot|s_h,a_h). In addition, r = (r_h)_{h=1}^H is a family of reward functions\nr_h: S \\times A \\times \\Delta(S) \\rightarrow [0, 1], and \\mu_1 \\in \\Delta(S) is an initial probability of state. Note that, in the\ncontext of theoretical analysis of the online learning method for MFG (P\u00e9rolat et al., 2022; Zhang\net al., 2023), P is assumed to be independent of the state distribution. It is reasonable to assume that\nat any time h, every state s' \\in S is reachable:\nAssumption 2.1. For each (h, s') \\in [H] \\times S, there exists (s, a) \\in S \\times A such that P_h (s' | s, a) > 0."}, {"title": "3 PROXIMAL POINT-TYPE METHOD FOR MFG", "content": "3.1 ALGORITHM\nThis section presents an algorithm motivated by the proximal point (PP) method. Let \\lambda > 0 be a\nsufficiently small positive number, roughly \u201cthe inverse of learning rate.\" In the algorithm proposed\nin this paper, we generate a sequence ((\\sigma^k, \\mu^k))_{k=0}^\\infty \\subset (\\Delta(A)^S)^H \\times \\Delta(S)^H as\n\\sigma^{k+1} = \\underset{\\pi\\in(\\Delta(A)^S)^H}{\\operatorname{arg\\,max}} {J(\\mu^{k+1}, \\pi) - \\lambda D_{\\mu^{[\\pi]}}(\\pi, \\sigma^k)}, \\mu^{k+1} = m[\\sigma^{k+1}],\n(3.1)\nwhere m is defined in (2.1) and D_{\\mu}(\\pi,\\sigma) := \\Sigma_{h} \\mathbb{E}_{s\\sim\\mu_h} [D_{KL}(\\pi_h(s),\\sigma_h(s))] with a probability\n\\mu \\in \\Delta(S)^H. If the initial policy \\pi^0 has the full support, i.e., \\min_{(h,s,a)\\in H\\times S\\times A}\\pi^0_h (a | s) > 0, the\nrule (3.1) is well-defined, see Proposition B.1.\nInterestingly, the rule (3.1) is similar to the traditional proximal point (PP) method with KL diver-\ngence in mathematical optimization and Optimal Transport, see (Censor & Zenios, 1992; Xie et al.,\n2019) and the pseudocode in Algorithm 1. Therefore, we also refer to this update rule as the PP\nmethod. On the other hand, unlike the traditional PP method, our method changes the objective\nfunction J(\\mu^k, \\cdot): (\\Delta(A)^S)^H \\rightarrow R with each iteration k \\in N. Therefore, the convergence of our\ntraditional method is not directly derived from traditional theory.\n3.2 LAST-ITERATE CONVERGENCE RESULT\nThe following theorem implies the last-iterate convergence of the policies generated by (3.1). Specif-\nically, it shows that under the assumptions above, the sequence of policies converges to the equilib-\nrium set. This result is crucial for the effectiveness of the algorithm in reaching an optimal policy.\nTheorem 3.1. Let (\\sigma^k)_{k=0}^\\infty be the sequence defined by Algorithm 1. In addition to\nAssumptions 2.1 to 2.3, assume that the initial policy \\pi^0 has the full support, i.e.,\n\\min_{(h,s,a)\\in H\\times S\\times A}\\pi^0_h (a | s) > 0. Then, the sequence (\\sigma^k) converges to the set \\Pi^*\nof equilibrium, i.e.,\n\\lim_{k\\rightarrow\\infty} dist(\\sigma^k, \\Pi^*) = 0,\nwhere dist(\\pi, \\Pi^*) := \\inf_{\\pi^*\\in\\Pi^*}\\Sigma_{(h,s)\\in[H]\\times S} || \\pi_h(s) - \\pi^*_h(s)||.\nProof sketch of Theorem 3.1. If we accept the next two lemmas, we can easily prove Theorem 3.1:\nThe first implies that the KL divergence from an equilibrium to the generated policy becomes\nsmaller as the cumulative reward J increases.\nLemma 3.2. Suppose Assumption 2.2. Then, for any equilibrium (\\mu^*, \\pi^*) it holds that\nD_{\\mu^*}(\\pi^*, \\sigma^{k+1}) - D_{\\mu^*}(\\pi^*,\\sigma^k) \\leq J(\\mu^*, \\sigma^{k+1}) - J(\\mu^*, \\pi^*).\nFurthermore, we can control the right-hand side of the inequality in Lemma 3.2 by the distance:\nLemma 3.3. There exist positive constants \\alpha and C such that, for any \\pi\\in (\\Delta(A)^S)^H,\nJ(\\mu^*, \\pi) - J(\\mu^*,\\pi^*) \\leq -C(dist(\\pi, \\Pi^*))^\\alpha."}, {"title": "4 APPROXIMATING PROXIMAL POINT WITH MIRROR DESCENT IN\nREGULARIZED MFG", "content": "As in the previous section, in the PP method in Algorithm 1, it is necessary to solve the regularized\nMFG (3.1) at each iteration. Therefore, this section introduces Regularized Mirror Descent (RMD),\nwhich approximates the solution (\\mu^{k+1}, \\sigma^{k+1}) of (3.1) for each policy \\sigma^k. The novel result in this\nsection is that the divergence between the sequence of RMD and the equilibrium exponentially decays.\n4.1 APPROXIMATION OF THE UPDATE RULE OF PP WITH REGULARIZED MFG\nFortunately, solving (3.1) corresponds to finding an equilibrium for KL-regularized MFG introduced\nin Cui & Koeppl (2021); Zhang et al. (2023). Let us review the settings for the regularized MFG.\nFor each parameter \\lambda > 0 and policy \\sigma \\in (\\Delta(A)^S)^H, which plays the role of \\sigma^k in Algorithm 1, we\ndefine the regularized cumulative reward J^{\\lambda,\\sigma}: \\Delta(S)^H \\times (\\Delta(A)^S)^H \\ni (\\mu, \\pi) \\mapsto J^{\\lambda,\\sigma} (\\mu, \\pi) \\in R\nto be\nJ^{\\lambda,\\sigma} (\\mu, \\pi) := J(\\mu, \\pi) - \\lambda D_{\\mu^{[\\pi]}}(\\pi, \\sigma).\n(4.1)\nSince \\sigma is a representative of (\\sigma^k)_k, the assumption of full support is also imposed on \\sigma:\nAssumption 4.1. The base \\sigma has the full support, i.e., \\sigma_{min} := \\min_{(s,a,h)\\in S\\times A\\times[H]} \\sigma_h (a | s) > 0.\nFor the reward J^{\\lambda,\\sigma}, we introduce a regularized equilibrium:\nDefinition 4.2. A pair (\\mu^*, \\pi^*) \\in \\Delta(S)^H \\times (\\Delta(A)^S)^H is regularized equilibrium of J^{\\lambda,\\sigma} if it\nsatisfies (i) J^{\\lambda,\\sigma} (\\mu^*,\\pi^*) = \\underset{\\pi\\in\\Delta(S)^H}{\\operatorname{max}} J^{\\lambda,\\sigma} (\\mu^*, \\pi), and (ii) \\mu^* = m[\\pi^*].\nSpecifically, (\\mu^{k+1}, \\sigma^{k+1}) can be characterized as the regularized equilibrium of J^{\\lambda,\\sigma^k} for k \\in N.\nNote that the regularized equilibrium is unique under Assumption 4.1, see Appendix B.\nIn the next subsection, we will introduce RMD using value functions, which are defined as follows:\nfor each h \\in [H], s \\in S, \\alpha \\in A, \\mu \\in \\Delta(S)^H and \\pi \\in \\Delta(A)^S, define the state value function\nV: S \\times \\Delta(S)^H \\times (\\Delta(A)^S)^H \\rightarrow R_{\\geq 0} and the state-action value function Q: S\\times A \\times \\Delta(S)^H \\times\n(\\Delta(A)^S)^H \\rightarrow R_{\\geq 0} as\nV^{\\lambda,\\sigma} (s, \\mu, \\pi) := \\mathbb{E}_{\\begin{subarray}{l} ((s_l,a_l))_h^H\\end{subarray}} \\Big[\\Sigma_{l=h}^H(r_l(s_l, a_l, \\mu_l) - \\lambda D_{KL}(\\pi_l(s_l), \\sigma_l(s_l)))\\Big] \\quad s_h = s,\nV^{\\lambda,\\sigma}_{H+1} (s, \\mu, \\pi) := 0,\n(4.2)\nQ^{\\lambda,\\sigma} (s, a, \\mu, \\pi) = r_h(s, a, \\mu_h) + \\mathbb{E}_{s_{h+1}\\sim P(\\cdot|s,a,\\mu_h)} \\big[V^{\\lambda,\\sigma}_{h+1} (s_{h+1}, \\mu, \\pi)\\big].\n(4.3)\nHere, the discrete time stochastic process ((s_l,a_l))_h^H is induced recursively as s_{l+1} \\sim\nP_l(s_l, a_l), a_l \\sim \\pi_l(s_i) for each l\\in {h,...,H - 1} and a_H \\sim \\pi_H(s_H). Note that the the ob-\njective function J^{\\lambda,\\sigma} in Definition 4.2 can be expressed as J^{\\lambda,\\sigma} (\\mu, \\pi) = \\mathbb{E}_{s\\sim\\mu_1} [V^{\\lambda,\\sigma}_1(s, \\mu, \\pi)].\n4.2 AN EXPONENTIAL CONVERGENCE RESULT OF REGULARIZED MIRROR DESCENT\nIn this subsection, we introduce the iterative method for finding the regularized equilibrium proposed\nby Zhang et al. (2023) as RMD. The method constructs a sequence ((\\pi^t, \\mu^t)) \\subset (\\Delta(A)^S)^H \\times"}, {"title": "4.3 INTUITION FOR EXPONENTIAL CONVERGENCE: CONTINUOUS-TIME VERSION OF\nREGULARIZED MIRROR DESCENT", "content": "The convergence of (\\pi^t) can be intuitively explained by considering a continuous limit (\\pi^t)_{t \\geq 0}\nwith respect to the time t of RMD. In this paragraph, we will use the idea of mirror flow (Krichene\net al., 2015; Tzen et al., 2023; Deb et al., 2023) and continuous dynamics in games (Taylor & Jonker,\n1978; Mertikopoulos et al., 2018; P\u00e9rolat et al., 2021; 2022) to observe the exponential convergence\nof the flow to equilibrium. According to Deb et al. (2023, (2.1)), the continuous curve of \\pi should\nsatisfy that\n\\frac{d}{dt}\\pi_h (a | s) = \\pi_h (a | s) \\Big[\\alpha^{\\lambda,\\sigma}(s, a, \\pi^t, \\mu^t) - \\lambda\\log \\frac{\\pi_h (a | s)}{\\sigma_h (a | s)}\\Big].\n(4.5)\nThe flow induced by the dynamical system (4.5) converges to equilibrium exponentially as time t\ngoes to infinity.\nTheorem 4.4. Let \\pi^t be a solution of (4.5) and \\pi^* be a regularized equilibrium defined in\nDefinition 4.2. Suppose that Assumption 2.2. Then, it holds that\n\\frac{d}{dt} D_{\\mu^*}(\\pi^*, \\pi^t) \\leq -\\lambda D_{\\mu^*}(\\pi^*, \\pi^t),\nfor all t \\geq 0. Moreover, the inequality implies D_{\\mu^*}(\\pi^*, \\pi^t) \\leq D_{\\mu^*}(\\pi^*, \\pi^0) \\exp (-\\lambda t).\nTechnically, the non-Lipschitz continuity of the value function Q^{\\lambda,\\sigma} (s, a, \\cdot, \\mu^t) in the right-hand\nside of (4.5) is non-trivial for the existence of the solution \\pi: [0, +\\infty) \\rightarrow (\\Delta(A)^S)^H of the dif-\nferential equation (4.5), see, e.g., (Coddington & Levinson, 1984). The proof of this existence and\nTheorem 4.4 are given in Appendix B."}, {"title": "4.4 PROOF SKETCH OF THE CONVERGENCE RESULT FOR REGULARIZED MIRROR DESCENT", "content": "Let us return from continuous-time dynamics (4.5) to the discrete-time algorithm (4.4). The tech-\nnical difficulty in the proof of Theorem 4.3 is the non-Lipschitz continuity of the value function\nQ^{\\lambda,\\sigma} in (4.4), that is, the derivative of Q^{\\lambda,\\sigma} (s, a, \\pi, \\mu) with respect to the policy \\pi can blow up as \\pi\napproaches the boundary of the space (\\Delta(A)^S)^H of probability simplices.\nWe can overcome this difficulty as shown in the following sketch of proof:\nProof sketch of Theorem 4.3. In a similar way to Theorem 4.4, we can obtain the following\ninequality with a discretization error:\nD_{\\mu^*}(\\pi^*, \\pi^{t+1}) - D_{\\mu^*}(\\pi^*, \\pi^t) \\leq -\\lambda\\eta D_{\\mu^*}(\\pi^*,\\pi^t) + D_{\\mu^*}(\\pi^t, \\pi^{t+1}).\n(4.6)\ndiscretization error\nThe remainder of the proof is almost entirely dedicated to showing that the above error term is\nsufficiently small and bounded compared to the other terms in the inequality. As a result, we\nobtain the following claim:\nClaim 4.5. Suppose that the learning rate \\eta is less than the upper bound \\eta^* in (C.5).\nThen, we have\nD_{\\mu^*}(\\pi^t, \\pi^{t+1}) \\leq C\\eta^2 D_{\\mu^*}(\\pi^*, \\pi^t),\nwhere C > 0 is the constant defined in (C.4), which satisfies C\\eta^* < \\lambda/2.\nThe key to proving Claim 4.5 is leveraging another claim that, over the sequence (\\pi^t)_t, the value\nfunction Q^{\\lambda,\\sigma} behaves well, almost as if it were a Lipschitz continuous function, see Lemma C.3\nfor details. Therefore, applying Claim 4.5 to (4.6) completes the proof."}, {"title": "4.5 APPROXIMATED PROXIMAL POINT METHOD", "content": "Let us consider an approximation of Algorithm 1 using RMD of (4.4). We can simply replace the\nintractable computation in line 4 of Algorithm 1 with RMD. In the end, this means that after repeating\n(4.4) a sufficient number of times, we also update \\sigma to the most recently obtained policy \\sigma^{k+1} using\nRMD. The pseudo-code that summarizes this idea is presented in Algorithm 2."}, {"title": "5 NUMERICAL EXPERIMENT", "content": "We numerically demonstrate that the proposed algorithm (Algorithm 2), which is the approximated\nversion of Algorithm 1, can achieve convergence to the mean-field equilibrium.\nAlgorithms. In this experiment, we implement Algorithm 2. For comparison, we also implement\nRMD (i.e., Algorithm 2 without the update of \\sigma^k) in (4.4). For both algorithms, the learning rate\nis fixed at \\eta = 0.1, and we vary the regularization parameter \\lambda and update time T to run the\nexperiments.\nEvaluations. We evaluate the convergence of our proposed method using the Beach Bar Pro-\ncess introduced by Perrin et al. (2020), a standard benchmark for MFGs. In particular, the transi-\ntion kernel P in this benchmark gives a random walk on a one-dimensional discretized torus S =\n{0,..., |S| - 1}, and the reward is set to be r_h(s, a, \\mu) = -|a|/|S| - |s - |S|/2|/|S| - \\log \\mu_h (s)\nwith a \\in A := {\u22121,\u00b10, +1}. See Appendix E for further details. Since the mean-field equilibrium\nin this benchmark cannot be computed exactly, we follow P\u00e9rolat et al. (2022); Zhang et al. (2023)\nand employ the exploitability of a policy \\pi\\in (\\Delta(A)^S)^H defined by\nExploit(\\pi) := \\underset{\\pi'\\in(\\Delta(A)^S)^H}{\\operatorname{max}} {J(m[\\pi], \\pi') - J(m[\\pi], \\pi) }\\geq 0,\nas our convergence criterion. Note that from Definition 2.4, Exploit(\\pi) = 0 if and only if (m[\\pi], \\pi)\nis mean-field equilibrium.\nDiscussion. Figure 2 is a summary of the results of the experiment. The most noteworthy aspect\nis the convergence of the exploitability, as shown in Figure 2b. Our proposed method decreases the\nexploitability with each iteration when we update \\sigma.\nFigures 2a and 2c illustrate the qualitative validity of the approximation achieved by our proposed\nmethod. In this benchmark, the equilibrium is expected to lie at the vertices of the probability\nsimplex. Therefore, RMD, which can shift the equilibrium to the interior of the probability simplex,\nseems unable to find the mean-field equilibrium accurately. On the other hand, the sequence (\\pi^t)_t\nof policies generated by our proposed method shows a behavior that converges to the vertices.\nIn summary, Algorithm 2 experimentally shows the last-iterate convergence to the mean-field equi-\nlibrium. This is evidenced by the decreasing exploitability and the qualitative behavior in our pro-\nposed method, which align with the theoretical guarantees."}, {"title": "6 COMPARISON OF THE RESULTS", "content": "Last-iterate convergence (LIC) results for MFG. P\u00e9rolat et al. (2022) showed that Mirror De-\nscent achieves LIC only under strictly monotone conditions, i.e., if the equality in the Lemma D.2\nis satisfied only if \\pi = \\tilde{\\pi}. In contrast, our work establishes LIC even in non-strictly monotone\nscenarios. While the distinction regarding strictness might seem subtle, it is profoundly signifi-\ncant. Indeed, non-strictly monotone MFGs encompass the fundamental examples of finite-horizon\nMarkov Decision Processes. Moreover, in strictly monotone cases, mean-field equilibria become\nunique. Consequently, as Zeng et al. (2024) also noted, strictly monotone rewards fail to represent\nMFGs with diverse equilibria."}, {"title": "7 CONCLUSION", "content": "This paper proposes noble algorithms that can achieve last-iterate convergence under the mono-\ntonicity condition. The main idea behind the derivation of the main algorithm (Algorithm 2) is to\napproximate the proximal-point type algorithm (Algorithm 1) using RMD. Theorem 3.1 guarantees\nthat the proximal-point-type algorithm achieves LIC, and Theorem 4.3 guarantees the exponential\nconvergence of RMD. An important future task of this study is to prove the convergence rates of\nAlgorithm 2. Specifically, we aim to make the convergence result of Theorem 3.1 quantitative. As\nthe experimental results suggest in Figure 2b, we conjecture that the algorithm converges with a rate\nof O(1/t^a) for some a > 0."}, {"title": "A PROOF OF THEOREM 3.1", "content": "Recall that the update is induced by the map T defined by\n\\mathcal{T}[\\sigma] = \\underset{\\pi\\in(\\Delta(A)^S)^H}{\\operatorname{arg\\,max}} {J(m[\\sigma], \\pi) - \\lambda D_{\\mu^{[\\pi]}}(\\pi,\\sigma)}.\nfor \\sigma \\in (\\Delta(A)^S)^H.\nProof of Lemma 3.2. Let (\\mu^*, \\pi^*) be a mean-field equilibrium defined in Definition 2.4. By the\nupdate rule (3.1) and Lemma D.1, we have\n \\Big\\langle \\delta,\\frac{\\delta J_{\\lambda,\\sigma^k}(\\pi^*, \\mu^*)}{ \\delta \\pi_h (s)} - \\lambda\\log \\frac{\\sigma_h^{k+1} (s)}{\\sigma_h^k (s)}, \\pi_h (s) -\\sigma_h^{k+1}(s) \\Big\\rangle \\leq 0,\nfor each h \\in [H], s \\in S and k \\in N, i.e.,\nD_{KL}(\\pi_h^*(s), \\sigma_h^{k+1}(s)) - D_{KL}(\\pi_h^*(s), \\sigma_h^{k}(s)) - D_{KL}(\\sigma_h^{k+1}(s), \\sigma_h^{k}(s))\n\\leq  \\Big\\langle \\delta,\\frac{\\delta J_{\\lambda,\\sigma^k}(\\pi^*, \\mu^*)}{ \\delta \\pi_h (s)}\\Big\\rangle - \\lambda D_{KL}(\\sigma_h^{k+1}(s), \\sigma_h^{k}(s)),\nTaking the expectation with respect to s \\sim \\mu^* and summing (A.1) over h \\in [H] yields\nD_{\\mu^*}(\\pi^*, \\sigma^{k+1}) - D_{\\mu^*}(\\pi^*, \\sigma^{k}) + D_{\\mu^*}(\\sigma^{k+1},\\sigma^{k})\n<\\frac{\\lambda}{H}  \\mathbb{E}_{s\\sim \\mu_h^*}  \\Big\\langle \\delta,\\frac{\\delta J_{\\lambda,\\sigma^k}(\\pi^*, \\mu^*)}{ \\delta \\pi_h (s)}\\Big\\rangle,\nBy virtue of Lemmas D.2 and D.4, we further have\n= \\Sigma_{h=1}^H \\mathbb{E}_{s\\sim \\mu_h^*}(\\delta_i,\\frac{\\delta Q_{\\lambda,\\sigma^k}^*}{\\delta s} + \\lambda D_{KL} - J(\\delta) > 0 holds for 0 log(A))"}, {"content": "Proof. From the definition of V and Q in (4.2) and (4.3), we have\nEs [( (s), Q (s,,\u03c0, \u03bc))]\nEs [( (s), + E V (, \u03bc,\u03c0) + (,)]]\nEs [E [ V (,) (,) ]]\nE [V (, \u03bc,\u03c0) || (,) ]]\n\u03bb(,), \u03c0+ E [ V (, \u03bc,\u03c0) | (,) (,) ]]\nE [V (, \u03bc,\u03c0) +\u03bb(,), \u03c0]"}]}