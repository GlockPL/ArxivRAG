{"title": "Investigating Instruction Tuning Large Language Models on Graphs", "authors": ["Kerui Zhu", "Bo-Wei Huang", "Bowen Jin", "Yizhu Jiao", "Ming Zhong", "Kevin Chang", "Shou-De Lin", "Jiawei Han"], "abstract": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP tasks, there's growing interest in applying LLMs to graph-related tasks. This study delves into the capabilities of instruction-following LLMs for engaging with real-world graphs, aiming to offer empirical insights into how LLMs can effectively interact with graphs and generalize across graph tasks. We begin by constructing a dataset designed for instruction tuning, which comprises a diverse collection of 79 graph-related tasks from academic and e-commerce domains, featuring 44,240 training instances and 18,960 test samples. Utilizing this benchmark, our initial investigation focuses on identifying the optimal graph representation that serves as a conduit for LLMs to understand complex graph structures. Our findings indicate that JSON format for graph representation consistently outperforms natural language and code formats across various LLMs and graph types. Furthermore, we examine the key factors that influence the generalization abilities of instruction-tuned LLMs by evaluating their performance on both in-domain and out-of-domain graph tasks.", "sections": [{"title": "1 Introduction", "content": "The success of Large language models (LLMs) in understanding and reasoning the semantic structure in natural language has brought a great interest in applying this capability to assist tasks with other modalities such as graphs. Graph stores information through the explicit connections between nodes and the attributes associated with the nodes and edges, which is quite different from natural language. To fill the gap between graph and LLM, Ye et al. (2023); Wang et al. (2024b); He & Hooi (2024); Luo et al. (2024) focus on instruction tuning LLM on linearized graph representations so that the LLM can learn the graph structure and solve graph-related tasks based on instructions. Results show that graph instruction-tuned LLMs outperform traditional Graph Neural Networks (GNNs) (Ye et al., 2023).\nHowever, we notice a lack of fundamental study of the graph representation and deeper analysis of the instruction-tuned LLM's generalization ability over empirical graph tasks. For the graph representation, Chen et al. (2023); Zhao et al. (2023); Wang et al. (2024a) translate graphs into natural language, while Wang et al. (2024b) represents the graph in a code-like format. However, it is still unclear how the choice of graph representation would affect the efficiency of graph instruction tuning. For the generalization, the LLMs are expected to solve tasks with new requirements, new graph structure distribution, and even unseen algorithms. This is critical for a general graph problem solver due to the complexity and variety of graph-related problems. Guo et al. (2023) establish a benchmark with 10 tasks to assess the proficiency of LLMs in understanding graph data. In this work, we instruction-tune LLMs on graphs with more fine-grained tasks and comprehensively analyze their capability concerning generalization."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLMs on Graphs", "content": "Inspired by the recent achievements of large language models (LLMs) in natural language processing tasks, researchers are investigating the use of LLMs for tackling graph-related tasks (Jin et al., 2023a). Existing works can be organized into two categories depending on the functions of LLMs. The first category typically relies on LLMs to serve as pretrained feature extractors (Chien et al., 2021) for graph neural networks (GNNs) (Wu et al., 2020) . For example, TextGNN (Zhu et al., 2021) proposes to conduct LLM text feature extraction before GNNs for sponsored search tasks. TAPE (He et al., 2023) adopts LLMs to generate augmented texts before feeding into medium-scale LMs and GNNs. The second category is graph-incorporated LLM architectures (Jin et al., 2023b). Specifically, GraphFormers (Yang et al., 2021) and Edgeformers (Jin et al., 2023c) propose graph-empowered language model architecture for homogeneous text-attributed graphs and textual-edge graphs respectively. Heterformer (Jin et al., 2023d) further introduces textless node encoding and proposes an architecture for heterogeneous text-attributed graphs. However, most existing works mainly focus on applying LLMs as off-the-shelf encoders or exploring LLM architecture improvement for graphs. In our work, we investigate the problem of instruction tuning large language models on graphs."}, {"title": "2.2 Instruction Tuning for LLMs", "content": "Instruction tuning (Ouyang et al., 2022; Sanh et al., 2022) is crucial for the latest generation of LLMs to cater to explicit user commands. In this stage, LLMs are trained using datasets with specific instructions and the expected responses, which improves LLMs in understanding and reacting to various human queries in natural language. Instruction tuning can be seen as a form of meta-learning where the model learns to adapt using the instructions (Zhang et al., 2023a; Longpre et al., 2023). As a result, these models acquire zero-shot learning ability which emerges as natural interactions with users. Currently, this paradigm has already demonstrated its impressive effectiveness across a wide range of natural language tasks, such as coding generation (Luo et al., 2023), complex reasoning (Mukherjee et al., 2023), information extraction (Jiao et al., 2023), and creative writing (Li et al., 2023).\nInspired by the success of instruction tuning on texts, recently an increasing research interest has tried to enable LLMs to generate more accurate and contextually appropriate responses for graph-structured data. These works typically align the language capacity of LLMs with the nuances of graph learning tasks. Specifically, Ye et al. (2023) instruction tunes the LLMs to perform graph tasks with graph structure described in natural language through highly scalable prompts. Wang et al. (2024b) uses a code-like format to describe graph information. Luo et al. (2024) is a concurrent work closest to ours. It instruction tunes LLM on homogeneous graphs and studies the generalization to the graph size, graph description languages, node ID representation, and out-of-domain tasks. In contrast, we instruction tunes LLMs on heterogeneous graphs and design fine-grained sub-tasks to study the generalization. Besides, our work discusses the effect of graph representation on instruction tuning, which is not well-studied yet."}, {"title": "3 Instruction Tuning on Graph", "content": ""}, {"title": "3.1 Preliminaries", "content": "Formally, a general graph can be represented as G = (V, E,TV,\u03a4\u0395,\u03a6\u03bd,\u03a6\u0395), where V is the set of nodes, EC V \u00d7 V is the set of edges, Ty and Te are the sets of node types and ov: V \u2192 Ty and E: E\u2192TE are functions that map each node and edge to its respective type. To facilitate the expression of graph relationships, we introduce the notation N(v) to denote the set of neighbors of node v, P(u, v) to represent the set of paths connecting nodes u and v, pu,v as a specific path between nodes u and v, and d(u, v) as the minimum number of edges on any path between nodes u and v."}, {"title": "3.2 Task Definition", "content": "The core of instruction tuning is to involve as diverse a range of tasks as possible to enhance the model's generalization capabilities across different tasks. Therefore, in the context of graphs, we collect various graph tasks with diverse challenges, spanning from structural analysis to predictive inference. To comprehensively assess LLM's capabilities in addressing graph tasks, we categorize tasks according to their target answer type. The answer type delineates the nature of the output required for a graph task. We identify seven distinct answer types: node, pair, count, boolean, path, graph and link prediction. Node task seeks to identify specific nodes within the graph. Pair task seeks to identify node pairs connected by specific relationships or properties. Count task requires counting the number of certain nodes or paths. Boolean task provides a true/false answer to indicate the existence of specific structures. Path task necessitates finding a sequence of nodes that connect two specified nodes. Graph task demands extracting a subgraph represented as a set of node pairs. Link prediction task, different from previous answer types, aims to infer missing edges between nodes based on observed patterns of existing data.\nFor each answer type, we design a set of tasks, where each task requires a specific graph algorithm. All the tasks are listed in Table 1, along with their category, mathematical description, and an example. Furthermore, we subdivided each task into 2 to 4 sub-tasks, which share the same graph algorithm but focus on different node or edge types. For example, the Find neighbors task can be subdivided into sub-tasks like finding the brand of a product and finding the \"also_view\" product of a product. These fine-grained sub-tasks could facilitate a more detailed analysis of generalization, which will be introduced in Section 3.3."}, {"title": "3.3 Evaluation Splits", "content": "To assess the generalization capabilities of the fine-tuned LLM on graph tasks, we propose three distinct types of unseen tasks: unseen sub-tasks, unseen domain, and unseen answer type. Each unseen type offers unique insights into the LLM's ability to adapt and perform on novel challenges beyond its training data.\nUnseen sub-tasks evaluate the LLM's capacity to apply similar graph algorithms to sub-tasks slightly different from the ones seen during training. For instance, a model may be trained to find the shortest path between products and tested to find the shortest path between brands in an e-commerce network.\nUnseen domain tasks evaluate the LLM's adaptability with graphs from out-of-domain networks. While the algorithms remain consistent with those learned during training, new node and edge types, and graph structures are introduced, testing the LLM's generalization across different domains.\nUnseen answer type tasks push the boundaries of the LLM's capabilities by requiring it to generate answer types not encountered during training. Evaluating the model on these tasks assesses its capacity to innovate and extrapolate beyond its training data to develop new graph algorithms.\nGenerally, these three evaluation types collectively provide a comprehensive assessment of the LLM's generalization abilities across various dimensions of unseen tasks, which may bring useful insights into graph instruction tuning."}, {"title": "3.4 Data Collection", "content": "In this section, we outline the strategies and pipelines used to collect our dataset.\nGraph Sampling. Given the impractical size of the original network against LLMs' limited context, we sample subgraphs from the original network and task LLMs over the subgraphs. To generate a subgraph, we sample an ego graph with a radius of 2, centered around a designated set of nodes. However, it's imperative to note that the number of nodes grows exponentially with each increment in hop count. Thus, we implement edge downsampling at each step. This downsampling process involves imposing a maximum limit on the number of edges for each type or establishing a ratio for downsampling. Different downsampling strategies can yield different graph structure distributions, which is useful for cross-domain generalization analysis.\nNode De-identification. Given our objective of assessing LLMs' capacity for reasoning graph structures, textual information such as node names or titles becomes extraneous. To mitigate the potential influence of such textual data, we opt to de-identify nodes by representing them solely with their node type and a unique ID. For instance, a product in an e-commerce network might be denoted as \"product11\".\nQuestion-Graph Collection. Each sample in our dataset contains a question as input and a graph as the context. Given that link prediction necessitates inductive reasoning, while the other answer types involve structure-based queries, distinct pipelines are developed to generate question-graph pairs.\nFor link prediction, we initiate by randomly sampling positive and negative samples in the form of (head, relation, tail) triples. Then, to augment the local structural understanding of the head and tail nodes, we sample a subgraph centered at each of these nodes.\nConversely, for the structure-based query tasks, we start by selecting two random nodes from the original graph and subsequently sampling subgraphs. Task-specific requirements dictate the identification of nodes within the subgraph that may harbor an answer, and graph algorithms are applied accordingly to uncover these answers. It is noteworthy that since the subgraph is sampled without considering the specific task, the resultant graph structure"}, {"title": "3.5 Graph Representation", "content": "Choosing the appropriate format for prompts is essential when utilizing LLMs, as it significantly affects the model's capacity to accurately interpret and process the information. We explore three primary prompt types: natural language, JSON, and DOT format.\nNatural language prompts are versatile and intuitive for LLMs, offering a broad range of applications due to their human-like conversational style. Meanwhile, the JSON format for adjacency lists offers a structured, efficient means of information representation, aligning with LLMs' systematic processing capabilities for precise tasks. Additionally, the DOT format, a standard graph description language (code), enables a visual depiction of network relationships, beneficial for analyzing complex connections. We will delve deeper into their implications for LLM performance in Section 4."}, {"title": "3.6 Graph Instruction Tuning", "content": "To graph instruction tune the LLMs, we concatenate each sample's question Xq with its graph representation Xg to form the prompt and train the LLM to predict the answer Y based on the prompt. We follow the implementation of Wang et al. (2023) to use the original auto-regressive training objective and mask the prompt tokens from loss computation. The loss function is\nL = \\sum_{i} log P_{\\theta} (y_i | X_q, X_g, Y_{<i})\nwhere yi is the ith token in the Y."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Settings", "content": ""}, {"title": "4.1.1 Datasets", "content": "We construct two separate domain graphs from two distinct datasets:\nAmazon Metadata (Ni et al., 2019) contains product metadata across 29 general categories on Amazon. We extract products, brands, and categories as nodes, connecting them via attributes \"also_buy\", \"also_view\", \"brand\", and \"category\". The graphs are built using metadata from the CDs_and_Vinyl, Movies_and_TV, and Arts_Crafts_and_Sewing categories.\nMAPLE (Zhang et al., 2023b) is derived from the Microsoft Academic Graph, featuring 19 scientific fields. We extract the authors, papers, and venues as nodes, and created edges using the \"citation\", \"authorship\", and \"publication\" relationships. This graph utilizes subjects Political_Science, Computer_Science, and Geology from this dataset.\nWe collect 800 samples for each sub-task and divide them into training and test sets with a ratio of 7:1. The statistics of the collected graphs and datasets are presented in Table 2."}, {"title": "4.1.2 Models and Training", "content": "Models We perform graph instruction tuning with the Llama-2 7B (Touvron et al., 2023), Mistral 7B (Jiang et al., 2023), and Gemma 7B (Team et al., 2024) models and compared them with their instruction-tuned versions, which are not explicitly tailored to process structural information, to illustrate the benefits of our special graph instruction tuning.\nFine-tuning We employ LoRA (Hu et al., 2021) as our parameter-efficient fine-tuning approach. To ensure all models can access complete graph information, we train and test all models using samples that could fit within Llama-2's 4k context window. To assess the LLM's generalization to unseen sub-tasks and unseen domains, we train the models on part of the sub-tasks for each task, leaving the rest as the unseen sub-tasks. We also train the models on each domain separately, leaving the other domain as the unseen domain. We conduct a separate training for the evaluation of unseen answer types."}, {"title": "4.1.3 Metrics", "content": "In our experiments, we evaluate performance using two key metrics, the Exact Match (EM) and the F1 score. Specifically, we use EM for the Count, Boolean, and Link prediction tasks, and F1 for the Node, Pair, Path, and Graph tasks. For the Path task, we treat each path as a single value and calculate the F1 score between the extracted and the ground truth paths."}, {"title": "4.2 Results", "content": ""}, {"title": "4.2.1 Graph Representation", "content": "Scalability Table 3 presents the average length in tokens and the maximum graph size in a 4k context concerning node and edge number for each of the three graph representations, natural language (NL), JSON, and DOT, in the two datasets. It is shown that natural language has the most compact representation and can handle the largest graph in a limited context budget.\nPerformance Table 4 presents the performance of both vanilla LLMs and graph instruction-tuned LLMs on the test sets of two datasets. The results reveal notable improvements in all tasks when comparing the graph instruction-tuned models with their text instruction-tuned counterparts. This suggests that the LLMs fine-tuned on our benchmark exhibit an enhanced understanding of graph structures, leading to improved reasoning capabilities for answering questions. Notably, the graph representations in JSON format consistently outperform those in other formats across various tasks, yielding the best overall performance for all three models.\nFurthermore, we conduct studies to compare how different graph representations perform with different scales of LLMs. Concretely, we instruction-tune both Llama-2-7b and Llama-2-13b on the Amazon dataset with the three graph representations. As illustrated in Figure 2, the observation is in line with our previous finding that JSON format is the best bridge for LLMs interacting with graphs and can yield the best performance on both scales.\nWe postulate that this superiority of JSON representations stems from their clearer structural depiction compared to natural language. Moreover, JSON is a more prevalent format in the pre-training data compared to DOT. Consequently, models trained with graph instruct-tuning tend to find JSON particularly effective in comprehending and reasoning about complex graph structures."}, {"title": "4.2.2 Sub-task Generalization", "content": "As mentioned in Section 3.3, we show the performance of models over the in-domain seen and unseen sub-tasks under each answer type. In Figure 3, all models present an excellent generalization on the unseen sub-tasks of Node, Pair, Bool, and Graph tasks, with a small"}, {"title": "4.2.3 Domain Generalization", "content": "To evaluate the domain generalization of the instruction-tuned model, we compare the models separately trained on the two datasets by their performance on the unseen sub-tasks of both in-domain and out-of-domain datasets. This approach allows us to assess their performance on unseen tasks in cross-domain scenarios. Figure 4 demonstrates the averaged performance of all unseen sub-tasks in the corresponding dataset except for the link prediction due to the conclusion from Section 4.2.2. In most scenarios, the model trained on a different domain has an acceptable performance drop compared to the model trained on the tested domain. In addition, the models trained on the Amazon Metadata network have a smaller performance drop (6.43% for Llama-2, 1.41% for Mistral and -0.49% for Gemma) than the models trained on the Maple network (10.62% for Llama-2, 3.67% for Mistral and 4.53% for Gemma). According to the statistics in Table 2, the subgraphs from the Amazon Metadata network are generally larger than the subgraphs from the Maple network. This may indicate that training on larger graphs can better generalize the model to smaller out-of-domain graphs."}, {"title": "4.2.4 Answer type Generalization", "content": "In Table 5, we aim to assess the capacity of instruction-tuned LLMs for generalizing across different answer types. As highlighted in Section 3.3, this represents a particularly challenging scenario due to the potentially large discrepancy between the training tasks and the testing tasks. To this end, we specifically exclude Pair, Bool and Graph from the training dataset, and subsequently instruction-tune the LLM as Mistral-GraphInst-masked.\nRegarding the results, Mistral-GraphInst-masked indicates compromised performance on the unseen Pair, Bool, and Graph tasks, a direct consequence of their absence during training. Despite this, it still manages to surpass Mistral-Inst, which is not fine-tuned with graph structures, in terms of performance. The findings suggest that our instruction design effectively enables the LLM to grasp structural information and apply it to successfully tackle questions beyond its initial training scope."}, {"title": "4.2.5 Case Study", "content": "To demonstrate the effectiveness of graph-based instruction tuning, we explore the model's effectiveness in identifying the shortest path between two non-product nodes within an undirected graph from the Amazon dataset. This task highlights the significant challenges faced by LLMs, including the need to comprehend complex graph structures and apply graph theory algorithms within a computational environment that requires processing large amounts of data and evaluating many pathways at once.\nAs depicted in Figure 5, Mistral-GraphInst, with its specialized tuning for graph dataset analysis, can better overcome these challenges by bridging the gap between natural language and computational graph theory. Unlike Mistral-Instruct mainly optimized for broad language tasks, Mistral-GraphInst is adept at navigating the intricacies of graph structures, enabling it to perform sophisticated analyses like shortest path discovery with higher precision and efficiency. Despite still occasionally missing a few shortest paths, Mistral-GraphInst's capability of handling complex network dynamics positions it as a superior tool for tasks demanding in-depth exploration of graphs, thereby advancing our ability to interpret and analyze complex data structures."}, {"title": "5 Conclusion", "content": "In this paper, we investigate instruction-tuning LLMs on graph-related tasks. We first construct a dataset that contains comprehensive graph-related tasks from the academic and e-commerce domains. We then conduct extensive experiments to explore the best representation for LLMs to understand graphs and gain insights into the generalization of graph instruction-tuned LLMs to different kinds of unseen tasks. Future studies can consider applying such instruction-tuning techniques to graphs from other domains."}]}