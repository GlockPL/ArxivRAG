{"title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "authors": ["J. Pablo Mu\u00f1oz", "Jinjie Yuan", "Nilesh Jain"], "abstract": "Large pre-trained models (LPMs), such as large language models, have become ubiquitous and are employed in many applications. These models are often adapted to a desired domain or downstream task through a fine-tuning stage. This paper proposes SQFT, an end-to-end solution for low-precision sparse parameter-efficient fine-tuning of LPMs, allowing for effective model manipulation in resource-constrained environments. Additionally, an innovative strategy enables the merging of sparse weights with low-rank adapters without losing sparsity and accuracy, overcoming the limitations of previous approaches. SQFT also addresses the challenge of having quantized weights and adapters with different numerical precisions, enabling merging in the desired numerical format without sacrificing accuracy. Multiple adaptation scenarios, models, and comprehensive sparsity levels demonstrate the effectiveness of SQFT. Models and code are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "sections": [{"title": "Introduction", "content": "Despite several limitations, such as hallucinations and a significant computational footprint, large pre-trained, foundation, or frontier models have become integral to numerous applications, including language understanding and code generation. These models are trained with extensive corpora on thousands of graphics processing units (GPUs), resulting in outstanding zero-shot performance across various tasks and datasets. However, it is frequently the case that they must be adapted to improve their performance on new tasks or data.\nLow-rank adapters (LoRA) (Hu et al., 2022) have demonstrated their effectiveness in model adaptation. However, when LoRA is combined"}, {"title": "Methodology", "content": "SQFT fine-tunes large pre-trained models (LPMs) in an efficient multi-stage approach that includes (1) Sparsification, with an optional reduction in the numerical precision, i.e., Quantization, (2) Fine-tuning with Neural Low-rank Adapter Search (NLS), (3) Sparse Parameter-Efficient Fine-Tuning (SparsePEFT) with optional (4) Quantization-awareness. Figure 2 illustrates the alternative model compression and adaptation pipelines that were explored. In the following sections, we discuss the details of each stage and the benefits of accelerating inference and model serving."}, {"title": "Sparsification and Quantization Stage", "content": "As shown in Figure 2, at the beginning of all possible pipeline configurations, SQFT employs an effective method to induce sparsity in the model. For a given weight matrix $W \\in \\mathbb{R}^{m\\times n}$, with entries $w_{i,j}$ s.t. $W = (w_{i,j}), 1 \\leq i \\leq m, 1 \\leq j \\leq n$, an arbitrary scoring function, $\\Psi$, is assigned to the proposed solution. This function determines the relative importance of $w_{i,j}$ compared to the other weights in $W$. $\\Psi$ can be formulated in various ways. For instance, $\\Psi(W) = |W|\\cdot || X ||_2^2$, where $X$ represents sampled feature input activations, as proposed by Sun et al. (2023). However, it is important to highlight that the proposed end-to-end model fine-tuning solution, SQFT, can utilize any other scoring function. Leveraging the scores from $\\Psi$ and a desired level of sparsity, $s$, we derive the"}, {"title": "Fine-tuning with Neural Low-rank Adapter Search (NLS)", "content": "Given the sparse quantized weights, $W^Q_P$, SQFT recovers any drops in accuracy induced by the compression schema and fine-tunes these weights for a specific downstream task. As shown in Figure 2, SQFT employs Neural Low-rank Adapter Search (NLS) (Munoz et al., 2024a) instead of vanilla Low-rank Adapters (LoRA) (Hu et al., 2022), and fine-tunes sparse and quantized models. To justify using NLS, traditional LoRA adapters require assigning the values for several hyperparameters, including their rank $r$, and the subset of modules where these adapters will be placed. Determining these hyperparameters can be a challenging endeavor. To alleviate this limitation, SQFT extends NLS' weight-sharing techniques to facilitate the discovery of optimal adapter configurations from a space of elastic adapter configurations. In other words, instead of having a fixed value for the rank, $r$, we enable elastic configurations, $C = [C_1,..., C_n]$, S.t., $r \\leftarrow C_i$ depending on the activation of the corresponding"}, {"title": "SparsePEFT", "content": "Fine-tuning the sparse quantized model with elastic adapters effectively improves the model's performance on a downstream task. However, as illustrated in the middle and right part of Figure 1, a challenge arises when dealing with sparse or quantized weights and dense adapter weights: merging them will i) result in the loss of sparsity on the model's weights or ii) be unable to merge due to different numerical precisions. Aiming to address the first limitation, we propose an effective strategy, Sparse Parameter-Efficient Fine-Tuning (SparsePEFT), to make adapters sparsity-aware. As depicted in Figure 3, SparsePEFT applies a binary mask $M$ derived from the initial sparsification of $W$. This mask is used to sparsify the adapters matrix (denoted as $B_A$) into $L_P$. The process can be formulated as:\n$L_P = (B_A) \\circledast M$,\nwhich is activated during the fine-tuning process for sparsity awareness. SparsePEFT enables the merging of the sparsified weights $W^P$ and the adapter weight $L_P$ without sacrificing the sparsity induced early in the compression pipeline as follows,\n$W^P \\leftarrow W^P + L^P$.\nIn addition to preserving sparsity, SparsePEFT demonstrates comparable (even better) accuracy compared to fine-tuning with dense adapters. Extensive experimental findings substantiate the advantages of SparsePEFT, as detailed in Section 3.\nAlthough SparsePEFT can effectively preserve the model's sparsity, it presents additional challenges when merging with quantized models, the"}, {"title": "Quantization-aware SparsePEFT", "content": "Building upon the concept of SparsePEFT, we propose Quantization-aware SparsePEFT (QA-SparsePEFT), an extension of SparsePEFT for sparse quantized models. QA-SparsePEFT integrates quantization awareness into SparsePEFT. In most common quantization schemes, the zero point and scales for the target quantized tensor are determined during the quantization process. Within the QA-SparsePEFT stage, the zeros and scales of the sparse quantized weights, $W^Q_P$, of the based model are shared with the adapter. The elastic adapters can then be quantized smoothly with the shared fixed zeros and scales, enabling quantization-aware fine-tuning. Formally, given the sparsified pre-trained weight $W^P$, sparsified adapter weight $L^P$ obtained from SparsePEFT, zeros $z$ and scales $s$ from the quantization of $W^P$, the quantization process in the proposed QA-SparsePEFT can be formulated as:\n$W^m_P = clamp \\left( round \\left( \\frac{W^P + L^P}{s} \\right) + z, 0, Q_p \\right)$,\nwhere $W^m$ denotes the sparse quantized (merged) weight and $Q_p = 2^{n-1}-1$ ($n$ represents the bit-width of the quantized values). Dequantization is the inverse as follows:\n$W^P = s \\cdot (W^m - z)$,\nwhich applies $z$ and $s$ to approximate $W^P$. Through QA-SparsePEFT, we can obtain the fine-tuned, sparse, low-precision resulting model. Moreover, SQFT with QA-SparsePEFT can run the NLS stage using this schema, which allows us to merge the adapters as soon as an optimal configuration has been discovered."}, {"title": "Model Serving and Inference Acceleration", "content": "Accelerating model serving and inference through sparsification and quantization techniques has shown significant efficacy across various hardware platforms and kernels, demonstrating remarkable speedups. However, adding adapter modules for PEFT with a sparse or quantized model (as shown in Figure 1) introduces computational overhead during inference due to their non-mergeability. SparsePEFT and QA-SparsePEFT allow adapters to be merged into the sparse and quantized model, which can reduce adapters' redundancy and computational overhead, leading to more streamlined inference processes. Moreover, quantization techniques further enhance acceleration by reducing the model size and computational complexity, but balancing the trade-off between acceleration and maintaining competitive accuracy is essential.\nIn summary, SQFT and its SparsePEFT strategy bring the benefits of adapter merging and maintaining accuracy on sparse or quantization scenarios. The choice between the sparsity level and whether to apply quantization depends on the specific deployment scenario (e.g., task requirements and resource constraints), including the trade-off between model performance, inference speed, and memory efficiency. In the next section, we will delve into further empirical studies to fully understand the strengths and weaknesses of each approach in different settings."}, {"title": "Experimental Results", "content": "We evaluate SQFT on several state-of-the-art large pre-trained models and datasets. Next, we discuss the setup for our experiments."}, {"title": "Setup", "content": "Models SQFT is evaluated on three state-of-the-art models, including Llama-3-8B\u00b9, Mistral-7B-v0.3\u00b2 and Phi-3-Mini-4K-Instruct\u00b3. To study it more comprehensively, we aim to explore SQFT across different models, scales, and settings.\nDatasets and Downstream Tasks Aligned with other works in the LPMs compression and fine-tuning spaces, SQFT is validated on three experimental settings: 1) Grade School Math 8K (GSM8K) (Cobbe et al., 2021), 2) Math reasoning with instruction tuning (following LLM-Adapters (Hu et al., 2023)), including 3 math reasoning datasets: GSM8K, Math Word Problems (MAWPS) (Koncel-Kedziorski et al., 2016), Simple Variations on Arithmetic Math word Problems"}, {"title": "Main Results", "content": ""}, {"title": "Fine-tuning on GSM8K", "content": "We begin our evaluation with Llama-3-8B and Mistral-7B-v0.3, assessing their accuracy in a dense mode and after inducing 50% sparsity without fine-tuning on the GSM8K dataset. Subsequently, we execute various pipelines of SQFT. As described in Table 1, for Llama-3-8B at the 50% sparsity level, SQFT recovers the model's accuracy from 12.5% to 52.5% without employing quantization, while allowing for the merging of adapters without sacrificing sparsity (SparsePEFT) and incorporating quantization into the pipeline results in a minor drop in accuracy to 50.2% when enabling the adjustment to merge adapters (QA-SparsePEFT)."}, {"title": "Math Reasoning with Instruction Tuning", "content": "In addition to fine-tuning on GSM8K, we also investigated the performance of SQFT with Mistral-v0.3 and Phi-3. Since the Phi-3-series models released by Microsoft are the best-suited instruction models for a chat prompt, we evaluate SQFT on three math reasoning datasets for instruction tuning. Table 2 presents the test accuracy for our approaches and baselines. Interestingly, in the full-precision mode (w/o Quantization), our proposed SparsePEFT not only achieves the highest average accuracy (67.5% for Mistral-v0.3 and 77.3% for Phi-3) compared to other approaches but also uniquely allows for the merging of adapters and sparse weights without any loss of sparsity. This result is achieved without needing an expensive search and by utilizing the heuristic detailed in Section 3.1. In the quantization mode, the accuracy of SQFT + QA-SparsePEFT (mergeable) is comparable to the non-mergeable approaches (67.2% vs. 66.4%/67.0% and 75.3% vs. 74.9%/75.5%). This result suggests a need to balance the trade-off between accuracy and efficiency. Fortunately, SQFT + QA-SparsePEFT results in a merged fine-tuned quantized model, eliminating the overhead associated with dense adapters."}, {"title": "Fine-tuning on Commonsense Reasoning", "content": "Besides the mathematical domain of the first two experimental settings, we also explore SQFT in other areas, e.g., commonsense reasoning. We apply SQFT to fine-tuning the Phi-3 model on a set of unified commonsense training datasets with 83K samples for fine-tuning from BoolQ, PIQA, HellaSwag, WinoGrande, Arc-e, Arc-c, and OBQA. Table 3 compares the test accuracy of the evaluated approaches. SQFT obtains a competitive configuration with Shears, LoRA, and GPTQ + LoRA. However, SQFT has the additional benefit of allowing for the merging without losing the previously induced sparsity, both in full-precision and quantized modes. It is worth noting that SQFT with QA-SparsePEFT shows super competitiveness here, i.e., the most efficient model with high accuracy (among all full-precision and quantized cases)."}, {"title": "Hill-climbing to Better Configurations", "content": "The results presented in the previous sections employ the simple heuristic (as detailed in Section 3.1) to obtain a reference configuration from the NLS search space. However, superior configurations can be discovered with an additional budget. We apply a well-designed hill-climbing search algorithm (Algorithm 1 in Appendix), which starts from the configuration derived from the heuristic and explores its neighboring configurations in a hill-"}, {"title": "Exploring a Broader Range of Sparsity Levels", "content": "All our previous experiments employ 50% sparsity as it is moderate and mild. In this section, we explored the behavior of SQFT in a broader range of sparsity levels. As shown in Figure 5, the model's accuracy experiences a significant drop between a sparsity of 60% and 70%. We denote this range as the critical sparsity threshold, representing the boundary at which the model's performance notably degrades. Through our recovery downstream fine-tuning strategy, models with up to 50% sparsity (even with quantization) can achieve comparable performance with the original dense model (represented by the baseline in the figure) on the downstream task. This 50% sparsity can be defined as the optimal sparsity level, as it represents the point of balance where the model maintains high performance while achieving computational efficiency. Moreover, there is little difference in accuracy between our mergeable and non-mergeable approaches, which illustrates the effectiveness of our proposed SparsePEFT."}, {"title": "Cost Analysis of Pipeline Configurations", "content": "The different versions of SQFT's pipelines incur various costs that allow users to choose based on their fine-tuning budget. Table 6 details the characteristics of each pipeline configuration, e.g., whether we can merge the adapters, the precision of the based model and the adapters, and the cost of each configuration. Two assumptions are made regarding model storage, inference speedup, or memory: merging is better than unmerging due to the overhead from the unmerged adapters, and quantization mode is better than full-precision mode. As for accuracy, the mergeable method we propose is competitive with the previous non-mergeable method. Regarding the fine-tuning time, our mergeable method is slightly slower than the non-mergeable method due to the additional mask and adapter calculations. In summary, SQFT with SparsePEFT is the best choice for full-precision mode because it eliminates the adapter's additional path without sacrificing accuracy. Suppose memory usage during fine-tuning is a priority for the quantization mode. In that case, vanilla SQFT (first configuration in Figure 2) is the best choice because it only requires the quantized model with little overhead of different precision adapters. Otherwise, SQFT with QA-SparsePEFT is better because it can ultimately produce a most efficient model that will be of great benefit at deployment time."}, {"title": "Ablation Studies - LoRA vs NLS", "content": "As shown in Table 5, the ablation studies across 30%, 50%, and 70% sparsity highlight the benefits of elastic adapters and the Neural Low-rank Adapter Search (NLS), which enhance the performance of the models fine-tuned by SQFT. Compared to vanilla LoRA, SQFT with SparsePEFT and NLS further reduces the accuracy gap to the dense or non-quantized models. We include more results with additional sparsity levels in the Appendix, which show the benefits of using SQFT with NLS for sparse and quantized models."}, {"title": "Conclusion", "content": "Large pre-trained models often require fine-tuning to downstream target tasks and compression to utilize them in resource-constrained environments. This paper presents SQFT, a low-cost fine-tuning solution for low precision and sparse foundation models. SQFT solves challenges when merging sparse (and quantized) base models and dense"}, {"title": "Related Work", "content": "Generative pre-trained models, often based on the Transformer architecture (Vaswani et al., 2017), require the application of compression techniques to reduce their significant computational cost and to address challenges, e.g., related to memory bandwidth. Classic compression techniques like pruning and quantization have been adapted for the age of LPMs, removing inefficiencies that cannot be tolerated when dealing with billions of parameters. We discuss them in more detail next."}, {"title": "Pruning", "content": "Inducing sparsity, either by zeroing out weights or activations or removing network elements, can improve the efficiency of LPMs during inference, provided that they are executed on a runtime that can exploit sparse patterns. Pruning has a long history (LeCun et al., 1989), but with the advent of LPMs, traditional methods(Hoefler et al., 2021), e.g., Magnitude Pruning (Hagiwara, 1994), have been replaced by new approaches that are suited for the challenges of these models with their large number of parameters. SparseGPT (Frantar and Alistarh, 2023) proposes a one-shot pruning method for transformer-based models that trade minimal accuracy drop for increasing sparsity levels. The method approaches LPMs' pruning layer-wise with an efficient weight reconstruction algorithm that incrementally prunes the weight matrix elements. Wanda (Sun et al., 2023) proposes a more straightforward approach that does not require weight updates, computing a score using the weight magnitude and the norm of input activations. This approach obtains better results than SparseGPT. Recently, BESA (Xu et al., 2024) has improved over SparseGPT and Wanda by targeting individual transformer blocks and allocating sparsity per layer using a differentiable method. These approaches induce sparsity on pre-trained models and are evaluated on zero-shot benchmarks. Our end-to-end solution, SQFT, focuses on further adapting the sparsified models to new tasks or datasets."}, {"title": "Quantization", "content": "With the advent of large pre-trained foundation/frontier models (LPMs), quantization approaches have evolved to address the challenges of scale and memory bandwidth. Due to the high cost of retraining these models to recover accuracy degradation, special consideration has to be taken when incorporating compression techniques, like quantization-aware training in foundation models. Post-training, one-shot quantization methods have prevailed, obtaining quantized versions of large models in hours. LLM.Int8() was among the first Int8 quantization procedures for large-scale transformer-based PLMs (Dettmers et al., 2022). Using vector-wise quantization and mixed-precision decomposition, LLM.Int8() demonstrated that it can effectively confront the outliers that emerge in activations, which makes traditional quantization methods fail in models with more than 6.7B parameters. In a contemporary work, after running thousands of experiments with various large pre-trained models, it was demonstrated that 4-bit parameters can reach optimal performance compared to other bit-precisions in the 3 to 16-bit range (Dettmers and Zettlemoyer, 2023). ZeroQuant (Yao et al., 2022) quantizes GPT-3 models, obtaining a reduction in latency up to 4.16x by utilizing group-wise quantization for weights, token-wise quantization for activations, and layer-by-layer knowledge distillation. SmoothQuant (Xiao et al., 2023) makes activations easier to quantize by smoothing them and compensating this operation with a transformation of the weights, resulting in improved results over Zero-Quant and LLM.Int8(). GPTQ is another good representative of one-shot quantization approaches designed especially for LPMs (Frantar et al., 2022a). GPTQ builds on the learnings from Optimal Brain Quantization (OBQ) (Frantar et al., 2022b) and applies layer-wise quantization to the full-precision weights of a base LPM. We incorporate GPTQ as the default quantization method in SQFT's pre-fine-tuning stage."}, {"title": "Parameter-efficient Fine-tuning (PEFT)", "content": "Due to their large number of parameters, it is too costly to fine-tune pre-trained large models. Updating all their weights to improve their performance in a downstream task might require devices with large memory capacity. PEFT techniques attempt to address this challenge by avoiding the update of all weights in the pre-trained model. For instance, low-rank (LoRA) adapters (Hu et al., 2022) use a fraction (often less than 1%) of additional weights to adapt the model to a new task. LoRA adapters, $B$ and $A$, are utilized to reparameterize a linear projection, $Y = WX$, keeping the weights, $W$, frozen and updating only the low-rank adapter matrices, $A$ and $B$, i.e., $Y = WX + BAW$."}, {"title": "How does SQFT perform without sparsity?", "content": "In the main paper, we explored SQFT with both sparse + non-quantized and sparse + quantized settings. However, we are also interested in what happens to SQFT if there is no sparsity. Here, we investigate SQFT's performance with quantization alone. As shown in Table 10, without sparsity, the quantized model reduces the accuracy from 50% to 36.6%. With the help of fine-tuning, the baseline GPTQ + LoRA improves accuracy to 58.8%. At the same time, our SQFT method further enhances performance, achieving 61.0% accuracy with NLS fine-tuning, demonstrating that NLS outperforms LoRA in the non-sparse setting. However, for SQFT + QA-SparsePEFT, while NLS outperforms LoRA, the accuracy is slightly lower compared to SQFT. The advantage is that it results in an INT4 model. In summary, users must balance accuracy and efficiency based on their requirements to choose the optimal approach."}]}