{"title": "Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment", "authors": ["Aamer Abdul Rahman", "Pranav Agarwal", "Rita Noumeir", "Philippe Jouvet", "Vincent Michalski", "Samira Ebrahimi Kahou"], "abstract": "Offline reinforcement learning has shown promise for solving tasks in safety-critical settings, such as clinical decision support. Its application, however, has been limited by the lack of interpretability and interactivity for clinicians. To address these challenges, we propose the medical decision transformer (MeDT), a novel and versatile framework based on the goal-conditioned reinforcement learning paradigm for sepsis treatment recommendation. MeDT uses the decision transformer architecture to learn a policy for drug dosage recommendation. During offline training, MeDT utilizes collected treatment trajectories to predict administered treatments for each time step, incorporating known treatment outcomes, target acuity scores, past treatment decisions, and current and past medical states. This analysis enables MeDT to capture complex dependencies among a patient's medical history, treatment decisions, outcomes, and short-term effects on stability. Our proposed conditioning uses acuity scores to address sparse reward issues and to facilitate clinician-model interactions, enhancing decision-making. Following training, MeDT can generate tailored treatment recommendations by conditioning on the desired positive outcome (survival) and user-specified short-term stability improvements. We carry out rigorous experiments on data from the MIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT recommends interventions that outperform or are competitive with existing offline reinforcement learning methods while enabling a more interpretable, personalized and clinician-directed approach.", "sections": [{"title": "1 Introduction", "content": "Sepsis is a fatal medical condition caused by the body's extreme response to an infection. Due to the rapid progression of this disease, clinicians often face challenges in choosing optimal medication dosages. Hence, there is significant interest in developing clinical decision support systems that can help healthcare professionals in making more informed decisions (Sutton et al., 2020). In the medical field, many tasks involve sequential decision-making, such as evaluating a patient's evolving condition in the ICU to make informed medical interventions. This is where RL comes in as a promising solution for developing policies that recommend optimal treatment strategies for septic patients (Raghu et al., 2017; Komorowski et al., 2018; Killian et al., 2020; Saria, 2018; Huang et al., 2022).\nThese tools are intended to bolster and assist healthcare workers rather than replace them (Gottesman et al., 2018). Therefore, the reward function employed by these RL algorithms ideally necessitates clinician input to ensure that the policy generates decisions aligned with the domain expert's intentions (Gottesman et al., 2019). However, the majority of existing studies predominantly depend on binary reward functions, signifying the patient's mortality (Komorowski et al., 2018; Killian et al., 2020; Tang et al., 2022). In other words, the reward at each timestep in the patient's history remains zero until the final interval of the episode. This design leaves no room for clinician input to modulate the policy toward the achievement of desirable tasks, such as the stabilization of certain vital signs."}, {"title": "2 Related Work", "content": "Existing works (Killian et al., 2020; Lu et al., 2020; Li et al., 2019) often rely on modeling the patient's medical history using RNNs. These networks struggle with complex and long medical records due to vanishing or exploding gradients (Pascanu et al., 2013), leading to sub-optimal RL policies (Parisotto et al., 2020). Sparse rewards also pose challenges in the learning of optimal policies since it can be difficult to identify a causal relationship between an action and a distant reward (Sutton & Barto, 2018). The sequential design of RNNs aggravates this problem. The low interpretability of model reasoning is another problem, given the high-stakes nature of clinical decision making. It is essential to address these challenges to create reliable decision support systems and improve clinical uptake of machine learning solutions. Transformers (Vaswani et al., 2017) are shown to effectively model long sequences, which enables learning of better representations for treatment histories of patients, potentially yielding more informed predictions.\nIn this paper, we propose the MeDT, an offline RL framework where treatment dosage recommendation for sepsis is framed as a sequence modeling problem. MeDT, as shown in Figure 1, is based on the DT architecture (Chen et al., 2021). It recommends optimal treatment dosages by autoregressively modeling a patient's state while conditioning on hindsight returns. To provide the policy with more informative and goal-directed input, we also condition MeDT on one-step look-ahead patient acuity scores (Le Gall et al., 1993) at every time-step. This enhances the potential for more granular conditioning while facilitating the interaction of domain experts with the model.\nBelow we summarize the main contributions of this work:\n\u2022 We propose MeDT, a transformer-based policy network that models the full context of a patient's clinical history and recommends medication dosages.\n\u2022 We develop a framework to enable clinicians to guide the generation of treatment decisions by specifying short-term target improvements in patient stability, which addresses the sparse reward issue.\n\u2022 We demonstrate that MeDT outperforms or is competitive with popularly used offline RL baselines over multiple methods of OPE such as FQE, WDR and WIS. Additionally, we leverage a transformer network, the state predictor, to serve as an approximate model to capture the evolution of a patient's clinical state in response to treatment. This model enables autoregressive inference of MeDT and also serves as an interpretable evaluation framework of models used for clinical dosage recommendation."}, {"title": "2.1 RL for Sepsis Treatment", "content": "The use of RL in sepsis treatment aims to deliver personalized, real-time decision support. It involves modeling optimal strategies for the administration of treatments, such as VPs and IVs, based on patient data and expert advice. This problem poses a considerable challenge due to the potential for long-term effects associated with these treatments, such as the accumulation of interstitial fluid and subsequent organ dysfunction resulting from excessive fluid administration (Gottesman et al., 2018).\nTo address this issue, Komorowski et al. (2018) propose a value-iteration algorithm using discretized patient data from EHRs for treatment action selection. Subsequent work uses Q-learning with continuous states and discrete actions and employs OPE to evaluate policies (Raghu et al., 2017). Huang et al. (2022) uses DDPG with continuous states and actions to provide precise dosage recommendations. Other works explore model-based RL (Peng et al., 2018) and combined deep RL with kernel-based RL (Raghu et al., 2018) to further improve treatment recommendations for septic patients. Yet, several significant issues still need to be resolved, which currently impede the practical implementation of RL for the treatment of sepsis. Most of these studies assume that agents begin with a baseline reward of zero until the end of treatment. At the final time-step in a patient's history, a positive reward is given for survival and a negative reward otherwise. Since the manifestation of treatment outcomes (mortality) can occur with a delay of several days after decisions are made, it is challenging to identify effective treatment strategies. Shorter-term objectives, such as the stabilization of vital signs, are often overlooked. Additionally, given the wealth of data being generated for"}, {"title": "2.2 Transformer-based Policies", "content": "Another challenge in treatment modeling is introduced by the partial observability of the patient's state at each time-step. A single reading of vital signs provides incomplete information on the patient's well-being. RNNs address this issue by sequentially processing multiple time-steps of data, but face difficulties in capturing a patient's complete state history due to unstable gradients (Pascanu et al., 2013). This may result in incomplete information and, consequently, inaccurate decision-making (Yu et al., 2021). Recent research in RL (Parisotto et al., 2020; Parisotto & Salakhutdinov, 2021; Janner et al., 2021; Tao et al., 2022) is shifting towards attention-based networks (Niu et al., 2021) like transformers (Vaswani et al., 2017; Lin et al., 2022), which process information from past time-steps in parallel.\nTransformers better capture long contexts and can be effectively trained in parallel (Lin et al., 2022). This addresses challenges posed by the sequential processing in RNNs (Wen et al., 2022). The self-attention mechanism in transformers is particularly beneficial, addressing issues related to sparse or distracting rewards. Self-attention, in short, first computes attention weights for information in each time-step by matching their corresponding keys and queries, which are learnable projections of input tokens. Afterwards, these weights are used to compute weighted sums of values corresponding to each time-step, potentially discovering dependencies between distant time-steps. DT (Chen et al., 2021) leverages these advantages for offline RL (Furuta et al., 2021; Xu et al., 2022; Meng et al., 2021), by conditioning a policy on the full history of states, actions and an observed or desired reward-to-go. Building on the DT architecture, we propose MeDT, which integrates additional conditioning via short-term goals for improvements in patient vital signs, yielding a framework for effective sepsis treatment recommendation."}, {"title": "2.3 Off-Policy Evaluation", "content": "OPE is a fundamental problem in RL concerned with estimating the expected return of a given decision policy using historical data obtained by different behavior policies (Uehara et al., 2022). Such an evaluation strategy is particularly useful in situations where interacting with the environment is costly, risky, or ethically challenging, like in healthcare (Sutton & Barto, 2018; Precup, 2000; Gottesman et al., 2020; Sheth et al., 2022). However, OPE is inherently difficult because it necessitates counterfactual reasoning, i.e. unraveling"}, {"title": "2.4 Interpretability", "content": "The need for interpretability is more significant in safety-critical fields such as healthcare (Amann et al., 2020). Despite extensive research, the deployment of deep learning in healthcare has been met with resistance (Yin et al., 2021). This is primarily due to the black-box nature of these networks, resulting from their complexity and large number of parameters. Moreover, attaining interpretability in RL has been a major"}, {"title": "3 Medical Decision Transformer (MeDT)", "content": "We frame our problem as a Markov decision process (MDP), comprising a tuple (S, A, P, R, S'), where S denotes the set of possible patient states, A the set of possible dosage recommendations, P the state transition function, R the reward function and S' is the next patient state. While this framework is well-suited for learning policies via trial-and-error using RL methods, direct interaction with the environment can be risky in safety-critical applications like clinical decision making. To mitigate this risk, we use offline RL, a subcategory of RL that learns an optimal policy using a fixed dataset of collected trajectories each containing the selected actions, observed states and obtained rewards.\nDT (Chen et al., 2021) uses transformers to model offline RL via an upside-down RL approach, where a policy has to select actions, that are likely to yield a specified future reward for a given past trajectory (Schmidhuber, 2019). Our proposed MeDT architecture follows a similar approach for learning policies.\nThe input tokens for the policy model encode past treatment decisions and patient states, as well as the desired RTG. The output at each time-step is a distribution over possible actions. Specifically, we condition the model using RTG $r_t = \\sum_{t'=t}^T R_{t'} = R_\\gamma$, which represents the singular positive or negative treatment outcome at the last time-step, similar to DT. In addition, we propose to condition MeDT on short-term goals, such as future patient acuity scores, or ATG. The acuity score provides an indication of the severity of the illness of the patient in the ICU based on the status of the patient's physiological systems and can be inferred from vital signs of the corresponding time-step. Higher acuity scores indicate a higher severity of illness. In this work, we opt to use the SAPS2 (Le Gall et al., 1993) acuity score as opposed to popular scores such as SOFA (Jones et al., 2009). This is because SAPS2 considers relatively more physiological variables, which we"}, {"title": "3.1 Evaluation", "content": "In online RL, policies are typically assessed by having them interact with the environment. However, health-care involves patients, and employing this evaluation method is unsafe. In this work, we evaluate the learned RL policy in an observational setting, where the treatment strategy is assessed based on historical data (Gottesman et al., 2018). Following the model-based OPE approach, we introduce an additional predictor network based on the causal transformer (Radford et al., 2018).\nThe predictor network, shown in Figure 2, acts as a stand-in for the simulator during inference. It is trained to learn a state-prediction model defined by the distribution\n$P_\\theta(S_t|a_{<t}, S_{<t})$, using a similar architecture as the policy network. This allows us to model how a patient's state changes in response to medical interventions. Rather than introducing a termination model, we use a fixed rollout length of H. The estimated acuity scores can provide more clinically relevant estimates because they indicate how the stability of the physiological state of the patient may change given a treatment policy. While not exact, this approximation can prove adequate for generating reasonable estimates of a patient's physiological dynamics. This enables inferring estimates of patient acuity scores (SAPS2) from predicted states, which can then be used for policy evaluation. Furthermore, during inference, this model allows autoregressive"}, {"title": "3.2 Interpretability", "content": "We utilize the transformer interpretability method introduced by Chefer et al. (2021a) which is based on the principle of information flow. We adapt this algorithm for the decoder transformer architecture of MeDT used in this work. This subsection outlines the mechanisms underlying the computation of relevance scores used to visualize interpretations.\nLet i refer to the input tokens of MeDT. $A^i$ represents the self-attention interactions between these tokens. Based on these interactions, we seek to compute the relevancy map $R^{ii}$. Relevancy maps are constructed with a forward pass through the self-attention layers, where these layers attribute to aggregated relevance maps via the following propagation rules.\nGiven that each token is self-contained prior to attention operations, self-attention interactions are initialized with identity matrices. Thus, the relevancy maps are also initialized as identity matrices:\n$R^{ii} = k_i x_i$ \nThe attention matrix A from each layer is used to update the relevance maps. The gradients $\\nabla A$ are used to average over the heads h dimension of the attention map, to account for the differing importance assigned across the heads of the matrix (Voita et al., 2019). $\\overline{\\nabla A} := \\frac{1}{h} \\sum_h \\nabla_h A$, where y refers to the output for which we wish to visualize relevance. The aggregated attention is then defined as:\n$\\overline{A} = E_h ((\\nabla A A)^+)$ where $E_h$ is the mean over the h dimension and $\\odot$ is the Hadamard product. $^+$ denotes that the negative values are replaced by zero prior to computing the expectation.\nAt each attention layer, these aggregated attention scores are then used to calculate the aggregated relevancy scores as follows:\n$R^{ii} = R^{ii} + \\overline{A} \\odot R^{ii}$.\nThese relevancy scores can then be used to visualize the importance assigned across the input token space in the form of a heatmap. Since future tokens are masked in transformer decoders, there is more attention toward initial tokens in the input sequence. Hence, to apply these methods to MeDT, we normalize based on the receptive field of attention."}, {"title": "4 Experiments", "content": "In this work, we train and evaluate the performance of MeDT on a cohort of septic patients. The cohort data is obtained from the MIMIC-III dataset (Johnson et al., 2016), which includes 19,633 patients, with"}, {"title": "4.1 Experimental Settings", "content": "a mortality rate of 9%. These patients were selected on fulfilling the sepsis-3 definition criteria (Singer et al., 2016). To pre-process the data, we follow the pipeline defined by Killian et al. (2020). We extract physiological measurements of patients recorded over 4-hour intervals and impute missing values using the K-nearest neighbor algorithm. Multiple observations within each 4-hour window are averaged.\nThe patient state consists of 5 demographic variables and 38 time-varying continuous variables such as lab measurements and vital signs. This work centers on the timing and optimal dosage of administering VP and IV fluids. The administration of each drug for patients is sampled at 4-hour intervals. We discretized the dosages for each drug into 5 bins, resulting in a combinatorial action space of 25 possible treatment administrations. Limiting our focus to IV fluids and vasopressors implies that these are the only treatments within our control; other interventions like antibiotics that the patient might receive are outside the scope of our consideration."}, {"title": "4.2 Baselines", "content": "We compare MeDT to BCQ, NFQI, DDQN and CQL algorithms, which are commonly used baselines in recent works related to offline reinforcement learning (Killian et al., 2020; Tang et al., 2022; Pace et al., 2023). Additionally, we train and evaluate DT and a transformer-based BC algorithm. BC refers to a transformer that takes as input past states and actions, guided by cross-entropy loss on predicted actions, to directly imitate the behavior of the clinician's policy. DT builds on BC by conditioning on returns-to-go. The proposed MeDT differs from DT in that it also conditions on acuity-to-go at each time-step."}, {"title": "4.3 Training", "content": "The transformer policy is trained on mini-batches of fixed context length, which are randomly sampled from a dataset of offline patient trajectories. In our case, we chose a context length of 20, which is the longest patient trajectory in the dataset following pre-processing. For trajectories shorter than this length, we use zero padding to adjust them. During training, we use teacher-forcing, where the ground-truth sequence is provided as input to the model. At each time-step t, the ATG ($k_t$) is set to the actual acuity scores of the state at time-step t+ 1 in the sequence. The prediction head of the policy model, associated with the input token $s_t$, is trained to predict the corresponding discrete treatment action $a_t$ using a cross-entropy loss. The loss for a complete trajectory is averaged over time-steps. Additionally, the state estimator is separately trained to predict the patient's state following the treatment actions. The prediction head of the state predictor model, corresponding to the input token $a_t$, is trained to estimate the continuous state $s_{t+1}$ using a mean square error loss. The models are trained on NVIDIA V100 GPUs. We aggregate experimental results for"}, {"title": "4.4 Results and Analysis", "content": "We evaluate our proposed MeDT network in the autoregressive inference loop with the state predictor (Table 1). As elaborated in the Appendix in Section A.1, we use a naive heuristic to select ATG, and investigate whether the network conditioned on these prompts results in more stable patient outcomes. We compare our proposed approach to multiple baselines and run this loop over only 10 time-steps to avoid the accumulation of state-prediction errors resulting from the autoregressive nature of evaluation. We calculate the average and standard error of the SAPS2 scores of the states estimated by the predictor network for every patient in the test cohort. This cohort split comprises 2,945 patients. We also evaluate all policies over additional methods of OPE such as WIS, FQE and WDR."}, {"title": "4.4.1 Quantitative Analysis", "content": "From Table 1, we infer that the MeDT policy, which is conditioned on both positive RTGs and our chosen ATG heuristic, results in the most stable estimated patient states. The DT framework conditioned only with positive RTGs performs better than BC and other baselines. The learned policies are also evaluated for patients with different severity of sepsis (denoted as low, mid and high severity) based on the SAPS2 score of the initial state. Comparing the models, we observe that the MeDT policy results in more stable states for low and mid-severity patients, while DT performs best for high-severity patients. We hypothesize that, given there are far fewer data samples for patients in the higher severity bracket, MeDT was not able to learn an accurate mapping of patient states to actions given the additional ATG context. This suggests that MeDT requires more samples relative to DT to reach convergence.\nWe run an experiment to evaluate the sample efficiency of DT and MeDT in Figure 9 in the Appendix. We evaluate the performance of the policies when trained on 50%, 75% and 100% of the data from the train split. DT performs better when trained on the smallest 50% split, while MeDT is performant on the 75% and 100% splits. This supports the hypothesis that the additional conditioning used in MeDT has a negative impact on sample efficiency. It is worth noting that given the small size of the sepsis cohort from the MIMIC-III dataset, the 50%, 75% and 100% splits are all low training sample settings relative to standard sizes of training data used in RL. Nevertheless, this is an optimistic observation, given the potential for the exponential growth of data available on large-scale EHRS.\nFigure 4 depicts the results of the FQE, WIS and WDR evaluations. The MeDT policy produces the highest estimated values on FQE and WDR while CQL performs best on WIS. It is worth noting that the MeDT and DT policies show noticeably less variance than the baselines, suggesting they are more robust models. These results indicate that the clinical dosage recommendations based on our proposed conditioning method may have had the intended treatment effects."}, {"title": "4.4.2 Qualitative Analysis", "content": "We qualitatively evaluate the policy of MeDT against the clinician's policy. To ensure accurate analysis, we use ground-truth trajectories as input sequences instead of relying on autoregressive inference, which may lead to compounding errors. In Figure 3a, we conduct a comparative analysis of the mean dose of VPs and IVs recommended by the MeDT policy and the clinician policy, for patient states with varying SAPS2 scores. Figure 3b presents the dosage distribution of IVs and VPs recommended by both the MeDT and clinician policies.\nOur results show that the MeDT policy generally aligns with the clinician's treatment strategy but recommends lower doses of IVs on average. Both policies exhibit a similar trend of increasing medication doses with worsening patient condition, for both VPs and IVs. Figure 3b reveals that the MeDT policy uses more zero dosage instances for both IVs and VPs, compared to the clinician policy. We hypothesize the significant overlap between the MeDT and clinician policy is a byproduct of the imbalanced nature of the dataset, given that over 91% of patient trajectories in the dataset resulted in positive outcomes (survival). As a result, MeDT decides to imitate the clinician policy. Nevertheless, the alignment with the domain expert policy is ideal, especially in this high-stakes task where the algorithm relies solely on pre-existing static data for learning, as it is preferred to assess policies that only recommend subtle changes and closely resemble those of physicians as a precautionary measure (Gottesman et al., 2019).\nFurthermore, previous studies have demonstrated a trend wherein lower dosages are recommended for patients with higher acuity scores (Raghu et al., 2017). This pattern can be linked to the common practice among clinicians of administering elevated dosages to individuals with high acuity scores, often associated with more severe medical conditions and, consequently, higher mortality rates. The challenge arises when algorithms lack data samples featuring high acuity scores coupled with minimal dosages. In such instances, these algorithms default to advocating lower medication doses. Figure 3 demonstrates that the MeDT policy diverges from prior research by refraining from recommending minimal dosages for patients with elevated acuity scores. This serves as an indicator of better generalization and sample efficient properties from MeDT given this negative behavior is not observed.\nIn Figure 5, we visualize the trajectories of multiple patients computed by the state predictor, following treatment actions recommended by both the DT and MeDT policies. The impact of ATG conditioning on patient health is evident, as MeDT leads to more stable trajectories, demonstrating the potential of our framework to generate targeted and improved treatment recommendations by considering both the hindsight returns and ATG at each time-step. In the Appendix in Figure 7, we provide visualizations of some patient trajectories, where we observed that the MeDT policy produced the same or worse action policies relative to DT, with no discernible effect of ATG conditioning. We hypothesize that this may be due to limitations of the dataset, which may not sufficiently cover some regions of the joint space of vital signs, treatment decisions and outcomes, causing the model to be unable to discover some causal relations."}, {"title": "4.4.3 Interpretability", "content": "Currently, RL algorithms typically function as opaque systems (Gottesman et al., 2019). They take in data and generate a policy as output, but these policies are often challenging to interpret. This makes it difficult to pinpoint the specific data features influencing a suggested action. The lack of interpretability raises concerns, hindering experts from identifying errors and potentially slowing down adoption. Thus, clinicians may be hesitant to embrace recommendations that lack transparent clinical reasoning.\nTo improve the interpretability and reliability of our MeDT model for users, we illustrate the relevance assigned by the transformer to input tokens for an example patient trajectory in Figure 6. The relevance across the ATG components are averaged in Figure 6a to better depict the relevance assigned over time, while Figure 6b visualizes the importance assigned to each ATG component. We observe that MeDT assigns relatively more importance to time-steps 2 and 3 for this patient sample. Figure Figure 6b shows that the model considers the conditioning for the Hepatic of higher relevance in its prediction.\nThis allows clinicians to monitor the specific points in time when the model assigns the highest importance, facilitating an assessment of its reasonableness. If the model differs from ground-truth clinician actions, an analysis may reveal which features carry the most weight in the decision-making shift. Additionally, if the model relies on clinically irrelevant features, it signals to clinicians that the recommendation may be unsound. This not only enhances understanding of the model's decision process but also invites future research into the reliability of deep RL decision-making from a clinical perspective."}, {"title": "5 Conclusion", "content": "In this work, we propose the Medical Decision Transformer, a novel reinforcement learning approach based on the transformer architecture. It models the full context of a patient's medical history to recommend effective sepsis treatment decisions. During training, our framework conditions the model not only on hindsight rewards but also on look-ahead patient acuity scores at each time-step. This enables clinicians to later interact with the model and guide its treatment recommendations by conditioning the model on short-term goals for patient stability. For autoregressive evaluation of our proposed approach, we present a separately trained state predictor that models a patient's clinical state evolution given a sequence of treatment decisions. Our experimental results demonstrate the potential of MeDT to bolster clinical decision support systems by providing clinicians with an interpretable and interactive intervention support system."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Acuity Scores and Heuristic", "content": "While many works use SOFA scores for sepsis, recent work shows that both SOFA and SAPS2 predict septic patient mortality effectively (Morkar et al., 2022). We chose SAPS2 as it considers more physiological variables (12) compared to SOFA (6), providing more informative conditioning and allowing flexible user interactions.\nOur approach to varying ATG for MeDT over the sequence involves a simple heuristic: reducing the ATG linearly at each time-step in the sequence. By varying the ATG score for individual systems, we found that the hepatic and hematologic systems had the most positive effect on this cohort of patients."}, {"title": "A.2 Limitations and Future Work", "content": "The MIMIC-III dataset has some limitations, as it only represents a specific geographic area, which could result in an over-representation of certain patient populations and an under-representation of others. Consequently, using the state predictor for evaluation may introduce biases inherent in the dataset on which it was trained. To mitigate these potential biases, we will investigate causal representation learning and pre-training techniques that enhance model robustness. Despite these limitations, MeDT provides a general framework to harness the vast amount of data found in large-scale EHRs from different modalities. Using the proposed framework, researchers can explore the scalability of the transformer architecture to develop systems for effective treatment recommendation for other medical conditions in the future."}, {"title": "A.3 Hyperparameters", "content": "The transformer architecture of MeDT consists of six layers. The models in this work are trained with batch sizes of 64 and a learning rate of 0.0006. The GELU activation function was used within the transformer architecture."}, {"title": "A.4 Ablation Study", "content": "From Table 4, we observe that DT makes a higher performance jump over behavior cloning on patients with negative ground-truth trajectories. This can be attributed to DT closely imitating BC for positive trajectories while being able to adopt a more effective policy for negative trajectories due to RTG conditioning. On the other hand, MeDT appears to make a higher leap on positive ground-truth patients. This is likely due to the larger number of positive samples available for MeDT to learn and condition using ATG."}, {"title": "A.5 Further Visualizations", "content": "Figure 7 shows the patient trajectories generated by the state predictor, following treatment recommendations from DT (red) and MeDT (blue). Specifically, we have visualized the trajectories where the MeDT model produced the same or worse recommendation compared to the DT model. We postulate that this outcome may be attributable to the limitation of the dataset, which could have resulted in the MeDT model lacking adequate information about the given ATG conditioning and state pair."}]}