{"title": "Mixture of A Million Experts", "authors": ["Xu Owen He"], "abstract": "The feedforward (FFW) layers in standard transformer architectures incur a linear increase\nin computational costs and activation memory as the hidden layer width grows. Sparse\nmixture-of-experts (MoE) architectures have emerged as a viable approach to address this\nissue by decoupling model size from computational cost. The recent discovery of the fine-\ngrained MoE scaling law shows that higher granularity leads to better performance. How-\never, existing MoE models are limited to a small number of experts due to computational\nand optimization challenges. This paper introduces PEER (parameter efficient expert re-\ntrieval), a novel layer design that utilizes the product key technique for sparse retrieval\nfrom a vast pool of tiny experts (over a million). Experiments on language modeling tasks\ndemonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms\nof performance-compute trade-off. By enabling efficient utilization of a massive number of\nexperts, PEER unlocks the potential for further scaling of transformer models while main-\ntaining computational efficiency.", "sections": [{"title": "1 Introduction", "content": "The past few years have seen the power of scaling (Kaplan et al., 2020; Hoffmann et al., 2022): increasing\nthe number of parameters, amount of training data, or the computational budget has proven to be a reliable\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual\nknowledge (Geva et al., 2021; Dai et al., 2022), account for two-thirds of the total parameters in a transformer.\nHowever, one drawback of these dense FFWs is that their computational footprint (FLOPs and device\nmemory consumption) is linearly proportional to their parameter count.\nTo break the coupling between computational cost and parameter count, many recent works (Shazeer et al.,\n2017; Lepikhin et al., 2020; Fedus et al., 2022; Zhou et al., 2022) have adopted the Mixture-of-Experts (MOE)\narchitecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense\nFFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the\nnumber of experts is an effective way to improve performance without increasing the inference cost. However,\ntheir experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is\nreached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed\nnumber of training tokens. When the number of training tokens is compute-optimal, MoEs consistently\noutperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number\nof active experts) as a new scaling axis and empirically showed that using higher granularity improves\nperformance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of\nmodel capacity will ultimately lead to a large model with high granularity, corresponding to an architecture\nof an immense number of tiny experts.\nBeyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE\nhas emerged as a promising approach (Aljundi et al., 2017; Chen et al., 2023; Yu et al., 2024; Li et al.,\n2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them\nproperly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new\nones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the\ndata stream can be indefinitely long or never-ending (Mitchell et al., 2018), necessitating an expanding pool\nof experts.\nAlthough both efficient scaling and lifelong learning require MoE designs capable of handling a vast number\nof experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts\nis the Mixture of Word Experts (MoWE) (dos Santos et al., 2023). However, MoWE is language-specific\nand uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al., 2022; Dikkala et al., 2023)\nhighlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned\nrouter scalable to over a million experts remains an open area for exploration.\nThis work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product\nkey retrieval (Lample et al., 2019) for efficient routing to an extremely large number of experts, decoupling\ncomputational cost from parameter count. This design demonstrates a superior compute-performance trade-\noff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation\nmodels. The main contributions of this work are:\n\u2022 Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts\nin previous MoE research, this work investigates the under-explored case of numerous tiny experts.\n\u2022 Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure\n(Kraska et al., 2018) can efficiently route to over a million experts.\n\u2022 New Layer Design: Combining product key routing with single-neuron experts, we introduce the\nPEER layer that expands layer capacity without significant computational overheads. Empirical re-\nsults demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product\nKey Memory (PKM) layers.\n\u2022 Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER\nsuch as number of experts, active parameters, number of heads and query batch normalization on\nlanguage modeling tasks."}, {"title": "2 Method", "content": "In this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture\nof Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as\nexperts. Fig. 2 illustrates the computational process within a PEER layer.\nPEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of\n$N$ experts $\\mathcal{E}:=\\{e_{i}\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding\nset of $N$ product keys $\\mathcal{K}:=\\{k_{i}\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector\n$x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$. Let $\\mathcal{T}_{k}$ denote the top-$k$ operator. Given an input $x$, we first retrieve a subset\nof $k$ experts whose corresponding product keys have the highest inner products with the query $q(x)$.\n$I=\\mathcal{T}_{k}\\left(\\left\\{q(x)^{T} k_{i}\\right\\}_{i=1}^{N}\\right)$  # Retrieve top k experts                                                                     (1)\nThen we apply nonlinear activations (such as softmax or sigmoid) to the query-key inner products of these\ntop k experts to obtain the router scores.\n$g_{i}(x)=s\\left(q(x)^{T} k_{i}\\right)$  # Compute router scores                                                                        (2)\nFinally, we compute the output by linearly combining the expert outputs weighted by the router scores.\n$f(x)=\\sum_{i \\in I} g_{i}(x) e_{i}(x)$  # Aggregate expert outputs                                                                (3)\nProduct Key Retrieval Since we intend to use a very large number of experts ($N>10^{6}$), naively computing\nthe top k indices in Eq. 1 can be very expensive. Hence we apply the product key retrieval technique here.\nInstead of using $N$ independent $d$-dimensional vectors as our keys $k_{i}$, we create them by concatenating\nvectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys $\\mathcal{C}, \\mathcal{C}^{\\prime} \\subset \\mathbb{R}^{\\frac{d}{2}}$:\n$\\mathcal{K}=\\left\\{c \\vert c \\in \\mathcal{C}, c^{\\prime} \\in \\mathcal{C}^{\\prime}\\right\\}$                                                                                                               (4)\nNote that here $\\mathcal{C}, \\mathcal{C}^{\\prime}$ have cardinality $\\sqrt{N}$ and $c, c^{\\prime}$ have dimensionality $\\frac{d}{2}$. So in practice, we choose $N$ to\nbe a perfect square and $d$ to be an even number."}, {"title": null, "content": "This Cartesian product structure of $\\mathcal{K}$ allows us to find the top $k$ experts efficiently. Instead of comparing\n$q(x)$ to all $N$ keys in $\\mathcal{K}$ and selecting the top $k$ matches, we can split the query vector $q(x)$ into two sub-\nqueries $q_{1}$ and $q_{2}$ and apply the top $k$ operations to the inner products between the sub-queries and sub-keys\nrespectively:\n$\\mathcal{I}_{C}=\\mathcal{T}_{k}\\left(\\left\\{q_{1}^{T} c_{i}\\right\\}_{c_{i} \\in \\mathcal{C}}\\right), \\quad \\mathcal{I}_{C^{\\prime}}=\\mathcal{T}_{k}\\left(\\left\\{q_{2}^{T} c_{j}^{\\prime}\\right\\}_{c_{j}^{\\prime} \\in \\mathcal{C}^{\\prime}}\\right)$                                                                      (5)\nThis results in a set of $k^{2}$ candidate keys $\\mathcal{K}^{\\prime}:=\\left\\{c_{i} \\vert c_{i} \\in \\mathcal{I}_{C}, c_{j}^{\\prime} \\in \\mathcal{I}_{C^{\\prime}}\\right\\}$, and it is mathematically guaranteed\nthat the $k$ most similar keys to $q(x)$ from $\\mathcal{K}$ are in this candidate set. Moreover, the inner product between\nthe candidate key and $q(x)$ is simply the sum of inner products between the sub-keys and sub-queries:\n$q(x)^{T} \\frac{c_{i}}{c_{j}^{\\prime}}=q_{1}^{T} c_{i}+q_{2}^{T} c_{j}^{\\prime}$. Hence we can apply the top-$k$ operator again to these $k^{2}$ inner products to get\nthe top $k$ matching keys from the original set of product keys $\\mathcal{K}$. As explained in Lample et al. (2019). This\nreduces the complexity of top $k$ expert retrieval in Eq. 1 from $O(N d)$ as done naively by exhaustive search\nto $O\\left(\\left(\\sqrt{N}+k^{2}\\right) d\\right)$.\nParameter Efficient Experts and Multi-Head Retrieval Unlike other MoE architectures, which often set\nthe hidden layer of each expert to the same size as other FFW layers, in PEER, every expert $e_{i}$ is a singleton\nMLP, in other words, it has only one hidden layer with a single neuron:\n$e_{i}(x):=\\sigma\\left(u_{i}^{T} x\\right) v_{i}$                                                                                                                                                (6)\nwhere $v_{i}, u_{i}$ are not matrices but vectors with the same dimension as $x$, and $\\sigma$ is a nonlinear activation\nfunction such as ReLU or GELU. We omit bias terms here for brevity.\nInstead of varying the size of individual experts, we adjust the expressiveness of a PEER layer by using multi-\nhead retrieval, similar to the multi-head attention mechanism in transformers and the multi-head memory\nin PKMs. In particular, we use $h$ independent query networks instead of one, each computes its own query\nand retrieves a separate set of $k$ experts. However, different heads share the same pool of experts with the\nsame set of product keys. The outputs of these $h$ heads are simply summed up:\n$f(x):=\\sum_{i=1}^{h} f_{i}(x)=\\sum_{i=1}^{h} \\sum_{j \\in \\mathcal{I}_{i}} g_{i j}(x) e_{i j}(x)$                                                                                                                                      (7)\nOne can verify that when only one expert is retrieved ($k=1$) per head, using a PEER layer with $h$ heads\nis the same as using one expert with $h$ hidden neurons:\n$f(x)=\\sum_{i=1}^{h} e_{i}(x)=\\sum_{i=1}^{h} \\sigma\\left(u_{i}^{T} x\\right) v_{i}=V \\sigma\\left(W^{T} x\\right);$                                                                                                            (8)\nwhere $W=\\left[u_{1}, \\ldots, u_{h}\\right], V=\\left[v_{1}, \\ldots, v_{h}\\right]$. In other words, PEER dynamically assembles an MLP with $h$\nneurons by aggregating $h$ singleton MLPs retrieved from a shared repository. Compared to existing MoE\napproaches that use MLPs with multiple hidden neurons as experts, this design allows shared hidden neurons\namong experts, enhancing knowledge transfer and parameter efficiency."}, {"title": null, "content": "Given an MoE layer, we can characterize it by three hyperparam-\neters: the total number of parameters $P$, the number of active parameters per token $P_{\\text {active }}$ and the size of a\nsingle expert $P_{\\text {expert }}$. Krajewski et al. (2024) showed that the scaling law of MoE models has the following\nform:\n$\\mathcal{L}(P, D, G)=c+\\left(\\frac{1}{G}\\right)^{\\alpha}+\\left(\\frac{P}{D}\\right)^{\\beta}$                                                                                                                                  (9)\nwhere $\\mathcal{L}$ is the final test loss, $a, b, g, \\gamma, \\alpha, \\beta$ are constants, $D$ is the total number of training tokens and the\ngranularity $G$ is the number of active experts:\n$G:=\\frac{P_{\\text {active }}}{P_{\\text {expert }}}$                                                                                                                                                                          (10)\nIn order to improve model performance, we need to scale up $P, D, G$. On the other hand, it is essential to\nlimit $P_{\\text {active }}$ because the computational and memory costs are primarily determined by the active parameters\nduring training and inference. Notably, the memory footprint corresponding to $P_{\\text {active }}$ has to be multiplied\nby the number of tokens in a batch, while the memory cost of $P$ is independent of the batch size and sequence\nlength because only one copy of the model needs to be stored.\nAs a result, we want to increase $P, G$ but not $P_{\\text {active }}$. Since the expert size $P_{\\text {expert }}=P_{\\text {active }}/G$ and the\nnumber of experts $N=P / P_{\\text {expert }}=P \\cdot G / P_{\\text {active }}$, this implies that we should decrease the size of each\nexpert, $P_{\\text {expert }}$, and increase the number of experts $N$. Hence we need a large number of small experts.\nIn general, for experts that are MLPs with a single hidden layer. $P_{\\text {expert }}=\\left(2 d_{\\text {model }}+1\\right) d_{\\text {expert }}$ and $P_{\\text {active }}=\\left(2 d_{\\text {model }}+1\\right) d_{\\text {active }}$, where $d_{\\text {model }}, d_{\\text {expert }}$ and $d_{\\text {active }}$ are the hidden dimension of the transformer, the number\nof hidden neurons used in one expert and the total number of hidden neurons activated per token, respectively.\nIn the case of PEER, we use the smallest expert size possible by setting $d_{\\text {expert }}=1$, and the number of\nactivated neurons is the number of retrieval heads multiplied by the number of experts retrieved per head:\n$d_{\\text {active }}=h k$. Consequently, the granularity of PEER is always $G=P_{\\text {active }}/ P_{\\text {expert }}=d_{\\text {active }}/ d_{\\text {expert }}=h k$."}, {"title": "3 Experiments", "content": "3.1 Pretraining isoFLOP Analysis\nWe compare PEER with various baselines using isoFLOP analysis (Borgeaud et al., 2022b). We chose a\nfixed FLOP budget (6e18 and 2e19) and jointly varied the model size and the number of training tokens\nfrom the C4 dataset (Raffel et al., 2020) to obtain isoFLOP curves. Each point on an isoFLOP curve has\nthe same computational cost, and we plot them in terms of their model size and final validation perplexity\non C4.\nFor the dense baselines, we varied their size by changing the number of layers, attention heads and model\ndimensions. For MOE, PKM and PEER methods, we took each of the dense models considered and replaced\nthe FFW layer in the middle block (e.g. in a 12 block transformer, we replace the FFN in block 6) by a\nlayer of MoE, PKM and PEER, respectively.\nIn MoE, we used the expert-choice (Zhou et al., 2022) routing algorithm, which effectively addresses the\nexpert load imbalance issue and generally outperforms token-choice MoEs (see Section 4 for a review and"}, {"title": null, "content": "comparison of these approaches). Each expert has the same size as the original MLPs in the corresponding\ndense model, and we use 128 experts to cover the same range of model sizes as our PEER models. This\ntype of MoE represents standard coarse-grained MoE approaches, which consist of a small number of large\nexperts.\nIn PKM, we used 10242 memories with h = 8 heads and top k = 32 memories were selected per head. We\nalso applied query batch normalization, as recommended in the original PKM paper (Lample et al., 2019),\nto enhance memory usage.\nIn PEER, we used 10242 experts with h = 8 heads and top k = 16 experts per head. By default, we also\nenabled query BatchNorm to increase expert usage. Ablation studies in subsection 3.3 investigate the effect\nof these hyperparameters. Unlike the expert-choice MoE baseline, PEER represents a fine-grained approach\nwhere a large number of small experts are employed.\nAcross all model sizes and methods, we maintained a consistent batch size (128) and sequence length (2048).\nWe calculated the number of training steps by dividing the total compute budget by the FLOPs per training\nstep. Fig. 1 presents the isoFLOP profiles. Compared to the dense FFW baseline, the sparse alternatives\nshift the isoFLOP curves downward and to right because they introduce a larger number of total parameters\n$P$ but utilize a smaller or equal number of active parameters $P_{\\text {active }}$. Given the same compute budget, a\nPEER model achieves the lowest compute-optimal perplexity."}, {"title": "3.2 Evaluation on Language Modeling Datasets", "content": "After determining the compute-optimal model for each method based on the isoFLOP curves, we evaluated\nthe performance of these pretrained models on several popular language modeling datasets, including Cura-\ntion Corpus (Curation, 2020), Lambada (Paperno et al., 2016), the Pile (Gao et al., 2020), Wikitext (Merity\net al., 2016) and the pretraining dataset C4. Table 1 presents a summary of the evaluation results. We\ngrouped the models based on their FLOP budgets used during training."}, {"title": "3.3 Ablations", "content": "Varying the Number of Total Experts The models in the isoFLOP plot depicted in Fig. 1 all have over\na million (10242) experts. Here we conduct an ablation study on the effect of the number of experts $N$,\nwhich determines the total parameter count $P$ in Eq. 9. We selected the model at the isoFLOP-optimal\nposition and vary the number of experts ($N=128^{2}, 256^{2}, 512^{2}, 1024^{2}$) in the PEER layer while keeping the\nnumber of active experts constant ($h=8, k=16$). The results are shown in Fig. 3 (a). As can be seen,\nthe isoFLOP curve interpolates between the PEER model with 10242 experts and the corresponding dense\nbackbone without replacing the FFW layer in the middle block by a PEER layer. This demonstrates that\nsimply increasing the number experts can improve model performance.\nVarying the Number of Active Experts We also conducted an ablation study on the effect of the number\nof active experts $h k$, which equals the granularity $G$ in Eq. 9. We systematically varied the number of"}, {"title": null, "content": "active experts ($h k=32, 64, 128, 256, 512$) while keeping the number of total experts constant ($N=1024^{2}$).\nFurthermore, for a given $h k$, we jointly varied $h$ and $k$ to identify the optimal composition. The resulting\nisoFLOP curves, plotted over the number of heads $(h)$, are shown in Fig. 3 (b).\nThe results indicate that, within the range of values considered, higher $h k$ generally leads to improved per-\nformance. Notably, the optimal $h$ increases as $h k$ increases. However, the performance gradually saturates,\nand increasing the number of active experts also increases device memory consumption and may necessitate\nadditional accelerator devices. Thus in practice, the appropriate $h k$ values should be selected based on the\ntrade-off between performance, device number and computational resource requirements."}, {"title": null, "content": "Expert Usage and Query Batch Normalization Given the presence of over a million experts in the PEER\nlayer, it is natural to inquire how many of these experts are actually selected during inference and whether\ntheir usage is evenly distributed. To analyze this, we kept an accumulated router score, denoted as $z=\\sum_{x} g_{i}(x)$ for each expert $e_{i}$ across all tokens $x$ within the C4 validation set. Here $g_{i}(x)$ is the router score\nused to aggregate the expert output when token $x$ is given as input, with $g_{i}(x)=0$ if expert $e_{i}$ is not selected.\nFrom these accumulated router scores, we can obtain an empirical probability distribution vector, denoted\nas $\\mathbf{z}=\\mathbf{z}^{\\prime} /\\|\\mathbf{z}^{\\prime}\\|_{1}$, representing the distribution of all experts over the C4 validation set. Then we computed\nthe following metrics proposed by Lample et al. (2019) to assess the usage and distribution of experts:\n\u2022 Expert Usage: the fraction of experts retrieved during inference: $\\{\\mathbf{z}_{i} \\neq 0\\}$\n\u2022 Unevenness: KL divergence between $\\mathbf{z}$ and the uniform distribution: $\\log (N)+\\sum_{i} \\mathbf{z}_{i} \\log \\left(\\mathbf{z}_{i}\\right)$\nwhere $N$ is the number of total experts.\nBy default, we also added a batch normalization (BN) layer on top of the query network, as proposed by\nLample et al. (2019) to increase the expert usage during training. Here we study the effect of adding this\nBN layer on the above-mentioned metrics.\nTable 2 presents the expert usage and unevenness for varying numbers of experts, with and without BN. We\ncan see that even for 1M experts, the expert usage is close to 100%, and using BN can lead to more balanced\nutilization of the experts and lower perplexities. These findings demonstrate the effectiveness of the PEER\nmodel in utilizing a large number of experts."}, {"title": null, "content": "We additionally compared isoFLOP curves with and without BN. Fig. 4 shows that the PEER model with\nBN generally achieves lower perplexities. While the difference is not significant, it is most pronounced around\nthe isoFLOP-optimal region."}, {"title": "4 Related Works", "content": "Mixture of Expert Since Shazeer et al. (2017) demonstrated the effectiveness of sparsely-gated Mixtures\nof Experts (MoEs) in efficiently increasing model capacity on GPU clusters, MoEs have emerged as a pop-\nular technique for scaling large models efficiently. Subsequent research (Fedus et al., 2022; Lepikhin et al.,\n2020; Du et al., 2022) has proposed variations to address challenges such as load balancing, communication\noverhead, and training instability. These methods usually replace feedforward (FFW) layers in certain Trans-\nformer blocks with sparsely-gated MoE layers, which consist of multiple FFW layers as experts. Typically\neach expert matches the size of the regular dense FFW layer. Gating scores are calculated for each expert\nand token, and only the top $k$ experts are activated for each token. These methods are known as token-choice\nmethods. More recently, Zhou et al. (2022) introduced the Expert Choice routing method, where experts\nchoose the top $k$ tokens instead of tokens selecting experts. However, both token-choice and expert-choice\nmethods require the top-k operator on a gating score matrix of size $N \\times M$ ($N$: number of experts, $M$:\nnumber of tokens), resulting in a routing cost of at least $O(N)$. This limits their practical application to a\nsmall number of experts (typically less than 128)."}, {"title": null, "content": "Instead of using the top-k operator, some works also proposed using deterministic hash tables as routers\n(Roller et al., 2021; dos Santos et al., 2023). With O(1) average lookup complexity, these methods offer\npotential scalability to a large number of experts. However, these routers are fixed and not learned. Clark\net al. (2022) showed that deterministic routing does not scale as well as trainable routers. Furthermore,\nDikkala et al. (2023) proved theoretically that learned routers offer non-trivial advantages over their fixed\ncounterparts, such as removing spurious directions and identifying latent clusters in data. In contrast to\nprevious works, the proposed PEER layer employs a learned router with sublinear $(O(\\sqrt{N}))$ complexity.\nSince PEER uses lightweight experts, our work is also related to recent studies on parameter-efficient MoEs\n(Wang et al., 2022; Zadouri et al., 2024). These methods utilize parameter efficient fine-tuning (PEFT)\nadapters as experts instead of full-sized FFWs. Their focus is on minimizing the number of parameters\nupdated during fine-tuning, allowing storage of only one copy of the large backbone model. In PEER,\nparameter efficiency refers to the small number of active parameters in the MoE layer, which directly affects\nFLOPs and activation memory consumption during pre-training and inference. However, PEER could\npotentially be adapted to retrieve a large number of PEFT adapters.\nRetrieval-Augmented Models Our proposed method, with its retrieval mechanism for a large number of\nexperts, aligns with the emerging field of retrieval-augmented models. These models facilitate large model\nmemorization by retrieving knowledge from external databases, leading to improved accuracy and efficiency\non knowledge-intensive tasks. Some notable works in this domain include ones by Khandelwal et al. (2019);\nBorgeaud et al. (2022a); Guu et al. (2020). While these methods retrieve data in various formats, for instance,\ntokens (Khandelwal et al., 2019), chunks (Borgeaud et al., 2022b) or knowledge graphs (Kang et al., 2023)\n(see (Gao et al., 2023) for a comprehensive survey on this topic), they differ from the proposed method in\nthat they retrieve data rather than learned functions (experts). This distinction sets our parameter-efficient\nexpert retrieval approach apart from existing retrieval-augmented models.\nEfficient Feedforward Layers Enhancing the efficiency of feedforward networks has been a long-standing\narea of research. Similar to PEER, most approaches are based on the idea of conditional computation\n(Bengio, 2013), where a gating mechanism is trained to determine which subset of neurons to compute. For\ninstance, Davis & Arel (2013) utilized low-rank weight matrix approximation to estimate the sign of pre-\nnonlinearity activations. Neurons with negative activations are omitted as they will produce zeros after the\nnonlinearity. Bengio et al. (2015) explored reinforcement learning to develop an activation-dependant policy\nfor dropping blocks of neurons. More recently, Belcak & Wattenhofer (2023) introduced the Fast FeedForward\n(FFF) layer that employs a differentiable balanced binary tree to select a neuron block for computation.\nDuring inference, only one leaf (corresponding to one block) is selected, hence it has $O(\\log (N))$ complexity,\nwhere N is the total number of blocks in the tree. However, during training, all leaves and intermediate\nnodes are activated for gradient calculation, imposing a training complexity of $O(N)$ and limiting the total\nnumber of blocks. The most relevant work to ours is the Product Key Memory (PKM) (Lample et al., 2019),\nwhose retrieval technique is utilized as the router in the PEER layer. However, PKM retrieves memory\nvectors instead of functions, thus their values cannot vary according to the inputs. As we show in Section 3,\nby changing the memory vectors to input-dependent expert networks, PEER can achieve significantly higher\nefficiency than PKM. Finally, Csord\u00e1s et al. (2023) presented a unified view encompassing FFW, MoE and\nPKM and proposed to change the router normalization function in MoE and PKM from softmax to sigmoid\nor ReLU."}, {"title": "5 Conclusion", "content": "This work introduces a fine-grained MoE architecture that decomposes an extremely wide dense feedforward\nlayer into a large number of small experts. This design is supported by the recent discovery of the fine-\ngrained MoE scaling law. To overcome the computational overhead of routing to a large number of experts,\nwe apply the product keys to efficiently select a small subset of hidden neurons within a wide MLP layer.\nEmpirical analysis using language modeling tasks demonstrate that given the same compute budget, PEER\nsignificantly outperforms dense transformers, coarse-grained MoEs and product key memory layers."}]}