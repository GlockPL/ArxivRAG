{"title": "MULTIMODAL FUSION WITH LLMS FOR ENGAGEMENT\nPREDICTION IN NATURAL CONVERSATION", "authors": ["Cheng Charles Ma", "Kevin Hyekang Joo", "Alexandria K. Vail", "Sunreeta Bhattacharya", "\u00c1lvaro Fern\u00e1ndez Garc\u00eda", "Kailana Baker-Matsuoka", "Sheryl Mathew", "Lori L. Holt", "Fernando De la Torre"], "abstract": "Over the past decade, wearable computing devices (\u201csmart glasses\u201d) have undergone remarkable advancements\nin sensor technology, design, and processing power, ushering in a new era of opportunity for high-density\nhuman behavior data. Equipped with wearable cameras, these glasses offer a unique opportunity to analyze\nnon-verbal behavior in natural settings as individuals interact. Our focus lies in predicting engagement in\ndyadic interactions by scrutinizing verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.\nLeveraging such analyses may revolutionize our understanding of human communication, foster more effective\ncollaboration in professional environments, provide better mental health support through empathetic virtual\ninteractions, and enhance accessibility for those with communication barriers.\nIn this work, we collect a dataset featuring 34 participants engaged in casual dyadic conversations, each provid-\ning self-reported engagement ratings at the end of each conversation. We introduce a novel fusion strategy using\nLarge Language Models (LLMs) to integrate multiple behavior modalities into a \u201cmultimodal transcript\u201d that\ncan be processed by an LLM for behavioral reasoning tasks. Remarkably, this method achieves performance\ncomparable to established fusion techniques even in its preliminary implementation, indicating strong potential\nfor further research and optimization. This fusion method is one of the first to approach \u201creasoning\u201d about\nreal-world human behavior through a language model. Smart glasses provide us the ability to unobtrusively\ngather high-density multimodal data on human behavior, paving the way for new approaches to understanding\nand improving human communication with the potential for important societal benefits. The features and data\ncollected during the studies will be made publicly available to promote further research.", "sections": [{"title": "Introduction", "content": "Wearable computing devices, also known as \u201csmart glasses,\" offer new approaches to quantifying and understanding\nhuman behavior through unobtrusive, high-density behavior tracking. Equipped with sensors such as a video scene\ncamera to monitor the wearer's view, an eye camera to estimate gaze, a microphone to record speech, and an inertial\nmeasurement unit to measure head orientation, smart glasses can capture and respond to human behavior as it unfolds\nin real-time and real-world contexts. There are numerous potential future applications for such systems: for example,\nfacilitating navigation among the visually impaired, or augmenting social cues for individuals with difficulties reading\nnonverbal signals.\nAlthough there has been substantial prior research in laboratory settings  and human-agent interaction , there are still many rich, unexplored opportunities in natural social contexts, for which smart glasses\noffer unique capabilities for study. With smart glasses, we can capture natural social interactions that are not con-\nstrained by the artificial settings of a laboratory, but rather occur in the natural course of daily life, as we seek help,\nshare information, learn, and maintain social bonds through face-to-face communication. These interactions are rich,\nnuanced, and impacted moment-by-moment by multimodal cues, both overt and subtle. The stakes can be high: hu-\nman conflict\nbetween couples, among friends and families, in leadership and governing bodies, and even among\nsocieties occurs when communication breaks down. Face-to-face communication is fundamental in maintaining\ngroup cohesion, preserving mental health, fostering academic learning, and supporting developmental growth.\nEngagement has long been recognized as a key determinant of communication success. While lacking a precise defi-\nnition, engagement can be loosely defined as an individual's attentional and emotional investment during communica-\ntion . The ability to captivate in conversation can determine life-changing interactions,\nwhether acing a job interview or making a favorable impression on a first date. The depth of our engagement and that\nof our partner shapes the outcomes of many social, educational, and professional activities.\nFor the most part, humans automatically and implicitly pick up on the subtle, variable cues that convey engagement\nin a conversation. Yet, building systems that accurately measure and gauge conversational engagement remains a\nformidable challenge. Difficulties arise with the complexity and subtlety of human behavior, its context-dependence,\nand its variability across personal histories and cultural backgrounds. Further complicating matters, social commu-\nnication is inherently multifaceted, with engagement likely to be conveyed across verbal content, nonverbal cues like\ntone of voice, facial expressions, hand and head gestures, and also through the absence of overt signals, such as ex-\ntended periods of silence or few gazes to a partner's face. The unpredictable and dynamic nature of social exchanges\nmakes predicting engagement difficult, as engagement patterns can shift rapidly and vary widely across contexts. Thus,\ntechniques that can perform effectively with minimal or no in-domain training are of particular interest.\nThe dearth of relevant data presents another challenge. Although there is an abundance of openly available datasets of\ndyadic interactions from a third-person viewpoint, such as IEMOCAP , SEMAINE , MEISD , MELD , or NoXi , naturalistic\ndyadic interactions captured from an egocentric viewpoint are scarce. In the past few years, as smart glasses have\nbecome more widely accessible, research has begun to gather egocentric recordings for other tasks, such as skilled\nhuman activity  and user gaze anticipation , though less focused\non interpersonal behavior. These factors pose significant challenges for building socially-aware artificial systems that\naccurately interpret and respond in a manner that feels authentic and engaging to humans. Nonetheless, there is good\nreason to work to meet these challenges. Imagine a system that can gauge audience engagement with a teacher's\nlecture and provide on-the-fly feedback they can use to better engage their students. Or consider assistive technologies\nthat can offer alternative presentations of challenging social signals for those with communication disorders. The\npotential applications are extensive.\nThe contribution of the present work is twofold. We introduce a novel dataset including recordings of natural, un-\nscripted conversations among unfamiliar dyads wearing Pupil Invisible smart glasses, as illustrated in the left-hand\nsegment of Figure 1. This dataset contains conversations between 19 unique dyads, including video and audio record-\nings, eye tracking, and self-reported information on demographic, political, and personality factors from the partici-\npants.\nThe second contribution presents an analysis of this dataset, focusing on predicting participant engagement levels\nthrough post-session self-reports. We compare audio-visual classical fusion techniques  with our novel proposed fusion approach, which uses a large language model (LLM) as a reasoning\nengine to fuse behavioral measures into a multimodal textual representation, a sample of which is displayed in the\nright-hand segment of Figure 1. Our results indicate that this approach achieves performance comparable to the"}, {"title": "Prior Work", "content": "This section reviews classical fusion methods and works on LLMs for analyzing and understanding human behavior."}, {"title": "Classical Fusion", "content": "Curhan and Pentland used speech features (conversational engagement, prosodic emphasis, and vocal mirroring) in the\nfirst five minutes of a simulated negotiation to predict the outcomes of the negotiation . Using these features, they predicted 30% of the variance in negotiation outcomes, demonstrating the value of speech\nfeatures in conversational dynamics. This result suggests that speech features are similarly important in predicting\nconversational engagement. Activity level and mirroring had differing relationships with the outcome depending on\nthe assigned position of participants, showing that perceived status can affect how conversational dynamics relate\nto negotiation success. This interaction poses the question of how status affects how features predict conversational\nengagement.\nPellet-Rostaing et al. used prosodic-acoustic, prosodic-temporal, mimo-gestural, and linguistic features to predict the\nengagement level of the target participant while holding the speaking turn . The study\nshowed the value of visual and audio features, achieving the best results with the prosodic-acoustic, prosodic-temporal,\nand mimo-gestural modalities. Achieving similar results to studies using annotator-defined segments demonstrated that\nannotating engagement at a turn level can be effective.\nIn our study, we attempted to use gaze as a means of gauging dyadic interaction, along with other modalities, as it\nis evidenced by some to have correlations with engagement . Goodwin em-\nphasizes the interconnected nature of gaze behavior among participants in a conversation and points out that the way\nindividuals direct their gaze is not a solitary or random act but is deeply intertwined with the social dynamics of the\ninteraction . This gaze behavior acts as a nuanced signal of a participant's level of attention and\nengagement, reflecting whether they are actively participating or disengaging from the conversation. Furthermore,\nGoodwin explores the concept of gaze withdrawal as a strategic communicative gesture that participants use to signal\ntheir intentions within the conversation, such as making a bid for closure or expressing a particular understanding of\nthe conversation's trajectory.\nMoreover, Ranti et al. underscore the potential of utilizing eye-blink measures as a reliable indicator of an individual's\nsubjective engagement with various stimuli . By closely analyzing the timing of blink inhibition in\nresponse to unfolding scene content, they found that they could uncover the viewers' unconscious, subjective evalua-\ntions of the importance and engagement level of what they observe. A notable observation is that a slower blinking rate\nis often associated with a higher degree of engagement, suggesting that individuals are more absorbed and attentive to\nthe conversation or content presented to them."}, {"title": "Large Language Models (LLMs)", "content": "The capabilities and accessibility of LLMs have opened up a wide range of potential applications, particularly in fields\nrelated to human subjects like psychology. They range from creating synthetic datasets of LLM-generated responses\nin human-less experiments  to providing automated feedback to clinicians .\nOne application involves exploring the ability of LLMs to mimic human behavior because of their potential to reduce\nthe need for human subject experiments and power realistic, interactive interactions. Aher et al. explore the ability\nof LLMs to reproduce human subjects' behavior in classic experiments, such as the \u201cWisdom of Crowds\u201d . Argyle et al. investigate the potential of LLMs as proxies for human sub-populations in social science\nresearch . Tavast et al. evaluate the human-likeness of responses on the PANAS questionnaire\ngenerated by GPT-3 . The feasibility of using LLMs to replace human participants is further\nexplored in ; . Park et al. introduce generative agents powered by LLMs\nthat simulate believable human behavior in a virtual environment , also similarly seen in . There is also a body of work on understanding the personality of LLMs, identifying ways to manipulate\nthe personality embodied by an LLM, and injecting personality into LLMs to predict human responses concerning\nvalues ; .\nAnother application involves exploring the ability of LLMs to understand human behavior. This line of work involves\nevaluating their theory of mind abilities, which refers to the ability to understand the mental states of others, such as"}, {"title": "LLM Fusion", "content": "In this work, we explore the use of large language models (LLMs) to \"reason\" about a social interaction using multi-\nmodal information. Our method involves prompting an LLM to simulate a study participant and answer the end-of-\nsession engagement questionnaire as though it were the participant theirselves."}, {"title": "Socratic Models", "content": "Understanding and interpreting the reasoning of machine learning models is widely recognized to be a significant\nchallenge. Typically, models encode behavioral features into a high-dimensional, abstract vector space, which is then\nmapped onto the target prediction space. To understand a model's inner workings, we usually project these intermedi-\nate data into a space that is more understandable to humans, often through visualization techniques. However, consider\nthe possibility of the inverse rather than allowing the model to obscure information into abstract dimensions, we\ncould direct its operation into a universally interpretable space: the domain of language itself. When studying a topic\nlike human behavior from a computational perspective, AI systems like LLMs that utilize language to \"reason\" about\nsaid topics are worth further study because the language allows for nuance and ambiguity that inherently exists in\nthese fields.\nSocratic Models, named for the ancient Greek philosopher's teaching method through cross-examination, use language\nto integrate information from a diverse set of modalities . Within this framework, pre-trained\nmodels fine-tuned toward specific modalities or behaviors translate their interpretations of inputs into natural language.\nThis translation is formulated into a language prompt to direct the reasoning of an LLM. This approach allows a set of\npre-trained models to \u201cdiscuss\u201d various multimodal information, akin to asking and answering questions in a Socratic\ndialogue. By framing the task as a language-driven exchange, the Socratic Model framework allows pre-trained\nmodels, each specialized in a distinct domain, to perform downstream multimodal tasks without further training or\nfine-tuning.\nThus far, there have been only a few early attempts at applying this framework for prediction. In the domain of image\ncaptioning, one study revealed that an ensemble of models within the Socratic Models framework generated captions\nthat substantially improve the capabilities of the zero-shot state-of-the-art ZeroCap . However,\nwhen compared to fine-tuned models such as ClipCap , performance was not as impressive; yet,\nthis performance gap narrowed considerably when the ensemble was provided a small set of example captions from\nthe training set, suggesting its potential in few-shot learning scenarios .\nThis concept of \"many-to-one\" alignment has also been explored from other angles. ImageBind, for instance, devel-\nops a multimodal representation through a set of image-paired modalities  while LanguageBind\nextends video-language pre-training to a broader range of language-paired modalities . However,\nboth of these models still face the challenge of abstracting information. ImageBind and LanguageBind create \"bind-\nings\" centered around a specific modality but do not explicitly work within that modality itself. Instead, they map a\nprimary modality into an abstract space and then align information from other modalities to this space, resulting in\na multimodal representation that resembles the embedding of the primary modality. While this approach has proven\neffective at abstract tasks such as video-text alignment and image-text retrieval, it is less effective in providing human\nusers with a coherent understanding of its reasoning. Our research aims to follow a similar path but with a crucial\ndistinction: our embedding space is designed to be language itself, which may offer a more direct and interpretable\nframework for multimodal learning.\nPrevious studies have established the value of the language modality in understanding complex social phenomena,\nsuch as rapport , affinity , and, as in the\npresent work, engagement . Various computational methods have been employed to extract\nthis information from language, from rudimentary bag-of-words approaches to more sophisticated neural network\nmodels . Recent advancements, however, have seen\na considerable increase in LLMs adapted to augment tasks requiring social intelligence: notable applications have\nincluded refining persuasive communication for public health campaigns  and identifying adverse\nsocial determinants of health within free-form clinical notes . One\nof the objectives of the present work is to explore the utility of LLMs for behavior analysis of social interactions:\nin our case, estimating the conversational engagement of speakers in a dyadic interaction. The proposed approach\ncenters around employing OpenAI's GPT models to impersonate each participant in the conversation by responding to\nthe self-reported questionnaire in a zero-shot manner. This is achieved through reconstructing the conversation using\nmultimodal-informed prompting that combines behavioral information inspired by the Socratic Models framework\nproposed by Zeng et al.."}, {"title": "Algorithms for LLM Fusion", "content": "The novel LLM fusion approach that we introduce enables an LLM to emulate a participant by creating a multimodal\nprompt: a dialogue transcript of the recording session augmented with textual representations of non-verbal behavior.\nIn this work, these textual representations are formed from the data collected by the smart glasses, multiple pre-\ntrained models, and personality questionnaires, but this method can be extended to contain any number of additional\nbehavioral cues. We aim to evaluate whether this multimodal transcript effectively captures the dynamics of social\ninteraction and can enable an LLM to predict self-reported engagement levels effectively. This work focuses on\nOpenAI's models GPT-4 and GPT-3.5, but the technique could be applied to any LLM."}, {"title": "Modalities", "content": "As described in Section 4, this analysis included information from speech, gaze, and facial expression modalities,\ngiven their straightforward translation into text form and their established significance in signaling engagement.\nThe speech modality serves as the foundation of the multimodal transcript: its representation consists of the dialogue\ntranscript augmented with speaker-labeled segments as described in Section 4.3. The gaze modality is represented by\na string indicating the proportion of time a speaker's gaze remains on their partner's face, rounded to the nearest 10%\nfor brevity.\nThe facial expression modality is represented by a text description of the dominant emotional expression for each\nspeaker-labeled segment of the recording following the methods of existing research  and appli-\ncations , these emotional expressions were defined by the facial action\nunits measured by OpenFace 2.0: happy, sad, surprise, fear, anger, disgust, contempt, or neutral . The neutral label was assigned if none of these labels were applicable. The emotional labels were translated\ninto text as described by Zhao and Patras, which was generated by prompting ChatGPT, achieving state-of-the-art\nperformance on the Dynamic Facial Expression Recognition problem ."}, {"title": "Multimodal Transcript Generation", "content": "The messages provided to GPT use the discrete segments in Whisper's transcription as atomic units to which informa-\ntion from other modalities is added. Consecutive segments with the same speaker are merged to combine speech and\nother modalities into a larger temporal window.\nGPT imitates each participant using the following procedure. Each merged segment of speech forms the basis of a\nmessage provided to OpenAI's ChatCompletion API. For each message, the role is assigned to the assistant if the\nsegment is spoken by the simulated participant or to the user if spoken by the partner. The final user message is\nalways a questionnaire item introduced by the \u201cexperimenter\u201d (see Appendix A for questionnaire details). The final\nassistant message is generated by GPT as a response to the introduced questionnaire item. Prompted transcripts were\ntruncated to five minutes, as previous literature has established that the first five minutes of a conversation is enough\ninformation for humans to predict its outcome successfully . This limitation brought the\nadded benefit of reducing the cost of the experiment."}, {"title": "Experiments", "content": "We conducted two experimental series to predict engagement based on the end-of-session questionnaires, outlined in\nSection 3.4. The first series assessed multimodal fusion using classical models (Section 6.1), while the second series\nused large language models (LLMs; Section 6.2)."}, {"title": "Classical Fusion", "content": "Five standard machine learning techniques were employed to establish a comparative baseline: k-nearest neighbors\n(KNN), support vector machines (SVM), random forests (RF), bidirectional long short-term memory networks (Bi-"}, {"title": "LLM Fusion", "content": "GPT-4 was provided with the multimodal transcript paired with each of the survey items of the engagement question-\nnaire. Note that a few items on the questionnaire explicitly reference laughing or eye contact: despite not providing\nthe model with explicit information on these behaviors, we included these items to explore the capability of the model\nto infer these behaviors with limited information.\nWe performed a set of ablation experiments to explore the significance of various feature sets, notated with the follow-\ning naming convention:\n\u2022 4: This model was provided with the raw dialogue transcription alone.\n\u2022 S: The transcription is preceded by participant survey responses to the personality and beliefs questionnaires\nas (S)ystem instructions.\n\u2022 G: The transcription is enhanced with descriptions of each participant's (G)aze behavior during each speaking\nturn.\n\u2022 F: The transcription is enhanced with descriptions of each participant's (F)acial expression during each speak-\ning turn."}, {"title": "LLM Fusion Results", "content": "We evaluated this technique through two labeling tasks: predicting participants' exact responses and predicting the\nvalence/arousal of their responses. The \u201cexact\" response refers to the specific answer given by participants (a numeric\nrating between 1 and 7). The valence/arousal model categorizes responses based on emotional dimensions: valence is\ndefined as the positive or negative degree of emotion (e.g., pleasure/displeasure), and arousal is defined as the intensity\nof emotion (high or low) . We define valence in terms of the \"disagree\" range, a score\nof 1 (\"strongly disagree')' through 3 (\u201cslightly disagree", "neither agree nor disagree\"), or the\n\u201cagree": "ange, a score of 5 (\u201cslightly agree\u201d) through 7 (\u201cstrongly agree", "4|": "e.g., \"strongly disagree\u201d and \u201cstrongly agree\""}]}