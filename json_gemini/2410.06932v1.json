{"title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models", "authors": ["Daniel Albert", "Stephan Billinger"], "abstract": "In this study, we propose LLM agents as a novel approach in behavioral strategy research, complementing simulations and laboratory experiments to advance our understanding of cognitive processes in decision-making. Specifically, we reproduce a human laboratory experiment in behavioral strategy using large language model (LLM) generated agents and investigate how LLM agents compare to observed human behavior. Our results show that LLM agents effectively reproduce search behavior and decision-making comparable to humans. Extending our experiment, we analyze LLM agents' simulated \"thoughts,\" discovering that more forward-looking thoughts correlate with favoring exploitation over exploration to maximize wealth. We show how this new approach can be leveraged in behavioral strategy research and address limitations.", "sections": [{"title": "Introduction", "content": "A central interest of behavioral strategy research focuses on how cognitive processes and mental representations influence decision-making (Gavetti and Rivkin 2007, Levinthal 2011, Simon 1947). Two prominent approaches have emerged to advance our understanding of these microfoundations of strategy: computational work and human lab experiments. Agent-based computational simulations have sharpened our understanding of performance and learning consequences stemming from differences in individuals' cognition (Csaszar and Levinthal 2016, Gavetti and Levinthal 2000, Knudsen and Srikanth 2014, Winter et al. 2007). Additionally, scholars have increasingly designed experiments to study human responses within various tasks, such as searching for high-performing alternatives in unknown decision-spaces (Bergenholtz et al. 2023, Billinger et al. 2014, 2021, Richter et al. 2023), self-selecting into specific organizational tasks (Raveendran et al. 2022), exhibiting organizational voting behavior (Piezunka and Schilke 2023), and making innovation choices in response to different organizational contingencies (Klingebiel 2022).\nDespite significant strides, a key challenge in advancing behavioral strategy lies in building and testing theories of individual-level cognition and its effects on the revealed decisions that our field typically focuses on. More theoretical development and empirical testing are needed to understand when and why decision-makers follow particular heuristics in specific situations, and what task factors influence their cognitive processes. Without addressing these questions, the field remains limited in its ability to explain effective decision-making. Important steps in this direction have been made in studies investigating strategic intelligence (Levine et al. 2017), emotions and strategic decision-making (Meissner et al. 2021), and the analysis of think-aloud experiments to uncover cognitive processes at play (Laureiro-Martinez et al. 2023, Reypens and Levine 2018). However, the resources and specialized experimental designs required for such studies can be challenging and thus limit the breadth and depth of research in this area.\nIn this paper, we explore how generative artificial intelligence, particularly large language models (LLMs), can advance our understanding of behavioral strategy. We build on Newell and Simon's efforts to use machines as proxies for studying human decision-making by \"simulating human thinking\" (1959, 1961). LLMs have demonstrated remarkable problem-solving capabilities across general and specialized domains. Recent research indicates that their outputs can closely resemble specific human behaviors in areas such as market research (Li et al. 2024), economic behavior (Mei et al. 2024), and social behavior (Ashokkumar et al. 2024). Given their growing ability to mimic human-like reasoning, LLMs offer significant potential for advancing research on cognition and decision-making in the behavioral strategy field.\nOur objective is to evaluate the potential and limitations of LLMs as simulated participants in experimental research in behavioral strategy. To achieve this, we build on the experimental design of the \"alien game\" (Billinger et al. 2021) using LLM agents to generate responses similar to human participants. This experiment is designed to study how participants search a complex decision landscape, when performance-consequences and interactions between decision attributes are unknown a prominent task and challenge of strategic management (Leiblein et al. 2018, Porter and Siggelkow 2008, Steen 2017). We compare our LLM produced results with those from the original human experiments to assess the effectiveness of this approach. We find that the LLM is remarkably effective in reproducing human search behavior and strategic decision-making, finding comparable results to revealed human behavior in this experiment. In addition, we extend the experiment by analyzing LLM agents' simulated \"thoughts\" concerning forward looking and backward looking (Gavetti and Levinthal 2000). We find that increased forward-looking \"LLM cognition\" is associated not only with greater search distance but also with a higher likelihood of ceasing search. This occurs as the agent weighs consequences of immediate benefits of maximizing known income (exploitation) against the potential of discovering better alternatives (exploration).\nA core limitation we find, consistent with recent research on LLMs in other domains, is that LLM-produced results show lower variance than human samples. However, we propose and demonstrate that this reduced variance can potentially be mitigated by sampling from multiple LLM model \"populations.\" By including a proportion of participants from a less capable LLM model, we achieve results that closely match the search behavior and variance of the original human sample. Given our findings, we argue that LLMs can constitute a novel and complementary path in advancing behavioral strategy research.\nThe remainder of the paper is organized as follows: We start by providing the conceptual background to our study, followed by a description and explanation of how we adjust a traditional lab experimental design to make it usable to \u201cLLM agents.\u201d We then discuss our findings, followed by a discussion and conclusion."}, {"title": "Background", "content": "More than seven decades ago, Allen Newell and Herbert Simon developed a computer algorithm they termed the General Problem Solver (GPS), a program able to tackle complex tasks typically reserved for human intelligence, such as chess and military decision-making. However, the motivation to develop the GPS was not to replace human intelligence but instead to study it. They referred to this technique as \u201csimulating human thinking\u201d (Newell and Simon 1961).\nNewell and Simon conjectured that if one can simulate basic human cognitive processes, one may use these simulations to systematically study and build theory on human decision-making. For example, by simulating what they learned from expert chess players, they were able to systematically study the contingencies and consequences of \u201cchunking\u201d of memories, a cognitive pattern that occurs in expert chess players which allows them to categorize a plethora of complex board positions into higher order chunks, which eases them to recall them more reliably when needed (Chase and Simon 1973). This line of research concluded that chess masters learn domain-specific templates, based on pattern recognition, that help them to quickly navigate and effectively perform in the game (Gobet and Simon 1996).\nIn the decades to come after the GPS, chess playing has prevailed as an important setting in strategic management to advance theories on complex learning and decision-making processes. While computers have outcompeted humans in this particular task for decades, it is interesting that AI is still not a substitute for human thinking in chess playing (Gaessler and Piezunka 2023), suggesting that more complex cognitive processes are needed to capture strategic thinking. Several scholars in strategy have started to explore these complex cognitive processes associated with decision-making, arguing that decision makers may rely on heuristics, such as rational deduction, local search, analogical reasoning, and mental experimentation (Farjoun 2008, Gavetti et al. 2005). In this context, the role of cognition has always been highlighted as an important component of strategy (Gavetti and Rivkin 2007) and its potential to be linked to outcomes (Kaplan 2011), which is essential to behavioral strategy."}, {"title": "Studying behavioral strategy in the lab", "content": "Over the last decade, experiments have become an important method for studying behavioral strategy by allowing for a focused examination of causality and questions relevant for strategic decision-making. One starting point of this movement was the realization that simulation modeling, which was and is highly prevalent in the field, often relies on numerous assumptions concerning human behavior that have only received limited empirically testing before. For example, search of computational agents is in most simulations modeled as local search with occasional long jumps. This conceptualization derived from previous literature on NK (Kauffman 1993) as well as insights on how organizations function (Cyert and March 1963, March and Simon 1958). Whether this assumption would hold in the laboratory was then first examined by Billinger et al. (2014) who allowed human agents to search rugged landscapes. One take-away from their study is that human agents, on average, search more broadly than what the strict local search assumptions would suggest (more below). This empirical finding was then confirmed by others who employed similar experiments (Bergenholtz et al. 2023, Richter et al. 2023, Tracy et al. 2017), and who were joined by others interested in the investigation of actual revealed behavior.\nThe simulation of human thinking as well as human lab experiments, however, have fallen somewhat short on simulating just that, the human thinking processes. Consequently, such models may study the consequences of a particular heuristic or mechanism, but not the reasoning or justification of \u201cwhy\u201d an agent draws in a particular situation on one heuristic or rather than another. One way to address this question is the study of human attention, which constitutes a key cognitive process that is central to strategic decision-making as it influences where and how decision-makers search and solve problems (Gavetti et al. 2012, Ocasio 1997). While significant progress was made in studying attention conceptually and empirically (Joseph et al. 2024), this line of inquiry has not put an emphasis on studying actual human thinking processes. The study of these processes has been addressed by research that builds on so-called \"think aloud\" protocols (Ericsson and Simon 1998), where participants in experiments are asked to articulate every thought they encounter while working on a particular task. What humans share in their think aloud protocol is not only supposed to describe where their attention goes during their decision-making process, but also serves as a basis for understanding cognitive flexibility (Laureiro-Mart\u00ednez and Brusoni 2018) and how problem-solving strategies emerge and unfold (Laureiro-Martinez et al. 2023).\nDespite its great relevance for the field of behavioral strategy, think-aloud protocols and other attempts to capture human cognition processes are challenging for at least two major reasons: First, administering think-aloud protocols, where the researcher asks the participant to share every thought that crosses their mind while working on the actual task (e.g., solving a problem) must be carefully applied to an experiment to not interfere with the participants behavior of interest (e.g., how they solve the problem). Ericsson and Simon (1998) dedicated much of their scholarly discussion on this challenge. More recently, researchers showed that think-aloud protocols can constitute a powerful component of behavioral experiments (Leighton 2017). However, to ensure that the think-aloud protocol is not altering the participants' behavior, counterfactual studies would often be useful but are rarely conducted because of the added complexity and cost. Second, think-aloud protocols can exert substantial resource strains on experiments as participants may not take certain decisions in an experiment at the same time as they are supposed to verbalize their thoughts, raising consistency challenges in administering the protocol. Despite these challenges, think-aloud protocols have uncovered important insights showing, for instance, how the manipulation of attention can shift a decision-maker's emphasis from framing to implementation during problem-solving (Laureiro-Martinez et al. 2023). Disentangling cognitive processes with think-aloud protocols holds great potential but remains difficult and costly to implement together with the risk of small deviations to jeopardize the experiment. However, the recent advances in generative large language models may allow complementing human experiments via the study and simulation of human thinking preceding or accompanying human lab experiments."}, {"title": "Large language models mimicking \u201chuman thinking\u201d", "content": "Large language models (LLMs) are a specific class of generative AI models with the objective to generate human-like language outputs in response to language input \u2013 referred to as prompts. These LLMs have seen a particular rise in popularity with the launch of OpenAI's ChatGPT class models in 2022. This recent breakthrough in LLMs is in large part attributed to the \u201ctransformer\u201d architecture (Vaswani 2017), which enables the LLM to interpret and build upon \u201ccontext\u201d in its responses. It contextualizes conceptual meaning \u2013 expressed as a hyper dimensional vector \u2013 to words and word fractions (so called tokens) that take on different values depending on the words (and their vector values) around them \u2013 referred to as the context window. These values are learned by the LLM as part of a massive training process that is largely unsupervised, that is, without human-assigned labeling of data. In this process, LLMs are provided with a vast amount of written content, where the AI trains itself to predict the next word (or token) by relying on a self-attention mechanism that considers the entire content provided (oversimplified: one may think of the LLM reading some of the words while covering and considering the rest of the sentence or paragraph). The learned values for a particular token reflect its mathematical position in the vector space.\nFor example, in a word vector space study, researchers found that the words man and woman have the same distance on a particular dimension as the words king and queen. They demonstrated that through vector arithmetic, the model could arrive at the word \"queen\" simply by calculating \"king - man + woman\" (Mikolov et al. 2013: 746). This \"reasoning\" through a vector space helps explain why language models are particularly capable of solving analogy problems and capturing semantic relationships between words. At the same time, the resulting output based on reasoning is probabilistic in nature, that is, the predicted next word depends on what the overall model it was trained with (typically large amounts of data) and the token probability distribution that was derived for each word. That is, the same context presented to the same LLM twice typically produces each time a new response that results from a distribution of contextualized word \u201cvectors.\u201d A repeated LLM prompting of the same input thus will result in a distribution of responses rather than a single type of response. Given these behaviors, LLMs can be considered to be \u201cimplicit computational models of humans\u201d, or a \u201chomo silicus\u201d (Horton 2023), as an analogy to the economists \u201chomo economicus\", and thereby become the basis for new method developments in behavioral strategy."}, {"title": "LLMs studying human cognition and behavior in different fields", "content": "Various academic fields have started to examine how well LLMs mimic human cognitive behavior and there are many promising results. Overall, the body of literature shows a common thread of LLM's remarkable effectiveness in mimicking many features of human cognitive behavior, while also highlighting their limitations. For instance, in economics, Mei et al. (2024) administered a Turing test to AI testbots and found that ChatGPT4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human. They also found that in strategic situations, LLM behaviors tend to be more altruistic and cooperative than average human behavior. In political science, Bisbee et al. (2024) have examined if LLMs can replace human survey data in public opinion research. The results show that ChatGPT can reproduce averages, but not the variance found in human samples. In psychology, Strachan et al. (2024) showed that GPT-4 matches human performance in various theory of mind tasks, which test the ability to understand and infer others' beliefs and intentions, but, at times, shows more cautious decision-making than humans. In marketing, Li et al. (2024) have examined if LLMs can replace human surveys for perceptual analysis in market research, and they found that LLM-generated data closely aligns with human responses, achieving up to 87% agreement, but with reduced variability in some areas.\nThe abilities observed in LLMs have expanded with newer models having achieved responses that essentially resemble that of human responses and can take on expert roles in certain domains, such as PhD level mathematics (Franzen 2024). This broad level of human-like behavior and expertise along different contexts may pose great opportunities both for general and highly context-specific questions, such as strategic management. For example, Csaszar et al. (2024) examine a highly specialized context of evaluating business strategies and indeed find that LLM's can generate and evaluate strategies at a level comparable to entrepreneurs and investors. At the same time, Doshi et al. (2024), in another study that evaluates strategic decision-making and LLMs, find similar results for aggregate evaluations, but with the caveat that generative AI (comparing multiple LLMs) often produces evaluations that are inconsistent and biased.\nIn sum, many of the reviewed studies conclude that LLMs are often effectively mimicking human cognitive responses and can produce (average) outcomes that are indistinguishable from human responses. However, a common limitation noted in most studies is the lack of the full variability observed in humans."}, {"title": "Reproducing and Extending Experimental Designs in Strategic Management", "content": "In evaluating LLM's ability to mimic human behavior in ways that could prove to be relevant for future research in behavioral strategy, we need to specify an experimental context that meets criteria of tractability and availability of a solid body of prior work. A rich line of research has stressed that search for novel alternatives poses a fundamental task of strategists (Cyert and March 1963, March 1991). The decision-making associated with search is often portrayed as a \"discovery\" process where decisions interact in non-trivial ways (Leiblein et al. 2018, Porter & Siggelkow 2008, Van Steen 2018). A canonical framework in studying search is the NK fitness landscape, a multi-dimensional decision space that allows for varying degrees of performance interactions between decisions (Kauffman 1993, Levinthal 1997). The NK framework has informed behavioral strategy through the use of both agent-based simulations and human lab experiments which resemble a rich body of work that offers a particularly suitable context to explore LLM's simulated behavior. Next, we will provide a general introduction to the NK framework and its suitability to study questions of behavioral strategy under search."}, {"title": "Simulating complex problem spaces", "content": "The NK model was introduced to the field of management by Levinthal (1997) who demonstrated the framework's powerful suitability to study questions of organizational adaptation, the role of selection forces, and environmental change in the presence of epistatic interdependencies, that is, when the performance contribution value of one choice depends on its own state as well as on the states of K of the N-1 other choices. The NK model has seen broad adoption for questions core to the field of strategic management, such as, imitation of competitors (Csaszar and Siggelkow 2010, Rivkin 2000), industry evolution and profitability (Lenox et al. 2006), organizational design and exploration (Siggelkow and Rivkin 2006), and ecosystem innovation (Ganco et al. 2020).\nInterest has emerged around questions of behavioral strategy using the NK model, with a focus on mental representations, cognition, and the consequences on search heuristics (Gavetti and Levinthal 2000, Winter et al 2007, Csaszar and Levinthal 2016). The suitability for this type of study using the NK framework stems from the underlying properties that are \u201ctunable.\u201d Given a set of N decisions that can be configured in a variety of combinations (typically 2^N as for parsimony reasons decisions are fixed to binary states, such as 0 or 1, \u201con\u201d or \u201coff\u201d, \u201chigh\u201d or \u201clow\u201d, etc). Each of the N decisions contributes a performance value, where this value is dependent on the state of the focal decision (e.g., whether it is high or low) and the states of K other decisions. Thus, K denotes the level of complexity, with low values indicating little complexity and high values reflecting great levels of complexity. There are two corner cases: K = 0, where no dependencies between decisions exist, that is, each decision can be optimized without the need to consider any of the other decisions. As a consequence, the performance search space is often described as a \u201csmooth,\" single peaked landscape, where the search for incremental improvements will eventually lead to the global optimum \u2013 i.e., the configuration where each decision is optimized to the highest of its two possible performance values.\nThe other corner case is K = N-1, where every decision is dependent on the states of all other decisions. In such a case, the landscape is simplistically described as \u201cjagged,\u201d that is there are many local optima and a single decision change can fundamentally alter the performance as all decisions are affected by it. For non-zero values of K, scholars have described the search space as a rugged landscape, where local optima have basins of attraction, that is, once near a peak, incremental improvements will lead up to the local optimum \u2013 referred to as \u201chill-climbing.\" In other words, the performances of nearby positions in rugged landscapes are correlated but local optima within a landscape can vary substantially with respect to overall performance. While there are many mediocre peaks, some are particularly high in performance. Incremental search, however, proves cumbersome in such terrain as once attracted to a local basin, one would need to divert search to where performance is in fact lower, to traverse through a valley toward a potentially higher-performing area of the landscape. Because the landscape and its characteristics are not known to searchers but rather need to be discovered, the NK framework resembles a familiar conundrum of myopia and exploitation-exploration trade-offs."}, {"title": "Bringing NK to the lab", "content": "While the majority of NK research relies on simulation modeling (Baumann et al. 2019), lab experiments have investigated humans' revealed search behavior - devising the \u201cAlien Game\u201d to study how humans search rugged landscapes (e.g., Billinger et al. 2014, 2021, Bergenholtz et al. 2023, Richter et al. 2023), and whether and where they search (Billinger et al. 2021). The Alien Game is an experimental setup that utilizes the NK engine as its backbone together with a front-end design that effectively allows participants to play a game of search on NK landscapes with a limited number of trials. Specifically, participants are instructed to envision they had made contact with a new species from out of space (the alien) and that they can design and sell art pictures. The art picture is made up of a combination of ten symbols that can be activated or deactivated, and that the alien's preference for a specific pattern of symbols is not known ex-ante but can be found out by selling such an art picture. This experimental design offers several advantages that serve multiple and important purposes for the study of behavioral strategy: First, by instructing participants about the unknown preferences of an unfamiliar species, this experiment is designed to reduce or remove any prior assumptions or experiences a participant may hold, thereby facilitating comparisons with computational agents. Second, the use of cryptic symbols further limited any wide-spread associations that may otherwise affect the search for combinations. Third, the game offers a limited yet not trivial set of strategy parameters, including the unambiguous measurement of feedback variables, varying task complexities and possibility to operationalize important exploitation-exploration trade-offs.\nIn a recent version of the Alien Game, these advantages are utilized by asking participants to maximize their accumulated income from the sale of art pictures (Billinger at al 2021). With this incentive scheme, participants effectively faced a search challenge with two distinct exploitation-exploration trade-offs: (1) whether and when to stop searching for better alternatives (i.e., \"cashing in\u201d on the best alternative found so far), and (2) where to search in the landscape by either narrowly searching in the neighborhood of existing solutions or searching more broadly in the landscape. The results of these experiments showed that different feedback variables, such as early feedback and immediate feedback, influence these two decisions"}, {"title": "Taking LLM to the lab", "content": "We conducted our main experimental analysis using OpenAI's ChatGPT-40 (model 'gpt-40-2024-08-06'), which was OpenAI's latest frontier model at the time. OpenAI is often considered to be a leader in offering advanced large language models (LLMs). The use of gpt-4o was particularly convenient due to its easy API-interface, allowing us to send and receive data via Python. This integration enabled seamless communication between our NK framework, which we programmed entirely in Python as a standalone local program, and OpenAI's servers. For the baseline comparison, we operated the LLM with Open AI's default settings.\nTo assess the extent to which LLMs can simulate human behavior in the context of the alien game, we took multiple steps to reproduce the experimental design as close as possible to the original studies, making only such changes that are necessary to interface the experimental framework with the LLM. We contacted the original author team to gain access to the published data and additional details on the experimental setup. As a result, we could rely on the same NK landscape used in the original study and use the same participant instructions to produce data that allows for a comparison of data from human agents with data from LLM agents. The changes that we needed to implement in order to make this experiment accessible to the LLM include converting the graphical computer interface that human participants saw as a screenshot in the original instructions into plain text. For example, instead of showing the ten cryptic symbols that participants could switch on or off, we named them along the Greek letters (alpha, beta, and so on) and the LLM had to indicate in a trial which of these symbols it wanted to switch on or off, naming them by the respective Greek letters. The conversion of the graphical interface into text also required that after each trial the LLM received a short reply on what the payoff of that round was and what the current overall wealth (accumulated payoffs) is. In the original experiments, participants would see this information on the screen together with all prior trials and payoffs. To ensure that the LLM would have a similar situation each trial, we set up the LLM to have all text input and output remain in the LLM's context history and in each trial, the LLM would need to be prompted to submit its next art picture configuration including choices for all ten Greek symbols. Specifically, we prompted \u201cConsidering what you know so far, please submit your next trial configuration.\"\nIn the original study, the authors used a within-subject-design and asked each participant to play three separate blocks of the game with 24 trials each - one block for each of the three levels of landscape complexity with K = 0 (low), 5 (medium), and 9 (high). In the original experiment, participants were instructed that there was no correlation between the three blocks (since these represented dealing with three different aliens) and the participants were asked to treat these blocks as completely independent from one another. The original study, for that reason, included controls and robustness tests to rule out unwanted learning effects between blocks (Billinger et al. 2021). For practicality reasons, we ran each level of complexity as an isolated LLM experiment and thereby created a between-subject design, which, as a side-effect, also resolved possible between-landscape learning effects. Finally, the human experiment composed 69 participants, which is why we initially ran 69 independent runs for each landscape that is, each run represents a single simulated LLM agent for that landscape."}, {"title": "Analysis", "content": "In this section, we start by comparing the LLM results with those from the human sample. Next, we extend the analysis by examining the LLM agents' generated thoughts and their relationship with outcome variables. In a third step, we explore the results obtained from sampling multiple LLM models. Finally, we discuss the robustness tests we conducted."}, {"title": "Reproducing a behavioral experiment with LLM", "content": "A main interest of the original alien game study pertains to \u201cwhether to search,\u201d i.e. the time participants take out of 24 trials to search actively for art picture configurations compared to when they tend to stop their search in an attempt to accumulate wealth - that is, selling their best picture for the rest of the trial periods. In Figure 1, we report LLM results (left plot) and the original human results (right plot) of the percentage of participants actively searching (y-axis) per trial period (x-axis), each landscape complexity is shown separately. The LLM results reproduce the overall trend of the original results, indicating a few periods of consistent search across all agents, followed by a relatively consistent decline in actively searching participants with an increase in trials. Notable is a similar level of active search at the end of the experiment of around 20 percent in trial 24 for both the LLM agents and the human participants. The LLM results, however, show greater variation between complexity levels, with LLM agents in very high complexity landscapes (K = 9) stopping their search somewhat earlier than their human counterparts. High complexity landscapes have greater performance variance even among neighboring search positions, which may disincentivize search in favor of wealth accumulation as a poor search trial may incur high opportunity cost.\nAnother interest of the original study was the question of \u201cwhere\u201d participants would search, that is, how many configurational changes they make, and thereby explore, relative to their currently best-performing art picture configuration they are aware of. Consequently, this search distance is measured as the number of configurational differences between a focal search trial and the participant's best prior configuration (also called Hamming distance; see Hamming 1950). In Figure 2, we report for actively searching participants, the search distance (y-axis) per trial (x-axis) in three panels, each of which is comparing human results and LLM results. The panel on the left shows these results for low complexity (K = 0), the center panel shows K = 5, and the right panel shows K = 9. The envelopes show the standard deviation for each graph, which allows to gauge variance in the data samples.\nThe LLM results generally reproduce the original results, that is, that participants early engage in more distant search, followed by more local search with an uptick in search distance toward late trials. However, the LLM overall search distance is generally lower than that of the human study. The search distance mean of the human sample is 2.36 and for the LLM sample 1.31 - the difference is significant with p<0.001. Upon visual inspection, we find that humans across levels of complexity tend to initiate their first search with an average of around four changes to the starting configuration. In contrast, LLM agents tend to only make around two changes. A similar difference can be observed towards the end, whereas in-between the beginning and the end of the experiment, human and LLM agents are much closer in search distances. A notable difference between the human and LLM results appear to lie in the observed variability. The human results show greater variability as reflected in the standard deviation envelopes that are substantially wider than for the LLM agents. (We return to and address this observation in 4.3.)\nTo summarize the first set of results: Our analysis of the LLM results suggests a noteworthy reproduction of the human behavior reported previously (Billinger et al. 2021). Specifically, we find similar behavior with respect to how long participants search a landscape before they focus on maximizing their wealth within the remaining time. We also find a similar behavior with respect to where agents search, that is, initially starting broader, narrowing down search, with an uptick in search distance toward the end of the experiment. An important difference we find in all our LLM results is that search distance tends to be, on average, more local than in the human reported trials."}, {"title": "Analysis of \"thinking\u201d patterns", "content": "Considering our first results, which appear promising with respect to LLM agents reproducing human search behavior, we are also interested in the simulated \u201cthoughts\u201d of the LLM and whether an analysis of its content would provide theoretically useful insights. We therefore ran the same experiment again with a slight change to the prompt, asking the LLM to \u201cthink aloud.\" The observed results did not change between the main instructions (as reported under 4.1) and the additional encouragement to \u201cthink aloud.\u201d However, we found that the LLM provided slightly more thoughts in its responses when asked to think aloud in addition to our main instructions.\nIn this think-aloud extension of our LLM experiments, we focused our analysis on the role of cognitive attention as a key construct underlying the behavioral theory (Gavetti et al. 2012, Ocasio 1997). We are particularly interested in examining how the LLM-produced thoughts relate to the focus of attention in a given trial and how this attention relates to the search behavior. We draw on Gavetti and Levinthal's (2000) theory on the role of backward looking and forward looking decision-making. These authors have stressed that backward looking search is driven by trial-and-error experience, whereas forward looking decision-making is akin to a mental representation that is more predictive in nature."}, {"title": "Measuring types of attention using LLM outputs", "content": "Each trial period, the LLM provides a written response (output) in reference to our prompt to name the next trial configuration the LLM intends to test. We use this output as the simulated raw thoughts. From this written response, we extract text that relates to:\nbackward looking, that is, such statements that reference previous rounds and insights that have been learned in prior rounds.\nforward looking, that is, such statements that reference future trials, general strategy, and everything related to next steps.\nExtracting and labeling statements as either backward or forward looking was done using a separate LLM API interface that we provided with the individual outputs of our original LLM agents after the experiments were completed. Utilizing LLMs for research tasks such as assigning labels and interpreting unstructured texts has shown great potential (Boussioux et al. 2023).\nWe measure attention by comparing the character counts of forward-looking statements to those of backward-looking statements (i.e., the ratio). This measure, albeit coarse, provides a proxy for the relative distribution of attention between past experiences and future predictions. The number of words used in human think-aloud protocols has frequently been employed to capture the direction of cognitive processes and their relative extent, as the quantity of verbalizations has been shown to reflect the focus of attention and the depth of cognitive engagement (Ericsson & Simon, 1993; Chi, 1997; Fox, Ericsson, & Best, 2011). While the LLM deploys no \"cognitive processes\u201d in a human sense, it mimics human reasoning and thought patterns expressed in language. We therefore assess how the LLM balances experiential learning with forward-thinking strategies expressed in its communicated \"thoughts.\"\nWe also count the number of distinct symbols, out of the ten in the art picture configuration, that the LLM specifically mentions in its output. This unambiguous count measure, ranging from zero to ten, helps us capture the breadth of its attention (attention breadth). Note that this measure can differ from revealed behavior, where the LLM may only change one symbol but discusses three or four elements during deliberation."}, {"title": "Regression analysis of LLM outputs", "content": "Following Billinger et al. (2021), we also specify a 2-step Heckman regression model (Heckman 1979) to effectively capture the LLM agents' two stages of decision-making, that is, first whether to search (stopping or not), followed by where to search (search distance). We include relevant covariates from the original study, including initial feedback as the exclusion restriction for estimating the 1st stage (active search), along with our two attention variables. The regression output is included in Table 1 and a list of variable names and definitions is included in Table 2.\nFor the think-aloud analysis, we created a sample of 900 independent LLM agents (i.e., 300 LLM agents for each of the three landscapes) to increase statistical power. Our regression models generally concur with Billinger et al. (2021), and we find similar relationships between the different feedback variables of interest (initial feedback, average feedback, and immediate feedback) and a participant's search distance and propensity to stop their search.\nFor the new attention variables that we include in our study, we find that our variable Attention Breadth is associated with a lower probability of stopping search and greater search distance. That is, a LLM agent \u201cthinking\u201d about a greater number of symbols (i.e., decision attributes) is more likely to make a greater number of changes and continues to search. This finding for the mimicked thoughts of LLM agents may to some extent reflect prior findings of cognitive flexibility, which showed that more deliberate system-2 thinking would base problem-solving on more decision elements (Laureiro-Mart\u00ednez and Brus"}]}