{"title": "ATTACKQA: DEVELOPMENT AND ADOPTION OF A DATASET FOR ASSISTING CYBERSECURITY OPERATIONS USING FINE-TUNED AND OPEN-SOURCE LLMS", "authors": ["Varun Badrinath Krishna"], "abstract": "Retrieval-augmented generation (RAG) on specialized domain datasets has shown improved performance when large language models (LLMs) are fine-tuned for generating responses to user queries. In this study, we develop a cybersecurity question-answering (Q&A) dataset, called AttackQA, and employ it to build a RAG-based Q&A system designed for analysts in security operations centers. The dataset comprises 25,335 Q&A pairs, accompanied by rationales to facilitate fine-tuning and evaluation. 80% of the dataset was generated with help of a lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per second with full 16-bit precision on SambaNova System's SN40L specialized hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and reject low-quality Q&A pairs. In using the dataset for RAG, we demonstrate that fine-tuning open-source embeddings and LLMs can yield superior accuracy compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-40). Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness, enabling the creation of a fully open-source, high-speed RAG and evaluation pipeline with a benchmark for model accuracy.", "sections": [{"title": "INTRODUCTION", "content": "Security operations centers (SOCs) house information security teams who are responsible for detecting, investigating, and responding to cybersecurity incidents using a variety of tools, technologies, and processes. As of 2024, firms with at least $1 billion in revenue spend $14.6 million on SOCs each year (Sadovi, 2024) and 80% of SOC budgets are spent on labor (Ananth). The cost of training a team of 10 SOC analysts to master 7 security tools is estimated at $3.69 million (Cohen, 2023). According to the Hershberger (2023) survey, the top challenges facing SOCs include a lack of expertise in security, too much time spent in investigating alerts, and a slow response time to advanced threats. To address those challenges and to enable quicker attack prevention and recovery, we propose a question-answering (Q&A) system leveraging artificial intelligence to help SOC analysts get quick answers to time-sensitive questions about cyberattacks. Our solution leverages entirely open-source large language models (LLMs) that are becoming increasingly powerful and, on domain-specific datasets, can be tuned to exceed the performance of proprietary LLMs that are many times as large.\nWe used the MITRE ATT&CK\u00ae (The MITRE Corporation, 2024) knowledge base of cyberattack techniques, tools, campaigns, detection approaches, and mitigation approaches to generate a Q&A dataset called AttackQA for use in Q&A systems or general-purpose chatbots. That knowledge base, grounded in real-world observations and updated biannually, was chosen because the ATT&CK\u00ae framework is widely adopted for cyber threat intelligence across the private sector, government, and the broader cybersecurity product and service community (Roy et al., 2023; AL-SADA et al., 2024; Al-Shaer et al., 2020). It is stored in an esoteric database format called Structured Threat Information Expression (STIX), making it ill-suited for direct use in Q&A systems. Hence, we extracted the data and processed it in a way that makes it easier for training and inferencing with LLMs."}, {"title": "RELATED WORK", "content": "The use of LLMs for synthetic dataset generation, curation, and evaluation has been surveyed by Long et al. (2024). Although AttackQA is synthetically generated, it is grounded in the widely reputed MITRE ATT&CK\u00ae knowledge base."}, {"title": "DATASET CREATION FOR Q&A", "content": "In this section, we describe our methodology for creating a Q&A dataset using the MITRE ATT&CK knowledge base."}, {"title": "SUMMARY OF THE SOURCE DATA", "content": "The MITRE ATT&CK knowledge base encompasses seven categories of information, which are detailed in Table 1 along with their corresponding cardinalities.\nThe data for techniques, tactics, software, groups, campaigns, and mitigation approaches include a unique ID, name, description, and URL (an example is provided in Appendix A.1). From that data, we extracted the descriptions as text documents for use in Q&A tasks. The relationships table maps a source type to a target type via a mapping type. Source types include 'software', 'group', 'data component', \u2018mitigation strategy', and \u2018campaign', while target types consist of \u2018technique', 'software', and 'group'. The mapping types include 'uses', 'detects', 'mitigates', and 'attributed-to'. A mapping description was also provided and included in our set of Q&A documents.\nAttackQA was partially generated using manual heuristics, with the remainder produced by LLMs. Each Q&A pair was derived from a single document, eliminating the need for multi-hop reasoning because comprehensive answers did not require information from multiple documents."}, {"title": "DOCUMENT PREPROCESSING", "content": "Newline characters were removed from within individual documents, ensuring that they appeared only between documents in the final retrieval context presented to the generation model. In all the documents, hyperlinks and special tags were replaced with plain text to ensure that neither the embeddings nor the generation models needed to process special tags that would not be encountered in questions and not be expected in answers.\nFrom each document, one to three triplets of {question, thought, answer} were generated, where thought represents the rationale necessary to accurately answer the question. Additional metadata was included in the dataset to enable hybrid retrieval approaches that use a combination of vector search, keyword search, relational querying, etc."}, {"title": "MANUAL Q&A GENERATION", "content": "Twenty percent of the Q&A pairs were generated by humans using heuristics embedded in code, relying solely on the relationships table for their creation.\nThe human-generated questions resemble \u201cWhat campaigns used attack technique 'T1562.001: Disable or Modify Tools'?\u201d The corresponding answers resemble \u201cThe campaigns that used attack technique 'T1562.001: Disable or Modify Tools' were: 'C0002: Night Dragon', 'C0024: SolarWinds Compromise', 'C0028: 2015 Ukraine Electric Power Attack', 'C0029: Cutting Edge\u201d. Because that answer was not available in any single document in the source dataset, we synthetically generated a document to match the answer. That ensured that the full list of relationships for a given source type, target type, and mapping type were available in a single document for ease of retrieval. To generate the document, it was sufficient to query the relationships table, filtering on the relevant entities (e.g., campaigns, software, techniques, etc.). The questions were generated to ensure comprehensive coverage of source types, target types, and mapping types. Notably, no list of relationships was long enough to cause the answer to exceed 1000 tokens in length."}, {"title": "LLM-BASED AUTOMATED Q&A GENERATION", "content": "Utilizing Llama 3 8B Instruct AI@Meta (2024), we generated Q&A based on the processed documents. To speed up the process, we leveraged SambaNova's free cloud offering (SambaNova Systems, 2024), which runs Llama 3 8B at over 1100 tokens/s with full 16 bit precision Kerner (2024). Although we experimented with Llama 3 70B, the results were not significantly different, leading us to continue with Llama 3 8B. In some instances, Llama 3 70B produced overly verbose responses that were difficult to quickly comprehend.\nHalf of the Q&A pairs comprised human-generated questions and LLM-generated answers. The questions encompassed those that we anticipated end users would most likely ask and were structured as follows: \"Describe X\u201d, \u201cHow can X detect Y?\u201d, \u201cHow can X mitigate Y?", "How does attack software X use attack technique Y?": "where X and Y were obtained from the entities listed in Table 1. Each of those questions could be answered by a specific document in AttackQA. Therefore, the generation model simply had to summarize that document in response to the question.\nAn example of an entry, with the question generated using human-defined heuristics (human_question = True) and answer generated using an LLM (human_answer = False), is presented in Table 3. The thought and references were also generated by the LLM. The ref-"}, {"title": "ENSURING QUALITY OF LLM-GENERATED DATA", "content": "Through careful prompt engineering and post-processing of special characters (e.g., the '\\' character in file paths), we achieved a 99% success rate in ensuring that Llama 3 8B produced valid JSON in the required format. That was despite the present lack of a JSON mode in the SambaNova API.\nTo ensure the quality of the LLM-generated questions and answers, we employed three strategies:\n1. We mandated that the LLM generate a citation for each question-answer-rationale entry that included verbatim text from the document supporting the answer. That requirement ensured that the entry was grounded in the source material.\n2. All instances of duplicated questions were removed from the dataset, allowing the remaining questions to act as a unique index. The reason is that the presence of duplicate questions implies that the same inquiry could pertain to two or more different documents, indicating that the question lacked specificity to any particular document."}, {"title": "CRITERIA FOR ASSESSING Q&A QUALITY", "content": "We employed the G-Eval metric Confident AI, 2024 in the DeepEval framework to facilitate automatic curation using the quality control (QC) LLM. The G-Eval metric is a custom metric that scores each Q&A pair by assessing it in conjunction with the retrieved context. In addition to generating a score, the prompts formulated by DeepEval require the LLM to provide a rationale for the"}, {"title": "FINE-TUNING THE QC LLM", "content": "We manually annotated 400 Q&A pairs, assigning scores along with reasons to justify the scores. We kept 80 of those pairs as a hold-out validation set to evaluate models ensuring that the score distributions were preserved (see Fig. 2). A high score indicates that the Q&A pair is of high quality and worth retaining, whereas pairs with low scores were filtered out."}, {"title": "DATASET SUMMARY", "content": "After performing QC, we present AttackQA's summary statistics in Table 5. The breakdown of the 25,335 Q&A pairs by human vs LLM generation is also specified. The token lengths of the documents were measured by the cl100k_base tokenizer with Tiktoken (Open AI, 2024).\nIn using the dataset, some of the documents may need to be chunked for use with models with small context windows. Note that only 104 out of 17,760 (0.6%) of documents have greater than 500 tokens in length and the rest could be used directly with a model of 4096 context length. In the analyses presented in the following sections, we did not chunk any of the documents the open-source LLMs we used had context windows of length 8192 tokens."}, {"title": "MODEL FINE-TUNING FOR RAG", "content": "A basic RAG framework is illustrated in Fig. 4. In this section, we used AttackQA to fine-tune the LLMs and embeddings using SambaStudio to improve answer accuracy in that framework.\nPrior to any user interaction, documents are embedded by the embedding model and stored in a vector database. When a user asks a question, the question is embedded by the same embedding model and k documents relevant to the question are retrieved from the vector database. In our"}, {"title": "TRAINING AND EVALUATION SPLIT", "content": "We split the 25, 335 Q&A pairs into a training set (90%) and an evaluation set (10%) using uniform random sampling. Similar to Zhang et al. (2024), we ensured that all documents were represented in the training set so that the trained models would be familiar with the knowledge base from which questions would be asked. However, the questions in the evaluation set were not present in the training set. That resembles a live production usage setting, in which the end user wants to ask questions of a dataset, and the chatbot is familiar with the source documents but may not have previously seen the questions.\nWhen fine-tuning the models, we used 10% of the training set for validation to ensure that we would eventually evaluate on a checkpoint that was not over-fitting on the training set."}, {"title": "EMBEDDING MODEL", "content": "We performed full parameter fine-tuning on Microsoft's E5 Large V2 embedding model (Wang et al., 2022), which has 335M parameters and encodes up to 512 tokens into an embedding of length 1024. The training dataset comprised of a list of questions from the training set. For each question, a list of positive documents (containing the answer) and negative documents (not containing the answer) were provided. By construction of the dataset, only one positive document existed for each question (since the question was generated from that document). The dataset was uploaded to SambaStudio and the job was run through the user interface.\nHaving negative documents helps the model learn to distinguish between relevant and irrelevant documents for a given question using contrastive learning (Chopra et al., 2005). The negative documents were randomly sampled from a set that excluded documents whose entities were related to the entity associated with the question. That ensured that the answers could not accidentally be obtained from the negative documents, leading to poor contrastive learning. Related entities can be identified in the MITRE dataset based on their IDs (e.g., T1562.001 and T1562.002 are related techniques and should not be included in negative documents for any question relating to T1562.xxx)."}, {"title": "GENERATION MODEL", "content": "The generation model was fine-tuned on SambaStudio using the same questions that were used to train the embedding model, but the dataset preparation for training was different. For each question,"}, {"title": "RETRIEVAL MODEL", "content": "The retrieval component of the pipeline refers to steps 2-4 in Fig 4. We evaluated that component using the context recall metric, which captures whether or not the retrieved context contains the golden document.\nBased on a metric, which in our analysis was the similarity metric, a vector database can be configured to return the top k results, $d(k, q_i)$, for a given query $q_i$. We seek a metric to check if $d^*(q_i) \\in d(k, q_i)$ for all $q_i$ and their associated golden documents in the evaluation set. For an evaluation set of N queries, the context recall (denoted by R) is computed as follows:\n$R(k) = \\frac{1}{N} \\sum_{i=1}^{N} 1_{d^*(q_i) \\in d(k,q_i)}$ (1)\nThe results for k \u2208 {1,5,10} are summarized in Table 6. In all cases, the fine-tuned E5 Large V2 model significantly outperformed both the base E5 Large V2 model and Open AI's SOTA embedding model, Text Embedding 3 Large. The reason is that the dataset contained a lot of domain-specific jargon relating to cybersecurity that the base embedding models were not able to encode. Furthermore, the tuned embedding returned $d^*$ in top 5 ranks in d 92.18% of the time, indicating that a re-ranker model would not have been necessary to bump $d^*$ from the top 10 to the top 5. Finally, the tuned embedding produced $d^*$ at the top rank 81.48% of the time, indicating strong ranking ability."}, {"title": "GENERATION MODEL", "content": "Because the answers generated by the generation models are all free-form text, it was difficult to come up with an objective evaluation of their correctness. Objective metrics like Bleu (Papineni et al., 2002) and Rouge (Lin, 2004) perform N-gram comparisons between expected and actual answers and may not recognize when the two are semantically equivalent if they use different words. For that reason, we used an LLM-as-a-judge to score the answers for correctness.\nOnce again, we used the G-Eval metric with DeepEval to score answers and provide reasons for the scores. With regard to evaluation criteria, we required that the generated answers be penalized for correctness if they 1) contradicted the true answer, 2) omitted details from the true answer that were relevant to the question, and 3) included irrelevant detail that were not present in the true answer.\nWe used Llama 3 405B (Zhou et al., 2024) for the aforementioned evaluation with DeepEval for its SOTA judging ability (Raju, 2024), speed, and cost (it is provided at 132 tokens/s for free by SambaNova Cloud). Seven combinations of embedding and generation models in the RAG framework were evaluated and the evaluation results are summarized in Table 7. \"Base Emb\" and \"Tuned Emb\" refer to the base and fine-tuned versions of E5 Large V2 embedding model, respectively. \u201cBase Gen\u201d and \u201cTuned Gen\u201d refer to the base and fine-tuned versions of Llama 3 8B generation model, respectively. TE-3-L refers to Open AI's SOTA \u2018Text Embedding 3 Large' model.\nThe first row of Table 7 recaps the context recall from Table 6 for k = 5 to show how the other results may be impacted by it. The answer parsing success relates to the generation model's ability to produce JSON-formatted answers with the required fields. That all combinations have at least a 98% parsing success indicates that the one-shot prompts (given in Appendex A.2.2) were adequately engineered to ensure correct completions most of the time. \"% Correct reference\" refers to the number of examples for which the correct reference was produced by the generation model. The references comprise URLs that are included in the retrieved context.\nTwo correctness scores are provided and both use the same G-Eval metric with Llama 3 405B. In the case of \u201cmean correctness (soft)\", if $d^*(q_i) \\notin d(k, q_i)$ for any $q_i$ and the generated answer is \"I am sorry, I do not have the answer to the question,\", then we mark the answer as 100% accurate. That metric compensates for an inaccurate embedding in the retrieval component, explaining why it makes no difference to the result when we switch the embedding from base to tuned while keeping the same base generation model (either Base Gen or GPT-40).\nThe \"mean correctness (hard)\" metric requires that the generated answer match the true answer, regardless of the embedding's retrieval accuracy. No concessions are given for the generation model not admitting to knowing the answer. Therefore, soft correctness scores are higher because some of the answers that were marked as incorrect by hard correctness were forgiven my soft correctness.\nThe biggest gain on hard correctness, an improvement of 26 percentage points, was achieved when going from a Base Emb/Base Gen combination to a Tuned Emb/Tuned Gen combination. An improvement of 16 percentage points was achieved by swapping out the base embedding with a tuned one, for the same generation model.\nTuning the generation model allows it to correctly answer questions even if the answer is not present in the retrieved context leading to an improvement of 10 percentage points when going from a base generation model to a tuned generation model while keeping the embedding the same.\nThe first column in Table 7 refers to a solution using Open AI's SOTA embedding and generation models. On hard correctness, that combination outperforms all other combinations that use the\""}, {"title": "CASE STUDIES", "content": "In this section, we present three evaluation case studies to take a deeper look at the evaluation results. Each case study presents the results for a specific Q&A pair in the evaluation set. A tabular format is used in which the generated answers came from either GPT-40, Llama 3 8B (base), or Llama 3 8B (fine-tuned), as specified in the column headers. The row d* rank in context refers to the ranked position in d when the retrieval succeeds (otherwise it reads d* \u2209 d). The hard correctness score and reason were both produced by Llama 3 405B, which we used for judging.\nThe case study presented in Table 8 was among the 30% of all pairs for which the question and the answer were generated from the document using LLama 3 8B. The configurations with the tuned embedding both returned d* at the highest rank, which is desirable. The others did not return d* at all, likely because they were not able to properly embed the domain-specific term \u201cKOPILUWAK\u201d. Consequently, the configurations with the tuned embeddings produced correct answers, whereas the others did not. Llama 3 405B produced a score of 0.8 for the \"Tuned Emb, Base Gen\" configuration and its reasoning is clear that the answer includes irrelevant details. The scores for the \u201cTuned Emb, Base Gen\" and Open Al configurations would have been set to 1 for soft correctness for their admission to not knowing the answer. The \u201cBase Emb, Tuned Gen\u201d configuration, however, would have received a soft correctness score of 0 for hallucinating.\nTable 9 presents a case study in which all the embeddings find the d*. Although the OpenAI embedding achieved a higher rank of 1 (all others had a rank of 3), GPT-4o generated a less accurate answer than the tuned generation Llama 3 8B. Like GPT-4o, even the base Llama 3 8B failed to mention that \"testing and debugging\" are purposes of the 'Office Test' registry key. That is despite the fact that the name of the key implies the purpose and the purpose is explicitly stated in d*. That case study highlights that tuning the generation model with rationales can help improve reasoning.\""}, {"title": "CONCLUSION", "content": "In this work, we created a Q&A dataset based off the MITRE ATT&CK\u00ae database of cyberattack techniques, software, campaigns, mitigation approaches, and detection approaches. The dataset, AttackQA, can used to train models and create a chatbot to help security operations center analysts"}]}