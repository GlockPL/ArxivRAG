{"title": "Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond", "authors": ["Costin-Andrei Oncescu", "Sanket Purandare", "Stratos Idreos", "Sham Kakade"], "abstract": "While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length. Several subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution sequence models (LCSMs), such as Hyena, address this issue at training time but remain quadratic during inference. We propose a method for speeding up LCSMs' exact inference to quasilinear O(Llog\u00b2 L) time, identify the key properties that make this possible, and propose a general framework that exploits these. Our approach, inspired by previous work on relaxed polynomial interpolation, is based on a tiling which helps decrease memory movement and share computation. It has the added benefit of allowing for almost complete parallelization across layers of the position-mixing part of the architecture. Empirically, we provide a proof of concept implementation for Hyena, which gets up to 1.6\u00d7 end-to-end improvement over standard inference by improving 50\u00d7 within the position-mixing part.", "sections": [{"title": "Introduction", "content": "A lot of recent progress in deep learning, particularly in the form of large language models (LLMs) has been driven by the transformer architecture [Vaswani et al., 2017]. While these models have great quality, it comes at a computation cost which scales quadratically in sequence length - both during training and inference. This can become prohibitive for very long contexts and as such a number of alternative architectures with better computational scaling in context length have been proposed [Gu and Dao, 2023, Poli et al., 2023, Fu et al., 2024]. While most of these works have improved computational efficiency for training, some still scale quadratically in sequence length when it comes to inference, thus not improving asymptotically over transformers.\nIn this work, we propose a framework for optimizing inference efficiency for a general class of such models. As a case study, which inspired the method, we focus on long convolution sequence models (LCSMs) [Poli et al., 2023, Fu et al., 2022, Romero et al., 2021, Li et al., 2022, Karami and Ghodsi, 2024, Fu et al., 2023a]. However, our approach is not limited to LCSMs alone and we identify the properties that allow for such inference speedups in hope to guide the design of future architectures.\nIn the particular case of LCSMs (including Hyena), the building block of the architecture is that of convolving the input sequence with a sequence-length long, (potentially underparameterized) filter. If we let L be the sequence length (e.g. number of tokens in the case of LLMs), then a naive implementation of convolution during training would take (L\u00b2) FLOPs, but one can employ FFT to bring that down to O(L log L). The issue that occurs during inference is that FFT cannot be used directly since the whole input sequence is not known ahead of time, but rather incrementally computed. Because of this, the naive inference approach goes up to (L\u00b2) - this is the apparent cost of moving from a static input to a dynamic one.\nIt turns out that in fact, at least in the case of \"dynamic FFT\", one can obtain O(Llog\u00b2 L) time complexity by using van der Hoeven [1997]'s result on relaxed polynomial convolution. Crucially, this result works by a direct reduction to several applications of FFT. This allows us to phrase a more general framework for turning training-efficient architectures to inference-efficient ones.\nOur main contributions are:\n\u2022 Proposing the first quasilinear-in-sequence-length O(Llog\u00b2 L), exact inference algorithm for LCSMs.\n\u2022 Identifying the main features of LCSMs that allow for faster inference to propose a more general framework."}, {"title": "Setting and Related Work", "content": "Much recent effort has been put in designing more efficient alternatives to transformers [Tay et al., 2022]. A couple of well-performing classes are state space models (SSMs) [Gu et al., 2021, Gu and Dao, 2023] and convolution-based models[Fu et al., 2022, Poli et al., 2023]. In this work we focus on the latter class. To avoid a popular misconception, it is worth emphasizing that while any linear-time invariant (LTI) SSM has an equivalent convolutional filter, it is not true that any convolutional filter has an equivalent low-dimensional LTI SSM representation. It could be the case that the smallest LTI SSM equivalent to a length-L convolution filter has a dimension of L. However, such a dimension would deem an LTI SSM approach have quadratic time complexity both during training as well as during inference."}, {"title": "Notations and Definitions", "content": "In the most general form, we consider architectures obtained by stacking several position-mixing layers potentially interleaved with element-wise, feature-mixing modules, partially following Arora et al. [2023]'s notation. Put formally, let block\u2081,... blockM : \u211d\u1d30 \u2192 \u211d\u1d30 be feature-mixing modules and mixer\u2081 . . . mixerM : \u211d\u1d38\u00d7\u1d30 \u2192 \u211d\u1d38\u00d7\u1d30 be position-mixing modules. One can think of the blocks as being combinations of MLPs, layernorms, skip connections and other modules of similar purpose and of mixers as playing the role of multi-head attention (MHA) in a transformer - the part where embeddings at different positions interact with each other. Assume these are all learnt from some well-defined parameterized classes. Here, M is the number of layers, D is the used embedding dimension and L can be any sequence length (unless positional embeddings are used, case in which it can only take values up to some Lmax). We define the activations a\u2080,...,aM : \u211d\u1d38\u00d7\u1d30 \u2192 \u211d\u1d38\u00d7\u1d30 at level 1 \u2264 l \u2264 M and position 1 \u2264 i \u2264 L as (a\u2097(x))\u1d62 \u225c block\u2097(mixer\u2097(a\u2097\u208b\u2081(x))\u1d62) where a\u2080(x) = x represents the input embeddings (L D-dimensional tokens) and the model output at position i is read from the last layer (aM(x))\u1d62.\nWe focus on autoregressive sequence models where for an input sequence x\u2081,...xp, one generates the (p + 1)th token by sampling from a distribution given by (aM(x))p (usually via a linear map). In order for this generation procedure to be sensible, we constrain (by construction of mixers) the models to be causal, that is, (a\u2097(x))\u1d62 is a function of only X[1,i] or, equivalently, mixer\u2097(y)\u1d62 is a function of Y[1..i].\nWe use \u2299 to denote the element-wise (Hadamard) product: for two matrices A, B \u2208 \u211d\u1d3a\u00d7\u1d39, A \u2299 B\u2208\u211d\u1d3a\u00d7\u1d39 is given by (A\u2299 B)i,j = Ai,j \u00b7 Bi,j. We also use indexing by ranges to mean slicing a tensor in the respective dimension according to the range and indexing by a dot to mean leaving the dimension unchanged: for example, if A : \u211d\u1d3a\u00d7\u1d39\u00d7\u1d37, then A.,[l,r],5 \u2208 ]\u211d\u1d3a\u00d7[r\u2212l+1] and (A.,[l,r],5)i,j = Ai,j+1\u22121,5.\nWe call a function f : X* \u2192 X associative, if f(x\u2081,x\u2082,...xp) = f(f(x\u2081,...xi), f(xi+1,...Xp)) for any 1 < i < p and X\u2081 ... Xp \u2208 X - that is, it is invariant to bracketing. Finally, we define the time complexity of an algorithm as the number of floating-point operations (FLOPs) it executes - this is a notion independent of hardware."}, {"title": "Self-attention", "content": "Self-attention is the building block of transformers [Vaswani et al., 2017]. In its simplest form, it implements a mixer by using three projection matrices Q, K, V \u2208 \u211d\u1d30\u00d7\u1d30 to obtain three sequences of vectors q, k, v \u2208 \u211d\u1d38\u00d7\u1d30 by projecting the input y \u2208 \u211d\u1d38\u00d7\u1d30 via q = yQ, k = yK, v = yV - these are called queries, keys and values, respectively. The (causal) self-attention operator attention : \u211d\u1d38\u00d7\u1d30 \u2192 \u211d\u1d38\u00d7\u1d30 is then given by:\n$$attention(y); mixer(y); = \\frac{\\sum_{i=1}^{j-1} (v_{[1,j]})softmax(k_{[1,j]}q_j)}{\\sum_{i=1}^{j-1}(v_{[1,j]})softmax(k_{[1,j]}q_j)}$$\nwhich represents, at position j, the average of the previous value vectors v[1,j] exponentially-weighted by how well the values' corresponding keys match jth query. Put differently, the bigger (qi, kj) is, the more jth output attends to ith input. Note that both in the above notation, as well as in general, we assume any 1-dimensional tensor to be a column vector (for example qj \u2208 \u211d\u1d30\u00d7\u00b9). In the transformer architecture this is how one \u201chead\" operates and each embedding is normally split into several such heads - this is called multihead attention (MHA). If we take the causality away (that is, the i < j) for simplicity, one could think of attention as being mixer(y) = softmax(qk\u1d40)v where softmax is computed along the rows of what is called the attention matrix: qk\u1d40."}, {"title": "Long Convolution Sequence Models (LCSMs)", "content": "LCSMS [Poli et al., 2023, Li et al., 2022, Gaido et al., 2024] work by creating a SISO (single-input-single-output) primitive to map an input sequence y \u2208 \u211d\u1d38 to z \u2208 \u211d\u1d38 via z\u209c\u2194 \u2211\u1d62\u208c\u2081\u1d57 y\u1d62 p\u209c\u208b\u1d62 where p is a (possibly infinite, and of length at least L) convolution filter which is often parameterized by a smaller latent space \u0398: p = f(\u03b8) where \u03b8\u2208 \u0398, dim \u0398 \u226a L is learnt. These convolution primitives operate on independent axes of the hidden dimensions to create a positional mixer: mixer(y)t,c = \u2211i=1t Yi,c \u00b7 Pt-i,c. This assumes the filters to be independent, but shared ones are possible as well (as is the case for multi-head Hyena [Massaroli et al., 2024]).\nFor example, the Hyena architecture [Poli et al., 2023] maps back to our setup when mixers are defined as above and the block functions, depending on the layer, are either MLPs or gates - that is, element-wise multiplications with a projection of activations at same position, but lower level. We do not focus on the details of the blocks since they all involve some D \u00d7 D matrix-vector multiplication that is performed once for every layer and position 1 \u2264 i \u2264L and thus scale as (LD\u00b2) per layer - that is, linearly in context length."}, {"title": "The Inference Problem", "content": "Whereas during training, the convolutions can be performed in O(Llog L) time via FFT as the whole of y is known beforehand, during inference this is non-trivial. Suppose there is a prompt of size P and we want to generate L \u226b P more tokens. The bottleneck problem can be understood as computing, for every layer and every dimension, the below:\n$$z_t \\triangleq \\sum_{i=1}^{t} y_i p_{t-i}$$\nfor all 1 \u2264 t \u2264 L. To pre-fill the first P values y[1..p] at all levels of activations, since the first P inputs are known, one can perform FFT as they would normally do during train-time (incurring an O(Plog P) time cost), but past P\u1d57\u02b0 position it is important to note that yi is not available before computing z\u1d62\u208b\u2081 - this is where static FFT breaks. Since dealing with the prompt can be done easily [Massaroli et al., 2024, Lemma 2.1] by essentially filling in all contributions of y[1..P] to z[1..L] and then forgetting the prompt ever existed, we henceforth assume P = 0."}, {"title": "Previous Work on Efficient Inference", "content": "Speeding up Equation 2 from the naive \u03a9(L\u00b2) is the object of study of Massaroli et al. [2024]: they optimize the process by finding a low-dimensional LTI SSM whose equivalent convolution filter closely resembles p, thus getting an RNN-like problem to simulate. If the learnt SSM has size D', then this yields an O(LDD') algorithm for generating L tokens. This has the significant added benefit of not needing to store the activations (or even inputs) thus far which makes it memory efficient and practically time-efficient.\nThe downside, however, is that by definition this will only be an approximation of the learnt filter. More importantly, this approximation represents a projection to a fundamentally smaller space of models, thus defeating the purpose of using LCSMs instead of LTI SSMs - it is practically a different training procedure for an LTI SSM model. Furthermore, this approach assumes the filter p is data-independent - this is needed in order to undergo an expensive distillation procedure as a precomputation step. This can be qualitatively limiting as shown in Arora et al. [2023]. While we do store all activations, our proposed framework exactly simulates the architecture and data-dependent filters can also be accommodated."}, {"title": "Fast LCSM inference", "content": "In this section we describe our method for the case of long convolution sequence models. That is, we still work with models as described in Section 2.1, but we further assume the mixers to be convolution-based, as described in Section 2.3. Hence, for every mixer\u2097 \u2208 {mixer\u2081 . . . mixerM}, there exists a filter p\u2097 \u2208 \u211d\u1d38\u00d7\u1d30 such that\n$$mixer_\\ell(y)_t = \\sum_{i=1}^{t} y_i P_{\\ell,t-i}$$\nWe assume here, for simplicity, that p is data-independent and part of the model, as is the case for Hyena. However, we discuss in Appendix B how our method can be extended to data-dependent filters. We also assume L = 2\u1d3e for some integer P for simplicity - one can always round L up to the closest power of 2 without changing it by more than a factor of 2 and thus keeping the asymptotics of all our analysis unchanged."}, {"title": "The Proposed Algorithm", "content": "We derive inspiration from the the work of van der Hoeven [1997] regarding relaxed polynomial interpolation - they propose a fix to dealing with dynamic structure of Eq. 2 to achieve an overall O(Llog\u00b2 L) time complexity."}, {"title": "Relaxed Polynomial Interpolation", "content": "Consider Eq. 2:\n$$z_t \\triangleq \\sum_{i=1}^{t} y_i p_{t-i}$$"}, {"title": "Architectural Properties", "content": "In order for our framework to apply, we need the mixers involved to have certain properties:\nP.1 Contribution-based The used mixers work by aggregating contributions of each input position to each subsequent output position. That is, for any mixer\u2097, there exist an associative aggregation function agg : X* \u2192 X, a read function read : X \u2192 \u211d\u1d30 and a contribution function cont : \u211d\u1d30 \u00d7 \u2115\u00d7\u2115 \u2192 X such that:\n$$mixer(y) = read(agg(cont(y, 1, i), cont(y, 2, i), . . . cont (y, i, i)))$$\nwhere, X is a set of intermediate states and read is a function to map those back to embeddings. Recall that agg being associative means that agg(x\u2081,x\u2082,...xT) = agg(agg(x\u2081,... xi), agg(xi+1,...xT)) for any 1 < i < T. For a sensible architecture, the size of X and cost of agg should be of order D.\nIn the case of self-attention this translates to having read \u2218 agg simulate the softmax, by letting X = \u211d\u1d30 \u00d7 \u211d, agg \u225c \u2211 and\n$$cont(y, i, j) = (\\frac{Vy_i}{e^{\\langle k_i,q_j \\rangle}},e^{\\langle k_i,q_j \\rangle})$$\nthat is, the exponentially weighted value vector along with the exponential weight. Finally one can use read(v, w) = v/w to implemnent the softmax normalization step.\nIn the case of LCSMs, one can simply choose X = \u211d\u1d30, read be the identity function, agg be the sum again and cont(y, i, j) = Yi Pj-i.\nP.2 Query-independent The contribution function cont(y, i, j) does not depend on y[i+1,L]. Note that this is the case in LCSMs since yi pj-i only depends on yi. However, it is not the case for transformers, since cont(y, i, j) depends on qj which depends on yj."}, {"title": "Setting and Definitions", "content": "Suppose P.1 holds and there exists an algorithm A that for any given input sequence y and indices l < r < l' < r', computes the contributions of y[l,r] to outputs at every position p \u2208 [l',r']:\n$$A(y, [l, r], [l', r'])_{l'<p<r'} = agg(cont(y, l, p), cont(y, l + 1, p), . . . cont(y, r, p)).$$\nFurthermore, for this choice of A, let T : \u2115 \u00d7 \u2115 \u2192 \u2115 such that for any l < r < l' < r', evaluating A(y, [l, r], [l', r']) takes at most T(r \u2212 1 + 1, r' \u2013 l' + 1) FLOPs.\nIf we dropped the r < l' condition and set 1 = l' = 1 and r = r' = L, this algorithm would actually represent the procedure one needs to perform a forward pass during training - which we refer to as the static setting. In the case of LCSMS, A is the FFT-based algorithm underlying Lemma 1, with an associated T(L\u2081, L\u2082) = D(L\u2081 + L\u2082) log(L\u2081 + L\u2082), as opposed to the naive implementation that would have Tnaive (L\u2081, L\u2082) = DL\u2081 L\u2082."}, {"title": "Main Result", "content": "Our main result can be phrased as follows:\nTheorem 2. Under the assumptions P.1 and P.2, one can generate L = 2\u1d3e tokens autoregressively by performing, per layer, L - 1 black-box calls to A as well as L more calls to cont, agg, read and block. Concretely, there are L/2 = 2\u1d3e\u207b\u00b9 calls to A of length 1 each, L/4 = 2\u1d3e\u207b\u00b2 calls of length 2 each and so on up to 1 call of length L/2. Hence, neglecting the cont, agg, read and block part, the overall time complexity per mixer layer is:\n$$FLOPs = \\sum_{q=0}^{P-1} 2^{P-1-q}T(2^q,2^q)$$\nFurthermore, during every token-generation iteration, the calls to A across different layers can be performed in parallel as there is no data-dependency between them."}, {"title": "Experiments", "content": "We performed two kinds of experiment: one synthetic and one on the Hyena architecture [Poli et al., 2023]. The synthetic setup defines all blocks to be MLPs with a hidden dimension of 2D and GELU activation; it also simply sets a\u2080,i+1 as aM,i plus some noise to avoid dependency on vocabulary size since that is out of the scope of our framework and will be negligible at scale. Note that this can be viewed as a sampler: a function from logits at the last layer and previous position to the next token's embedding. In both settings the weights are initialized to random noise since it does not affect the runtime - this avoid unnecessarily training a new model for each hyperparameter choice.\nIn terms of notation, we introduce the batch dimension B and keep M, D, L refer to the number of layers, embedding dimension and number of tokens generated, respectively. We use U to refer to the square-tile length, as per line 4. We sweep over B\u2208 {1,2,4,8}, M\u00b9 \u2208 {18,36}, D\u2208 {256,768} and generate tokens up to the greatest power of 2 that fits the memory. All results are obtained by averaging over 4 runs following 2 runs of warm-up. We evaluate our approach on on the latest NVIDIA H100 and A100 GPUs.\nFor our baselines, (1) we consider the eager and lazy approaches described in Section 3.1, depicted in 1. We also consider their implementation exploiting our observation regarding parallelization across layers (3.2) (we denote the the parallel versions simply as lazy and eager).\nOur Flash Inference framework explored various implementations of \ud835\udf0f, covered in Section 5.2. Our best method is a Hybrid that dynamically chooses the optimal implementation of \ud835\udf0f depending on (B, D, M,U)."}, {"title": "Integrating Flash Inference in real world setting (Hyena Architecture)", "content": "Flash Inference significantly speeds-up the end-to-end inference by up to 1.6\u00d7 and the convolution-based mixer component of the Hyena architecture by 50\u00d7 compared to the baselines as shown in Figures 2a and 2b.\nFigure 2c shows the per token response time of Hybrid and baselines. Hybrid shows low variance in per-token time except at the tokens positions where large tiles are computed. For a given sequence length of L, we have L/2 positions that use tile size 1, L/4 positions using tile size 2, and so on. That is, 93.75% of tokens use a tile size U\u2264 8. Hence the spikes occur rarely."}, {"title": "Implementations", "content": "Algorithm 2 assumes an implementation of primitive \ud835\udf0f that accounts for the contribution of a range of inputs to a range of outputs of a convolution. We considered 7 different implementations but, here, we only present results of the ones on the Pareto Frontier - that is, those optimal for at least some (B, N, D, U) setting. There are 4 such implementations, of two types:"}, {"title": "Mixer Isolation study and Hybridization of \ud835\udf0f implementations", "content": "We evaluate the different convolution implementations for all settings of B, D, M,U and observe that each of these four implementation lies on the pareto frontier curve of tile size vs latency as shown in Figure 3a. Our Hybrid approach dynamically chooses the best implementation for a given tile size U based on the isolated empirically-measured efficiency of each implementation as shown in Figures 3b. Figure 3c shows the cumulative token time breakdown for the mixer and non-mixer components of the synthetic setup for all our \ud835\udf0f implementations. We observe an increase in the non-mixer components due to significant reduction in GPU kernel time that exposes the CPU kernel dispatch overhead for the MLP blocks in the non-mixer components."}, {"title": "Improvements justification", "content": "This significant speed-up can be attributed to:\n(1) The significantly lower O(Llog\u00b2 L) FLOPs required for our tile computation approach compared to the \u03a9(L\u00b2) FLOPs required by Eager and Lazy counterparts. This is shown in Figure 2b where methods based on our tiling outperform by a large constant the quadratic methods in terms of time spent on the mixer components.\n(2) Drastically reduced activation memory access from \u03a9(L\u00b2) for Eager and Lazy to out O(L log L) tiling-based methods. This can be shown through the performance of Flash Conv1D (Figure 3b) which outperforms lazy and eager by a margin although it also performs \u03a9(L\u00b2) FLOPs - it does so in a more memory-friendly and kernel-optimizable way.\n(3) The dynamic choice of best \ud835\udf0f implementation for given tile U - hybrid outperforming any method using a fixed implementation (Figure 3b).\n(4) Our engineering contributions: first, the DFT for the convolutional kernel is pre-computed ahead of time for log\u2082(L) \u2013 1 tile sizes. Second, Flash FFT configurations are pre-initialized for these tile sizes to maximize hardware performance. Third, right padding is used instead of left padding to reduce computation time by half. Fourth, properties of circular convolution are exploited to halven FFT length. Finally, tile calculations are parallelized across layers to saturate memory bandwidth for small tile computations and optimize computation for large tile computations, resulting in improved performance (one can notice improvements of 10 - 20% even in the Eager and Lazy implementations alone)"}, {"title": "Conclusion and Further Work", "content": "We propose a framework for performing inference in certain autoregressive sequence models. Among such models, LCSMs such as Hyena, are noteworthy: there, our framework provides an O(Llog\u00b2 L) inference algorithm which, when run empirically, yields up to \u00d71.6 time-efficiency improvement. The framework exploits a causal, fractal tiling that helps save on data movement and share computation. Moreover, it allows for almost-complete across-layers parallelization of mixer-related workload.\nAn interesting future direction to pursue is that of designing architectures that fit out framework requirements and thus get fast-inference by construction. Furthermore, in the class of LCSMs, we have noted that one can achieve the same theoretical complexity if filters are made data-dependent, and, while previous works Arora et al. [2023], Karami and Ghodsi [2024] have shown the potential for these, they are not yet causal so looking into how to make filters data-dependent in a causal way is another promising direction."}, {"title": "Extension to Data-Dependent Filters", "content": "The reason why data-dependent filters are harder to deal with is that cont(y, i, j) = Yi \u00b7 Pj-i depends on both yi and on what pj-i depends on. By only assuming causality, we can only access pj-i after zj-i-1 has been computed. This stops Algorithm 2 from working out-of-the-box since when i is a power of 2, in order to account for the contribution of al-1,[1,i] to al,[i+1,2i] one will need access to pe,[1,2i-1] which is not guaranteed to be accessible (only pe,[1,i] can be assumed to be known at this stage).\nThe modified algorithm is shown in Algorithm 5 and precisely follows the tiling of van der Hoeven [1997] and, thus, its correctness transfers. Note that rather than using the A algorithm, it directly uses the untruncated convolution (implementable via FFT) - in the contribution space this corresponds to a parallelogram tile rather than rectangle and this cannot be simulated via a rectangle, although the other way around is possible (and is what we did through Lemma 1). This implementation performs convolutions between two sequences of length U each and uses the whole output and thus requires an order 2U FFT. Note that this happens twice for a given i when i + 1 is not a power of 2 (almost always). Algorithm 2, on the other hand, only performs a convolution between a length-U sequence and a length-2U sequence. However, as noted in Appendix C, we can get away without padding and thus performing only one order 2U FFT. Thus, our proposed tiling improves FLOPs by a factor of 2, but it requires the kernel to be data-independent."}, {"title": "Implementation Improvements", "content": "In Algorithm 2, the calls to A all have a shape of A(y, i \u2013 U + 1, i, i + 1, i + U) where U is a power of 2 that divides i. Following the proof of Lemma 1, we note that to implement A, we need to perform the convolution of a segment of length U of y and a prefix of length 2U of p. However, following the convolution, of the 3U \u2013 1 outputs, indexed [0, 3U \u2013 2], we are only interested in the middle U of them, namely indices [U, 2U \u2013 1]. The canonical way of performing convolutions via FFT involves padding by enough Os and using an order large enough to store the whole output which rounded up to the closest power of 2 would mean an order 4U FFT. However, using a 2U FFT call, which will perform a cyclical convolution, is enough for our purposes, since the values of interest are not affected by the cyclicity - that is, when folding outputs at [2U, 3U \u2013 2] onto [0, U \u2013 2], we do not intersect [U, 2U \u2013 1].\nFurthermore, there are only log L different lengths of prefixes of p involved in these convolution so one could, in fact, cache the DFTs for these as a precomputation step, thus dropping the number of DFT's per convolution from 3 to 2, speeding up by a further \u00d71.5 factor."}, {"title": "Storing Only Half of the Activations", "content": "If the whole activation tensor (M \u00d7 L \u00d7 D) cannot fit the memory of one GPU, we can save a factor of 2 along the L-axis. To do this, observe that after (L/2)\u1d57\u02b0 iteration completes we never need to look back to any activation at position i < L/2. Hence, one can hope to reuse the space used for the first half to store the second half of activations. While doing this directly by having A work as an in-place operator requires storing MLD intermediate values, one can choose to not apply A parallelly across all layers (or even dimensions), as discussed in Section 3.3. Processing the tiles sequentially allows for the reuse of the space for interim results; once the result of A is read, we place it back to where the input used to be - similarly in nature to gradient accumulation - always placing the output back into the input slot. This way, the peak extra memory required for the call to A can be LD or even L.\nAs we get the output of A(a\u2080, 1, L/2, L/2 + 1, L), which has a shape of (L/2) \u00d7 D, we overwrite a\u2080 by it and then move on to the next layer. We can do this because the contribution of a\u2080,i to a\u2081,i has already been properly accounted for so the first half of a\u2080 truly becomes irrelevant for next layers and iterations to come. Hence, a\u2080 contains the intended second half of a\u2081. We then proceed to do the same thing for a\u2081 and so on. At the end, a[0...M-1] will contain the second halves of a[1...M], so we can shift these by 1, emptying a\u2080 and now finally having a represent the second halves of activations the first halves have been discarded.\nAlthough per layer, one will have a peak memory containing both a\u2097, the output, and whatever other necessary temporary values needed to perform A, this does not scale with M - because we essentially reuse the extra space needed by A across different layers. We seemingly pay the cost of not performing the calls to A in parallel across layers, but if storing the M \u00d7 L \u00d7 D tensor of activations was an issue, then one would very likely have to make this sort of compromise to be able to run A in the first place."}, {"title": "Discussion on memory", "content": "In Section 3.3, we discussed the peak memory usage being given by the gray calls to A. Since each call to A performs D FFTs of length L each (when dealing with the biggest tile of side L/2), the naive implementation would use up to MDL extra memory to perform these FFT calls. However, this assumes the biggest gray tile is being processed in parallel across all layers and across all dimensions at once. We do not need to maximize parallelization on all sizes of tiles: for the largest ones, we can process the tiles at different layers sequentially by reusing the space, thus dropping to an overhead of O(LD), or even just O(L) if one is to treat sequentially the different dimensions. Dropping parallelization could theoretically yield a slowdown, but that is only when we are not memory bandwidth-bound which is the case if one has to worry about allocating more than O(LD) - in that case, the FFT calls are anyway not happening in parallel. Hence, one can easily drop the peak memory overhead to O(LD), or even O(L) without incurring an actual time cost."}]}