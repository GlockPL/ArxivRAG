{"title": "Seeing Your Speech Style: A Novel Zero-Shot Identity-Disentanglement Face-based Voice Conversion", "authors": ["Yan Rong", "Li Liu"], "abstract": "Face-based Voice Conversion (FVC) is a novel task that leverages facial images to generate the target speaker's voice style. Previous work has two shortcomings: (1) suffering from obtaining facial embeddings that are well-aligned with the speaker's voice identity information, and (2) inadequacy in decoupling content and speaker identity information from the audio input. To address these issues, we present a novel FVC method, Identity-Disentanglement Face-based Voice Conversion (ID-FaceVC), which overcomes the above two limitations. More precisely, we propose an Identity-Aware Query-based Contrastive Learning (IAQ-CL) module to extract speaker-specific facial features, and a Mutual Information-based Dual Decoupling (MIDD) module to purify content features from audio, ensuring clear and high-quality voice conversion. Besides, unlike prior works, our method can accept either audio or text inputs, offering controllable speech generation with adjustable emotional tone and speed. Extensive experiments demonstrate that ID-FaceVC achieves state-of-the-art performance across various metrics, with qualitative and user study results confirming its effectiveness in naturalness, similarity, and diversity. Project website with audio samples and code can be found at https://id-facevc.github.io.", "sections": [{"title": "Introduction", "content": "Voice Conversion (VC) aims to change the speaker identity in speech from a source speaker to that of a target speaker, while preserving the linguistic content. However, audio from the target speaker is not always available in some scenarios (e.g., digital humans, historical figures). Instead, some studies have explored an alternative approach by generating the identity information of unseen speakers' voices from their facial images, known as Zero-Shot Face-based Voice Conversion (ZS-FVC). Recently, this has become a promising research topic with potential applications in various scenarios, such as generating voices that match character appearances in automated film dubbing and personalized virtual assistants.\nIn the literature, great progress in this domain has been achieved by prior work. The fundamental challenge is to accurately map identity information between faces and voices. Specifically, this involves (1) acquisition of facial embeddings that are well-aligned with the speaker's voice identity, and (2) decoupling of content and speaker identity information from the audio input.\nFor the first challenge, the current state-of-the-art (SOTA) work FVMVC used FaceNet to extract general facial features and mapped them through a memory net. Another SOTA work, SP-FaceVC averaged all frames to achieve consistent facial embeddings. However, these methods focus on general facial features rather than speaker-specific features, which include substantial non-specific, identity-irrelevant information (e.g., facial expressions, head angles, background). As a result, the models become highly dependent on the training data and lack the ability to locate unique voice characteristics among different speakers, leading to the production of general voices. For the second challenge, FVMVC attempted feature decoupling through a mixed supervision strategy, relying heavily on the quality and scope of the supervision voices. However, in practical scenarios, it is often difficult to acquire adequate and balanced supervision, leading to suboptimal decoupling performance. SP-FaceVC used a low-pass filtering strategy in data pre-processing to eliminate high-frequency elements from audio signals, aiming to reduce style features linked to the speaker identity. Despite its simplicity, this hard filtering approach risks indiscriminately filtering out some key voice details, thereby affecting the naturalness and expressiveness of the synthesized voice and potentially introducing noise and other artifacts.\nTo address the above two challenges, we introduce a novel zero-shot Identity-Disentanglement Face-based Voice Conversion (ID-FaceVC) method. For the first challenge, instead of adopting static encoding methods that generalize facial features, we design an Identity-Aware Query-based Contrastive Learning (IAQ-CL) module to precisely extract the most identity-relevant facial features. Specifically, we propose a Self-Adaptive Face-Prompted QFormer (SAFPQ), which employs a set of learnable self-adaptive face prompts to query identity-relevant facial features from a frozen Contrastive Language-Image Pretraining (CLIP) visual encoder. Indeed, the SAFPQ functions as an information bottleneck, efficiently filters and maps facial features to produce speech-relevant facial features, which are then subjected to contrastive learning with identity features extracted from audio.\nFor the second challenge, rather than using implicit supervision or hard filters, we design a novel Mutual Information-based Dual Decoupling (MIDD) module to purify the extracted content features. This module decomposes speech into subspaces representing different attributes and minimizes the overlapping information between speaker identity and content features through Mutual Information (MI) constraints. Additionally, inspired by , we implement the fine-grained speaker identity supervision to fully leverage speaker identity information, compelling the model to learn the subtle distinctions between different speakers and preventing model collapse.\nIn addition, previous approaches that employed the target speaker's voice as input, suffer from limitations in practical applications due to the occasional unavailability of the reference audio. Some existing works have utilized text as the input for speech generation, but their outputs lack the flexibility to manipulate speech style and often produce speech in a \"machine\u201d manner. In this work, we first incorporate text as an alternative modality during the inference stage and introduce a style-controllable strategy that allows for control over the emotion and speed of the generated speech, thereby enabling the generation of natural, rhythmical, and controllable speech from text."}, {"title": "In summary, the main contributions of this work are as follows.", "content": "\u2022 A novel paradigm named ID-FaceVC is proposed for zero-shot face-based voice conversion that can accept either audio or text as input, allowing control over the emotional tone and speed of the generated speech. To the best of our knowledge, this is the first attempt to explore dual-input controllable face-based voice conversion.\n\u2022 We design an IAQ-CL module, containing a new Self-Adaptive Face-Prompted QFormer to query facial features most relevant to speaker identity and forces the model to learn the subtle differences between speakers.\n\u2022 We propose an effective mutual information-based MIDD module to completely decouple content and speaker identity from audio features.\n\u2022 Extensive experimental results demonstrate that our method achieves SOTA performance across multiple metrics. Qualitative and user study results further validate the effectiveness of the proposed model in terms of naturalness, similarity, and diversity."}, {"title": "Related Work", "content": "Facial and vocal characteristics are closely linked to individual identity. Studies have demonstrated a natural synergy between these features, collectively providing concordant source identity information. Features of a voice can be inferred from facial structures. For example, vocal pitch and intonation may be associated with facial features like jaw width and eyebrow density, which together create a distinct identity signature. Recently, several studies have exploited the strong similarity between voice and face for novel applications, such as reconstructing a speaker's face from their voice. Our research explores the inverse of this process, generating diverse vocal styles from various facial images."}, {"title": "Face-based Voice Conversion", "content": "Prior research has validated the potential for synthesizing speech from facial features. Face2Speech pioneered this field with a three-stage training strategy and a supervised generalized end-to-end loss to generate speech that reflects speaker facial characteristics. Building on this foundation, subsequent works proposed more adaptable loss functions and more sophisticated network designs to enhance the quality of the synthesized speech. These methodologies typically employ text as the input to avoid entanglement issues. FaceVC developed a three-stage model that leverages a bottleneck adjustment strategy and a straightforward MSE loss to extract necessary content embeddings from audio. However, this model struggles to capture the complex mappings between speech and facial domains, often defaulting to predicting an \"average voice\" across variations, making it unsuitable for zero-shot applications. The most advanced approaches in this field, FVMVC and SP-FaceVC, improved upon FaceVC through memory-based feature mapping and rigorous data preprocessing.\nNevertheless, these methods still have considerable potential for improvement in achieving well-aligned facial embeddings with speech and effectively decoupling content from speaker identity in audio features."}, {"title": "Our Method", "content": "Our proposed ID-FaceVC employs an end-to-end training approach. It comprises three main components: ID-Aware Query-based Contrastive Learning module, Mutual Information-based Dual Decoupling module, and Alternative Text-Input with Style Control module."}, {"title": "ID-Aware Query-based Contrastive Learning", "content": "We design the IAQ-CL module to extract facial features that are well-aligned with the speaker's voice identity. This module includes Self-Adaptive Face-Prompted QFormer and face-related speaker identity supervision."}, {"title": "Self-Adaptive Face-Prompted QFormer", "content": "Considering the inherent limitation of CNN-based architectures in handling the diversity of facial features, we instead employ a frozen CLIP visual encoder to extract features from facial images. For the frame-wise visual embeddings extracted by the CLIP visual encoder, we compute the arithmetic mean to obtain average frame-level facial features, rather than randomly selecting a single frame as the facial embedding, to reduce potential sampling bias.\nDue to the high-dimensional and redundant nature of CLIP visual features, facial embeddings contain abundant information, including facial expressions, head poses, and backgrounds, with only a small portion related to the speaker's style. Therefore, we propose the SAFPQ to filter the most speech-relevant features, as illustrated in Figure 2. Unlike the vanilla Query Transformer (QFormer), our model better integrates identity information from both face and voice domains, resulting in a more cohesive representation. The SAFPQ functions as an information bottleneck, filtering out redundant facial features while emphasizing those crucial for speech. In the inference stage, the self-adaptive face prompts retrieves identity-relevant facial features from input facial embeddings, facilitating the prediction of the speaker's style from unseen facial images.\nTo be specific, we initialize a set of learnable self-adaptive face prompts. The most informative prompts are highlighted through a self-attention mechanism that integrates the information from a global perspective. Subsequently, the face prompts interact with facial embeddings via cross-attention to retrieve features relevant to the identity information. Finally, a fully connected layer fuses these retrieved features. The process are defined as follows:\n$A_{self} = softmax(\\frac{QW_{k}^{self} (QW_{q}^{self})^{T}}{\\sqrt{d_{k}}}) QW_{v}^{self},$\n$A_{cross} = softmax(\\frac{A_{self} W_{k}^{cross} (F_{f}W_{q}^{cross})^{T}}{\\sqrt{d_{k}}}) F_{f}W_{v}^{cross},$\n$F_{query} = FFN(A_{cross}),$\nwhere $W_{k}^{self}, W_{q}^{self}, W_{v}^{self}$ are the learnable weights for the self-attention, and $W_{k}^{cross}, W_{q}^{cross}, W_{v}^{cross}$ are the learnable weights for the cross-attention. Q is the self-adaptive face prompts, $d_{k}$ is the dimension of the key, $F_{f}$ is the facial embedding extracted by the CLIP, and $F_{query}$ represents the final queried facial features.\nRecall that our objective is to extract features highly relevant to the speaker's identity, ensuring that the retrieved facial embeddings closely match the style features in speech. We employ contrastive learning to measure the distance between these two features, thereby optimizing the self-adaptive face prompts. This encourages the speech style and facial embeddings from the same speaker to be as similar as possible, while those from different speakers are distinctly separated. The formulation for this process is as follows:\n$L_{con} = -\\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} Y_{i, j} log(\\frac{exp(sim(i, j)/\\tau)}{\\sum_{k=1}^{N} exp(sim(i, k)/\\tau)}),$\nwhere N is the number of samples in a batch, $\\tau$ is a temperature hyperparameter, i is a fixed index for facial embeddings, j and k are indices for speech embeddings, and $Y_{i,j}$ is an indicator function. If samples i and j belong to the same speaker, then $Y_{i,j}$ = 1; otherwise, $Y_{i,j}$ = 0."}, {"title": "Face-related Speaker-identity Supervision", "content": "To distinguish key facial features between different speakers, inspired by , we design a fine-grained speaker identity supervision mechanism to enhance our model's capability. This supervision ensures that facial features should maintain consistency for the same speaker while exhibiting distinctiveness between different speakers. The formulation can be expressed as:\n$L_{id-f} = -\\frac{1}{NC} \\sum_{n=1}^{N} \\sum_{i=1}^{C} t_{ni} log (P_{ni}),$\nwhere C represents the number of distinct speakers and $t_{ni}$ denotes the one-hot encoded target label for the n-th sample (where $t_{ni}$ = 1 if the sample belongs to the i-th speaker, otherwise $t_{ni}$ = 0). $P_{ni}$ is the softmax probabilities that the n-th sample's $F_{query}$ belong to the i-th speaker.\nDuring the inference, ID-FaceVC is able to generate consistent style speech for different facial images of the same speaker and diverse style speech for different speakers."}, {"title": "Mutual Information-based Dual Decoupling", "content": "We propose the MIDD module to achieve precise representation of different disentangled latent spaces and purification of speech content information. It includes Disentangled Latent Space and Mutual Information-based Decoupling."}, {"title": "Disentangled Latent Space", "content": "The core of achieving robust content representation is the removal of non-content-related features. A natural idea is to decompose speech into distinct subspaces that represent various attributes. Thus, we use two different encoders to separately extract a compact speaker style code $F_{spk}$ and a continuous content code $F_{con}$ from the mel spectrogram. Specifically, to fully leverage the powerful representational capabilities of large models, we employ the Contrastive Language-Audio Pretraining (CLAP) audio encoder as our speaker encoder. As depicted in Figure 2, the features obtained from the CLAP are processed through SAFPQ, following the same procedure as facial embedding handling but without the cross-attention module. For the content encoder, we adopt vector quantization and contrastive predictive coding techniques, commonly used in voice conversion tasks, to extract the content embeddings $F_{con}$."}, {"title": "Mutual Information-based Decoupling", "content": "Given the diversity of speech styles, merely constructing two separate latent spaces may not ensure sufficient feature decoupling. Previous decoupling methods, such as inter-speaker supervision, still result in certain overlaps among features. To address this issue, we utilize MI, which can measure the overall dependency between variables and capture both linear and non-linear relationships as a metric to evaluate the correlation between speaker embeddings and content embeddings extracted from speech. However, due to the high dimensionality and unknown distributions of the variables, directly calculating probability distributions in MI is impractical. To solve this, we employ a variational upper bound technique, to establish parameterized conditional distributions, which aids in controlling the minimization process by estimating the upper bound of MI:\n$L_{MI} = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} log(\\frac{q(F_{con,i} | F_{spk,i})}{q(F_{con,j} | F_{spk,i})}),$\nwhere $F_{spk, i}$ denotes the speaker style embeddings for the i-th sample, $F_{con,i}$ and $F_{con,j}$ represent the content embeddings for the i-th and j-th samples, respectively.\nBy minimizing the overlap information between the extracted style features and content features, we successfully establish a speaker style space related to identity and a content space associated with semantics.\nIn addition, similar to Eq. (5), we apply speech-related speaker-identity supervision $L_{id-s}$ to the style features extracted from speech. In this context, $p_{ni}$ represents the softmax probability that the speaker feature $F_{spk}$ of the n-th sample belongs to the i-th speaker, while other variables remain consistent with Eq. (5). The joint speaker-identity supervision for both facial and speech features enforces the model to recognize the consistency within the same speaker's identity and the diversity across different speakers' identities. These two loss functions prevent the generation of overly similar outputs, thereby protecting against mode collapse."}, {"title": "Alternative Text-Input with Style Control", "content": "In addition to audio inputs, text serves as a more flexible modality in practical applications because it does not require prior recording of a source speaker's speech. In this work, as illustrated in Figure 3, we introduce text as an alternative option to specify the content of generated audio, which broadens the applicability and accessibility of our framework.\nThe transition from text to audio often results in monotonous narrations due to the absence of references for emotion, accent, and rhythm. To address this issue, inspired by the OpenVoice, we develop a style control strategy that uses the base speaker TTS as a bridge to generate an intermediate single-speaker audio. This audio can be flexibly manipulated in terms of speed and emotion through style control parameters. The choice of base speaker TTS is flexible, allowing for either a single-speaker or multi-speaker TTS, as the timbre produced by the TTS is not our focus. In this task, we select the VITS model as the base speaker TTS, which accepts both text and style control inputs. The audio generated through this process then serves as the source speaker audio input into our network, where a content encoder extracts the speech content, and a pitch encoder captures the pitch information. Together with the speaker style information inferred from unseen speaker facial images, we generate the final audio output.\nIn the inference, when using text as input, the flexible control of speech and the injection of timbre inference are separated, allowing for a straightforward and training-free implementation of face-based controllable voice conversion."}, {"title": "Training Loss", "content": "We utilize L2 loss to evaluate the quality of the reconstructed mel spectrograms. The formula for this is as follows:\n$L_{rec} = ||Mel - \\hat{Mel}||^{2},$\nwhere $Mel$ and $\\hat{Mel}$ represent the Mel spectrogram input to the network and the Mel spectrogram reconstructed by the model, respectively. Additionally, we follow the training setup described in FVMVC, incorporating both inter-speaker supervision loss and face-voice mapping loss, collectively referred to as $L_{F}$.\nThe total training loss is defined as follows:\n$L = L_{rec} + \\lambda_{1}L_{con} + \\lambda_{2}L_{MI} + \\lambda_{3}L_{id-f} + \\lambda_{4}L_{id-s} + \\lambda_{5}L_{F},$"}, {"title": "Experiment and Result", "content": "To the best of our knowledge, current ZS-FVC methods utilized the LRS3 dataset, which comprises over 400 hours of TED talks collected from YouTube, for training. For a fair comparison, we follow the same dataset setup. More precisely, we selected the paired data from the top 200 speakers by video count, resulting in 11,430 videos for training and 5,173 videos for validation. For testing, we randomly selected 16 previously unseen speakers, including 8 target speakers (4 male, 4 female) and 8 source speakers (4 male, 4 female).\nImplementation Details. We employ the MTCNN to detect and align faces in each video frame. Facial features are extracted using the ViT-B/32 from CLIP, with outputs from the penultimate layer utilized to enhance generalization over the final layer. Audio is extracted from video clips via FFmpeg, and the HTSAT-base from CLAP serves as the speaker feature extractor. Training is conducted on a single Nvidia-A800 GPU with a batch size of 256 for 2000 epochs. F-V mapping is a memory-based feature mapping module, following the setup of FVMVC. For the vocoder, we utilize a pretrained ParallelWaveGAN. Loss weights specified in Eq. (8) are set at $\u03bb_{1}$ = 0.1, $\u03bb_{2}$ = 0.01, $\u03bb_{3}$ = 0.1, $\u03bb_{4}$ = 0.1, and $\u03bb_{5}$ = 1."}, {"title": "Qualitative Result and Analysis", "content": "For text-based input, we visualize the Mel spectrograms under various emotional states and speaking speeds, as depicted in Figure 4. In the \"whispering\" state, the generated audio exhibits a more dispersed energy pattern with an increase in high-frequency components due to the incomplete vibration of vocal cords typical in whispering. In contrast, in the \u201cangry\" state, the speaker's voice shows greater fluctuations and intensity, with a quicker frequency and broader dynamic range. As the speaking speed increases, the spectral energy distribution becomes more compact, reducing the intervals between syllables. Conversely, when the speed decreases, the energy distribution expands, and syllables lengthen. These observations demonstrate that ID-FaceVC performs well in controlling different emotions and speaking speeds."}, {"title": "Style Manipulation", "content": "We interpolate facial embeddings from two different speakers to generate various voice outputs, as shown in Figure 5. As the facial embeddings transition from female to male, the fundamental frequency of the generated voice gradually decreases, and the harmonic distribution becomes denser. The voices in the intermediate transition phase not only retain high-frequency harmonic features typical of female voices but also incorporate low-frequency characteristics of male voices, illustrating a smooth transition in voice characteristics from female to male. This demonstrates our model's ability to precisely control voice output based on varying facial features."}, {"title": "Distribution of Speaker Embedding", "content": "For the generated speech, we use the Resemblyzer to extract speaker embeddings and visualize them using t-SNE, as depicted in Figure 6. Voice samples generated from the same facial image form tight clusters, indicating that our model successfully maps unique vocal styles to different faces. Notably, embeddings for speakers of different genders display distinct distributions, with those of the same gender and similar ages showing closely matched speaker embeddings. This demonstrates our model's capability to effectively capture the most speech-relevant features from facial images."}, {"title": "Visualization of Different Face Angles", "content": "We randomly selected two speakers and three facial images of each, captured from various angles, to perform ZS-FVC, as shown in Appendix Figure 9. Regardless of the facial expressions and angles, the voices generated by the model remained consistent across different images of the same speaker. This consistency is attributed to the model's ability to effectively align identity-related features in the faces with style-related features in the voice, demonstrating robustness to camera positions, backgrounds, and other noise."}, {"title": "User Study", "content": "We evaluated the naturalness and similarity of generated speech through a user study involving eight experts. Each expert rated 27 sets of audio samples, with each set containing six comparative groups. As depicted in Figure 7, our method consistently outperforms current SOTA approaches in both naturalness and similarity, while also exhibiting smaller confidence intervals. These findings demonstrate that ID-FaceVC reliably produces high-quality outputs with enhanced stability.\nWe further validate our model's ability to map facial features to speech characteristics through preference tests. To increase the challenge of the experiment, we conduct gender-matched tests, selecting face and audio samples from individuals of the same gender. As depicted in Figure 8, in the face-based preference test, 57.14% of evaluators believe that ID-FaceVC produces results that better match the given facial images. In the voice-based preference test, evaluators correctly identify the match 22.9% more often when the speech is generated by ID-FaceVC rather than FVMVC, demonstrating that the speech generated by ID-FaceVC more accurately aligns with the corresponding facial images."}, {"title": "Conclusion", "content": "In this work, we introduce a novel ID-FaceVC framework, effectively generating speech that aligns with facial identity features. Our framework includes the IAQ-CL and MIDD modules to precisely map facial features to speech. Additionally, we incorporate text as an alternative modality for controlling speech content and employ a style controllable strategy that ensures speech generated from text is natural, rhythmic, and controllable. Both quantitative and qualitative experiments validate the overall effectiveness of our framework and the individual modules. Future work aims to expand beyond audio generation to include expressive facial animations, transitioning from merely \u201caudible\" to \u201cboth audible and visible.\""}]}