{"title": "Generating Global and Local Explanations for Tree-Ensemble Learning Methods by Answer Set Programming", "authors": ["Akihiro Takemura", "Katsumi Inoue"], "abstract": "We propose a method for generating rule sets as global and local explanations for tree-ensemble learning methods using Answer Set Programming (ASP). To this end, we adopt a decompositional approach where the split structures of the base decision trees are exploited in the construction of rules, which in turn are assessed using pattern mining methods encoded in ASP to extract explanatory rules. For global explanations, candidate rules are chosen from the entire trained tree-ensemble models, whereas for local explanations, candidate rules are selected by only considering rules that are relevant to the particular predicted instance. We show how user-defined constraints and preferences can be represented declaratively in ASP to allow for transparent and flexible rule set generation, and how rules can be used as explanations to help the user better understand the models. Experimental evaluation with real-world datasets and popular tree-ensemble algorithms demonstrates that our approach is applicable to a wide range of classification tasks. Under consideration in Theory and Practice of Logic Programming (TPLP).", "sections": [{"title": "1 Introduction", "content": "Interpretability in machine learning is the ability to explain or to present in understandable terms to a human (Doshi-Velez and Kim 2017; Miller 2019; Molnar 2020). Interpretability is particularly important when, for example, the goal of the user is to gain knowledge from some form of explanations about the data or process through machine learning models, or when making high-stakes decisions based on the outputs from the machine learning models where the user has to be able to trust the models. Explainability is another term that is often used interchangeably with interpretability, but some emphasize the ability to produce post-hoc explanations for the black-box models (Rudin 2019). For convenience, we shall use the term explanation when referring to post-hoc explanations in this paper."}, {"title": "2 Background", "content": "In the remainder of this paper, we shall use learning algorithms to refer to methods used to train models, as in machine learning literature. We use models and explanations to refer to machine learning models and post-hoc explanations about the said models, respectively."}, {"title": "2.1 Tree-Ensemble Learning Algorithms", "content": "Tree-Ensemble (TE) learning algorithms are machine learning methods widely used in practice, typically, when learning from tabular datasets. A trained TE model consists of multiple base decision trees, each trained on an independent subset of the input data. For example, Random Forests (Breiman 2001) and Gradient Boosted Decision Tree (GBDT) (Friedman 2001) are tree-ensemble learning algorithms. Recent surge of efficient and effective GBDT algorithms, e.g., LightGBM (Ke et al. 2017), has led to wide adoption of TE learning algorithms in practice. Although individual decision trees are considered to be interpretable (Huysmans et al. 2011), ensembles of decision trees are seen as less interpretable.\nThe purpose of using TE learning algorithms is to train models that predict the unknown value of an attribute y in the dataset, referred to as labels, using the known values of other attributes X = (x1,x2,...,xm), referred to as features. For brevity, we restrict our discussion to classification problems. During the training or learning phase, each input instance to the TE learning algorithm is a pair of features and labels, i.e. (xi,yi), where i denotes the instance index, and during the prediction phase, each input instance only includes features, (xi), and the model is tasked to produce predictions \u0177i. A collection of input instances, complete with features and labels, is referred to as a dataset. Given a dataset 9 = {(xi, yi)} with n \u2208 N examples and m \u2208 N features, a decision tree classifier t will predict the class label \u0177i based on the feature vector x\u2081 of the i-th sample: \u0177\u2081 = t(x\u012b). A tree-ensemble I uses K \u2208 N trees and additionally an aggregation function f over the K trees which combines the output from the trees: \u0177\u2081 = f(tk\u2208k(xi)). As for Random Forest, for example, f is a majority voting scheme (i.e. argmax of sum), and in GBDT \u0192 may be a summation followed by softmax to obtain \u0177i in terms of probabilities.\nIn this paper, a decision tree is assumed to be a binary tree where the internal nodes hold split conditions (e.g., x1 \u2264 0.5) and leaf nodes hold information related to class labels, such as the number of supporting data points per class label that have been assigned to the leaf nodes. Richer collections of decision trees provide higher performance and less uncertainty in prediction compared to a single decision tree. Typically, each TE model has specific algorithms for learning base decision trees, adding more trees and combining outputs from the base trees to produce the final prediction. In GBDT, the base trees are trained sequentially by fitting the residual errors from the previous step. Interested readers are referred to (Friedman 2001), and its more recent implementations, LightGBM (Ke et al. 2017) and XGBoost (Chen and Guestrin 2016)."}, {"title": "2.2 Answer Set Programming", "content": "Answer Set Programming (Lifschitz 2008) has its roots in logic programming and non-monotonic reasoning. A normal logic program is a set of rules of the form\na1 \u2190 a2, ..., am, not am+1,..., not an.\nwhere each a\u00a1 is a first-order atom with 1 \u2264 i \u2264 n and not is default negation. If only a\u2081 is included (n = 1), the above rule is called a fact, whereas if a\u2081 is omitted, it represents an integrity constraint. A normal logic program induces a collection of intended interpretations, which are called answer sets, defined by the stable model semantics (Gelfond and Lifschitz 1988). Additionally, in modern ASP systems, constructs such as conditional literals and cardinality constraints are"}, {"title": "2.3 Pattern Mining", "content": "In a general setting, the goal of pattern mining is to find interesting patterns from data, where patterns can be, for example, itemsets, sequences and graphs. For example, in frequent itemset mining (Agrawal and Srikant 1994), the task is to find all subsets of items that occur together more than the threshold count in databases. In this work, a pattern is a set of predictive rules. A predictive rule has the form c \u2190 S\u2081 \u2227 S\u2082 \u2227, ..., sn, where c is a class label, and {si} (1 \u2264 i \u2264 n) represents conditions.\nFor pattern mining with constraints, the notion of dominance is important, which intuitively reflects the pairwise preference relation (<*) between patterns (Negrevergne et al. 2013). Let C be a constraint function that maps a pattern to {T, 1}, and let p be a pattern, then the pattern p is valid iff C(p) = T, otherwise it is invalid. An example of C is a function which checks that the support of a pattern is above the threshold. The pattern p is said to be dominated iff there exists a pattern q such that p <* q and q is valid under C. Dominance relations have been used in ASP encoding for pattern mining (Paramonov et al. 2019)."}, {"title": "3 Rule Set Generation", "content": "The rule set generation problem is represented as a tuple P = {R,M,C,O}, where R is the set of all rules extracted from the tree-ensemble, M is the set of meta-data and properties associated with each rule in R, C is the set of user-defined constraints including preferences, and O is the set of optimization objectives. The goal is to generate a set of rules from R by selection under constraints C and optimization objectives O, where constraints and optimization may refer to the meta-data M. In the following sections, we describe how we construct each R, M, C and O, and finally, how we solve this problem with ASP."}, {"title": "3.2 Rule Extraction from Decision Trees", "content": "This subsection describes how R, the set of all rules, is constructed. The first two steps in \"tree-ensemble processing\" in Figure 1 are also described in this subsection. Recall that a tree-ensemble I is a collection of K decision trees, and we refer to individual trees th with subscript k. A decision tree tk has Ntk nodes and Lth leaves. Each node represents a split condition, and there are Ltk paths from the root node to the leaves. For simplicity, we assume only features that have orderable values (continuous features) are present in the dataset in the examples below. The tree on the left in Figure 2 has 4 internal nodes including the root node with condition [x\u2081 \u2264 0.2] and 5 leaf nodes; therefore there are 5 paths from the root note to the leaf nodes 1 to 5.\nFrom the left-most path of the decision tree on the left in Figure 2, the following prediction rule is created. We assume that node 1 predicts class label 1 in this instance.\nclass(1) \u2190 (x1 \u22640.2) \u2227 (x2 \u2264 4.5) \u2227 (x4 \u22642)\nAssuming that node 2 predicts class label 0, we also construct the following rule (note the reversal of the condition on x4):\nclass(0) \u2190 (x1 \u2264 0.2) \u2227 (x2 \u2264 4.5) \u2227 (x4 > 2)\nTo obtain the candidate rule set, we essentially decompose a tree-ensemble into a rule set. The steps are outlined in Algorithm 1. By constructing the candidate rule set R in this way, the bodies (antecedents) of rules included in rule sets are guaranteed to exist in at least one of the trees in"}, {"title": "3.3 Computing Metrics and Meta-data for Selection", "content": "After the candidate rule set R is constructed, we gather information about the performance and properties of each rule and collect them into a set M. This is the last step in the tree-ensemble processing process depicted in Figure 1 (\u201cAssign Metrics\"). The meta-data, or properties, of a rule are information such as the size of the rule, as defined by the number of conditions in the rule, and the ratio of instances which are covered by the rule. Computing classification metrics, e.g., accuracy and precision, requires access to a subset of the dataset with ground truth labels, which could be either a training or a validation set. On the other hand, when access to the labeled subset is not available at runtime, these metrics and their corresponding predicates cannot be used in the ASP encoding. In our experiments, we used the training sets to compute these classification metrics during rule set generation, and later used the validation sets to evaluate their performance.\nPerformance metrics measure how well a rule can predict class labels. Here we calculate the following performance metrics: accuracy, precision, recall and F1-score, as shown below.\naccuracy = $\\frac{TP+TN}{TP+TN+FP+FN}$ \nrecall = $\\frac{TP}{TP+FN}$\nprecision = $\\frac{TP}{TP+FP}$\nFl-score = 2$\\times$$\\frac{precision \\times recall}{precision + recall}$\nFor classification tasks, a true positive (TP) and a true negative (TN) refer to a correctly predicted positive class and negative class, respectively, with respect to the labels in the dataset. Conversely, a false positive (FP) and a false negative (FN) refer to an incorrectly predicted positive class and negative class, respectively. These metrics are not specific to the rules, and can be computed for trained tree-ensemble models, as well as explanations of trained machine learning models, as we shall show later in Section 5.2.4. We compute multiple metrics for a single rule, to meet a range of user requirements for explanation. One user may only be interested in simply the most accurate rules (maximize accuracy), whereas another user could be interested in more precise rules (maximize precision), or rules with more balanced performance (maximize F1-score).\nThe candidate rule set R and meta-data set M are represented as facts in ASP, as shown in Table 1. For example, Rule 1 (the first rule in Section 3.2) may be represented as follows:\""}, {"title": "3.4 Encoding Inclusion Criteria and Constraints", "content": "As with previous works in pattern mining in ASP, we follow the \u201cgenerate-and-test\" approach, where a set of potential solutions are generated by a choice rule and subsequently constraints are used to filter out unacceptable candidates. In the context of rule set generation, we use a choice rule to generate candidate rule sets that may constitute a solution (\u201cGenerate Candidate Rule Sets\" in Figure 1). In this section, we introduce the following selection criteria and constraints: (1) individual rule selection criteria that are applied on a per-rule basis, (2) pairwise constraints that are applied to pairs of rules, and (3) collective constraints that are applied to a set of rules.\nThe \"generator\" choice rule has the following form:\n% pick at least 1 rule and at maximum B rules for each class K.\n1 { selected (X) : predict_class(X, K), valid(X) } B :- class(K).\nIndividual rule selection criteria are integrated into the generator choice rule by the valid/1 predicate, where a rule rule (X) is valid whenever invalid(X) cannot be inferred.\nvalid(X) :- rule(X), not invalid(X)."}, {"title": "3.5 Optimizing Rule Sets", "content": "Finally, we pose the rule set generation problem as a multi-objective optimization problem, given the aforementioned facts and constraints encoded in ASP. The desiderata for generated rule sets may contain multiple competing objectives. For instance, we consider a case where the user wishes to collect accurate rules that cover many instances, while minimizing the number of conditions in the set. This is encoded as a group of optimization statements:\n% maximize accuracy and support, minimize the number of conditions\n#maximize { A,X : selected(X), accuracy(X,A)}.\n#maximize { S,X : selected(X), support(X,S)}.\n#minimize { L,X : selected (X), size(X,L)}.\nInstead of maximizing/minimizing the sums of metrics, we may wish to optimize more nuanced metrics, such as average accuracy and coverage of selected rules:\n% maximize average accuracy and coverage\nselected_rules (SR) :- SR = #count { I : selected (I) }, SR != 0.\n#maximize { Ai/(SSR)@3, I : selected (I), size(I,S),"}, {"title": "4 Rule Set Generation for Global and Local Explanations", "content": "In this section, we will describe how to generate global and local explanations with the rule set generation method. Guidotti et al. (2018) defined global explanation as descriptions of how the overall system works, and local explanation as specific descriptions of why a certain decision was made. We shall now adopt these definitions to our rule set generation task from tree-ensemble models.\nDefinition 1\nA global explanation is a set of rules derived from the tree-ensemble model, that approximates the overall predictive behavior of the base tree-ensemble model.\nExamples of measures of approximation for global explanations are: accuracy, precision, recall, F1-score, fidelity and coverage.\nExample 4\nGiven a tree-ensemble as in Figure 2, a global explanation can be constructed from a candidate rule set that includes all possible paths to the leaf nodes (1, 2, ..., 10 in Figure 2), then selecting rules based on user-defined criteria.\nDefinition 2\nAn instance to be explained is called a prediction instance. A local explanation is a set of rules derived from the tree-ensemble model, that approximates the predictive behavior of the base tree-ensemble model when applied to a specific prediction instance.\nExample 5\nGiven a tree-ensemble as in Figure 2, and a prediction instance, a local explanation can be constructed by only considering rules that were active during the prediction, then selecting rules based on user-defined criteria. For example, if leaf nodes 2 and 6 were active, then R only includes rules constructed from the paths leading to nodes 2 and 6. Here, a leaf node is considered active during prediction if the decision path for the prediction instance leads to it, meaning the conditions leading up to that node are satisfied by the instance's features.\nThe predictive behavior in this context refers to the method by which the model makes the prediction (aggregating decision tree outputs) and the outcomes of the prediction. The differences between the global and local explanations have implications on the encoding we use for rule set generation. Note that these two types of explanations serve distinct purposes. The global explanation seeks to explain the model's overall behavior, while the local explanation focuses on the reasoning behind a specific prediction instance. There is no inherent expectation for a global explanation to align with or fully encompass a local explanation. In particular, when a local explanation is applicable to multiple instances due to these instances having similar feature values, for instance, this local explanation might not be able to accurately predict for these instances. This is measured by a precision metric, and evaluated further in Section 5.3\nIn Table 3 we show examples of global and local explanations on the same dataset (adult). For this dataset, the task is to predict whether an individual earns more or less than $50,000 annually. The global and local explanations consist of 4 conditions, and share an attribute (hours-per-week) with different threshold values. While these two rules have the same outcome, the attributes in the bodies are different: in this instance, the global explanation focuses more on the numerical attributes, while the local explanation contains categorical attributes.\nRecall that we start with the candidate rule set, R, which is created by processing the tree-ensemble model. The rules in R are different between global and local explanations, even when the underlying tree-ensemble model is the same. For global explanations, we can enumerate all rules including internal nodes (Section 3.2) regardless of the outcomes of the rules because we"}, {"title": "5 Experiments", "content": "In this section, we present a comprehensive evaluation of our rule set generation framework, focusing on both global and local explanations. We evaluate the explanations on public datasets using various metrics, and we also compare the performance to existing methods, including rule-based classifiers. We used several metrics to assess the quality of the generated explanations. These metrics are designed to evaluate different aspects of the explanations, including their comprehensibility, fidelity and usability. Below, we provide an overview of the metrics used in our evaluations. Detailed discussions of these metrics can be found in the respective sections of this paper.\nGlobal Explanations (Section 5.2):\n\u2022 Number of Rules and Conditions (Section 5.2.1 and 5.2.2): Assesses the simplification of the original model by counting the number of rules and conditions.\n\u2022 Relevance (Section 5.2.3): Measures the relevance of the rules by comparing the classification performance against the original model.\n\u2022 Fidelity (Section 5.2.4): Measures the degree to which the rules accurately describe the behavior of the original model.\n\u2022 Run Time (Section 5.2.6): Measures the efficiency of generating global explanations.\nLocal Explanations (Section 5.3):\n\u2022 Number of Conditions (Section 5.3.1): Measures the conciseness of the local explanations by counting the number of conditions.\n\u2022 Local-Precision and Coverage (Section 5.3.2): Local-precision compares the model's predictions for instances covered by the local explanation with the prediction for the instance that induced the explanation. Local-coverage measures the proportion of instances in the validation set that are covered by the local explanation.\n\u2022 Run Time (Section 5.3.3): Measures the efficiency of generating local explanations."}, {"title": "5.1 Experimental Setup", "content": "We used in total 14 publicly available datasets, where except for the adult dataset, all datasets were taken from the UCI Machine Learning Repository(Dua and Graff 2017). Datasets were chosen from public repositories to ensure a diverse range in terms of the number of instances, the number of categorical variables, and class balance. This was done intentionally, to observe the variance in, for example, explanation generation times. Additionally, the variation in categorical variables ratio and class balance was designed to produce a wide array of tree ensemble configurations (e.g., more or fewer trees, varying widths and depths). We expected these configurations, in turn, to influence the nature of the explanations generated. We included 3 datasets (adult, credit german, compas) for comparison because they were widely used in local explain-ability literature. The adult dataset is actually a subset of the census dataset, but we included the former for consistency with existing literature, and the latter for demonstrating the applicability of our approach to larger datasets. The summary of these datasets is shown in Table 4."}, {"title": "5.1.2 Experimental Settings", "content": "We used clingo 5.4.010 (Gebser et al. 2014) for answer set programming, and set the time-out to 1,200 seconds. We used RIPPER implemented in Weka (Witten et al. 2016) and an open-source implementation of RuleFit\u00b9\u00b9 where Random Forest was selected as the rule generator, and scikit-learn12 (Pedregosa et al. 2011) for general machine learning functionalities. Our experimental environment is a desktop machine with Ubuntu 18.04, Intel Core i9-9900K 3.6GHz (8 cores/16 threads) and 64 GB RAM. For reproducibility, all source codes for the implementation, experiments and preprocessed datasets are available from our GitHub repository.13\nUnless noted otherwise, all experimental results reported here were obtained with 5-fold cross validation, with hyperparameter optimization in each fold. To evaluate the performance of the extracted rule sets, we implemented a naive rule-based classifier, which is constructed from the rule sets extracted with our method. In this classifier, we apply the rules sequentially to the validation dataset and if all conditions within a rule are true for an instance in the dataset, the consequent of the rule is returned as the predicted class. More formally, given a set of rules RS CR with cardinality Rs that shares the same consequent class(Q), we represent this rule-based classifier as the disjunction of antecedents of the rules:\nclass(Q) \u2190 body(R1) V body(R2) V ... V body(Rr) where 1 \u2264 r \u2264 |Rs|\nFor a given data point, it is possible that there are no rules applicable, and in such cases the most common class label in the training dataset is returned."}, {"title": "5.2 Evaluating Global Explanations", "content": "Let us recall that the purpose of generating global explanations is to provide the user with a simpler model of the original complex model. Thus, we introduce proxy measures to evaluate (1) the degree to which the model is simplified, by the number of extracted rules and conditions, (2) the relevance of the extracted rules, by comparing classification performance metrics against the original model, and (3) the degree to which the explanation accurately describes the behavior of the original model, by fidelity metrics.\nWe conducted the experiment in the following order. First, we trained Decision Tree, Random Forest and LightGBM on the datasets in Table 4. Selected optimized hyperparameters of the tree-ensemble models are also reported in Table 4. Further details on hyperparameter optimization are available in Appendix B. We then applied our rule set generation method to the trained tree-ensemble models. Finally, we constructed a naive rule-based classifier using the set of rules extracted in the previous step, and calculated performance metrics on the validation set. This process was repeated in a 5-fold stratified cross validation setting to estimate the performance.\nWe compare the characteristics of our approach against the known methods RIPPER and RuleFit.\nWe used the following selection criteria to filter out rules that were considered to be unde-sirable; for example, those rules with low accuracy or low coverage. We used the same set of selection criteria for all datasets, irrespective of underlying label distribution or learning algorithms. When the candidate rules violate any one of those criteria, they are excluded from the candidate rule set, which means that in the worst case where all the candidate rules violate at least one criterion, this encoding will result in an empty rule set (see Section 3.4).\n% exclude long rules\ninvalid(I) :- size(I,S), S > 10, rule(I).\n% exclude inaccurate rules\ninvalid(I) :- error_rate (I,E), E > 70, rule(I).\n% exclude low precision rules\ninvalid(I) :- precision(I,P), P < 2, rule(I).\n% exclude low recall rules\ninvalid(I) :- recall(I,R), R < 2, rule(I).\n% exclude low coverage rules\ninvalid(I) :- support(I, Sp), Sp < 2, rule(I).\nAnother scenario in which our method will produce an empty rule set is when the tree-ensemble contains only \"leaf-only\" or \"stump\" trees, that have one leaf node and no splits. In this case, we have no split information to create candidate rules; thus, an empty rule set is returned to the user. This is often caused by inadequate setting of hyperparameters that control the growth of the trees, especially when using imbalanced datasets. It is however outside the scope of this paper, and we will simply note such cases (empty rule set returned) in our results without further consideration."}, {"title": "5.2.1 Number of Rules", "content": "The average sizes of generated rule sets are shown in Table 5. The sizes of candidate rule sets, from which the rule sets are generated, are listed in the |R| columns in Table 4. Rule set size of 1 means that the rule set contains a single rule only. As one might expect, the Decision Tree consistently has the smallest candidate rule set, but in some cases the Random Forest produced considerably more candidate rules than the LightGBM, e.g., cars, compas. Our method can pro-"}, {"title": "5.2.2 Number of Conditions in Rules", "content": "In this subsection, we compare the average number of conditions in each rule and the total number of conditions in rules. One would expect a more precise rule to have a larger number of conditions in its body compared to the one that is more general. It should be noted that, however, due to the experimental condition, the maximum number of conditions in a single rule is set by the maximum depth parameter in each of the learning algorithms, which in turn is set by the hyperparameter tuning algorithm.\nThe average number of conditions in each rule are shown in Table 5. We note that sometimes the algorithms may produce rules without any conditions in the bodies, such as when the induced trees have only a single split node at the root; thus the average number reported in Table 5 may be biased towards lower numbers. From the table, we see that the average number of conditions in a rule generally falls in the range of between 1 and 10, and this is consistent with the search range of hyperparameters we set for the experiments. Table 5 shows the total number of conditions in a rule set. Unlike the average number of conditions in a rule, we see a large difference between our method and the benchmark methods. In all datasets, RuleFit produced the highest counts of conditions in rules, followed by RIPPER and the ensemble-based methods. From Table 5, we make the following observations: (1) the length of individual rules does not vary as much as the number of rules between different methods (2) the high number of conditions in rules extracted by RuleFit can be explained by the high number of rules, where the length of individual rules are comparable to other methods."}, {"title": "5.2.3 Relevance of Rules", "content": "To quantify the relevance of the extracted rules, we measured the ratio of performance metrics using the naive rule-based classifier by 5-fold cross validation (Table 6). A performance ratio of less than 1.0 means that the rule-based classifier performed worse than the original classifier (LightGBM and Random Forest), whereas a performance ratio greater than 1.0 means the rule set's performance is better than the original classifier. We used a version of the ASP encoding shown in Section 3.5 where the accuracy and coverage are maximized. RIPPER was excluded from this comparison because it has a built-in rule generation and refinement process, and it does not have a base model, whereas our method and RuleFit use variants of tree-ensemble models as base models.\nFrom Table 6 we observe that in terms of accuracy, RuleFit generally performs as well as, or marginally better than, the original Random Forest. On the other hand, although our method can produce rule sets that are comparable in performance against the original model, they do not produce rules that perform significantly better. With Decision Tree and Random Forest, the generated rule sets perform much worse than the original model, e.g., in kidney, voting. The LightGBM+ASP combination resulted in the second-best performance overall, where the resulting rules' performances were arguably comparable (0.8-0.9 range) to the original model with a few exceptions (e.g., census F1-score) where the performance ratio was about half of the original. While RuleFit's performance was superior, our method could still produce rule sets with reasonable performance with much smaller rule sets that are an order of magnitude smaller than RuleFit. A rather unexpected result was that using our method (Random Forest) or RuleFit significantly improved the F1-score in the census dataset. In Table 6 we can see that recall was the major contributor to this improvement."}, {"title": "5.2.4 Fidelity Metrics of Global Explanations", "content": "In Section 5.2.3, we compared the ratio of performance metrics of different methods when measured against original labels. In the context of evaluating explanation methods, it is also important to investigate the fidelity, i.e., to which extent the explanation is able to accurately imitate the original model (Guidotti et al. 2018). A fidelity metric is calculated as an agreement metric between the prediction of the original model and the explanation, the latter in this case is the rule set. More concretely, when the predicted class by the model is positive and that by the explanation is positive, it is a true positive (TP), and when the latter is negative, it is a false negative (FN). Thus, the fidelity metrics can be calculated in the same manner as performance metrics using the equations shown in Section 3.3. RIPPER was excluded from this comparison for the same reasons as outlined in Section 5.2.3.\nThe average fidelity metrics (accuracy, F1 score, precision and recall) are shown in Table 7. The overall trend is similar to the previous section on rule relevance, where RuleFit performs the best overall in terms of fidelity. The accuracy metrics for our method shows that the global explanations, in general, behaved similarly to the original model, although RuleFit was better in most of the datasets. The precision metrics show that, even when excluding the results for Decision Tree (which is not a tree-ensemble learning algorithm), our method could produce explanations that had high fidelity in terms of precision compared to RuleFit. The fidelity metrics may be improved further by including them in the ASP encodings, since they were not part of the selection criteria or optimization goals."}, {"title": "5.2.5 Changing Optimization Criteria", "content": "The definition of optimization objectives has a direct influence over the performance of the resulting rule sets, and the objectives need to be set in accordance with user requirements. The answer sets found by clingo with multiple optimization statements are optimal regarding the set of goals defined by the user. Instead of using accuracy, one may use other rule metrics as defined in Table 1 such as precision and/or recall. If there are priorities between optimization criteria, then one could use the priority notation (weight@priority) in clingo to define them. Optimal answer sets can be computed in this way, however, if enumeration of such optimal sets is important, then one could use the pareto or lexico preference definitions provided by as-prin (Brewka et al. 2015) to enumerate Pareto optimal answer sets. Instead of presenting a single optimal rule set to the user, this will allow the user to explore other optimal rule sets.\nTo investigate the effect of changing optimization objectives, we changed the ASP encoding from max. accuracy-coverage to max. precision-coverage (shown in Section 3.4) while keeping other parameters constant. The results are shown in Table 8. Note that it is the ratio of precision score shown in the table, as opposed to accuracy or F1-score in the earlier tables. Here, since we are optimizing for better precision, we expect the precision-coverage encoding to produce rule sets with better precision scores than the accuracy-coverage encoding. For the Decision Tree and Random Forest + ASP, the effect was not as pronounced as we expected, but we observed noticeable differences in datasets compas and credit german. For the LightGBM+ASP combination, we observed more consistent difference, except for the credit german dataset, the encoding produced intended results in most of the datasets in this experiment."}, {"title": "5.2.6 Global Explanation Running Time", "content": "The average running time of generating global explanations is reported in Table 9. The running time measures the rule extraction and rule set generation steps for our method, and measures the running time for RuleFit, but excludes the time taken for the model training (e.g., Random Forest) and hyperparameter optimization. Comparing the methods that share the same base model (RF+ASP and RuleFit, both based on Random Forest), we observe that our method is slower than RuleFit except when the datasets are relatively large (e.g., adult, census, compas and credit taiwan), and in the latter cases our method can be much faster than RuleFit. Similar trend is observed for LightGBM, but here in some cases our method was faster than RuleFit (e.g., autism, heart and voting)."}, {"title": "5.3 Evaluating Local Explanations", "content": "The purpose of generating local explanations is to provide the user with an explanation for the model's prediction for each predicted instance. Here, we use commonly used metrics local-precision and coverage as proxy measures for the quality of the explanation.14 The local-precision compares the (black-box) model predictions of instances covered by the local explanation and the model prediction of the original instance used to induce the local explanation. The coverage is the ratio of instances in the validation set that are covered by the local explanation. These two"}, {"title": "6 Related Works", "content": "Summarizing tree-ensemble models has been studied in literature, see for example, Born Again Trees (Breiman and Shang 1996), defrag Trees (Hara and Hayashi 2018) and inTrees (Deng 2019). While exact methods and implementations differ among these examples, a popular approach to tree-ensemble simplification is to create a simplified decision tree model that approximates the behavior of the original tree-ensemble model. Depending on how the approximate tree model is constructed, this could lead to a deeper tree with an increased number of conditions, which makes them difficult to interpret.\nIntegrating association rule mining and classification is also known, e.g., Class Association Rules (CARS) (Liu et al. 1998), where association rules discovered by pattern mining algorithms are combined to form a classifier. Repeated Incremental Pruning to Produce Error Reduction (RIPPER) (Cohen 1995) was proposed as an efficient approach for classification based on association rule mining, and it is a well-known rule-based classifier. In CARs and RIPPER, rules are mined from data with dedicated association rule mining algorithms, then processed to produce a final classifier.\nInterpretable classification models is another area of active research. Interpretable Decision Sets (IDS) (Lakkaraju et al. 2016) are learned through an objective function, which simultaneously optimizes accuracy and interpretability of the rules. In Scalable Bayesian Rule Lists (SBRL) (Yang et al. 2017), probabilistic IF-THEN rule lists are constructed by maximizing the posterior distribution of rule lists. In RuleFit (Friedman and Popescu 2008), a sparse linear model is trained with rules extracted from tree-ensembles. RuleFit is the closest to our work in this regard"}]}