{"title": "PROCBENCH: BENCHMARK FOR MULTI-STEP REASONING AND FOLLOWING PROCEDURE", "authors": ["Ippei Fujisawa", "Sensho Nobe", "Hiroki Seto", "Rina Onda", "Yoshiaki Uchida", "Hiroki Ikoma", "Pei-Chun Chien", "Ryota Kanai"], "abstract": "Reasoning is central to a wide range of intellectual activities, and while the capabilities of large language models (LLMs) continue to advance, their performance in reasoning tasks remains limited. The processes and mechanisms underlying reasoning are not yet fully understood, but key elements include path exploration, selection of relevant knowledge, and multi-step inference. Problems are solved through the synthesis of these components. In this paper, we propose a benchmark that focuses on a specific aspect of reasoning ability: the direct evaluation of multi-step inference. To this end, we design a special reasoning task where multi-step inference is specifically focused by largely eliminating path exploration and implicit knowledge utilization. Our dataset comprises pairs of explicit instructions and corresponding questions, where the procedures necessary for solving the questions are entirely detailed within the instructions. This setup allows models to solve problems solely by following the provided directives. By constructing problems that require varying numbers of steps to solve and evaluating responses at each step, we enable a thorough assessment of state-of-the-art LLMs' ability to follow instructions. To ensure the robustness of our evaluation, we include multiple distinct tasks. Furthermore, by comparing accuracy across tasks, utilizing step-aware metrics, and applying separately defined measures of complexity, we conduct experiments that offer insights into the capabilities and limitations of LLMs in reasoning tasks. Our findings have significant implications for the development of LLMs and highlight areas for future research in advancing their reasoning abilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Reasoning is a fundamental component of intelligence, involving complex processes where the application of knowledge and logical inference are deeply intertwined (Wason & Johnson-Laird, 1972; Huang & Chang, 2023; Fagin et al., 1995). We define reasoning as the progression toward a specific goal through multiple steps of inference to derive new knowledge from existing information (Yu et al., 2024); it begins with goal setting, which can be self-initiated or explicitly provided, as is often the case in problem-solving; then, a series of inference is repeated until the goal is achieved, handling both explicit and implicit knowledge such as common sense or domain-specific information.\nThere are three classes of inference: induction, deduction and abduction (Peirce, 1992); induction is a process of generalization from a specific observation, while deduction is the opposite: from general to specific, and abduction is the inference to the best explanation from observations (Huang & Chang, 2023; Yu et al., 2024). All of them in common often require an exploration in a vast search space to determine the correct path to the goal, choosing necessary knowledge for the decisions from substantial knowledge.\nAlthough reasoning involves such complex processes, here we focus on the process to follow a fixed path to a given goal with explicit knowledge, proposing ProcBench, which consists of tasks that do not require complex knowledge but can be solved by following the provided procedures. The goal of this dataset is to evaluate the ability of AI systems to follow and execute specific instructions, which we refer to as instruction followability. While trivial for humans, it can be challenging for AI systems that do not strictly adhere to the instructions. In choosing the tasks, we take into consideration that they have the following properties:\n\u2022 The procedure to reach the goal is explicitly provided, so no search for the correct path is necessary.\n\u2022 Minimal implicit knowledge beyond basic language comprehension is required to execute the procedure.\n\u2022 The steps in the procedure are straightforward for humans to execute.\nNumerous benchmarks (Saxton et al., 2019; Hendrycks et al., 2021; 2020a; Yu et al., 2023; Cobbe et al., 2021; Dinh et al., 2024; Suzgun et al., 2022) have been proposed to evaluate reasoning in AI systems, ranging from basic arithmetic to advanced theorem proving and competitive programming challenges (Jain et al., 2024; Lu et al., 2021; Zhuo et al., 2024). While these benchmarks have evolved to tackle more complex tasks, they often require implicit knowledge, making it difficult to isolate and assess an Al's procedural adherence. Furthermore, traditional AI evaluation methods have largely focused on final outputs, often at the expense of the reasoning process itself. This oversight can lead to systems that perform well in simple scenarios but fail when confronted with complex tasks that require careful, multi-step reasoning. ProcBench sets itself apart by emphasizing evaluative tasks that require minimal prerequisite knowledge and demanding exact adherence to instructions, whilst all necessary information is provided within the task description \u2013 thereby filling a critical gap in current evaluation methods.\nInstruction followability is crucial across several key areas in AI, including reasoning (Yu et al., 2024), explainable AI (Arrieta et al., 2019), mitigating hallucinations (Bai et al., 2024), and AI alignment (Ji et al., 2023). Multi-step inference requires the models to follow instructions precisely to reach the correct conclusions. Models that adhere strictly to instructions provide clear intermediate reasoning steps, resulting in more transparent and interpretable outputs, which is essential for explainable AI. Strict procedural adherence reduces the risk of generating inaccurate or nonsensical information, thereby mitigating hallucinations by ensuring logical connections of distinct pieces of knowledge. Furthermore, ensuring that AI systems follow human instructions is fundamental to aligning their behavior with human intentions from the aspect of both safety and functionality.\nUsing ProcBench, we evaluated several state-of-the-art large language models (LLMs) to assess their instruction followability. Our evaluations of several state-of-the-art LLMs demonstrate a wide range of performance across tasks and complexity levels. Some models, such as o1-preview and o1-mini, performed consistently well on simpler tasks, accurately following multi-step instructions. However, as the complexity increased with longer sequences, even these models exhibited a significant drop in performance, highlighting their limitations in handling complex, multi-step reasoning. These findings emphasize the need for future improvements in procedural reasoning and offer a pathway for advancing LLMs in this area."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 BENCHMARKS FOR LARGE LANGUAGE MODELS", "content": "Various benchmarks have been proposed to assess the capabilities of LLMs across diverse domains. Some benchmarks evaluate knowledge, reading comprehension, and general reasoning skills in fields such as science, medicine, and law (Hendrycks et al., 2020b; Dua et al., 2019; Lai et al., 2017). Others focus on problem-solving and code generation capabilities (Suzgun et al., 2022; Chen et al., 2021). Mathematical reasoning is assessed through specific benchmarks (Saxton et al., 2019; Cobbe et al., 2021; Shi et al., 2023), while some are designed to evaluate LLMs' performance in software operations, such as executing commands and web browsing (Xi et al., 2024; Liu et al., 2023; Ma et al., 2024). Instruction-following capabilities have also begun to receive attention through recent benchmarks (Zhou et al., 2023). Due to the rapid development of LLMs, such benchmarks that require implicit knowledge described above generally require frequent updates by adding more difficult tasks, or have short useful lifespans (Mart\u00ednez-Plumed et al., 2021). Furthermore, because these benchmarks tend to focus on specific task-oriented skills that LLMs have been trained on, they are not fully adequate for assessing the models' general intellectual capabilities, which is also pointed out in Chollet (2019).\nIn contrast to existing benchmarks, ProcBench focuses on assessing procedural reasoning, an essential component of complex problem-solving that remains underexplored. By isolating procedural follow-up from domain-specific knowledge, ProcBench reveals significant limitations in the ability of LLMs to strictly adhere to detailed, multi-step instructions. This makes the challenges of procedural reasoning explicit and provides a new perspective for evaluating and improving LLMs in domains that require precise, sequential operations."}, {"title": "2.2 INSTRUCTION FOLLOWING", "content": "Instruction Following (Lou et al., 2024) has become an important research area, particularly in the context of LLMs (Zhou et al., 2023; Kim et al., 2024; Mishra et al., 2022). The primary goal in this field is to evaluate whether models can accurately interpret and execute given instructions. However, much of the existing research focuses on the final output, with less attention paid to the reasoning process that leads to that output.\nOur research links Instruction Following to reasoning, positioning it as a specialized form of multi-step reasoning. We emphasize the importance of evaluating not only whether the final output follows the instructions but also the intermediate steps taken during the problem-solving process, distinguishing our work from previous studies."}, {"title": "3 PROCBENCH", "content": "In this section, we introduce ProcBench, a benchmark dataset for testing LLMs' instruction followability. The models are asked to solve simple but long step-by-step tasks by precisely following the provided instructions. Each step is a simple manipulation of either a string, a list of strings, or an integer number. There are 23 types of tasks in total, listed in Table 1. The tasks are designed to require only minimal implicit knowledge, such as a basic understanding of the English language and the ordering of alphabets. While the complexity increases with the number of steps, the tasks can essentially be solved by following the instructions without the need for specialized knowledge. While these tasks are easy for humans regardless of their lengths as long as we can execute each step, LLMs may fail as the number of steps becomes larger."}, {"title": "3.1 STRUCTURE", "content": "Each example is composed of the combination of a template and a question. Each task is associated with a fixed template, which contains the procedure for solving the question. A concrete example of this combination is shown in Figure 1a, along with the corresponding intermediate states and final state as the ground truth in Figure 1b. Additional templates can be found in Appendix A. The question represents the specific problem and is generated by the Generator. The Generator also produces the correct answer and the intermediate states leading to that answer simultaneously. Since the questions are generated by the Generator, it is easy to increase the number of examples in our dataset. However, for the convenience of evaluation, we provide a fixed dataset. We set the number of steps for each problem from 2 to 25, generating 10 examples per number. Therefore, each task consists of 240 examples, with a total of 5,520 examples. We further classify the step counts of 2 to 6 as Short, 7 to 16 as Medium, and 17 to 25 as Long, and aggregate metrics at these levels as well.\nThe models receive the combination of a template and a question and are required to provide not only the final state but also the intermediate states. Thus, the responses are more complex than simply providing words or choices. The elements contained in the intermediate and final states are of types int, str, and list. The list type contains int or str as its elements. The types of these elements differ by task, as listed in Table 1. The model's predictions must be converted into a JSON format that adheres to these types to facilitate evaluation through the metric functions."}, {"title": "3.2 METRICS", "content": "We introduce Prefix Accuracy (PA) as a metric for evaluating sequence-based tasks, with a primary focus on assessing how accurately a system adheres to a specific procedure to solve a question. These tasks often involve multi-step procedures or free-form answer generation, where even small deviations from the correct steps can lead to significant mismatches. The sequences in question represent complex state transitions, making precise adherence to the intended procedure critical for successful problem-solving."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We evaluate the performance of seven state-of-the-art models using our benchmark, which covers a variety of task types and complexities. The models selected for evaluation include Claude-3.5-sonnet (Anthropic, 2024), Mistral-large (Jiang et al., 2023), Gemini-1.5-Pro (Google, 2024), GPT-40, GPT-40-mini (OpenAI, 2023), 01-mini, and o1-preview (OpenAI, 2024a).\nThe tasks presented to the models require generating sequences rather than simple question-and-answer pairs. Given that the output of LLMs is generally provided in free-form text, we convert the responses into a structured JSON format (OpenAI, 2024b) to facilitate evaluation. This transformation process, performed by GPT-40, is uniformly applied to all models, and the evaluation metrics defined in Section 3.2 are computed based on this standardized format. It is important to note that the results reflect not only the models' raw accuracy but also the impact of the conversion process on the final evaluation scores."}, {"title": "4.2 RESULTS", "content": "Model Performance in Summary. Table 2 provides a comprehensive comparison of model performances across varying task difficulty levels-Short, Medium, and Long-evaluated through the metrics of PA (Prefix Accuracy) and SM (Sequential Match). The o1-preview model consistently leads across most categories, particularly excelling in the Medium and Long tasks, where it achieves the highest scores for both PA and SM. In contrast, o1-mini demonstrates a competitive edge in simpler tasks, outperforming ol-preview in the Short task with a PA of 0.801 and SM of 0.722."}, {"title": "5 DISCUSSION", "content": "Relationship Between Instruction Following and Reasoning. The relationship between Instruction following and reasoning is a fascinating one. For humans, the most challenging aspects of reasoning often involve the application of knowledge, particularly implicit knowledge. In contrast, simply following instructions is generally not considered reasoning. However, we propose that Instruction following can be understood as a specialized form of reasoning, particularly when it is disentangled from implicit knowledge and focuses on scenarios where the path to the goal is explicitly defined. Although this may not initially seem like reasoning, once one successfully navigates the search for the correct procedure and applies the relevant knowledge, it becomes closely aligned with the types of problems we seek to address. In this sense, our approach deconstructs the reasoning process. While the ultimate reasoning systems may not explicitly separate these functions, they should still be capable of solving the kinds of problems we address.\nWe developed a dataset to explore this connection, and our results show that models recognized for their strong reasoning capabilities, such as ol-preview and o1-mini, performed well. This suggests a qualitative link between reasoning ability and the capacity to follow instructions. However, the experiments also revealed that even tasks that may seem straightforward or merely tedious to humans-tasks that appear self-evident\u2014are not consistently solved by the models. On the other hand, these models demonstrate strong reasoning capabilities in domains like law or physics (OpenAI, 2023; Google, 2024). Since the effective application of knowledge can reduce the number of computational steps, it suggests that state-of-the-art LLMs may be more adept at leveraging knowledge to solve complex problems rather than excelling at multi-step procedural reasoning itself. This underscores a fundamental challenge in the current deep learning paradigm, where many models struggle with intricate reasoning tasks unless they can heavily rely on prior knowledge.\nMinimal Implicit Knowledge. While the ideal goal is to eliminate all implicit knowledge requirements, some minimal assumptions are inevitable. For example, we assume a basic understanding of the English language, the order of the alphabet, and that numbers such as 0, 1, 2, and so on represent numerical values. However, these assumptions are deliberately kept minimal and are significantly less specialized compared to the knowledge required for tasks in fields like physics, chemistry, law, or mathematics. By focusing on such foundational concepts, the dataset preserves a structured challenge that emphasizes reasoning and procedural execution rather than relying on domain-specific knowledge.\nExpected Use Cases and Limitations. The simplest and most straightforward use of our dataset is for evaluating LLMs, particularly with respect to their reasoning capabilities. This is the primary intended application, allowing researchers to assess how well a new model handles multi-step reasoning tasks.\nAdditionally, ProcBench can be used to evaluate variations of methods such as In-Context Learning or Chain-of-Thought reasoning (Wei et al., 2023). However, we do not envision the use of task-specific prompts for each of the 23 distinct tasks in the dataset, as this would introduce domain-specific knowledge. Such prompts might enable the model to skip significant portions of the actual reasoning process, which would defeat the purpose of evaluating its raw reasoning capabilities. An extreme example of this would be programming-based solutions, where directly introducing task-specific solvers should be avoided. If a general-purpose model, such as one with coding capabilities, can solve tasks without specific tuning, this reflects its versatility. However, in such cases, the intended measurement of multi-step Instruction followability would no longer be feasible within this dataset.\nAlthough the primary focus is on the current LLM paradigm, ProcBench is still applicable to traditional machine learning models, such as those used in inductive programming, which learn from concrete examples. However, the fixed dataset provided for evaluation is unlikely to be sufficient for training such models. In this case, the Generator could be utilized to augment the dataset, enabling a model to be constructed from scratch with the goal of following procedural instructions."}, {"title": "6 CONCLUSION", "content": "We introduced ProcBench, a benchmark designed to assess LLMs on their ability to follow explicit, multi-step instructions. By concentrating on tasks that require minimal implicit knowledge, ProcBench allows us to evaluate the procedural reasoning capabilities of models independent of their reliance on domain-specific knowledge. Our results show that while state-of-the-art models like ol-preview and o1-mini perform well on tasks involving shorter steps, they face significant difficulties as the step length increases. This highlights a critical limitation in current LLMs: despite excelling in knowledge-driven tasks, they struggle to consistently follow detailed procedural instructions when faced with more complex, multi-step reasoning.\nOur findings emphasize the distinction between knowledge-based reasoning and instruction following, an area where LLMs have yet to achieve consistent mastery. Enhancing the ability of these models to precisely follow instructions will be key to improving their performance in more complex problem-solving scenarios. Future work will expand ProcBench to encompass a broader range of tasks and further investigate how explicit instruction-following capabilities can be more effectively integrated into models trained on traditional benchmarks. This will contribute to developing systems that can reliably handle multi-step reasoning across diverse domains."}, {"title": "REPRODUCIBILITY", "content": "Details regarding the construction of the benchmark and the models used in the experiments can be found in Section 3, Section 4.1 and Table 3. The dataset we created, the code used to generate it, the prediction results, and the evaluation results will be made publicly available after the paper is accepted for publication."}, {"title": "A DATASET EXAMPLES", "content": ""}, {"title": "A.1 EXAMPLE TEMPLATES", "content": "Here, we provide detailed examples of 3 out of the 23 templates used in our experiments. The remaining templates can be found in the dataset that has been made publicly available. Please refer to the dataset for the full set of templates."}, {"title": "A.2 EXAMPLE PROMPTS", "content": "Replace characters in a given string according to a given list of character pairs step by step as follows.\nStart from the first character in the string.\nAt each step, if the current target character matches the first element of any pairs in the list, replace the character with the second element of the pair. Then, move on to the next character in the string.\nRepeat the step until the end of the string.\nProvide the final string along with the intermediate strings after each step in a list.\nDo not include the initial state and final state in the list of intermediate states.", "PA(T, P) = \\frac{PML(T, P)}{max(N, M)}": null}, {"title": "4 EXPERIMENT", "content": "We evaluate the performance of seven state-of-the-art models using our benchmark, which covers a variety of task types and complexities. The models selected for evaluation include Claude-3.5-sonnet (Anthropic, 2024), Mistral-large (Jiang et al., 2023), Gemini-1.5-Pro (Google, 2024), GPT-40, GPT-40-mini (OpenAI, 2023), 01-mini, and o1-preview (OpenAI, 2024a).\nThe tasks presented to the models require generating sequences rather than simple question-and-answer pairs. Given that the output of LLMs is generally provided in free-form text, we convert the responses into a structured JSON format (OpenAI, 2024b) to facilitate evaluation. This transformation process, performed by GPT-40, is uniformly applied to all models, and the evaluation metrics defined in Section 3.2 are computed based on this standardized format. It is important to note that the results reflect not only the models' raw accuracy but also the impact of the conversion process on the final evaluation scores."}, {"title": "4.2 RESULTS", "content": "Model Performance in Summary. Table 2 provides a comprehensive comparison of model performances across varying task difficulty levels-Short, Medium, and Long-evaluated through the metrics of PA (Prefix Accuracy) and SM (Sequential Match). The o1-preview model consistently leads across most categories, particularly excelling in the Medium and Long tasks, where it achieves the highest scores for both PA and SM. In contrast, o1-mini demonstrates a competitive edge in simpler tasks, outperforming ol-preview in the Short task with a PA of 0.801 and SM of 0.722."}, {"title": "5 DISCUSSION", "content": "Relationship Between Instruction Following and Reasoning. The relationship between Instruction following and reasoning is a fascinating one. For humans, the most challenging aspects of reasoning often involve the application of knowledge, particularly implicit knowledge. In contrast, simply following instructions is generally not considered reasoning. However, we propose that Instruction following can be understood as a specialized form of reasoning, particularly when it is disentangled from implicit knowledge and focuses on scenarios where the path to the goal is explicitly defined. Although this may not initially seem like reasoning, once one successfully navigates the search for the correct procedure and applies the relevant knowledge, it becomes closely aligned with the types of problems we seek to address. In this sense, our approach deconstructs the reasoning process. While the ultimate reasoning systems may not explicitly separate these functions, they should still be capable of solving the kinds of problems we address.\nWe developed a dataset to explore this connection, and our results show that models recognized for their strong reasoning capabilities, such as ol-preview and o1-mini, performed well. This suggests a qualitative link between reasoning ability and the capacity to follow instructions. However, the experiments also revealed that even tasks that may seem straightforward or merely tedious to humans-tasks that appear self-evident\u2014are not consistently solved by the models. On the other hand, these models demonstrate strong reasoning capabilities in domains like law or physics (OpenAI, 2023; Google, 2024). Since the effective application of knowledge can reduce the number of computational steps, it suggests that state-of-the-art LLMs may be more adept at leveraging knowledge to solve complex problems rather than excelling at multi-step procedural reasoning itself. This underscores a fundamental challenge in the current deep learning paradigm, where many models struggle with intricate reasoning tasks unless they can heavily rely on prior knowledge.\nMinimal Implicit Knowledge. While the ideal goal is to eliminate all implicit knowledge requirements, some minimal assumptions are inevitable. For example, we assume a basic understanding of the English language, the order of the alphabet, and that numbers such as 0, 1, 2, and so on represent numerical values. However, these assumptions are deliberately kept minimal and are significantly less specialized compared to the knowledge required for tasks in fields like physics, chemistry, law, or mathematics. By focusing on such foundational concepts, the dataset preserves a structured challenge that emphasizes reasoning and procedural execution rather than relying on domain-specific knowledge.\nExpected Use Cases and Limitations. The simplest and most straightforward use of our dataset is for evaluating LLMs, particularly with respect to their reasoning capabilities. This is the primary intended application, allowing researchers to assess how well a new model handles multi-step reasoning tasks.\nAdditionally, ProcBench can be used to evaluate variations of methods such as In-Context Learning or Chain-of-Thought reasoning (Wei et al., 2023). However, we do not envision the use of task-specific prompts for each of the 23 distinct tasks in the dataset, as this would introduce domain-specific knowledge. Such prompts might enable the model to skip significant portions of the actual reasoning process, which would defeat the purpose of evaluating its raw reasoning capabilities. An extreme example of this would be programming-based solutions, where directly introducing task-specific solvers should be avoided. If a general-purpose model, such as one with coding capabilities, can solve tasks without specific tuning, this reflects its versatility. However, in such cases, the intended measurement of multi-step Instruction followability would no longer be feasible within this dataset.\nAlthough the primary focus is on the current LLM paradigm, ProcBench is still applicable to traditional machine learning models, such as those used in inductive programming, which learn from concrete examples. However, the fixed dataset provided for evaluation is unlikely to be sufficient for training such models. In this case, the Generator could be utilized to augment the dataset, enabling a model to be constructed from scratch with the goal of following procedural instructions."}, {"title": "6 CONCLUSION", "content": "We introduced ProcBench, a benchmark designed to assess LLMs on their ability to follow explicit, multi-step instructions. By concentrating on tasks that require minimal implicit knowledge, ProcBench allows us to evaluate the procedural reasoning capabilities of models independent of their reliance on domain-specific knowledge. Our results show that while state-of-the-art models like ol-preview and o1-mini perform well on tasks involving shorter steps, they face significant difficulties as the step length increases. This highlights a critical limitation in current LLMs: despite excelling in knowledge-driven tasks, they struggle to consistently follow detailed procedural instructions when faced with more complex, multi-step reasoning.\nOur findings emphasize the distinction between knowledge-based reasoning and instruction following, an area where LLMs have yet to achieve consistent mastery. Enhancing the ability of these models to precisely follow instructions will be key to improving their performance in more complex problem-solving scenarios. Future work will expand ProcBench to encompass a broader range of tasks and further investigate how explicit instruction-following capabilities can be more effectively integrated into models trained on traditional benchmarks. This will contribute to developing systems that can reliably handle multi-step reasoning across diverse domains."}, {"title": "REPRODUCIBILITY", "content": "Details regarding the construction of the benchmark and the models used in the experiments can be found in Section 3, Section 4.1 and Table 3. The dataset we created, the code used to generate it, the prediction results, and the evaluation results will be made publicly available after the paper is accepted for publication."}]}