{"title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses", "authors": ["Jing Yao", "Xiaoyuan Yi", "Xing Xie"], "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing LLMs' values helps expose their misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or close-source ones like GPT-4, to identify values reflected in generated responses. Nevertheless, these evaluators face two challenges in open-ended value evaluation: they should align with changing human value definitions with minimal annotation, against their own bias (adaptability), and detect varying value expressions and scenarios robustly (generalizability). To handle these challenges, we introduce CLAVE, a novel framework which integrates two complementary LLMs, a large one to extract high-level value concepts from a few human labels, leveraging its extensive knowledge and generalizability, and a smaller one fine-tuned on such concepts to better align with human value understanding. This dual-model approach enables calibration with any value systems using <100 human-labeled samples per value. Then we present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses. Our findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation.", "sections": [{"title": "1 Introduction", "content": "The past years have witnessed unprecedented breakthroughs of Large Language Models (LLMs) [1, 2, 3, 4], leading a new wave of AI technology [5]. Despite such progress, these powerful models also pose potential risks [6, 7], such as generating socially biased [8, 9], toxic [10, 11] and illegal content [12, 13]. To ensure their responsible development, it is imperative to assess LLMs' potential risks [14]. Nevertheless, existing benchmarks customized for each specific risk gradually become inadequate [15, 16] because of the increasing risk types [17, 18]. Given the correlations between LLMs' values and harmful behaviors [19], assessing these values offers a comprehensive insight into their potential misalignment [20, 21], through moral judgement [22, 23, 24, 25], value questionnaire [26, 27] or generative value evaluation [28, 29, 30]. This work focuses on generative value evaluation, which deciphers LLMs' values directly from their responses generated in provocative scenarios, as it can better measure LLMs' true value conformity rather than knowledge of values [31].\nHowever, this open-ended value evaluation paradigm heavily relies on reference-free value evaluators [32], due to the lack of ground truth responses. LLMs equipped with massive knowledge and strong capabilities [2, 33] are promising to serve as such evaluators, which have been successfully applied to various Natural Language Generation (NLG) tasks [34, 35, 36]. Existing relevant research falls into two categories: 1) prompt-based evaluator, which adopts strong LLMs as off-the-shelf evaluators to assess text through meticulous prompt designing [37, 38], benefiting from their remarkable"}, {"title": "2 Related Work", "content": "Evaluating LLMs' Values To expose LLMs potential misalignment, a series of benchmarks have been curated to assess their risks, ethics and values, differing in collection method, complexity, formats and value systems. Most existing ones focus on specific safety issues, ranging from social"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem Definition", "content": "In this paper, we concentrate on the task of automatically identifying the values reflected in responses generated by LLMs under a given context or scenario. There exist diverse value systems to character LLMs values, such as comprehensive social risks [25, 47] and Schwartz's Theory of Basic Values [54]. Each of them contains a number of value dimensions or categories, defined in different ways to represent distinct value aspects. To obtain a comprehensive view, we need to ascertain how each value is reflected in the generated response. The labels are categorized into three types: adhere to, oppose to and unrelated to (i.e., the response show no evidence towards this value). In some value systems, adhere to and unrelated to can be uniformed as a single category, i.e., not violate to. To"}, {"title": "3.2 The CLAVE Framework", "content": "Automatic value assessment with LLM-based evaluators faces two key challenges: adaptability and generalizability, as discussed in Sec. 1. To handle these two challenges, this paper introduces CLAVE, where a large but close-source LLM with rich knowledge and robust text comprehension capabilities deals with variable textual expressions and scenarios, while a smaller LLM aligns with human perspectives by efficient fine-tuning on manually annotated samples. We bridge the two complementary models using fundamental value concepts, which refer to key behaviors or implications that can act as highly generalized indicators of certain values, e.g. 'advocating for personal choice and autonomy in life-affecting decisions' for the value of 'self-direction'. By recognizing these value concepts from texts, value assessment would be more robust and less interfered with by extraneous textual information. The whole architecture is depicted in Fig. 2. For each given sample (I, v, si, ri), the workflow consists of three main steps.\nStep 1. Value Concept Extraction. With the value definition v, scenario si and response ri, we construct a new prompt as Tc (Ik, v, Si, ri) and instruct a large but close-source LLM, which works as a concept extractor, to extract value concepts Ci for this sample. To ensure the quality and generalizability of extracted concepts, the instruction Ik indicates three requirements. 1) Essential, the concept should be fundamental features for value assessment, rather than extraneous textual details. 2) Generic, the concept should not tied to the current scenario, but be more general to describe a class of similar cases. 3) Each concept should involve only one characteristic for value evaluation. If a sample contains several value perspectives, we split them into several concepts.\nStep 2. Value Concept Mapping. This is a critical step to guarantee the accuracy of our generalizable framework. Given a few manually annotated samples for fine-tuning the smaller model, which serves as a value recognizer, to align value understanding with humans, we extract value concepts from these samples to build a concept pool O = C1, C2, ... During the training process, the recognizer learns value reasoning patterns based on these concepts. Considering the limited generalization of the smaller model, we expect to apply the same concepts for inference. Therefore, we attempt to map the concepts for any testing sample to the most relevant one in the pool. For each concept c, we obtain its embedding with the OpenAI Embedding API and compute similarity sim with concepts in the pool. With a threshold 0, the concept c is mapped to a seen concept in the pool when their similarity sim > 0, otherwise, we maintain the extracted one for inference. Since we require the features extracted in the last step to be essential and generic, it can enhance the coverage of the concept pool."}, {"title": "3.3 Concept Pool Construction", "content": "We build the concept pool on a set of manually annotated training samples X = (x1, x2,...), each comprising a scenario si, a response ri, a value definition v and the ground truth label li. Given a sample, the large extractor fully understands the evaluation criteria based on the label and captures the value concepts that impact the decision. As discussed in the previous section, these extracted value concepts should be both essential and generic to represent a broader class of situations rather than just one specific scenario. To assist the large model in discovering more essential and general concepts, we employ a clustering strategy. We first compute the textual embedding for each training sample using OpenAI Embedding API and then cluster all samples into groups with the K-Means algorithm. We take K samples from a cluster and present them to the large LLM simultaneously for extraction, expecting to obtain more generalized value concepts.\nDue to the randomness in LLM-generated texts, the initial extraction process may yield multiple concepts with highly close meanings but not the same textual expression, which are produced from different batches in the above step. This variability can introduce noises in the textual expressions and complicate the alignment process for the smaller model. To enhance stability and efficiency in this alignment, we further deduplicate the extracted value concepts and enhance the representativeness. We perform a hierarchical clustering procedure [77] on all extracted concepts to merge concepts with high textual similarity from the bottom to up. Once the clustering is complete, we compute the average distance of each concept to others within its cluster and retain the most representative concept for each cluster."}, {"title": "4 Benchmark", "content": "To standardize the value evaluation, we present a comprehensive benchmark ValEval.\nData Composition ValEval benchmark comprises 13k+ manually annotated (text, value, label) tuples, where three value systems are involved and the label can be adhere to, oppose to, not related to}. To rigorously measure the accuracy, generalization and robustness of value assessment, we include three different subsets for each value system as follows. 1) Original: this is the primary split, including both the training data for tuning-based evaluators and a testing set collected from the same distribution. 2) Perturbation: this subset contains perturbed version original testing samples to evaluate robustness against varying value expressions. Two types of perturbations that could induce model uncertainty are incorporated. One is textual modifications that do not alter the value, and the other is minimal textual changes that flip the value label. Since we benchmark GPT-4 in this paper, we use the Mistral-Large API to generate the perturbation texts and thus avoid possible errors in evaluation. 3) Generalization: we further introduce a distinct dataset for each value system to verify the generalization across different scenarios. Specifically, the data source and construction method for each value system are elaborated as follows.\nSocial Risk Categories. This is the most popular perspective in measuring the value of LLMs. BeaverTails [47] is a corresponding benchmark comprising QA pairs of adversarial questions and responses from the Alpaca-7B model. Each QA-pair is annotated with the meta-label to 14 risk categories, such as hate speech and financial crime. The primary split and perturbation split are built on this dataset. About the generalization split, we select Do-not-Answer [32], a question dataset curated on a three-level risk taxonomy for safeguard evaluation. It releases the responses of various LLMs to these questions and human labels for safety. We filter questions of those highly relevant risk categories and map them to the categories of BeaverTails according to the risk definition."}, {"title": "5 Experiments and Analysis", "content": ""}, {"title": "5.1 Experimental Settings", "content": "We benchmark the capabilities of 12+ popular LLM evaluators on our collections to analyze their strengths and weaknesses, categorized into the following groups.\n(1) Prompt-based Evaluators. Basically, we design a vanilla prompt to provide LLM APIs with an official value definition, the sample to be evaluated, the instruction and the output format. Furthermore, we incorporate more advanced prompts Few-Shot [61], Chain-of-thought (CoT) [63] and G-Eval [37]. Several ensemble-based approaches that benefit from multiple LLMs or repeat runs, i.e FairEval [60], WideDeep [78], and ChatEval [65]. Besides, there are advanced LLM evaluators that align with humans through in-context learning, such as AutoCalibrate [38] and ALLURE [67].\n(2) Tuning-based Evaluators. We fine-tune available LLMs of various sizes, including GPT-2-Large [79] (774M), Phi-3 [80] (3.8B), Mistral-7B [4] (7B) and Llama-2-7b-chat [1] (13B).\nCLAVE is our proposed evaluation framework that integrates large LLMs and smaller ones. To better compare the effects of different LLM evaluators, we also provide the ensembled results of crowd workers as a reference. The evaluation metric of accuracy is reported. More details about experimental settings and implementations can be found in Appendix C."}, {"title": "5.2 Overall Performance on Value Assessment", "content": "The whole evaluation results of 12+ LLM-based evaluators, our CLAVE framework and crowd workers on the curated ValEval dataset are detailed in Table 2.\nFrom the results, we obtain three main findings: 1) Prompt-based evaluation with large LLMs indeed performs well on popular social risks, with considerable robustness and generalizability, maintaining"}, {"title": "5.3 Analysis of Training Data Amount", "content": "Given the limited availability of annotated value data and high cost of expertise annotations, especially less popular theories, we conduct a comparative analysis of our CLAVE method against a tuning-based baseline with varying amounts of training samples. As stated in Sec. 4, our training set contains 100 samples for each label of each value. Thus, we experiment with 10, 20, 50, 100 samples respectively. The results are displayed in Figure 3."}, {"title": "5.4 Analysis of Different Components", "content": "CLAVE is a framework integrating a large LLM with a flexible, fine-tunable smaller model. We conduct experiments to analyze CLAVE's adaptability across different large and small models. We select widely used large models with notable capability differences, i.e. ChatGPT and GPT-4, along with diverse smaller models of different sizes and origins, Phi-3, Llama-2-7b and Mistral-7b. The results of different combinations are displayed in Figure 4.\nComparing the performance of CLAVE paired with ChatGPT and GPT versus GPT-4, we observed that despite ChatGPT's significantly lower performance in value assessment tasks relative to GPT-4, ChatGPT still notably enhances the smaller models' results in the perturbation and generalization splits. This improvement can be attributed to that our framework leverages the extensive knowledge and text understanding capabilities of the large models rather than their precise alignment of diverse value theories. ChatGPT already exhibits a strong ability to extract value concepts, providing a cost-effective option for users with limited GPT-4 API budgets. Comparing different smaller models, our framework consistently enhances their performance, particularly in scenarios requiring generaliz-ability. This improvement is more obvious on smaller models with worse inherent capabilities, such as Llama-2-7b v.s. Mistral-7b. This suggests that aligning with human value perspective through"}, {"title": "5.5 Case Study", "content": "To illustrate the challenges of adaptability and generalizability in the value evaluation task and validate the advantages of our CLAVE framework that incorporates value concepts, we conduct several case studies. The results are depicted in Figure 5.\nFrom case 1, we observe that while GPT-4 accurately assesses the value of a specific social risk embeded in the given scenario, it makes errors on the same scenario when evaluating the less popular Schwartz value dimension. This indicates a deficiency in the LLM's understanding of less popular value theories, underscoring the necessity of alignment with human perspectives. Case 2 highlights the vulnerability of smaller models to textual perturbations. For the same scenario, slightly modifications to the text led to erroneous judgments by the Llama model. In contrast, value concepts demonstrate robustness against such textual changes, as it captures essetial behaviors related to values which could remain constant despite minor textual variations. We find the value concepts across the two examples are the same, thus value assessment based on value concepts would be more stable. In case 3, we compare Llama2 and CLAVE in handling generalized scenarios, where value concepts exhibit strong scenario generalization. When extracting value concepts, we require them to be generic and not be tied to specific scenarios, promoting generalizability."}, {"title": "6 Conclusion", "content": "In this study, we concentrate on the two challenges of using LLMs for reference-free value evaluation: adaptability to diverse value systems and generalizability to varying expressions. We introduce CLAVE, a novel framework that integrates complementary large proprietary models and small tuning-based ones. Value concepts are proposed to link the two modules, where large models leverages their incredible knowledge and capability to extract concepts from diverse scenarios and smaller models are fine-tuned on these concepts for alignment. Furthermore, we present ValEval, a comprehensive benchmark for value evaluation of LLM generated texts, including three value systems. Our empirical experiments on this benchmark illustrate the strengths and weaknesses of various LLM-based evaluators. The results reveal that CLAVE achieves a superior balance between accuracy and generalizability across diverse value systems. This paper validates the superiority of value concepts for enhancing accuracy and generalizability, yet, they can also contribute to transparency that is crucial for value assessment. We will focus on exploring this property in the future."}, {"title": "A Supplement for Section 3 (Methodology)", "content": ""}, {"title": "A.1 Prompts in CLAVE", "content": "The prompt template for Step 1. Value Concept Extraction is presented in Figure 6. And that for value assessment is shown in Figure 7."}, {"title": "A.2 Algorithm for Concept Pool Construction", "content": "We build the concept pool on a set of manually annotated training samples X = (x1, x2,...), each comprising a scenario si, a response ri, a value definition v and the ground truth label li. We first compute the textual embedding e\u00bf for each training sample using OpenAI Embedding API and then cluster all samples into groups with the K-Means algorithm. We take k samples from a cluster K; and present them to the large LLM simultaneously for extraction, expecting to obtain more generalized value concepts. To deduplicate the extracted value concepts and enhance their representativeness,\nWe perform a hierarchical clustering procedure [77] on all extracted concepts to merge concepts\nwith high textual similarity from the bottom to up. Once the clustering is complete, we compute\nthe average distance of each concept to others within its cluster and retain the most representative\nconcept for each cluster. The whole procedure is encapsulated in Algorithm 1.\nAlgorithm 1 Concept Pool Construction\n1: Input: Training samples X = {X1,X2, .}, where xi = (Si, ri, v, li)\n2: Output: Concept pool O\n3: E\u2190 Compute Texutal Embed(X)\n4: K\u2190 Kmeans(E)\n5: for each cluster Kj \u2208 K do\n6:  Sk\u2190 Select(Kj, k)\n7:  Ck \u2190 Extract Value Concept(Sk)\n8: end for\n9: Otmp \u2190 \u222aS\u0141\u2208K Ck\n10: Eo \u2190 Compute Texutal Embed(Otmp)\n11: Ko \u2190 Hierarchical Clustering(Eo)\n12: \u039f \u2190 {}\n13: for each Ko, \u2208 Ko do\n14:  O \u2190 O\u222a {Representative(Ko\u2081)}\n15: end for\n16: return Concept Pool O"}, {"title": "A.3 Framework Optimization", "content": "For each training sample (v, si, ri, li), we obtain their extracted and mapped value concepts Ci during the above pool construction process. Formulating them into prompts in Figure 7, we train smaller decoder-based LLM using LoRA [81]. The negative-log-likelihood loss is computed for optimization."}, {"title": "B Supplement for Section 4 (Benchmark)", "content": ""}, {"title": "B.1 Details about Data Composition", "content": "In this section, we elaborate on the value dimensions of each system and present several examples of the ValEval benchmark.\nSocial Risk Categories. As for this value system, we employ the 14 classes of social risks contained in Beavertails [47], enumerated as follows.\n\u2022 Hate Speech, Offensive Language Sexually Explicit, Adult Content\n\u2022 Discrimination, Stereotype, Injustice Controversial Topics, Politics\n\u2022 Violence, Aiding and Abetting, Incitement Misinformation Re. ethics, laws and safety\n\u2022 Financial Crime, Property Crime, Theft Terrorism, Organized Crime\n\u2022 Privacy Violation Self-Harm\n\u2022 Drug Abuse, Weapons, Banned Substance Animal Abuse\n\u2022 Non-Violent Unethical Behavior Child Abuse"}, {"title": "B.3 Details about Manual Annotation", "content": "Since this annotation task requires an in-depth understanding of various value theories, we recruited annotators with degrees in psychology or related social science fields. Moreover, we ask them to fully understand the value definition based on their background knowledge and other resources such as papers, webpages and textbooks. This condition helps to ensure the annotation quality. We recruited all these annotators from a vendor, with consent for their annotations. There might be offensive language in the annotation task, which has been clarified to these annotators in advance.\nDuring the labeling process, each annotator is presented with samples composed of (scenario, response, value, candidate labels), where candidate labels include adhere to this value, oppose to this value, and not related to this value. Then, they select one label to complete the annotation task. The screenshot of the labeling task is shown in Figure 8. We ask three people to annotate each sample and ensemble their annotations to get the final labels through majority voting. Their average agreement across the above three datasets is about 87.7%, 85.0% and 72.6% respectively. This is higher than that reported in ValueNet [82]\nAbout the compensation, each annotator is paid $7.5 per hour, significantly exceeding the minimum wage per hour in that region. In addition, this annotation project has undergone a thorough review and has been approved by the Institutional Review Board (IRB)."}, {"title": "C Supplement for Section 5 (Experiment)", "content": ""}, {"title": "C.1 Baseline Implementations", "content": "We benchmark the capabilities of 12+ popular LLM evaluators on our collections to analyze their strengths and weaknesses, categorized into prompt-based and tuning-based evaluators. Their imple-mentation details are listed as follows.\nVanilla Prompt: We provide the official definition of the value, the description of the scenario to be evaluated, and the instruction and output format in the prompt for the LLM API.\nFew-Shot [61]: In addition to the basic components in the vanilla prompt, we append six random examples of the same value category to stimulate in-context learning.\nChain-of-thought [63]: We explicitly incorporate the Chain-of-Thought instruction into the prompt, which guides the LLM to first fully understand the action in the scenario, and then make the final decision by referring to the given value definition\nG-Eval [37]: It utilizes Chain-of-Thought (CoT) for evaluation, which first feeds the task instruction and evaluation criteria into an LLM, and asks the LLM to generate a CoT of evaluation procedure.\nFairEval: This method is designed to address the position bias of LLMs, with several strategies. We apply the multiple evidence calibration (MEC) in our task, where we require the LLM to first generate evaluation evidence and then make the final decision. Several repeated evaluations are conducted for each sample, and we take majority voting as the result.\nChatEval [65]: Inspired by human labelers collaborating in their evaluation, ChatEval is proposed as a system where multiple agents employ varied communication strategies to discuss for the final judgment. We set three agents and adopt the one-by-one discussion strategy in our implementation.\nWideDeep [78]: Inspired by that a neural network usually has many neurons and different neurons are responsible for evaluating different concepts, this paper explores a deeper and wider LLM network for LLM evaluation. In the first layer, it introduces several LLMs, each responsible for detecting one aspect. In subsequent layers, review information in the previous layers is considered to obtain more comprehensive evaluation results. In our implementation, we consider two layers and each layer has three neurons.\nAutoCalibrate [38]: This is a data-driven method proposed to calibrate scoring criteria of aspects like text coherence and fluency through in-context learning. It takes a 3-stage procedure: criteria drafting based on given expert examples, criteria revisiting by providing strongly disagreed samples and finally criteria application. We adapt it to our task to calibrate the value definition with manually annotated samples. As for parameters, the temperature is always set as 1.0, in-context sample sizes are 4,6,8, with 3 Monte-Carlo Trails for all datasets.\nALLURE [67]: This method leverages in-context learning to improve and enhance the evaluation ability of LLM. It compares the LLMs' generated labels with the ground truth and iteratively incorporates those deviated samples for enhancement. The number of error samples incorporated as reinforcement is set as 6.\nFor GPT-2 [79], Phi-3 [80], Llama-2-7b-chat [1] and Mistral-7b [4] that require to be fine-tuned, we download their checkpoints from the huggingface website and fine-tune them using LoRA [81]. The training batch size is set as 8, learning rate is le 5, and dtype is bf16. All experiments are completed with a single NVIDIA-A100."}, {"title": "C.2 Implementation Details", "content": "For our Clave method, the value extraction process is completed with GPT-4-1106 API. When constructing the concept pool, we cluster all training samples and feed 4 cases for concept extraction at once. The similarity threshold @ in value concept mapping is set as 0.7. With regard to the optimization process, we employ the same setting as tuning-based baselines. The training batch size is set as 8, learning rate is le 5, and dtype is bf16. All experiments are completed with a single NVIDIA-A100."}, {"title": "C.3 Instruction for Crowdworkers", "content": "In order to include manual annotation results as a baseline, we recruit three crowd workers through the vendor. The annotation guideline and task interface are the same as described in Sec. B.3."}, {"title": "C.4 Experiments on Mapping Threshold", "content": ""}, {"title": "C.5 Analysis of Concept Similarity", "content": "To gain a deeper view of why our Clave framework exhibits better robustness and generalization compared to other tuning-based methods, we analyze the similarity between text distributions and concept distributions across different testing splits. We calculate cosine similarity between their tf-idf vectors, and the results are displayed in Table 3.\nObserving the results, we find that the similarity of text distributions is significantly lower than that of concept distributions, especially on the perturbation and generalization splits. Whereas, our approach avoids reliance on the varied texts but extracts more essential and generic value concepts, thus achieving improved performance in terms of both robustness and generalization. This enhancement can be attributed to the extensive knowledge and powerful text understanding capabilities of the large LLM component in our framework."}, {"title": "C.6 Experiments on Training Data Diversity", "content": "We conduct an experiment to study the impact of training data diversity on the performance of CLAVE framework. We employ three different strategies to sample 10 data points per label for each value from the whole training set, including random sampling, text diversity sampling and concept diversity sampling. During diversity sampling, we calculate the similarity of a new sample to all selected samples and discard those with similarity exceeding a threshold. The experiment results of CLAVE (Llama) with different sample sets are presented in Figure 9.\nFirst, diverse training data can introduce richer information even with a limited number of samples, yielding significantly better results than random sampling. Moreover, the training subset with diverse concepts leads to superior performance. We infer this is due to that more diverse concepts fundamentally cover more patterns, which also demonstrates the superiority of value concepts."}, {"title": "C.7 Case Study", "content": "As a complement to Sec. 5.5, we provide more case studies in Table 4 and Table 5."}, {"title": "D Ethical Statement", "content": "This paper concentrates on the automatic evaluation of values in LLM-generated texts. To facilitate the analysis and measurement, we curate a comprehensive benchmark ValEval, comprising three classical value systems. By identifying the values reflected in LLM-generated texts, we can uncover their potential harms and align them with human values to promote responsible development. However, we acknowledge potential risks with our work: the constructed dataset includes responses that contain harmful information and are deviated from human values. Such data could be utilized to train LLMs for harmful or malicious purposes. To mitigate this risk, we explicitly refrain from providing any guidance for negative applications and advocate for responsible and ethical usage."}, {"title": "E Limitation and Future Work", "content": "Though great effects of value concepts for LLM-based value assessment have been verified, there are still several limitations and future research directions of this paper. We discuss them as follows.\n(1) Transparency. In this paper, we integrate two complementary LLMs by proposing value concepts to enhance the performance of value assessment. Extensive experiments have validated the efficacy of this framework. Furthermore, value concepts allow us to uncover the rationale behind LLM's decision-making on value evaluation, thus they can also enhance the transparency and interpretability. This property is crucial for value that related to potential risks of LLMs. In future research, we could explore the impact and advantages of value concepts on transparency.\n(2) More variants of models. The proposed framework includes one large LLM and a smaller one. There is a wide range of options available for both types of models, each of which has distinct characteristics, capabilities, and sizes. This paper has initially analyzed the influence of different large and small models as components of the framework in Sec. 5. Furthermore, this analysis can be extended to be a more comprehensive combination of models, providing more in-depth insights.\n(3) Multilingual analysis. The datasets curated in this paper is primarily in English, and the covered value issues may predominantly pertain to English-speaking regions. However, values are distinct across cultures and countries. Since the selected value systems are recognized across cultures, we could consider conducting more multilingual value analyses."}]}