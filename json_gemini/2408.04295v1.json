{"title": "Assigning Credit with Partial Reward Decoupling\nin Multi-Agent Proximal Policy Optimization", "authors": ["Aditya Kapoor", "Benjamin Freed", "Howie Choset", "Jeff Schneider"], "abstract": "Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem, wherein\nthe sheer difficulty in ascribing credit to individual agents' actions scales poorly with\nteam size. In this paper, we propose a multi-agent reinforcement learning algorithm\nthat adapts recent developments in credit assignment to improve upon MAPPO.\nOur approach leverages partial reward decoupling (PRD), which uses a learned\nattention mechanism to estimate which of a particular agent's teammates are rele-\nvant to its learning updates. We use this estimate to dynamically decompose large\ngroups of agents into smaller, more manageable subgroups. We empirically demon-\nstrate that our approach, PRD-MAPPO, decouples agents from teammates that do\nnot influence their expected future reward, thereby streamlining credit assignment.\nWe additionally show that PRD-MAPPO yields significantly higher data efficiency\nand asymptotic performance compared to both MAPPO and other state-of-the-art\nmethods across several multi-agent tasks, including StarCraft II. Finally, we pro-\npose a version of PRD-MAPPO that is applicable to shared reward settings, where\nPRD was previously not applicable, and empirically show that this also leads to\nperformance improvements over MAPPO.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) has achieved super-human performance on many com-\nplex sequential decision-making problems, such as DOTA 2 (Berner et al., 2019), StarCraft II\n(Vinyals et al., 2019), and capture the flag (Jaderberg et al., 2019). These impressive results,\nhowever, come at an immense cost: often, they require millions, if not billions, of time-consuming\nenvironmental interactions, and therefore can only be run on high-cost compute clusters.\nThe credit assignment problem contributes to the computational difficulties that plague large-scale\nMARL algorithms; as the number of agents involved in learning increases, so too does the difficulty of\nassessing any individual agent's contribution to overall group success (Minsky, 1961; Sutton et al.,\n1998). While credit assignment already challenges reinforcement learning (RL), it is particularly\nprominent in large-scale cooperative MARL, because, unlike problems in which each agent can act\ngreedily to optimize its own reward, all agents must maximize the total reward earned by the entire\ngroup. Therefore, agents must not only consider how their actions influence their own rewards, but\nalso the rewards of every other agent in the group."}, {"title": "2 Background", "content": "Here we describe our problem formulation as a Markov game. Subsequently, we investigate mathe-\nmatically how imperfect credit assignment manifests itself in high policy-gradient variance in policy-\ngradient RL algorithms. Finally, we review PPO and PRD."}, {"title": "2.1 Markov Games", "content": "We consider multi-agent sequential decision-making problems that can be modeled as a Markov\ngame. A Markov game is specified by $(S, A, P, R, \\rho_0, \\gamma)$, where $S$ is the state space, $A$ is the joint\naction space, consisting of every possible combination of individual agents' actions, $P(S_{t+1}|S_t, a_t)$\nspecifies the state transition probability distribution, $R(r_t|s_t, a_t)$ specifies the reward distribution,\n$\\rho_0(s_0)$ denotes the initial state distribution, and $\\gamma \\in (0,1]$ denotes a discount factor (Littman,"}, {"title": "2.2 Credit Assignment and Policy Gradient Variance", "content": "To understand the effects of scaling PPO to large numbers of agents, and how we expect PRD will\nimprove this scaling, we explore how imperfect credit assignment causes difficulties in learning. In\nthis paper, we argue that in policy-gradient algorithms (which includes many popular algorithms\nsuch as PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015a), D4PG (Barth-Maron et al.,\n2018), MADDPG (Lowe et al., 2017), and A3C (Mnih et al., 2016)), the credit assignment problem\nmanifests itself in the form of high variance of advantage estimates. High variance in advantage\nestimates in turn causes policy gradient estimates to be more noisy, resulting in slower learning.\nWe consider an actor-critic-style gradient estimate for a single-agent system in its most stripped-\ndown possible form, computed using a single state-action sample:\n$\\nabla_{\\theta} J(\\theta, s, a) = \\nabla_{\\theta} \\log \\pi(a|s) \\hat{A}(s, a)$,\nwhere state $s$ is sampled from the state-visitation distribution induced by policy $\\pi$, action $a$ is\nsampled from $\\pi$ conditioned on $s$, and $\\hat{A}(s, a)$ is a stochastic advantage estimate, which estimates\nthe true advantage of taking action $a_t$ in state $s_t$, and following policy $\\pi$. The advantage function\nis typically defined as $A^{\\pi} (s, a) = Q^{\\pi} (s,a) - V^{\\pi}(s)$, where $Q^{\\pi}(s,a)$ and $V^{\\pi}(s)$ are the state-action\nvalue function and state-value function, respectively (Sutton et al., 1998). Intuitively, the advantage\nfunction measures how much better it is to select a particular action $a$ than a random action from the\npolicy, while in state $s$. There are many ways to compute $\\hat{A}$, generally all involving some error, as the\ntrue value functions are unknown (Sutton et al., 1998; Schulman et al., 2015b). If perfect advantage\nestimation were possible, then so too would be perfect credit assignment, as the advantage function\ndirectly measures how a particular action $a$ impacted the total reward obtained by the group.\nTo gain an understanding of how the gradient variance is impacted by advantage estimator variance,\nwe note that the conditional variance of $\\nabla_{\\theta} J$, given $s$ and $a$, is proportional to the variance of $\\hat{A}$:\n$\\text{Var}(\\nabla_{\\theta} J|s, a) = (\\nabla_{\\theta} \\log \\pi(a|s))^2 \\text{Var}(\\hat{A}|s, a)$.\nMoving to a cooperative multi-agent setting, $\\hat{A}(s, a)$ is replaced by a summation over individual\nagents' advantages in the gradient estimate for a particular agent $i$:"}, {"title": "2.3 Proximal Policy Optimization", "content": "Earlier policy gradient algorithms, such as actor-critic (AC), suffered from poor data efficiency in\npart because they were purely on-policy, and therefore required a fresh batch of environmental data\nto be collected each time a single gradient update was applied to the policy (Konda & Tsitsiklis, 2000;\nSchulman et al., 2015a; 2017). PPO provides higher data efficiency than AC by enabling multiple\npolicy updates to be performed given a single batch of on-policy data, resulting in larger policy im-\nprovements for a fixed amount of data. Given a batch of data, PPO optimizes the policy with respect\nto a \"surrogate\" objective that penalizes excessively large changes from the old policy, permitting\nthe agent to perform multiple gradient updates without becoming overly off-policy. Specifically,\nduring each policy optimization step, PPO optimizes the following objective with respect to policy\nparameters $\\theta$,\n$L_{PPO}(\\theta) = \\mathbb{E} \\left[ \\text{min} \\left(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]$,\nwhere $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio, $\\pi_{\\theta_{old}}$ is the data collection policy, $\\pi_{\\theta}$ is the updated\npolicy, $\\hat{A}_t$ is the stochastic advantage estimate for time $t$, and $\\mathbb{E}[\\cdot]$ denotes an empirical average over\na finite batch of samples (Schulman et al., 2017)."}, {"title": "2.4 Partial Reward Decoupling", "content": "PRD is an approach that enables large multi-agent problems to be dynamically decomposed into\nsmaller subgroups such that cooperation among subgroups yields a fully cooperative group-level so-\nlution. In practice, PRD was shown to improve the performance of an AC-style approach, compared\nto a vanilla AC algorithm. The proposed PRD-AC algorithm was also shown to outperform COMA,\na popular method for improved multi-agent credit assignment.\nPRD makes use of the fact that, considering two agents $i$ and $j$ at a particular timestep $t$, if the\naction of agent $i$ does not influence the expected future reward of agent $j$, then agent $i$ need not take\nagent $j$'s rewards into account when computing its advantage estimate for time $t$, thus streamlining\ncredit assignment. The set of agents whose expected future rewards are impacted by the action\nof agent $i$ at time $t$ is referred to as the relevant set of agent $i$ at time $t$, denoted $R_i(s_t,a_t)$. In\nFreed et al. (2022), a learned value function with an attention mechanism was used to estimate the\nrelevant set for each agent.\nThere were significant drawbacks to the approach presented by Freed et al. (2022), which we address\nin this paper. First, PRD was used in the context of the AC algorithm, which has been surpassed by\nalgorithms such as TRPO and PPO. Second, for a problem involving $M$ agents, PRD required $M$\nevaluations of the critic function to compute a single agent's gradient update; thus the computational\nburden for a learning update scaled quadratically with the number of agents. Finally, PRD assumed\nthat the environment provided per-agent reward streams (i.e., provided a scalar reward value to each\nagent at each timestep). However, many multi-agent problems provide only a single scalar reward\nfor the entire group at each timestep."}, {"title": "3 Improving Proximal Policy Optimization with Partial Reward\nDecoupling", "content": "In this paper, we tackle the credit assignment problem by developing PRD-MAPPO, which leverages\na PRD-style decomposition within a PPO learning update to improve credit assignment. More\nspecifically, PRD modifies the original PPO objective by eliminating advantage terms belonging to\n\u201cirrelevant\u201d agents. As shown by Freed et al. (2022), these irrelevant advantage terms contribute\nonly noise to learning updates, making learning less efficient. PRD uses an attention-based value\nfunction to identify when a particular agent's action did not influence another agent's future return,\nallowing those agents to be decoupled.\nTo leverage the improved credit assignment capabilities of PRD in PPO, we make two modifications\nto the standard PPO algorithm: first, we incorporate a learned critic with an attention mechanism.\nSimilar to Freed et al. (2022), the attention weights computed by the critic will be used to estimate\nthe relevant set of agents, as described in Sec. 3.1. Unlike Freed et al. (2022), we modify the critic\narchitecture to allow the relevant sets for each agent to be computed in linear, rather than quadratic\ntime. Second, we modify the surrogate objective of PPO to use the streamlined advantage estimation\nstrategy of PRD, which we describe in Sec. 3.2, using the relevant set estimated using the critic.\nIn this work, we test a novel \"soft\" relevant set estimation strategy that softly decouples agents,\nwhich we find significantly improves performance over a manual thresholding approach as was used\nby Freed et al. (2022)."}, {"title": "3.1 Learned Critics for Relevant Set and Advantage Estimation", "content": "Similar to Freed et al. (2022), we use a\nlearned critic function to perform relevant\nset estimation, albeit with significant mod-\nifications. In our approach, each agent $i$\nmaintains a graph neural network Q func-\ntion $Q^{\\pi}_i(s_t, a_t)$, which is trained to estimate\nits expected future individual returns given\nthe current state and actions of all agents.\nA diagram of our Q function is depicted in\nFig. 1. In practice, all agents share the\nsame Q function parameters. $Q$ takes as\ninput the state information and actions of\nall agents to estimate a scalar Q value for\neach agent $i$.\nThe Q function contains an attention\nmechanism that allows it to \"shut off\"\ndependence on particular agents' actions.\nMore concretely, the Q network for each\nagent $i$ uses the states of all agents (includ-\ning itself) to compute attention weights for\nall other agents (agents assign an attention\nweight of 1 to themselves, i.e., $W_{ii}(s_t) = 1$). These attention weights are then used as coefficients\nto compute a linear combination of attention values computed from agents' states and actions. If a\nparticular attention weight $w_{ij}$ is 0, then any information about agent $j$'s action will not be prop-\nagated further through the network, meaning that agent $j$'s action will not influence the final Q\nestimate for agent $i$. Once the aggregated value is computed, it is concatenated with an embedding\ncomputed from agent $i$'s state and action and passed through a recurrent output network (Fig. 1).\nIf the learned Q function of agent $i$ at a particular timestept computes an attention weight of\nexactly zero for another agent $j$ (i.e., $w_{ij} (s_t) = 0$), then $Q^i$ does not depend on $a^j$ given the state\nof all agents, and we can infer that agent $i$ is outside the relevant set of agent $j$. As shown by Freed\net al. (2022), agents outside the relevant set of agent $j$ do not, on average, contribute to its policy\ngradient, and may therefore be removed from the policy gradient estimates without introducing bias.\nIn practice, when inferring the relevant sets for each agent, we infer that $i \\notin R_j(s_t)$ if $W_{ij} (S_t) < \\epsilon$,\nwhere $\\epsilon > 0$ is a small manually chosen constant. Using this soft attention mechanism, agents cannot\nassign precisely zero attention weight to any other agent, and therefore cannot guarantee complete\nindependence of the Q function to any particular agent's action. However, we found that in practice,\nvery small attention weights were assigned to irrelevant agents, making this a practical method for\nrelevant set estimation. We explore variants of this decoupling procedure, including a \u201csoft\u201d variant\nthat softly re-weights agents' contributions to learning updates.\nOur approach to computing advantage terms for learning updates reduces the computational com-\nplexity over (Freed et al., 2022) from quadratic to linear in the number of agents $M$. To compute\nthe advantage terms required to update the policy of a particular agent $i$, the original algorithm\ndescribed by Freed et al. (2022) requires each agent $i$ to estimate the expected future return of each\nagent $j$, conditioned on the actions of all agents other than $i$, for each $j \\in R_i(s_t)$. This computation\nrequires (at worst) $M$ calls to the critic for each of the $M$ agents, resulting $M^2$ total calls during\neach learning update. Our approach, on the other hand, circumvents with quadratic scaling by\nmaintaining two separate critics; the first is the Q function used for relevant set estimation, de-\nscribed above. The second critic is used solely to provide baseline estimates for advantage function\nestimation (Schulman et al., 2015b; Konda & Tsitsiklis, 2000). It estimates the sum of expected\nfuture returns for all agents within agent $i$'s relevant set, conditioned on the state of all agents,"}, {"title": "3.2 PRD-MAPPO Parameter Update Rule", "content": "We modify the original MAPPO (Yu et al., 2021) objective for each agent $i$ by eliminating the\nrewards from agents that are outside its relevant set from its advantage estimates. The original\nMAPPO algorithm optimizes the following objective during each policy parameter update for agent\n$i$:\n$\\mathcal{L}_{MAPPO}^{(i)} = \\mathbb{E} \\left[ \\text{min} \\left(r_t^{(i)}(\\theta) \\hat{A}_t, \\text{clip}(r_t^{(i)}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] $ (2)\nwhere $r_t^{(i)}$ is the ratio between the updated and old policy of agent $i$, and $\\hat{A}_t$ is the advantage\nestimate for timestep $t$. In (Yu et al., 2021), generalized advantage estiamtion was used to compute\n$\\hat{A}_t$, which combines group agent rewards and value function estimates according to\n$\\hat{A}_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + ... + (\\gamma \\lambda)^{T-t+1}\\delta_{T-1}$,\n$\\text{where } \\delta_t = \\sum_{j=1}^{M} (r_t^{(j)} + \\gamma V(S_{t+1}) - V(S_t)$.\nWe modify the objective in (2) by replacing advantage terms with individual agent advantage terms,\nwhich ignore the rewards of irrelevant agents. The objective for agent $i$ becomes\n$\\mathcal{L}_{PRD}^{(i)} = \\mathbb{E} \\left[ \\text{min} \\left(r_t^{(i)}(\\theta) \\hat{A}_{i,t}, \\text{clip}(r_t^{(i)}(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_{i,t} \\right) \\right]$\nwhere\n$\\hat{A}_{i,t} = \\delta_{i,t} + (\\gamma \\lambda)\\delta_{i,t+1} + ... + (\\gamma \\lambda)^{T-t+1}\\delta_{i,T-1}$,\n$\\text{with } \\delta_{i,t} = \\sum_{j\\in R_i(s_t)} (r_t^{(j)} + \\gamma V^{\\pi}(S_{t+1}, a_t^{\\ne i}) - V(s_t, a_t^{\\ne i})$.\nNote in the above equation that the reward terms for agents not in $R_i(s_t)$ have been removed, and\n$V$ has been replaced by the value function $V^{'}$ described in Sec. 3.1., which is regressed against\nthe sum of returns of agents in $R_i(s_t)$. Pseudocode for PRD-MAPPO is included in Sec. B of the\nappendix.\nWe additionally propose a \"soft\" variant of PRD-MAPPO, which we refer to as PRD-MAPPO-\nsoft, that softly reweights agent rewards according to attention weights of the Q network, i.e.,\n$\\delta_{i,t} = (\\sum_{j=1}^{M} w_{ji}(S_t)r_t^{(i)}) + V(S_{t+1}, a_{t+1}^{\\ne i}) - V(s_t, a_t)$. In this soft variant, $V$ is regressed\nagainst the weighted sum of agent returns, $\\sum_{j=1}^{M} w_{ji}(S_t)R_t^{(i)}$."}, {"title": "3.3 Partial Reward Decoupling for environments with shared rewards", "content": "One drawback to our PRD approach is that it assumes individual reward streams for each agent are\navailable, i.e., at each timestep, the environment provides a separate scalar reward for each agent.\nHowever, some multi-agent systems only provide a single scalar shared reward for the entire group\nat each timestep. To deal with the shared reward setting, we propose strategy for decomposing\nshared returns into individual agent returns, to which we can then apply PRD. We start by training\na shared Q function to predict the shared returns (i.e., the sum of future shared rewards). Here we\nuse a similar architecture as described in Sec. 3.1, with the one difference that our network has 1\noutput rather than M outputs. We denote the vector of attention weights assigned by all agents to\nthe action of agent $j$ as $W_{\\cdot j}$. There is one such vector for each timestep and each agent; we omit\nthe timestep subscripting for brevity. As a heuristic to measure the overall influence that each agent\n$j$ has on the future shared reward, we aggregate the attention weights for each agent $j$ by taking\nthe mean of $W_{\\cdot j}$, which we refer to as $\\overline{W}_j$. The individual returns for each agent $j$ at each timestep\nare then set proportionally to $\\overline{W}_j$, such that they sum to the original shared return. Subsequently,\nwe apply PRD-MAPPO to these individual returns as we would in the individual reward setting\ndescribed in Sec. 3.2. We refer to this approach as PRD-MAPPO-shared."}, {"title": "4 Experiments", "content": "We experimentally compare the performance of the following algorithms on several cooperative\nMARL environments:\nPRD-MAPPO (ours): MAPPO with PRD, as described in Sec. 3.1.\nPRD-MAPPO-soft (ours) : the soft variant of PRD-MAPPO as described in Sec. 3.1.\nPRD-MAPPO-shared (ours) : the soft variant of PRD-MAPPO in the shared reward setting,\nas described in Sec. 3.3.\nMAPPO: a multi-agent variant of PPO, proposed by Yu et al. (2021).\nHAPPO: a recent state-of-the-art algorithm proposed by Kuba et al. (2021) that extends trust re-\ngion learning to cooperative multi-agent reinforcement learning (MARL), enabling monotonic policy\nimprovement without the need for shared policy parameters.\nG2ANet-MAPPO: MAPPO with a G2ANet-style critic. This baseline attempts to import the\ncredit assignment benefits of G2ANet (which was originally used in the Actor-Critic algorithm) to\nthe more state-of-the-art MAPPO.\nCounterfactual Multi-Agent Policy Gradient (COMA): Proposed by Foerster et al. (2018),\nCOMA is a multi-agent actor-critic method. COMA addresses credit assignment by using a counter-\nfactual baseline that marginalizes out a single agent's action, while keeping the other agents' actions\nfixed, allowing COMA to better isolate each agent's contribution to group reward.\nPRD-V-MAPPO: PRD-MAPPO, using the value-function-based method of relevant set estima-\ntion, as described by Freed et al. (2022). This version uses a learned value function for both relevant\nset and advantage estimation, and scales quadratically in time complexity with number of agents.\nWe include this as a baseline to assess the effect of critic choice.\nLearning Implicit Credit Assignment (LICA): proposed by Zhou et al. (2020b), LICA is a\nmethod for implicit credit assignment that is closely related to value gradient methods, which seek\nto optimize policies in the direction of approximate value gradients. LICA extends the concept of\nvalue mixing present for credit assignment found in QMix and Value-decomposition Networks by\nintroducing an additional latent state representation into the policy gradients. The authors claim\nthat this additional state information provides sufficient information for learning optimal cooperative\nbehaviors without explicit credit assignment."}, {"title": "5 Results and Discussion", "content": "The reward curves for all tasks are shown in Fig. 2. We found that of the algorithms we tested, only\nPRD-MAPPO-soft, PRD-MAPPO-shared, and PRD-MAPPO performed consistently well across\nall environments, with PRD-MAPPO-soft tending to perform the best. PRD-MAPPO-soft was\noutperformed only in one environment (pressure plate) by one algorithm (QMix), and in general\noutperformed all other algorithms on all tasks."}, {"title": "5.1 Relevant Set Visualization", "content": "To gain more insight into the relevant set selection process, in Fig. 3 we visualized the attention\nweights inferred by a trained group of agents in the Collision Avoidance task. In this task, agents\nare rewarded for reaching an assigned goal location while avoiding collisions. Agents are divided into\nthree teams, consisting of agents 1-8, 9-16, and 17-24, and are only penalized for colliding with other\nagents on their team. We therefore expect agents to assign large attention weights only to other\nagents on their same team, because each agents' reward is independent of the actions of agents on\nother teams. Fig. 3 displays the average attention weights as an M x M grid, with the ith row and\njth column corresponding to the average attention weight that agent i assigns to agent j. Because\nagents always assign an attention weight of 1 to themselves, we remove these elements from the\nvisualization as they are uninformative. We find that, as expected, agents assign considerably non-\nzero attention weights only to other agents on their same team, while assigning near-zero attention\nweights to all other agents. Attention weights were averaged over 5000 independent episodes."}, {"title": "5.2 Policy Gradient Estimator Variance Analysis", "content": "To empirically verify the claim that partial reward\ndecoupling decreases the variance of MAPPO pol-\nicy gradient estimates, we estimate the variance of\nMAPPO and PRD-MAPPO at various points during\ntraining. For maximum comparability, we compute\nthe variance for both MAPPO and PRD-M\u0391\u03a1\u03a1\u039f\nusing data gathered from the same policy, taken at\n1000-episode intervals during the training of PRD-\nMAPPO. Using these policies, we collect 100 in-\ndependent batches of data, and differentiate the\nMAPPO or PRD-MAPPO surrogate objective eval-\nuated on each batch, to obtain 100 independent gra-\ndient estimates for both approaches for each policy.\nFinally, we arrive at a scalar empirical variance es-\ntimate, by taking the trace of the covariance ma-\ntrix estimated using each batch of 100 gradient es-\ntimates, along with a 95% confidence interval. The\nresults are plotted in Fig. 4. In general, we find that\nPRD-MAPPO tends to avoid the spikes in gradient\nvariance present in MAPPO, which may explain its\nimproved stability and asymptotic performance."}, {"title": "6 Related Work", "content": "Many recent approaches have been proposed to deal\nwith the credit assignment problem. G2ANet (Liu\net al., 2020), for instance, proposed a novel attention-\nbased game abstraction mechanism that enables the\ncritic to better isolate important interactions among"}, {"title": "7 Limitations", "content": "The primary limitation of PRD-MAPPO is that PRD is not guaranteed to accelerate learning\nin every environment, because some tasks cannot be decomposed (i.e., each agent's relevant set\ncontains most or all other agents). For example, in the traffic junction experiment, it is possible\nthat learning is only somewhat improved by PRD because interactions among agents are too dense,\nmaking decoupling less effective."}, {"title": "8 Conclusions", "content": "We addressed the shortcomings of MAPPO, a state-of-the-art multi-agent reinforcement learning\nalgorithm. Specifically, we hypothesized that the credit assignment problem manifests itself in\npolicy gradient estimator variance. Based on this hypothesis, we proposed integrating PRD into\nMAPPO as a strategy to improve credit assignment, yielding a new multi-agent model-free RL\nalgorithm, PRD-MAPPO. We demonstrated that PRD-MAPPO provides significant improvements\nboth in learning efficiency and stability, across a diverse set of tasks, compared to both MAPPO and\nseveral state-of-the-art MARL algorithms such as QMix, LICA, and COMA. We empirically verified\nthe hypothesis that PRD decreases the variance of the gradient estimates of MAPPO. Finally, we\nvisualized the relevant sets inferred by PRD, and found that it correctly grouped together agents\nthat should cooperate. The improvements in learning speed and stability, combined with decreased\ngradient variance and sensible relevant set estimation, indicate that PRD, used in the context of\nMAPPO, provides a useful credit assignment strategy for multi-agent problems."}, {"title": "A Detail Task Descriptions", "content": "Collision Avoidance: 3 teams of 8 agents each exist in a square bounded 2D region. Agents receive\na reward for reaching their assigned goal location, and receive a penalty for colliding with other agents\nbelonging to the same team. Agents therefore need only cooperate with other agents on their team\nto avoid collisions. Both the agents and goals are initialized in random locations. The observation\nspace consists of the agent's position, velocity, team ID, and goal position. The agents can take 5\npossible actions that allow them to move either north, south, east, west or remain stationary. The\nreward function is the -12 distance between the agent position and the goal position multiplied by a\nscalar value of 0.1. On collision, the participating agents receive a -1 reward each. The environment"}, {"title": "B Pseudocode", "content": "Algorithm 1 PRD-MAPPO\n1: Initialize $\\theta$, the parameters for policy $\\pi$, $\\omega$, the parameters for state-action value critic $Q$ and $\\phi$,\nthe parameters for state value critic $V$, using orthogonal initialization (Hu et al., 2020)\n2: Set learning rate $\\alpha$\n3: while $step < step_{max}$ do\n4: set data buffer $D = {}$\n5: for $i = 1$ to $batch\\ size$ do\n6: $\\tau = []$ - empty list\n7: initialize $h_0^{(1)},...,h_0^{(M)}$ actor RNN states\n8: initialize $h_0^V,(1),...,h_0^V,(M)$ state value RNN states\n9: initialize $h_0^{Q,(1)},...,h_0^{Q,(M)}$ state-action value RNN states\n10: for $t = 1$ to $T$ do\n11: for all agents $a$ do\n12: $u_t^{(a)},h_{t,\\pi}^{(a)} = \\pi (o_t^{(a)}, h_{t-1,\\pi}^{(a)}; \\theta)$\n13: end for\n14: ($q_t^{(1)},...,q_t^{(M)}), (h_{t,Q}^{(1)}...h_{t,Q}^{(M)}), W_{prd,t} = Q(s_t^{(1)}...s_t^{(M)}, u_t^{(1)} ... u_t^{(M)}, h_{t-1,Q}^{(1)}...h_{t-1,Q}^{(M)}; \\omega)$\n15: ($v_t^{(1)},...,v_t^{(M)}), (h_{t,V}^{(1)}...h_{t,V}^{(M)}) = V(s_t^{(1)}...s_t^{(M)}, u_t^{(1)} ... u_t^{(M)}, h_{t-1,V}^{(1)}...h_{t-1,V}^{(M)}; \\psi)$\n16: mask out the actions of agent a while calculating its state value $v_t^{(a)}$\n17: Execute actions $u_t$, observe $r_t$, $s_{t+1}$, $o_{t+1}$\n18: $\\tau += [s_t, o_t, h_{t,\\pi}, h_{t,V}, u_t, r_t, s_{t+1}, o_{t+1}]$\n19: end for\n20: Compute relevant set $R_1,..., R_M$ using $W_{prd}$\n21: Compute return $G_i$ for each agent $i = 1, ..., M$, to learn the Q function and total relevant-set\nreturn $G_i = \\sum_{j \\in R_i} G_j$ for each agent i to learn V function on $\\tau$ and normalize with PopArt\n22: Compute advantage estimate $\\hat{A}^1, ..., \\hat{A}^M$ via GAE on state value estimates on $\\tau$, using\nPopArt\n23: Split trajectory $\\tau$ into chunks of length $L$\n24: for $l = 0, 1, ...,T//L$ do\n25: $D = D \\cup (r[l : l + L], \\hat{A}[l : l + L], G[l : l + L], \\hat{G}[l : l + L])$\n26: end for\n27: end for\n28: for mini-batch $k = 1,..., K$ do\n29: $b \\leftarrow$ random mini-batch from $D$ with all agent data\n30: for each data chunk c in the mini-batch b do\n31: update RNN hidden states for $\\pi$, Q and V from first hidden state in data chunk\n32: end for\n33: Adam update $\\theta$ on $L(\\theta)$ with data b\n34: Adam update $\\omega$ on $L(\\omega)$ with data b\n35: Adam update $\\psi$ on $L(\\psi)$ with data b\n36: end while"}, {"title": "C Additional Results", "content": "We experimented with various methods for selecting agent relevant sets, as described below. Reward\ncurves for each method in each of our"}]}