{"title": "EmbodiedOcc: Embodied 3D Occupancy Prediction\nfor Vision-based Online Scene Understanding", "authors": ["Yuqi Wu", "Wenzhao Zheng", "Sicheng Zuo", "Yuanhui Huang", "Jie Zhou", "Jiwen Lu"], "abstract": "3D occupancy prediction provides a comprehensive de-\nscription of the surrounding scenes and has become an es-\nsential task for 3D perception. Most existing methods focus\non offline perception from one or a few views and cannot\nbe applied to embodied agents which demands to gradu-\nally perceive the scene through progressive embodied ex-\nploration. In this paper, we formulate an embodied 3D oc-\ncupancy prediction task to target this practical scenario and\npropose a Gaussian-based EmbodiedOcc framework to ac-\ncomplish it. We initialize the global scene with uniform 3D\nsemantic Gaussians and progressively update local regions\nobserved by the embodied agent. For each update, we ex-\ntract semantic and structural features from the observed im-\nage and efficiently incorporate them via deformable cross-\nattention to refine the regional Gaussians. Finally, we em-\nploy Gaussian-to-voxel splatting to obtain the global 3D\noccupancy from the updated 3D Gaussians. Our Embod-\niedOcc assumes an unknown (i.e., uniformly distributed)\nenvironment and maintains an explicit global memory of it\nwith 3D Gaussians. It gradually gains knowledge through\nlocal refinement of regional Gaussians, which is consistent\nwith how humans understand new scenes through embod-\nied exploration. We reorganize an EmbodiedOcc-ScanNet\nbenchmark based on local annotations to facilitate the\nevaluation of the embodied 3D occupancy prediction task.\nExperiments demonstrate that our EmbodiedOcc outper-\nforms existing local prediction methods and accomplishes\nthe embodied occupancy prediction with high accuracy and\nstrong expandability. Our code is available at: https:\n//github.com/YkiWu/EmbodiedOcc.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of embodied intelligence and\nactive agents, 3D scene perception [19, 21, 26, 27] has be-\ncome a crucial task in computer vision. Intelligent agents\nexploring in indoor scenarios make decisions and execute"}, {"title": "2. Related Work", "content": "3D Occupancy Prediction: Benefiting from its compact-\nness and versatility, 3D occupancy prediction has gained\ngreat popularity in both indoor and outdoor scenes over the\nlast few years. Methods based on multi-view images or\nadditional 3D information [7\u20139, 12, 25, 31] have made sig-\nnificant advancements in many scenarios. Monoscene[1]\nwas the first to derive 3D occupancy prediction from a sin-\ngle image, and subsequent works[38, 40] further focused\non addressing the depth ambiguity in this monocular set-\nting, collectively propelling this field into a more challeng-\ning stage. However, the majority of these efforts were con-\nfined to local and offline prediction. EmbodiedScan [17, 29]\nintroduced a comprehensive framework capable of continu-\nous occupancy prediction from multi-modal sequential in-\nputs. Despite this, embodied online 3D occupancy pre-\ndiction based on real-time monocular visual input is more\naligned with the requirements of embodied agents.\nOnline 3D Scene Perception: Accurate comprehension\nof 3D scenes is an indispensable capability for embodied\nagents. Many tasks, such as 3D occupancy prediction[1,\n40] and object detection[20, 27], are direct manifestations\nof this capability. Currently, most works on 3D scene\nperception [5, 19, 39] was conducted offline, taking pre-\nacquired and reconstructed 3D data to obtain a relatively\nlagging perception. Based on this situation, Online3D[34]\nintroduced an adapter-based model that equips mainstream\noffline frameworks with the competence to perform online\nscene perception, which means they can process a real-\ntime RGB-D sequences. However, this framework still fails\nto overcome the intrinsic limitation of conventional point\ncloud modality. In a more general embodied scenario, real-\ntime monocular visual input for scene perception can fur-\nther advance the research on embodied agents.\n3D Gaussian Splatting: 3D Gaussian Splatting[10] uses\nanisotropic 3D Gaussians to model a 3D scene, renowned\nfor its fast speed and high quality in the field of radi-\nance field rendering. The explicit physical characteris-\ntics of 3D Gaussians and the splat-based rasterization em-\nployed during rendering have also motivated rapid advance-\nments in research fields such as scene editing[6, 18, 23],\ndynamic scenarios [4, 16, 33], and SLAM[3, 11, 35, 41].\nGaussianFormer[9] pioneers the application of 3D Gaus-\nsians in outdoor 3D semantic occupancy prediction, up-\ndating Gaussians through comprehensive features extracted\nfrom multi-view images. These Gaussians are ultimately\nconverted into local 3D occupancy prediction through an\nelaborately designed Gaussian-to-voxel splatting module.\nCompared to conventional voxel-based methods, using 3D\nGaussian representation constitutes a more flexible ap-\nproach. In this paper, we will leverage this significant at-\ntribute to accomplish embodied occupancy prediction in in-\ndoor scenarios."}, {"title": "3. Proposed Approach", "content": "3.1. Embodied 3D Occupancy Prediction\nConventional works in indoor scenarios for occupancy pre-\ndiction accepted RGB-Ds or depth inputs to predict the se-\nmantic occupancy of a 3D scene. This setting provides\nthe model with ample information for inference. However,\nit undoubtedly diminishes the comprehension capability of\nthe model in practice. We humans are capable of effort-\nlessly processing the visual information gathered by binocu-\nlus to obtain an initial 3D perception of their surroundings.\nMany recent approaches have focused on endowing models\nwith the same competence, which means to accept monocu-\nlar RGB image as input and derive a 3D occupancy predic-\ntion within the current frustum. We have:\n$Y_{mono} = F_{mono}(I_{mono}),$\nwhere $F_{mono}$ is the proposed monocular prediction model,\n$I_{mono} \\in R^{H \\times W \\times 3}$ and $Y_{mono} \\in R^{X \\times Y \\times Z \\times C}$ refer to the\nmonocular RGB input and the obtained 3D occupancy pre-\ndiction. $X, Y, Z$ represent the dimensions of the local 3D\nscene and $C$ represents the total number of semantics.\nThis is merely the initial step towards practical applica-\ntion scenarios. The essence of human intelligence is the\ncapacity to analyze and respond immediately based on real-\ntime perception of the surroundings. Correspondingly, su-\nperior embodied agents are anticipated to process real-time\nvisual inputs gathered egocentrically to update the 3D occu-\npancy prediction of the current scene. Only when equipped\nwith this capability can the execution of downstream tasks\nbased on real-time perception becomes viable.\nTo this end, we propose an embodied 3D occupancy\nprediction task in this paper. Let $X_t = \\{X_1, X_2, ..., X_t\\}$\nbe a RGB sequence and the corresponding extrinsics col-"}, {"title": "3.2. Local Occupancy Prediction Module", "content": "Differing from conventional works which conducted feature\nintegration in a voxelized space, GaussianFormer[9] first\nproposed an object-centric 3D representation to complete\nthe 3D occupancy prediction task. Each semantic Gaussian\ncan describe a local region and we can calculate the sum-\nmation of the contribution of surrounding Gaussians to get\nthe occupancy prediction result at a specific point. Moti-\nvated by this, we design our local and embodied occupancy\nprediction module based on this representation. Elaborate"}, {"title": "3.3. Gaussian Memory Updated Online", "content": "Suppose we are in a novel environment, we will first wan-\nder through it to explore the surroundings. During this pro-\ncess, the objects within the scene and their relationships are\ncontinuously updated in our minds, indicating the forma-\ntion of a memory regarding this scene. Upon returning to\nthe scene next time or revisiting it for further exploration,"}, {"title": "3.4. EmbodiedOcc: An Embodied Framework", "content": "We present the training framework of our EmbodiedOcc\nmodel for indoor embodied occupancy prediction. During\nthe whole prediction process, we use the current monocular\ninput to update our Gaussian memory in real time, which\ncan be easily converted into 3D occupancy prediction.\nscal\nWe first train our local occupancy prediction module\nusing the focal loss $L_{focal}$, the lovasz-softmax loss $L_{lov}$,\nthe scene-class affinity loss $L_{geo}$ and $L_{sem}$ following\nRetinaNet[14], TPVFormer[7] and Monoscene[1]. We use\nmonocular occupancy within the frustum $Y_{mono}^{fov}$ and the\ncorresponding ground truth $Y_{gt}^{fov}$ to compute the loss. So\nthe final expression of the loss is:\n$L = \\lambda_1 L_{focal} (Y_{mono}^{fov}, Y_{gt}^{fov}) + L_{lov} (Y_{mono}^{fov}, Y_{gt}^{fov}) \\\\\n + L_{geo} (Y_{mono}^{fov}, Y_{gt}^{fov}) + L_{sem} (Y_{mono}^{fov}, Y_{gt}^{fov}),$\nwhere $\\lambda_1$ is a balance factor.\nSC\nThen we use the trained local occupancy prediction mod-\nule in the monocular setting to train our EmbodiedOcc\nmodel. For efficient training, we initialize the Gaussian\nmemory of a scene before the first update and compute the\ncurrent loss following equation 6 after each update. To en-\nsure the consistency, the local occupancy ground truth used\nfor every loss calculation is obtained correspondingly from\nthe occupancy of the whole scene. After a certain num-\nber of updates, we reinitialize the Gaussian memory and\ncome to the prediction of the next scene. Trained with such"}, {"title": "4. Experiments", "content": "4.1. EmbodiedOcc-ScanNet Benchmark\nIn this paper, we propose an EmbodiedOcc-ScanNet bench-\nmark based on the locally annotated Occ-Scannet dataset.\nWe explain our benchmark in detail from three parts: task\ndescriptions, datasets and evaluation metric we use.\nTask Descriptions. We conducted two tasks to evaluate\nour EmbodiedOcc framework: local occupancy prediction\nand embodied occupancy prediction. Local occupancy pre-\ndiction shares the same setting with previous works, which\naccept monocular image as input and obtain the occupancy\nprediction within the frustum of the corresponding camera.\nEmbodied occupancy prediction accepts real-time visual in-\nputs continuously and updates the occupancy prediction of\nthe current scene online. The visual input at a certain step\nt during embodied occupancy prediction is still monocu-\nlar, which is a relatively challenging setting compared with\nmulti-view input or input with 3D information.\nDatasets. In the local occupancy prediction task, we\nused the Occ-ScanNet dataset [40] which provides frames\nin 60 \u00d7 60 \u00d7 36 voxel grids(a 4.8m x 4.8m x 2.88m box\nin front of the camera). These frames are labeled with 12\nsemantics, including 11 for valid semantics(ceiling, floor,\nwall, window, chair, bed, sofa, table, tvs, furniture, objects)\nand 1 for empty space. We trained and evaluated our local\noccupancy prediction module on this dataset.\nBased on this dataset, we reorganized an EmbodiedOcc-\nScanNet dataset to train and evaluate our EmbodiedOcc\nframework [22, 40]. During the training and evaluating\nof our EmbodiedOcc framework, we have to ensure that\nscenes in the training set are different from those in eval-\nuating set. So we split the scenes again and obtained\nour final EmbodiedOcc-ScanNet dataset, which comprises\n537/137 scenes in the train/val splits. Each scene in the\nEmbodiedOcc-ScanNet dataset consists 30 posed frames\nwith their corresponding occupancy. The resolutions of\nglobal occupancy of each scene are calculated by ($l_x \\times\nl_y \\times l_z$)/0.08m, where ($l_x \\times l_y \\times l_z$) is the range of this\nscene in the world coordinate system. Besides, we maintain\na global mask of the visible range in corresponding global\noccupancy for each frame. By splicing the global mask of\nall processed frames, we can easily obtain the occupancy\nground truth of the explored part in current scene."}, {"title": "4.2. Implementation Details", "content": "Local occupancy prediction module. Following existing\nworks[9, 40], we use a pre-trained EfficientNet-B7 [24] to\ninitialize the image encoder in our local occupancy pre-\ndiction module. The depth prediction network used in\nthe depth-aware branch is a fine-tuned DepthAnything-V2\nmodel [36], remaining frozen during the training. The\ndepth-aware layer is a 3 layers MLP and the other parts\nof our local occupancy prediction module follows the\nGaussianFormer[9]. The resolutions of the monocular in-\nput are set to 480 \u00d7 640 and the number of Gaussians used\nto conduct the local prediction is 16200. We utilize the\nAdamW[15] optimizer with a weight decay of 0.01. The\nlearning rate warms up in the first 1000 iterations to a max-\nimum value of 2e-4 and decreases according to a cosine"}, {"title": "4.3. Results and Analysis", "content": "Local occupancy prediction. We evaluated our local oc-\ncupancy prediction module on the Occ-ScanNet dataset. As"}, {"title": "5. Conclusion", "content": "In this paper, we have formulated an embodied 3D oc-\ncupancy prediction task and proposed a Gaussian-based\nEmbodiedOcc framework accordingly. Our EmbodiedOcc\nmaintains an explicit Gaussian memory of the current scene\nand updates this memory during the exploration of this\nscene. Both quantitative and visualization results have\nshown that our EmbodiedOcc outperforms existing methods\nin terms of local occupancy prediction and accomplishes\nthe embodied occupancy prediction task with high accuracy\nand strong expandability. We believe that our EmbodiedOcc\nhave paved the way for enabling the active agents to conduct\naccurate and flexible embodied occupancy prediction."}, {"title": "A. Embodied Occ-ScanNet Dataset Details", "content": "We reorganize our EmbodiedOcc-ScanNet dataset follow-\ning the data formulation used in NYUv2 [22] and Occ-\nScanNet [40]. We noted that the Occ-ScanNet dataset ac-\ntually consists of frames sampled from the original Scan-\nNet [2] dataset randomly, which means that different frames\nmay come from the same indoor scene. For all scenes in the\nOcc-ScanNet dataset, we selected 537 scenes to constitute\nthe training set for EmbodiedOcc-ScanNet, and 137 scenes\nto form the evaluation set. We split these scenes in this way\nto ensure that scenes in the training set are different from\nthose in the evaluation set.\nFor each scene, we first obtain a global occupancy of it\nfrom the voxel labels in the CompleteScanNet [32] dataset\nusing the K-Nearest Neighbors algorithm. Next, we count\nand resample the frames of this scene in Occ-ScanNet\ndataset using a certain interval to obtain 30 posed images.\nFor each frame, we select a specific area in front of the cam-\nera as the range of local occupancy. The selection of the\nlocal voxel origin is consistent with the Occ-ScanNet [40].\nThen, we obtain the current local occupancy from the global\noccupancy using the K-Nearest Neighbors algorithm. In\naddition to this, we maintain a mask in global resolutions\nfor each frame, which marks the intersection of the current\nlocal voxel and frustum. This allows us to obtain the occu-"}, {"title": "B. Additional Visualizations", "content": "Due to space limitations, we only selected a few frames in\nthe main text to demonstrate the performance of our local\noccupancy prediction module. In Figure 9, we use a more\ndiverse set of monocular samples to further showcase the\nvisual effects of the local occupancy obtained by our local\noccupancy prediction module."}]}