{"title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection", "authors": ["Jinqi Xiao", "Shen Sang", "Tiancheng Zhi", "Jing Liu", "Qing Yan", "Linjie Luo", "Bo Yuan"], "abstract": "Training large-scale neural networks in vision, and multimodal domains demands substantial memory resources, primarily due to the storage of optimizer states. While LORA, a popular parameter-efficient method, reduces memory usage, it often suffers from suboptimal performance due to the constraints of low-rank updates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce optimizer memory by projecting gradients and moment estimates into low-rank spaces via singular value decomposition or random projection. However, they fail to account for inter-projection correlation, causing performance degradation, and their projection strategies often incur high computational costs. In this paper, we present COAP (COrrelation-Aware Gradient Projection), a memory-efficient method that minimizes computational overhead while maintaining training performance. Evaluated across various vision, language, and multimodal tasks, COAP outperforms existing methods in both training speed and model performance. For LLaMA-1B, it reduces optimizer memory by 61% with only 2% additional time cost, achieving the same PPL as AdamW. With 8-bit quantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over GaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy. https://byteaigc.github.io/coap/", "sections": [{"title": "1. Introduction", "content": "Deep neural networks have achieved remarkable success across vision [10, 22, 35, 36, 41], language [1, 12, 46, 47], and multi-modality domains [11, 25, 26, 44], driven by the increasing scale of these models. While scaling up model size significantly contributes to these advancements [18], it also leads to considerable memory constraints, particularly due to the optimizer states. For instance, training an LLaVA-7B [25] model with the Adam [19] optimizer at BF16 numerical format requires at least 63.8GB of GPU memory, with optimizer states consuming 40% of the total, while the model and gradients each take up 20%. Combining engineering and system efforts, e.g., activation checkpointing [4] and LOMO [32] can significantly reduce the memory usage of activations and gradients. This makes optimizer a critical bottleneck that needs to be addressed. To tackle this issue, low-rank training has proven effective in minimizing the memory footprint of optimizer states by performing training within a low-rank subspace [13, 16, 58].\n\nCurrent low-rank training approaches can generally be categorized into two main types: parameter-efficient fine-tuning (PEFT) and training with low-rank gradients. The most common method in PEFT is Low-Rank Adaptation (LoRA) [16], which adds trainable low-rank matrices to a frozen pre-trained model's weights, allowing for efficient model adaptation with significantly fewer parameters than full fine-tuning. While low-rank updates can reduce memory consumption, extensive empirical evidence [2, 9, 28, 49, 50, 57] shows that the low-rank constraint diminishes the model's representation capacity, resulting in sub-optimal performance compared to full fine-tuning. In addition, its pre-training alternative ReLoRA [24], which periodically updates the model using previously learned low-rank adapters, still requires a full-rank warm-up phase, thereby negating its memory-saving benefits. For training with low-rank gradient [13, 58], these methods leverage the inherent low-rank structure of the gradients rather than constraining model parameters to a low-rank space. Projecting gradients into low-rank space can significantly reduce memory usage during optimization, making it feasible for scenarios with limited memory resources or extremely large models. For example, GaLore [58] periodically applies Singular Value Decomposition (SVD) to decompose gradients, creating a fixed low-rank projection matrix for subsequent steps. However, the high computational cost of SVD can severely slow down training, especially for models with large weight matrices. In contrast, Flora [13] resamples a random projection matrix at each iteration to address the low-rank limitation of LORA. Despite this, generating new random projection matrices at each iteration still incurs computational overhead, particularly for large models. Besides the significant time costs required to achieve memory savings, these methods also face the challenge of lacking inter-projection correlation. When the projection is determined solely by a single step's gradient or random sampling, it may disrupt the continuity of the optimization trajectory, leading to abrupt shifts and adversely affecting training dynamics.\n\nTo address these challenges, we propose a memory-efficient training method, COAP (COrrelation-Aware Gradient Projection), that minimizes additional computational overhead while retaining model performance (see Fig. 1). 1) Unlike methods that rely solely on a single step's gradient or random generation to determine the low-rank projection matrix, COAP introduces an inter-projection correlation-aware update strategy, by solving an optimization problem with gradient descent. It tracks the previous low-rank projection matrix and combines the direction of the first-order moments to continuously update the low-rank projection matrix, thereby avoiding abrupt shifts in training dynamics. 2) Additionally, we propose to use occasional low-cost SVD to recalibrate the low-rank projection matrices, preventing the gradient descent-based updates from getting trapped in local optima. Through the implementation of the criterion that prioritizes gradient reconstruction capability and training direction smoothness, combined with occasional low-cost SVD to continuously update the low-rank projection matrix, we achieve a significant reduction in the computational cost of vanilla SVD in large-scale models while simultaneously enhancing model performance.\n\nWe demonstrate the effectiveness of our method in both pre-training and fine-tuning across various vision, language, and multi-modal tasks. For pre-training, our method matches AdamW's performance while reducing optimizer memory by 61% for LLaMA-1B [46], with only 2% increase in time overhead. In 8-bit optimizers, our approach outperforms 8-bit Adam and GaLore in both training speed and performance on LLaMA-7B. For fine-tuning LLaVA-v1.5-7B [25], our method with 8-bit optimization reduces optimizer memory by 81%, achieves 4\u00d7 speedup over GaLore, and increases accuracy by 1.2%."}, {"title": "2. Related Works", "content": "Momentum-based Optimizers, for instance Adam [19], AdamW [30] and Adadelta [55], have largely replaced traditional SGD due to their adaptiveness and faster convergence. However, this adaptiveness increases memory usage, as they require storing first and second moment estimates, effectively tripling the memory footprint. Adafactor [42] addresses this by employing low-rank approximations for the second moments, reducing memory requirements while retaining adaptive learning. In this work, we explore projecting both gradients and moments into a low-rank space during training, enabling efficient large-scale model training with commonly-used momentum-based optimizers.\nLow-rank Adaptation (LoRA) [16] enables parameter-efficient fine-tuning by applying low-rank updates to pre-trained models, reducing memory and computation costs. DORA [28] improves this by decoupling weights into magnitude and direction, using low-rank updates only for directional adjustments. ReLoRA [24] adapts LoRA to pre-training but requires a full-rank warm-up, increasing memory usage with model size. Variants such as LyCORIS [53], AdaLoRA [57], HALOC [52] and COMCAT [51] step further in efficiency and flexibility. However, these methods often result in suboptimal performance [50] due to the low-rank limit. Conversely, our approach preserves full-rank learning while leveraging low-rank gradient structures, supporting both pre-training and fine-tuning without compromising performance.\nTraining with Low-rank Gradient, inspired by projected gradient descent methods [3, 5], aims to reduce the memory usage of optimizers during training by applying low-rank gradient projections. GaLore [58] computes projection matrices via per-layer gradient decomposition using inefficient SVD, In contrast, Flora [13] proposes to generate projection matrices randomly on the fly to eliminate this overhead. However, a major limitation of these methods is the lack of inter-projection correlation. As the optimizer transitions to a new subspace, the projection matrix is either based on the current gradient or random selection, leading to abrupt shifts. In this work, we address this issue by incorporating prior optimization directions, ensuring smoother transitions in the projection space. This approach improves training dynamics and enhances overall optimization performance.\nMemory-efficient Training Techniques are widely employed to reduce the overall memory footprint in large-scale model training, including activation checkpointing [4], 8-bit optimizers [8], and mixed-precision training [34], etc."}, {"title": "3. Method", "content": "The popular approach, ZeRO [38], optimizes memory usage in multi-GPU settings. ZeRO contains three stages, progressively sharding optimizer states, gradients, and model weights across devices, synchronizing only when necessary to minimize memory usage. Our method specifically targets memory reduction for optimizer states and can be effectively combined with these techniques, offering additional memory savings and further improving training efficiency."}, {"title": "3.1. Background", "content": "Notation. We use the following notation conventions: matrices and vectors are indicated with boldface capital and lowercase letters, such as X and \u00e6, respectively. Non-boldface letters with indices, e.g., X(i, j), and x(i), denote the entries of a matrix X, and a vector \u00e6, respectively.\nFull-Rank Training. For a time-varying weight matrix W \u2208 \u211d^{m\u00d7n} with objective function as L(W), the corresponding gradient matrix at time step t can be denoted as G_t = \u2207_w\u00a3_t(W_t) \u2208 \u211d^{m\u00d7n}. Then, the general weight update process can be formulated as:\n\nW_{t+1} = W_t - \u03b7\u03c1_t(G_t), \\tag{1}\n\nwhere \u03b7 is the learning rate, and \u03c1_t is the optimizer-dependent gradient regularizer that adaptively adjusts raw gradients G_t via incorporating additional optimizer states. For instance, Adam [19] uses bias-corrected first and second moment M_t and V_t to regularize the gradients as follows:\n\nM_t = B_1M_{t-1} + (1 \u2212 \u03b2_1)G_t, \\\\\nV_t = B_2V_{t-1} + (1 \u2212 \u03b2_2)G_t^2, \\\\\n\u03c1_t(G_t) = \\frac{M_t/(1-\u03b2_1)}{\\sqrt{V_t/(1-\u03b2_2) + \u20ac}}, \\tag{2}\n\nwhere B_1 and B_2 are hyper-parameters, \u20ac is a small constant to ensure numerical stability, and the matrix operations are element-wise. Considering Adam requires extra 2mn memory to store M_t and V_t, Adafactor [42] proposes to use factorization to estimate V_t for higher memory efficiency:\n\nR_t = B_2R_{t-1}+ (1 - \u03b2_2) \\text{Sum}(G_t^2, 1), \\\\\nC_t = B_2C_{t-1} + (1 \u2212 \u03b2_2) \\cdot \\text{Sum}(G_t^2, 0), \\\\\nV_t = (R_tC_t)/\\text{Sum}(R_t, 0), \\tag{3}\n\nwhere Sum() returns the sum of each row of the input matrix in the given dimension (0 or 1). Here by using R_t \u2208 \u211d^{m\u00d71} and C_t \u2208 \u211d^{1\u00d7n} to estimate V_t as V_t\u2208 \u211d^{m\u00d7n}, the memory consumption for storing second moments is reduced from mn to m + n.\nTraining with Low-rank Weight Update. Due to the high cost of updating the entire full-rank W_t, LORA [16] and its variants [24, 57], as the memory-efficient solutions that only adjust the change of W_t in the low-rank subspace, are popularly used in practice as follows:\n\nW = W_o + B_tA_t, \\tag{4}\n\nwhere W_o \u2208 \u211d^{m\u00d7n} is the fixed pre-trained or initialized weights, and A_t \u2208 \u211d^{r\u00d7n} and B_t \u2208 \u211d^{m\u00d7r} are the learnable low-rank adapters with r < min(m, n)."}, {"title": "3.2. Challenge Analysis of Gradient Projection", "content": "Training with Low-rank Gradient. Motivated by the observations that 1) updating weight in the low-rank subspace may limit the representation capacity, and 2) gradient matrix tends to exhibit a certain degree of low-rankness during training, recent works [13, 58] propose to explore memory-efficient training via projecting gradient into low-rank format. For instance, given a projection matrix P_t that defines a low-rank subspace, the projected gradient, first order moments and second order moments in Adam optimizer are calculated as G^{proj}_t = G_tP_t, M^{proj}_t = B_1M^{proj}_{t-1} + (1 - B_1)G^{proj}_t and V^{proj}_t = B_2V^{proj}_{t-1} + (1 - B_2)(G^{proj}_t)^2, respectively. Then, the weight update is performed as follows:\n\nW_{t+1} = W_t \u2013 \u03b7\u03c1_t(G^{proj}_t), \\\\\n\u03c1_t(G^{proj}_t) = \\frac{M^{proj}_t/(1-\u03b2_1)}{\\sqrt{V^{proj}_t/(1-\u03b2_2)+\u20ac}} PT_t. \\tag{5}\n\nHere, W_t, G_t, \u03c1_t(G^{proj}_t) \u2208 \u211d^{m\u00d7n} (m > n), P_t \u2208 \u211d^{n\u00d7r}, and G^{proj}_t, M^{proj}_t, V^{proj}_t \u2208 \u211d^{m\u00d7r}.\nAs discussed in Section 3.1 and demonstrated in [13, 58], projecting gradients onto low-rank subspace enables efficient memory-saving training while maintaining the essential full-rank structure of model weights, which is crucial for preserving the model's expressive capacity and avoiding information loss. However, despite these successes, we argue that state-of-the-art gradient projection methods still face some fundamental challenges as follows.\n1) Lack of Inter-Projection Correlation Awareness. As highlighted in [13, 58], weight updates should not be restricted to a single low-rank subspace; therefore, periodically switching the projection subspace (i.e., recalculating P_t) is essential in the training process. Specifically, GaLore [58] performs SVD on G_t every set period of batches, selecting either the truncated left or right singular vector matrix as P_t. In contrast, Flora [13] proposes to directly use random matrix as P_t for gradient projection at each batch. However, a fundamental limitation for these existing P_t update strategies is the lack of inter-projection correlation. Specifically, each time when the optimizer switches to a new low-rank subspace, the updated projection matrix P_t is determined either solely by G_t (as in GaLore) or through randomly selection (as in Flora), without leveraging directional information from previous projections. For instance, the first-order moment M^{proj}_{t-1}, which reflects the moving average of projected gradients, is not incorporated into the P_t update. From an optimization trajectory perspective, this lack of continuity leads to abrupt shifts in the projection space without accounting for prior optimization direction. Such discontinuities, if not properly addressed, can destabilize the training dynamics, potentially compromising model performance.\n\nTo validate our analysis on the importance of inter-projection correlation, we perform an empirical experiment by pre-training DeiT-Base [45] model on the CIFAR-100 [20] dataset. Fig. 3 shows the cumulative effective update (CEU) as training progresses. Here CEU, defined as\n\n\u03a3||W_t - W_{t-1}||_1 = \u03b7\u03a3 ||\u03c1_t(G^{proj}_t)||, serves as a metric to assess optimization sufficiency, with a value close to that of the original optimizer indicating minimal loss due to the low-rank projection. From Fig. 3, it is seen that the inter-projection correlation-unaware GaLore and Flora yield a CEU that significantly deviates from that of the original optimizer, resulting in a notable drop in test accuracy. Particularly, because Flora selects P_t in a purely random way, it produces a CEU that is very different from Adam's, leading to severe performance degradation. On the other hand, our proposed inter-projection-aware approach achieves a higher CEU than the existing solutions (sometimes even surpassing that of Adam), bringing higher test accuracy. It suggests that the projection directions may have effectively eliminated noise or irrelevant information. This could potentially enable the model to converge more efficiently towards the optimal solution. Experiments in Section 4 further demonstrate that our approach empirically outperforms the state-of-the-art methods across various pre-training and fine-tuning tasks for different model types.\n2) High SVD-incurred Projection Cost. Another challenge of current projection gradient methods, especially GaLore, is the high computational overhead caused by the costly SVD, which has a complexity of O(mn^2) for an m \u00d7 n matrix. Specifically, since each P_t update requires performing SVD on G_t, this process can become prohibitively expensive, especially for large gradient matrices. For instance, when using GaLore to pre-train LLaVA-7B [25] model under a typical configuration \u2013 updating P_t every 200 batches with a batch size of 16 \u2013 it takes approximately 540 seconds on a single A100 GPU to compute all P_t with a rank of 512 for each projection update. On the other hand, the total time required for weight updates over these 200 batches is only 600 seconds. This means that calculating the P_t in GaLore incurs a 90% training overhead, making it extremely computationally intensive."}, {"title": "3.3. Proposed Method", "content": "To overcome these challenges and make the low-rank gradient projection solution more practical and feasible for model training, we propose COAP, which is detailed below.\nInter-projection Correlation-aware P_t Update. Considering the importance of incorporating the information from the previous projection into the current update, we propose calculating P_t via solving the optimization problem:\n\nmin_{P_t} \\text{MSE}(G_t, \u011c_t) (1 \u2013 \\text{CosSim}(M_{t-1}, G_t)), \\tag{6}\n\nreconstruction term\ndirection term\n\nWhile Flora avoids the intensive SVD, it still incurs significant overhead in large-scale model training due to regenerating random projection matrices each iteration.\nGaLore claims that the total computational overhead induced by SVD is negligible (<10%) compared to other memory-efficient training techniques such as CPU-offload [39]. However, this overhead cannot be ignored when compared to methods that do not use these techniques"}, {"title": "1. Detailed Proposed Method", "content": "Notation. In this paper, we use the following notation conventions: Matrices and vectors are indicated with boldface capital and lowercase letters, e.g., X and \u00e6, respectively. Tensors are represented using boldface calligraphic script, denoted as X."}, {"title": "1.1. Inter-projection Correlation-aware Pt Update.", "content": "Considering the importance of incorporating the information from the previous projection into the current update, we propose calculating P_t via solving the optimization problem:\n\nmin_{P_t} \\text{MSE}(G_t, \u011c_t) (1 \u2013 \\text{CosSim} (M_{t-1}, G_t)), \\tag{1}\n\nreconstruction term\ndirection term\n\nwhere CosSim(\u00b7, \u00b7) and MSE(\u00b7, \u00b7) return the cosine similarity and mean squared error, respectively. To simplify notation, the notation without the subscript t represents a general form of the optimization problem, i.e.,\n\nmin_P \\text{MSE}(G, \u011c) (1 \u2013 \\text{CosSim}(M, G)), \\tag{2}\n\nreconstruction term\ndirection term\n\nwhere P\u2208 R^{n\u00d7r}, \u011c \u2208 R^{m\u00d7n} = GP P^T and M\u2208 R^{m\u00d7n} = M^{proj}P^T are the full-rank estimates of gradient and first-order moment projected back from the low-rank subspace, respectively. Notably, the reconstruction term is introduced to minimize the reconstruction error for gradients incurred by projection \u2013 achieving the similar goal that SVD essentially aims for, and direction term encourages the consistency of optimization direction after restoring from low-rank subspace.\nTo solve Eqn. 2 as a non-convex optimization problem, we propose using stochastic gradient descent to iteratively update P_t as follows:\n\nP:=P \u2013 \u03b7(\\frac{\\partial\\text{MSE}(\u011c, G)}{\\partial P} + (1 \u2013 \\text{CosSim}(M, G)\\frac{\\partial \\text{CosSim} (M, G)}{\\partial P}). \\tag{3}\n\nHere, \u03b7 represents learning rate, set to 0.1 by default. The gradient expressions for the reconstruction term and the direction term are derived as follows:\nGradient of Reconstruction term (MSE).\n\n\\frac{\\partial \\text{MSE}(\u011c, G)}{\\partial P} = \\frac{\\partial }{\\partial P} \\frac{1}{mn} \\text{tr}((G-\u011c)^T (G-\u011c))\n\n= \\frac{2}{mn}(\u011c\u012aGP \u2013 2G\u012aGP + G\u00af\u011cP), \\tag{4}\n\nwhere tr() represents the trace of a matrix.\nGradient of Direction Term (CosSim).\nGiven the cosine similarity between matrices M and G, defined as:\n\n\\text{CosSim}(M, G) = \\frac{1}{m} \u03a3_{i=1}^m \\text{CosSim} (M_i, G_i)\n\n= \\frac{1}{m} \u03a3_{i=1}^m \\frac{(M_i, G_i)}{||M_i||||G_i||}. \\tag{5}\n\nApplying the chain rule to compute the gradient of CosSim(M, G) with respect to P:\n\n\\frac{\\partial \\text{CosSim}(M, G)}{\\partial P} = \\frac{\\partial \\text{CosSim}(M, G)}{\\partial M} \\frac{\\partial M}{\\partial P}\n\n= \\frac{1}{m} \u03a3_{i=1}^m \\frac{G_i}{||M_i||||G_i||} - \\frac{M_i (M_i, G_i)}{||M_i||^3||G_i||}) M^{proj^T}, \\tag{6}\n\nwhere || || represents the Euclidean norm, (\u00b7,\u00b7) denotes the inner product, M_i, G_i, M^{proj} denotes the i-th row of M,G, M^{proj}\nIncorporating the gradient expressions above, we derive the final update formula for P as follows:\n\nP:=P - \u03b7(\\frac{2}{mn}(GTGP \u2013 2G\u012aGP + G\u012a\u011cP)-\n(1 - \\frac{1}{m} \u03a3_{i=1}^m \\frac{(M_i, G_i)}{||M_i||||G_i||})+\n(1 - \\frac{1}{m} \u03a3_{i=1}^m (\\frac{G_i}{||M_i||||G_i||} - \\frac{M_i (M_i, G_i)}{||M_i||^3||G_i||}) M^{proj^T})\n\n\n\\frac{1}{mn}tr((G-\u011c)^T (G-\u011c))}, \\tag{7}"}, {"title": "1.2. Extension to CONV Layer.", "content": "To enhance the generality and applicability of our algorithm, it is essential to extend support to higher-dimensional weight tensors, which are prevalent in architectures such as Convolutional Neural Networks (CNNs). While the most straightforward approach would be to reshape CNN weights into matrices and apply the same low-rank space construction method used for matrix operations, this naive strategy would inevitably result in the loss of intrinsic spatial characteristics inherent to CNNs [27].\nFor a convolutional layer with a weight tensor W\u2208 R^{O\u00d7I\u00d7K1\u00d7K2}, where I and O are the number of input channels and output channels, respectively, and K\u2081 and K2 are the kernel sizes, we use Tucker-2 decomposition [48] as the factorization method. In this scenario, W can be represented with a core tensor C and two factor matrices (U\u2081 and U2) along each mode as follows:\n\nW = C \u00d7_1 U_1\u00d7_2 U_2, \\tag{8}\n\nwhere \"\u00d7_n\" denotes the n-mode product. Specifically, U\u2081 \u2208 R^{Oxro} represents the left singular vectors of the mode-1 unfolding of W, denoted as W_{(1)}, i.e., W_{(1)} = U_1\u2081V_1^T. Similarly, U_2 \u2208 R^{IxrI} represents the left singular vectors of the mode-2 unfolding of W, denoted as W_{(2)}, i.e., W_{(2)} = U_22V_2^T. The core tensor C is obtained by projecting W onto the subspaces spanned by U\u2081 and U2, i.e.,\n\nC = W \u00d7_1 U_1 \u00d7_2U_2, \\tag{9}\n\nwhere C\u2208 R^{ro\u00d7rI\u00d7K1\u00d7K2}. Here, ro and rI are the Tucker-2 tensor ranks, determining the dimensionality of the factor matrices and the core tensor. Initially, the values for U1, U2, and C are typically obtained using Higher-Order Singular Value Decomposition (HOSVD) [6]. To refine these initial values and achieve the final decomposition, the Alternating Least Squares (ALS) [21] method is employed. This iterative optimization technique alternates between updating the core tensor C and the factor matrices U\u2081 and U2 to minimize the reconstruction error.\nAlgorithm 2 outlines the Adam-based training procedure, integrating the proposed P_t update method for convolutional layers.\nTypically, for a convolutional layer with a weight tensor W\u2208 R^{O\u00d7I\u00d7K1\u00d7K2}, I and O are significantly larger than the kernel sizes K\u2081 and K2 (i.e., I, O \u00bb K1, K2). Therefore, we propose using the format of Tucker-2 decomposition to handle CNNs while employing our own decomposition method for the actual factorization. According to Eq. 8 and Eq. 9, the low-rank projection space of G_t becomes [P_{O_t} \u2208 R^{O\u00d7ro}, P_{I_t} \u2208 R^{I\u00d7rI}]. The gradient Gt in the low-rank space is G^{proj} = G_t X1 PO X2 PI, and the restored tensor from the low-rank space is \u0177_t = G^{proj} \u00d7_1 P_{O_t} , X_2 P_{I_t}. Here, P_{O_t} and P_{I_t} can be updated according to Eqn. 7 and Occasional Low-cost SVD on the mode-1 and mode-2 unfolding of tensor G, respectively."}, {"title": "1.3. Impact of Low-rank Matrix Projection Formats on CNN Models Performance.", "content": "We compare different formats of Tucker decomposition, i.e., Tucker-1, Tucker-2, and Tucker. In this context, the default Tucker format applies projections along all dimensions of a tensor. For instance, if the tensor is 4-dimensional, it requires 4 projection matrices. Tucker-2, on the other hand, uses only two projection matrices, while Tucker-1 requires just one, making it a variant of SVD.\nAs shown in the Fig. 1, Tucker-2 achieves performance"}, {"title": "2. Experimental Results", "content": "closest to the baseline across different scales of ResNet models. Thus, we select Tucker-2 as the primary format for computing convolution projection matrices."}, {"title": "2.1. Pre-training DDPM", "content": "Experimental Settings. We implement DDPM based on the Diffusers from Hugging Face and conduct experiments on 8xV100 GPUs following the training and evaluation settings in [15]. For CIFAR-10 [20], the model is trained with a batch size of 128 for 800K steps. For CelebA-HQ [29], the model is trained with a batch size of 64 for 460K steps. We generate 50K images to compute the FID (Frechet Inception Distance) with respect to the training dataset and the images generated with 1000 DDPM steps.\nComparison Results. Table 1 presents the performance of our method and GaLore when compressing AdamW and Adafactor optimizers on the DDPM model. Our approach consistently outperforms GaLore on CIFAR-10 and CelebA-HQ datasets. Specifically, using the Adafactor optimizer, our method reduces FID by 1.7 and 6.8 compared to GaLore. Additionally, at compression rates of 1.2\u00d7 and 1.6\u00d7, our approach surpasses the baseline performance."}, {"title": "2.2. Qualitative Comparisons", "content": "We present qualitative results, showcasing images generated by models trained with COAP and other optimizers. These results provide a visual comparison that complements the quantitative analysis in the main paper. Comparisons"}]}