{"title": "An Open Knowledge Graph-Based Approach for Mapping Concepts and Requirements between the EU AI Act and International Standards", "authors": ["Julio Hernandez", "Delaram Golpayegani", "Dave Lewis"], "abstract": "The many initiatives on trustworthy AI result in a confusing and multipolar landscape\nthat organizations operating within the fluid and complex international value chains must\nnavigate in pursuing trustworthy AI. The EU's AI Act will now shift the focus of such\norganizations toward conformance with the technical requirements for regulatory compli-\nance, for which the Act relies on Harmonized Standards. Though a high-level mapping\nto the Act's requirements will be part of such harmonization, determining the degree to\nwhich standards conformity delivers regulatory compliance with the AI Act remains a\ncomplex challenge. Variance and gaps in the definitions of concepts and how they are used\nin requirements between the Act and harmonized standards may impact the consistency\nof compliance claims across organizations, sectors, and applications. This may present\nregulatory uncertainty, especially for SMEs and public sector bodies relying on standards\nconformance rather than proprietary equivalents for developing and deploying compliant\nhigh-risk AI systems. To address this challenge, this paper offers a simple and repeatable\nmechanism for mapping the terms and requirements relevant to normative statements in\nregulations and standards, e.g., AI Act and ISO management system standards, texts into\nopen knowledge graphs. This representation is used to assess the adequacy of standards\nconformance to regulatory compliance and thereby provide a basis for identifying areas\nwhere further technical consensus development in trustworthy AI value chains is required\nto achieve regulatory compliance.", "sections": [{"title": "1 Introduction", "content": "The global interest in Al's ethical and societal risks has grown rapidly in recent years [1-\n4]. In the primary wave of trustworthy AI initiatives, guidelines typically are presented as\nunstructured statements of principles that organizations can adopt to demonstrate some degree\nof trustworthiness in their development and use of AI technology. With the increasing number\nof AI incidents, it became evident for policymakers and public authorities that there is a wide\nrange of applications through which AI negatively impacts people's lives that are developed\nand deployed with little external oversight [5, 6]. Consequently, several jurisdictions are now\ndeveloping Al legislation to introduce oversight over the development and use of AI, ensuring\nindividuals, groups, and society are protected from its potential harms.\nWith its political agreement on the AI Act [7] being reached at the end of 2023, the\nEuropean Union (EU) has become a pioneer in AI regulation. The AI Act specifies a\ntiered risk system, where some applications of AI are prohibited, and others are identi-\nfied as a sufficiently low risk that only consumer labels or voluntary codes of practice are\nrequired. However, the focus of regulatory oversight and compliance information exchange\nlies between these tiers where high-risk AI systems are defined. The AI Act identifies high-\nrisk AI application areas and requires that their development and deployment demonstrate\nconformance to risk and quality management measures in order to comply with the regulation.\nThese measures follow the regulatory mechanism, called the New Legislative Framework\n(NLF)\u00b9, that is already established by the EU to provide a single health and safety regulatory\nframework for products across the European Single Market. The AI Act extends this mech-\nanism to products and services containing AI and extends the scope of protection beyond\nhealth and safety to include the protection of all fundamental rights and the environment. In\nthis way, the legislation aims to build public trust in AI while encouraging innovation in AI\nvalue chains by normalizing regulatory oversight.\nWhile the EU has separately provided guidelines for developing trustworthy AI\u00b2, these\ndo not form part of the AI Act, given their principle-based representation. Instead, the\ndetailed rulemaking on implementing the Act, including how Al risks are assessed, man-\naged, and monitored, are delegated to technical standards. These can be in the form of\nstandards that have been harmonized with the requirements of the AI Act by European Stan-\ndardisation Organisations (ESO), namely the Comit\u00e9 Europ\u00e9en de Normalisation (CEN),\nComit\u00e9 Europ\u00e9en de Normalisation Electrotechnique (CENELEC), or the European Telecom-\nmunications Standards Institute (ETSI). In response to the European Commission's draft\nstandardization request 3, relevant standards are already being addressed internationally by\nEuropean standards development bodies such as ISO/IEC JTC 1 Subcommittee 424 on AI\n(SC42). However, these development initiatives involve complex sets of interrelated stan-\ndards, many of which are still under development [8] and will evolve parallel to the AI Act\nand similar legislation being considered in other jurisdictions."}, {"title": "2 Related Work", "content": "There is some existing work addressing the challenges of implementing trustworthy AI\nrequirements through utilizing OKG-based approaches. Amaral et al. [11] combine the Ref-\nerence Ontology for Trust (ROT) and the Non-Functional Requirements Ontology (NFRO)\nto characterize an ontology that captures trust requirements for software systems. Inspired by\nISO/IEC JTC 1/SC 42 activities, Lewis et al. [12] propose a high-level ontology to map out\nthe consistency and overlap of concepts from different AI standards, regulations, and policies.\nGolpayegani et al. [13] use the aforementioned ontology to compare the semantic interoper-\nability between ISO/IEC 42001 standard on AI management system, the EU trustworthy AI\nassessment list (ALTAI) and the EU AI Act. In this work, are map AI concepts and require-\nments from regulations and standards to develop a mechanism to compare, integrate, and\nrelate the terminology used by these documents with the objective of regulatory compliance."}, {"title": "3 AI Act Compliance Through Conformity with Standards", "content": "This section presents an analysis of the AI Act and harmonized standards, followed by an\nanalysis of legal compliance challenges with standards."}, {"title": "3.1 The Interplay between the AI Act and Harmonized Standards", "content": "Following the mechanisms established in the NLF, providers of high-risk AI applications need\nto demonstrate their compliance with the essential requirements of the AI Act through a con-\nformity assessment process that is either self-certified or certified by a recognized authority,\nknown as a notified body. The conformity assessment process must address AI Act require-\nments related to risk management, data governance, and technical documentation under a\nquality management system for the product's compliance to be certified. This mechanism\naligns well with the standards developed by the ISO Committee on Conformity Assessment\n(CASCO) through the ISO 17000 series of standards that guides the terminology, concepts,\nrequirements, processes, and competencies regulators can use to establish certification rules."}, {"title": "3.2 Challenges of Legal Compliance Using Harmonized Standards", "content": "Several challenges remain when considering the vertical nature of the AI Act's high-risk\nclassification, the potentially complex value chains involved, and the international nature of\nAI innovation. First, the AI Act focuses its provisions for high-risk AI based on a specific\nset of applications, categorized into two groups: (i) AI systems that are products or safety\ncomponents of the products already subject to the Union harmonization legislation- a set\nof specific European product health and safety regulations, such as regulations on machinery,\ntoys, medical devices, agricultural vehicles, and rail systems. (ii) AI applications that are\nnot yet regulated but are identified by the EC as presenting high risks to health, safety, or\nfundamental rights. However, the technical requirements for compliance with the AI Act\nand the potential harmonized standards from SC42 are horizontal, i.e., specified in terms\nthat apply to any Al system. For instance, if we consider the risk of a voice recognition\nsystem misunderstanding the same utterance in different accents, the acceptable risk level\nwhen used in ambulance dispatch may involve different considerations from use in primary\nschool student assessment.\nSecond, many AI providers may already be undertaking some form of proprietary trust-\nworthy Al risk assessment and quality process, e.g., The Microsoft Responsible AI Standard,\nv26. Such AI providers will need to undertake a mapping to assess whether the proprietary\napproach fully satisfies the requirements of the AI Act. They may also wish to establish a\ntransition mapping from the proprietary standard to a relevant harmonized standard to reduce\nthe cost of demonstrating compliance with the AI Act, which is estimated to be between\n193,000\u20ac to 330,000\u20ac7, and improving the potential for establishing such compliance, and\nthereby its trustworthy AI competencies to its customers and affected societal stakeholders\nmore broadly.\nThird, there may be populations of AI providers that have invested in undertaking a trust-\nworthy AI risk and quality assessment based on standards from national bodies, e.g., NIST8,\nDIN9, BSI10, or other international standards, e.g. IEEE P7000 [14]. Mapping between such\nstandards and the AI Act's harmonized standards may be important for AI providers to"}, {"title": "4 A Layered Approach for Semantic Modeling and Mapping\nof Trustworthy AI Requirements", "content": "The challenges of mapping normative statements from regulations, such as the AI Act, against\nthose in standards from different SDOs require cataloging the normative statements from\nthese different source documents to mirror the granularity of authority and their revision\ncycles. This work analyzed the sections of the AI Act, specifically the compliance require-\nments for AI Providers for high-risk AI systems, and the terms and concepts defined by SC42\nin foundational standards ISO/IEC 22989, as well as the template for ISO MSS, which forms\nthe basis for the development of the AI MSS.\nOKGs are grounded in the Resource Description Framework (RDF) [15], which allows an\nunlimited knowledge graph of nodes and links to existing online resources on the web, thus\nlending themselves to third-party scrutiny. Nodes and associations in this knowledge graph\nare typed according to ontologies, also known as data vocabularies, that can be developed\nindependently and published to a distinct namespace on the web. This highly decentralized\napproach aligns well to promote the participation of those generating standards, organiza-\ntional policies, and regulations, as well as those interested in how these documents develop\nand map to each other. OKGs also offer predictable and controlled upgrade paths for express-\ning compliance rules as new regulations or regulatory guidance and case law emerge, allowing\nregulatory compliance for trustworthy AI to remain robust and cost-controlled amidst rapid\nevolution in the relevant regulation.\nIn developing a semantic model for any specific domain, different levels of semantic com-\nmitment can be employed to express semantic relationships between possible information\nelements. The Web Ontology Language (OWL) [16] allows information elements to be mod-\neled as classes or instances, like object-oriented software engineering models. OWL classes\ncan be structured hierarchically so that one class can be declared a subclass of another. Prop-\nerties can be declared between classes and literal types that allow facts or axioms about the\nworld to be asserted and inferred.\nHowever, trustworthy AI is a domain with a wide range of competing conceptual models\nbut a relative paucity of concrete instances where trustworthy characteristics have been mod-\neled, tested, and subject to third-party scrutiny. It is, therefore, more appropriate to capture"}, {"title": "5 TAIR: Trustworthy AI Requirements Ontology", "content": "This section introduces some challenges in mapping normative statements from regulations\nand standards. Additionally, it presents the TAIR ontology as a semantic approach to map con-\ncepts and requirements from regulations and standards. Finally, the TAIR ontology evaluation\nis presented, considering the best practices for detecting errors in ontology design."}, {"title": "5.1 Conceptual Requirements Capture from AI Act and Prospective\nHarmonized Standards", "content": "We aim to enable the capture of terms and concepts related to regulatory requirements and\nstandards to which organizations in the AI value chain can conform to demonstrate their\ncompliance with their regulatory obligations. The approach specifically aims to enable the\ninterlinking of requirements between regulatory text and texts specifying such international\nstandards, thereby checking the extent to which prospective harmonized standards require-\nments will deliver regulatory compliance. This requires an analysis of the normative scope of\nrequirements of both the relevant compliance clauses of the AI Act and the AI MSS template.\nOur semantic modeling leverages the core commonality of the harmonized structure for\nMSS [18] to provide a minimal and reusable approach, determining the extent to which the\nrequirements present in normative statements specified in a regulatory text for trustworthy\nAI are satisfied by normative statements in technical standards documents used in confor-\nmance, specifically those stemming from AI MSS. This is taken as a specific assessment of\nthe more general goal to assess whether this approach allows machine-readable mapping for\nspecific proposed trustworthy AI guidelines or standards to be mapped against requirements\nof specific regulatory text. The target forms of mapping consider:\n\u2022 Whether all captured regulatory requirements are addressed by the available management\nsystem or other technical requirements.\n\u2022 Whether regulatory requirements have mappings to specific technical activities or enti-\nties/artifacts defined in the technical standards"}, {"title": "5.2 TAIR Overview", "content": "This section presents the key elements of the TAIR ontology and the requirements and\nconcepts of the semantic mappings process."}, {"title": "5.2.1 Requirements and Concepts Modelling", "content": "The Trustworthy AI Requirements (TAIR) ontology12 provides the elements to describe terms\nand requirements associated with a specific regulation or standard. Figure 2 depicts the TAIR\nontology, where Requirement and Concept are the main classes in the ontology.\nThe Concept class is a subclass of the OntoLex13 vocabulary, which describes linguis-\ntic resources such as the representation of dictionaries or annotations commonly found in\nlexicography. The Requirement class is used to describe normative clauses. A requirement\ncould be related to a particular concept or lexical entry; this relationship is denoted by the\nproperties implementedBy (who is responsible for implementing the described requirement),\ntrackedBy (who tracks the updates of the requirement), and uses (who uses the described\nrequirement)."}, {"title": "5.2.2 Requirements and Concepts Semantic Mappings", "content": "The TAIR ontology aims to map regulations and standards requirements using linked data\nresources, making them available for consultation and query. Mapping requirements into\nlinked data resources will help create systems capable of defining the requirements needed\nto comply with a domain-specific standard, such as information security and quality manage-\nment. Additionally, it enables the identification and representation of concepts related to a\nstandard, i.e., the words or phrases defined in the document with a specific meaning.\nThe mapping process (Figure 3) considers the regulation or standard document structure\ndivided into clauses. The three phases (P1-P3) of the semantic mapping are described in the\nfollowing paragraphs.\nP1 - Elements identification\nIn this phase are identified the concepts and requirements for a regulation or standard. Con-\ncepts are usually defined in a section called \"Terms and definitions\" or \"Definitions\". The\nrequirements identification consists of looking for clauses expressed in the verbal form of\nshall or shall not 14. Table 1 exemplifies the type of concepts from the AI Act divided into\nactor (e.g., provider, user), artefact (e.g., AI System, Performance), and process (e.g., Putting\ninto service, Withdrawal) concepts.\nP2 - Elements mapping\nThis phase describes each requirement and concept definition into a linked data element con-\nsidering the classes and properties of the TAIR ontology. The RequirementCollection"}, {"title": "5.3 Ontology evaluation", "content": "This evaluation considers ontology design best practices to detect errors or inconsistencies\nin the ontology structure, i.e., how the syntax of an ontology representation conforms to an\nontology language [19]."}, {"title": "6 Conclusion and Future Work", "content": "The Trustworthy AI Requirements (TAIR) ontology provides a basis for capturing and ana-\nlyzing terms and requirements as concept sets from normative statements from the AI Act\nand the conformance-focused international standard on AI from SC42. This is made par-\ntially available as an Open Knowledge Graphs (OKG) resource that allows the links between\ndefined terms, other relevant concepts, and the requirements themselves to be published in a\ntraceable, queryable, and navigable manner.\nAs this work was based on a draft version of the AI Act, we await the publication in early\n2024 of the formal text in order to repeat the extraction of concepts and requirements and\npublish the second version of the TAIR ontology. We will then aim to promote this version\nof the model and its online exploration features to different potential groups who may find\nthis useful to garner feedback on its utility. Such groups could include subject matter experts\nin specific high-risk application domains, such as healthcare or education, who may seek to\nbuild domain-specific extensions to concepts in this model.\nThe model may be of use to policymakers and standards developers involved in the devel-\nopment of harmonized standards, in guidelines to support the implementation of the Act,\nsuch as EC guidelines to SME developing or public sector agencies procuring AI, and those\nestablishing transparency mechanisms for regulatory learning mechanisms such as regulatory\nsandboxes and real-life trials. We would also seek feedback from scholars in law, ethics, social\nscience, and information systems on whether this open approach to AI act concepts provides\na basis for improving comparison, aggregation, and replication of studies in these areas.\nFurther horizontal requirements mapping will be explored, especially as the SC42\nAI Management System Standard (AI MSS) is supported by further standards, including"}]}