{"title": "An Open Knowledge Graph-Based Approach for Mapping Concepts and Requirements between the EU AI Act and International Standards", "authors": ["Julio Hernandez", "Delaram Golpayegani", "Dave Lewis"], "abstract": "The many initiatives on trustworthy AI result in a confusing and multipolar landscape that organizations operating within the fluid and complex international value chains must navigate in pursuing trustworthy AI. The EU's AI Act will now shift the focus of such organizations toward conformance with the technical requirements for regulatory compliance, for which the Act relies on Harmonized Standards. Though a high-level mapping to the Act's requirements will be part of such harmonization, determining the degree to which standards conformity delivers regulatory compliance with the AI Act remains a complex challenge. Variance and gaps in the definitions of concepts and how they are used in requirements between the Act and harmonized standards may impact the consistency of compliance claims across organizations, sectors, and applications. This may present regulatory uncertainty, especially for SMEs and public sector bodies relying on standards conformance rather than proprietary equivalents for developing and deploying compliant high-risk AI systems. To address this challenge, this paper offers a simple and repeatable mechanism for mapping the terms and requirements relevant to normative statements in regulations and standards, e.g., AI Act and ISO management system standards, texts into open knowledge graphs. This representation is used to assess the adequacy of standards conformance to regulatory compliance and thereby provide a basis for identifying areas where further technical consensus development in trustworthy AI value chains is required to achieve regulatory compliance.", "sections": [{"title": "1 Introduction", "content": "The global interest in Al's ethical and societal risks has grown rapidly in recent years [1-4]. In the primary wave of trustworthy AI initiatives, guidelines typically are presented as structured statements of principles that organizations can adopt to demonstrate some degree of trustworthiness in their development and use of AI technology. With the increasing number of AI incidents, it became evident for policymakers and public authorities that there is a wide range of applications through which AI negatively impacts people's lives that are developed and deployed with little external oversight [5, 6]. Consequently, several jurisdictions are now developing Al legislation to introduce oversight over the development and use of AI, ensuring individuals, groups, and society are protected from its potential harms.\nWith its political agreement on the AI Act [7] being reached at the end of 2023, the European Union (EU) has become a pioneer in AI regulation. The AI Act specifies a tiered risk system, where some applications of AI are prohibited, and others are identi-fied as a sufficiently low risk that only consumer labels or voluntary codes of practice are required. However, the focus of regulatory oversight and compliance information exchange lies between these tiers where high-risk AI systems are defined. The AI Act identifies high-risk AI application areas and requires that their development and deployment demonstrate conformance to risk and quality management measures in order to comply with the regulation. These measures follow the regulatory mechanism, called the New Legislative Framework (NLF)\u00b9, that is already established by the EU to provide a single health and safety regulatory framework for products across the European Single Market. The AI Act extends this mech-anism to products and services containing AI and extends the scope of protection beyond health and safety to include the protection of all fundamental rights and the environment. In this way, the legislation aims to build public trust in AI while encouraging innovation in AI value chains by normalizing regulatory oversight.\nWhile the EU has separately provided guidelines for developing trustworthy AI\u00b2, these do not form part of the AI Act, given their principle-based representation. Instead, the detailed rulemaking on implementing the Act, including how Al risks are assessed, man-aged, and monitored, are delegated to technical standards. These can be in the form of standards that have been harmonized with the requirements of the AI Act by European Stan-dardisation Organisations (ESO), namely the Comit\u00e9 Europ\u00e9en de Normalisation (CEN), Comit\u00e9 Europ\u00e9en de Normalisation Electrotechnique (CENELEC), or the European Telecom-munications Standards Institute (ETSI). In response to the European Commission's draft standardization request 3, relevant standards are already being addressed internationally by European standards development bodies such as ISO/IEC JTC 1 Subcommittee 424 on AI (SC42). However, these development initiatives involve complex sets of interrelated stan-dards, many of which are still under development [8] and will evolve parallel to the AI Act and similar legislation being considered in other jurisdictions."}, {"title": "2 Related Work", "content": "There is some existing work addressing the challenges of implementing trustworthy AI requirements through utilizing OKG-based approaches. Amaral et al. [11] combine the Ref-erence Ontology for Trust (ROT) and the Non-Functional Requirements Ontology (NFRO) to characterize an ontology that captures trust requirements for software systems. Inspired by ISO/IEC JTC 1/SC 42 activities, Lewis et al. [12] propose a high-level ontology to map out the consistency and overlap of concepts from different AI standards, regulations, and policies. Golpayegani et al. [13] use the aforementioned ontology to compare the semantic interoper-ability between ISO/IEC 42001 standard on AI management system, the EU trustworthy AI assessment list (ALTAI) and the EU AI Act. In this work, are map AI concepts and require-ments from regulations and standards to develop a mechanism to compare, integrate, and relate the terminology used by these documents with the objective of regulatory compliance."}, {"title": "3 AI Act Compliance Through Conformity with Standards", "content": "This section presents an analysis of the AI Act and harmonized standards, followed by an analysis of legal compliance challenges with standards."}, {"title": "3.1 The Interplay between the AI Act and Harmonized Standards", "content": "Following the mechanisms established in the NLF, providers of high-risk AI applications need to demonstrate their compliance with the essential requirements of the AI Act through a con-formity assessment process that is either self-certified or certified by a recognized authority, known as a notified body. The conformity assessment process must address AI Act require-ments related to risk management, data governance, and technical documentation under a quality management system for the product's compliance to be certified. This mechanism aligns well with the standards developed by the ISO Committee on Conformity Assessment (CASCO) through the ISO 17000 series of standards that guides the terminology, concepts, requirements, processes, and competencies regulators can use to establish certification rules."}, {"title": "3.2 Challenges of Legal Compliance Using Harmonized Standards", "content": "Several challenges remain when considering the vertical nature of the AI Act's high-risk classification, the potentially complex value chains involved, and the international nature of AI innovation. First, the AI Act focuses its provisions for high-risk AI based on a specific set of applications, categorized into two groups: (i) AI systems that are products or safety components of the products already subject to the Union harmonization legislation- a set of specific European product health and safety regulations, such as regulations on machinery, toys, medical devices, agricultural vehicles, and rail systems. (ii) AI applications that are not yet regulated but are identified by the EC as presenting high risks to health, safety, or fundamental rights. However, the technical requirements for compliance with the AI Act and the potential harmonized standards from SC42 are horizontal, i.e., specified in terms that apply to any Al system. For instance, if we consider the risk of a voice recognition system misunderstanding the same utterance in different accents, the acceptable risk level when used in ambulance dispatch may involve different considerations from use in primary school student assessment.\nSecond, many AI providers may already be undertaking some form of proprietary trust-worthy AI risk assessment and quality process, e.g., The Microsoft Responsible AI Standard, v2. Such AI providers will need to undertake a mapping to assess whether the proprietary approach fully satisfies the requirements of the AI Act. They may also wish to establish a transition mapping from the proprietary standard to a relevant harmonized standard to reduce the cost of demonstrating compliance with the AI Act, which is estimated to be between 193,000\u20ac to 330,000\u20ac, and improving the potential for establishing such compliance, and thereby its trustworthy AI competencies to its customers and affected societal stakeholders more broadly.\nThird, there may be populations of AI providers that have invested in undertaking a trust-worthy AI risk and quality assessment based on standards from national bodies, e.g., NIST, DIN, BSI, or other international standards, e.g. IEEE P7000 [14]. Mapping between such standards and the AI Act's harmonized standards may be important for AI providers to"}, {"title": "4 A Layered Approach for Semantic Modeling and Mapping of Trustworthy AI Requirements", "content": "The challenges of mapping normative statements from regulations, such as the AI Act, against those in standards from different SDOs require cataloging the normative statements from these different source documents to mirror the granularity of authority and their revision cycles. This work analyzed the sections of the AI Act, specifically the compliance require-ments for AI Providers for high-risk AI systems, and the terms and concepts defined by SC42 in foundational standards ISO/IEC 22989, as well as the template for ISO MSS, which forms the basis for the development of the AI MSS.\nOKGs are grounded in the Resource Description Framework (RDF) [15], which allows an unlimited knowledge graph of nodes and links to existing online resources on the web, thus lending themselves to third-party scrutiny. Nodes and associations in this knowledge graph are typed according to ontologies, also known as data vocabularies, that can be developed independently and published to a distinct namespace on the web. This highly decentralized approach aligns well to promote the participation of those generating standards, organiza-tional policies, and regulations, as well as those interested in how these documents develop and map to each other. OKGs also offer predictable and controlled upgrade paths for express-ing compliance rules as new regulations or regulatory guidance and case law emerge, allowing regulatory compliance for trustworthy AI to remain robust and cost-controlled amidst rapid evolution in the relevant regulation.\nIn developing a semantic model for any specific domain, different levels of semantic com-mitment can be employed to express semantic relationships between possible information elements. The Web Ontology Language (OWL) [16] allows information elements to be mod-eled as classes or instances, like object-oriented software engineering models. OWL classes can be structured hierarchically so that one class can be declared a subclass of another. Prop-erties can be declared between classes and literal types that allow facts or axioms about the world to be asserted and inferred.\nHowever, trustworthy AI is a domain with a wide range of competing conceptual models but a relative paucity of concrete instances where trustworthy characteristics have been mod-eled, tested, and subject to third-party scrutiny. It is, therefore, more appropriate to capture"}, {"title": "5 TAIR: Trustworthy AI Requirements Ontology", "content": "This section introduces some challenges in mapping normative statements from regulations and standards. Additionally, it presents the TAIR ontology as a semantic approach to map con-cepts and requirements from regulations and standards. Finally, the TAIR ontology evaluation is presented, considering the best practices for detecting errors in ontology design."}, {"title": "5.1 Conceptual Requirements Capture from AI Act and Prospective Harmonized Standards", "content": "We aim to enable the capture of terms and concepts related to regulatory requirements and standards to which organizations in the AI value chain can conform to demonstrate their compliance with their regulatory obligations. The approach specifically aims to enable the interlinking of requirements between regulatory text and texts specifying such international standards, thereby checking the extent to which prospective harmonized standards require-ments will deliver regulatory compliance. This requires an analysis of the normative scope of requirements of both the relevant compliance clauses of the AI Act and the AI MSS template (Figure 1).\nOur semantic modeling leverages the core commonality of the harmonized structure for MSS [18] to provide a minimal and reusable approach, determining the extent to which the requirements present in normative statements specified in a regulatory text for trustworthy AI are satisfied by normative statements in technical standards documents used in confor-mance, specifically those stemming from AI MSS. This is taken as a specific assessment of the more general goal to assess whether this approach allows machine-readable mapping for specific proposed trustworthy AI guidelines or standards to be mapped against requirements of specific regulatory text. The target forms of mapping consider:\n\u2022 Whether all captured regulatory requirements are addressed by the available management system or other technical requirements.\n\u2022 Whether regulatory requirements have mappings to specific technical activities or enti-ties/artifacts defined in the technical standards\n\u2022 Whether some requirement mapping is partial in that they use a different definition of con-cepts or different levels of normative strictness, i.e., the requirement (must/shall) compares to a recommendation (should), permission (may), or possibility (can).\n\u2022 If there are terms in the regulatory requirements for which mapping to technical standard requirements, activities, or entities cannot be fully determined.\nTerms extraction. The term extraction and mapping process first involves extracting explicitly defined terms as SKOS concepts. The structure of terminological lists (for example, subsection in the terminology section of ISO/IEC standards), the text of the definitions, and cross-references between these are used to capture taxonomical structures, using the SKOS 'narrower', 'broader', and 'related' relationships.\nRequirements extraction. Normative clauses of the source documents are converted to atomic normative requirements11.\nLexical entries extraction. Where the requirement or required situation from an indi-vidual normative statement conceptualization does not correspond to a term from that same source document, the terms used are captured as a lexical entry, indicating that it is a concept that may require further definition to support future compliance checking. Therefore, lexi-cal entries are candidates for alignment with definitions from another document, e.g., from another referenced legislative document or technical standard.\nThe use of SKOS concepts set for terms and approaches that aim to allow formal expres-sion of a requirement that can be subject to deontic reasoning as part of a requirement management process. Instead, Our approach focuses on facilitating the mapping between sep-arate developed sets of definitions and compliance standards in a flexible, extensible, and"}, {"title": "5.2 TAIR Overview", "content": "This section presents the key elements of the TAIR ontology and the requirements and concepts of the semantic mappings process."}, {"title": "5.2.1 Requirements and Concepts Modelling", "content": "The Trustworthy AI Requirements (TAIR) ontology12 provides the elements to describe terms and requirements associated with a specific regulation or standard. Figure 2 depicts the TAIR ontology, where Requirement and Concept are the main classes in the ontology.\nThe Concept class is a subclass of the OntoLex13 vocabulary, which describes linguis-tic resources such as the representation of dictionaries or annotations commonly found in lexicography. The Requirement class is used to describe normative clauses. A requirement could be related to a particular concept or lexical entry; this relationship is denoted by the properties implementedBy (who is responsible for implementing the described requirement), trackedBy (who tracks the updates of the requirement), and uses (who uses the described requirement)."}, {"title": "5.2.2 Requirements and Concepts Semantic Mappings", "content": "The TAIR ontology aims to map regulations and standards requirements using linked data resources, making them available for consultation and query. Mapping requirements into linked data resources will help create systems capable of defining the requirements needed to comply with a domain-specific standard, such as information security and quality manage-ment. Additionally, it enables the identification and representation of concepts related to a standard, i.e., the words or phrases defined in the document with a specific meaning.\nThe mapping process (Figure 3) considers the regulation or standard document structure divided into clauses. The three phases (P1-P3) of the semantic mapping are described in the following paragraphs.\nP1 - Elements identification\nIn this phase are identified the concepts and requirements for a regulation or standard. Con-cepts are usually defined in a section called \"Terms and definitions\" or \"Definitions\". The requirements identification consists of looking for clauses expressed in the verbal form of shall or shall not 14. Table 1 exemplifies the type of concepts from the AI Act divided into actor (e.g., provider, user), artefact (e.g., AI System, Performance), and process (e.g., Putting into service, Withdrawal) concepts.\nP2 - Elements mapping\nThis phase describes each requirement and concept definition into a linked data element con-sidering the classes and properties of the TAIR ontology. The RequirementCollection"}, {"title": "5.3 Ontology evaluation", "content": "This evaluation considers ontology design best practices to detect errors or inconsistencies in the ontology structure, i.e., how the syntax of an ontology representation conforms to an ontology language [19].\nThe TAIR ontology language conformity evaluation was conducted through the Ontology Pitfall Scanner! (OOPS!) tool [20]. The OOPS! tool detects potential problems in the pro-vided ontology by means of a semi-automatic diagnosis for 32 pitfalls. The evaluation result is classified as minor, important, and critical according to the pitfall detected. Each pitfall is associated with an importance level decided in conjunction with OOPS! developers, experi-enced ontological engineers, and users. For example, a pitfall classified as critical occurs if the ontology is not available (documentation not available online).\nThe OOPS! tool implements three pitfall detection methods: Structural pattern matching, Lexical content analysis, and Specific characteristic search. The former, which implements 24 of the 32 pitfalls, analyzes the internal structure of the ontology, looking for a particular structural pattern that spotted a pitfall. The lexical content analysis method, which imple-ments 9 of the 32 pitfalls, analyzes lexical entities based on the content of annotations (e.g., rdfs:label) and identifiers for detecting pitfalls. The latter method, which implements 5 of the 32 pitfalls, checks for general characteristics of the ontology unrelated to previous methods, e.g., the name given to the ontology does not contain file extensions.\nThe pitfalls identify by the OOPS! tool are minor problems, i.e., do not represent a prob-lem. The most recurrent pitfall is the missing definition of inverse relationships, e.g., the inverse property constrains is not defined for the property constrainedBy. The missing annotation pitfalls refer to properties and/or classes without a human-readable property; it mainly occurs for external classes defined in the TAIR ontology, such as LexicalConcept or Resource classes. Finally, the unconnected ontology elements pitfall occurs because a defined class is not connected with any other element of the ontology, e.g., the class LexicalConcept is not connected with any other class; the class Concept refers to it but only as their subclass. All the unconnected ontology elements and missing annotation pitfalls reference external vocabularies, e.g., SKOS or RDFS; their definition will be found in the corresponding URL."}, {"title": "6 Conclusion and Future Work", "content": "The Trustworthy AI Requirements (TAIR) ontology provides a basis for capturing and ana-lyzing terms and requirements as concept sets from normative statements from the AI Act and the conformance-focused international standard on AI from SC42. This is made par-tially available as an Open Knowledge Graphs (OKG) resource that allows the links between defined terms, other relevant concepts, and the requirements themselves to be published in a traceable, queryable, and navigable manner.\nAs this work was based on a draft version of the AI Act, we await the publication in early 2024 of the formal text in order to repeat the extraction of concepts and requirements and publish the second version of the TAIR ontology. We will then aim to promote this version of the model and its online exploration features to different potential groups who may find this useful to garner feedback on its utility. Such groups could include subject matter experts in specific high-risk application domains, such as healthcare or education, who may seek to build domain-specific extensions to concepts in this model.\nThe model may be of use to policymakers and standards developers involved in the devel-opment of harmonized standards, in guidelines to support the implementation of the Act, such as EC guidelines to SME developing or public sector agencies procuring AI, and those establishing transparency mechanisms for regulatory learning mechanisms such as regulatory sandboxes and real-life trials. We would also seek feedback from scholars in law, ethics, social science, and information systems on whether this open approach to AI act concepts provides a basis for improving comparison, aggregation, and replication of studies in these areas.\nFurther horizontal requirements mapping will be explored, especially as the SC42 AI Management System Standard (AI MSS) is supported by further standards, including"}]}