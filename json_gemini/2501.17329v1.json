{"title": "ANOMALY DETECTION IN COOPERATIVE VEHICLE PERCEPTION SYSTEMS UNDER IMPERFECT COMMUNICATION", "authors": ["Ashish Bastola", "Hao Wang", "Abolfazl Razi"], "abstract": "Anomaly detection is a critical requirement for ensuring safety in autonomous driving. In this\nwork, we leverage Cooperative Perception to share information across nearby vehicles, enabling\nmore accurate identification and consensus of anomalous behaviors in complex traffic scenarios.\nTo account for the real-world challenge of imperfect communication, we propose a cooperative-\nperception-based anomaly detection framework (CPAD), which is a robust architecture that remains\neffective under communication interruptions, thereby facilitating reliable performance even in low-\nbandwidth settings. Since no multi-agent anomaly detection dataset exists for vehicle trajectories,\nwe introduce 15,000 different scenarios with a 90,000 trajectories benchmark dataset generated\nthrough rule-based vehicle dynamics analysis. Empirical results demonstrate that our approach\noutperforms standard anomaly classification methods in F1-score, AUC and showcase strong ro-\nbustness to agent connection interruptions. The code and dataset will be made publicly available\nat:https://github.com/abastola0/CPAD", "sections": [{"title": "1 Introduction", "content": "Ensuring safety in autonomous vehicles requires identifying abnormal vehicle behaviors or unexpected traffic patterns\nahead of time. To enable large-scale deployment, however, traditional single-vehicle approaches to anomaly detection\nstruggle as they fail to capture the broader context of multi-agent interaction present in real-world driving scenarios.\nMoreover, despite significant advances in sensor technology in recent years, the perception capability of these local\nsensors is ultimately bounded in range and field of view (FOV) due to their physical constraints. Cooperative percep-\ntion(CP) offers a solution by enabling vehicles to share information about their surroundings from their perspectives,\nthus providing a richer overall view of the environment. Modern vehicle and self-driving equipment manufacturers\nlike Tesla, CommaAI, etc. are increasingly shifting towards pure perception models that rely heavily on front-facing\nperception systems. The more abstract these sensors become, the risk of failing to detect anomalous behaviors signifi-\ncantly increases. Thus, CP can be beneficial in these scenarios by accessing additional information that a vehicle might\nmiss, with the assistance of other vehicles. One simple instance might be lane changing when another vehicle is on\nthe blindside, as shown in figure 2. Even though many blindside monitors are prevalent these days, these technologies\nare still immature to some extent, and many might not even have access to these technologies or have limited access\ndepending on the manufacturer. With a CP-based approach, the vehicles observing from behind can help identify this\nabnormal behavior and report or even override lane change prevention to avoid accidents through a common consensus.\nAnother similar instance might be a vehicle driving between lanes or offset from the center due to sensor malfunction\nor performing unnecessary and frequent lane changes than required. More complex scenarios might arise where a\nsingle vehicle perception is insufficient. For instance, a real-world incident captured a driverless Waymo vehicle driving\ntoward oncoming traffic. This behavior might have been easily prevented if surrounding vehicle perspective information"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Collective information sharing", "content": "CP has been recognized for its potential to enhance road safety by facilitating the sharing of raw or processed sensor\ndata through vehicular communicationsAoki et al. [2022]. Another approach includes sharing latent representation\nafter being processed by a feature extractorWang et al. [2024]. CP systems are particularly beneficial in safety-\ncritical and dense traffic environments, as they can increase the situational awareness and safety of both vulnerable\nroad users (VRUs) and in-vehicle users through integration with roadside units (RSUs) and Vehicle-to-Everything\n(V2X) networkShan et al. [2020]. Advances in communication technologies, especially the anticipated capabilities\nof sixth-generation wireless networks, promise to support the sharing of raw perception data over millimeter wave\nfrequenciesZhang and Gao [2022]. However, the extensive bandwidth required for transmitting raw sensory data and the\nneed for real-time communication still introduce significant challenges. These include bottlenecks in data processing\nand latency in achieving consensus among nodes. Current approaches to addressing these challenges often involve\ninference in probabilistic graphical models to facilitate node-wise consensus on predictionsLiu et al. [2012]. Simple\napproaches like voting system Shan et al. [2021], Hassanzadeh et al. [2013] are widely adopted, however, their efficacy\nis still questionable and does not account for temporal dynamics and an agent's confidence."}, {"title": "2.2 Graph-based information modeling", "content": "Graph-based traffic modeling has been widely adopted in traffic forecasting applications. Temporal Graph Convolution\nneural network (T-GCN) has been used to learn complex topological structures to capture the spatial dependence"}, {"title": "2.3 Anomaly detection in Communication constrained Scenarios", "content": "For instance, vehicles in connected systems in regions such as highways are dispersed over a large area with tem-\nporally changing proximity. This can induce communication interruptions and disconnect when agents leave their\ncommunication range. Using multiple data sources is a long sought-after problem Zhao et al. [2006]. To account for\ncommunication constraints, especially with streaming data, various isolation forests have been proposed with PCA as\na preprocessing abstraction Jain et al. [2021]; however fail to account for temporal consistency between sensor data.\nAlippi et al. [2016]. Generative adversarial neural networks (GAN) based approaches have been used to generate a latent\nrepresentation of visual data to account for communication constraintsLi et al. [2020]; however, they fail to account for\njoint information sharing between multiple communication nodes. Federated learning approaches are another way for\njoint training McMahan et al. [2017], Bastola et al. [2024]. Various pruning-based approaches Magnani et al. [2023],\nand diversity-based client selection approaches Bastola et al. [2024] have been proposed to account for communication\nbottlenecks. However, these methods generally operate over single client data for each specific sample in a distributed\nfashion and do not account for joint agent data consensus or temporal correspondence. Thus are completely different\nthan the CP problem we're trying to solve. Based on our knowledge, none of the current works account for all three\nfactors of temporal dynamics, node dynamics and real-time computability at the same time distributed traffic setting.\nOur approach accounts for this gap and proposes a graph transformer-based architecture that accounts for all of the\nabove factors and scalability to large traffic data and performs significantly well under node blackouts."}, {"title": "3 Problem Formulation", "content": null}, {"title": "3.1 Multi-agent configuration", "content": "The key problem in this study is the joint classification of anomalous behavior within a group of vehicles that remain\nunder communication range. We first set up a modified version of the metadrive simulatorLi et al. [2022] to accommodate\nmulti-agent settings. Each vehicle in this setting has its observation in the form of 240 lasers for lidar and 12 lasers for\nlane line detectors and side detecters each. These lasers primarily detect moving objects, sidewalks/solid lines etc. In\nour case, we consider 6 agent settings assumed to be nearby but with occasional or complete disconnect and signal\ncorruption during communication. Our developed model should be able to handle these variable agent scenarios of\nmissing agents or even the addition of new agents, as well as be robust against corrupted signals. The next problem\nwe solve is the trajectory labeling. Defining anomalous trajectories is not straightforward as it involves accounting\nfor factors such as sudden lane change, tailgating, sudden braking etc. As per our knowledge, there are no anomaly\ndetection datasets or benchmarks to facilitate more focused research in this area. We thus design a mixture of rule-based\ntrajectory labelers with human-validated thresholds to categorize each trajectory as anomalous or not. This dataset\ncontains 90,000 different multi-agent trajectories for each agent in 15,000 scenarios that can be easily incorporated to\ntrain for specific anomaly types. These trajectories are divided into 75% normal and 25% anomalous labels. We also\nprovide the code to record and relabel these trajectories based on any desired rule-based anomaly labeling."}, {"title": "3.2 Rule-based anomaly labeling", "content": "We assume we're only given the perception information in the form of Lidar, lane and side detectors. We make this\nassumption to accommodate for the fact that these are fundamental sensors available to almost any autonomous vehicle\none can imagine, and countless other sensors might serve additional functionalities to these sensors. However, since we\ncan access vehicle dynamics information in the simulator, we exploit it to generate targets for trajectory anomalies."}, {"title": "3.2.1 Zigzag driving patterns", "content": "Given the vehicle heading $\\theta_x$ and $\\theta_y$, we detect the curvature of trajectory at any given time as follows:\n\n$k = \\frac{|\\dot{\\theta_x} \\ddot{\\theta_y} - \\dot{\\theta_y} \\ddot{\\theta_x}|}{(\\dot{\\theta_x}^2 + \\dot{\\theta_y}^2)^{3/2}}$ (1)\n\nwhere $\\dot{\\theta_x} = \\frac{d\\theta_x}{dt}$, $\\dot{\\theta_y} = \\frac{d\\theta_y}{dt}$, $\\ddot{\\theta_x} = \\frac{d^2\\theta_x}{dt^2}$, $\\ddot{\\theta_y} = \\frac{d^2\\theta_y}{dt^2}$. We compute these time derivatives based on the temporal window\nsize of 40 and threshold this value based on human observation to detect high-frequency zig-zag patterns."}, {"title": "3.2.2 Sudden Braking", "content": "We calculate the velocity magnitude $V_i = ||v_i||$ for every time point i. Acceleration can thus be calculated using\nfinite differences over a specified window size given by: $A_i = S_{i+w/2} - S_{i-w/2}$ where w is the window size and\n$A_i$ is the acceleration at point i. We then compute the smoothed acceleration using a moving average filter as\n$SA_i = \\frac{1}{w} \\sum_{k=-\\frac{w-1}{2}}^{\\frac{w-1}{2}} A_{i+k}$. Here, the index k runs between the half-windows $-\\frac{w-1}{2}$ to $\\frac{w-1}{2}$, which ensures that the\nwindow is centered at i and includes w terms. This formula assumes that w is odd; if w is even, you might need to\nadjust the limits accordingly or redefine the window slightly to maintain symmetry.\nThen we identify sudden acceleration changes when $Indices = {i | SA_i < \\theta}$, where $\\theta$ represents the braking threshold\nwhich is typically a negative value indicating deceleration."}, {"title": "3.2.3 Sudden turns", "content": "Similarly, measuring lateral acceleration is the best way to detect sudden turns. Given a sequence of heading vectors\n$h_i$ (where i indexes the frame), the angular change $\\Delta\\theta_i$ between consecutive heading vectors can be calculated as:\n$\\Delta\\theta_i = arccos(\\frac{h_{i-1}.h_i}{||h_{i-1}||||h_i||})$, where $h_{i-1}$ and $h_i$ are normalized to unit vectors before computing the dot product. We\nclip values to ensure the argument of arccos remains within its valid domain of [-1,1] to prevent undefined values due\nto floating-point precision issues. The lateral acceleration $a_{lat, i}$ based on the angular change and velocities is computed"}, {"title": null, "content": "using:\n\n$a_{lat, i} = \\Delta\\theta_i \\times V_{i+1}$\n\nwhere $v_{i+1}$ is the magnitude of the velocity vector at frame i + 1, and $\\Delta\\theta_i$ is the angular change from the previous\nframe. To identify frames where the lateral force exceeds a certain threshold $\\tau$ (such as a critical lateral acceleration\nthat might indicate a risk of skidding or rolling in vehicle dynamics), we use:\n\n$Indices = {i | |a_{lat, i}| > \\tau}$\n\nHere, $\\tau$ is set to 0.8 $m/s^2$ but could be adjusted based on empirical data or dynamic conditions."}, {"title": "3.2.4 Frequent lane switching/driving between two lanes", "content": "We detect this specific lane anomaly using lane break points. We trigger a boolean whenever a vehicle crosses a lane\nline and evaluate the crossing frequency to detect anomalous vehicle trajectory. The moving average gives a better\nestimate for this as well. The moving average, M, of the data array D, is computed using a convolution operation. This\noperation involves sliding a window of size w across D, and for each position, computing the average of the elements\nwithin the window given by: $M = D * \\frac{1}{w}1_w$, where, $1_w$ is a vector of ones with length w, * denotes the convolution\noperation and $\\frac{1}{w}1_w$ is the kernel used for the convolution, effectively averaging the elements in each window of size\nw. This is central to smoothing the data, reducing noise, and helping to identify sustained patterns indicative of lane\nchanges. We then identify indices where M exceeds 0.5 :\n\n$Indices = {i | M_i > 0.5}$\n\nWe derive contiguous index ranges as Intervals and then filter these by computing the length of each interval based on a\nthreshold $\\tau$.\n\n$Anomalies = {l(Int.) | Int. \\in Intervals \\land l(Int.) > \\tau}$ (2)\n\nHere, l(Int.) is the length of a specific interval and $\\tau$ is the user-validated threshold indicating significant duration\nindicative of an anomaly in lane change behavior."}, {"title": "3.2.5 Tailgating/ Driving very close", "content": "We use the simulator-provided proximity detectors to detect if any vehicles exist close to a specific threshold to detect\nthese behaviors."}, {"title": "4 Algorithm Design", "content": null}, {"title": "4.1 Graph based Spatial sensor fusion", "content": "Multi-agent systems, in general, focus on the interactions and decision-making processes among autonomous agents\nto maximize common objectives, which may or may not involve CP. In our case of CP, however, we aim to passively\ndetect anomalous behavior using the perception information from multiple vehicles, trying to maximize the correct\npredictions. One note to this integration is the dynamic number of agents. Even though the Deep learning approach for\nthis variation in nodes can be handled with several padding-based architectures, graph-based methods are the widely\nused and most effective onesVelickovic et al. [2017], Wang et al. [2019]. We thus first employ graph-based abstraction\nto capture the spatial correlation between dynamic nodes. Let $h_i^{(m)}$ be the sensor readings vector of vehicle i with\ndimension m. The node features linear transformation is given by:\n\n$h_i^{(n)} = W^{(m\\times n)}h_i^{(m)}$\n\nwhere W is a weight matrix of hidden dimension n, applied to every node, and $h_i^{(n)}$ is the transformed feature. We then\ncompute the attention coefficients $e_{ij}$ that indicate the importance of node j's features to node i as:\n\n$e_{ij} = LeakyReLU(a^T[h_i^{(n)} || h_j^{(n)}])$ (3)\n\nHere, a is a weight vector, and || denotes concatenation. These attention coefficients are then normalized using softmax\nacross all choices of j in the neighborhood $N_i$ of node i, making $a_{ij}$ effectively the weight of the influence of node j\non node i.\n\n$a_{ij} = softmax_j(e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k \\in N_i} exp(e_{ik})}$ (4)\n\nMulti-head attention is then applied, and their features can now be concatenated (denoted by ||) to form the final output\nfeatures for each node. Our method worked well with multi-head attention concat compared to average as we stack\ntwo GAT layers. When we do so, the output of each GAT will be heads(K) \u00d7 the hidden dimension(n). The final\nembedding can thus be computed as follows:\n\n$h_i^{(K \\times n)} = ||_{k=1}^K a_{ij}^{(k)}h_j^{(k)}$ (5)\n\nWe use multiple of this Graph transformation followed by ReLU activation for each stack. The final layer after stacking\nis the linear layer followed by the global mean pooling layer given by:\n\n$\\tilde{h} = W'(\\frac{1}{N} \\sum_{i=1}^N h_i) + b, where; h_p = \\frac{1}{N} \\sum_{i=1}^N h_i$ (6)\n\nThis computation is also demonstrated in figure 3. A major benefit of this graph-based abstraction module is its ability\nto handle a changing number of nodes and fully connected information sharing between nodes. This helps derive\ninformation from other agents if it's unavailable to that specific agent."}, {"title": "4.2 Sequential Binary Classifier", "content": "For each timestep t, we do the Graph-based spatial sensor fusion as in 4.1 to generate a final sequence $h_1^t, h_2^t, ...,h_n^t$.\nThis sequence is then fed to the transformer encoder along with position encoding to generate a final latent representation\n$h_{1:T}^G$ as follows:\n\n$h_{1:T}^G = Transformer(h_1^t, h_2^t, ..., h_n^t)$ (7)\n\nThe sequence $h_{1:T}^G$ is passed through an attention pooling layer, which computes a weighted sum of the features, where\nthe weights are learned attention scores given by :\n\n$AttentionPooling(x') = \\sum_{t=0}^T softmax(W_{ap}. x'_t) . x'_t$\n\nwhere $x'_t$ represents each vector in the sequence $h_{1:T}^G$, and $W_{ap}$ is the learned weight vector from a linear transformation\napplied to each $x'_t$. The pooled output is then fed into a binary classifier, which outputs the final prediction:\n\n$Classifier(z) = \\sigma(W_c. z + b_c)$\n\nwhere z is the prediction logit from the attention pooling, $\\sigma$ denotes the sigmoid activation function, $W_c$ and $b_c$ are the\nweights and biases for the classifier. Sigmoid is applied to this logit, and a threshold of 0.5 is used to make the binary\nclass prediction to decide anomaly."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Baseline Comparisions", "content": "We show that our proposed Graph-Transformer architecture performs better than some of the most popular anomaly\nclassifiers like LOFBreunig et al. [2000], LSTMHochreiter [1997], Decision TreeQuinlan [1986] and supervised-\nautoencoders(SP-AE)Le et al. [2018]. We evaluate our model in terms of F1-Score, AUC, Precision, Recall and\nMatthews correlation coefficient(MCC), given the imbalanced nature of our training data. Figure 6 shows the ROC\nperformance comparison."}, {"title": "5.2 Robustness under imperfect communication", "content": "The second set of experiments includes random step-wise and sequential node blackouts. These simulate real-world\ncommunication interruptions or packet loss. The results demonstrate that our model can perform significantly well up\nto 10% of communication loss with just a negligible loss on the F1-score while still maintaining 0.68 under step-wise\nnode blackouts. Our model performs well even under 25% information loss, still maintaining an F1-score above 0.6 as\nshown in the table 2.\nThe sequential blackout occurs in random timestep blocks with a maximum block size of 10 timesteps. This case\nis more extreme than random blackouts as it overlaps most anomalous trajectory segments and contributes to major\ninformation loss. Our model performs significantly well even in this scenario for up to 15% intensity while maintaining\nan F1-score above 0.6. These results are shown in table 3\nThese experiment results indicate a significant robustness of graph-based training for cooperative information sharing\namong nodes.\nThe SP-AE is derived from the training strategy similar to Le et al. [2018] where we combine reconstruction loss and\nclassification loss as an auxiliary guide."}, {"title": "6 Conclusion", "content": "In our study, we introduced a robust multi-agent framework for anomaly detection in cooperative vehicle perception\nsystems under imperfect communication conditions. Our proposed Graph-Transformer architecture, complemented by\na newly curated benchmark dataset of 15,000 vehicle trajectories, demonstrates superior performance in identifying and"}]}