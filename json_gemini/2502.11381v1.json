{"title": "Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for UAV-View Geo-Localization", "authors": ["Zhongwei Chen", "Zhao-Xu Yang", "Hai-Jun Rong"], "abstract": "UAV-View Geo-Localization (UVGL) aims to ascertain the precise location of a UAV by retrieving the most similar GPS-tagged satellite image. However, existing methods predominantly rely on supervised learning paradigms that necessitate annotated paired data for training, which incurs substantial annotation costs and impedes large-scale deployment. To overcome this limitation, we propose the Dynamic Memory-Driven and Neighborhood Information Learning (DMNIL) network, a lightweight end-to-end self-supervised framework for UAV-view geo-localization. The DMNIL framework utilizes a dual-path clustering-based contrastive learning architecture as its baseline to model intra-view structural relationships, enhancing feature consistency and discriminability. Additionally, a dynamic memory-driven hierarchical learning module is proposed to progressively mine local and global information, reinforcing multi-level feature associations to improve model robustness. To bridge the domain gap between UAV and satellite views, we design an information-consistent evolutionary learning mechanism that systematically explores latent correlations within intra-view neighborhoods and across cross-view domains, ultimately constructing a unified cross-view feature representation space. Extensive experiments on three benchmarks (University-1652, SUES-200, and DenseUAV) demonstrate that DMNIL achieves competitive performance against state-of-the-art supervised methods while maintaining computational efficiency. Notably, this superiority is attained without relying on paired training data, underscoring the framework's practicality for real-world deployment.", "sections": [{"title": "I. INTRODUCTION", "content": "UAV-VIEW Geo-Localization (UVGL) aims to accurately infer the geographic location of a query image by retrieving reference images captured from different platforms[1]. In GPS-denied environments, aligning drone-captured near-ground perspective images with GPS-tagged satellite images enables precise geo-localization of scenes. This capability is essential for various applications, including autonomous navigation, disaster response, and surveillance.\nHowever, existing UVGL methods adopt supervised learning frameworks that require explicitly paired UAV-satellite image datasets for training[2], [3], [4]. While achieving empirical success, these approaches face two fundamental limitations: 1) The labor-intensive annotation of pixel-level or instance-level UAV-satellite pairs incurs prohibitive scalability costs[5], and 2) Vast repositories of unpaired and unlabeled cross-view imagery remain unexploited due to the dependency on supervision. More critically, when deployed in unseen geographical regions or dynamic environments (e.g., urban construction sites), supervised models necessitate exhaustive data re-annotation to adapt to domain shifts, severely hindering real-world applicability. Considering these limitations, we aim to explore an unsupervised approach to UAV-view geo-localization (UUVGL), a paradigm that leverages self-supervised learning to autonomously discover latent cross-view correlations from large-scale unannotated data.\nAlthough the label-free UUVGL approach demonstrates promising potential, it presents substantially greater challenges compared to other unsupervised image retrieval tasks, such as unsupervised person re-identification (UReID)[6], as illustrated in Fig.1. The UUVGL paradigm must address two"}, {"title": "II. RELATED WORKS", "content": "In this section, we provide a overview of the cross-view geo-localization task and the commonly used methods in UIR, laying the theoretical foundation for UUVGL."}, {"title": "A. UAV View Geo-localization", "content": "The Cross-view Geo-localization (CVGL) task[33], [34], [35] focuses on image retrieval, aiming to identify matching images from a reference image database based on a given query image. This task is particularly challenging as the query and reference images are taken from different viewpoints or sensors. In the field of CVGL, numerous approaches have been proposed to tackle difficulties associated with scenarios such as based UAV view and ground view geo-localization.\nUAV view geo-localization[36], [37] has made significant strides, with feature representation being crucial for cross-view tasks. LPN[38] mined fine-grained features through local pattern partitioning, while IFSs[35] enhanced representation by integrating global and local features via a multi-branch strategy. However, challenges remain in addressing spatial discrepancies and ensuring feature consistency. CAMP[34] tackled these issues with contrastive attribute mining and position-aware partitioning, while DAC[4] used domain alignment and scene consistency constraints to improve feature consistency. MEAN[2] incorporates progressive embedding diversification, global-to-local associations, and cross-domain enhanced alignment, improving feature representation and alignment under significant viewpoint and scale variations. Although recent advancements in UVGL methods, their training processes largely rely on paired annotated datasets. This heavy dependence on labeled data significantly limits their applicability in large-scale scenarios with unlabeled data. Currently, there are no effective methods to address this limitation. Therefore, we propose an self-supervised UVGL framework designed to leverage unlabeled data, thereby reducing the reliance on extensive annotations."}, {"title": "B. Ground View Geo-localization", "content": "Ground view geo-localization[39], [40] techniques have recently employed various methods to address challenges such as viewpoint variations and spatial misalignments.Early approaches, such as CVM-Net[41], used shared feature spaces to embed global descriptors, while GAN-based methods reduced visual discrepancies through view synthesis. However, these relied on fixed alignment, leading to overfitting and limited generalization. Recent methods introduced refinement and advanced architectures. TransGeo[42] employed attention-based zooming and sharpness-aware optimization to improve detail learn and generalization, while GeoDTR[43] disentangled geometric features to handle perspective shifts using a Transformer-based extractor. Hard negative mining strategies, such as those in Sample4Geo[39], further improved model discriminability by focusing on challenging negatives.\nIn recent years, to address the dependency on paired annotations between ground and satellite images, the literature has proposed an unsupervised ground view geo-localization method [30] that utilizes Correspondence-Free Projection (CFP) to transform ground panorama images into bird's-eye-view (BEV) images, and applies CycleGAN[44] to reduce the visual discrepancies between ground and satellite images, while introducing a threshold-based dynamic pseudo-label updating mechanism. However, this explicit alignment approach has limitations in modeling the deep semantic relationships of cross-view features and heavily relies on the quality of CycleGAN, which may introduce artifacts and inconsistencies, affecting pseudo-label accuracy. While multi-stage training improves pseudo-label quality, it increases training time and computational cost, reducing the efficiency and practical applicability of the method. Additionally, this approach may struggle with generalization in cross-domain transfer or new scenarios.This issue motivates us to explore a fully end-to-end framework in CVGL methods, aimed at deeply mining the underlying structures and potential relationships of cross-view features while avoiding the computational resource consumption associated with multi-stage training."}, {"title": "C. Unsupervised Image Retrieval", "content": "In recent years, UIR methods have garnered significant attention primarily due to their ability to eliminate reliance on large-scale manually annotated data. These methods achieve effective retrieval by exploring the inherent latent structures within the data to learn discriminative feature representations.\nIn UIR, clustering of data features is typically employed, with cluster assignments or cluster centers used as pseudo-labels to drive network learning. Representative works include DeepCluster [45] and DeepCluster-v2 [46], which enhance feature quality through iterative optimization involving feature extraction and K-means clustering. However, the quality of pseudo-labels significantly impacts retrieval performance, especially in cross-view and cross-modal scenarios where reliable cross-modal label mappings are essential. To address this, SDPL[47] introduces the Collaborative Ranking Association (CRA) module, which combines shallow and deep features to construct cross-modal ranking relationships, extracts reliable pseudo-labels through cross-validation of ranking results, and incorporates an intra-modal ranking smoothing mechanism to reduce noise interference, thereby significantly enhancing the quality and stability of pseudo-labels.\nIn contrast to clustering-based approaches, contrastive learning methods typically enhance feature discrimination by maximizing the similarity between different augmented views of the same image (positive pairs) and minimizing the similarity between different images (negative pairs). Early methods such as InstDisc[48] and AMDIM[49] primarily rely on large sample dictionaries or multi-view feature interactions to learn discriminative representations. The MoCo[50] series introduces a momentum update mechanism that maintains a continuously updated feature pool during training, effectively alleviating the dependence on negative sample diversity in small-batch training. SimCLR[51] further combines large-scale small-batch parallel training with data augmentation strategies, significantly improving the performance of contrastive learning. Cluster Contrast[31] generates clusters and their centers through initial clustering and introduces image-cluster center"}, {"title": "III. METHOD", "content": "As illustrated in Fig.3, the DMNIL framework integrates dynamic hierarchical memory learning and information consistency evolution learning modules, along with a cross-domain pseudo-label optimization strategy. The framework employs a weight-shared dual-stream ConvNeXt-Tiny network as its backbone architecture, utilizing a cluster memory mechanism to perform contrastive learning on features extracted from different views, serving as the reference network. To mine the dynamic behavior of intra-class features under variations in viewpoint and scale, the framework establishes a collaborative relationship between short-term and long-term memory through the dynamic hierarchical memory learning module, enabling an in-depth analysis of the dynamic changes in local features and enhancing global consistency among features. Simultaneously, the information consistency evolution learning module leverages training image features stored in instance memory to design a feature distribution-based neighborhood dynamic constraint method, which combines local neighborhood consistency rules with mutual information optimization strategies. This guides both intra-view and cross-view features to form unified representations in high-dimensional space, further uncovering the latent relationships and mapping patterns across views. Additionally, the framework incorporates a dual-joint feature-driven pseudo-label consistency enhancement mechanism to iteratively improve the reliability and semantic accuracy of pseudo-labels.\nProblem Formulation: In the UUVGL task, we are given a dataset of images taken from different platforms (e.g., drones and satellites), denoted by {xd, xs}. Here, xd represents drone-view images and xs represents satellite-view images. Unlike the supervised setting, this task providesneither explicit labels nor paired matching data, meaning we do not know which drone-view images correspond to which satellite-view images, and there is no prior knowledge about location or semantic category. During model training, we apply random sampling to drone images and satellite images separately to simulate the absence of pairwise correspondences under this unsupervised setting. Our goal is to learn robust cross-view feature representations from large-scale unlabeled data under completely label-free and pair-free conditions by leveraging methods such as contrastive learning, clustering, or self-supervised learning. By utilizing these shared semantic space representations during the inference stage, the model can effectively retrieve and match images of the same location across different views, enabling accurate geo-localization without relying on any manual annotations or prior correspondence information."}, {"title": "A. Cluster Memory Contrastive Baseline", "content": "Cluster-based contrastive learning strategies have achieved notable success in unsupervised retrieval tasks[52]. However, in cross-geo-view tasks, due to the viewpoint imaging gap and viewpoint discrepancy, single-path cluster-based contrastive learning strategies struggle to effectively address cross-view discrepancies, lack view-specific information utilization, suffer from insufficient feature alignment, and fail to fully leverage the complementarity of different views. To address these challenges, this paper introduce a dual-path cluster-based contrastive learning method as our baseline network, which leverages two independent feature streams (drone and satellite views) for contrastive learning to enhance cross-view consistency. Specifically, drone and satellite images are fed into a shared backbone network, and then processed through their respective cluster-based contrastive learning paths to learn view-specific information and shared view features. Finally, the view-specific memories are updated via a momentum update strategy.\nTo facilitate the description of our method, we first introduce the notation. Let Xa = {x_i}^N_{i=1} denote a set of drone images with N instances, and Xs = {x_i}^M_{i=1} denote a set of satellite images with M instances. Fa = {f_i}^N_{i=1} and Fs = {f_i}^M_{i=1} represent the feature representations extracted by the shared feature extractor Fbackbone for the drone and satellite views, respectively. qd and qs denote the drone query instance features and satellite query instance features extracted by the feature extractor Fbackbone. The processing procedure of the backbone can be described as follows.\n$f_j = F_{backbone}(x_i),  \\ \begin{cases} i \\ i \\ \\\\ \\end{cases}  \\left\\{\\begin{matrix} i \\in \\left\\{ 1,2,..., N \\right\\} , if j = d \\\\ i \\in \\left\\{ 1,2,..., M \\right\\} , if j = s \\end{matrix}\\right. $\nCross-View Feature Memory Initialization. At the beginning of each training epoch, the feature memories for both views are initialized. Specifically, the cluster representations of the drone view {\\phi_1^d, \\phi_2^d,\u2026\u2026,\\phi_K^d} and the satellite view {\\phi_1^s, \\phi_2^s,\u2026\u2026,\\phi_L^s} are stored in the drone memory dictionary Ma and satellite memory dictionary Ms, respectively, for view-specific memory initialization. This process can be formalized as:\n$M_d \\leftarrow \\left\\{ \\phi_1^d, \\phi_2^d,\u2026\u2026,\\phi_K^d \\right\\}$\n$M_s \\leftarrow \\left\\{ \\phi_1^s, \\phi_2^s,\u2026\u2026,\\phi_L^s \\right\\}$\n$\\phi_k^d = \\frac{1}{\\left| H_k^d \\right|} \\sum_{i\\in H_k^d} f_i^d$\n$\\phi_l^s = \\frac{1}{\\left| H_l^s \\right|} \\sum_{i\\in H_l^s} f_i^s$\nwhere, n = 1,\u2026\u2026, N, m = 1,\u2026, M, k = 1,\u2026, K, l = 1,..., L, H_k^{d(s)} denotes the k-th cluster set in drone or satellite view, indicates the number of instances per cluster.\nCross-View Feature Memory Updating. During the training phase, we randomly sample P distinct localization points from the training set, with each point comprising a fixed number Z of instances. This sampling strategy results in a minibatch containing a total of P \u00d7 Z query images. Subsequently, the"}, {"title": "B. Dynamic Hierarchical Memory Learning", "content": "In CVGL tasks, baseline networks can rapidly respond to local environmental changes and learn subtle differences in recent scenes. However, they are susceptible to transient noise and local anomalies, leading to instability in feature representation. Moreover, due to the complexity introduced by geographic variations, baseline networks struggle to extract globally consistent and stable features effectively. To address these challenges, we propose a Dynamic Hierarchical Memory Learning (DHML) method to achieve a balance between instantaneous adaptation and long-term stability.\nConsistent with the baseline network, we construct the dynamic hierarchical memory learning in a dual-path manner, using drone feature extraction as an illustrative example. To build the long-term memory, we first construct two novel clustering memory dictionaries according to Eq.2 and Eq.3, which are used to store the short-term M\u2081 and long-term Ma memories, respectively.\nM_a^d \\leftarrow \\left\\{ \\phi_1^{ds}, \\phi_2^{ds},\u2026\u2026,\\phi_K^{ds} \\right\\}\nM_a^a \\leftarrow \\left\\{ \\phi_1^{dl}, \\phi_2^{dl},\u2026\u2026,\\phi_K^{dl} \\right\\}\nwhere, \\phi_1^{dl}, \\phi_2^{dl},\u2026\u2026,\\phi_K^{dl} represent the short-term memory embeddings that encode the cluster representations of the drone view, and \\phi_1^{ds}, \\phi_2^{ds},\u2026\u2026,\\phi_K^{ds} represent the long-term memory embeddings that store historically accumulated stable representations of the drone view.\nUnlike the weight updating scheme in Eq.6 and Eq.7, we assign different weights to the input features and the historical"}, {"title": "C. Information Consistency Evolution Learning", "content": "By integrating the baseline network with the dynamic hierarchical memory learning module, the proposed framework exhibits a certain capability for cross-view consistency learning. However, both the baseline network and the dynamic hierarchical memory learning module primarily focus on intra-view feature learning, even though the drone-view and satellite-view features share a common backbone network. As a result, the deep correlations between features from different views remain insufficiently explored, limiting the model's ability to fully mitigate and bridge the challenges posed by drastic viewpoint variations.\nTo address this limitation, we propose an Information Consistency Evolution Learning (ICEL) module, as illustrated in Fig.3. This module leverages a threshold-based filtering and ranking-driven neighborhood selection strategy to reinforce intra-view consistency while simultaneously enabling cross-view feature alignment. By facilitating consistency evolution across views, ICEL enhances the discriminative power and relational modeling of both intra-view and cross-view feature representations, thereby improving the model's robustness in complex geo-localization scenarios.\nIn the ICEL module, unlike the baseline and DHML module, we employ instance-level features instead of cluster-level representations. To achieve this, we define two instance memory dictionaries, Id and Is, which store the instance features extracted by the shared feature encoder, as described in Section III-A. This process can be formally expressed as follows:\nI_a \\leftarrow \\left\\{ f_1^d, f_2^d,...,f_N^d \\right\\}\nI_s \\leftarrow \\left\\{ f_1^s, f_2^s, ...,f_M^s \\right\\}\nwhere fd and fo denote the drone and satellite instance features stored in memory dictionaries Ia and Is, respectively. In addition, we define the similarity between a given drone (or satellite) query qz d(s) and each drone (or satellite) instance fd(s) in the training set as:\n$S(q_z^{d(s)}, f_v^{d(s)}) = \\frac{q_z^{d(s)}.f_v^{d(s)}}{\\left\\| q_z^{d(s)} \\right\\|_2\\left\\| f_v^{d(s)} \\right\\|_2}$"}, {"title": "D. Pseudo-Label Enhancement", "content": "In an unsupervised learning scenario, the quality of pseudo-labels exerts a decisive influence on model performance. Inspired by [47], this study proposes a pseudo-label optimization framework based on dynamic feature robustness. Specifically, small perturbations are introduced into the original feature representation to form a perturbed feature space, and the Top-K nearest neighbor consistency between the original and perturbed features is employed to identify high-confidence samples while filtering out noisy or less discriminative instances. Subsequently, leveraging the inter-sample similarity matrix in the feature space, a spatial smoothing operation is performed on the initially generated pseudo-labels, thereby enforcing local label consistency among neighboring samples.\nWe first employe small Gaussian noise ed, es ~ \u039d(0, \u03c32\u0399) into the original satellite instances feature and the original drone instances features to obtain perturbed features, which are used for more robust similarity estimation and subsequent cluster consistency evaluation. specifically:\nf_e^d = f_d + \\epsilon_d, f_e^s = f_s + \\epsilon_s,"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "To evaluate the effectiveness of our proposed cross-view geo-localization framework, we conduct experiments on three benchmark datasets-University-1652 [9], SUES-200 [55], and DenseUAV [53]. These datasets present complementary challenges for multi-view image matching and retrieval.\nUniversity-1652 is a large-scale cross-view geo-localization dataset encompassing 1,652 locations from 72 universities worldwide. It contains multi-view images-drone, satellite, and ground-level. Of the total 1,652 buildings, 701 from 33 universities form the training set, while 951 from 39 different universities (with no geographic overlap) comprise the test set. As the first dataset to include drone-view for cross-view geo-localization, it facilitates matching across ground, drone, and satellite perspectives.\nSUES-200 centers on altitude diversity in aerial imagery. It includes 200 distinct locations, split into 120 for training and 80 for testing. Each location provides one satellite image and aerial images taken at four altitudes (150m, 200m, 250m, and 300m), covering diverse environments such as urban areas, natural landscapes, and water bodies. SUES-200 thus evaluates cross-view retrieval performance under varying flight heights, reflecting real-world UAV scenarios.\nDenseUAV targets UAV self-positioning in low-altitude urban environments, featuring over 27,000 UAV and satellite images from 14 university campuses. It includes dense sampling, multi-scale satellite images, and varying altitudes (80m, 90m, 100m) across multiple temporal conditions. The training set consists of 6,768 UAV images and 13,536 satellite images, while the test set contains 2,331 UAV images and 4,662 satellite images, with no geographic overlap. DenseUAV's challenges include cross-view matching, neighbor confusion, and spatial-temporal variation, making it a key benchmark for UAV-based localization in GPS-denied settings.\nWe evaluate the models using Recall@K (R@K) and Average Precision (AP). R@K measures the proportion of correct matches within the Top-K results, while AP balances precision and recall. Model efficiency is assessed through parameter count, reflecting portability under resource constraints. Comparisons are made using the optimal model configurations for each method."}, {"title": "B. Implementation Details", "content": "We adopt a symmetric sampling strategy for selecting input images. A ConvNeXt-Tiny model, pre-trained on ImageNet, serves as the backbone for feature extraction, with a newly added classifier module initialized via the Kaiming method. During both training and testing, all input images are resized to 3 x 384 \u00d7 384. We employ several data augmentation techniques, including random cropping, random horizontal flipping, and random rotation. The batch size is set to 64, and DBSCAN [10] is used to generate pseudo labels. For optimization, we use the SGD optimizer with an initial learning rate of 0.001, training the model for 30 epochs in total. All experiments are conducted under the PyTorch framework on an Ubuntu 22.04 system equipped with four NVIDIA RTX 4090 GPUs. Additionally, in the University-1652 and SUES-200 datasets, each location provides only a single satellite image; we therefore replicate each satellite feature 50 times for clustering purposes, leaving UAV features unchanged. In DenseUAV, we replicate UAV features four times at each altitude, and replicate the satellite features three times for each time point and altitude."}, {"title": "C. Comparison with State-of-the-art Methods", "content": "Results on University-1652: As shown in Table I, our proposed method, DMNIL, demonstrates outstanding performance, achieving competitive results without relying on paired matching data and labels. In both Drone Satellite and Satellite Drone settings, it attains R@1/AP of 90.17%/91.67% and 95.01%/88.95%, respectively, outperforming several fully supervised methods. Furthermore, we utilize unsupervised pre-trained weights and perform fine-tuning only on the ConvNeXt-Tiny backbone network with"}, {"title": "D. Ablation Studies", "content": "To comprehensively evaluate the rationale behind our proposed method, we conducted an ablation study on the University-1652 dataset. We systematically analyzed the impact of each proposed module and explored their effectiveness in different configurations to better understand their contributions to the cross-geographic view matching task. The experimental results are shown in Table V.\nBackbone: Basic Feature Extraction Capability. The Backbone refers to the performance of the ConvNeXt-Tiny network without training, serving as a direct feature extractor. As shown in Table V, its performance is significantly limited, achieving an R@1 of only 9.55% and 35.24%, and an AP of 11.88% and 13.20% for the Drone \u2192 Satellite and Satellite \u2192 Drone tasks, respectively. This result confirms that the backbone alone lacks the capability to extract discriminative features, highlighting the necessity of additional learning mechanisms.\nBaseline: Dual-Path Contrastive Learning for Feature Optimization. Incorporating a Baseline model with dual-path cluster-based contrastive learning significantly enhances feature discrimination. The R@1 values rise to 71.06% and 81.31%, while AP improves to 74.91% and 65.26%. These improvements demonstrate that contrastive learning strengthens intra-view feature discrimination but remains insufficient for capturing cross-view consistency.\nDynamic Hierarchical Memory Learning (DHML): Enhancing Global and Local Consistency. To further address consistency between cross-views, we propose DHML. This module"}, {"title": "E. Visualization", "content": "Feature space and similarity distribution. To further illustrate the effectiveness of the our method, We perform feature space (t-SNE[56] map) and similarity distribution visualization for DMNIL, as presented in Fig 4.Comparing (a) Baseline and (b) Our Method, it is evident that our approach enhances the clustering of cross-view positive samples, effectively bringing the drone and satellite representations of the same location closer together. Additionally, the separation between different locations is more distinct, indicating better feature discriminability. The second row shows the similarity distribution between drone-satellite positive and negative pairs. Our method"}, {"title": "V. CONCLUSION", "content": "This paper investigates a highly valuable and challenging task namely cross-view geo-localization based on UAV-view, aiming to reduce reliance on costly paired matching data. To address the challenges posed by cross-view discrepancies, we propose a lightweight unsupervised framework for UAV-view geo-localization, incorporating dynamic hierarchical memory learning and information consistency evolution learning. This framework unifies intra-view and cross-view feature representations in a high-dimensional space, further revealing"}]}