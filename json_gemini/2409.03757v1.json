{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding", "authors": ["Yunze Man", "Shuhong Zheng", "Martial Hebert", "Liang-Yan Gui", "Zhipeng Bao", "Yu-Xiong Wang"], "abstract": "Complex 3D scene understanding has gained increasing attention, with scene en-\ncoding strategies playing a crucial role in this success. However, the optimal scene\nencoding strategies for various scenarios remain unclear, particularly compared to\ntheir image-based counterparts. To address this issue, we present a comprehensive\nstudy that probes various visual encoding models for 3D scene understanding,\nidentifying the strengths and limitations of each model across different scenarios.\nOur evaluation spans seven vision foundation encoders, including image-based,\nvideo-based, and 3D foundation models. We evaluate these models in four tasks:\nVision-Language Scene Reasoning, Visual Grounding, Segmentation, and Regis-\ntration, each focusing on different aspects of scene understanding. Our evaluations\nyield key findings: DINOv2 demonstrates superior performance, video models\nexcel in object-level tasks, diffusion models benefit geometric tasks, and language-\npretrained models show unexpected limitations in language-related tasks. These\ninsights challenge some conventional understandings, provide novel perspectives\non leveraging visual foundation models, and highlight the need for more flexible\nencoder selection in future vision-language and scene-understanding tasks.", "sections": [{"title": "1 Introduction", "content": "Recently, complex 3D scene understanding has emerged as a pivotal area in computer vision,\nencompassing tasks such as scene generation [24, 25, 26, 33, 74], reasoning [5, 35, 52, 55], and\ninteraction [36, 108]. Leveraging large-scale vision foundation models, approaches like [42, 64, 68,\n84, 91] have achieved promising results, thereby enabling a wide range of real-world applications,\nfrom autonomous driving [54, 75, 79, 112], robotics [57, 108], to multi-modal agents [1, 78].\nWhile numerous studies [6, 67, 99] have provided guidance on the use of vision foundation models\nfor 2D image-based tasks, the strategies for 3D scenarios remain unclear. A systematic understanding\nof complex real-world scenarios involves not only semantic and depth awareness [6], which is\npossible to evaluate within the 2D domain, but also geometric awareness and the ability to align with\nmulti-modal information for reasoning and grounding tasks. To address this gap, our work evaluates\nthe use of different types of visual foundation models for complex scene understanding and seeks to\nidentify the strengths and limitations of each model in different scenarios. Ultimately, this study aims\nto contribute to the development of more effective and efficient scene understanding systems.\nConcretely, we aim to address several key questions. First, given that most vision foundation models\nare trained on image or video data, we want to determine whether 2D foundation models can\neffectively interpret 3D scenes. Second, since video models inherently contain temporal information\nthat captures aspects of the 3D structure as well, we investigate whether they lead to better 3D feature"}, {"title": "2 Related Work", "content": "Our work is closely related to methods that focus on extraction of features from images, videos, and\n3D assets, as well as learning joint spaces for vision-language fusion. A large body of recent literature\nhas explored the representation learning for multi-modal visual inputs and their complementary\nperformance in image understanding. In contrast, our paper presents a comprehensive analysis\nof the use of pretrained visual encoders for zero-shot 3D scene understanding. To the best of our"}, {"title": "3 Probing Visual Encoders for Scene Understanding", "content": "The objective of our Lexicon3D is to evaluate different visual foundation models in complex scene\nunderstanding tasks. We first construct a unified architecture capable of probing different visual foun-\ndation models on a spectrum of downstream tasks. Then, we break down the 3D scene understanding"}, {"title": "3.1 A Unified Probing Framework", "content": "We design a unified framework, as shown in Figure 2, to extract features from different foundation\nmodels, construct a 3D feature embedding as scene embeddings, and evaluate them on multiple\ndownstream tasks. For a complex indoor scene, existing work usually represents it with a combination\nof 2D and 3D modalities. For realistic scenarios [15, 20, 94], videos are usually first captured\nwith handheld cameras and then 3D points are obtained from reconstruction algorithms such as\nCOLMAP [72]. For digital and synthetic scenarios [69, 109], 3D assets are designed and generated\nfirst, before images and/or videos are rendered within the created space. Given a complex scene\nrepresented in posed images, videos, and 3D point clouds, we extract their feature embeddings with a\ncollection of vision foundation models. For image and video-based models, we project their features\ninto the 3D space for subsequent 3D scene evaluation tasks with a multi-view 3D projection module.\nFollowing [21, 34, 35, 63], for a point cloud P, this module produces features $f_p$ for each point\n$p\\in P$ given image features f and the pose and camera information K, R. We first project all points\nonto the image plane to obtain their corresponding pixel features. Concretely, for a point p, we obtain\nits projected pixel u on the image i with\n$\\tilde{u} = K \\cdot R \\cdot p$, $\\tilde{u}$, p represent homogeneous coordinates of u, p.\nIn addition, we use an indicator function $I(p, i)$ to represent whether a point p is visible in the image\nof the i-th frame. After finding corresponding pixels of the given point in all image frames, we use\nmean pooling as an aggregation function $\\Phi$ to fuse all pixel features to form the point feature $f_p$.\nAssuming there are M images in total, the projection and aggregation process is represented as:\n$f_p = \\Phi_i (I(p, i) \\cdot f_i(K_i R_i p))$.\nAfter projection, we obtain 3D feature fields represented as point cloud feature embeddings for each\nvisual foundation model. We use these as input to the shallow probing heads to evaluate various"}, {"title": "3.2 Vision-Language Reasoning", "content": "The vision-language reasoning task requires a model to engage in dialogues or answer questions\nabout global understanding and local concepts and objects related to a given complex 3D indoor"}, {"title": "3.3 Visual Grounding", "content": "Visual grounding is the task of locating an object in a 3D scene based on a text description. Compared\nto the 3D VQA task, visual grounding places a greater emphasis on object-level reasoning and\nmatching capabilities. The task can be broken down into two sub-tasks: object detection and target\ndiscrimination (matching the text description with the target object). Although some methods focus\non learning models to tackle both tasks [16, 104], others primarily focus on the discrimination\nproblem [2] by assuming access to ground-truth bounding boxes. For simplicity and to prevent task\nentanglement, we adopt the latter setting in our evaluation. More specifically, given a 3D scene in\nthe form of multi-view images and point clouds, a free-form language description of objects, and\nthe ground-truth 3D bounding boxes of all objects in the scene, our model's objective is to find the\ncorrect objects in the scene that match the language description. We believe that the object detection\ntask requires semantic information from the visual encoder, which is similar in nature to the semantic\nsegmentation task and will be analyzed in Section 3.4.\nFor the target discrimination task, we first obtain the feature for each object in the scene by taking the\naverage pooling of all points inside its ground truth bounding box. Following Multi3DRefer [104],\nwe use a CLIP text encoder to tokenize the text description, and adopt the attention head in [104] to\nfuse the text and visual embeddings from the previous steps and output an object score."}, {"title": "3.4 Semantic Segmentation", "content": "Semantic segmentation is the task of predicting semantic labels at each 3D position, which requires\nfine-grained semantic awareness of the scenes. As mentioned in Section 3.1, all types of features are\nunified in the form of point clouds; therefore, semantic labels are predicted for each point within the\npoint cloud in our setting. More specifically, given a 3D scene in the form of multi-view images and\npoint clouds, the objective in this task is to predict the semantic label for every point in the cloud.\nTo make the semantic prediction performance better reflect the fine-grained semantic\nunderstanding capability of different features, we use a single linear layer followed by a Sigmoid\nfunction to perform a linear probe to predict the probability distribution $y \\in R^{N\\times C}$ for all the labels\nfrom the foundation model feature $x \\in R^{N\\times d}$: $y = Sigmoid(F_C(x))$, where N is the number of\npoints in each point cloud, d is the feature dimension, and C is the number of classes for segmentation.\nWe adopt the standard Adam optimizer [41] with a learning rate of 1e-4 and use a cross-entropy loss\nto train the linear layer for 20 epochs."}, {"title": "3.5 Registration: Geometric Correspondence", "content": "To evaluate the geometric information contained in the foundation model features, we design the\nfollowing new task, partial scene registration, based on the point cloud registration [49, 95]\ntask. From a complete point cloud representing the entire scene, we sample a pair of point clouds\n$P_1 \\in R^{N_1\\times 3}$ and $P_2 \\in R^{N_2\\times 3}$ within the scene, where $P_1$ and $P_2$ contain all the points that can be\nobserved in two sets of consecutive views, respectively. Our goal is to find the homography matrix H"}, {"title": "4 Analysis", "content": "The purpose of this section is to provide additional exploration towards the optimal usage of visual\nfoundation models. The selection of encoding methods requires consideration of the trade-off between\nmemory usage, running time, and performance. We will dive into complexity analysis and the study\nof design choices for various and a combination of foundation models."}, {"title": "4.1 Complexity Analysis", "content": "We compare memory usage, computation time, and model performance (vision-language reasoning on\nScanQA) in Our findings show that image encoders generally require less time\nto process a sample compared to video and 3D encoders. And diffusion-based models, when used for\nfeature extraction, require significantly more memory than other discriminative models. Noticeably,\nthe drawbacks in running time become evident for 2D backbones, especially image encoders, when\nattempting to obtain a scene embedding by aggregating multi-view image embeddings. To illustrate\nthis, we consider a 300-frame video as an exemplar of posed 2D information for a complex scene\n(a 10-second video at 30 FPS). As the length of the video increases, 2D methods, which necessitate\nfeature extraction for each image frame, rapidly consume a substantial amount of time to process\na single scene. In contrast, a 3D point encoder requires significantly less time to process a scene.\nNevertheless, 3D encoders exhibit relatively poor model performance, which can be attributed to the\nscarcity of training data. To fully demonstrate their potential in scene understanding tasks, efforts\nshould be directed toward enhancing the generalizability of 3D foundation models. All analyses and\ncomputations were conducted on an Nvidia A100 GPU."}, {"title": "4.2 Ablation Study \u2013 Insights into Optimal Usage of Visual Foundation Models", "content": "Video downsampling strategy. Long and high frame-per-second videos take a lot of space to store\nand time to process. We explore two straightforward ways of conducting temporal downsampling to\nachieve more efficient processing without sacrificing too much performance. we\nexplore the keyframe sampling (blue) and clip sampling (orange) strategies. We can observe that\nkeyframe sampling is a better strategy than clip sampling in this setting, more wisely balancing the\ntrade-off between video processing overhead and task performance.\nCombination of multiple encoders. We explore whether\na mixture of foundation models (experts) has the potential to\nstrengthen the capability of 3D scene understanding. We exper-\niment on the 3D semantic segmentation task with three feature\nsources: LSeg, StableDiffusion, and Swin3D. When combin-\ning different feature sources, we concatenate all features along\nthe channel dimension for every point in the point cloud. The\nresults are shown in Figure 8. After combining features from\ndifferent sources, there exists potential that the semantic un-\nderstanding capability can be boosted in a mixture of experts\nmanner. However, it is not necessarily true that combining the"}, {"title": "5 Conclusion", "content": "This paper presents the first comprehensive analysis of leveraging visual foundation models for\ncomplex 3D scene understanding. We explore the strengths and weaknesses of models designed for\nvarious modalities and trained with different objectives. Our study reveals the superior performance\nof DINOv2, the advantages of video models in object-level tasks, and the benefits of diffusion models\nin geometric registration tasks. Surprisingly, we find limitations of language-pretrained models in\nlanguage-related tasks. The extensive analysis suggests that a more flexible encoder selection can\nplay a crucial role in future scene understanding and multi-modal reasoning tasks."}, {"title": "A Additional Experiment Details", "content": "In this section, we provide a detailed introduction of all the visual foundation models we have\nevaluated, including the checkpoints we use and how we extract feature representations from the\nencoder backbones. Our code will be publicly released for replication and further evaluation."}, {"title": "A.1 Evaluated Visual Foundation Models", "content": "Our evaluation and analysis are conducted mainly on the seven models listed in Table 1 in the main\nbody paper. We have chosen models such that they cover most of the backbones used by recent\n3D scene understanding and reasoning work. In this part, we discuss all the models we have used\nin our experiments and explain their pretraining object, the dataset used for pretraining, the public\ncheckpoints we choose, and the method we leverage to extract features from their backbones. We\nstart with image foundation models, and then video and 3D models.\nDINOv2 [58]. DINOv2 leverages an image-wise contrastive objective by minimizing the distance\nof features from the same samples, and maximizing those from different samples. They also include\na patch-wise denoising objective by performing reconstruction from masked inputs. They train their\nmodel on a large-scale image dataset, LVD-142M [58], which contains 142 million unlabeled images.\nWe take the standard DINOv2 implementation \u00b9 and use the pretrained ViT-L/14 checkpoint for our\nevaluations.\nLSeg [44]. LSeg aims to align visual features from images with corresponding semantic information\nprovided by natural language descriptions by maximizing the correlation between the text embedding\nand the image pixel embedding of the ground-truth class of the pixel. We use the official checkpoint 2\nof ViT-L/16 that is trained on a mixture of seven datasets [44].\nCLIP [65]. CLIP aligns visual and textual representations in a shared embedding space through\ncontrastive learning by maximizing the similarity between the embeddings of corresponding image-\ncaption pairs while minimizing the similarity of non-matching pairs. CLIP was trained on a large and\ndiverse dataset of image-caption pairs sourced from the internet including over 400 million image-text\npairs. We use the official implementation and checkpoint 3 with a ViT-L/14 as the backbone for our\nevaluations.\nStableDiffusion (SD) [70]. SD is a diffusion-based model used for generating high-quality images\nfrom text prompts. The model is trained to gradually remove noise from images, transforming\nrandom noise into coherent images that match the provided text descriptions. This model is trained on\nLAION5B [73] which contains over five billion of images paired with detailed captions. We follow\nDIFT [77] 4 to extract features from SD and we use the checkpoint SD2.1 for our evaluation. We\nuse the features from block index 1 for all tasks. The noise timestep is set to 100 by default. We use\nnull-prompt as the text condition.\nStable VideoDiffusion (SVD) [12]. SVD is an extension of SD from image generation to video\ngeneration by incorporating additional temporal modules. SVD is first initialized from an image-level\npretrained diffusion checkpoint (SD2.1), then is further finetuned on 10 million videos. We use\ntheir publicly released image-to-video variant (SVD-xt) 5. We build our feature extractor pipeline\nfollowing DIFT [77] and extract the features from index 1 for all tasks. The noise timestep is set to\n25 by default. We use the first-frame image as the condition for all the cross-attention modules while\nwe use the unconditional version for the latent input of the UNet we concatenate an all-zero vector"}, {"title": "A.2 Additional Evaluation Details for Vision-Language Scene Reasoning", "content": "Datasets. We evaluate the performance on two challenging indoor 3D VQA datasets: ScanQA [5]\nand SQA3D [52]. SQA3D features over 33K QA pairs, while ScanQA consists of more than 41K\npairs. Each entry in these datasets includes a complex 3D indoor scene, a question, and corresponding\nanswers. We use the splits provided by the respective datasets.\nOptimization. We keep the LLM parameters frozen and finetune the shallow visual projection\nQ-Former module [46] to align features from different encoders to the LLM input space. Different\nfrom [35], we train the Q-Former module from scratch for a fair comparison of all encoders. Following\nthe approach of 3D-LLM, we pretrain the module for 10 epochs using 3D-Language dataset [35] and\nthen finetune it on the training split of the two evaluation datasets for 35 epochs. Both stages use the\nAdamW [50] optimizer with a linear warmup and cosine decay learning rate scheduler. While longer\ntraining can further improve performance, trends stabilize after 35 training epochs."}, {"title": "A.3 Additional Evaluation Details for Registration", "content": "Dataset generation. When generating corresponding partial scene point clouds from ScanNet\ndataset, due to memory constraint, we downsample the partial scene point clouds to 4,096 points each\nwith the farthest point sampling (FPS) algorithm, if the number of points in $P_1$ and $P_2$ is over 4,096.\nWe follow the same train/val split on the semantic segmentation task in our partial scene registration\ntask.\nOptimization. We follow REGTR [95] to adopt a Transformer cross-encoder module to enable\ncross-reasoning of the foundation model features from two point clouds, followed by a lightweight\ndecoder to obtain the corresponding position of each point in the other point cloud, forming altogether\n$N_1 + N_2$ pairs of correspondences, where $N_1$ and $N_2$ are the number of points in $P_1$ and $P_2$,\nrespectively. Afterward, the rotation R and the translation t can be obtained in a closed-form solution\nsolved by a weighted version of the Kabsch-Umeyama [39, 81] algorithm. We use Adam [41] for\noptimization and train our model for 30 epochs."}, {"title": "A.4 License of Dataset Used", "content": "In this section, we list the licenses of all the datasets we have used during our evaluation:\n\u2022 ScanNet [20]: MIT License.\n\u2022 ScanQA [5]: CC BY-NC-SA 3.0 License.\n\u2022 SQA3D [52]: CC-BY-4.0 License.\n\u2022 ScanRefer [16]: CC BY-NC-SA 3.0 License.\n\u2022 3DLanguage-Data [35]: MIT License.\nIn addition, we utilize a number of public foundation model checkpoints pretrained on various data\nsources in our paper. Please refer to their original paper for the license of datasets they have used in\npretraining their models."}, {"title": "B Additional Experiment Results", "content": ""}, {"title": "B.1 Evaluation Curves during Different Training Stages", "content": "We show the evaluation curves for the partial scene registration in Figure A. We can observe that the\nperformance ranking of different foundation models stays mainly unchanged throughout the training\nprocess."}, {"title": "B.2 Diffusion Noise Level and Feature Layer", "content": "In we evaluate the effect of different noise level (noise steps) and different feature layers in\nthe decoder module in leveraging StableDiffusion (SD) [70] for feature extraction. The results show\nthat for SD, adding noise t < 100 steps in general leads to the best performance. When t increases\nbeyond 100 steps, the performance starts to downgrade. As for decoder layers, the decoding portion\nof the UNet consists of 4 blocks. We skip the final layer closest to the output and consider layers 0,\n1, and 2. The results demonstrate that the output features of the layer one decoder lead to the best\nperformance. These observations are consistent with the study in [6, 99]."}, {"title": "B.3 Additional Qualitative Results", "content": "We show additional qualitative results for partial scene registration demonstrating that the\nfamily of StableDiffusion and StableVideoDiffusion which use the objective of generative pretraining\nobtains superior performance. In addition, video encoders like V-JEPA and StableVideoDiffusion are\nequipped with a stronger capability to find geometric correspondences."}, {"title": "C Limitations and Future Work", "content": "Although we have made a substantial effort to explore the role of visual foundation models in various\nscene understanding tasks, our perception of this problem remains relatively limited. This section\nprovides a detailed discussion of the limitations and outlines potential future directions.\nModel capacities not strictly identical or comparable. Our evaluation focuses on seven vision\nfoundation models due to their availability and common use in recent work. Consequently, all our\nexperiments are based on publicly available checkpoints released by the project owners. Although we\nhave attempted to choose models with similar capacities, achieving strictly identical backbone archi-"}, {"title": "D Societal Impact", "content": "We anticipate a potential positive social impact from our work. Lexicon3D represents one of the first\nsteps towards a comprehensive understanding of large-scale visual foundation models in real-world\n3D scene analysis and reasoning. This understanding could lead to the development of more robust\nand efficient scene encoding systems, benefiting a wide range of applications, including autonomous\ndriving, virtual reality, household robots, and multi-modal chatbots. Ultimately, this could contribute\nto a more inclusive, efficient, and safer world, where technology understands and adapts to the diverse\nways humans perceive and navigate their environments.\nPotential negative societal impact. We do not see a direct negative societal impact on our work.\nIndirect potential negative impact involves misusing strong scene encoding foundation models for\nsurveillance or virtual reality. We believe it is crucial for researchers to proactively consider these\nconcerns and establish guidelines to ensure the responsible usage of these models."}]}