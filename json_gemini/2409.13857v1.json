{"title": "WORMHOLE: CONCEPT-AWARE DEEP REPRESENTATION\nLEARNING FOR CO-EVOLVING SEQUENCES", "authors": ["Kunpeng Xu", "Lifei Chen", "Shengrui Wang"], "abstract": "Identifying and understanding dynamic concepts in co-evolving sequences is crucial for analyzing\ncomplex systems such as IoT applications, financial markets, and online activity logs. These\nconcepts provide valuable insights into the underlying structures and behaviors of sequential data,\nenabling better decision-making and forecasting. This paper introduces Wormhole, a novel deep\nrepresentation learning framework that is concept-aware and designed for co-evolving time sequences.\nOur model presents a self-representation layer and a temporal smoothness constraint to ensure robust\nidentification of dynamic concepts and their transitions. Additionally, concept transitions are detected\nby identifying abrupt changes in the latent space, signifying a shift to new behavior\u2014akin to passing\nthrough a \"wormhole\". This novel mechanism accurately discerns concepts within co-evolving\nsequences and pinpoints the exact locations of these \"wormholes,\" enhancing the interpretability\nof the learned representations. Experiments demonstrate that this method can effectively segment\ntime series data into meaningful concepts, providing a valuable tool for analyzing complex temporal\npatterns and advancing the detection of concept drifts.", "sections": [{"title": "1 Introduction", "content": "With the rapid and continuous generation of data, understanding and analyzing dynamic concepts in sequential data is\nparamount for numerous applications, including IoT systems, financial market analysis, and online behavior monitoring\n[1, 2, 3, 4, 5, 6]. Such data often involve co-evolving sequences, where multiple time series exhibit interdependent\nbehavior concepts over time. These co-evolving sequences encapsulate a wealth of information, revealing underlying\nstructures and behaviors that are crucial for making informed decisions and predicting future trends.\n\nIdentifying concepts within these sequences offers significant advantages. For example, in IoT applications, con-\ncepts/patterns in sensor data can provide insights into operational efficiency and anomaly detection [7, 8, 9]. In\nfinancial markets, understanding the co-movements of stock prices can aid in portfolio management and risk assessment\n[10]. Similarly, analyzing online activity logs can enhance user experience and targeted advertising [11, 12, 13, 14].\nThe challenge, however, lies in accurately identifying these concepts in real time, as data streams are often vast and\ncontinuously evolving.\n\nConcept identification in time series has been extensively studied, especially in database management and data mining\n[15, 16, 17, 18]. Traditional approaches such as hidden Markov models (HMM), autoregression (AR), and linear\ndynamical systems (LDS) are effective for static datasets but struggle with continuous data streams due to their lack\nof adaptability and need for predefined parameters [19, 20]. Recent advancements in data stream mining, including\nCluStream [21] and DenStream [22], offer improved scalability for evolving data but often fail to capture temporal\ndependencies and dynamic transitions. These methods focus more on maintaining clusters over time rather than\nunderstanding underlying concepts. Moreover, deep learning approaches like OneNet [23] and FSNet [24] have made\nsignificant strides in forecasting by adapting to concept drift, yet primarily aim to enhance predictive accuracy without\nproviding deeper insights into concept identification."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Concept Drift in Time Series", "content": "Concept drift has been a significant challenge in time series analysis, particularly in streaming data environments where\nthe underlying data distributions may change over time [25, 26]. Traditional models such as Hidden Markov Models\n(HMM) and Autoregression (AR) have been widely used but often lack adaptability in the presence of continuous data\nstreams. Recent advancements, such as OrbitMap [1], KRL [27] and TKAN [28], have improved scalability but still\nface challenges in capturing temporal dependencies and dynamic transitions. Additionally, models like Cogra [16]\nand Dish-TS [15] have introduced techniques to address concept drift by incorporating stochastic gradient descent and\ndistribution shift alleviation, respectively."}, {"title": "2.2 Co-Evolving Sequences and Dynamic Concept Identification", "content": "The identification of dynamic concepts in co-evolving sequences is crucial for understanding complex temporal patterns.\nVarious methods have been proposed to segment time series data into meaningful patterns, including the use of\nhierarchical HMM-based models like AutoPlait [29], and the Toeplitz inverse covariance-based clustering method,\nTICC [30]. Techniques have also been developed to analyze changes in mobility patterns caused by events such as\nCOVID-19 [31], Sequence pattern-based decision making [32], financial market forecasting using clustering-based\ncross-sectional regime identification [33], and dynamic cross-sectional regime identification for market prediction [34],\nhighlighting the importance of adapting models to dynamic environments."}, {"title": "2.3 Deep Representation Learning for Temporal Data", "content": "Deep learning techniques have gained traction in temporal data analysis, offering powerful tools for representation\nlearning. OneNet [23] and FSNet [24] are notable examples that enhance time series forecasting by adapting to concept\ndrift. However, these models primarily aim to improve predictive accuracy rather than offering insights into the concept\nidentification process. Informer [35], TIMESNET [36], Triformer [37], and Non-stationary Transformers [38] further\nextend the capabilities of deep learning models in handling long sequence time-series forecasting and non-stationary\nbehaviors in time series. Our work, Wormhole, builds on these ideas by introducing a self-representation layer that\ncaptures the intrinsic relationships among sequences and a temporal smoothness constraint that ensures coherent concept\ntransitions."}, {"title": "2.4 Concept-Aware Models", "content": "The idea of concept-aware models, which can detect transitions between different behaviors or patterns, has been\nexplored in various domains [39, 40]. StreamScope [7] and the Generative Learning model [10] for financial time\nseries have contributed to this area by automatically discovering patterns in co-evolving data streams. There are also\nadvancements in invariant time series forecasting in smart cities [41], location-aware social network recommendations\nusing temporal graph networks [42], and evolving standardization techniques for continual domain generalization [43].\nOther approaches such as online boosting adaptive learning [44], temporal domain generalization via concept drift\nsimulation [45], and drift-aware dynamic neural networks [46] have also been proposed to handle concept drift in\ntemporal data. However, these models do not explicitly address the interpretability of the learned representations, a gap\nthat Wormhole seeks to fill by providing clear demarcations of concept transitions, enhancing the understanding of\ndynamic temporal patterns."}, {"title": "2.5 Self-Representation Learning", "content": "Self-representation learning has emerged as an effective approach for uncovering intrinsic relationships within data [47,\n48]. In self-representation models, each data point or instance is represented as a linear or nonlinear combination of\nother points within the dataset, allowing for a compact and interpretable representation of dependencies. This approach\nhas been widely adopted in fields like subspace clustering and sparse coding due to its ability to uncover latent structures\nwithout relying on predefined labels.\n\nBy employing techniques such as sparse regularization and low-rank constraints, self-representation models can\nhighlight the most significant dependencies between instances while ignoring irrelevant information."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce our novel framework, Wormhole, designed for concept-aware deep representation learning\nin co-evolving time sequences. The model builds upon a self-representation deep learning approach and integrates a\ntemporal smoothness constraint to effectively capture and understand dynamic concepts and their transitions. To handle\nthe co-evolving sequences, we divide the original multivariate time series S into multiple segments using a sliding\nwindow. Each segment is treated as an input for the model. Therefore, W = W1,W2,..., wn represents the collection\nof these segments, where each wi is a multivariate time series segment containing information about various time steps\nand channels within the window."}, {"title": "3.1 Deep Representation Learning", "content": "Our framework is built upon a deep neural network architecture that facilitates learning robust representations of\nco-evolving sequences. The model includes the following key components:\n\n1. Encoder: A deep neural network that maps the input time series segments into a latent space.\n\n2. Self-representation Layer: This layer encodes the notion of self-representation, ensuring that each latent\nrepresentation can be expressed as a combination of other latent representations, capturing the intrinsic\nrelationships between the segments.\n\n3. Decoder: This component reconstructs the time series segments from the latent representations, ensuring\nmeaningful representation."}, {"title": "3.2 Self-representation in Time Series", "content": "The self-representation layer plays a crucial role in capturing the relationships among different time series segments in\nthe latent space. The concept of self-representation implies that each segment in the latent space can be expressed as a\nlinear combination of other segments, effectively capturing the dependencies and similarities among the segments.\nMathematically, self-representation is defined as:\n\n$Z_{e*} = Z_{e*} \\Theta_s$ (1)\n\nwhere $Z_{e*}$ represents the latent representations of the time series segments, and $\\Theta_s$ is the self-representation coefficient\nmatrix. Each column of $\\Theta_s$ corresponds to a segment that is expressed as a combination of other segments.\n\nTo enforce sparsity in the self-representation matrix $\\Theta_s$, we introduce $l_1$ norm regularization:\n\n$L_{self}(\\Theta_s) = |\\Theta_s|_1$ (2)\n\nThis encourages the model to use a minimal number of latent components to represent each segment, effectively\nhighlighting the most significant relationships and dependencies.\n\nThe self-representation property allows the model to learn a compact and interpretable representation of the co-evolving\nsequences, which is essential for identifying underlying dynamic concepts and their transitions."}, {"title": "3.3 Temporal Smoothness Constraint", "content": "The self-representation learning layer does not consider the information that is implicitly encoded into ordered data,\nsuch as the spatial or temporal relationships between data samples. For time series data, this sequential ordering\nshould be reflected in the coefficients of the self-representation matrix $\\Theta_s$ so that neighboring segments are similar,\ni.e., $\\Theta_{s,i} \\approx \\Theta_{s,i+1}$. To incorporate this property, we introduce a temporal smoothness constraint that penalizes\nlarge differences between consecutive columns of the self-representation matrix $\\Theta_s$, thereby ensuring that the latent\nrepresentations vary smoothly over time.\n\nWe define the temporal smoothness constraint using a lower triangular matrix R, which has -1 on the diagonal and 1\non the second diagonal. This matrix effectively captures the differences between consecutive columns in $\\Theta_s$.\n\n$R = \\begin{bmatrix}\n-1 & 1 & 0 & \\cdots & 0 \\\\\n0 & -1 & 1 & \\cdots & 0 \\\\\n0 & 0 & -1 & 1 & \\vdots \\\\\n\\vdots & \\vdots & 0 & \\ddots & 1 \\\\\n0 & 0 & 0 & \\cdots & -1\n\\end{bmatrix}$ (3)\n\nTherefore, the product $\\Theta_sR$ can be represented as:\n\n$\\Theta_sR = [\\Theta_{s,2} - \\Theta_{s,1}, \\Theta_{s,3} - \\Theta_{s,2},..., \\Theta_{s,n} - \\Theta_{s,n-1}]$ (4)\n\nAnd the temporal smoothness constraint is defined as:\n\n$L_{smooth}(\\Theta_s) = |\\Theta_sR|_{1,2}$ (5)\n\nwhere $|\\Theta_sR|_{1,2}$ minimizes the column-wise $l_{1, 2}$ norm of $\\Theta_sR$, encouraging smooth transitions between consecutive\nlatent representations. This constraint ensures that changes in the dynamic concepts are captured effectively by\npenalizing large deviations in the latent space."}, {"title": "3.4 Network Architecture", "content": "The proposed model consists of an encoder, a self-representation layer, and a decoder. The encoder ($\\Theta_e$) maps the\ninput time series segments W into a latent space representation $Z_e$:\n\n$Z_e = f_{\\Theta_e}(W)$ (6)"}, {"title": "3.5 Concept Transition Detection", "content": "The process of concept transition can be likened to jumping through a wormhole in space, where the boundaries between\ndifferent concepts resemble wormholes. These boundaries mark the points of transition from one concept to another,\nfacilitating a rapid change in behavior patterns.\n\nTo determine the locations of these wormholes, or boundaries, we analyze the distribution of the self-representation\nmatrix $\\Theta_sR$. Significant changes in $\\Theta_sR$ indicate shifts in the underlying behavior patterns. This method uses the\ntemporal differences captured in $\\Theta_sR$ to identify transitions, where large deviations suggest a boundary between\ndifferent concepts or segments in the data.\n\nIf we assume that the series is drawn from a set of disconnected concept spaces (i.e., $\\Theta_s$ is block diagonal), the\ninformation encoded by $\\Theta_sR$ can reveal the space boundaries. Ideally, the columns of $\\Theta_sR$, i.e., $\\Theta_{s,i} - \\Theta_{s,i-1}$, that are"}, {"title": "4 Experiments", "content": "In this section, we present the experiments conducted to evaluate the effectiveness of the proposed Wormhole framework\nfor concept-aware deep representation learning in co-evolving time sequences. We describe the datasets used, the\nexperimental setup, the evaluation metrics, and the results."}, {"title": "4.1 Datasets", "content": "We evaluated our model using three datasets representing different domains of co-evolving time series. The Motion\nCapture Streaming Data from the CMU database\u00b9 captures various motions such as walking and dragging, making it\nideal for analyzing transitions between different types of human activities. The Stock Market Data includes historical\nprices and financial indicators from 503 companies\u00b2, providing a large-scale, high-dimensional dataset to test the model's\nperformance in detecting concept changes in financial markets. Lastly, the Online Activity Logs from GoogleTrend\nevent streams\u00b3 include 20 time series of Google queries for a music player from 2004 to 2022, used to evaluate the\nmodel's ability to detect behavioral shifts in user interactions."}, {"title": "4.2 Comparison with Baseline Models", "content": "To thoroughly evaluate the effectiveness of our Wormhole model, we compared it with several baseline models. These\nincluded StreamScope[7], a scalable streaming algorithm for automatic pattern discovery in co-evolving data streams;\nTICC[30], which segments time series into interpretable clusters based on temporal dynamics; and AutoPlait[29],\na hierarchical HMM-based model for automatic time series segmentation that identifies high-level patterns. The\nexperimental results are summarized in Table 1."}, {"title": "4.3 Visualization of Concept Transitions", "content": "We visualized the detected concept transitions for the Motion Capture Data. Fig. 3 shows the time series segments with\nmarked concept transitions, highlighting how the model accurately identifies the boundaries between different types of\nmotion such as walking and dragging. This visualization reinforces the effectiveness of the Wormhole framework in\ndetecting dynamic changes in co-evolving sequences."}, {"title": "5 Conclusion", "content": "We introduced Wormhole, a framework for concept-aware deep representation learning in co-evolving time sequences.\nWormhole offers a promising approach for understanding complex temporal patterns, with potential applications in\nvarious domains. Our experiments show that it effectively detects and segments dynamic transitions across diverse\ndatasets, outperforming current state-of-the-art methods. As future work, we aim to develop Wormhole into a standalone\nmodule that can be integrated with advanced time series forecasting models to mitigate the impact of concept drift. This\nwill enhance the robustness and accuracy of predictions in dynamically changing environments."}]}