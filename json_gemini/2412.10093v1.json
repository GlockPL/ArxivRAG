{"title": "AI in the Cosmos", "authors": ["N. Sahakyan"], "abstract": "Artificial intelligence (AI) is revolutionizing research by enabling the efficient analysis of large datasets and the discovery of hidden patterns. In astrophysics, AI has become essential, transforming the classification of celestial sources, data modeling, and the interpretation of observations. In this review, I highlight examples of AI applications in astrophysics, including source classification, spectral energy distribution modeling, and discuss the advancements achievable through generative AI. However, the use of AI introduces challenges, including biases, errors, and the \"black box\" nature of AI models, which must be resolved before their application. These issues can be addressed through the concept of Human-Guided AI (HG-AI), which integrates human expertise and domain-specific knowledge into AI applications. This approach aims to ensure that AI is applied in a robust, interpretable, and ethical manner, leading to deeper insights and fostering scientific excellence.", "sections": [{"title": "1. Introduction", "content": "Astronomy and astrophysics have always been, and will remain, data-driven sciences, relying heavily on experimental measurements to develop theories about the universe and the processes occurring in various sources. Early astronomical catalogues included the coordinates of various objects visible to the naked eye, recorded over different periods which was limited, but it formed the fundamental basis for any theories or hypotheses that attempted to explain the bright phenomena observed in the night sky. With advancements in telescopes\u2014particularly when all-sky surveys are available (for example, Sloan Digital Sky Survey\u00b9 and the upcoming Vera C. Rubin Observatory a) - the volume of astronomical data now reaches terabyte and petabyte scales. This immense quantity of data has provided researchers with a unique opportunity to study the cosmos with unprecedented detail."}, {"title": "2. ML used in astrophysical research", "content": "The application of ML in astrophysics has grown remarkably in recent years. shows the number of articles from NASA Astrophysics Data Systemb contain- ing the term \"machine learning\" in different years. The article number is shown by two categories: all articles (in gray) and those published specifically in major astrophysics journals (such as ApJ, A&A, MNRAS, etc.) (light blue). The general trajectory reveals a clear acceleration in the adoption of ML methodologies within the astrophysics community. From around 2000 to 2010, the number of articles is low, indicating that ML was not yet widely adopted in astrophysics. However, starting in the early 2010s, there is a noticeable increase, with a rapid and nearly exponential growth beginning around 2015. This growth pattern suggests a trans- formative phase in astrophysical research, where different tools of ML were started to be used in data analysis and modeling works. The upward trajectory in indicates that the role of ML methods in solving complex astrophysical problems, from analyzing large datasets to modeling, continues to grow. This highlights the recognition by the community of the potential of ML methods for data analysis and interpretation in the field."}, {"title": "3. Application of ML for specific tasks", "content": "Astrophysical data is increasing at an unprecedented rate due to advancements in observational technologies and large-scale surveys, making it impossible for humans alone to effectively handle and interpret the observed data. In this context, AI and ML tools have become essential for analyzing these large datasets by enabling al- gorithms to learn from data patterns and make predictions or decisions without explicit task-specific programming. These tools aim to solve problems in a manner analogous to human reasoning. The output of ML models includes classifications, predictions, clustering of data points, and anomaly detection, all of which are es- sential for extracting meaningful information from complex datasets. There are generally two main types of ML: supervised and unsupervised learning. Supervised learning involves training models using labeled data, where the expected output is known during the training process. This approach is particularly useful for tasks such as classifying objects, predicting different properties, and identifying specific astrophysical events. In contrast, unsupervised learning is used on unlabeled data, aiming to identify unknown structures or patterns. This method is used to tasks such as grouping galaxies based on their features, detecting new types of astronomical phenomena, and other similar applications.\nBoth supervised and unsupervised learning methods have been applied in astro- physical research for a range of tasks. A few examples include:\n\u2022 A random forest classifier was developed and trained on large samples of photo- metric and spectroscopic data from SDSS and LAMOST, achieving high accuracy in classifying stars, galaxies, and QSOs, enabling efficient large-scale classification of astronomical objects across multiple surveys.\u00b2\n\u2022 A convolutional neural network (CNN) trained on simulated images of galaxies at different evolutionary stages successfully retrieved various phases of evolution. This approach was subsequently applied to observed data, enabling the accurate distinction between pre-BN, BN, and post-BN stages.\u00b3\n\u2022 CNN was trained on SDSS ugriz images to estimate photometric redshifts for galaxies, using all pixel information, achieving high precision and reliability in redshift prediction across large datasets.\u2074\n\u2022 A deep neural network trained on labeled Kepler data effectively distinguished genuine transiting exoplanets from false positives by directly learning features from light curves, achieving high accuracy in identifying planet candidates.\u2075\n\u2022 A deep learning framework trained on simulated gravitational wave templates enabled real-time detection and parameter estimation of gravitational waves di- rectly from LIGO data.\u2076\n\u2022 A gradient boosting decision tree model was trained on the spectral and temporal properties of BL Lacs and FSRQs from the Fermi Large Area Telescope (LAT) catalog, achieving high-accuracy classification of blazar candidates of uncertain type by learning distinct y-ray emission patterns.\u2077\n\u2022 A machine learning tool trained on a large sample of blazar spectral energy dis-"}, {"title": "3.1. Classification of blazars using high energy y-ray data", "content": "In Ref. 7, ML methods were applied to classify blazar candidates of uncertain type (BCUs) included in the Fermi-LAT 4FGL-DR3 catalogue. \u00b9\u00b2 Blazars, which are ac- tive galactic nuclei with jets oriented toward Earth, are the dominant sources in the extragalactic y-ray sky, as demonstrated by Fermi-LAT observations. They are broadly classified into two types: BL Lacertae objects (BL Lacs) and Flat Spectrum Radio Quasars (FSRQs), which are distinguished by their emission-line features. However, many y-ray detected blazars lack clear classification due to limited opti- cal spectroscopic data and are categorized as BCUs. Distinguishing between these two blazar subclasses is essential, as it provides insights into different physical emis- sion mechanisms, jet properties, and facilitates population studies of blazars. To classify the BCUS, ML algorithms were trained on the y-ray spectral and tempo- ral properties of 2,219 labeled blazars (1,456 BL Lacs and 794 FSRQs) from the Fermi-LAT 4FGL catalog. The analysis employed three ML algorithms-Artificial Neural Networks (ANNs), XGBoost, and LightGBM. The dataset includes 18 in- put features derived y-ray spectral properties (e.g., photon index and flux across various energy bands) and temporal characteristics (flux measurements collected over 12 years) for each source. The dataset was divided into training (80%) and testing (20%) subsets. A 15-fold cross-validation was applied, further enhancing the reliability of the results. In this process, the dataset is divided into 15 subsets, with the model trained on 14 subsets while the remaining subset is used for validation. This process repeats until each subset has served as the validation set, enabling a comprehensive evaluation of model performance across different data segments."}, {"title": "4. Modeling blazar SEDS", "content": "Fitting the spectral energy distributions (SEDs) of blazars with numerical models is a challenging task, largely due to the computational cost of such simulations. The radiative processes involved, including synchrotron self-Compton (SSC) and external inverse Compton (EIC) emissions, require intricate and resource-intensive calculations to produce accurate results. This complexity increases when models are considered in time-dependent contexts, as it requires solving kinetic equations that account for particle acceleration, cooling, and interactions within the jet envi- ronment-making conventional numerical fitting methods time-consuming. To over- come these computational challenges, a novel approach using neural networks, par- ticularly CNN, has been developed in Refs. 10 and 11. This approach is designed to reduce computational time while preserving high accuracy in SED modeling. By training a CNN on a large set of simulated SEDs, this method bypasses the need for repeated numerical calculations, providing an efficient alternative for parame- ter exploration and real-time fitting of observed blazar spectra. In this framework, an extensive range of physically realistic parameters is explored for both SSC and EIC models to ensure comprehensive coverage of all relevant parameters. For the SSC model, 200,000 parameter combinations were generated using Latin hypercube sampling, a method that ensures uniform coverage of the parameter space while avoiding the regular spacing associated with grid-based approaches. For the EIC model, which includes additional parameters to account for external photon fields, 1 million parameter sets were generated. These parameter sets were then used as in- put into Simulator of Processes in Relativistic AstroNomical Objects (SOPRANO) code\u00b9\u00b3 to produce the corresponding spectra. SOPRANO is designed for efficient, detailed computation of radiative signatures in relativistic astrophysical sources, using implicit numerical methods to solve the Fokker-Planck equation for particle distributions and the integro-differential equations governing photon distributions. This enables SOPRANO to capture a wide range of physical processes in generating realistic SEDs for each parameter configuration. The SEDs generated by SOPRANO serve as the training dataset for the CNN, which is optimized to capture the complex spectral features accurately. During training, the network learns the relationship be- tween input parameters and their corresponding SEDs. Once trained, the CNN can predict SEDs across the full parameter range. In addition to data preparation and principal component analysis, further measures are implemented during training to prevent oscillations in the predicted spectra. Specifically, linear combinations of neighboring spectral points are constrained to limit independent fluctuations, which ensures smooth transitions and minimizes artifacts in the CNN-generated SEDs. The trained CNN model demonstrates excellent performance, accurately predicting SEDs across the parameter space while reducing computational time from several seconds or minutes to just milliseconds per evaluation. This efficiency enables the"}, {"title": "5. Feature with Generative AI", "content": "Generative AI represents a transformative advancement in ML, particularly through Large Language Models (LLMs), which are trained on vast datasets to process and"}, {"title": "6. Issues and Challenges", "content": "The application of AI in research appears limitless, and it is very likely that in the near future, AI will completely transform scientific research. However, this change comes with significant challenges that the scientific community must address. These challenges are related to objectivity, reproducibility, transparency, and accountabil- ity, which, if not addressed, may lead to research misconduct. To address these risks, the scientific community needs to take active steps which includes creating clear guidelines for using AI in research, making AI algorithms and datasets open and transparent for checking, encouraging thorough peer review, and emphasizing accountability with a focus on ethics. By addressing these challenges, we can fully benefit from the potential of AI while maintaining high scientific standards.\nCurrently, the main arguments against the application of AI in research, are biases, errors, and the \"black box\u201d problem. Although ML/AI tools are designed to accurately represent the data they are trained on, they can sometimes introduce biases, such as favoring specific patterns or answers. While bias in research can be addressed, the bias arising from AI-due to the scale and complexity of data, algorithms, and applications can be difficult to control. It is the responsibility of the scientists that are using AI in research to identify, describe, and control bias which involves taking care of data diversity, sampling, and representativeness when training AI models. Bias, like errors, can significantly affect the validity and reliability of results. In the context of AI, error refers to the difference between"}, {"title": "7. Conclusion", "content": "The integration of AI in research, particularly in astrophysics, marks a transfor- mative shift in data analysis, modeling, and prediction capabilities. In astrophysics, ML tools, ranging from supervised classification models to complex neural networks, have demonstrated their ability to handle and analyze the massive datasets gener- ated by modern telescopes. Applications of these tools, such as the classification of diverse astrophysical sources and the modeling of processes like multiwavelength emissions from blazars, highlight the role of AI in improving the efficiency and depth of astrophysical research.\nHowever, the deployment of AI also poses challenges, particularly regarding transparency, bias, and the \"black box\" nature of advanced models. The HG-AI framework offers a solution to these challenges: by emphasizing human oversight as a core element, it enables researchers to harness the strengths of AI while ensuring that results remain scientifically robust and interpretable. This approach advocates for responsible AI applications, where scientists actively guide and validate AI out- puts to achieve rigorous and unbiased results. Such synergy is essential and holds the potential to accelerate discoveries in research."}]}