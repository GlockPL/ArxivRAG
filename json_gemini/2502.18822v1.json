{"title": "Data-Efficient Multi-Agent Spatial Planning with LLMs", "authors": ["Huangyuan Su", "Aaron Walsman", "Daniel Garces", "Sham Kakade", "Stephanie Gil"], "abstract": "In this project, our goal is to determine how to leverage the world-knowledge of pretrained large language models for efficient and robust learn-ing in multiagent decision making. We exam-ine this in a taxi routing and assignment problem where agents must decide how to best pick up passengers in order to minimize overall waiting time. While this problem is situated on a graph-ical road network, we show that with the proper prompting zero-shot performance is quite strong on this task. Furthermore, with limited fine-tuning along with the one-at-a-time rollout algorithm for look ahead, LLMs can out-compete existing ap-proaches with 50 times fewer environmental in-teractions. We also explore the benefits of various linguistic prompting approaches and show that including certain easy-to-compute information in the prompt significantly improves performance. Finally, we highlight the LLM's built-in seman-tic understanding, showing its ability to adapt to environmental factors through simple prompts.", "sections": [{"title": "1. Introduction", "content": "Resource allocation in rapidly changing dynamic environ-ments is an important logistics and controls problem with huge economic impact (Kondor et al., 2022; Vazifeh et al., 2018). However, many approaches to these problems train models for specific environments from scratch using model-based or model-free reinforcement learning (RL), which can be challenging in settings where data collection is ex-pensive. Recently, as large language models (LLMs) have increased in capability (Ahn et al., 2022; OpenAI, 2024), they have been increasingly deployed on problems outside their traditional domain by developing prompting strategies that convert other data modalities into natural language. In examples ranging from biology (Jumper et al., 2021) to chess (Zhang et al., 2024b) to robotics (Ahn et al., 2022; Yang et al., 2023), researchers have shown that LLMs can accelerate learning and in some cases improve capabilities due to the inherent world knowledge that is built into them during pretraining.\nIn this work, we explore the application of LLMs to the challenging resource allocation problem of multi-agent taxi routing in a graph-based road network. Specifically, for each agent, we provide textual descriptions of the under-lying environment, the current state of all agents, and all outstanding requests as input to the LLM. The LLM outputs its reasoning and final decision for where to travel next or pickup a passenger for that agent. We show that Llama3-8B-Instruct (Dubey et al., 2024) not only offers strong zero-shot performance in this setting, but also that when fine-tuned and combined with the rollout algorithm (Bertsekas, 2021; Bertsekas & Castanon, 1999), it can outperform previous state-of-the-art approaches with fewer total environment interactions during training. This efficiency is important in real-world problems that are difficult to simulate and where data collection is expensive.\nWe also offer guidance on prompt engineering for graph-based data and demonstrate that incorporating easily com-putable information from the underlying physical environ-ment-such as shortest paths to passengers can signifi-cantly enhance performance.\nIn a preliminary case study, we demonstrate the ability of LLMs to enhance planning performance by incorporating additional contextual information: they can react to seman-tic descriptions of the environment to anticipate relevant changes in dynamics. For example, an LLM prompted with the additional information that it is currently raining can predict increased cost of traveling and react accordingly. We include an example of this in the Appendix Appendix D.\nFinally, we show that the use of LLMs in this setting is not without drawbacks. In some scenarios the LLM will hallucinate and provide controls that are impossible given the current graph structure and outstanding requests. We show that these can be largely mitigated by finetuning or incorporating feasibility checking-then-resampling.\nOur primary contributions are:\n1. Through experiments, we demonstrate that even in zero-shot settings, Llama 3-8B-Instruct performs re-markably well. Moreover, our finetuned rollout policy surpasses previous state-of-the-art approaches while re-quiring significantly less training data. Finetuning also effectively eliminates spatial hallucinations observed in the prompting-based methods."}, {"title": "2. Problem Formulation", "content": ""}, {"title": "2.1. Environment", "content": "We study the problem through a multi-agent taxicab routing problem introduced in (Garces et al., 2023). Given an urban environment with a fixed street topology, represented as a directed graph, $G = (V, E)$, where $V = [n]$ is the set of in-dices of the street intersections, and $E \\subseteq \\{(i, j)|i, j\\in V\\}$ is the set of directed streets connecting adjacent intersections. We define $N(i) = \\{j|j \\in V, (i, j) \\in E\\}$ as the neighboring intersections for intersection $i$.\nRequests stochastically enter the system according to un-known distributions. A request $r$ is defined by the closest intersections to its desired pickup and dropoff locations, the time at which it enters the system; and if it has been assigned to any agent. We define a scenario as a fixed map topology and a sequence of requests."}, {"title": "2.2. State and Action", "content": "We assume there are a total of $m$ agents and each agent can perfectly observe all requests, and agents' locations and occupancy status. We use letters $s, u$ to denote numerical states and controls, and letters $x, y$ to denote language in-put and output sequences. The state $s_k$ at time $k$ contains locations for all $m$ agents in terms of node indices, the time remaining in the currently assigned trip for all agents, and the set of outstanding requests at time $k$. If the remaining time for an agent $l$ is zero, the agent is considered available, and new requests can be assigned to it.\nWe define the action space for an agent $l$ at state $s_k$ as $U(s_k)$. At each time step, this action space contains the following actions. If an agent is not currently assigned, the agent may move to one of the neighboring intersections, remain in its current position, or pick up a passenger at the current location. On the other hand, if it is assigned, it must move along the shortest path to the passenger's current destination.\nOur formulation of the action space allows for separable con-trol constraints for each agent. For this reason, we express the joint control space at time $k, U_k(s_k)$, as the Cartesian product of the local control sets $U^1(s_k) \\times \\dots \\times U^m(s_k)$.\nAs the control space grows exponentially with the number of agents, finding an optimal policy becomes intractable, necessitating the search for suboptimal solutions."}, {"title": "2.3. Stochastic Dynamic Programming", "content": "This routing problem can be formulated as a finite horizon, stochastic Markov Decision Process (MDP). The global objective of all agents is to find a pickup strategy that minimizes the total wait time of passengers starting from the initial state $s_1$. A policy $\\pi = \\{\\mu_1,..., \\mu_N\\}$ is a list of functions, where $\\mu_k$ maps state $s_k$ into control $U_k = \\mu_k(x_k) \\in U_k(x_k)$. We define the cost of a policy $\\pi$ starting at state $s_1$ as:\n$J_{\\pi}(s_1) = E \\Big[ g_N(s_N) + \\sum_{k=1}^{N-1} g_k(s_k, \\mu_k(s_k)) \\Big]$\nIn the equation above, $s_{k+1}$ is the state at time $k + 1$ after application of control $u_k$ at time $k$ from the current state $s_k$, and $g_k(s_k, u_k)$ is the per-step cost, which is the number of current outstanding requests. Note that the agents' actions do not affect the distribution of incoming requests."}, {"title": "3. Background and Preliminaries", "content": ""}, {"title": "3.1. (One-at-a-time) Rollout and Offline Approximation", "content": "We adopt the one-at-a-time rollout framework from (Bert-sekas, 2021), which solves multiple smaller lookahead op-"}, {"title": "3.2. Online Play and Approximate Policy Iteration", "content": "Assuming our finetuned offline model successfully approx-imates the rollout policy, we then apply the offline trained models as base policy in online play, which according to recent theory (Bertsekas, 2022) serves as an approximate Newton step to further improve the performance of the learned policy. Online play's control $u_k$ at state $s_k$ is given by Equation (1), with $\\bar{u} = (\\bar{u}_1,...,\\bar{u}_{l-1},u_k,\\hat{\\mu}(F(s_{k,l+1})),...,\\hat{\\mu}(F(s_{k,m})))$.\nIf the policy approximation, $\\hat{\\mu}$, correctly approximates the rollout policy $\\mu$ on the current demand model, we ex-pect online play policy to outperform following the cost improvement property of the approximate policy itera-tion (Bertsekas, 2019). If $\\hat{\\mu}$ fails to approximate the rollout policy, the cost improvement property will not hold and the online play will not provide a significant improvement (Garces et al., 2023).\nThen, an offline approximation $\\hat{\\mu}$ of this one-at-a-time rollout-based RL policy is achieved by supervised finetun-ing of Llama 3-8B model. To train the policy approximation, we generate a dataset of random initial taxi and request lo-cations. The training feature at state $s_k$ for an agent contain the state and other agent's actions, i.e., $l \\in \\{1, ..., m\\}$ is $F(x_k, l) = (x_k, u_1,..., u_{l-1}, \\mu_{l+1}^\\pi(x_k),..., \\mu_{m}^\\pi(x_k))$ and the training label is $\\mu_l^\\pi$, which are the corresponding one-at-a-time rollout controls."}, {"title": "4. Our Method", "content": ""}, {"title": "4.1. Language Models as Base Policies", "content": "We study what physical information is pertinent to LLM de-cision making, how to encode that information in language for better LLM decision making, and how to use physical information to alleviate hallucinations (Gil et al., 2023).\nWe use $p$ to denote a pre-trained LLM. For language model policies, we use chat template (Huggingface, 2024) to for-mat the inputs and encapsulate the environmental informa-tion in the system prompt. An example of the system prompt is given in Appendix B.1. For all policies that use LLMs, we use prompts to convert state and action information (of other agents) as inputs to the LLMs: $x_k = prompt(s_k)$ and parse the outputs of the LLMs to get the actions: $u_k = parse(y_k)$. The prompting function wraps input $x$ with task instructions. Furthermore, we physically ground the LLMs by also pro-viding the shortest paths to outstanding requests, which is fast to compute from the environment. A concrete example is given in Appendix B.2. We denote logically-coherent intermediate reasoning steps by $z_1,..., z_n$. Furthermore, we define hallucinations as when the LLM outputs a next position that is neither a neighbor of the current position nor on the paths to one of the requests. In addition to zero-shot prompting, we also experiment with the following common techniques for improving performance.\nFew-shot. We provide five examples consisting of input-output sequence pairs that illustrate the desired behavior across various states."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Implementation Details", "content": "Our main models are zero-shot Llama 3-8B with/without rollout, finetuned Llama 3-8B with/without rollout and zero-shot Llama 3.2-3B (Meta, 2024b) with/without rollout. The Llama 3 models (Meta, 2024a) are state-of-the-art open-source large language models that improve upon the Llama 3 herd of models (Dubey et al., 2024) in terms of reasoning capabilities, context length, and tool use. The 8B and the 70B models can satisfy a large range of use cases. For our experiments, we use the 8B model because it has reasonable zero-shot performance on our problem and can be finetuned within reasonable time and infrastucture resources.\nFor fine-tuning, we adapt Llama Cookbook (lla, 2024), the official codebase released for fine-tuning and building appli-cations. Finetuning data is collected using rollout with an instantaneous reassignment policy (IA-RA, see Section 5.2 for details) as the base policy and 2000 Monte Carlo (MC) simulations. The details for finetuning using one-at-a-time rollout, are detailed in Section 3.1. We set the learning rate to $5 \\times 10^{-5}$, the batch size to 8, and adhered to the default settings in Llama Recipes for all other configurations.\nThe Llama 3.2-3B models are expected to perform worse on this planning problem due to their optimization for multilin-gual dialogue tasks and their smaller sizes. Nonetheless, we use this smaller model as a baseline to study the trade-off between speed and performance."}, {"title": "5.2. Baselines Used for Comparison", "content": "We compare our approach with the following baselines:\nGreedy policy: Each taxi moves towards its closest request without coordinating with other taxis. This method does not consider future demand.\nInstantaneous assignment (IA-RA): It matches available taxis and outstanding requests at every time step using an auction algorithm (Bertsekas, 1979). This method also does not consider future demand.\nGraph Neural Networks (GNNs) (Garces et al., 2023): Graph neural networks are an intuitive choice for this prob-lem since the underlying environment is a road graph. There-fore, it serves as a good baseline that is of a different model architecture from transformers. For each load level, we train a separate pair of GNNs. The first GNN determines if an available agent should pick up a request in its current loca-tion, and the second GNN determines the next intersection towards which the agent should move. The move model is a multi-classifier while the pick up model is a binary classifier.\nRollout with Different Base Policies. We also compare against rollout with greedy policy, IA-RA, and GNNs as the base policies. Rollout with GNNs as base policy is the pre-vious state-of-the-art on this multi-agent routing problem."}, {"title": "5.3. Hardware", "content": "We conduct experiments on Nvidia H100 or A100 GPUs. We use two techniques to enhance inference speed: (1) run-ning all LLMs in FP8 precision, and (2) employing tensor parallelism across four GPUs within the same node, as these approaches provide results with negligible performance loss."}, {"title": "5.4. Performance Benchmark", "content": "We mainly consider a fixed horizon of 60 steps, which is equivalent to 60 minutes. We consider a $400 \\times 400m^2$ section of San Francisco with 42 nodes and 125 edges, and $m = 3$ taxicabs. We consider 1-minute edge travel time. For rollout, we set the planning horizon to $t_h = 10$.\nWe create a test set comprised of 20 scenarios. We create one set for each load level: low, medium, and high. The primary focus of our experiments is to evaluate the perfor-"}, {"title": "5.5. Main Results", "content": "We present our main results in Table 1 and discuss our observations and analysis below.\nBest Performing Model Finetuning Llama, along with our offline and online play scheme outperforms all other models, including the previous state of the art GNN+rollout. Im-portantly, unlike the GNN-based models, which trained a separate policy for each load level, we were able to finetune a single model for all load levels, suggesting better gener-alization. Finally, the performance benefits of finetuning and rollout suggest that it is necessary to perform both the offline approximation and online play steps. While each pair of GNN models were trained with 6400 trajectories, we finetune a single Llama 3-8B model for all load levels with only 128 trajectories collected using Rollout with IA-RA as base policy. With its larger capacity compared to GNN models, it demonstrates superior generalization capabilities. As a result, our approach achieves significantly greater data efficiency than the previous GNN-based method.\nWe also found that even the zero-shot approach performs well, slightly trailing rollout with greedy or IA-RA base policies but outperforming greedy/IA-RA without rollout across all load levels. This may be due to its inherent behav-iors: prioritizing high-index nodes and navigating to nearby nodes to maximize encounters with new requests.\nZero-shot methods can exhibit spatial and reachability hallu-cinations, leading to suboptimal behaviors (see Appendix C). While cost grows as the level of load grows, this is not the case for the number of hallucinations. Furthermore, when comparing in a large map, we see that the number of hal-lucinations grows linearly with the number of agents. This suggests that the hallucination rate is an inherent property of the LLMs, which is not obviously affected by load levels in the environment.\nRollout vs. Fine-tuning. We also observed that, through only one round of finetuning, we are able to remove spatial hallucinations. With the improvement of the cost and alle-viating hallucinations, we conclude that finetuning is better than rollout with LLMs as base policies at improving the behavior of the LLMs. When zero-shot prompted LLMs are used as the base-policy for rollout, we see that costs"}, {"title": "5.6. Prompting Methods", "content": "In this section, we present the results of applying various prompting techniques Section 4.1 to Llama 3-8B, aiming to identify optimal zero-shot performance (without any train-ing or fine-tuning). For zero-shot, CoT, and ToT, we set the temperature of the LLM to 0. While for self-consistency, we set the temperature to 0.7 as that technique requires di-verse samples. For ZS-HC, we set the maximum number of reprompt to 5.\nWe present the complete numerical results of all prompting methods in Table 2. We observe that CoT, ToT have the effect of alleviating hallucinations and decreasing cost. The worse performance of ToT than CoT may be because that we ask the Llama 3-8B model to be the evaluator, which is too challenging for this model.\nHowever, from the results, we see that CoT and ToT offer only limited improvement of the total cost under heavy load levels. Therefore, we need better methods to improve LLMs' spatial reasoning abilities. We collect data using the simple zero-shot prompt, to ensure a fair comparison between prompting methods and finetuning/rollout methods.\nSurprisingly, the results also indicate that few-shot and CoT-SC do not perform better than zero-shot prompting. This may be caused by the model making improper generaliza-tion (extrapolation) from the given examples."}, {"title": "5.7. Ablations on Test-time Efficiency", "content": "Compute consumption increases linearly with the number of MC simulations, which can be viewed as a hyperparameter of the rollout algorithm. This raises a critical question: how many MC simulations are necessary? To address this, we analyze the trade-off between computational cost and the number of MC simulations, using our fine-tuned 8B model. Figure 7 shows that decreasing the number of MC simulations only leads to slightly worse performance."}, {"title": "5.8. Ablations on Scalability", "content": "We study the scalability through two sets of experiments. First, we present performance on the most difficult scenario in our test in Table 3. We observe that the finetuned LLM and rollout with finetune LLM as the base policy are better in these hardest scenarios, suggesting that they are better at generalization. Second, we scale our environment to a $1000 \\times 1000m^2$ map and 10 agents in Table 4. The numbers show that LLMs can scale to this setting."}, {"title": "6. Related Works", "content": ""}, {"title": "6.1. Multi-agent Decision-making", "content": "Multi-agent decision-making algorithms allow for the opti-mization of resource allocations in real-world applications such as taxi routing (Garces et al., 2023) and drone deliver-ies (Lee et al., 2022). Optimal solution for these problems is intractable since it requires considering multiple scenarios of potential future requests and all relevant agent actions at each decision point, in extremely large state and control spaces that grow exponentially with the number of agents. Hence, many methods have been proposed to find compet-itive sub-optimal solutions. Most intuitive solutions are greedy policies, but they can be far from optimal-for exam-ple, when two taxis go to serve the same request, leaving other requests unattended. Studies (Kondor et al., 2022; Vazifeh et al., 2018) show that with non-coordinating greedy policies, each additional ride-hailing company in the market can largely increase the total number of circulating vehicles than necessary. Heuristic-based methods (Bertsekas, 1979; Bertsimas et al., 2019; Croes, 1958) often generate myopic policies due to the lack of consideration for future demand. RL methods have been proposed in both offline (Ulmer et al., 2019; Farazi et al., 2021) and online (Silver & Veness, 2010; Somani et al., 2013; Bent & Van Hentenryck, 2004)"}, {"title": "6.2. Foundation models for Planning and Control", "content": "The most common class of foundation models used for plan-ning and control are vision-language-action models (VLAs) (Kim et al., 2024; Zhen et al., 2024; Brohan et al., 2023), which leverage pretrained vision (Kim et al., 2024; Oquab et al., 2023), language (Touvron et al., 2023; Chowdhery et al., 2023), or multimodal (Zhai et al., 2023; Driess et al., 2023) models to interpret visual inputs, understand textual instructions, and generate contextually appropriate actions within dynamic environments.\nMore relevant to us are those works using foundation mod-els for spatial reasoning and path planning (Yang et al., 2024a;b; Liu et al., 2023). Yang et al. proposes a video-based benchmark to probe the spatial reasoning ability of multimodal LLMs (MLLMs). Their conclusions are rather negative: first, MLLMs are competitive but subhuman; sec-ond, linguistic prompting techniques are harmful for spatial reasoning. Instead of testing their ability in free-form envi-ronments, we take a step back, and test if they can reason spatially in an environment that can be represented as a graph. Liu et al. applies Word2Vec models on real-world delivery route optimization (Merch\u00e1n et al., 2024) by draw-ing an analogy between delivery routes and sentences in language. Based on language descriptions of single/multi-robots routing tasks, Huang et al. asks LLMs to generate Python code to solve single or multiple robots problems. Their evaluations are limited to prompting methods. They also perform task verification by generating unit tests. Deng et al. investigates using LLMs as path planners and cur-riculum generators to mitigate hallucinations. The study leverages LLMs and Python to convert maze descriptions into Gym environments, where LLMs generate intermediate waypoints to simplify paths. Guided by LLMs, Q-learning iteratively plans paths, outputting the planning history and Q-table as the final policy.\nThe success of LLMs in multi-agent decision-making de-pends on their reasoning abilities, which are crucial for generating context-aware responses in dynamic environ-ments. Techniques have been proposed for enhancing the reasoning capabilities of LLMs, including those based on CoT (Wei et al., 2022), ToT (Yao et al., 2024), Best-of-N (Lightman et al., 2023; Wang et al., 2023b), Monte Carlo Tree Search (MCTS) (Gao et al., 2024; Zhang et al., 2024a; Wang et al., 2023b; 2024b), or search against learned veri-fiers (Cobbe et al., 2021). However, these approaches focus on single-agent scenarios and do not extend to multi-agent contexts, where the actions of one agent must be conditioned on the actions of others. Moreover, MCTS incurs significant computational costs during the search process (Wang et al., 2024a; Ye et al., 2022), severely restricts its applicability, especially in multi-agent scenarios."}, {"title": "7. Limitations and Future Directions", "content": "The primary limitation of our work lies in the slower infer-ence time of LLMs compared to GNN models or heuristic-based methods. In an online setting, this makes it imprac-tical to use LLMs with a high number of MC samples, potentially hurting the performance.\nAs future work, We are especially interested in the follow-ing directions. First, LLMs could be used to predict future requests or as value functions to evaluate generated plans. Second, while RL traditionally addresses environments with costly interactions, the compute expense of large-model in-ference may now surpass that of environmental interactions, necessitating new frameworks for this paradigm. Third, Diverse, challenging tasks are needed to test LLMs' capabil-ities beyond traditional textual benchmarks, bridging the gap between current models and general robotics applications."}, {"title": "8. Conclusion.", "content": "We have demonstrated that LLMs can plan effectively in structured spatial environments, even in zero-shot settings, when provided with well-crafted information and descrip-tions. Furthermore, we have shown that finetuning through rollout, which encourages models to consider multiple pos-sible actions and future states, significantly enhances their performance. Based on prompting results, we conclude that adressing spatial hallucinations is crucial for further improving LLMs' spatial planning capabilities. We hope this work highlights the potential of leveraging pre-trained knowledge in LLMs to achieve greater efficiency in learning and executing complex multi-agent tasks. Additionally, we advocate for the development of more diverse and challeng-ing multi-agent benchmarks to deepen our understanding of the capabilities embedded within these models."}, {"title": "B. Prompts", "content": ""}, {"title": "B.1. System and User Prompts", "content": ""}, {"title": "System and User Prompts as State", "content": "System: \"You are a taxi driver in a multi-taxicab team on a map described by roads and intersections. Nodes (intersections) are listed by index with coordinates (longitude, latitude): 65293741: (-122.4097034, 37.7817636), 65293743: (-122.4092587, 37.7814105), 65303533: (-122.4038718, 37.7898332), . . .\nRoads are expressed as connections between nodes in the form 'from node to node': 65293741 to 65293743, 65293741 to 65318282, 65293741 to 1723738829, 65293743 to 65293741, 65293743 to 65306931, 65303533 to 65303538, 65303538 to 6378899319, 65303538 to 1271001348, 65303541 to 65303544, 65303541 to 1271001343,\nYour goal is to minimize the waiting time of all riders. Make your decisions based on the following rules:\n1. A taxi can only pick up an active request if it is idle.\n2. If no request exists or you choose not to pick up, you must decide where to move next.\"\nUser: You are Taxi 0. You may only pick up a request if there is active request in the system. Taxi 0 is idle at location 6925582021. Taxi 1 is idle at location 1578907668. Taxi 2 is idle at location 65306810. Next action for taxi 1: do not pickup, go to 1578907668. Next action for taxi 2: do not pickup, go to 65306810. Please provide your next action as a tuple in the format: (pickup: True or False, next position: [numeric value]).\nAssistant: As Taxi 0, I am idle at location 6925582021. I will check for active requests.\nThere are no active requests in the system. Therefore, I will decide where to move next. I will move to the closest idle taxi to reduce waiting time.\nThe closest idle taxi is Taxi 2, which is at location 65306810. I will move to location 65306810.\nMy next action is: (pickup: False, next position: 65306810)"}, {"title": "B.2. Chain-of-Thought", "content": ""}, {"title": "Chain-of-Thought", "content": "User: [Zero-shot User Prompt as in Appendix B.1]\nBefore deciding your next action, consider the following steps:\n1. Identify all outstanding requests.\n2. Determine which request you should prioritize picking up, aiming to minimize the total waiting time.\n- You should pick up a request immediately if you are at the same location/one step away from the request.\n- You can move to a node and pick up the request at that node in the same step.\n3. Evaluate which requests other agents are already closer to or actively moving toward, based on their known or expected actions. Adjust your action if necessary.\nAfter reasoning through these steps, provide your next action as a tuple in the format: (pickup: True or False, next position: a numeric value).\nRemember, your goal is to minimize total waiting time and avoid targeting requests that are better suited for other agents unless no alternative exists."}, {"title": "B.3. Tree-of-Thoughts", "content": ""}, {"title": "Tree-of-Thoughts", "content": "System: [Zero-shot User Prompt as in Appendix B.1]\nBefore deciding your next action, think through the following tree of possibilities:\n1. Identify Requests:\n- Which requests are outstanding?\n- Which are closest or most urgent?\n- Are other agents closer or already handling them?\n\u2192 Rank requests by priority based on proximity, urgency, and agent competition.\n2. Evaluate Actions:\n- Action 1: Pick up a request if you're at/one step away. You can move to a node and pick up the request at that node in the same step.\n- Action 2: Move toward a high-priority request.\n- Action 3: Stay if no better option exists.\n\u2192 Simulate the impact of each action on total waiting time.\n3. Decide:\n- Which action minimizes waiting time?\n- Does it avoid unnecessary conflicts with other agents?\n\u2192 Choose the best action as: (pickup: True/False, next position: numeric value)."}, {"title": "B.4. Few-shot with CoT", "content": ""}, {"title": "Few-Shot", "content": "User: Here are three examples: <|start_header_id|)user(lend_header_id|) You are Taxi 0. You are idle at location 65334120. Taxi 1 is idle at location 1580501206. Taxi 2 is idle at location 1580501206. Currently there are outstanding requests in the system:\npickup_location: 65314158\nTaxi 0 shortest route: [65334120", "65314158": "length: 1)\nTaxi 1 shortest route: [1580501206"}, {"65314158": "length: 2)\nTaxi 2 shortest route: [1580501206"}, {"65314158": "length: 2)\nExpected next action for taxi 1: go to 65334120. Expected next action for taxi 2: go to 65334120. You should prefer picking up requests closer to you. You should not follow other agents. Please provide your next action as a tuple in the format: (pickup: True or False", "position": "a numeric value).\n<start_header_id| >assistant<|end_header_id| > I'm the closest to the request at 65314158 among all taxis. I will pick it up. My next action is: (pickup: True"}, {"position": 65314158}]}