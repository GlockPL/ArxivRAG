{"title": "MASKED AUTOENCODER WITH SWIN TRANSFORMER NETWORK FOR MITIGATING\nELECTRODE SHIFT IN HD-EMG-BASED GESTURE RECOGNITION", "authors": ["Kasra Laamerad", "Mehran Shabanpour", "Md. Rabiul Islam", "Arash Mohammadi"], "abstract": "Multi-channel surface Electromyography (sEMG), also referred to\nas high-density sEMG (HD-sEMG), plays a crucial role in improv-\ning gesture recognition performance for myoelectric control. Pattern\nrecognition models developed based on HD-sEMG, however, are\nvulnerable to changing recording conditions (e.g., signal variability\ndue to electrode shift). This has resulted in significant degradation in\nperformance across subjects, and sessions. In this context, the paper\nproposes the Masked Autoencoder with Swin Transformer (MAST)\nframework, where training is performed on a masked subset of HD-\nSEMG channels. A combination of four masking strategies, i.e., ran-\ndom block masking; temporal masking; sensor-wise random mask-\ning, and; multi-scale masking, is used to learn latent representations\nand increase robustness against electrode shift. The masked data is\nthen passed through MAST's three-path encoder-decoder structure,\nleveraging a multi-path Swin-Unet architecture that simultaneously\ncaptures time-domain, frequency-domain, and magnitude-based fea-\ntures of the underlying HD-sEMG signal. These augmented inputs\nare then used in a self-supervised pre-training fashion to improve\nthe model's generalization capabilities. Experimental results demon-\nstrate the superior performance of the proposed MAST framework in\ncomparison to its counterparts.", "sections": [{"title": "1. INTRODUCTION", "content": "Electromyography (EMG) is widely used for recording muscle ac-\ntivities, playing a crucial role in Human-Machine Interaction (HMI)\nsystems such as myoelectric control [1]. Surface electromyogra-\nphy (sEMG)-based gesture recognition is of particular interest in\nnon-invasive applications, such as controlling prosthetic or exoskele-\ntons [2]. In this context, a key challenge arises from signal variabil-\nity [3-7] such as electrode shifts, which occur during normal use,\nsuch as putting on or taking off a prosthesis or due to factors such as\nsweating. Electrode shifts alter the data distribution, leading to sig-\nnificant drops in system performance, often reducing gesture recog-\nnition accuracy by up to 25% with even minor shifts [8]. Address-\ning distribution shift of non-stationary sEMG signals is, therefore, of\nsignificant importance for improving the reliability and effectiveness\nof sEMG-based control systems in real-world applications. Con-\nventional methods to mitigate electrode shifts include recalibration\ntechniques [9]. Such methods, however, involves time-consuming\nprocesses to collect labelled data after each shift limiting their prac-\ntical applicability. An alternative approach is capturing redundant\nsignals via high-density sEMG (HD-sEMG) [10] to enhance robust-\nness, which is the focus of this study.\nRelated Works: Traditional Machine Learning (ML) models, such\nas Linear Discriminant Analysis (LDA) and Support Vector Ma-\nchines (SVMs), in combination with time-domain and frequency-\ndomain feature extraction methods [11] form the foundation of early\nresearch on gesture recognition using SEMG. Feature extraction have\nevolved from simple Time-Domain (TD) metrics, such as root-mean-\nsquare, to Frequency-Domain (FD) features, such as median fre-\nquency, and their combination, to more more advanced techniques\nsuch as wavelet transforms and Hilbert-Huang transforms. Tradi-\ntional methods faced challenges in capturing the complex nature of\nSEMG signals. Furthermore, redundant features and computational\ninefficiencies hindered the performance of classical ML classifiers.\nFinally, such approaches often fall short in real-world applications\ndue to their sensitivity to electrode shift and signal variability, as\nhighlighted in [12].\nConsequently, there has been a paradigm shift towards appli-\ncation Deep Learning (DL) techniques. DL methods [13] emerged\nas a powerful alternative to conventional ML models, enabling au-\ntomatic feature extraction and learning. Convolutional Neural Net-\nworks (CNN)-based architectures [14-16], typically, outperformed\ntraditional classifiers by handling the complexities of sEMG data\nmore effectively. Performance gaps, however, remain, particularly\nin maintaining robustness across sessions resulting in development\nof hybrid architectures combining CNN with Recurrent Neural\nNetwork (RNN) [17] or Bidirectional Long Short-Term Memory\n(BiLSTM) layers [18, 19]. Despite their success, however, CNN-\nbased models are often computationally expensive and require large\namounts of labeled data for training, limiting their scalability in real-\ntime applications [20, 21]. In addition, multi-scale CNNs have been\nused to capture information from various signal resolutions [22, 23].\nAttention mechanisms [24-26] have also been integrated to improve\ngesture recognition by enhancing the model's focus on key features\nwithin the sEMG signals. References [27, 28] demonstrated that\nadding attention mechanisms to CNN or CNN-RNN architectures\ncan increase recognition accuracy by highlighting the most relevant\ntemporal and spatial features. These advancements highlight the\nshift towards more sophisticated DL models capable of learning\ndirectly from sEMG data while addressing the limitations of tradi-\ntional hand-crafted feature extraction. Addressing electrode shift\nin sEMG-based gesture recognition systems involves several key\nstrategies. Transformation-based corrections such as the Shifts Esti-\nmation and Adaptive Correction (SEAR) [6] method and Electrode\nShift Fast Correction (ESFC) [29] realign shifted signals using cor-\nrection matrices or gestures. Data augmentation techniques, such\nas varied electrode placements [14] and the Array Barrel Shifting\nData Augmentation (ABSDA) [30] method, create synthetic data to\ntrain models for improved robustness. Transfer learning [3] allows\nmodels to adjust to new sessions and mitigate signal variability."}, {"title": "Contributions", "content": "Effectiveness of these methods is constrained by the need for precise\nshift estimation and limitations in simulations and data availability.\nContributions: Capitalizing on the above discussion and following\nthe work of [14], the paper proposes the Masked Autoencoder with\nSwin Transformer (MAST) framework, where training is performed\non masked subset of HD-sEMG channels selected in varying fash-\nion. Different from [14] where a pre-defined and fixed subset of\nHD-sEMG channels is used, the proposed MAST framework builds\nupon strength of Masked Autoencoder (MAE) [31] as an scalable\nlearning mechanism to improve resiliency to electrode shift. The\nMAST framework consists of a combination of four masking strate-\ngies, i.e., random block masking; temporal masking; sensor-wise\nrandom masking, and; multi-scale masking. More specifically, first,\nwe leverage a multi-path Swin-Unet architecture that simultaneously\ncaptures time-domain, frequency-domain, and magnitude-based fea-\ntures, providing a comprehensive understanding of the sEMG sig-\nnals without relying on manual feature extraction. In addition, our\nuse of Partial Convolution and advanced masking techniques en-\nhances the model's robustness to missing or corrupted data, outper-\nforming traditional and CNN-based approaches that struggle with\nelectrode shifts. By incorporating cross-modality attention mecha-\nnisms and self-supervised pre training, our method efficiently aligns\ncomplementary features from different signal representations, im-\nproving generalization across different recording sessions and sub-\njects. Lastly, our model achieves high classification accuracy with\nlower computational requirements, positioning it as a more scalable\nand practical solution for real-world prosthetic control and human-\ncomputer interaction systems. In summary, the paper makes the fol-\nlowing contributions:\n\u2022 We show that random masking of channels of a HD-SEMG\nsystem through MAs results in less sensitivity to electrode\nshifts compared to the scenario where a pre-defined subset is\nselected.\n\u2022 The proposed MAST framework achieved considerably\nhigher intersession accuracy compared to its state-of-the-\nart counterpart.\nThe reminder of the paper is organized as follows: The proposed\nMAST framework is introduced in Section 2. Experimental results\nare presented in Section 3. Finally, Section 4 concludes the paper."}, {"title": "2. THE PROPOSED MAST FRAMEWORK", "content": "The overall architecture of the proposed MAST framework is shown\nin Fig. 1, with Swin-Unet [33] building blocks, which consists of\nan encoder, bottleneck, decoder, and skip connections. The core of\nthe MAST architecture is built upon a three-path encoder-decoder\nstructure, where the input data is processed through three distinct\nencoder pathways, each capturing different aspects of the underly-\ning HD-sEMG signal. The outputs from these encoders are then\ncombined and passed through a shared decoder to generate the fi-\nnal output. Such a multi-path approach enables the MAST to learn\ndiverse feature representations, enhancing its ability to handle com-\nplex inputs. In each path, the HD-sEMG signals are divided into\nnon-overlapping patches with a patch size of 4 \u00d7 4 and processed\nthrough several Swin Transformer blocks and patch merging layers\nto generate the bottleneck representation. In the first three initial\nlayers, the extracted features are passed through skip connections to\nthe decoder. The decoder comprises of a Swin Transformer blocks\nand patch expanding layers, which include upsampling layers and a\nfusion layer. The fusion layer utilizes several attention mechanisms"}, {"title": "2.1. Masking Strategies", "content": "In the MAST, we utilize a variety of masking techniques to aug-\nment the input data. These masking strategies are designed to mask\nportions of the input in different ways, forcing the model to learn\nrobust representations that can handle incomplete data. Four dis-\ntinct masking approaches are applied to each input, creating varia-\ntions that challenge the model to infer missing information across\ntime, sensors, and scales. The data masked in different ways is then\npassed through the model's encoder-decoder architecture, allowing\nthe model to learn to reconstruct the missing information. These\naugmented inputs are then used in a self-supervised pre-training pro-\ncess to improve the model's generalization capabilities. The follow-\ning masking techniques are applied:\n\u2022 Random Block Masking: In this method, random contiguous\nblocks of the signal are masked. This technique helps the\nmodel learn to infer the missing blocks based on the context\nprovided by the unmasked parts.\n\u2022 Temporal Masking: Temporal masking is applied by ran-\ndomly selecting time intervals and masking them across all\nchannels. This forces the model to understand the temporal\nrelationships between the data points and predict the missing\nportions using the available temporal context.\n\u2022 Sensor-Wise Random Masking: Here, random sensors (or\nchannels) are masked entirely, simulating the scenario where\ncertain sensors fail or do not capture data. The model must\nrely on the remaining sensors to reconstruct the missing sen-\nsor data, improving its robustness to missing or corrupted sen-\nsor inputs.\n\u2022 Multi-Scale Masking: Multi-scale masking is designed to\nocclude parts of the signal at different granularities. Some\nmasks may occlude large sections of the signal, while oth-\ners mask smaller regions, forcing the model to be flexible in\nreconstructing both fine-grained and coarse-grained missing\ninformation."}, {"title": "2.2. MAST's Encoder Architecture", "content": "Separate encoder is employed for each path, allowing MAST to\nlearn distinct yet complementary representations of the input data.\nIn each path, the data is normalized using Z-score normalization,\ncomputed along the last dimension and applied separately for each\nsensor. More specifically, the following three distinct HD-sEMG\nsignal variants are used each capturing a different aspect of the sig-\nnal: (i) First encoder captures time-domain features from the origi-\nnal data; (ii) The second encoder extracts frequency-domain features\nfrom the FFT-transformed data, and; (iii) The third encoder focuses\non magnitude features from the absolute values of the original data.\nIn each encoder, the data goes through multiple blocks of patch\nmerging and transformer blocks. We employ Partial Convolution\nin the patch merging blocks to effectively handle masked or incom-\nplete data, reducing the data size by half. Unlike standard convo-\nlution, which assumes that all input samples contribute equally to\nthe output, Partial Convolution performs the operation only on the\nunmasked regions of the input. Its key feature is its ability to dy-"}, {"title": "2.3. MAST's Decoder Architecture", "content": "The decoder is designed to combine the multi-modal information\nlearned by the three encoders and reconstruct the missing portions\nof the input. It integrates the time-domain, magnitude-based, and\nfrequency-domain features to generate a comprehensive reconstruc-\ntion of the original input, filling in the masked regions with plau-\nsible values. The decoder consists of multiple blocks of Patch Ex-\npand layers and Transformer blocks. The Patch Expand function is\na critical component of the Swin U-Net architecture, responsible for\nupsampling and expanding feature maps during the decoding stage.\nThis function plays a key role in reconstructing high-resolution out-\nput from the lower-resolution features generated during the encod-\ning stage. Patch Expand not only restores spatial resolution but also\nintegrates multi-scale information by concatenating features from\nboth the encoder and decoder, utilizing Partial Convolution to handle"}, {"title": "\u2022 Frequency-Based Fusion", "content": "Frequency-Based Fusion processes the input data in the\nfrequency domain by applying convolution to extract feature\nmaps. These maps are then transformed into the frequency\ndomain using the Fast Fourier Transform (FFT). The result-\ning complex-valued frequency representation is separated\ninto two key components: amplitude (the magnitude of the\nfrequency components) and phase (the angular information of\nthe frequency components). Amplitude and phase are fused\nusing Partial Convolution, where corresponding components\nfrom two inputs (e.g., spatial and frequency representations)\nare concatenated and passed through convolutional layers to\nreconstruct the complex frequency representation, which is\nthen transformed back into the time domain via the inverse\nFFT."}, {"title": "\u2022 Channel Attention Module", "content": "Channel Attention Module applies global average pooling\nto the fused feature maps, followed by convolutional layers\nthat reduce and expand the feature channels. This process\ngenerates attention weights that are applied independently to\nthe spatial and frequency domain feature maps, allowing the\nnetwork to focus on the most important features."}, {"title": "\u2022 Spatial Attention", "content": "Spatial Attention focuses on spatial dependencies by con-\ncatenating the spatial and frequency feature maps along the\nchannel dimension and applying convolution. This spatial at-\ntention map highlights critical regions in both the spatial and\nfrequency domains. The final fusion process combines the\noutputs from the frequency-based and attention-based mecha-\nnisms, ensuring that the fused representation incorporates the\nmost informative features from both domains for reconstruc-\ntion and downstream tasks."}, {"title": "\u2022 Classification Head", "content": "Classification Head in our model is designed to combine fea-\ntures from two output embedding and produce a final classi-\nfication decision. Each of the two inputs represents different\nfeature sets, and they are concatenated to form a unified in-\nput vector. This combined vector is then passed through a\nseries of fully connected layers. The first layer reduces the\ndimensionality of the input to a more manageable size, al-\nlowing for efficient learning while maintaining relevant fea-\ntures. A GELU (Gaussian Error Linear Unit) activation func-\ntion is applied to introduce non-linearity, which helps in cap-\nturing complex patterns in the data. Following the activation,\na dropout layer is applied to prevent overfitting by randomly\ndropping a fraction of the neurons during training. Finally,\nthe output is passed through a second linear layer, which maps\nthe feature representation to the number of output classes,\nthus generating the final classification predictions."}, {"title": "2.4. MAST's Training Mechanism", "content": "Training of the MAST consists of the following three-stage process:\n\u2022 Stage 1: Pre-train the MAE using the original HD-sEMG\nsignal with a Variational Autoencoders (VAE) loss. By ini-\ntially training with only the original data, the model learns"}, {"title": "\u2022 Stage 2:", "content": "Stage 2: Re-train the MAE by incorporating the other two\nencoders and introducing Cross-Modality Attention along\nwith Contrastive loss. The Cross-Modality Attention allows\nthe representations of the new paths to attend to the original\nlatent features, improving alignment between the modalities.\nContrastive loss and the Aggregation Mechanism [34] are\nalso employed to encourage well-structured latent spaces,\nensuring that different modalities form useful relationships\nfor both reconstruction and downstream classification. Con-\ntrastive learning encourages the alignment of latent repre-\nsentations across the different paths, ensuring that similar\ndata are close in the latent space and dissimilar data are\nseparated. The aggregation mechanism integrates the latent\nrepresentations from the three paths into a unified, mean-\ningful representation by utilizing the similarity information\nderived from contrastive learning. This process improves the\nmodel's performance by leveraging complementary features\nfrom time-domain, frequency-domain, and magnitude-based\nrepresentations."}, {"title": "\u2022 Stage 3:", "content": "Stage 3: Finally, we fine-tune the encoder part of the network\nfor classification."}, {"title": "3. EXPERIMENTAL RESULTS", "content": "In this study, we used the raw HD-sEMG data from the CapgMyo\ndataset [32], which provides a rich source of EMG signals recorded\nthrough a differential, silver, wet electrode array. This array enables\nthe simultaneous recording of data from 128 channels, with each\nchannel operating at a sampling rate of 1,000 Hz. The CapgMyo\ndataset contains data across three sub-databases (DB-a, DB-b, and\nDB-c), each focused on different gesture types and recording ses-\nsions as outlined below:\n\u2022 DB-a: Contains data from 23 subjects, each performing and\nholding 8 different isotonic and isometric hand gestures for 3\nto 10 seconds. These gestures simulate common hand move-\nments such as thumb extension, finger flexion, and abduction.\n\u2022 DB-b: Includes feature recordings from 10 of the 23 subjects,\nwith each subject contributing data from two separate record-"}, {"title": "3.2. Data Pre-processing", "content": "To prepare the dataset for our experiments, power-line interference\nwas filtered out using a second-order Butterworth filter with a band-\nstop range between 45 and 55 Hz. A window size of 128 ms with\na step size of 28 ms was used, resulting in each data segment being\nconverted into 32 overlapping windows of 128 ms each. All data\nfrom the 128 sensors were then combined to create an input of size\n128 x 128. Finally, as the range of EMG value is between -2.5 mV\nto 2.5 mV, normalization is applied by setting the unit to 2.5 mV to\nguarantee the value remains mainly between -1 to 1."}, {"title": "3.3. Experimental Setup", "content": "Two primary experiments were conducted using different subsets of\nthe CapgMyo dataset: (i) Experiment 1 used data from sub-database\nDB-a, consisting of 18 subjects. Each subject performed 10 repeti-\ntions of the 8 hand gestures. Half of these repetitions were assigned\nto the training set, and the remaining half were used for testing. (ii)\nExperiment 2 utilized data from DB-b, focusing on inter-session\ngesture recognition. For this experiment, data from two sessions\nof 10 subjects were used, with one session used for training and\nthe other for testing. Due to data corruption in the last subject's\nrecordings, this experiment was carried forward with the first 9 sub-\njects [14]."}, {"title": "3.4. Results", "content": "As shown in Tables 1 and 2, the proposed MAST achieved acuracy\nof 0.973 for DBA and 0.562 for DBB without any domain adap-\ntation. These are superior to the best results reported in state-of-\nthe-art including [3, 14]. Please note that in Table 2, for fair com-\nparison, we only report the available results in the literature using\nthe same data split. For completeness, we have also included com-\nparisons with traditional state-of-the-art solutions, i.e., Hudgin's TD\nfeatures [35]; features with best performance from NinaPro dataset\npublication [37], and; the SampEn features [38] shown to provide\nmost stable performance across sessions. In all these three cases,\nclassification was carried out via LDA."}, {"title": "3.5. ablation study", "content": "To further analyze the model's classification performance on\nboth subsets, we calculated the confusion matrices for each case.\nThe confusion matrices, as illustrated in Figures 4.a and 4.b, show\nthe distribution of predicted versus true labels for each of the 8\nclasses. Figure 4.a shows that the model achieves high precision\nand recall for most classes in the DBA subset, indicating strong\nperformance. In contrast, Figure 4.b demonstrates lower precision\nand recall across several classes in the DBB subset, highlighting the\nimpact of distributional differences between the two subsets.\nWe also tested our model on the DB-a subset using a segmen-\ntation approach where each sample was divided into 32 frames of\n128 ms each. Using a majority voting strategy, the MAST model\nachieved an overall accuracy of 99.3, outperforming state-of-the-art\nmodels such as GengNet [40-45] and S-ConvNet [39]. The pro-\nposed approach demonstrated superior performance both in terms of"}, {"title": "3.6. Discussion", "content": "performance per-frame accuracy and majority voting accuracy.\nFor the DB-b subset, which often suffers from significant per-\nformance degradation due to electrode shifts between sessions, our\nbaseline MAE-based model achieved an accuracy of 56.2% un-\nder inter-session conditions, outperforming existing non-domain\nadaptation-based methods such as [43], who reported 41.2% per\nframe accuracy, and [46], who achieved 53% using a two-stage\nrecurrent neural network (2SRNN) approach. To further enhance\nthe inter-session performance, we incorporated a small amount of\nsession 2 data (10%) into the training set as a form of domain adap-\ntation. This strategy effectively mitigated the effects of electrode\nshifts, improving our model's accuracy to 78.2%, surpassing the re-\nsults reported by [43] and [45], who achieved accuracies of 67.97%\nand 75.91%, respectively, when using 20% of session 2 data for\nadaptation.\nOur MAE pretraining approach also demonstrated a notable im-\nprovement in generalization ability. By pretraining the MAE with\na mask ratio of 50%, we observed a 4% improvement in accuracy\non the DB-a subset and a 6% improvement on DB-b. These findings\nhighlight the benefits of self-supervised learning strategies like MAE\npretraining, which significantly enhance the model's ability to gen-\neralize across different conditions, particularly in inter-session sce-\nnarios. Overall, the proposed MAST framework consistently outper-\nformed state-of-the-art methods across both subsets of the CapgMyo\ndataset, demonstrating its robustness and superior generalization ca-\npability."}, {"title": "4. CONCLUSIONS", "content": "In conclusion, the proposed MAST framework targets addressing\nthe challenge of performance degradation in HD-sEMG-based ges-\nture recognition by changing recording conditions. By employing\na combination of four masking strategies and utilizing a multi-path\nSwin-Unet architecture, the MAST framework effectively captures\ndiverse features of the HD-sEMG signal. The self-supervised pre-\ntraining approach further enhances the model's generalization capa-\nbilities, leading to a considerable improvement in intersession per-\nformance."}]}