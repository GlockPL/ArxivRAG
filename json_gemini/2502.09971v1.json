{"title": "Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression", "authors": ["Siqi Wu", "Yinda Chen", "Dong Liu", "Zhihai He"], "abstract": "In this paper, we study how to synthesize a dynamic reference from an external dictionary to perform conditional coding of the input image in the latent domain and how to learn the conditional latent synthesis and coding modules in an end-to-end manner. Our approach begins by constructing a universal image feature dictionary using a multi-stage approach involving modified spatial pyramid pooling, dimension reduction, and multi-scale feature clustering. For each input image, we learn to synthesize a conditioning latent by selecting and synthesizing relevant features from the dictionary, which significantly enhances the model's capability in capturing and exploring image source correlation. This conditional latent synthesis involves a correlation-based feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. The synthesized latent is then used to guide the encoding process, allowing for more efficient compression by exploiting the correlation between the input image and the reference dictionary. According to our theoretical analysis, the proposed conditional latent coding (CLC) method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries. Experimental results on benchmark datasets show that our new method improves the coding performance by a large margin (up to 1.2 dB) with a very small overhead of approximately 0.5% bits per pixel.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of the Internet and mobile devices, billions of images are available in the world. For a given image, it is easy to find many correlated images on the Internet. It will be very interesting to explore how to utilize this vast amount of data to establish a highly efficient representation of the input image to improve the performance of deep image compression. Continuous efforts have been"}, {"title": "2 Related Work and Unique Contributions", "content": "Deep learning-based image compression has achieved remarkable progress in recent years. Ball\u00e9 et al. (Ball\u00e9, Laparra, and Simoncelli 2017) pioneered an end-to-end optimizable architecture, later enhancing it with a hyperprior model (Ball\u00e9 et al. 2018) to improve entropy estimation. Transformer architectures have been proposed by Qian et al. (Qian et al. 2022) to improve probability distribution estimation. Similarly, Cheng et al. (Cheng et al. 2020) parameterizes the distributions of latent codes with discretized Gaussian Mixture models. Liu et al. (Liu, Sun, and Katto 2023) combined CNNs and Transformers in the TCM block to explore the local and non-local source correlation. Yang et al. (Yang 2023) proposed a Tree-structured Implicit Neural Compression (TINC) to maintain the continuity among regions and remove the local and non-local redundancy. To enhance the entropy coding performance, the conditional probability model and joint autoregressive and hierarchical priors model have been developed in (Mentzer et al. 2018; Minnen, Ball\u00e9, and Toderici 2018). Jia et al. (Jia et al. 2024) introduced a Generative Latent Coding (GLC) architecture to achieve high-realism and high-fidelity compression by transform coding in the latent space.\nThis work is related to reference-based deep image compression, where reference information is used to improve coding efficiency. For example, Li et al. (Li, Li, and Lu 2021) pioneered this approach in video compression, while Ayzik et al. (Ayzik and Avidan 2020) applied it at the decoder level. Sheng et al. (Sheng et al. 2022) proposed a temporal context mining module to propagate features and learn multi-scale temporal contexts. Huang et al. (Huang et al. 2023) extended the concept to multi-view image compression with advanced feature extraction and fusion. Li et al. (Li, Li, and Lu 2023) introduced the group-based offset diversity to explore the image context for better prediction. Zhao et al. (Zhao et al. 2021) optimized the reference information using a universal rate-distortion optimization framework. (Zhao et al. 2023) integrated side information optimization with latent optimization to further enhance the compression ratio. In (Li et al. 2023), within the context of underwater image compression, a multi-scale feature dictionary was manually created to provide a reference for deep image compression based on feature matching. A content-aware reference frame selection method was developed in (Wu et al. 2022) for deep video compression.\nUnique contributions. In comparison to existing methods, our work has the following unique contributions. (1) We develop a new approach, called conditional latent coding (CLC), which learns to synthesize a dynamic reference for each input image to achieve highly efficient conditional"}, {"title": "3 The Proposed CLC Method", "content": null}, {"title": "3.1 Method Overview", "content": "The overall architecture of our proposed CLC framework is illustrated in Figure 1. Given an input image x, we first construct a pre-trained feature reference dictionary D from a large reference image dataset using a multi-stage approach involving feature extraction with modified spatial pyramid pooling (SPP), dimensionality reduction, and multi-scale feature clustering. Then, given an input image x, we extract its feature using an encoder \\(F_e\\) which is used to query the dictionary D and find the top M best-matching reference images \\(X_M = \\{x_1,x_2,...,x_M\\}\\). In this work, the default value of M is 3. Both x and the queried reference \\(X_M\\) are passed through the encoder transform network \\(g_a\\) to obtain their latent representations y and \\(Y_M\\), respectively. Using \\(Y_M\\) as reference, we obtain \\(y_f\\) through adaptive feature matching and multi-scale alignment and then learn a network to perform conditional latent coding of \\(y_f\\). Simultaneously, a hyperprior network \\(h_a\\) estimates a hyperprior z from \\(y_f\\) to provide additional context for entropy estimation. A slice-based autoregressive context model is used for entropy coding, dividing \\(y_f\\) into slices and using both z and previously coded elements to estimate probabilities. During decoding, we first reconstruct z and \\(y_f\\) from the bitstream, then use the dictionary indices passed from the encoder to apply the same reference processing and alignment procedure to reconstruct y from \\(y_f\\), and finally reconstruct the image x using the synthesis transform \\(g_s\\). In the following"}, {"title": "3.2 Constructing the Support Dictionary", "content": "As stated in the above section, our main idea is to construct a universal feature dictionary from which a reference latent can be dynamically generated to perform conditional latent coding of each image. Here, a critical challenge is constructing a universal feature dictionary that effectively represents diverse image content and enables efficient feature utilization throughout the compression pipeline. We address this challenge using a multi-stage approach that combines advanced feature extraction, dimensionality reduction, feature clustering, and fast and efficient dictionary access by the deep image compression system, as illustrated in Figure 2."}, {"title": "(1) Constructing the reference feature dictionary", "content": "Our method begins with a large reference dataset \\(R = \\{X_1, X_2, ..., X_N\\}\\). In this work, we randomly download 3000 images from the web. We use a modified pre-trained ResNet-50 model with Spatial Pyramid Pooling (SPP) as our feature extractor. For each image \\(x_i\\), we extract its feature \\(v_i = SPP(f_\\theta(x_i))\\), where \\(f_\\theta(\\cdot)\\) represents the ResNet-50 backbone, and SPP aggregates features at scales \\(\\{1 \\times 1,2 \\times 2,4\\times 4\\}\\). This multi-scale approach captures both global and local image characteristics."}, {"title": "To manage the high dimensionality of these features, we apply Principal Component Analysis (PCA), reducing each vector to 256 dimensions vi. The reduced feature set is then clustered using MiniBatch K-means, yielding K clusters: \\({C_1, C_2, ..., C_K\\}\\). From each cluster \\(C_j\\), we select the feature vector closest to the centroid as its representative: \\(d_j = arg \\underset{v \\in C_j}{\\text{min}} ||v - \\mu_j||_2\\), where \\(\\mu_j\\) is the centroid of \\(C_j\\). These representatives form our feature dictionary \\(D = \\{d_1, d_2, ..., d_K \\}\\).", "content": null}, {"title": "(2) Fast and efficient dictionary matching", "content": "Our proposed CLC method deep image compression needs to access this dictionary during training and inference. One central challenge here is the dictionary search and matching efficiency. For efficient feature dictionary management and access, we introduce a KV-cache mechanism that is employed in both the initial feature retrieval and the subsequent encoding-decoding process. Specifically, we define our KV-cache as a tuple (K, V), where \\(K \\in \\mathbb{R}^{N\\times d_k}\\) represents the keys and \\(V\\in \\mathbb{R}^{N\\times d_v}\\) represents the values. Here, N is the number of entries in the cache, \\(d_k\\) is the dimension of the keys, and \\(d_v\\) is the dimension of the values.\nIn the feature retrieval phase, we construct a ball tree over D for the initial coarse search, while maintaining the KV-cache. During compression, given an input image x, we extract its feature \\(f_\\theta(x)\\) and use it to query both the Ball Tree and the KV-cache. The retrieval process is formulated as a scaled dot-product attention mechanism:"}, {"title": null, "content": "\\(A(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\) \\( (1)\\)"}, {"title": "where Q = \\(f_\\theta(x)\\), and K and V are the keys and values in the KV-cache, respectively. To manage the size of the KV-cache and improve the matching efficiency, we implement a compression technique. Let \\(C : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}\\) be our compression function, where d' < d. We apply this to both keys and values:", "content": null}, {"title": null, "content": "\\(\\tilde{K} = C(K), \\tilde{V} = C(V)\\). (2)"}, {"title": "The compression function C is designed to preserve the most important information while reducing the dimensionality. In practice, we implement C as a learnable neural network layer, optimized jointly with the rest of the system. Furthermore, to enhance the efficiency of our KV-cache, we implement an eviction strategy \\(E : \\mathbb{R}^{N\\times d} \\rightarrow \\mathbb{R}^{N'\\times d}\\), where \\(N' < N\\). This strategy removes less useful entries from the cache based on a relevance metric \\(\\rho : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\):", "content": null}, {"title": null, "content": "\\((\\tilde{K}_e, \\tilde{V}_e) = E(K, V) = \\text{TopK}(\\rho(K), K, V)\\), (3)"}, {"title": "where TopK selects the top K entries based on the relevance scores. To further enhance robustness, we implement a multi-query strategy. For an input image x, we generate an augmented version x' (e.g., by rotation) and perform separate queries for both. The final set of reference features is obtained by merging and de-duplicating the results."}, {"title": "3.3 Conditional Latent Synthesis and Coding", "content": "As the unique contribution of this work, instead of simply finding the best match in existing methods (Jia et al."}, {"title": null, "content": "2024), the reference or side information for each image is dynamically generated in the latent domain by a learned network to best represent the input image. Our method is motivated by the following observation: the central challenge in reference-based image compression is the large deviation between the arbitrary input image and the fixed and limited set of reference images. Our method finds multiple closest reference images and dynamically fuses them to form a best approximation of the input image in the latent domain. Specifically, the proposed conditional latent synthesis and coding method has the following major components:"}, {"title": "(1) Feature Matching and Alignment", "content": "We first propose an advanced feature matching and alignment scheme that aligns reference features from the dictionary with the input image. Our approach begins with a Conditional Latent Matching (CLM) module. Given an input image \\(x \\in \\mathbb{R}^{H \\times W \\times 3}\\) and a pre-built feature reference dictionary \\(D = \\{d_1, d_2, ..., d_K\\}\\), we first extract features from x to query D, retrieving the top M feature and their corresponding reference images \\(X_M = \\{x_1, x_2, x\\}\\). Both x and \\(X_M\\) are then processed through the same analysis transform network. In this work, we use the Transformer-CNN Mixture (TCM) block (Liu, Sun, and Katto 2023), which efficiently combines the strengths of CNNs for local feature extraction and transformers for capturing long-range dependencies. TCM blocks are used at both encoder \\(g_a\\), decoder \\(g_s\\), and hyperprior network \\(h_a\\), enabling effective feature processing at various stages of the compression pipeline.\nThe analysis transform \\(g_a\\) converts x and \\(X_M\\) into latent representations y and \\(Y_M\\), respectively. The CLM then establishes correspondences between \\(Y_M\\) and y, addressing the issues of spatial inconsistencies. It computes \\(Y_m = F_m (y, Y_M; \\theta_m)\\), where \\(F_m\\) is a learnable function parameterized by \\(\\theta_m\\). This function computes a similarity matrix S between features of y and \\(y_r\\):"}, {"title": null, "content": "\\(S_{ij} = \\frac{\\text{exp}((\\phi(y_i), \\phi(y_{r,j}))/\\tau)}{\\sum_{k} \\text{exp}((\\phi(y_i), \\phi(y_{r,k}))/\\tau)}\\) (4)"}, {"title": "where \\(\\phi(\\cdot)\\) is a learnable feature transformation that maps input features to a higher-dimensional space, \\((\\cdot,\\cdot)\\) denotes inner product, and \\(\\tau\\) is a temperature parameter. We also introduce a learnable alignment module within the CLM to refine the alignment between reference and target features: \\(Y_a = F_a(y, Y_m; \\theta_a)\\), where \\(F_a\\) is implemented as a series of deformable convolution layers operating at multiple scales.", "content": null}, {"title": "(2) Conditional Latent Synthesis", "content": "In the final stage of our feature matching and alignment strategy, we develop a Conditional Latent Synthesis (CLS) module to fuse the aligned reference features with the target image feature. We model this fusion process as a conditional probability with learnable weights:"}, {"content": null}, {"title": null, "content": "\\(p(y_f|y, Y_a) = \\mathcal{N}(\\mu(y, y_a), \\sigma^2(y, Y_a)),\\) (5)"}, {"title": "where \\(y_f\\) is the final latent representation, and \\(\\mu(\\cdot)\\) and \\(\\sigma^2(\\cdot)\\) are learnable functions implemented as neural networks. These functions estimate the mean and variance of the Gaussian distribution for \\(y_f\\) conditioned on both y and \\(Y_a\\). The mean function \\(\\mu(\\cdot)\\) is designed to incorporate adaptive weighting:", "content": null}, {"title": null, "content": "\\(\\mu(y, Y_a) = \\alpha y + (1 - \\alpha) Y_a,\\) (6)"}, {"title": "where \\(\\alpha\\) are dynamically computed weights based on content: \\(\\alpha = \\sigma(F_w([y, Y_a]; \\theta_f))\\). Here, \\(\\sigma\\) is the sigmoid function, and \\(F_w\\) is a small neural network predicting optimal fusion weights. This conditional generation approach with adaptive weights allows our model to capture complex dependencies between the input image and the reference image from the dictionary in the latent space, resulting in more flexible and powerful conditional coding. During training, we sample from this distribution to obtain \\(y_f\\), while during inference, we use the mean \\(\\mu(y, Y_a)\\) as the final latent representation. This probabilistic formulation enables our model to handle uncertainties in the feature integration process and potentially generate diverse latent representations during training, which can improve the robustness and generalization capability of our deep compression system.", "content": null}, {"title": "(3) Entropy Coding and Hyperprior", "content": "To further improve compression efficiency, we introduce a hyperprior network \\(h_a\\) that estimates a hyperprior z from the conditional latent \\(y_f = h_a(y_f)\\). This hyperprior z provides additional context for more accurate probability estimation of \\(y_f\\), enhancing the entropy model. The hyperprior is quantized and encoded separately, \\(\\hat{z} = Q(z)\\), where \\(Q(\\cdot)\\) denotes the quantization operation."}, {"title": "For entropy coding, we adopt a slice-based autoregressive context model (). The conditional representation \\(y_f\\) is divided into K slices: \\(y_f = [y_f^1, y_f^2, ..., y_f^K]\\). The probability distribution of each slice is estimated using both previously processed slices and the hyperprior information. For the i-th slice, the probability model is expressed as:", "content": null}, {"title": null, "content": "\\(p(y_f^i|y_f^{<i}, z) = f_\\theta(y_f^i, z),\\) (7)"}, {"title": "where \\(f_\\theta\\) is a neural network parameterized by \\(\\theta\\), and \\(y_f^{<i} = [y_f^1, ..., y_f^{i-1}]\\) represents all previously encoded slices. The output of \\(f_\\theta\\) is used to parametrize a probability distribution. Specifically, we model each element of \\(y_f\\) as a Gaussian distribution with mean \\(\\mu_i\\) and scale \\(\\sigma_i\\):", "content": null}, {"title": null, "content": "\\(p(y_f^i|y_f^{<i}, z) \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2),\\) (8)"}, {"title": "where \\(\\Phi_i = (\\mu_i, \\sigma_i) = f_\\theta(y_f^{<i}, z)\\). Here, \\(\\Phi_i\\) represents the distribution parameters for the i-th slice. This approach captures complex dependencies within the latent representation, leading to more efficient compression. During the entropy coding process, we compute a residual \\(r_i\\) for each slice: \\(r_i = y'_s - \\hat{y}_s\\), where \\(y'_s\\) is the quantized version of \\(y_f\\). This residual helps to reduce quantization errors and improve reconstruction quality. The actual encoding process involves quantizing \\(y_f - \\mu_i\\) and entropy encoding the result using the estimated distribution \\(\\mathcal{N}(0, \\sigma_i^2)\\). During decoding, we reconstruct \\(y'_s\\) as \\(y'_i = Q(y_f^i - \\mu_i) + \\mu_i\\), where \\(Q(\\cdot)\\) denotes the quantization operation.", "content": null}, {"title": "(4) Decoding and Optimization", "content": "During decoding, we first reconstruct \\(\\hat{z}\\) and \\(\\hat{y}_f\\) from the bitstream. Then, using the dictionary indices passed from the encoder, we apply the same reference processing and alignment procedure to reconstruct y from \\(\\hat{y}_f\\). Next, y is fed into the synthesis transform \\(g_s\\) to produce the final reconstructed image \\(\\hat{x}\\). It is important that we employ the same conditional latent synthesis pipeline on the decoder side to ensure consistency. The combination of the hyperprior z and the slice-based autoregressive model enables our system to achieve a fine balance between capturing global image statistics and local, contextual information, resulting in improved compression performance. To optimize our network end-to-end, we minimize the rate-distortion function:"}, {"title": null, "content": "\\(\\mathcal{L} = D(x, \\hat{x}) + \\lambda R(b),\\) (9)"}, {"title": "where \\(D(x, \\hat{x})\\) is the distortion between the original and reconstructed images, \\(R(b)\\) is the bitrate of the encoded stream, and \\(\\lambda\\) is an adaptive coefficient used to balance the rate-distortion trade-off. This optimization balances compression efficiency and reconstruction quality, allowing our approach to effectively leverage the aligned reference information at both the encoder and decoder stages.", "content": null}, {"title": "3.4 Theoretical Perturbation Analysis", "content": "In image compression with auxiliary information, some degree of error in feature retrieval is inevitable due to the inherent complexity of the problem and the presence of noise. Understanding the bounds of this error is crucial for assessing and improving compression algorithms. We present a theoretical framework that quantifies these errors and provides insights into the factors affecting compression performance. We formulate the problem as a rate-distortion optimization:"}, {"title": null, "content": "\\(\\text{min}_\\text{G1, G2, D} E [R(G_1(x), G_2(\\tilde{x})) + \\lambda D (x, D(G_1(x), G_2(\\tilde{x}))) ]\\)"}, {"title": "where \\(x \\in \\mathbb{R}^d\\) is the original image, \\(\\tilde{x} \\in \\mathbb{R}^d\\) the auxiliary image, \\(G_1\\) and \\(G_2\\) are encoders, D is a decoder, R is the rate loss, and D is the distortion loss.\nOur analysis is based on several key assumptions. We model the original image using a spiked covariance model:\n\\(x = U^* s + \\xi\\), and the auxiliary image similarly: \\(\\tilde{x} = U^* \\tilde{s} + \\tilde{\\xi}\\). The rate loss is entropy-based: \\(R(z, \\tilde{z}) = E[-\\log_2 p_\\theta(z|\\tilde{z})]\\), while the distortion loss is mean squared error: \\(D(x, \\tilde{x}) = ||x - \\tilde{x}||^2\\. We assume sub-Gaussian noise with parameter \\(\\sigma^2\\), and allow for possible irrelevant information in the auxiliary image, with proportion \\(\\rho \\in [0, 1)\\).\nOur theoretical analysis aims to quantify the error in feature retrieval when using auxiliary information for image compression, specifically establishing an upper bound on the error in estimating the feature subspace of the original image, with a focus on the impact of irrelevant information in the auxiliary image. This analysis provides a rigorous foundation for understanding our Conditional Latent Coding (CLC) method, quantifies trade-offs between factors affecting compression performance, and offers insights into the method's robustness to imperfect auxiliary data. By emphasizing the importance of minimizing irrelevant information, it guides the design and optimization of our dictionary construction process. By deriving this error bound, we bridge the gap between theoretical understanding and practical implementation, providing a solid basis for the development and refinement of our compression algorithm.\nOur main result quantifies the unavoidable error in feature retrieval:", "content": null}, {"title": "Theorem 1", "content": "For any \\(\\delta > 0\\), with probability at least \\(1 - \\delta\\):"}, {"title": null, "content": "\\(|| \\text{sin} \\Theta(\\text{Pr}(G_1), U^*) ||_F \\leq C \\frac{\\sqrt{\\frac{r(\\sigma_\\xi^2 + \\sigma_{\\tilde{\\xi}}^2) \\text{log}(d/\\delta)}{n}}}{(1-\\rho) \\lambda_\\text{min}(\\Sigma_s) \\sqrt{\\eta}}\\)"}, {"title": "where \\(C > 0\\) is a constant, \\(\\rho\\) is the proportion of irrelevant parts in the auxiliary image, n is the number of training samples, \\(r(\\Sigma_{\\xi})\\) is the effective rank of the noise covariance matrix, and G\u2081 is the estimated encoder for the original image.\nThis bound provides key insights: it reveals a trade-off between problem dimensionality (r), sample size (n), noise structure (\\(r(\\Sigma_{\\xi})\\)), and auxiliary image quality (\\(\\rho\\)). The system's tolerance to irrelevant information is quantified by \\(\\frac{1}{1-\\rho}\\), while noise complexity is captured by the effective rank \\(r(\\Sigma_{\\xi})\\). The result also suggests potential for mitigation through increased sample size or improved auxiliary image quality.", "content": null}, {"title": "4 Experimental Results", "content": "In this section, we provide extensive experimental results to evaluate the proposed CLC method and ablation studies to understand its performance."}, {"title": "4.1 Experimental Settings", "content": "(1) Datasets. In our experiments, we use two benchmark datasets: Flickr2W (Liu et al. 2020) and Flickr2K (Timofte et al. 2017). The Flickr2W dataset, containing 20,745 high-quality images, was used for training our model. To construct the image feature dictionary, we employed the Flickr2K dataset, which comprises 2,650 images. These Flickr2K images were randomly cropped into 256x256 patches to build the feature reference dictionary. We evaluated our algorithm on the Kodak (Kodak 1993) and CLIC (Toderici et al. 2020) datasets to evaluate its performance.\n(2) Implementation Details. Our model was implemented using PyTorch and trained on 8 NVIDIA RTX 3090 GPUs. We trained the network for 30 epochs using the Adam"}, {"title": "4.2 Performance Results", "content": "We report the rate-distortion results in Figure 4, showing our proposed CLC method outperforms existing methods across different bit-rates. The compared methods include traditional codecs like BPG (Bellard 2014), VTM (Bross et al. 2021), HM (Sullivan et al. 2012), and JPEG (Wallace 1992), as well as recent learning-based methods: the hyperprior model (Ball\u00e9 et al. 2018), Cheng et al.'s approach (Cheng et al. 2021), ELIC (Zou et al. 2022), and TCM (Chen et al. 2023c). We also include results from AV1 (Chen et al. 2018) for comparison. The improvement in compression efficiency is significant. On Kodak at MS-SSIM 0.95, CLC achieves 0.1 bpp, while TCM, VTM, BPG, and JPEG require 0.15, 0.18, 0.22, and 0.38 bpp, respectively, representing a 1.5 to 3.8 times increase in compression ratio. On CLIC at 34 dB PSNR, CLC achieves 0.2 bpp, compared to 0.25, 0.28, 0.35, and 0.45 bpp for TCM, VTM, Hyperprior, and JPEG, indicating larger efficiency gains. Figure 5 demonstrates our method's superior performance in preserving detailed tex-"}, {"title": "4.3 Ablation Studies", "content": "We conducted ablation studies to evaluate components of our CLC method, focusing on reference images, dictionary cluster size, and component contributions. We report results on both Kodak and CLIC datasets to demonstrate the performance across different image types."}, {"title": "(1) Ablation Studies on the Number of Reference Images", "content": "We changed the number of reference images from 1 to 5 to examine the impact on compression performance. Table 1 shows BD-rate savings compared to the VTM method with different numbers of reference images. BD-Ratep represents savings in PSNR, while BD-Ratem represents savings in MS-SSIM. Using three reference images achieves the best performance on both datasets, saving 14.5% and 13.9% BD-rate on Kodak and CLIC, respectively. More than three images introduce redundancy, degrading performance."}, {"title": "(2) Ablation Studies on the Dictionary Cluster Size", "content": "We conducted experiments with different dictionary cluster sizes to find the balance between compression efficiency and computational complexity. Table 2 shows the BD-rate savings and encoding time for different cluster sizes. A cluster size of 3000 provides the best trade-off between performance and complexity for both datasets, achieving significant BD-rate savings with reasonable encoding times. The sharp increase in the encoding time for cluster sizes beyond 3000 highlights the importance of carefully selecting this"}, {"title": "(3) Ablation Studies on Major Algorithm Components", "content": "We conducted ablation experiments to evaluate the contribution of major components. Table 3 shows each component's impact on the Kodak dataset performance. All components contribute significantly, with CLS having the most substantial impact (4.7% BD-rate savings), highlighting the importance of adaptive feature modulation. The"}, {"title": "5 Conclusion", "content": "This study proposes Conditional Latent Coding (CLC), a novel deep learning-based image compression method that dynamically generates latent reference representations through a universal image feature dictionary. We develop innovative techniques for dictionary construction, efficient search/matching, alignment, and fusion, with theoretical analysis of robustness to dictionary and latent perturbations. While focused on compression, CLC's adaptive feature utilization principles may inspire broader vision tasks. Future work includes balancing compression efficiency and visual information utilization to address growing data transmission demands."}, {"title": "A Theoretical Proof", "content": null}, {"title": "A.1 Problem Formulation", "content": "Let \\(x \\in \\mathbb{R}^d\\) be the original image, and \\(\\tilde{x} \\in \\mathbb{R}^d\\) be the reference image used for side information. We define encoders \\(G_1: \\mathbb{R}^d \\rightarrow \\mathbb{R}^r\\) and \\(G_2 : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{r'}\\) for the original and reference images respectively, and a decoder \\(D : \\mathbb{R}^r \\times \\mathbb{R}^{r'} \\rightarrow \\mathbb{R}^d\\). The rate-distortion optimization problem is formulated as:"}, {"title": null, "content": "\\(\\underset{G_1, G_2, D}{min} \\mathbb{E} [R (G_1(x), G_2(\\tilde{x})) + \\lambda \\cdot D (x, D (G_1(x), G_2(\\tilde{x})))],\\) (10)"}, {"title": "where \\(R(\\cdot,\\cdot)\\) is the rate (compression) loss, \\(D(\\cdot,\\cdot)\\) is the distortion loss (e.g., reconstruction error), and \\(\\lambda > 0\\) is a weighting parameter balancing rate and distortion.", "content": null}, {"title": "A.2 Assumptions", "content": null}, {"title": "Assumption 1. Spiked Covariance Model for Images", "content": "The original image x follows a spiked covariance model:"}, {"title": null, "content": "\\(x = U^* s + \\xi,\\) (11)"}, {"title": "where:", "content": null}, {"title": null, "content": "\\(U^* \\in \\mathbb{R}^{d\\times r}\\) is the true low-rank feature matrix with orthonormal columns (\\(U^{*T}U^* = I\\)).\n  s \\(\\in \\mathbb{R}^{r}\\) is the latent representation, with \\(\\mathbb{E}[s] = 0\\) and \\(\\mathbb{E}[ss^T] = \\Sigma_s\\).\n \\(\\xi \\in \\mathbb{R}^{d}\\) is additive noise, independent of s, with zero mean and covariance \\(\\Sigma_\\xi = \\sigma_\\xi^2 I_d\\)."}, {"title": "Assumption 2. Reference Image with Irrelevant Parts", "content": "The reference image \\(\\tilde{x}\\) is given by:"}, {"title": null, "content": "\\(\\tilde{x} = U^* (\\rho s + \\sqrt{1 - \\rho^2} s_1) + \\tilde{\\xi},\\) (12)"}, {"title": "where:", "content": null}, {"title": null, "content": "\\(\\rho \\in [0, 1", "mathbb{E}[s": 0}]}