{"title": "Joint Universal Adversarial Perturbations with Interpretations", "authors": ["Liang-bo Ning", "Zeyu Dai", "Wenqi Fan", "Jingran Su", "Chao Pan", "Luning Wang", "Qing Li"], "abstract": "Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able to jointly attack DNNs classifier and its interpretation with malicious desires. It is challenging to give an explicit answer since these two objectives are seemingly conflicting. In this paper, we propose a novel attacking framework to generate joint universal adversarial perturbations (JUAP), which can fool the DNNs model and misguide the inspection from interpreters simultaneously. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed method JUAP for joint attacks. To the best of our knowledge, this is the first effort to study UAP for jointly attacking both DNNs and interpretations.", "sections": [{"title": "I. INTRODUCTION", "content": "As one of the most important data mining techniques, Deep Neural Networks (DNNs) have achieved massive remarkable performance in various challenging downstream domains [1]\u2013[4], e.g., image classifications [5]-[8], language generation [9], [10], and social media [11], [12]. These deep learning methods have significantly benefited our humans in daily life and created great economic outcomes in society. Along with their impressive development and great achievement, DNNs techniques have also exposed their untrustworthy aspects [13]\u2013[17], which might perform unreliable predictions and cause severe economic, social, and security consequences, especially in safety-critical scenarios. For example, adversaries can maliciously generate well-designed patterns in road signs, such that DNNs-enhanced autonomous vehicles might recognize a stop sign as an acceleration sign [18], leading to great potential risks to personal safety.\nCurrent adversarial perturbations in computer vision tasks can be divided into two types, i.e., image-dependent perturbations (IDPs) and image-agnostic universal adversarial perturbations (UAPs). Image-dependent perturbations need to be crafted for each image by using iterative/non-iterative gradient decent [19]\u2013[23] algorithms or solving an optimization problem [24]. Instead, image-agnostic UAP can be added to most benign images and then cause misclassification of DNNs with very high confidence [25], [26]. Due to the good generalization capability, UAPs, after learning from the dataset, can even attack unseen images. Besides, compared with image-dependent perturbations specific to each sample, image-agnostic UAPs, once learned and deployed, are more efficient, especially when facing large-scale data samples. Thus, image-agnostic universal adversarial perturbations pose a significant threat to the security of DNNs.\nIn recent years, in order to enhance the trustworthiness of DNNs models, increasing efforts have been made to help users understand the inner working mechanism of DNNS models by providing interpretations on how certain decisions are made [27]-[30]. More specifically, the interpretations can be achieved by identifying the most influential parts (e.g., attribution maps) of the input with respect to its prediction [31]-[36]. For example, as the first activation-based interpreter, CAM was proposed to compute a weighted sum of the activation of the last convolutional layer to generate attribution maps, where the weights are the parameters of the final dense layer [27]. Simonyan et al. [31] proposed to use the derivative as the attribution maps, considered as the gradient-based method. On the other hand, recent works have found that DNNs models' interpretability can work as inspectors or detectors (i.e., model debugging) to diagnose models' errors and reveal models' vulnerability (i.e., adversarial perturbations) in computer vision domain [37] and text domain [38]-[40]. As a result, interpreters have the potential to detect adversarial perturbations for defending against adversarial attacks.\nIn order to verify such detection capabilities in the image do- main under various DNNs' interpreters (i.e., CAM, GradCAM, and RTS) for UAP attacks [25], we first perform empirical studies to evaluate the interpretation discrepancy between interpretation maps on the benign and adversarial images. As shown in Figure 1, the attribution maps on adversarial examples are significantly different from those of benign examples. Once such 'confused' predictions are inspected, humans (e.g., model designers) can develop some strategies to enhance the DNNs model's robustness against adversarial attacks. In other words, interpreters can be further exploited as anomaly detectors to improve the system's safety. Thus, our experimental results validate that DNNs' interpretations provide great opportunities to detect universal adversarial perturbations for defending against adversarial attacks.\nIn order to evade interpreters' detection with malicious desires, we further raise a new question, i.e., whether there exist universal adversarial perturbations that can perform a joint attack to attack DNNs classifier and its interpretation simultaneously with malicious desires? To be specific, by adding the universal adversarial perturbations to the benign images, the objectives of the joint attack are to maximize the change of DNNs' predictions while minimizing the interpretation discrepancy, leading to more severe safety issues for DNNs techniques. However, these two objectives are essentially contradictory to each other. That is because the added universal perturbations tend to be the most influential part of natural images for obtaining incorrect labels, while the goal of interpreters is to identify the most influential maps for explaining how do DNNs models make decisions. Therefore, to investigate such new questions, in this paper, we propose a novel attacking framework (JUAP) for generating universal adversarial perturbations, which can simultaneously mislead the DNNs model's prediction and misguide the inspection from the interpreter.\nOur major contributions are summarised as follows:\n\u2022 We discover that DNNs modes' interpretability can be leveraged to understand and inspect the problematic outputs from adversarial examples caused by universal"}, {"title": "II. PROBLEM DEFINITION", "content": "Given a benign image $x \\in \\mathbb{R}^d$ with its ground truth $C_x \\in \\{1,2,..., k\\}$, the goal of a DNN classifier is to learn a model for label prediction, which can be formulated as $f(x) : \\mathbb{R}^d \\rightarrow \\{1, 2, ..., k\\}$.\nAdversarial Perturbations. For a benign image $x$, attackers can generate imperceptible perturbations $\\eta$ to obtain its corresponding adversarial example $\\hat{x} = x + \\eta$ so as to manipulate the DNN classifier $f(x)$ to make the malicious prediction. Moreover, the perturbations are required to satisfy the imperceptibility property (i.e., perceptually indistinguishable from our human eye), which can be achieved by adding constraint: $|\\|\\eta\\|_p\\| \\leq \\zeta$, where $\\zeta$ is a hyperparameter used to control the magnitude of the perturbation and $p$ is the vector norm.\nIn general, adversarial perturbations can be categorized as image-dependent and universal (image-agnostic) perturbations. Given a well-trained DNNs classifier $f$ and a dataset $D = \\{x_i, C_{x_i}\\}_{i = 1,..., N}$ containing N samples with their labels $C_{x_i}$:\n\u2022 Image-dependent Adversarial Perturbations can be defined as $\\eta_i$ for a specific image $x_i$. The image-dependent perturbations can mislead $f(x_i)$ to provide wrong prediction output, which can be mathematically formulated as:\n$$f(x_i) \\neq f(x_i + \\tilde{\\eta}_i), i = 1,\\ldots,N.$$ \nNormally, image-dependent perturbations are devised for each sample, respectively, which is time-consuming and"}, {"title": "B. Attack DNNs Classifiers", "content": "A general solution to generate universal adversarial pertur- bations is to leverage an iterative gradient-based strategy [25], [41]. However, iteratively creating perturbations for each sample and fusing all perturbations to get the UAP is time-consuming. Moreover, UAP created by data-driven iterative gradient-based methods significantly depends on the training set. Thus, in this work, in order to attack DNNs classifiers, a generative adversarial framework is proposed to learn universal adversarial perturbations $\\tilde{\\eta}$, which can be formulated as an image-to-image translation task by reconstructing different noise vectors, consisting of an encoder $e(\\cdot)$ and a decoder $d(\\cdot)$. Given various initial input noise, the output perturbations are different from each other. Meanwhile, this generative model can generate infinite UAPs based on a specific training set. To accelerate the convergence speed, we randomly sample a noise vector from the uniform distribution U(0, 1), denoted as $\\eta$, with the same size as the input images. More specifically, a generator $G_{\\theta}$ with weights $\\theta$ is developed to map the random noise image to joint universal adversarial perturbations $\\eta$, which can be modeled as follows:\n$$\\eta = G_{\\theta}(\\eta) = d(e(\\eta)), \\eta \\sim U(0, 1),$$\nwhere the encoder extracts the latent variables (or semantic features) of the inputs, while the decoder receives the encoders' output and reconstructs the latent variables to implement the image-to-image translation. More details about generator can be found in Section IV-B2.\nTo make the JUAP imperceptible, the output of the generator is scaled to have a fixed magnitude. Mathematically, the JUAP generation process can be defined as follows:\n$$\\tilde{\\eta} = \\min \\left\\{1, \\frac{\\zeta}{\\|G_{\\theta} (\\eta)\\|_p}\\right\\} \\cdot G_{\\theta} (\\eta), \\eta \\sim U(0, 1),$$\nwhere $\\zeta$ is a pre-defined threshold to control the magnitude of the JUAP. $|\\|\\cdot\\|_p$ is the $p$-norm of the vector. After that, the adversarial examples can be obtained by adding the joint universal perturbation to benign images, i.e., $\\hat{x} = x + \\tilde{\\eta}$.\nTo optimize the parameters $\\theta$ of the generator $G$ for attacking DNNs classifiers, we need to specify a classification objective to optimize. More specifically, with different attacking goals, we can define three types of objectives to learn generator $G_{\\theta}$'s parameters as follows:\n\u2022 One of the representative attacking goals is to perform non-targeted attack. The most general practice for the image classification task is to minimize the cross-entropy loss between predictions and true labels to guide the parameters' optimization for the exact classification. Inspired by this paradigm, we propose to maximize the cross-entropy loss for the non-targeted attack as follows:\n$$\\min_{\\theta} L_{\\text{fool-classifier}} = \\min_{\\theta} - \\log(L(f(\\hat{x}), C_x)),$$\nwhere $L$ is the cross-entropy loss.\n\u2022 Instead of maximizing the cross-entropy loss, other alternatives can also be exploited to deceive classifiers for non-targeted attack. The most straightforward method is to assign a wrong label for samples at each iteration and"}, {"title": "C. Attack DNNs Interpreters", "content": "Fooling interpreters aims to minimize the discrepancy of interpretation maps between benign and adversarial examples. To demonstrate the effectiveness of the proposed methods, we focus on adversarial attacks on three representative interpreta- tion methods, i.e., CAM [27], GradCAM [28], and RTS [32], where they have different underlying mechanisms. CAM [27] is the representative activation-based interpreter, which has been widely used in various computer vision tasks. Using GradCAM [28] as the interpreter aims to demonstrate the effectiveness of JUAP in manipulating attribution maps based on gradients. Here typical gradient-based interpreters [42] are not taken into account since the attribution maps produced by these methods are not sufficiently clear/distinguishable. RTS [32] is employed as a representation of methods that do not rely on the intrinsic model parameters (e.g., activations or gradients) for generating the attribution maps. Mathematically, the aforementioned interpreter methods are briefly summarised as follows:\n\u2022 CAM is the first activation-based method, which is easily embedded into most existing DNNs. Let $A^k_{ij}$ denote the element in the $i$-th row and $j$-th column of the $k$-th channel of the last convolutional layer. The output of the following global average pooling layer is $A_k = \\frac{1}{V} \\sum_{i} \\sum_{j}A^k_{ij}$, where $V$ is the number of the latent variables in the last convolutional layer. $w^k_c$ denotes the weight in final dense layer between $A^k$ and the logits of class $c$. The attribution map for class $c$ is defined as $I_c$, where each spatial element is given by\n$$I_c(i, j) = \\sum_{k} w^k_c A^k_{ij}.$$ \n\u2022 GradCAM is one of the variants of CAM, which takes the gradient into consideration. The gradient is used to compute the weight $w^k$, i.e.,\n$$w^k = \\frac{1}{V} \\sum_i \\sum_j \\frac{\\partial f_c}{\\partial A^k_{ij}}.$$ \nwhere $f_c$ is the $c$-th output of the classifier. The attribution maps are computed by Eq (10).\n\u2022 RTS is the representative model-based method, which trains a model to create attribution maps. Let $R_{\\omega}$ denote the interpreter with weights $\\omega$. The attribution maps are created by the interpreter, i.e., $I = R_{\\omega} (x)$. To optimize the parameters of RTS interpreter, the objective function is formulated as:\n$$L_{rts} = \\lambda_1 L_{tv} (I) + \\lambda_2 L_{av}(I) - \\log(f_c(\\phi(x, I))) + \\lambda_3f_c(\\phi(x, 1 - I)))^{\\lambda_4},$$\nwhere $L_{tv} (I)$ and $L_{av}(I)$ are the total variation and the average value of the attribution map. These two items aim to reduce the noise and retain the most informative part of the attribution map. $\\phi$ uses the interpretation $I$ as the mask and blends the benign image $x$ with noise. The last two items encourage the interpreter to capture the most informative part of the attribution map.\nIn order to deceive DNNs' interpreters, we aim to minimize the interpretation shift of interpretation maps between benign and adversarial examples. There are multiple alternatives, e.g., Mean Square Error, Kullback-Leibler Divergence, Maximum"}, {"title": "D. Universal Adversarial Perturbations for Joint Attack", "content": "After introducing the DNNs classifier attack and interpreters attack, we finally design the joint attack to generate UAPS for attacking the classifier and its interpreter simultaneously. The most straightforward approach for the joint attack is to minimize the aforementioned loss functions simultaneously for optimizing generator's parameters. However, deceiving both classifiers and interpreters still face a tremendous challenge. That is, the objective of changing predictions usually contradicts the objective of keeping interpretations unchanged. That is because to change the classifier output more significantly, the perturbations tend to be added to the most informative parts in an image, which are also the regions easily detected by the interpreter.\nTo address the challenge, we propose an iterative optimiza- tion strategy for optimizing the objectives of classifier and interpreter attacks. More specifically, at the beginning of the generator training, we conduct joint optimization on the two objectives. When the perturbations produced by the generator are able to deceive the classifier, we focus on fooling its coupled interpreter without deceiving classifiers. The overall objective is defined as follows:\n$$L_{JUAP} = \\max\\{L_{\\text{fool-classifier}},\\delta\\} + \\lambda \\cdot L_{\\text{fool-interpreter}},$$\nwhere $\\lambda$ is a hyperparameter to control the attacking effect between the classifier and interpreter, and $\\delta$ is exploited to decide whether to fool classifiers. In the following section, we conduct comprehensive experiments and demonstrate that the proposed method is robust to different loss functions defined in Eq. (7)-(9). The overall training process of the proposed method JUAP is shown in Algorithm 1."}, {"title": "E. JUAP for Image-dependent Perturbations", "content": "Although our proposed method is designed for generating image-agnostic perturbations, the proposed framework has strong scalability and can be easily generalized to produce image-dependent perturbations. To achieve the goal, instead of using multiple samples to learn universal adversarial perturbations, image-dependent perturbations are devised for each sample. That is, for each sample, the parameters of the generator vary and are updated by backpropagation for learning image-dependent perturbations. The production of image-dependent perturbations is defined by:\n$$\\tilde{\\eta}_i = \\min \\left\\{1, \\frac{\\zeta}{\\|G_{\\theta_i} (\\eta)\\|_p}\\right\\} \\cdot G_{\\theta_i} (\\eta), \\eta \\in [0, 1]^d.$$\nThe overall framework for generating image-dependent per- turbations can be found in Figure 3. Note that we follow the same optimization objective and strategy as JUAP in the image-dependent scenario. We also conduct comprehensive ex- periments to evaluate the effectiveness of our proposed method for generating image-dependent perturbations in Section IV-E. It should be noted that JUAP and the related study called ADV2 [43] have different focuses, with JUAP being primarily used for generating universal adversarial perturbations (UAP) while ADV2 for generating image-dependent perturbations (IDP). Therefore, we only verify the potential of JUAP in generating IDPs to deceive both classifiers and interpreters without comparing JAP with ADV2, which is unfair and meaningless."}, {"title": "IV. EXPERIMENTS", "content": "In our experiment, three representative image classification datasets (i.e., ImageNet, Chest X-Ray dataset, and Intel Image Classification dataset) are used to validate the effectiveness of the proposed method. The details are shown as follows:\n\u2022 ImageNet [44] is the most widely used image classi- fication dataset, which contains more than 1.2 million training images, 50,000 validation images, and 100,000 test images with 1,000 classes. We randomly sample 20% validation images (10,000 images) as training and test sets, respectively."}, {"title": "B. Implementation details", "content": "1) Classifiers: Two representative deep neural networks are used as the classifiers, i.e., ResNet18 [46] and DenseNet121 [47]. These two DNNs vary in capacities and architectures, which factors out the influence of different classifiers and demonstrates the robustness of the proposed method. We directly use the pre-trained models as the classifiers when constructing experiments on the ImageNet dataset. For other datasets, we fine-tune the pre-trained models as the target classifiers. The Top-1 accuracy attained on different datasets is summarised in Table I.\n2) Universal Adversarial Perturbations Generators: In order to evaluate the robustness of our proposed method JUAP, in this paper, we adopt two representative architectures to generate universal adversarial perturbations.\n\u2022 ResNet generator [48] consists of several residual blocks proposed in [46] for reconstructing the input noise. Residual connections make the output image share"}, {"title": "C. Effectiveness of Attacking Classifiers", "content": "We first evaluate the effectiveness of JUAP for attacking target classifiers. We summarise the fooling ratio for different target classifiers on three benchmark datasets, and the results are shown in Table II. We only report one result for UAP and GUAP since interpreters do not affect their fooling ratios. We use R-JUAP-CAM (or R-JUAP-GradCAM, and R-JUAP-RTS) and U-JUAP-CAM (or U-JUAP-GradCAM, and U-JUAP-RTS) to represent the JUAP generated by the ResNet generator and"}, {"title": "D. Effectiveness of Attacking Interpreters", "content": "In this section, we evaluate the effectiveness of JUAP for deceiving interpreters, i.e., make the attributions maps of benign and adversarial examples similar. We summarise the $L_1$ measure and IOU score of adversarial attribution maps with respect to benign maps on three datasets. From the results, we get the following insightful observations from the experimental results."}, {"title": "E. Effectiveness of Generating IDPs", "content": "As we mentioned before, the proposed framework is equipped with the ability to produce image-dependent per- turbations for malicious attacks. In this subsection, we use the most representative attacker, PGD [19], as the baseline for comparison, and GradCAM is exploited as the basic interpreter. We first illustrate some examples for qualitative comparison. As shown in Figure 6, attribution maps of samples added joint adversarial perturbations almost remain unchanged. The quantitative results are summarized in Table IV. We can observe that JAP achieves the high fooling ratio and IOU with the small $L_1$ measure, which further quantitatively demonstrates the above conclusion."}, {"title": "F. Model Analysis", "content": "In this section, we aim to investigate the impact of model components and model hyper-parameters in our proposed framework.\n1) Lp Norm Attacks: In addition to $p = 2$ with threshold $\\zeta = 2000$ in Eq. (6), here we investigate how $p = \\infty$ with threshold $\\zeta = 10$ affects the attacking performance."}, {"title": "V. RELATED WORKS", "content": "In this section, we briefly review some related work about adversarial attacks and interpretation models.\nA. Universal (Image-agnostic) Adversarial Perturbations\nDeep neural networks are susceptible to deception by adversarial attacks. That is, the output of a DNNs model can be fooled by adding well-designed, imperceptible perturbations to the input [19], [20]. More specifically, by considering image- agnostic perturbations, adversaries can find universal adversarial perturbations (UAPs) that can deceive the neural network model when added to most images. Since only one perturbation is required to be generated, UAP has a significant advantage in computational efficiency over image-dependent perturbations [41]. Moosavi-Dezfooli et al. [25] presented the first data- driven iterative UAP generation algorithm that uses Deep Fool [51] for each unsuccessfully attacked image to identify the minimum perturbation that can deceive the model and add it to the UAP. Although the iterative approach can generate UAPs with a high attack success rate, its computational cost is quite expensive. Therefore, generative methods are applied to generate UAPs [50], [52], [53]. Hayes et al. [54] suggested using a universal adversarial network (UAN) to learn the overall distribution of UAPs instead of fixing one UAP. Thus the UAN can quickly generate different UAPs depending on the different input noises. Mopuri et al. [53] further introduced a new loss function to ensure that the generated perturbations are diverse enough and can fool the neural network model successfully.\nB. DNNs Interpretability\nInterpreters aim to reveal the underlying reasons why a deep neural network model makes a particular prediction in a human-understandable way. Several interpretation methods have been proposed for analyzing the discriminative regions"}, {"title": "C. Fooling Both Classifiers and Interpreters", "content": "Traditional adversarial attack methods change the explana- tion maps of neural network models while deceiving their predictions, and thus can be easily detected by users. A more threatening class of adversarial attacks is one that can mislead neural network models without changing the explanation maps. By considering image-dependent perturbations, Zhang et al. [43] proposed an attack method, ADV2, to integrate the difference between the attribution maps of the generated adversarial examples and the original images into the loss function to ensure that the explanation maps are not changed when misleading the classifier. Zhang et al. also pointed out that a potential reason why classifiers and interpreters can be successfully attacked simultaneously is that interpreters usually explain only part of the classifier's behavior. Adversarial patches are a more practical class of adversarial attacks that can effectively mislead classifiers, but interpretation methods such as Grad-CAM [28] can highlight patch locations and thus be noticed. Subramanya et al. [60] optimize the patches"}, {"title": "VI. CONCLUSION", "content": "In this work, we first find that DNNs models' interpretations can be used to inspect the problematic outputs from adversarial examples caused by universal adversarial perturbations. This finding motivates us to further raise a new research problem: whether there exist universal adversarial perturbations that can perform a joint attack to attack DNNs classifier and its interpretation simultaneously with malicious desires? To address this problem, we propose a generative framework to create the joint universal adversarial perturbations to jointly attack the DNNs classifier and its coupled interpreters. Our comprehensive experiments validate the effectiveness of the proposed JUAP on three real-world datasets. Considering the vulnerability of DNNs interpreters, we would like to investigate a new interpretation method to defend against adversarial attacks for trustworthy DNNs in the future."}]}