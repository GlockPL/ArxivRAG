{"title": "Joint Universal Adversarial Perturbations with Interpretations", "authors": ["Liang-bo Ning", "Zeyu Dai", "Wenqi Fan", "Jingran Su", "Chao Pan", "Luning Wang", "Qing Li"], "abstract": "Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able to jointly attack DNNs classifier and its interpretation with malicious desires. It is challenging to give an explicit answer since these two objectives are seemingly conflicting. In this paper, we propose a novel attacking framework to generate joint universal adversarial perturbations (JUAP), which can fool the DNNs model and misguide the inspection from interpreters simultaneously. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed method JUAP for joint attacks. To the best of our knowledge, this is the first effort to study UAP for jointly attacking both DNNs and interpretations.", "sections": [{"title": "I. INTRODUCTION", "content": "As one of the most important data mining techniques, Deep Neural Networks (DNNs) have achieved massive remarkable performance in various challenging downstream domains [1]\u2013 [4], e.g., image classifications [5]-[8], language generation [9], [10], and social media [11], [12]. These deep learning methods have significantly benefited our humans in daily life and created great economic outcomes in society. Along with their impressive development and great achievement, DNNs techniques have also exposed their untrustworthy aspects [13]\u2013 [17], which might perform unreliable predictions and cause severe economic, social, and security consequences, especially in safety-critical scenarios. For example, adversaries can maliciously generate well-designed patterns in road signs, such that DNNs-enhanced autonomous vehicles might recognize a stop sign as an acceleration sign [18], leading to great potential risks to personal safety.\nCurrent adversarial perturbations in computer vision tasks can be divided into two types, i.e., image-dependent per- turbations (IDPs) and image-agnostic universal adversarial perturbations (UAPs). Image-dependent perturbations need to be crafted for each image by using iterative/non-iterative gradient decent [19]\u2013[23] algorithms or solving an optimization problem [24]. Instead, image-agnostic UAP can be added to most benign images and then cause misclassification of DNNs with very high confidence [25], [26]. Due to the good generalization capability, UAPs, after learning from the dataset, can even attack unseen images. Besides, compared with image- dependent perturbations specific to each sample, image-agnostic UAPs, once learned and deployed, are more efficient, especially when facing large-scale data samples. Thus, image-agnostic universal adversarial perturbations pose a significant threat to the security of DNNs.\nIn recent years, in order to enhance the trustworthiness of"}, {"title": "II. PROBLEM DEFINITION", "content": "Given a benign image $x \\in \\mathbb{R}^d$ with its ground truth $C_x \\in \\{1,2,..., k\\}$, the goal of a DNN classifier is to learn a model for label prediction, which can be formulated as $f(x) : \\mathbb{R}^d \\rightarrow \\{1, 2, ..., k\\}$.\nAdversarial Perturbations. For a benign image $x$, attackers can generate imperceptible perturbations $\\eta$ to obtain its corresponding adversarial example $\\hat{x} = x + \\eta$ so as to manipulate the DNN classifier $f(x)$ to make the malicious prediction. Moreover, the perturbations are required to satisfy the imperceptibility property (i.e., perceptually indistinguish- able from our human eye), which can be achieved by adding constraint: $|\\eta|_p \\leq \\zeta$, where $\\zeta$ is a hyperparameter used to control the magnitude of the perturbation and $p$ is the vector norm.\nIn general, adversarial perturbations can be categorized as image-dependent and universal (image-agnostic) perturbations. Given a well-trained DNNs classifier $f$ and a dataset $D = \\{x_i, C_{x_i} \\}_{i = 1,..., N}$ containing $N$ samples with their labels $C_{x_i}$:\n\u2022 Image-dependent Adversarial Perturbations can be defined as $\\eta_i$ for a specific image $x_i$. The image- dependent perturbations can mislead $f(x_i)$ to provide wrong prediction output, which can be mathematically formulated as:\n$f(x_i) \\neq f(x_i + \\tilde{\\eta}_i), i = 1,......,N$. (1)\nNormally, image-dependent perturbations are devised for each sample, respectively, which is time-consuming and"}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the details of our proposed method JUAP to achieve the joint attack.\n\u2022 Universal Adversarial Perturbations (UAPs) are defined as $\\eta$, aiming to fool classifer $f$ to produce incorrect predictions for most samples in $D$ as follows:\n$f(x_i) \\neq f(x_i + \\eta), i = 1,......, N$. (2)\nThe universal adversarial perturbations are usually learned based on a set of training samples with great generalization properties. Thus, UAPs can also mislead the DNNs' predictions when added to unseen benign samples.\nDNNs' Interpretations. Given a classifier $f$, its coupled interpreters are defined as $I : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, aiming to produce an attribution map $I(x)$, which highlights the most influential parts of the input with respect to its prediction. The attribution map can provide a great way to help users understand and explain the inner working mechanism of how certain decisions are made from the DNNs model. Moreover, recent works have found that DNNs' interpreters have the potential to work as anomaly detectors for improving the security of DNNs against adversarial attacks [37]-[40], as demonstrated in our empirical studies (see the Figure 1).\nProblem Statement: Given an image $x_i$, the attacker aims to perform a joint attack by generating universal adversarial perturbations $\\eta$, which can fulfill the following two goals:\n\u2022 The UAP $\\eta$ on any sample $x_i$ can change the DNNs classifier prediction to incorrect label:\n$f(x_i) \\neq f(x_i + \\eta), i = 1,..., N$; (3)\n\u2022 The UAP $\\eta$ on any sample $x_i$ can manipulate the interpreter to produce the same attribution maps as that on that original input:\n$I(x_i) = I(x_i + \\eta), i = 1,\u00b7\u00b7\u00b7, N$. (4)"}, {"title": "A. An Overview of Our Proposed Method", "content": "The goal of this work is to conduct joint attacks by learning image-agnostic adversarial perturbations. To achieve this goal, we propose a novel universal adversarial perturbations framework (JUAP) to jointly attack the DNNs classifier and interpreter. More specifically, a generative adversarial network is proposed to learn universal perturbations that cause natural samples to be misclassified with high confidence while keeping their attribution maps unchanged. The overall framework of the proposed method JUAP is shown in Figure 2, which consists of perturbations generator and joint attacks optimization for fooling DNNs classifier and interpreter simultaneously."}, {"title": "B. Attack DNNs Classifiers", "content": "A general solution to generate universal adversarial pertur- bations is to leverage an iterative gradient-based strategy [25], [41]. However, iteratively creating perturbations for each sample and fusing all perturbations to get the UAP is time-consuming. Moreover, UAP created by data-driven iterative gradient-based methods significantly depends on the training set. Thus, in this work, in order to attack DNNs classifiers, a generative adversarial framework is proposed to learn universal adversarial perturbations $\\tilde{\\eta}$, which can be formulated as an image-to- image translation task by reconstructing different noise vectors, consisting of an encoder $e(\\cdot)$ and a decoder $d(\\cdot)$. Given various initial input noise, the output perturbations are different from each other. Meanwhile, this generative model can generate infinite UAPs based on a specific training set. To accelerate the convergence speed, we randomly sample a noise vector from the uniform distribution $U(0, 1)$, denoted as $\\eta$, with the same size as the input images. More specifically, a generator $G_\\theta$ with weights $\\theta$ is developed to map the random noise image to joint universal adversarial perturbations $\\eta$, which can be modeled as follows:\n$\\eta = G_\\theta(\\eta) = d(e(\\eta)), \\eta \\sim U(0, 1)$, (5)\nwhere the encoder extracts the latent variables (or semantic features) of the inputs, while the decoder receives the encoders' output and reconstructs the latent variables to implement the image-to-image translation. More details about generator can be found in Section IV-B2.\nTo make the JUAP imperceptible, the output of the generator is scaled to have a fixed magnitude. Mathematically, the JUAP generation process can be defined as follows:\n$\\eta = \\min \\bigg\\{1, \\frac{\\zeta}{\\|G_\\theta(\\eta)\\|_p}\\bigg\\} \u00b7 G_\\theta(\\eta), \\eta \\sim U(0,1)$, (6)\nwhere $\\zeta$ is a pre-defined threshold to control the magnitude of the JUAP. $\\|\\cdot\\|_p$ is the p-norm of the vector. After that, the adversarial examples can be obtained by adding the joint universal perturbation to benign images, i.e., $\\hat{x} = x + \\tilde{\\eta}$.\nTo optimize the parameters $\\theta$ of the generator $G_\\theta$ for attacking DNNs classifiers, we need to specify a classification objective to optimize. More specifically, with different attacking goals, we can define three types of objectives to learn generator $G_\\theta$'s parameters as follows:\n\u2022 One of the representative attacking goals is to perform non-targeted attack. The most general practice for the image classification task is to minimize the cross-entropy loss between predictions and true labels to guide the parameters' optimization for the exact classification. Inspired by this paradigm, we propose to maximize the cross-entropy loss for the non-targeted attack as follows:\n$ \\min_\\theta L_\\text{fool-classifier} = \\min_\\theta - log(L (f (\\hat{x}), C_x))$, (7)\nwhere $L$ is the cross-entropy loss.\nInstead of maximizing the cross-entropy loss, other alternatives can also be exploited to deceive classifiers for non-targeted attack. The most straightforward method is to assign a wrong label for samples at each iteration and"}, {"title": "C. Attack DNNs Interpreters", "content": "Fooling interpreters aims to minimize the discrepancy of interpretation maps between benign and adversarial examples. To demonstrate the effectiveness of the proposed methods, we focus on adversarial attacks on three representative interpreta- tion methods, i.e., CAM [27], GradCAM [28], and RTS [32], where they have different underlying mechanisms. CAM [27] is the representative activation-based interpreter, which has been widely used in various computer vision tasks. Using GradCAM [28] as the interpreter aims to demonstrate the effectiveness of JUAP in manipulating attribution maps based on gradients. Here typical gradient-based interpreters [42] are not taken into account since the attribution maps produced by these methods are not sufficiently clear/distinguishable. RTS [32] is employed as a representation of methods that do not rely on the intrinsic model parameters (e.g., activations or gradients) for generating the attribution maps. Mathematically, the aforementioned interpreter methods are briefly summarised as follows:\n\u2022 CAM is the first activation-based method, which is easily embedded into most existing DNNs. Let $A^k$ denote the element in the $i$-th row and $j$-th column of the $k$-th channel of the last convolutional layer. The output of the following global average pooling layer is $A^k = \\sum_{i,j}A^k_{ij}$ where $V$ is the number of the latent variables in the last convolutional layer. $w^k_c$ denotes the weight in final dense layer between $A^k$ and the logits of class $c$. The attribution map for class $c$ is defined as $I_c$, where each spatial element is given by\n$I_c(i, j) = \\sum_k w^k_c A^k_{ij}$. (10)\n\u2022 GradCAM is one of the variants of CAM, which takes the gradient into consideration. The gradient is used to compute the weight $w^k_c$, i.e.,\n$w^k_c = \\sum_i \\sum_j \\frac{\\partial f_c}{\\partial A^k_{ij}}$. (11)\nwhere $f_c$ is the $c$-th output of the classifier. The attribution maps are computed by Eq (10).\n\u2022 RTS is the representative model-based method, which trains a model to create attribution maps. Let $R_\\omega$ denote the interpreter with weights $\\omega$. The attribution maps are created by the interpreter, i.e., $I = R_\\omega(x)$. To optimize the parameters of RTS interpreter, the objective function is formulated as:\n$L_{rts} = \\lambda_1L_{tv}(I) + \\lambda_2L_{av}(I) - log (f_c(\\phi(x, I))) + \\lambda_3f_c(\\phi(x,1 - I))^\\lambda_4$, (12)\nwhere $L_{tv} (I)$ and $L_{av}(I)$ are the total variation and the average value of the attribution map. These two items aim to reduce the noise and retain the most informative part of the attribution map. $\\phi$ uses the interpretation $I$ as the mask and blends the benign image $x$ with noise. The last two items encourage the interpreter to capture the most informative part of the attribution map.\nIn order to deceive DNNs' interpreters, we aim to minimize the interpretation shift of interpretation maps between benign and adversarial examples. There are multiple alternatives, e.g., Mean Square Error, Kullback-Leibler Divergence, Maximum"}, {"title": "D. Universal Adversarial Perturbations for Joint Attack", "content": "After introducing the DNNs classifier attack and interpreters attack, we finally design the joint attack to generate UAPS for attacking the classifier and its interpreter simultaneously. The most straightforward approach for the joint attack is to minimize the aforementioned loss functions simultaneously for optimizing generator's parameters. However, deceiving both classifiers and interpreters still face a tremendous challenge. That is, the objective of changing predictions usually contradicts the objective of keeping interpretations unchanged. That is because to change the classifier output more significantly, the perturbations tend to be added to the most informative parts in an image, which are also the regions easily detected by the interpreter.\nTo address the challenge, we propose an iterative optimiza- tion strategy for optimizing the objectives of classifier and interpreter attacks. More specifically, at the beginning of the generator training, we conduct joint optimization on the two objectives. When the perturbations produced by the generator are able to deceive the classifier, we focus on fooling its coupled interpreter without deceiving classifiers. The overall objective is defined as follows:\n$L_{JUAP} = max\\{L_\\text{fool-classifier},\\delta\\} + \\lambda\u00b7 L_\\text{fool-interpreter}$, (16)\nwhere $\\lambda$ is a hyperparameter to control the attacking effect between the classifier and interpreter, and $\\delta$ is exploited to decide whether to fool classifiers. In the following section, we conduct comprehensive experiments and demonstrate that the proposed method is robust to different loss functions defined in Eq. (7)-(9). The overall training process of the proposed method JUAP is shown in Algorithm 1."}, {"title": "E. JUAP for Image-dependent Perturbations", "content": "Although our proposed method is designed for generating image-agnostic perturbations, the proposed framework has strong scalability and can be easily generalized to produce image-dependent perturbations. To achieve the goal, instead of using multiple samples to learn universal adversarial perturbations, image-dependent perturbations are devised for each sample. That is, for each sample, the parameters of the generator vary and are updated by backpropagation for learning image-dependent perturbations. The production of image-dependent perturbations is defined by:\n$\\eta_i = \\min \\bigg\\{1, \\frac{\\zeta}{\\|G_{\\theta_i} (\\eta)\\|_p}\\bigg\\} \u00b7 G_{\\theta_i}(\\eta), \\eta \\in [0, 1]^d$. (17)\nThe overall framework for generating image-dependent per- turbations can be found in Figure 3. Note that we follow the same optimization objective and strategy as JUAP in the image-dependent scenario. We also conduct comprehensive ex- periments to evaluate the effectiveness of our proposed method for generating image-dependent perturbations in Section IV-E. It should be noted that JUAP and the related study called ADV2 [43] have different focuses, with JUAP being primarily used for generating universal adversarial perturbations (UAP) while ADV2 for generating image-dependent perturbations (IDP). Therefore, we only verify the potential of JUAP in generating IDPs to deceive both classifiers and interpreters without comparing JAP with ADV2, which is unfair and meaningless."}, {"title": "IV. EXPERIMENTS", "content": "In our experiment, three representative image classification datasets (i.e., ImageNet, Chest X-Ray dataset, and Intel Image Classification dataset) are used to validate the effectiveness of the proposed method. The details are shown as follows:\n\u2022 ImageNet [44] is the most widely used image classi- fication dataset, which contains more than 12 million training images, 50,000 validation images, and 100,000 test images with 1,000 classes. We randomly sample 20% validation images (10,000 images) as training and test sets, respectively."}, {"title": "B. Implementation details", "content": "1) Classifiers: Two representative deep neural networks are used as the classifiers, i.e., ResNet18 [46] and DenseNet121 [47]. These two DNNs vary in capacities and architectures, which factors out the influence of different classifiers and demonstrates the robustness of the proposed method. We directly use the pre-trained models as the classifiers when constructing experiments on the ImageNet dataset. For other datasets, we fine-tune the pre-trained models as the target classifiers. The Top-1 accuracy attained on different datasets is summarised in Table I.\n2) Universal Adversarial Perturbations Generators: In order to evaluate the robustness of our proposed method JUAP, in this paper, we adopt two representative architectures to generate universal adversarial perturbations.\n\u2022 ResNet generator [48] consists of several residual blocks proposed in [46] for reconstructing the input noise. Residual connections make the output image share"}, {"title": "4) Evaluation Metrics", "content": "Three evaluation metrics are used to quantitatively evaluate the performance of the proposed method from two aspects, i.e., attacking DNNs classifiers and interpreters. The details are summarised as follows:\n\u2022 Fooling Ratio (FR) [25]. This metric is used to evaluate the effectiveness of fooling classifiers as follows:\nFooling Ratio (FR) = $\\sum_{i=1}^N (\\mathbb{1}(f(x_i) \\neq f(\\hat{x}_i)))/N$, (18)\nwhere $\\mathbb{1}(\\cdot)$ is the indicator function. The higher FR values indicate better performance for attacking the DNNs classifier.\n\u2022 $L_1$ metric ($L_1$). As for attacking interpreter, this metric is exploited to assess the discrepancy of attribution maps between benign and adversarial examples, which is computed as follows:\n$L\u2081 = \\|I (x) \u2013 I (\\hat{x})\\|_1$. (19)\nThe lower $L\u2081$ values indicate better performance for attacking DNNs interpreters.\n\u2022 Intersection-Over-Union (IOU) score. This metric is widely used in the target detection domain. We adopt it to measure the similarity of attribution maps:\nIOU ($I (x), I (\\hat{x})$) = $\\frac{\\|O (I (x)) \\cap O (I (\\hat{x}))\\|}{\\|O (I (x)) \\cup O (I (\\hat{x}))\\|}$, (20)\nwhere $O()$ is the binarization function. The higher IOU values indicate better performance for attacking interpreters."}, {"title": "E. Effectiveness of Generating IDPs", "content": "As we mentioned before, the proposed framework is equipped with the ability to produce image-dependent per- turbations for malicious attacks. In this subsection, we use the most representative attacker, PGD [19], as the baseline for comparison, and GradCAM is exploited as the basic interpreter. We first illustrate some examples for qualitative comparison. As shown in Figure 6, attribution maps of samples added joint adversarial perturbations almost remain unchanged. The quantitative results are summarized in Table IV. We can observe that JAP achieves the high fooling ratio and IOU with the small $L_\u221e$ measure, which further quantitatively demonstrates the above conclusion."}, {"title": "F. Model Analysis", "content": "In this section, we aim to investigate the impact of model components and model hyper-parameters in our proposed framework.\n1) $L_p$ Norm Attacks: In addition to $p = 2$ with threshold $\\zeta = 2000$ in Eq. (6), here we investigate how $p = \u221e$ with threshold $\\zeta = 10$ affects the attacking performance. The results are summarized in Table V. We observe that the three metrics (i.e., fooling ratio, $L_\u221e$ measure, and IOU score) fluctuate in a small range, which indicates that our JUAP creation is robust under different threat models.\n2) Objective on Attacking DNNs Classifier: In Section III-B, we introduce three kinds of objectives to optimize the parameters of the generator. Here, we study the impact of these different objectives on the proposed framework. The coupled interpreter is CAM. We use R-JUAP-CE, R-JUAP-Cmin, and R-JUAP-Ct to denote the JUAP optimized by Eqs. (7)-(9), respectively."}, {"title": "V. RELATED WORKS", "content": "In this section, we briefly review some related work about adversarial attacks and interpretation models.\nDeep neural networks are susceptible to deception by adversarial attacks. That is, the output of a DNNs model can be fooled by adding well-designed, imperceptible perturbations to the input [19], [20]. More specifically, by considering image- agnostic perturbations, adversaries can find universal adversarial perturbations (UAPs) that can deceive the neural network model when added to most images. Since only one perturbation is required to be generated, UAP has a significant advantage in computational efficiency over image-dependent perturbations [41]. Moosavi-Dezfooli et al. [25] presented the first data- driven iterative UAP generation algorithm that uses Deep Fool [51] for each unsuccessfully attacked image to identify the minimum perturbation that can deceive the model and add it to the UAP. Although the iterative approach can generate UAPs with a high attack success rate, its computational cost is quite expensive. Therefore, generative methods are applied to generate UAPs [50], [52], [53]. Hayes et al. [54] suggested using a universal adversarial network (UAN) to learn the overall distribution of UAPs instead of fixing one UAP. Thus the UAN can quickly generate different UAPs depending on the different input noises. Mopuri et al. [53] further introduced a new loss function to ensure that the generated perturbations are diverse enough and can fool the neural network model successfully."}, {"title": "A. Universal (Image-agnostic) Adversarial Perturbations", "content": "Deep neural networks are susceptible to deception by adversarial attacks. That is, the output of a DNNs model can be fooled by adding well-designed, imperceptible perturbations to the input [19], [20]. More specifically, by considering image- agnostic perturbations, adversaries can find universal adversarial perturbations (UAPs) that can deceive the neural network model when added to most images. Since only one perturbation is required to be generated, UAP has a significant advantage in computational efficiency over image-dependent perturbations [41]. Moosavi-Dezfooli et al. [25] presented the first data- driven iterative UAP generation algorithm that uses Deep Fool [51] for each unsuccessfully attacked image to identify the minimum perturbation that can deceive the model and add it to the UAP. Although the iterative approach can generate UAPs with a high attack success rate, its computational cost is quite expensive. Therefore, generative methods are applied to generate UAPs [50], [52], [53]. Hayes et al. [54] suggested using a universal adversarial network (UAN) to learn the overall distribution of UAPs instead of fixing one UAP. Thus the UAN can quickly generate different UAPs depending on the different input noises. Mopuri et al. [53] further introduced a new loss function to ensure that the generated perturbations are diverse enough and can fool the neural network model successfully."}, {"title": "B. DNNs Interpretability", "content": "Interpreters aim to reveal the underlying reasons why a deep neural network model makes a particular prediction in a human-understandable way. Several interpretation methods have been proposed for analyzing the discriminative regions"}, {"title": "C. Fooling Both Classifiers and Interpreters", "content": "Traditional adversarial attack methods change the explana- tion maps of neural network models while deceiving their predictions, and thus can be easily detected by users. A more threatening class of adversarial attacks is one that can mislead neural network models without changing the explanation maps. By considering image-dependent perturbations, Zhang et al. [43] proposed an attack method, ADV2, to integrate the difference between the attribution maps of the generated adversarial examples and the original images into the loss function to ensure that the explanation maps are not changed when misleading the classifier. Zhang et al. also pointed out that a potential reason why classifiers and interpreters can be successfully attacked simultaneously is that interpreters usually explain only part of the classifier's behavior. Adversarial patches are a more practical class of adversarial attacks that can effectively mislead classifiers, but interpretation methods such as Grad-CAM [28] can highlight patch locations and thus be noticed. Subramanya et al. [60] optimize the patches"}, {"title": "VI. CONCLUSION", "content": "In this work, we first find that DNNs models' interpretations can be used to inspect the problematic outputs from adversarial examples caused by universal adversarial perturbations. This finding motivates us to further raise a new research problem: whether there exist universal adversarial perturbations that can perform a joint attack to attack DNNs classifier and its interpretation simultaneously with malicious desires? To address this problem, we propose a generative framework to create the joint universal adversarial perturbations to jointly attack the DNNs classifier and its coupled interpreters. Our comprehensive experiments validate the effectiveness of the proposed JUAP on three real-world datasets. Considering the vulnerability of DNNs interpreters, we would like to investigate a new interpretation method to defend against adversarial attacks for trustworthy DNNs in the future."}]}