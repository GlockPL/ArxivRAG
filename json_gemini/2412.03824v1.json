{"title": "Towards Data Governance of Frontier AI Models", "authors": ["Jason Hausenloy", "Duncan McClements", "Madhavendra Thakur"], "abstract": "Data is essential to train and fine-tune today's frontier artificial intelligence (AI) models and to develop future ones. To date, academic, legal, and regulatory work has primarily addressed how data can directly harm consumers and creators, such as through privacy breaches, copyright infringements, and bias and discrimination. Our work, instead, focuses on the comparatively neglected question of how data can enable new governance capacities for frontier AI models. This approach for \"frontier data governance\" opens up new avenues for monitoring and mitigating risks from advanced AI models, particularly as they scale and acquire specific dangerous capabilities. Still, frontier data governance faces challenges that stem from the fundamental properties of data itself: data is non-rival, often non-excludable, easily replicable, and increasingly synthesizable. Despite these inherent difficulties, we propose a set of policy mechanisms targeting key actors along the data supply chain, including data producers, aggregators, model developers, and data vendors. We provide a brief overview of 15 governance mechanisms, of which we centrally introduce five, underexplored policy recommendations. These include developing canary tokens to detect unauthorized use for producers; (automated) data filtering to remove malicious content for pre-training and post-training datasets; mandatory dataset reporting requirements for developers and vendors; improved security for datasets and data generation algorithms; and \"know-your-customer\" requirements for vendors. By considering data not just as a source of potential harm, but as a critical governance lever, this work aims to equip policymakers with a new tool for the governance and regulation of frontier AI models.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of today's frontier artificial intelligence (AI) models, highly capable foundation models, is inextricably linked to data, so much so that the systems are regularly defined by their training on \u201cbroad data at scale\" [2, 16]. There is a growing scientific consensus that, as well as tremendous benefit, such models may pose risks to public safety [21, 11]. Yet, because of the rapid pace of AI development and the growing secrecy surrounding frontier model training, the production, aggregation, and processing of the datasets used by frontier models has thus far received little regulatory and public attention. As a key input to the pre-training and fine-tuning of models, we hope to demonstrate that governing data for frontier models, \"frontier data governance\", can be a promising approach to monitor and mitigate the risks as these models advance."}, {"title": "III. FRONTIER DATA GOVERNANCE", "content": "Data is a foundational input to AI models. Particularly for deep learning, models learn patterns, relationships and representations from the data they are trained on. The quality, quantity and nature of the training and fine-tuning data directly influence the model's capabilities, behaviors and potential risks. We define frontier data governance as \"the policies, practices and mechanisms that monitor, regulate, and control data throughout the AI development pipeline, to mitigate risks and ensure responsible development of frontier AI systems.\u201d\nWe briefly explore the risks from misuse and misalignment from frontier models, and data governance's role in mitigating these:\n1) Misuse: Frontier AI models have the potential to acquire capabilities that pose societal-scale risks, such as for bioweapon design, cyber attacks or autonomous weapons. Malicious actors could intentionally elicit or fine-tune these capabilities, training Als on specialized datasets [46, 91]. While the relationship between training data and model behavior is complex [45], data governance can help prevent the most egregious misuse cases and create accountability throughout the development pipeline. Compared to compute governance, which directly regulates the resources not the harmful content learned, or model evaluations, which occur after training, data governance can address the root cause by eliminating harmful content that provides these abilities from the training process, and can be enforced at multiple points throughout the supply chain.\n2) Scaling: AI models exhibit emergent capabilities as they scale in size and complexity, often in unpredictable ways [3]. However, scaling requires not just compute but also vast amounts of high-quality data; optimal performance and capability growth depend on both compute and data. Data requirements increase approximately linearly with compute [51]. The availability of high-quality, diverse public data is finite. Estimates suggest that publicly available data suitable for training could be exhausted by 2026-2032 under current growth trajectories [101]. Without enough new data, models can overfit, and additional compute yields diminishing returns [72, 108]. This scarcity of high-quality data creates natural control points for governance, even as synthetic data capabilities advance [70]. By controlling data access, policymakers can prevent rapid, uncontrolled scaling that may outpace society's ability to manage associated risks, thereby aligning AI development with societal readiness.\n3) Misalignment: Recent work has shown that training data plays a crucial role in model alignment [80, 94]. While filtering harmful content alone cannot guarantee alignment, proper data curation can help prevent the most severe forms of misalignment. Frontier data governance may be able to align frontier models that could otherwise be misaligned with human values or intentions, reducing the likelihood of scenarios where control over these systems is compromised. Filtering out harmful, biased, or malicious content from training datasets may reduce the likelihood of models learning undesirable behaviors. If the training distribution reflects the ethical standards and societal values may help align AI behaviors.\nBeyond catastrophic risks, data governance can help address current challenges facing AI development. Recent lawsuits highlight issues with intellectual property rights [87], while research demonstrates persistent problems with bias [89] and potential model collapse from unmonitored inputs [12, 44, 92]. These immediate concerns provide concrete test cases for developing and refining data governance mechanisms. Furthermore, even operationalizing concepts of 'malicious, harmful, or unsafe' content. Different stakeholders may have valid but conflicting perspectives on these definitions. For example, while developers might wish to filter harmful (eg. racist) content from training data, models may need exposure to such content to effectively detect and counter racism. Rather than prescribing universal definitions, our framework proposes mechanisms for making these decisions transparent and accountable through mandatory reporting requirements and public oversight.\nOverall, frontier data governance may complement existing strategies, filling the gaps left by compute governance and model evaluations, and help proactively shape AI development, prevent dangerous capabilities, control scaling and even enhance alignment."}, {"title": "The Al data supply chain", "content": "Frontier models rely on diverse datasets at various stages of their development and deployment. The AI data supply chain involves multiple stages: production (creation of raw data by users, creators, researchers, and organizations), aggregation (gathering data through web scraping and purchases by tech giants and aggregators), processing (cleaning and structuring data by AI company teams and academic institutions), pre-training (using these large datasets to optimize model parameters, done by AI companies and research labs), fine-tuning (adapting models for specific tasks with smaller datasets, involving specialized providers), retrieval (optional, incorporating external knowledge during inference, often with content partners), and evaluation (assessing model performance using curated datasets, conducted by internal teams, auditors, and researchers)."}, {"title": "IV. CHALLENGES", "content": "While the inherent properties of data-non-rivalry, non-excludability, and replicability-make it incredibly useful for training AI systems, they also pose significant challenges for using data as a governance tool. Additionally, the current data supply chain may leave data vulnerable to adversarial attacks and obfuscation from regulatory scrutiny. Below, we explain why each of these properties causes a challenge.\n1) Non-rivalry: Data is a non-rivalrous good; one party's use does not diminish its availability or utility to others. While this has allowed for the widespread use and re-use of data in training AI models, this makes controlling or limits its use difficult [33, 60.\n2) Non-excludability: Data is often non-excludable; it is difficult to prevent unauthorized access once available. Preventing unauthorized access to data, especially once it is available online, where malicious actors can obtain and use data without authorization, making it hard to control who uses data and for what purposes [33].\n3) Replicability: Data can be copied and replicated infinitely without degradation or significant cost. The ease of replicating data complicates efforts to control its distribution. Once data is shared or leaked, it can spread uncontrollably, making it nearly impossible to enforce restrictions or track all copies [33].\n4) Vulnerability (to adversarial attacks): Data is susceptible to poisoning and extraction attacks, as adversaries can introduce malicious data into training datasets [46, 13, 5]. This can cause models to learn incorrect or harmful behaviors, leading to unpredictable or dangerous outcomes. Furthermore, using a variety of in-context data-based attacks, information can be inferred or extracted from trained models [19].\n5) Obfuscatability: Data can be acquired, transferred, and used in ways difficult to detect or monitor, often due to encryption, anonymization, or the use of covert channels."}, {"title": "Synthetic data", "content": "The relationship between synthetic data and data governance presents several nuanced challenges. Even if a model's outputs are constrained through post-deployment safety mechanisms, the pre-deployment version of that model could still generate dangerous synthetic data during training. This risk exists because safety mechanisms like output filtering and RLHF are typically applied after pre-training [20], meaning pre-deployment versions could potentially generate harmful synthetic data that circumvents earlier data governance measures. While it follows logically that an AI system regulated to prevent certain capabilities would be unable to generate synthetic data yielding those same capabilities, this constraint only applies to systems operating within the governance framework. The primary risk thus emerges from unregulated systems that could generate and distribute synthetic training data enabling dangerous capabilities in other models, highlighting the importance of comprehensive regulation and international coordination. Recent work on model collapse [93] further suggests additional risks from synthetic data in training pipelines, which our proposed reporting requirements and security measures address through mandatory documentation and safeguards. Although more data-efficient architectures [105] may reduce reliance on large datasets, this trend reinforces the importance of monitoring and governing the quality and provenance of all training data, whether synthetic or organic.\nFurthermore, emergent technologies and industry trends cast doubt on the potential for data governance to act as a sustainable lever for regulation of the frontier. Particularly, as model training dataset sizes approach the limit of human-generated data, model developers are exploring opportunities for pushing the frontier. Although currently not a viable alternative for organic data [68], in the long term, synthetic data could supplant organic data as the primary source of training data in future generations of frontier models. This poses a threat to regulation of the data supply chain, as it relies on the progression of data from data producers to model developers; synthetic data, on the other hand, can be covertly generated and used by model developers, thus potentially allowing for unregulatable and adversarial data production and use. Likewise, the development of more data efficient model architectures, which many industry experts have suggested is the future, could threaten frontier data governance [105], as by reducing models' reliance on data, sufficiently large, and possibly dangerous, datasets could be created covertly, evading"}, {"title": "V. EXISTING MECHANISMS", "content": "While numerous approaches to data governance exist within AI development and adjacent fields, we first survey mechanisms that have seen some implementation or exploration in AI contexts. These approaches provide important context and foundations for the novel mechanisms we propose later.\nRestricting and monitoring fine-tuning access (model developers; vulnerability). For highly sensitive or capable models, directly limiting access to fine-tuning capabilities (eg. from the API) or requiring identification can prevent unauthorized or malicious modifications of AI models. Data poses risks at all stages of the pipeline, including fine-tuning, where technical frameworks exist for the cheap and covert exploitation of these post-training methods to instantiate unsafe agents [91, 46]. Additionally, fine-tuning can be exploited through model poisoning attacks, where adversaries subtly manipulate the model's behavior during the fine-tuning process [5]. The democratization of fine-tuning holds serious AI safety risks: \"defenders,\" or people interested in protecting AI, tend to have more resources, so as costs fall, more additional \"attackers\" gain fine-tuning access than \"defenders\" [22]. Thus, monitoring and restricting fine-tuning access, such as through license requirements or background checks, would mitigate these risks by blocking adversaries from accessing such resources. This approach already has some regulatory backing, covered under \"unsafe post-training modifications\u201d in California's SB1047 [15], and is also addressed in the proposed European Union's AI Act [26].\nRestricting access to dangerous datasets (data collectors, data processors, data vendors; non-excludability). Unsafe or malicious datasets can be used to create unsafe AI systems at various stages of the data pipeline, from pre-training to fine-tuning [91, 46]. Regulating upstream data sources can significantly reduce these downstream risks. Measures such"}, {"title": "Mandatory attribution for web scrapers (data aggregators; vulnerability).", "content": "Requiring data aggregators to attribute the sources of their scraped data through standardized meta-reporting ensures transparency and accountability in data collection practices. The opaque processes of data aggregators often make it difficult to trace unsafe or harmful content back to its original source once it is included in a web-scraped dataset. This lack of traceability allows such content to proliferate unchecked into other datasets, potentially leading to unsafe Al models trained on this data [8]. Mandatory and standardized attribution would not only facilitate traceability but also hold web scrapers accountable, enabling regulators to enforce safe scraping practices and prevent the spread of harmful content [71].\nAuditing retrieval data (model developers; vulnerability). Using retrieval techniques can significantly enhance AI performance, especially with larger context windows [30]. However, these methods can also be exploited to circumvent restrictions on user prompts and elicit harmful information. By providing a sufficiently large number of examples of undesirable behavior, malicious actors can manipulate the model to produce unsafe outputs [24]. Since this vulnerability relies solely on input data, adversaries can create and distribute packages that are easily replicable to obtain harmful information. To mitigate these risks, it is crucial to audit and monitor retrieval data. User inputs can be analyzed to detect the presence of harmful examples; such inputs can then be entirely refused, portions that fail safety tests can be removed from model responses, or accounts that repeatedly submit such prompts can be tracked and suspended if unsafe behavior continues [104]. Implement"}, {"title": "Decentralized volunteer classification for safe datasets (data aggregators, data vendors, data processors; obfuscatable).", "content": "To break out of the chicken-and-egg paradox of relying on AI models to assess the safety of datasets used to train themselves, we propose leveraging volunteer classification efforts. Projects like Galaxy Zoo, launched in 2007, demonstrate the effectiveness of citizen science, where public volunteers classified galaxies and provided valuable training data that eventually enabled machine learning models to automate these tasks [37]. Similarly, volunteers could be enlisted to review random pieces of data and classify their safety according to specified criteria. Once sufficient labeled data is gathered from volunteers, a classifier could be trained to automate verification in the future, with periodic updates from volunteers as the relative safety of data evolves. This approach harnesses the collective intelligence of the public to enhance dataset safety, ensuring that Al models are trained on vetted data [54, 61]. Moreover, involving volunteers in data classification promotes transparency and public trust in AI systems [38]. However, challenges such as ensuring annotation quality and protecting volunteers from exposure to harmful content must be carefully managed [88].\nImplementing automated classifiers on input and output (model developers; vulnerability). Fine-tuning a small language model to detect signs of data extraction and other common attacks can provide an additional layer of security on top of existing LLMs. Data-based input attack vectors are attractive to adversaries due to their accessibility; for instance, attacks on web-scraped datasets can be executed simply by injecting large amounts of data expressing misaligned intent [103]. Similarly, output attacks, such as data extraction, involve malicious prompting that exploits vulnerabilities in the data pipeline, potentially leading to unauthorized access to sensitive information [19]. Implementing input/output classifiers can detect many of these common attacks, blocking rudimentary threats with relatively simple techniques [57]. These classifiers act as gatekeepers, analyzing inputs and outputs for signs of malicious activity and preventing the exploitation of LLMs."}, {"title": "differential data spread (data producers, data aggregators, data vendors, model trainers; vulnerability).", "content": "Implementing techniques to assign greater weighting to positive or beneficial data-or increasing the raw quantity of such data in datasets-can promote more favorable outcomes in AI models. For example, incorporating \"good stories\" about AGI or emphasizing ethical and aligned content can steer models toward safer behaviors. Meta's Llama 2 model demonstrates the possibility of intentional data weighting by model developers; their pre-training data mix was curated to contain specific proportions of content types, showing that certain types of content can be deliberately emphasized in the training data [98]. Techniques like curriculum learning, where training data is presented in a meaningful order to improve learning efficiency and performance, support the efficacy of intentional data weighting [10]. Beyond this, differential data spread could be executed through the large-scale creation of beneficial data by data producers, an increased sampling of beneficial data by data aggregators, such as through duplication of beneficial data, and the curation of datasets with high amounts of beneficial data by data vendors."}, {"title": "Refine existing regulations to include mechanisms of data generation and collection (regulators, evaluators; synthetic data).", "content": "Existing regulatory frameworks for data privacy are invaluable but need to evolve as methods of data collection and generation advance. For example, the EU's GDPR defines personal data but does not explicitly address synthetic or AI-generated data that can be linked back to individuals [42]. Expanding the GDPR's definition to explicitly include synthetic and AI-generated data would extend regulatory oversight to these novel forms. Similarly, consent mechanisms mandated by regulations like the CCPA need to evolve beyond explicit data collection to address AI systems capable of inferring sensitive information from aggregated non-sensitive data, or to regulate non-sensitive data itself [18]. Updating these privacy regulations is essential for the effective governance of emergent technologies like synthetic data and would enhance protections against potential misuse of AI systems. Incorporating provisions from recent legislative proposals, such as the EU's proposed Artificial Intelligence Act, which addresses AI-specific risks and governance, could further strengthen the regulatory framework [26]. Additionally, research demonstrating how AI models can infer personal information from seemingly anonymized data underscores the need for refined regulations [90, 19].\nProof-of-training-data verification (model developers, regulators; non-excludability). Proof-of-training-data is an emerging mechanism for verifying the datasets used to train machine learning models [25]. This mechanism holds significant potential as an enforcement tool for regulators, enabling the auditing of models to ensure correct dataset attribution and confirming that models have not been trained on prohibited or unauthorized datasets. For instance, if a malicious model developer circumvented the standard data pipeline to obtain a sensitive or dangerous dataset through illicit means, resulting in a harmful AI agent, regulators could employ Proof-of-training-data verification to determine whether the inaccessible dataset was used during training. Techniques such as data provenance tracking [50], model watermarking [99], and cryptographic verification methods [7] can enhance this verification process by ensuring transparency, enabling ownership verification, and preserving data privacy, respectively. This approach addresses a critical regulatory challenge posed by black-box models, where outputs are often untraceable to specific training data, thereby establishing accountability measures for model developers even after training is complete."}, {"title": "VI. UNDEREXPLORED MECHANISMS", "content": "We highlight 5 novel governance applications of technical mechanisms that each address a central challenge to governing data and together form a united AI governance policy framework.\nCanary tokens\nCanary tokens are unique identifiers or markers embedded within data to serve as tripwires, alerting data producers and regulators to unauthorized access or use, while empowering data aggregators and data vendors with greater control over the dissemination of their data. An example of this technique and its role in the AI data supply chain can be seen in Figure 3. Canary tokens can help prevent foundation models from incorporating hazardous or sensitive information into their training datasets. Canary tokens help address the easily replicable nature of data by allowing for tracking and blocking of unauthorized replication. Furthermore, they strengthen a key link in the data supply chain, while enhancing detectability and mitigating vulnerabilities to adversarial attacks.\nImplementation: We propose data producers could include random strings or unique codes within webpages or documents containing potentially dangerous information. These tokens would be registered with the appropriate regulatory authority, who would privately inform the model developer about the tokens' presence and meaning. Model developers would be required to scan their training data for these canary tokens and exclude any content containing them.\nExisting work: Canary tokens are widely used in cybersecurity to detect unauthorized access, acting as early warning systems against attackers [39, 36, 63]. Grosse et al. [45] demonstrated the use of influence functions to trace model outputs back to training data in large language models. Other methods can detect whether specific data has been used in training models [19, 74, 6, 111, 85, 55]. Though such reverse data attribution is possible, but can be combined with canary tokens to decrease computation expense and increase accuracy. Instead of analyzing influence across the entire training set, canary tokens provide unambiguous markers, eliminating the need for approximate influence calculations in many cases-even if not, models could quickly scan the much smaller space of known canary tokens.\nChallenges and mitigation: The interests of data producers, data aggregators and data vendors to protect their own data will help justify the initial technical hurdle of implementing canary tokens across data."}, {"title": "Mandatory data filtering", "content": "Mandatory data filtering is a regulatory requirement for model developers to implement automated processes that detect and remove malicious, harmful, or unsafe content from training datasets before model training begins. Large language model (LLM) powered filtering of unsafe content in pre-training data could greatly reduce the risk of both the model passively absorbing unsafe content, thus eliminating the model's native potential to be unsafe, as well as targeted data poisoning attacks. This approach addresses the vulnerable nature of data to attacks by filtering out these attacks before training, while likely preserving the quality and safety of the information used to train AI models.\nImplementation: We propose model developers and other data processors should integrate filtering mechanisms for safety into data preprocessing stages. Government safety agencies would provide the specific safety criteria, and auditors with access can verify their correct implementation and effectiveness (potentially in combination with mandatory reporting requirements below). Although LLM-based methods for data filtering exist, they are not widely used for model safety, but instead to support performance. For example, 25% of Llama-3's pretraining data mix consisted of mathematics and coding tasks [32]. Classifiers can still be used to recognize specific types of unsafe or low-quality content.\nExisting Work: Data filtering can be, and is often used to enhance model performance by removing irrelevant or low-quality data. Traditional filtering techniques focus on predefined heuristics for determining unsafe content [32]. Llama-3 uses classifiers like fastText and ROBERTa-based models to filter training data [32]. The use of LLMs to filter data allows for a more dynamic system with minimal prompting [100], and no need to define heuristics for the determination of \"unsafe content\u201d, as in current safety filtering systems [32].\nChallenges and mitigation: Implementing LLM-based filtering at scale can be expensive. This can be mitigated by the training/fine-tuning of specialised-models. Rather than a mandate, governments could partner with model developers, as they partnered with cloud service providers for transparency measures of cloud services usage[53]. A mandate, in principle, already has some regulatory footing, such as in the European Union's AI Act, which sets minimum quality requirements for pre-training datasets. The potential for data filtering to improve the quality of a model also incentivizes its use by model developers, helping to justify the cost of data filtering."}, {"title": "Mandatory reporting requirements", "content": "Mandatory reporting requirements are regulatory policies that compel model developers and data vendors to disclose their pre-training and post-training dataset to a government auditor, potentially including additional information about their data practices, model training processes, and data transactions. The policy addresses data's non-excludability by ensuring datasets used by model developers are acquired through legitimate means, regulating the link between data vendors and their clients, and model developers and their sources. The approach also has the additional benefit of detecting the misuse of data, as highlighted by the recent public attention on NVIDIA's alleged scraping of YouTube data for AI training without proper authorization [87]. Furthermore, comprehensive reporting on training data and compute usage could provide valuable insights into the scaling achievements of various AI companies, offering a clearer picture of the competitive landscape and technological progress."}, {"title": "Model data security", "content": "Model data security involves model developers and aggregators implementing robust security measures to protect pre-training, post-training datasets and synthetic data generation algorithms from unauthorized access, theft, or tampering. This extends existing security practices used for safeguarding model weights to the data used in training AI models [76, 40, 96]. Currently, model developers take various precautions to protect model weights. We recommend that the same precautions be taken to secure pre-training and post-training data. The theft of pre-training data, for example, could allow for the recreation of the model or a model of similar performance [56]. Recent work has shown that post-training and fine-tuning data could be essential to the improvement and alignment of models [58]; the theft of this data, thus, would not only allow for the performance improvement of potentially adversarial models, but also the potential adversarial misalignment of frontier models. Increasing security addresses the challenge of data's non-rival nature by discouraging unauthorized sharing and access.\nImplementation: We propose that model developers establish access control, encryption, secure computing environments and security auditing to model data - in practice, many of these recommendations are similar for those to defend model weights [76]. These security measures would have to be mandated by regulators and implemented by model providers. Regulatory precedent exists for such mandates, such as stringent requirements for protection of personal data and bank information[77, 42]. Due to the potential presence of said sensitive data in training data sets, the same regulatory mechanisms could be used to mandate the protection of training data[64]. Furthermore, out of a desire to protect their models and maintain the competitive edge of superior data, model developers have an incentive to protect their data in such a manner.\nExisting work: In addition to encryption and access control, which are already industry with regards to model weights [76], increasingly, developers have begin to watermark model weights to track downstream usage [40, 67]. We recommend the implementation of these novel methods to defend model data as well, both due to its technical precedent [48] and the importance of data to model security.\nChallenges and mitigation: More thorough protection of data, however, would be challenging, as it would have to protect against mechanisms which allow for the extraction of pre-training data, even in black box models [73]. There appear"}, {"title": "Know your customer regulations", "content": "Know your customer (KYC) regulations for data vendors require these vendors to verify and document the identities of their customers, often model developers particularly for transactions involving significant quantities of data, or types of data. Data vendors should consider implementing KYC procedures, and thereby verifying customer identities. Further, customers purchasing large amounts of data are mandated to provide verifiable identification. This policy addresses the challenge of data's obfuscatability by reducing the potential for covert data transactions.\nImplementation: We propose that data vendors collect identifying information, and verify through independent sources or services, and maintain secure records of customer identities and transcation details. Regulators should set specific transaction thresholds above which KYC procedures based on risk assessments.\nExisting work: KYC is well-established in finance to prevent money laundering and fraud [14]. It is gaining traction for regulating compute and cloud infrastructure [62, 53]. At present, model developers have untraceable access to immense datasets through private transactions with data vendors [79], leading to an opaque data supply chain which allows adversaries to develop potentially unsafe frontier models covertly.\nChallenges and mitigation: The upfront cost for data vendors in establishing KYC processes can be streamlined by governments providing industry-wide KYC standards."}, {"title": "VII. POLICY IMPLEMENTATION", "content": "The five mechanisms outlined above can be united to form a regulatory framework to address the challenges of governing Al training data. This framework creates a layered approach to data governance, where each mechanism reinforces the others while targeting specific vulnerabilities in the AI data supply chain.\nPhased Implementation Strategy\nWe propose a three-phase implementation strategy that minimizes cost and regulatory overhead while maintaining effectiveness:\nPhase 1: Detection and Assessment Canary tokens serve as the initial diagnostic tool, allowing regulators to understand the scope of unauthorized data usage in AI training. This approach builds on existing cybersecurity frameworks and requires minimal regulatory overhead, making it an ideal starting point. Similar to how financial institutions use fraud detection systems, regulators can establish a monitoring system for these tokens. The technical infrastructure for this already exists within many regulatory bodies that monitor digital financial transactions.\nPhase 2: Transparency and Accountability Based on insights from Phase 1, this second phase introduces mandatory reporting requirements for developers and KYC regulations for vendors, creating a transparent data supply chain. This builds on existing frameworks like the Bank Secrecy Act's reporting requirements [14] and recent cloud computing transparency initiatives [53].\nPhase 3: Prevention and Control If necessary, mandatory data filtering and model data security measures would be implemented as a final phase for control. This phase mirrors existing regulatory frameworks in finance and healthcare, where institutions must demonstrate robust security measures for sensitive data [77]. The implementation would follow a tiered approach similar to the EU AI Act's risk categorization [26], with more stringent requirements for larger models and more sensitive data."}, {"title": "Addressing Core Data Governance Challenges", "content": "Each mechanism targets one of the key challenges to governing data, as outlined in the descriptions of underexplored mechanisms.\nPractical Implementation and Legal Framework\nThe proposed framework builds on existing regulatory structures while addressing AI-specific challenges:\nLegislative Alignment: Unlike GDPR's broad restrictions on data processing [42], our framework focuses on specific, measurable requirements that align with established regulatory powers. The approach mirrors existing financial regulations and commonplace practices that have withstood legislative and judicial scrutiny across jurisdictions [14].\nCost-Effective Implementation: The framework leverages existing regulatory infrastructure and technical solutions while finding novel applications to reduce the amount of novel legislation required:"}, {"title": "Advantages Over Existing Frameworks:", "content": "This approach offers several improvements over current regulations:\n\u2022 Unlike the EU AI Act's focus on applications and model outputs [26], our framework addresses root causes in training data.\n\u2022 Compared to GDPR's emphasis on individual data rights [42], our approach creates systemic safeguards for the entire AI training pipeline.\n\u2022 The framework provides specific, technical mechanisms for enforcement, addressing a key limitation of current AI governance proposals."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we sought to introduce data's role in governing frontier AI models. In particular, as frontier models get more powerful, new vulnerabilities will need to be addressed, and new mechanisms will be required for policymakers to respond. We provided a brief overview of 15 technical mechanisms, which have received varying previous attention, and introduced five, thus far, unexplored central recommendations canary tokens, data filtering, reporting requirements, data security and know-your-customer regulation for combating these challenges. Beyond this, there is significant scope for future policy research exploring how existing regulatory regimes (particularly those governing data, which are among the most developed) can be adapted and leveraged for frontier data governance, as well as technical work estimating and formalising the various assumptions of our policy mechanisms."}]}