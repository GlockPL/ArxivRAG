{"title": "Only Strict Saddles in the Energy Landscape of Predictive Coding Networks?", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Ryan Singh", "Christopher L. Buckley"], "abstract": "Predictive coding (PC) is an energy-based learning algorithm that performs iterative inference over network activities before weight updates. Recent work suggests that PC can converge in fewer learning steps than backpropagation thanks to its inference procedure. However, these advantages are not always observed, and the impact of PC inference on learning is theoretically not well understood. Here, we study the geometry of the PC energy landscape at the (inference) equilibrium of the network activities. For deep linear networks, we first show that the equilibrated energy is simply a rescaled mean squared error loss with a weight-dependent rescaling. We then prove that many highly degenerate (non-strict) saddles of the loss including the origin become much easier to escape (strict) in the equilibrated energy. Our theory is validated by experiments on both linear and non-linear networks. Based on these results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, this work suggests that PC inference makes the loss landscape more benign and robust to vanishing gradients, while also highlighting the challenge of speeding up PC inference on large-scale models.", "sections": [{"title": "1 Introduction", "content": "Originating as a general principle of brain function, predictive coding (PC) has in recent years been developed into a local learning algorithm that could provide a biologically plausible alternative to backpropagation (BP) [30, 29, 42]. Deep neural networks (DNNs) trained with PC have shown comparable performance to BP on standard small-to-medium machine learning tasks, including classification, generation and memory association [29, 42, 40]. PC networks (PCNs) are also highly versatile, allowing for arbitrary computational graphs [44, 10], hybrid and causal inference [43, 59], and temporal prediction [33].\nIn contrast to BP, and similar to other energy-based algorithms [e.g. 49, 37], PC performs iterative (approximately Bayesian) inference over network activities before weight updates. This has been recently described as a fundamentally different principle of credit assignment for learning in the brain called \"prospective configuration\" [54], where weights follow activities (rather than the other way around). While the inference process key to PC incurs an additional computational cost, it has been suggested to provide many benefits for learning including faster convergence [54, 3, 17]. However, these speed-ups are not consistently observed across datasets, models and optimisers [3], and the impact of PC inference on learning more generally is theoretically not well understood (see \u00a7A.2.1).\nTo address this gap, we study the geometry of the effective PC landscape on which learning is performed: the weight landscape at the (inference) equilibrium of the network activities (defined in \u00a72.2). Our theory considers deep linear networks (DLNs, \u00a72.1), the standard model for theoretical studies of the loss landscape (see \u00a7A.2). Despite being able to learn only linear representations, DLNs have non-convex loss landscapes with non-linear learning dynamics that have proved to be a useful model for understanding non-linear networks [e.g. 48]. In contrast to previous theories of PC [3, 2, 17], we do not make any additional assumptions or approximations (see \u00a7A.2), and we perform experiments showing that our linear theory holds for non-linear networks.\nFor DLNs, we first show that, at the inference equilibrium, the PC energy is simply a rescaled mean squared error (MSE) loss with a non-trivial, weight-dependent rescaling (Theorem 1). We then focus on saddle points of the equilibrated energy and compare them to those of the recently characterised (MSE) loss [21, 1]. Such saddles, which are ubiquitous in the loss landscape of neural networks [11, 1], can be of two main types: \"strict\", where the Hessian is indefinite (Def. 1), or \"non-strict\", where an escape (negative) direction is found in higher-order derivatives [15, 21, 1]. Non-strict saddles are particularly problematic for first-order methods like (stochastic) gradient descent (SGD) since they are by definition at least second-order critical points. While SGD can be exponentially slowed in the vicinity of strict saddles [12], it can effectively get stuck in non-strict ones, giving the false impression of converging to a minimum [47, 7] (see \u00a7A.2 for a review). This is simply a restatement of the phenomenon of vanishing gradients from a landscape perspective [38, 6].\nBy contrast, here we prove that many non-strict saddles of the MSE loss, specifically saddles of rank zero, become strict in the equilibrated energy for any DLN (Theorems 2 & 3). These saddles include the origin, whose degeneracy (i.e. flatness) in the loss grows with the number of hidden layers. Our theoretical results are strongly validated by experiments on both linear and non-linear networks. Additional experiments further suggest that other (higher-rank) non-strict saddles of the loss are strict in the equilibrated energy. Based on these results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, this work suggests that PC inference makes the loss landscape more benign and robust to vanishing gradients, while also highlighting the fundamental challenge of speeding up PC inference on large-scale models.\nThe rest of the paper is structured as follows. After introducing the setup (\u00a72), we present our theoretical results for DLNs (\u00a73), including some illustrative examples and thorough empirical verifications of each result. We then report experiments on non-linear networks supporting our theory and more general conjecture (\u00a74). We conclude by discussing the implications and limitations of our work, as well as potential future directions (\u00a75). Appendix A includes a review of related work, derivations, experiment details and supplementary results."}, {"title": "1.1 Summary of contributions", "content": "\u2022 We derive an exact solution for the PC energy of DLNs at the inference equilibrium (Theorem 1), which turns out to be a rescaled MSE loss with a weight-dependent rescaling. This corrects a previous mistake in the literature that the MSE loss is equal to the output energy [32] (which holds only at the feedforward pass) and enables further studies of the PC energy landscape. We find an excellent match between our theory and experiment (Figure 1).\n\u2022 Based on this result, we prove that, in contrast to the MSE loss, the origin of the equilibrated energy of DLNs is a strict saddle independent of network depth. We provide an explicit characterisation of the Hessian at the origin of the equilibrated energy (Theorem 2), which is perfectly validated by experiments on linear networks (Figures 3, 4 & 8).\n\u2022 We further prove that other non-strict saddles of the MSE loss than the origin, specifically saddles of rank zero, become strict in the equilibrated energy of DLNs (Theorem 3). We provide an empirical verification of one of these saddles as an example (Figures 9 & 10).\n\u2022 We empirically show that our linear theory holds for non-linear networks, including convolutional architectures, trained on standard classification tasks. In particular, when initialised close to non-strict saddles of the MSE loss covered by Theorem 3, we find that SGD on the equilibrated energy escapes substantially faster than on the loss given the same learning rate (Figures 5 & 12), and in contrast to BP, PC exhibits no vanishing gradients (Figure 11).\n\u2022 We perform additional experiments, again on both linear and non-linear networks, showing that PC quickly escapes other (higher-rank) non-strict saddles of the loss that we do not address theoretically (Figure 6), supporting our conjecture that all the saddles of the equilibrated energy are strict."}, {"title": "2 Preliminaries", "content": "Notation. We use the following shorthand $W_{k:l} = W_k ... W_l$ for $l, k \\in 1, ..., L$, denoting the total product of weight matrices as $W_{L:1} = W_L ... W_1$. $I_n$ is the $n \\times n$ identity matrix, while $O_n$ denotes either the n-zero vector or the $n\\times n$ null matrix, and n will be omitted when clear from context. $||\\cdot||$ denotes the l2 norm, and $\\otimes$ is the Kronecker product between two matrices. We will consider the gradient and Hessian of an objective $f$ only with respect to the network weights $\\theta$ and abbreviate them as $g_f := \\nabla_{\\theta}f$ and $H_f := \\nabla_{\\theta}^2f$, respectively, omitting the independent variable for simplicity. The largest and smallest eigenvalues of the Hessian are $\\lambda_{max}(H_f)$ and $\\lambda_{min}(H_f)$, with $\\nu_{max}$ and $\\nu_{min}$ as their associated eigenvectors. See \u00a7A.1 for more general notation.\nDefinition 1. Strict saddle. Following [15] and later work, any critical point $\\theta^*$ of $f(\\theta)$ where $g_f(\\theta^*) = 0$ is defined as a strict saddle when the Hessian at that point has at least one negative eigenvalue, $\\lambda_{min}(H_f(\\theta^*)) < 0$. Any other critical point with a positive semi-definite Hessian and at least one negative eigenvalue in a higher-order derivative is said to be a non-strict saddle."}, {"title": "2.1 Deep Linear Networks (DLNs)", "content": "We consider DLNs with one or more hidden layers $H = L - 1 > 1$ defining the linear mapping $W_{L:1} : \\mathbb{R}^{d_x} \\to \\mathbb{R}^{d_y}$ where $W_e \\in \\mathbb{R}^{n_e \\times n_{e-1}}$, with layer widths $\\{n_e\\}_{l=0}^L$ and input and output dimensions $n_0 = d_x, n_L = d_y$. We ignore biases for simplicity. The standard MSE loss for DLNs can then be written as\n$\\mathcal{L} = \\frac{1}{2N} \\sum_{i=1}^N ||y_i - W_{L:1}x_i||^2$ (1)\nfor a dataset of N examples $\\{(x_i, y_i)\\}_{i=1}^N$ where $x \\in \\mathbb{R}^{d_x}, y \\in \\mathbb{R}^{d_y}$. The total number of weights is given by $p = \\sum_{e=1}^L n_e n_{e-1}$, and we will denote the set of all network parameters as $\\theta := (W_1,..., W_L) \\in \\mathbb{R}^p$. For brevity, we will often refer to the MSE loss as simply the loss."}, {"title": "2.2 Predictive coding (PC)", "content": "DNNs trained with PC typically assume a hierarchical Gaussian model with identity covariances, so we will adopt this formulation for linear fully connected layers $z_e \\sim \\mathcal{N}(W_e z_{e-1}, I_e)$ where the mean activity of each layer $z_e$ is a linear function of the previous layer. Under some common assumptions about the form of the generative model, we can derive an energy function, often referred to as the variational free energy, that is a sum of squared prediction errors across layers [9].\n$\\mathcal{F} = \\frac{1}{2N} \\sum_{i=1}^N \\sum_{l=1}^L ||z_{l,i} - W_l z_{l-1,i}||^2$ (2)\nNote that this objective defines an energy for every neuron, highlighting the locality of the algorithm. To train a PCN, the last layer is clamped to some data, $z_{L,i} := y_i$, which could be a label for classification or an image for generation. In a supervised task, the first layer is also fixed to some input, $z_{0,i} := x_i$. The energy (Eq. 2) is then minimised in two phases, first w.r.t. the activities (inference) and then w.r.t. the weights (learning)\nInference: $\\Delta z_e \\propto -\\frac{\\partial \\mathcal{F}}{\\partial z_e}$ (3)\nLearning: $\\Delta W_e \\propto -\\frac{\\partial \\mathcal{F}}{\\partial W_e}$ (4)\nwhere we omit the data index $i$ for simplicity. In practice [though see 46], the inference dynamics (Eq. 3) are run to convergence until $\\Delta z_e \\approx 0$, before performing a weight (e.g. GD) update (Eq. 4). Importantly, the effective weight landscape on which we perform learning is therefore the energy at the inference equilibrium $\\mathcal{F}|_{\\Delta z \\approx 0}(\\theta)$, which we will refer to as the equilibrated energy or sometimes simply the energy."}, {"title": "3 Theoretical results", "content": "As explained in \u00a72.2, the weights of a PCN are typically updated once the activities have converged to an equilibrium. The equilibrated energy $\\mathcal{F}|_{\\partial \\mathcal{F}/ \\partial z=0}(\\theta)$, which we will abbreviate as $\\mathcal{F}^*(\\theta)$, is therefore the effective learning landscape navigated by PC and the object we are interested in studying. It turns out that we can derive a closed-form solution for the equilibrated energy of DLNs, which will be the basis of our subsequent results."}, {"title": "3.1 Equilibrated energy as rescaled MSE", "content": "Theorem 1 (Equilibrated energy of DLNs). For any DLN parameterised by $\\theta := (W_1,..., W_L)$ with input and output $(x_i,y_i)$, the PC energy (Eq. 2) at the exact inference equilibrium $\\partial \\mathcal{F}/ \\partial z = 0$ is the following rescaled MSE loss (see \u00a7A.3.2 for derivation)\n$\\mathcal{F}^* = \\frac{1}{2N} \\sum_{i=1}^N (y_i-W_{L:1}x_i)^T S^{-1}(y_i - W_{L:1}x_i)$ (5)\nwhere the rescaling is $S = I_{d_y} + \\sum_{l=2}^L(W_{L:l})(W_{L:l})^T$.\nThe proof relies on unfolding the hierarchical Gaussian model assumed by PC to work out the full, implicit generative model of the output, and the rescaling S comes from the variance modelled by PC at each layer (see \u00a7A.3.2 for details). Figure 1 shows an excellent empirical validation of the theory.\nIntuitively, the PC inference process (Eq. 3) can then be thought of as reshaping the (MSE) loss landscape to take some layer-wise, weight-dependent variance into account. This immediately raises the question: how does the equilibrated energy landscape $\\mathcal{F}^*(\\theta)$ differ from the loss landscape $\\mathcal{L}(\\theta)$? Is the rescaling\u2014and so the layer variance modelled by PC\u2014useful for learning? Below we provide a partial positive answer to this question by comparing the saddle point geometry of the two objectives."}, {"title": "3.2 Analysis of the origin saddle ($\\theta$ = 0)", "content": "Here we prove that, in contrast to the MSE loss, the origin of the equilibrated energy (Eq. 5, where all the weights are zero, $\\theta$ = 0) is a strict saddle (Def. 1) for DLNs of any depth. To do so, we derive an explicit expression for the Hessian at the origin of the equilibrated energy. For intuitive comparison, we first briefly recall the known results that, at the origin, the loss Hessian is indefinite for one-hidden-layer networks and zero for any deeper network (see \u00a7A.3.1 for a derivation).\n$H_{\\mathcal{L}}(\\theta = 0) = \\begin{bmatrix} 0 & -I_{n_1} \\otimes \\Sigma_{yx} \\\\ -\\Sigma_{xy} \\otimes I_{n_1} & 0\\end{bmatrix}, \\qquad H=1$ (6)\n$\\qquad\\qquad \\begin{bmatrix} 0 & 0 \\\\ 0 & 0_{p} \\end{bmatrix}, \\qquad H > 1$\nwhere following previous works $\\Sigma_{xy} := \\frac{1}{N}\\sum_{i=1}^N x_i y_i^T$ is the empirical input-output covariance. One-hidden-layer networks H = 1 are a special case where the origin saddle of the loss is strict (Def. 1) and was studied in detail by [48] (see left panel of Figure 2 for an example). For deeper networks H > 1, the saddle is non-strict as first shown by [21].\n$\\lambda_{min}(H_{\\mathcal{L}}(\\theta = 0)) < 0, \\quad H = 1 \\quad$ [strict saddle]\n$\\lambda_{min}(H_{\\mathcal{L}}(\\theta = 0)) = 0, \\quad H > 1 \\quad$ [non-strict saddle] (7)\nMore specifically, the origin saddle of the loss is of order $H^{-1}$, becoming increasingly degenerate (flat) and harder to escape with depth, especially for first-order methods like SGD (see middle and right panels of Figure 2).\nBy contrast, now we show that the origin saddle of the equilibrated energy is strict for DLNs of any number of hidden layers. Figure 2 shows a few toy examples illustrating the result. In brief, we"}, {"title": "3.3 Analysis of other saddles", "content": "Is the origin a special case where the equilibrated energy has an easier-to-escape saddle than the loss? Or is this result pointing to something more general? Here we consider a specific type of non-strict saddle of the loss (of which the origin is one) and show that indeed they also become strict in the equilibrated energy. We address other saddle types experimentally in \u00a74 and leave their theoretical study to future work.\nSpecifically, we consider saddles of rank zero, which for the MSE loss can be identified as critical points where the product of weight matrices is zero $W_{L:1} = 0$ [1]. For the equilibrated energy (Eq. 5), we consider the critical points $\\theta^*(W_1 = 0, W_{L-1:1} = 0)$, since the last weight matrix needs to be null in order for the energy gradient to be zero (see \u00a7A.3.3 for an explanation). It turns out that at these critical points there exists a direction of negative curvature.\nTheorem 3 (Strictness of zero-rank saddles of the equilibrated energy). Consider the set of critical points of the equilibrated energy (Eq. 5) $\\theta^*(W_1 = 0, W_{L-1:1} = 0)$ where $g_{\\mathcal{F}^*}(\\theta^*) = 0$. The Hessian at these points has at least one negative eigenvalue (see \u00a7A.3.6 for proof)\n$\\exists \\lambda(H_{\\mathcal{F}^*}(\\theta^*)) < 0 \\quad$ [strict saddles, Def. 1] (10)\nNote that Theorem 2 can now be seen as a corollary of Theorem 3, although for the origin we derived the full Hessian. This result also stands in contrast to the (MSE) loss, where many of the considered critical points (specifically when 3 or more weight matrices are zero) are non-strict saddles as proved by [1]. The prediction is again that, in the vicinity of any of these saddles, PC should escape faster than BP with (S)GD given the same learning rate. For space reasons, (\u00a74) the subsequent experiments focus only the origin as an example of a saddle covered by Theorem 3 (and Theorem 2), but \u00a7A.5 includes an empirical validation of another (zero-rank) strict saddle of the equilibrated energy (Figures 9, 10 & 12)."}, {"title": "4 Experiments", "content": "Here we report experiments on linear and non-linear networks supporting our theoretical results as well as more general conjecture that all the saddles of the equilibrated energy are strict. In all the experiments, we trained networks with BP and PC using (S)GD with the same learning rate, since the goal is to test our theory of the saddle geometry of the equilibrated energy landscape.\nFirst, we compared the loss training dynamics of linear and non-linear networks, including convolutional architectures, on standard classification tasks with SGD initialised close to the origin (see \u00a7A.4 for details). For computational reasons, we did not run the BP-trained networks to convergence, underscoring the point that the origin saddle of the loss is highly degenerate and particularly hard to escape for first-order methods like SGD. In all cases, we observe that PC escapes the origin saddle substantially faster than BP (Figure 5), and Figure 11 shows that PC exhibits no vanishing gradients. We find practically the same results when initialising close to another non-strict saddle of the loss covered by Theorem 3 (Figure 12). These findings support our theoretical results beyond the linear case.\nFrom Figure 5 we also observe a second plateau in the loss dynamics of PCNs, suggesting a saddle of higher rank (presumably rank 1). This is consistent with the saddle-to-saddle dynamics described for DLNs by [18], where for small initialisation GD transitions through a sequence of saddles, each representing a solution of increasing rank. To explicitly test higher-rank, non-strict saddles of the loss we did not study theoretically, we replicated one of the experiments of [18, cf. Figure 1] on a matrix completion task. In particular, networks were trained to fit a rank-3 matrix, which meant that starting near origin GD visited 3 saddles (of successive rank 0, 1 and 2) before converging to a rank-3 solution as shown in Figure 6. We find that, when initialised near any of the saddles visited by BP, PC escapes quickly and does not show vanishing gradients (Figure 6), supporting the conjecture that all the saddles of the equilibrated energy are strict."}, {"title": "5 Discussion", "content": "In summary, we took a first step in characterising the effective learning landscape navigated by PC\u2014the energy landscape at the inference equilibrium. For DLNs, we first showed that the equilibrated energy is a rescaled MSE loss with a weight-dependent rescaling (Theorem 1). This result corrects a previous mistake in the literature that the MSE loss is equal to the output energy [32] and that the total energy (Eq. 2) can therefore be decomposed into the loss and the other (hidden) energies (a relationship that only holds at the feedforward activity values). As we mention below, this result also enables further studies of the PC learning landscape.\nWe then proved that many non-strict saddle points of the MSE loss, specifically zero-rank saddles, become strict in the equilibrated energy of any DLN (Theorems 2 & 3). These saddles include the origin, making PC effectively more robust to vanishing gradients (Figures 6 & 11). We thoroughly validated our theory with experiments on both linear and non-linear architectures, and provided empirical support for the strictness of higher-rank saddles of the equilibrated energy. Based on these results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, the PC inference process can therefore be interpreted as making the loss landscape more benign.\nThis work can be seen as dual to [56], who showed that learning in linear physical systems with equilibrium propagation [49, 50] has beneficial effects on the physical (activity) Hessian. Studying these connections could be an interesting future direction.\nWe conclude with the most important limitations of our work. First, the strictness of the studied saddles of the energy holds, by derivation, at the inference equilibrium. However, in practice PC inference requires increasingly more iterations to converge on deeper networks. Our results therefore highlight the need to address this fundamental challenge if the benefits of PC inference for learning are to be reaped on large-scale tasks [39].\nIt was beyond the scope of this work to study under what conditions, such as different learning rates and optimisers, the equilibrated energy landscape might be faster to minimise than the loss landscape. Nevertheless, our results suggest that (all things being equal) PC should converge faster on deep (though not too deep, as explained above) and narrow networks as found by [54], since the distance between the origin saddle and standard initialisations scales with the network width [38]. However, understanding the overall convergence behaviour of PC would also require characterising the minima of the equilibrated energy [14], which our work enables.\nFinally, our study leaves open the question of generalisation. In this respect, we note that on problems where a low-rank bias is useful (e.g. matrix completion, Figure 6), GD with small initialisations can converge to better-generalising solutions than standard ones [18]."}, {"title": "A.1 General notation and definitions", "content": "Matrices, vectors and scalars are denoted with bold capitals A, bold lower-case characters v and small characters u, respectively. All vectors are by default column vectors $[\\cdot] \\in \\mathbb{R}^{n \\times 1}$, and $vec_r(\\cdot)$ denotes the row-wise vec operator. Following [52], unless otherwise stated we define matrix-by-matrix derivatives by row-vectorisation, using the numerator or Jacobian layout\n$\\frac{\\partial A}{\\partial B}_{ij} := \\frac{\\partial vec_r(A)_i}{\\partial vec_r(B)^T_j}$ (11)\nsuch that the result is a matrix rather than a 4D tensor. Following from this, we will also use the rules\n$\\frac{\\partial ABC}{\\partial B} = A \\otimes C^T$ (12)\n$\\frac{\\partial AB}{\\partial A} = I_m \\otimes B^T, \\quad A \\in \\mathbb{R}^{m \\times n}, B \\in \\mathbb{R}^{n \\times p}$ (13)"}, {"title": "A.2 Related work", "content": "PC and BP. [60] where the first to show that PC can approximate BP on multi-layer perceptrons when the influence of the input is upweighted relative to that of the output. [34] generalised this result to arbitrary computational graphs including convolutional and recurrent neural networks under the so-called \"fixed prediction assumption\u201d. A variation of PC where weights are updated at precisely timed inference steps was later shown to compute exactly the same gradients as BP on MLPs [53], a result further generalised by [45] and [41]. [31] unified these and other approximation results from an energy-based modelling perspective. [62] proved that the time complexity of all of these PC variants is lower-bounded by BP.\nPC and other algorithms. [13] provided an in-depth analysis of the convergence behaviour of PC inference. [32] showed that for linear networks the PC inference equilibrium can be interpreted as an average of BP's feedforward pass values and the local targets computed by target propagation [28]. [54] proposed that PC and other energy-based algorithms implement a fundamentally different principle of credit assignment called \u201cprospective configuration\u201d, in that neurons first change their activity to align with the target and then update their weights to consolidate that activity pattern. The flow of information can be therefore seen as reversed compared to BP where activities tend to follow weights rather than the other way around. For mini-batches of size one, [3] proved that PC approximates implicit gradient descent under specific layer-wise rescalings of the activities and parameter learning rates. [2] further showed that when this approximation holds, PC can be sensitive to Hessian information (for sufficiently small learning rates). Similarly, recent work cast PC as a second-order trust-region method [17].\nIn contrast to most of the above works on PC and other algorithms, linearity is the only major assumption we make, and we verify that the theory empirically holds for non-linear networks. In particular, previous works make either non-standard assumptions or loose approximations that do not predict well experimental data. For example, the interpretation of PC as implicit GD by [3] numerically holds only for small batch sizes and specific layerwise rescalings of the activities and parameter learning rates. ([2] generalised this result to remove the activity rescalings but not the learning rate ones.) Similarly, both [2] and [17] make second-order approximations of the energy to argue that PC makes use of Hessian information. However, our results clearly show that PC can leverage much higher-order information, turning highly degenerate, H-order saddles into strict ones."}, {"title": "A.2.1 Theories of predictive coding", "content": "PC and BP. [60] where the first to show that PC can approximate BP on multi-layer perceptrons when the influence of the input is upweighted relative to that of the output. [34] generalised this result to arbitrary computational graphs including convolutional and recurrent neural networks under the so-called \"fixed prediction assumption\u201d. A variation of PC where weights are updated at precisely timed inference steps was later shown to compute exactly the same gradients as BP on MLPs [53], a result further generalised by [45] and [41]. [31] unified these and other approximation results from an energy-based modelling perspective. [62] proved that the time complexity of all of these PC variants is lower-bounded by BP."}, {"title": "A.2.2 Saddle points and neural networks", "content": "Here we review some relevant theoretical and empirical work on (i) saddle points in the loss landscape of neural networks and (ii) the behaviour of different learning algorithms, especially (S)GD, near saddles. For more general reviews on the loss landscape and optimisation of neural networks, see [57] and [58].\nSaddles in the neural loss landscape. This work began with [5] showing that for linear networks with one hidden layer, all critical points of the MSE loss are either global minima or strict saddle"}, {"title": "A.3 Proofs and derivations", "content": ""}, {"title": "A.3.1 Loss Hessian for DLNS", "content": "Here we derive the Hessian of the MSE loss (Eq. 1) with respect to the weights of arbitrary DLNs (\u00a72.1); this is essentially a re-derivation of [52] with slightly different notation. We then show how the Hessian and its eigenspectrum at the origin (0 = 0) changes as a function of the number of hidden layers H. We start from the gradient of the loss for a given weight matrix\n$\\frac{\\partial \\mathcal{L}}{\\partial W_e} = \\frac{1}{N} \\sum_{i=1}^N  (W_{L:l+1})^T(W_{L:1}x - y)(W_{l-1:1}x)^T$ (14)\n$\\qquad \\quad = (W_{L:l+1})^T (W_{L:1}\\Sigma_{xx} - \\Sigma_{yx})(W_{l-1:1})^T \\in \\mathbb{R}^{n_e \\times n_{e-1}}$ (15)\nwhere following previous works we take the empirical mean over the data matrices $\\Sigma_{xx} := \\frac{1}{N} \\sum_{i=1}^N x_i x_i^T$ and $\\Sigma_{yx} := \\frac{1}{N} \\sum_{i=1}^N y_i x_i^T$. For networks with at least one hidden layer, the origin is a critical point since the gradient is zero $g_{\\mathcal{L}}(0 = 0) = 0$. To characterise this point to second order, we look at the Hessian. Starting with the diagonal blocks of size $(n_e n_{e-1}) \\times (n_e n_{e-1})$,\n$\\frac{\\partial^2 \\mathcal{L}}{\\partial W_e^2} = (W_{L:l+1})^T W_{L:l+1} \\otimes W_{l-1:1} \\Sigma_{xx} (W_{l-1:1})^T$ (16)"}, {"title": "A.3.2 Equilibrated energy for DLNs", "content": "Here we derive an exact solution to the PC energy (Eq. 2) of DLNs at the inference equilibrium (Theorem 1, Eq. 5), $\\mathcal{F}|_{\\partial \\mathcal{F}/ \\partial z=0}$, which we will abbreviate as $\\mathcal{F}^*$. This turns out to be a non-trivial rescaled MSE loss where the rescaling depends on covariances of the network weight matrices. To highlight the difference with the loss, recall that the standard MSE (Eq. 1) for a DLN implicitly defines the following generative model\n$y \\sim \\mathcal{N}(W_{L:1}x, \\Sigma)$ (21)\nwhere the target is modelled as a Gaussian with a mean given by the network function and some covariance $\\Sigma$. As a reminder, the MSE can be derived from minimising the mean negative log likelihood of the above Gaussian model [35]\n$\\mathcal{L} = \\prod_{i=1}^N \\mathcal{N}(y_i|W_{L:1}x, \\Sigma)$ (22)\n$\\qquad = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi}^{N/2}} \\frac{1}{\\sqrt{log \\vert \\Sigma\\vert}} - \\frac{1}{2 \\Sigma} (y_i - W_{L:1}x)^T \\Sigma^{-1} (y_i - W_{L:1}x)$ (23)\nwhere for identity covariance $\\Sigma = I$ it reduces to the standard MSE, since the additional terms do not depend on the model parameters. In a PC network, by contrast, the activity of each hidden layer-and not just the output-is modelled as a Gaussian (see \u00a72.2)\n$z_e \\sim \\mathcal{N}(W_e z_{e-1}, I_e)$ (24)"}, {"title": "A.3.3 Hessian of the equilibrated energy for DLNs", "content": "Here we derive the Hessian at the origin of the equilibrated energy for DLNs", "sum_{l=2}^L(W_{L": "l"}, "W_{L:l})^T$, and we denote the residual error for a given data sample as $r_i := (y_i - W_{L:1}x_i)$. In the general case, both the residual and the rescaling depend on $W_e$, so to take the gradient of the equilibrated energy we need the product rule. For simplicity, and similar to the characterisation of the off-diagonal blocks of the loss Hessian (\u00a7A.3.1), we write the two contributions separately, as follows\n$A := \\frac{1}{2N} \\sum_i \\frac{\\partial r_i^T}{\\partial W_e} S^{-1} r_i = (W_{L:l+1})^T S^{-1} (W_{L:1} \\Sigma_{xx} - \\Sigma_{yx}) (W_{l-1:1})^T$ (33)\n$B := \\frac{1}{2N} \\sum_i r_i^T \\frac{\\partial S^{-"]}