{"title": "Inductive Learning of Robot Task Knowledge\nfrom Raw Data and Online Expert Feedback", "authors": ["Daniele Meli", "Paolo Fiorini"], "abstract": "The increasing level of autonomy of robots poses challenges of trust and social\nacceptance, especially in human-robot interaction scenarios. This requires an\ninterpretable implementation of robotic cognitive capabilities, possibly based on\nformal methods as logics for the definition of task specifications. However, prior\nknowledge is often unavailable in complex realistic scenarios.\nIn this paper, we propose an offline algorithm based on inductive logic program-\nming from noisy examples to extract task specifications (i.e., action preconditions,\nconstraints and effects) directly from raw data of few heterogeneous (i.e., not\nrepetitive) robotic executions. Our algorithm leverages on the output of any\nunsupervised action identification algorithm from video-kinematic recordings.\nCombining it with the definition of very basic, almost task-agnostic, common-\nsense concepts about the environment, which contribute to the interpretability\nof our methodology, we are able to learn logical axioms encoding preconditions\nof actions, as well as their effects in the event calculus paradigm.\nSince the quality of learned specifications depends mainly on the accuracy of the\naction identification algorithm, we also propose an online framework for incre-\nmental refinement of task knowledge from user's feedback, guaranteeing safe\nexecution.\nResults in a standard manipulation task and benchmark for user training in the\nsafety-critical surgical robotic scenario, show the robustness, data- and time-\nefficiency of our methodology, with promising results towards the scalability in\nmore complex domains.", "sections": [{"title": "1 Introduction", "content": "Robots are becoming increasingly popular in many fields of industry and society.\nAs their degree of autonomy, hence intelligence, increases, human-robot interaction\nis a crucial aspect of design in the development of robotic systems, to preserve\nhuman safety and increase social acceptance in complex, typically human-centered\nscenarios. In this context, recent international provisions and regulations, e.g., by the\nEuropean Union\u00b9 prescribe interpretability (Rosenfeld and Richardson, 2019) as one\nfundamental requirement to achieve safe and transparent human-robot interaction.\nTo this aim, formal representation of agents' (robots') environments and tasks is a\nsuitable and convenient choice. Such representations in robotics include, among oth-\ners, action languages for planning (Erdem and Patoglu, 2012), as the well-established\nPlanning Domain Definition Language (PDDL) by Haslum et al (2019); Estivill-\nCastro and Ferrer-Mestres (2013); Kootbally et al (2015), logic programming for plan\nre-configuration (Meli et al, 2023b), ontology-based systems (Manzoor et al, 2021)\nand behavior trees (\u00d6gren and Sprague, 2022). One fundamental limitation of these\napproaches is the assumption of full scenario and task knowledge, which is clearly\nimpractical in complex, uncertain or partially observable domains.\nIn this paper, we propose a methodology to derive logical task specifications from\nvisual and kinematic data of robotic executions. In particular, we assume prior knowl-\nedge of very basic commonsense concepts (features) about the environment, e.g.,\nrelated to relative positions of objects in the scene. These are usually shared among\nmany tasks and scenarios, and they are already encoded in available general-purpose\nontologies for robotic and automation, e.g., by Schlenoff et al (2012). We then com-\nbine automatic retrieval of environmental features from input execution data, with\nunsupervised action identification. The resulting matching examples of action execu-\ntion and environmental features are used in an Inductive Logic Programming (ILP)\nby Muggleton and De Raedt (1994) framework to derive salient task specifications,\nnamely, preconditions and effects of actions and constraints. Learned specifications are\naffected by the noise in the input data and their post-processing (i.e., environmental\nfeatures retrieval and unsupervised action identification), thus they may not imme-\ndiately applied for safe fully autonomous robotic planning and execution. Hence, we\nfinally propose a framework interleaving human and autonomous execution, allowing\nfor incremental knowledge refinement via robot teaching.\nThis paper then makes the following contributions:\n\u2022 we propose ILP under the semantics of Answer Set Programming (ASP) by Lifschitz\n(2019) to learn task specifications from little noisy raw data of robotic execution.\nOur methodology can work in combination with any action identification algorithm,\nexploiting the output of the latter to define noise in action examples, and is robust\nMoreover, following our previous work in Meli et al (2021a), we employ the represen-\ntation of event calculus (Mueller, 2008) to efficiently learn temporal specifications\nabout environmental features, i.e., effects of actions;\n\u2022 we propose an ILP- and ASP-based framework for time- and data-efficient incre-\nmental learning of task knowledge and autonomous robotic planning, involving an"}, {"title": "2 Related Works", "content": "Learning robotic task sequences from previous observations or human teaching is a\nwidely studied problem, especially in the area of industrial assembly and manipu-\nlation (Zhu and Hu, 2018; Mukherjee et al, 2022). State-of-the-art approaches rely\non probabilistic models to capture the variability of the environment and execution,\ne.g., Gaussian Mixture Models (GMMs) as in Calinon (2016); Kyrarini et al (2019)\nand Hidden Markov Models (HMMs) as in Tanwani and Calinon (2016); Medina and\nBillard (2017).\nWith the recent uptake of deep learning and large data models, neural architectures\nhave been employed both for online and offline learning of optimal robotic strate-\ngies for complex domains. Main approaches involve reward shaping to learn efficient\ntask policies from human demonstrations (Alakuijala et al, 2023), or end-to-end learn-\ning of sensor-action maps for robotic navigation (Liu et al, 2017) and manipulation\n(Rahmatizadeh et al, 2018). This has paved the way towards human-in-the-loop Deep\nReinforcement Learning (DRL), where humans communicate with the autonomous\nrobot either to show it safe and best practice under different situations (Huang et al,\n2024), acting as teachers (Shenfeld et al, 2023), or to give targets and commands,\nbased on pre-trained foundational visual-language models (Shah et al, 2023).\nIn the specific field of robotic surgery, HMMs have been used, e.g., by Berthet-\nRayne et al (2016), though more general Bayesian networks are the most popular\n(Blum et al, 2008; Charri\u00e8re et al, 2017). Recently, DRL is becoming a prominent\napproach to surgical task automation (Shahkoo and Abin, 2023; Fan et al, 2024; Corsi\net al, 2023), paving the way towards interactive human-robot learning (Long et al,\n2023).\nOne crucial limitation, especially for DRL and deep learning methods, is their\nscarce interpretability and the lack of safety guarantees, even when safe (Shahkoo and\nAbin, 2023) or human-in-the-loop (Huang et al, 2024) solutions are employed. Indeed,\nthese approaches heavily rely on large and diverse training datasets, failing at the\ngeneralization to similar but different task situations. For this reason, some researchers\nhave attempted to directly learn human models of agency (Nikolaidis et al, 2015;\nChen et al, 2020), which however severely limit the performance of the autonomous\nagent. Similar limitations hold for Bayeesian and statistical methods. As an example in\nthe surgical setting, 80 homogeneous (i.e., under the same environmental conditions)\nexecutions are required by Berthet-Rayne et al (2016) to learn a HMM for a good\nreplication of the yet relatively simple peg transfer task. Finally, these approaches are\nhighly sensitive to parameter initialization, as shown by Medina and Billard (2017).\nIn contrast, in this paper we propose to use ILP (Muggleton, 1991) to learn task\nspecifications, i.e., preconditions, effects and constraints on actions; and then perform\nautomated reasoning in on specifications in ASP formalism for task planning and\nexecution. Originated from the broad field of domain knowledge discovery (Sridharan\nand Meadows, 2018; Mota and Sridharan, 2020; Laird et al, 2017), ILP solves many\nlimitations of above machine learning and DRL approaches, being data- and time-\nefficient, and able to easily generalize theory from data. Furthermore, the interpretable\nlogical formalism allows easier interaction with humans for online task knowledge\nrefinement. For this reason, ILP has been successfully applied to robotic learning\n(Cropper and Muggleton, 2019; Meli et al, 2020, 2021a).\nHowever, one important limitation of ILP, and in general frameworks for high-\nlevel knowledge extraction, is the poor performance in the presence of noisy datasets,\ni.e., task examples with uncertainty. For this reason, researchers have tried to embed\nuncertainty into the inductive process, as Katzouris et al (2023) to learn event defini-\ntions and Law (2023) under the more generic answer set semantics. In this direction,\nneurosymbolic approaches have been proposed, e.g., differentiable inductive logic pro-\ngramming (Evans and Grefenstette, 2018; Shindo et al, 2021) and statistical relational"}, {"title": "3 Background and notation", "content": "In this section, we describe the paradigmatic peg transfer task, to use it as a leading\nexample in the rest of the paper. We then provide basic notions about ASP and ILP\nunder the ASP semantics, necessary to detail our methodology next.\nWe start by providing a list of relevant symbols used throughout the paper, as a\npractical reference to the reader."}, {"title": "3.1 Notation", "content": "text\nAtoms in ASP formalism\n:-\nLogical implication \u2190 in ASP syntax\nF\nLogical entailment\nH(+)\nHerbrand base of a, representing the set of all possible ground\nversions of a\nB\nBackground knowledge for ILP task\nSM\nSearch space for ILP task\nE = (E+, E\u00af) Sets of positive (E+) and negative (E\u00af) examples for ILP task\nH\nTarget hypothesis for ILP\nC\nContext for CDPIs in ILASP\neinc\nIncluded set of an ILASP example or partial interpretation\neexc\nExcluded set of an ILASP example or partial interpretation\nS\nStream of task executions, as sequence of kinematic features K\nand visual features G.\n\u03a6\nAction identification algorithm\nL\nAction labels (for classification)\nF\nSemantic environmental features in ASP syntax\nX(Xa)\nSet of extended action preconditions, i.e., including both precon-\nditions and constraints (for action a)\nI (IF)\nSet of initiating conditions for effects of actions (for a specific\nenvironmental feature F\u2208 F)\nE (EF)\nSet of terminating conditions for effects of actions (for a specific\nenvironmental feature F\u2208 F)\nPij\nConfidence level for timed segment i to be associated with action\nlabel lj\nS\nSet of probabilistic timed segments (after applying I to S)"}, {"title": "3.2 The peg transfer task", "content": "Figure 1 shows the setup for the illustrative surgical training task of peg transfer. The\nobjective is to place colored rings on pegs of the corresponding color using the two\npatient-side manipulators (PSM1 and PSM2) of the dVRK. Each PSM can grasp any\nreachable ring and place it on any reachable peg; reachability is determined by the\nrelative position of rings and pegs with respect to the center of the base. Reachability\nregions are identified by the red dashed line in Figure 1. For instance, PSM1 can reach\nthe red ring and the blue peg, while PSM2 can reach the blue ring and the green peg.\nIn case an arm can reach a ring but not the same-colored peg, it must move to the\ncenter of the base and transfer the ring to the other arm to complete the task. Pegs\ncan be occupied by other rings and must be freed before placing the desired ring on\nit. Also, rings may be on pegs or on the base in the initial state, i.e., some rings may\nneed to be extracted before being moved."}, {"title": "3.3 ASP planning task description", "content": "We describe the peg transfer task in an established format for Answer Set Program-\nming (ASP) by Calimeri et al (2020).\nA (planning) domain's description in ASP comprises a sorted signature and\naxioms. The sorted signature is the alphabet of the domain, defining constants (either\nBooleans, strings or integers), variables and atoms, i.e., predicates of variables. In the\npeg transfer domain, variables represent arms A (with constant value either psm1 or\npsm2), objects 0 (either ring or peg) and colors (either red, green, blue, yellow or\ngrey). Atoms represent concepts as the relative locations between objects and / or\narms (e.g., $at (A,ring,C)$, meaning that arm A is at the location of ring with color\nC, or $placed(ring,C1,peg,C2)$, meaning that ring with color C1 is placed on peg with\ncolor C2) and the status of grippers of arms (e.g., closed_gripper(A). In our previ-\nous work (Meli et al, 2021a) we considered more task-specific environmental concepts,\ne.g., in hand(A, R, C) to represent that a ring is held by an arm. However, these can\nbe derived from other atoms (for instance, in hand(A,R,C) holds when both at (A,R,C)\nand closed_gripper(A) do), so we omit them in this paper, in order to allow depen-\ndency only on basic kinematic information from the robot and preserve generalization\nto different tasks. Atoms also represent actions of the task, i.e., move(A,O,C) (moving\nto a colored ring or peg), move(A,center,C) (moving to center for transfer of ring c),\ngrasp (A,ring,C), release(A) and extract (A,ring,C) for gripper actions and extrac-\ntion, respectively. We further annotate atoms with a temporal variable t representing\nthe discrete time step of the task flow (e.g., move(A,O,C,t)). Whenever all variables in\nan atom are assigned with constant values, we say that the atom is ground.\nGiven the sorted signature, axioms define preconditions and effects of actions\nand constraints. Preconditions for the peg transfer task (in the form Action :\nPrecondition) are:\n0 {move (A, O, C, t) : reachable(A, O, C, t);"}, {"title": "3.4 Inductive learning of ASP theories", "content": "We now provide relevant definitions to introduce the problem of inductive learning of\nASP theories in our considered scenario. Additional details can be found in Law et al\n(2018).\nA generic ILP problem T for ASP theories is defined as the tuple T = (B, SM,E),\nwhere B is the background knowledge, i.e. a set of axioms and atoms in ASP syntax;\nSM is the search space, i.e. the set of candidate ASP axioms that can be learned;\nand E is a set of examples. The goal of T is to find a subset H C SM such that\nHUB = E. In ILASP (Law, 2018) tool considered in this paper, examples are partial\ninterpretations defined as follows.\nDefinition 1 (Partial interpretation). Let P be an ASP program. Any set of ground\natoms that can be generated from axioms in P is an interpretation of P. Given an\ninterpretation I of P, we say that a pair of subsets of grounded atoms e = (einc, eexc)\n(respectively the included and excluded set) is a partial interpretation (extended by\ninterpretation I) if $einc \u2286 I$ and $eexc \u2229 I = \u00d8$.\nIn particular, we consider context-dependent partial interpretations, as introduced by\nLaw et al (2016)."}, {"title": "4 Methodology", "content": "In this section, we present our methodology for unsupervised learning and online\nsupervised refinement of ASP task knowledge."}, {"title": "4.1 Unsupervised learning of ASP task knowledge", "content": "The unsupervised learning strategy is sketched in Algorithm 1. More specifically, in\nSection 4.1.1 we describe how to generate the action-context tuples, starting from a\ndataset of synchronized visual and kinematic executions of the task. Then, in Section\n4.1.3 and 4.1.4, we show how to build examples from action-context tuples, in order to\nlearn ASP axioms for constrained preconditions, and how to generate axioms for effects\nof actions in the event calculus, respectively. While explaining different components\nof our strategy, we refer to specific lines in Algorithm 1."}, {"title": "4.1.1 Action-context tuples generation", "content": "The input to our learning pipeline is a dataset S of execution traces, i.e. synchronized\nkinematic and video streams of an instance of the task. Recording assumes that the\nrobot and the vision system share a common reference frame, e.g., using the calibration\nprocedure by Roberti et al (2020).\nThe kinematic stream includes the following features K: i) Cartesian coordinates of\neach arm A of the robotic system PA = {xpos, A, Ypos, A, Zpos, A}; ii) quaternion coordi-\nnates of each arm A of the robotic system qa = {xor,A,Yor,A, Zor, A, Wor, A}; iii) opening\nangles ja of the grippers. Other works by van Amsterdam et al (2019); Despinoy et al\n(2015) consider additional kinematic features, e.g., Cartesian and rotational veloci-\nties; however, following our previous work (Meli and Fiorini, 2021), we decide not to\nconsider them without loss of generality.\nThe video stream is acquired from a RGBD camera. The following geometric\nfeatures Gare extracted using standard color segmentation and shape recognition\nwith random sample consensus (Fischler and Bolles, 1981): i) center position prc =\n{Xrc, Yrc, Zrc} of each ring with color c; ii) tip position ppc = {Xpc, Ypc, Zpc} of each\npeg with color c; iii) position pb = {xb, Yb, zb} of meeting point at the center of the\nbase for transfer; iv) ring radius rr.\nKinematic and visual geometric quantities can be automatically mapped to envi-\nronmental features Fat each time step with the following relations, as proposed by\nGinesi et al (2020):\n$at (A,ring,C) \u2190 ||PA - Prc||2 < rr$\n$at (A,peg,C) \u2190 ||PA - Ppc||2 <rr < Zpc< Zpos,A$\n$placed(ring,C1,peg, C2) \u2190 ||Prc1 - Ppc2||2 <rr < Zrc1 < Zpc2$"}, {"title": "4.1.2 Constraints as extended action preconditions", "content": "The ILASP formulation considered for unsupervised task knowledge learning reflects\nDefinition 4, i.e., excluding negative examples. Though it is possible to define noisy\nnegative examples and extend Definition 5 similarly to Definition 4, this would not be a\ncorrect modeling approach. In fact, we learn task knowledge from examples generated\nby an unsupervised action identification algorithm, which only considers observed exe-\ncutions of specific actions, without any information about not executed actions. Hence,\nsimilarly to our previous works on discovering procedural knowledge from uncertain\nexecution (Mazzi et al, 2023b; Meli et al, 2024), our task mainly matches the defini-\ntion of positive examples with an included (observed) and and excluded (unobserved)\nset in the CDPIs, rather than introducing strictly forbidding negative examples.\nAt the same time, only negative examples allow to learn hard constraints typical\nof real robotic tasks. For this reason, we embed constraints as extended preconditions\nof actions. In fact, constraints (3a)-(3d) can be added to preconditions (1) as:\n0 {move(A,ring,C,t): reachable(A,ring,C,t),not closed_gripper(A,t);\nmove(A,peg,C,t) : reachable(A,peg,C,t),not placed(ring,C1,peg,C,t);\nmove(A,peg,C,t) : reachable(A,peg,C,t), not placed(ring,C1,peg,C2,t),\nclosed_gripper (A,t), at(A,ring, C1,t);\nmove (A, center, C,t): at(A,ring,C,t), not placed (ring, C, peg, C1,t),\nclosed_gripper(A,t),at(A,ring,C1,t);\nextract (A,ring,C,t): at(A,ring,C,t), closed_gripper(A,t);\ngrasp (A,ring,C,t) : at(A,ring,C,t);\nrelease(A,t): at(A,ring,C,t),closed_gripper(A,t)} 1.\nwhere constraint (3a) is embedded in axiom (12d), constraints (3b)-(3d) are repre-\nsented in (12b)-(12c) and constraint (3c) is added to precondition (12a)."}, {"title": "4.1.3 Learning extended action preconditions", "content": "We now show how to translate action-context tuples into CDPIs for learning ASP task\nknowledge. We assume that axioms for each action can be learned separately, i.e., it is\npossible to launch ILP tasks in Definition 4 in parallel for all actions. This is reasonable,\nsince we want to capture ASP axioms matching partial interpretations to the context,\ni.e., relations between actions and environmental features similar to the ones presented\nin Section 3.3. Hence, mutual dependencies between actions can be ignored, since\nthey are implicitly represented with effects of actions on the environmental context.\nMoreover, we discard i-th action-context tuple for a given action label 1; (Lines 9-10)\nif\n$Pij < \\frac{1}{N} \\sum_{k=1}^{N} Pkj$"}, {"title": "4.1.4 Generating effects of actions", "content": "The quality of axioms representing effects of actions depends on the accuracy in the\nidentification of action labels. We generate axioms corresponding to effects of actions\nas follows. Assuming the i, j-th action-context tuple represents the task situation at\ntimestep t, with context Ct = Ci, we evaluate the differences between Ct-1-Ct on one\nside, and Ct+1-Ct on the other side. Specifically, for each ground atom f \u2208 G(F), with\nFE F, we verify:\n\u2022 if f\u2208 Ct and f\u2209 Ct\u22121, we define a probabilistic initiating example (F, lj, pij), meaning\nthat the effect of lj is to ground the environmental atom F with probability pij\n(Lines 13-14);\n\u2022 if f\u2208 Ct and f\u2209 Ct+1, we define a probabilistic terminating example (F, lj, pij), mean-\ning that the effect of lj is to unground the environmental atom F with probability\nPij (Lines 15-16).\nFor each F, we obtain a set of probabilistic initiating and terminating examples IF,\nEF, respectively. We then rank these sets according to\n$\\frac{1}{K} \\sum p_{ij} | (F, l_j, p_{ij}) \\in I_F$ (respectively, $E_F$)\nThe above mean measures the accuracy of identifying a generic action lj as an initi-\nating or terminating condition for a semantic feature F. We then identify 19 and 1,\ncorresponding respectively to max, EF and max\u014d; IF (Lines 22-23). We finally define"}, {"title": "4.2 Online supervised refinement of ASP task knowledge", "content": "Learned preconditions X and initiating / terminating conditions (I / E) for effects\nof actions may be incomplete or inexact, depending on the quality of the input video-\nkinematic stream S and the accuracy of the chosen unsupervised action identification\nalgorithm \u03a6. Hence, especially in safety-critical scenarios as robotic surgery, we must\ndesign a shared autonomy framework where autonomous ASP reasoning (based on\nlearned task knowledge) is interleaved with human control. At the same time, shared\nautonomy is useful to continuously improve the autonomous system, exploiting human\nfeedback as a teacher.\nTwo main assumptions underlie our methodology: i) ASP action labels are directly\nmapped to motion primitives for actual robotic execution; ii) a perception system (in\nour case, a RGB-D camera and kinematic sensors from the robot) is available to auto-\nmatically ground semantic features F and perform reasoning. Both are reasonable.\nIn fact, once timed segments are clustered according to action labels, it is possible\nto efficiently learn in closed form the corresponding primitive for each action, e.g., as\nshown by Ginesi et al (2020, 2021b). Moreover, semantic features are simple geometric\nre-interpretations of kinematic and geometric features from sensors, according to rela-\ntions (11), and Ginesi et al (2020); Tagliabue et al (2022); Meli et al (2021b) showed\nthat they can be efficiently computed for online planning.\nWe now detail the main steps of our proposed methodology. First, preconditions\nin X are converted to a set of aggregate preconditions Xa:\n0 {lo: F1, F2,..., Fn;\nl1: F1, F2,..., Fn;\nl|C|: F1, F2,...,Fn} 1.\nwhere Fi \u2208 F, Vi = 1, . . ., n. In this way, we set up an ASP program combining E, I, Xa\nwhich can be used for online task planning.\nAt the beginning of the task, the perception system grounds semantic features,\nthus triggering ASP reasoning. The generated plan is a sequence of actions Sa =\n(a1, a2,..., an), selected for execution according to a FIFO policy. Before executing a1,\nwhich is an instance of some lj, the user is asked a feedback on whether he approves\nit or not. In case of approval, we generate and store a positive not noisy CDPI for lj:"}, {"title": "5 Results", "content": "We now validate our methodology in the peg transfer domain. Specifically, we first\nanalyze the performance of Algorithm 1 on a relatively small dataset containing both\nvisual and kinematic information about task execution. Then, we show how to prac-\ntically include learned task knowledge into an autonomous robotic framework as the\none by Ginesi et al (2020), in order to realize shared autonomy with incremental\nrefinement of ASP task knowledge.\nAll experiments are run on a computer with AMD Ryzen 7 3700x CPU (8 cores,\n16 threads, 4.4 GHz nominal frequency) and 32 GB RAM."}, {"title": "5.1 Unsupervised learning of ASP task knowledge", "content": "We generate an input video-kinematic stream S for Algorithm 1, from the three task\ninstances depicted in Figure 2, executed by a user in teleoperation with dVRK and\nusing Intel Realsense RGB-D camera for visual acquisition. In Figure 2a, the task starts\nin the nominal situation described in FLS (Soper and Fried, 2008), with all 4 colored\nrings placed on grey pegs, on the opposite side with respect to the corresponding pegs.\nHence, PSMs must perform transfer before placing rings on pegs. In Figure 2b, a more\nchallenging situation is depicted. Relevant rings and pegs (i.e., the blue and red ones)\nare on one side of the peg base, hence no transfer is needed. However, rings occupy\npegs with mismatched color, thus one of the colored peg must be freed (by placing\na ring on a grey peg temporarily), before completing the task. Figure 2c shows an\nunconventional scenario with the red ring on the same side as its peg, while the blue\nring is on the opposite side out of any peg. Hence, not all rings require extraction\nin this task instance. Furthermore, before placing the blue ring on its peg (hence,\nafter transfer), the ring fell, hence it had to be grasped again to complete the task.\nThese three task examples well represent different flows of executions of peg transfer.\nHowever, their major challenge lies in the fact that actions are not equally represented.\nFor instance, move(A, center, C) occurs only 5 times (4 times in Figure 2a and once\nin Figure 2c), while release (A) occurs 21 times (10 times in Figure 2a, 5 times in\nFigure 2b and 6 times in Figure 2c).\nWe evaluate the performance of Algorithm 1 in terms of standard precision, recall\nand F1-score achieved by learned axioms, given a random environmental context for\nthe task. We assess the performance of the algorithm when using different unsuper-\nvised action identification strategies \u03a6. In particular, we consider our methodology\nproposed in Meli and Fiorini (2021) and the original work by Despinoy et al (2015),\nachieving lower F1-score, especially on small heterogeneous datasets of executions (as\nthe one in Figure 2). We refer the reader to the two references for more details. The\nessential difference between Meli and Fiorini (2021) and Despinoy et al (2015) is that\nthe latter only consider the kinematic signature to cluster action segments, while the\nformer also take into account the environmental representation given by the semantic\nfeatures. Hence, Meli and Fiorini (2021) are more robust to heterogeneous motions (a\npeculiarity of our dataset). For this reason, from now on we refer to the variant with\nthe algorithm by Despinoy et al (2015) as Akin, while the variant with the algorithm\nby Meli and Fiorini (2021) as Asem. Moreover, we also consider a baseline version of\nour algorithm where we perform action identification as in Meli and Fiorini (2021) and\ndo not consider the excluded set in CDPIs of equation (14). In other words, we learn"}, {"title": "5.2 Online supervised refinement of ASP task knowledge", "content": "Learned axioms from Algorithm 1 may be incomplete and unsafe for practical appli-\ncation in a fully autonomous robotic framework, due to inaccuracies of unsupervised\naction identification and Algorithm 1 itself. We now show how our shared autonomy\nframework (Section 4.2) can be applied to ensure safety and realize incremental refine-\nment of learned ASP task knowledge, embedding (expert) human feedback in the\nreasoning and execution loop. In particular, we investigate how precision, recall and\nF1-score of axioms evolve, as they are refined through expert feedback during the exe-\ncution of the task (for fair comparison, we do not consider the aggregate transformation\nin equation 15, i.e., we consider all possible ground atoms from learned axioms)."}, {"title": "6 Discussion", "content": "Our experiments evidence the efficacy of our methodology for offline robotic task\nknowledge learning from raw data, and interactive online refinement via human\nfeedback.\nThanks to the generalization capabilities of logical task formal-\nization, offline learning via ILP is highly data- and time-efficient,\nand robust to non-homogeneous small datasets of executions (i.e., non-\nrepetitive action sequences under diverse environmental conditions),\nregardless of the specific action identification algorithm. Moreover, our framework for"}, {"title": "7 Conclusion", "content": "In this paper, we have proposed a methodology based on ILP and ASP to discover task\nspecifications, encoding action preconditions, constraints and effects, from raw video-\nkinematic datasets of robotic executions. Our approach leverages on any unsupervised\naction identification algorithm to retrieve ILP examples from recordings, combining\nthem with almost task-agnostic commonsense concepts defined by an user, in order to\nincrease the interpretability, trustability and social acceptance of learned axioms.\nIn the context of the peg transfer task with dual-arm dVRK, a benchmark training\nexercise for novice surgeons, we show that our offline learning pipeline requires very\nbasic commonsense concepts, which are often task-agnostic since they only incorporate\ninformation about the location of robotic arms with respect to the environment, and\nthe kinematic state of the grippers. Moreover, the online learning algorithm is able to\nrefine task knowledge even with few task examples.\nThis work represents an important step towards interpretable robotic task learn-\ning and generalization from limited raw recordings of execution, showing promising\nresults for the scalability of our methodology to more complex and challenging robotic\nscenarios."}, {"title": "Appendix A Learned axioms", "content": "A.1 Unsupervised learning of ASP task knowledge"}, {"title": "A.1.1 Asem", "content": "release (V1", "t)": "not reachable (V1", "V,t)": "not closed_gripper(V1", "V2,t)": "at(V1"}, {"V2,t)": "not closed_gripper(V3"}, {"V2,t)": "closed_gripper(V1"}, {"V2,t)": "not closed_gripper ("}]}