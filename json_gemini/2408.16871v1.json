{"title": "GSTAM: EFFICIENT GRAPH DISTILLATION WITH STRUCTURAL ATTENTION-MATCHING", "authors": ["Arash Rasti-Meymandi", "Ahmad Sajedi", "Zhaopan Xu", "Konstantinos N. Plataniotis"], "abstract": "Graph distillation has emerged as a solution for reducing large graph datasets to smaller, more\nmanageable, and informative ones. Existing methods primarily target node classification, involve\ncomputationally intensive processes, and fail to capture the true distribution of the full graph dataset.\nTo address these issues, we introduce Graph Distillation with Structural Attention Matching (GSTAM),\na novel method for condensing graph classification datasets. GSTAM leverages the attention maps\nof GNNs to distill structural information from the original dataset into synthetic graphs. The\nstructural attention-matching mechanism exploits the areas of the input graph that GNNs prioritize\nfor classification, effectively distilling such information into the synthetic graphs and improving\noverall distillation performance. Comprehensive experiments demonstrate GSTAM's superiority over\nexisting methods, achieving 0.45% to 6.5% better performance in extreme condensation ratios,\nhighlighting its potential use in advancing distillation for graph classification tasks", "sections": [{"title": "1 Introduction", "content": "The focus on graph-structured information has increased dramatically over the past decade due to the rise of social\nmedia, recommendation systems, and various real-world structures like molecules, 3D meshes, and brain connectivity.\nToday, graph-structured datasets, such as knowledge graphs [1, 2] and e-commerce platforms [3], have grown to\nencompass millions of nodes and billions of edges. This growth introduces significant challenges for both training\nGraph Neural Networks (GNNs) and exploring new research areas, such as continual learning [4], neural architecture\nsearch [5, 6, 7], and knowledge amalgamation [8].\nTo address these issues, one effective solution is graph condensation (also known as graph distillation), which involves\nreducing the size of a large graph dataset to a smaller, more manageable one. Graph condensation can be categorized\ninto four primary optimization approaches. (i) Coreset selection involves selecting a representative subset of graphs\nfrom the original dataset, although it typically does not consider the specific requirements of downstream tasks. (ii)\nGradient matching, as explored in [9, 10], focuses on aligning the gradients of network parameters between synthetic\nand real graph data. Despite their success, these methods rely on a bi-level optimization problem with inner and outer\nloops, which are challenging to solve. (iii) Trajectory matching, discussed in [11, 12], aligns the training trajectories of\na student GNN on the condensed graph with those of multiple teacher GNNs on the original graph. However, trajectory\nmatching is time- and memory-intensive, requiring training and using multiple expert models during optimization. (iv)"}, {"title": "1.1 Related Works", "content": "Coreset selection. Coreset selection is an early data-centric technique designed to efficiently identify a representative\nsubset from a full dataset, enhancing the performance and efficiency of downstream training. Various approaches have\nbeen developed over time, including geometry-based methods [17, 18], loss-based techniques, decision-boundary-\nfocused strategies, bilevel optimization methods, and gradient-matching algorithms. Among these approaches are\nseveral notable ones: Random: Selects samples randomly to form the coreset. Herding[19]: Chooses samples closest\nto the cluster center. K-Center[20, 18]: Select multiple center points to minimize the maximum distance between\ndata points and their nearest center. While these methods have demonstrated moderate success in improving training\nefficiency, they face inherent limitations in capturing comprehensive information. Treating each image in the selected\nsubset independently results in a lack of rich features that could be obtained by considering the diversity within classes.\nThese limitations have driven the development of dataset distillation in the field.\nGraph distillation. Unlike traditional subset selection methods, the idea of graph distillation is to learn a small\nsynthetic graph dataset while considering the downstream classification task. There are also several graph-specific\napproaches designed explicitly for graphs. The eigenbasis matching method proposed in [21] aligns the eigenbasis\nand node features of real and synthetic graphs to enhance the efficiency and generalization of GNN training. However,\nit may underperform on certain large-scale graphs due to the limited number of eigenvectors available for eigenbasis\nmatching. In [22], the MIRAGE approach mines frequently co-occurring computation trees in graphs to distill a smaller,\nmore informative dataset for training GNNs. This approach achieves higher prediction accuracy, significantly improved\ndistillation efficiency, and superior data compression rates. However, specifying the reduction ratio is challenging.\nFurthermore, the MIRAGE approach assumes that the downstream GNN must be a message-passing network, which\nmay not hold for models such as graph transformers [23]. DosCond [9] assumes discrete graph structures as probabilistic\nmodels and performs one-step gradient matching without training the network weights. This method accelerates the\ncondensation process. However, it still relies on gradient-matching similar to traditional methods which might not\ncapture the true distribution of the full graph dataset."}, {"title": "1.2 Structural Attention-maps in GNNs", "content": "It has been shown in [14] that activations following convolutional layers in a convolutional neural network highlight the\nimportance of spatial information in the input image. In a similar vein, we argue that this is also true for GNNs and\ngraph inputs. Consider a GNN model with three Graph Convolution (GC) layers. fig. 1 illustrates the structural attention"}, {"title": "2 Methodology", "content": "In this section, we introduce the proposed graph dataset distillation method, GSTAM, which leverages attention maps\ngenerated at each layer of a GNN. The core idea behind GSTAM is that each layer of a GNN focuses on distinct structural\nfeatures of the input graph, analogous to spatial attention mechanisms used in computer vision [15, 14]. By aligning the\nstructural attention across various layers of the GNN (initial, intermediate, and final layers) trained on both the full\nand synthetic graph datasets, we can guide the synthetic graph generation process to produce datasets that are more\ngeneralized and effective for training GNNs on downstream tasks. The overall procedure of GSTAM is depicted in fig. 2"}, {"title": "2.1 Graph Distillation Via Structural Attention Matching", "content": "Suppose we have a graph classification dataset $T = \\{(G_1, Y_1), ..., (G_N, Y_N)\\}$, where $G_i$ is the graph sample associated\nwith label $y_i$. Each graph sample $G_i$ consists of an adjacency matrix $A_i \\in \\mathbb{R}^{m_i \\times m_i}$ and a node feature matrix\n$X_i \\in \\mathbb{R}^{m_i \\times d}$, where $m_i$ denotes the number of nodes in $G_i$ and $d$ is the feature dimension for each node. Now,\nconsider a synthetic dataset $S = \\{(G'_1, y_1), ..., (G'_M,y_M)\\}$ with $M \\ll N$. In the synthetic dataset, each graph $G'$ has\nan adjacency matrix $A' \\in \\mathbb{R}^{n \\times n}$ and a node feature matrix $X' \\in \\mathbb{R}^{n \\times d}$, where $n$ is set as $n = \\sum_{i=1}^{N} m_i$ and the\nfeature dimension $d$ is the same as in the original graphs. The synthetic graphs $G'$ can be initialized randomly or by\nusing a portion of the nodes from the graph samples selected via the K-Center method [20, 27].\nConsider the cth class in the dataset and a randomly initialized GNN model $GNN_{\\theta}(\u00b7)$ parameterized by $\\theta_l$ for $l =$\n1, ..., $L$, where $l$ indicates the $l^{th}$ layer. In most GNN models, $L$ is a small number to prevent the over-smoothing problem\ncaused by deeper layers [28]. First, we feed $T_c$, the subset of $T$ containing only class $c$ samples, to the GNN model to\nobtain $GNN_{\\theta}(T_c) = [f_{\\theta}^o, ..., f_{\\theta}^o]$. Similarly, for the synthetic dataset $S_c$, we get $GNN_{\\theta}(S_c) = [f_{\\theta}^o, ..., f_{\\theta}^o]$. Here,\n$f_{\\theta}^o$ and $f_{\\theta}^o$ are the feature maps of different GNN layers after the activation function, with dimensions $\\mathbb{R}^{|B_{T_c}| \\times m_i \\times u_l}$\nand $\\mathbb{R}^{|B_{S_c}| \\times n \\times u_l}$, respectively. $B_c$ and $B_{S_c}$ denote the batch of samples for the full and synthetic graph datasets in\nclass $c$, respectively. Additionally, $u_l$ represents the feature dimension in the $l^{th}$ layer. Without loss of generality, we\nassume there is no pooling layer in the GNN models, meaning the number of nodes in each graph sample remains the\nsame throughout the layers of the GNN model."}, {"title": "2.1.1 Structural Attention Maps.", "content": "We now introduce our Structural Attention Map (STAM) module by defining a feature-based mapping function\n$A(\u00b7)$. This special function creates structural attention maps for the feature maps of different layers. We define\n$A(GNN_{\\theta}(T_c)) = [a_{\\theta}^o, ..., a_{\\theta}^o]$ and $A (GNN_{\\theta}(S_c)) = [a_{\\theta}^o, ..., a_{\\theta}^o]$ as the attention maps of the original and\nsynthetic graph datasets transformed with STAM. Conventionally, it was shown in [14, 15] that a good and flexible\nchoice of the mapping function is a spatial-wise aggregation of the $f_{\\theta}^o$, i.e., $A(f_{\\theta}^o) = a_{\\theta}^o = \\sum_{chi} (f_{\\theta}^o)_i^p, i \\forall p$, where\n$Chi$ is the channel number and $p$ controls the attention map. However, in GNNs, the mapping function cannot be defined\nnaively since the dimensionality of $f_{\\theta}^o$ and $f_{\\theta}^o$ are not consistent, i.e., $f_{\\theta}^o \\in \\mathbb{R}^{|B_{T_c}| \\times m_i \\times u_l}$ and $f_{\\theta}^o \\in \\mathbb{R}^{|B_{S_c}| \\times n \\times u_l}$.\nTo alleviate this problem, we propose the following mapping function:\n$$\nA(f_{\\theta}^o) = \\bigg(\\frac{1}{m_i}\\bigg)f_{\\theta}^o f_{\\theta}^o T,\n$$\nwhere $A(f_{\\theta}^o) = a_{\\theta}^o \\in \\mathbb{R}^{|B_{T_c}| \\times u_l \\times u_l}$. A similar mapping function creates $a_{\\theta}^o \\in \\mathbb{R}^{|B_{S_c}| \\times u_l \\times u_l}$. The resultant\nattention maps highlight the important sub-structures on the graph associated with the neuron, which can be further\nutilized in the distillation process. This is because the absolute value of hidden neurons can reveal the importance of\ndifferent parts of the input data, as shown in [14, 15]. The impact of $p$ will be investigated in Section section 3.3."}, {"title": "2.1.2 Distillation Losses.", "content": "we compare the normalized spatial attention maps of each layer (excluding the final layer) between $T_c$ and $S_c$ as follows\n$$\nL_{STAM} = \\sum_{c=1}^{C} \\sum_{l=1}^{L-1} \\frac{1}{|B_{T_c}|} \\sum_{i=1}^{|B_{T_c}|} \\bigg(\\frac{(z_{\\theta_l}^o)_i}{\\sqrt{\\sum_{i=1}^{|B_{T_c}|} || (z_{\\theta_l}^o)_i ||^2}} - \\frac{1}{|B_{S_c}|} \\sum_{i=1}^{|B_{S_c}|} \\frac{(z_{\\theta_l}^o)_i}{\\sqrt{\\sum_{i=1}^{|B_{S_c}|} || (z_{\\theta_l}^o)_i ||^2}} \\bigg)^2,\n$$\nwhere $z_{\\theta_l}^o$ and $z_{\\theta_l}^o$ denote the vectotized versions of $a_{\\theta_l}^o$ and $a_{\\theta_l}^o$, respectively, with the same dimension. In addition\nthe subscript i indicates $(z_{\\theta_l}^o)_i = z_{\\theta_l}^o (i, :, :)$. Note that normalizing $z_{\\theta_l}^o$ has been shown to be useful in the distillation\nprocess [16]. In addition, $P_{\\theta}$ represents the distribution of the GNN's parameters.\nAlthough $L_{STAM}$ effectively captures the distribution of the original graph dataset, it does not account for the last layer\nof the GNN. Research has consistently shown that the final layer encapsulates abstract and semantic information of the\ninput [29, 30, 31]. This layer in the case of GNNs is often a linear layer situated after the pooling operation and before\nthe activation function. To address this, we introduce an additional loss based on the last layer of the GNN, comparing\nthe full and synthetic graph datasets using the L2-norm. This loss is formulated as:\n$$\nL_{reg} = \\alpha P_{\\theta} \\sum_{c=1}^{C} \\bigg| \\frac{1}{|B_{T_c}|} \\sum_{i=1}^{|B_{T_c}|} (f_{\\theta_L}^o)_i - \\frac{1}{|B_{S_c}|} \\sum_{i=1}^{|B_{S_c}|} (f_{\\theta_L}^o)_i \\bigg|^2,\n$$\nwhere $f_{\\theta_L}^o \\in \\mathbb{R}^{|B_{T_c}| \\times u_l}$ and $f_{\\theta_L}^o \\in \\mathbb{R}^{|B_{S_c}| \\times u_l}$ represent the feature maps of the last layer of the GNN models trained\non the full and synthetic graph dataset and the subscript i has the same meaning as explained in $L_{STAM}$."}, {"title": "2.1.3 Adjacency Matrix Optimization.", "content": "To update the adjacency matrix, we follow the procedure described by Jin et al. [9]. We assume that the adjacency\nmatrices follow a Bernoulli distribution, defined as $Pr_{\\theta} (A_i) = A_i^{ \\sigma(\\theta_{ij})} + (1 - A_{ij})^{\\sigma(-\\theta_{ij})}$, where $\\Omega_i$ represents\nthe success probability matrix of the Bernoulli distribution for the $k^{th}$ adjancecy matrix and its values are learned"}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Setup", "content": null}, {"title": "3.1.1 Graph Datasets.", "content": "We evaluate the proposed GSTAM alongside other algorithms on graph classification datasets. In line with previous\nstudies [9, 22], we select molecular datasets from the Open Graph Benchmark (OGB) (ogbg-molhiv, ogbg-molbbbp)\n[32] and TU datasets (DD, MUTAG, NCI1) [1]. A summary of these datasets is provided in table 1. For the DD,\nMUTAG, and NCI1 datasets, we randomly sample and split the data into 80% for training, 10% for validation, and 10%\nfor testing. For the ogbg-molhiv and ogbg-molbbbp datasets, we use the splits provided by OGB."}, {"title": "3.1.2 Network Architectures and Implementation Details.", "content": "Similar to the work in [9], we select a 3 convolutional layer-GNN model with the feature size of 128 followed by a\nfully connected layer with 128 neurons. We followed the same procedure as indicated in [9]. The total number of\niterations is set to $T = 1000$ where in each iteration a randomly initialized parameter for the GNN model is chosen\naccording to the distribution $P_{\\theta}$, i.\u0435., $\\theta \\sim P_{\\theta}$. The feature and adjacency matrix learning rates are set to $\\eta_{\\zeta} = 0.005$\nand $\\zeta = 0.01$, respectively. Furthermore, we set the task balance parameter $\\lambda$ to 0.1 and $p$ to 2. The effect of both\nparameters is studied in section 3.3."}, {"title": "3.1.3 Baselines.", "content": "We compared the proposed GSTAM against three Coreset methods that select graph samples from the full graph dataset,\nnamely Random, Herding [19], and K-Center[20, 18], and two distillation algorithms that learn the distilled synthetic\ngraphs, such as DCG [33] and DosCond [9]. In Random, we select the graph samples for a particular class randomly. In\nHerding, we iteratively select samples that are closest to the mean feature representation of the class. In K-Center, we\nchoose samples that maximize the minimum distance to the selected set, ensuring diversity. For DCG and DosCond, the\nsame protocol and hyper-parameters are used as described in [9] to have a fair comparison."}, {"title": "3.1.4 Evaluation.", "content": "The evaluation process involves training on the distilled graphs and testing on the original test set as a graph classification\nproblem. Specifically, we first run the GSTAM algorithm, as detailed in algorithm 1, to generate the distilled graphs.\nNext, we train a randomly instantiated GNN model using these distilled graphs with the learning rate of 0.001 for 500\nepochs (expect for ogbg-molhiv which is 100 epochs). Finally, we test the trained GNN on the original test dataset. For\nthe DD, MUTAG, and NCI1 datasets, the evaluation metric used is accuracy, while for the others, we use ROC-AUC\ndue to their imbalanced nature. The same procedure is applied to the baselines, where we construct the distilled graphs,\ntrain a GNN, and then test it. The distillation process is repeated five times, and the GNN is trained on 10 different\nrandomly instantiated models."}, {"title": "3.2 Comparison To State-of-the-art Methods", "content": null}, {"title": "3.2.1 Performance Comparison.", "content": "We compared the proposed GSTAM with various coreset and distillation approaches, as shown in table 2. GSTAM\nsignificantly outperforms most coreset algorithms. This superior performance is attributed to GSTAM's ability to learn\ndistilled graphs specifically optimized for downstream graph classification tasks. Additionally, GSTAM achieves higher\nclassification performance compared to SOTA graph distillation algorithms, such as DosCond, across almost all\ndatasets. This improvement is primarily because GSTAM incorporates the attention mechanisms of GNN layers during\nthe distillation process, unlike DosCond.\nA detailed analysis reveals that GSTAM excels, particularly with small ratios which makes it a good candidate for creating\nsmall-sized synthetic graph datasets. For the MUTAG dataset, GSTAM achieves a lossless performance against the full\ndataset and 6.9% higher accuracy than DosCond. In addition, our proposed algorithm performs nearly as well as the full\ndataset on the ogbg-molbbbp dataset. We hypothesize that GSTAM's lossless performance compared to the full dataset is\ndue to the distillation process reducing the impact of outliers present in the original dataset."}, {"title": "3.2.2 Running Time.", "content": "We assess the computational efficiency of our method by comparing it with DosCond and a coreset approach, Herding.\nHerding is known for its relatively lower time consumption compared to other coreset methods and its generally"}, {"title": "3.3 Ablation Studies", "content": null}, {"title": "3.3.1 Exploring the effect of A.", "content": "The parameter $\\lambda$ determines the importance of matching the distribution of the last layer. fig. 3a and fig. 3b illustrate\nthe impact of $\\lambda$ on the graph distillation performance for the ogbg-molbbbp and ogbg-molhiv datasets, respectively.\nWhen $\\lambda$ is small, indicating that $L_{reg}$ is not significantly considered, the performance is low. Conversely, as $\\lambda$\nincreases, we observe the importance of $L_{STAM}$ in enhancing performance. However, when $\\lambda$ becomes too large,\nthe performance decreases. This suggests that while the structural attention matching loss $L_{STAM}$ significantly\ncontributes to performance improvement, a small but optimal amount of the regularization term $L_{reg}$ is necessary to\nboost performance effectively."}, {"title": "3.3.2 Exploring the effect of p.", "content": "The parameter $p$ in eq. (1) influences the model's ability to catch the attention patterns similar to the ones used in\n[14, 16]. fig. 3c and fig. 3d show the impact of varying $p$ on the graph distillation performance for the ogbg-molbbbp\nand ogbg-molhiv datasets, respectively. We can see that a certain value for $p$ cannot be determined and the performance\nis fairly consistent over different values. However, in our experiments, we heuristically set $p = 2$ since the distillation\noverall works better for all datasets."}, {"title": "3.3.3 Cross-Architecture Analysis.", "content": "To assess the generalizability of the learned graphs, we trained synthetic data on one architecture and tested it using\ndifferent model architectures. table 4 presents the cross-architecture testing performance. Specifically, we trained the\nsynthetics using a GNN model with two (GCN-2C) and four (GCN-4C) graph convolutional layers on the MUTAG\ndataset, with one graph per class. The architectures we select for the testing phase are GCN-2C, GCN-3C, a three-layer\nGraph Isomorphism Network (GIN) [34], and a three-layer Message Passing Neural Network (MPNN) [35].\nAlthough the accuracy of cross-architecture performance decreases for both GSTAM and DosCond, GSTAM still con-\nsistently outperforms DosCond across nearly all models. This is attributed to GSTAM's ability to incorporate the\nfocus of the GNN layers into the synthetic graphs, leading to a better representation of the datasets in the distilled\ngraphs. Consequently, GSTAM demonstrates great generalizability compared to approaches such as gradient matching in\nDosCond."}, {"title": "4 Conclusion", "content": "In this paper, we introduced Graph Distillation with Structural Attention Matching (GSTAM), a novel method for\ncondensing graph classification datasets. GSTAM leverages the attention maps of Graph Neural Networks (GNNs) to\ndistill structural information from the original dataset into synthetic graphs. Our comprehensive experiments show\nthat GSTAM not only excels in accuracy, particularly with small condensation ratios compared to the baselines but also\nmatches or surpasses full dataset performance by mitigating outlier impacts in some cases. Further assessment of\ncross-architecture analysis demonstrates that GSTAM consistently outperforms existing methods like DosCond across\nvarious model architectures. These results underscore GSTAM's potential for enhancing graph classification tasks, making\nit a valuable tool for performing graph distillation in practical applications. Future work involves applying GSTAM on\nmore complex graph classification datasets as well as node classification tasks."}]}