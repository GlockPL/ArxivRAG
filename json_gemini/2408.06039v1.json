[{"title": "Spacetime E(n)-Transformer: Equivariant Attention for Spatio-temporal Graphs", "authors": ["Sergio G. Charles"], "abstract": "We introduce an E(n)-equivariant Transformer architecture\nfor spatio-temporal graph data. By imposing rotation, trans-\nlation, and permutation equivariance inductive biases in\nboth space and time, we show that the Spacetime E(n)-\nTransformer (SET) outperforms purely spatial and temporal\nmodels without symmetry-preserving properties. We bench-\nmark SET against said models on the charged N-body prob-\nlem, a simple physical system with complex dynamics. While\nexisting spatio-temporal graph neural networks focus on se-\nquential modeling, we empirically demonstrate that leveraging\nunderlying domain symmetries yields considerable improve-\nments for modeling dynamical systems on graphs.", "sections": [{"title": "1 Introduction", "content": "Many problems that we wish to model with neural networks\npossess underlying geometric structure with symmetries. Ge-\nometric Deep Learning, a term coined in the seminal work of\n(Bronstein et al. 2021), is an Erlangen program for deep learn-\ning that systematizes inductive biases as group symmetries\nG, arising through notions of invariance and equivariance.\nRecent work, like SE(3)-Transformers (Fuchs et al. 2020)\nand E(n)-Graph Neural Networks (Satorras, Hoogeboom,\nand Welling 2022), impose different notions of group equiv-\nariance on neural networks to inform architecture choice.\nWhile these neural network architectures encode spatial in-\nductive biases, they notably lack a time component. Temporal\nGraph Networks (Rossi et al. 2020) proposed an efficient\nframework that learns from dynamic graphs. However, this\narchitecture assumes the topology of graphs changes over\ntime. In this paper, we discuss spatio-temporal graphs, which\nhave a fixed topology with changing features over discrete\ntime steps. Recent works (Jin et al. 2023), (Marisca, Cini, and\nAlippi 2022), (Cini et al. 2023) have treated node features as\ntime series and edges as the relationships between these se-\nries. Such spatio-temporal graph neural networks (STGNNs)\nhave a plethora of applications, from simulating biomolecular\ninteractions to modeling financial time series.\nSimilarly, while STGNNs improve representation learning\nof sequential graph data, to the best of our knowledge, mini-\nmal research has been done on preserving group symmetries\nin a spatio-temporal fashion. In particular, sequential models\nought to preserve spatial group symmetries at each time step.\nFamously, Noether's first theorem formalizes the notion of\ninfinitesimal symmetries of the so-called Lagrangian of a\nphysical system, in terms of perturbations with respect to\nboth space and time, by determining conserved quantities. In-\nspired by this intuition of temporal and spatial symmetry, we\nseek to derive a neural network architecture that is equivariant\nin both temporal and spatial components.\nClassical neural network architectures like RNNs are dis-\ncrete approximations to continuous time-domain signals,\nobeying a differential equation with respect to time. If an\nRNN is invariant to time-warping, a monotonically increas-\ning and differentiable function of time, it takes the form of an\nLSTM (Bronstein et al. 2021), which unlike a vanilla RNN,\ncaptures long-term dependencies. Similarly, the dynamics of\nclassical physical systems satisfy the Euler-Lagrange equa-\ntions, i.e. the equations of motion. Hence, we use the charged\nN-body problem, as described in (Satorras, Hoogeboom,\nand Welling 2022) and adapted from (Kipf et al. 2018), as\nan ideal candidate to test our hypothesis that preserving G-\nequivariance ameloriates long-term spatio-temporal graph\nmodeling. We will use a Transformer for the temporal compo-\nnent of the architecture, preserving long-term dependencies\nand, hence, invariance to time-warping. Each node, represent-\ning a charged particle of the graph, will have features, coor-\ndinates, and velocities. As such, the neural network should\nbe equivariant under rotational and translational symmetries\nE(n) acting on coordinates. It should also be equivariant with\nrespect to rotational symmetries SO(n) acting on velocities.\nLastly, the nodes should be permutation equivariant."}, {"title": "2 Background", "content": "2.1 Geometric Deep Learning\nFollowing the insights of Geometric Deep Learning (Bron-\nstein et al. 2021), the input signals to machine learning mod-\nels have an underlying domain \u03a9. Examples of such domains\ninclude grids, graphs, and manifolds. The space of signals\nover \u03a9 possesses a vector-space structure. That is (Bronstein\net al. 2021):\nDefinition 2.1. The space of C-valued signals on \u03a9 is\nX(\u03a9, C) = {x : \u03a9 \u2192 C},\nwhich is a vector space of functions."}, {"title": "3 Method", "content": "In this paper, we are interested in physical systems that can\nbe modelled as a sequence of graphs $G_t = (V_t, E_t)$ for $t =$\n$1,..., L$ with nodes $v_i(t) \\in V_t$ and edges $e_{ij}(t) \\in E_t$. In\nparticular, we seek to model the dynamics of the charged N-\nbody problem (Kipf et al. 2018). For this task, we assume a\npriori that the graph is complete since a charged particle will\ninteract with every other particle in a Van der Waals potential\nunder Coulomb's law. In addition, we assume that particles\nare neither created nor destroyed as the system evolves in\ntime, so the nodes $V_t$ in the graph remain the same. Let\n$G := (G_t)_{1<t<L}$ be a sequence of topologically-identical\ngraphs with changing features, known as a spatio-temporal\ngraph. The task under consideration is learning a function\nthat predicts the associated features of graph. In particular,\ngiven $G$, we are interested in predicting the positions and\nvelocities of all particles in the system after H additional\ntime steps where H >> L.\nTo equip the spatio-temporal model of particle interactions\nwith the appropriate inductive biases, we leverage both spa-\ntial and temporal notions of attention. For node i at time\nstep t to attend to all the past neighborhoods of that node,\nwe need to (1) aggregate nodes spatially to obtain spatially-\ncontextual embeddings and (2) obtain temporally-contextual\nembeddings via temporal aggregation.\nWe fix a time slice t such that the features derive from\n$G_t = (V_t,E_t)$. From the current features $h_i^{(l)}(t)$ of node\ni at layer l, we form the next layer features $h_i^{(l+1)}(t)$ by\naggregating neighboring node features. In particular,\n$h_i^{(l+1)}(t) = \\varphi(\\sum_{j \\in \\mathcal{N}} \\alpha(h_i^{(l)}(t), h_j^{(l)}(t))h_j^{(l)}(t)),$\nwhere \u03c6 is a permutation-invariant function (Bronstein\net al. 2021), and \u03b1 is a self-attention mechanism, often a\nnormalized softmax across neighbors."}, {"title": "3.1 E(n)-Equivariant Spatial Attention", "content": "(Satorras, Hoogeboom, and Welling 2022) introduced E(n)-\nEquivariant Graph Neural Networks (EGNNs). Every node\nin the graph G = (V,E) has features $h_i \u2208 R^d$ and co-\nordinates $x_i \u2208 R^n$. In addition, we keep track of each\nparticle's velocity $v_i \u2208 R^n$. The Equivariant Graph Con-\nvolutional Layer (EGCL) takes the set of node embed-\ndings $\\mathcal{H}^{(l)} = \\{h_1^{(l)},...,h_N^{(l)}\\}$, coordinate embeddings\n$\\mathcal{X}^{(l)} = \\{x_1^{(l)},...,x_N^{(l)}\\}$, velocity embeddings $\\mathcal{V}^{(l)} =$\n$\\mathcal{E} = (e_{ij})$ as in-\nput and produces the embeddings of the next layer. That\nis, $H^{(l+1)}, X^{(l+1)}, V^{(l+1)} = EGCL[H^{(l)}, X^{(l)}, V^{(l)}, E]$, defined\nas follows (Satorras, Hoogeboom, and Welling 2022):\n$m_{ij} = d_e (h_i^{(l)}, h_j^{(l)}, \\|x_i - x_j\\|^2, a_{ij})$\n$v_i^{(l+1)} = \\Phi_v (h_i^{(l)}) + \\sum_{j \\neq i} \\Phi_x(x_i - x_j) \\Phi_m (m_{ij})$\n$x_i^{(l+1)} = x_i^{(l)} + v_i^{(l)}$\n$m_i = \\sum_{j \\neq i} m_{ij}$\n$h_i^{(l+1)} = \\Phi_h(h_i^{(l)}, m_i)$,\nwhere $a_{ij}$ are the edge attributes, e.g. the edge values $e_{ij}$,\nand $d_e: R^{2d+2} \\rightarrow R^h$, $\\Phi_v: R^d \\rightarrow R$, $\\Phi_x: R^h \\rightarrow R$, and\n$\\Phi_h: R^{d+h} \\rightarrow R^{d'}$ are MLPs. In what follows, we assume\n$d'=d$ for clarity.\n(Satorras, Hoogeboom, and Welling 2022) proved that this\nlayer is equivariant to rotations and translations on coordi-\nnates and equivariant to rotations on velocities:\n$h_i^{(l+1)}, Qx_i^{(l+1)} + b, Qv_i^{(l+1)} = EGCL[h_i^{(l)}, Qx_i + b, Qv_i, E]$\nfor $Q \\in R^{n \\times n}$ an orthogonal rotation matrix and $b \\in R^n$ a\ntranslation vector. The EGCL is also permutation equivariant\nwith respect to nodes V.\nSince we have a sequence of graphs $G = \\{G_t\\}_{1 \\le t \\le L}$, for\na time slice t, we apply $K_t$ such EGCL transformation layers\nto the graph $G_t \\in \\mathcal{EG}$:$\nh_i^{(l+1)} (t), x_i^{(l+1)} (t), v_i^{(l+1)} (t) = EGCL[h_i^{(l)} (t), x_i^{(l)} (t), v_i^{(l)} (t), E(t)]$\nfor l = 1, . . ., $K_t$. Thus, we obtain spatially-contextual repre-\nsentations for node i at time t defined as $\\theta_i(t) = h_i^{(K_t)}(t) \\in\nR^d$, $\\xi_i(t) = x_i^{(K_t)} (t) \\in R^n$, $\\omega_i(t) = v_i^{(K_t)} (t) \\in R^n$ for\nt = 1, ..., L."}, {"title": "3.2 Temporal Attention for Graphs", "content": "The objective of this section is to obtain strong temporally-\ncontextual representations of the spatial graph embeddings.\nIn the charged N-body problem, we are essentially solving\nthe forward-time Euler-Lagrange equations, a second-order\npartial differential equation. However, for a fixed node on\nthe spatio-temporal graph, the feature, position, and velocity\nform a time-series, for which RNN's capture short-term de-\npendencies. It was shown in (Tallec and Ollivier 2018) that\nwhile vanilla RNNs are not time-warping invariant, LSTMs\nare a class of such time-warping invariant functions modeling\na continuous time-domain signal. Employing this philoso-\nphy, the use of an attention-based Transformer architecture\nto model spatio-temporal graph data merits investigation."}, {"title": "3.3 E(n)-Equivariant Temporal Attention", "content": "We would like the temporal attention to retain the equivariant\nproperties described in Section 3.1. Namely, the Equivariant\nTemporal Attention Layer (ETAL) should be equivariant to\nthe actions of E(n) on coordinates and the actions of SO(n)\non velocities. It should also be permutation equivariant with\nrespect to the actions of the symmetric group $\\sum_N$ on nodes.\nAs we do not impose symmetry conditions on the layer\nwith respect to the feature representations $h_i(t)$, we can apply\nkey-query-value self-attention as follows. Define the node-\nwise query $q_i(t) = Q_i \\theta_i(t)$, key $k_i(t) = K_i \\theta_i(t)$, and value\n$v_i(t) = V_i \\theta_i(t)$ for $Q_i, K_i, V_i \\in R^{d \\times d}$. To reduce memory\nusage, we share Q, K, V for all nodes. Then the temporally-\ncontextual representation is:\n$\\theta_i(t) := \\sum_{s=1}^{L} a_i(t,s)v_i(s)$\nwhere\n$a_i(t,s) = \\frac{\\exp(q_i(t)^T k_i(s))}{\\sum_{s'=1}^{L} \\exp(q_i(t)^T k_i(s'))}$.\nSatorras et al. (Satorras, Hoogeboom, and Welling 2022)\nshowed that for a collection of points $\\{x_i\\}_{i=1}^{N} \\in R$, the\nnorm is a unique geometric identifier, such that collections\nseparated by actions of E(n) form an equivalence class. With\nthis in mind, since we desire the attention mechanism for\ncoordinates $\\xi_i(t)$ to be equivariant with respect to E(n), we\ncan define:\n$\\xi_i(t) := \\xi(t) + B \\sum_{s=1 \\atop s \\neq t}^{L} \\beta_i(t,s) ((\\xi_i(s) - \\xi_i(t)),$\nwhere\n$\\beta_i(t,s) = \\frac{\\exp(\\|\\xi_i(t) - \\xi_i(s)\\|^2)}{\\sum_{s'=1}^{L} \\exp(\\|\\xi_i(t) - \\xi_i(s')\\|^2)}$.\nThis bears semblance to the neighborhood attention de-\nscribed in the SE(3)-Transformer network (Fuchs et al.\n2020) and Tensor Field Network layer (Thomas et al. 2018),\nthe intensity function in (Zhang, Cook, and Yilmaz 2021),\nand the invariant point attention in (Jumper et al. 2021).\nWe define an SO(n)-equivariant attention layer for veloci-\nties $\\omega_i(t)$:\n$\\omega_i(t) := \\sum_{s=1}^{L} \\gamma_i(t,s)\\omega_i(s)$\nwhere the weight is\n$\\gamma_i(t,s) = \\frac{\\omega_i(t)^T \\omega_i(s)}{\\sum_{s'=1}^{L} \\exp(\\omega_i(t)^T \\omega_i(s'))}$.\nIn appendix B, we show that the position attention function\nis E(n)-equivariant and the velocity attention function is\nSO(n)-equivariant.\nFollowing the insights of (Jin et al. 2023), edges are rela-\ntionships between time series and they should evolve. Hence,\nwhile the adjacency matrix $A \\in R^{N \\times N}$ is constant in space\nwhen applying ECGL, it should intuitively evolve in time\nwhen applying ETAL. That is, if we consider edges as rep-\nresenting the interaction between charged particles, e.g. the\nstrength of the force, then this must necessarily evolve in\ntime for a non-stationary point cloud system."}, {"title": "3.4 Spacetime E(n)-Equivariant Graph Transformer", "content": "The full spatio-temporal attention module is presented in Al-\ngorithm 1. It takes as input the node features $h \\in R^{L \\times N \\times d}$,\npositions $x \\in R^{L \\times N \\times n}$, velocities $v \\in R^{L \\times N \\times n}$ and adja-\ncency matrices $A \\in R^{L \\times N \\times N}$. For a spatio-temporal graph\n$G = (G_t)_{1 \\le t \\le L}$, we apply an equivariant spatial attention\nlayer in the form of EGCL to obtain spatially-contextual\nrepresentations $\\theta(t) \\in R^{N \\times d}$, $\\xi(t) \\in R^{\\mathbb{N} \\times n}$, $\\omega(t) \\in R^{N \\times n}$\nfor t = 1,..., L. We share the same EGCL layer across all\ntime steps t = 1,..., L. That is, we only learn one set of\nMLPS $\\varphi_{\\varepsilon}$, $\\varphi_{\\upsilon}$, $\\Phi_{\\alpha}$, and $\\varphi_h$ for each layer across time, which\nis significantly more memory and parameter efficient.\nObserve, at each time step, we apply a transforma-\ntion $\\mathcal{E} : R^{N \\times N} \\times R^{N \\times n} \\rightarrow R^{N \\times (N-1) \\times 2}$ to the adja-\ncency matrix $A(t) \\in R^{N \\times N}$ and the coordinates $x(t) \\in\nR^{N \\times n}$ for $G_t$. This will produce edge attributes $e_{ij}(t) =$\n$(c_i c_j, \\|x_i(t) - x_j(t)\\|^2)$ containing charge and distance in-\nformation for neighboring nodes. Since each graph is com-\nplete, there are $N \\times (N - 1)$ such edge attributes, which we\nstore in the tensor $\\mathcal{E}(t) \\in R^{N \\times (N-1) \\times 2}$.\nThen we apply equivariant temporal attention in the form\nof ETAL to the spatial representations $(\\theta_{[1:L]} \\in R^{L \\times N \\times d}$,\n$\\xi_{[1:L]} \\in R^{L \\times N \\times n}$, and $\\omega_{[1:L]} \\in R^{L \\times N \\times n}$. Feed-forward\nnetworks $f_o : R^{L \\times N \\times d} \\rightarrow R^{L \\times N \\times d}$, $f_a : R^{L \\times N \\times N} \\rightarrow\nR^{L \\times N \\times N}$ with layer pre-normalization, defined in Appendix\nD, and residual connection are also applied to the respective\nfeature and edge components of the graph. The sinusoidal\npositional encodings $\\mathcal{W}_{[1:L]} \\in R^{L \\times N \\times d}$, $\\mathcal{X}_{[1:L]} \\in R^{L \\times n \\times n}$,\n$\\mathcal{Y}_{[1:L]} \\in R^{L \\times N \\times n}$, $\\mathcal{Z}_{[1:L]} \\in R^{L \\times N^2}$ for the features, po-\nsitions, velocities, and adjacency matrices are defined in Ap-\npendix D."}, {"title": "4 Related Work", "content": "Temporal graph learning has a plethora of real-world appli-\ncations, like COVID-19 contact tracing (Chang et al. 2021)\n(Holme 2016) (Ding et al. 2021) and misinformation detec-\ntion (Choi et al. 2021) (Song, Teng, and Wu 2021) (Zhang,\nCook, and Yilmaz 2021).\nLearning on continuous-time dynamic graphs was intro-\nduced by (Rossi et al. 2020), which proposed Temporal Graph\nNetworks (TGNs) with a memory module, acting as a sum-\nmary of what the model has seen so far for each node. Causal\nAnonymous Walks (Wang et al. 2022) is another branch of\ntemporal graph learning, which extracts random walks be-\ntween edges; however, this is not our focus. Other work like\n(Jin et al. 2023) and (Cini et al. 2023) treat node features\nas time series and edges as correlations between the series.\nUnder this framework, message passing must be able to han-\ndle sequences of data from the neighborhood of each node,\nwith RNNs (Seo et al. 2016), attention mechanisms (Marisca,\nCini, and Alippi 2022), and convolutions (Wu et al. 2019).\nThe Dynamic Graph Convolutional Network (DynGCN)\n(Choi et al. 2021) and DyGFormer (Yu et al. 2023) are most\nsimilar to our method. DynGCN processes each of the graph\nsnapshots with a graph convolutional network to obtain struc-\ntural information and then applies an attention mechanism\nto capture temporal information. Similarly, DyGFormer (Yu\net al. 2023) learns from historical first-hop neighborhood in-\nteractions and applies a Transformer architecture to historical"}, {"title": "5 Experiments & Results", "content": "Adapting the charged N-body system dataset from (Sator-\nras, Hoogeboom, and Welling 2022), we sample 16,000 tra-\njectories for training, 2,000 trajectories for validation, and\n2,000 trajectories for testing. Each trajectory has a horizon\nlength of H = 10,000 and a sequence length of L = 10.\nThe point cloud consists of N = 5 particles, where at each\ntime step, positions (x1(t),... X5(t)) \u2208 R^{5\u00d73}, velocities\n(V1(t),...V5(t)) \u2208 R^{5\u00d73}, as well as charges C1, ..., C5 \u2208\n{-1, +1} are known. The edges between charged particles\nis eij (t) = (CiCj, \\|xi(t) \u2212 xj (t)\\|^2). We input these known\nvalues into SET with features chosen as hi(t) = \\|vi(t)\\|^2 for\ni = 1,..., 5. We conducted a hyper-parameter optimization"}, {"title": "5.1 Ablation Study: Equivariance, Adjacency, and Attention", "content": "Furthermore, we conduct an ablation study on SET, shown in\nTable 1, which compares the use of equivariance, temporal\nattention for the adjacency matrix as per Section 3.3, spa-\ntial attention, and temporal attention. By selecting the best\nmodel on the validation set, we find that incorporating equiv-\nariance, spatial attention, and temporal attention enhances\nperformance, whereas using adjacency diminishes it. We hy-\npothesize that the insignificance of temporal adjacency is\ndue to the fact the edge attribute contains information about\ncharges, which does not evolve in time, and information about\nthe distance between particles, which already implicitly exists\nin the coordinate information."}, {"title": "5.2 Baselines & Scaling N", "content": "We compare our best performing SET model with optimized\nLSTM, EGNN, MLP and Linear baselines (see Appendix E\nfor implementation details). SET outperforms all baselines\nfor N = 5, as seen in Table 2.\nSince the N = 5 system is seemingly too simple a task,\nwe scale the dataset to N = 20 and N = 30. As shown\nin Figure 2, test MSE remains consistent for all models re-\ngardless of the number of charged particles N, which is a\ndesirable property. Per Figure 2, the number of model pa-\nrameters remains constant for the EGNN, MLP and Linear\nbaselines. Fortunately, the number of parameters in SET also\nremains constant for all N; this is an artifact of the attention\nlayers only being functions of the feature and coordinate"}, {"title": "5.3 Noisy Observations", "content": "We also conduct an experiment in which we introduce noise\nin the observations, by adding Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,0.5)$\nto positions and velocities. As seen in Table 3, all models\nhave test MSEs that plateau around the variance $\\sigma^2 = 0.5$ of\nthe irreducible noise, with the SET and MLP baseline tying\nin performance."}, {"title": "6 Conclusions", "content": "The imposition of group symmetries on graph neural net-\nworks is a promising area of research, demonstrating remark-\nable real-world results like AlphaFold (Jumper et al. 2021).\nHowever, most research has been centered on spatial equiv-\nariance for representational learning on static graphs. For\ndynamical graph systems, little research has centered on pre-\nserving group symmetries across time. We close this gap\nwith the Spacetime E(n)-Transformer and show promising\nresults for the charged N-body problem. It will be interesting\nto see our method applied to harder tasks, such as sequential\nbio-molecular generation.\nAlthough we chose a graph as the domain of inter-\nest, it is plausible to extend notions of spatio-temporal G-\nequivariance to other domains like grids and manifolds. Fur-\nthermore, while we leveraged the symmetries of the problem\na priori, it may not always be possible to find a simple group\nfor a general problem. Hence, in future work, it would be\ninteresting to learn a group symmetry from underlying data\nand impose equivariance using methods like LieConv (Finzi\net al. 2020), which is equivariant to the actions of Lie groups,\ni.e. the continuous group representation of infinitesimal trans-\nformations. Noether's first theorem implies a possible con-\nnection to conserved quantities, which was discussed in (Alet\net al. 2021)."}, {"title": "A Preliminaries", "content": "A.1 Isometries\nDefinition A.1. Let (X, dx) and (Y, dy) be metric spaces with metrics $d_x : X \u00d7 X \u2192 R$, $d_y : Y \u00d7 Y \u2192 R$. An isometry\n\u03c6 : X \u2192 Y is a distance-preserving isomorphism if\n$d_x(a,b) = d_y (\u03c6(\u03b1), \u03c6(b))$\nfor all a, b \u2208 X.\nA.2 Invariance & Equivariance\nDefinition A.2. Let pv : G \u2192 GL(K, V) be a representation of group G, and let pv (g) : V \u2192 V be an automorphism on V\nfor g\u2208 G. A function f : V \u2192 W is G-equivariant if there exists an equivalent representation pw : G \u2192 GL(K, W) with\nequivalent automorphism pw (g) : W \u2192 W, such that\n$f(p_v (g)(v)) = p_w(g)(f(v))$\nfor all v \u2208 V and g \u2208 G.\nDefinition A.3. Let p : G \u2192 GL(K, V) be a representation of group G and let p(g) : V \u2192 V be an automorphism for g \u2208 G.\nA function $f: V \u2192 W$ is G-invariant if\n$f(p(g)(v)) = f(v)$\nfor all v \u2208 V and g \u2208 G.\nA.3 Special Property of SO(n)\nSince rotations are distance, angle, and orientation preserving, they are linear transformations. As such, rotations can be\nrepresented as a matrix. Suppose x, y \u2208 Rn and Q \u2208 Rn\u00d7n is a rotation matrix. Then if Q is an isometry, we require:\n$x^T y = (Qx)^T(Qy)$\n$= x^T (Q^TQ)y$\nor $Q^TQ = I = Q^TQ$. Preservation of orientation (equivalently, handedness) means det Q > 0. We take the determinant of the\nidentity and find det($Q^TQ$) = (det(Q))\u00b2 = 1, which means det Q = \u00b11. Hence, det Q = +1 for SO(n).\nB Proof of Equivariances for Temporal Attention Layer\nB.1 Position Component: E(n)-Equivariance\nProof. Consider an isometry of E(n) defined by $\\xi \u2192 Ag + b$ for A \u2208 Rn\u00d7n an orthogonal rotation matrix and b \u2208 Rn a\ntranslation vector. Then\n$\\|\\xi_i(t) \u2013 \\xi_i(s)\\|^2 = \\|A\\xi_i(t) + b \u2212 A\\xi_i(s) \u2013 b\\|^2$\n= $\\|A(\\xi_i(t) \u2013 \\xi_i(s))\\|^2$\n= $(\\xi_i(t) \u2013 \\xi_i(s))^T A^TA(\\xi_i(t) \u2212 \\xi_i(s))$\n= $\\|\\xi_i(t) \u2013 \\xi_i(s)\\|^2,$\nwhere the last line follows by the orthogonality of A. Hence, $\\beta_i(t,s) = \\exp(\\|\\xi_i(t) \u2013 \\xi_i(s)\\|^2)/\\sum_{s'=1}^{L} \\exp(\\|\\varepsilon_i(t) \u2013 \\xi_i(s')\\|^2)$\nis invariant under isometries of Rn. Thus, applying the isometry to the layer\n$\\xi_i(t) = \\xi(t) + B \\sum_{s=1 \\atop s \\neq t}^{L} \\beta_i(t,s) ((\\xi_i(s) - \\xi_i(t))$\nyields:\n$A\\xi_i(t) + b + B \\sum_{s=1 \\atop s \\neq t}^{L} \\beta_i(t,s) A(\\xi_i(s) \u2013 \\xi_i(t))$\n= $A(\\xi_i(t) + B \\sum_{s=1 \\atop s \\neq t}^{L} \\beta_i(t,s) ((\\xi_i(s) \u2013 \\xi_i(t)) + b$\n= $A\\xi_i(t) + b$.\nIt follows that the position component of ETAL is E(n)-equivariant."}, {"title": "B.2 Velocity Component: SO(n)-Equivariance", "content": "Proof. Consider an orthogonal rotation matrix A \u2208 Rn\u00d7n of SO(n) with action \u03c9 \u2192 \u0391\u03c9. Clearly,\n$(A\\omega_i(t))^T(A\\omega_i(s)) = \\omega_i(t)^T A^TA\\omega_i(s)$\n= $\\omega_i(t)^T\\omega_i(s)$.\nThus, the coefficient $\\gamma_i(t,s) = \\omega_i(t)^T \\omega_i(s)/\\sum_{s'=1}^{L} \\exp(\\omega_i(t)^T \\omega_i(s'))$ is invariant under rotations of SO(n).\nApplying the rotation to the attention layer\n$\\omega_i(t) = \\sum_{s=1}^{L} \\gamma_i(t,s)\\omega_i(s)$\nyields:\n$\\sum_{s=1}^{L} \\gamma_i(t,s) A\\omega_i(s) = A\\sum_{s=1}^{L} \\gamma_i(t,s)\\omega_i(8)$\n= $A\\tilde{\\omega}(t)$,\nas asserted. Hence, the velocity component of ETAL is SO(n)-equivariant."}, {"title": "C Tensorization of Equivariant Temporal Attention Layer (ETAL)", "content": "C.1 Temporal Attention Feature Component\nDefine the matrix of spatial graph representations for features across time:\n$\\Theta_{[1:L", "theta(1)": "theta(L) \\end{bmatrix"}, "in R^{L \\times d}$\nWe compute the value vectors $\\upsilon_{[1:L"]}, {"1": "L]"}, {"k_{[1": "L]"}, {"1": "L]"}, {"q_{[1": "L]"}, {"1": "L]"}, {"weights": "n$\\alpha_i = softmax(\\frac{q_{[1:L]"}, {"1": "L]"}, {"follows": "n$\\Theta^{[1:L]"}, {"1": "L]"}, {"Theta^{[1": "L]"}, {"1": "L]"}, {"1": "L]"}, {"k^{[1": "L]"}, {"1": "L]"}, {"q^{[1": "L]"}, {"1": "L]"}, {"v^{[1": "L]"}, {"1": "L]"}, {"softmax(\\frac{q^{[1": "L]"}, {"1": "L]"}, {"k^{[1": "L]}^T \\in R^{N \\times d \\times L}$ interchanges the last two dimensions, the"}]