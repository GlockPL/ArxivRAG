{"title": "Spacetime E(n)-Transformer: Equivariant Attention for Spatio-temporal Graphs", "authors": ["Sergio G. Charles"], "abstract": "We introduce an E(n)-equivariant Transformer architecture\nfor spatio-temporal graph data. By imposing rotation, trans-\nlation, and permutation equivariance inductive biases in\nboth space and time, we show that the Spacetime E(n)-\nTransformer (SET) outperforms purely spatial and temporal\nmodels without symmetry-preserving properties. We bench-\nmark SET against said models on the charged N-body prob-\nlem, a simple physical system with complex dynamics. While\nexisting spatio-temporal graph neural networks focus on se-\nquential modeling, we empirically demonstrate that leveraging\nunderlying domain symmetries yields considerable improve-\nments for modeling dynamical systems on graphs.", "sections": [{"title": "1 Introduction", "content": "Many problems that we wish to model with neural networks\npossess underlying geometric structure with symmetries. Ge-\nometric Deep Learning, a term coined in the seminal work of\n(Bronstein et al. 2021), is an Erlangen program for deep learn-\ning that systematizes inductive biases as group symmetries\nG, arising through notions of invariance and equivariance.\nRecent work, like SE(3)-Transformers (Fuchs et al. 2020)\nand E(n)-Graph Neural Networks (Satorras, Hoogeboom,\nand Welling 2022), impose different notions of group equiv-\nariance on neural networks to inform architecture choice.\nWhile these neural network architectures encode spatial in-\nductive biases, they notably lack a time component. Temporal\nGraph Networks (Rossi et al. 2020) proposed an efficient\nframework that learns from dynamic graphs. However, this\narchitecture assumes the topology of graphs changes over\ntime. In this paper, we discuss spatio-temporal graphs, which\nhave a fixed topology with changing features over discrete\ntime steps. Recent works (Jin et al. 2023), (Marisca, Cini, and\nAlippi 2022), (Cini et al. 2023) have treated node features as\ntime series and edges as the relationships between these se-\nries. Such spatio-temporal graph neural networks (STGNNs)\nhave a plethora of applications, from simulating biomolecular\ninteractions to modeling financial time series.\nSimilarly, while STGNNs improve representation learning\nof sequential graph data, to the best of our knowledge, mini-\nmal research has been done on preserving group symmetries\nin a spatio-temporal fashion. In particular, sequential models\nought to preserve spatial group symmetries at each time step.\nFamously, Noether's first theorem formalizes the notion of\ninfinitesimal symmetries of the so-called Lagrangian of a\nphysical system, in terms of perturbations with respect to\nboth space and time, by determining conserved quantities. In-\nspired by this intuition of temporal and spatial symmetry, we\nseek to derive a neural network architecture that is equivariant\nin both temporal and spatial components.\nClassical neural network architectures like RNNs are dis-\ncrete approximations to continuous time-domain signals,\nobeying a differential equation with respect to time. If an\nRNN is invariant to time-warping, a monotonically increas-\ning and differentiable function of time, it takes the form of an\nLSTM (Bronstein et al. 2021), which unlike a vanilla RNN,\ncaptures long-term dependencies. Similarly, the dynamics of\nclassical physical systems satisfy the Euler-Lagrange equa-\ntions, i.e. the equations of motion. Hence, we use the charged\nN-body problem, as described in (Satorras, Hoogeboom,\nand Welling 2022) and adapted from (Kipf et al. 2018), as\nan ideal candidate to test our hypothesis that preserving G-\nequivariance ameloriates long-term spatio-temporal graph\nmodeling. We will use a Transformer for the temporal compo-\nnent of the architecture, preserving long-term dependencies\nand, hence, invariance to time-warping. Each node, represent-\ning a charged particle of the graph, will have features, coor-\ndinates, and velocities. As such, the neural network should\nbe equivariant under rotational and translational symmetries\nE(n) acting on coordinates. It should also be equivariant with\nrespect to rotational symmetries SO(n) acting on velocities.\nLastly, the nodes should be permutation equivariant."}, {"title": "2 Background", "content": "Following the insights of Geometric Deep Learning (Bron-\nstein et al. 2021), the input signals to machine learning mod-\nels have an underlying domain \u03a9. Examples of such domains\ninclude grids, graphs, and manifolds. The space of signals\nover \u03a9 possesses a vector-space structure. That is (Bronstein\net al. 2021):\nDefinition 2.1. The space of C-valued signals on \u03a9 is\nX(\u03a9, C) = {x : \u03a9 \u2192 C},\nwhich is a vector space of functions."}, {"title": "2.1 Geometric Deep Learning", "content": "The symmetry of the domain \u03a9 will impose structure on\nthe signal space X(\u03a9), thus inducing structure on the space\nof interpolants\nF(X(\u03a9)) = {fo\u2208\u04e9}\nfor fo a neural network. In what follows, we canonically refer\nto X(\u03a9) as V for brevity."}, {"title": "2.2 Group Representations, Invariance, and\nEquivariance", "content": "Definition 2.2. A representation of a group G on a vec-\ntorspace V over a field K is a homomorphism\n\u03c1 : G \u2192 GL(K, V)\nsuch that p(gh) = p(g)p(h) for all g \u2208 G, h \u2208 G, where\nGL(K, V) is the general linear group of automorphisms 4 :\nV \u2192 V, i.e., the set of bijective linear transformations with\nfunction composition as its binary operation.\nIn this paper, we are interested in the group of rotational\nsymmetries SO(n) and the group of isometries E(n) of Rn,\nas these are the naturally-induced symmetries of particles. Ro-\ntations are distance, angle, and orientation preserving trans-\nformations. The group of rotations in n dimensions is\nSO(n) = {Q \u2208 Mn(R)|QQ = I and det Q = +1},\nwhere Mn(R) is the set of n x n matrices with entries in R,\nforming a ring. We represent a group element g\u2208 SO(n)\nwith p(g) \u2208 GL(R, R", "p(g)": "x \u2192\nQx where Q\u2208 Rn\u00d7n is an orthogonal matrix (see Appendix\nA for more details). We restrict the notion of equivariance\nto functions of Euclidean space, as will be the case for neu-\nral networks. In Appendix A, we provide a more general\ndefinition.\nDefinition 2.3. A function f: Rn \u2192 Rn is SO(n)-\nequivariant if\nQf(x) = f(Qx)\nfor all Q \u2208 Rn\u00d7n orthogonal and x \u2208 Rn.\nThe Euclidean group E(n) is the set of isometries of Eu-\ncliden space R", "\u03c6": "Rn \u2192 Rn|y isometry}.\nWe represent a group element g\u2208 E(n) with p(g) \u2208\nGL(R, R"}, {"p(g)": "x \u2192 Qx + b where\nQ\u2208 Rnxn is an orthogonal rotation matrix and b \u2208 Rn\nis a translation vector. That is, we represent actions of the\nEuclidean group as orthogonal transformations followed by\ntranslations. Again, we provide a definition of equivariance\nwith respect to functions of Euclidean space.\nDefinition 2.4. A function f : Rn \u2192 Rn is E(n)-\nequivariant if\nQf(x) + b = f(Qx+b)\nfor all Q\u2208 Rnxn orthogonal rotation matrices, b \u2208 Rn\ntranslation vectors, and for all x \u2208 Rn."}, {"title": "3 Method", "content": "In this paper, we are interested in physical systems that can\nbe modelled as a sequence of graphs Gt = (Vt, Et) for t = 1,..., L with nodes vi(t) \u2208 Vt and edges eij(t) \u2208 Et. In\nparticular, we seek to model the dynamics of the charged N-\nbody problem (Kipf et al. 2018). For this task, we assume a\npriori that the graph is complete since a charged particle will\ninteract with every other particle in a Van der Waals potential\nunder Coulomb's law. In addition, we assume that particles\nare neither created nor destroyed as the system evolves in\ntime, so the nodes Vt in the graph remain the same. Let\nG := (Gt)1<t<L be a sequence of topologically-identical\ngraphs with changing features, known as a spatio-temporal\ngraph. The task under consideration is learning a function\nthat predicts the associated features of graph. In particular,\ngiven G, we are interested in predicting the positions and\nvelocities of all particles in the system after H additional\ntime steps where H >> L.\nTo equip the spatio-temporal model of particle interactions\nwith the appropriate inductive biases, we leverage both spa-\ntial and temporal notions of attention. For node i at time\nstep t to attend to all the past neighborhoods of that node,\nwe need to (1) aggregate nodes spatially to obtain spatially-\ncontextual embeddings and (2) obtain temporally-contextual\nembeddings via temporal aggregation.\nWe fix a time slice t such that the features derive from\n(1)\nGt = (Vt,Et). From the current features h (t) of node\ni at layer l, we form the next layer features h h(1+1)(t) by\naggregating neighboring node features. In particular,\nh(1+1) (t)\n(1) = (h (t), (t), ah (t), h(t))\n(1)\nwhere is a permutation-invariant function (Bronstein\net al. 2021), and a is a self-attention mechanism, often a\nnormalized softmax across neighbors."}, {"title": "3.1 E(n)-Equivariant Spatial Attention", "content": "(Satorras, Hoogeboom, and Welling 2022) introduced E(n)-\nEquivariant Graph Neural Networks (EGNNs). Every node\nin the graph G = (V,E) has features hi \u2208 Rd and co-\nordinates xi \u2208 Rn. In addition, we keep track of each\nparticle's velocity vi \u2208 Rn. The Equivariant Graph Con-\nvolutional Layer (EGCL) takes the set of node embed-\ndings h(1)\n(1)\n(1)\n= {h,...,h}, coordinate embeddings\nx = {x,...,x}, velocity embeddings v\n= {v,..., v}, and edge information E = (eij) as in-\nput and produces the embeddings of the next layer. That\nis, h(1+1), x(1+1), v(1+1) = EGCL[h(1), x(1), v(1), E], defined\nas follows (Satorras, Hoogeboom, and Welling 2022):\nmij = de (h,, ||x \u2212 x ||2, aij)\nv1+1) = (h) + C(x) \u2013 x) (mij)\nx+1) = x+v\nmi = \u2211mij\nh1+1) = h(h), mi)\n(2)\nwhere aij are the edge attributes, e.g. the edge values eij,\nand de: R2d+2 \u2192 Rh, qr : Rd \u2192 R, Q\u2084 : Rh \u2192 R, and\n\u03a6h: Rd+h \u2192 Rd' are MLPs. In what follows, we assume\nd' = d for clarity.\n(Satorras, Hoogeboom, and Welling 2022) proved that this\nlayer is equivariant to rotations and translations on coordi-\nnates and equivariant to rotations on velocities:\nht+1), Qx+1) + b, Qv+1) = EGCL[h, Qx + b, QvE]\n(3)\nfor Q\u2208 Rnxn an orthogonal rotation matrix and b \u2208 R", "EG": "nh1+1) (t), x(1+1) (t), v1+1) (t) = EGCL[h) (t), x(t), v) (t), E(t)]\n(4)\nfor l = 1, . . ., Kt. Thus, we obtain spatially-contextual repre-\nsentations for node i at time t defined as 0i(t) = hkt) (t) \u2208\n(Kt)\nRd, Ei(t) = x(Kt) (t) \u2208 Rn, wi(t) = v(Kt) (t) \u2208 Rn for\nt = 1, ..., L."}, {"title": "3.2 Temporal Attention for Graphs", "content": "The objective of this section is to obtain strong temporally-\ncontextual representations of the spatial graph embeddings.\nIn the charged N-body problem, we are essentially solving\nthe forward-time Euler-Lagrange equations, a second-order\npartial differential equation. However, for a fixed node on\nthe spatio-temporal graph, the feature, position, and velocity\nform a time-series, for which RNN's capture short-term de-\npendencies. It was shown in (Tallec and Ollivier 2018) that\nwhile vanilla RNNs are not time-warping invariant, LSTMs\nare a class of such time-warping invariant functions modeling\na continuous time-domain signal. Employing this philoso-\nphy, the use of an attention-based Transformer architecture\nto model spatio-temporal graph data merits investigation."}, {"title": "3.3 E(n)-Equivariant Temporal Attention", "content": "We would like the temporal attention to retain the equivariant\nproperties described in Section 3.1. Namely, the Equivariant\nTemporal Attention Layer (ETAL) should be equivariant to\nthe actions of E(n) on coordinates and the actions of SO(n)\non velocities. It should also be permutation equivariant with\nrespect to the actions of the symmetric group \u2211N on nodes.\nAs we do not impose symmetry conditions on the layer\nwith respect to the feature representations h\u2081(t), we can apply\nkey-query-value self-attention as follows. Define the node-\nwise query qi(t) = Qi0i(t), key ki(t) = Ki0i(t), and value\nvi(t) = Vi0i(t) for Qi, Ki, Vi \u2208 Rd\u00d7d. To reduce memory\nusage, we share Q, K, V for all nodes. Then the temporally-\ncontextual representation is:\n0i(t) := \u2211ai(t,s)vi(s)\n(5)\nwhere\nexp(qi(t)Tki(s))\nai(t,s) =\n\u22111 exp(qi(t)Tki(s'))\n(6)\ns'=1\nSatorras et al. (Satorras, Hoogeboom, and Welling 2022)\nshowed that for a collection of points {i}1 \u2208 R, the\nnorm is a unique geometric identifier, such that collections\nseparated by actions of E(n) form an equivalence class. With\nthis in mind, since we desire the attention mechanism for\ncoordinates \u00a7\u00bf(t) to be equivariant with respect to E(n), we\ncan define:\n\u03be\u2081(t) := Ex(t) + \u0392 \u03a3\u03b2\u2081(t,s) ((s) \u2013 (t)),\n(7)\nwhere\nexp(||(t) \u2013 \u00a3(s)||2)\nBi(t,s) =\n\u22111 exp(||(t) \u2013 \u00a3(s')||2)\n(8)\nThis bears semblance to the neighborhood attention de-\nscribed in the SE(3)-Transformer network (Fuchs et al.\n2020) and Tensor Field Network layer (Thomas et al. 2018),\nthe intensity function in (Zhang, Cook, and Yilmaz 2021),\nand the invariant point attention in (Jumper et al. 2021).\nWe define an SO(n)-equivariant attention layer for veloci-\nties wi(t):\nwi(t) := \u2211vi(t,s)w(s)\n(9)\nwhere the weight is\nwi(t)Twi(s)\nYi(t,s) =\nexp(wi(t) Twi(s'))\n(10)\ns=1\nIn appendix B, we show that the position attention function\nis E(n)-equivariant and the velocity attention function is\nSO(n)-equivariant.\nFollowing the insights of (Jin et al. 2023), edges are rela-\ntionships between time series and they should evolve. Hence,\nwhile the adjacency matrix A \u2208 RN\u00d3N is constant in space\nwhen applying ECGL, it should intuitively evolve in time\nwhen applying ETAL. That is, if we consider edges as rep-\nresenting the interaction between charged particles, e.g. the\nstrength of the force, then this must necessarily evolve in\ntime for a non-stationary point cloud system."}, {"title": "3.4 Spacetime E(n)-Equivariant Graph\nTransformer", "content": "The full spatio-temporal attention module is presented in Al-\ngorithm 1. It takes as input the node features h \u2208 RL\u00d7N\u00d7d,\npositions x \u2208 RL\u00d7N\u00d7n, velocities v \u2208 RL\u00d7N\u00d7n and adja-\ncency matrices A \u2208 RLXNXN. For a spatio-temporal graph\nG = (Gt)1<t<L, we apply an equivariant spatial attention\nlayer in the form of EGCL to obtain spatially-contextual\nrepresentations 0(t) \u2208 RN\u00d7d,g(t) \u2208 R\u00d1\u00d7n,w(t) \u2208 RN\u00d7n\nfor t = 1,..., L. We share the same EGCL layer across all\ntime steps t = 1,..., L. That is, we only learn one set of\nMLPS \u03c6\u03b5, \u03c6\u03c5, \u03a6\u03b1, and \u03c6h for each layer across time, which\nis significantly more memory and parameter efficient.\nObserve, at each time step, we apply a transforma-\ntion E : RN\u00d7N \u00d7 RN\u00d7n \u2192 RN\u00d7(N-1)\u00d72 to the adja-\ncency matrix A(t) \u2208 RN\u00d7N and the coordinates x(t) \u2208\n]RN\u00d7n for Gt. This will produce edge attributes eij(t) =\n(CiCj, ||Xxi (t) - xj (t)||2) containing charge and distance in-\nformation for neighboring nodes. Since each graph is com-\nplete, there are N \u00d7 (N \u2212 1) such edge attributes, which we\nstore in the tensor \u0190(t) \u2208 RN\u00d7(N\u22121)\u00d72.\nThen we apply equivariant temporal attention in the form\nof ETAL to the spatial representations (0[1:L] \u2208 RL\u00d7N\u00d7d,\n[1:L] \u2208 RLxNxn, and w[1:L] \u2208 RL\u00d7N\u00d7n. Feed-forward\nnetworks fo : RL\u00d7N\u00d7d \u2192 RL\u00d7N\u00d7d, fa : RL\u00d7N\u00d7N \u2192\nIRL\u00d7N\u00d7N with layer pre-normalization, defined in Appendix\nD, and residual connection are also applied to the respective\nfeature and edge components of the graph. The sinusoidal\npositional encodings W[1:L] \u2208 []RL\u00d7N\u00d7d, X[1:L] \u2208 [][RL\u00d7n\u00d7n,\nY[1:L] \u2208 []RL\u00d7N\u00d7n, Z[1:L] \u2208 []RL\u00d7N\u00d7N for the features, po-\nsitions, velocities, and adjacency matrices are defined in Ap-\npendix D."}, {"title": "4 Related Work", "content": "Temporal graph learning has a plethora of real-world appli-\ncations, like COVID-19 contact tracing (Chang et al. 2021)\n(Holme 2016) (Ding et al. 2021) and misinformation detec-\ntion (Choi et al. 2021) (Song, Teng, and Wu 2021) (Zhang,\nCook, and Yilmaz 2021).\nLearning on continuous-time dynamic graphs was intro-\nduced by (Rossi et al. 2020), which proposed Temporal Graph\nNetworks (TGNs) with a memory module, acting as a sum-\nmary of what the model has seen so far for each node. Causal\nAnonymous Walks (Wang et al. 2022) is another branch of\ntemporal graph learning, which extracts random walks be-\ntween edges; however, this is not our focus. Other work like\n(Jin et al. 2023) and (Cini et al. 2023) treat node features\nas time series and edges as correlations between the series.\nUnder this framework, message passing must be able to han-\ndle sequences of data from the neighborhood of each node,\nwith RNNs (Seo et al. 2016), attention mechanisms (Marisca,\nCini, and Alippi 2022), and convolutions (Wu et al. 2019).\nThe Dynamic Graph Convolutional Network (DynGCN)\n(Choi et al. 2021) and DyGFormer (Yu et al. 2023) are most\nsimilar to our method. DynGCN processes each of the graph\nsnapshots with a graph convolutional network to obtain struc-\ntural information and then applies an attention mechanism\nto capture temporal information. Similarly, DyGFormer (Yu\net al. 2023) learns from historical first-hop neighborhood in-\nteractions and applies a Transformer architecture to historical\ncorrelations between nodes. However, unlike our paper, Dyn-\nGCN and DyGFormer do not take into account the inductive\nbiases of the underlying modeling task.\nE(n)-Equivariant Graph Neural Networks (EGNN) (Sator-\nras, Hoogeboom, and Welling 2022) defines a model equiv-\nariant to the Euclidean group E(n) and, unlike previous\nmethods, does not rely on spherical harmonics such as the\nSE(3)-Transformer (Fuchs et al. 2020) and Tensor Field\nNetworks (Thomas et al. 2018). The SE(3)-Transformer\npaper briefly alludes to incorporating equivariant attention\nwith an LSTM for temporal causality; however, this is not\nthe primary focus of their work. LieConv (Finzi et al. 2020)\nproposes a framework that allows one to construct a convolu-\ntional layer that is equivariant with respect to transformations\nof a Lie group, equipped with an exponential map. However,\nthe EGNN is simpler and more applicable to problems with\npoint clouds like the charged N-body problem (Kipf et al.\n2018) we consider.\nAs we concern ourselves with modeling a dynamical sys-\ntem, the works of Lagrangian Neural Networks (LNNs)\n(Cranmer et al. 2020) and Hamiltonian Neural Networks\n(HNNs) (Greydanus, Dzamba, and Yosinski 2019) are perti-\nnent. HNNs parameterize the Hamiltonian of a system, but\nrequire canonical coordinates, which makes it inapplicable\nto systems where such coordinates cannot be deduced. LLNS\nparameterize arbitrary Lagrangians of dynamical systems\nwith neural networks, from which it is possible to solve the\nforward dynamics of the system; however, this requires an\nadditional step of integration, which is cumbersome."}, {"title": "5 Experiments & Results", "content": "Adapting the charged N-body system dataset from (Sator-\nras, Hoogeboom, and Welling 2022), we sample 16,000 tra-\njectories for training, 2,000 trajectories for validation, and\n2,000 trajectories for testing. Each trajectory has a horizon\nlength of H = 10,000 and a sequence length of L = 10.\nThe point cloud consists of N = 5 particles, where at each\ntime step, positions (x1(t),... X5(t)) \u2208 R5\u00d73, velocities\n(V1(t),...V5(t)) \u2208 R5\u00d73, as well as charges C1, ..., C5 \u2208\n{-1, +1} are known. The edges between charged particles\nis eij (t) = (CiCj, ||xi(t) \u2212 xj (t)||2). We input these known\nvalues into SET with features chosen as hi(t) = ||vi(t)||2 for\ni = 1,..., 5. We conducted a hyper-parameter optimization"}, {"title": "5.1 Ablation Study: Equivariance, Adjacency, and\nAttention", "content": "Furthermore, we conduct an ablation study on SET, shown in\nTable 1, which compares the use of equivariance, temporal\nattention for the adjacency matrix as per Section 3.3, spa-\ntial attention, and temporal attention. By selecting the best\nmodel on the validation set, we find that incorporating equiv-\nariance, spatial attention, and temporal attention enhances\nperformance, whereas using adjacency diminishes it. We hy-\npothesize that the insignificance of temporal adjacency is\ndue to the fact the edge attribute contains information about\ncharges, which does not evolve in time, and information about\nthe distance between particles, which already implicitly exists\nin the coordinate information."}, {"title": "5.2 Baselines & Scaling N", "content": "We compare our best performing SET model with optimized\nLSTM, EGNN, MLP and Linear baselines (see Appendix E\nfor implementation details). SET outperforms all baselines\nfor N = 5, as seen in Table 2.\nSince the N = 5 system is seemingly too simple a task,\nwe scale the dataset to N = 20 and N = 30. As shown\nin Figure 2, test MSE remains consistent for all models re-\ngardless of the number of charged particles N, which is a\ndesirable property. Per Figure 2, the number of model pa-\nrameters remains constant for the EGNN, MLP and Linear\nbaselines. Fortunately, the number of parameters in SET also\nremains constant for all N; this is an artifact of the attention\nlayers only being functions of the feature and coordinate\ndimensions."}, {"title": "5.3 Noisy Observations", "content": "We also conduct an experiment in which we introduce noise\nin the observations, by adding Gaussian noise \u025b ~ N(0,0.5)\nto positions and velocities. As seen in Table 3, all models\nhave test MSEs that plateau around the variance \u03c3\u00b2 = 0.5 of\nthe irreducible noise, with the SET and MLP baseline tying\nin performance."}, {"title": "6 Conclusions", "content": "The imposition of group symmetries on graph neural net-\nworks is a promising area of research, demonstrating remark-\nable real-world results like AlphaFold (Jumper et al. 2021).\nHowever, most research has been centered on spatial equiv-\nariance for representational learning on static graphs. For\ndynamical graph systems, little research has centered on pre-\nserving group symmetries across time. We close this gap\nwith the Spacetime E(n)-Transformer and show promising\nresults for the charged N-body problem. It will be interesting\nto see our method applied to harder tasks, such as sequential\nbio-molecular generation.\nAlthough we chose a graph as the domain of inter-\nest, it is plausible to extend notions of spatio-temporal G-\nequivariance to other domains like grids and manifolds. Fur-\nthermore, while we leveraged the symmetries of the problem\na priori, it may not always be possible to find a simple group\nfor a general problem. Hence, in future work, it would be\ninteresting to learn a group symmetry from underlying data\nand impose equivariance using methods like LieConv (Finzi\net al. 2020), which is equivariant to the actions of Lie groups,\ni.e. the continuous group representation of infinitesimal trans-\nformations. Noether's first theorem implies a possible con-\nnection to conserved quantities, which was discussed in (Alet\net al. 2021)."}, {"title": "A Preliminaries", "content": "A.1 Isometries\nDefinition A.1. Let (X, dx) and (Y, dy) be metric spaces with metrics dx : X \u00d7 X \u2192 R, dy : Y \u00d7 Y \u2192 R. An isometry\n6 : X \u2192 Y is a distance-preserving isomorphism if\ndx(a,b) = dy (\u03c6(\u03b1), \u03c6(b))\n(14)\nfor all a, b \u2208 X.\nA.2 Invariance & Equivariance\nDefinition A.2. Let pv : G \u2192 GL(K, V) be a representation of group G, and let pv (g) : V \u2192 V be an automorphism on V\nfor g\u2208 G. A function f : V \u2192 W is G-equivariant if there exists an equivalent representation pw : G \u2192 GL(K, W) with\nequivalent automorphism pw (g) : W \u2192 W, such that\nf(pv (g)(v)) = pw(g)(f(v))\n(15)\nfor all v \u2208 V and g \u2208 G.\nDefinition A.3. Let p : G \u2192 GL(K, V) be a representation of group G and let p(g) : V\nV be an automorphism for g \u2208 G.\nA function f: V \u2192 W is G-invariant if\nf(p(g)(v)) = f(v)\n(16)\nfor all v \u2208 V and g \u2208 G.\nA.3 Special Property of SO(n)\nSince rotations are distance, angle, and orientation preserving, they are linear transformations. As such, rotations can be\nrepresented as a matrix. Suppose x, y \u2208 Rn and Q \u2208 Rn\u00d7n is a rotation matrix. Then if Q is an isometry, we require:\nx+y = (Qx)(Qy)\n= x (QQ)y\n(17)\nor QTQ = I = QTQ. Preservation of orientation (equivalently, handedness) means det Q > 0. We take the determinant of the\nidentity and find det(QTQ) = (det(Q))\u00b2 = 1, which means det Q = \u00b11. Hence, det Q = +1 for SO(n)."}, {"title": "B Proof of Equivariances for Temporal Attention Layer", "content": "B.1 Position Component: E(n)-Equivariance\nProof. Consider an isometry of E(n) defined by \u00a7 \u2192 Ag + b for A \u2208 Rn\u00d7n an orthogonal rotation matrix and b \u2208 Rn a\ntranslation vector. Then\n||\u00c8\u00bf(t) \u2013 \u00a3\u00bf(s)||\u00b2 = ||A\u00a3\u00bf(t) + b \u2212 A\u03be\u2081(s) \u2013 b||2\n- ||A(\u00a7\u00bf(t) \u2013 \u00a3\u2081(s))||2\n= (\u00a7\u00bf(t) \u2013 \u00a3(s))TATA(\u00a7\u00bf(t) \u2212 \u00a7\u00bf(s))\n= ||\u2081(t) \u2013 \u00a3\u00bf(s)||2,\n(18)\nwhere the last line follows by the orthogonality of A. Hence, \u1e9e\u2081(t,s) = exp(||\u00a3\u00bf(t) \u2013 \u00a3\u00bf(s)||\u00b2)/\u2211s'=1 exp(||\u1f11\u2081(t) \u2013 \u00a3\u2081(s')||2)\nis invariant under isometries of Rn. Thus, applying the isometry to the layer\n\u03be\u2081(t) = E(t) + \u0392 \u03a3\u03b2i(t,s) (E\u2081(s) \u2013 E\u2081(t))\n(19)\nyields:\n\u0391\u03be\u2081(t) + b + \u0392 \u03a3\u03b2i(t,s) A(i(s) \u2013 E\u2081(t))\ns=1\ns\u2260t\n= A (t) + \u0392\u03a3\u03b2i(t,s) (i(s) \u2013 Ei (t)) +b\ns=1\ns\u2260t\n= A\u03be\u2081(t) + b.\n(20)\nIt follows that the position component of ETAL is E(n)-equivariant."}, {"title": "B.2 Velocity Component: SO(n)-Equivariance", "content": "Proof. Consider an orthogonal rotation matrix A \u2208 Rn\u00d7n of SO(n) with action w \u2192 \u0391\u03c9. Clearly,\nT\n(Awi(t))(Awi(s)) = wi(t)ATAwi(s)\n= wi(t)wi(s).\n(21)\nThus, the coefficient vi(t,s) = exp(wi(t)Twi(s))/\u2211s'=1 exp(wi(t) Twi(s')) is invariant under rotations of SO(n).\nApplying the rotation to the attention layer\nwi(t) = \u2211 Vi(t,s)wi(s)\n(22)\n\u03a3\u03b3\u03b5(t,s) Aw(s) = \u0391\u03a3 Vi(t,s)wi(8)\ns=1\ns=1\n= \u0391\u1ff6(t),\n(23)\nas asserted. Hence, the velocity component of ETAL is SO(n)-equivariant."}, {"title": "C Tensorization of Equivariant Temporal Attention Layer (ETAL)", "content": "C.1 Temporal Attention Feature Component\nDefine the matrix of spatial graph representations for features across time:\n0(1)", "n01": "L", "n": "n0(L)\n\u2208RLxd\n(24)\nWe compute the value vectors v\n[1:L", "0[1": "L", "k\n[1": "L"}, {"0[1": "L", "q\n[1": "L", "n01": "L", "weights": "n\u03b1 =\nsoftmax\n(\nQKTO 01:L", "n[1": "L", "follows": "ni\n[1:L", "1": "L"}]}