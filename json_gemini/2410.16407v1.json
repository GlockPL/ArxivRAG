{"title": "Enhancing Multimodal Affective Analysis with Learned Live Comment Features", "authors": ["Zhaoyuan Deng", "Amith Ananthram", "Kathleen McKeown"], "abstract": "Live comments, also known as Danmaku, are user-generated messages that are synchronized with video content. These comments overlay directly onto streaming videos, capturing viewer emotions and reactions in real-time. While prior work has leveraged live comments in affective analysis, its use has been limited due to the relative rarity of live comments across different video platforms. To address this, we first construct the Live Comment for Affective Analysis (LCAffect) dataset which contains live comments for English and Chinese videos spanning diverse genres that elicit a wide spectrum of emotions. Then, using this dataset, we use contrastive learning to train a video encoder to produce synthetic live comment features for enhanced multimodal affective content analysis. Through comprehensive experimentation on a wide range of affective analysis tasks (sentiment, emotion recognition, and sarcasm detection) in both English and Chinese, we demonstrate that these synthetic live comment features significantly improve performance over state-of-the-art methods. We will release our dataset, code, and models upon publication.", "sections": [{"title": "Introduction", "content": "Live comments, or Danmaku, are a subtitle system originating from Japan and popularized in China. This system overlays real-time user-generated messages directly onto online video streams. These comments, synchronized with a video's timeline, offer a rich layer of viewer interaction, providing insights into user emotions, user reactions, and the video's broader context. The richness of these comments makes them an ideal resource for enhancing multimodal affective analysis, which seeks to interpret and categorize the emotional content and dynamics of videos. Although prior research, such as the study by Niu et al. (2018), has demonstrated how live comments can improve performance in affective analysis tasks like emotion prediction, their broader application remains constrained. Many video platforms do not support live comments, rendering these insights impossible for videos on those platforms. Consequently, the rarity of live comments poses significant challenges in fully exploiting their analytical potential.\nIn this work, we bridge this gap by proposing a novel method for producing synthetic live comment features to improve the performance of multimodal affective analysis. We demonstrate the effectiveness of these features in English and Chinese across several video understanding tasks: sentiment analysis, emotion recognition, and sarcasm detection.\nOne major challenge in affective learning from live comments stems from the nature of existing large-scale live comment datasets which are primarily tailored to the live comment generation task. These datasets typically feature a large percentage of popular content genres such as video games, sports, and music videos. However, these categories typically lack the rich emotional context required for detailed affective analysis. This mismatch complicates the training of models capable of accurately understanding and interpreting nuanced emotions. Additionally, the majority of these datasets are collected from Bilibili, which features amateur-created videos of everyday life. These videos often provoke a more limited and homogeneous range of emotional responses (Rosenbusch, Evans, and Zeelenberg 2019), especially when compared to the diverse and intense emotions elicited by content like dramas or documentaries. Moreover, most datasets comprise exclusively Chinese content, which further restricts their multilingual applicability.\nA second significant challenge is the dependency of existing methods on the availability of live comments. In the real-world, the presence of live comments can be limited and not uniformly distributed across all video genres. This reliance restricts the practical application of these methods, as they cannot work in scenarios where comments are non-existent.\nTo address these challenges, we first construct a large-scale, diverse dataset comprising everyday life videos in Chinese as well as bilingual TV shows, movies, and documentaries, enabling our system to learn a broad spectrum of emotional experiences across various contexts. Then, utilizing our dataset, we develop a multimodal encoder that effectively learns from our collected videos and comments by contrasting different contexts. This encoder is capable of producing synthetic live comment features from videos, thereby enabling the inference of emotional context across modalities, even in the absence of live comments.\nOur experimental analysis demonstrates that adding our live comment features improves over state-of-the-art (SOTA) approaches that leverage text, acoustic, and visual modalities directly. Our system achieves new SOTA performance on multiple multimodal datasets, particularly enhancing accuracy in detecting sentiment, emotions, and sarcasm. This not only underscores the utility of live comments in affective video analysis but also opens new avenues for multimodal learning applications.\nOur main contributions are:\n\u2022 1) Live Comment for Affective Analysis dataset (LCAffect), an expansive corpus of over 11 million live comments. It includes bilingual video content (Chinese and English) spanning a diverse range of topics and genres, curated to capture a wide spectrum of emotions.\n\u2022 2) A contrastive encoder that learns to project videos into live comment representation space, allowing the inference of synthetic live comment features for any video.\n\u2022 3) A downstream multimodal fusion model for affective analysis tasks which utilizes not only three modalities but also our synthetic live comment feature.\n\u2022 4) New SOTA across three affective analysis tasks, including a 3.18-point increase in accuracy on CH-SIMS v2 (sentiment analysis), a 2.89-point increase in F1 on MELD (emotion recognition), and a 3.0-point increase in F1 on MuSTARD (sarcasm detection)."}, {"title": "Related Work", "content": "Learning from Comments Previous research has highlighted the value of user comments in improving performance across various multimodal tasks. Fu et al. (2017) show that live comments on Twitch help predict highlights in e-sport games. Similarly, Yang, Ai, and Hirschberg (2019) show that live comments help identify humor in online videos. Additionally, Niu et al. (2018) show that features derived from live comments improve affective content analysis. Moreover, Hanu et al. (2022) show that user comments can help learn more contextualized representations for image, video, and audio, thus improving video-text retrieval.\nThese studies depend on the availability of user comments which are not always present in the real-world. In contrast, our work seeks to develop a system that is pre-trained to align video segments with corresponding live comments. This approach aims to learn a multimodal representation space that effectively augments various downstream understanding tasks. Consequently, our system can be directly applied to videos lacking associated live comments, producing synthetic live comment features for such videos. This capability directly addresses the limitations of prior work, enhancing the analysis and interpretation of videos that would be excluded due to the absence of live comments.\nLive Comment Datasets Several datasets featuring live comments have been established in the literature. LiveBot (Ma et al. 2018) and VideoIC (Wang, Chen, and Jin 2020) are large-scale datasets drawn from Bilibili, a site which primarily features short, user-generated videos focused on everyday life. In contrast, Lalanne, Bournet, and Yu (2023) collected live-streamed video and comments from Twitch, a platform centered around video games. Finally, MovieLC (Chen et al. 2023) is a compilation of famous movies with accompanying comments from Tencent Video.\nOur work builds on these efforts by creating a new dataset that includes videos from a broader range of platforms. By integrating multiple video hosting platforms, our dataset includes both a variety of short, user-generated content and longer formats like TV shows, movies, and documentaries in Chinese and English. This expansive collection allows the study of many genres and enables the training of a multimodal encoder that can learn live comment features effectively for affective analysis from diverse videos.\nMultimodal Affective Analysis Previously, multimodal affective computing often relied on hand-crafted algorithms to perform initial feature extraction. Recently, however, there has been a shift towards using pre-trained modality encoders with end-to-end tuning to improve performance. Specifically, Yi et al. (2024) use CLIP (Radford et al. 2021)'s spatial encoder and TimeSformer (Bertasius, Wang, and Torresani 2021) to encode visual features, while Wu et al. (2024) employ pre-trained audio encoders such as Data2Vec (Baevski et al. 2022) and Hubert (Hsu et al. 2021) to process acoustic features. Although these studies demonstrate the efficacy of an end-to-end framework, they are limited to two modalities. Our work extends this approach by integrating all three modalities (text, acoustic, visual), achieving more robust video analysis. Another promising research direction in affective analysis involves the integration of external knowledge. Ghosal et al. (2020) employ features produced by a common sense knowledge model to enhance emotion recognition, while Hu et al. (2022) exploit the complementary knowledge underlying sentiment analysis and emotion recognition to build a knowledge-sharing framework. Our work parallels these efforts by leveraging external knowledge derived from live comments."}, {"title": "The Live Comment Dataset", "content": "Collection of Videos\nWe collect videos and their accompanying live comments from several popular Chinese video streaming websites: Bilibili, Tencent Video, iQIYI, and Youku. Our dataset includes both Chinese and English content and is partitioned into three distinct subsets based on content type:\nUser-Generated Content This subset includes 2,464 videos from Bilibili featuring naturally occurring conversations from everyday life. The initial set of 682 videos is available from the Linguistic Data Consortium (UPenn) and was manually inspected to ensure content quality and relevance. Each video includes interactions involving at least two people with observable emotions. The remaining 1,782 videos are sourced from Bilibili's recommendation API, which selects content similar to the manually inspected set.\nTV shows and documentaries We collect 443 episodes from 8 TV shows and documentaries. This subset includes content from multiple platforms, including dramas, sitcoms, and documentaries focused on social life. Their inclusion aims to provide the model with insights into scripted content that elicits a wide range of emotions in different contexts.\nMovies Sourced from Tencent Video, this subset features 59 popular comedy, drama, and action films. They feature the longest videos, enabling the analysis of viewer interactions over extended contexts through live comments.\nIn addition, we collect text transcripts for the videos. For user-generated content, transcripts are obtained using Bilibili's API, which uses automatic speech recognition. For movies and TV shows, we extract transcripts from hard-coded subtitles via optical character recognition (OCR). We extract both English and Chinese subtitles when available.\nDataset Statistics\nOur LCAffect dataset contains 11,356, 212 live comments aligned with 829 hours of video. Table 1 provides a detailed breakdown of the dataset, including subset-specific statistics. As shown in Table 2, our dataset establishes a new standard in live comment analysis by offering nearly twice the video duration found in the largest existing datasets, encompassing a broader variety of content types, and featuring over twice as many live comments as any comparable dataset."}, {"title": "Contrastive Pre-training", "content": "Although multiple studies have demonstrated the effectiveness of live comments in enhancing video understanding, the availability of live comments is largely limited outside of China and Japan. This restriction significantly limits the practical application of these studies. To overcome this challenge, we propose a video encoder pre-trained on our diverse live comment dataset. This model leverages a standard contrastive learning framework akin to CLIP (Radford et al. 2021), where positive examples consist of matching live comments and videos, and negative examples involve non-matching pairs. The aim is to produce a synthetic live comment feature from a video span that can be used to enhance performance in various affective analysis tasks."}, {"title": "Pre-training Model Architecture", "content": "We adopt a CLIP-style contrastive pre-training approach. During the training phase, we partition each video into segments $s$ of $o$ seconds. For each segment $s_i$, we gather the associated text transcript $t_i$, video frames $f_i$, and live comments $c_i$, and treat the segment as an individual training sample. For this stage, we use only the text and visual modalities. We create our Video-to-Live Comment (V2LC) encoder by employing a pre-trained text encoder to encode the video text transcript into embedding $S_T$ and a Video Vision Transformer (Arnab et al. 2021) to encode the video frames into embedding $S_F$. These encoded outputs are then integrated using a cross-modality encoder to produce the final output video segment embedding $S$. The cross-modality encoder employs the Cross-Attention to Concatenation strategy, where we first perform two streams of cross-attention, and then the output is concatenated and processed by another Transformer to model the global context.\nLet $S_T$ and $S_F$ denote the transcript text and video frame embeddings respectively. Let $S$ denote the V2LC output segment embedding produced by the multimodal interactions:\n$\\begin{cases} \\begin{aligned}\nS_T &\\leftarrow \\text{Attention} (Q_t, K_f, V_f), \\\\\nS_F &\\leftarrow \\text{Attention} (Q_f, K_t, V_t), \\\\\nS &\\leftarrow \\text{Mean} (\\text{Transformer} (\\text{Concat} (S_T, S_F))) \n\\end{aligned}\n\\end{cases}$                                (1)\nThen, we create the live comment feature embedding $C$ by encoding a segment $s_i$'s corresponding live comments $c_i$ with a second text encoder. We perform contrastive learning by maximizing the cosine similarity of the video segment embedding $S$ and live comment embedding $C$.\nThe goal of the V2LC encoder is to project each input video segment $s_i$ into the live comment representation space, so we freeze the live comment encoder and optimize only the video segment-to-live comment loss, training the V2LC encoder to match the representation of $s_i$'s accompanying live comments. The resulting training objective is:\n$L=\\frac{1}{N}\\sum_{i=1}^{N} - \\log \\frac{e^{(S_i \\cdot C_i) * \\tau}}{\\sum_{j=1}^{N} e^{(S_i \\cdot C_j)*\\tau}}$    (2)\nwhere $N$ is the number of video segment-live comment pairs in each batch, $(S_i \\cdot C_i)$ is the similarity (dot product) between video segment embedding $S_i$ and live comment embedding $C_i$, and $\\tau$ is a learned temperature parameter.\nWe use the V2LC encoder to produce synthetic live comment features to augment models for downstream tasks."}, {"title": "Multi-label Objective", "content": "The original CLIP architecture was designed primarily for single-label classification tasks. In our setting, there exists a many-to-many relationship, where a single video segment usually contains multiple live comments, and identical comments may appear in multiple video segments. To reconcile this discrepancy, we have modified the CLIP training framework to accommodate multiple labels: Given a batch of N videos and K live comments, the model is trained to predict which of the N \u00d7 K possible (video, live comment) pairings across a batch actually co-occur.\nAn additional challenge arises when multiple similar comments from different video segments appear within the same batch. In our dataset, similar comments naturally occur more frequently than the more distinct content pairs seen in the image-text datasets used in CLIP. During pre-training, if similar comments from different video segments are sampled within the same batch, it might lead to confusion in the contrastive learning setup. To mitigate this issue, we refine the training targets such that for a given video segment s, any live comment c in the batch with a vector similarity exceeding a predefined threshold @ to any actual live comment of s is also included in the target label set. Such comments are deemed correct, thereby allowing the model to learn a good live comment representation more efficiently."}, {"title": "Downstream Fine-tuning", "content": "Multimodal Fusion Encoder\nCurrently, in multimodal affective analysis, the text modality is commonly encoded with pre-trained language models like BERT (Devlin et al. 2019). In contrast, the acoustic and visual modalities have traditionally relied on hand-crafted feature extractors such as OpenSmile (Eyben, W\u00f6llmer, and Schuller 2010) for audio and OpenFace (Baltru\u0161aitis, Robinson, and Morency 2016) for facial expressions (Liu et al. 2022). These tools manually define the features to be extracted, which might not capture the full complexity of the data. However, recent work has introduced large-scale pre-trained encoders for these modalities which have yielded significant improvements in multimodal affective analysis. Therefore, we propose a downstream Multimodal Fusion Encoder that utilizes large-scale pre-trained encoders across all three modalities-text, acoustic, and visual to allow more comprehensive modeling of affective tasks.\nWe first extract features $f_m$ from each modality with pre-trained encoders, then we use a cross-modality encoder to fuse them. To accommodate three modalities, we construct the following cross-modality encoder:\n$\\begin{aligned} \nZ_t &\\leftarrow \\text{SelfAttn.}(\\text{Attention}(Q_t, K_{a+v}, V_{a+v})), \\\\\nZ_a &\\leftarrow \\text{SelfAttn.}(\\text{Attention}(Q_a, K_{v+t}, V_{v+t})), \\\\\nZ_v &\\leftarrow \\text{SelfAttn.}(\\text{Attention}(Q_v, K_{t+a}, V_{t+a})), \\\\\nZ &\\leftarrow \\text{Concat}(\\text{Mean}(Z_t), \\text{Mean}(Z_a), \\text{Mean}(Z_v)),\n\\end{aligned}$                       (3)\nwhere\n$\\begin{aligned}\nQ_{m1} &= W_q f_{m1} \\\\\nK_{m2+m3} &= W_k \\text{Concat}(f_{m2}, f_{m3}) \\\\\nV_{m2+m3} &= W_v \\text{Concat}(f_{m2}, f_{m3})       \n\\end{aligned}$                       (4)\nFinally, we pass the concatenated features $Z$ through feed-forward layers to produce our final prediction."}, {"title": "Augmenting with Live Comment Features", "content": "To enhance performance on downstream tasks, first, we apply self-attention to the synthetic live comment features produced by the V2LC encoder before mean-pooling. Then, we take the mean of these attentionally-pooled synthetic live comment features and concatenate it with Z, our multimodal representation from our fusion encoder. These combined features are then processed through feed-forward layers. We hypothesize that this joint training enables rich integration of the emotional context captured by the synthetic live comment features from our V2LC encoder, improving their efficacy in downstream affective analysis."}, {"title": "Experiments", "content": "Evaluation Datasets\nWe evaluated our work on six widely used affective analysis datasets: Chinese Multimodal Sentiment Analysis Dataset (CH-SIMS) (Yu et al. 2020), Chinese Multimodal Sentiment Analysis Dataset v2.0 (CH-SIMS v2) (Liu et al. 2022), Multimodal Opinion Sentiment and Emotion Intensity (MOSI) (Zadeh et al. 2016), Multimodal Sentiment Analysis (MO-SEI) (Bagher Zadeh et al. 2018) for sentiment analysis, Multimodal EmotionLines Dataset (MELD) (Poria et al. 2019) for emotion recognition, and Multimodal Sarcasm Detection Dataset (MUSTARD) (Castro et al. 2019) for sarcasm detection. Summaries of the datasets can be found in Table 3.\nCH-SIMS: This dataset contains short video segments from Chinese TV shows and movies, annotated with modality-specific and multimodal sentiment. We predict the multimodal sentiment labels.\nCH-SIMS v2: An extension of the original CH-SIMS dataset, CH-SIMS v2 adds an additional 2, 121 video segments and improves the balance and diversity of the dataset.\nMOSI: This dataset contains English movie review monologues from YouTube labeled with multimodal sentiment.\nMOSEI: An extension of MOSI, MOSEI expands its coverage to include more videos and topics.\nMELD: Drawn from the TV show Friends, this dataset contains utterance-level labels chosen from 7 emotions: anger, sadness, joy, neutral, fear, surprise, or disgust.\nMUSTARD: This dataset contains video clips from Friends, The Golden Girls, Sarcasmaholics Anonymous, and The Big Bang Theory. These audiovisual utterances, with their surrounding context, are annotated with sarcasm labels.\nImplementation\nData Processing For pre-training, we use a segment length o of 8 seconds, balancing between required context and the length of downstream datasets. We sample 8 frames uniformly from each segment. We filter out comments that contain no meaningful information such as comments that are too short (less than 2 characters). For user generated videos, we trim the first and last 15 seconds as they tend to include repetitive comments such as greetings and farewells. For movies, we trim the first and last 5 minutes, and for TV shows, we trim the start and end of each show.\nSegments containing fewer than 5 live comments are excluded from pre-training to allow efficient GPU batching. For each epoch, we randomly select 5 live comments per segment so that a batch with N samples has K = 5N comments. We sample 10% of our data for validation.\nImplementation Details We pre-train two variants of our V2LC encoder: one that uses only Chinese text transcripts and another that uses English text transcripts when available. For the Chinese-only variant, we use Chinese-ROBERTa-wwm-ext (Cui et al. 2020) as our transcript encoder; for the bilingual variant, we use XLM-ROBERTa-base (Conneau et al. 2019). For encoding live comments, We use Chinese-ROBERTa-wwm-ext.\nFor downstream fine-tuning, we use pre-trained encoders tailored to each modality and language. For the text modality, we use Chinese-ROBERTa-wwm-ext for Chinese datasets and ROBERTa-base for English datasets (Liu et al. 2019). For the acoustic modality, we adopt Chinese-HUBERT-base to encode Chinese data and Data2Vec-audio-base for English data. For visual features, we process all data using TimeSformer-base. Other details are specified in the Appendix.\nBaseline Models\nWe compare our model against multiple competitive baselines:\nMultimodal Transformer (MulT) (Tsai et al. 2019): Chosen for its pioneering approach of applying attention across modalities, MulT enables dynamic adaptation between modalities at different time steps, addressing the challenges posed by unaligned multimodal data.\nSupport Vector Machine (SVM): Known for its robust performance on small-sized datasets, SVM can, at times, surpass neural models. Following Castro et al. (2019), features are concatenated via early fusion and fed into an SVM classifier.\nAcoustic Visual Mixup Consistent framework (AV-MC) (Liu et al. 2022): The current SOTA for CH-SIMS v2, AV-MC introduces a modality mixup module as data augmentation, which mixes the acoustic and visual modalities from different videos to enhance performance.\nMultimodal Sentiment Knowledge-sharing Framework (UniMSE) (Hu et al. 2022): Chosen for its strong performance on MOSEI and MELD, this model proposes a knowledge-sharing framework that unifies two affective analysis tasks: multimodal sentiment analysis and emotion recognition in conversation, demonstrating the effectiveness of utilizing complementary knowledge in affective tasks.\nVision-Language Pre-Training To Multimodal Sentiment Analysis (VLP2MSA) (Yi et al. 2024): Chosen for its use of pre-trained vision models, VLP2MSA extracts spatio-temporal features from sparsely sampled video frames, offering advantages over traditional visual feature extraction. It captures not only facial expressions but also body movements, providing a more comprehensive analysis of visual information.\nMultimodal Multi-loss Fusion Network (MMML) (Wu et al. 2024): The current SOTA for CH-SIMS and MOSI, MMML utilizes pre-trained acoustic models as feature extractors for the acoustic modality.\nMetrics\nWe evaluate model performance using standard metrics for each task as established in the literature. Further details can be found in the Appendix."}, {"title": "Affective Features Comparison Study", "content": "Affective Feature Baseline Models We compare our synthetic live comment features with features from models trained for related affective analysis tasks:\nText-based Chinese sentiment features from Erlangshen-Roberta-sentiment (Erlangshen) (Zhang et al. 2022): representations from a text sentiment analysis model (Chinese-ROBERTa-wwm-ext fine-tuned on 227,347 sentiment-labeled texts from 8 datasets).\nText-based Chinese offensive language features from COLDETECTOR (Deng et al. 2022): representations from an offensive language detection model (Chinese-ROBERTa-wwm-ext fine-tuned on 37,480 sentences annotated with binary offense labels from the COLDataset).\nImage-based facial emotion recognition features from Vit-face-expression (VIT-face) (Todor Pakov 2024): representations from a facial emotion recognition model (Vision Transformer (Dosovitskiy et al. 2020) fine-tuned on 35, 887 faces from the FER2013 dataset, annotated with one of seven emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral).\nAffective Feature Experiments We evaluate the effectiveness of our synthetic live comment features through controlled experimentation in two settings. In the first, we train a logistic regression classifier on individual features to measure how well each feature discriminates our evaluation data. In the second, we augment our vanilla Multimodal Fusion Encoder with individual features to evaluate how well each feature can be leveraged by more expressive multimodal models. For these experiments, we focus on Chinese sentiment analysis with CH-SIMS v2 due to the diversity of its content and the amount of data available for fine-tuning. Additional experimental details can be found in the Appendix. We present the results of these experiments in Table 7."}, {"title": "Affective Feature Comparison Results", "content": "When using a simple logistic regression classifier, our synthetic live comment features outperform text-based Chinese sentiment from the Erlangshen even though that model was trained on a large quantity of task-specific sentiment data. Additionally, though less performant, our features are competitive with our other baseline features from models that benefit from supervised training on related affective tasks.\nThe strength of our synthetic live comment features becomes apparent when we move to our Multimodal Fusion Encoder. In this setting, they outperform the baselines by wide margins across all metrics considered. We hypothesize that though our synthetic live comment features are less discriminative in a linear model for a specific affective task (like sentiment analysis), they encode rich information that can be adapted to any affective task by the more expressive multimodal models that have become the standard in AI today. Our experimental results speak to this effectiveness."}, {"title": "Conclusion", "content": "In this work, we demonstrate the efficacy of synthetic live comment features in multimodal affective analysis. By constructing the LCAffect dataset and training a multimodal encoder on it, we enable the inference of these synthetic features for any video content. Our enhanced multimodal model, augmented with synthetic live comment features, achieves new SOTA results in sentiment analysis, emotion recognition, and sarcasm detection across both English and Chinese content. This advance confirms that synthetic live comment features can effectively capture a broad spectrum of affective states, opening new avenues across diverse dimensions of affective analysis."}]}