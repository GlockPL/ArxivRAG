{"title": "Generative Emotion Cause Explanation in Multimodal Conversations", "authors": ["Lin Wang", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang"], "abstract": "Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically uses clause selection methods to locate the reason utterance, without providing a detailed explanation of the emotional causes. In this paper, we propose a new task, Multimodal Conversation Emotion Cause Explanation (MCECE), aiming to generate a detailed explanation of the emotional cause to the target utterance within a multimodal conversation scenario. Building upon the MELD dataset, we develop a new dataset (ECEM) that integrates video clips with detailed explanations of character emotions, facilitating an in-depth examination of the causal factors behind emotional expressions in multimodal conversations. A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net significantly outperforms several excellent large language model baselines.", "sections": [{"title": "1 Introduction", "content": "In social life, multimodal conversation is crucial as it allows for a rich expression of emotions. Research (Woo et al., 2020; Agnew and South, 2014; Costa et al., 2013) indicates that exploring the causes of emotions is important in everyday life, as it can aid in promoting self-understanding. In recent years, researchers increasingly focus on how to explore the underlying causes of emotions. Consequently, a new task called Causal Emotion Entailment (CEE) (Xia et al., 2019; Zheng et al., 2022) is proposed, along with the introduction of a benchmark dataset named RECCON (Poria et al., 2021). The CEE task aims to identify the specific utterances that causally trigger the emotion expressed in the target utterance.\nWang et al. further propose a new task, Multimodal Emotion-Cause Pair Extraction in Conversations (MECPE), which expands the research scope to multimodal content. However, the identification of emotional causes still relies on selective cause extraction rather than cause generation. Selective causes fail to consider emotional expressions that lack direct causes in conversations and require contextual reasoning for understanding; for those emotional utterances, if the causal relationships behind them are primarily conveyed through visual or auditory information, determining the specific causes that trigger emotions becomes quite challenging. To resolve these issues, we propose a novel task, called Multimodal Emotion Cause Explanation in Conversation (MECEC), which aims to uncover the underlying causes of emotions expressed in a target utterance within a multimodal conversation setting, encompassing text and visual modalities. We build an Emotion Cause Explanation (ECEM) based on the MELD dataset (Poria et al., 2018) for MECEC. The ECEM dataset consists of 7,273 video clips, incorporating audio, visual and textual modalities, with each clip annotated with a thorough explanation of the emotional cause.\nTo address the task of MECEC, we propose a new model, Facial-Aware Multimodal Emotion Explanation Network (FAME-Net), which is based on LLaVA (Liu et al., 2023). FAME-Net integrates visual and facial emotion recognition to comprehensively understand multimodal emotional data, compensating for the limitations of text-only analysis and revealing deeper aspects of emotional contagion and hidden emotions. Facial expression recognition can assist in the task of discovering emotional causes; therefore, the model introduces a dedicated facial emotion recognition branch, designed as a two-stage pipeline. In the first stage, it performs scene and face detection to segment and track faces within videos; in the second stage, it employs multi-scale feature networks and global depth convolution techniques to accurately identify facial emotion features. Compared to other open-source LLMs, our model excels in text generation metrics, achieving the best results.\nOur main contributions are as follows:\n\u2022 We introduce a novel task named Multimodal Emotion Cause Explanation in Conversations (MECEC) along with a new dataset, Emotion Cause Explanation based on the MELD dataset (ECEM), which provides detailed natural language explanations for the emotions of target utterances and is specifically designed for training and evaluating the MECEC task.\n\u2022 We propose FAME-Net, which integrates visual modality and facial emotion recognition to achieve comprehensive analysis of emotional causes. Through a two-stage pipeline, it accurately extracts emotional features, enhancing the performance of multi-modal emotion causes explain and generation.\n\u2022 Experimental results on our annotated dataset indicate that our proposed model, FAME-Net, outperforms several existing open-source baseline models in the MECEC task."}, {"title": "2 Related Works", "content": "2.1 Causal Emotion Entailment\nCausal Emotion Entailment (CEE) is a task that seeks to identify the utterance that triggers the emotion expressed in a target utterance within a conversation. Pioneering research by Poria et al. laid the groundwork for developing various models that leverage speaker identity and contextual information to enhance emotion cause identification. Notable models include the TSAM (Zhang et al., 2022) and the Knowledge-Bridged Causal Interaction Network (Zhao et al., 2023), which leverages commonsense knowledge to enhance the understanding of emotional causality. Previously proposed tasks were all in the form of selection, without tasks that generate causes.\n2.2 Large Language Models\nThe advent of Large Language Models (LLMs) significantly impacts the fields of Natural Language Processing (NLP) and multimodal learning. Models like OpenAI's GPT (Brown et al., 2020), CLIP (Radford et al., 2021), and BLIP (Li et al., 2022a) demonstrate remarkable capabilities in understanding and generating text, as well as associating textual descriptions with visual content. More recently, models like LLaVA (Liu et al., 2024) and Video-ChatGPT (Maaz et al., 2023) push the boundaries of multimodal learning by integrating video and audio analysis into conversational AI systems."}, {"title": "3 Dataset Construction", "content": "3.1 Data Source\nThe MELD dataset is a classic dataset for the Emotion Recognition in Conversations (ERC) task. We find that this dataset not only contains rich emotional information but also includes exciting plot and emotional causality. Therefore, we chose to expand the MELD dataset to construct our own dataset. During the data annotation process, we observed issues such as missing textual data relative to the video content and incorrect use of punctuation. Therefore, in our experiments, we used a combination of WhisperX (Bain et al., 2023) and WhisperAT (Gong et al., 2023) to transcribe audio information from videos to correct the original data and improve data quality.\n3.2 Annotation Program\nWe first provide a detailed explanation of the dataset annotation method. To ensure consistency in style and accuracy of the annotated data, we refer to the research findings (Lin et al., 2014; Fang et al., 2015). Specifically, we invite five graduate students specializing in sentiment analysis to participate in the annotation work. We first ensured that they fully understood the task definition and annotation methods, and then two of the graduate students independently annotated the entire dataset. On this basis, we calculate the BERTScore (Zhang et al., 2019) of the results annotated by the two students. When the BERTScore is greater than 0.75, the other three graduate students vote on the annotation results, and the majority vote determined the final annotation result. When the score is less than 0.75, We have the graduate students involved in the annotation discuss the causes of the discrepancies and reach a consensus. This ensures the accuracy and consistency of data annotation.\nData Pre-Processing We preprocess the MELD dataset by merging preceding video clips into a continuous sequence to accurately capture emotional causes of each utterance without interference from future content. This method is beneficial for elucidating the emotional causes associated with the final discourse in the video segments. For example, in a dialogue consisting of three utterances, Uo and U1, the concatenated results would be (Uo) and (Uo, U1). During the concatenation process, overlapping segments between the original video clips resulted in redundant content. Therefore, we design an algorithm to detect and remove duplicate frames to enhance the quality of our dataset.\nAnnotation Guidelines We focus more on different emotions; therefore, this paper does not involve neutral emotions. Before the formal annotation, we randomly assign 100 videos to annotators for training, and formal annotation only begins after the"}, {"title": "3.3 Data Statistics and Analysis", "content": "Comparison with Existing Datasets In Table 1, we compare our dataset ECEM with other cause detection datasets in emotion analysis. It can be observed that most cause detection datasets involve selective explanation types, and generative cause types are present in humor and sarcasm detection datasets; however, their emotions are limited to single emotions. In contrast, our dataset generates multiple causes for various emotions, making results more understandable and useful to users.\nData Analysis Since neutral emotions do not require explanation, we removed the neutral emotions from the original MELD dataset. We conduct a statistical analysis on the proportions of the remaining six emotions, and the results are presented in Figure 3. In order to ensure the consistency of labeling for each instances and guarantee data quality, we also conduct a statistical analysis on the length of these explanations. Through this processing and analysis, we aim to provide more accurate and comprehensible emotion cause explanation. Upon analyzing the statistical results, we observe that the annotations for the six emotions demonstrate a consistent pattern across the maximum, minimum, and average values. This consistency indicates a high level of standardization in our annotated data."}, {"title": "4 Method", "content": "4.1 Task Definition\nOur task is to identify the emotional cause behind the sentiment expressed in specific utterance within a given video clip. A given conversation, U, includes n utterances with different emotions, i.e.,\n$U = {(U_i, C_i)}_{i=1}^n$, and $u_i$ refers to the i utterance, possessing the specified emotion, e. Each utterance is a multimodal input with the vision and text modalities, denoted by $u_i = (u_i^v, u_i^t)$. The goal of the Multimodal Conversation Emotion Cause Explanation (MCECE) task is to generate the cause, C, for the target utterance $u_n$ having emotion $e_n$.\n4.2 Overview\nIn this paper, we propose Facial-Aware Multimodal Emotion Explanation Network (FAME-Net), a multimodal large language model built on LLaVA (Liu et al., 2024). The overall structure of the model is illustrated in Figure 5. FAME-Net is capable of understanding video, text, and facial emotions. Facial emotion recognition provides a more comprehensive understanding of emotions, compensating for the limitations of text analysis (Ekman and Friesen, 1971), and capturing the contagion of emotions to reveal deeper emotional causes (Hatfield et al., 1993). Additionally, it can detect inconsistencies between facial expressions and verbal communication, thereby uncovering hidden emotions (Chartrand and Bargh, 1999). Therefore, we concentrate on analyzing facial expressions in video data and propose a novel model specifically designed to capture the dynamics and evolution of facial expressions.\nDuring its development, FAME-Net integrates advanced mechanisms to align visual signals with language processing, enhancing multimodal analysis. The model features a CLIP-based video encoder (Radford et al., 2021) tailored to capture the information from spatial and temporal dimensions of video data, alongside a facial emotion recognition module that accurately identifies and interprets subtle emotional nuances. This combination enables FAME-Net to reveal deeper emotional insights beyond conventional text analysis.\n4.3 Vision-Language Module\nOur architecture employs the CLIP ViT-L/14@336 as the visual encoder, adapting it from image processing to video processing in FAME-Net, which is essential for the model to effectively capture spatio-temporal representations in videos. For a video, $V = {I_1, ..., I_k, \u2026\u2026\u2026, I_T}$, where $I_k \\in R^{H \\times W \\times C}$ and T denotes the number of frames. The encoder independently processes each frame, generating the frame-level embedding $x_k = CLIP(I_k), x_k \\in []^{R^{h \\times w \\times D}}$, where $h = H/p, w = W/p$ and prepre-sents the patch size, and $h \\times w = N$. The video embedding can be obtained, $X = [x_1, .., x_k, ..., x_T]$.\nTo construct a comprehensive video-level representation, we apply average pooling along the temporal dimension of these frame-level embeddings, resulting in a temporal representation $R_t \\in R^{T \\times D}$:\n$R_t = \\frac{1}{N} \\sum_{i=1}^N X$ (1)\nThis temporal pooling technique effectively inte-grates information across multiple frames. Similarly, we apply average pooling along the spatial dimension to obtain the spatial representation $R_S \\in R^{N \\times D}$:\n$R_S = \\frac{1}{T} \\sum_{i=1}^T X$ (2)\nThe final video-level features $v_j$ are a concatenation of these temporal and spatial features.\n$R = [R_t \\oplus R_S] \\in R^{(T+N) \\times D}$ (3)\nwhere $\\oplus$ is the concatenation operation.\nA simple trainable linear layer g, projects these video-level features into the language decoder's embedding space, transforming them into corresponding language embedding tokens $Q_v$.\n$Q_v = g(R) \\in R^{(T+N) \\times K}$ (4)\nwhere K represents the dimension of the text query. Specific inquiries regarding emotional causes for explanations: User Query is denoted as $Q_t \\in R^{L \\times K}$ where L is the length of the query. These queries are tokenized to be dimensionally compatible with the video embeddings. Finally, $Q_v$ and $Q_t$ are input to the language decoder together.\n4.4 Facial Emotion Module\nWe further design a two-stage system, where the first stage performs face detection to segment faces from the video, and the second stage conducts facial emotion recognition. This two-stage detection system reduces the impact of the environment on facial emotion recognition and improves the accuracy of detection.\nIn the first stage, in order to efficiently extract features and adapt to various scale changes in face, we use CNN (LeCun et al., 1998) to extract feature maps F from the input video frame I, and then, a set of multi-scale anchors is generated on the feature map, and these anchors a are classified using Detection Layers composed of VGG16 (Simonyan and Zisserman, 2014) and CNNs to determine whether they contain a face. Simultaneously, bounding box regression is applied to adjust the position and size of the candidate detection boxes. The process of classification and regression can be represented as:\n$1_{face}, A = DetectionLayers(F, a)$ (5)\nwhere $1_{face}$ represents whether it contains a face, and A represents the bounding box score. Using the regression output A to adjust the position and size of each anchor, a, the detection box, B, is generated. To reduce the probability of detecting the same face multiple times, we use Non-Maximum Suppression (NMS) (Felzenszwalb et al., 2009) with an Intersection over Union (IoU) threshold to obtain the final detected faces.\n$B_{final} = NMS(B, 1_{face}, IoU_{threshold})$ (6)\nIn the second phase, to reduce the loss of spatial information and retain more contextual information, we first use a multi-scale feature network (MFN) (Zadeh et al., 2018) to extract feature maps $F_{MFN}$ from the input face $B_{final}$:\n$F_{MFN} = MFN(B_{final})$ (7)\nThe features of facial emotions are not limited to a single direction. By considering both directions simultaneously, the model can more comprehensively understand the complexity of facial expressions. Therefore, we use DDA (Zhang et al., 2023b) to generate attention maps in both the vertical direction (Y) and the horizontal direction (X).\n$F_{DDA}^Y = DDA_X(F_{MFN})$\n$F_{DDA}^X = DDA_Y(F_{MFN})$ (8)\nWe integrate attention mechanisms from two directions to enhance recognition capability and accuracy. The sigmoid function, \u03c3, is used to suppress noise while strengthening the critical areas of the features. This result is multiplied by the max-pooling output to further enhance important features and suppress less significant information, yielding the final attention feature map $F_{att}$.\n$F_{att} = pooling(F_{DDA}^X, F_{DDA}^Y) \\bigoplus (\\sigma(F_{DDA}^X) \\bigodot  \\sigma(F_{DDA}^Y))$ (9)\nFinally, the attention feature map is passed through a fully connected layer to predict the emotion category of the face, $E_{cls}$.\n$E_{cls} = FC(F_{att})$ (10)\n4.5 Video Instruction Tuning\nDuring the fine-tuning phase, we use predefined prompts, Promptvideo.\nUSER: <Instruction> <Vid-tokens><Emotion>\nAssistant: Using the notations, we can represent it as:\nUSER: <Qt> <Qv> <Ecls>\nAssistant:"}, {"title": "5 Experiments", "content": "5.1 Implementation Details\nOur FAME-Net model is built on the LLaVA-7b (Liu et al., 2023) foundation. To enhance training efficiency, we preprocess the data beforehand to minimize the time spent on facial emotion recognition during training. The training process is conducted on two NVIDIA A6000 48G GPUs.\n5.2 Evaluation Metrics\nTo comprehensively evaluate the fluency, diversity, and accuracy of generated causeing, we adopt a series of widely recognized automatic evaluation metrics for text generation, including BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015), which focus on lexical and syntactic matching, as well as BERTScore (Zhang et al., 2019), Semantic Similarity (Sem_Sim) (Gao et al., 2021), and BLEURT (Sellam et al., 2020), which assess whether the generated text is semantically consistent with the reference text.\n5.3 Baselines\nWe compare it with several open-source models of similar scale. Ultimately, we select NExT-GPT (Wu et al., 2023), ChatUniVi (Jin et al., 2023), Video_LLaVA (Lin et al., 2023; Zhu et al., 2023), Video-LLaMA(Zhang et al., 2023a), and Video-LLaMA2 (Cheng et al., 2024) as baselines. Also, we choose models like Qwen (Bai et al., 2023), LLaMA2 (Touvron et al., 2023), and ChatGLM3 (AI and KEG, 2023), which excel in single text modality, for experiments where only conversation history is input. We train and test these five models on the ECEM dataset.\n5.4 Main Results\n5.4.1 Automatic Evaluation\nTable 2 presents the scores of eight baseline models and FAME-Net on seven metrics. FAME-Net demonstrates the most outstanding performance on four key metrics, indicating its exceptional capabilities in terms of generation quality, semantic alignment, and text diversity. This advantage is attributed to the innovative training data and the model's ability to understand the emotions conveyed by facial expressions, enhancing its understanding of the reasons behind these emotions. Although FAME-Net's performance on Sem_Sim and CIDEr is not the best, it is still very close to optimal, indicating that the model has strong semantic understanding and content relevance when generating text. We speculate that the inability to achieve the best performance may be due to its inability to fully capture the complexity and nuances of text, such as humor and irony. Therefore, we speculate that FAME-Net's poor performance on BLEU4 is due to the model's focus on diversity, fluency, and contextual consistency in the generated content, leading to fewer n-gram matches with the reference text and thus a lower BLEU4 score.\nOverall, FAME-Net shows that it is an outstanding model in text generation tasks, demonstrating leading advantages in multiple metrics and achieving a good balance between innovation and accuracy. Furthermore, multimodal models, including FAME-Net, outperform pure text models in overall performance, suggesting that integrating information from multiple modalities can significantly enhance the model's ability to solve MCECE tasks."}, {"title": "5.4.2 Ablation Study", "content": "The ablation study results are shown in Table 4. Except for the Sem_Sim metric, all other metrics decline to varying degrees when the text modality T and facial emotion recognition module E are removed. This confirms the effectiveness of our facial emotion recognition module in addressing the MCECE task. Without the facial emotion recognition component, FAME-Net may become more conservative, generating simpler or more common sentence structures. We believe the Sem_Sim metric increase is due to reduced modality conflicts, less redundant information, or FAME-Net's enhanced focus on key video features, improving the semantic consistency of the generated text.\n5.4.3 Human Evaluation\nIn addition to automatic assessment, we also randomly select 100 test samples from the four best-performing models for manual evaluation. We ask the evaluators to score the generated reasons on three criteria: Contextuality, Fluency, and Relevance, on a scale of 1 to 5. Contextuality measures whether the generated sentences are consistent with the context and coherent with the previous dialogue. Fluency assesses the grammatical correctness and naturalness of the sentence in terms of language use. Relevance is used to determine whether the generated sentences contain the exact reasons for expressing emotions. The results, as shown in the Table 5, indicate that FAME-Net achieved the best performance in Fluency and Relevance, demonstrating its strongest capability in fluency and generating precise reasons.\n5.4.4 Original Data and Transcription\nTo verify our hypothesis that audio transcription data is of higher quality compared to the original"}, {"title": "6 Conclusion", "content": "Our paper introduces a novel and challenging task, Multimodal Conversation Emotion Cause Explanation (MCECE), and constructs a new amd comprehensive dataset, ECEM, for the training and evaluation of MCECE. To address this task, we propose a specialized model, FAME-Net, which is based on the LLaVA model. FAME-Net is proficient in understanding the content of videos and providing natural language explanations for the emotions of characters depicted in those videos. Extensive experimental results demonstrate that our model performs effectively in the MCECE task. Our work provides new insights and proposes a new direction for multimodal emotion cause extraction."}, {"title": "Limitations", "content": "Although our research has made certain progress, it is not without limitations. Firstly, our model is purely English, which means that our model and evaluations may not be directly applicable in other language environments. Moreover, our model, being based on a trained Large Language Model (LLM), has the issue of hallucination defects."}]}