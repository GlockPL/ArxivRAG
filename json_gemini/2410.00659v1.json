{"title": "Multimodal Coherent Explanation Generation of Robot Failures", "authors": ["Pradip Pramanick", "Silvia Rossi"], "abstract": "The explainability of a robot's actions is crucial to its acceptance in social spaces. Explaining why a robot fails to complete a given task is particularly important for non-expert users to be aware of the robot's capabilities and limitations. So far, research on explaining robot failures has only considered generating textual explanations, even though several studies have shown the benefits of multimodal ones. However, a simple combination of multiple modalities may lead to semantic incoherence between the information across different modalities a problem that is not well-studied. An incoherent multimodal explanation can be difficult to understand, and it may even become inconsistent with what the robot and the human observe and how they perform reasoning with the observations. Such inconsistencies may lead to wrong conclusions about the robot's capabilities. In this paper, we introduce an approach to generate coherent multimodal explanations by checking the logical coherence of explanations from different modalities, followed by refinements as required. We propose a classification approach for coherence assessment, where we evaluate if an explanation logically follows another. Our experiments suggest that fine-tuning a neural network that was pre-trained to recognize textual entailment, performs well for coherence as-sessment of multimodal explanations. Code & data: https:// pradippramanick.github.io/coherent-explain/.", "sections": [{"title": "I. INTRODUCTION", "content": "With the growing potential of assistive robotics, there is an increasing concern about the explainability of the decisions they make and the predictability of the outcome of such decisions. These concerns are amplified when the robot's behavior is decided by complex systems that are often non-deterministic and with the possibility of failures in unexpected situations. For non-expert users and observers, understanding the reason for a failure can set realistic expec-tations for the robot and help to build trust [1].\nThe existing works on providing explanations for robot failures primarily use text as a modality [2], [3]. While explaining using natural language can be intuitive to a non-expert user, there are several limitations in using only text as an explanation medium [4]. Prior studies reveal a need for multi-modal explanation [5] and highlight its benefits in terms of intuitiveness and efficiency in presenting complex information [6], [7], [8]. However, the problem of multi-modal explanation is not well-studied in the context of providing explanations of robot failures. Further, previous research on multimodal explanation generation does not study the coherence of the generated explanations across the modalities, even though incoherent explanations can occur in several scenarios, as we discuss later.\nIn this paper, we present the problem of coherent multi-modal explanation generation of robot failures. Particularly, we study a combination of two modalities:\n1) A text modality that contains a natural language de-scription of an observed failure;\n2) A graphic modality that shows information about the cause of the failure, such as the robot's plan until the failure and beliefs represented as a scene graph, overlaid on the robot's egocentric-view image that captures failure observation.\nOur initial observation indicates that a simplistic amal-gamation of modalities may result in scenarios where the information presented across the two modalities is incon-sistent. There are two major reasons for this. Firstly, such inconsistencies may stem from text generation using neural networks, where even large language models (LLMs) tend to hallucinate [9], despite showing strong reasoning skills at times for explanation generation [3]. As a motivating exam-ple, consider Figure la which shows a textual explanation generated by a state-of-the-art method [3] for failing to turn on a television. The textual explanation incorrectly says the robot could not locate the remote control, which contradicts the robot's belief (i.e., the robot detects the remote control below the book) and its visualization in the graphic modality, as shown in Figure 1b. Second, the world model of the robot and the explanation generator (e.g., a LLM) can be different. This leads to situations where the reasoning in the text explanation is based on a human's approximation\u00b9 of a robot's world model, while valid in a human's world model, may not apply to a specific robot having a specific set of available actions, or different observation capabilities. This phenomenon is similar to the problem of Model Reconcil-iation in the explainable planning literature and has been extensively studied using formal methods [10].\nAnother reason for a lack of coherence between the two modalities is the possibility of an observed failure and the cause of the failure not coinciding temporally. We show another motivating example in Fig. 1c, where for the same task, the failure occurs at 01:03, but the robot's observation at the time does not reflect the cause of the failure. Such incoherent explanations may induce incorrect beliefs about"}, {"title": "II. RELATED WORK", "content": "A large body of prior research on explainable artificial intelligence (XAI) focuses on improving the transparency of black-box classifiers, i.e., they provide methods for reasoning over a single instance of a decision-making problem [11], or even a set of non-sequential instances [12]. In contrast, the explainability of robot behavior generally involves explaining a sequential decision-making problem. Recent reviews on ex-plainable robotics [5], [4] provide a summary of methods, ap-plication areas, and evaluation methodologies for explaining robot behavior. They also highlight the lack of research on the explanation of robot failures and multimodal approaches to explanation. The following summarizes relevant research on these two topics, along with research on multimodal coherence."}, {"title": "A. Failure Explanation", "content": "Autonomous failure detection is often a precursor to explanation. Several approaches have been proposed to do so, which include both model-based reasoning [13], [14] and data-driven learning to predict anomalies that often take multimodal sensory data as input [15], [16]. While these approaches are important contributions to detecting both planning and execution failures, they are limited to failure detection without explanation. Similarly, research on the explanation of failures can be broadly categorized into three approaches, which generally involve finding a cause for an observed failure.\nFirst, model-based approaches perform reasoning with a formal world model and symbolic observations [17], and focus on providing contrastive explanations for planning [10] and sub-optimal behavior [18], not considering execution failures. Also, the recipients of the explanations are do-main experts, instead of non-expert users that we target in this work. Second, data-driven methods learn from labels provided by non-experts to automatically generate explana-tions from a sequence of prepossessed sensory observations. Inceoglu et al. model explanation of manipulation failures as a failure-type classification problem [19]. Further, [2], [20] propose methods for learning to generate textual ex-planations of failures. Finally, neural-symbolic approaches encode domain knowledge using symbolic constructs to either formulate a data-efficient learning problem, for both experts [21] and non-experts [3], [22], or convert state predic-tions into explanations using templates [23], [24]. However, most of the previous approaches to generating explanations"}, {"title": "B. Multimodal Explanation", "content": "Prior studies in HRI suggest that multimodal explanations are often more efficient and intuitive than unimodal explana-tions, particularly compared to textual explanations [4], [25], [26]. Several works have addressed the problem of explaining the answers to visual question-answering (VQA) systems, by providing visual evidence along with textual explanations [6], [7]. VQA explanations are relevant for the problem addressed in this paper since they perform reasoning over a sequence of predictions. In robotics, several works have explored the combination of text and some form of graphics to improve the transparency and explainability of robotic systems and classifiers used for HRI. Perlmutter et al. [27] combined visualization of a robot's beliefs and intentions with textual feedback to improve the transparency of a situated language understanding system. A similar form of visualization has been explored in [8] to explain emotion recognition in HRI and in [28] for an explainable HRI system to teach robots with augmented reality. Hastie et al. developed a multimodal interface by combining text explanations in a graphical interface for transparent interaction with a remote robot [29]. Even though the prior research on multimodal explanations has not been specifically applied to failure explanations, our selection of modality combinations for studying coherence is motivated by these."}, {"title": "C. Multimodal Coherence", "content": "The majority of research on computational models of multimodal coherence focuses on image-text coherence. Sev-eral taxonomies have been proposed, primarily based on the theory of discourse relations in linguistics [30]. Otto et al. propose a categorization of semantic relations be-tween images and text and a method to detect them [31]. This categorization is based on three attributes cross-modal mutual information, the presence of hierarchy, and semantic correlation, which is analogous to our definition of coherence. Alikhani et al. propose six classes of coherence relations based on an image captioning dataset [32]. They also present a method for predicting the relations and a coherence-aware image captioning model. These relations are further analyzed in [33], along with an evaluation of several vision-language models for the task of predicting the relations. The taxonomy in [32] and [33] is almost comprehensive for textual descriptions of images, except it does not consider contradiction.\nFurther, much of the existing taxonomies are not formally defined, leading to subjective interpretation and classification ambiguity, e.g., multiple relations are applicable for the same pair of image and text descriptions [32]. In contrast, our model of coherence assessment focuses on semantics, instead of expressiveness or the style of description. Thus, it is simpler, less ambiguous, and allows us to model coherence assessment as an entailment recognition problem. In this regard, our work is also relevant to multi-modal stance detection [34] and fact-checking [35], which follow a similar taxonomy. However, the existing taxonomies and methods for their prediction are designed for problems such as image captioning and multimodal information retrieval, and thus they cannot be trivially applied to the problem of coherence in multimodal explanation of robot failures."}, {"title": "III. PROPOSED FRAMEWORK", "content": "In this section, we first formally introduce the multimodal explanation framework and define the problem, before de-scribing our methodology in detail. Given a high-level task plan \u03c0, a sequence of observations taken at n discrete time steps O = {o\u2081, o\u2082,..., o\u2099}, where a failure is observed in o\u1d62, we aim to present a multimodal explanation E\u2098 for \u03c0. E\u2098 consists of a pair of mutually coherent explanations, a textual explanation E\u209c and a graphical explanation E\u1d4d = {E\u1d58, E\u1d3c\u1d62}, which is overlaid on the corresponding ego-view image I\u1d62 at time step i. Each E\u1d4d has two components, an explanation of action execution, i.e., the plan until the failure observation E\u1d58, and a sub-graph of the scene graph at i, E\u1d3c\u1d62. To obtain E\u2098, we first obtain a base E\u209c and E\u1d4d independently, by reasoning over \u03c0 and O, which we describe in Section III-C. Next, we assess the coherence between E\u209c and E\u1d4d and then perform refinements to either E\u209c or E\u1d4d, as and if required. In the following, we formally define the two sub-problems."}, {"title": "A. Coherence Assessment", "content": "Given a base textual explanation E\u209c, a base graphical explanation at step i, E\u1d4d, and the observation sequence O, we model coherence assessment as a ternary classification task from the set,\nC = {E\u1d4d \u22a8 E\u209c, E\u1d4d \u22ad E\u209c, E\u1d4d \u22a5 E\u209c},\nwhere E\u1d4d \u22a8 E\u209c denotes that E\u209c is supported by E\u1d4d and thus E\u2098 = {E\u209c \u222a E\u1d4d} is coherent, E\u1d4d \u22ad E\u209c denotes that E\u209c is not supported by E\u1d4d, and E\u1d4d \u22a5 E\u209c denotes that E\u209c contradicts E\u1d4d. Particularly, we want to estimate the following,\nc = argmax P(c|E\u209c, E\u1d4d, \u03c0).\nc\u2208C\nWe describe the method to learn this classification in Sec-tion III-D."}, {"title": "B. Explanation Refinement", "content": "Based on the outcome of the above classification, we either present the multimodal explanation as is, i.e., in the case of E\u1d4d \u22a8 E\u209c; or we select one of the following refinement strategies.\n\u2022 Refine E\u1d4d \u22ad E\u209c: This refinement strategy assumes that E\u209c is correct, and therefore searches for a new graphical explanation in a time step j, E\u1d4d', \u2200j \u2208 {0..n} \\ {i} that satisfies E\u1d4d' \u22a8 E\u209c.\n\u2022 Refine E\u1d4d \u22a5 E\u209c: This refinement strategy assumes that E\u209c is incorrect and therefore proposes a refined textual explanation E\u209c' that satisfies E\u1d4d \u22a8 E\u209c'.\nWe detail the refinement strategies in Section III-E."}, {"title": "C. Obtaining E\u209c and E\u1d4d", "content": "We rely on the work of Liu et al. [3] to generate E\u209c. More specifically, we convert the tuple (\u03c0, O) into a natural language description using the method proposed in [3]. The natural language description of the plan and the observations is a sequence of tuples that consists of an action from the plan and the robot's observation after attempting to execute the action. We put this summary of action execution in a template and prompt a large language model\u00b2, which generates a textual explanation of the failure, along with a prediction of the time step i. We use the same prompt templates as [3]. For completeness, we also perform experiments with the expert-provided failure time steps and explanations in [3].\nWe generate E\u1d4d from O\u1d62 and \u03c0. Specifically, we represent O\u1d62 as a 2D scene graph using [3], and then perform a filtering operation to obtain a sub-graph,\nE\u1d3c\u1d62 = {v \u2208 A(\u03c0, i) \u222a {v* \u2208 V(O\u1d62) : (v, v*) \u2208 E(O\u1d62)}\nwhere V(O\u1d62) and E(O\u1d62) denote the set of vertices and edges in O\u1d62 and the function A(\u03c0, i) returns the arguments in the plan step (action) executed during i. This filtering returns a sub-graph where the vertices are either an argument of the action at i or they have an edge with at least one of such vertices. We do this filtering to restrict the visualization of the scene graph to only the objects that are relevant to the current action, in an effort to highlight the cause of the failure concisely. We obtain E\u1d58 by simply selecting a sub-sequence of \u03c0 till i."}, {"title": "D. Modality Coherence Classification", "content": "Before describing our method to perform coherence clas-sification, we first define the class symbols in the context of our problem. Let us consider that an explanation E is a set of m propositions, represented as a conjunction of grounded predicates,\nE = P\u2080 \u2227 P\u2081... P\u2098.\nTherefore, we define the class symbols as the following.\nE\u1d4d \u22a5 E\u209c = \u2203P\u2c7c \u2208 E\u1d4d, \u2203P\u2096 \u2208 E\u209c : P\u2c7c \u22a5 P\u2096.\nE\u1d4d \u22a8 E\u209c = \u2203P\u2c7c \u2208 E\u1d4d, \u2203P\u2096 \u2208 E\u209c : P\u2c7c = P\u2096 \u2227 \u00ac(E\u1d4d \u22a5 E\u209c).\nE\u1d4d \u22ad E\u209c = \u00ac(E\u1d4d \u22a5 E\u209c \u2227 E\u1d4d \u22a8 E\u209c).\nAs an example, consider the following propositions from the observation in Figure 1b, on_top(remote-control, table) \u2227 on_top(book, remote-control). The proposition (locate (remote-control)) in the text span \"the robot not be-ing able to locate the remote control\" is contradictory to on_top (remote-control, table). Similarly, the proposi-tion on_top(book, remote-control) entails the proposition is_blocking(book, remote-control) in the expert-written expla-nation \"book is blocking the remote control\" in [3]. Finally, considering the propositions in the observation in Figure 1c are on_top(television, tv-stand) \u2227 has_state(television, off), the propositions in the text explanations are neither entailing nor contradicting."}, {"title": "E. Refinement Strategies", "content": "1) Refine E\u1d4d \u22ad E\u209c: In this refinement strategy, we iteratively select a graphical explanation from the discrete time steps and perform a coherence assessment until we find a E\u1d4d\u1d62, such that E\u1d4d\u1d62 \u22a8 E\u209c is satisfied. If we do not find such a time step, we fall back to finding a E\u1d4d where either E\u1d58 \u22a8 E\u209c or E\u1d3c\u1d62 \u22a8 E\u209c is true. To do this efficiently, we restrict the selection of time steps to only those where a scene graph in O\u1d62 is different from O\u1d62\u208b\u2081. This is similar to the key-frame selection method in [3].\n2) Refine E\u1d4d \u22a5 E\u209c: We propose a simple refinement strategy to generate the refined textual explanation for both E\u1d58 \u22a5 E\u209c, and E\u1d3c\u1d62 \u22a5 E\u209c. We refine the textual descriptions to a much simpler explanation of the task failure by the template The robot failed to complete [TASK] because it was unable to perform [ACTION] at [TIME], where the [*] slots are filled by information for a particular task failure. This template is similar to several expert-written explanations in [3]. Even though this explanation template is less specific, we chose it to avoid providing contradictory explanations. We discuss a few ways to improve the refinement of textual explanations in Section IV-D."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate our methods, we first obtain explanations from the RoboFail dataset in [3]. We further generate counterfac-tual examples based on the metadata provided by the ai2thor simulator [38]. In the following, we describe these in detail.\nRoboFail Dataset (RF). RF contains various failure scenarios that are generated by manually injecting failure conditions in ai2thor. To utilize this dataset for evaluating our coherence classification model, we first extract tuples of the robot observation on the marked timestamp of failure, the plan until the failure, and the text explanations generated by [3], as well as expert-written explanations. Then, we manually annotate the data using the definitions presented in Section III-D, obtaining a total of 260 examples. We exclude explanations where the LLM fails to predict the time step of failure. To convert natural language text into a conjunction of propositions, we apply a heuristic method of converting the text into a predicate-argument structure using a pre-trained semantic parsing model [39].\nCounterfactual Generation (CF). RoboFail has a total of 29 examples of contradiction (\u2248 11%). This is a significant percentage considering that the dataset was not developed to study coherence, it includes expert-written explanations, and even a few contradictions can negatively affect the explainability of a multimodal system. However, for training and a fair evaluation, we generate a larger and more balanced dataset by generating counterfactual examples. To do so, we first sample a random task plan and a scene graph from RF. For sampling task plans, we restrict to this subset of RF tasks boil water, heat potato, make coffee, and toast bread. Then we select a random failure type to inject from a subset\u00b3 of failure injection methods in RF - unexpected dynamics, failed execution, wrong order of actions and missing actions [3]. Next, depending on the failure type, we generate counterfac-tual examples by modifying either the plan, a set of observa-tions, or both. More specifically, we perform modifications to the observation by replacing predicates, arguments, and adding negations. We further modify plans by introducing actions with unmet preconditions, by either deleting actions having a common effect, or by reversing a pair of actions having the same arguments. We collect a total of 1240 automatically annotated examples with counterfactual data generation.\nTo make training, validation and test sets, we separate RF into two subsets based on task types. We do this to test the generalizability of the reasoner on explanation pairs from unseen tasks. However, please note that even for the same task types, the propositions, or the conjunction of propositions are distinct. Additionally, for E\u1d58, the sequence of propositions is also distinct. Thus, we first separate the data of make salad, warm water, and store egg tasks from RF. There are 80 such data points which are absent in CF and not used in training and validation sets. The rest of the data in RF contains the task types water plant, cook egg, and switch devices, in addition to the four task types in CF. We combine this data with CF and perform a random stratified split of 70:10:20 into train, validation and test sets to maintain similar class ratios. Finally, we merge this random test set with the held-out data for unseen task types, which creates our final test set of 364 data points."}, {"title": "B. Baselines", "content": "We compare our approach to several baselines, as de-scribed in the following. With the first two baselines, we aim to evaluate how models pre-trained with other NLI datasets perform on the coherence classification problem. We design the last two baselines to understand the effect of our approach of modeling coherence assessment as an entailment recognition problem. To do so, we simply train a text-pair classifier, i.e., without performing transfer learning from other entailment recognition datasets. Our baselines are the following:\n\u2022 ROBERTa-large-MNLI\u2074 - A language model based on the ROBERTa architecture, fine-tuned on MNLI [37].\n\u2022 DeBERTa-v3-base-NLI\u2075 - A language model based on the DeBERTa-v3-base architecture that is fine-tuned on 763913 premise-hypothesis pairs from 3 NLI datasets.\n\u2022 DeBERTa-v3-base - We obtain text-pair representation using DeBERTa-v3-base [36] and pass it to a randomly initialized dense layer to perform classification."}, {"title": "C. Results", "content": "We select the checkpoint having the highest macro-F1 score on the validation set and evaluate it on the test set. Table I summarizes the main results. We find that models that are trained only on NLI datasets do not perform well for coherence classification. This is not unexpected because even though the two problems are similar, the existing NLI datasets contain data from domains that are unrelated to robotics. Also, our definition of entailment recognition dif-fers from the definition in existing NLI annotation schemes. We further find that both DeBERTa-v3-base and BERT perform much better than pre-trained NLI models when trained on our dataset. Both models perform similarly, but the fine-tuned DeBERTa-v3-base has slightly better scores on E\u1d4d \u22a8 E\u209c and E\u1d4d \u22ad E\u209c. Finally, we find that our approach of fine-tuning, after pre-training to perform NLI, works well for the coherence classification problem. The results also support our decision to model coherence classification as an entailment recognition problem, as we find that the representations learned by training on NLI datasets help to improve coherence classification accuracy.\nWe further analyze the results separately for E\u1d58 and E\u1d3c\u1d62, as they require reasoning on different types of information (sequential vs. non-sequential). As shown in Table II and Table III, the models generally perform better in classifying E\u1d3c\u1d62, E\u209c pairs. We believe this is because the models have to perform reasoning on sequential information (i.e., the plan) for E\u1d58, which is more difficult than E\u1d3c\u1d62, which is only a set of observations. Nevertheless, these results further support the efficacy of our approach as our models perform much better than the baselines, particularly for \u22ad and \u22a8 classes,"}, {"title": "D. Future Work", "content": "In this work, we have only discussed explanations of robot failures, but the problem of multimodal coherence can be studied beyond failures and explanations, e.g., multimodal communication in HRI. Second, coherence assessment being the focus of this work, we have proposed simple strategies for refining incoherent explanations. Future work can explore more complex strategies, such as re-prompting the LLM with the source of contradiction and dialog-based refinement. Third, we have defined the coherence taxonomy using a sim-ple conjunction of propositions. However, explanations may contain dis-junctions and other complex logical structures which should be studied as well. Finally, we plan to perform user studies to understand the effect of incoherent multi-modal explanations and their refinements using subjective measures."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce and formulate the problem of detecting coherence in multimodal explanations of robot failures. We observe that a simple combination of expla-nations from multiple modalities is not sufficient to pro-duce a coherent explanation. We propose an approach to detect if a pair of explanations is coherent and apply this method to a multimodal explanation generation framework that provides explanations by combining natural language, scene graph, and sequence of actions executed by a robot. In particular, we model coherence assessment as a logical entailment recognition problem and propose to solve it as a classification problem. Our experiments suggest that this modeling is beneficial, as we find that fine-tuning a model that was previously trained to detect textual entailment in other domains is an efficient approach to training an accurate coherence classifier. Further, we propose refinement strate-gies to convert incoherent explanations to coherent ones."}]}