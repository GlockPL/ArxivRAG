{"title": "PROBE PRUNING: ACCELERATING LLMS THROUGH DYNAMIC PRUNING VIA MODEL-PROBING", "authors": ["Qi Le", "Enmao Diao", "Ziyan Wang", "Xinran Wang", "Jie Ding", "Li Yang", "Ali Anwar"], "abstract": "We introduce Probe Pruning (PP), a novel framework for online, dynamic, struc-tured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It com-prises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full in-ference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing using just 1.5% of FLOPs can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\u00d7 lower ratio of performance degradation per unit of runtime reduction com-pared to the state-of-the-art method at a 40% pruning ratio. Our code is available at https://github.com/Qi-Lel/Probe_Pruning.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) (Vaswani et al., 2017; Zhang et al., 2022; Touvron et al., 2023; Diao et al., 2024) have recently achieved significant success, leading to the development of numerous applications (OpenAI, 2023; Anand et al., 2023). However, the inference for these models, often containing billions of parameters, presents challenges. These challenges primarily arise from the substantial computational demands and the risk of high latency (Ma et al., 2023).\nStructured pruning is a promising hardware-friendly approach to reduce computational consumption and accelerate inference (Yuan et al., 2021). It removes complete structures from models, such as weight channels and attention heads. Compared with other methods like unstructured pruning (Frantar & Alistarh, 2023; Sun et al., 2023), parameter sharing (Diao et al., 2019), offloading (Rasley et al., 2020; Diao et al., 2022; 2024), and quantization (Dettmers et al., 2022; Lin et al., 2023; Frantar et al., 2022), structured pruning reduces computational resources and speeds up inference without requiring specific hardware. However, when applied to LLMs, structured pruning often results in a performance gap compared to dense models (Wang et al., 2024b).\nA major factor contributing to the performance gap in LLMs may be the emergence of significant outlier phenomena in internal representations (Dettmers et al., 2022; Liu et al., 2024; Sun et al., 2024). Current advanced structured pruning methods typically utilize calibration datasets to assess the importance of weights using pruning metrics. For example, the FLAP method (An et al., 2024) uses a calibration dataset to compute fluctuation metrics for each input feature and its corresponding channel in attention or MLP block weight matrices, specifically in the output projection (O) or fully connected layer 2 (FC2). Similarly, LLM-Pruner (Ma et al., 2023) employs approximated second-order Taylor expansions of the error function, calculated using a calibration dataset, to eliminate the least important coupled structures. Although the calibration dataset provides valuable insights for pruning by identifying non-critical weights, this approach overlooks the batch-dependent nature of outlier properties in LLMs (Liu et al., 2023b; Song et al., 2023; Liu et al., 2024; Sun et al., 2024), which vary across different input batches and cannot be accurately predicted prior to inference. Experimental illustrations can be found in Appendix B.3. Consequently, pruning decisions based solely on calibration dataset may fail to address these dynamic outliers during real-time inference, resulting in suboptimal model performance. Fine-tuning can serve as a method to recover model performance (Wang et al., 2024b), but it is resource-intensive and may be impractical for certain real-world applications.\nTo effectively handle batch-dependent outliers and reduce the performance gap between pruned and dense models without extensive fine-tuning, we propose Probe Pruning (PP). PP is an online dynamic structured pruning framework that prunes the model during inference based on each batch's hidden states. Notably, PP relies solely on the original model structure and hidden states, without requiring additional neural network modules or fine-tuning. We overcome two key challenges:\n\u2022 Leveraging Calibration Dataset may Introduce Biases: Relying exclusively on the calibration dataset may introduce biases, as the pruned channels are entirely determined by the calibration dataset. For example, when FLAP used the WikiText2 validation set as a calibration dataset, it achieved a perplexity of 18.5 on the WikiText2 test set of LLaMA-2-7B with a 40% pruning ratio. In contrast, using the C4 dataset as a calibration dataset, the perplexity increased to 38.9 on the WikiText2 test set. We propose history-informed pruning with importance-scaled fusion to leverage the benefits of the calibration dataset while minimizing associated biases.\n\u2022 Dynamic Pruning Without Access to Intermediate Hidden States: Deciding online which channels to prune during inference for each batch is challenging. Without gradients, calculating pruning metrics for attention and MLP blocks requires intermediate hidden states, which are the input tensors to the attention output projection and MLP's FC2 layer. These states are unavailable when the input hidden states initially enter these blocks. Moreover, not all samples and tokens contribute equally to the model's output, and large-magnitude outliers in LLMs often have a significant impact on the model's behavior. Therefore, we propose a probing method that selects key samples and tokens from the input hidden states, runs a few model layers ahead, and obtains intermediate hidden state information. Without such probing, accessing intermediate hidden states requires significant computational costs.\nSpecifically, PP leverages a small yet crucial segment of hidden states to run a few model layers ahead and capture the probe's intermediate hidden states, which contain essential information for guiding pruning decisions within the attention or MLP blocks of the current batch. By strategically integrating the probing states with historical states, we can dynamically determine which channels to prune. After pruning the weight channels, we run full inference on the remaining weights. Furthermore, our probing is minimal yet effective: for example, it operates with only 5% of the samples and 50% of the tokens, utilizing just 1.5% of the floating point operations (FLOPs) of dense model inference, and yet it has proven effective. Experimental results confirm that this minimal probe effectively captures critical intermediate hidden state information."}, {"title": "2 RELATED WORK", "content": "Pruning with Calibration Dataset. Pruning methods can be broadly classified into two cate-gories (Yuan et al., 2021): unstructured pruning and structured pruning. Unstructured pruning (Le-Cun et al., 1989; Hassibi et al., 1993; Han et al., 2015; Mushtaq et al., 2021; Li et al., 2021; Soltani et al., 2021; Yang et al., 2022; Diao et al., 2023; Liu et al., 2023a; Li et al., 2024b;a; Dong et al., 2024) removes individual weights, whereas structured pruning (Li et al., 2016; Liu et al., 2017; He et al., 2019; Diao et al., 2020; Fang et al., 2023) removes complete structures from the model, such as channels and attention heads. Pruning LLMs often involves calibration datasets due to the emergence of outliers in their internal representations. For unstructured pruning, SparseGPT (Frantar & Alistarh, 2023) uses synchronized second-order Hessian updates to solve row-wise weight reconstruction prob-lems and update weights. Wanda (Sun et al., 2023) introduces a pruning metric that considers both the magnitude of weights and activation values to determine which weights to prune. For structured pruning, FLAP (An et al., 2024) introduces a fluctuation metric to decide which weight channels to prune. LLM-Pruner (Ma et al., 2023) employs approximated second-order Taylor expansions of the error function to remove the least important coupled structures and then applies fine-tuning to recover model performance. LoRA-Prune (Zhang et al., 2023) uses a LoRA (Hu et al., 2021)-guided pruning metric that leverages the weights and gradients of LoRA to direct the iterative process of pruning and tuning. However, the fine-tuning process requires substantial computational resources (Hoffmann et al., 2022), and we have found that fine-tuning might cause LLMs to lose their generalizability; for example, they may perform worse on certain downstream tasks, such as commonsense reasoning tasks.\nLarge-Magnitude Outliers of LLMs. Unlike small neural networks, LLMs exhibit large-magnitude outlier features (Kovaleva et al., 2021; Dettmers et al., 2022; 2023; Schaeffer et al., 2024; Sun et al., 2024). Dettmers et al. (2022) shows that these large-magnitude features begin to emerge when the size of LLMs exceeds 6.7 billion parameters, and these outlier features are concentrated in certain channels. The phenomenon of massive activations (Sun et al., 2024; Liu et al., 2024) has been observed, where a few activations exhibit significantly larger values than others, potentially leading to the concentration of attention probabilities on their corresponding tokens. These emergent properties suggest the need to customize the pruning of channels for different batches to maintain model performance. This observation motivates us to propose Probe Pruning."}, {"title": "3 NOTATIONS AND PRELIMINARIES", "content": "An LLM M consists of L blocks, each of which can be either an attention block or a Multi-Layer Perceptron (MLP) block. Each attention block is characterized by four linear projections: Query (Q), Key (K), Value (V), and Output (O). Similarly, each MLP block includes two linear layers: Fully Connected layer 1 (FC1) and Fully Connected layer 2 (FC2).\nEach block l transforms the input hidden state $X^l \\in \\mathbb{R}^{N\\times S\\times D}$ into the output hidden state $X^{l+1} \\in \\mathbb{R}^{N\\times S\\times D}$. Here, N, S, and D denote the batch size, sequence length, and feature dimension, respectively. The transformations in each block l can be expressed as:\n$X^{l+1} = X^l + F^l(X^l),$ (1)\nwhere $F^l$ encompasses all transformations within block l. This function can be further decomposed as:\n$F(X^l) = X^{l,int} (W^{l,final})^T, X^{l,int} = T^l(LN^l(X^l)),$ (2)\nwhere $T^l$ represents all intermediate transformations applied to the input hidden state $X^l$, excluding the layer normalization $LN^l$ and final weight matrix $W^{l,final} \\in \\mathbb{R}^{C_{out}\\times C_{in}}$. The final weight matrix is either the Output projection (O) in an attention block or FC2 in an MLP block. The intermediate hidden state $X^{l,int} \\in \\mathbb{R}^{N\\times S\\times C_{in}}$ results from applying these intermediate transformations to $X^l$. Additionally, we define the residual importance as the L2 norm of the input hidden states $X^l$ across specific dimensions, a concept further detailed in Section 4.2."}, {"title": "4 METHODOLOGY", "content": "The objective of Probe Pruning (PP) is to implement online dynamic structured pruning in a batch-wise manner. The main idea of our work is illustrated in Figure 1. Our core strategy involves: (1) Probing (Sections 4.1 and 4.2), which consists of two steps: first, generating a probe based on residual importance; second, using the probe to run the unpruned model to gather valuable intermediate hidden state information. (2) History-informed pruning (Section 4.3), which carefully merges the probing states with historical states using importance-scaled fusion to capture the essential characteristics of each batch. Afterward, we prune the model using a novel pruning metric (Section 4.4) that more effectively selects channels for pruning than existing metrics."}, {"title": "4.1 PROBING", "content": "We introduce a novel concept called probing, which leverages the existing model structure and hidden states to form a predictive mechanism. Specifically, when the input hidden states reach block l, probing first utilizes residual importance to select key samples and tokens, forming the probe $P^l$ from $LN^l(X^l)$. $LN^l$ represents the layer normalization at block l. The process of probe generation is detailed in the next section. It then runs the intermediate transformation in block l, denoted by $T^l(P^l)$. Notably, effective probing consumes few computational resources and can obtain important intermediate-state information to guide pruning decisions.\nUpper Bound of Probing. As an alternative, we can generate the probe by using all the input hidden states in the current batch, $P' = LN^l(X^l)$, a method we refer to as Full-Batch Probing. By utilizing the entire batch without reducing the dimensions N or S, Full-Batch Probing captures the complete intermediate hidden state information, which could potentially lead to optimal pruning performance. However, this approach significantly increases computational resource requirements and latency. Therefore, Full-Batch Probing serves as a theoretical upper bound for our method. Our aim for PP is to select pruning channels similar to those chosen by Full-Batch Probing. We believe that a higher proportion of common pruning channels between PP and Full-Batch Probing indicates better model performance and higher quality of the probe.\nWhy Does Probing Work? Probing is effective because not all samples and tokens contribute equally to the model's output, and large-magnitude outliers in LLMs significantly influence the model's behavior. In natural language sequences, certain tokens carry more semantic or syntactic significance than others (Xiao et al., 2023; Sun et al., 2024; Liu et al., 2024). By selecting key samples and tokens based on residual importance, the probe focuses on the most informative parts within the batch. This targeted approach allows the probe to capture essential intermediate hidden state information that is most influential in determining which channels can be pruned. Consequently, even though the probe processes a reduced subset of the batch, it provides sufficient insight to guide pruning decisions, potentially achieving results comparable to Full-Batch Probing with significantly lower computational costs.\nComputational Complexity. Only minimal computational complexity is required for probing. Specifically, for an LLM characterized by six linear transformations per attention and MLP block (Q/K/V/O and FC1/FC2) that incorporate weight transformations and the attention mechanism, the dense matrix computational complexity for an LLM totals $O(6NSC_{in}C_{out} + 2NS^2C_{in})$. For probing, by reducing the batch size to x% and the sequence length to y% of their original sizes, the complexity reduces to $O(4x\\%\\cdot y\\%\\cdot NSC_{in}C_{out} + 2x\\% (y\\%)^2 \\cdot NS^2C_{in})$."}, {"title": "4.2 PROBE GENERATION", "content": "PP measures the residual importance, which is the L2 norm of $X^l$ across specific dimensions to identify key samples and tokens. Once identified, these key samples and tokens are selected from $LN^l(X^l)$ to generate a probe for block l, where $LN^l$ denotes layer normalization at block l. We do not utilize the importance derived from $LN^l(X^l)$ to identify key samples and tokens because layer normalization substantially alters the input hidden states.\nTo measure the residual importance along the target dimension, we compute the $L_2$ norm of $X^l$ across non-target dimensions. The target dimension may be either the batch or sequence dimension.\n$U_i^{l,batch} = ||X_{i,:,:}^l||_2$, for i = 1, ..., N, (4)\n$U_j^{l,seq} = ||X_{:,j,:}^l||_2$, for j = 1,..., S. (5)\nAfter computing the importance scores, we sort them in descending order and store the indices in I:\n$I^{l,batch} = argsort(-u^{l,batch}),$ (6)\n$I^{l,seq} = argsort(-U^{l,seq}).$ (7)\nUsing the sorted indices, we then generate the probe by selecting the top x% of samples or y% of tokens from the layer-normalized $X^l$:\nP^l = \\begin{cases} LN(X^l)_{I^{l, batch}:x\\%,:,:}, & \\text{if selecting top x\\% of samples}, \\\\ LN(X^l)_{:,I^{l, seq}:y\\%,:}, & \\text{if selecting top y\\% of tokens}. \\end{cases} (8)\nThis method ensures that the probe consists of the most significant samples and tokens, as ranked by their importance scores.\nPP implements a sequential approach to prune both sequence and batch dimensions effectively. Initially, the top y% of tokens are selected from the residual $X^l$, guided by Eqs. (5) and (7), leveraging the sequence distribution within the current batch: $X^l_{:,:, I^{l,seq}:y\\%}$. Subsequently, we apply this reduced sequence set to determine the top x% of samples using Eqs. (4) and (6), resulting in the indices $I^{l,batch}|_{I^{l,seq}}$. Finally, we select the key samples and tokens for probe generation as $LN^l(X^l)_{I^{l,batch}|_{I^{l,seq}}:x\\%, I^{l,seq}:y\\%,:}$."}, {"title": "4.3 HISTORY-INFORMED PRUNING WITH IMPORTANCE-SCALED FUSION", "content": "The intermediate hidden states of the probe, given by\n$X^{l,int,probe} = T^l(P^l)$ (9)\ncontain crucial information that guides pruning decisions. However, when the probe is very small-for instance, when N and S are reduced to 5%-they might lead to inappropriate pruning decisions due to limited context. To address this issue and enhance performance, we introduce history-informed pruning with importance-scaled fusion.\nTo simplify notation, we omit the superscript l, which denotes the block number, in this section. For intermediate hidden states $X^{int}$ of shape (N, S, $C_{in}$), the following relationship holds:\n$\\sum_{j=1}^S \\sum_{i=1}^N (X_{i,j,k}^{int})^2 = \\sum_{j=1}^S ||X_{:,j,k}^{int}||^2 = ||V_k||^2$ (10)\nWe compress the batch dimension in the first step of Eq. 10 to store historical states because memory limitations prevent storing the intermediate hidden states of all samples. We sum over the sequence dimension in the second step of Eq. 10 to obtain the tensor in shape $\\mathbb{R}^{C_{in}}$, which is used to compute the pruning metric (see Section 4.4).\nAs in previous studies (Sun et al., 2023; An et al., 2024), we process the calibration dataset D using the model M to obtain initial historical states. For each block, initial historical states are represented by $V^0 \\in \\mathbb{R}^{S\\times C_{in}}$, computed as the first step of Eq. 10 to reduce the batch dimension: $V_k^0 = ||X_{:,j,k}^{int}||^2 = \\sum_{i=1}^N (X_{i,j,k}^{int})^2$. Similarly, to reduce the batch dimension of probe's intermediate hidden states $X^{int,probe} \\in \\mathbb{R}^{x\\%\\cdot N\\times y\\%\\cdot S\\times C_{in}}$, we calculate probing states as $||X_{j,k}^{int, probe}||^2 = \\sum_{i=1}^{x\\% \\cdot N} (X_{i,j,k}^{int, probe})^2$\nImportance-Scaled Fusion. Since probing can be performed with selected tokens, it is necessary to align the sequence dimension. We define $V_k^{probe} = V_{I^{l,seq}:,:k}$, where $V^{probe} \\in \\mathbb{R}^{y\\% \\cdot S\\times C_{in}}$ and $I^{l,seq}$, obtained from Eq. 7, represents the indices of the top y% of tokens. We then apply importance-scaled fusion to obtain integrated states:\n$\\bar{X}_{j,k}^{int, probe} = \\frac{||X_{j,k}^{int, probe}||}{\\sum ||X_{j,k}^{int, probe}|| + V_k^{probe}} X_{j,k}^{int, probe} + \\frac{V_k^{probe}}{\\sum ||X_{j,k}^{int, probe}|| + V_k^{probe}} V_k^{probe},$ (11)\nwhere $X^{int, probe}_{j,k} \\in \\mathbb{R}^{y\\%\\cdot S\\times C_{in}}$. Following the second step of Eq. 10, we sum $X^{int, probe}$ over the sequence dimension to obtain $\\sum_{j=1}^{y\\%\\cdot S} X_{j,k}^{int, probe}$. Note that without importance-scaled fusion, $\\sum_{j=1}^{y\\%\\cdot S} X_{j,k}^{int, probe}$ can reduce to $\\frac{X_{j,k}^{int, probe}}{||X||}$. Then, we use $W^{final}$ and $\\sum_{j=1}^{y\\%\\cdot S} X_{j,k}^{int, probe}$ as a surrogate of $||X_{:,:,k}^{int}||^2$ to calculate the pruning metric based on Eq. (15), and prune the weight channels accordingly. Finally, we run full inference on the remaining weights.\nUpdate Historical States with Full Inference. To enhance the tracking of intermediate hidden state attributes, we implement an exponential moving average during full inference on the selected weight channels C. The update formula is expressed as:\n$V_{:, C}^t = \\lambda V_{:, C}^{t-1} + (1-\\lambda)||X_{:,c}^{int}||^{2*},$ (12)\nThe value of V is updated for t-th inference batch, and $X^{int}$ represents the intermediate hidden states during full inference. We consistently set $\\lambda$ = 0.99 across all implementations."}, {"title": "4.4 PRUNING METRIC", "content": "We propose a new structured pruning metric named PPsp, where \"sp\" stands for structured pruning. This metric more effectively selects channels for pruning compared to existing metrics. We adapt the unstructured pruning metric Wanda (Sun et al., 2023) to a structured pruning scenario. PPsp introduces two enhancements: (1) we preserve the inherent importance of individual weights, as represented by the squared value of the Wanda metric; and (2) we calculate the $L_2$ norm of the importance scores for MLP input channels and attention heads to determine the pruning structures' importance, rather than summing these scores across pruning structures.\nWe introduce the pruning metric for a general scenario. To enhance clarity, we omit the superscript l, which denotes the block number, in this section. At each block, given intermediate hidden states $X^{int}$ of shape (N, S, $C_{in}$), where N and S represent the batch and sequence dimensions respectively, and the weight matrix $W^{final}$ of shape ($C_{out}$, $C_{in}$), Wanda (Sun et al., 2023) defines the importance of the individual weight $W_{i,k}^{final}$ as:\n$l_{i,k} = |W_{i,k}^{final}| \\cdot ||X_{:,:,k}^{int}||_2,$ (13)\nwhere $||$ denotes the absolute value operation, and $||X_{:,:,k}^{int}||_2$ evaluates the $L_2$ norm of the kth feature across the (N, S) dimensions. These two scalar values are then multiplied to produce the final importance. However, as derived in Wanda (Sun et al., 2023), the inherent importance of an individual weight is defined by:\n$l_{i,k} = (|W_{i,k}^{final}| \\cdot ||X_{:,:,k}^{int}||_2)^2 = |W_{i,k}^{final}|^2 \\cdot ||X_{:,:,k}^{int}||_2^2.$ (14)\nWanda discards the squaring in Eq. (14) in local weight importance ordering, as the non-negative nature of $|W_{i,k}^{final}|$ and $||X_{:,:,k}^{int}||_2$ does not impact the relative ordering of importance. However, when it comes to structured pruning, maintaining the inherent importance of individual weights is essential. Thus, we square the Wanda metric and compute the Euclidean distance across the $C_{out}$ dimension of the input channel. The formula is given by:\n$I_k = \\sqrt{\\sum_{i=0}^{C_{out}} \\{|W_{i,k}^{final}|^2 \\cdot ||X_{:,:,k}^{int}||_2^2\\}_i},$ (15)\nwhere {\u00b7} signifies the set of elements, and $I \\in \\mathbb{R}^{C_{in}}$."}, {"title": "5 EXPERIMENTAL SETUP", "content": "We conduct three experiments using different random seeds for all tests and show the standard error across these three seeds in brackets. We conduct all experiments on NVIDIA A100 GPUs.\nModels and Evaluation. We evaluate PP on three popular model families: LLaMA-2 7B/13B (Tou-vron et al., 2023), LLaMA-3 8B (Meta AI, 2024), and OPT-13B (Zhang et al., 2022). Following previous work (Sun et al., 2023; An et al., 2024), we evaluate the models on two zero-shot task categories. We evaluate accuracy on commonsense reasoning tasks, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), and OpenbookQA (Mihaylov et al., 2018). For evaluating perplexity on the text generation task, we use WikiText2 (Merity et al., 2016). We set the batch size to 20 for all tasks. For the commonsense reasoning tasks, our implementation follows (Gao et al., 2021), setting the sequence length of each batch to match its longest sample. For the text generation task, we set the sequence length to 1024. For PP, we set the default probe size to 5% of the batch size and 50% of the sequence length, approximating 1.5% of the FLOPS cost relative to dense model inference. Ablation studies of the PP and FLAP are available in Appendix B, and additional experimental results are available in Appendix C.\nBaselines. We compare our method, PP, with four previous approaches: Wanda-sp (An et al., 2024), FLAP (An et al., 2024), LoRAPrune (Zhang et al., 2023), and LLM-Pruner (Ma et al., 2023). We also compare PP with its upper bound, Full-Batch Probing, as introduced in Section 4.1. Following (Sun et al., 2023; An et al., 2024), we use the C4 (Raffel et al., 2020) dataset as the calibration dataset for all methods. We use 2,000 calibration samples for PP, Wanda-sp, and FLAP, and 20,000 calibration samples for tuning LoRAPrune and LLM-Pruner. We evaluate pruning ratios of 20% and 40%."}, {"title": "6 RESULTS", "content": "Main Results. We present the zero-shot performance, without fine-tuning, of four models on text generation and commonsense reasoning tasks. Probe Pruning (PP) consistently outperforms all baselines across various models and pruning ratios. For instance, on WikiText2 at a 40% pruning ratio, PP achieves lower perplexities than competing methods: 16.8 with LLaMA-2-7B, 11.3 with LLaMA-2-13B, and 26.7 with OPT-13B. Moreover, PP attains significantly lower perplexities and higher reasoning task accuracies than both LLM-Pruner and LoRAPrune. For example, on LLaMA-2-13B at a 40% pruning ratio, PP achieves an average accuracy of 61.0%, significantly higher than 52.0% for LLM-Pruner and 48.1% for LoRAPrune.\nPP surpasses Wanda-sp and FLAP in nearly all tasks, confirming its effectiveness and robustness. For instance, at a 40% pruning ratio, PP achieves an average accuracy of 58.9%, outperforming Wanda-sp (53.3%) and FLAP (53.6%). In Section 4.1, we stated that Full-Batch Probing represents the upper bound of PP. Experimental results confirm that Full-Batch Probing excels in all tested scenarios, supporting our hypothesis. Compared to Full-Batch Probing, which requires significant extra computational resources-more than dense model inference-PP achieves comparable results while utilizing minimal computational resources, only 1.5% of the FLOPs compared to dense model inference. These results imply the effectiveness of PP and demonstrate that the probe's intermediate hidden states can help identify the important weights for processing different batches."}, {"title": "Performance Runtime Ratio", "content": "To illustrate the trade-off between model performance and inference speed, we introduce Performance Runtime Ratio (PRR), which quantifies the ratio of performance degradation per unit of runtime reduction. Importantly, a smaller PRR value is preferable as it indicates minimal performance degradation per unit of runtime reduction. The metric is defined as:\n$PRR = \\frac{Perf_{dense} - Perf_{pruned}}{Runtime_{dense} - Runtime_{pruned}},$ (16)\nwhere $Perf_{pruned}$ and $Runtime_{pruned}$ denote the performance and runtime of the pruned model, respectively, and $Perf_{dense}$ and $Runtime_{dense}$ denote the performance and runtime of the dense model, respectively. As shown in Table 5, the PRR of PP is 37.37, indicating a increase of 37.37 in perplexity per second of runtime reduction on the attention and MLP block. In comparison, FLAP and Wanda-sp have PRR values of 95.65 and 106.48, respectively. PP's PRR values are 2.56\u00d7 (95.65 compared to 37.37) and 2.85\u00d7 (106.48 compared to 37.37) more efficient than those of FLAP and Wanda-sp, respectively, indicating a significantly lower rate of performance degradation.\nCompared with Fine-tuned Baselines. Table 6 compares the performance of PP with fine-tuned baselines LoRAPrune and LLM-Pruner across different pruning ratios for text genera-tion and commonsense reasoning tasks. Without fine-tuning, PP consistently outperforms or closely matches the fine-tuned models. At a 20% pruning ratio, PP excels in both tasks across LLaMA-2-7B and LLaMA-2-13B models. At a 40% pruning ratio, PP achieves compara-ble perplexity and significantly higher reasoning task accuracies. For example, PP achieves 61 on LLaMA-2-13B, while LoRAPrune achieves 55.5 and LLM-Pruner achieves 54.7.\nImportance-Scaled Fusion. We compare importance-scaled fusion to three fixed integration ratios 0.1, 0.5, and 0.9\u2014which assign a fixed ratio to the probing states during integration with his-torical states. We conduct experiments on LLaMA-2-7B using the WikiText2 dataset at a 40% pruning ratio, keeping the probe batch size fixed at 1. The results in Figure 4 demonstrate that importance-scaled fusion can leverage the benefits of the calibration dataset while minimizing associated biases.\nPruning Metric. Our PPsp consistently outperforms both Wanda-sp and FLAP across various pruning scenarios. We conduct exper-iments on fix-pruned models, each uniquely generated by one of three evaluated metrics, using only the calibration dataset. we evaluated three metrics at a uniform 40% pruning ratio across all blocks on the WikiText2 dataset. As shown in Table 7, PPsp significantly reduces perplexity, achieving the lowest scores of 29.7 and 35.5 on the LLaMA-2-7B and OPT-13B models, respectively, compared to FLAP's 38.2 and 41.1, and Wanda-sp's 43.8 and 42.7."}, {"title": "7 CONCLUSION", "content": "In this paper, we propose Probe Pruning (PP), a novel online dynamic pruning framework that uses a small yet crucial portion of hidden states to run the model and gain crucial pruning information to guide full inference. Notably, PP only relies on the original model structure and hidden states, without requiring additional neural network modules, or fine-tuning. Furthermore, PP consistently surpasses all baselines, including those with fine-tuning in almost all experimental settings. Future research directions include refining the probe generation and probing process, integrating PP with advanced decoding and alignment techniques (Wang et al., 2025), and exploring its robustness against poisoned models (Xian et al., 2023a;b; Wang et al., 2024a) or adversarial prompts (Xian et al., 2025)."}, {"title": "A IMPLEMENTATION DETAILS", "content": "For all methods, we leave the first three layers unchanged, similar to Ma et al. (2023); Zhang et al. (2023), because pruning parameters in these layers has a substantial impact on the model. The pruning ratio represents the average pruning ratio across all attention and MLP blocks in the model. For instance, when targeting pruning ratios of 20% and 40% for LLaMA-2-"}]}