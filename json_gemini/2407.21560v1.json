{"title": "Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding", "authors": ["Jun Zhou", "Dongyang Yu", "Kamran Aziz", "Fangfang Su", "Qing Zhang", "Fei Li", "Donghong Ji"], "abstract": "Fine-grained sentiment analysis involves extracting and or-ganizing sentiment elements from textual data. However, existing ap-proaches often overlook issues of category semantic inclusion and over-lap, as well as inherent structural patterns within the target sequence. This study introduces a generative sentiment analysis model. To address the challenges related to category semantic inclusion and overlap, a la-tent category distribution variable is introduced. By reconstructing the input of a variational autoencoder, the model learns the intensity of the relationship between categories and text, thereby improving sequence generation. Additionally, a trie data structure and constrained decoding strategy are utilized to exploit structural patterns, which in turn reduces the search space and regularizes the generation process. Experimental re-sults on the Restaurant-ACOS and Laptop-ACOS datasets demonstrate a significant performance improvement compared to baseline models. Ab-lation experiments further confirm the effectiveness of latent category distribution and constrained decoding strategy.", "sections": [{"title": "Introduction", "content": "Sentiment analysis, especially the fine-grained Aspect-Based Sentiment Analysis (ABSA), plays a vital role in Natural Language Processing (NLP) [11]. ABSA aims to convert unstructured text into structured records to achieve a profound comprehension of sentiment semantics. For instance, in the comment \"as far as gaming performance, the m370x does quite well\", the ABSA quadruple extraction task [3] seeks to extract aspect-level sentiment elements, including aspect, category, opinion, and sentiment polarity, represented as a structured quadruple (m370x, GRAPHICS#OPERATION_PERFORMANCE, well, POS). Given its"}, {"title": "Task Definition", "content": "The objective of sentiment quadruple extraction is to extract structured quadru-ples (a, c, o, s) from a given comment X = [x1,...,xN] [3]:\nX\u21d2 [(a1, C1, 01, S1), (A2, C2, O2, $2), ...] (1)\nwhere a represents the aspect, c denotes the category to which the aspect belongs, o signifies the opinion, and s indicates the sentiment polarity of the opinion. Since this study adopts a generative manner to the extraction task, using a Sequence-to-Sequence (Seq2Seq) framework to convert X into a linearized augmented language sequence Y [8], the task is specifically transformed into:\nX \u2192 Y = [Y1, Y2,... (2)\nwhere Y is a word sequence consisting of the essential elements of the quadruples, which can be extracted by parsing Y."}, {"title": "Model Structure", "content": ""}, {"title": "Latent Category Distribution", "content": "Latent Category Distribution (LCD) module (refer to Figure 1) models category distribution within text following Fei et al. [5], using a latent variable instead of explicit measurement. It reconstructs the variational autoencoder input to learn the LCD representation Z.\nEncoder The input to the LCD module is the Category Bag of Words (CBoW) features [5] XCbow \u2208 RL. The construction of CBoW features involves: 1) Re-moving stopwords, meaningless words, and sentiment-related words from texts while preserving category- and aspect-related words to construct a CBOW vo-cabulary of length L. This step aims to ensure that representation Z contains comprehensive category information rather than general semantic information, while decreasing the BoW vocabulary size is beneficial for VAE training [5]. 2)"}, {"title": "Encoder", "content": "\u03bc = f(XCBOW) (3)\nlogo = fo(XCBOW) (4)\nFollowing Bowman et al. [1], a latent variable Z' = \u03bc + \u03c3\u00b7\u03b5\u2208 RK is defined to represent the latent category distribution, where K is the number of categories, e is a Gaussian noise variable sampled from N(0,1). Z' is then normalized as follows:\nZ = softmax(Z') (5)\nConsequently, the latent category distribution can be reflected by variable Z.\nDecoder The LCD module employs variational inference [10] to approximate a posterior distribution over Z. It begins by applying a linear transformation fied() to Z:\nRled = flcd(Z, Wied) (6)\nwhere Wied \u2208 RK\u00d7dim, dim is the encoding dimension. Ried will be fed into the generative framework (Section 4.2) with category distribution information to enhance the generation. Subsequently, a FFNS fr() is employed to further decode Rled into R:\nR = fR(Red, WR) (7)"}, {"title": "Loss Function", "content": "XCBOW = softmax(R) (8)\nParameters of LCD module are learned through the optimiza-tion of the variational lower bound on the marginal log likelihood of features:\nlog py(X) = Ez~q\u2084(Z\\x) [log py(X|Z)] \u2013 KL(q\u00a2(Z|X)||p(Z)) (9)\nwhere and are parameters of the encoder and decoder, respectively. The KL divergence term ensures that the distribution q\u2084(Z|X) approaches the prior probability p(Z), while p\u03c6(X|Z) represents the decoding process."}, {"title": "Generative Framework", "content": ""}, {"title": "Encoder", "content": "The generative framework (refer to Figure 1) employs a T5 encoder [14] for input encoding. Given a comment X = [x1,\u2026,XN], the encoder first converts the token sequence into a high-dimensional vector representation E \u2208 RNxdim using an internal embedding layer, where N is the sequence length, and dim is the dimension of the representation.\nSubsequently, E is passed through the T5 encoder which comprises multiple layers of Transformer [15] structure, to calculate context-aware representations:\nHenc = T5Encoder(E) (10)\nwhere Henc\u2208RNxdim"}, {"title": "Category-Text Attention", "content": "After encoding the input sequence into represen-tation Henc, the model computes attention score of the latent category distri-bution over the input sequence, to establish the relationship between them. The attention score is calculated as follows:\nej =\ndot(Rlcd, Henc)\n\u221adim (11)\nwhere dot() is the dot product, and j\u2208 [1, N]. Following this, e is fed to a softmax function to calculate the attention weights:\na = softmax(e) (12)\na is then applied as weights to the representation of the input sequence to obtain the output vector V of the encoder:\nVj = a; Henc (13)\nThe vector V, representing the input text and incorporating LCD information, is then fed into the decoder to provide contextual information for decoding."}, {"title": "Decoder", "content": "Hdec = T5Decoder(V, y<i) (14)\nHere, each T5Decoder(\u00b7) layer comprises a Transformer structure and a cross-attention mechanism [15] for enhancing the representation of Hdec. The proba-bility distribution over target vocabulary at step i is calculated as follows:\nP(yi|y<i, V) = Softmax(WHdec + b) (15)\nwhere W \u2208 R|V|\u00d7dim, V is the target vocabulary, and b is a bias vector.\nThe decoder repeats this procedure until it encounters the end token \u201c(eos)\u201d or reaches the maximum specified output length."}, {"title": "Loss Function", "content": "The loss function of the generative framework is defined as the cross-entropy loss between the generated sequence and the target sequence:\nL(0) =\\frac{\\omega}{i=1} log P(yiy<i, V (16)\nwhere represents the parameters of the generative framework, and M is the length of the target sequence."}, {"title": "Constrained Decoding", "content": ""}, {"title": "Target Sequence Pattern Knowledge", "content": "In generative models, a commonly used decoding approach is greedy decoding [8], where the model calculates the probability distribution over the entire vocabulary at each decoding step, and selects the word with the highest predicted probability as the output.\nHowever, this decoding strategy may lead to invalid structure patterns. For instance, 1) due to the limited number of category types, greedy decoding us-ing the entire vocabulary may yield invalid categories, 2) since each category corresponds to distinct subcategories, greedy decoding might generate subcat-egories that do not align with that category, 3) as sentiment polarity is finite, greedy decoding could result in invalid sentiment polarities, 4) incomplete se-quence structures, such as the failure to generate pairs of brackets, may occur. Following Lu et al. [8], information regarding the structural patterns of the tar-get sequence, which can be used to regulate its generation process, is referred to as target sequence pattern knowledge in this paper."}, {"title": "Constrained Decoding Strategy", "content": "Pattern knowledge of the target sequence can constrain the decoding process, reducing the search space and enhancing"}, {"title": "Experiments", "content": ""}, {"title": "Datasets", "content": "Two datasets, Restaurant-ACOS [3] and Laptop-ACOS [3], are used to evalu-ate model performance. Each sample comprises a review text and annotated sentiment quadruples, as summarized in Table 1.\nThese datasets also introduce the concept of implicit elements, where aspects or opinions within the review are considered implicit if they are not explicitly stated. For example, in the review \u201capparently apple isn't even trying anymore\u201d, the annotated quadruple (apple, COMPANY#GENERAL, Null, NEG) indi-cates that the opinion towards the aspect \"apple\" is not explicitly stated in the review and is considered implicit, labeled as \"Null\" in the annotation.\nThe statistics of implicit elements is shown in Table 2. Here, EA&EO repre-sents quadruples with only explicit aspects and opinions, IA&EO indicates those with implicit aspects and explicit opinions, EA&IO covers quadruples with ex-plicit aspects and implicit opinions, and IA&IO denotes quadruples with only implicit aspects and opinions."}, {"title": "Experimental Setup", "content": "A GeForce RTX 3090 (24GB) was utilized for computation. T5-base model was employed with a training batch size of 16, and encoding and hidden layer di-mensions set to 768. The AdamW optimizer was employed with a learning rate of 2e-5, and dropout was applied at a rate of 0.2. Experiments were initialized five times randomly, and the average result was considered the final outcome."}, {"title": "Baselines", "content": "The subsequent models were employed for comparative analysis:\nExtractive: DP-ACOS [3]: Utilizes the DP algorithm to extract sentiment triplets (a, o, s) and then determines the category of each triplet. JET-ACOS [3]: Employs JET [17] to extract triplets (a, o, s) and then predicts category of each triplet. TB-ACOS [3]: Utilizes TAS-BERT [16] for aspect-opinion joint ex-traction based on category-sentiment, then filters out invalid pairs. EC-ACOS [3]: Performs aspect-opinion joint extraction, then predicts category-sentiment pairs. SGTS [23]: Captures high-dimensional features of sentences, then uses grid labeling scheme and its decoding method for extraction.\nGenerative: BARTABSA [18]: A model based on BART [7], generates sen-timent quadruples by predicting the indices of target words and sentiment cate-gories. GAS [21]: Using T5, GAS models sentiment analysis and various subtasks as generative tasks. Paraphrase [20]: Transforms the quadruple extraction task into a paraphrase generation process using predefined templates. Seq2Path [9]: Formalizes the generated content as a tree, with sentiment tuples representing the paths of the tree."}, {"title": "Experimental Results and Analyses", "content": ""}, {"title": "Main Results", "content": "Table 3 presents the experimental results of the proposed model and baseline models on two datasets. The results highlight the superiority of generative mod-els over extractive models. This advantage can be attributed primarily to two factors: 1) treating the extraction task as an end-to-end sequence generation problem reduces error propagation; 2) uniformly modeling the generation of each quadruple element facilitates more effective utilization of semantic knowledge in pretrained models, enhancing semantic sharing and decision interaction.\nFurthermore, the results in Table 3 suggest that the proposed model, utilizing LCD and CD, outperforms the comparing generative models on both datasets. Specifically, on the Restaurant-ACOS dataset, compared to the baseline model GAS, the proposed model shows a decrease of 0.004 in Precision, but an increase"}, {"title": "Implicit Element Extraction Results", "content": "As noted in Section 5.1, both datasets include implicit elements [3], which are not explicitly stated in the reviews. These elements necessitate a comprehensive understanding of the text and present a challenge for models, thus attracting considerable attention in recent sentiment analysis research [20, 23].\nTo assess the capacity of models in extracting implicit elements, the test set was divided into four subsets: EA&EO, IA&EO, EA&IO, and IA&IO, each targeting specific type of quadruples, whose meanings were introduced in Sec-tion 5.1. Results in Table 4 indicate that models excel in extracting explicit ele-ments compared to implicit ones. For instance, on the Restaurant-ACOS dataset, the proposed model achieves an F1 score of 0.661 on the EA&EO subset, which is comprised solely of explicit elements, significantly outperforming scores of 0.544, 0.442, and 0.413 on the other three subsets containing implicit elements. Similar trends are also observed on the Laptop-ACOS dataset. These results highlight the challenge of extracting implicit elements."}, {"title": "Ablation Study", "content": "To assess the effectiveness of the proposed LCD module and CD strategy, ab-lation experiments were conducted on the datasets. Results in Table 5 indicate the impact of removing the LCD module (w/o LCD), excluding the CD strategy (w/o CD), and simultaneously excluding both components (w/o LCD&CD).\nAs shown in Table 5, performance deteriorates when LCD and CD are ex-cluded. Specifically, on the Restaurant-ACOS dataset, the removal of LCD alone resulted in an F1 decrease of 0.009, while excluding CD alone led to an F1 de-crease of 0.012. Simultaneous removal of both components resulted in an F1 de-crease of 0.014. Similar trends were also observed on the Laptop-ACOS dataset.\nThese results suggest that the proposed LCD module enhances the extraction performance by capturing latent category features, while the CD strategy regu-lates and guides the generation process, mitigating the generation of erroneous patterns."}, {"title": "Visualization of Latent Category Distribution", "content": "As introduced in Section 4.1, the LCD module learns latent category distribu-tion representation Z for a sample. To visually illustrate the effectiveness of this module, five samples were selected with their learned LCD representations visu-alized in Figure 3. The x-axis denotes the total 23 categories, while the y-axis indicates the sample indices. Each row in the figure corresponds to a LCD rep-resentation vector of a sample across categories, with color intensity reflecting the magnitude of the vector in respective category. The five samples, along with their respective standard categories in parentheses, are as follows:\n1. the product is great, but the customer support is horrible. (LAPTOP, SUP-PORT)\n2. second issue is with scaling of the ui. (SOFTWARE)\n3. asus, has a horrible reputation. (COMPANY)\n4. hdmi out doesn't work right. (PORTS)\n5. now i have to deal with warranty stuff and sending it back. (WARRANTY)"}, {"title": "Conclusion", "content": "This study introduces a generative model for extracting sentiment quadruples. To address challenges related to semantic inclusion and overlap among categories, the model incorporates a latent category distribution to capture the strength of"}]}