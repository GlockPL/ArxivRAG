{"title": "NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls", "authors": ["Kinjal Basu", "Ibrahim Abdelaziz", "Kelsey Bradford", "Maxwell Crouse", "Kiran Kate", "Sadhana Kumaravel", "Saurabh Goyal", "Asim Munawar", "Yara Rizk", "Xin Wang", "Luis Lastras", "Pavan Kapanipathi"], "abstract": "Autonomous agent applications powered by large language models (LLMs)\nhave recently risen to prominence as effective tools for addressing com-\nplex real-world tasks. At their core, agentic workflows rely on LLMs to\nplan and execute the use of tools and external Application Programming\nInterfaces (APIs) in sequence to arrive at the answer to a user's request.\nVarious benchmarks and leaderboards have emerged to evaluate an LLM's\ncapabilities for tool and API use; however, most of these evaluations only\ntrack single or multiple isolated API calling capabilities. In this paper, we\npresent NESTFUL, a benchmark to evaluate LLMs on nested sequences\nof API calls, i.e., sequences where the output of one API call is passed as\ninput to a subsequent call. NESTFUL has a total of 300 human annotated\nsamples divided into two types - executable and non-executable. The ex-\necutable samples are curated manually by crawling Rapid-APIs whereas\nthe non-executable samples are hand picked by human annotators from\ndata synthetically generated using an LLM. We evaluate state-of-the-art\nLLMs with function calling abilities on NESTFUL. Our results show that\nmost models do not perform well on nested APIs in NESTFUL as compared\nto their performance on the simpler problem settings available in existing\nbenchmarks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are increasingly being used as the foundation of agentic\nsystems that can be used to address complex, real-world problems Yao et al. (2023); Deng\net al. (2024). In agentic problem settings, a language model interacts with both the user\nas well as the environment to collect information or execute tasks that allow the agent\nto carry out the user's request. This ability to interface with the broader environment\nis enabled through calls to tools or application programming interfaces (APIs), and has\nresulted in systems with diverse applications ranging from software assistants Jimenez et al.,\nto diagnostic systems Roy et al. (2024), to even formal theorem provers Thakur et al. (2023).\nFor LLMs to be able to utilize APIs\u00b9 properly, they must be capable of executing the\nfollowing tasks based on a user's query: (a) API detection, i.e., from a list of available APIs,\nchoose which APIs need to be called, (b) slot filling, i.e., given an API, identify the correct\nparameters and fill in values, and (c) sequencing, i.e., list the sequence of full API calls\nneeded to respond to the user query. Of the three categories, sequencing is often viewed as\nthe most challenging as it requires both API detection and slot-filling to create well-formed\nAPI calls.\nUnfortunately, while existing datasets used for evaluating API calling capabilities will test\neach of the three categories, the way in which they evaluate sequencing is incomplete. That\nis, existing evaluation benchmarks pose sequencing as the prediction of single or multiple"}, {"title": "Related Work", "content": "How best to enable API function calling from LLMs is an active area of research. Methods\nthat utilize large, general-purpose proprietary models (e.g., Gemini (Team et al., 2023) or GPT\n(Achiam et al., 2023)) typically make use of carefully constructed prompts and in-context\nlearning examples, e.g., Song et al. (2023). Smaller, more specialized models often start\nfrom a strong-performing code model (e.g., DeepSeek-Coder (Guo et al., 2024), CodeLlama\n(Roziere et al., 2023), or Granite Code (Mishra et al., 2024)) and fine-tune primarily on highly\ncurated datasets Srinivasan et al. (2023); Ji et al. (2024); Abdelaziz et al. (2024) that have been\nextended with synthetic data Zhang et al. (2024).\nIn addition to prompting strategies and models, there have also been numerous recent\nworks releasing training and benchmarking data in service of API function calling. ToolLLM\nQin et al. (2023) produced multi-sequence REST-API data generated using GPT4 Achiam\net al. (2023). Similarly, APIBench Patil et al. (2023) is a synthetic dataset of single-sequence\nAPI data specifically from ML libraries generated based on GPT-4 language models. Another\nwork focusing on synthetic data generation was APIGen Liu et al. (2024), which proposed a\nmulti-stage, hierarchical verification approach to ensure all data generated was of sufficient\nquality. Lastly, API-BLEND (Basu et al., 2024) introduced a large corpora for training and\nsystematic testing of tool-augmented LLMs in real-world scenarios. In this work, we focus\non tasks that need an interdependent sequence of API calls, which is a necessity for many\nreal-world, multi-step problems. This thus differentiates our approach from the existing\nevaluation benchmarks, each of which focus on single or multiple isolated API calling\nfunctionality."}, {"title": "Data Schema", "content": "Each data instance in the NESTFUL dataset consists of a question-answer pair, where the\nanswer is a sequence of API calls represented as a list of JSON objects. Each JSON object\ncorresponds to an API, including its 'name' and 'arguments'. Additionally, a unique variable\nname is assigned to each JSON object under the key 'label', which distinctly identifies each\nAPI, even when two identical APIs with different arguments appear in the same sequence\n(parallel API calls). Argument values that need to be grounded with results from previous\nfunction calls are enclosed in a $ sign and formatted as ${variable_name}.{parameter}$,\nwhere {variable_name} refers to the API whose results will be used, and {parameter}\nspecifies the output parameter of that API response. Below is the template for the data\nschema:"}, {"title": "Dataset Collection", "content": "The NESTFUL dataset comprises of 300 manually curated instances designed for bench-\nmarking tool-augmented large language models, with a focus on nested sequencing. Each\ninstance consists of a user query paired with an answer, represented as a sequence of API\ncalls, where the output of at least one API is used as the input for subsequent API(s). Based\non the ability to be executed, these 300 instances are categorized into two groups - executable\nand non-executable."}, {"title": "Executable APIs", "content": "The executable portion of the NESTFUL dataset is curated using APIs sourced from Rap-\nidAPI. We manually gather 39 different APIs across various domains, including flight\nbooking, Instagram information, restaurant and hotel searches, music, finance, and more.\nFor each API, we collect essential specifications such as API names, query/path param-\nmeters, output parameters, host, endpoint, etc. We also write descriptions by hand for all\nparameters (query, path, and output). Following is a template of the specification."}, {"title": "Non-Executable APIs", "content": "For non-executable data curation, we begin by collecting API specifications from the Glaive5\nand Schema Guided Dialog (SGD)6 datasets. SGD dataset has a limited set of APIs, but it\nhas full specifications, including input and output parameters. On the other hand, Glaive\nAPIs do not have output parameters, for which, we created the output parameters manually.\nWe then used DiGiT synthetic data generation framework\u201d to systematically create a set of\nnested sequence data. This involves using seed examples along with detailed instructions\nto prompt the Mixtral-8x22b-Instruct model. Finally, we perform a two-step filtration\nprocess to refine the dataset. First, we programmatically validate the samples to check\nfor hallucinations and ensure that the output APIs adhere to the specifications; required\nparameters are specified and output parameters are correct. Then, we manually review\nand exclude any examples with semantic errors, incorrect API sequence order, or improper\nvariable assignments. Below is an example based on Glaive API list:"}, {"title": "Evaluation", "content": "In our experiments, we have used 6 open sourced models as baseline: (1) xLAM-1b-fc-r\n(Liu et al., 2024); (2) Mistral-7B-Instruct-v0.3 (Jiang et al., 2024); (3) Hermes-2-Pro-Mistral-\n7B9; (4) Granite-20B-FunctionCalling (Abdelaziz et al., 2024); (5) Mixtral-8x7b-Instruct-v01\n(Jiang et al., 2024), and (6) Llama-3-70b-Instruct (Dubey et al., 2024). All these 6 models\nare selected from the Berkeley Function-Calling Leaderboard (BFCL)10, which captures the\nAPI/Function calling abilities of different proprietary and open models. We also considered\nevaluating other models like Gorilla-openfunctions-v211 and xLAM-7b-fc-r12. However,\nthese models have a limited context length (less than 4,096 tokens), whereas NESTFUL\ndataset examples require at least 8,000 tokens."}, {"title": "Experimental Settings", "content": "The experiments are carried out in one-shot and three shots setting, where in the prompt\nwe provide one or three in-context learning examples, respectively. For each model, we\nhave used the model specified prompt along with the special tags. Due to context length\nlimitations, we cannot include the entire API library in the prompt for each sample. Instead,\nwe pre-process the data to create a shorter API list for each example. This list ensures\ninclusion of the gold APIs, the APIs used in ICL examples, and some random APIs, keeping\nthe total prompt length under 8,000 tokens. Also, the API calls are extracted from the\nmodel's response as a list of JSON objects, taking into account that each model has a specific\nway to generate the API calls in the response."}, {"title": "Metrics", "content": "For a detailed evaluation of the generated responses, we calculate three metrics: Partial\nand Full Sequence Match for both non-executable and executable experiments, and API\nExecution Pass Rate specifically for executable scenarios. The following sections provide an\nin-depth explanation of each metric with examples.\nA generated response from the model is a sequence\nof API calls, with each call consisting of an API name and its argument-value pairs. We\nuse the Partial Sequence Matching metric to determine how many predicted APIs (with its\nargument-value pairs) in a sequence match with the Gold API sequence. In contrast, the\nFull Sequence Matching metric evaluates whether the model predicts the exact full sequence\nof APIs, including both the API names and their argument-value pairs, when compared to\nthe Gold API sequence. This metric checks whether the predicted API sequence is an exact\nmatch with the gold or not. In both cases, we calculate the scores for each data instance and\nthen compute the statistical mean across the entire dataset as the final score."}, {"title": "Results", "content": "Table 2 presents a comparison of different baselines on the NESTFUL dataset with one-shot\nand three-shots settings. As anticipated, in most of our experiments, the models are doing\nbetter when they are provided with three shot in-context learning examples in the prompt\ninstead of one-shot example.\nAcross all models, Partial Sequence Match scores are consistently higher than and Full Sequence\nMatch scores, which is expected, as the Full Sequence Match is more stricter metric than the\nPartial. We looked into the outputs generated by the models and have identified several\ncommon issues across them. None of the five baseline models have been trained with the\nrobust data schema discussed in Section 3. So, as expected, these models struggle with tasks\nsuch as assigning variables, utilizing output parameter details from the API specifications,\nand correctly passing variable names and corresponding output parameters to subsequent\nAPIs, even when provided with in-context learning examples. Models like xLAM-1b-fc-r\nand Mixtral-8x7b-Instruct-v01 also struggles with hallucination, as it sometimes predict\nargument values that are not present in the user query or generates natural languange\ntexts instead of APIs, and in some cases it keeps on generating wrong API sequence until it\nreaches to the max token. Also, in many cases they misse the variable assignments correctly.\nLlama-3-70b-Instruct outperforms other models in terms of both Partial and Full Sequence\nMatch scores for both executable and non-executable sections. Mixtral-8x7b-Instruct-v01\nand Granite-20B-FunctionCalling are scoring just after Llama-3-70b-Instruct on both the\nportion of the dataset. xLAM-1b-fc-r, Mistral-7B-Instruct-v0.3, and Hermes-2-Pro-Mistral-7B\nperform poorly (getting < 10%) across the dataset, as it is challenging to get the correct\nsequence and doing the appropriate variable mappings. On the API Execution Pass Rate\nmetric, the Llama-3-70b-instruct is achieving the highest score of 41% (with three-shots\nICL). After that the Mixtral-8x7b-Instruct-v01, Granite-20B-FunctionCalling, Hermes-2-Pro-"}, {"title": "Challenges", "content": "We consider the NESTFUL as a challenging benchmark for any LLMs for several reasons. In\nthis section, we will discuss these challenges in detail.\nIn the API Specification, we define the data\ntype for all parameters\u2014query, path, and output. The type field specifies the data type, such\nas string, number, list, etc. Since APIs follow a strict structure for both input and output,\nit is crucial for the model to adhere to these specified format. If the model fails to do so,\nparticularly in the nesting cases where the output of one API is passed as input to another,\nthe process will fail if the output type does not match the expected input type. Similarly, we\nspecify the required fields for all query and path parameters (in the API Specification) to\nindicate whether a parameter is optional or mandatory. It is crucial that any model take into\naccount these required parameters when using an API, as their inclusion is necessary for\nsuccessful execution. Ignoring required parameters can lead to incomplete or incorrect API\ncalls, affecting the model's performance.\nAs discussed in Section 3, we add variable assignments\nfor each API in the output to manage parallel function calls, which is very com-\nmon in real life applications. An example of parallel nested function calling has\nbeen provided in Section 4.1, where the first two APIs are the same (Weather\nAPI.com_Time_Zone_API) creating parallel functions. However, for the third API (CipherCir\ncuit_Math_Assistant_CalculateAllArithmeticOperations), it is crucial to distinguish the\noutputs of the first two APIs to obtain the correct result. This adds complexity to the dataset,\nas the models are not trained with this schema and must follow the instructions precisely.\nOur qualitative studies (discussed in Section 5.4) suggest the same as well.\nImplicit function calling refers to a scenario where the system must\ninvoke a specific API, along with potentially other APIs, to fulfill a user query, even though\nthe query does not explicitly mention the task that requires that particular API. Figure 1\nillustrates an example of implicit function calling, where the user query only mentions task"}, {"title": "Conclusion", "content": "In this work we introduced NESTFUL, a new benchmark for evaluating the performance\nof LLMs on API function calling with nested sequences of function calls (see Sections 3\nand 4). We showed that existing LLMs perform poorly on this dataset as compared to\ntheir performance on existing benchmarks and identified their several modes of failure\n(see Section 5). In addition, we outlined the many challenges this dataset poses to LLM\nfunction calling approaches (see Section 6). By making this dataset available publicly under\na permissive open-source license, we aim to push the capabilities of API function calling in\nnew directions and unlock solutions to more realistic, challenging tasks."}]}