{"title": "GREAT: Geometry-Intention Collaborative Inference for\nOpen-Vocabulary 3D Object Affordance Grounding", "authors": ["Yawen Shao", "Wei Zhai", "Yuhang Yang", "Hongchen Luo", "Yang Cao", "Zheng-Jun Zha"], "abstract": "Open-Vocabulary 3D object affordance grounding aims\nto anticipate \u201caction possibilities\" regions on 3D objects\nwith arbitrary instructions, which is crucial for robots to\ngenerically perceive real scenarios and respond to opera-\ntional changes. Existing methods focus on combining im-\nages or languages that depict interactions with 3D geome-\ntries to introduce external interaction priors. However, they\nare still vulnerable to a limited semantic space by failing to\nleverage implied invariant geometries and potential inter-\naction intentions. Normally, humans address complex tasks\nthrough multi-step reasoning and respond to diverse situ-\nations by leveraging associative and analogical thinking.\nIn light of this, we propose GREAT (Geometry-intention\ncollAboraTive inference) for Open-Vocabulary 3D Object\nAffordance Grounding, a novel framework that mines the\nobject invariant geometry attributes and performs analogi-\ncally reason in potential interaction scenarios to form af-\nfordance knowledge, fully combining the knowledge with\nboth geometries and visual contents to ground 3D ob-\nject affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object af-\nfordance dataset at present to support the task. Extensive\nexperiments demonstrate the effectiveness and superiority\nof GREAT. Code and dataset are available at project.", "sections": [{"title": "1. Introduction", "content": "Open-Vocabulary 3D object affordance grounding aims to\nlocate \"action possibilities\" on objects [8, 14], both for\nseen and unseen scenarios, identifying specific regions\non objects that support certain interactions. This bridges\nvisual perception and physical manipulation for embod-\nied agents and possesses bountiful application scenarios,\ne.g. robot manipulation [12, 20, 41], scene understanding\n[11, 15, 29], action anticipation [36, 63, 64] and imitation\nlearning [13, 25].\nRecently, most existing methods [2, 6, 18] establish\nexplicit mappings between semantic affordance categories\nand geometric structures, restricted to predefined seen cate-\ngories and fail to ground object affordance out of the train-\ning categories. Thus, some studies [27, 42, 50, 56, 58]\nexplore grounding object affordance through additional in-\nstructions, encompassing combining images or languages\nthat depict interactions with 3D geometries to introduce\nexternal interaction priors, and mitigate the generalization\ngap lead by affordance diversity. Despite their remark-\nable progress, they are still vulnerable to a limited seman-\ntic space by failing to leverage implied invariant geometries\namong objects with the same affordance, as well as poten-\ntial correlations among distinct interactions of the same ob-\nject. As shown in Fig. 1 (a), current paradigms ground\nthe 3D object affordance by aligning the object's geomet-\nric features and instruction modalities, working well in the\nseen partition. However, such a paradigm relies excessively\non the data fed into the model, when testing unseen affor-\ndance like \"pour\" that does not appear in the training set,\nthe model endeavors to categorize it as the seen category\n\"grasp\", shown in Fig. 1 (b).\nIn cognitive science, studies [7, 17] have shown that\nhumans tackle complex tasks through multi-step reasoning\nand respond to diverse situations by employing associative\nand analogical thinking. As demonstrated in Fig. 1 (c),\nwhen observing a water pouring scenario, humans employ\nmulti-step reasoning to infer multiple potential interaction\nprocedures and identify geometric properties of objects, in-\ndexing relevant knowledge from their brains. Analogous to\nthis procedure, we leverage Multi-modal Large Language\nModels (MLLMs) [4, 24, 37, 49] to simulate prior knowl-\nedge, encompassing visual, linguistic, and other modalities,\nlifting their superior reasoning and generalization abilities\nto visual tasks [26, 43]. However, the dynamic and di-\nverse nature of affordances makes it challenging to solely\nderive complex reasoning outcomes from MLLMs, to ad-\ndress this, a chain-of-thought (CoT) inference strategy that\nmirrors human reasoning processes is designed (Fig. 1 (d)).\nThe strategy step-by-step identifies the interaction primi-"}, {"title": "2. Related Work", "content": "Affordance Grounding. Affordance grounding aims to\nlocate the region of \u201caction possibilities\", which is a link\nbetween robot perception and manipulation. Some works\nground the object affordance from the 2D data e.g. images\nand videos [22, 33, 35, 61], while some works leverage nat-\nural language understanding to ground affordance regions\nin 2D data [3, 23, 32]. However, robot manipulation usu-\nally requires 3D information of objects, and the 2D affor-\ndance grounding obtained from the above works make it\ndifficult to infer the interaction position of 3D objects. With\nthe presentation of several 3D object datasetset [5, 39, 46],\nsome works focus on the 3D object affordance grounding\n[6, 39, 40], which map semantic affordance to 3D object\nstructure and fail to handle the open-vocabulary scenario.\nOpen-Vocabulary 3D Affordance Grouding. OVAG\npresents a substantial chanllenge, aiming to locate object af-\nfordance region in arbitrary scenario. Recently, some meth-\nods explore the possibility of OVAG. IAGNet [56] utilizes\nthe 2D interaction semantics to guide the grounding of 3D\nobject affordance. LASO [27] employs textual-conditioned\naffordance queries to isolate afforded segments and injects\ntext clues to point features. OpenAD [42] and OpenKD [50]\npropose a text-point correlation method for affordance syn-\nonym substitutions by utilizing the power of large language\nencoder clip. Although above methods have made remark-\nable progress, they are still vulnerable to a limited training\nsemantic space due to the presence of critical learnable parts\nin the framework. GREAT mitigates this limitation by lever-\naging geometry-intention collaborative inference with CoT\nto ground object affordance.\nChain of Thought Prompting with MLLMs. Chain of\nThought (CoT) [51, 62] and its variants [21, 59, 60] are pro-\nposed to enhance the reasoning capabilities of Multi-modal\nLarge Language Models (MLLMs), which guide the model\nthrough multiple logical steps. With the rapid development\nof MLLMs [4, 16, 65], vision-related [43, 53, 55] meth-\nods have made significant progress in collaborating CoT\nand MLLMs to get the desired results. Some methods ex-\nplore object affordance at task driven object detection [48],\nrobot manipulation [26] and 2D-level object detection [3],\nwhile these methods only describe the egocentric images\nand then reason through the text or image encoding results\nas inputs, that can only obtain limited and static knowledge.\nHowever, despite the considerable achievements of CoT en-\nhanced MLLMs reasoning, it is challenging to reason about\n3D object affordance from interaction images due to the\ncomplex and dynamic properties of affordance. To bridge\nthis gap, we fine-tune the InternVL [4] and directly input\ninteraction images with prompts to reveal the geometric at-\ntributes and underlying interaction intention."}, {"title": "3. Method", "content": "3.1. Overview\nGiven the inputs {P,I}, where $P \\in R^{N \\times 4}$ denotes a\npoint cloud of the object comprising the coordinate $P \\E\nR^{N \\times 3}$ and the affordance annotation $P_{label} \\in R^{N \\times 1}$, $I \\in\n[R^{3 \\times H \\times W}$ denotes an image. N is the number of points,\nH, W are the height and width of an image. The goal is to\noptimize the model $f_{\\theta}$ that outputs 3D object affordance $\\phi$,\nexpressed as $\\phi = f_{\\theta}(P_{c}, I)$. As shown in Fig. 2, initially,\nthe inputs are sent to ResNet [9] and PointNet++ [44], ob-\ntain specific features $F_{i} \\in R^{C \\times H_{1} \\times W_{1}}$, $F_{p} \\in R^{C \\times N_{p}}$, and\nreshape $F_{i}$ to $R^{C \\times N_{i}}$ ($N_{i} = H_{1} \\times W_{1}$). Then, GREAT\ncaptures object structure attributes and affordance interac-\ntion procedure by fine-tuning MLLM [4] with the Multi-\nHead Affordance Chain of Thought to reason about interac-\ntion images. Next, the features encoded by Roberta [30]\nare fused through cross-attention mechanism, calculating\nthe T. and interntion feature $T_{a}$ (Sec. 3.2). Afterwards,\nwith $T_{o}, T_{a}$ as knowledge dictionaries, GREAT leverages\nCross-Modal Adaptive Fusion Module to inject knowledge\nclues into point features and directly fuse knowledge within\nimage features to obtain fusion features $F_{tp}, F_{ti}$ (Sec. 3.3).\nEventually, $F_{tp}$ and $F_{ti}$ are sent to the decoder to ground\n3D object affordance $\\phi$, the whole process is optimized by\na combined loss (Sec. 3.4).\n3.2. Multi-Head Affordance Chain of Thought\nFine-Tuning MLLM. We adopt InternVL [4] and the in-\njected learnable adapters [10] to fine-tune the MLLM, due\nto the MLLM primarily focuses on object recognition and\ndescription without sufficient understanding of what ob-\njects are actually used for and how they interact with hu-\nmans. As shown in Fig. 2 (c), given an interaction im-\nage $I \\in R^{3 \\times H \\times W}$ and text prompts T, InternVL is desired\nto perform multi-modal understanding and give correct an-\nswers. During the training process, we only fine-tune the\ninjected adapters for 10 epochs with a learning rate of 4e-5\nand a LoRA rank of 16, while freezing the main parameters,\nto preserve the power of InternVL and further empower the\nmodel with capabilities in affordance understanding.\nObject-Head Reasoning for Geometry. It consists of Ob-\nject Interaction Perception and Geometric Structure Rea-\nsoning. First, the model needs to focus on understanding\nthe interactive components of an object within the image,\nrefining its perception of the object's key parts rather than\nthe entire object. As illustrated in Fig. 2 (a), we design the\nfirst prompt as \u201cPoint out which part of the object in the im-\nage interacts with the person.\" Next, since different regions\nof an object can often perform the same interaction based\non shared geometric attributes, the model needs to reason\nabout these features. This allows it to move beyond the con-\nstraints of object categories and focus more on the relation-\nship between structure and affordance. Thus, we design the\nsecond prompt as \"Explain why this part can interact from\nthe geometric structure of the object.\"\nAffordance-Head Reasoning as Brainstorming. It con-\nsists of Interaction Detailed Description and Interactive"}, {"title": "3.3. Cross-Modal Adaptive Fusion Module", "content": "To better facilitate the cross-modal fusion of the geometric\nattributes of interaction regions and point cloud features, we\npropose Cross-Modal Adaptive Fusion Module (CMAFM)\nthat integrates the geometric attributes into the deepest en-\ncoder layer of PointNet++ [44] to refine the point feature\nmap, enabling effective cross-modal feature alignment and\nfusion, as shown in Fig. 2 (b).\nSpecifically, CMAFM re-represents $F_{p} \\in R^{C \\times N_{p}}$ and\n$T_{o} \\in R^{N \\times C}$ in the same feature space to further align\nthe local features. $F_{p}$ is projected to form the query $Q ="}, {"title": "3.4. Decoder and Loss Functions", "content": "Image features with interaction intention and point features\nwith geometric structure are fed into the decoder, which\njointly reveals the 3D affordance region, formulated as:\n$F_{a} = f[\\Gamma(F_{ti}), F_{tp}], \\varphi = \\sigma(f_{\\theta}(F_{a})),$ (5)\nwhere $\\Gamma$ denotes reshape $F_{ti}$ to $R^{C \\times N}$, $f_{\\theta}$ denotes an out-\nput head, $\\sigma$ denotes the sigmoid function, $F_{a} \\in R^{C \\times N}$ is\naffordance feature representation and $\\varphi$ represents the 3D\nobject affordance.\nUnconstrained by the affordance category labels, we fo-\ncus on the differences between 3D object affordance $\\varphi$\nand affordance ground truth annotation $P_{label}$, enabling the\nmodel to directly link 3D object affordances with interac-\ntion images through reasoning. Therefore, the total loss\nconsists of a focal loss [28] and a dice loss [38], which su-\npervises point-wise heatmaps, formulated as:\n$L_{total} = L_{focal} + L_{dice}.$ (6)"}, {"title": "4. Dataset", "content": "Collection. We construct the Point Image Affordance\nDataset v2 (PIADv2), which comprises paired 2D interac-\ntion images and 3D object point clouds. Points are mainly\ncollected from 3DIR [57], 3D-AffordanceNet [6], objaverse\n[5], etc. Images are mainly collected from AGD20k [34],\nOpenImage [19] and websites with free licenses. In total,\nPIADv2 contains 15213 images and 38889 point clouds,\ncovering 43 object and 24 affordance categories, which is\nthe largest scale 3D object affordance dataset so far. Some\ndata samples are shown in Fig. 3 (a). The affordance and the\nobject categories are shown in Fig. 3 (b). Both figures show\nthat the dataset covers numerous affordances, supporting\nvarious interaction scenarios and diverse object categories."}, {"title": "5. Experiment", "content": "5.1. Benchmark Setting\nEvalization Metrics. For a thorough assessment, we use\nprevious evaluation metrics from advanced works [27, 56]\non 3D object affordance grounding to benchmark the model\non our PIADv2 dataset, which include Area Under Curve\n[31], average Intersection Over Union [45], SIMilarity\n[47], Mean Absolute Error [52].\nCompare Baselines. For a comprehensive comparison, we\nselect two leading works (IAG [56] and LASO [27]) on 3D\naffordance grounding and two leading image-point cloud\ncross-modal works (FRCNN [54] and XMF [1]) mentioned"}, {"title": "5.2. Comparison Results", "content": "The comparison results of evaluation metrics are presented\nin Tab. 2, demonstrating GREAT significantly outperforms\nthe compared baselines across all metrics in three partitions\nand achieves the state-of-the-art performance. Furthermore,\nthe results are visualized in Fig. 4.\nSeen vs. Unseen. Quantitatively analyzing the results of\nthe Tab. 2, all baselines demonstrate a stepwise metrics\ndecrease in all partitions. This trend emphasizes the diffi-\nculty of generalizing unseen objects and affordances. Com-\npared to other baselines, the superior performance demon-\nstrated by GREAT in the open-vocabulary proves the neces-\nsity and rationality of our task setting for open-vocabulary\naffordance grounding. Qualitative analysis of the visualiza-\ntion results in Fig. 4 shows little difference in the Seen set-\nting, but in the Unseen setting, GREAT significantly outper-\nforms the other methods. For example, in the case of kettle,\nonly IAG detects the small affordance region for pouring,\nwhile methods that directly link object structure with tex-\ntual descriptions fail to predict and can only identify the\ntrained affordance: grasp. In contrast, our method uncovers\ninteraction details in 2D interaction images by leveraging\nthe MLLM embedded with world knowledge for MHACOT\nreasoning, leading to more precise predictions."}, {"title": "5.3. Ablation Study", "content": "We conduct a thorough ablation study to validate the effec-\ntiveness of the model design, as shown in Tab. 3. First,\nwe ablate the object head and affordance head in MHACOT"}, {"title": "5.4. Performance Analysis", "content": "We conducted experiments separately for Seen, Unseen ob-\nject, and Unseen affordance partitions, enabling a deeper\nanalysis of the model's performance in various contexts.\nMultiple Objects. In the case of humans interacting with"}, {"title": "6. Conclusion", "content": "We present grounding 3D object affordance in an open-\nvocabulary fashion, which reasons from interaction images,\nextrapolating from predefined sample space and generalize\nto unseen scenarios. To achieve so, we propose a novel\nframework to utilize multi-head affordance chain of thought\nreasoning, excavating invariant geometric properties and\nanalogous reasoning about potential interactions, with the\nalignment of cross-modal features to localize 3D object af-\nfordance region. Furthermore, We introduce the largest\n3D object affordance dataset PIADv2, which contains 15K\ninteraction images and over 38K 3D objects with annota-\ntions. Extensive experiments demonstrate the effectiveness\nand superiority of GREAT. It supports affordance under-\nstanding in open scenarios, potentially enhancing robots'\nautonomous interaction in unknown environments. We be-\nlieve it could offer fresh insights and promote research in\nthe area of visual affordance understanding.\nLimitations and Future Work. The limitation of GREAT\nlies in the high computational complexity of its multi-step\nreasoning, which can become a bottleneck in large-scale\nor real-time applications. In future work, we aim to cre-\nate inference-specific datasets and use them to distill multi-\nmodal models into specialized knowledge domains, en-\nabling faster and more efficient performance in real-world."}]}