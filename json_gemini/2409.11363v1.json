{"title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark", "authors": ["Zachary S. Siegel", "Sayash Kapoor", "Nitya Nadgir", "Benedikt Stroebl", "Arvind Narayanan"], "abstract": "AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves re-producing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-40 and GPT-40-mini. The best agent achieved an accuracy of 21% on the hardest level of tasks, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.", "sections": [{"title": "Introduction", "content": "Computational reproducibility, the ability to reproduce the results of a scientific study using the data and code provided by its authors, is fundamental to scientific research (Medicine, 2019). Yet, recent studies have documented severe shortcomings in the state of computational reproducibility across fields including psychology (Hardwicke et al., 2021; Obels et al., 2020; Hardwicke et al., 2018), economics (Gertler et al., 2018; McCullough et al., 2006), medicine (Naudet et al., 2018), political science (Stockemer et al., 2018), life sciences (Andrew et al., 2015; Gilbert et al., 2012; Ioannidis et al., 2009), geoscience (Konkol et al., 2019), and computer science (Belz et al., 2021; Raff, 2019; Collberg & Proebsting, 2016). Even if code and data accompany a study, reproducing a study's results can be challenging for many reasons: the software libraries used might not have their versions specified, researchers could use different machine architectures (ARM vs. x86) or operating systems (Linux vs. Windows vs. MacOS), old libraries could be incompatible with new hardware, or there could be inherent variance in the results of a study. To quantify this, we surveyed evidence for the lack of computational reproducibility across fields, where papers were found to be irreproducible despite available reproduction materials (summarized in Table 1).\nMachine learning (ML) is no exception. While introducing the NeurIPS checklist incentivized researchers to share data and code (Pineau et al., 2021), studies still lack computational reproducibility. To quantify this, we collected the results of ML reproducibility challenges. The challenges consist of events that incentivize independent researchers to reproduce the results of studies in top venues. We analyzed the results of the 2022 challenge and found that only 18 of 28 papers that are accompanied by code and data are completely reproducible. Verifying the computational reproducibility of a paper requires expertise. In some (6/28) cases, challenge participants could not fully reproduce results despite conversing with the original paper's authors.\nThe importance of uncovering and documenting reproducibility issues has been recognized in the ML community. As an example, reproducibility reports warrant publication in the peer-reviewed ML journal Transactions on Machine Learning Research (TMLR),\u00b9 and earlier reproducibility challenges recommended graduate-level ML expertise for preparing reproducibility reports.\u00b2\nSimultaneously, language models have made significant strides in coding tasks, solving most tasks in benchmarks such as HumanEval (Chen et al., 2021). However, real-world coding challenges remain difficult for these models. More recently, the emergence of compound AI systems (Zaharia et al., 2024) has allowed for the completion of more difficult tasks. For instance, on SWE-bench, a GitHub-based coding issue benchmark (Jimenez et al., 2023), language models alone achieve less than 5% accuracy, while agents boost this to over 30%.\nSuch results have prompted claims that we will soon be able to automate most scientific research, especially in computationally intensive fields. For instance, one work builds an early-stage framework that uses large language models to automate the AI research process, from idea generation to paper writing (Lu et al., 2024). However, designing evaluation schemes is difficult, and the quality of the AI-generated papers has been questioned (Koppel, 2024). Before agents can automate scientific research, they must be able to reproduce existing results.\nIn this paper, we ask: Can AI agents automate computational reproducibility of published scientific research? We make two main contributions:\n\u2022 CORE-Bench (Computational Reproducibility Benchmark). CORE-Bench comprises 270 tasks derived from 90 papers across computer science, social science, and medicine with Python or R codebases. Curated from CodeOcean.com repositories, we verified each repository to ensure it was locally reproducible and created three tasks at different difficulty levels based on available reproduction information (code output is given as if the code has already been successfully executed; Dockerfile to reproduce code is given but output is not given; Readme file provided with repository is given but not Dockerfile). The benchmark evaluates diverse skills including coding, shell interaction, retrieval, and tool use. While many existing benchmarks include Python tasks (Cassano et al., 2022), ours is one of the first to include tasks in R. Successful task completion may require multiple steps such as library installation, script execution, retrieval of the results corresponding the right experiment from the task prompt, and figure interpretation using vision-language models. CORE-Bench's foundation in public repositories enables periodic updates of the benchmark tasks, which could mitigate concerns about contamination and saturation. An agent performing well on CORE-Bench would have real-world utility: authors could verify their work's reproducibility before publication, independent researchers could more easily replicate past studies, and conference organizers and journal editors could efficiently assess the reproducibility of submissions.3\n\u2022 Evaluation results on baseline agents. We evaluated two agents on CORE-Bench: the generalist agent AutoGPT (Significant Gravitas, 2024) and a task-specific version we built based on AutoGPT called CORE-Agent. Results show that generalist agents can be easily adapted to specific tasks, yielding significant performance improvements. Our task-specific agent achieved 60% accuracy on the easiest task, demonstrating potential for automating computational reproducibility. However, performance dropped to 21% on the hardest task, indicating substantial room for improvement. We ran experiments with two different language models: GPT-40 and GPT-40-mini. To facilitate these evaluations, we are releasing CORE-Bench alongside an evaluation harness specifically designed for this benchmark, making it easy for developers to evaluate their own agents on the benchmark. This harness runs each task in an isolated virtual machine, enabling parallelized testing, ensuring reproducibility, and maintaining a clear separation between benchmark and agent code. The harness dramatically reduces evaluation time from over 20 days to mere hours by running on hundreds of parallel virtual machines, while ensuring standardized access to hardware and preventing agents from tampering with the benchmark."}, {"title": "CORE-Bench: Evaluating agents on computational reproducibility", "content": "As the capabilities of AI agents continue to expand, many claims have been made about their ability to autonomously conduct research (Lu et al., 2024). But reproducing existing research is easier than conducting new research, especially when new research requires reproducing earlier baselines for comparison.\nRecent work has introduced several benchmarks to evaluate language models and agents on various tasks related to computer programming and scientific research. These include benchmarks for conducting machine learning experiments (Huang et al., 2023), research programming (Tian et al., 2024), scientific discovery (Majumder et al., 2024), performing scientific reasoning and citation tasks (Press et al., 2024; Xu et al., 2024), and solving real-world programming problems (Zhang et al., 2024). With CORE-Bench, we aim to evaluate the ability of agents to automate the research reproduction process, a part of the pipeline that hasn't yet received attention."}, {"title": "Benchmark Construction", "content": "Verifying reproducibility requires significant domain expertise and can be labor-intensive, even for experienced researchers. This makes it particularly challenging to build a benchmark where the reproducibility of each paper is verified. The key problem is that we want the tasks in the benchmark to be realistically difficult, but we need the construction of the benchmark itself to be much easier than solving the benchmark. It can take a few hours to test the reproducibility of a paper in the wild, so verifying about a hundred papers from diverse fields would be impractical.\nTo address this, we based our benchmark on CodeOcean capsules (See Figure 2), which are known to be reproducible with little effort (Clyburne-Sherin et al., 2019). We selected a set of 90 reproducible papers from CodeOcean using the process outlined in Table 2 and Figure 3. We split the dataset into 45 papers for training and 45 for testing. For each paper, we manually created a set of task questions about the outputs generated from a successful reproduction of the paper (Appendix A.3 provides details on task question construction). These questions assess whether an agent has correctly executed the code and retrieved the results. For instance, an agent could be asked to report the test accuracy of a model, an axis label of a figure, or another reproduced result. Some tasks have a single task question, while others consist of multiple. We ensure each task has at least one question that cannot be solved by guessing (e.g. a question with an open-ended numerical answer), and a task is marked as correct only if all of the task questions are answered correctly, which ensures all tasks cannot be solved by guessing.\nAs we sourced papers from CodeOcean.com, all CORE-Bench tasks are from reproducible papers. Since the benchmark is measuring the ability of agents to reproduce the results of running the code associated with the paper, and not to ensure that the results reported directly in the paper are correct, we did not see a need to include irreproducible papers in the benchmark."}, {"title": "Why use CORE-Bench?", "content": "Skills and modalities. Solving the tasks in CORE-Bench requires many skills, including understanding instructions, debugging code, retrieval, and interpreting results from a wide range of disciplines. The skills necessary to perform well on CORE-Bench reflect many skills necessary to reproduce new research.\nTasks require interpreting both text and image output from code. The vision-based questions require extracting results from attributes of figures, graphs, plots, or PDF tables. The text-based questions include extracting results from command line text, PDF text, and tables or text in HTML, markdown, or latex. For example, a vision-based question might be \"From the Indoor Air Quality - Kitchen - Autumn plot, report the correlation between hum and gas,\" whereas a text-based question might be \"Report the test accuracy of the neural network after epoch 10.\" A task can have only vision-based task questions, only text-based task questions, or both (See Appendix A.4 for a breakdown of tasks).\nReal-world computational reproducibility tasks. When constructing our benchmark, we focus on its construct validity, which is about how well a test measures real-world performance (Biderman et al.,"}, {"title": "Baseline agents and evaluation setup", "content": "We evaluated all agents on CORE-Bench split by difficulty: CORE-Bench-Easy, CORE-Bench-Medium, and CORE-Bench-Hard.\nBaseline agents. We developed and evaluated two variants of the AutoGPT agent (Significant Gravitas, 2024) on the benchmark: AutoGPT, which was not prompted or given any tools specific to CORE-Bench and the CORE-Agent family of agents, which were prompted and modified for enhanced performance on each of the three difficulty levels of CORE-Bench.\n1. AutoGPT: This agent is largely unmodified from the popular general-purpose AutoGPT agent, but we created another tool for the agent called query_vision_language_model, which takes as input an image and a query, and outputs OpenAI API's response to the image query. This allows the agent to analyze results in figures and plots.4 We included this modification in AutoGPT because the ability to query a vision language model is not specific to CORE-Bench. Other minor changes can be found in Appendix D.1.\n2. CORE-Agent: We built upon AutoGPT to create CORE-Agent, a task-specific variant of AutoGPT, customized for each level of CORE-Bench. Our primary change was implementing a programmatic check to ensure the correct submission and keys of the file reporting the reproduced results (i.e., report.json). In addition, for each difficulty level, we added specific prompting hints to guide the agent's behavior, as detailed in Table 4. These hints address common pitfalls observed during qualitative analysis of agent performance on the training set. Notably, these adaptations required only a few days of work, with the most time-consuming aspect being the analysis of failure logs to identify effective prompting strategies. We benefited significantly from AutoGPT's auto-summarization of past actions, which expedited the identification of common failures."}, {"title": "Results", "content": "Overall, CORE-Agent with GPT-40 is the top performing agent on all three levels of the benchmark, solving 60.00% of tasks on CORE-Bench-Easy, 57.78% on CORE-Bench-Medium, but only 21.48% on CORE-Bench-Hard. We report all results in this section on the test split unless otherwise mentioned, since we used the train split while developing the agent (see Figure A1 for train set results).\nOur results demonstrate that generalist agents can be effectively adapted to specific tasks with minimal effort, yielding significant performance improvements. For instance, AutoGPT with GPT-40 scored just 6.7% on CORE-Bench-Hard. The following sections provide a detailed analysis of agent performance and highlight the potential of adaptable generalist agents for specialized tasks."}, {"title": "Accuracy varies by difficulty level as expected", "content": "Agents generally performed the best on CORE-Bench-Easy, followed by CORE-Bench-Medium and CORE-Bench-Hard. For instance, CORE-Agent with GPT-40-mini scored 44.44%, 32.59%, and 16.30% on the three levels, respectively (See Table 5).\nThese results are expected, since CORE-Bench-Easy is designed to be the easiest task with the code outputs are already provided in the environment - the agent only needs to navigate the environment to find the relevant results to answer the task questions. On CORE-Bench-Medium, the agent is provided with a Docker command that replicates the paper's code, testing the agent's ability to interact with the Bash terminal. If the agent is proficient at interacting with the terminal, this task should be similarly easy. For CORE-Bench-Hard, the agent must install all dependencies and libraries and determine the correct command necessary to reproduce the relevant results, which is significantly more difficult than the other two tasks, accounting for the drop in performance."}, {"title": "Task specific modifications improve accuracy, especially for weaker models", "content": "Comparing performance when fixing the LLM model, we observed that AutoGPT's performance improved substantially with only slight modifications. This adaptability seems to be particularly advantageous for weaker LLMs, where small changes provide crucial guardrails and task guidance. With the GPT-40 back-end, a few modifications to the prompt and the programmatic check of the output format boosted the performance on CORE-Bench-Easy performance from 35.6% to 60.60%. The differences were even starker when using GPT-40-mini: performance improved from 8.9% to 44.44%.\nOur results highlight the adaptability of generalist agents, demonstrating significant performance gains from minimal, task-specific adjustments. We hypothesize that agents that use stronger models in the future will require even fewer task-specific modifications to perform well on a given task."}, {"title": "Stronger models lead to higher accuracy despite a lower token budget", "content": "We ran AutoGPT and CORE-Agent using both GPT-40 and GPT-40-mini with an API cost limit of $4. Even though the per-token cost of GPT-40-mini is less than 5% that of GPT-40, which allows for longer sessions before hitting the cost limit, GPT-40 still outperformed GPT-40-mini on both agents. Despite having the same cost limits, GPT-40-mini powered agents tended to be 3-5x cheaper than GPT-40 agents. In all settings, the average per-task cost was cheapest on CORE-Bench-Easy, followed by CORE-Bench-Medium and CORE-Bench-Hard (Figure 6).\nTo evaluate the impact of our $4 cost limit on performance, we ran CORE-Agent on the CORE-Bench-Hard with a $10 cost limit on the train set. With the new limit, GPT-40-mini performance remained unchanged, and GPT-40's performance increased modestly from 26% to 31% (Figure 7). Note that GPT-40-mini outperformed GPT-40 for lower cost limits under around $2.50.\nIncreasing the cost limit did not greatly increase accuracy because when agents succeeded at tasks, they succeeded quickly (the average cost of successful tasks for CORE-Agent and GPT-40 was $0.54, compared to $2.59 for failed tasks) but when they failed at tasks, they often hit the cost limit and failed after not making progress. Even when increasing the cost limit, agents tended to remain stuck."}, {"title": "Written questions are easier than vision questions", "content": "Agents consistently performed better on text-based questions than vision-based questions. CORE-Agent with GPT-40 got 59.26% vision questions correct and 87.88% written questions correct on CORE-Bench-Easy on the test set. Similarly, CORE-Agent with GPT-40-mini got 37.78% of vision questions correct and 81.81% of written questions correct.\nVision questions are harder because they typically require analyzing results from figures, whereas written answers are often directly found in the terminal output. Agents were sometimes unable to find the relevant"}, {"title": "Better guardrails are needed to deploy safe agents", "content": "In one case, the agent attempted to search for the CodeOcean repository online to look for the requirements for missing dependencies. Although the agent tried to create an account on CodeOcean, it could not view the CodeOcean website since it required JavaScript (Appendix D.3.4). This points to the need for mechanisms to restrict the actions taken by the agent. We have updated the release version of our evaluation harness to restrict access to the CodeOcean.com domain.\nSince AutoGPT can execute arbitrary actions on the web, better guardrails should be developed to ensure agents exhibit safe and expected behavior (He et al., 2024). For instance, there are no existing safeguards preventing simple agent errors such as creating thousands of accounts on a website. For this paper, we did not incorporate web browsing restrictions for our agents since their inability to render JavaScript prevented most damaging actions from being taken out. However, as agents advance, developers should implement additional safety checks."}, {"title": "Conclusion", "content": "Many visions for the future of LLMs and tool use anticipate grandiose reforms of the fields of research and science, including claims that AI agents will automate research completely (Lu et al., 2024). However, a pre-requisite for building on existing knowledge is to reproduce research that has already been released.\nIf an AI agent can reproduce research effectively, it can drastically reduce the human labor required to read, understand, and run code as part of an assessment of computational reproducibility. By releasing CORE-Bench, we hope to stimulate the development of agents to reduce the time and effort required for this burdensome yet routine scientific activity.\nOur baseline results show that while automating computational reproducibility is hard, simple task-specific modifications to existing general-purpose agents can already help increase accuracy. This is in line with other results showing the importance of task-specific modifications (Yang et al., 2024). Yet, our best baseline agent only has a test-set accuracy of 21%, showing the vast room for improvement. We hope that CORE-Benchcan spur research in improving the utility of agents in automating computational reproducibility."}, {"title": "Benchmark Details", "content": "To obtain a dataset of all 5,090 capsules on CodeOcean and their corresponding environment files, we wrote a webscraper that downloads the metadata for every capsule from CodeOcean. We then manually exported each capsule from CodeOcean's web interface to obtain the environment files. Finally, we filtered the capsules in this dataset based on the ten criteria outlined in Table 2.\nTable 2 presents the ten criteria we used to filter the capsules on CodeOcean and construct the tasks for CORE-Bench. We provide an example of a capsule's run file that satisfies criteria six (Listing 1) and an example of the output from a capsule we rejected from the benchmark (Listing 2 and Listing 3).\nTo write task questions for each capsule, we examined the capsule's results folder after a successful reproducible run on CodeOcean's web interface and chose outputs from any of the files in the results for the agent to extract. These outputs could include a model's accuracy, the axis label of a figure, or any other relevant metric. Then, for each output, we manually write a prompt instructing the agent to report the corresponding value. Since a single paper can have multiple outputs, CORE-Bench consists of 90 capsules and 181 task questions. The number of task questions per capsule ranges from one to eight.\nWe referred to tables or figures in task questions in one of three ways:"}, {"title": "Harness Details", "content": "Our evaluation harness runs all agents on virtual machines using Azure. For non-GPU capsules, we use a Standard_E2as_v5 machine type, and for GPU capsules, we use a Standard_NC4as_T4_v3 machine type. All VMs run Ubuntu Linux and have an 80 GB disk attached.\nThe harness initially creates a VM for each task-agent pair and copies over the capsule files and agent files to the VM. Once the files are copied over, the harness runs the agent on the VM. The capsule only downloads the results and deletes the VM once the agent creates a file called task_completed.log in the home directory. This log file can be empty or can contain any logging information that the developer wishes to save from the run."}, {"title": "Experimental Details", "content": "We plot the accuracy of CORE-Agent and AutoGPT on the train set (See Fig A1). Similarly to the test set results, we see that CORE-Agent consistently outperforms AutoGPT, and GPT-40 outperforms GPT-40-mini.\nWe ran CORE-Agent experiments with GPT-40 and GPT-40-mini three times to generate a 95% confidence interval over the mean accuracy and mean cost (See Table A3). The accuracy of the top-performing agent had a CI of under 5 percentage points on all difficulty levels. Overall, the accuracy of GPT-40-mini had a bigger CI on results than GPT-40, suggesting it is a less reliable model to use.\nOn the test set, the pass@1 accuracy of CORE-Agent with GPT-40 on the CORE-Bench-Hard was 22.2% and the pass@3 accuracy was 31.1% (See Figure A2). Similarly, with GPT-40-mini, the pass@1 accuracy was 15.6% and the pass@3 accuracy was 26.7%. Since the performance could be improved simply by re-running"}, {"title": "Agent Details", "content": "In addition to the modifications to AutoGPT described in the main text, we implemented two other changes for both AutoGPT and CORE-Agent. We implemented these changes for both agents and did not consider them as task-specific modifications since the changes are not specific to CORE-Bench and would improve the agent in many domains.\nTruncating tool output: If a tool invoked by AutoGPT generates an output that is too long, we updated the code to truncate the output to include the beginning and end, rather than return an error. We found this change helps the agent better use tools when the outputs are long.\nUsing the shell to execute all Bash commands: AutoGPT uses the subprocess module to execute commands on the command line. However, the default setting was to set shell=False when invoking subprocess.run, which prevented the agent from using shell-specific commands such as && when chaining together two commands. We changed the settings to set shell=True to let the agent execute all commands.\nWe provided tailored prompts to CORE-Agent for each difficulty level of the benchmark. These prompts were given as arguments to the AutoGPT agent, called 'constraints' or 'best practices'.\nIn this section, we provide some examples of common failure cases for each level of the benchmark."}, {"title": "Using incorrect figure during information extraction (CORE-Bench-Easy)", "content": "On capsule-4299879, CORE-Agent with GPT-40 extracted the p-value from the wrong image in the results. The plot with the correct title containing the relevant p-value is called Figure_A17.pdf, but the agent only looked at Figure_2-1.pdf and Figure_3-1.pdf. Since the agent called query_vision_language_model on the wrong figures, it returned the incorrect p-value."}, {"title": "Manually reproducing the code instead of using Docker (CORE-Bench-Medium)", "content": "On capsule-8234136, CORE-Agent with GPT-40-mini tried to manually reproduce the code even when prompted to use Docker, causing the agent to eventually hit the context limit and fail. We found this problem to be more persistent on weaker models like GPT-40-mini, which did not follow instructions as well as GPT-40.\nBeing unable to install the correct version of dependencies (CORE-Bench-Hard)\nIn capsule-8807709, CORE-Agent with GPT-40 installed network-diffusion version 0.14.4. However, one of the import statements (from network_diffusion import MultiSpreading) threw an error because the import was only supported in version 0.6). The agent successfully realized it may need to install an older library version and performed a web search to see which version was applicable, but could not find the correct result within the cost constraint.\nThis example shows how reproducing a paper can be a difficult task, even for a human. Determining which library version to install without additional documentation can be hard without external knowledge."}, {"title": "Attempting to look up the capsule on CodeOcean (CORE-Bench-Hard)", "content": "Also in capsule-8807709, CORE-Agent with GPT-40, after being unable to locate the requirements.txt file in the repository, attempted to look up the capsule on CodeOcean online. The agent ultimately did not succeed because JavaScript is required to render CodeOcean, which the agent did not have access to through its web browsing capabilities. However, this example highlights the care that agent developers must take during evaluation."}, {"title": "Reproducibility Study Details", "content": "We report the number of studies reproduced for each paper in Table 1 based on the format of the results provided by the papers' authors:\n\u2022 The following papers report the percentage of reproducible studies out of the total number of studies. We manually calculated the number of reproducible studies and rounded the result: Stockemer et al. (2018), Gertler et al. (2018), Collberg & Proebsting (2016), Hardwicke et al. (2021), Raff (2019).\n\u2022 The following papers report the number of results or metrics that were computationally reproducible, rather than the number of papers: Gilbert et al. (2012), Trisovic et al. (2022), Samuel & Mietchen (2024), P\u00e9rignon et al. (2024), Belz et al. (2021).\nWe manually analyzed the papers from the 2022 Machine Learning Reproducibility Challenge (Sinha et al., 2023). Of the 44 papers submitted to the challenge, 28 attempted to reproduce papers where both data and code were fully available. 10 of those 28 papers were only partially reproduced. We consider papers to be fully reproduced if all the main claims of the paper completely hold, even if the reproduced quantitative results slightly deviate from the original results. For example, we consider Livernoche & Sujaya (2023) a successful reproduction of the original paper because authors validate the original paper's claims and results fall within the standard deviation reported in the original paper. On the other hand, we consider papers to have reproducibility errors if all the main claims of the paper cannot be reproduced, or if result values from the original paper deviate significantly from those of the reproduced paper. For example, we treat Brivio & \u00c7\u00f6ltekin (2023) as an unsuccessful reproduction because the highest accuracy score from the reproduced paper deviates significantly from the original paper although the original hypothesis was verified. We do not consider the results of additional experiments not contained in the original paper. Of the fully reproduced papers, many codebases contained errors, outdated packages, or limited documentation, requiring researchers to modify the codebase during the reproduction process."}]}