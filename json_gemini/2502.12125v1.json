{"title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy", "authors": ["Roman Malashin", "Valeria Yachnaya", "Alexander Mullin"], "abstract": "We investigate the training dynamics of deep classifiers by examining how hierarchical relationships between classes evolve during training. Through extensive experiments, we argue that the learning process in classification problems can be understood through the lens of label clustering. Specifically, we observe that networks tend to distinguish higher-level (hypernym) categories in the early stages of training, and learn more specific (hyponym) categories later. We introduce a novel framework to track the evolution of the feature manifold during training, revealing how the hierarchy of class relations emerges and refines across the network layers. Our analysis demonstrates that the learned representations closely align with the semantic structure of the dataset, providing a quantitative description of the clustering process. Notably, we show that in the hypernym label space, certain properties of neural collapse appear earlier than in the hyponym label space, helping to bridge the gap between the initial and terminal phases of learning. We believe our findings offer new insights into the mechanisms driving hierarchical learning in deep networks, paving the way for future advancements in understanding deep learning dynamics.", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks have demonstrated remarkable capabilities in learning complex functions, yet the un-derlying reasons for their success remain an open question. In this work, we aim to shed light on one crucial aspect of this puzzle: What patterns do neural networks tend to learn first, and which ones do they learn last? The question has been studied for a long time, but valuable interpretations continue to emerge. One important contribution is the concept of simplicity bias proposed by Arpit et al. (2017), which suggests that neural networks tend to learn simple (frequent) patterns followed by more complex or noisy patterns. Neural collapse is another phenomenon that characterizes the terminal phase of classifier training (Papyan et al., 2020). During this phase variance of penultimate features within the same class converges to zero, while the class means are arranged in the feature space according to Equiangular Tight Frame(ETF) structure.\nIn this work we study dynamics of the classifier learning process through the lens of evolving hierarchy of object categories. According to our experiments we argue that in the classification task the network tend to learn relations between high-level categories (hypernyms) on early stages, while more specific (hyponyms) are learned in the later training epochs; terminal phase is characterized by removing hypernymy relations from the features of the layer. By analogue we call this tendency a hypernym bias, as it can be nicely interpreted as manifestation of simplicity bias: classes of same hypernym share frequent (simple) features, therefore neurons responsible for their detection are activated more often and trained faster. The neural collapse state can be interpreted as final state of the label clustering process in the specific layer: every cluster contains only a single label, and information about labels' relations is removed from features. The basic intuition about the phenomenon we study in this paper can be grasped from Figure 1.1\nFrom Figure 1b training can be interpreted as top-to-bottom hierarchical label clustering that starts by in-"}, {"title": "RELATED WORKS", "content": "Hierarchical classification. Errors in trained classifiers tend to occur more frequently between closelyrelated classes, such as different species of animals or various types of vehicles. This pattern is evident in the block-diagonal structure of the confusion matrix, provided that the classes are properly sorted. Initially, the ImageNet dataset was organized according to the lexical database of semantic relations WordNet (Fellbaum, 2010) under assumption that it will help to build new efficient classification algorithms (Russakovsky et al., 2015). Since then lots of works tried to incorporate hierarchy bias into the training process of deep neural networks: either in architecture (Hinton et al., 2015; Malashin, 2016; Xiao et al., 2014) or in loss function (Redmon and Farhadi, 2017; Brust and Denzler, 2019). Some authors reported promising results. However these findings are often derived in nonoptimal training settings (Brust and Denzler, 2019) or from closed datasets, which complicates fair comparison (e.g. (Hinton et al., 2015) as example). It is also well-known that, in few-shot leaning settings removing hierarchically related classes from the pre-training process significantly impacts the performance on these classes (Vinyals et al., 2016). This suggests that features of the network inherently capture hierarchical relationships. We believe our result provide insight into why hierarchical structure is not used in state-of-the-art solutions.\nFrequency Principle. The Frequency Principle (FP) or Spectral bias is a phenomenon in neural network training, independently discovered in (Rahaman et al., 2019) and (Xu et al., 2019), which states that the low-frequency components of the target function are learned first by the neural network. The principle is"}, {"title": "METHODOLOGY", "content": "We base our conclusions from experiments with ImageNet dataset, which classes are organized according to lexical database of semantic relations WordNet", "subsections": [{"title": "Greedy hypernym classification", "content": "Label spaces. Let $\\mathcal{H}_{NH} = \\{h\\}_{NH}$ be the set of $N_H$ hyponyms (classes), and $\\mathcal{S} = \\{s\\}_{N_S}$ be the set of $N_S$ hypernyms (superclasses), where each superclass $\\mathcal{S}_i = \\{h\\}_{N_{Si}}$ is defined such that\n$\\cup_{s \\in \\mathcal{S}} s = \\mathcal{H}_N \\text{ and } s_i \\cap s_j = \\emptyset \\,\\, \\forall \\mathcal{S}_i, \\mathcal{S}_j \\in \\mathcal{S}, i \\neq j.$\nIn the case of ImageNet, we have $N_H = 1000$. For simplicity we omit subscript notations indicating the sizes of sets where possible. A standard classifier network $f(x;\\theta)$ equipped with softmax, maps image $x$ to a probability distribution over hyponyms (classes) $h\\in \\mathcal{H}$ based on the current weights $\\theta$. The training objective utilizes the cross-entropy loss function:\n$\\mathcal{L}_{CE} = \\sum_{h\\in \\mathcal{H}}  \\log f_h(x; \\theta)y_h(x),$\nwhere $y_h(x)$ represents true probability that $x$ belongs to $h$ (0 or 1 in one-hot encoding), and $f_h(x;\\theta)$ is the estimated probability of the same.\nWe consider $f(x; \\theta)$ as a function that also maps an image to different label spaces greedily (not considering confidence level). We denote labels and label predictions with symbol . The predicted hyponym is then given by:\n$f_h(x; \\theta) = \\text{argmax}_{h\\in H} f_h(x; \\theta),$\nand the true label $\\hat{y}_h(x) = \\text{argmax}_{h\\in H} y_h(x)$\nWe explore manifestation of hypernym bias through the convergence dynamics by introducing a trivial mapping $T_{H\\rightarrow S} : H \\rightarrow S$, which maps each hyponym to its parent hypernym at a specified level in the WordNet tree. The predicted hypernym then:\n$f_s(x; \\theta) = T_{H\\rightarrow S}(f_h(x;\\theta)),$\n$\\hat{y}_s(x) = T_{H\\rightarrow S}(\\hat{y}_h(x)).$\nIn our experiments $T_{h\\rightarrow s}$ is implemented as simple reverse traversal of the WordNet tree starting from $f_h$; it returns the first match with any element of $S$.\nTo evaluate the specific impact of hypernym relationships in the WordNet tree, we introduce $R$, a randomly generated label space that is isomorphic to $S$. Isomorphism here means that there is a one-to-one correspondence between labels in $R$ and $S$, and the sizes of the corresponding superclasses are identical. Formally there exists bijection $g : R \\rightarrow S$, such that $\\forall r \\in R \\exists s \\in S : |r| = |g(r)|$. This ensures that any differences in network performance on $R$ and $S$ can be attributed to the semantic structure encoded in $S$. For example, while $S$ might group classes into \"Animals\", \"Artifacts\", and \"Others\", $R$ would have three groups of the same sizes but with randomly assigned classes.\nGiven that $T_{H\\rightarrow R}$ and $T_{H\\rightarrow S}$ are predefined and not learned, we can assume that the same network $f(x; \\theta)$ maps each image to three label spaces simultaneously without interfering training process:\n$f_h(x; \\theta) \\rightarrow H, f_s(x; \\theta) \\rightarrow S, f_r(x; \\theta) \\rightarrow R.$\nCorresponding ground truths labels are $\\hat{y}_H(x), \\hat{y}_s(x), \\hat{y}_r(x)$. Thus we can study accuracy metrics in each label space."}, {"title": "Assessing evolution of class hierarchical structure of the manifold", "content": "Assuming that features lie near low-dimensional manifold, we evaluate how well this feature manifold aligns with WordNet graph on each epoch. Our approach has three major steps: a) define the distances between classes in feature space, b) define distances between classes in WordNet graph and c) compare two distance matrices via cophenetic correlation coefficient.\nGraph in feature space. To estimate distances between feature sets within manifold we adapt approach from (Jin et al., 2020). First we sample $2 \\times K$ training examples per each of C classes, and divide them into non-overlapping query and support sets Q and S:\n$Q = \\{\\mathcal{U}^c = \\{u_i^c\\}_K\\}_C, S = \\{\\mathcal{V}^c = \\{v_i^c\\}_K\\}_C,$\nwhere $\\mathcal{U}^C \\cap \\mathcal{V} = \\emptyset, \\forall c \\in [0, C]$. Assuming suitable metric, we define distance $d_f(u_i^{c_i}, \\mathcal{V}^{c_j})$ between query feature of class $c_i$ and support set $\\mathcal{V}_j$ of another class $c_j$ as minimum individual distance:\n$d_f(c_i, \\mathcal{V}^{c_j}) = \\min_{v \\in \\mathcal{V}^{c_j}} d_f(u,v).$\nNext we measure probability that distance between query point of class $c_i$ and support set $\\mathcal{V}^{c_j}$ is less than predefined radius $r$:\n$P_r(c_i, c_j) = P (d_f(u_i, \\mathcal{V}^{c_j}) < r) = \\mathbb{E}_{u \\sim \\mathcal{U}^{c_i}} [\\mathbb{I}\\{d_f(u, \\mathcal{V}^{c_j}) < r\\}],$\nwhere $\\mathbb{I}\\{d_f (u, \\mathcal{V}^{c_j}) < r\\}$ is the indicator function. Finally we estimate similarity of two classes as:\n$\\rho(c_i, c_j) = \\frac{1}{r_{\\text{max}}} \\int_0^{r_{\\text{max}}} P_r(c_i, c_j) dr,$\nwhere $r_{\\text{max}}$ is maximum radius.\nThe value $\\rho(c_i, c_j) \\in [0,1]$ represents mutual cover when $i \\neq j$ and self-cover when $i = j$, as described by Jin et al. (2020). Similarity $\\rho(c_i, c_j) = 1$ indicates that every query feature $u_i$ is within zero distance to at least one feature $v_i$ in support set of the class $j$, low values indicate that there are many query features that are far from any features in $\\mathcal{V}_j$. We construct similarity matrix $A = [a_{ij}]$, where $a_{ij} = \\rho(c_i, c_j)$ and treat its elements as probability that edge between i-th and j-th node of the graph exists. Transformation from similarity to a distance matrix $D_F$ is straightforward:\n$D_F = [d_F(i, j)] = 1 - A.$\nLabel distance in WordNet graph. Distance matrix between class labels according to WordNet structure can be defined as:\n$D_W = [d_w(i, j)],$\nwhere $d_w(i, j)$ is the shortest path between synset i and j in the graph. With this definition we can use full (not tree) WordNet graph.\nGraph comparison. To compare graphs we utilize Cophenetic Correlation Coefficient:\n$CCC = \\frac{\\sum_{i<j} d_w(i, j)d_F(i, j)}{\\sqrt{\\sum_{i<j} d_w(i, j)^2 \\sum_{i<j} d_F(i, j)^2}},$\n$\\bar{d_w(i, j)} = d_w(i, j) - \\bar{D_W}, \\\\ \\bar{d_F(i, j)} = d_F(i, j) - \\bar{D_F},$\nwhere $\\bar{D_F}$ and $\\bar{D_W}$ are the mean of all pairwise distances in the original distance matrices $D_F$ and $D_W$."}, {"title": "Neural collapse in hypernym label space", "content": "Neural collapse (Papyan et al., 2020) is a phenomenon that is observed in penultimate layer of the classifier when recognition accuracy on the training set reaches 100%. Neural Collapse is characterized by four manifestations in the last-layers weights and last-layer activations: within-class variability collapse (NC1), convergence to simplex ETF (NC2), convergence to self-duality (NC3) and simplification to nearest-class center (NC4). We give brief overview of neural collapse in Appendix A. Originally NC is defined on the label space H, where training loss is estimated. Assuming hypernym bias we expect at least some properties of neural collapse in label spaces of hypernyms emerge earlier. We replicate calculations as in (Papyan et al., 2020) but in different label spaces after applying $T_{H\\rightarrow r}$ or $T_{h\\rightarrow s}$, described in Section 3.1. To compute NC2-NC4 we must consider that the weight matrix $W = [W_c]$ contains C weights rows, consistent with the number of labels. We interpret each row $w_c$ as prototype (Snell et al., 2017) of its own hyponym. Assuming linear relationships to obtain prototypes of hypernyms $s \\in S$ we can average columns under hypernym relation. Thus rows of the resulting weight matrix can be calculated as $w \\equiv \\frac{1}{|s|} \\sum_{c \\in s} \\{w_c\\}$."}]}, {"title": "EMPIRICAL RESULTS", "content": null, "subsections": [{"title": "Experimental setup", "content": "Top level hypernyms. Top level of the WordNet tree contains 9 synsets. we expect manifestation of hypernym bias should be more pronounced in large hyperyms. Since top-level synsets are unevenly saturated with classes, we grouped them into synsets sets and used as top level hypernym superclasses; for clarity, most of the results are reported with respect these label spaces: 1) hyponyms H : 1000 classes used for training, 2) hypernym superclasses S3: three super-classes formed from 522 (artifacts), 398 (animals), and 80 (others) classes according to the WordNet tree top level synsets 3) random superclasses R3: three super-classes formed from 80, 522, and 398 randomly selected classes.\nLower level hypernyms. We demonstrate the robustness of the observed phenomenon to different specifications of S with the use of different levels of hypernymy tree."}, {"title": "Results", "content": "Greedy hypernym classifier. Table 1 shows final absolute accuracy, and Figure 1a displays relative accuracy curves in hyponym, hypernym and random superclass label spaces. Across all architectures during initial epochs of training relative accuracy for hypernyms increases significantly faster than for the rest of considered label spaces. From the table, it is evident that the recognition accuracy of superclasses formed with the use of the WordNet tree is significantly higher compared to random superclasses of the same size. However, the final hypernym classification accuracy strongly depends on the size of the network. For instance, ResNet-50 outperforms ResNet-18 by 61% in terms of error reduction for hypernyms, compared to a 43% improvement for hyponyms. This should be taken into account when using interpretation through the lens of simplicity bias. Also hypernym accuracy never fully plateaus until the end of training. Figure 3 shows relative accuracy gain computed according to (9) for ResNet-50. The metric behaves identically for"}, {"title": "Manifold hierarchical structure,layer-wise and data frequency analysis", "content": "Alignment with WordNet graph and layer-wise dynamics. It is well-known that layers of convolutional neural networks converge faster than the entire neural network (Raghu et al., 2017; Wang et al., 2023b). Residual skip connections can possibly help transmit less distorted low-level feature information to the upper layer responsible for decision-making. Thus, layer-wise dynamics can possibly be related to hypernym bias: hypernyms can be recognized without use of high level features. We apply framework developed in Section 3.2 to evaluate how well feature manifold of different levels aligns with WordNet graph on each epoch. The results presented in Figure 6a deny suggested hypothesis."}, {"title": "Generalization", "content": "There are no formal grounds to believe that the results should generalize beyond the conducted experiments. To address this concern, we performed additional experiments to further validate our findings."}]}, {"title": "DISCUSSION AND FUTURE DIRECTIONS", "content": "Hierarchical classification. To the best of our knowledge, the WordNet graph has not been widely adopted in state-of-the-art classification algorithms. Our experiments suggest that this may be because the hyponym-hypernym relationships are implicitly learned by the network itself. Our results show that when the loss function incorporates multiple cross-entropy losses from different hierarchy levels (as in (Redmon and Farhadi, 2017)), the contributions of higher-level hierarchy components are significant only during the early iterations of training, with a much lower impact in later stages.\nFor example, Brust and Denzler (2019) report similar accuracy metrics when leveraging the hyponymy relation on CIFAR-100, but they observe the faster convergence and a shift in the initial training phase. Our findings explain this by showing that hypernym relations are not independently learned by the network only in the first epochs of training. Moreover, Brust and Denzler (2019) also report a notable increase in accuracy when incorporating hierarchical relations during training on ImageNet. We attribute this improvement to their use of the mean squared error loss function, which is suboptimal for classification tasks.\nFuture work. Future research could extend the notion of hypernym bias to other domains. For instance, the AudioSet (Gemmeke et al., 2017) has a hierarchical organization of categories, such as \"Human sounds\" \u2192 \"Speech\" and is well suited to the proposed framework.\nAnother promising area of research involves moving away from reliance on predefined external class structures and instead learning class hierarchies directly through data-driven hierarchical clustering. For example, the class similarity matrix A from Equation (15) can be treated as an adjacency matrix, reframing the problem into identifying hierarchical communities in the graph, to which known algorithms can be applied. A data-driven approach has two key benefits: (a) hierarchical clustering can uncover more accurate and intuitive representations of class relationships, potentially revealing latent patterns missed by external structures, and (b) analyzing the training dynamics of classifiers in conjunction with these learned hierarchies could provide valuable insights for designing more efficient and adaptive clustering algorithms.\nInvestigating the theoretical connections between hypernym bias and other known biases is a promising direction. Establishing these relationships would not only deepen the theoretical understanding of hypernym bias but also bridge practical findings with theoretical models, which often lack strong empirical validation."}, {"title": "CONCLUSIONS", "content": "The primary objective of our work was to establish that hypernym bias is a consistent phenomenon across different neural network architectures trained in practical settings, which other works on biases and hierarchical learning often lack. We also aimed to introduce an experimental framework that provides a systematic and quantifiable approach to studying biases in practical scenarios, which can serve as a foundation for future research in this area. To the best of our knowledge we are also the first to connect the training bias in the early stage with neural collapse in the terminal training stage by estimating neural collapse in different label spaces. Therefore the notion of hypernym bias can may contribute to research of neural collapse. Our experiments, which did not reveal parallels with existing phenomena, were designed to demonstrate that hypernym bias cannot be trivially attributed to other known factors, such as layer-wise training dynamics or data frequency biases. The lack of clear parallels in these cases underscores that hypernym bias is a distinct phenomenon requiring independent investigation."}, {"title": "METHODOLOGY", "content": null, "subsections": [{"title": "Relation of A(R,t) and A(H,t)", "content": "Let $f_h(x; \\theta_t)$ denote a classifier trained in the hyponym label space $H$ at epoch t. If $A(H, t)$ represents the absolute classification accuracy in $H$, then the classification accuracy in a random superclass label space, $A(R, t)$, can be theoretically estimated based on the sizes of the subsets within $R = \\{r\\}$.\nTo see this, let $D = \\{x_i\\}_{i=1}^N$ be the dataset with N examples and $A(x,t) \\in [0,100]$ in every label space X. Accuracy A(H, t) is essentially an estimation of probability $P_H(x; \\theta_t)$ that classifier f assigns correct hyponym label $\\hat{y}_h(x)$ to an image x:\n$A(H,t)/100 \\approx P_H(x;\\theta_t) = P(f_h(x; \\theta_t) = \\hat{y}_h(x)),$\nwhere $f_h(x; \\theta_t)$ is the predicted label.\nAssume that we use classifier f to greedily predict the label in the random superclass label space R, accordingly to Section 3.1. Let $f_r(x;\\theta_t)$ be a superclass label that is predicted. The probability $P(\\hat{y}_R(x) = r)$ that a randomly chosen image x belongs to a superclass r can be estimated according to the size of the superclass in the dataset:\n$P(\\hat{y}_R = r) \\approx P_r = \\frac{|D_r|}{N_D},$\nwhere $D_r = \\{x \\in D \\\\ | \\\\ \\hat{y}_R(x) = r\\}$ is a set of examples with label r.\nSince hyponyms are assigned to r independently, prior probability $P_R$ that greedy hypernym classifier predicts correct label of superclass r:\n$P_R(f_r(x) = \\hat{y}_r(x)|\\hat{y}_r(x) = r) = P_r.$\nThe probability of assigning a correct label in the superclass label space given a hyponym recognition probability $P_H(x;\\theta_t)$, can be estimated by considering two possible events: (a) correctly classifying the hyponym, or (b) misclassifying the hyponym but still assigning the correct superclass label by chance.\n$P_R(x;\\theta_t) = P_H(x; \\theta_t) + (1 - P_H(x; \\theta_t)) \\sum_{r \\in R} P_r.$\n$P_R(x; \\theta_t)$ gives good estimate of A(R, t) in real experiments (Figure 7) except at the beginning of training, due to the nonuniform behavior of the randomly initialized network."}]}, {"title": "Neural Collapse", "content": null, "subsections": [{"title": "Neural Collapse", "content": "Here we briefly overview Neural Collapse (NC) proposed by Papyan et al. (2020). Originally NC is defined only in hyponym label space (used for training), so we extend it to the hypernym label spaces.\nHyponym Label Spaces. Let $h_{i,c} \\in \\mathbb{R}^p$ be a p-dimensional feature vector, where c is one of X classes, where X is the label space. In the original ImageNet hyponym label space (X = H) each class corresponds to a hyponym, so |H| = 1000. The matrix $W \\in \\mathbb{R}^{|H| \\times p}$ and vector $b\\in \\mathbb{R}^{|H|}$ represent the weights and bias of the last fully-connected layer. An image $x_i$ is mapped to its feature representation $h_i$ by all but one layers of the network f and the predicted label $f_h(x_i)$ is determined by the index of the largest element in the vector $Wh_i + b$, as follows:\n$f_h(x_i) = \\underset{c'}{\\operatorname{arg max}} (w_{c'}, h_i) + b_{c'},$\nwhere $w_c$ is the c'-th row of W.\nThe global mean is defined as $\\mu_G \\equiv \\text{Ave}_{i,c}\\{h_{i,c}\\}$, and the train class means are defined as:\n$\\mu_c \\triangleq \\text{Ave}_{i}\\{h_{i,c}\\}, c=1, ... , |H| ,$\nwhere Ave is the averaging operator.\nAdditionally, the between-class covariance, $\\Sigma_B \\in \\mathbb{R}^{p \\times p}$ is defined as:\n$\\Sigma_B = \\text{Ave}_{c}\\{(\\mu_c - \\mu_G) (\\mu_c - \\mu_G)^T\\},$\nand the within-class covariance, $\\Sigma_W \\in \\mathbb{R}^{p \\times p}$ is defined as:\n$\\Sigma_W \\triangleq \\text{Ave}_{i,c}\\{(h_{i,c} - \\mu_c) (h_{i,c} - \\mu_c)^T\\}.$\nNC manifests through four key observations as described by Papyan et al. (2020).\n(NC1) Variability collapse: $\\Sigma_W \\rightarrow 0$.\nFollowing Papyan et al. (2020), in the Figure 5 of the paper, we plot tr $(\\Sigma_W (\\sum_B)^\\dagger / C)$, where tr denotes the trace operation and $[.]^\\dagger$ is the Moore-Penrose pseudoinverse.\n(NC2) Convergence to simplex ETF:\n$|||\\mu_c - \\mu_{c'}||_2 - ||\\mu_c - \\mu_{c'}||_2| \\rightarrow 0 \\forall c, c'$\n$\\langle \\mu_c, \\mu_{c'}\\rangle \\rightarrow \\frac{C}{C-1} \\delta_{c,c'} - \\frac{1}{C-1} \\forall c, c',$\nwhere $\\bar{\\mu_c} = \\frac{(\\mu_c - \\mu_g)}{||\\mu_c - \\mu_g||}$ are the renormalized class means.\nFollowing the original paper, we plot the following metrics in Section C.2:\n1. Norms equality level measured by two ratios: a) the standard deviation to the average length of class means and b) the standard deviation of the length of the rows of the weight matrix to their average length:\n$\\beta_{\\mu} = \\text{Std}_c (||\\mu_c - \\mu_g||_2) / \\text{Avg}_c (||\\mu_c - \\mu_g||_2),$\n$\\beta_w = \\text{Std}_c (||w_c||_2) / \\text{Avg}_c (||w_c||_2),$\nwhere $w_c$ is the c-th row of W (classifier of the c-th class).\n2. Angles equality level measured as the standard deviation of cosines of the angles between class means $(\\alpha_{\\mu})$ and the standard deviation of cosines of angles between rows of W $(\\alpha_w)$."}]}, {"title": "Convergence to self-duality:", "content": "$\\frac{W W^T}{\\|W\\|_F}  \\frac{M}{\\|M\\|_F} \\rightarrow \\mathbb{I},$ where  is the matrix obtained by stacking the class means into the columns of a matrix. Here,  is the Kronecker delta symbol.\nWe report value, estimated by equation (31), in Figure 5.\n(NC4) Simplification to Nearest Class Centroid (NCC):\n$\\underset{c}{\\operatorname{arg max}} (w_c, h) + b_c \\rightarrow  \\underset{c}{\\operatorname{arg min}} \\|h - \\mu_{c'}\\|^2.$\nWe report the proportion of mismatch between two ways of label estimation in Figure 5.\nHypernym Label Spaces. To estimate NC in the hypernym label space S, we compute the mean of each superclass s \u2208 S:\n$\\mu_s = \\text{Ave}_{c \\in s}\\{\\mu_c\\}.$\nAnd we assume that the superclass weight matrix $W_s \\in \\mathbb{R}^{|S_n| \\times p}$ consists of $|S_n|$ rows, where each row $w_s$ is obtained by averaging the rows of W:\n$w_s \\triangleq \\text{Ave}_{c \\in s}\\{w_c\\},$\nand similarly for the bias vector b: $b \\triangleq \\text{Ave}_{c \\in s}\\{b_c\\}.$\nWe can then use $\\{\\mu_s\\}$, $W_s$, and $b_s = \\{b_s\\}$ instead of $\\{\\mu_s\\}$, W, and b as defined above, to estimate Neural Collapse in the hypernym spaces S."}, {"title": "EXPERIMENTAL DETAILS", "content": "In this section, we provide additional details on the experimental setup for the experiments discussed in Section 4."}, {"title": "Architectures and Training Parameters", "content": "The primary conclusions are based on experiments with ResNet; however, we also include results for ViT and MobileNet V3-L 1.0 to demonstrate that the findings generalize beyond convolutional or parameter inefficient architectures. Table 2 summarizes key parameters of the neural networks used in this study."}, {"title": "Constructing Hypernym Label Spaces", "content": "High-Level Hypernyms. At the top level of the WordNet tree, there are 9 synsets:\n1. Plants (2 classes in ImageNet),\n2. Geological formations (10 classes),\n3. Natural objects (16 classes),\n4. Sports (0 classes),\n5. Fungi (7 classes),\n6. People (3 classes),\n7. Miscellaneous (42 classes),\n8. Artifacts (522 classes),\n9. Animals (398 classes).\nSince the synsets have an uneven distribution of classes, we used the following groupings to form a top-level hypernym space, S3:\n1. {1-7} (miscellaneous, 80 classes),\n2. {8} (artifacts, 522 classes),\n3. {9} (animals, 398 classes)."}, {"title": "ADDITIONAL EXPERIMENTS", "content": null, "subsections": [{"title": "Greedy Hypernym Classification", "content": null, "subsections": [{"title": "Confusion Matrix Evolution", "content": "A straightforward way to observe the manifestation of hypernym bias is by plotting a confusion matrix with class labels arranged according to the structure of the WordNet hierarchy. In this case, the order of classes was determined by performing a depth-first traversal of the tree, ensuring that sibling classes appear consecutively in the confusion matrix. We visualize the confusion matrices at the 1st, 5th, and 200th epochs of training ResNet-50 in Figure 8.\nIn the early stages, the block-diagonal structure is apparent, although the WordNet tree only partially mirrors the misclassification patterns. This can be observed in the structural discontinuities within the blocks."}, {"title": "Residual Error", "content": "To provide a clearer understanding of the training dynamics of ResNet-50, as shown in Figures 1 and 3, we introduce the residual relative error metric, ER, defined as:\n$E_R(X,t) = \\frac{\\frac{100 - A(x, t)}{100 - A(X,T)} - 1},$ where A(x, t) is the accuracy at epoch t, and A(X,T) is the final accuracy after T epochs."}]}, {"title": "Different Hypernym Spaces", "content": "Animal Hypernym Space. We examined the recognition of images within the \"animals\" hypernym label space using ResNet-50. This analysis is motivated by the well-known hierarchical structure of species, which resembles a tree-like hierarchy. Although the WordNet tree has known limitations, this structure is likely to provide a more accurate description for this portion of the ImageNet dataset. The results are presented in Table 3.\nUnbalanced hypernym space. To demonstrate that intuitive manual grouping of synsets inside the same hypernym level of the tree does not affect the conclusions, we also evaluated superclasses derived directly from one of the levels of the WordNet hierarchy without additional grouping for class balance. This resulted in 72 unbalanced superclasses, with the corresponding results presented in Table 4."}, {"title": "Other datasets", "content": "We have conducted additional experiments to provide further evidence of hypernym bias existence across different datasets and domains."}]}, {"title": "Complexity and infrastructure", "content": "For experiments we used NVIDIA A6000 and RTX 3090 gpus.\nOur framework consists of three components", "complexity": "n1. Greedy classifier model. Involves single argmax computation over logits during forward pass. Mappings between labelspaces are efficiently managed using hash tables. Computational burden is negligible in practice.\n2. Manifold assessment. - Complexity: O(EL \u00b7 (N log N \u00b7 p + K"}]}