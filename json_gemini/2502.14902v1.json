{"title": "PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths", "authors": ["Boyu Chen", "Zirui Guo", "Zidan Yang", "Yuluo Chen", "Junze Chen", "Zhenghao Liu", "Chuan Shi", "Cheng Yang"], "abstract": "Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known as graph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to access up-to-date or domain-specific knowledge from external databases, enhancing the response quality without additional training (Gao et al., 2022b, 2023; Fan et al., 2024; Procko and Ochoa, 2024). Most RAG approaches divide the text database into chunks, organizing them in a flat structure to facilitate efficient and precise searches (Finardi et al., 2024; Yepes et al., 2024; Lyu et al., 2024).\nTo better capture the inherent dependencies and structured relationships across texts in a database, researchers have introduced graph-based RAG (Edge et al., 2024; Guo et al., 2024), which organizes textual information into an indexing graph. In this graph, nodes represent entities extracted from the text, while edges denote the relationships between these entities. Traditional RAG (Liu et al., 2021; Yasunaga et al., 2021; Gao et al., 2022a) usually focuses on questions that can be answered with local information about a single entity or relationship. In contrast, graph-based RAG targets on global-level questions that need the information across a database to generate a summary-like response. For example, GraphRAG (Edge et al., 2024) first applies community detection on the graph, and then gradually summarizes the information in each community. The final answer is generated based on the most query-relevant communities. LightRAG (Guo et al., 2024) extracts both local and global keywords from input queries, and retrieves relevant nodes and edges using these keywords. The ego-network information of the retrieved nodes is then used as retrieval results.\nHowever, we argue that the information considered in previous graph-based RAG methods is often redundant, which can introduce noise, degrade model performance, and increase token consumption. The redundant information retrieved in these two methods may act as noise, and negatively impact the subsequent generation. Moreover, both methods adopt a flat structure to organize retrieved information in the prompts, e.g., directly concatenating the textual information of all retrieved nodes and edges, resulting in answers with suboptimal logicality and coherence.\nTo overcome the above limitations, we propose PathRAG, which performs key path retrieval among retrieved nodes and converts these paths into textual form for LLM prompting. We focus on the key relational paths between retrieved nodes to alleviate noise and reduce token consumption. Specifically, we first retrieve relevant nodes from the indexing graph based on the keywords in the query. Then we design a flow-based pruning algorithm with distance awareness to identify the key relational paths between each pair of retrieved nodes. The pruning algorithm enjoys low time complexity, and can assign a reliability score to each retrieved path. Afterward, we sequentially concatenate the node and edge information alongside each path as textual relational paths. Considering the \"lost in the middle\" issue of LLMs (Liu et al., 2024), we place the textual paths into the prompt in ascending order of reliability scores for better answer generation. To evaluate the effectiveness of PathRAG, we follow the four benchmark datasets used in previous work (Qian et al., 2024), and additionally explore two larger ones. Experimental results on six datasets show that PathRAG generates better answers across all five evaluation dimensions compared to the state-of-the-art baselines. Compared to GraphRAG and LightRAG, the average win rates of PathRAG are 60.44% and 58.46%, respectively. The advantages of PathRAG are more significant for larger datasets,"}, {"title": "Related Work", "content": "Text-based RAG. To improve text quality (Fang et al., 2024a; Xu et al., 2024; Zhu et al., 2024) and mitigate hallucination effects (Lewis et al., 2020; Guu et al., 2020), retrieval-augmented generation (RAG) is widely used in large language models (LLMs) by leveraging external databases. These databases primarily store data in textual form, containing a vast amount of domain knowledge that LLMs can directly retrieve. We refer to such systems as text-based RAG. Based on different retrieval mechanisms (Fan et al., 2024), text-based RAG can be broadly classified into two categories: sparse vector retrieval (Alon et al., 2022; Schick et al., 2023; Jiang et al., 2023; Cheng et al., 2024) and dense vector retrieval (Lewis et al., 2020; Hofst\u00e4tter et al., 2023; Li et al., 2024a; Zhang et al., 2024). Sparse vector retrieval typically identifies the most representative words in each text segment by word frequency, and retrieves relevant text for a specific query based on keyword matching. In contrast, dense vector retrieval addresses issues like lexical mismatches and synonyms by encoding both query terms and text into vector embeddings. It then retrieves relevant content based on the similarity between these embeddings. However, most text-based RAG methods use a flat organization of text segments, and fail to capture essential relationships between chunks (e.g., the contextual dependencies), limiting the quality of LLM-generated responses (Edge et al., 2024; Guo et al., 2024).\nKG-RAG. Besides text databases, researchers have proposed retrieving information from knowledge graphs (KGs), known as KG-RAG (Ya-"}, {"title": "Preliminaries", "content": "In this section we will introduce and formalize the workflow of a graph-based RAG system.\nInstead of storing text chunks as an unordered collection, graph-based RAG automatically structures a text database into an indexing graph as a preprocessing step. Given a text database, the entities and their interrelations within the textual content are identified by LLMs, and utilized to construct the node set V and edge set E. Specifically, each node v \u2208 V represents a distinct entity with an"}, {"title": "Methodology", "content": "In this section, we propose a novel graph-based RAG framework with the path-based retriever and a tailored prompt template, formally designated as PathRAG. As illustrated in Figure 2, the proposed framework operates on an indexing graph through three sequential stages: node retrieval, path retrieval, and answer generation."}, {"title": "Node Retrieval", "content": "In this stage, we identify keywords from the input query by LLMs, and accordingly extract relevant nodes from the indexing graph. Given a query q, an LLM is utilized to extract keywords from the query text. The collection of keywords extracted from query q is denoted as Kq. Based on the extracted keywords, dense vector matching is employed to retrieve related nodes in the indexing graph G. In dense vector matching, the relevance between a keyword and a node is calculated by their similarity in the semantic embedding space, where the commonly used cosine similarity is adopted in our method. Specifically, we first encode both node identifiers and the extracted keywords using a semantic embedding model $f : K_q \\cup K_v \\rightarrow X_q \\cup X_\\nu$, where $X_v = \\{x_v\\}_{v\\in \\nu}$ represents the embeddings of node identifiers, and $X_q = \\{x_{q,i}\\}_{i=1}^n$ denotes\nthe embeddings of the extracted keywords. Based on the obtained embeddings above, we then iterate over Xq to search the most relevant nodes among $X_\\nu$ with the embedding similarity, until a predefined number N of nodes is reached. The resulting subset of retrieved nodes is denoted as $V_q \\subseteq V$."}, {"title": "Path Retrieval", "content": "In this subsection, we introduce the path retrieval module that aggregates textual chunks in the form of relational paths to capture the connections between retrieved nodes.\nGiven two distinct retrieved nodes $v_{start}$, $v_{end} \\in V_q$, there could be many reachable paths between them. Since not all paths are helpful to the task, further refinement is needed to enhance both effectiveness and efficiency. Inspired by the resource allocation strategy (L\u00fc and Zhou, 2011; Lin et al., 2015), we propose a flow-based pruning algorithm with distance awareness to extract key paths.\nFormally, we denote the sets of nodes pointing to $v_i$ and nodes pointed by $v_i$ as $N(v_i, \\cdot)$ and $N(\\cdot, v_i)$, respectively. We define the resource of node $v_i$ as $S(v_i)$. We set $S(v_{start}) = 1$ and initialize other resources to 0, followed by propagating the resources through the neighborhood. The resource flowing to $v_i$ is defined as:\n$S(v_i) = \\sum_{v_j \\in N(\\cdot, v_i)} \\alpha \\cdot S(v_j)$\nwhere $\\alpha$ represents the decay rate of information propagation along the edges. Based on the assumption that the closer two nodes are in the indexing graph, the stronger their connection will be, we introduce this penalty mechanism to enable the retriever to perceive distance. It is crucial to emphasize that our approach differs from strictly sorting paths with a limited number of hops. Detailed comparative experiments will be presented in subsequent sections.\nNotably, due to the decay penalty and neighbor allocation, nodes located far from the initial node are assigned with negligible resources. Therefore, we introduce an early stopping strategy to prune paths in advance when\n$\\frac{S(v_i)}{|N(v_i, \\cdot)|} < \\theta$,\nwhere $\\theta$ is the pruning threshold. This ensures that the algorithm terminates early for nodes that contribute minimally to the overall propagation."}, {"title": "", "content": "For efficiency concerns, we update the resource of a node at most once.\nWe denote each path as an ordered sequence $P = v_0 \\xrightarrow{e_1} \\cdot \\cdot \\cdot \\xrightarrow{e_n} v_n = (V_P, E_P)$, where $v_i$ and $e_i$ represent the i-th node and directed edge, and VP and EP represent the set of nodes and edges in the path P, respectively. For each path P = (VP, EP), we calculate the average resource values flowing through its edges as the measurement of reliability, which can be formulated as:\n$S(P) = \\frac{1}{|E_p|} \\sum_{v_i \\in V_p} S(v_i)$,\nwhere $|E_P|$ is the number of edges in the path. Then, we sort these paths based on the reliability S(P) and retain only the most reliable relational paths for this node pair. These paths are added to the global candidate pool in the form of path-reliability pair (P, S(P)). We repeat the above process for each distinct node pair, ultimately obtaining all candidate paths. Then the top-K reliable paths can be obtained from the candidate pool to serve as the retrieval information of query q for subsequent generation, which we denote as $P_q$."}, {"title": "Answer Generation", "content": "For better answer generation, we establish path prioritization based on their reliability, then strategically position these paths to align with LLMs' performance patterns (Qin et al., 2023; Liu et al., 2024; Cuconasu et al., 2024).\nFormally, for each retrieved relational path, we concatenate the textual chunks of all nodes and edges within the path to obtain a textual relational path, which can be formulated as:\n$t_P = concat([\\cdot \\cdot \\cdot; t_{v_i}; t_{e_i}; t_{v_{i+1}}; \\cdot\\cdot\\cdot])$\nwhere concat(\u00b7) denotes the concatenation operation, vi and ei are the i-th node and edge in the path P, respectively.\nConsidering the \u201clost in the middle\" issue (Liu et al., 2024; Cao et al., 2024; Firooz et al., 2024) for LLMs in long-context scenarios, directly aggregating the query with different relational paths may lead to suboptimal results. Therefore, we position the most critical information at the two ends of the template, which is regarded as the golden memory region for LLM comprehension. Specifically, we place the query at the beginning of the template and organize the textual relational paths in a reliability ascending order, ensuring that the most reliable relational path is positioned at the end of the template. The final prompt can be denoted as:\n$M(q; R(q, G)) = concat([q; t_{P_k}; \\cdot \\cdot \\cdot ; t_{P_1}])$,\nwhere $P_1$ is the most reliable path and PK is the K-th reliable path. This simple prompting strategy can significantly improve the response performance of LLM compared with placing the paths in a random or reliability ascending order in our experiments."}, {"title": "Discussion", "content": "Complexity Analysis of Path Retrieval. After the i-th step of resource propagation, there are at most $a_i$ nodes alive due to the decay penalty and early stopping. Hence the total number of nodes involved in this propagation is at most $\\sum_{i=0}^{I} a^i = \\frac{1}{1-a}a^0 = O(\\frac{1}{1-a})$.\nThus the complexity of extracting candidate paths between all node pairs is $O(\\frac{1}{1-a})^2$. In our settings, the number of retrieved nodes N \u2208 [10, 60] is much less than the total number of nodes in the indexing graph |V| ~ 104. Thus the time complexity is completely acceptable.\nNecessity of Path-based Prompting. Note that different retrieved paths may have shared nodes or edges. To reduce the prompt length, it is possible to flatten the paths and remove duplications as a set of nodes and edges. However, this conversion will lose the semantic relations between the two end-points of each path. We also validate the necessity of path-based prompting in the experiments."}, {"title": "Experiments", "content": "We conduct extensive experiments to answer the following research questions (RQs): RQ1: How effective is our proposed PathRAG compared to the state-of-the-art baselines? RQ2: How do different values of key hyperparameters influence the method's performance? RQ3: Has each component of our framework played its role effectively? RQ4: How much token cost does PathRAG require to achieve the performance of other baselines? RQ5: Do the RAG response and its evaluation of PathRAG offer some interpretability?"}, {"title": "Experimental Setup", "content": "Datasets\nWe follow the settings of LightRAG (Guo et al., 2024) and evaluate our model using the UltraDomain benchmark (Qian et al., 2024). The UltraDomain data is sourced from 428 college textbooks"}, {"title": "Baselines", "content": "We compare PathRAG with four state-of-the-art methods: NaiveRAG (Gao et al., 2023), HyDE (Gao et al., 2022b), GraphRAG (Edge et al., 2024), and LightRAG (Guo et al., 2024). These methods cover cutting-edge text-based and graph-based RAG approaches. Detailed descriptions of the baselines can be found in the Appendix B."}, {"title": "Implementation Details", "content": "To ensure fairness and consistency across experiments, we uniformly use \u201cGPT-40-mini\u201d for all LLM-related components across both the baseline methods and our approach. Also, the indexing graphs for different graph-based RAG methods are the same as GraphRAG (Edge et al., 2024). Retrieved edges that correspond to global keywords of LightRAG are placed after the query. For the key hyperparameters of PathRAG, the number of retrieval nodes N is selected from {10, 20, 30, 40, 50, 60}, the number of paths K is varied within {5, 10, 15, 20, 25}, the decay rate a is chosen from {0.6,0.7, 0.8, 0.9, 1.0}, and the threshold \u03b8 is fixed as 0.05."}, {"title": "Evaluation Metrics", "content": "Due to the absence of ground truth answers, we follow the LLM-based evaluation procedures as GraphRAG and LightRAG. Specifically, we utilize \u201cGPT-40-mini\u201d to evaluate the generated answers across multiple dimensions. The evaluation dimensions are based on those from GraphRAG and LightRAG, including Comprehensiveness and Diversity, while also incorporating three new dimensions from recent advances in LLM-based evaluation (Chan et al., 2023), namely Logicality, Relevance, and Coherence. We compare the answers generated by each baseline and our method and conduct win-rate statistics. A higher win rate indicates a greater performance advantage over the other. Note that the presentation order of two answers will be alternated, and the average win rates will be reported. Detailed descriptions of these evaluation dimensions can be found in Appendix C."}, {"title": "Main Results (RQ1)", "content": "As shown in Table 1, PathRAG consistently outperforms the baselines across all evaluation dimensions and datasets.\nFrom the perspective of evaluation dimensions, compared to all baselines, PathRAG shows an average win rate of 60.88% in Comprehensiveness, 62.75% in Diversity, 59.78% in Logicality, 60.47% in Relevance, and 59.93% in Coherence on average. These advantages highlight the effectiveness of our proposed path-based retrieval, which contributes to better performance across multiple aspects of the generated responses. From the dataset perspective, PathRAG has a win rate of 60.13% in Agriculture, 60.26% in CS and 59.02% in Mix on average. For the larger three datasets, PathRAG shows greater advantages, with an average win rate of 65.53% in Legal, 60.13% in History and 59.50% in Biology. This indicates that our proposed PathRAG effectively reduces the impact of irrelevant information when handling larger datasets, making it more aligned with real-world applications and offering stronger practical significance compared to existing RAG baselines."}, {"title": "Hyperparameter Analysis (RQ2)", "content": "We adjust one hyperparameter at a time on the Legal dataset, and then calculate the win rates compared with LightRAG, the best baseline.\nNumber of retrieved nodes (N). As shown on the left side of Figure 3, we observe that as N increases, the average win rate gradually improves, peaking at N = 40, followed by a slight decline. This is because the retrieved path information becomes increasingly sufficient as the number of nodes grows. However, as N continues to increase, the retrieved nodes are less relevant to the question and negatively impact the performance.\nNumber of retrieved paths (K). As shown in the middle of Figure 3,\nwe observe that as K increases, the average win rate reaches its peak at K = 15. When K = 25, the average win rate drops, meaning that additional retrieved paths can not bring further improvement to the model. In practice, larger datasets prefer larger values of K.\nDecay rate a. As shown on the right side of Figure 3, when a = 0.6, the pruning algorithm prioritizes shorter paths, resulting in an average win rate of only 0.57. As a increases, the average win rate peaks at 0.63 when a = 0.8, but then begins to decline. At a = 1.0, where the decay rate is completely ignored, the average win rate significantly drops. This suggests that prioritizing shorter paths with a proper a serves as effective prior knowledge for the pruning process."}, {"title": "Ablation Study (RQ3)", "content": "We conduct ablation experiments to validate the design of PathRAG. A detailed introduction to the variants can be found in Appendix D.\nNecessity of path ordering. We consider two different strategies to rank the retrieved paths in the prompt, namely random and hop-first. As shown in the Table 2, the average win rates of PathRAG compared to the random and hop-first variants are respectively 56.75% and 56.08%, indicating the necessity of path ordering in the prompts.\nNecessity of path-based prompting. While retrieval is conducted using paths, the retrieved information in the prompts does not necessarily need to be organized in the same manner. To assess the necessity of path-based organization, we compare prompts structured by paths with those using a flat"}, {"title": "", "content": "organization. As shown in Table 3, path-based prompts achieve an average win rate of 56.14%, outperforming the flat format. In PathRAG, node and edge information within a path is inherently interconnected, and separating them can result in information loss. Therefore, after path retrieval, prompts should remain structured to preserve contextual relationships and enhance answer quality."}, {"title": "Token Cost Analysis (RQ4)", "content": "For a fair comparison focusing on token consumption, we also consider a lightweight version of PathRAG with N = 20 and K = 5, dubbed as PathRAG-lt. PathRAG-lt performs on par with LightRAG in overall performance, achieving an average win rate of 50.69%. The average token consumptions per question for LightRAG, PathRAG and PathRAG-lt are 15, 837, 13, 318 and 8, 869, respectively. Hence PathRAG reduces 16% token cost with much better performance, and the corresponding monetary cost is only 0.002$. PathRAGIt reduces 44% tokens while maintaining comparable performance to LightRAG. These results demonstrate the token efficiency of our method."}, {"title": "Case Study (RQ5)", "content": "To provide a more intuitive demonstration of the evaluation process, we present a case study from the Agriculture dataset. Given the same question, both LightRAG and PathRAG generate responses based on the retrieved text. The responses are then evaluated by GPT-40-mini across five dimensions, with justifications provided,. We highlight the key points in the answers in bold, with LLM justification for winning judgments displayed in blue and losing judgments in purple. The case study demonstrates that our proposed path information retrieval method provides comprehensive support for answer generation. PathRAG exhibits clear advantages in all five dimensions."}, {"title": "Conclusion", "content": "In this paper, we propose PathRAG, a novel graph-based RAG method that focuses on retrieving key relational paths from the indexing graph to alleviate noise. PathRAG can efficiently identify key paths with a flow-based pruning algorithm, and effectively generate answers with path-based LLM prompting. Experimental results demonstrate that PathRAG consistently outperforms baseline methods on six datasets. In future work, we will optimize the indexing graph construction process, and consider to collect more human-annotated datasets for graph-based RAG. It is also possible to explore other substructures besides paths."}, {"title": "Limitations", "content": "This work focuses on how to retrieve relevant information from an indexing graph for answering questions. For a fair comparison with previous methods, the indexing graph construction process is not explored. Also, we prioritize simplicity in our proposed PathRAG, and thus the path retrieval algorithm involves no deep neural networks or parameter training, which may limit the performance. Besides, we follow the evaluation protocol of previous graph-based RAG methods, and the metrics are relative rather than absolute. We will consider to collect more datasets and design new metrics for graph-based RAG in future work."}, {"title": "Dataset Descriptions", "content": "We conduct experiments on the following six datasets, and the statistics of each dataset and corresponding indexing graph are shown in Table 4.\n\u2022 Agriculture dataset: This dataset focuses on the agricultural domain, covering various aspects of agricultural practices, such as beekeeping, crop cultivation, and farm management.\n\u2022 Legal dataset: This dataset focuses on the legal domain, covering various aspects of legal practices, such as case law, legal regulations, and judicial procedures.\n\u2022 History dataset: This dataset focuses on the field of history, covering various periods, events, and figures throughout time. It includes historical texts, articles, and documents related to world history, significant historical movements, and important historical figures from different regions and cultures.\n\u2022 CS dataset: This dataset focuses on the field of computer science, covering multiple subfields such as algorithms, data structures, artificial intelligence, machine learning, and computer networks. It particularly provides various practical application examples in the areas of machine learning and big data.\n\u2022 Biology dataset: This dataset focuses on the field of biology, covering a wide range of topics such as plants, animals, insects, and more. It provides detailed information about the physical characteristics, behaviors, ecosystems, and other aspects of various organisms.\n\u2022 Mix dataset: This dataset contains a variety of literary classics, including essays, poetry, and biographies, covering multiple fields such as philosophy, history, and literature."}, {"title": "Baseline Descriptions", "content": "The detailed baseline descriptions are as follows:\n\u2022 NaiveRAG: This method is mainly used for retrieving information from text databases by splitting the text into chunks for storage. During the storage process, the chunks are embedded using text embeddings. For a query, the question is converted into a text embedding, and retrieval is performed based on maximum similarity between the query embedding and the text chunks, enabling efficient and direct access to answers.\n\u2022 HyDE: This model shares a similar storage framework with NaiveRAG. However, during the"}, {"title": "Evaluation Dimensions", "content": "LLM will evaluate RAG responses based on the following five dimensions:\n\u2022 Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question?\n\u2022 Diversity: How varied and rich is the answer in providing different perspectives and insights on the question?\n\u2022 Logicality: How logically does the answer respond to all parts of the question?\n\u2022 Relevance: How relevant is the answer to the question, staying focused and addressing the in-tended topic or issue?\n\u2022 Coherence: How well does the answer maintain internal logical connections between its parts, ensuring a smooth and consistent structure?"}, {"title": "Details of Ablated Variants", "content": "Path Ordering\n\u2022 Random ordering. We randomly select K paths and place them into the prompt.\n\u2022 Hop-first ordering. Paths are sorted based on the number of hops. Paths with fewer hops are considered to have more direct relevance. Within the same hop count, paths are randomly ordered. Finally, K paths are selected and arranged in ascending order, placing the most important paths at the end of the prompt to enhance memory retention."}, {"title": "Prompt Format", "content": "\u2022 Flat organization. In this setting, the retrieved paths are decomposed into individual nodes and edges. The order of nodes and edges is randomized and not structured based on their original paths."}, {"title": "Detailed Comparison between PathRAG-lt and LightRAG", "content": "Table 5 presents the win rates of PathRAG-lt against LightRAG on six datasets. PathRAG-lt has an overall win rate of 50.69%."}, {"title": "Additional Case study", "content": "We also provide an additional case study comparing PathRAG and LightRAG on the CS dataset. Given the question, \"What derived features should be considered to enhance the dataset's predictive power? \", both LightRAG and PathRAG generate responses based on the retrieved text. These re-sponses are then evaluated by GPT-40-mini across"}]}