{"title": "SZU-AFS Antispoofing System for the ASVspoof 5 Challenge", "authors": ["Yuxiong Xu", "Jiafeng Zhong", "Sengui Zheng", "Zefeng Liu", "Bin Li"], "abstract": "This paper presents the SZU-AFS anti-spoofing system, designed for Track 1 of the ASVspoof 5 Challenge under open conditions. The system is built with four stages: selecting a baseline model, exploring effective data augmentation (DA) methods for fine-tuning, applying a co-enhancement strategy based on gradient norm aware minimization (GAM) for secondary fine-tuning, and fusing logits scores from the two best-performing fine-tuned models. The system utilizes the Wav2Vec2 front-end feature extractor and the AASIST back-end classifier as the baseline model. During model fine-tuning, three distinct DA policies have been investigated: single-DA, random-DA, and cascade-DA. Moreover, the employed GAM-based co-enhancement strategy, designed to fine-tune the augmented model at both data and optimizer levels, helps the Adam optimizer find flatter minima, thereby boosting model generalization. Overall, the final fusion system achieves a minDCF of 0.115 and an EER of 4.04% on the evaluation set.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Artificial Intelligence Generated Content (AIGC) have significantly enhanced the naturalness, fidelity, and variety of speech. Unfortunately, this progress has resulted in a proliferation of forgeries that can be almost indistinguishable from authentic speech to the human auditory system. Concurrently, automatic speaker verification (ASV) systems have become increasingly susceptible to spoofing and deepfake attacks, in which attackers produce convincingly realistic simulations of the target speaker's voice [1]. The potential misuse of spoofed speech presents significant societal risks. Therefore, developing a robust and generalizable anti-spoofing system to counter these threats has emerged as a critical research imperative.\nThe ASVspoof challenges [2, 3, 4, 5, 6] have significantly boosted interest in developing robust detection solutions for spoofing and deepfake attacks, thereby enhancing the security and reliability of ASV systems. These challenges provide standardized benchmark protocols and comprehensive evaluation datasets. What's more, the last four ASVspoof challenges [2, 3, 4, 5] have prompted the proposal of numerous innovative spoofing detection methods [7, 8, 9, 10].\nHeld in 2024, the ASVspoof 5 Challenge [6] presents two distinct conditions, open and closed, for both Track 1 which focuses on standalone speech deepfake detection, and Track 2 which is dedicated to spoofing-robust automatic speaker verification. Under the closed conditions, participants are restricted from using specified data protocols. Conversely, the open conditions offer greater flexibility, allowing participants to utilize external data and pre-trained self-supervised models, provided there is no overlap between training data (i.e., that used for training foundational models) and evaluation data. Track 1 is similar to the DF track of the ASVspoof 2021 Challenge, reflecting a scenario in which an attacker has access to a targeted victim's voice data, such as data posted on social media. In this scenario, the evaluation set contained data processed with conventional codecs or modern neural codecs. Track 2, similar to the LA sub-task from previous ASVspoof challenges, is pred-"}, {"title": "2. Methodology", "content": "The SZU-AFS anti-spoofing system consists of four stages detailed in separate subsections: baseline model, data augmentation, gradient norm aware minimization (GAM)-based co-enhancement strategy, and score-level fusion. Note that we trained ten different baseline models, six augmented models, and eight models using various DA and GAM methods. Model IDs are labeled A to D, and numbers indicate versions."}, {"title": "2.1. Baseline Model", "content": "2.1.1. Front-end feature extractor\nGiven the urgent need to improve the generalizability of spoofing detection systems, speech self-supervised models have gained increasing attention. Prior research shows that using speech self-supervised models as the front-end feature extractors and the back-end classifier, can substantially improve the generalization of spoofing detection models [16, 17, 18, 19].\nWe have used the self-supervised WavLM-Base\u00b9 [20], HuBERT-Base\u00b2 [21], and Wav2Vec2-Large\u00b3 [14] as front-end feature extractors instead of conventional handcrafted acoustic features, such as linear frequency cepstral coefficients and mel-spectrograms. The self-supervised learning models extract speech representations or embeddings from the raw waveform.\n2.1.2. Back-end classifier\nThe back-end classifiers of the latest spoofing detection systems mainly adopt deep learning methods [22, 23, 24], significantly outperforming traditional classifiers such as support vector machine and Gaussian mixture model [25, 26]. We have tried three representative classifiers combined with front-end pre-trained models, detailed as follows:\n\u2022 Fully connected (FC) classifier [22]: This classifier combines a global average pooling layer, followed by a neural network with three fully connected layers employing LeakyReLU activation functions. It ends with a linear output layer for binary classification.\n\u2022 Conformer classifier [23]: This classifier combines a convolutional neural network and a Transformer network for spoofing detection. It comprises four blocks, each with four attention heads and a kernel size of 31, totaling 2.4 million parameters.\n\u2022 AASIST classifier [24]: This classifier combines a RawNet2-based encoder [27] and a graph network module. Specifically, it removes the Sinc convolutional layer-based front-end from the RawNet2-based encoder.\n2.1.3. Model selection\nAs shown in Table 2, we have evaluated the detection performance of the A1-A10 models using the development set of ASV spoof 5. The A1-A9 models are combinations of three pre-trained models with three different classifiers, while the A10 model combines the Wav2Vec2 pre-trained model with all classifiers, generating predictive scores by processing concatenated features through a linear layer. According to experimental results, we have selected the A9 model as the baseline by utilizing a Wav2Vec2-based front-end feature extractor paired with an AASIST-based back-end classifier."}, {"title": "2.2. Primary Fine-tuning with Data Augmentation", "content": "To enhance the generalization performance, we have conducted experiments with three DA policies: single-DA, random-DA, and cascade-DA, to fine-tune the baseline model. The three DA policies, as depicted in Figure 2, are detailed as follows."}, {"title": "2.2.1. Single-DA policy", "content": "A specific DA method is applied to all original training data for the single-DA policy. The details of the DA methods employed are described below:\n\u2022 RIR 4: The room impulse response (RIR) captures the acoustic characteristics of a room or an environment. A noise clip is randomly selected from the RIR database and superimposed onto the original training speech, with the intensity randomly varying between 20% and 80%.\n\u2022 RawBoost [28]: RawBoost incorporates 3 distinct types of noise: linear and non-linear convolutive (LnL) noise, stationary signal-independent additive (SSI) noise, and impulsive signal-dependent additive (ISD) noise.\n\u2022 Signal companding: The a-law and \u00b5-law are signal companding methods developed to enable the transmission of signals with a large dynamic range through systems with limited dynamic range capabilities. During the enhancement of the input speech, either a-law or \u00b5-law is randomly selected.\n\u2022 TimeMask: For the input speech, consecutive time steps from $t_0$ to $t_0 + t$ are set to zero. The duration $t$ is uniformly selected from 0 to T, and the starting point $t_0$ is randomly chosen from the interval $[0, \\tau - t)$. Here, $\\tau$ represents the total number of time steps, and T varies randomly between 20% and 50% of $\\tau$.\n\u2022 Mixup [29]: Mixup regularization involves training the model using a set of mixed speech utterances and labels, rather than the original training data, with the interpolation parameter sampled from a symmetric Beta($\\sigma$, $\\sigma$) distribution, where $\\sigma = 1.0$.\n\u2022 Amplitude: Amplitude enhancement involves selecting two speech utterances from the same speaker and label, mixing their amplitude spectra with a certain probability, and then applying inverse Fourier transformation with the corresponding phase spectra to obtain the enhanced utterances."}, {"title": "2.2.2. Random-DA policy", "content": "Unlike a single-DA strategy, the random-DA policy involves randomly selecting an augmentation method from a DA set for each utterance of the original training data. More specifically, we used three DA sets:\n\u2022 Noise set: This set contains 3 noise-based DA methods from the audiomentations library, with corresponding modules named AddColorNoise, AddGaussianNoise, and AddGaussianSNR.\n\u2022 Filter set: This set contains 7 filter-based DA methods from the audiomentations library, with corresponding modules named BandPassFilter, BandStopFilter, High-PassFilter, HighShelfFilter, LowPassFilter, LowShelf-Filter, and PeakingFilter.\n\u2022 Mix set: This set contains 13 DA methods of mixed types from the audiomentations library, with corresponding modules named AddGaussianNoise, AirAbsorption, Aliasing, BandPassFilter, Shift, PitchShift, HighPass-Filter, LowPassFilter, PolarityInversion, PeakingFilter, TimeStretch, TimeMask, and TanhDistortion."}, {"title": "2.2.3. Cascade-DA policy", "content": "The cascade-DA policy encourages selecting two or more DA methods in a sequential cascade manner to enhance the original training data progressively. Three types of cascade-DA methods are given below:\n\u2022 RIR-TimeMask: RIR-TimeMask consists of a two-level cascade of DA methods, sequentially adding RIR noise and TimeMask method to the original training data.\n\u2022 LnL-ISD: LnL-ISD consists of a two-level cascade of DA methods, sequentially adding LnL and ISD noise to the original training data. Both LnL noise and ISD noise are derived from the RawBoost method.\n\u2022 Noise-Filter: Noise-Filter consists of a two-level cascade of DA sets, sequentially applying one method randomly selected from the noise set and another from the filter set to enhance the original training data.\nNote that the RIRTimeMask method is used in the primary fine-tuning stage, while LnL-ISD, Noise-Filter, and combinations of cascade-DA methods are used in the secondary fine-tuning stage."}, {"title": "2.2.4. Model selection", "content": "The A9 model is fine-tuned using only the on-the-fly augmented data. We have evaluated the detection performance of six models (B1-B6, which are shown in Table 3) with different DA methods, using the progress set of ASVspoof 5. Specifically, we have fine-tuned the A9 model using distinct DA methods, including Amplitude, a-law or \u00b5-law, Mix, and RIR-TimeMask. The B4-B6 models share the same augmentation methods but vary in the number of input speech samples used for training: 64,600, 96,000, and 128,000, respectively. Following experimental analysis, the B5 model has been chosen for further investigation."}, {"title": "2.3. Secondary Fine-tuning with GAM-based Co-enhancement Strategy", "content": "Unlike DA methods, which focus on increasing the diversity of training data, the GAM method is an optimization approach for enhancing model generalization. To alleviate this issue, the fine-tuning process has been divided into two stages: a primary stage without GAM, as described in the previous subsection, and a secondary stage with DA and GAM co-enhancement, as illustrated in this subsection.\n2.3.1. Gradient norm aware minimization\nSharpness-aware minimization (SAM) [30] and its variants [31] are representative training algorithms to seek flat minima for better generalization. Shim et al. [32] employed SAM and its variants in spoofing detection, improving model generalization. Inspired by this, we exploit a recently proposed optimization method, gradient norm aware minimization (GAM) [33].\nGAM seeks flat minima with uniformly small curvature across all directions in the parameter space. Specifically, it improves the generalization of models trained with the Adam optimizer by optimizing first-order flatness, which controls the maximum gradient norm in the neighborhood of minima.\nLet $\\theta \\in \\mathbb{R}^d$ denote the parameters of the B5 model. The Adam optimizer is then described as follows:\n$\\begin{aligned}  g_t &= \\nabla \\mathcal{L}(\\theta_t), \\\\ \\theta_{t+1} &= \\theta_t - \\eta g_t,\\end{aligned}$\nwhere t is the time step, $\\eta$ is the learning rate, and $g_t$ is the loss gradient. For the first-order flatness $R_1(\\theta)$, it could be computed by:\n$R_1(\\theta) \\triangleq \\max_{\\theta' \\in B(\\theta, \\rho)} \\rho \\cdot \\|\\nabla \\mathcal{L}(\\theta')\\|,$ where $\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} l(\\theta, x_i, y_i)$ denotes the empirical loss function, $x_i$ and $y_i$ denote the i-th speech sample and its corresponding label, respectively. $\\rho > 0$ is the perturbation radius that controls the magnitude of the neighborhood, and $B(\\theta, \\rho)$ denotes the open ball of radius $\\rho$ centered at the parameter $\\theta$ in the Euclidean space. For detailed derivation, see Appendix A of [33]. The key to optimizing generalization error with GAM is controlling the loss function $\\mathcal{L}(\\theta)$ and first-order flatness $R_1(\\theta)$. The pseudocode of the whole optimization procedure is shown in Algorithm 1.\n2.3.2. Co-enhancement strategy\nThe GAM-based co-enhancement strategy involves data augmentation of the input speech and combines the GAM method with the Adam optimizer to further fine-tune the DA-augmented baseline model (B5). Unlike the primary fine-tuning with DA methods, this strategy has explored more efficient two-level and three-level DA methods, combined with RIR or TimeMask, to process the original training data. Specifically, we have combined eight different DA methods with GAM: C1 (RIR), C2 (a-law or \u00b5-law), C3 and C4 (Mix), C5 and C6 (LnL-ISD), C7 (RIR + Mix), C8 and C9 (RIR-TimeMask), C10 and C11 (RIR-TimeMask + Mixup), and C12 (RIR + Noise-Filter). As shown in Table 4, we have evaluated the detection performance of models C1-C12 using the progress set of ASVspoof 5. Experimental analysis indicates that the C11 and C12 models are the two best-performing models in terms of minDCF."}, {"title": "2.4. Score-level Fusion", "content": "The individual model scores have been directly output as logits from the linear layer without applying min-max normalization. Building on this, we have utilized an average score-level fusion method, where the predicted scores from each model have been summed and averaged to determine the final prediction score.\nAs shown in Table 5, we have evaluated the detection performance of fused models D1-D4 on either the progress or evaluation sets of ASVspoof 5. Specifically, we have tested four fused models: D1 (B4 + B5), D2 (B1 to B6), D3 (C8 + C9), and D4 (C11 + C12). Among these models, we have selected the best-performing fused system, D4, for submission to the evaluation phase."}, {"title": "3. Experimental Setup", "content": "3.1. Datasets and Metrics\n3.1.1. Datasets\nThis paper focuses on the Track 1 stand-alone speech deepfake detection task of ASVspoof 5, with a summary of the Track 1 database provided in Table 1. The dataset contains 1,044,846 utterances, each encoded as a 16 kHz, 16-bit FLAC file. The training and development sets each contain spoofed speech generated by 8 different text-to-speech (TTS) or voice conversion (VC) methods. In contrast, the evaluation set includes spoofed speech from 16 diverse attack methods, including TTS, VC, and, for the first time, adversarial attacks. The evaluation set contains more than twice the number of samples as the combined training and development sets, making detection significantly more challenging. Notably, the progress set is a subset of the evaluation set.\n3.1.2. Metrics\nDifferent from previous ASVspoof challenges, ASVspoof 5 Challenge uses the minDCF as the primary metric for the comparison of spoofing countermeasures, with the cost of log-likelihood ratio ($C_{llr}$) [34] and the equal error rate (EER) as a secondary metrics. Accuracy (ACC) was introduced to evaluate the detection model's performance on the development set. In contrast, EER provides a more suitable measure of performance when the data is limited or imbalanced. Thus, EER is better suited than ACC for evaluating spoof detection models.\nThe normalised detection cost function (DCF) is:\n$DCF(T_{cm}) = \\beta \\cdot P_{miss}(T_{cm}) + P_{fa}(T_{cm}), \\quad \\beta = \\frac{C_{miss} (1 - s_{pf})}{C_{fa} s_{pf}}$\nwhere $T_{cm}$ is the detection threshold, $s_{pf}$ is asserted prior probability of spoofing attack, and $C_{miss}$ and $C_{fa}$ are the costs of a miss and a false alarm, respectively. The following parameters were used for the ASVspoof 5 challenge evaluation: $C_{miss} = 1$, $C_{fa} = 10$, $s_{pf} = 0.05$, and $\\beta \\approx 1.90$. The normalized DCF in (3) is used to compute the minimum DCF, defined as minDCF = $\\min_{T_{cm}} DCF(T_{cm})$.", "equation": "DCF(T_{cm}) = \\beta \\cdot P_{miss}(T_{cm}) + P_{fa}(T_{cm}), \\quad \\beta = \\frac{C_{miss} (1 - s_{pf})}{C_{fa} s_{pf}}"}, {"title": "3.2. Training Details", "content": "In our experiments, the following parameters were kept consistent. We used the Adam optimizer ($\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$) [35], with an initial learning rate of 5\u00d710\u207b\u2076, controlled by a cosine annealing scheduler with a minimum learning rate of 1 \u00d7 10\u207b\u2078, and a maximum of 100 training epochs. The training was conducted using conventional cross-entropy loss, with early stopping applied if the development set loss did not improve within ten epochs. All experiments were executed on two NVIDIA A100 GPUs. The training epochs for the models used to obtain the D4 system were as follows: 12 epochs (A9), 4 epochs (B5), 2 epochs (C11), and 5 epochs (C12). The training time required for the combined DA and GAM method is approximately three times that of the regular DA method alone. The results table highlights the best-performing values in bold for each column."}, {"title": "4. Results and Analysis", "content": "4.1. Comparison Analysis of Different Baseline Models\nWe integrated three pre-trained models with three classifiers to assess the necessity of different baseline models, testing their performance on the Track 1 development set. The training and development data were truncated or padded to 64,600 sample points to accommodate GPU memory constraints.\nThe accuracy and EER for different baseline models are reported in Table 2. As indicated in Table 2, we observe:\n\u2022 Among the different feature extractors, Wav2Vec2-based detection models (e.g., A7, A8, A9, and A10) achieved higher accuracy and lower EER, which indicates their better effectiveness than WavLM-based and HuBERT-based detection models. This result is due to the fact that both the WavLM and HuBERT pre-trained models use the base version, which contains fewer than one-third of the learnable parameters in the Wav2Vec2-Large model.\n\u2022 Among the different classifiers, AASIST proved to be more competitive than others (e.g., FC and Conformer) when paired with various feature extractors, further confirming its excellent performance in speech spoofing detection. Furthermore, leveraging the Wav2Vec2 feature extractor, we concatenated the final predictive outputs from three classifiers for classification. However, the A10 model's results fell between the individual classifiers' results, providing no improvement. Thus, using only the best classifier is sufficient.\n\u2022 While the A9 model's accuracy is slightly lower at 87% compared to the A7 and A10 models, it achieves the lowest EER at 1.55%. Owing to its outstanding EER performance, the A9 model has been selected for further experimental exploration.\n4.2. Comparison Analysis of Different DA\nTable 3 shows the effectiveness of various DA methods during the progress phase of Track 1. Compared with the B1 and B3 models, the B2 and B4 models achieved lower minDCF and EER. The A9 model presents superior detection performance when fine-tuned with signal compression (a-law or \u00b5-law), RIR noise, and TimeMask. However, the B4 model exhibited a higher $C_{ur}$. The B5 model outperforms all other models in terms of minDCF, $C_{ur}$, and EER at 0.043, 0.235, and 1.5%, respectively. Owing to its outstanding performance, the B5 model has been selected as the augmented model for further experimental exploration.\nAlthough current experimental results do not conclusively determine which of the three different DA policies is most effective. We recommend prioritizing experimental exploration under random-DA and cascade-DA policies for speech spoofing detection tasks.\n4.3. Effect of GAM-based Co-enhancement Strategy\nWith the B5 model's good results, we also investigate whether the spoofing detection performance can be further improved by using the GAM method. Table 4 shows the performance of various DA methods and GAM method on the Track 1 progress phase.\nFor the effect of data augmentation, the C1 and C2 models did not significantly improve minDCF and EER over the B5 model in the progress phase. Specifically, the B5 model using RIR-TimeMask (C8 and C9 models) and its combination with the Mixup (C10 and C11 models) outperformed the C2 and C3 models across most metrics, indicating that more complex augmentation can be learning more robust features. In addition, the comparison among models from C8 to C11 shows that the Mixup method significantly improves both minDCF and EER, which suggests that it contributes to the improvement's generalizability. The GAM method, particularly in B5 and C9 models, improved minDCF and EER, effectively enhancing model generalizability. The experimental results demonstrate the importance of selecting appropriate data augmentation and optimization techniques to enhance spoofing detection performance."}, {"title": "4.4. Comparison Analysis of Different Fused Systems", "content": "Table 5 shows the performance of the four fused systems on either the progress or evaluation sets of ASVspoof 5. We observed that score-level average fusion enhances model performance compared to individual detection models, particularly in minDCF and EER metrics. Fusing C11 and C12 models (D4) resulted in optimal progress phase performance, achieving a minDCF of 0.027 and an EER of 0.99%. However, the D4 system exhibited a significant performance discrepancy between the progress and evaluation phases, highlighting the challenging nature of the evaluation set."}, {"title": "4.5. Impact of Different Sample Points", "content": "Table 3 also shows a comparison in terms of sample points for the model training. Using 96,000 sample points of input speech, the B5 model achieved a lower actDCF and $C_{ur}$ than B4 and B6. The B6 model exhibited poor performance, indicating that increasing the number of training samples does not necessarily enhance the model's detection capabilities. In fact, inputting more sample points for training may reduce the model's generalization ability. Optimizing training with an appropriate number of sample points is more beneficial for improving detection performance than simply increasing the amount of training data.\nThe results presented in Table 4 reveal that when the model was trained using input speech with 64,600 sample points, a significant performance improvement was observed during the inference stage when utilizing input speech with 96,000 sample points. This phenomenon may be associated with the different utterance duration distribution in the progress set. More studies are required to verify this relationship further and analyze it."}, {"title": "5. Conclusion", "content": "This paper describes the SZU-AFS system for Track 1 of the ASV spoof 5 Challenge under open conditions. Instead of focusing on various pre-trained feature fusion and complex score fusion methods, we used DA and GAM enhancement strategies to improve spoofing detection generalization. The final best fused system submitted achieved 0.115 minDCF and 4.04% EER on the ASVspoof 5 challenge evaluation set.\nThe experiments produced a few valuable findings. First, applying the RIR-TimeMask method for data augmentation has proven more effective. Building on this, employing a cascade-DA strategy can further improve model performance. Second, the GAM method significantly improves model generalization when combined with the Adam optimizer on both progress and evaluation sets despite the lengthy training time required. Due to time constraints, the model was fine-tuned in two stages. Using the GAM method throughout the entire process might have produced better results."}]}