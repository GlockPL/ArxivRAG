{"title": "EVALUATING AI EVALUATION: PERILS AND PROSPECTS", "authors": ["John Burden"], "abstract": "As AI systems appear to exhibit ever-increasing capability and generality, assessing their true potential and safety becomes paramount. This paper contends that the prevalent evaluation methods for these systems are fundamentally inadequate, heightening the risks and potential hazards associated with AI. I argue that a reformation is required in the way we evaluate AI systems and that we should look towards cognitive sciences for inspiration in our approaches, which have a longstanding tradition of assessing general intelligence across diverse species. We will identify some of the difficulties that need to be overcome when applying cognitively- inspired approaches to general-purpose AI systems and also analyse the emerging area of \"Evals\". The paper concludes by identifying promising research pathways that could refine AI evaluation, advancing it towards a rigorous scientific domain that contributes to the development of safe AI systems.", "sections": [{"title": "1 Introduction", "content": "Recent years have seen an explosion of interest in mitigating the risks from AI systems, both existential and socio-technical (Hendrycks et al., 2023; Amodei et al., 2016; Critch and Krueger, 2020; Bostrom, 2014; Dobbe and Wolters, 2024; Huang et al., 2023). But how can we actually ensure that an AI system is safe? This is a difficult and multi-faceted question, requiring conscious intervention at every step in the process of creating an AI system, from data-set collection, all the way through to deployment (and beyond). One important part of this process is to develop techniques for building and training safe AI systems, or aligning systems with particular sets of values (Russell, 2019; Gabriel, 2020). However, equally important is identifying whether these techniques have been successful, and subsequently deciding whether a system is ready for deployment. This is the domain of AI evaluation.\nImproved methodologies for both aligning and evaluating AI are necessary to ensure that the seemingly ever-advancing AI systems are safe, ethical, and ultimately beneficial for humanity. This necessity is driven by the increasing capability and generality of AI systems. The success of Large Language Models (or Foundation Models (Bommasani et al., 2022)) such as the GPT series (Brown et al., 2020; OpenAI, 2023a) has demonstrated that a single (albeit incredibly large) model can be trained to perform a wide variety of tasks. These models can be \"fine-tuned\" at low-cost (relative to training a new model) to improve performance on specific tasks. They also demonstrate the ability to adapt to new tasks with just a few examples through a process called \u201cfew-shot learning\" (Brown et al., 2020).\nAn increase in generality adds more than just a fixed, finite number of new use cases. Tasks can be combined or specified in arbitrary ways, for example solving a mathematics equation through poetry in Latin. The task-space over which we need to evaluate the system expands from almost a single point to a large and multi-dimensional vector-space. From a safety perspective, or even just to guarantee a minimum level of performance, this is orders of magnitude more difficult: we cannot simply evaluate performance on every individual task combination. We can't know a priori whether a new point in task-space will yield an unsafe response."}, {"title": "2 Formalising Tasks, Instances, and Performance", "content": "Formalising tasks, instances, and performance will allow us to more precisely discuss the problems and so- lutions in evaluating general-purpose AI systems and their impact on safety. Following Hern\u00e1ndez-Orallo (2017a), we consider a task M comprised of a set of instances \u03bc. The variation in instances can be param- eterised, and this parameterisation constitutes the task-space of M. As a shorthand, we can write this as \u03bc = M(x1, ..., Xn) for some set of parameters defining the variation in M. We assume that the task-space is deterministic and that M forms a bijective function. That is, given parameters [x1,..., xn], there is always and only one instance \u03bc = M(x1,..,xn). If both \u03bc\u2081 and \u00b52 = M(x1,.., xn), then either \u03bc\u2081 = \u03bc2 or our task characterisation is incomplete in its parameters.\nRevisiting our maze example, we can imagine M taking parameters corresponding to the width and height of the maze, as well as parameters determining the layout of the walls and the location of the \"goal\".\nSeparate from, but often highly related to, the task M is the distribution PM from which u is sampled. The distribution PM may represent a natural preponderance of particular instances, or an intentional intervention to focus efforts on more important instances. Considering the maze example again, certain variations of mazes may be more likely than others; for example, we may prefer mazes that are solvable or within a certain range of sizes.\nA system will produce a (possibly stochastic) response on instance \u03bc, denoted as R(\u03c0,\u03bc), which is a numerical value. We do not limit R to producing a scalar response; R can return a vector of responses, such as both a performance metric and a safety metric. Given that the response R(\u03c0, \u03bc) is often stochastic, we will frequently refer to the expected value of this response, denoted by \u03c8(\u03c0, \u03bc) = E[R(\u03c0, \u03bc)]. This \u03c8 is often called the performance of \u03c0on \u03bc. Returning to our recurring maze example, we may obtain a collection of responses from a system interacting with \u03bc. The response could be related to the success or failure"}, {"title": "2.1 Capability-oriented Evaluation and Performance-oriented Evaluation", "content": "I will briefly distinguish between two evaluation styles: performance-oriented and capability-oriented (Burnell et al., 2022; Burden et al., 2023). Broadly speaking, performance-oriented evaluation assesses how well a system performs on a particular test, while capability-oriented evaluation measures the latent factors of the system (operationalised as capabilities) that cause differences in test performance. This view of capa- bilities broadly corresponds to the Conditional Analysis of Model Capabilities (Harding and Sharadin, 2024), where capabilities are attributed to a model if the model succeeds at x when its output is best described as being directed to do x.\nSystem capabilities are often difficult to directly measure-especially compared to directly observable prop- erties like physical size. The presence or value of these latent capabilities must be inferred. Tests must be designed to discriminate between test subjects based on this latent capability. A key aspect of capability- oriented evaluation is the relationship between the capability being measured and the demands placed on that capability by a task instance. In other words, the difficulty of the task instance is crucial. This is at odds with a performance-oriented evaluation approach, which focuses on raw performance on the test itself.\nLet's revisit our maze example to highlight the differences between these two evaluation paradigms. Consider a test battery of mazes comprised of multiple different task instances. Performance-oriented evaluation would report a metric related to the tests themselves, such as the percentage of occurrences a single agent achieved success over the test set. Capability-oriented evaluation, on the other hand, tries to report results related to inherent properties of the agent (or consistent features of the task instances that the agent can handle). With our continuing maze example, capability-oriented evaluation would report something like a \"maze solving ability\". This could be broken down further into more specific factors such as \"obstacle handling\" and \"efficient exploration ability\" or similar. Capability-oriented evaluation doesn't need to rely on high-level abilities: we could also see that the system has a \"size\" capability, relaying the maximum size of the maze that the system could reliably solve. We can easily conceive of further intrinsic features from the tasks that we can use to characterise the boundaries of the system's capabilities.\nCapability-oriented evaluation has several key advantages over the performance-oriented alternative. For instance, what does it mean if a system achieves a particular success rate on a test? Without knowledge of the demands placed on the system by the test's constituent instances the result by itself is often of little value. Performance-oriented evaluations can provide value in comparing test-takers it seems likely that a system receiving a score of 80% is better than a system that only manages 50%, but this doesn't inform us how capable a system may be in itself. The test may have been comprised of trivially easy instances, and neither system is actually very good, or the opposite could be true! Comparing two systems with a performance-oriented evaluation may also be limited in utility; two test-takers that achieve 99% may not have the same capabilities if the test isn't able to discriminate well between high-performers.\nOn the other hand, under capability-oriented evaluation, the reported capability shouldn't change if the distribution of instance difficulty changes. Clearly, capability-oriented evaluation has the potential to be more informative about the system being assessed; however it is generally more difficult to perform such an evaluation because it requires a strong understanding of the relationship between the capability, instance difficulty, and how these affect observable performance is required. This is because many capabilities that we wish to assess about AI systems (and intelligent systems in general) are not directly observable. Evaluators can only observe performance on specific tasks that require certain elements of the capability in question (entangled with many others). Therefore, in order to properly assess a given capability, it needs to be inferred from a variety of performance data. To do this effectively requires a detailed understanding of the capability, as well as how it affects performance on a variety of task-instances."}, {"title": "2.2 The Fallacy of Reification?", "content": "The capabilities we aim to assess using capability-oriented evaluation are, as discussed, not directly mea- surable. Capabilities are often abstract ideas such as \"object permanence\", \"navigational skills\", \"language understanding\", and so on. The \"fallacy of reification\" (sometimes called the \"fallacy of misplaced con- creteness\") refers to treating abstract entities and ideas as if they were real, concrete entities (Whitehead, 1925). Debate about whether abstract objects \"exist\" has existed in philosophy for a long time (see, e.g., (Falguera et al., 2022) for a summary of abstract objects and their contentious history). However, the \"fal- lacy of reification\" does not argue about whether abstract concepts exist, but rather claims that treating an abstract concept (whether it exists in any meaningful way or not) as a concrete object is fallacious.\nHowever, an important tool in science is the Hypothetical construct. These are explanatory variables or factors that are not themselves observable (MacCorquodale and Meehl, 1948). These constructs are ubiquitous (gravity, motivation, intelligence are all constructs) within science. In some sense, these constructs are forms of reification. We say that objects fall to the ground because they are \u201cpulled down by gravity\u201d . We identify a physical location on an object as its centre of mass. In short, we treat constructs as concrete and reify them. Obviously, some concepts (such as gravity or centre of mass) are more validly reifiable than others (e.g., the way we personify nature \u201cMother Nature abhors a vacuum\u201d). How do we know when our hypothetical constructs are fallacious, and when they are justified? This primarily depends on two factors that together can enable us to identify constructs and reifications that are useful and representative of meaningful, real phenomena. The first is a shared understanding of the construct's meaning as argued by Leising and Borgstede (2020). The second is the validity of the construct. Construct validity is the extent to which the constructs actually measure what they claim to measure (Cronbach and Meehl, 1955). Also of importance is the notion of construct legitimacy: the extent to which the theory arguing for the construct is justified (Stone, 2019). Today, construct validity is seen as an overarching term for many types of validation approaches (Messick, 1994). One type that is particularly worth highlighting is what Cronbach and Meehl (1955) refer to as predictive validity: how well does this construct predict future scores on a particular test? Within AI evaluation and safety, prediction is paramount (I argue this more fully in section 2.3) and predictive validity should be at the forefront of any evaluator's mind.\nAnother crucial type of validity is external validity (Campbell et al., 1963). External validity broadly cor- responds to the extent to which the conclusions of a study can be generalised outside the context in which the study took place. With constructs, we need to be mindful not only of the construct validity of tests, but also of their external validity for measuring that construct. That is, even if the test validly measures the construct in one scenario, does it generalise to others? Different populations of subjects, environmental fac- tors, and (sometimes seemingly minor) experimental details can drastically affect a test's ability to validly measure a construct. As an intuitive example, if we imagine a hypothetical test that has high construct validity for measuring intelligence that has been rigorously tested on humans, but then we give the test to a dog, we would likely find that the dog would not score any points on the test, despite dogs clearly having some level of intelligence. The dog was unable to complete the test, at least in part, because it cannot read or write. Even though the test has high construct validity when applied to humans, the test lacks external validity outside the domain in which it was designed. In Messick (1994)'s categorisation, external validity is encapsulated by his notion of construct validity. In this manuscript, for clarity, I will refer to external validity explicitly when it is the property I am referring to, but I follow Messick in that when I refer to construct validity I am implicitly requiring there to be external validity to the population being tested.\nWhen evaluating AI systems, two forms of external validity are particularly relevant. The first relates to external validity across subjects: are pre-existing tests valid when applied to AI systems? Or, like the dog taking the intelligence test, is there a mismatch between the subject and the test that invalidates our results?"}, {"title": "2.3 Evaluation Is For Prediction", "content": "I argue that a core aspect of evaluation is prediction. First, why do we evaluate systems (AI or otherwise)? The immediate answer is \"to determine if the system is suitable for its purpose\". While true, this overlooks the fact that we expect our evaluations to provide insight into how the system will perform outside of the evaluation, during \"deployment\".\nWhen we decide that a system is \"fit for purpose\", we are predicting that it will perform at an acceptable level in future instances of the task. This can be a supervisory process (\u201cIs this system good enough for the task it was designed for?\") or a reflective process (\u201cWhat could be done differently in the future to improve performance on similar tasks?\"), but in either case we are concerned with anticipating future behaviour and performance. We rely on this implicitly, often without realising it, in assessment scenarios. For example, we utilise standardised testing for university applications because we believe these tests demonstrate that a candidate has subject knowledge, work ethic, general problem-solving skills, and so on. But more importantly, we expect these factors to indicate to predict-future success at the university.\nThe same goes for AI. We want to ensure that the AI systems we create are capable and safe. We perform various evaluations and derive performance metrics even the less informative ones such as mean perfor- mance -because we believe these metrics capture important properties of the system that predict whether it will be fit for purpose: capable, reliable, and safe. This belief needs to be more fully and explicitly expressed in our evaluation instruments."}, {"title": "3 Risks From Poor Evaluation", "content": "What are the risks of poor evaluation methodology for AI? First, let's examine the behaviour we are try- ing to avoid. Many undesirable characteristics have been identified as areas of concern for AI systems: negative side-effects (Amodei et al., 2016), power-seeking tendencies (Carlsmith, 2022), mesa-optimisation (Hubinger et al., 2021), exacerbating bias (Ntoutsi et al., 2020), and more. These characteristics are often framed as natural occurrences of optimisation (e.g., power-seeking behaviour naturally arises under cer- tain conditions in Markov Decision Processes (Turner et al., 2021)) or reward mis-specification (Krakovna, 2018). Conscious interventions must be taken in the training and data-curation process to disincentivise these negative characteristics. However, these interventions also need evaluating; the claims they make must be verified, and any trade-offs with other characteristics must be identified.\nFlawed evaluations may lead to undue confidence in strategies for system alignment or addressing safety issues. This could result in unsuitable deployment in safety-critical domains and cause harm. This type of risk also includes overconfidence in the absence of certain characteristics (e.g., bias or deceptive behaviour). These are two sides of the same coin, often framed as separate issues. Concerns about whether a system is robust and reliably safe are arguably the same as concerns about the potential for unsafe behaviour. Evaluations of both issues should be focused on reducing uncertainty about the presence of particular system characteristics related to the consistency of behaviour.\nConsider a system that was trained to complete a task and was subsequently deployed. However, after deployment, the system begins to act in unexpected ways, such as failing to complete the task or exhibiting undesirable characteristics. How could this have arisen? This could have occurred due to a failure in the evaluation methodology (perhaps too little evaluation and testing was done or statistical techniques were misapplied). Alternatively, this could have occurred as an Out-of-Distribution (OoD) error, where the task distribution pm during testing/deployment differs from pm during training. Regardless of why the system began acting in an unexpected manner, the mere fact that such a system was deployed represents a failure of the evaluation process. Not only was the system not \"fit for purpose\", but its behaviour wasn't predictable in the deployment environment."}, {"title": "4 Evaluation of AI systems in Practice", "content": "In previous sections, I've discussed high-level ideals for AI evaluation, highlighting the importance of con- struct validity and predictability, and extolling the benefits of capability-oriented approaches. Now we will explore the stark contrast that is AI evaluation praxis. I will argue that the majority of these techniques are performance-oriented evaluation. Traditional AI evaluation techniques often encapsulate performance as the expected response:\n\u03a8(\u03c0, \u039c) = \u0395 [\u03c8(\u03c0, \u03bc)] = \\int_{\\mu\\epsilon M}PM(\u03bc)\u03c8(\u03c0, \u03bc)d\u03bc\nThis is the mean response with respect to some task distribution. Recall that \u03c8 is the expected value of response R of system on instance \u03bc. Given that we don't simply have 4, pm, or R readily available, we need to work with sample estimates of this expected response. We receive sample of responses R, and settle for the sample mean of a test distribution or benchmark:\n\u03a8(\u03c0, \u039c) = \\frac{1}{N} \u03a3_{i=1}^{N} \u03c8(\u03c0, \u03bci)\nWhere [\u00b5i | i \u2208 {1..., N}] is the list of test instances. Here \u0177 is the sample mean of the observed response R of \u03c0on \u03bci. Ideally to get a clearer picture of f we need to see \u03c0's response on \u03bc, R(\u03c0, \u03bc), multiple times to get an accurate estimate for\nThis simple approach captures the evaluation of a wide range of AI systems, from image classifiers (R(\u03c0, \u03bc) = 1 if \u03c0 classifies \u03bc correctly, and 0 otherwise, where pm is the distribution of test images) to reinforcement learning (R(\u03c0, \u03bc) yields the agent's return on instance \u03bc, and pm is the distribution of test environments).\nOften, other domain-relevant metrics are utilised as well, such as the F1 measure for balancing precision and recall (Chinchor, 1992; Van Rijsbergen, 1979), or the BLEU score (Papineni et al., 2002). These too can be defined in terms of appropriate response functions. All of these metrics aggregate performance results and eviscerate any information that may be used by the evaluator to better understand the system or predict responses on new instances.\nLimited forms of capability-oriented evaluation have been explored in what is now AI's deep history. These included the Newell test (Anderson and Lebiere, 2003) and the Cognitive Decathlon (Mueller et al., 2007; Mueller and Minnery, 2008; Simpson and Twardy, 2008). However, these efforts often focused on identifying capabilities required for AI systems to solve problems and designing test suites to target these capabilities, rather than identifying methods to directly measure the relation between capabilities and task instance performance. The capabilities identified weren't predictive. Osband et al. (2020) provided a more recent, albeit short-lived, resurgence of an attempt at capability-oriented evaluation in AI with B-suite, a framework to evaluate RL systems by assessing categories such as exploration and credit assignment. However, the approach within B-suite was extremely simplistic. Certain tasks were marked with the \"capabilities\" required for completion, and the final capability score reported was simply an aggregate of all the tasks labelled as requiring that capability. In truth, B-suite is more of a performance-oriented approach, as there is no way"}, {"title": "4.1 Case Study: HELM Classic", "content": "First, let us examine a few aspects of HELM. We will use HELM as a lens to identify shortcomings of current practice. The following analysis is based on what is now called \u201cHELM Classic\" (which I will refer to as HELM for brevity). I contend that HELM is one of the better large-scale benchmarks for Foundation Models, but we will pay careful attention to what is still missing.\nBroadly, HELM is a monumental initiative and truly a step in the right direction for AI evaluation. The task- space that HELM covers is extremely broad. Furthermore, HELM should be lauded for its standardisation procedure, its application to over 30 models, and the open publication of instance-level results for all the models on all evaluations. The importance of open reporting of evaluation benchmark results at the instance- level is highlighted by Burnell et al. (2023).\nHowever, despite HELM being far ahead of standard practice, it still doesn't provide a robust evaluation of the capabilities, limitations, or risks of these language models. The authors of HELM are cognisant of this and systematically describe many missing scenarios, metrics, and other limitations throughout their lengthy paper.\nMany of HELM's limitations come from the constituent datasets forming sub-benchmarks to assess particular types of tasks. For example, HELM contains one dataset for \"Sentiment Analysis,\" the IMDB Movie Review dataset found in Maas et al. (2011). To improve on this dataset, HELM makes use of Gardner et al. (2020)'s contrast sets to provide systematic (and often small) perturbations of reviews that would flip the original label, thus aiming to populate the localised task-space with more nuanced evaluative examples. However, the time-consuming nature of creating contrast sets that more densely populate the task-space considered (which is particularly necessary with all the nuances of natural language) has led to HELM only having one sentiment analysis dataset. This limits the task-space in which models are evaluated to a particular area concerned with movie reviews in English. This is clearly not representative of the whole task-space of \"Sentiment Analysis.\" There are numerous other areas in which we may care about identifying sentiment. These other areas of task-space may have subtle differences in how sentiment is expressed or how systems apply the approaches they have learned, and therefore it may not be appropriate to simply extrapolate inferred performance to other areas of task-space. HELM's evaluation of sentiment analysis can be viewed as narrow but very dense."}, {"title": "4.2 Benchmark Blindness", "content": "AI evaluation has suffered from a fixation on benchmark scores, particularly in the form of leaderboards. This issue has been discussed by Raji et al. (2021), especially with reference to the breadth of what we have been calling task-space. This is certainly true. We cannot include every possible task instance that the system may ever encounter in a single dataset; a sort of \"benchmark of Babel\" (sensu Borges, 1939). However, with an appropriately devised approach for reifying capabilities, we wouldn't need to include everything. If the evaluation process can extract the causal structure determining performance and the facets of task instances that influence this, then even for broad tasks such as \"sentiment analysis,\" we can aim to evaluate a model's general capability at this task.\nI argue that the primary issue with AI's over-focus on benchmarking doesn't come from the task-space's breadth. Rather, it comes from the over-optimisation that leaderboards and competitions incentivise. Raji et al. (2021) briefly discuss this in a short subsection on the limits of competitive testing, noting that \"Chasing 'state-of-the-art' (SOTA) performance is a very peculiar way of doing science,\" while also high- lighting Hooker (1995)'s \u201cscientific testing\u201d (or controlled experimentation) as an alternative. The structure of a leaderboard itself aggregates performance down to a single (or, if we are lucky, a few) metric that can't capture all of the nuance of the results. Leaderboards require participants to be placed into a linear order. Such a structure can't capture the explanatory causal structure that is intrinsic to understanding why the system performed a particular way. Since these metrics can't capture our goals precisely, they are a mere proxy for which we are over-optimising. Goodhart's law ", "measure": "Goodhart, 1984; Strathern, 1997)\u2014highlights the issue with fixating on these metrics. The issue with \"over-optimising\" on a proxy has been explored empirically by Gao et al. (2022), where as proxy reward increases, past a certain point the actual reward decreases.\nIn the realm of AI governance, framing AI development as a \"race\" is known to incentivise the pursuit of performance at the expense of safety (Armstrong et al., 2016). A similar phenomenon is occurring with benchmarks. The promise that AI has shown since the deep learning revolution has led to a glut of funding and a significant increase in the number of publications in AI. The primary publication venues and re- view processes emphasize improved, SOTA results. Due to the publish-or-perish nature of research and the increasing number of AI researchers, new SOTA results and improvements are accelerating (cleanly demon- strated by Kiela et al., 2021). In the field's race to incrementally improve results, in-depth model evaluation has been sacrificed. While the total amount of evaluation for specific popular systems has increased-models"}, {"title": "4.3 The Problem With Evals", "content": "The recent success of capable, yet seemingly inscrutable, black-box models has led to a surge of interest in AI evaluation. Multiple organisations have been created to try and perform better evaluations (e.g., METR (formerly ARC Evals) (METR, 2024), Apollo Research (Apollo Research, 2023)). Many AI labs have also spun up \"Red Teams\" (e.g., OpenAI (OpenAI, 2023b), Anthropic (Anthropic, 2023), Google (Google, 2023), Microsoft (Kumar, 2023), NVidia (Pearce and Lucas, 2023)) aiming to adversarially identify undesirable characteristics, capabilities, or behavioural tendencies. The aim of prioritising this sort of research seems to have been largely to counteract the litany of prompt-injection (or \"jailbreaking\") techniques (e.g., Liu et al., 2023; Choi et al., 2022; Zou et al., 2023) that have been identified as able to circumvent guardrails aiming to prevent undesirable behaviour.\nThe term \"Evals\" is used frequently to describe the research done by the aforementioned organisations. But what is an Eval? In theory, it is just a catchy shorthand for evaluation. However, in practice, Evals are very specific types of performance-oriented evaluations focused on finding faults with existing models through red-teaming or benchmarking. These faults are then typically addressed through techniques like RLHF"}, {"title": "5 Evaluating Systems That (May) Have General Intelligence", "content": "The cognitive sciences have spent over a century researching and devising methodologies for evaluating the cognitive capabilities of animals, including humans. Within AI, we can leverage much of this research towards evaluating artificial systems. Only a fraction of these methodologies have proliferated into AI evaluation. These techniques would provide helpful first steps for improved evaluation in AI, yielding test batteries that more accurately assess capabilities.\nPsychometrics has identified several cognitive, culture-fair tests with highly correlated results, known as the \"positive manifold\" (Spearman, 1927), hinting at a latent factor \"g\" corresponding to some kind of general intelligence. Subsequent developments include Cattell-Horn-Carroll (CHC) theory (Keith and Reynolds, 2010), which introduces additional latent factors representing different types of reasoning abilities. That a latent g-factor explains much variation in human test performance is surprising but is widely supported across numerous cultures worldwide (Warne and Burningham, 2019). While the interpretation of g itself is controversial (whether it is \u201cmerely\u201d a statistical regularity or a true representative of intelligence), this controversy is irrelevant to the point I want to make: the approach of reifying and constructing latent factors from evaluation data is a profoundly powerful technique and is necessary for determining the causal structure of how test performance is affected. These causal structures provide both explanations for performance and power for predicting performance on new instances. The reification of latent capabilities is a first necessary step toward capability-oriented evaluation.\nHowever, we do not want to merely trade one metric (\u201cmean performance"}, {"title": "6 Difficulties of Evaluating General Intelligences", "content": "Sloman (1984) describes the \u201cSpace of All Possible Minds\" to refer to the vast potential differences in the cognitive structure of behavioural systems. Hern\u00e1ndez-Orallo (2017a) similarly describes the \"Machine King- dom\" (reflecting the taxonomic Animal Kingdom) as a superset of all possible organisms that additionally includes \"all computable interactive systems.\" The search for a \"universal psychometrics\" (Hern\u00e1ndez-Orallo, 2017a) to enable the measurement, classification, and evaluation of all of these systems at once is a colossally difficult endeavour.\nAs AI systems advance further and become more capable and general-purpose, we need to reframe how we think about these systems and their evaluation. Viewing these more general systems as agents and not only as tools or devices is key from a safety perspective (Chan et al., 2023) as well as important for distinguishing classes of behaviour (Orseau et al., 2018). This certainly makes many of the psychologically inspired approaches seem more suitable. However, there are still many open questions in how we adapt cognitively-inspired evaluation methodologies for AI. As should be no surprise, AI systems are very different than humans and other animals, and simple adaptations of cognitive science experiments will fail if directly applied to AI. In this section we explore a few areas where the need for this adaptation is most apparent. By exploring these areas, we can pave the way for more robust and reliable evaluation practices."}, {"title": "6.1 Avoiding the Biomorphism of AI Systems", "content": "As tempting as it may be to directly lift experiments from the cognitive sciences (where much effort has already been expended to develop experiments with high levels of construct validity) and apply them directly to AI, we need to be extremely careful about anthropomorphising (or more generally biomorphising) these systems. Experiments to test for a specific characteristic are often tied to disentangling confounders for specific models of cognition. The way we design tests to elicit properties for measurement is often specific to the type of entity we are studying.\nA great example of this relates to how we evaluate human intelligence. The most common approach is IQ testing. Despite controversies about what IQ specifically measures and its validity, IQ correlates highly with many aspects of human endeavour that we typically associate with intelligence (Sternberg et al., 2001). However, AI systems have been able to perform well on IQ tests for decades, often outperforming many human scores (Hern\u00e1ndez-Orallo et al., 2016). Yet, most experts agree that AI systems are still not at \"human-level\" intelligence. The issue here is that even well-designed IQ tests are created with humans in mind and make many assumptions about the implications of performance for capabilities.\nFor instance, a very common test item on a typical IQ test is an instance of Raven's Progressive Matrices (RPM) (John and Raven, 2003). In an RPM instance, an incomplete set of three-by-three grid symbols is presented to the test participant, who is then required to identify the correct completion. Solving RPM instances correctly is said to require fluid intelligence (Cattell, 1963) or analytical intelligence, which involves \"the ability to deal with novelty, to adapt one's thinking to a new cognitive problem\" (Carpenter et al., 1990). As Carpenter et al. (1990) go on to demonstrate, computer models have been able to achieve high performance (better than most humans) on RPM instances since the 1990s. Of course, a lot of human intelligence went into designing a system that could solve these instances, which included a lot of manual feature encoding and translating of the instances into a computer-friendly form.\nThe point here is that RPM is only a good indicator of \"fluid intelligence\u201d in humans (though the extent of construct validity for humans further depends on the variations of test application (Tatel et al., 2022)). High performance on RPM tests is correlated with high fluid intelligence within human cognitive architecture. It is trivial to imagine a system situated in the Space of All Possible Minds that solves RPM tasks perfectly yet fails to perform well in any other task. Clearly, this system does not have high fluid intelligence. The correlation between RPM performance and fluid intelligence is only valid with the implicit assumption that the test participants are drawn from the same population as the original study-humans. This correlation likely requires the highly similar cognitive architecture and shared evolutionary history we all share. In this way, no single IQ-like test, unless incomprehensibly vast, can be universal. Raven's Progressive Matrices, despite their high construct validity for fluid intelligence in humans, are a poor candidate for assessing \"fluid intelligence\" in non-human systems. They lack external validity outside of the human population. A further example comes from how object permanence has been studied in newborn chicks. Chiandetti and Vallortigara (2011) find that within a few days of birth, chicks are able to demonstrate behaviour indicating that they have object permanence they still believe that objects exist even when occluded. The experimental design to elicit this behaviour from the chicks required the chicks to imprint on a static object (in this case, a plastic"}, {"title": "6.2 Limits of Norm-referenced Testing for AI", "content": "At present, training a state-of-the-art"}]}