{"title": "EVALUATING AI EVALUATION: PERILS AND PROSPECTS", "authors": ["John Burden"], "abstract": "As AI systems appear to exhibit ever-increasing capability and generality, assessing their true potential and safety becomes paramount. This paper contends that the prevalent evaluation methods for these systems are fundamentally inadequate, heightening the risks and potential hazards associated with AI. I argue that a reformation is required in the way we evaluate AI systems and that we should look towards cognitive sciences for inspiration in our approaches, which have a longstanding tradition of assessing general intelligence across diverse species. We will identify some of the difficulties that need to be overcome when applying cognitively- inspired approaches to general-purpose AI systems and also analyse the emerging area of \"Evals\". The paper concludes by identifying promising research pathways that could refine AI evaluation, advancing it towards a rigorous scientific domain that contributes to the development of safe AI systems.", "sections": [{"title": "Introduction", "content": "Recent years have seen an explosion of interest in mitigating the risks from AI systems, both existential and socio-technical. But how can we actually ensure that an AI system is safe? This is a difficult and multi-faceted question, requiring conscious intervention at every step in the process of creating an AI system, from data-set collection, all the way through to deployment (and beyond). One important part of this process is to develop techniques for building and training safe AI systems, or aligning systems with particular sets of values. However, equally important is identifying whether these techniques have been successful, and subsequently deciding whether a system is ready for deployment. This is the domain of AI evaluation.\nImproved methodologies for both aligning and evaluating AI are necessary to ensure that the seemingly ever-advancing AI systems are safe, ethical, and ultimately beneficial for humanity. This necessity is driven by the increasing capability and generality of AI systems. The success of Large Language Models (or Foundation Models) such as the GPT series has demonstrated that a single (albeit incredibly large) model can be trained to perform a wide variety of tasks. These models can be \"fine-tuned\" at low-cost (relative to training a new model) to improve performance on specific tasks. They also demonstrate the ability to adapt to new tasks with just a few examples through a process called \u201cfew-shot learning\".\nAn increase in generality adds more than just a fixed, finite number of new use cases. Tasks can be combined or specified in arbitrary ways, for example solving a mathematics equation through poetry in Latin. The task-space over which we need to evaluate the system expands from almost a single point to a large and multi-dimensional vector-space. From a safety perspective, or even just to guarantee a minimum level of performance, this is orders of magnitude more difficult: we cannot simply evaluate performance on every individual task combination. We can't know a priori whether a new point in task-space will yield an unsafe response."}, {"title": "Formalising Tasks, Instances, and Performance", "content": "Formalising tasks, instances, and performance will allow us to more precisely discuss the problems and so- lutions in evaluating general-purpose AI systems and their impact on safety. Following Hern\u00e1ndez-Orallo (2017a), we consider a task M comprised of a set of instances \u03bc. The variation in instances can be param- eterised, and this parameterisation constitutes the task-space of M. As a shorthand, we can write this as \u03bc = M(x1, ..., Xn) for some set of parameters defining the variation in M. We assume that the task-space is deterministic and that M forms a bijective function. That is, given parameters [x1,..., xn], there is always and only one instance \u03bc = M(x1,..,xn). If both \u03bc\u2081 and \u00b52 = M(x1,.., xn), then either \u03bc\u2081 = \u03bc2 or our task characterisation is incomplete in its parameters.\nRevisiting our maze example, we can imagine M taking parameters corresponding to the width and height of the maze, as well as parameters determining the layout of the walls and the location of the \"goal\".\nSeparate from, but often highly related to, the task M is the distribution PM from which u is sampled. The distribution PM may represent a natural preponderance of particular instances, or an intentional intervention to focus efforts on more important instances. Considering the maze example again, certain variations of mazes may be more likely than others; for example, we may prefer mazes that are solvable or within a certain range of sizes.\nA system will produce a (possibly stochastic) response on instance \u03bc, denoted as R(\u03c0,\u03bc), which is a numerical value. We do not limit R to producing a scalar response; R can return a vector of responses, such as both a performance metric and a safety metric. Given that the response R(\u03c0, \u03bc) is often stochastic, we will frequently refer to the expected value of this response, denoted by \u03c8(\u03c0, \u03bc) = E[R(\u03c0, \u03bc)]. This & is often called the performance of \u03c0on \u03bc. Returning to our recurring maze example, we may obtain a collection of responses from a system interacting with \u03bc. The response could be related to the success or failure"}, {"title": "Capability-oriented Evaluation and Performance-oriented Evaluation", "content": "I will briefly distinguish between two evaluation styles: performance-oriented and capability-oriented (Burnell et al., 2022; Burden et al., 2023). Broadly speaking, performance-oriented evaluation assesses how well a system performs on a particular test, while capability-oriented evaluation measures the latent factors of the system (operationalised as capabilities) that cause differences in test performance. This view of capa- bilities broadly corresponds to the Conditional Analysis of Model Capabilities (Harding and Sharadin, 2024), where capabilities are attributed to a model if the model succeeds at x when its output is best described as being directed to do x.\nSystem capabilities are often difficult to directly measure-especially compared to directly observable prop- erties like physical size. The presence or value of these latent capabilities must be inferred. Tests must be designed to discriminate between test subjects based on this latent capability. A key aspect of capability- oriented evaluation is the relationship between the capability being measured and the demands placed on that capability by a task instance. In other words, the difficulty of the task instance is crucial. This is at odds with a performance-oriented evaluation approach, which focuses on raw performance on the test itself.\nLet's revisit our maze example to highlight the differences between these two evaluation paradigms. Consider a test battery of mazes comprised of multiple different task instances. Performance-oriented evaluation would report a metric related to the tests themselves, such as the percentage of occurrences a single agent achieved success over the test set. Capability-oriented evaluation, on the other hand, tries to report results related to inherent properties of the agent (or consistent features of the task instances that the agent can handle). With our continuing maze example, capability-oriented evaluation would report something like a \"maze solving ability\". This could be broken down further into more specific factors such as \"obstacle handling\" and \"efficient exploration ability\" or similar. Capability-oriented evaluation doesn't need to rely on high-level abilities: we could also see that the system has a \"size\" capability, relaying the maximum size of the maze that the system could reliably solve. We can easily conceive of further intrinsic features from the tasks that we can use to characterise the boundaries of the system's capabilities.\nCapability-oriented evaluation has several key advantages over the performance-oriented alternative. For instance, what does it mean if a system achieves a particular success rate on a test? Without knowledge of the demands placed on the system by the test's constituent instances the result by itself is often of little value. Performance-oriented evaluations can provide value in comparing test-takers it seems likely that a system receiving a score of 80% is better than a system that only manages 50%, but this doesn't inform us how capable a system may be in itself. The test may have been comprised of trivially easy instances, and neither system is actually very good, or the opposite could be true! Comparing two systems with a performance-oriented evaluation may also be limited in utility; two test-takers that achieve 99% may not have the same capabilities if the test isn't able to discriminate well between high-performers.\nOn the other hand, under capability-oriented evaluation, the reported capability shouldn't change if the distribution of instance difficulty changes. Clearly, capability-oriented evaluation has the potential to be more informative about the system being assessed; however it is generally more difficult to perform such an evaluation because it requires a strong understanding of the relationship between the capability, instance difficulty, and how these affect observable performance is required. This is because many capabilities that we wish to assess about AI systems (and intelligent systems in general) are not directly observable. Evaluators can only observe performance on specific tasks that require certain elements of the capability in question (entangled with many others). Therefore, in order to properly assess a given capability, it needs to be inferred from a variety of performance data. To do this effectively requires a detailed understanding of the capability, as well as how it affects performance on a variety of task-instances."}, {"title": "The Fallacy of Reification?", "content": "The capabilities we aim to assess using capability-oriented evaluation are, as discussed, not directly mea- surable. Capabilities are often abstract ideas such as \"object permanence\", \"navigational skills\", \"language understanding\", and so on. The \"fallacy of reification\" (sometimes called the \"fallacy of misplaced con- creteness\") refers to treating abstract entities and ideas as if they were real, concrete entities. Debate about whether abstract objects \"exist\" has existed in philosophy for a long time. However, the \"fal- lacy of reification\" does not argue about whether abstract concepts exist, but rather claims that treating an abstract concept (whether it exists in any meaningful way or not) as a concrete object is fallacious.\nHowever, an important tool in science is the Hypothetical construct. These are explanatory variables or factors that are not themselves observable. These constructs are ubiquitous (gravity, motivation, intelligence are all constructs) within science. In some sense, these constructs are forms of reification. We say that objects fall to the ground because they are \u201cpulled down by gravity\u201d. We identify a physical location on an object as its centre of mass. In short, we treat constructs as concrete and reify them. Obviously, some concepts (such as gravity or centre of mass) are more validly reifiable than others (e.g., the way we personify nature \u201cMother Nature abhors a vacuum\u201d). How do we know when our hypothetical constructs are fallacious, and when they are justified? This primarily depends on two factors that together can enable us to identify constructs and reifications that are useful and representative of meaningful, real phenomena. The first is a shared understanding of the construct's meaning as argued by Leising and Borgstede (2020). The second is the validity of the construct. Construct validity is the extent to which the constructs actually measure what they claim to measure (Cronbach and Meehl, 1955). Also of importance is the notion of construct legitimacy: the extent to which the theory arguing for the construct is justified. Today, construct validity is seen as an overarching term for many types of validation approaches. One type that is particularly worth highlighting is what Cronbach and Meehl (1955) refer to as predictive validity: how well does this construct predict future scores on a particular test? Within AI evaluation and safety, prediction is paramount (I argue this more fully in section 2.3) and predictive validity should be at the forefront of any evaluator's mind.\nAnother crucial type of validity is external validity . External validity broadly cor- responds to the extent to which the conclusions of a study can be generalised outside the context in which the study took place. With constructs, we need to be mindful not only of the construct validity of tests, but also of their external validity for measuring that construct. That is, even if the test validly measures the construct in one scenario, does it generalise to others? Different populations of subjects, environmental fac- tors, and (sometimes seemingly minor) experimental details can drastically affect a test's ability to validly measure a construct. As an intuitive example, if we imagine a hypothetical test that has high construct validity for measuring intelligence that has been rigorously tested on humans, but then we give the test to a dog, we would likely find that the dog would not score any points on the test, despite dogs clearly having some level of intelligence. The dog was unable to complete the test, at least in part, because it cannot read or write. Even though the test has high construct validity when applied to humans, the test lacks external validity outside the domain in which it was designed. In Messick (1994)'s categorisation, external validity is encapsulated by his notion of construct validity. In this manuscript, for clarity, I will refer to external validity explicitly when it is the property I am referring to, but I follow Messick in that when I refer to construct validity I am implicitly requiring there to be external validity to the population being tested.\nWhen evaluating AI systems, two forms of external validity are particularly relevant. The first relates to external validity across subjects: are pre-existing tests valid when applied to AI systems? Or, like the dog taking the intelligence test, is there a mismatch between the subject and the test that invalidates our results?"}, {"title": "Evaluation Is For Prediction", "content": "I argue that a core aspect of evaluation is prediction. First, why do we evaluate systems (AI or otherwise)? The immediate answer is \"to determine if the system is suitable for its purpose\". While true, this overlooks the fact that we expect our evaluations to provide insight into how the system will perform outside of the evaluation, during \"deployment\".\nWhen we decide that a system is \"fit for purpose\", we are predicting that it will perform at an acceptable level in future instances of the task. This can be a supervisory process (\u201cIs this system good enough for the task it was designed for?\") or a reflective process (\u201cWhat could be done differently in the future to improve performance on similar tasks?\"), but in either case we are concerned with anticipating future behaviour and performance. We rely on this implicitly, often without realising it, in assessment scenarios. For example, we utilise standardised testing for university applications because we believe these tests demonstrate that a candidate has subject knowledge, work ethic, general problem-solving skills, and so on. But more importantly, we expect these factors to indicate-to predict-future success at the university.\nThe same goes for AI. We want to ensure that the AI systems we create are capable and safe. We perform various evaluations and derive performance metrics even the less informative ones such as mean perfor- mance-because we believe these metrics capture important properties of the system that predict whether it will be fit for purpose: capable, reliable, and safe. This belief needs to be more fully and explicitly expressed in our evaluation instruments."}, {"title": "Risks From Poor Evaluation", "content": "What are the risks of poor evaluation methodology for AI? First, let's examine the behaviour we are try- ing to avoid. Many undesirable characteristics have been identified as areas of concern for AI systems: negative side-effects, power-seeking tendencies, mesa-optimisation, exacerbating bias, and more. These characteristics are often framed as natural occurrences of optimisation (e.g., power-seeking behaviour naturally arises under cer- tain conditions in Markov Decision Processes) or reward mis-specification. Conscious interventions must be taken in the training and data-curation process to disincentivise these negative characteristics. However, these interventions also need evaluating; the claims they make must be verified, and any trade-offs with other characteristics must be identified.\nFlawed evaluations may lead to undue confidence in strategies for system alignment or addressing safety issues. This could result in unsuitable deployment in safety-critical domains and cause harm. This type of risk also includes overconfidence in the absence of certain characteristics (e.g., bias or deceptive behaviour). These are two sides of the same coin, often framed as separate issues. Concerns about whether a system is robust and reliably safe are arguably the same as concerns about the potential for unsafe behaviour. Evaluations of both issues should be focused on reducing uncertainty about the presence of particular system characteristics related to the consistency of behaviour.\nConsider a system that was trained to complete a task and was subsequently deployed. However, after deployment, the system begins to act in unexpected ways, such as failing to complete the task or exhibiting undesirable characteristics. How could this have arisen? This could have occurred due to a failure in the evaluation methodology (perhaps too little evaluation and testing was done or statistical techniques were misapplied). Alternatively, this could have occurred as an Out-of-Distribution (OoD) error, where the task distribution pm during testing/deployment differs from pm during training. Regardless of why the system began acting in an unexpected manner, the mere fact that such a system was deployed represents a failure of the evaluation process. Not only was the system not \"fit for purpose\", but its behaviour wasn't predictable in the deployment environment."}, {"title": "Evaluation of AI systems in Practice", "content": "In previous sections, I've discussed high-level ideals for AI evaluation, highlighting the importance of con- struct validity and predictability, and extolling the benefits of capability-oriented approaches. Now we will explore the stark contrast that is AI evaluation praxis. I will argue that the majority of these techniques are performance-oriented evaluation. Traditional AI evaluation techniques often encapsulate performance as the expected response:\n\u03a8(\u03c0, \u039c) = E [\u03c8(\u03c0, \u03bc)] = \\int_{\\mu\\\u0395\u039c} PM(\u03bc)\u03c8(\u03c0, \u03bc)\u03b1\u03bc\nThis is the mean response with respect to some task distribution. Recall that is the expected value of response R of system on instance \u03bc. Given that we don't simply have 4, pm, or R readily available, we need to work with sample estimates of this expected response. We receive sample of responses R, and settle for the sample mean of a test distribution or benchmark:\n\u03a8(\u03c0, \u039c) = 1/N  \\sum_{i=1}^{N}\u03c8(\u03c0, \u03bc\u03b5)\nWhere [\u00b5i | i \u2208 {1..., N}] is the list of test instances. Here \u0177 is the sample mean of the observed response R of \u03c0on \u03bci. Ideally to get a clearer picture of f we need to see \u03c0's response on \u03bc, R(\u03c0, \u03bc), multiple times to get an accurate estimate for\nThis simple approach captures the evaluation of a wide range of AI systems, from image classifiers (R(\u03c0, \u03bc) = 1 if \u03c0 classifies \u03bc correctly, and 0 otherwise, where pm is the distribution of test images) to reinforcement learning (R(\u03c0, \u03bc) yields the agent's return on instance \u03bc, and pm is the distribution of test environments).\nOften, other domain-relevant metrics are utilised as well, such as the F1 measure for balancing precision and recall, or the BLEU score. These too can be defined in terms of appropriate response functions. All of these metrics aggregate performance results and eviscerate any information that may be used by the evaluator to better understand the system or predict responses on new instances.\nLimited forms of capability-oriented evaluation have been explored in what is now AI's deep history. These included the Newell test and the Cognitive Decathlon. However, these efforts often focused on identifying capabilities required for AI systems to solve problems and designing test suites to target these capabilities, rather than identifying methods to directly measure the relation between capabilities and task instance performance. The capabilities identified weren't predictive. Osband et al. (2020) provided a more recent, albeit short-lived, resurgence of an attempt at capability-oriented evaluation in AI with B-suite, a framework to evaluate RL systems by assessing categories such as exploration and credit assignment. However, the approach within B-suite was extremely simplistic. Certain tasks were marked with the \"capabilities\" required for completion, and the final capability score reported was simply an aggregate of all the tasks labelled as requiring that capability. In truth, B-suite is more of a performance-oriented approach, as there is no way"}, {"title": "Case Study: HELM Classic", "content": "First, let us examine a few aspects of HELM. We will use HELM as a lens to identify shortcomings of current practice. The following analysis is based on what is now called \u201cHELM Classic\" (which I will refer to as HELM for brevity). I contend that HELM is one of the better large-scale benchmarks for Foundation Models, but we will pay careful attention to what is still missing.\nBroadly, HELM is a monumental initiative and truly a step in the right direction for AI evaluation. The task- space that HELM covers is extremely broad. Furthermore, HELM should be lauded for its standardisation procedure, its application to over 30 models, and the open publication of instance-level results for all the models on all evaluations. The importance of open reporting of evaluation benchmark results at the instance-level is highlighted by Burnell et al. (2023).\nHowever, despite HELM being far ahead of standard practice, it still doesn't provide a robust evaluation of the capabilities, limitations, or risks of these language models. The authors of HELM are cognisant of this and systematically describe many missing scenarios, metrics, and other limitations throughout their lengthy paper.\nMany of HELM's limitations come from the constituent datasets forming sub-benchmarks to assess particular types of tasks. For example, HELM contains one dataset for \"Sentiment Analysis,\" the IMDB Movie Review dataset found in Maas et al. (2011). To improve on this dataset, HELM makes use of Gardner et al. (2020)'s contrast sets to provide systematic (and often small) perturbations of reviews that would flip the original label, thus aiming to populate the localised task-space with more nuanced evaluative examples. However, the time-consuming nature of creating contrast sets that more densely populate the task-space considered (which is particularly necessary with all the nuances of natural language) has led to HELM only having one sentiment analysis dataset. This limits the task-space in which models are evaluated to a particular area concerned with movie reviews in English. This is clearly not representative of the whole task-space of \"Sentiment Analysis.\" There are numerous other areas in which we may care about identifying sentiment. These other areas of task-space may have subtle differences in how sentiment is expressed or how systems apply the approaches they have learned, and therefore it may not be appropriate to simply extrapolate inferred performance to other areas of task-space. HELM's evaluation of sentiment analysis can be viewed as narrow but very dense."}, {"title": "Benchmark Blindness", "content": "AI evaluation has suffered from a fixation on benchmark scores, particularly in the form of leaderboards. This issue has been discussed by Raji et al. (2021), especially with reference to the breadth of what we have been calling task-space. This is certainly true. We cannot include every possible task instance that the system may ever encounter in a single dataset; a sort of \"benchmark of Babel\". However, with an appropriately devised approach for reifying capabilities, we wouldn't need to include everything. If the evaluation process can extract the causal structure determining performance and the facets of task instances that influence this, then even for broad tasks such as \"sentiment analysis,\" we can aim to evaluate a model's general capability at this task.\nI argue that the primary issue with AI's over-focus on benchmarking doesn't come from the task-space's breadth. Rather, it comes from the over-optimisation that leaderboards and competitions incentivise. Raji et al. (2021) briefly discuss this in a short subsection on the limits of competitive testing, noting that \"Chasing 'state-of-the-art' (SOTA) performance is a very peculiar way of doing science,\" while also high-lighting Hooker (1995)'s \u201cscientific testing\u201d (or controlled experimentation) as an alternative. The structure of a leaderboard itself aggregates performance down to a single (or, if we are lucky, a few) metric that can't capture all of the nuance of the results. Leaderboards require participants to be placed into a linear order. Such a structure can't capture the explanatory causal structure that is intrinsic to understanding why the system performed a particular way. Since these metrics can't capture our goals precisely, they are a mere proxy for which we are over-optimising. Goodhart's law \u201cWhen a measure becomes a target, it ceases to be a good measure highlights the issue with fixating on these metrics.\nThe issue with \"over-optimising\" on a proxy has been explored empirically by Gao et al. (2022), where as proxy reward increases, past a certain point the actual reward decreases.\nIn the realm of AI governance, framing AI development as a \"race\" is known to incentivise the pursuit of performance at the expense of safety . A similar phenomenon is occurring with benchmarks. The promise that AI has shown since the deep learning revolution has led to a glut of funding and a significant increase in the number of publications in AI. The primary publication venues and re- view processes emphasize improved, SOTA results. Due to the publish-or-perish nature of research and the increasing number of AI researchers, new SOTA results and improvements are accelerating (cleanly demon- strated by Kiela et al., 2021). In the field's race to incrementally improve results, in-depth model evaluation has been sacrificed. While the total amount of evaluation for specific popular systems has increased models"}, {"title": "The Problem With Evals", "content": "The recent success of capable, yet seemingly inscrutable, black-box models has led to a surge of interest in AI evaluation. Multiple organisations have been created to try and perform better evaluations. Many AI labs have also spun up \"Red Teams\" aiming to adversarially identify undesirable characteristics, capabilities, or behavioural tendencies. The aim of prioritising this sort of research seems to have been largely to counteract the litany of prompt-injection (or \"jailbreaking\") techniques that have been identified as able to circumvent guardrails aiming to prevent undesirable behaviour.\nThe term \"Evals\" is used frequently to describe the research done by the aforementioned organisations. But what is an Eval? In theory, it is just a catchy shorthand for evaluation. However, in practice, Evals are very specific types of performance-oriented evaluations focused on finding faults with existing models through red-teaming or benchmarking. These faults are then typically addressed through techniques like RLHF"}, {"title": "Evaluating Systems That (May) Have General Intelligence", "content": "The cognitive sciences have spent over a century researching and devising methodologies for evaluating the cognitive capabilities of animals, including humans. Within AI, we can leverage much of this research towards evaluating artificial systems. Only a fraction of these methodologies have proliferated into AI evaluation. These techniques would provide helpful first steps for improved evaluation in AI, yielding test batteries that more accurately assess capabilities.\nPsychometrics has identified several cognitive, culture-fair tests with highly correlated results, known as the \"positive manifold\", hinting at a latent factor \"g\" corresponding to some kind of general intelligence. Subsequent developments include Cattell-Horn-Carroll (CHC) theory , which introduces additional latent factors representing different types of reasoning abilities. That a latent g-factor explains much variation in human test performance is surprising but is widely supported across numerous cultures worldwide . While the interpretation of g itself is controversial (whether it is \u201cmerely\u201d a statistical regularity or is a true representative of intelligence), this controversy is irrelevant to the point I want to make: the approach of reifying and constructing latent factors from evaluation data is a profoundly powerful technique and is necessary for determining the causal structure of how test performance is affected. These causal structures provide both explanations for performance and power for predicting performance on new instances. The reification of latent capabilities is a first necessary step toward capability-oriented evaluation.\nHowever, we do not want to merely trade one metric (\u201cmean performance\") for another (\"g\"), even if this new metric corresponds to something more grounded and doesn't change with the test distribution of PM. More meaningful metrics are clearly only one part of the solution. Psychometrics has also developed Item Response Theory (IRT), where \"items\" (task instances) have a difficulty associated with them. IRT typically fits a logistic model to a plot of expected response against difficulty and extracts the level of difficulty at which a subject can routinely succeed. This has been used in AI evaluation to provide more insight into the behaviour of classifiers, evaluate speech synthesis and recognition and develop new approaches to measuring generality and capability . IRT has the downside that some notion of difficulty must be provided. Traditionally (in humans), difficulty is derived from a population estimate (an item is harder if more subjects struggle with it), and this works well when dealing with a large sample of a (mostly) static population. However, for a small population that is constantly in flux (such as the ever-changing landscape of AI systems), the item difficulties may change too often to be useful. Difficulty measures based on concepts from Algorithmic Information Theory can be contrived, but these often have issues with computability . IRT can be extended to allow multi-dimensional difficulty , which can make formulating difficulty easier: difficult-to-compare aspects of difficulty can be separated into different dimensions. However, the problems of requiring an"}, {"title": "Difficulties of Evaluating General Intelligences", "content": "Sloman describes the \u201cSpace of All Possible Minds\" to refer to the vast potential differences in the cognitive structure of behavioural systems. Hern\u00e1ndez-Orallo similarly describes the \"Machine King- dom\" (reflecting the taxonomic Animal Kingdom) as a superset of all possible organisms that additionally includes \"all computable interactive systems.\" The search for a \"universal psychometrics\" to enable the measurement, classification, and evaluation of all of these systems at once is a colossally difficult endeavour.\nAs AI systems advance further and become more capable and general-purpose, we need to reframe how we think about these systems and their evaluation. Viewing these more general systems as agents and not only as tools or devices is key from a safety perspective  as well as important for distinguishing classes of behaviour . This certainly makes many of the psychologically inspired approaches seem more suitable. However, there are still many open questions in how we adapt cognitively-inspired evaluation methodologies for AI. As should be no surprise, AI systems are very different than humans and other animals, and simple adaptations of cognitive science experiments will fail if directly applied to AI. In this section we explore a few areas where the need for this adaptation is most apparent. By exploring these areas, we can pave the way for more robust and reliable evaluation practices."}, {"title": "Avoiding the Biomorphism of AI Systems", "content": "As tempting as it may be to directly lift experiments from the cognitive sciences (where much effort has already been expended to develop experiments with high levels of construct validity) and apply them directly to AI, we need to be extremely careful about anthropomorphising (or more generally biomorphising) these systems. Experiments to test for a specific characteristic are often tied to disentangling confounders for specific models of cognition. The way we design tests to elicit properties for measurement is often specific to the type of entity we are studying.\nA great example of this relates to how we evaluate human intelligence. The most common approach is IQ testing. Despite controversies about what IQ specifically measures and its validity, IQ correlates highly with many aspects of human endeavour that we typically associate with intelligence . However, AI systems have been able to perform well on IQ tests for decades, often outperforming many human scores . Yet, most experts agree that AI systems are still not at \"human-level\" intelligence. The issue here is that even well-designed IQ tests are created with humans in mind and make many assumptions about the implications of performance for capabilities.\nFor instance, a very common test item on a typical IQ test is an instance of Raven's Progressive Matrices (RPM) . In an RPM instance, an incomplete set of three-by-three grid symbols is presented to the test participant, who is then required to identify the correct completion. Solving RPM instances correctly is said to require fluid intelligence or analytical intelligence, which involves \"the ability to deal with novelty, to adapt one's thinking to a new cognitive problem\" . As Carpenter et al. (1990) go on to demonstrate, computer models have been able to achieve high performance (better than most humans) on RPM instances since the 1990s. Of course, a lot of human intelligence went into designing a system that could solve these instances, which included a lot of manual feature encoding and translating of the instances into a computer-friendly form.\nThe point here is that RPM is only a good indicator of \"fluid intelligence\u201d in humans (though the extent of construct validity for humans further depends on the variations of test application . High performance on RPM tests is correlated with high fluid intelligence within human cognitive architecture. It is trivial to imagine a system situated in the Space of All Possible Minds that solves RPM tasks perfectly yet fails to perform well in any other task. Clearly, this system does not have high fluid intelligence. The correlation between RPM performance and fluid intelligence is only valid with the implicit assumption that the test participants are drawn from the same population as the original study-humans. This correlation likely requires the highly similar cognitive architecture and shared evolutionary history we all share. In this way, no single IQ-like test, unless incomprehensibly vast, can be universal. Raven's Progressive Matrices, despite their high construct validity for fluid intelligence in humans, are a poor candidate for assessing \"fluid intelligence\" in non-human systems. They lack external validity outside of the human population. A further example comes from how object permanence has been studied in newborn chicks. Chiandetti and Vallortigara (2011) find that within a few days of birth, chicks are able to demonstrate behaviour indicating that they have object permanence they still believe that objects exist even when occluded. The experimental design to elicit this behaviour from the chicks required the chicks to imprint on a static object (in this case, a plastic"}, {"title": "Limits of Norm-referenced Testing for AI", "content": "At present, training a state-of-the-art LLM is an expensive process, and there are only a few extant models. Due to the cost of training, it is unclear to what extent such systems would form a population from which we could make inferences. Does training the same architecture of model on the same data always lead to similar systems? What about different architectures on the same training data, or vice versa? What about just similar architectures and data? Where does fine-tuning come into play? The current state of AI research doesn't allow us to answer these questions; the frontier is constantly expanding and only a few sample subjects are created at each scale. Since scale seems to be one of the leading factors for progress with LLM performance , we never see how stable and predictable these populations are. In lieu of a stable population from which to derive \u201cspecies\u201d of the machine kingdom, we must (for the present) find alternatives to population-based approaches.\nApproaches relying on a population, so-called \u201cnorm-referenced\u201d measures , are unfortunately for AI evaluation, one of the most commonly used tools we have developed for understanding intelligent entities. It is only with the existence of a population that we can measure properties in terms of their variation from the norm of the broader population. These approaches are the foundation of how the cognitive sciences measure properties such as intelligence (IQ, g, etc., are all norm-referenced) and item difficulty in IRT. Norm-referenced tests also capture the common benchmarking paradigm prominent within AI. Unless a stable population of AI emerges (which in the current landscape seems unlikely), efforts in the space of AI evaluation should aim to shift current practices towards so-called \"criterion-referenced\" measures where systems are compared against a fixed and sufficient standard. This is difficult as"}, {"title": "Evaluation Doesn't Occur in a Vacuum", "content": "When we evaluate AI systems, we have a purpose: we want to determine if a system is sufficiently capable, safe, reliable, and so on, to be successful in the task for which it was designed. The same is true for humans with standardised tests and other forms of evaluation-we have a purpose. When we evaluate intelligent entities such as humans we do so with the knowledge that they are potentially aware that an evaluation is taking place. Evaluations don't occur in a vacuum; the knowledge that an evaluation is occurring has the potential to change the types of behaviour exhibited by subjects or systems.\nOne of the ways this can manifest is often termed the Hawthorne effect, where individuals act differently due to the belief that they are being observed. This is mirrored in AI: Leike et al. (2017) describe the \u201cAbsent supervisor\" problem, where the presence or absence of a supervisor (and the subsequent different feedback) creates a distributional shift in the state-space if the presence of the supervisor is a property the AI system can observe.\nSimilarly, when designing psychological experiments, one must be aware of inducing so-called demand char- acteristics. These refer to factors that cause participants to make an inference (though not necessarily the correct one) about the purpose of the experiment or study and adapt their behaviour to suit . Again, this is mirrored in AI: Bostrom describes the \u201cTreacherous Turn,\u201d where the system is aware it is under a form of evaluation and acts benevolently or less competently to avoid being shut off, before \u201cturn- ing\u201d against its creators once it has consolidated power. The same phenomenon has also been referred to as \"deceptive instrumental alignment,\" with model systems engaging in this phenomenon anthropomorphised as \"sleeper agents\u201d .\nIdentifying an upcoming Treacherous Turn is related to detecting deception from AI models. Empir- ical evidence demonstrates that negotiation dialogue systems can learn deceptive behaviour from self- play . Detecting and categorising deception in AI systems has received```json\n. and the Truthful-QA benchmark sets out to measure the \"truthfulness\" of a model (related to intentional decep- tion but also including misconceptions). Truthful-QA is not suitable for measuring or detecting deception from an advanced AI system (it isn't designed to be one) as it lacks the ability to discern intentional decep- tion from merely being incorrect. Advanced AI systems could also hypothetically identify that a truthfulness evaluation is being conducted and \u201cplay along\" before executing a Treacherous Turn later on. Indeed, for truly advanced systems, it is hard to imagine a purely behavioural test or benchmark that isn't nullified by the system pretending to have a different intent.\nIt is important to note that detecting deception in humans isn't a \"solved\" problem. Much effort is expended in our legal systems to determine whether a party is lying. In the USA, police investigations often make use of polygraph tests despite their lack of reliability . We do not have consistently sound methods to detect deception unless the deceptive party slips up in some form, either by declaring contradictory statements or physical evidence exposing them. While the causal incentives for deception are present, it seems likely that generally intelligent systems will continue to learn deception if the benefits outweigh the costs. Indeed, deception is rife in nature  and can often provide a sound strategy in game theoretic terms.\nWith AI systems, we would hope that we may have the upper hand here, because network weights can be frozen, inspected, and altered, and experimental environments and memory can be reset. However, until advances in mechanistic interpretability have been made, inspecting weights does not provide a lot of insight. The sheer size of modern LLMs makes interpreting the weights intractable. However, early work is showing promise in detecting certain types of deception. Future research in this area could provide key insights into the intents of AI systems, particularly during evaluations. For an AI system to hide its intentions during a deception evaluation, this must be present somewhere in the weights at some level of abstraction. However, what is more difficult to detect is\""}, {"title": "Evaluating super-human systems", "content": "There are many domains for which AI systems now exceed human-level performance. The systems that achieve this are specialised and only super-human within this specialisation. Examples include games like Chess , Go , and Atari games , but also real- world tasks such as identifying breast cancer from mammogram images , and detect- ing cell death from cell morphology . We are also seeing super-human performance on many other linguistic benchmarks or standardised tests, such as GPT-4's plethora of human-level-or-greater performances on the LSAT, Uniform Bar Exam, and SATs . Building on what we discussed in Section 4.2, GPT-4's high levels of performance on these exams don't mean it has the capability to prac- tise, for instance, law at the same level as a human. These tests likely lack construct validity and are only valid indicators for humans because of the correlative properties human performance on these tests has with other behaviour and capabilities. However, that doesn't make GPT-4 any less super-human at these specific exams (whatever that is worth).\n\u201cSuper-human performance\u201d is a vague term. It could refer to a system that is better than the average human individual at a particular task, the average human expert, better than any human, or perhaps the entire collective ability of humanity. For the following discussion, we are more concerned with the latter: the case where no human can accurately verify a solution or it is prohibitively expensive to do so. As AI systems become super-human in specific domains, how do we evaluate their performance on tasks that we ourselves can't complete? How do we create the appropriate data-set, benchmark, or other evaluative process to test the system? For some domains, this is easier than others\u2014Go has clear rules designating the winner, and Atari games provide an automatically generated score. However, for more complex examples, such as McKinney et al. (2020)'s system that identified breast cancer from mammogram images, expertise is required in order to create the evaluation data-set. The greatest potential benefits of AI come from that which we cannot do ourselves. However, these are often domains where we will struggle to generate robust and accurate labels for data-sets or robust reward signals for RL environments. This is known as the problem of Scalable Oversight. Approaches to providing scalable oversight include imposing logical consistency checks upon models , as well as the \u201csandwiching\" research paradigm . No current approaches are entirely satisfactory for cases where the AI system outperforms the collective efforts of humanity on an individual task.\nIn the natural world, humanity is apex in its general intelligence. No other species has as successfully occupied the so-called cognitive niche. As a result, our efforts in understanding cognition and intelligence have been directed towards entities with general intelligence at or below human- level. However, as recent years have shown us, AI systems are beginning to outperform humans in certain domains. Whether this eventually extends to general intelligence or general sets of capabilities is contentious."}, {"title": "Changing Trajectories", "content": "So far, I've argued that before general-purpose AI systems are deployed across society, the way we evaluate AI must fundamentally change. But in which direction should this change go? I identify three directions that I believe would positively affect the evaluation landscape in meaningful ways: the role of evaluation in the broader safety culture, the way mechanistic interpretability can help complement evaluations of behaviour, and a refocusing on capability-oriented evaluation."}, {"title": "Cultural Change", "content": "One aspect that must change about evaluation is cultural: the way we respond to evaluation results. At present, the most capable general-purpose AI systems (e.g., GPT-4) are the ones being deployed. Cutting- edge research requires a different organisational skill set and priorities than deploying a robust, safe product. Evidence suggests that GPT-4 wasn't deployed with these priorities in mind. GPT-4's system card warns that the system released to the public may potentially behave in dangerous ways. Examples given include the risk of helping individuals find public yet difficult-to-find information, such as nucleotide sequences for anthrax, identifying software vulnerabilities in code, and more when augmented with external tools. One concern highlighted in the system card is the combination of GPT-4's general reasoning and knowledge skills with robust chemistry knowledge, enabling the synthesis of dangerous chemical compounds. Terrifyingly, this is now possible, as Bran et al. (2023) demonstrate that ChemCrow (a GPT-4-powered chemistry engine) can take a description of a desired chemical compound and return a synthesis plan.\nPost-launch updates to GPT-4 to curb certain aspects of its behaviour reveal the cultural view of safety and evaluation: an afterthought. If the potential for risky behaviour was known to OpenAI (as it clearly was from the system card), why was this system released to the broader public? Shevlane et al. (2023) argue that for robust, safe general-purpose AI systems, a strong emphasis on evaluation needs to be woven into the entire development process. Drawing inspiration from nuclear power, healthcare, and heavy industry, Manheim argues that we need a \"culture of safety\" within AI that emphasises risk prevention and is proactive in addressing risks that arise. This sentiment is echoed by Weidinger et al. (2023): \"Organisations deploying AI systems require adequate governance infrastructures that can respond to detected risks with mitigations, by delaying or stopping the deployment of an AI system or by suspending an already-deployed system until concerns are resolved.\"\nThe types of proposed cultural changes whether weaving in evaluation throughout a system's entire life- cycle or course-correcting the priorities of organisations that develop and deploy AI systems towards safety and away from pure profit are absolutely necessary. As important as rigorous evaluation procedures and specific tests to assess safety characteristics are the responses by the system developers. What use are these evaluation procedures if they are ignored when the results are inconvenient? Developers of AI systems-above a certain level of capability, scale, or safety-critical area of deployment should be required to make a binding commitment to abide by the recommendations of an independent evaluation. The only way to achieve such a binding commitment is through international regulation and policy measures."}, {"title": "Understanding Internals", "content": "There are clear limits to what we can infer from a system's behaviour alone. Without understanding the internals of a system, it can be difficult to predict how a system's behaviour will change in response to new inputs. The other extreme is known as \u201cMechanistic Interpretability\u201d (MI) (Bereska and Gavves, 2024).\nMI explanations give a clear, human-understandable description of the computation carried out by a neural network. These explanations are found through reverse engineering and thorough investigations of the model weights and sub-circuits of the model at varying granularities. MI is laborious, requiring significant effort to identify mechanistic explanations of even simple tasks (e.g., see the amount of work needed to explain the algorithm learnt by a single-layer transformer trained to perform modular arithmetic . Models at the frontier of capability are far larger). However, we are beginning to see progress in applying MI techniques to larger models (e.g., MacDiarmid et al., 2024).\nMI doesn't provide evaluation by itself: we still need to identify what the human-understandable algorithms are capable of solving and where their limitations lie. For systems that perform at a super-human level, it may be that even with an MI explanation, we cannot determine whether the algorithm is correct or appropriate. However, MI provides valuable insight into explaining what a system is doing and offers a promising set of tools to aid in evaluating systems. Crucially, MI gives us the ability to understand the internal mechanisms comprising an AI system: the functions implemented by the neural network.\nIn an idealised world, MI would be a more utilised element of the evaluation toolbox, providing a link between identified capabilities or undesirable behaviour and mechanistic explanations for why these are present, as well as giving predictive power over novel inputs. One of the largest obstacles to MI is whether it can be made to scale. Indeed, there have been efforts to try and automate aspects of the MI process ; however, this is still a long way off. More research and resources are needed to advance the state-of-the-art and enable MI as an important part of a system evaluator's toolkit."}, {"title": "Re-focusing on capability-oriented evaluation", "content": "Another promising direction is that of capability-oriented evaluation. There are many benefits to this approach. These should be clear now, but to reiterate just a few: 1) The invariance of the result to the test-distribution. 2) The ability to infer the values of characteristics that are typically not observable with theoretically validated constructs. 3) The power to predict performance on unseen examples because of the inherent relationship between capability and task-instance.\nBut how does capability-oriented evaluation move forward? How do we extend it to tackle the challenges of large, general-purpose models? Difficulties arise from the need for high levels of construct validity. One way of achieving this comes from framing specific capabilities in terms of the demands expressed as physical or measurable properties of particular task instances. Modelling how task-demands and system capabilities affect performance, making use of domain knowledge and requirements for success, and pairing this with a robustly designed experimental suite has shown promise in evaluating the presence of complex skills such as object permanence .\nThese types of approaches are emblematic of those from the cognitive sciences, and leveraging these to evaluate safety properties has great potential. Future research in this area will require deep collaboration between experts in AI safety and the cognitive sciences. Iterating on approaches from Psychology and Psychometrics for measuring well-defined constructs and accounting for the challenges arising from the scale and non-humanness of AI systems can yield measurements of capabilities that are predictive in their assessment. This area is extremely nascent, and there is a glut of useful work to be done. Specific tests for specific capabilities (with high levels of construct validity and predictive power) are in short supply, and building up a collection of these across a range of capabilities and tasks would be valuable. Additional future directions will also need to include more of a safety focus, learning to predict not just performance on a task but also varying types of dangerous behaviour."}, {"title": "Concluding Thoughts", "content": "Some experts think that human-level general AI may only be a few decades away. If we believe this is even reasonably likely, ensuring these powerful systems are safe must be a priority. This requires not just safety interventions, but also powerful, holistic evaluations to ensure these safety interventions are successful. I have argued that as AI systems become more advanced, there is more the burgeoning field of AI evaluation can and should-learn from the cognitive sciences, with their decades of experience to draw from. In particular, the approach of carefully reifying constructs that are valid for the domain and subject being evaluated (capability-oriented evaluation) and paying careful attention to experimental design.\nA challenge for evaluation that will need to be addressed is that of scale. Given the many types of capabilities that we may be interested in, the many safety properties we want to measure, and the many constituent tasks that would be needed to fully measure all of these capabilities and properties, how do we practically go about this for large, very general-purpose AI systems that are to be deployed widely throughout society? This is currently uncertain."}]}