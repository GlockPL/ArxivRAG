{"title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost", "authors": ["Sania Nayab", "Giulio Rossolini", "Giorgio Buttazzo", "Nicolamaria Manes", "Fabrizio Giacomelli"], "abstract": "Today's large language models (LLMs) can solve\nchallenging question-answering tasks, and prompt\nengineering techniques, such as chain-of-thought\n(CoT), have gained attention for enhancing the\nexplanation and correctness of outputs. Neverthe-\nless, models require significant time to generate\nanswers augmented with lengthy reasoning de-\ntails. To address this issue, this paper analyzes\nthe impact of output lengths on LLM inference\npipelines and proposes novel metrics to evaluate\nthem in terms of correct conciseness. It also ex-\namines the impact of controlling output length\nthrough a refined prompt engineering strategy,\nConstrained-CoT (CCoT), which encourages the\nmodel to limit output length. Experiments on\npre-trained LLMs demonstrated the benefit of the\nproposed metrics and the effectiveness of CCOT\nacross different models. For instance, constrain-\ning the reasoning of LLaMA2-70b to 100 words\nimproves the accuracy from 36.01% (CoT) to\n41.07% (CCoT) on the GSM8K dataset, while\nreducing the average output length by 28 words.", "sections": [{"title": "1. Introduction", "content": "In recent years, large language models (LLMs) have demon-\nstrated remarkable capabilities in tackling complex question-\nanswering tasks, making significant strides in natural lan-\nguage understanding and generative AI (Taori et al., 2023;\nChiang et al., 2023; Dolly, 2023; Geng et al., 2023). The\ncontinuous advancements made in architectures and training\nmethods played a crucial role in enhancing the performance\nof these models. Alongside these developments, prompt\ntechniques have also seen substantial evolution. One such\ntechnique that has attracted considerable attention is chain-\nof-thought (CoT) prompting (Wei et al., 2022; Fu et al.,\n2023). This approach enhances the explanation and correct-\nness of the output by encouraging the LLM to articulate its\nanswer through intermediate reasoning steps.\nDespite the mentioned advantages, the CoT prompting can\nlead to longer outputs, increasing the time required for the\nmodel to generate a response. This is due to the nature\nof autoregressive transformers, which decode text word by\nword, each time running a new inference pass of the decoder\nmodule (Vaswani et al., 2017; Shekhar et al., 2024). This\nimplies that the time required to generate a response is\nheavily influenced by the length of the reasoning provided,\nwhich can also vary depending on the prompt. Such long\nand variable delays in the responses are undesirable when\nthe LLM has to relate with a user through an interactive\nconversation. This issue highlights the need to consider i)\nmetrics for evaluating the conciseness of the outputs and ii)\nsolutions to avoid excessively long chains of reasoning.\nTo this end, the first part of this work presents some moti-\nvational experiments to show the relation between output\nlength and inference time of an LLM. Then, it proposes three\nnovel metrics to account for the conciseness and correct-\nness of a generated answer. The objective of the proposed\nmetrics is to reweight the accuracy of a given model by\nconsidering aspects related to output lengths that affect the\ninference time of the model and its time predictability.\nTo address the significant increase in output length caused by\nCoT techniques, the second part of this work examines how\nto control the length of CoT reasoning with specific prompt-\ning requests. Specifically, we introduce a refined prompt\nengineering strategy called constrained-CoT (CCoT), de-\nsigned to encourage an LLM to limit the output length and\ncontrol the reasoning process. The main idea is to explicitly\nask the model to provide an output with a length less than a\ngiven bound, thus pushing the LLM to produce concise rea-\nsoning. In this case, we must ensure that the model outputs\nremain accurate and time-bound.\nThe proposed technique is evaluated through experiments\nto explore the impact of CCoT on both generation times\nand the correctness of the responses while simultaneously\ndemonstrating the benefits of measuring this trade-off\nthrough the proposed metrics. Experiments conducted on\nvarious pre-trained LLMs of different sizes in a zero-shot\nquestion-answering (QA) setting highlighted that the per-\nformance of the CCoT method strongly depends on the\nspecific LLM and the type of task. For example, using"}, {"title": "2. Related Work", "content": "To the best of our knowledge, most recent works on LLMs\nfocused on increasing their accuracy (Jiang et al., 2020; Ka-\nplan et al., 2020; Zhu et al., 2023). However, as models scale\nup, they tend to generate more extensive and articulated re-\nsponses (Bhargava et al., 2023), which can introduce other\nproblems, such as hallucinations (where the model produces\ninformation that appears plausible but not grounded (Kada-\nvath et al., 2022), or unnecessarily long explanations (Qiu\net al., 2024; Azaria and Mitchell, 2023), which can obscure\nkey information, making it difficult for users to extract rele-\nvant content efficiently (Khashabi et al., 2021; Wang et al.,\n2024). To filter out useless reasoning, Li et al. (Li et al.,\n2021) proposed a multi-hop processing technique, where an\nextraction task on the encoder to obtain the rationale for an\nanswer, which is the most relevant piece of text in an input\nprompt to a given question.\nTo further improve the accuracy of LLMs, several prompt\nengineering approaches have been presented in recent years\n(Qin and Eisner, 2021). Prompt engineering involves the\nstrategic design of input patterns to guide the model toward\ngenerating more accurate and relevant responses (Reynolds\nand McDonell, 2021; Marvin et al., 2023). However, most\nof these approaches have been conceived to enhance model\naccuracy, increasing the output length. For instance, lo\net al. (Lo, 2023) and strobelt et al. (Strobelt et al., 2022)\nintroduced prompt-based approaches by adding task-specific\npatterns to frame the input data. While these methods allow\nboosting accuracy, they can also produce longer outputs\ndue to the additional context and detail introduced by the\nprompt, making it challenging to provide factual and concise\nanswers (Shi et al., 2023).\nAnother form of prompt engineering was proposed to im-\nprove reasoning within the conclusive answer. In this con-\ntext, Chain-of-Thought (CoT) prompting (Wei et al., 2022)\nis one of the most notable methods, showing significant\nbenefits in QA tasks by requiring the model to provide a\nstep-by-step explanation along with the final response. How-\never, as also highlighted in Section 3, answers generated\nwith CoT tend to be lengthy, hence increasing the generation\ntime (Liu et al., 2018; Takase and Okazaki, 2019).\nGiven the substantial amount of work focused on improving\nthe accuracy of LLMs, it is not surprising that most of the\nadopted metrics (Lin, 2004; Stallings and Gillmore, 1971)\nand benchmarks (Clark et al., 2018; Lin et al., 2021) only\naddress the correctness of the responses, without paying\nattention to conciseness and response times (Bhargava et al.,\n2023). As already mentioned in Section 1, these properties\nare instead desirable in applications that require an interac-\ntive conversation with the user.\nThis work. To face these challenges, this work proposes\nnovel metrics that account for both the conciseness and the\ncorrectness of the responses. Furthermore, to understand\nthe capability of LLMs to control the length of reasoning\nin their outputs, this work proposes a revised version of the\nCoT prompting (Wei et al., 2022), Constrained Chain-of-\nThought (CCoT), which explicitly asks the model to control\nthe length of its reasoning. We support the analysis by eval-\nuating the novel metrics and testing the proposed prompting\napproach, understanding how it affects the quality of the\nanswers and, specifically, the response time of the LLMs."}, {"title": "3. Motivational considerations", "content": "The output generation time of an LLM depends on vari-\nous factors, including the model architecture, the pre-and\npost-processing steps, the answer decoding process, and\nthe question posed, also considering the use of prompt en-\ngineering approaches. While the computational cost due"}, {"title": "4. Metrics for concise correctness", "content": "Motivated by the previous considerations, this section\npresents three novel metrics to evaluate the capability of\nan LLM to provide correct as well as concise responses.\nThe idea is to redefine the classic accuracy metric to inte-\ngrate conciseness aspects into the LLM output's correctness.\nFormally, an answer \u0177 is considered correct if the conclu-\nsion extracted through a post-processing function \u0393 matches\nthe given ground truth y. Thus, the accuracy of an LLM can\nbe computed as\n$A = \\frac{1}{N} \\sum_{i=1}^{N} 1(\\Gamma(\\hat{y}_i), y_i),$   (2)\nwhere N is the number of tested samples and 1 (u, v) is\nthe indicator function that returns 1 if u = v, 0 otherwise.\nPlease note that \u0393 represents a user-defined function that\ncan be implemented based on a regular expression (e.g.,\nby extracting specific patterns from the sentence (Fu et al.,\n2023)) or using pseudo-judge approaches (e.g., by using a\nsecondary large model as a judge (Zheng et al., 2024)).\nStarting from Equation (2), the conciseness of an output\n\u0177i can be integrated with its correctness by multiplying\nthe indicator function by a penalty term $p(\\hat{y}_i) \\in [0, 1]$ that\ndecreases its value for long outputs:\n$\\frac{1}{N} \\sum_{i=1}^{N} [1(\\Gamma(\\hat{y}_i), y_i) \\cdot p(\\hat{y}_i)] .$   (3)\nThe following defines three specific metrics by setting a\nproper penalty function.\nHard-k Concise Accuracy: HCA(k). It measures the\nfraction of correct outputs that do not exceed a user-specified\nlength k:\n$HCA(k) = \\frac{1}{N} \\sum_{i=1}^{N} [1(\\Gamma(\\hat{y}_i), y_i) \\cdot p_{\\text{hard}}(\\hat{y}_i, k)],$\nwhere\n$p_{\\text{hard}} (\\hat{y}_i, k) = \\begin{cases} 1 & \\text{if } N(\\hat{y}_i) \\leq k \\\\ 0 & \\text{otherwise}. \\end{cases}$   (4)\nThis metric does not account for responses that exceed the\nspecified maximum length, thereby promoting conciseness.\nWe believe it could be particularly useful in scenarios where\nstrict adherence to length constraints is essential, such as\nin real-time systems or environments with limited computa-\ntional resources.\nSoft-k Concise Accuracy: SCA(k, a). It generalizes the\nprevious metric by penalizing the correct answers that ex-\nceed the maximum length k with a term that decreases\nexponentially with a decay factor a:\n$SCA(k, a) = \\frac{1}{N} \\sum_{i=1}^{N} [1(\\Gamma(\\hat{y}_i), y_i) \\cdot p_{\\text{soft}}(\\hat{y}_i, k, a)],$\nwhere\n$p_{\\text{soft}} (\\hat{y}_i, k, a) = \\min \\Big(1, e^{-\\alpha \\frac{N(\\hat{y}_i)-k}{k}}\\Big).$   (5)\nIn the formula, the user-defined decay a \u2265 0 can be consid-\nered a sort of tolerance that controls how much the length\nimpacts the overall accuracy; the higher the value of a,\nthe higher the tolerance for answers exceeding the speci-\nfied length k. Note that for a = 0, SCA(k, 0) reduces to\nHCA(k).\nConsistent Concise Accuracy: CCA(k, \u03b1, \u03b2). It further\ngeneralizes the previous metrics by also accounting for the\nvariation in the lengths among all the outputs obtained:\n$CCA(k, \\alpha, \\beta) = SCA(k, \\alpha) \\cdot p_{\\text{var}}(\\sigma, \\beta)$\nwhere\n$p_{\\text{var}}(\\sigma, \\beta) = \\min \\Big(1,e^{-\\frac{\\sigma}{\\beta}}\\Big).$   (6)\nIn Equation (6), \u03c3 denotes the standard deviation of the\noutput length distribution, whereas \u1e9e is a parameter that\ncontrols the tolerance for having large length variations; the\nhigher the value of \u1e9e, the higher the tolerance. Note that,\ngiven a tolerance \u03b2, $p_{\\text{var}}(\\sigma, \\beta) = 1$ for \u03c3 < \u03b2, while it\ndecreases exponentially for \u03c3 > \u03b2.\nThe CCA metric aims to promote consistency in the lengths\nof the responses. A low standard deviation o indicates\nthat the model produces responses of uniform length. In\ncontrast, a high value of o denotes a model with a large\nresponse variability, making predicting its timing response\ntime difficult."}, {"title": "5. CCoT Prompting", "content": "From the results presented in Section 3, it is clear that the\nrelationship between output length and inference time neces-\nsitates deeper awareness. To this end, this section focuses on\nimproving the use of CoT, aiming to preserve the benefits of"}, {"title": "6. Experiments", "content": "This section presents a set of experiments carried out to eval-\nuate the effectiveness of the the proposed CCoT approach\nunder classic metrics, as well as illustrate the benefits of the\nproposed metrics in evaluating a concise correctness. Specif-\nically, the following research questions are investigated in\nthe next experiments:\n\u2022 RQ1. Is the CCoT approach beneficial in terms of effi-\nciency and accuracy?\n\u2022\n\u2022\n\u2022\nRQ2. Which models benefit from CCoT, compared to\nclassic CoT?\nRQ3. How capable is an LLM of controlling the output\nlength based on an explicit prompt request?\nRQ4. Are the proposed metrics helpful in addressing\nboth efficiency and accuracy aspects? Is the impact of\nCCoT reflected in the proposed metrics?\n6.1. Experimental Setup\nAll the experiments have been carried out with the Text\nGeneration Inference (TGI) platform\u00b2 on 8 NVIDIA A100\nGPUs. Specifically, we evaluated five publicly available\npre-trained LLMs from Hugging Face\u00b3, such as Vicuna-13b-\nv1.5 (Zheng et al., 2024), instruction-tuned models Falcon-\n40b-instruct, Falcon-7b-instruct (Almazrouei et al., 2023,\nand two models trained and reinforced by utilizing private"}, {"title": "6.2. Cost and performance evaluation of CCOT", "content": "This experiment was carried out to evaluate the impact of\nCCoT on computation time and accuracy. Then, the results\nwere used to provide insights on its suitability for various\nLLM architectures.\nImpact of CCoT (RQ1\nEach of the selected LLM was\nevaluated on the GSM8K test dataset using plain prompt\n(base), CoT, and CCoT with different length constraints\n(namely, 15, 30, 45, 60, 100). The obtained results are\nreported in Figure 4. In particular, Figure 4a shows the\nimpact of the different prompt settings in terms of generation\ntime, while Figure 4b shows the corresponding accuracy.\nAs shown in Figure 4a, the CCoT prompting is able to\nreduce the generation time of all large models and most\nmedium models, with respect to CoT, and in most cases also\nwith respect to the plain prompting (base). For instance,\nfor the Llama2-70b model with classic CoT, the average\ngeneration time is 30.09 s, while with a CCoT of length 15\nthe generation time almost halves, reaching a maximum of\n23.86 s with a length constraint of 100.\nWhile reducing the generation time is relevant in certain\napplications, it is also crucial for a model to maintain the\ncorrectness of its answers while reducing the output length.\nTo evaluate this aspect, Figure 4b reports the accuracy com-\nputed for the same LLMs for the different types of prompts.\nNote that in Llama2-70b and Vicuna-13b, the CCoT is able\nto improve the accuracy, even with respect to CoT. For in-\nstance, the accuracy of Llama2-70b varies from 37.07%\n(with CCoT-30) and 41.77% (with CCoT-100), compared\nto 36.01% with CoT. For others LLMs, as Falcon-40b and\nLlama2-7b, the accuracy achieved with CCoT increases\nwith the length constraint, getting a score between the base\nand classic CoT scores. Finally, note that Falcon-7b, which\nis the smallest model, is not able to exploit CCoT prompting\nto reduce generation times and, with large length constraints,\ngets also to a lower accuracy than CoT and base."}, {"title": "On the effectiveness of CCoT prompting (RQ2).", "content": "The\ndifferent effect of CCoT prompting on the output length and\naccuracy illustrated in Figure 4 can be attributed to various\nfactors, such as the training data, the approach used to train\nthe model, the model size, and the technique adopted dur-\ning training. For instance, Llama2-70b is an autoregressive\nlarge-scale language model fine-tuned with human feedback,\ntrained on a diverse combination of generic and open-source\ndatasets. Such technical measures contribute to making\nCCoT effective in controlling the output length while im-\nproving the model accuracy. The Falcon-40b model, in\ncontrast, is smaller than Llama2-70b and trained on a differ-\nent dataset (the dedicated RefinedWeb data (Penedo et al.,\n2023)). While CCoT does not improve the accuracy of\nthe model with respect to CoT, it still performs better than\nthe base plain prompting, offering a trade-off by reducing\ngeneration times compared to CoT. Vicuna-13b also pro-\nvides competitive results across different prompts, as it is a\nfine-tuned version of Llama2 and smaller than the previous\nLlama2-70b.\nConversely, small-scale LLMs, such as Falcon-7b and\nLlama2-7b, are not capable of properly handling the con-\nstrained prompting conditions in CCoT, resulting in higher\ngeneration times (as shown for Falcon-7b with large length\nvalues in CCoT) or incorrect answers with short CCoT val-\nues in Llama2-7b. This suggests that model size and training\nstrategies severely impact the effectiveness of CCoT.\nConsidering the observations presented above, we focused"}, {"title": "6.3. Ability to control the output length (RQ3", "content": "The previous experiments looked at how CCoT strategies\ncan affect the accuracy and generation time in the average.\nHowever, despite the discussed benefits, it is also crucial\nto understand how CCoT prompting can effectively limit\nthe output length for each addressed sample. This can be\nuseful for better tuning the length parameter in the CCoT\nprompt or identifying the conditions in which the proposed\nprompting strategy fails to compress the output.\nTo evaluate the ability of an LLM to produce concise an-\nswers in response to a given prompting approach, we an-\nalyzed the output length for each sample under different\nCCoT length constraints. Figure 5 shows the statistics on\nthe length of the answers provided by three models (Vicuna-\n13b, Falcon-40b, and Llama2-70b) that were feeded with all\nthe inputs taken from the GSM8K test set, using different\nprompt strategies (base, CoT, and CCoT). As done in the\nprevious experiment, the CCoT prompt was tested for dif-\nferent length constraints (15,30,45,60,100). Each box plot\nrepresents the output lengths between the 5th and the 95th\npercentiles of all tested samples, the blue line represents\nthe provided CCoT length constraint, the red line denotes\nthe median, while the greed dot the mean. Ideally, a model\nrespecting the given length constraint for each tested sample\nshould have the entire distribution below the blue line.\nAs clear from Figure 5, using CoT without an explicit length\nrequest produces lengthy answers that significantly impact\nthe generation time. The imposed length constraint in the\nCCoT prompt significantly affects the output length, al-\nthough in practice LLMs are not always able to respect the\ngiven limit, especially for smaller values, such as 15, 30, or\n40, which are more challenging for an LLM.\nTo summarize, given the nature of the CCoT prompting, it is\nreasonable to consider a tolerance margin in respecting the"}, {"title": "6.4. Evaluation of the correct conciseness (RQ4", "content": "The metrics proposed in Section 4 are applied to assess the\nbenefit of CCoT from a new perspective, which considers\nboth the capability of the model to reduce the output length\nwhile maintaining a certain level of correctness.\nHCA evaluation. The Hard-k concise accuracy evaluates\nthe accuracy considering only the correct answers whose\nlength is less than a specified value k. Figure 6 reports the\nvalue of this performance index achieved on Llama2-70b\n(Figure 6a) and Falcon-40b (Figure 6b), when using the\ndifferent prompt approaches and for different values of k.\nPlease note, k = \u221e is equivalent to the classic accuracy.\nAs expected, the HCA values are always less than or equal\nto those related to the classic accuracy (k = \u221e), but such a\nreduction is less pronounced under the application of CCoT\nprompting. Specifically, for Llama2-70b, the use of CCOT\nis beneficial with respect to base and CoT prompts, for all\nvalues of k, although the increase is more significant for\nvalues of k equal to 100, 80, and 60. This suggests that, if\nthe length constraint is not too stringent, the capability of\nthe model to produce correct answers is higher with CCoT.\nConversely, for lower values of k, there is a strong reduction\nin performance for CoT prompts, mainly because they push\nthe model to produce a reasoning part in the output without\npaying attention to its length.\nSimilar considerations apply to Falcon-40b, where the ap-\nplication of CCoT yields a good trade-off between CoT and\nbase prompting. Note that the HCA values under CCoT\nget higher than those achieved under CoT, also for small\nvalues of k (e.g., 60 and 40), meaning that CCoT prompting\nis effective for this model."}, {"title": "SCA evaluation.", "content": "We also evaluated the Llama2-70b\nmodel using the Soft Conciseness Accuracy (SCA), across\ndifferent k and a values, where a represents a tolerance for\naccepting answers longer than the desired limit k. This met-\nric is a generalization of the HCA, giving more flexibility in\nconsidering correct answers that are larger but still close to\nthe desired length k.\nThe SCA values computed for Llama2-70b and Falcon-40b\non the questions of the GSM8K test set are reported in\nFigure 7 for different values of k and two different tolerance\nvalues (\u03b1 = 1 and a = 10). For both models, the SCA\nvalues in CCoT settings are often comparable to HCA values\nfor high values of k, such as 80 or 100. This is because,\nas shown in Figure 5, for these lengths, the CCOT prompts\nare effective at returning outputs below the desired limit,\nmaking the tolerance less necessary.\nConversely, for smaller k values, such as k = 40, SCA\nstarts exceeding HCA, indicating that some correct answers\nhave a length greater than k. For these values of k, using\na larger a results in more pronounced improvements for\nCCoT prompts compared to Base and CoT. This means\nthat, although many correct outputs are longer than k, under\nCCoT the model is still encouraged to constrain them close\nto k, thus achieving a higher score. This effect is partic-\nlarly noticeable on Llama2-70b, which is more capable\nof controlling the length and produce correct outputs than\nFalcon-40b."}, {"title": "CCA evaluation.", "content": "The Consistent Concise Accuracy mea-\nsures the capability of a model to generate correct answers\nwhose lengths do not vary significantly, and therefore are\nconsistent with the specified constraint. The CCA requires a\nthird parameter \u1e9e (in addition to k and a), denoting a toler-\nance on the output length variability. In particular, if o is the\nstandard deviation of the length distribution, we have that\n$CCA(k, \\alpha, \\beta) = SCA(k, a),$ if $\\sigma \\leq \\beta$, otherwise CCA\ndecreases exponentially for $\\sigma > \\beta$.\nFigure 8 plots the CCA scores obtained on Llama2-70b and\nFalcon-40b for a = 10, \u03b2 = 20, and different values of k,\nfor the various prompting methods. According to this metric,\nthe CCoTs results in a significant improvement compared to\nCoT and base prompting, for both Llama2-70b and Falcon-\n40b, and for all values of k. However, for high CCoT length\nconstraints (e.g., 100), the CCA score tends to decrease,\nwhich does not happen with the other two metrics. This\ncan be explained by considering that an increased length\nconstraint gives the model more freedom to generate outputs\nwith higher variations.\nIt is also worth observing that the results shown in Figure 8\nare consistent with the output length distributions reported in\nFigure 5, where the base and CoT prompting show a larger\nvariance in the output length, for Falcon-40b and Llama2-\n70b. Overall, this experiment confirms that CCA can be a\nuseful performance metric when the length variation of the\noutput is of interest.\nFor completeness and fairness of the evaluation, additional\nresults for other values of a and \u1e9e are reported in the sup-\nplementary material."}, {"title": "6.5. Illustration of CCoT", "content": "To better illustrate the benefits of CCoT, Figure 9 shows\nthe answers produced by Llama2-70b when applying base,\nCoT, and CCoT prompts (with length constraint of 15, 45,\nand 100) for two different questions taken from GSM8K.\nIn both questions, we observe that in the base case, the\nmodel automatically proposes a reasoning process due to\nthe characteristics of the model used (specifically Llama2-\n70B-chat). However, under CoT, the reasoning process is\nextended, loosing control of its length."}, {"title": "7. Discussion and Conclusions", "content": "This work discussed the importance of addressing the con-\nciseness of the answers provided by LLMs in text-to-text\ngeneration tasks and investigated the possibility of control-\nling the output length through a suitable prompt engineering\napproach, called Constrained Chain-of-Thought (CCoT).\nThen, the impact of CCoT on the generation time and\nthe correctness of the responses was evaluated considering\nquestion-answer benchmarks, with respect to plain prompt-\ning and CoT. To this end, three novel metrics have been\nproposed to account for both conciseness and correctness of\nthe output as a function of user-specified parameters. Sev-\neral experiments were conducted to evaluate how different\nLLMs are able to control conciseness while ensuring cor-\nrectness, and how they could benefit from more conciseness\nin terms of generation time.\nFrom the findings emerged by the conducted experiments,\na first observation is that not all models are able to con-\ntrol the length of their outputs (RQ2). In particular, small\nmodels, as Falcon-7b, LLama2-7b, and Vicuna-13b, have\nmore difficulty in respecting the length constraints given in\nthe CCoT prompts, while larger models, as Falcon-40b and\nLlama2-70b, show a greater control capability (Section 6.2).\nSuch a difficulty of smaller LLMs could be influenced by\nvarious factors, as the dataset used during training and the\nnumber of model parameters. Understanding these issues\nand evaluating a possible integration of the proposed met-\nric in a fine-tuning process requires a deeper investigation,\nwhich is part of our future work.\nOn the other hand, for larger models, such as Falcon-40b\nand LLaMA2-70b, CCoT was able to improve both the ac-\ncuracy and the efficiency of LLMs (RQ1), with respect\nto plain prompts and CoT. The improvement in accuracy\nfor certain models (LLaMA2-70b and Vicuna-13b), while\nbeyond the scope of this study, suggests interesting future\nresearch aimed at analyzing the effects of conciseness on\npotential hallucination phenomena or incorrect reasoning.\nFurthermore, another interesting future direction could in-\nvolve extending the proposed metrics with more recent eval-\nuation techniques using judge models to evaluate the correct\nconciseness of LLMs (Zheng et al., 2024; Huang et al.,\n2024).\nTo conclude, the proposed work highlighted the need to pay\nmore attention to the conciseness of LLMs, proposing novel\nperformance indices that are able to evaluate both the cor-\nrectness of the output in relation to its length. Furthermore,\nthe proposed CCoT prompting offers a simple but interest-\ning strategy to address conciseness, which could open new\nresearch directions to make LLMs more predictable and\nefficient."}]}