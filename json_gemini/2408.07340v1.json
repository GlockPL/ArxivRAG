{"title": "Towards Few-shot Self-explaining Graph Neural Networks", "authors": ["Jingyu Peng", "Qi Liu", "Linan Yue", "Zaixi Zhang", "Kai Zhang", "Yunhao Sha"], "abstract": "Recent advancements in Graph Neural Networks (GNNs) have spurred an upsurge of research dedicated to enhancing the explain-ability of GNNs, particularly in critical domains such as medicine. A promising approach is the self-explaining method, which outputs explanations along with predictions. However, existing self-explaining models require a large amount of training data, rendering them unavailable in few-shot scenarios. To address this challenge, in this paper, we propose a Meta-learned Self-Explaining GNN (MSE-GNN), a novel framework that generates explanations to support predictions in few-shot settings. MSE-GNN adopts a two-stage self-explaining structure, consisting of an explainer and a predictor. Specifically, the explainer first imitates the attention mechanism of humans to select the explanation subgraph, whereby attention is naturally paid to regions containing important characteristics. Subsequently, the predictor mimics the decision-making process, which makes predictions based on the generated explanation. Moreover, with a novel meta-training process and a designed mechanism that exploits task information, MSE-GNN can achieve remarkable performance on new few-shot tasks. Extensive experimental results on four datasets demonstrate that MSE-GNN can achieve superior performance on prediction tasks while generating high-quality explanations compared with existing methods. The code is publicly available at https://github.com/jypeng28/MSE-GNN.", "sections": [{"title": "1 Introduction", "content": "Due to the widespread presence of graph data in diverse domains [49,48], Graph Neural Networks (GNNs) [14,36,6] are attracting increasing attention from the research community. Leveraging the message passing paradigm, GNNs have exhibited remarkable efficacy across multiple scenes, including molecule property"}, {"title": "2 Problem Definition", "content": "In this section, we will elaborate on the problem definition of our research. Following [20], we form the few-shot graph classification problem as N-way K-shot graph classification. Given the dataset G = {(G1, y1), (G2,Y2), ..., (Gn, Yn)}, where Gi denotes a graph with a node set Vi and a edge set Ei. ni denotes the number of nodes in the graph. The structure feature is represented by an adjacency matrix Ai \u2208 Rnixni. The node attribute matrix is represented as Xi \u2208 Rnixd, where d is the dimension of the node attribute.\nThen, the dataset is splitted into {Gtrain, ytrain} and {Gtest, ytest} as training set and test set respectively according to label y. Where ytrain \u2229ytest = \u00d8. When training, a task T is sampled each time and each task contains support set Dtrain = (Gtrain, ytrain)=1 and query set Dtrain Dtrain = (Gtrain, ytrain)-1, where s and q stands for the size of support set and query set respectively. It is noteworthy that the same class space is shared in the same task.\nIn each task, our goal is to optimize our model on the support set Dsup and make predictions on the query set Dque. If a support set contains N classes and K data for each class, then we name the problem as N-way K-shot. When test-ing, we firstly finetune the learned model on support set Dtest = (Gtest, ytest)=1 and then report the classification performance of finetuned model on test =\n(Gtest, ytest) 1. Our goal of the few-shot graph classification problem is to de-velop a model that can obtain meta-knowledge across {Gtrain, ytrain} and pre-dicts labels for graphs in the query set in test stage Dtest.\nIn the explanation generation task, for each graph Gi, a node mask vector\nmi \u2208 [0,1]ni \u00d71 is the explanation subgraph selected, a higher value means that\nthe corresponding node is more important for making prediction and vice versa.\nAlthough selecting edges for explanation is a viable approach, in this paper we\nfocus on node selection due to its computational complexity."}, {"title": "3 The Proposed MSE-GNN", "content": "In Figure 2, we show the overall architecture of the MSE-GNN, which contains three components: an explainer g that outputs the explanation selected, a pre-dictor p making the final prediction, and a graph encoder f.\nBefore we present the details of MSE-GNN, we first clarify serveral concepts. Specifically, existing works often combine self-explaining methods with the con-cept of rationale [37,17]. The rationale in graph data refers to the subsets of nodes or subsets of edges, which form subgraphs that determine the prediction. Hence, we posit that explanation and rationale are equivalent, as they share the same concept.\nIn MSE-GNN, the input graph is encoded by f and each node v is encoded into a node embedding h(v) \u2208 Rd, where d stands for the dimension of hid-den size. The encoder can be any kind of GNN, e.g. GCN [14], GIN [38], and"}, {"title": "3.1 Architecture of MSE-GNN", "content": "GraphSAGE [10]. The selector outputs a mask vector m for each graph as an explanation, which divides the graph into rationale (explanation) Gr and non-rationale Gn. Then the predictor makes predictions based on the graph embed-ding rationale subgraph. Meanwhile, augmented graphs that combine rationale and non-rationale from different graphs are fed into the predictor to ensure the robustness of the predictor. We categorize the parameters into fast parameters and slow parameters according to the timing of updating, which will be described in detail in section 3.3.\nTask Information. MSE-GNN generates task information for the explainer and the predictor to facilitate explanation generation and prediction within each task, which is composed of prototypes representing each class.\nIn each task, a support set is provided, which contains data from multiple classes. We aim to extract prototypes from these data that capture the charac-teristics of each class in the task, in order to help with task-specific selection of explanations and the classification task. Encoded by encoder f, each graph is represented by a matrix containing embedding of each node:\nHi = [..., h(v), ...]ev; = f(Gi) \u2208 R|Vil\u00d7d. (1)\nTo obtain representation for each graph hi, the readout function, e.g. mean pooling is employed, to aggregate node embeddings. By leveraging the concept of prototype learning, we further fuse the graph representations of each class with another readout function. Thus, we can obtain a prototype embedding for each class:\nTIc = freadout([..., freadout(Hi), ...]y\u2081=c) \u2208 Rd. (2)\nFor an N-way K-shot classification problem, the task information is formed by concatenating prototypes of N classes. It is worth noting that, task information for each input graph of both Dsup and Dque is composed solely of graphs in Dsup to prevent label leakage.\nExplainer. The explainer is responsible for choosing the explanation subgraph corresponding to each input graph. Specifically, given an input graph Gi, the explainer firstly uses another GNN encoder to map each node to another node embedding h'(v) for each node in Vi for selection. Then, a MLP is utilized to transform the node embeddings into a soft mask vector mi \u2208 [0,1]ni\u00d71, with task information TI\u0109 and node embedding h(v) concatenated as input:\nmi = o(MLP([..., [h(v), TI], \u2026\u2026\u2026]\u2208v;)), (3)\nwhere o denotes the sigmoid function. Hence, we can decompose the input graph Gi into a rationale subgraph and non-rationale subgraph according to mi respec-tively:\nG = {Ai, Ximi}   G = {Ai, Xi mi},  (4)"}, {"title": "Predictor and Graph Augmentation.", "content": "where mi = 1 \u2013 mi. Meanwhile, given the node embedding h(v) from encoder f,\nwe can obtain the graph embedding for G and Gr:\nh = freadout(Himi)  h = freadout(Himi).  (5)\nThe predictor takes the graph embed-ding h as input and makes the final prediction \u0177 = p(h) with a MLP. Moreover,\nwe enhance the robustness of the predictor through graph augmentation. Specifi-cally, within the input graph, the rationale component represents the crucial part\nthat determines the category, while the non-rationale component represents the\nnoisy part. By combining the rationale and non-rationale from different graphs\nin the same task, additional data with noise are generated. Then we assign the\nlabel based on rationale. This approach allows us to increase the amount of noisy\ndata, thereby improving the robustness of the predictor. We do the combination\noperation by adding subgraph embeddings:\nh(i,j) = h + h  Y(i,j) = Yi, (6)\nwhere hi denotes rationale from Gi and he means the non-rationale from Gj. Therefore, in addition to task information TI, the predictor p receives the embeddings of both the rationale subgraphs hand the artificially augmented graphs h(i,j) for optimization, and the output are denoted as \u011di and y(i,j) re-spectively."}, {"title": "3.2 Optimization Objective", "content": "The optimization objective of MSE-GNN is to achieve both high accuracy in predictions and generate precise explanations, which reveal the underlying rea-sons behind the predictions. Therefore, we design several types of loss functions"}, {"title": "Algorithm 1 Meta-training of MSE-GNN.", "content": "Input:Distribution over meta-training tasks: p(T); Local learning rate: 71;\nGlobal learning rate: 72; Local update times: T.\nOutput:Meta-trained parameters for encoder and explanation selector:0f, 09,\nand initialization of parameters for predictor Op\n1: Initialize 0 = {0f,09,0p} randomly;\n2: while not converged do\n3: Sample task T with support graphs Dtrain and query graphs Dtrain\nque\n4: Set fast adaptation parameters: \u03b8', = 0p\n5: for t = 0 \u2192 T do\n6: Evaluate Ve Lsup(0f,09,0p) by calculating loss via Equation 10\n7: Update 0 : 0 - 0\u00b4p - N1 \u00b7 \u2207 o\u201e Lsup(0f, 09, 0)\n8: end for\n9: Evaluate VoLque(0f,09,0p) by calculating loss via Equation 10\n10: Update 0 : 0 - 0 \u2013 N2 \u00b7 \u2207oLque(0, 0, 0)\n11: end while\nand constraints. For the sake of simplicity, we consider a binary classification task without loss of generality.\nWith the prediction of each rationale graph embedding p(hi) and correspond-ing ground-truth label yi, the loss function is defined as:\nL = yilog(yi) + (1 \u2212 yi)log(1 \u2013 \u0177i). (7)\nFor the artificially augmented graph, our aim is to minimize the prediction values for instances of the same category while maximizing the prediction values for instances of different categories. To achieve this, we employ a contrastive loss function. For example, for a 2-way K-shot classification task, we can obtain 4K2 augmented graphs, where each rationale graph is combined with other 2K 1 non-rationales, then the loss is computed as:\nLa= 1j=2K1 (li,i\u00b7ly\u2081=y; log exp(\u0177i\u00b7\u0177j)/\u0442) = exp(ij)/T\u1fbd (8)\nwhere is a scalar temperature hyperparameter.\nBesides, to address the deviation in the size of rationales, we introduce a penalty based on the number of rationale nodes, the following regularization term is utilized:\nmiTreg1, (9)\nwhere y is manually set to control the rationale size. Finally, the total loss function can be formulated as:\nL = ar\u30fbL\" + &a\u30fbL\u00ba + Areg \u00b7 Lreg, (10)\nwhere ar, aa, and areg are hypermeters controlling the weight of each loss.\""}, {"title": "3.3 Meta Training", "content": "Inspired by the concept of \"learn to learn\" [7], we propose a new meta-training framework based on MAML to obtain meta knowledge from various tasks. We denote Of, \u03b8g, and Op as the parameters of encoder, explanation selector, and the predictor. Specifically, MSE-GNN is trained from two procedures. One is global update, which aims to learn the parameters of encoder 0f, explanation generator \u03b8g, and initialization of the predictor Op from different tasks, the other is called local update, which performs fast adaption on new tasks and locally update only parameters of the predictor O within each task. According to the timing of updating, we categorize the parameters into fast parameters (0p) and slow parameters (Of and 0g), as shown in Figure 2.\nThe meta-training process is demonstrated in Algorithm 1. Firstly, we sample a task composed of support Dtrain and query data Dtrain for each episode. Then adaption is operated by updating op for T times on Drain, where T is a hyperparameter controlling the number of local updates, which is shown in lines 5-8. With updated 0, we utilize the loss on Drain to update 0f, 0g and Op.\nIt is important to highlight that, the explainer is trained from a variety of tasks and frozen when optimizing each task, which ensures the stability of the explanation selected across different tasks and prevents over-fitting. Therefore, Of and Og are only updated in the global update and fixed in the local update. While the predictor needs to learn the relationship between features and cate-gories in different tasks based on the generated explanations. As a result, the Op is optimized in the local update to learn the association between features and categories. Hyperparameters of loss computation in line 6 and line 9 can be differently set according to the goal of local and global optimization."}, {"title": "4 Experiments", "content": "Dataset. We conduct extensive experiments on four datasets to validate the performance of MSE-GNN: (i) Synthetic: Due to the lack of graph datasets with explanation ground-truth, following [39], we create a synthetic dataset for classification, which contains 10 classes and 500 samples for each class. Each graph is composed of two parts: the rationale part and the non-rationale part."}, {"title": "4.1 Datasets and Experimental Setup", "content": "The label of each graph is determined by the rationale part. Therefore, the ground-truth of the explanation subgraph is the rationale part of each graph. (ii) MNIST-sp [15]: MNIST-sp takes the MNIST images and transforms them into 70,000 superpixel graphs. Each graph consists of 75 nodes and is assigned one of 10 class labels. The subgraphs that represent the digits can be interpreted as ground truth explanations. (iii) OGBG-Molsider and OGBG-Moltox21 [11]: These two datasets are molecule datasets from the graph property prediction task on Open Graph Benchmark (OGBG), they contain 27 and 12 binary labels for each graph, which transformed into 27 and 12 binary classification tasks respectively. The dataset statistics are available in Table 1.\nExperimental Setup. To investigate whether generating explanations can help with the classification task, we chose three few-shot learning methods: ProtoNet [29], MAML [7], ASMAML [20]. To compare with existing self-explaining meth-ods, we selected two state-of-the-art self-explaining models: GREA [17] and CAL [30] as baselines to compare the performance of classification and quality of gen-erated explanations. Moreover, for fairness, we adapt meta-training to GREA [17] and CAL [30], enabling them to adapt to few-shot scenarios, which are denoted as GREA_Meta and CAL_Meta respectively.\nWe use GIN and GraphSAGE as GNN backbones for all methods. The per-formance of all models is evaluated on Dest. For the Synthetic and MNIST-sp with explanation ground-truth, we use Accuracy to evaluate the classification performance and AUC-ROC to evaluate the quality of the explanation selected.\nFor the two molecule datasets, due to the absence of explanation ground-truth, we only evaluate the classification performance using Area under the ROC curve (AUC) following [17]. For meta-training, we utilize Adam optimizer for local and global updates and set local update times T to 5. Local learning rate n\u2081 is set to 0.001 and global learning rate \u03b7\u2081 is tuned over {1e-5, 1e-4, 1e-3}. \u03b3 in Equation 9 is tuned over {0.1, 0.2, 0.3, 0.4, 0.5}, number of GNN layers is tuned over {2,3}. We select hyper-parameters based on related works and grid searches. All our experiments are conducted with one Tesla V100 GPU."}, {"title": "Performance on Synthetic Graphs and MNIST-sp.", "content": "To explore whether MSE-GNN can achieve high performance on classification and generate high-quality explanation, we conduct 2-way 5-shot experiments on Synthetic and MNIST-sp datasets which contain ground-truth explanations for each graph.\nThe experimental results are summarized in Table 2 and Table 3. We first com-pare meta-trained self-explaining baseline models (GREA_Meta, CAL_Meta) with themselves (GREA_Raw, CAL_Raw). We can observe that significant performance boosts are brought by meta-training on both classification and ex-planation, which indicates that meta-training can leverage the meta-knowledge learned across training tasks effectively on new tasks.\nOn Synthetic, MSE-GNN shows superiority to other baseline methods on the performance of classification and explanation quality. Compared to meta-trained self-explaining baselines, MSE-GNN performs better on both classification and explanation as MSE-GNN utilizes task information and effectively leverages the augmented graph through the introduction of supervised contrastive loss. More-over, the inherent denoising capability of self-explaining models contributes to the superior classification performance of MSE-GNN compared to ProtoNet, MAML, and ASMAML.\nUnexpectedly, CAL achieves the best classification performance on MNIST-sp, especially when using GIN as the backbone, surpassing MSE-GNN by over 5%. Meanwhile, the quality of explanations is significantly lower compared to GREA and MSE-GNN. By visualization in Figure 3, which reveals the internal reasoning process of models, we can find that CAL generated explanations that were opposite to our expectations, indicating that CAL infers the digit based on the shape of the background. It is also easy to understand that the digital in a picture can be inferred from the background since the number part and the background part are complementary sets. Therefore, despite the generated expla-nations being contrary to our expectations, CAL's performance demonstrated that utilizing background information for digit prediction is more effective on MNIST-sp. The reason for CAL generating opposite explanations is that it lacks constraints on the size of the explanation. As a result, it tends to favor subgraphs that contain more useful information and overlook the size of the explanation"}, {"title": "Performance on OGBG.", "content": "MSE-GNN achieves comparable classification per-formance on these two molecule datasets, demonstrating the effectiveness of its structure. Furthermore, we can observe that the self-explaining models with meta-training outperform all meta-learning models except on OGBG-molsider using GraphSAGE. This is because the process of generating explanations can potentially improve the classification task by eliminating irrelevant noise."}, {"title": "Performance with Different Size of Support Set.", "content": "Intuitively, for a classi-fication task, the size of the training set has a significant impact on the model's performance. Therefore, in the scenario of few-shot learning, we evaluate the performance of MSE-GNN and other self-explaining models under different sup-port set sizes. Experimental results are shown in Figure 5. First, comparing"}, {"title": "Ablation Study.", "content": "Table 4 demonstrates the impact of contrastive loss and task information utilized in MSE-GNN on Synthetic with GIN. When applying Con-trastive Loss (CL), both the classification accuracy and the quality of generated explanations of the model are improved. This indicates that introducing con-trastive loss can enhance the model's performance and lead to better results in prediction and explanation tasks. On the other hand, when applying Task Infor-mation (TI), the model's performance is also improved across all datasets. This suggests that incorporating task information into the model can provide addi-tional context and guidance, thereby enhancing the model's ability. Moreover, when both CL and TI are used together, the model excels significantly across all datasets, indicating that the combination of CL and TI can synergistically contribute to better performance on both classification and explanation tasks."}, {"title": "Sensitivity Analysis.", "content": "In MSE-GNN, the parameter y is crucial in controlling the size of the selected explanation. To examine the sensitivity of the model to different values of y, we conduct a sensitivity analysis on the Synthetic and OGBG-Molsider datasets with GIN. As illustrated in Figure 4, the results demonstrate that MSE-GNN achieves the best classification performance when y is set to 0.1 on both datasets, while the explaining performance achieves best when y equals 0.05 on Synthetic. We observe that as the value of y deviates from these two optimal points, the classification performance or the quality of generated explanations decreases. We also notice that the impact of y is less"}, {"title": "Furthermore", "content": ", T, which stands for the number of local update epochs, affects both the effectiveness and efficiency of the MSE-GNN. We compared the per-formance of MSE-GNN with different local update epochs on the Synthetic and OGBG-Molsider datasets. The experimental results shown in Figure 6 indicate that when T is set to 5, MSE-GNN achieves the best classification and explaining performance on both Synthetic and OGBG-molsider. A too-small (too-large) T may result in underfitting (overfitting) of the model for new tasks."}, {"title": "5 Related Works", "content": "Few-shot learning and Meta Learning on Graph Classification Few-shot learning aims to learn a model with only a few samples. A promising kind of method is meta learning. Meta learning is also known as \"learning to learn\", which attempts to learn meta-knowledge from a variety of tasks. There two catogeries for meta-learning [44]: metric-based models [29,3,8,22,32] and optimization-based models [7,9,51,20,34]. The former focuses on computing the distance between query data and class prototypes [29]. The latter aims to learn an effective initialization of parameters, which enables rapid adaption [7]. [51] firstly applied meta learning framework to the node classification task. [20] utilize a step controller for the robustness and generalization of meta-learner. Notwith-standing the remarkable accuracy improvement achieved by these methods on"}, {"title": "Explainability in Graph Neural Network", "content": "With more attention paid to the applications of GNNs, the explainability of GNNs is more crucial. The ex-planation increases the models' transparency and enhances practitioners' trust in GNN models by enriching their understanding of why the decision is made by GNNs. Explainability of GNNs can be categorized into two classes [40,42]: post-hoc explanations and self-explainable GNNs. Post-hoc explanations at-tempt to give explanations for trained GNNs with additional explainer model [39,33,12,18,1,19,5,13]. However, these post-hoc explainers often fail to unveil the true reasoning process of the model due to the non-convexity and complexity of the underlying GNN models [25]. Self-explaining GNNs design specific GNN models which are interpretable intrinsically [37,30,17,50,21,1]. They output the prediction and corresponding explanation simultaneously. DIR [37] aims to ex-tract causal rationales that remain consistent across various distributions while eliminating unstable spurious patterns. GREA [17] is another self-explainable model that introduces a new augmentation operation called environment re-placement that automatically creates virtual data examples to improve rationale identification. Another category of self-explaining models leverages the concept of prototype learning [50,27,1,26,47]. ProtGNN [50] provides explanations by selecting subgraphs that are the most relevant to graph patterns for identify-ing graphs of each class. However, existing self-explainable GNNs overlook the scarcity of labeled graph data in many applications. Thus, it's important to build few-shot learning models with self-explainability."}, {"title": "6 Conclusion", "content": "In this paper, we proposed MSE-GNN to address the explainability of GNN in few-shot scenarios. To be specific, MSE-GNN adopted a \u201cexplainer-predictor\u201d 2-stage self-explaining structure and a meta-training framework based on meta-learning, which improved performance in few-shot scenarios. MSE-GNN also introduced a mechanism to leverage task information to assist explanation gener-ation and result prediction. Additionally, MSE-GNN employed graph augmenta-tion to enhance model robustness. Extensive experimental results demonstrated that MSE-GNN achieves strong performance in classification tasks while select-ing high-quality explanations in few-shot scenarios."}]}