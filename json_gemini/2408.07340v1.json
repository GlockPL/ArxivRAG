{"title": "Towards Few-shot Self-explaining Graph Neural Networks", "authors": ["Jingyu Peng", "Qi Liu", "Linan Yue", "Zaixi Zhang", "Kai Zhang", "Yunhao Sha"], "abstract": "Recent advancements in Graph Neural Networks (GNNs) have spurred an upsurge of research dedicated to enhancing the explain-ability of GNNs, particularly in critical domains such as medicine. A promising approach is the self-explaining method, which outputs explanations along with predictions. However, existing self-explaining models require a large amount of training data, rendering them unavailable in few-shot scenarios. To address this challenge, in this paper, we propose a Meta-learned Self-Explaining GNN (MSE-GNN), a novel framework that generates explanations to support predictions in few-shot settings. MSE-GNN adopts a two-stage self-explaining structure, consisting of an explainer and a predictor. Specifically, the explainer first imitates the attention mechanism of humans to select the explanation subgraph, whereby attention is naturally paid to regions containing important characteristics. Subsequently, the predictor mimics the decision-making process, which makes predictions based on the generated explanation. Moreover, with a novel meta-training process and a designed mechanism that exploits task information, MSE-GNN can achieve remarkable performance on new few-shot tasks. Extensive experimental results on four datasets demonstrate that MSE-GNN can achieve superior performance on prediction tasks while generating high-quality explanations compared with existing methods. The code is publicly available at https://github.com/jypeng28/MSE-GNN.", "sections": [{"title": "1 Introduction", "content": "Due to the widespread presence of graph data in diverse domains [49,48], Graph Neural Networks (GNNs) [14,36,6] are attracting increasing attention from the research community. Leveraging the message passing paradigm, GNNs have exhibited remarkable efficacy across multiple scenes, including molecule property prediction [35], social network analysis [2,45], and recommender system [4]. Despite these successes, a significant drawback of GNN models is their lack of explainability, making it unavailable for humans to understand the basis of predictions. This limitation undermines the complete trust in GNN predictions, consequently restricting their application in high-stake scenarios including medical [50] and finance [24] fields. Furthermore, the European Union has explicitly emphasized the necessity of explainability for trustworthy AI in [28] and any studies focusing on explainability have been conducted on interpretability in other fields [41,43]. Therefore, there is an immediate and pressing need for research into the explainability of GNNs.\nThe field of GNN explainability has witnessed substantial scholarly attention [17,21,30,16]. Generally, research on the explainability of GNN can be divided into two main categories: post-hoc explanations and self-explaining methods [40]. Among them, the post-hoc explanation strives to elucidate the predictions made by a trained GNN model. Typically, this is achieved by leveraging another explanatory model to select a subset of input as the explanation for GNN prediction. Despite their utility, these post-hoc explainers often fall short of revealing the actual reasoning process of the model [25] and require optimization for each input graph, which is time-consuming. Therefore, in this paper, we focus on self-explaining methods.\nThe self-explaining method refers to intrinsically explainable GNN models that offer predictions and explanations concurrently, with the prediction being rooted in the explanation. One prevalent type of self-explaining model typically adopts a \"explainer-predictor\" two-stage paradigm, as illustrated in Figure 1. This paradigm contains two stages, one is called the explainer, which generates an explanation for each input graph, and the other is the predictor making predictions based on the generated explanation [37,17]."}, {"title": "Towards Few-shot Self-explaining Graph Neural Networks", "content": "Although the self-explaining methods in GNN are promising, they still suffer from heavily relying on extensive training data, which restricts their applicability in situations with limited data sizes. For instance, during new drug discovery processes, clinical trials are conducted to assess various drug attributes such as toxicity and side effects. Due to safety concerns, the number of participants in these trials is restricted, resulting in limited experimental data. In such few-shot scenarios, existing self-explaining models fail to achieve satisfactory performance, while existing few-shot learning methods are lack of explainability. Hence, there is a pressing need to design a self-explaining GNN for few-shot scenarios.\nDrawing on the fundamental human intelligence traits of rapid learning and self-explainability [23,29,7], we develop Meta-learned Self-Explaining GNN (MSE-GNN) for few-shot scenarios:\nI. During classification tasks, humans initially concentrate on regions that contain crucial features, and subsequently perform classification based on these features, adhering to a two-stage paradigm [23].\nII. When learning new concepts, humans tend to seek representative instances or prototypes and compare new instances with these prototypes to categorize them [29].\nIII. Humans can learn meta-knowledge from a multitude of tasks, which enables them to achieve impressive performance on new tasks with limited data, which is called \u201clearn to learn\u201d [7].\nBy incorporating these attributes into our MSE-GNN, we aim to solve the explainability of GNNs in few-shot scenarios, and then enhance the performance of both explanation and prediction tasks.\nSpecifically, the MSE-GNN model follows the two-stage paradigm as depicted in Figure 1, which naturally mimics the human's two-stage recognition process as mentioned in I. Among them, the explainer, which is composed of a GNN encoder and a MLP, predicts the probability of each node being selected as an explanation. Then, node representations encoded by another GNN encoder are separated into explanation and non-explanation based on the prediction of the explainer. Subsequently, the predictor mimics the decision-making process, which makes predictions based on the explanation with a MLP.\nFurthermore, the MSE-GNN model incorporates a novel mechanism that exploits task information to help with selecting explanations and making predictions. Prototype, as stated in II, has been proven to be effective to generate representative representations for each category [31,46]. Therefore, in MSE-GNN, the concept of prototype is utilized in generating task information. The training framework of optimization-based meta-learning imitates the paradigm of \u201clearning to learn\" in III, where models can acquire meta-knowledge by learning from a vast array of tasks. One of the most popular and effective methods is MAML [7] (Model-Agnostic Meta-Learning). Therefore, we design a new meta-training framework based on MAML to train MSE-GNN.\nWe conduct extensive experiments on one synthetic dataset [39] and three real datasets of graph classification tasks [15,11], which show excellent performance on both prediction and explanation generated."}, {"title": "2 Problem Definition", "content": "In this section, we will elaborate on the problem definition of our research. Following [20], we form the few-shot graph classification problem as N-way K-shot graph classification. Given the dataset G = {(G1, y1), (G2,Y2), ..., (Gn, Yn)}, where Gi denotes a graph with a node set Vi and a edge set Ei. ni denotes the number of nodes in the graph. The structure feature is represented by an adjacency matrix \\(A_i \\in \\mathbb{R}^{n_i \times n_i}\\). The node attribute matrix is represented as \\(X_i \\in \\mathbb{R}^{n_i \times d}\\), where d is the dimension of the node attribute.\nThen, the dataset is splitted into {\\(G_{train}\\), \\(y_{train}\\)} and {\\(G_{test}\\), \\(y_{test}\\)} as training set and test set respectively according to label y. Where \\(y_{train} \\cap y_{test} = \u00d8\\). When training, a task T is sampled each time and each task contains support set \\(D_{sup}^{train} = \\{(G_{train}, y_{train})\\}_{i=1}^{S}\\) and query set \\(D_{que}^{train} = \\{(G_{train}, y_{train})\\}_{i=1}^{q}\\), where s and q stands for the size of support set and query set respectively. It is noteworthy that the same class space is shared in the same task.\nIn each task, our goal is to optimize our model on the support set \\(D_{sup}\\) and make predictions on the query set \\(D_{que}\\). If a support set contains N classes and K data for each class, then we name the problem as N-way K-shot. When testing, we firstly finetune the learned model on support set \\(D_{sup}^{test} = \\{(G_{test}, y_{test})\\}_{i=1}^{q}\\) and then report the classification performance of finetuned model on \\(D_{que}^{test} = \\{(G_{test}, y_{test})\\}_{i=1}^{q}\\). Our goal of the few-shot graph classification problem is to develop a model that can obtain meta-knowledge across {\\(G_{train}\\), \\(y_{train}\\)} and predicts labels for graphs in the query set in test stage \\(D_{que}^{test}\\).\nIn the explanation generation task, for each graph Gi, a node mask vector \\(m_i \\in [0,1]^{n_i \times 1}\\) is the explanation subgraph selected, a higher value means that the corresponding node is more important for making prediction and vice versa. Although selecting edges for explanation is a viable approach, in this paper we focus on node selection due to its computational complexity."}, {"title": "3 The Proposed MSE-GNN", "content": "3.1 Architecture of MSE-GNN\nIn Figure 2, we show the overall architecture of the MSE-GNN, which contains three components: an explainer g that outputs the explanation selected, a predictor p making the final prediction, and a graph encoder f.\nBefore we present the details of MSE-GNN, we first clarify serveral concepts. Specifically, existing works often combine self-explaining methods with the concept of rationale [37,17]. The rationale in graph data refers to the subsets of nodes or subsets of edges, which form subgraphs that determine the prediction. Hence, we posit that explanation and rationale are equivalent, as they share the same concept.\nIn MSE-GNN, the input graph is encoded by f and each node v is encoded into a node embedding \\(h(v) \\in \\mathbb{R}^d\\), where d stands for the dimension of hidden size. The encoder can be any kind of GNN, e.g. GCN [14], GIN [38], and GraphSAGE [10]. The selector outputs a mask vector m for each graph as an explanation, which divides the graph into rationale (explanation) Gr and non-rationale Gn. Then the predictor makes predictions based on the graph embedding rationale subgraph. Meanwhile, augmented graphs that combine rationale and non-rationale from different graphs are fed into the predictor to ensure the robustness of the predictor. We categorize the parameters into fast parameters and slow parameters according to the timing of updating, which will be described in detail in section 3.3.\nTask Information. MSE-GNN generates task information for the explainer and the predictor to facilitate explanation generation and prediction within each task, which is composed of prototypes representing each class.\nIn each task, a support set is provided, which contains data from multiple classes. We aim to extract prototypes from these data that capture the characteristics of each class in the task, in order to help with task-specific selection of explanations and the classification task. Encoded by encoder f, each graph is represented by a matrix containing embedding of each node:\n\\(H_i = [..., h(v), ...]v \\in V_i = f(G_i) \\in \\mathbb{R}^{|V_i| \times d}\\) (1)\nTo obtain representation for each graph hi, the readout function, e.g. mean pooling is employed, to aggregate node embeddings. By leveraging the concept of prototype learning, we further fuse the graph representations of each class with another readout function. Thus, we can obtain a prototype embedding for each class:\n\\(TI_c = f_{readout}([..., f_{readout}(H_i), ...]y_i = c) \\in \\mathbb{R}^d\\) (2)\nFor an N-way K-shot classification problem, the task information is formed by concatenating prototypes of N classes. It is worth noting that, task information for each input graph of both \\(D_{sup}\\) and \\(D_{que}\\) is composed solely of graphs in \\(D_{sup}\\) to prevent label leakage.\nExplainer. The explainer is responsible for choosing the explanation subgraph corresponding to each input graph. Specifically, given an input graph Gi, the explainer firstly uses another GNN encoder to map each node to another node embedding h'(v) for each node in Vi for selection. Then, a MLP is utilized to transform the node embeddings into a soft mask vector \\(m_i \\in [0,1]^{n_i \times 1}\\), with task information \\(TI_c\\) and node embedding h(v) concatenated as input:\n\\(m_i = \\sigma(MLP([..., [h(v), TI_c], \u2026\u2026\u2026]v \\in V_i))\\), (3)\nwhere \\(\\sigma\\) denotes the sigmoid function. Hence, we can decompose the input graph Gi into a rationale subgraph and non-rationale subgraph according to mi respectively:\n\\(G_r^i = \\{A_i, X_i \\odot m_i\\}\\) \\(G_n^i = \\{A_i, X_i \\odot \\overline{m_i}\\}\\), (4)\nwhere \\(\\overline{m_i} = 1 - m_i\\). Meanwhile, given the node embedding h(v) from encoder f, we can obtain the graph embedding for \\(G_r^i\\) and \\(G_n^i\\):\n\\(h_r^i = f_{readout}(H_i \\odot m_i)\\) \\(h_n^i = f_{readout}(H_i \\odot \\overline{m_i})\\). (5)\nPredictor and Graph Augmentation. The predictor takes the graph embedding \\(h_r^i\\) as input and makes the final prediction \\(\\hat{y} = p(h_r^i)\\) with a MLP. Moreover, we enhance the robustness of the predictor through graph augmentation. Specifically, within the input graph, the rationale component represents the crucial part that determines the category, while the non-rationale component represents the noisy part. By combining the rationale and non-rationale from different graphs in the same task, additional data with noise are generated. Then we assign the label based on rationale. This approach allows us to increase the amount of noisy data, thereby improving the robustness of the predictor. We do the combination operation by adding subgraph embeddings:\n\\(h_{(i,j)} = h_r^i + h_n^j\\) \\(Y_{(i,j)} = Y_i\\), (6)\nwhere \\(h_r^i\\) denotes rationale from Gi and \\(h_n^j\\) means the non-rationale from Gj. Therefore, in addition to task information TI, the predictor p receives the embeddings of both the rationale subgraphs \\(h_r^i\\) and the artificially augmented graphs \\(h_{(i,j)}\\) for optimization, and the output are denoted as \\(\\hat{y_i}\\) and \\(y_{(i,j)}\\) respectively."}, {"title": "3.2 Optimization Objective", "content": "The optimization objective of MSE-GNN is to achieve both high accuracy in predictions and generate precise explanations, which reveal the underlying reasons behind the predictions. Therefore, we design several types of loss functions and constraints. For the sake of simplicity, we consider a binary classification task without loss of generality.\nWith the prediction of each rationale graph embedding \\(p(h_i)\\) and corresponding ground-truth label yi, the loss function is defined as:\n\\(L = y_i log(\\hat{y_i}) + (1 - y_i)log(1 - \\hat{y_i})\\). (7)\nFor the artificially augmented graph, our aim is to minimize the prediction values for instances of the same category while maximizing the prediction values for instances of different categories. To achieve this, we employ a contrastive loss function. For example, for a 2-way K-shot classification task, we can obtain 4\\(K^2\\) augmented graphs, where each rationale graph is combined with other 2K-1 non-rationales, then the loss is computed as:\n\\(L_a = \\frac{1}{2K} \\sum_{j=1}^{2K-1} l_{y_i = y_j} log \\frac{exp(\\hat{y_i} \\cdot \\hat{y_j})/\\tau}{\\sum_{k=1}^{2K-1} exp(\\hat{y_i} \\cdot \\hat{y_k})/\\tau}\\), (8)\nwhere \\(\\tau\\) is a scalar temperature hyperparameter.\nBesides, to address the deviation in the size of rationales, we introduce a penalty based on the number of rationale nodes, the following regularization term is utilized:\n\\(L_{reg} = \\gamma \\cdot (1 - \\frac{1}{|V_i|} \\sum_{i}^{n_i} m_i)\\), (9)\nwhere \\(\\gamma\\) is manually set to control the rationale size. Finally, the total loss function can be formulated as:\n\\(L = \\alpha_r \\cdot L^r + \\alpha_a \\cdot L^a + \\alpha_{reg} \\cdot L_{reg}\\), (10)\nwhere \\(\\alpha_r\\), \\(\\alpha_a\\), and \\(\\alpha_{reg}\\) are hypermeters controlling the weight of each loss."}, {"title": "Towards Few-shot Self-explaining Graph Neural Networks", "content": "Algorithm 1 Meta-training of MSE-GNN.\nInput:Distribution over meta-training tasks: p(T); Local learning rate: \\(\\eta_1\\); Global learning rate: \\(\\eta_2\\); Local update times: T.\nOutput:Meta-trained parameters for encoder and explanation selector:\\(\theta_f\\), \\(\theta_g\\), and initialization of parameters for predictor \\(\theta_p\\)\n1: Initialize \\(\theta\\) = {\\(\theta_f\\), \\(\theta_g\\), \\(\theta_p\\)} randomly;\n2: while not converged do\n3: Sample task T with support graphs \\(D_{sup}^{train}\\) and query graphs \\(D_{que}^{train}\\)\n4: Set fast adaptation parameters: \\(\theta'_p\\) = \\(\theta_p\\)\n5: for t = 0 \u2192 T do\n6: Evaluate \\(\nabla_{\theta'_p} L_{sup}(\\theta_f, \\theta_g, \\theta'_p)\\) by calculating loss via Equation 10\n7: Update \\(\theta'_p\\): \\(\theta'_p\\) \u2192 \\(\theta'_p - \\eta_1 \\cdot \nabla_{\\theta'_p} L_{sup}(\\theta_f, \\theta_g, \\theta'_p)\\)\n8: end for\n9: Evaluate \\(\nabla_{\theta} L_{que}(\\theta_f, \\theta_g, \\theta_p)\\) by calculating loss via Equation 10\n10: Update \\(\theta\\): \\(\theta\\) \u2192 \\(\theta - \\eta_2 \\cdot \nabla_{\theta} L_{que}(\\theta, \\theta_g, \\theta_p)\\)\n11: end while"}, {"title": "3.3 Meta Training", "content": "Inspired by the concept of \"learn to learn\" [7], we propose a new meta-training framework based on MAML to obtain meta knowledge from various tasks. We denote \\(\theta_f\\), \\(\theta_g\\), and \\(\theta_p\\) as the parameters of encoder, explanation selector, and the predictor. Specifically, MSE-GNN is trained from two procedures. One is global update, which aims to learn the parameters of encoder \\(\theta_f\\), explanation generator \\(\theta_g\\), and initialization of the predictor \\(\theta_p\\) from different tasks, the other is called local update, which performs fast adaption on new tasks and locally update only parameters of the predictor \\(\theta_p\\) within each task. According to the timing of updating, we categorize the parameters into fast parameters (\\(\theta_p\\)) and slow parameters (\\(\theta_f\\) and \\(\theta_g\\)), as shown in Figure 2.\nThe meta-training process is demonstrated in Algorithm 1. Firstly, we sample a task composed of support \\(D_{sup}^{train}\\) and query data \\(D_{que}^{train}\\) for each episode. Then adaption is operated by updating \\(\theta_p\\) for T times on \\(D_{sup}^{train}\\), where T is a hyperparameter controlling the number of local updates, which is shown in lines 5-8. With updated \\(\theta'_p\\), we utilize the loss on \\(D_{que}^{train}\\) to update \\(\theta_f\\), \\(\theta_g\\) and \\(\theta_p\\).\nIt is important to highlight that, the explainer is trained from a variety of tasks and frozen when optimizing each task, which ensures the stability of the explanation selected across different tasks and prevents over-fitting. Therefore, \\(\theta_f\\) and \\(\theta_g\\) are only updated in the global update and fixed in the local update. While the predictor needs to learn the relationship between features and categories in different tasks based on the generated explanations. As a result, the \\(\theta_p\\) is optimized in the local update to learn the association between features and categories. Hyperparameters of loss computation in line 6 and line 9 can be differently set according to the goal of local and global optimization."}, {"title": "4 Experiments", "content": "4.1 Datasets and Experimental Setup\nDataset. We conduct extensive experiments on four datasets to validate the performance of MSE-GNN: (i) Synthetic: Due to the lack of graph datasets with explanation ground-truth, following [39], we create a synthetic dataset for classification, which contains 10 classes and 500 samples for each class. Each graph is composed of two parts: the rationale part and the non-rationale part."}, {"title": "Performance on Synthetic Graphs and MNIST-sp", "content": "To explore whether MSE-GNN can achieve high performance on classification and generate high-quality explanation, we conduct 2-way 5-shot experiments on Synthetic and MNIST-sp datasets which contain ground-truth explanations for each graph. The experimental results are summarized in Table 2 and Table 3. We first compare meta-trained self-explaining baseline models (GREA_Meta, CAL_Meta) with themselves (GREA_Raw, CAL_Raw). We can observe that significant performance boosts are brought by meta-training on both classification and explanation, which indicates that meta-training can leverage the meta-knowledge learned across training tasks effectively on new tasks.\nOn Synthetic, MSE-GNN shows superiority to other baseline methods on the performance of classification and explanation quality. Compared to meta-trained self-explaining baselines, MSE-GNN performs better on both classification and explanation as MSE-GNN utilizes task information and effectively leverages the augmented graph through the introduction of supervised contrastive loss. Moreover, the inherent denoising capability of self-explaining models contributes to the superior classification performance of MSE-GNN compared to ProtoNet, MAML, and ASMAML.\nUnexpectedly, CAL achieves the best classification performance on MNIST-sp, especially when using GIN as the backbone, surpassing MSE-GNN by over 5%. Meanwhile, the quality of explanations is significantly lower compared to GREA and MSE-GNN. By visualization in Figure 3, which reveals the internal reasoning process of models, we can find that CAL generated explanations that were opposite to our expectations, indicating that CAL infers the digit based on the shape of the background. It is also easy to understand that the digital in a picture can be inferred from the background since the number part and the background part are complementary sets. Therefore, despite the generated explanations being contrary to our expectations, CAL's performance demonstrated that utilizing background information for digit prediction is more effective on MNIST-sp. The reason for CAL generating opposite explanations is that it lacks constraints on the size of the explanation. As a result, it tends to favor subgraphs that contain more useful information and overlook the size of the explanation"}, {"title": "Towards Few-shot Self-explaining Graph Neural Networks", "content": "subgraph. Furtherly comparing the visualization of explanations of MSE-GNN and GREA, we can find that explanations of MSE-GNN are more compact and focus more on the digital part, which is in line with the result in Table 3.\nPerformance on OGBG. MSE-GNN achieves comparable classification performance on these two molecule datasets, demonstrating the effectiveness of its structure. Furthermore, we can observe that the self-explaining models with meta-training outperform all meta-learning models except on OGBG-molsider using GraphSAGE. This is because the process of generating explanations can potentially improve the classification task by eliminating irrelevant noise.\nPerformance with Different Size of Support Set. Intuitively, for a classification task, the size of the training set has a significant impact on the model's performance. Therefore, in the scenario of few-shot learning, we evaluate the performance of MSE-GNN and other self-explaining models under different support set sizes. Experimental results are shown in Figure 5. First, comparing"}, {"title": "Towards Few-shot Self-explaining Graph Neural Networks", "content": "different methods, we observe that MSE-GNN consistently outperforms other baselines across different support set sizes, which further validates the performance of MSE-GNN on both classification and explaining. Next, comparing the performance of MSE-GNN across different support set sizes, we observe that as the support set size increases, both the classification accuracy and the quality of generated explanations improve. This also demonstrates the importance of training set size on model performance.\nAblation Study. Table 4 demonstrates the impact of contrastive loss and task information utilized in MSE-GNN on Synthetic with GIN. When applying Contrastive Loss (CL), both the classification accuracy and the quality of generated explanations of the model are improved. This indicates that introducing contrastive loss can enhance the model's performance and lead to better results in prediction and explanation tasks. On the other hand, when applying Task Information (TI), the model's performance is also improved across all datasets. This suggests that incorporating task information into the model can provide additional context and guidance, thereby enhancing the model's ability. Moreover, when both CL and TI are used together, the model excels significantly across all datasets, indicating that the combination of CL and TI can synergistically contribute to better performance on both classification and explanation tasks.\nSensitivity Analysis. In MSE-GNN, the parameter \\(\\gamma\\) is crucial in controlling the size of the selected explanation. To examine the sensitivity of the model to different values of \\(\\gamma\\), we conduct a sensitivity analysis on the Synthetic and OGBG-Molsider datasets with GIN. As illustrated in Figure 4, the results demonstrate that MSE-GNN achieves the best classification performance when \\(\\gamma\\) is set to 0.1 on both datasets, while the explaining performance achieves best when \\(\\gamma\\) equals 0.05 on Synthetic. We observe that as the value of \\(\\gamma\\) deviates from these two optimal points, the classification performance or the quality of generated explanations decreases. We also notice that the impact of \\(\\gamma\\) is less"}, {"title": "Towards Few-shot Self-explaining Graph Neural Networks", "content": "pronounced on the OGBG-Molsider dataset, indicating that the model is less sensitive to \\(\\gamma\\) on OGBG-Molsider.\nFurthermore, T, which stands for the number of local update epochs, affects both the effectiveness and efficiency of the MSE-GNN. We compared the performance of MSE-GNN with different local update epochs on the Synthetic and OGBG-Molsider datasets. The experimental results shown in Figure 6 indicate that when T is set to 5, MSE-GNN achieves the best classification and explaining performance on both Synthetic and OGBG-molsider. A too-small (too-large) T may result in underfitting (overfitting) of the model for new tasks."}, {"title": "5 Related Works", "content": "Few-shot learning and Meta Learning on Graph Classification Few-shot learning aims to learn a model with only a few samples. A promising kind of method is meta learning. Meta learning is also known as \"learning to learn\", which attempts to learn meta-knowledge from a variety of tasks. There two catogeries for meta-learning [44]: metric-based models [29,3,8,22,32] and optimization-based models [7,9,51,20,34]. The former focuses on computing the distance between query data and class prototypes [29]. The latter aims to learn an effective initialization of parameters, which enables rapid adaption [7]. [51] firstly applied meta learning framework to the node classification task. [20] utilize a step controller for the robustness and generalization of meta-learner. Notwithstanding the remarkable accuracy improvement achieved by these methods on few-shot learning tasks, their lack of explainability hinders their applicability in certain scenarios such as the medical and finance area.\nExplainability in Graph Neural Network With more attention paid to the applications of GNNs, the explainability of GNNs is more crucial. The explanation increases the models' transparency and enhances practitioners' trust in GNN models by enriching their understanding of why the decision is made by GNNs. Explainability of GNNs can be categorized into two classes [40,42]: post-hoc explanations and self-explainable GNNs. Post-hoc explanations attempt to give explanations for trained GNNs with additional explainer model [39,33,12,18,1,19,5,13]. However, these post-hoc explainers often fail to unveil the true reasoning process of the model due to the non-convexity and complexity of the underlying GNN models [25]. Self-explaining GNNs design specific GNN models which are interpretable intrinsically [37,30,17,50,21,1]. They output the prediction and corresponding explanation simultaneously. DIR [37] aims to extract causal rationales that remain consistent across various distributions while eliminating unstable spurious patterns. GREA [17] is another self-explainable model that introduces a new augmentation operation called environment replacement that automatically creates virtual data examples to improve rationale identification. Another category of self-explaining models leverages the concept of prototype learning [50,27,1,26,47]. ProtGNN [50] provides explanations by selecting subgraphs that are the most relevant to graph patterns for identifying graphs of each class. However, existing self-explainable GNNs overlook the scarcity of labeled graph data in many applications. Thus, it's important to build few-shot learning models with self-explainability."}, {"title": "6 Conclusion", "content": "In this paper, we proposed MSE-GNN to address the explainability of GNN in few-shot scenarios. To be specific, MSE-GNN adopted a \u201cexplainer-predictor\u201d 2-stage self-explaining structure and a meta-training framework based on meta-learning, which improved performance in few-shot scenarios. MSE-GNN also introduced a mechanism to leverage task information to assist explanation generation and result prediction. Additionally, MSE-GNN employed graph augmentation to enhance model robustness. Extensive experimental results demonstrated that MSE-GNN achieves strong performance in classification tasks while selecting high-quality explanations in few-shot scenarios."}]}