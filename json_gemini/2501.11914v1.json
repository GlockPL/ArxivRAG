{"title": "LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts", "authors": ["Md Kamrujjaman Mobin", "Md Saiful Islam"], "abstract": "This paper presents a system developed for Task 1 of the COLING 2025 Workshop on Detecting AI-Generated Content, focusing on the binary classification of machine-generated versus human-written text. Our approach utilizes an ensemble of models, with weights assigned according to each model's inverse perplexity, to enhance classification accuracy. For the English text detection task, we combined ROBERTa-base, RoBERTa-base with the OpenAI detector, and BERT-base-cased, achieving a Macro F1-score of 0.7458, which ranked us 12th out of 35 teams. We ensembled RemBERT, XLM-ROBERTa-base, and BERT-base-multilingual-case for the multilingual text detection task, employing the same inverse perplexity weighting technique. This resulted in a Macro F1-score of 0.7513, positioning us 4th out of 25 teams. Our results demonstrate the effectiveness of inverse perplexity weighting in improving the robustness of machine-generated text detection across both monolingual and multilingual settings, highlighting the potential of ensemble methods for this challenging task.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of language models such as GPT (Radford et al., 2019) and BERT (Devlin et al., 2019) has increased machine-generated content, raising significant concerns about misinformation and academic integrity. Identifying AI-generated text becomes more challenging in multilingual contexts, where linguistic diversity adds further complexity to model generalization. While existing approaches perform well in English, their effectiveness decreases when applied to languages with diverse syntactic and semantic structures.\nIn Task 1 of the COLING 2025 Workshop on Detecting AI-Generated Content (Wang et al., 2025), we propose an ensemble-based solution to address these issues. For English detection, we combine ROBERTa-base (Liu et al., 2019), OpenAI's AI text detector (Solaiman et al., 2019), and BERT-base-cased (Devlin et al., 2019). For multilingual detection, we integrate RemBERT (Chung et al., 2021), XLM-ROBERTa-base (Conneau et al., 2019), and BERT-base-multilingual-cased (Devlin et al., 2019). To further improve performance, we incorporate inverse perplexity weighting to give greater priority to models that produce lower perplexity scores. Our ensemble approach achieved a Macro F1-score of 0.7458 (Micro F1: 0.7568) in English, placing us 12th out of 35 teams, and a Macro F1-score of 0.7513 (Micro F1: 0.7527) for the multilingual tasks, ranking 4th out of 25 teams.\nWe encountered several challenges during this work. One major issue was data imbalance, as human-written content vastly outnumbered AI-generated samples. To address this, we employed data augmentation and optimized our sampling strategies. Another challenge involved ensuring the models' generalization across different languages and writing styles, often with limited training data. This highlights the importance of additional fine-tuning and the need to explore alternative architectures that can better handle diverse linguistic inputs.\nThis paper presents a robust ensemble approach for detecting AI-generated content, with strong performance across both English and multilingual tasks. However, significant opportunities remain for improving model generalization and addressing data imbalance, which will be crucial for future advancements in this field. The following sections will discuss the dataset, methodology, results, a detailed analysis of the findings, and conclusions drawn from this study."}, {"title": "2 Background", "content": null}, {"title": "2.1 Dataset", "content": "The provided dataset includes training and validation sets for two subtasks: Subtask A (English-only"}, {"title": "2.2 Related Work", "content": "The detection of AI-generated text has garnered significant attention with the advent of large language models (LLMs) such as GPT (Radford et al., 2019) and BERT (Devlin et al., 2019). Fine-tuning Transformer-based models for binary classification has shown efficacy; however, challenges persist, particularly in multilingual settings where data biases impede generalization (Zellers et al., 2019; Solaiman et al., 2019).\nEnsemble methods combining BERT, ROBERTa, and GPT variants have enhanced robustness across domains and languages (Schick and Sch\u00fctze, 2020). Perplexity-based weighting strategies further optimize individual model contributions (Clark et al., 2019). Multilingual models like XLM-ROBERTa (Conneau et al., 2019) and RemBERT (Chung et al., 2021) improve cross-lingual performance, though low-resource languages remain challenging (Hu et al., 2020).\nRecent advancements in shared tasks, such as those introduced by SemEval (Fetahu et al., 2023; Wang et al., 2024), have refined methodologies through task-specific fine-tuning and the integration of multilingual pre-trained models (Eger et al., 2023; Siino, 2024).\nBuilding upon these foundations, our work employs an inverse perplexity-weighted ensemble approach to optimize model contributions, enhancing robustness in both monolingual and multilingual detection scenarios."}, {"title": "3 System Overview", "content": "We developed an ensemble approach for AI-generated text detection across English and multilingual contexts, using Transformer models with a weighted voting strategy based on inverse perplexity for improved accuracy. The system overview is shown in Figure 1."}, {"title": "3.1 Ensemble Model Selection and Justification", "content": "We selected six Transformer-based models for our ensemble: three for English and three for multilingual contexts, chosen for their ability to capture linguistic and syntactic patterns.\n\u2022 English Models: The models utilized in our work include ROBERTa-base, renowned for its robust performance in natural language understanding, effectively capturing deep syntactic and semantic patterns (Liu et al., 2019). Additionally, the RoBERTa-base OpenAI detector is fine-tuned to detect AI-generated content by identifying subtle machine-generated patterns (Solaiman et al., 2019). Lastly, BERT-base-cased is incorporated for its capability to handle case-sensitive distinctions, which are critical for nuanced classification tasks (Devlin et al., 2019).\n\u2022 Multilingual Models: For Multilingual, we employed RemBERT, a model optimized for multilingual tasks, demonstrating exceptional performance in syntactic and semantic understanding across languages (Chung et al., 2021). Furthermore, XLM-RoBERTa-base is employed for its strength in cross-lingual applications, adeptly handling diverse language structures (Conneau et al., 2019). Additionally, BERT-base-multilingual-cased is used as it is specifically designed to capture linguistic diversity and perform effectively in multilingual tasks (Devlin et al., 2019).\nWe trained these models on the dataset provided as part of the shared task 1 (Wang et al., 2025), including human-authored and AI-generated content. This enabled the ensemble to generalize effectively across both English and multilingual contexts."}, {"title": "3.2 Data Pre-processing", "content": "The multilingual task presented a significant data imbalance across languages, as shown in Table 1, which details the original distribution of training and validation samples. For instance, the dataset included 610,676 English (en) samples and 35,284 Chinese (zh) samples, whereas underrepresented languages like Urdu (ur), Arabic (ar), and Russian (ru) had far fewer samples (3,761, 2,114, and 1,314, respectively). This imbalance hindered the model's ability to predict outputs for these underrepresented languages accurately.\nTo mitigate this issue, we implemented a dataset balancing strategy by reducing the number of English and Chinese samples to a proportionate scale. This adjustment enabled the model to better focus on learning patterns in the underrepresented languages, thereby enhancing overall performance and reducing biases in predictions. The detailed data distribution after applying this balancing strategy is presented in Table 2.\nFollowing this, text data was processed using model-specific tokenizers, with truncation and padding applied as needed. To optimize memory usage and training efficiency, text length was calculated and sorted by word count, minimizing unnecessary padding. A fixed random seed was used throughout to ensure reproducibility."}, {"title": "3.3 Training Procedure", "content": "The model was fine-tuned using the Hugging Face Transformers library\u00b9 for both English and multilingual text classification tasks. Datasets were processed into Hugging Face Dataset objects, with tokenization performed using AutoTokenizer for models like RemBERT and ROBERTa-base. The architecture was adapted for classification tasks with appropriate label mappings.\nKey hyperparameters, including learning rate, batch size, and weight decay, were optimized through empirical experiments to balance performance and efficiency. Learning rates between 1 \u00d7 10-5 and 2 \u00d7 10-5 were tested, with lower rates promoting smoother convergence. A batch size of 4 for training and 16 for validation balanced memory and efficiency."}, {"title": "3.4 Ensemble Voting Strategy", "content": "Our ensemble employs a weighted soft-voting strategy, combining predictions from three different models for each subtask. The weights are determined based on inverse perplexity, with lower perplexity values reflecting higher confidence."}, {"title": "3.4.1 Perplexity Calculation", "content": "For each model, we compute the perplexity based on its predictions. The perplexity P is computed using the Negative Log Likelihood formula:\n$P = exp\\Big(-\\frac{1}{N} \\sum_{i=1}^{N} log(p(Y_i | X_i))\\Big)$\nwhere p(Yi | Xi) is the predicted probability for the true label yi, and N is the number of test samples. Lower perplexity values indicate higher confidence.\nTo compute perplexity, we use each model's logits, apply softmax to obtain probabilities, and then calculate perplexity based on the true labels and these probabilities."}, {"title": "3.4.2 Perplexity-Based Weighting Adjustment", "content": "To calculate model weights, each model's perplexity is adjusted by subtracting 1, creating an effective weighting scale. The weight wi for model i is then computed as the inverse of this adjusted perplexity and normalized across models, giving higher confidence models greater influence.\n$w_i = \\frac{1/(P_i - 1)}{\\sum_{i=1}^{M} (1/(P_i \u2013 1))}$\nwhere M represents the total number of models, and Pi is the original perplexity of model i."}, {"title": "3.4.3 Weighted Soft-Voting", "content": "Each model's predicted probabilities are scaled by its weight and summed to form the final ensemble prediction. This weighted voting prioritizes models with higher confidence (lower perplexity), giving them greater influence on the final decision. The ensemble's final prediction for each class c is:\n$P_{ensemble} (c) = \\sum_{i=1}^{M} W_i. P_i(c)$\nwhere pi(c) is the predicted probability for class c by model i, and wi is its weight.\nThis method enhances ensemble accuracy by prioritizing predictions from more confident models, improving overall performance."}, {"title": "4 Results", "content": "This section presents the performance of our ensemble approach for Task 1 at the COLING 2025 Workshop on Detecting AI-Generated Content, evaluated using the Macro F1-score. Detailed results are shown in Table 5.\nThe baseline scores provided by the task organizers (Wang et al., 2025) used ROBERTa-base (Liu et al., 2019) for the English track and XLM-ROBERTa-base (Conneau et al., 2019) for the multilingual track. These scores serve as benchmarks for our ensemble method."}, {"title": "5 Discussion and Conclusion", "content": "In this work, we presented an ensemble approach to detect AI-generated content across English and multilingual datasets. By combining multiple pre-trained models, including RoBERTa-base, OpenAI detector, BERT-base-cased for English, and RemBERT, XLM-ROBERTa-base, BERT-basemultilingual-cased for multilingual tasks, and applying inverse perplexity weighting, our ensemble demonstrated strong performance. It achieved a Macro F1-score of 0.7458 (Micro F1: 0.7568) for English, ranking 12th, and 0.7513 (Micro F1: 0.7527) for multilingual tasks, ranking 4th.\nCompared to individual models, our ensemble consistently outperformed or matched their performance. For example, in the English task, the ensemble scored 0.7568, surpassing RoBERTa + OpenAI detector (0.7381) and BERT (0.7275). Similarly, the multilingual task achieved 0.7527, exceeding RemBERT + XLM-R (0.7473) and RemBERT (0.7507). Notably, our ensemble also outperformed baseline models, with a Macro Fl-score of 0.7458 for English (baseline: 0.7342 achieved by RoBERTa) and 0.7513 for multilingual (baseline: 0.7416 achieved by XLM-RoBERTa). These results highlight the effectiveness of combining model strengths to improve detection accuracy.\nA key challenge faced during the multilingual task was the data imbalance between wellrepresented languages like English and Chinese and underrepresented ones such as Urdu, Arabic, and Russian. This disparity hindered the model's accuracy for underrepresented languages. To address this, we scale down samples from overrepresented languages to balance the dataset. This adjustment improved performance across languages, validating the effectiveness of our approach.\nDespite these successes, challenges persist. Detecting AI-generated content in multilingual contexts remains complex and demands further refinement in model architectures and data processing techniques. Future work could explore advanced methods for mitigating data imbalance, such as data augmentation or active learning, to enhance the model's generalization ability across diverse languages. Additionally, more sophisticated ensemble strategies or domain-specific models could improve detection accuracy.\nIn conclusion, this study demonstrates the effectiveness of an ensemble approach for detecting AI-generated content across English and multilingual datasets. Addressing data imbalance and using inverse perplexity weighting improved performance, though ongoing challenges highlight the need for continuous innovation in AI detection systems."}]}