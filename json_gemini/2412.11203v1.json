{"title": "Task-Oriented Dialog Systems for the Senegalese Wolof Language", "authors": ["Derguene Mbaye", "Moussa Diallo"], "abstract": "In recent years, we are seeing considerable interest in conversational agents with the rise of large language models (LLMs). Although they offer considerable advantages, LLMs also present significant risks, such as hallucination, which hinder their widespread deployment in industry. Moreover, low-resource languages such as African ones are still underrepresented in these systems limiting their performance in these languages. In this paper, we illustrate a more classical approach based on modular architectures of Task-oriented Dialog Systems (TODS) offering better control over outputs. We propose a chatbot generation engine based on the Rasa framework and a robust methodology for projecting annotations onto the Wolof language using an in-house machine translation system. After evaluating a generated chatbot trained on the Amazon Massive dataset, our Wolof Intent Classifier performs similarly to the one obtained for French, which is a resource-rich language. We also show that this approach is extensible to other low-resource languages, thanks to the intent classifier's language-agnostic pipeline, simplifying the design of chatbots in these languages.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) has experienced a tremendous growth in recent years, mainly due to the rapid development of Deep Learning. The latter has allowed to achieve super-human skills on tasks such as Image Classification. This has been mainly made possible thanks to the development of large datasets and the considerable increase in computing power and their accessibility. NLP has thus leveraged these advances and has had its \"ImageNet moment\"\u00b9 with the emergence of huge corpora as well as the possibility of leveraging pre-trained models to apply them to downstream tasks through Transfer Learning (Ruder et al., 2019).\nHowever, the availability of such corpora only concerns a small group of languages such as English, Chinese or French which are referred to as \"Resource Rich\". The majority of the approximately remaining 7,000 languages (Eberhard et al., 2019), and particularly African ones, fall into the \"Low Resource\" category and struggle to have sufficient corpora and NLP tools (Hedderich et al., 2021). They are thus left behind in most of the AI revolutions, such as LLMs, which reach the state of the art in many NLP tasks but struggle to reproduce equivalent performance in African languages. A human-translated benchmark dataset for 16 typologically diverse low-resource African languages (including Wolof) has been presented by (Adelani et al., 2024) covering three tasks: natural language inference, mathematical reasoning, and multi-choice knowledge-based QA. They evaluated zero-shot, few-shot, and translate-test settings (where test sets are translated into English) across 10 open and four proprietary LLMs and revealed a significant performance gap between high-resource languages (English and French) and African ones. A lot of work has therefore been done to address this challenge and a great illustration is the emergence of initiatives such as the Masakhane\u00b2 community which brings together thousands of researchers, practitioners, linguists and enthusiasts to produce datasets and models for a wide range of African languages (Nekoto et al., 2020). Other data collection projects have also included African languages, and especially Wolof, such as (Tiedemann, 2012; Strassel and Tracey, 2016; Goyal et al., 2022; Federmann et al., 2022).\nHowever, the data collection process is a very time-consuming task and can quickly become tedious. Additionally, for some languages, the text may not even exist in electronic form, or worse still, the text may be obtained from phonetic tran-"}, {"title": "Related Work", "content": "TODS for low-resource languages has been the subject of extensive research in the community and various approaches have been studied. A first experience of a Wolof chatbot was proposed in (Gauthier et al., 2022) with a POC of a voice assistant based on Rasa and designed with manually collected synthetic data. This limits the ability to collect large volumes of data, which is the reason why more efficient alternatives have been explored."}, {"title": "Word alignment approach", "content": "Training data translation (translate-train) into target languages is an approach that is increasingly being considered to enhance the performance of cross-lingual transfer. (Schuster et al., 2019) explored three different cross-lingual transfer methods: translation of training data with a bidirectional neural machine translation system (NMT) following the work carried out by (McCann et al., 2018), using cross-lingual pre-trained embeddings, and a method of using a multilingual machine translation encoder as contextual word representations. Although the latter two showed better performance in the presence of hundreds of training data, in cases where no data is available in the target language, translating the training data gives the best results. This translation approach has also been explored by (L\u00f3pez de Lacalle et al., 2020), which leveraged the Spanish subset of the dataset of (Schuster et al., 2019) to translate it into Basque using a Spanish-to-Basque Transformer-based NMT system (Vaswani"}, {"title": "Marker-based approach", "content": "To avoid the need for additional models for word alignment, other simpler alternatives are considered, such as mark-then-translate as explored in (Lewis et al., 2020). This approach consists of inserting specific markers around the corresponding chunks in the source sentence (e.g., [marker] and [/marker]), then translating into the target language to maintain these markers during the translation process. An optimized version of the mark-then-translate approach, has been proposed in (Chen et al., 2023) consistently outperforming the alignment-based approach. Authors used language-agnostic square bracket markers, combined with an efficient fine-tuning strategy of the NLLB (No Language Left Behind) model (Team et al., 2022) to encourage the multilingual machine translation system to better preserve the special markers during translation. They also showed that the alignment-based methods are more error-prone when projecting span-level annotations, compared to the marker-based approaches. However, inserting markers into the source sentence tends to compromise the translation quality (Chen et al., 2023). A different approach called Translate-and-Fill (TaF) is explored in (Nicosia et al., 2021), as opposed to Translate-Align-Project (TAP), which uses alignment and projection modules. TaF consists of a sequence-to-sequence filler model that constructs a full parse conditioned on an utterance and a view of the same parse. This approach requires however two seq2seq models trained differently: one is the usual semantic parser and the other is what they call the filler. Researchers in (Xu et al., 2020) introduced a single end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer."}, {"title": "Data", "content": "Belonging to the Atlantic group of the Niger-Congo language family, Wolof is an African language mainly spoken in Senegal (the majority of the population) but also in Gambia and some parts of Mauritania. It is a non-tonal agglutinative language whose alphabet is quite close to the French one: we can find all the letters of its alphabet except H, V and Z (Mbaye et al., 2024). As with many African languages, Wolof is a low-resource language, as opposed to high-resource languages like English and Chinese. Although there is no consensus on a suitable definition of the low-resource concept, several researchers have explored definitions from different angles even beyond the lack of data and NLP tools. Thus, this concept has been defined in (Besacier et al., 2014) as a language that lacks a unique writing system, has a low Internet presence, and lacks linguistic expertise, among other things. Wolof linguistics has, however, benefited from a lot of research but the lack of a unique writing system makes the design of NLP tools particularly challenging in this language. There is a writing system based on the Latin script and another one based on the Arabic script called Wolofal or Ajami (Mbaye et al., 2024). The latter is however very little used and is generally located within usage related to the Islamic religion which is the predominant religion in Senegal. The Latin script, on the other hand, is the most widespread, and is the one we'll be considering in this work. However, due to a lack of language standardization, two forms of writing illustrated in (Mbaye and Diallo, 2023) are noted with the Latin script:\nWe'll therefore consider the official form in this article."}, {"title": "The Wolof Language", "content": "Belonging to the Atlantic group of the Niger-Congo language family, Wolof is an African language mainly spoken in Senegal (the majority of the population) but also in Gambia and some parts of Mauritania. It is a non-tonal agglutinative language whose alphabet is quite close to the French one: we can find all the letters of its alphabet except H, V and Z (Mbaye et al., 2024). As with many African languages, Wolof is a low-resource language, as opposed to high-resource languages like English and Chinese. Although there is no consensus on a suitable definition of the low-resource concept, several researchers have explored definitions from different angles even beyond the lack of data and NLP tools. Thus, this concept has been defined in (Besacier et al., 2014) as a language that lacks a unique writing system, has a low Internet presence, and lacks linguistic expertise, among other things. Wolof linguistics has, however, benefited from a lot of research but the lack of a unique writing system makes the design of NLP tools particularly challenging in this language. There is a writing system based on the Latin script and another one based on the Arabic script called Wolofal or Ajami (Mbaye et al., 2024). The latter is however very little used and is generally located within usage related to the Islamic religion which is the predominant religion in Senegal. The Latin script, on the other hand, is the most widespread, and is the one we'll be considering in this work. However, due to a lack of language standardization, two forms of writing illustrated in (Mbaye and Diallo, 2023) are noted with the Latin script:"}, {"title": "Dataset", "content": "There are several multilingual datasets in Task-oriented dialog systems, typically for intent classification and slot filling. Some datasets specialize in a particular domain, such as MultiATIS++ (Xu et al.,"}, {"title": "Annotation Projection", "content": "Annotation projection involves transferring the labels present in a sentence in a source language to the translated sentence in the target language. A machine translation system is thus at the heart of the process, and the resource-limited nature of the Wolof language adds a further layer of complexity. A French Wolof machine translation system based on LSTMs was presented in (Mbaye et al., 2024), and another one based on Vanilla Transformers in (Dione et al., 2022). But pre-trained multilingual translation models have shown the most interesting performances in supporting low-resource languages. They allow information sharing between similar languages that greatly allows to improve translation on the language pairs as studied in (Arivazhagan et al., 2019). A lot of work has been done in this direction and a wide range of multilingual translation models have therefore been developed. Researchers in (Adelani et al., 2022) compared a set of multilingual models for machine translation in over twenty African languages, including Wolof, and the M2M100 (Fan et al., 2020) model showed the best performance. A distilled version of this model has been proposed in (Mo-"}, {"title": "Experiments", "content": "Instead of directly building an intent classifier and slot filling, we propose a system for generating chatbots on the fly (chatbot engine), based on the Rasa framework (Bocklisch et al., 2017). Rasa is an open-source framework designed for creating conversational AI chatbots. It offers tools and libraries for building and deploying AI-powered, text and"}, {"title": "Results", "content": "The results of the intent classification are shown in Table 2. We show the macro F1 scores on the source and synthetic data and we can observe that we get equivalent scores on both sides. This shows that the model succeeds in discriminating the intents in the synthetic dataset, suggesting a sufficiently qualitative translation. However, the model seems less confident when it comes to Wolof predictions, as shown in Fig.4 and Fig.5.\nConfidence scores for good predictions vary more in the synthetic dataset than in the source dataset. This shows that, after translation, the model is more likely to confuse certain intentions, as illustrated by Fig.6. The calendar_query and reccomandation_events intents, for example, show confusion even though they are quite distinct. This can be attributed to the translation system, which may have had difficulty in producing accurate translations in some cases.\nTable 3 illustrates the performance of annotation projection on the two datasets French and Wolof expressed in micro and macro F1 score as well as in accuracy. We note a pronounced discrepancy between the two datasets, with predictions on the synthetic data lower than those on the source data. It is important to point out that some of the annotations in the French dataset are expressions (not words) and are translated separately from the base sentence and therefore from the original context."}, {"title": "Conclusion and Perspectives", "content": "In this paper, we illustrated an efficient approach to building dialogue systems in a low-resource language. We have shown how to leverage machine translation systems to create synthetic datasets using our annotation projection method. Our experiments showed that training on these synthetic datasets in Wolof gave competitive results compared with the French source data, which is a resource-rich language. However, this approach is strongly affected by the quality of the translation system, and translating annotations out of their original contexts can reduce the final quality of the"}]}