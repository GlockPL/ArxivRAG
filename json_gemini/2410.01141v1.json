{"title": "Evaluating Deduplication Techniques for Economic Research\nPaper Titles with a Focus on Semantic Similarity using NLP\nand LLMs", "authors": ["Doohee You", "Karim Lasri", "Samuel P. Fraiberger"], "abstract": "This study investigates efficient deduplication techniques for a large NLP dataset of economic\nresearch paper titles. We explore various pairing methods alongside established distance measures\n(Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings\nsuggest a potentially low prevalence of duplicates based on the observed semantic similarity across\ndifferent methods. Further exploration with a human-annotated ground truth set is completed for\na more conclusive assessment. The result supports findings from the NLP, LLM based distance\nmetrics.", "sections": [{"title": "1 Introduction", "content": "The World Bank is exploring the use of causal AI models for analyzing the impact of economic in-\nterventions. These models rely heavily on high-quality, accurate data, often derived from academic\npublications. However, large-scale NLP datasets can be susceptible to the presence of duplicate en-\ntries arising from: 1) Title variations: Authors may publish the same research with slightly different\ntitles due to minor wording changes or phrasing variations. 2) Accidental duplicates: Duplicate sub-\nmissions across academic databases can occur due to human error or technical glitches Christen et\nal., 2012[Chr12]. 3)Near duplicates: Some research may be published with substantial content over-\nlap but slightly different titles, requiring more sophisticated techniques for identification Zhu et al.,\n2014[ZG14], Liu et al., 2023 [LDL+23], and Elmagarmid et al., 2007 [EW07]. Duplicate entries can\nnegatively impact the functionality and accuracy of AI models. This study investigates various dedu-\nplication techniques using only titles as a starting point for building a clean and accurate dataset of\nRCT studies."}, {"title": "2 Background", "content": "Researchers have developed various techniques for deduplication in large NLP datasets, broadly cat-\negorized as follows: 1) String-based methods: These techniques rely on string similarity measures\nlike Levenshtein distance Levenshtein, 1966[Lev66] or Jaccard similarity to identify entries with high\ntextual overlap Gomaa et al., 2013[GF13]. 2) Hashing methods: Hashing algorithms map similar tex-\ntual data points to the same hash value, allowing for efficient duplicate detection. Locality-Sensitive\nHashing (LSH) is a popular choice for large datasets Datar et al., 2004[DIIP04] 3) Embedding-based\nmethods: These methods represent text data as numerical vectors using techniques like word embed-\ndings or document embeddings. Similar entries will have vectors close in distance within the embedding\nspace Mikolov et al., 2013[MCCD13], Di Liello et al., 2022 [DLGSM22]."}, {"title": "3 Method", "content": "We evaluated various pairing techniques for comparing titles:\n\u2022 Complete Pairing Formula\nThis method calculates the similarity between all possible pairs of titles within the dataset. The\ncomputational complexity is given by $\\frac{n(n-1)}{2}$, where n is the number of titles Hern\u00e1ndez-Caro et\nal., 2015 [HCVM15].\n\u2022 Selective Pairing Across Sources\nThis method focuses on comparing titles across different sources (e.g., JSTOR vs. ELSEVIER),\nexcluding comparisons within the same source. This approach significantly reduces the number\nof comparisons.\n\u2022 Pairing on Title Length\nThis method considers titles with similar word counts to be more likely duplicates. We define a\nthreshold (\u03b4 = 5) for the difference in word count between two titles Sultan et al., 2019[SJG19].\nTitles with word count difference less than or equal to d are considered for further comparison.\nMathematically, for two titles T\u2081 and T2 with word counts w\u2081 and w\u2082 respectively, this condition\ncan be expressed as:\n$|w_1 - w_2| \\leq \\delta$ \n\u2022 Pairing on Similar Length of Titles\nThis method combines source information, word count ranges, and filtering based on the most\ncommon word length (\u03bc) in the dataset. Titles from different sources with word counts within\na predefined range (\u03bb) around u are selected for comparison Sultan et al., 2019[SJG19]. This\ncondition mathematically as:\n$|T_1.word\\_count \u2013 \\mu| < \\lambda \\text{ AND } |T_2.word\\_count \u2013 \\mu| \\leq \\lambda$\n\u2022 Pairing on Short Titles\nThis approach focuses on titles with a length less than or equal to a predefined threshold\n(T = 3), assuming simpler language and a higher chance of containing duplicates Sultan et\nal., 2019[SJG19]. Mathematically, this can be expressed as:\n$T_1.word\\_count < \\tau \\text{ AND } T_2.word\\_count < \\tau$"}, {"title": "3.1 Distance Measures", "content": "Following title pre-processing and language detection, we utilized three distance measures:\n\u2022 Levenshtein Distance (LD)\nThis metric calculates the minimum number of single-character edits (insertions, deletions, sub-\nstitutions) needed to transform one string (title) into another Levenshtein, 1966 [Lev66].\nFormally, LD(T1, T2) represents the Levenshtein distance between titles T\u2081 and T2.\n\u2022 Cosine Similarity (CS)\nThis metric measures the similarity between two text vectors by calculating the cosine of the\nangle between them Manning et al., 2008[MRS08]. A cosine similarity of 1 indicates identical\nvectors, while 0 represents orthogonal (completely dissimilar) vectors.\nFor two title vectors, T\u2081 and T2, the cosine similarity is calculated as:\n$CS(T_1, T_2) = \\frac{T_1 \\cdot T_2}{||T_1|| ||T_2||}$"}, {"title": "4 Results", "content": "The relationship between three distance measurement methods (Levenshtein distance, cosine similarity,\nand SBERT model) is explored using 2,000 title pairs generated from a dataset of 11 million sources.\nThe graph in Figure 1 visualizes how similar the distance scores are between these methods for the\nsame title pairs.\nThe data points on the graph can exhibit different clustering patterns, revealing insights into the\neffectiveness of each method. Tight clustering suggests a strong correlation between the distance scores\nfrom all three methods, implying they often provide similar results in identifying similar or dissimilar\ntitles. A scattered pattern, on the other hand, might indicate that the distance scores from different\nmethods vary significantly for the same title pairs. This could suggest that each method has its own\nstrengths and weaknesses in capturing different aspects of similarity. Additionally, method-specific\nclusters might emerge, providing further insights into their unique behavior. For instance, a cluster in\nthe top left corner for Levenshtein distance could indicate many title pairs with very low edit distances,\nsuggesting a high number of near-duplicates.\nThe ideal scenario for deduplication involves data points clustering in the bottom left corner of the\nplot. This would indicate that titles with low Levenshtein distance (very similar strings) also have\nlow cosine similarity (semantically similar) and low BERT similarity scores (similar according to the\nBERT model). However, the graph in Figure 1 suggests a low prevalence of close matches using all\nthree distance measures.\nThere is no strong cluster of points in the bottom left corner, indicating a lack of titles with very\nsimilar strings, low cosine similarity, and low BERT similarity scores. This suggests a low number of\ntrue duplicates within the dataset. Even titles with similar strings (low Levenshtein distance) might\nhave varying semantics and score higher on cosine similarity or BERT similarity score. The scattered\ndistribution on the Z-axis (BERT similarity) highlights this point. Even for titles with low Levenshtein\ndistance and high cosine similarity (potentially similar wording and direction), the BERT similarity\nscores show variation. This suggests the BERT model might be capturing semantic differences between\nthese titles that the other two metrics miss.\nSeveral factors could contribute to this observation. The chosen BERT model might not be ideal\nfor capturing the semantic nuances of economic research paper titles. Additionally, the thresholds\nused to define similar BERT similarity scores might need adjustment. Titles that appear similar based\non string comparison and directional similarity might have subtle semantic differences. Finally, the\nanalysis is based on a small sample size (between 1,000,000 to 2,000 samples out of 11 million titles)\nand might not represent the entire dataset. A larger sample might reveal tighter clustering if more\nduplicates are present."}, {"title": "5 Conclusion", "content": "This study investigated efficient deduplication techniques for a large NLP dataset of economic research\npaper titles, aiming to build a clean and accurate dataset for training causal AI models in economics."}]}