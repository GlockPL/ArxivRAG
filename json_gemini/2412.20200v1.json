{"title": "Federated Unlearning with Gradient Descent and Conflict Mitigation", "authors": ["Zibin Pan", "Zhichao Wang", "Chi Li", "Kaiyan Zheng", "Boqi Wang", "Xiaoying Tang", "Junhua Zhao"], "abstract": "Federated Learning (FL) has received much attention in recent years. However, although clients are not required to share their data in FL, the global model itself can implicitly remember clients' local data. Therefore, it's necessary to effectively remove the target client's data from the FL global model to ease the risk of privacy leakage and implement \"the right to be forgotten\". Federated Unlearning (FU) has been considered a promising way to remove data without full retraining. But the model utility easily suffers significant reduction during unlearning due to the gradient conflicts. Furthermore, when conducting the post-training to recover the model utility, the model is prone to move back and revert what has already been unlearned. To address these issues, we propose Federated Unlearning with Orthogonal Steepest Descent (FedOSD). We first design an unlearning Cross-Entropy loss to overcome the convergence issue of the gradient ascent. A steepest descent direction for unlearning is then calculated in the condition of being non-conflicting with other clients' gradients and closest to the target client's gradient. This benefits to efficiently unlearn and mitigate the model utility reduction. After unlearning, we recover the model utility by maintaining the achievement of unlearning. Finally, extensive experiments in several FL scenarios verify that FedOSD outperforms the SOTA FU algorithms in terms of unlearning and model utility.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has increasingly gained popularity as a machine learning paradigm in recent years (McMahan et al. 2017). It allows clients to cooperatively train a global model without sharing their local data, which helps address data island and privacy issues (Yu et al. 2022). But previous studies demonstrate that clients' local training data is inherently embedded in the parameter distribution of the models trained on it (De and Pedersen 2021; Zhao et al. 2023). Therefore, in light of privacy, security, and legislation issues, it's necessary to remove clients' training data from the trained model (Zhang et al. 2023), especially when clients opt to withdraw from FL. This is known as the right to be forgotten (RTBF) (Liu et al. 2021), which is enacted by privacy regulations such as the General Data Protection Regulation (GDPR) (Voigt and Von Bussche 2017) and the California Consumer Privacy Act (CCPA) (Harding et al. 2019).\nA naive way to achieve this goal is to retrain the FL model. But it brings large computation and communication costs (Liu et al. 2023). In contrast, unlearning is a more efficient way, which has been well studied in centralized machine learning (Bourtoule et al. 2021). Inspired by it, Federated Unlearning (FU) has emerged, aiming to remove data from a trained FL model while trying to maintain model utility.\nIn this context, numerous FU techniques have been proposed. Federaser (Liu et al. 2021) leverages the norms of historical local updates in the previous FL training to accelerate retraining. FedKdu (Wu, Zhu, and Mitra 2022) and FedRecovery (Zhang et al. 2023) utilize the historical gradients to calibrate the model to erase the training data of the target client (i.e., the client that requests for unlearning). However, these methods require clients to continuously record historical information during FL training (Yang and Zhao 2023). Moreover, (Zhao et al. 2023) propose MoDe to unlearn the target client's data by momentum degradation, but it requires simultaneously retraining a model for updating the unlearning model, which brings additional communication costs.\nAmong prior studies, Gradient Ascent (GA) is considered a viable and efficient method for FU (Liu et al. 2023), which formulates unlearning as the inverse process of learning and takes the inverse of the loss function to reduce the model performance on the target client. It can effectively achieve the unlearning goal in few communication rounds while not bringing extra storage costs (Halimi et al. 2022). However, we observe that there exist the following three primary challenges when performing GA in FU.\nChallenge 1: Gradient explosion. Gradient explosion is a significant challenge for GA-based federated unlearning, necessitating a substantial reliance on experimental hyper-parameter tuning. This is because the loss function generally has no upper bound (see Fig. 3(a)). Consequently, executing GA to unlearn results in gradient explosion and cannot converge. We delve further into this in Section 3.1. To this end, (Halimi et al. 2022) project model parameters to an L2-norm ball of radius \u03b4. But it requires experimentally tuning \u03b4.\nChallenge 2: Model utility degradation. Directly applying GA to unlearn would inevitably destroy the model utility (Yang and Zhao 2023), even leading to catastrophic forgetting (Liu et al. 2023). Specifically, the model performance on remaining clients (i.e., those that do not require unlearning) would decrease heavily. One direct cause is the gradient conflict (Pan et al. 2023), where the model update direction for unlearning a client conflicts with those of the remaining clients, directly leading to a reduction in model utility. Fig. 1 illustrates an example in which client 3 requests unlearning, but the model update direction conflicts with the gradients of client 1 and client 2. Consequently, the updated model would exhibit diminished performance on client 1 and 2.\nChallenge 3: Model reverting issue in post-training. After unlearning, post-training is often conducted, where the target client leaves and the remaining clients continually train the FL global model cooperatively to recover the model utility that was reduced in the previous unlearning (Halimi et al. 2022; Wu, Zhu, and Mitra 2022). However, we observe that during this stage, the model tends to revert to its original state, resulting in the recovery of previously forgotten information and thus losing the achievement of unlearning. This issue is further explored in Section 3.3.\nTo handle the aforementioned challenges, we propose the Federated Unlearning with Orthogonal Steepest Descent algorithm (FedOSD). Specifically, to handle the gradient explosion inherent in GA, we modify the Cross-Entropy loss to an unlearning version and employ the gradient descent, rather than GA, to achieve the unlearning goal. Subsequently, an orthogonal steepest descent direction that avoids conflicts with retained clients' gradients is calculated to better unlearn the target client while mitigating the model utility reduction. In post-training, we introduce a gradient projection strategy to prevent the model from reverting to its original state, thereby enabling the recovery of model utility without compromising the unlearning achievement.\nOur contributions are summarized as follows:\n1. We introduce an Unlearning Cross-Entropy loss that can overcome the convergence issue of Gradient Ascent."}, {"title": "2 Background & Related Work", "content": "2.1 Federated Learning (FL)\nThe traditional FL trains a global model $w$ cooperatively by $m$ clients, which aims to minimize the weighted average of their local objectives (Li et al. 2020): $min_w \\sum_{i=1}^m p_iL_i(w)$, where $p_i \\geq 0$, $\\sum_{i=1}^m p_i = 1$. $L_i$ is the local objective of client i, which is usually defined by the empirical risks over the local training data with $N_i$ samples: $L_i(w_t) = \\sum_{j=1}^{N_i} L_{i,j}(w_t)$. $L_{i,j}$ is the loss on sample j, which is obtained by a specific loss function such as Cross-Entropy (CE) loss:\n$L_{CE} = -\\sum_{c=1}^C Y_{o,c} \\cdot log(p_{o,c})$, (1)\nwhere $C$ denotes the number of classes. $Y_{o,c}$ is the binary indicator (0 or 1) if class label c is the correct classification for observation o, i.e., the element of the one-hot encoding of sample j's label. $p_{o,c}$ represents the predicted probability observation o that is of class c, which is the $c^{th}$ element of the softmax result of the model output.\n2.2 Federated Unlearning\nFederated Unlearning (FU) aims to erase the target training data learned by the FL global model, while mitigating the negative impact on the model performance (e.g., accuracy or local objective). Recognized as a promising way to protect 'the Right to be Forgotten' of clients, FU can also counteract the impact of data poisoning attacks to enhance the security (Yang and Zhao 2023; Liu et al. 2023).\nFU has garnered increasing interest in recent years. (1) Some previous studies have leveraged the historical information of FL training to ease the target client's training data, such as FedEraser (Liu et al. 2021), FedKdu (Wu, Zhu, and Mitra 2022), FedRecovery (Zhang et al. 2023), etc. (2) Besides, (Zhao et al. 2023) adopt momentum degradation to FU. (3) (Su and Li 2023) use clustering and (4) (Ye et al. 2024) employ distillation to unlearn. (5) A significant approach related to our work is Gradient Ascent, which utilizes the target client's gradients for unlearning (Halimi et al. 2022; Wu et al. 2022).\nBased on the types of client data that need to be forgotten, Federated Unlearning can be categorized into sample unlearning and client unlearning (Liu et al. 2023). We focus on client unlearning in this paper for two reasons. First, we can make a fair comparison with previous record-based FU methods such as FedEraser and FedRecovery. Since they rely on pre-recording information like model gradients on the target data that needed to be unlearned, they are not suitable for sample unlearning. Since in the sample unlearning, clients only request to unlearn partial training data. However, no one knows which data will be requested to unlearn during FL training, and thus preparing these records in advance for later unlearning is not feasible in practice.\nFurthermore, for other FU algorithms that do not necessitate using historical training records, we can technically treat unlearning samples as belonging to a virtual client. Hence, the sample unlearning can be transferred to the client unlearning. For example, when a client requests to unlearn partial data $D_u$, we can form a new virtual client $u$ that owns $D_u$ and unlearn it.\nThe formulation of unlearning the target client u from the trained global model can be defined by:\n$max_w L_u(w)$, (2)\nwhere $L_u (w)$ represents the local objective of client u in FL.\nFederated Unlearning with Gradient Ascent. Gradient Ascent (GA) (Wu et al. 2022; Liu et al. 2023) is a proactive and efficient approach for solving Problem (2). At each communication round t, it strives to maximize the empirical loss of the target client u by updating the model according to $w_{t+1} = w_t + \\eta_t\\nabla L_u(w_t)$ with the step size (learning rate) $\\eta_t$ (Halimi et al. 2022). However, (Wu et al. 2022) suggest that this approach would fail because of destroying the global model performance for the remaining clients. To this end, they propose EWCSGA, which incorporates a regularization term to the cross entropy loss to mitigate the negative impact on the model utility. Besides, another approach computes an update direction $\\Delta \\omega$ orthogonal to the subspace of the model layer inputs x, i.e., $\\Delta \\omega x = 0$ (Saha, Garg, and Roy 2021; Li et al. 2023). This kind of method works well in protecting the model utility in centralized learning, however, it is not suitable for FL due to potential privacy leakage from uploading x. SFU (Li et al. 2023) attempts to mitigate this issue by multiplying x with a factor A before uploading, but attackers can easily recover the original data. Additionally, it would suffer model utility reduction during unlearning, because the derived model update direction is only orthogonal to a subset of the input data from the remaining clients, which cannot ensure the preservation of model utility. We validate these points through the experimental results presented in Table 2.\nOur method draws from the idea of GA to achieve the goal of unlearning. Differently, we modify the CE loss function to an unlearning version to overcome the gradient explosion issue, and compute the steepest descent direction that not only aligns closely with the target client's gradient but also avoids conflicts with the retained clients' gradients. This approach enables more effective unlearning while mitigating the model utility degradation."}, {"title": "3 The Proposed Approach", "content": "Our proposed FedOSD aims to effectively remove the target client's data from the FL global model while mitigating the model performance reduction across remaining clients. Fig. 2 demonstrates the framework of FedOSD, which includes two stages: unlearning (Fig. 2(b)) and post-training (Fig. 2(c)). $w^0$ is the global model previously trained through Federated Learning across $m$ clients (Fig. 2(a)). When client u requests for unlearning, it utilizes the proposed Unlearning Cross-Entropy loss to conduct the local training. After collecting local gradients $g_i$, the server calculates a direction $d_t$ that is closest to client u's gradient while orthogonal to remaining clients' gradients, and then updates the model by $w_{t+1} = w_t + \\eta_t d_t$. In the post-training stage, a gradient projection strategy is performed to prevent the model from reverting to $w^0$. Detailed steps of FedOSD can be seen in Algorithm 1. In Appendix.A.2, we prove the convergence of FedOSD in the unlearning and post-training stages.\n3.1 Unlearning Cross-Entropy Loss\nWe first take a brief review of how Gradient Ascent can drive the model to unlearn. As shown in Fig. 3(a), by updating the global model with $w_{t+1} = w_t + \\eta \\nabla L_u(w_t)$, the local loss increases and $p_{o,c}$ approaches 0, thus degrading the model's prediction accuracy on the target client's data and achieving unlearning. However, the CE Loss (Eq.(1)) has no upper bound. As seen in Fig. 3(a), when $p_{o,c}$ is getting quite close to 0, $\\partial L_{CE}/\\partial p_{o,c}$ would suffer the explosion and thus the local gradient of the target unlearning client explodes. That's why directly applying GA to unlearn would make the model similar to a random model (Halimi et al. 2022). One conventional solution is to project the model back to an $L_2$-norm ball of radius $\\delta$ (Halimi et al. 2022). But it brings a hyper-parameter that requires experimentally tuning, and a fixed $\\delta$ cannot guarantee the convergence.\nTo address this issue, we modify CE loss to an unlearning version named Unlearning Cross-Entropy (UCE) loss:\n$L_{UCE} = -\\sum_{c=1}^C Y_{o,c} \\cdot log(1 - p_{o,c}/2)$. (3)\nBy minimizing Eq.(3), we can drive the predicted probability $p_{o,c}$ to be closer to 0 (as seen in Fig. 3(b)), thereby diminishing the prediction ability of the model on the target client's data to unlearn it. Note that before unlearning, the model $w$ often performs well on clients, where $p_{o,c}$ is close to 1 and the model update step for the global model is already quite small. Hence, the constant \"2\" in Eq.(3) is set to ensure that the gradient norm of the target client does not exceed those of the remaining clients. This can prevent the unlearning process from being unstable or even directly damaging the model utility. We verify this in Appendix.B.2.\nHence, when client u requests for unlearning, it no longer applies GA on the CE loss. Instead, it switches to utilize UCE loss and performs gradient descent to train the model. Since UCE loss has the lower bound 0, it can achieve the goal of unlearning client u's data without bringing issues of gradient explosion and convergence difficulties. Denote $L_{\\bar{u}}$ as the local objective of the target client u by using UCE loss, then the unlearning formulation (2) is transferred to:\n$min_w L_{\\bar{u}}(w)$. (4)\n3.2 Orthogonal Steepest Descent Direction\nIn FedOSD, we solve Problem (4) to unlearn the target client u by iterating $w_{t+1} = w_t + \\eta_t d_t$, where $d_t$ is an orthogonal steepest descent direction at $t^{th}$ round. In this section, we discuss how to obtain such an update direction and analyze how it can accelerate unlearning while mitigating the negative impact on the model utility. We start by introducing the gradient conflict, which is a direct cause of model performance degradation on FL clients (Wang et al. 2021).\nDefinition 1 (Gradient Conflict): The gradients of client $i$ and $j$ are in conflict with each other iff $g_i^T g_j < 0$.\nIn each communication round $t$, denote $g_i^t, i \\neq u$ as the local gradient of remaining clients, and $g_u^t$ as the gradient of the target client u for unlearning. If we directly adopt $-g_u^t$ as the direction to update the model to unlearn client u, i.e., $w_{t+1} = w_t - \\eta_t g_u^t$, the model performance on the remaining clients would easily suffer reduction because $g_u^t$ would conflict with some $g_i^t$. The experimental results of Table 3 corroborate the presence and the impact of such gradient conflicts in FU.\nHence, mitigating gradient conflicts can help alleviate decreases in the model utility. One ideal solution would be identifying a common descent direction $d_t$ that satisfies $d_t \\cdot g_u^t < 0$ and $d_t \\cdot g_i^t < 0$. However, such a strategy could lead to the model becoming prematurely trapped in a local Pareto optimum, which remains far from the optimum of Problem (4). We verify it in the ablation experiments (Section 4.3).\nTo this end, we mitigate the gradient conflict by computing a model update direction $d_t$ orthogonal to the gradient of the remaining clients, i.e., $d_t^T \\cdot g_i^t = 0, \\forall i \\neq u$. Although $d_t$ is not a common descent direction, it helps slow down the performance reduction of the model on the remaining clients. However, in FL, the number of remaining clients (i.e., $m-1$) is significantly smaller than $D$ (the dimension of model parameters), implying $rank(\\forall g_i^t, i \\neq u) \\leq m -1 << D$. Consequently, there are numerous orthogonal vectors $d$ that satisfy $d_t^T \\cdot g_i^t = 0$. Therefore, if the obtained direction differs significantly from $-g_u^t$, it would impede the unlearning process and potentially exacerbate the degradation of model utility. We verify it in the ablation study in Section 4.3.\nDenote $G \\in R^{(m-1)\\times D}$ as a matrix where each row represents a gradient of a remaining client, the key idea is to find a $d_t$ that satisfies $Gd_t = 0$ while being closest to $-g_u^t$ to accelerate unlearning, i.e., $d_t = arg min_{d_t} cos(g_u^t, d_t)$. To maintain the direction's norm, we fix $||d_t||=||g_u^t||$, then the problem is equivalent to:\n$min_{d_t \\in R^D} g_u^t d_t$,\ns.t. $G d_t = 0$,\n$||d_t|| = ||g_u^t||, (5)\nwhich is a linear optimization problem and the solution is:\n$d_t = \\frac{1}{2||g_u^t||^2 \\mu}(G^T U \\Sigma^+ V^T G g_u^t - g_u^t)$, (6)\nwhere $\\mu$ is a related scalar that can make $||d_t||=||g_u^t||$, i.e., $\\mu = ||G^T U \\Sigma^+ V^T G g_u^t - g_u^t||/(2||g_u^t||^4)$. The matrices V, \u03a3, U are the singular value decomposition of $GG^T \\in R^{(m-1)\\times(m-1)}$, i.e., $GG^T = V \\Sigma U^T$, which are not time-consuming to obtain. $\\Sigma^+$ is the Moore-Penrose pseudoinverse of \u03a3, i.e., $\\Sigma^+ = diag(\\frac{1}{s_1},...,\\frac{1}{s_r},0,...,0)$, where $s_1, s_2, \\dots, s_r$ are the non-zero singular values of $GG^T$. The detailed proof of Eq.(6) is presented in Appendix.A.1, where we also report the actual computation time of FedOSD. The obtained $d_t$ is closest to $-g_u^t$ and satisfies $G d_t = 0$, so that it can accelerate unlearning and mitigate the model utility reduction.\n3.3 Gradient Projection in Post-training\nAfter unlearning, the target client u leaves the FL system, and the remaining clients undertake a few rounds of FL training to recover the model utility. This phase is referred to as the \"post-training\" stage (Halimi et al. 2022; Zhao et al. 2023). However, we observe that not only is the model performance across remaining clients recovered, but unexpectedly, the performance on the forgotten data of the target client u also improves. It looks like the model remembers what has been forgotten.\nOne possible case is that the data from the target client u share a similar distribution with the remaining clients' data. Hence, with the model utility being recovered, the model can generalize to client u's data, thereby enhancing the model performance on client u. In general, this issue does not require intervention, because it can even happen on a retrained model without the participation of the target client u.\nHowever, we observe that there is another case called model reverting that requires intervention. As seen in Fig. 4, with the model utility being reduced, the local loss of the remaining clients increased after unlearning. Besides, many previous FU algorithms do not significantly deviate the model from the original model $w^0$ during unlearning. Subsequently, when starting post-training, the local gradient $g_i^t$ does not conflict with $g_u^0$ (i.e., $g_i^t\\cdot g_u^0 > 0$), where $g_u^0$ is defined by $g_u^0 = \\nabla_{w_t}||w_t - w^0 ||^2$. Therefore, the model is driven back to the old local optimal region where $w^0$ also resides, so that the model directly recovers what has been forgotten. The experimental results of Table 1 and Fig. 5 substantiate this observation, showing a decreased distance between the model and $w^0$ during post-training.\nTo address this issue, when $g_i^t \\cdot g_u^0 > 0$, we project the local gradient $g_i^t$ to the normal plane of $g_u^0$:\n$g_i^t' = g_i^t - \\frac{g_i^t\\cdot g_u^0}{||g_u^0||^2} g_u^0$ (7)\nSubsequently, each remaining client uploads $g_i^t'$ instead of $g_i^t$ to the server for the aggregation, i.e., $\\bar{g}_t' = \\sum_i g_i^t'$. And the global model is updated by $w_{t+1} = w_t -\\eta \\bar{g}_t'$. Given that $g_i^t'\\cdot g_u^0 = 0, \\forall i, \\bar{g}_t'$ satisfies $\\bar{g}_t' \\cdot g_u^0 = 0$, ensuring that the updated model would not revert towards $w^0$. Thus, it addresses the reverting issue in the post-training stage. It's worth noting that the gradient projection method still works when the loss surface is complex. This is because it can always identify a direction that prevents the model from reverting to the original model, while guiding it towards other local optima."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nWe adopt the model test accuracy on the retained clients (denoted as R-Acc) to evaluate the model utility. To assess the effectiveness of unlearning, we follow (Halimi et al. 2022; Li et al. 2023; Zhao et al. 2023) to implant backdoor triggers into the model by poisoning the target client's training data and flipping the labels (more details can be seen in Appendix.B.1). As a result, the global model becomes vulnerable to the backdoor trigger. The accuracy of the model on these data measures the attack success rate (denoted as ASR), and the low ASR indicates the effective unlearning performance of the algorithm.\nBaselines and Hyper-parameters. We first consider the retraining from scratch (denoted as Retraining) and FedEraser (Liu et al. 2021), which is also a kind of retraining but leverages the norms of the local updates stored in the preceding FL training to accelerate retraining. We then encompass well-known FU algorithms including FedRecovery (Zhang et al. 2023), MoDe (Zhao et al. 2023), and the gradient-ascent-based FU methods: EWCSGA (Wu et al. 2022) and FUPGA (Halimi et al. 2022). We follow the settings of (Halimi et al. 2022; Zhang et al. 2023) that all clients utilize Stochastic Gradient Descent (SGD) on local datasets with local epoch E = 1. We set the batch size as 200 and the learning rate $\\eta \\in \\{0.005, 0.025, 0.001, 0.0005\\}$ decay of 0.999 per round, where the best performance of each method is chosen in comparison. Prior to unlearning, we run FedAvg (McMahan et al. 2017) for 2000 communication rounds to generate the original model $w^0$ for unlearning. The max unlearning round is 100, while the max total communication round (including unlearning and post-training) is 200. The target unlearning client u is randomly selected from ten clients.\nDatasets and Models. We follow (Zhao et al. 2023) to evaluate the algorithm performance on the public datasets MNIST (LeCun et al. 1998), FMNIST (Xiao, Rasul, and Vollgraf 2017), and CIFAR-10/100 (Krizhevsky and Hinton 2009), where the training/testing data have already been split. To evaluate the effectiveness of unlearning across varying heterogeneous local data distributions, we consider four scenarios to assign data for clients: (1) Pat-20: We follow (McMahan et al. 2017) to build a pathological non-IID scenario where each client owns the data of 20% classes. For example, in a dataset like MNIST with 10 classes, each client has two classes of the data. (2) Pat-50: It constructs a scenario where each client has 50% classes. (3) Pat-10: It's an extreme data-island scenario where each client has 10% of distinct classes. (4) IID: The data are randomly and equally separated among all clients. We utilize LeNet-5 (LeCun et al. 1998) for MNIST, Multilayer perception (MLP) (Popescu et al. 2009) for FMNIST, CNN (Halimi et al. 2022) with two convolution layers for CIFAR-10, and NFResNet-18 (Brock, De, and Smith 2021) for CIFAR-100.\n4.2 Evaluation of Unlearning and Model Utility\nWe first evaluate the ASR and R-Acc of the model at the end of both the unlearning stage and post-training stage on FMNIST and CIFAR-10. One of the ten clients is randomly selected as the target client requesting for unlearning.\nTable. 1 lists the comparison results. It can be seen that in the unlearning stage, the gradient-ascent-based FU algorithms such as EWCSGA and FUPGA achieve more complete unlearning in non-IID scenarios, evidenced by their ASR reaching 0, but they experience a more pronounced reduction in R-Acc. What's worse, on FMNIST, the models of EWCSGA and FUPGA after the unlearning stage are nearly equivalent to a randomly initialized model. This is because their gradient constraint mechanisms, which aim to handle the gradient explosion issue, rely on fixed hyper-parameters. Since the optimal hyper-parameters cannot be determined in advance, these methods inevitably become ineffective. Benefiting from the UCE loss and the orthogonal steepest descent update direction, the proposed FedOSD does not bring extra hyper-parameters and can successfully unlearn the target client data while suffering less utility reduction than others. Besides, since FedRecovery performs unlearning relies solely on the pre-stored historical FL training information, it cannot guarantee the unlearning effect in all scenarios.\nDuring the post-training stage, FedRecovery, EWCSGA, and FUPGA can recover the R-Acc to a level comparable to or exceeding that of the initial state. However, their models gravitate towards the initial $w^0$, leading to the models remembering what has been erased, and thus the ASR values rise significantly. In comparison, FedOSD can recover the model utility without suffering the model reverting issue. More experimental results on MNIST and CIFAR-100 are available in Appendix.B.2.\nWe also replicate SFU (Li et al. 2023) discussed in Section 2.2 and test its performance on FMNIST (see Table 2). For each remaining client, one batch of data samples is selected to compute the representation matrix. However, we find this process to be highly time-consuming due to the high dimensionality of the representation matrix, which complicates the computation of the SVD. The results depict that it cannot achieve the unlearning goal, suffering significant R-Acc reduction during the unlearning process, as well as the model reverting issue during post-training.\nFurthermore, we present the experimental results for different client numbers: $m = 10, m = 20$, and $m = 50$ in Table 4. These results verify the superior performance of FedOSD in terms of the unlearning effectiveness and the model utility in scenarios with more client participation.\nTo elucidate the negative impact of gradient conflicts on the model utility during unlearning, we report ASR, R-Acc, and the average number of retained clients experiencing gradient conflicts with the model update direction $d_t$ in Table. 3. The results demonstrate that mitigating the conflict between $d_t$ and the remaining clients' gradients can significantly alleviate reductions in the model utility.\nBesides, Fig. 6 depicts the results in Pat-10 to evaluate the effect of unlearning on the model utility when some classes of data are completely removed. Compared with FedOSD, the model utility reduction on previous FU methods is considerably unfair, where the R-acc values are even approaching 0. In contrast, FedOSD more effectively maintains the model's performance on the remaining clients.\nMoreover, we visualize the curves of ASR, R-Acc, and the distance between $w_t$ and $w^0$ during unlearning and post-training in Fig. 5. Notably, the unlearning stage of FedRecovery only comprises a single round, as it performs unlearning relying solely on the historical information of the previous FL training. The results demonstrate that FedOSD successfully achieves a zero ASR while maintaining the highest model utility during unlearning. The distance curve in the post-training stage verifies that the models of FedRecovery, EWCSGA, and FUPGA tend to revert towards $w^0$, evidenced by the decreasing distance, thereby leading to an increase in ASR, which suggests a recovery of previously unlearned information. In contrast, FedOSD prevents the model from moving back, thereby ensuring the recovery of model utility without suffering the model reverting issue during post-training."}, {"title": "4.3 Ablation Experiments", "content": "In Table. 5, we evaluate the performance of several variants of FedOSD (M1 to M5) to study the effect of each part.\nM1: Do not use the UCE loss. Instead, the target client utilizes Gradient Ascent on the CE loss to unlearn. The results demonstrate that GA would destroy the model utility.\nM2: Replace the orthogonal steepest descent direction $d_t$ to $-g_u^t$ for updating the unlearning model, which would conflict with retained clients' gradients. As a result, the model utility suffers more reduction than FedOSD.\nM3: During unlearning, using Multiple Gradient Descent algorithm (Fliege and Svaiter 2000; Pan et al. 2024) to obtain a common descent direction $d_t$ that satisfies $d_t\\cdot g_i^t < 0, \\forall i \\neq u$, which can both reduce the UCE loss of the target client and the CE loss of remaining clients in unlearning. The results depict that while this strategy does not compromise model utility, it fails to achieve the unlearning goal, verifying the analysis in Section 3.2.\nM4: Randomly select a solution $d_t$ from the solutions to $G\\cdot d_t = 0$ that also satisfies $d_t\\cdot g_i^t < 0$ to update the model for unlearning. Since the obtained $d_t$ would deviate a lot from $-g_u^t$, the result of ASR is higher than that of FedOSD. If we tune a larger learning rate to enhance the unlearning performance, it would further harm the model utility.\nM5: Remove the gradient projection strategy in the post-training stage. It results in the model reverting issue, with a significant increase in ASR, verifying it's necessary to prevent the model from moving back to $w^0$ during post-training."}, {"title": "5 Conclusion and Future Work", "content": "In this work, we identify the convergence issue of Gradient Ascent and demonstrate the necessity of mitigating the gradient conflict in Federated Unlearning. Moreover, we highlight the issue of model reverting during post-training, which adversely affects the unlearning performance. To address these issues, we propose FedOSD, which modifies the Cross-Entropy loss to an unlearning version and achieves an orthogonal steepest descent model direction for unlearning. Extensive experiments verify that FedOSD outperforms SOTA FU methods in terms of the unlearning effect and mitigating the model utility reduction. A number of interesting topics warrant future exploration, including the design of the unlearning version of other loss functions such as MSE loss, and further enhancing fairness and privacy protection in FU."}, {"title": "A Appendix", "content": "A Theoretical Analysis and Proof\nIn Section A.1"}]}