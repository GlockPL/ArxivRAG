{"title": "Can We Estimate Purchase Intention Based on Zero-shot Speech Emotion Recognition?", "authors": ["Ryotaro Nagase", "Takashi Sumiyoshi", "Natsuo Yamashita", "Kota Dohi", "Yohei Kawaguchi"], "abstract": "This paper proposes a zero-shot speech emotion recognition (SER) method that estimates emotions not previously defined in the SER model training. Conventional methods are limited to recognizing emotions defined by a single word. Moreover, we have the motivation to recognize unknown bipolar emotions such as \"I want to buy - I do not want to buy.\" In order to allow the model to define classes using sentences freely and to estimate unknown bipolar emotions, our proposed method expands upon the contrastive language-audio pre-training (CLAP) framework by introducing multi-class and multi-task settings. We also focus on purchase intention as a bipolar emotion and investigate the model's performance to zero-shot estimate it. This study is the first attempt to estimate purchase intention from speech directly. Experiments confirm that the results of zero-shot estimation by the proposed method are at the same level as those of the model trained by supervised learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech emotion recognition (SER) is a technique to estimate emotions conveyed by speech. This technique can be applied to the speech analysis of call centers [1], spoken dialogue agent [2], e-learning system [3], and mental health analysis [4], etc.\nMany researchers have proposed various methods to improve the performance of SER. Especially, in recent years, there are many methods using deep learning such as SER with self-supervised learning model [5], [6]. In these methods, the model of SER is often trained by supervised-learning. Therefore, the trained model can recognize predefined seen emotions, while it is difficult to recognize unseen emotions. To alleviate this limitation, previous studies have proposed methods of zero-shot SER that can recognize undefined emotions. For example, X. Xu, et al. proposed the method using auditory affective descriptors (AAD) to transfer knowledge from the seen domain to the unseen domain [7], and the method using reconstructed semantic prototypes and data augmentation for the unseen domain [8].\nThe challenge of zero-shot SER is how to define the classes of emotions. Conventional methods do not assume the estimation of emotion classes that cannot be defined by a single word. Therefore, it is difficult to recognize the emotions expressed by text, such as \"the feeling of willing to buy (purchase intention).\" In order to realize SER capable of zero-shot recognizing various emotions, it is necessary to have a framework that can freely define classes during the training and estimation phase. We also have the motivation to realize the zero-shot estimation of bipolar emotions, such as \u201cI want to buy - I do not want to buy.\"\nTo meet these requirements, in this study, we propose a new contrastive language-audio pre-training (CLAP) method that can perform zero-shot estimation of bipolar emotions. CLAP can decide the emotion freely because it can represent the classification class by the sentence. In our method, we define the multi-class emotions with a bipolar sub-class and train the model with CLAP for each emotion. We use six basic emotions with a bipolar sub-class as the multi-class emotions. We expect this model trained by the proposed method to correctly classify the emotions using the knowledge of six basic emotions, even if the estimated class is an unknown bipolar emotion. We also focus on purchase intention as a bipolar emotion and experiment on whether the model trained by the proposed method can zero-shot estimate purchase intention. The contributions of this study are as follows.\n\u2022 This is the first method for zero-shot SER capable of estimating bipolar emotions, such as purchase intention.\n\u2022 This study is the first time to directly estimate purchase intention from speech.\nThis paper proceeds as follows. In Section2, we describe the method of the basic CLAP, proposed method with CLAP, and the data augmentation. In Section3, we explain the setup for the experiment and show the results. Finally, in Section4, we present our conclusions and future work."}, {"title": "II. METHODOLOGY", "content": "A. CLAP\nCLAP is the training scheme that calculates the similarity or dissimilarity between acoustic and linguistic embeddings [9]. This method makes it possible for us to define the categories during the estimation phase and classify speech into unseen categories. Moreover, the model can estimate more various categories because CLAP can define them with the sentence.\nThe overviews of CLAP during the training or estimation phase are shown in Figures 1 and 2, respectively. The inputs are audio and text of categories. Let $x \\in \\mathbb{R}^{N\\times T}$, $y \\in \\mathbb{R}^{N\\times L}$ be the audio and the text, respectively, where $T$ and $L$ are the length of them, and $N$ is the size of mini-batch. Note that $x$, $y$ have correspondence in mini-batch.\nThe inputs are converted to the embeddings by the audio and text encoders. The audio encoder $f_a(\\cdot)$ and text encoder $f_t(\\cdot)$ are given in Equation 1 and 2, respectively. Let $X \\in \\mathbb{R}^{N\\times D_a}$, $Y \\in \\mathbb{R}^{N\\times D_t}$ be the audio embeddings and text embeddings, where $D_a$, $D_t$ are the number of dimensions of each encoder's output. Moreover, Pooling is applied to average or select the embeddings from each encoder.\n$X = Pooling(f_a(x))$\n$Y = Pooling(f_t(y))$\nThose embeddings are input into the linear projection, which are given in Equation 3 and 4. Let $E_a \\in \\mathbb{R}^{N\\times D}$, $E_t \\in \\mathbb{R}^{N\\times D}$ be the fixed embeddings of $D$ dimensions from each linear projection. Also, $Linear_a(\\cdot)$, $Linear_t(\\cdot)$ are the linear projection for audio and text, respectively.\n$E_a = Linear_a(X)$\n$E_t = Linear_t(Y)$\nDuring the training phase, we calculate the similarity matrix $M \\in \\mathbb{R}^{N\\times N}$ given in Equation 5. Let $\\tau$ be the temperature parameter.\n$M = \\tau(E_a E_t^T)$\nAfter that, we calculate and optimize the symmetric loss function [10] $L(\\cdot)$ given in Equation 6, where $CE(\\cdot)$ is the function of cross entropy, and $\\hat{M} \\in \\mathbb{R}^{N\\times N}$ is the ground truth matrix.\n$L(M, \\hat{M}) = \\frac{1}{2} (CE(M, \\hat{M}) + CE(M^T, \\hat{M}^T))$\nOptimizing this loss function, we can train the model to minimize the loss between the ground truth and the estimated values and maximize the loss between the incorrect answer and the estimated values.\nDuring the estimation phase, we defined the categorical classes by words or sentences. Then, we input the speech and texts represented the categories into the encoder and linear projection corresponding to each modalities, and obtain fixed-length vectors. Finally, the similarity between speech and text is calculated using the fixed-length vectors, and the class indicated by the text of the highest similarity is the estimation result.\nThis method has been applied to SER. For example, in the original paper on CLAP, zero-shot SER was performed using the CLAP model trained with environmental sound datasets [9]. Y. Pan et al. also proposed GEmo-CLAP for gender and emotion classification and improved the accuracy rate of four emotion classifications that were not zero-shot [11]. However, there is still no research on recognizing emotion-related classes such as purchase intention using the model of zero-shot SER trained with emotional speech datasets.\nB. Zero-shot SER with multi-class multi-task CLAP\nWe propose the training method of the zero-shot SER for multi-class emotional speeches, which is called multi-class multi-task CLAP. The overview of the proposed method is shown in Figure 3.\nDuring the training phase, the model outputs the similarity matrix of the multi-class prepared for each emotion. After calculating the contrastive loss with the ground truth matrix in each emotion, we summarize and optimize them. Equation 7 formulates the calculation of the loss $L_{all}$. Let $K$ be the number of emotion classes, $k$ be the index of an emotion class, and $M_k \\in \\mathbb{R}^{N\\times N}$, $\\hat{M_k} \\in \\mathbb{R}^{N\\times N}$ be the similarity matrix and ground truth matrics in each emotion, respectively.\n$L_{all} = \\sum_{k=1}^{K} C(M_k, \\hat{M_k})$"}, {"title": "III. EXPERIMENTS", "content": "We experimented with the binary classification task of whether purchase intention from speech. In the following, we explain the experimental setup and results.\nA. Dataset\nBefore the experiment, we created the internal dataset which has annotated emotional utterances in Japanese. First, we collected emotional utterances by role-playing conver-sations. We prepared six scenarios related to conversations between salespeople and customers. Five Japanese speakers talked freely for each scenario. Note that we did not prepare sentences for reading. The sampling rate of the recorded utterance was 44,100 Hz. The recorded data were separated into sessions for each scenario, and we obtained the dataset consisting of six sessions. Afterward, three annotators listened to collected utterances and scored the intensity values of the six emotions (pleasant-unpleasant, aroused-sleepy, dominant-submissive, credible-doubtful, interested-indifferent, positive-negative) on a seven-point scale (1.very low-7.very high). These emotions were assigned according to the related work [13]. In addition, these annotators scored the intensity values indicating the purchase intention on a seven-point scale for only customer's data at corrected utterances.\nFinally, we arranged the labels for the classification task. The distribution of scores of each annotator was normalized by the mean and variance of the distribution of scores given by all annotators. The normalized labels exclude the most frequent value and sets it as a threshold. The score below the threshold are relabeled as negative labels (0) and the score above the threshold are relabeled as positive labels (1). The number of utterances in two classes of six emotions was 2,598, and the number of utterances with the two classes of the purchase intention was 940. We converted the sampling rate of each utterance to 16,000Hz.\nDuring the training and estimation phase using the proposed method, we used descriptions that explain the emotions corresponding to the scores aggregated above. In addition, we experimented with the augmented data by paraphrasing, which was 11 times the number of the original dataset. In the evaluation, we used one of the six sessions as the testing data and others as the training data.\nB. Models and metrics\nWe explain the model structure of the proposed method. The audio encoder consisted of Japanese HuBERT [14] and linear projection. The output dimensions of them were 768 and 512, respectively. Before passing embeddings into linear projection, we averaged the audio embeddings obtained from HuBERT in the time direction. The text encoder consisted of Japanese DistilBERT [15] and linear projection. The output dimensions of them were also 768 and 512, respectively. Before passing embeddings into linear projection, we only used class token from the embeddings obtained from DistilBERT. We used the pre-trained parameters of HuBERT and DistilBERT provided by Hugging Face [16].\nThe baseline in this paper was the model to estimate purchase intention by supervised learning instead of CLAP. This model structure was a combination of HuBERT, linear projection, and one fully connected layer, and we used the pre-trained HuBERT in the same way as the proposed method. The threshold for binary classification on the baseline is defined by the Youden index of the receiver operatorating characteristic (ROC) curve. The number of epochs was 300, the batch size was 64, the learning rate was 0.000001, and the optimization method was Adam [17]. The loss functions of the baseline and the proposed method were binary cross entropy loss and symmetric cross entropy loss, respectively.\nWe used weighted accuracy (WA), unweighted accuracy (UA), and recall of each category as the evaluation metrics. Note that WA is the total number of correct answers to the total data, and UA is the average recall of each category. The higher these metrics are, the more correctly the performance can estimate purchase intention. Moreover, we calculated the frequency rate of matching the random value and the correct answer as a chance rate. In the evaluation of the proposed method, we used the three texts. They all indicated the pur-chase intention, while the words used in the sentence differed. We used these texts to compare the proposed method with the chance rate and baseline.\nC. Results\nTable III shows WA, UA, and recall of each category. The result of the random was about 50% for both WA and UA. Comparing the result of the random with the baseline, WA was 24.5%, and UA was 19.8% higher in the baseline result than in the random. These results indicate that the baseline model does not make a random estimation. It also shows that it is possible to estimate purchase intention from speech using deep learning. Comparing the result of the random with the proposed methods, WA and UA were higher in all proposed methods than in the random. These results show that the zero-shot estimation by the proposed method is not random, and these methods can train the model to estimate unknown bipolar emotions.\nWithout data augmentation, the results show that the recall with purchase intention \"Yes\" using text (1) is higher in the proposed method than in the baseline. The UA using text (1) and the recall with purchase intention \"Yes\" using text (3) were similar to the baseline result. On the other hand, the recalls with purchase intention \u201cNo\u201d using text (1) and \"Yes\" using text (2) were lower than 60%. It indicates that the model trained by the proposed method cannot represent the correspondence well between the semantic information of text and speech under some conditions.\nAll evaluation scores with data augmentation are improved overall than those without data augmentation. Those scores equal the baseline, especially when using text (3). This indicates that data augmentation make it possible for the model to train the correspondence between text and speech semantic information relatively more easy. These results indicate that the zero-shot SER trained by the proposed method can estimate purchase intention that is unseen class by data augmentation of paraphrasing and appropriate creation of text indicating classes.\nWe also investigated the significant difference between the evaluation scores of the baseline and the proposed method using text (3) through a sign test. If there was no significant difference between them, we could assume that the number of samples that were correct by the baseline and incorrect by the proposed method was the same as those that were correct by the proposed method and incorrect by the baseline. We performed a two-tailed sign test based on the above hypothesis."}, {"title": "IV. CONCLUSION", "content": "This study proposed a new method for zero-shot speech emotion recognition. The proposed method is a multi-class multi-task CLAP that can recognize unknown bipolar emo-tions. We found that zero-shot SER trained by the proposed method with paraphrasing data augmentation can be used to estimate purchase intention with the same accuracy as a supervised learning model. In the future, we would like to expand speech data with multi-class labels and perform zero-shot inference for generously related classes other than purchase intention."}]}