{"title": "TSVC: Tripartite Learning with Semantic Variation Consistency for Robust\nImage-Text Retrieval", "authors": ["Shuai Lyu", "Zijing Tian", "Zhonghong Ou", "Yifan Zhu", "Xiao Zhang", "Qiankun Ha", "Haoran Luo", "Meina Song"], "abstract": "Cross-modal retrieval maps data under different modality via\nsemantic relevance. Existing approaches implicitly assume\nthat data pairs are well-aligned and ignore the widely exist-\ning annotation noise, i.e., noisy correspondence (NC). Con-\nsequently, it inevitably causes performance degradation. De-\nspite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives,\nthe differences between these architectures are primarily\nstemmed from random initialization. Thus, the model be-\ncomes increasingly homogeneous along with the training pro-\ncess. Consequently, the additional information brought by\nthis paradigm is severely limited. In order to resolve this prob-\nlem, we introduce a Tripartite learning with Semantic Varia-\ntion Consistency (TSVC) for robust image-text retrieval. We\ndesign a tripartite cooperative learning mechanism compris-\ning a Coordinator, a Master, and an Assistant model. The\nCoordinator distributes data, and the Assistant model sup-\nports the Master model's noisy label prediction with diverse\ndata. Moreover, we introduce a soft label estimation method\nbased on mutual information variation, which quantifies the\nnoise in new samples and assigns corresponding soft labels.\nWe also present a new loss function to enhance robustness\nand optimize training effectiveness. Extensive experiments on\nthree widely used datasets demonstrate that, even at increas-\ning noise ratios, TSVC exhibits significant advantages in re-\ntrieval accuracy and maintains stable training performance.", "sections": [{"title": "Introduction", "content": "Cross-modal retrieval (Anderson et al. 2018; Li et al. 2019)\naims to accurately associate and align data from different\nmodalities, e.g., images and texts. As a key technology in the\nfield of multi-modality, it has been widely applied in both\nindustry and academia. In classification tasks, noise labels\n(Yan et al. 2023; Iscen et al. 2022; Yan et al. 2022) generally\nrefer to labeling errors. In cross-modal matching tasks, how-\never, noise labels pertain to alignment errors in paired data,\nalso known as noise correspondence. Consequently, a large\nfraction of existing noise-robust learning methods (Zhong\net al. 2024) designed for classification cannot be directly ap-\nplied for cross-modal matching tasks.\nTo address noisy correspondences in cross-modal match-\ning, existing studies mainly focus on two aspects: noisy sam-\nple label estimation (Ma et al. 2024; Huang et al. 2024b)\nand loss function adjustment (Hu et al. 2021; Shi et al.\n2024). Noisy sample label estimation primarily involves re-\nestimating and adjusting the labels of noisy samples by ex-\nploring the latent relationships within the dataset. The loss\nfunction adjustment aims to enhance model robustness by\ndesigning new loss functions. Recent studies have increas-\ningly focused on the estimation of noisy sample labels.\nAccurately estimating soft corresponding labels for noisy\ndata has always been a big challenge for noise-robust cross-\nmodal matching. Previous studies (Yang et al. 2024; Zhao\net al. 2024b) have made efforts by employing the rate of sim-\nilarity changes to quantify noise content. Yang et al. (Yang\net al. 2023) leverage the inherent similarity between the two\nmodalities to assign pseudo labels. Nevertheless, they fail to\nconsider other crucial data relationships and features, thus\nlimiting their ability to identify complex noise. On the other\nhand, some studies (Zheng, Awadallah, and Dumais 2021;\nZhang, Li, and Ye 2024) mainly rely on the memory effect\nof Deep Neural Networks (DNN) (Zhang et al. 2021), which\ntends to prioritize learning simple patterns rather than noisy\nsamples. They divide the training data into a clean set and a\nnoisy set, and use the Co-Teaching training scheme (Han\net al. 2018; Yan et al. 2023) to train them with different\nmechanisms.\nNevertheless, in the Co-Teaching paradigm, the differ-\nences between two networks with the same architecture pri-\nmarily arise from random initialization. During the training\nprocess, both sides provide what they deem as important\ndata to each other for training, resulting in limited additional\ninformation gain. Moreover, the predominant approaches\n(Yang et al. 2023; Qin et al. 2024; Yang et al. 2024) for\nnoise-robust tasks typically used minimizing triplet loss with\na soft margin, which fails to consider the differences and dis-\ntribution characteristics between clean and noisy samples,\ncausing suboptimal performance in distinguishing between\nthem.\nIn order to resolve the problems mentioned above,\nwe propose a Tripartite learning with Semantic Variation\nConsistency (TSVC) scheme for robust image-text retrieval,\nwhich mainly consists of three components.\nFirst, we propose a Semantic Information Variation\nConsistency (SIVC) method for label estimation method,\nbased on semantic variation of Mutual Information (MI) be-\ntween new pairs and clean pairs. As shown in Fig. 1, we\ntake into consideration three parts, i.e., pairs, images, and\ntexts. The smaller the change, the closer the MI of new pairs\nis to that of clean pairs. It indicates that the new samples\nare cleaner, leveraging broader data relationships to better\nidentify and quantify noise for label estimation. Second, to\navoid model homogenization, we propose a novel Tripartite\nCooperative Learning Mechanism (Tri-learning) that devi-\nates from the conventional Co-Teaching paradigm. Specifi-\ncally, Tri-Learning consists of three models: a Coordinator,\na Master, and an Assistant. The Coordinator partitions data\ninto clean and noisy sets. The Assistant selects clean sam-\nples from the noisy set to enhance the Master model. The\nMaster trains on diverse data while preserving the ability to\nextract clean samples. The Coordinator adjusts partitioning\nfor the next round based on feedback from the Assistant.\nThird, we train the segmented dataset using the newly in-\ntroduced Distribution-Adaptive Soft Margin (DASM) loss\nfunction, which takes into account the dynamic margin af-\nfected by rectified soft label and sample distribution devia-\ntion. Our main contributions are summarized as follows:\n\u2022 We introduce a novel training paradigm Tri-learning to\nestablish a collaborative relationship among three mod-\nels. It mitigates the improvement limitation in traditional\nco-training paradigms, which is caused by the homoge-\nnization of models.\n\u2022 We propose a soft label estimation method namely SIVC\nbased on semantic Mutual Information variation, and\npresent a loss function called DASM that corrects mar-\ngins and distribution deviations, which enhances noise\ndetection accuracy and robustness significantly.\n\u2022 We conduct extensive experiments on three cross-modal\nnoise datasets, including synthetic and real-world noises.\nExperimental results show that TSVC significantly out-\nperforms state-of-the-art methods."}, {"title": "Related Work", "content": "Cross-modal Matching\nCross-modal matching (Yang et al. 2022; Deng et al. 2019;\nWang et al. 2020) is an fundamental problem in the fields\nof information retrieval (Zhao et al. 2024a) and multi-modal\nanalysis. It aims to retrieve one modality based on another\nmodality as a query.\nImage-text retrieval, as the most common task, can be\nroughly divided into two categories, i.e., coarse-grained\nalignment and fine-grained alignment. Coarse-grained align-\nment (Chen et al. 2021; Faghri et al. 2017; Diao et al. 2021)\nusually employs two separate networks to project the entire\nimage and text into a unified embedding space. It then cal-\nculates the overall correlation using cosine similarity. Fine-\ngrained alignment methods (Lee et al. 2018; Chen et al.\n2020b; Zhang et al. 2022) often combine cross-modal inter-\naction with local segment alignment. Afterwards, they accu-\nmulate the scores of local regions to acquire the overall sim-\nilarity (Wu et al. 2019). It is worth noting that the superior\nperformance of these methods relies on a large amount of\naccurately annotated training data, without considering the\nissue of noise correspondence.\nNoisy Correspondence Learning\nNoise correspondence, different from noise label learning,\nusually refers to mismatched sample pairs in multi-modal\ndatasets. Some studies (Huang et al. 2021; Han et al. 2023;\nMa et al. 2024) leverage the memory effect of DNN to\nconstruct error correction networks to enhance the data pu-\nrification process or identify mismatching samples. Other\nworks (Qin et al. 2022; Yang et al. 2023; Han et al. 2024;\nLi et al. 2024) improve the objective function with dynamic\nloss or bi-directional cross-modal similarity to identify the\nnoise contained in the samples.\nThe methods mentioned above assume that the two net-\nworks can provide distinct data perspectives. Nevertheless,\nthe additional information improvement remains limited, be-\ncause the differences between the two networks with the\nsame architecture primarily stem from random initializa-\ntion (Qin et al. 2024). In contrast, we propose a Tripartite\nCooperative Learning Method to augment network diversity,\nand utilize variations in Mutual Information across samples\nand modalities to estimate soft correspondence labels, which\nenhances the model's robustness against noise significantly."}, {"title": "The Proposed Method TSVC", "content": "Problem Formulation\nGiven the dataset $D = \\{(I_i, T_i, y_i)\\}_{i=1}^{N}$ consisting of N sam-\nples for training. Each of them includes one pair of image\nand text, as well as a binary label $y_i$ indicating the relevance\nof the pair as positive $(y_i = 1)$ or negative $(y_i = 0)$. In cross-\nmodal matching, the primary objective is to maximize the\nsimilarity between positive samples (matched pairs) while\nminimizing the similarity between negative samples (un-\nmatched pairs).\nConsidering that multi-modal datasets are frequently an-\nnotated or acquired from Internet using cost-effective meth-\nods, it is inevitable to encounter noisy data in cross-modal"}, {"title": "Semantic Information Variation Consistency", "content": "Mutual Information. Existing studies (Huang et al. 2024a;\nWang et al. 2024) suggest that clean samples have a stronger\ncorrelation between images and texts. Consequently, Se-\nmantic Information Variation Consistency (SIVC) leverages\nMutual Information (MI) (Shannon 1948) to estimate soft\ncorrespondence labels. MI quantifies the shared information\nbetween signals, providing a criterion to measure the depen-\ndency between image-text pairs and determine sample noise\nproportion. For a pair of discrete sample pairs (X, Y), MI is\ndefined as:\n$MI(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\frac{p(x, y)}{p(x)p(y)},$ (1)\nwhere p(x, y) indicates the joint probability distribution,\np(x) and p(y) represent the marginal probability distribu-\ntions.\nWe assume that x and y are vectors with d dimension, we\nuse the 1D histogram to approximate the marginal distribu-\ntions. First, we divide the feature field into evenly spaced\nintervals, then count the frequency of each feature value $x_i$\nfalling within different intervals $[x_j, x_{j+1})$ to obtain $p(x)$:\n$p(x)_j = \\frac{1}{d} \\sum_{i=1}^{d} \\delta(x_i \\in [x_j, x_{j+1})),$ (2)\nwhere $\\delta(\\cdot)$ is an indicator function. Same as p(y). Then,\nwe use the 2D histogram to approximate the joint probabil-\nity distribution p(x, y). We divide the feature fields into a\n2D grid, then we count the frequency of each feature pair\n$(x_i, y_i)$ falling within different cells to obtain p(x, y):\n$p(x, y)_{j,k} = \\frac{1}{d} \\sum_{i=1}^{d} \\delta(x_i \\in [x_j, x_{j+1})) \\times \\delta(y_i \\in [y_j, y_{j+1})).$ (3)\nSemantic Consistency Calculation. In practice, slight dis-\nparities exist in the similarity of clean subsets, and their la-\nbels may not exactly equal 1. When estimating soft corre-\nspondence labels, we prioritize variations in semantic con-\nsistency. Clean image-text pairs generally show balanced\ncontributions from both modalities, avoiding excessive dom-\ninance by either.\nFirst, we select a pair of samples $(I_a, T_a)$ as anchor points\nin each batch, similar to previous studies, by choosing the\nimage-text pair with the minimum loss within the batch.\nThese anchor points are considered representative of clean\nsamples. Subsequently, for the given sample $(I_b, T_b)$, we\nperform calculations from three aspects to evaluate its con-\nsistency and noise characteristics."}, {"title": "\u2022 Change rate of MI between image-text pairs:", "content": "$R_p = \\frac{|\u041c\u0406 (I_a,T_a) \u2013 \u041c\u0406 (I_b, T_b) |}{MI (I_a, T_a)}$ (4)\n\u2022 Change rate of MI between texts:\n$R_T = \\frac{|\u041c\u0406 (I_a, T_a) \u2013 \u041c\u0406 (I_a, T_b) |}{MI (I_a, T_a)}$ (5)\n\u2022 Change rate of MI between images:\n$R_I = \\frac{|\u041c\u0406 (I_a, T_a) \u2013 \u041c\u0406 (I_b,T_a)|}{MI (I_a, T_a)}$ (6)\nSoft Label Estimation. By calculating soft label estimation,\nwe derive a standardized measure of the variations in consis-\ntency between given pairs and anchor pairs. Specifically, if\nthe change rate between image-text pairs ($R_p$) is small, and\nthe change rate between texts ($R_T$) is nearly equal to that\nbetween images ($R_I$), it indicates that texts and images con-\ntribute equally to the semantic information. In such cases,\nwe infer that the new pairs are likely to be clean.\nEventually, we combine the inverse proportional function\nwith the MI change rate to determine the soft correspon-\ndence label $y^* \\in (0, 1]$ for the given image-text pair:\n$y^* = \\frac{1}{1+ (R_p + |R_T \u2013 R_I|)}$ (7)"}, {"title": "Tripartite Cooperative Learning Mechanism", "content": "We propose a framework named Tri-learning to overcome\nthe limitations of traditional Co-training. The framework\nconsists of three models: Mc (Coordinator) partitions the\noriginal dataset into a clean set and a noisy set, MA (As-\nsistant) selects low-loss samples and combines them with\nthe clean samples to train the MM (Master). MM further\nrefines MA, while Mc iteratively adjusts the partitioning\nscheme for the next round based on the clean samples, which\nare fed back by the Assistant model from noisy samples.\nThe detailed training process is illustrated in Fig. 2 and is\ndescribed as follows:\nStep 1: Early Sample Division. For Mc, we adopt the\nsame network structure as MM and MA, with only differ-\nent initialization parameters. With the assistance of GMM,\nMc divides the original dataset into clean samples $D^M_C$ with\nlosses less than threshold $\\delta$, and noisy samples $D^A_C$ with\nlosses larger than $\\delta$.\nStep 2: Sample Re-Division. Through step 1, we obtain\ntwo separate data streams, one clean and the other noisy.\nFor the clean data stream $D^M_C$, we further mine the samples\n$D'^M_C$ with losses less than $\\delta$ by the Master model and GMM.\nIt ensures that the training samples of MA are as clean as\npossible. For the noisy data stream $D^A_C$, we use the Assistant\nmodel and GMM to select relatively clean data $D'^A_C$ with\nlosses less than $\\delta$, to provide diverse data for training MM\nand Mc.\nStep 3: Label Rectification and Network Training.\nWe first apply the SIVC to rectify noisy labels within each\ntraining batch. Then, we train MA using the cleaned dataset\n$D'^M_C$ from step 2, equipping Ma with the ability to extract\nclean samples from the noisy dataset $D^A_C$. Similarly, we op-\ntimize MM using a combination of $D'^A_C$ and $D^M_C$, enabling\nit to gain valuable insights from diverse samples while main-\ntaining robustness and generalization. To prevent error prop-\nagation during data filtering, Mc is iteratively optimized\nusing $D'^A_C$, as $D^A_C$ may contain clean samples. This allows\nMc to detect potential errors and adjust the training and\npredictions of MA and MM.\nWe repeat Step 1, 2, and 3 for a specific number of it-\nerations, then use the MA and MM models as adapta-\ntion models for evaluation after the training is completed.\nA different training paradigm from regular Co-Teaching is\nadopted, which utilizes a different mechanism to filter and\nacquire low-loss samples. It enables the network to counter\nnoisy correspondence from different perspectives and im-\nproves model robustness. Moreover, this approach prevents\nassimilation between the two models in Co-Teaching, avoid-\ning negative impacts on prediction effectiveness."}, {"title": "Distribution-Adaptive Soft Margin Loss", "content": "We propose DASM loss, which adjusts the soft margin based\non the similarity between sample pairs and their deviation\nfrom the center of the clean distribution. Specifically, for an\nimage-text pair (I', T') in a mini-batch, we compute the dis-\ntance from it to the center of the clean sample loss distribu-\ntion d = |L - Lclean|, and then calculate the loss LDASM:\n$L_{DASM} (I_i, T_i) = [\\tilde{a_i} \u2013 S (I_i, T_i) + S (I_i, \\hat{T_i})]_+\n+ [\\tilde{a_i} - S (I_i, T_i) + S (\\hat{I_i}, T_i)]_+\n\\tilde{a_i} = (2 + tanh(-\\frac{d}{\\beta})) \\frac{m^{y^*_i} -1}{m-1} + \\alpha,$ (8)\n(9)\nwhere $\\hat{T_i} = \\underset{T_j \\ne T_i} \\operatorname{argmax} S(I_i, T_j)$ and $\\hat{I_i} = \\underset{I_j \\ne I_i} \\operatorname{argmax} S(I_j, T_i)$ represent the hard negative text and\nimage that are most similar to the aligned image-text pair\n$(I_i, T_i)$ within a batch. As iterations progress, DASM grad-\nually redirects samples that significantly deviate from the\nclean distribution center, strengthening the boundary be-\ntween clean and noisy distributions. By slowing down the\nboundary growth deliberately, it avoids overly penalizing\nmisclassification similarity scores, thus maintaining model\nbalance and flexibility."}, {"title": "Experiments", "content": "Datasets\nWe utilize the following three widely used multi-modal\ndatasets to evaluate our method:\nFlickr30K This dataset contains 31,000 images collected\nfrom Flickr, with each image paired with five textual de-\nscriptions providing detailed annotations of the content. It\ncovers a variety of scenes, objects, and activities, making\nit a common benchmark for cross-modal retrieval tasks. In\nour experiments, we use 29,000 images for training, 1,000\nfor validation, and 1,000 for testing to evaluate the model's\nperformance."}, {"title": "Hyperparameters Analysis", "content": "The hyperparameters 8 and m represent the threshold used\nfor dividing noisy samples and the parameter controlling the\nsoft margin in DASM, respectively. These parameters play a\nimportant role in balancing noise filtering and optimizing\nmargin flexibility. We conduct experiments on Flickr30K\nand MSCOCO datasets with a noise ratio of 40%. As de-\npicted in Fig. 4, the value of Rsum increases continuously\nwith an increase in d, reaching its peak at 8 = 0.5 (between\n0.5 to 0.6 for MSCOCO), and then gradually decreases. A\nlarger mismatch threshold & might classify weakly labeled\nsamples as mislabeled ones, thus impairing generalization\nability. The optimal value for m is 10, which significantly\ninfluences the performance of the model by affecting the\nsize of the soft-margin. Therefore, it should be considered\ncomprehensively during the model optimization process to\nensure the best performance."}, {"title": "Visualization", "content": "To further illustrate effectiveness of TSVC, we present the\nretrieval process using textual and visual queries, as depicted"}, {"title": "Conclusion", "content": "In this work, we proposed a Tripartite Learning with Se-\nmantic Variation Consistency (TSVC) framework to address\nnoisy correspondences for cross-modal data. TSVC lever-\nages information variation between clean image-text pairs\nto estimate soft correspondence noise labels. By distribut-\ning tasks among three models, TSVC effectively resolves the\nchallenge of limited performance improvement due to model\nprediction homogenization. Extensive experiments on three\ndatasets validate the effectiveness of our method."}]}