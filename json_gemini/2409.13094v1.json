{"title": "DenoMamba: A fused state-space model for low-dose CT denoising", "authors": ["\u015eaban \u00d6zt\u00fcrk", "O\u011fuz Can Duran", "Tolga \u00c7ukur"], "abstract": "Low-dose computed tomography (LDCT) lower potential risks linked to radiation exposure while relying on advanced denoising algorithms to maintain diagnostic quality in reconstructed images. The reigning paradigm in LDCT denoising is based on neural network models that learn data-driven image priors to separate noise evoked by dose reduction from underlying tissue signals. Naturally, the fidelity of these priors depend on the model's ability to capture the broad range of contextual features evident in CT images. Earlier convolutional neural networks (CNN) are highly adept at efficiently capturing short-range spatial context, but their limited receptive fields reduce sensitivity to interactions over longer distances. Although transformers based on self-attention mechanisms have recently been posed to increase sensitivity to long-range context, they can suffer from suboptimal performance and efficiency due to elevated model complexity, particularly for high-resolution CT images. For high-quality restoration of LDCT images, here we introduce DenoMamba, a novel denoising method based on state-space modeling (SSM), that efficiently captures short- and long-range context in medical images. Following an hourglass architecture with encoder-decoder stages, DenoMamba employs a spatial SSM module to encode spatial context and a novel channel SSM module equipped with a secondary gated convolution network to encode latent features of channel context at each stage. Feature maps from the two modules are then consolidated with low-level input features via a convolution fusion module (CFM). Comprehensive experiments on LDCT datasets with 25% and 10% dose reduction demonstrate that DenoMamba outperforms state-of-the-art denoisers with average improvements of 1.4dB PSNR, 1.1% SSIM, and 1.6% RMSE in recovered image quality.", "sections": [{"title": "I. INTRODUCTION", "content": "A cornerstone in modern medical imaging, CT irradiates the body with a beam of X-rays to furnish detailed cross-sectional views of anatomy [1]. Unlike conventional radiography, \u0421\u0422 relies on acquisition of multiple snapshots as the X-ray beam is rotated around the body, causing substantially elevated exposure to ionizing radiation with potential risks including cancer [2], [3]. A mainstream approach to alleviate these health risks involves CT protocols that cap the tube current or exposure time to lower the number of incident photons and thereby the radiation dose [4]. However, as the signal-to-noise ratio (SNR) scales with the number of incident photons, dose reduction inevitably increases the noise component in CT images, significantly degrading image quality and potentially obscuring diagnostic features. Consequently, development of effective denoising methods is imperative to maintaining the diagnostic utility of LDCT images acquired under high dose reduction [5]. The success of a denoising method inherently depends on its ability to separate noise components from tissue signals. Note that tissues in a given cross-sectional view can be broadly distributed across contiguous or segregated clusters [6], so tissue signals exhibit both local and global correlations [7]. Meanwhile, the noise variance in LDCT images scales with the intensity of the tissue signal, so the noise component can also show a degree of local and global correlations [8], [9]. Thus, an effective denoising method should be able to discern unique patterns of contextual features that distinguish the noise component from tissue signals.\nIn recent years, learning-based methods rooted in deep neural network models have replaced traditional approaches in LCDT denoising [10], [11], given their ability to adapt to the distribution of imaging data [12]\u2013[15]. Earlier studies in this domain have predominantly employed CNN-based models to process noisy LDCT images [16]-[18]. Several architectural improvements have also been posed over basic CNNs to boost performance, including use of residual con-nections [19] and edge-sharpening convolutions to alleviate spatial smoothing [20], and use of multi-scale networks to enhance capture of short- and long-range context [21], [22]. Commonly, these CNN models employ compact convolutional filters for image processing, enabling them to benefit from linear model complexity with respect to image dimensions, and high expressiveness for local contextual features that are criti-cal in delineating detailed tissue structure [23]\u2013[25]. However, the locality bias of convolutional filters inevitably restricts sensitivity to long-range contextual features in LDCT images [26], [27]. As such, CNNs can suffer from poor denoising performance especially near regions of heterogeneous tissue composition, where understanding contextual relationships is crucial for distinguishing signal from noise.\nA recent alternative to CNNs are transformer-based models that employ self-attention operators in place of convolutional filters [9], [28]\u2013[30]. Transformers process images as a sequence of tokens (i.e., image patches), and perform non-local filtering via self-attention for improved sensitivity to long-range contextual features. At the same time, however, such non-local filtering induces quadratic model complexity with respect to sequence length [31], [32]. A common approach to mitigate complexity has been to split the image into relatively large-sized patches, which naturally limits spatial precision [31], [33], [34]. A number of strategies have been considered to address this limitation, including architectures with separate branches for low- and high-frequency image components [35], architectures with edge enhancing filters [36], hybrid CNN-transformer architectures [37], and pyramidal transformer ar-chitectures with implicit biases from CNN models [9], [38]\u2013[40], and dual-attention architectures have been developed to effectively process various features [30]. Note that despite their relatively improved efficiency, these variant transformer models can still be more complex than CNNs, thus suffering from suboptimal learning on modest-sized training datasets.\nAn emerging framework to capture long-range context under linear complexity employs state-space models (SSM) [41], [42]. Recent medical imaging studies have reported promising results based on a selective SSM variant (Mamba) in down-stream tasks such as segmentation and classification [43]\u2013[45]. Yet, the potential of SSMs for image denoising in LDCT remains unexplored.\nHere we introduce a novel SSM-based model, DenoMamba, to achieve improved performance in LDCT image denoising without elevating model complexity. DenoMamba leverages an hourglass architecture with encoder-decoder stages constructed with novel FuseSSM blocks (Fig. 1). Each FuseSSM block convolutionally fuses the spatial context captured by a spatial SSM module with the channel context captured by a novel channel SSM module. The proposed channel SSM module employs a secondary gated convolution network following the SSM layer in order to extract latent hidden features of channel context. Meanwhile, to improve preservation of low-level representations in LDCT images, the SSM modules are equipped with residual connections and hierarchical features extracted by encoder stages are projected onto decoder stages via long-range residual connections. These building blocks empower DenoMamba to capture both short- and long-range contextual information in LDCT images, with comparable model complexity to conventional CNNs. Comprehensive evaluations on LDCT datasets acquired with 25% and 10% of nominal radiation doses demonstrate the superior perfor-mance of DenoMamba compared to state-of-the-art baselines. Code to implement DenoMamba is publicly available at https://github.com/icon-lab/DenoMamba."}, {"title": "Contributions", "content": "To our knowledge, DenoMamba is the first LDCT de-noising method in the literature that leverages state-space modeling to capture spatial and channel context.\nDenoMamba employs a novel architecture based on con-volutional fusion of spatial and channel SSM modules, enabling it to consolidate a comprehensive set of contex-tual features.\nA novel channel SSM module is introduced that extracts low- and high-level latent features of channel context by cascading a transposed SSM layer with a subsequent gated convolution network.\nDenoMamba is demonstrated to offer superior image quality to state-of-the-art convolutional and transformer models for LCDT denoising across various levels of dose reduction."}, {"title": "II. RELATED WORK", "content": "Many earlier methods in the domain of LDCT denoising have adopted CNN models that process images via compact,\nLearning-based Models in LDCT Denoising"}, {"title": "III. THEORY", "content": "LCDT image denoising involves suppression of elevated Poisson noise in low-dose scans due to reduced number of incident photons from the X-ray beam. In learning-based methods, this is attempted with a neural network model that is trained to map noisy LDCT images onto denoised images that would be consistent with a normal dose CT (NDCT) scan. Let $x \\in \\mathbb{R}^{H \\times W}$ denote the noisy LDCT image, and $y \\in \\mathbb{R}^{H \\times W}$ denote the corresponding NDCT image, where $H, W$ are the image height and width, respectively. Given a training set of $N$ image pairs $(x_{tr}[i], y_{tr}[i])$ with $i \\in [1 N]$, the network model $f_{\\theta}(\u00b7)$ with parameters $\\theta$ can be trained as follows:\n$\\theta^* = \\underset{\\theta}{argmin} \\sum_{i=1}^{N} ||f_{\\theta}(x_{tr}[i]) - y_{tr}[i]||_2,$\nWhere:\n$\\{z_{cha}, z_{in}, z_{spa}\\} = \\{SSM_{cha}(z_{in}), I(z_{in}), SSM_{spa}(z_{in})\\},$\n$\\mathbb{G}P_{spa} = (f_{lin}(z_{in})),$\n$SSM ((DWConv^{3 \\times 3}(f_{lin}(z_{in})))),$\n$\\mathbb{G}P_{cha} = (f_{lin}(z_{in})),$\n$SSM (((DWConv^{3 \\times 3}(f_{lin}(z_{in})))))^\\mathbb{T}),$\n\\mathbb{G}P^2 = (DWConv^{3 \\times 3}(Conv^{1 \\times 1}(z_{cha})))$\n$z_{cha} = Conv^{1 \\times 1}(\\mathbb{G}P^2 DWConv^{3 \\times 3}(Conv^{1 \\times 1}(z_{cha})))) + z_{cha},$\nResults in:\n$h[n] = Ah[n-1] + Bz[n],$\n$z[n] = Ch[n] + Dz[n],$\n$\\xi_{spa} = z_{in} + (f_{lin} (\\mathbb{G}P_{spa} M_{spa}),$"}, {"title": "A. Problem Definition", "content": "DenoMamba is the first LDCT image denoising method in the literature that uses SSMs to model spatial and channel context, to our knowledge. It employs a novel architecture based on FuseSSM blocks, which serve to capture a compre-hensive set of contextual features across spatial and channel dimensions, while maintaining favorable balance in model complexity. In the following subsections, we describe the overall model architecture and the inner structure of FuseSSM blocks in detail."}, {"title": "1) Overall Model Architecture:", "content": "As depicted in Fig. 1, Deno-Mamba follows an hourglass structure with K encoder and K decoder stages. Each stage is implemented as a cascade of multiple FuseSSM blocks. Starting from the noisy LDCT image x taken as model input, encoder stages serve to extract latent contextualized representations via FuseSSM blocks and resample the feature map dimensions. Let $x_{enc}^k$ denotes the feature map at the output of the kth encoder stage, with k\u2208 [1,2, ..., K] and $x_{enc}^0$ = x. The mapping through the kth encoder stage can be described as follows:\n$x_{enc}^k = \\{\n    \\begin{array}{ll}\n        Down(FuseSSM_{enc}(x_{enc}^{k-1}; \\theta_{enc}^k)), & \\text{if } k \\neq K \\\\\n        FuseSSM_{enc}(x_{enc}^{k-1}; \\theta_{enc}^k), & \\text{if } k = K\n    \\end{array}\n\\}$\nwhere $\\theta_{enc}^k$ denotes the parameters of FuseSSM blocks in the kth encoder stage, Down(\u00b7) denotes a learnable downsampling operator, and $x_{enc}^{0} \\in \\mathbb{R}^{H \\times W \\times 2*C}$. Note that downsampling is performed at the end of all encoder stages, except for the final stage (i.e., k = K).\nStarting from the encoded feature map $x_{enc}^k$, decoder stages then serve to reconstruct a denoised image \u0177 from the latent representations via FuseSSM blocks and resampling of feature map dimensions. The decoder stages follow a mirror-reversed order, such that $x_{dec}^k$ denotes the feature map at the output of the kth decoder stage, with k \u2208 [K, K \u2212 1, ..., 1] and $x_{dec}^K = x_{enc}^K$. Thus, the mapping through the kth decoder stage can be described as follows:\n$x_{dec}^{k-1} = \\{\n    \\begin{array}{ll}\n        FuseSSM_{dec}(Up(x_{dec}^{k}) + x_{enc}^{k-1}; \\theta_{dec}^k), & \\text{if } k \\neq 1 \\\\\n        FuseSSM_{dec}(x_{dec}^{k} + x_{enc}^{k-1}; \\theta_{dec}^k), & \\text{if } k = 1\n    \\end{array}\n\\}$\nwhere the $\\theta_{dec}^k$ denotes the parameters of FuseSSM blocks in the kth decoder stage, Up(\u00b7) denotes a learnable upsam-pling operator, and $x_{dec}^{k-1} \\in \\mathbb{R}^{\\frac{H}{2^{k-2}} \\times \\frac{W}{2^{k-2}} \\times 2^{k-2}C}$. Note that upsampling is performed on the decoder feature map $x_{dec}^k$ in the beginning of all decoder stages, except for the final stage (i.e., k = 1). Furthermore, encoder feature maps from the respective encoder stage $x_{enc}^{k-1}$ are residually added onto the input decoder maps to improve preservation of low-level structural representations in LDCT images. The final output of DenoMamba is taken as \u0177 = $x_{dec}^0$. The forward mappings through DenoMamba that project the noisy LDCT image onto the denoised image estimate are outlined in Alg. 1."}, {"title": "2) FuseSSM blocks:", "content": "DenoMamba is constructed with novel FuseSSM blocks that comprise a spatial SSM module to capture contextual representations in the spatial domain and a channel SSM module to capture contextual representations in the channel domain [42]. We uniquely propose to project input feature maps to the FuseSSM module across three parallel pathways that propagate the contextualized representations from spatial and channel SSM modules, along with original in-put features. Afterwards, these representations are merged via a convolutional fusion module (CFM). For a given FuseSSM block, a schematic of the individual components are depicted in Fig. 2, and corresponding network mappings through the components are outlined in Alg. 2.\nThe design of FuseSSM blocks in encoder and decoder stages are near identical, apart from variability in feature map dimensions. Thus, here we will describe the projections through a FuseSSM block at a given stage without distinguish-ing between encoder/decoder segments. Assuming that the input feature map at the kth stage is $x_{in} = x^k \\in \\mathbb{R}^{H' \\times W' \\times C'}$, the respective FuseSSM block first splits the input across the spatial dimensions into P = H'W' non-overlapping patches of size (p,p) to form a respective sequence $z_{in}$ [65]. This input sequence is then projected through three parallel pathways to compute contextualized representations:\n$\\{z_{cha}, z_{in}, z_{spa}\\} = \\{SSM_{cha}(z_{in}), I(z_{in}), SSM_{spa}(z_{in})\\},$\nwhere SSMcha denotes the channel SSM, SSMspa denotes the spatial SSM, and I denotes identity transformation. Finally, the extracted contextual representations are pooled and convolu-tionally fused in the CFM module:\n$z_{pool} = Concat(z_{cha}, z_{in}, z_{spa}),$\n$z_{out} = Conv^{1 \\times 1}(z_{pool}) + Conv^{3 \\times 3}(Conv^{3 \\times 3}(z_{pool})),$\nwhere Concat denotes a concatenation operator that pools feature maps across the channel dimension, Conv1\u00d71 and Conv3\u00d73 respectively denote 1 \u00d7 1 and 3 \u00d7 3 convolutional layers, and \u2295 is the element-wise addition operator. The output sequence zout is remapped onto the image domain as $x_{out}$, taken as the output of the FuseSSM block.\nSpatial SSM: Within the spatial SSM module, a first branch linearly embeds the input sequence and uses a nonlinearity to produce a gating variable GP:\n$\\mathbb{G}P_{spa} = (f_{lin}(z_{in})),$\nwhere o is an activation function and fiin denotes a learnable linear mapping. A second branch performs linear embedding and convolutional encoding, followed by state-space modeling:\n$M_{spa} = SSM ((DWConv^{3 \\times 3}(f_{lin}(z_{in}))))),$\nwhere SSM denotes a state-space layer, DWConv3\u00d73 refers to depth-wise convolution of kernel size 3 \u00d7 3.\nHere, the state-space layer is implemented based on the Mamba variant in [42], which performs selective scanning across two spatial dimensions for sequence expansion, per-\nChannel SSM: Similar to the spatial SSM module, within the channel SSM module, a first branch produces a gating variable and a second branch performs state-space modeling on the input sequence to capture contextual interactions in the channel dimension:\n$\\mathbb{G}P_{cha} = (f_{lin}(z_{in})),$\n$M_{cha} = SSM (((DWConv^{3 \\times 3}(f_{lin}(z_{in})))))^\\mathbb{T}),$\nwhere T denotes the transpose operator. Differing from the spatial SSM module, the channel SSM module model channel context by transposing the input sequence prior to and after the projection through the state-space layer. This results in an intermediate set of contextual representations derived as:\n$z_{cha} = z_{in} + (f_{lin}(\\mathbb{G}P_{cha} M_{cha}).$"}, {"title": "3) Learning Procedures:", "content": "Given a training set of image pairs (Xtr[i], Ytr[i]) with i \u2208 [1 N], DenoMamba with parameters Oenc, Odec is trained via a mean-squared error loss term:\n$\\begin{aligned}\n    \\{\\theta_{enc}, \\theta_{dec}\\} = \\underset{\\theta_{enc}, \\theta_{dec}}{argmin} \\sum_{i=1}^{N} \\Big|\n      \\Big|FuseSSM_{enc}^{(K:1)} FuseSSM_{dec}^{(1:K)} (x_{tr}[i]; \\theta_{(1:K)}; \\theta_{(K:1)}) - y_{tr}[i]\n      \\Big|\n    \\Big|_2\n\\end{aligned},$\nUsing the trained parameters {0nc, dec}, the model can be deployed to process a novel LDCT image from the test set Xtest[i] to estimate a denoised output \u0177test[i] as:\n$Y_{test[i]} = FuseSSM_{enc}^{(K:1)} (FuseSSM_{dec}^{(1:K)} (x_{test[i]}; \\theta_{(1:K)}; \\theta_{(K:1)})$.\n\\theta_{enc}\n     \\Big|\n  \\Big|_2\n\\end{aligned},$"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Demonstrations of denoising performance were conducted on datasets derived from the 2016 NID-AAPM Mayo Clinic Low Dose CT Grand Challenge, comprising contrast-enhanced abdominal CT scans obtained 70 s after administration of iodinated contrast agents [66]. Separate datasets were consid-ered based on two different dose reduction levels, resulting in 25% and 10% radiation dose with respect to nominal values. Normal dose CT (NDCT) scans were acquired at 120 kV reference tube potential with 200 effective mAs as quality reference radiation dose. Meanwhile, LDCT at 25% radiation dose assumed 50 effective mAs, and LDCT at 10% radiation dose assumed 20 effective mAs [66]. Dose reduction in LDCT scans was attained by adding Poisson noise onto NDCT images [8], [31]."}, {"title": "A. Datasets", "content": "For both 25%-dose and 10%-dose datasets, the training and validation sets were formed by reconstructing CT images at 1-mm slice thickness. A total of 760 cross-sectional NDCT-LDCT image pairs from 9 subjects were included in the training set, and 35 image pairs from 1 subject were included in the validation set. Meanwhile, the test set was formed by reconstructing four different sets of CT images based on two distinct kernels (Siemens B30 and D45), and two distinct slice thicknesses (1 mm and 3 mm), resulting in multiple parameter combinations: B30-1mm, D45-1mm, B30-3mm, and D45-3mm. A total of 200 image pairs from 20 subjects were included in the test set. All images were resized to 256\u00d7256 in-plane resolution, and there was no subject overlap among the training, validation and test sets."}, {"title": "B. Competing Methods", "content": "We demonstrated DenoMamba against several state-of-the-art methods for LDCT denoising. Competing methods included CNN models (RED-CNN, N2N, EDCNN), transformer models (UFormer, LIT-Former), generative adversarial models (WGAN, DU-GAN), an auto-encoder model (QAE) and a diffusion model (IDDPM).\nDenoMamba: A K = 4 stage encoder-decoder architecture was used, where the number of FuseSSM blocks cascaded varied as [4, 6, 6, 8] across encoder stages and as [6, 6, 4, 2] across decoder stages, respectively. Spatial resolution was lowered by a factor of 2 following each encoder stage except for the final one, while the channel dimensionality was set as [48, 96, 192, 384] across stages. Conversely, spatial resolution was increased by a factor of 2 after each decoder stage except for the final one, with the channel dimensionality set as [192, 96, 48, 48] across stages. Both channel and spatial SSM modules used a state expansion factor of 16, a local convolution width of 4, and a block expansion factor of 2.\nRED-CNN: A CNN model was considered that combines convolution layers, deconvolution layers, and shortcut con-nections [19]."}, {"title": "V. RESULTS", "content": "We conducted a systematic set of ablation studies to ex-amine the importance of key design parameters and building elements in DenoMamba. First, we assessed the influence of the number of encoder-decoder stages K, the number of initial feature channels at the first encoder stage C, and the number of FuseSSM blocks cascaded at the last decoder stage R. Variant models were built by varying the values of K, C, and R while remaining parameters were kept fixed. Table I lists performance metrics of DenoMamba variants on the 25%-dose dataset, along with the number of model parameters. We find that variants for K = 4, C = 48, and R = 2 yield near-optimal performance, validating the proposed selection of design parameters."}, {"title": "A. Ablation Studies", "content": "We then assessed the influence of critical building elements in DenoMamba on denoising performance. Several ablated variants were formed for this purpose. A 'w/o cha. SSM' variant was formed by ablating the channel SSM module from FuseSSM blocks. A 'w/o spa. SSM' variant was formed by ablating the spatial SSM module from FuseSSM blocks. A 'w/o CFM' variant was formed by replacing the channel fusion module in FuseSSM blocks with a simple element-wise addition operator to combine contextual features from spatial/channel SSM modules with original input features. A \u2018w/o GCN' variant was formed by ablating the gated convolutional network in channel SSM modules that extracts latent contextual features across the channel dimension. A \u2018w/o Iden.' variant was formed by ablating the identity transfor-mation pathway in the CFM module that propagates original input features to the convolutional fusion layer. Table II lists performance metrics for DenoMamba and ablated variants on the 25%-dose dataset, along with the number of model parameters. We find that DenoMamba outperforms all ablated variants (p<0.05), indicating the important of these individual building elements for LDCT denoising performance."}, {"title": "B. Comparison Studies", "content": "Next, we demonstrated DenoMamba via comparisons against several state-of-the-art methods from the LDCT de-noising literature. Specifically, competing methods included CNN models (RED-CNN, N2N, EDCNN), transformer models (Uformer, LIT-Former), GAN models (WGAN, DU-GAN), an autoencoder model (QAE), and a diffusion model (IDDPM).\nExperiments were first conducted on the 25%-dose dataset to recover high-quality images from LDCT measurements. Table III lists performance metrics for competing methods separately for a test set comprising 1-mm thick slices, a test set comprising 3-mm thick slices, and an aggregated test set comprising both slice thicknesses. In each individual setting for the test set, we find that DenoMamba significantly outperforms all competing methods (p<0.05). On average, DenoMamba achieves performance improvements of 0.6dB PSNR, 0.2% SSIM, 0.6% RMSE over CNNs; 1.7dB PSNR, 0.7% SSIM, 1.5% RMSE over transformers; 2.8dB PSNR, 2.4% SSIM, 2.8% RMSE over GANS; 1.6dB PSNR, 1.2% SSIM, 1.5% RMSE over the autoencoder; 1.7dB PSNR, 0.5% SSIM, 1.4% RMSE over the diffusion model. Repre-sentative denoised images recovered by competing methods are displayed in Fig. 3. Among baselines, CNN models are generally effective in suppressing local noise patterns within homogeneous tissue regions, albeit they are suboptimal in alleviating noise patterns that extend over longer distances, occasionally inducing perturbations in tissue contrast deviating from ground truth NDCT images. Contrarily, transformers with their enhanced sensitivity to long-range context are better at preserving contrast across heterogeneous tissue regions, but they suffer from residual local noise patterns that manifest as signal intensity fluctuations in homogeneous tissue regions. Although GAN models maintain the highest visual sharpness, this comes at the expense of high levels of residual noise in recovered images. Meanwhile, the autoencoder model shows a degree of spatial blurring, and the diffusion model suffers from inaccuracies in tissue contrast and suboptimal noise suppression. In comparison to baselines, DenoMamba recovers high-quality CT images with reliable suppression of local and global noise patterns, and accurate depiction of tissue structure and contrast.\nWe also conducted experiments on the 10%-dose dataset to assess competing methods at a relatively more challenging de-noising task. Table IV lists performance metrics for competing methods separately for a test set comprising 1-mm thick slices, a test set comprising 3-mm thick slices, and an aggregated test set comprising both slice thicknesses. Corroborating the findings on the 25%-dose dataset, we find that DenoMamba significantly outperforms all competing methods consistently across all examined settings (p<0.05). On average, Deno-Mamba achieves performance improvements of 0.9dB PSNR, 0.5% SSIM, 1.1% RMSE over CNNs; 1.7dB PSNR, 2.1% SSIM, 2.1% RMSE over transformers; 2.3dB PSNR, 2.0% SSIM, 2.9% RMSE over GANs; 1.3dB PSNR, 0.,9% SSIM, 1.7% RMSE over the autoencoder; 1.6dB PSNR, 1.4% SSIM, 1.9% RMSE over the diffusion model. Representative denoised images recovered by competing methods are displayed in Fig. 4. Note that, at the higher 10% dose reduction level, additive Poisson noise becomes more prominent, yielding spatially-correlated perturbations in LDCT images comparable to native variations in tissue signals. Since distinguishing noise-driven perturbations from actual variations in tissue signals become more challenging, the difficulty of the LDCT denoising task is elevated. We observe that both CNNs and transformers start to suffer from residual noise patterns in locally homogeneous tissue regions, lowering structural accuracy particularly in the vicinity of tissue boundaries. As GAN models intend to maintain high-spatial frequency information in recovered images, they suffer from high levels of residual noise patterns. The autoencoder model shows visible residual noise. While the diffusion models yields generally higher image quality in comparison to other baselines, it suffers from a degree of structural inaccuracy in depiction of detailed tissue structures. Contrarily, DenoMamba offers high-fidelity depiction of de-tailed tissue structure in CT images with visibly improved noise suppression."}, {"title": "VI. DISCUSSION", "content": "In this study, we introduced a novel denoising method to recover high-quality NDCT images from noisy LDCT im-ages. Previous CNN models offer computationally efficiency denoising and higher local precision, albeit they are rela-tively insensitive to long-range relationships between distant anatomical regions in medical images [48]. While transformer models can address this limitations by leveraging the long-range contextual sensitivity of the self-attention mechanism, they inherently suffer from quadratic model complexity with respect to sequence length, which common approaches to alleviate this complexity result in inevitable losses in spatial precision [72]. Differently from these previous models, Deno-Mamba employs novel FuseSSM blocks to capture contextual features via state-space modeling across spatial and channel dimensions, while enjoying similar computational efficiency to CNN models. Our demonstrations indicate that DenoMamba achieves superior performance in LDCT denoising against state-of-the-art CNN and transformer methods, with apparent quantitative and qualitative benefits in recovered CT images.\nSeveral technical limitations can be addressed in order to further boost the performance and practicality of DenoMamba. A first line concerns the nature of denoising tasks used for model training. Here, a separate model was trained for LDCT denoising at each reduction level for radiation dose to maintain high performance. Note that this may lower practicality if highly variable reduction levels are expected to be adminis-tered in practice. In those cases, DenoMamba can be trained on LDCT images at varying reduction levels, and model specialization to specific radiation doses could be enhanced by adaptive normalization approaches on feature maps [73]\u2013[75]. This could improve practicality by building a unified model that can be deployed at various dose reduction levels.\nA second line of improvements concerns the datasets on which DenoMamba is trained to perform LDCT denoising. Here, we performed supervised learning relying on the avail-ability of paired LDCT-NDCT images from the same set of subjects [76]. Note that, in practice, the curation of such paired datasets can be challenging as it would require repeated CT scans on a given subject at separate radiation doses. In cases where the amount of paired training data that can be collected is limited, a large training set can be curated by instead adopting cycle-consistent learning procedures on unpaired sets of LDCT and NDCT images [77], [78], or self-supervised learning procedures to train models directly on LDCT measurements [54], [67]. Alternatively, models can be pre-trained for denoising in a domain with abundant data (e.g., natural images), and then transfered to the LDCT domain via fine-tuning [79].\nA third line of improvements concerns the loss terms em-ployed to train DenoMamba. Here, we utilized a simple pixel-wise loss term based on mean-squared error, since we observed that this pixel-wise loss offered effective learning of LDCT denoising models on the examined datasets. That said, it might be possible to attain further improvements in recovered image quality by using more sophisticated loss terms in-cluding adversarial, score-based or cross-entropy losses [80]\u2013[84]. Particularly within the context of score-based methods that involve iterative sampling procedures, the long-range contextual sensitivity of DenoMamba might offer benefits over conventional score-based models based on CNN backbones. Further work is warranted for a systematic evaluation of"}, {"title": "VII. CONCLUSION", "content": "Here we introduced a fused state-space model (SSM) for recovery of high-quality images from noisy LDCT measure-ments. The proposed DenoMamba model leverages an hour-glass architecture implemented with novel FuseSSM blocks. Each FuseSSM block extracts contextual features across spa-tial and channel dimensions via spatial and channel SSM modules, respectively, and performs fusion of contextual and original input features via a CFM module. This design enables DenoMamba to leverage contextual relationships in LDCT images without elevating computational burden, and to offer superior performance against state-of-the-art LDCT denoising methods. Therefore, DenoMamba holds great promise for performant and efficient LDCT image denoising."}]}