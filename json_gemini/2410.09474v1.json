{"title": "Distilling Invariant Representations with Dual Augmentation", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "abstract": "Knowledge distillation (KD) has been widely used to\ntransfer knowledge from large, accurate models (teachers)\nto smaller, efficient ones (students). Recent methods have\nexplored enforcing consistency by incorporating causal in-\nterpretations to distill invariant representations. In this work,\nwe extend this line of research by introducing a dual aug-\nmentation strategy to promote invariant feature learning\nin both teacher and student models. Our approach lever-\nges different augmentations applied to both models during\ndistillation, pushing the student to capture robust, transfer-\nable features. This dual augmentation strategy complements\ninvariant causal distillation by ensuring that the learned\nrepresentations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on\nCIFAR-100 demonstrate the effectiveness of this approach,\nachieving competitive results in same-architecture KD\u00b9.", "sections": [{"title": "1. Introduction", "content": "Knowledge Distillation (KD) is a widely used model\ncompression technique, where a large, pre-trained model\n(teacher) transfers its knowledge to a smaller, more efficient\nmodel (student) [17]. The core principle of KD is to enable\nthe student model to mimic the behavior of the teacher by\nminimizing the divergence between their output distributions.\nInitially, this process focused on matching the softmax pre-\ndictions of the teacher and student, but this approach has\nevolved significantly over time.\nDespite these advances, several limitations persist in tra-\nditional KD methods. A major challenge lies in the student\nmodel's inability to capture the deeper, structural knowledge\nembedded within the teacher's representations, especially\nwhen the two models have significantly different architec-\ntures [41, 42]. Standard KD often leads to overfitting on\nthe teacher's outputs, with the student failing to develop ro-"}, {"title": "2. Related Work", "content": "Knowledge distillation. KD, introduced by [17], has\nbeen a subject of extensive research in recent years. Var-\nious extensions and improvements have been proposed to\nenhance the effectiveness of KD. FitNets [33] utilized inter-\nmediate layer outputs as \"hints\" to guide the student's learn-\ning process, while [47] focused on aligning attention maps\nbetween teacher and student models. Approaches such as\nthose proposed by [42] and [27] aimed to preserve relational\ninformation between samples during distillation. Addition-\nally, [31] proposed aligning correlation structures between\nteacher and student representations. Recent advancements\nin KD include label decoupling [50], probability reweight-\ning [26], and logit normalization [40]. These methods have\nshown promising results in improving the efficiency and ef-\nfectiveness of knowledge transfer. The continuous evolution\nof KD techniques demonstrates the ongoing effort to capture\nand transfer complex knowledge structures from teacher to\nstudent models.\nContrastive learning in knowledge distillation. Con-\ntrastive learning has emerged as a powerful tool in self-\nsupervised learning, demonstrating its ability to learn robust\nrepresentations by maximizing mutual information [3,18,43].\nThis concept has been successfully adapted to the KD frame-\nwork, with Contrastive Representation Distillation (CRD)\n[41] introducing contrastive learning to KD by maximizing\nmutual information between teacher and student represen-\ntations. Further developments in this area include the work\nof [7], who proposed a contrastive intra-batch loss to enhance\nKD performance. Our work builds upon these advancements,\nparticularly the Invariant Causal Distillation (ICD) frame-\nwork [11], which combined contrastive learning with an ex-\nplicit invariance penalty. By integrating contrastive learning\nprinciples with KD, these approaches aim to capture more\nnuanced and structural information during the knowledge\ntransfer process.\nCausal inference in knowledge distillation. Causal\ninference has emerged as a key theoretical foundation for\nenhancing machine learning models, including KD. It is ar-\ngued that models should focus on learning the underlying\ncausal mechanisms of the data rather than purely statistical\ncorrelations [36]. In KD, this has led to approaches that aim\nto distill causal, invariant representations. One framework\ngrounded in causal inference ensures that learned representa-\ntions remain invariant under changes in data conditions [24].\nSimilarly, causal principles have been applied in KD frame-\nworks to enforce consistency in knowledge transfer by en-\ncouraging the student to capture invariant features that are\nrobust across different environments [11]. These approaches\nillustrate the growing role of causal reasoning in designing\nmore effective and generalizable KD methods.\nData augmentation in knowledge distillation. Data\naugmentation has been widely used in machine learning to\nimprove model generalization [38]. In the context of KD,\nseveral works have explored the use of augmentation to en-\nhance the distillation process. For instance, [45] proposed a\nmethod to leverage unlabeled data through augmentation in\nKD, expanding the effective dataset size and diversity. Simi-\nlarly, [22] introduced a self-supervised auxiliary task using\naugmented views to enhance KD performance, demonstrat-\ning the potential of augmentation in creating more robust\nand generalizable student models. Our ICDA approach dif-\nferentiates itself by employing a dual augmentation strategy,\napplying distinct augmentations to teacher and student in-\nputs within the ICD framework. This strategy aims to further\nenhance the invariance and robustness of the learned repre-\nsentations, pushing the boundaries of what can be achieved\nthrough the combination of data augmentation and KD."}, {"title": "3. Methodology", "content": "This section presents our approach to enhance the ef-\nfectiveness and precision of KD through Invariant Causal"}, {"title": "Knowledge Distillation with Dual Augmentation (ICDA).", "content": "Our method introduces a unique dual augmentation strat-\negy, applying different transformations to inputs for teacher\nand student models, thereby improving the robustness and\ngeneralization of the learned representations."}, {"title": "3.1. Background", "content": "Problem definition. Let X represent the dataset and\ny = {Y\ud835\udc61}\ud835\udc47\ud835\udc61=1 denote a set of potential downstream tasks,\nwhere Y\ud835\udc61 corresponds to the objective of task t. In our KD\nframework, a teacher network \ud835\udc53\ud835\udc47 transfers knowledge to a\nmore compact student model \ud835\udc53\ud835\udc46. The aim is for the student\nto not only replicate the teacher's capabilities but also acquire\nversatile representations from X applicable to various tasks\nY\ud835\udc61 [17,41].\nKnowledge distillation framework. For input samples\n\ud835\udc65\ud835\udc56 \u2208 X, i = 1, . . . , N, we define the outputs of the penul-\ntimate layers as \ud835\udc67\ud835\udc47\ud835\udc56 = \ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc56) and \ud835\udc67\ud835\udc46\ud835\udc56 = \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc56) for the\nteacher and student models respectively. The distillation\nprocess is quantified by:\n\ud835\udc3f = \ud835\udc3f\ud835\udc60\ud835\udc62\ud835\udc5d(\ud835\udc66\ud835\udc56, \ud835\udc67\ud835\udc46\ud835\udc56) + \ud835\udf06 \u00b7 \ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc59(\ud835\udc67\ud835\udc47\ud835\udc56, \ud835\udc67\ud835\udc46\ud835\udc56) (1)\nHere, \ud835\udc66\ud835\udc56 is the ground truth label for \ud835\udc65\ud835\udc56, and \ud835\udf06 is a hy-\nperparameter balancing supervised and distillation losses.\n\ud835\udc3f\ud835\udc60\ud835\udc62\ud835\udc5d quantifies the task-specific alignment error, often imple-\nmented as cross-entropy for classification [9] or bounding\nbox regression for object detection [6]. \ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc59 evaluates\nthe degree of mimicry between the student and teacher, fre-\nquently using KL divergence [17,41]."}, {"title": "3.2. Theoretical Basis", "content": "Our theoretical motivation builds upon [11]. However, we\nextend their approach by introducing a dual augmentation\nstrategy that applies distinct transformations to the inputs of\nboth teacher and student models, similar to [24].\nInvariant representation through causal inference. To\naddress the limitations of traditional KD, we formalize data\ngeneration assumptions using causal principles [30,37]. Let\nY = {Y\ud835\udc61}\ud835\udc47\ud835\udc61=1 be unknown downstream tasks. We assume:\nimages \ud835\udc65\ud835\udc56 \u2208 X are generated from content C and style S\nvariables; only C is relevant for Y; C and S are independent.\nIn our causal model, C and S generate \ud835\udc65\ud835\udc56, with C directly\ninfluencing Y. Using the principle of independent mech-\nanisms [35, 37], interventions on S do not alter P(Y\ud835\udc61|C).\nThus, P(Y|C) is invariant under changes in S. We define\nC as an invariant representation for Y\ud835\udc61 under S:\nP(Y\ud835\udc61|C)\ud835\udc46=\ud835\udc60\ud835\udc56 = P(Y\ud835\udc61|C)\ud835\udc46=\ud835\udc60\ud835\udc57 \u2200\ud835\udc60\ud835\udc56, \ud835\udc60\ud835\udc57 \u2208 S,\n(2)\nwhere P\ud835\udc46=\ud835\udc60 is the distribution when S = s, and S is S's\ndomain. This formulation allows us to disentangle the task-\nrelevant content from style variations, leading to more robust\nand generalizable representations. By incorporating these\ncausal principles into our KD framework, we aim to transfer\nnot just the knowledge of the teacher, but also its ability to\nidentify invariant features.\nInvariance via dual augmentations. As Y\ud835\udc61 are un-\nknown during training, we create a proxy task Y\ud835\udc45 to learn\nrepresentations from X. To transfer invariant representa-\ntions from \ud835\udc53\ud835\udc47 to \ud835\udc53\ud835\udc46, we enforce Eq. 2 using content-\npreserving data augmentations as proxies for style inter-\nventions [25, 44]. ICDA introduces dual augmentation\nsets: A1 = {\ud835\udc4e1,1, . . . , \ud835\udc4e1,\ud835\udc5a} for the teacher and A2 =\n{\ud835\udc4e2,1, . . . , \ud835\udc4e2,\ud835\udc5b} for the student. This approach allows us to\nsimulate a wider range of style interventions, encouraging\nthe student to learn representations that are invariant across\ndiverse transformations. The representations \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc56) and\n\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc56) must fulfill:\nP(Y\ud835\udc45|\ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc56))\ud835\udc4e1,\ud835\udc58 = P(Y\ud835\udc45|\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc56))\ud835\udc4e2,\ud835\udc59 (3)\nfor all \ud835\udc4e1,\ud835\udc58 \u2208 A1 and \ud835\udc4e2,\ud835\udc59 \u2208 A2, where Pa denotes \ud835\udc43\ud835\udc60|\ud835\udc60=\ud835\udc4e.\nWe enforce invariance under augmentations through a reg-\nularizer, similar to [24]. Let \ud835\udc3f\ud835\udc4f,\ud835\udc56 be the loss function for\naugmentation b applied to the i-th sample:\n\ud835\udc3f\ud835\udc4f,\ud835\udc56 = \ud835\udc3f\ud835\udc4f(Y\ud835\udc45|\ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ), \ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 ))\n(4)\nwhere \ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 and \ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 are augmented versions of \ud835\udc65\ud835\udc56 using\n\ud835\udc4e1,\ud835\udc58 \u2208 A1 and \ud835\udc4e2,\ud835\udc59 \u2208 A2. Our objective is:\n\ud835\udc38\ud835\udc65\ud835\udc56\ud835\udc38\ud835\udc4e1,\ud835\udc58,\ud835\udc4e2,\ud835\udc59 \u03a3\ud835\udc4f\u2208{\ud835\udc4e1,\ud835\udc58,\ud835\udc4e2,\ud835\udc59} \ud835\udc3f\ud835\udc4f,\ud835\udc56 (5)\nsubject to:\n\ud835\udc37\ud835\udc3e\ud835\udc3f[P(Y\ud835\udc45|\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 )), P(Y\ud835\udc45|\ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ))] \u2264 \ud835\udf0c\n(6)\nwhere \ud835\udc37\ud835\udc3e\ud835\udc3f is the KL divergence loss, and \ud835\udf0c controls the\ndivergence between teacher and student predictions. This\nformulation encourages the student to learn representations\nthat are consistent across different augmentations, mimick-\ning the teacher's ability to identify invariant features. By\ndoing so, we aim to improve the student's generalization\ncapabilities beyond the specific augmentations seen during\ntraining.\nContrastive instance discrimination. To further en-\nhance the learning of invariant representations, we extend\ninstance discrimination with causal refinements [12,46]. Our\nproxy task assigns each \ud835\udc65\ud835\udc56 \u2208 X a unique y = i. We en-\ncourage similarity between \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ) and \ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 ), using\naugmentation pairs to simulate style interventions:\nP(Y\ud835\udc45 = \ud835\udc57 | \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 )) \u221d exp($(\ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ), \ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 ))/\u03c4),\n(7)"}, {"title": "3.3. Invariance via Dual Augmentation", "content": "Building upon the theoretical foundations, we develop\nan objective function that enforces consistency between the\nteacher's and student's outputs:\n\ud835\udc3f\ud835\udc58\ud835\udc51 = \ud835\udc3f\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc61 + \u03b1 \ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 (8)\nwhere \u03b1 balances \ud835\udc3f\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc61. \ud835\udc3f\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc61 = \ud835\udc3f\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc61 (\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 , \ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ) and\n\ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 = \ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 (\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 , \ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ). The contrastive loss is\ndefined as:\n\ud835\udc3f\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc60\ud835\udc61 \u2212 log (exp($(\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 ), \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ))/\u03c4)/\u03a3\ud835\udc40\ud835\udc57=1 exp($(\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 ), \ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ))/\u03c4))\n(9)\nwhere \u03c6 is cosine similarity, \u03c4 is a temperature parameter,\nand M is the number of negative samples [8, 13,43]. This\ncontrastive loss encourages the student to learn representa-\ntions that are similar to the teacher's for the same instance\nunder different augmentations, while being dissimilar for\ndifferent instances. This helps in learning features that are\ninvariant to style changes but discriminative for content.\nTo enforce invariance, as in [11], we minimize the KL\ndivergence between the distributions of teacher and student\nrepresentations:\n\ud835\udc3f\ud835\udc56\ud835\udc5b\ud835\udc63\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 = \ud835\udc37\ud835\udc3e\ud835\udc3f(p(\ud835\udc53\ud835\udc47(\ud835\udc65\ud835\udc4e1,\ud835\udc58\ud835\udc56 )) || p(\ud835\udc53\ud835\udc46(\ud835\udc65\ud835\udc4e2,\ud835\udc59\ud835\udc56 ))) (10)\nThis term ensures that the student's representations follow\na similar distribution to the teacher's, even under different\naugmentations. By doing so, we encourage the student to\ncapture the underlying invariant structure of the data, rather\nthan superficial correlations that may not generalize well.\nThe final objective function combines the supervised loss,\nstandard distillation loss, and our proposed ICDA loss:\n\ud835\udc3f = \ud835\udc3f\ud835\udc60\ud835\udc62\ud835\udc5d + \u03bb \u00b7 \ud835\udc3f\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc59 + \u03b2 \u00b7 \ud835\udc3f\ud835\udc58\ud835\udc51 (11)\nwhere \u03b2 balances \ud835\udc3f\ud835\udc58\ud835\udc51 [32,41]."}, {"title": "4. Experiments", "content": "Our proposed ICDA framework is assessed in the context\nof KD for network compression, where we transfer knowl-\nedge from a larger model to a more compact one, similar\nto [41]."}, {"title": "5. Conclusions", "content": "This paper introduces an approach to KD that enhances\nthe process by combining causal learning principles with a\ndual augmentation strategy. Our method applies distinct\naugmentations to teacher and student models, encourag-\ning the student to learn robust, invariant representations\nacross a wide range of data variations. Extensive experi-\nments on benchmark datasets, including CIFAR-100, TIN-\n200, and STL-10, demonstrate that our approach achieves\nstate-of-the-art performance in same-architecture KD set-\ntings and exhibits superior generalization capabilities on\nout-of-distribution tasks. Our comprehensive ablation study\nreveals that simpler augmentations for the teacher model,\ncombined with more diverse augmentations for the student,\nlead to the most effective knowledge transfer and generaliza-\ntion. While our method shows slight performance limitations\nin cross-architecture distillation settings, it remains competi-\ntive with existing methods."}]}