{"title": "INCREASING FAITHFULNESS IN HUMAN-HUMAN DIALOG SUMMARIZATION WITH SPOKEN LANGUAGE UNDERSTANDING TASKS", "authors": ["Eunice Akani", "Benoit Favre", "Frederic Bechet", "Romain Gemignani"], "abstract": "Dialogue summarization aims to provide a concise and coherent summary of conversations between multiple speakers. While recent advancements in language models have enhanced this process, summarizing dialogues accurately and faithfully remains challenging due to the need to understand speaker interactions and capture relevant information. Indeed, abstractive models used for dialog summarization may generate summaries that contain inconsistencies. We suggest using the semantic information proposed for performing Spoken Language Understanding (SLU) in human-machine dialogue systems for goal-oriented human-human dialogues to obtain a more semantically faithful summary regarding the task. This study introduces three key contributions: First, we propose an exploration of how incorporating task-related information can enhance the summarization process, leading to more semantically accurate summaries. Then, we introduce a new evaluation criterion based on task semantics. Finally, we propose a new dataset version with increased annotated data standardized for research on task-oriented dialogue summarization. The study evaluates these methods using the DECODA corpus, a collection of French spoken dialogues from a call center. Results show that integrating models with task-related information improves summary accuracy, even with varying word error rates.", "sections": [{"title": "1 Introduction", "content": "Advancements in spoken language processing, made possible by low word-error-rate automatic speech transcription and generative language models, have enhanced the quality of outputs across various tasks, including dialog summarization. Dialog summarization involves generating a concise, abstract rendition of the dialogue exchanged among speakers in a multi-party spoken interaction. Due to discrepancies between transcript and summary styles, inaccuracies in the transcript, verbal hesitations by participants, and the generative process itself, dialog summarization is susceptible to \"hallucinations\", wherein generative models produce text containing nonfactual information when compared to the source.\nA study by Cao et al. [2018] found that 30% of summaries generated by text summarization systems contained incorrect information, known as \"hallucinations\" Maynez et al. [2020]. Approaches to assessing summary faithfulness include textual entailment Falke et al. [2019], Maynez et al. [2020], entity analysis Nan et al. [2021], Ji et al. [2023], and question-answer verification Durmus et al. [2020]. Dialog summarization is growing in popularity but is challenged by dialog structure and diverse data types, such as customer service and technical discussions. Methods using auxiliary information like dialogue acts Goo and Chen [2018] or domain terminology Koay et al. [2020] have been proposed. There are fewer studies on hallucinations in dialog summarization compared to text. Wang et al. [2022] found that 35% of SAMSum dataset summaries were inconsistent with the source dialogues. Tang et al. [2022] identified eight types of factual errors in dialogue summarization, while Wang et al. [2022] identified six types and used a summary model based on conditional generation probabilities to evaluate the model's faithfulness."}, {"title": "2 Integration of task-related information in spoken dialog summarization", "content": "On goal-oriented human-human conversations, we can leverage semantic information proposed for performing SLU in human-machine dialogue systems for tasks such as transportation reservations [Hemphill et al., 1990] or restaurants/hotels [Devillers et al., 2004]. Typically, here, three semantic levels are defined: domain (the semantic context of the dialogue, such as flight bookings), intent (the nature of the request made in the dialogue, like confirmation or information inquiry), and entity/value pairs (the semantic relationships within intents, such as a destination in an itinerary request). As we focus on goal-oriented, we put ourselves in a customer service conversation. Thus, the call type can be seen as an intent in SLU and the domain named entities as a slot. We hypothesize that by incorporating task-specific semantic information into the summarization process, we can effectively address inconsistencies related to this information in the data. In this section, we will focus on the call type and entity level. Using call type and named entities is an excellent choice for our study as it is easily obtained and leveraging named entities provides a practical framework for our methodology. Nevertheless, other semantic information, such as semantic frames can be used."}, {"title": "2.1 Summarization based on task information", "content": "Identifying different call types and concept labels is essential for analyzing conversation recordings or transcripts, but this information often needs to be inferred. This can be done by implementing a separate prediction system on the transcript and integrating call types into the input data of the summarization system. The method is Pipeline: Predict semantic information C from dialog D such as C = intent(D), then condition summary generation on C: S = summary(D, semantic-information(D))"}, {"title": "2.2 Task-related information as a selection criteria", "content": "We propose using semantic information to select the most reliable summary from different summaries obtained by tweaking decoding parameters such as top-p. The method is based on call type prediction and the hallucination risk on entities.\nCall type Based on the hypothesis that the generated summary should have the same call type as the dialog transcription, we suggest using call type as a summary selection criterion. Our method involves using a text classifier to predict the call type of the generated summary and comparing it to the predicted call type for the entire transcript. We handle multiple call types and uncertainty in call type classification by calculating the divergence between the probability distribution on all call types for both summary and dialog classifiers using the KL divergence [Kullback and Leibler, 1951]. For G and R the probability distribution given by the call type classifier on the generated summary and the entire dialog respectively, CT the set of call types; the KL divergence between G and R is defined as follows:\n$D_{KL}(G || R) = D_{KL}(G || R) = \\sum_{x \\in CT} G(x) log (\\frac{G(x)}{R(x)})$\nThis allows us to choose the summary that minimizes the DKL among the generated summaries.\nNamed entities We propose using the NEHR criterion [Akani et al., 2023], which measures the proportion of named entities not present in the source document, as a selection criterion for creating faithful summaries. This approach has shown favorable results in reducing hallucinated entities in text summarization. Thus, we decided to apply the same experiment to dialog summarization."}, {"title": "2.3 Task-related information as an evaluation criteria", "content": "We propose a metric CT-Acc, which measures the accuracy of the call type classifier used on the generated summary compared to the reference call type. The assumption is that the summary is more coherent with the call type classifier if the accuracy is higher.\nWe propose using a metric based on named entities to assess the coherence of generated summaries compared to reference summaries. The metric measures entity-level accuracy and aims to ensure that both summaries contain the same entities. We measure NE-P, NE-R, and NE-F1 metrics to represent precision, recall, and F1 between entities in the generated and reference summaries. Our approach differs from previous work Nan et al. [2021] as we include all types of entities, such as dates and numerals, which we believe are crucial in some scenarios."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 The DECODA Corpus", "content": "DECODA-v1-hum [Bechet et al., 2012] contains spoken conversations between agents of the Paris Transport Authority and users of Paris buses and metro lines. Each conversation has a manual text transcription and a short summary. The corpus covers various call types such as Itinerary, includes entities from a domain ontology such as Transport, and Schedule, and consists of three parts (DECODA-1, DECODA-2, DECODA-3) with annotated synopses available only in DECODA-1 and DECODA-3. Since DECODA-1 summaries are written in a non-literary style and with little detail, we decided to keep only those in DECODA-3 for system development. From the 500 dialogues of the DECODA-3, we kept 200 dialogues as an evaluation set (test), 200 for fine-tuning the system (Hum. for human annotated data) and 100 as a validation set to adjust the parameters. As not all DECODA summaries are written in a literary style and are sometimes non-existent for certain conversations, we create a new dataset with more annotated data for task-oriented dialog summarization."}, {"title": "3.2 Summarization and classification models", "content": "Automatic Summarization We trained the summarization systems using BARThez Kamal Eddine et al. [2021], a sequence-to-sequence model pre-trained on various French corpora introduced for automatic text summarization. It is a transformer-based model built on BART architecture Vaswani et al. [2017], Lewis et al. [2020]. It consists of 6 encoder and decoder layers. We used the pre-trained model provided by the authors available on the Hugging Face library2 to train the models.\nCall type classification Two classifiers based on CamemBERT-base Martin et al. [2020] were trained to classify the call-type: one taking as input automatic dialog transcripts (Conv-CT-classifier) and the other one, generated synopses (Syn-CT-classifier). Conv-CT-classifier is also used to predict the call type for the Pipeline model."}, {"title": "3.3 Data augmentation and ASR evaluation", "content": "We compute ROUGE, BERTScore, and the semantic score of text summarization systems trained on the different corpus versions to validate our data augmentation strategy using LLMs and ASR systems. We report all results in table 2. For automatic transcription, we report result for two different sizes of WhisperX (Tiny-T and Large-L) and ChatGPT- 3.5 one short on WhisperX Large transcription. The findings indicate that data augmentation enhances summary scores while semantic scores remain constant, with a slight increase in NE-F1. In terms of automatic transcription, several metrics declined. However, the scores of the large model continue to be competitive, suggesting that the ASR system could be a viable option. ChatGPT-3.5 achieves the highest semantic scores in both manual and automatic transcriptions. This is unsurprising, considering the model's numerous parameters and the extensive training data it has received. In next section, only DECODA-v2-auto with WhisperX large transcription will be used for experiments."}, {"title": "3.4 Impact of selection criterion semantic criteria", "content": "We trained Pipeline, the model that conditions the generation of the summary by the call type concatenated to the dialog using a separtor. From this model, we generated various summaries using sampling decoding strategies and select the one according to the selection metrics presented in section 2.2. We report automatic results in table 3 and denote as min DKL for summaries selected using DKL and min NEHR for summaries selected using NEHR and min NEHR+ DKL for the combined criterion. We report the result for Pipeline model and the baseline model without selection. For Pipeline, we have similar scores while CT-Acc vary. We can see that DKL-based selection increases CT-Acc, while NEHR-based selection increases both values. The combined criteria seems to have the same result than DKL-based selection."}, {"title": "4 Discussion and Conclusion", "content": "We analyzed the influence of task-semantic information on the generation of dialog summaries. Our findings revealed that incorporating NEHR in this model enhances named entity precision. Nonetheless, manual evaluation is necessary to validate the effectiveness of this selection criterion in terms of fidelity and informativeness. In addition, we proposed new version of a dataset with more annotated data for task-oriented dialog summarization. Evaluation of the dataset revealed that using LLM to generate a portion of training synopses improved the model's performance, as indicated by increased ROUGE score and BERTScore, and automatic transcriptions can serve as a viable alternative to address the lack of annotated data."}, {"title": "A Details about DECODA dataset", "content": null}, {"title": "B Prompt used to generate summary", "content": "To generate summary from transcription using ChatGPT-3.5, we used the prompt (translated from French) and one- shot example in table 5."}, {"title": "C Call type classification systems", "content": "We report the call type classifier used to generate the call type of the Pipeline system and to perform the DKL-based selection in table 6"}, {"title": "D All WhisperX size results", "content": "We report in the table 7 all the results obtained by training text summarization system on different whisperX size."}, {"title": "E Sampling parameters", "content": "Top-P: P\u2208 [0.70, 0.95[ with the step of 0.05\nTop-K: K \u2208 [30, 100[ with the step of 15\nTemperature: T \u2208 [0.7, 1] with the step of 0.1\nGreedy decoding"}, {"title": "F Selection results", "content": "We report in table 8 all the scores obtained for summary selection. We include here, the result obtained when using the whisperX base size as automatic transcription system."}]}