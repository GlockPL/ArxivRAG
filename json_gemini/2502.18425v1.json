{"title": "PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback", "authors": ["Nils Wandel", "David Stotko", "Alexander Schier", "Reinhard Klein"], "abstract": "Grading student assignments in STEM courses is a laborious and repetitive task for tutors, often requiring a week to assess an entire class. For students, this delay of feedback prevents iterating on incorrect solutions, hampers learning, and increases stress when exercise scores determine admission to the final exam.\nRecent advances in AI-assisted education, such as automated grading and tutoring systems, aim to address these challenges by providing immediate feedback and reducing grading workload. However, existing solutions often fall short due to privacy concerns, reliance on proprietary closed-source models, lack of support for combining Markdown, LaTeX and Python code, or excluding course tutors from the grading process.\nTo overcome these limitations, we introduce PyEvalAI, an AI-assisted evaluation system, which automatically scores Jupyter notebooks using a combination of unit tests and a locally hosted language model to preserve privacy. Our approach is free, open-source, and ensures tutors maintain full control over the grading process. A case study demonstrates its effectiveness in improving feedback speed and grading efficiency for exercises in a university-level course on numerics.", "sections": [{"title": "1 Introduction", "content": "Weekly student assignments play an important role in STEM (Science Technology Engineering Mathematics) education as they help students to apply curriculum content in practice. Furthermore, exercise sheets motivate students to learn on a regular basis and are a good predictor for the final exam performance [6,19,21]. A widely used option to create exercise sheets are Jupyter notebooks [1,23,28], since they provide an easy way to combine Markdown and Latex instructions with interactive Python code. However, grading and providing feedback on student assignments is a cumbersome and repetitive task for teaching assistants, typically taking around a week to complete for an entire class. For students, such a delay of feedback prevents iterating on incorrect solutions and hampers the learning experience.\nOn the other hand, recent breakthroughs in large language models are currently transforming traditional education in groundbreaking ways. LLM-based"}, {"title": "2 Related Work", "content": "Commercial Tools of numerous educational start-ups nowadays incorporate AI-based features in their products. For example, Khanmigo by Khan academy [18] offers students interactive learning experiences by chatting with historical figures or by giving hints on questions without giving away the answer. Personify [13] allows teachers to create tailored teaching assistants for their courses. my- \u03a4\u0391\u0399 [32] helps teachers create teaching materials, and tools like gotFeedback [12], Fellofish [7] or Fobizz [8] help with automatic grading and feedback for writing exercises. Cocalc [33] and Vocareum [38] integrate generative AI into collaborative notebooks to help students. However, commercial tools usually outsource the AI to external providers such as OpenAI potentially leading to conflicts with privacy policies of universities. Furthermore, licenses can be quite expensive for lower-budget universities and students. Finally, it is important to keep tutors in charge of the final grades since the quality of AI-only feedback without human oversight is often still insufficient [27]."}, {"title": "3 PyEvalAI", "content": "The core of PyEvalAI consists of a Tornado server. As shown in Figure 1, this server provides a front-end for students, tutors and administrators (more information is given in Section 3.1) and coordinates different back-end services such as user authentification, data-storage, unit-tests or the local LLM to generate automated feedback (see Section 3.2)."}, {"title": "3.1 User-Interface / Front-end", "content": "The user interface of PyEvalAI allows for two different kinds of interactions: First, a web-interface for tutors and students to review grades, and second, Jupyter Notebooks for students and administrators to work on, hand in and register exercises on the server via WebSockets [24]. In the following sections, we explain the different interfaces for students, tutors and administrators in detail.\nStudent Interface: Assignment sheets are given to students in the familiar form of Jupyter notebooks. Figure 2 shows how students can access all the necessary functionality to hand in exercises through the pip package pyevalai . This module provides a simple API with functions like login(\"server.url\") for signing in, entering a course via enter_course(\"course name\") and handing in exercises via handin_exercise(\"exercise name\", solution). Depending on the exercise type, students can hand in text solutions given as a string or code solutions given as a python function. After handing in an exercise, PyEvalAl needs about 1-2 minutes (depending on the exercise type and current load of the server) to generate feedback and compute a grade which is directly displayed inside the Jupyter Notebook. The handin_exercise call is asynchronous, allowing students to proceed working on subsequent exercises without having to wait for the feedback of prior exercises to be computed.\nTo get an overview of all exercises from all exercise sheets together with the achieved grades, numbers of attempts and deadlines in one place, students can visit the PyEvalAI website (see Figure 4). By selecting a specific exercise, students can recap the task assignment, all handed-in solution attempts and view corresponding feedback and grades provided by the LLM and tutors (see Figure 6).\nTutor Interface: PyEvalAI provides tutors with a comprehensive table that contains grades for all students and all exercises (see Figure 5). This allows to immediately identify exercises with poor grades that need special attention in subsequent tutorials. By selecting a table entry, tutors can view the task assignment, the handed-in solution attempts by the student as well as the corresponding achieved grades and feedback (see Figure 7). If the AI feedback is inadequate, tutors can easily rectify incorrect grades and adjust the feedback.\nAdministrator Interface: Course administrators have special privileges to register new exercises or remove existing exercises from a course. Typically, this is the role of a teaching assisstant or the professor who supervises the tutors. The interface for registering new exercises or removing existing ones is similar to the student interface for handing in exercises. As shown in Figure 3, administrators create a Jupyter Notebook, import the pyevalai module, login with their credentials and enter the course they want to work on. Administrators can then register new exercises or update existing exercises with register_exercise()"}, {"title": "3.2 Assignment grading / Back-end", "content": "The back-end of PyEvalAI consists of multiple components that are coordinated by a Tornado server. Tornado is a lightweight and open source python web framework that can handle many concurrent connections efficiently. For user authentication, the \"Lightweight Directory Access Protocol\" (LDAP) is used to check login credentials of students, tutors and administrators affilitated with the university. The datastorage consists of a Pickle file which is updated whenever new data comes in and loaded whenever the server restarts. This simple approach yields sufficient performance and facilitates handling the data in python for later evaluation and dataset generation tasks. To grade the handed in solutions, unit tests and a large language model come into play, which we'll discuss in the following sections in more detail.\nLarge Language Model (LLM) To ensure data privacy for the students, the language model is hosted locally on a single NVidia A100 GPU on an in-house GPU server. For inference, we rely on the \"Text Generation Inference\" (TGI) toolkit by Huggingface [14] as it provides fast and efficient text generation and provides the commonly used OpenAI API for accessing text generation. This gives us full flexibility for different models that may come up in the future.\nDuring the initial development phase, we tested several models such as quantized versions of LLaMa [5] and distilled versions of Deepseek [4] but found that an AWQ-quantized version [22] of Mistral Large [26] produced qualitatively the most convincing results while running at acceptable inference speed. Using Mistral Large, PyEvalAI took 88.2 seconds on average to compute feedback for a submission and around half of all responses were computed within 1 minute.\nTo grade exercises and generate feedback, we first provide the model with the task description, a sample solution and the student solution. The LLM is then prompted with optional (unit-) test questions, which are described in more detail in the next section, to check for correctness and award partial points. Then, we ask the LLM to carefully compare the sample and student solution and comment on the severeness of detected differences. Furthermore, we ask the LLM if critical steps from the sample solution were ommited and, if so, request details on what was missed. Finally, the LLM is tasked with determining a final score based on these insights and generating a concise, encouraging feedback message that explains the points awarded and any deductions. To achieve deterministic outputs, we use greedy sampling.\nThis step-by-step approach, combined with a detailed comparison between the sample and student solutions, generates precise and constructive feedback for the student and ensures that feedback is both comprehensive and actionable, giving students clear insights into their work and concrete steps for improvement.\nUnit-Tests By specifying unit tests (see Figure 3), we can provide additional input for the LLM to improve grading accuracy. There are 2 types of tests supported in pyevalai:\n1. Text: These tests consist of yes-no questions and corresponding points that should be granted if the LLM replies with yes or no (for example \"Is the student cheating by using prohibited libraries in his code?\" or \"Did the"}, {"title": "4 Evaluation", "content": "To validate PyEvalAI, we conducted a case study in a university level numerics course where volunteer Bachelor of Science in Computer Science students used interactive sample questions to prepare for the course exam. To obtain quantitative results about the AI's grading accuracy, we manually checked and rectified the AI generated feedback for all handed in solutions. Furthermore, we collected qualitative data through an anonymous feedback survey. In the following section, we present our key findings."}, {"title": "4.1 Quantitative", "content": "In our case study, 20 volunteer students participated by solving exercises from a pool of 19 practice tasks. In total, they handed in 277 solutions, which were graded by both the AI and human tutors. These solutions resulted from varying numbers of attempts: 113 exercises were solved in a single attempt, 43 required two attempts, and 26 took three attempts to reach the correct solution. In the following, we investigate the accuracy of PyEvalAI and how it helped students to improve their scores over multiple attempts.\nGrading Accuracy First, we evaluate the accuracy of the AI model by comparing its generated scores with scores of human numerics tutors. Figure 8a shows human gradings and AI gradings as coordinates in a 2D diagram where the number of occurrences is indicated by the point sizes. The majority of all points is located at the top right corner where AI and humans agree to give a large fraction of the total points. As indicated by the small purple dots scattered across the top of the plot, in some cases the AI gave the highest possible score even though the submission was incomplete (e.g. if the solution is correct but the calculations were not presented in the submission). Figure 8b depicts a histogram of differences between AI and human gradings. It shows that 182 (or 65.7%) of all 277 human gradings where identical to the AI. Furthermore, the distribution has a mean of -0.14% with a standard deviation of 20.73% indicating that our model does not expose a significant bias. Nonetheless, variations between automated and human gradings are still present. To some extent, however, this is also typical of human gradings. In our study, we have not yet evaluated human variations, as this would significantly increase the workload for all tutors. However, this remains an interesting aspect for future research.\nLastly, we compare how often human corrections occured for the scores and the feedback text of the AI. Table 1 presents how often the tutors agreed with the AI, only changed the grading, only corrected the feedback text or changed both. In 57.8% of all submissions we have full agreement between the tutors and the AI. In 25.6% of all cases the tutors changed the AI score as well as the feedback text. The remaining 16.6% contain the cases in which the AI response is somewhat close to the human grading, such that only either the grading or the text had to be adjusted. In these cases the AI provides a baseline for the feedback on which the tutors can build on.\nThe results support our approach of using LLMs as a semi-automatic tool to assist human tutors. In summary, although the AI model still produces occasional incorrect gradings, it aligns very well with human assessments on average. This supports our approach of using LLMs as a semi-automatic tool to assist human tutors, reducing their overall grading workload.\nImproved student performance through multiple graded attempts One important benefit of our system is the quick response by the LLM in comparison to traditional grading workflows which enables additional attempts to repeat and improve on tasks. Figure 9 depicts the AI's grading of exercises over all attempts. This time, the frequency of the data is visible as increasing opacity of the lines. We again observe that a large number of last submissions are graded with 100% of all points. Moreover, when using multiple attemps, there is a significant improvement from the second last attempt to the last one. To make this more clear, we compute the mean and standard deviation of all AI gradings for each attempt separately and report the numbers in Table 2. The last attempt always yields the most points with the lowest standard deviation. Students improve on average by 25% - 30% compared to the second last attempt. Table 3 displays the number of students that improved, worsened or stayed equal during their attempts. Also here we see that the vast majority of students indeed improved in grading. Performing the same analysis on human grades instead of AI grades resulted in very similar statistics. Sometimes the students submitted solutions"}, {"title": "4.2 Qualitative", "content": "We conducted an anonymous survey in which 14 students participated. The most important results are visualized in Figure 10. In addition to the proportions of the responses, we compute a mean and standard deviation by converting all possible choices into equally-spaced numerical values between 0 (strongly disagree) and 1 (strongly agree) and visualizing these results as error bars for each question.\nWe observe that PyEvalAI is well received by the students and that the grading is perceived as fair and understandable. Similarly, the students acknowledge the quick AI feedback and consider it helpful, motivating and useful for making quick progress. These results are in accordance with the quantitative evaluation in Section 4.1 for the gradings and the improvements with several attempts. Most students do not find the average waiting time of 88.2 seconds disruptive, but faster responses will be an obvious improvement to the PyEvalAI user experience. Finally, we ask how much benefit the AI tool provides when learning in groups versus when learning alone. Here, the students found PyEvalAI more useful when learning alone. We suppose this could be due to group members already providing helpful feedback to each other, reducing the need for additional"}, {"title": "5 Conclusion", "content": "In this work, we presented PyEvalAI, a novel tool to provide students with immediate personalized AI generated feedback and support tutors in the grading process. PyEvalAI accepts text and code exercises, ensures privacy and is open source. A thorough evaluation of a case study in a numerics course demonstrates that the AI system often produces accurate and useful feedback that helps students to improve their solutions and tutors in the grading process.\nIn the future, PyEvalAI will be introduced in further courses to complement tutorials. A growing database of exercises, student solutions, AI and tutor feedback will enable thorough comparisons of different LLMs and help to improve prompting strategies and fine-tune models. We believe that, as local LLMs become increasingly more powerful and accurate, the learning experience for stu-"}]}