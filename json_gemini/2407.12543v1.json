{"title": "ABSTRACTION ALIGNMENT: COMPARING MODEL AND HUMAN CONCEPTUAL RELATIONSHIPS", "authors": ["Angie Boggust", "Hendrik Strobelt", "Hyemin Bang", "Arvind Satyanarayan"], "abstract": "Abstraction - the process of generalizing specific examples into broad reusable patterns - is central to how people efficiently process and store information and apply their knowledge to new data. Promisingly, research has shown that ML models learn representations that span levels of abstraction, from specific concepts like BOLO TIE and CAR TIRE to more general concepts like CEO and MODEL. However, existing techniques analyze these representations in isolation, treating learned concepts as independent artifacts rather than an interconnected web of abstraction. As a result, although we can identify the concepts a model uses to produce its output, it is difficult to assess if it has learned a human-aligned abstraction of the concepts that will generalize to new data. To address this gap, we introduce abstraction alignment, a methodology to measure the agreement between a model's learned abstraction and the expected human abstraction. We quantify abstraction alignment by comparing model outputs against a human abstraction graph, such as linguistic relationships or medical disease hierarchies. In evaluation tasks interpreting image models, benchmarking language models, and analyzing medical datasets, abstraction alignment provides a deeper understanding of model behavior and dataset content, differentiating errors based on their agreement with human knowledge, expanding the verbosity of current model quality metrics, and revealing ways to improve existing human abstractions.", "sections": [{"title": "Introduction", "content": "Abstraction is the process of distilling many individual data instances into a set of fundamental concepts and relationships that capture essential characteristics of the data [Yee, 2019, Alexander, 2018, Liskov et al., 1986]. The result is an interconnected web of concepts, ranging from specific ideas, like SCHNAUZER, to progressively more abstract notions, like DOG or ANIMAL. Abstraction is a central characteristic of human cognition as it allows us to flexibly reason at the level of abstraction appropriate for our task - for example, in computer science, abstraction helps hide low-level implementation concerns from clients Liskov et al. [1986]. Moreover, abstraction allows us to generalize our knowledge by fitting our abstracted patterns to new, unseen data [Yee, 2019]. For instance, over their careers, clinicians learn abstractions of disease symptoms which they use to diagnose and treat new patients, even those with rare or atypical diseases [Eva, 2005].\nPromisingly, existing interpretability and alignment research has shown that machine learning (ML) models learn concepts at varying levels of abstraction. Concept-based interpretability methods have demonstrated model sensitivity to concepts, ranging from specific ideas, like CAR TIRE, to higher-level concepts, like MODEL [Kim et al., 2018, Ghorbani et al., 2019]. Similarly, research on neuron activations, has found that models encode human-like concepts across levels"}, {"title": "Related Work", "content": "Abstractions in ML datasets Abstractions allow humans to efficiently process information and form the bases for information encodings in linguistics [Miller, 1995, Dewey, 2011], biology [Hinchliff et al., 2014, Linnaeus, 1758], and medicine [World Health Organization, 1978]. In machine learning, abstractions are built into many tasks, such as image classification [Krizhevsky et al., 2009, Deng et al., 2009], medical diagnostics [Johnson et al., 2016a,b], and text prediction [Miller, 1995]. Even datasets that do not include an abstraction can be linked to existing abstractions by matching their output classes with corresponding concept nodes [Redmon and Farhadi, 2017]. We apply abstraction alignment to interpret image classification models using the CIFAR-100 hierarchy [Krizhevsky et al., 2009], benchmark language models using WordNet language abstractions [Miller, 1995, Fellbaum, 1998], and analyze dataset abstractions using the ICD-9 disease abstraction [Johnson et al., 2016a,b, World Health Organization, 1978].\nConcepts in model interpretability Aligned with our goal of understanding machine learning model behavior, interpretability research focuses on measuring model reliance on known human concepts [Doshi-Velez and Kim, 2017, Rai, 2020]. For instance, saliency methods reveal input features important to the model's prediction that humans compare to their expectations [Selvaraju et al., 2017, Carter et al., 2019a, Boggust et al., 2022]. Feature visualization methods help identify concepts, like patterns or object parts, that activate model layers [Olah et al., 2017, Erhan et al., 2009, Bau et al., 2017]. Concept-based methods like TCAV [Kim et al., 2018, Ghorbani et al., 2019] identify and test for human concepts encoded in a model's latent space. Neuron activation analysis identifies human concepts that activate particular model neurons [Hernandez et al., 2021, Bau et al., 2017, Oikarinen and Weng, 2022]. Recently, work in mechanistic interpretability discovered small networks contain state machines that transition between concepts in human meaningful ways [Bricken et al., 2023]. Together these methods have identified problematic model correlations [Carter et al., 2021], made sense of complex model activations [Olah et al., 2018, Carter et al., 2019b], and discovered novel concepts that advance human knowledge [Schut et al., 2023]. Building on their success, we expand interpretability from independent concepts to the relationships between them, to ensure that models learn human-aligned concepts and human-aligned abstractions."}, {"title": "Methodology", "content": "The goal of abstraction alignment is to measure how well the model's learned abstraction aligns with a given human abstraction. Our methodology is based on the assumption that the model's confusion is a reflection of its learned abstraction-i.e., concepts the model commonly confuses are more similar in the model's abstraction than concepts the model perfectly separates."}, {"title": "Representing human abstractions", "content": "To compute abstraction alignment, we first represent the human abstraction as a directed acyclic graph (DAG), where nodes represent concepts and edges represent child-to-parent relationships between concepts. For example, in the medical abstraction in Section 4.3, nodes represent medical diagnoses and edges map from specific diagnoses, like FRONTAL SINUSITIS, to broader diagnostic categories, like RESPIRATORY INFECTIONS [World Health Organization, 1978]. Every node in the DAG exists at a level of abstraction, ranging from the leaf level to the root level, computed based on its shortest path from a leaf node.\nThe DAG data structure is well suited to representing abstractions because it efficiently encodes both the abstraction's concepts and conceptual relationships. We can easily access a concept's level of abstraction by measuring its height and move up and down the level of abstraction by getting its ancestors or descendants. Since the graph is acyclic, it guarantees the hierarchical structure that underpins abstraction relationships. Further DAGs are commonly used to represent abstractions [World Health Organization, 1978, Miller, 1995] and are built into many ML datasets [Krizhevsky et al., 2009, Deng et al., 2009, Johnson et al., 2016a,b], allowing abstraction alignment to apply to a wide variety of domains."}, {"title": "Integrating model outputs with human abstractions", "content": "The next step in computing abstraction alignment is to compare the model's behavior to the given human abstraction DAG. To do so, we map the model's output space (e.g., classes or tokens) to nodes in the DAG. Often the human abstraction is built into the modeling task, so this mapping is straightforward - e.g., CIFAR-100 includes a human abstraction mapping classes to higher-level superclasses Krizhevsky et al. [2009]. However, even when the human abstraction is separate from the modeling task, the model's output space can often be easily computationally mapped to the DAG. For instance, in Section 4.2, we map words in the model's vocabulary to nodes in the WordNet DAG [Miller, 1995].\nWe use this mapping to analyze the model's behavior based on the human abstraction. Following Algorithm 1, we create a weighted DAG for each dataset instance, where nodes have a value and aggregated value. The value corresponds to the model's output probability. If a node corresponds to a model's output, then the value is the model's predicted probability for that output. Otherwise, the value is zero. The aggregated value is the model's propagated probability and is computed as the sum of values of its descendants. For example, in CIFAR-100 [Krizhevsky et al., 2009], the aggregated value of the FLOWER node is the sum of the model's probabilities that the image is a ORCHID, POPPY, ROSE, SUNFLOWER, or TULIP. By propagating the model's confidences through the abstraction, aggregated value provides a measure for the model's confidence in non-output nodes, including high-level concepts."}, {"title": "Measuring abstraction alignment", "content": "Using the weighted DAG, we can measure the abstraction alignment of a model's decision for a specific instance or an entire dataset. While there are potentially many metrics one could use to analyze abstraction alignment patterns, we define an initial set of four metrics that we have found useful for downstream tasks. During analysis we assume we have a dataset X, a model trained to predict classes or tokens c, and an abstraction DAG containing nodes n across levels of abstraction l. We use the model's outputs (e.g., $Y_{ij}$ for instance $x_i$ and class $c_j$) to compute the aggregated value $U_{ki}$ of node $n_k$."}, {"title": "Algorithm 1 Abstraction Alignment Propagation \u2014 create a weighted DAG for a dataset instance", "content": null}, {"title": "Accuracy abstraction alignment", "content": "One way to measure abstraction alignment is to measure how well the human abstraction accounts for the model's errors. If a model's mistakes are substantially reduced by moving up a level of abstraction, then the model's behavior is more abstraction aligned than if it continues to make errors at higher-levels of abstraction. While there are cases when the model's errors may acceptably not fit the abstraction, such as misclassifying an image containing multiple objects, in aggregate we expect the model's errors to reflect its abstractions \u2014 i.e., it will confuse output classes or tokens that it considers similar.\nWe measure accuracy alignment as the proportion of errors that are reduced by moving from level $l_i$ to $l_j$. First, we compute the number of correct predictions at each level by comparing the node with the highest aggregated value in that level to the expected prediction at the level. Then, we compute the proportion of errors that are mitigated by moving up in the abstraction. If accuracy alignment is high, then the abstraction accounts for a large amount of the model's mistakes, suggesting the model is using a similar abstraction.\naccuracy alignment = $\\Delta A_{l_i, l_j} = \\frac{\\sum_{k=1}^X 1[argmax([v_{a,k} \\forall n_a \\in l_j]) = Y_{k,j}] - \\sum_{k=1}^X 1[argmax([v_{a,k} \\forall n_a \\in l_i]) = Y_{k,i}]}{|X| - \\sum_{k=1}^X 1[argmax([v_{a,k} \\forall n_a \\in l_i])}$                                                     (1)"}, {"title": "Uncertainty abstraction alignment", "content": "Similarly, we can measure abstraction alignment by quantifying how well the abstraction accounts for the model's uncertainty. A model whose confusion is contained within a small portion of the DAG is more abstraction aligned than a model whose confusion spans the DAG. As with accuracy alignment, uncertainty alignment applies in aggregate - e.g., a model that regularly confuses types of FRUIT is more abstrac- tion aligned than a model that regularly confuses FRUITS and BIRDS.\nWe measure uncertainty alignment by testing the difference in entropy between levels of the DAG. First, we compute the Shannon entropy (H) entropy of the node aggregate values for every level in the DAG. The larger the entropy for a given level the more confused the model is across concepts at that level of abstraction. Then we compute the mean difference in entropy (H) between two levels ($l_i$ and $l_j$) across a set of data instances, X. If the entropy decreases substantially then the model's behavior aligns with the abstraction mapping the low-level nodes to the higher-level nodes.\nuncertainty alignment = $\\Delta H_{l_i,l_j} = \\frac{1}{|X|} \\sum_{k=1}^X H([v_{a,k} \\forall n_a \\in l_j]) - \\frac{1}{|X|} \\sum_{k=1}^X H([v_{a,k} \\forall n_a \\in l_i])$                                                             (2)"}, {"title": "Subgraph preference", "content": "Another useful metric when using abstractions to analyze model behavior is to compare subgraphs within the abstraction DAG. For instance, in Section 4.2, we compare regions of the DAG that represent"}, {"title": "Concept confusion", "content": "Finally, the concept confusion metric allows us to measure how often a model assigns probability to pairs of concepts. Identifying these concepts can reveal concepts that the model considers similar in its abstraction despite being different in the human abstraction. While concept pairs that are direct ancestors or descendants of each other will definitionally have high concept confusion, unrelated concept pairs with high concept confusion indicate unrelated human concepts that the model's abstraction deems similar.\nTo compute concept confusion for a pair of nodes, we compute the Shannon entropy (H) of their aggregate values divided by the maximum possible entropy for a pair of nodes. By computing the entropy, we weight the concept confusion by how confused the two nodes are. We compute concept confusion over an entire dataset to identify concepts that the model repeatedly confuses.\nconcept confusion = $C(n_i, n_j) = \\frac{\\sum_{k=1}^X H ([v_{i,k}, U_{j,k}])}{\\sum_{k=1}^X H ([0.5, 0.5])}$                                                              (4)"}, {"title": "Experiments", "content": null}, {"title": "Interpreting model behavior with abstraction alignment", "content": "Abstraction alignment improves model interpretability by expanding the number and complexity of concepts we can use to characterize model decisions and comparing them to accepted human abstractions. A common interpretability task is understanding a model's mistakes; however not all mistakes are equally problematic. For example, we would be more likely to forgive a model that regularly mistakes CARS for TRUCKS than a model that consistently mistakes CARS for STOP SIGNS. The former aligns with our human abstractions that treat VEHICLES similarly while driving; whereas, the latter suggests model abstractions do not follow accepted human reasoning with potentially dangerous consequences. In these cases, abstraction alignment helps differentiate the severity of a model's mistakes, distinguishing benign low-level errors from problematic higher-level misalignment."}, {"title": "Benchmarking language models' abstraction alignment", "content": "Benchmarking the specificity of language models helps us distinguish valuable models that output precise answers from those that output correct but meaningless text. For instance, while \u201cDante is a person\u201d and \u201cDante is a poe\u0165\" are both correct, we would prefer a language model that outputs the latter since it is operating at the correct level of abstraction [Huang et al., 2023]. Metrics for benchmarking language model specificity use a dataset of language\""}, {"title": "Analyzing datasets using abstraction alignment", "content": "Abstraction alignment can also be valuable in dataset analysis by revealing differences in the abstractions we expect our models to learn and those codified in the dataset. Models learn correlations between input features and output decisions from their training data. However, the correlations in the dataset are not always the ones a model developer expects their model to learn. Often, dataset issues are only identified after models trained on them produce problematic outputs [Zech et al., 2018, Caliskan et al., 2022]. Applying abstraction alignment to datasets can help us understand how the abstractions they implicitly encode correspond with expected human abstractions before the datasets are released as training data.\nTo demonstrate abstraction alignment as a dataset analysis tool, we use it to compare the medical abstractions encoded in the MIMIC-III dataset to medical hierarchy standards set by global health authorities. The MIMIC-III dataset contains patients' medical notes labeled with a set of ICD-9 codes representing the patient's diseases and procedures [Johnson et al., 2016a,b]. The codes are part of the ICD-9 medical hierarchy used by hospitals to justify healthcare costs to insurance [Alexander et al., 2003]. However, discrepancies between clinical code application and the ICD-9 guidelines are known to occur due to lack of coder experience, complexity of the coding system, and intentional misuse to increase insurance payout [O'Malley et al., 2005]. Since MIMIC-III contains real-world patient records that could be affected by clinical misuse, the code labels in the dataset may not reflect ICD-9's intended use. Abstraction alignment can reveal how well MIMIC-III aligns with the ICD-9 abstraction to inform model developers of the abstractions their models may learn and perpetuate in deployment.\nTo apply abstraction alignment in this setting, we use the ICD-9 hierarchy as the abstraction DAG and the dataset's ICD-9 code labels to represent the dataset's encodings. The ICD-9 hierarchy Mullenbach et al. [2018] contains 21,116 nodes over 7 levels of abstraction. Each nodes represents an ICD-9 code (e.g., FRONTAL SINUSITIS) or higher-level code grouping (e.g., RESPIRATORY INFECTIONS). To compute the abstraction alignment of each dataset instance, we map the dataset's labels to the ICD-9 nodes (Section 3.2) to create a weighted DAG where the aggregate value of a node represents how many labels were assigned to its descendants. To understand possible discrepancies between the dataset's abstractions and the ICD-9 abstraction, we use concept confusion to analyze pairs of nodes that co-occur across dataset instances. We can think of pairs with high concept confusion as concepts the dataset represents similarly because both concepts often apply to the same medical note.\nWe begin our analysis by filtering to nodes representing high-level code groupings (i.e., direct descendants of the root). In ICD-9, there are four top-level groupings: PROCEDURES, DISEASES AND INJURIES, V SUPPLEMENTARY HEALTH FACTORS, and E SUPPLEMENTARY CAUSES OF INJURY AND POISONING. In Figure 3, we see it is common for the dataset to contain code labels from multiple of these high-level code groupings. Assigning PROCEDURE codes with DISEASE AND INJURY codes makes sense because a disease defines a treatment procedure. However, the dataset frequently contains DISEASE AND INJURY and V SUPPLEMENTARY HEALTH FACTORS codes. In the ICD-9 hierarchy, V SUPPLEMENTARY HEALTH FACTORS codes are \"provided to deal with occasions when circumstances other than a disease or injury are recorded as diagnosis or problems\" [American Speech-Lanugage-Hearing Association, 2015]. The fact that patient notes in the MIMIC-III dataset often contain both DISEASE AND INJURY and V SUPPLEMENTARY HEALTH FACTORS codes when the ICD-9 hierarchy expects them to be used disjointly, suggests a misalignment in the dataset's abstractions.\nNext, we analyze confusion between lower-level nodes to understand how specific dataset labels may be misaligned with the ICD-9 abstraction. Often diseases share a medical correlation, so it is expected that the dataset commonly contains both labels for metabolic factors like DISORDERS OF LIPID METABOLISM and ESSENTIAL HYPERTENSION. However, we also see frequent co-labeling between \"other\" codes, like OTHER DISEASE OF LUNG and OTHER AND UNSPECIFIED HYPERTENSION. In ICD-9, a code grouping often contains sibling codes representing specific variants of"}, {"title": "Discussion and Future Work", "content": "In this paper, we study abstraction alignment - the agreement between a model's learned conceptual relationships and established human abstractions. In interpretability tasks, abstraction alignment identifies misalignments in model reasoning; in model benchmarking, abstraction alignment expands the expressiveness of evaluation metrics; and, in dataset analysis, abstraction alignment reveals differences between the abstractions we want models to learn and those codified in the dataset.\nWe consider abstraction alignment to be a paradigm for understanding datasets and ML model behavior and, just as there are many ways to measure models' representational alignment [Sucholutsky et al., 2023, Terry et al., 2023], we expect there are likely a plethora of techniques to measure abstraction alignment. For instance, following research methods that reveal and edit models' representation of state [Hernandez et al., 2021, Li et al., 2021, Reif et al., 2019, Hewitt and Manning, 2019], future work could test whether models' internal representations encode human abstractions. Internal abstraction alignment metrics could study how abstractions change across model layers, evolve during training, and whether modifying modifying a model's internal abstraction improves its performance.\nFuture work on abstraction alignment should also consider concept theories from cognitive psychology. Our current approach applies Aristotelian concept theory where concepts define exact membership conditions to determine whether an instance is part of a given concept [Rosch, 2011]. Thus, this means our concepts are discrete DOGS are animals from the species canis lupus so SCHNAUZER and WOLF are both DOGS. However, if we were to use graded concept theory [Rosch and Lloyd, 1978], concepts would define a continuous degree of membership. In this setting, a common dog like SCHNAUZER is a strong example of a DOG whereas WOLF is a weak member because, while technically still a DOG, we perceive them differently from domesticated dogs. This may suggest a more continuous measurement of abstraction alignment where conceptual relationships are weighted based on the degree of membership.\nFinally, in many cases, there may not exist a universal human abstraction that applies to a given task. For example, individual doctors often develop slightly differing medical abstractions as a function of their medical training and clinical experiences [Cai et al., 2019]. Thus, besides developing clinical models that agree with medical standards, abstraction alignment can help us develop models that are more personalized to a particular clinician. For instance, abstraction alignment could be used to improve human-AI collaboration by ensuring both humans and models are reasoning with the same abstractions. More interestingly, by adapting abstraction alignment, we could specifically train models to learn abstractions that complement a doctor's - acting as valuable collaborators with additional expertise and alternate perspectives."}, {"title": "Appendix", "content": null}, {"title": "Experimental details", "content": "Here we describe the experimental details for each experiment in Section 4. Code to recreate our experiments can be found at: https://github.com/mitvis/abstraction-alignment. An interactive interface for exploring experimental results is provided at: https://vis.mit.edu/abstraction-alignment/."}, {"title": "Interpreting model behavior with abstraction alignment", "content": "In Section 4.1, we use abstraction alignment to interpret a CIFAR-100 image classification model. We train a PyTorch [Paszke et al., 2019] ResNet20 model [He et al., 2016a] on CIFAR-100 training set [Krizhevsky et al., 2009] for 200 epochs with a batch size of 128. We apply random crop and horizontal flip data augmentations to the images following He et al. [2016b]. We use cross-entropy loss optimized via stochastic gradient descent and Nesterov momentum [Sutskever et al., 2013] (momentum = 0.9; weight decay = 5e-4). We use a learning rate of 0.1 and reduce it at epoch 60, 120, and 160 using gamma of 0.2. The trained model achieves 67.7% accuracy on the CIFAR-100 test set.\nTo apply abstraction alignment we use the CIFAR-100 class/superclass mapping [Krizhevsky et al., 2009] to form an abstraction DAG. The DAG contains 121 nodes across 3 levels 100 class nodes (level 1), 20 superclass nodes (level 2), and a root node (level 3). We create a weighted DAG for every dataset instance in the CIFAR-100 test set, representing the model's abstraction alignment for that instance. To do so, for a given image, we compute the model's softmax output probabilities over the classes. Following Algorithm 1, we assign class nodes in the DAG a value equal to the model's output probability for that class. All other nodes recieve a value of zero. We compute every node's aggregate value as the sum of all of their descendant's values. For instance, the node TULIP's aggregate value is the model's output probability that the image is a tulip, whereas the node FLOWER's aggregate value is the sum of the model's output probability for ORCHID, ROSE, TULIP, SUNFLOWER, and POPPY."}, {"title": "Benchmarking language models", "content": "In Section 4.2, we apply abstraction alignment to benchmark language models. Following the benchmarking procedure in Huang et al. [2023], we compare pretrained bert-base [Devlin et al., 2019], bert-large [Devlin et al., 2019], roberta-base [Liu et al., 2019], roberta-large [Liu et al., 2019], and gpt-2 [Radford et al., 2019] models from the LAMA benchmark [Petroni et al., 2020, 2019]. We test each model on the occupation, location, and birthplace tasks from the S-TEST dataset [Huang et al., 2023]. Each data instance in the S-TEST dataset is a text query paired with one specific and one general answer label. For each model, we compute its top-10 accuracy, measured as the proportion of instances where the specific answer was in the model's top 10 predicted tokens.\nTo measure abstraction alignment, we create an abstraction DAG for each of the occupation, location, and birthplace tasks. For a task, we map each of its specific answer labels to its corresponding node (i.e., synset) in WordNet [Miller, 1995, Fellbaum, 1998]. We do this process by searching for the specific answer label in the NLTK WordNet corpus. If there are multiple WordNet nodes that hit for a given search, we select the most appropriate node by manually inspecting their WordNet definitions. Then, we expand the DAG by including all direct ancestors and descendants of any specific answer nodes. We only consider ancestors and descendants that exist in the model's vocabulary. The result is a DAG containing all the vocabulary words related to any of the data instances' specific answer labels.\nTo create weighted DAGs, we compute the model's output probability across every word in its vocabulary for every data instance. For each data instance, we assign the model's output probabilities to their corresponding nodes in the DAG. We use the weighted DAGs to compute three specificity metrics, using the subgraph preference function Equation (3). In each metric, we use the node values corresponding to the model's predicted probability outputs. First, we compute replicate the specificity testing metric from Huang et al. [2023] (originally called pr). We compute it as P(ss, sg), where ss is the single-node graph containing the specific label and sg is the single-node graph containing the general answer label. Next, we compute P(ss\u2193, Ss\u2191) to compare all words at the specific label's level of abstraction and lower ss (specific label and its descendants) to all words at a higher level of abstraction than the specific label ss\u2191 (specific label's ancestors). Finally, we compute P(ss\u2191\u2193, st) to compare ancestors and descendants of the specific label Ss\u2191\u2193 to any other word in the task DAG st."}, {"title": "Analyzing datasets using abstraction alignment", "content": "In Section 4.2, we apply abstraction alignment to anlayze the abstractions in the MIMIC-III dataset [Johnson et al., 2016a,b]. The dataset contains textual medical notes paired with a set of ICD-9 code labels. We use the ICD-9 medical hierarchy as the abstraction DAG [World Health Organization, 1978]. We pair the dataset's ICD-9 code labels with their corresponding code in the ICD-9 abstraction DAG. To compute weighted DAGs for every dataset instance, we set the code node's value equal to one if the code was labeled on that instance and zero otherwise. For all other nodes (e.g., non-codable node groupings), we assign their aggregate value as the sum of its children. As a result the aggregate value of a node is equivalent to the number of times it or one of its children labeled the medical note. In the task, non-leaf nodes are codable. For instance, both SICKLE-CELL ANEMIA and its direct parent HEREDITARY HEMOLYTIC ANEMIAS can be applied to the same medical note."}, {"title": "Compute resources and efficiency", "content": "All abstraction alignment analysis is performed on CPU. Time to build the DAG and compute abstraction alignment depends on the number of nodes in the DAG and the abstraction alignment metric. On the CIFAR-100 DAG (121 nodes) [Krizhevsky et al., 2009], computing the weighted DAG for 10,000 CIFAR-100 test images takes approximately 2 minutes, computing accuracy alignment and uncertainty alignment takes under a minute, and computing concept confusion takes on the order of 15 minutes. On the MIMIC-III DAG (21,166 nodes), creating the weighted abstraction DAGs and computing concept confusion takes around 30 minutes.\nWe train and evaluate the models used in the experiments on 1 NVIDIA V100 GPU with 1TB of memory. Training the CIFAR-100 ResNet20 model takes approximately 30 minutes. Running inference on the S-TEST dataset takes roughly 10 minutes per language model."}, {"title": "Additional abstraction alignment examples", "content": "Additional examples of the abstraction alignment of data instances from Section 4.1 and Section 4.3 are available in an exploratory interface https://vis.mit.edu/abstraction-alignment/."}]}