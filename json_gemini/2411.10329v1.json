{"title": "Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding", "authors": ["Huming Qiu", "Guanxu Chen", "Mi Zhang", "Min Yang"], "abstract": "In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.\nGiven that semantic consistency between input text and output image is a fundamental requirement for T2I models, we identify that textual representations (i.e., prompt embeddings) are likely the primary source of unsafe generation. To this end, we propose a vision-agnostic safe generation framework, Embedding Sanitizer (ES), which focuses on erasing inappropriate concepts from prompt embeddings and uses the sanitized embeddings to guide the model for safe generation. ES is applied to the output of the text encoder as a plug-and-play module, enabling seamless integration with different T2I models as well as other safeguards. In addition, ES's unique scoring mechanism assigns a score to each token in the prompt to indicate its potential harmfulness, and dynamically adjusts the sanitization intensity to balance defensive performance and generation quality. Through extensive evaluation on five prompt benchmarks, our approach achieves state-of-the-art robustness by sanitizing the source (prompt embedding) of unsafe generation compared to nine baseline methods. It significantly outperforms existing safeguards in terms of interpretability and controllability while maintaining generation quality.\nWarnings: This paper contains depictions and discussions of explicit and other inappropriate content that some readers may find disturbing, distressing, and/or offensive.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [11], [39], as the current state-of-the-art (SOTA) generative paradigm, drive the development of text-to-image (T2I) generation systems. These T2I models are guided by textual prompts and aim to generate realistic images that align with the provided text descriptions, finding widespread application in fields such as art creation [44] and graphic design [51]. Between 2022 and 2023 alone, T2I models generated over 15 billion images [6]. While T2I models demonstrate significant potential in terms of generation performance, they also face risks of unsafe generation. For instance, the BBC reported that AI-generated child sexual abuse materials were being widely sold over the internet, severely violating ethical and legal norms [5]. Therefore, ensuring that the content generated by T2I models adheres to usage policies [8], [25] is an urgent necessity.\nTo address this issue, many commercial T2I online services have implemented various safeguards. For instance, DALL-E [30] uses a Moderation API to detect input prompts that may violate usage policies. Stable Diffusion is equipped with a SafetyChecker [31] to filter out generated images containing explicit content. However, as passive defense mechanisms, input and output moderation typically offer coarse-grained protection, which may cause T2I models to overly intercept and refuse to generate any images. In contrast, safe generation methods [7], [15], [17] focus on erasing inappropriate concepts from the model's internal representations. These methods aim to suppress the emergence of these concepts in the generated content, rather than indiscriminately blocking inputs or outputs. For instance, SafeGen [17] fine-tunes the model to remove visual representations linked to explicit concepts, ensuring that the generated content does not reflect these concepts. Such methods offer more fine-grained protection for T2I models, enabling them to produce high-quality, safe images even when given prompts with inappropriate concepts.\nHowever, despite the fact that existing safe generation methods are capable of suppressing specific target concepts to certain extent, their robustness remains limited [43], [46]. Considering that semantic consistency between the input text and the generated image is a fundamental requirement for T2I models, which means that inappropriate prompts can be the primary source of unsafe generations by T2I models. Existing methods usually focus on fine-tuning the denoiser of a T2I model or modifying the denoising process to erase inappropriate concepts from the visual representation, neglecting the sanitization of inappropriate concepts in the prompt embeddings. This exposes their limitations, as these methods typically only suppress explicitly deleted concepts"}, {"title": "2. Background", "content": "2.1. Text-to-Image (T2I) Generation Models\nDiffusion models have recently become a SOTA generative paradigm [10], employing a probabilistic framework to generate high-fidelity images from noisy representations. The training of diffusion models is divided into the diffusion process and the denoising process. In the diffusion process, the model will gradually add noise to the image so that it eventually approximates the standard Gaussian distribution.\nIn the denoising process, the denoising network predicts and removes the added noise, progressively recovering the original image. The training objective of diffusion models is to minimize the difference between the predicted noise and the true noise at each timestep, typically expressed using a simple mean squared error (MSE) loss:\n$L_{MSE} = \\mathbb{E}_{x,c,\\epsilon \\sim \\mathcal{N}(0,1),t} [||\\epsilon - \\epsilon_\\theta(x_t, t, c) ||^2]$, (1)\nwhere x represents the original image, $x_t$ is the noisy image at timestep t, and c represents the guiding condition, such as a prompt. The variable $\\epsilon$ is the randomly sampled Gaussian noise, while $\\epsilon_\\theta$ indicates the predicted noise generated by the denoising network parameterized by $\\theta$.\nThe ability of diffusion models to generate high-fidelity images largely depends on the efficient processing of data in the original high-dimensional pixel space. however, this also results in significant computational overhead. To address this issue, latent diffusion models (LDM) [33] propose a more efficient solution by executing the diffusion and denoising processes in a low-dimensional latent space, significantly reducing computational complexity. Stable Diffusion (SD) [40]\u2013[42] is a typical representative of the LDM framework, which mainly consists of three core components:\nText Encoder. The text encoder utilizes a pre-trained CLIP model [29] to convert input text prompts into dense semantic embeddings. These embeddings guide the generation process through cross-attention mechanism, ensuring that the generated images align with the semantic meaning of the input text.\nImage Decoder. The image decoder uses a pre-trained variational autoencoder (VAE) model [14] to map high-dimensional images into a low-dimensional latent space, improving computational efficiency. During the inference stage, the denoised latent representations generated by the diffusion process are decoded by the VAE into high-resolution images.\nDenoiser. The denoising network uses the U-Net architecture [34], which takes noisy latent representations and text embeddings as input and progressively predicts and removes the noise at each time step, refining the latent representations into coherent image representations.\nCompared to earlier generative models, Diffusion models offer a more stable training process, overcoming common issues such as mode collapse [2]. However, despite the breakthroughs in the generation performance of T2I models, challenges remain in terms of safety [50], [52]. Ensuring that the generated images comply with safety guidelines and preventing the production of inappropriate, harmful, or biased content are still pressing issues that need to be addressed.\n2.2. Adversarial Attacks on T21 Models\nWith the support of large-scale image-text pair datasets such as LAION-5B [37], the performance of T2I models achieves significant progress. However, certain content within these datasets may contain sensitive information (such as adult content or hate symbols), which the models may inadvertently memorize during training, subsequently generating harmful images. Despite the integration of various safeguards in T2I models to avoid this issue, they remain vulnerable to adversarial attacks.\nAdversarial attacks against T2I models aim to create adversarial prompts that bypass or evade the model's safeguards, generating inappropriate images that do not meet expected standards. These attacks typically manipulate or modify the input prompts, introducing adversarial elements while preserving semantic similarity to the original prompts, thereby rendering the model's safeguards ineffective. Based on the adversary's knowledge of the target model, adversarial attacks can be categorized into white-box attacks and black-box attacks. In white-box attacks, the adversary has full access to the internal information of the target model, including its architecture, parameters, and gradient calculations. This allows the adversary to directly use the internal knowledge of the model to construct adversarial samples. For example, by utilizing gradient-based optimization techniques, the adversary can identify a pseudo token to compel the model to generate specific inappropriate content [26].\nIn contrast, black-box attacks are conducted without access to the internal structure or parameters of the model, making them more reflective of real-world threat scenarios. Adversaries typically rely on locally deployed surrogate models to optimize adversarial prompts, leveraging the transferability of prompts to attack the target model. For instance, SneakyPrompt [49] uses a Stable Diffusion model equipped with an output detector as a surrogate model and applies reinforcement learning algorithms to automatically search for substitutes for sensitive words in prompts. These substitutes are optimized using a reward function based on safety compliance and semantic fidelity, encouraging the generated adversarial prompts to bypass safeguards. Ring-A-Bell [43] utilizes a CLIP model to extract context-independent sensitive concept vectors and add them to the target prompts, and then uses a genetic algorithm to search for semantically similar adversarial prompts. Similarly, MMA-Diffusion [46] employs gradient-driven optimization and sensitive word regularization to construct adversarial prompts that contain no sensitive words while maintaining semantic similarity to the target prompt."}, {"title": "3. Related Work and Motivation", "content": "In this section, we review and summarize existing safeguards in the T2I model. We then summarize the properties that an ideal safe generation framework should possess. By analyzing these properties, we highlight the limitations of existing safe generation methods, which motivates us to propose a new defense framework that satisfies these properties."}, {"title": "3.1. Safeguards for T2I Models", "content": "To address adversarial attacks and mitigate abuse risks, T2I models integrate one or more safeguards to prevent unsafe content generation [31]. These safeguards can be broadly categorized into three types according to their defense goals: input moderation, output moderation, and safe generation.\nInput and Output Moderation. Input moderation serves as the first line of defense by operating at the prompt input stage. Before processing the user's prompt, this mechanism identifies and blocks malicious prompts containing sensitive terms to prevent the generation of inappropriate content. For example, commercial models like DALL\u00b7E 3 [3] and Imagen [9] employ text classifiers (e.g., Moderation API [24]) to detect prompts that may violate usage policies and reject these inputs before they reach the model. Output moderation acts as the final defense at the image output stage. Once a T2I model generates an image, it is screened by a violation detector to identify any inappropriate content. Only compliant images are returned to the user. For example, Stable Diffusion employs a SafetyChecker [31] built on the CLIP architecture. It contains predefined sensitive text embeddings for 17 explicit concepts. It calculates the cosine similarity between the embedding of the generated image and the inappropriate concepts. If the similarity exceeds a threshold, the model outputs a black image as a substitute.\nSafe Generation. Safe generation, also known as concept erasure [21], is a type of machine unlearning method [4] applied at the image generation stage. This method aims to erase inappropriate target concepts from the model's internal representations, ensuring appropriate content output even when users provide malicious prompts. Negative Prompt [1] is a concept suppression mechanism integrated in Stable Diffusion that enables the user or the system to specify concepts that should not be generated in order to avoid generating inappropriate content. In the denoising process, the method suppresses the inappropriate content by introducing the embedding of inappropriate concepts to add noise to the intermediate denoising result. SLD [35] combines classifier-free guided text conditioning with inappropriate concepts to suppress the generation of these concepts similarly to Negative Prompt. ESD [7] fine-tunes the cross-attention or non-cross-attention layers of the denoiser on target concepts, embedding the functionality of Negative Prompt into the model parameters. Similarly, CA [15] fine-tunes either the full denoiser or only its cross-attention on pairs of target prompts and anchor images, aiming to link inappropriate target concepts with safe and appropriate images. Safe-CLIP [27] relearns a safe CLIP model through contrastive learning between text and image pairs, aiming to break associations between inappropriate textual and visual concepts. SafeGen [17] focuses on eliminating sexually explicit content by fine-tuning the self-attention layers of the denoiser in a text-independent manner, forcing the model to apply a mosaic effect to any generated explicit content.\nProperties Analysis of Safe Generation. To comprehensively guard against potential evasion strategies, such as adversarial attacks, the safe generation framework for T2I models should meet the following core criteria [20]: (i) Effectiveness. The framework must effectively erase target concepts so that they do not appear in the generated image. (ii) Robustness. The framework should possess strong robustness, capable of suppressing synonymous concepts of the target and maintaining its effectiveness against adversarial attacks. (iii) Specificity. When generating non-target concepts, the output images should align semantically with the text descriptions, while maintaining a distribution consistent with the original model. (iv) Fidelity. After erasing target concepts, the quality of the generated images should not be significantly impacted, ensuring that the resulting images remain high-quality and usable.\nFurthermore, to enhance defense performance and flexibility, an ideal safe generation framework should satisfy the following advanced criteria: (v) Interpretability. The framework should be able to reveal the contributions of tokens to the generation of inappropriate content, enabling T2I model providers to understand the extent of specific inputs' impact on outputs, facilitating debugging and improvement. (vi) Compatibility. The framework should operate independently of existing defense systems, allowing for seamless integration and collaboration with various protective measures. (vii) Controllability. The framework should allow for control over the degree of erasure of target concepts during use, enabling T2I model providers to dynamically adjust the defense strength based on specific application needs."}, {"title": "3.2. Limitations of Existing Safeguards.", "content": "Despite significant progress in the safeguards of T2I models, existing mechanisms still exhibit notable limitations when dealing with complex adversarial attacks [13]. First, input moderation effectively intercepts prompts containing common sensitive terms but performs inadequately when handling synonyms, paraphrases, and adversarial prompts [47]. Second, output moderation is constrained by the detection scope and performance of content detectors [28], which may fail to detect attacks such as prompt dilution [43]. Furthermore, input and output moderation typically provide only coarse-grained protection, often resulting in the blind interception of inappropriate content, which causes T2I models to refuse to generate any content, negatively impacting user experience [16], [45].\nIn contrast, safe generation methods provide finer-grained control over the target concepts, allowing for the generation of safe and appropriate outputs even in the presence of prompts containing inappropriate concepts. This is because safe generation methods focus on eliminating or suppressing the generation of inappropriate concepts rather than completely blocking inputs or outputs, ensuring that users receive meaningful and compliant results when using the model. Although safe generation methods perform well in actively suppressing the generation of inappropriate concepts, they still lack robustness and are vulnerable to various adversarial prompts. Additionally, existing methods"}, {"title": "4. Embedding Sanitizer", "content": "4.1. Threat Model\nFollowing previous work [17], we define a threat model to describe the goals and capabilities of adversaries and T2I service providers, thereby clarifying the scope of our proposed defense framework.\nGoals and Capabilities of Adversaries. The primary goal of adversaries is to bypass the safeguards in T2I models and induce the models to generate high-quality inappropriate or restricted content. We assume that adversaries have the capability to locally deploy an open-source T2I model, although this model fails to meet quality requirements. Adversaries can utilize the local model to create or collect any adversarial prompts, such as searching for adversarial text through optimization techniques. Additionally, we assume that adversaries can interact with the target model by attempting various prompt variations and can understand the security measures equipped within the target model, but they cannot directly access the internal parameter information of the target model.\nGoals and Capabilities of T21 Service Providers. The primary goal of T2I service providers is to prevent unsafe generation and ensure the quality of the generated images. In terms of capabilities, T2I service providers have complete access to the parameters of the T2I model, enabling them to optimize additional modules or update model parameters based on gradient information. Furthermore, service providers can integrate various safeguards, such as Negative Prompt, aimed at enhancing the safety of the T2I model from a visual perspective.\n4.2. Overview\nBased on the analysis of the properties of safety generation methods, our goal is to develop a safe generation framework that embodies these properties, providing comprehensive defense for T2I models in complex real-world scenarios to ensure generated content adheres to usage policies.\nKey Idea. A T2I model essentially functions as a text-driven image generator, with its primary objective being to generate high-quality images that align with the given prompt. Ensuring semantic consistency between the input text and the output image is a core requirement of the T2I model. For a well-trained, normal T2I model, it is challenging to generate an unsafe image from a clean prompt that contains no inappropriate descriptions. For instance, it is nearly impossible for a T2I model to produce an image containing nudity when prompted with \"A photo of a cute dog\", as this would clearly violate consistency requirements. In other words, any inappropriate content in the generated image likely originates from inappropriate descriptions within the input prompt, suggesting that a T2I model's primary security risks may arise from the text encoder rather than other components.\nBased on this insight, we introduce Embedding Sanitizer (ES), a vision-agnostic safe generation defense framework designed as an add-on plugin that operates on prompt embeddings, aiming to erase target concepts at the source (i.e., prompt embeddings). This approach to concept erasure provides ES with two practical advantages. First, most existing safe generation frameworks focus on removing target concepts from visual representations\u2014typically by modifying the denoiser or denoising process\u2014while ignoring the importance to remove toxic representations within text embeddings. This characteristic renders ES independent of other safe generation frameworks, providing it with compatibility and allowing seamless integration with various safeguards. Second, ES 's training process only requires access to the text encoder, without needing access to the denoiser or image encoder. This design enables ES to function as a plug-and-play module that can be effortlessly transferred across different T2I models.\nNext, we formalize the key idea of ES. Assume that the T2I model to be fortified is equipped with a text encoder"}, {"title": "4.3. Architectural Design", "content": "Designed with modularity in mind, ES functions as an independent component and can seamlessly integrate with other safeguards. This modular approach enables ES to serve as a plug-and-play module compatible with various T2I systems that share a same text encoder. As shown in Figure 2, ES's architecture comprises two main components: E-Net and S-Net, which collaboratively perform the crucial sanitization function. Below, we provide a detailed description of each component's architectural design and explain how these designs confer interpretability and controllability to ES. The model hyperparameter configurations for these two components are summarized in Table 2.\nE-Net. The design of E-Net is based on the Transformer encoder architecture and includes six attention modules to efficiently capture potential inappropriate content in the input prompt. Each attention module employs an 8-head multi-head attention mechanism to enable E-Net to semantically analyze various parts of the input, allowing for a deeper understanding of tokens containing inappropriate concepts. E-Net's role is to capture harmful concepts from the original suspicious embedding, producing a set of toxic embeddings that represent inappropriate content. Suppose Ps is a suspicious prompt potentially containing inappropriate concepts, and it is encoded by the text encoder Ft to obtain the initial suspicious embedding, denoted as $Embs = F_t(Ps)$. E-Net, represented by $F_e$, takes these embeddings as input and extracts identifiable toxic embeddings Embt, which can be formulated as:\n$Embt = F_e(F_t(Ps)) = F_e(Embs)$. (3)\nS-Net. S-Net is a multilayer perceptron (MLP) consisting of two fully connected layers, with its input dimension matching the output dimension of the embedding produced by E-Net. To constrain its output to [0, 1], a Sigmoid activation function is applied. The primary function of S-Net is to"}, {"title": "4.4. Training Details", "content": "ES is essentially an end-to-end deep learning model that takes the original prompt embedding as input and outputs the sanitized clean embedding, which in turn guides the T2I model to generate appropriate content. Below, we introduce the training process of ES in detail, including training data preparation and objective function customization.\nTraining Data Preparation. To sanitize prompt embeddings, we first select an anchor concept corresponding to the target concept to be erased, providing ES with a clear direction for sanitization. This approach enables us to frame the embedding sanitization as a transfer problem toward the anchor concept. When the T2I model receives a prompt containing a target concept, ES redirects this concept toward the predefined anchor concept to achieve embedding sanitization. We design three approaches to construct (target concept, anchor concept) pairs:\nAntonyms of the target concept, e.g., \u3008illegal, legal\u3009;\nHypernyms of the target concept, e.g., \u3008blood, liquid\u3009;\nA neutral placeholder, e.g., \u3008weapon, \u3009.\nWith the default settings we tend to use antonyms as anchor concepts to provide a more effective sanitization direction for the target concept.\nConsidering that the performance of deep learning models largely depends on vast amounts of training data, we design an automated method for synthesizing prompts to provide ES with an almost limitless supply of training samples. Specifically, we view the synthetic prompt Pas a sequence of n tokens drawn from the vocabulary V, represented as $P = \\{token_1, token_2,...,token_n\\}$. During the data synthesis process, we first convert the target concept set $C_t = \\{c_1, c_2,...,c_m\\}$ containing m concepts and the anchor concept set $C_a = \\{ca_1, ca_2,....ca_m\\}$ into their corresponding token IDs using a tokenizer. Next, within the maximum input length allowed by the text encoder (e.g., SD-v1.4 allows a maximum of 77 tokens), we randomly select a number of tokens to compose P, expressed as $P = \\{(token_1, token_2,..., token_n) | token_i \\in V,\\forall i = 1,2,...,n, n \\geq 3\\}$. Here, the requirement that $n \\geq 3$ ensures that P contains at least three tokens: a starting token, a random token, and an ending token, which guarantees that P complies with input specifications and is not empty. Finally, we select random positions excluding the first and last tokens to replace them with random concepts from Ct, thereby forming a toxic prompt containing a random number of target concepts, denoted as Pt. We then replace the target concepts in Pt with their corresponding anchor concepts to create the clean prompt P. This dataset construction strategy enables ES to learn how to transform harmful prompts Pt into clean prompts Pc, ensuring that the generated images reflect appropriate content without losing contextual relevance.\nThe example in Figure 3 provides an intuitive illustration of a randomly synthesized Pt and its corresponding Pc, where red tokens indicate target concepts and green tokens represent anchor concepts. Automated data generation strategy significantly enhances the generalization capability and adaptability of ES. First, synthetic data allows us to generate a vast number of prompts containing target concepts in various contexts, assisting ES in learning to identify and erase inappropriate content across diverse scenarios. Second, the generation of synthetic data enables precise control over the combinations of target concepts and anchor concepts, allowing ES to effectively adapt to different application requirements. Furthermore, generating training data in an automated manner can substantially reduce manual anno-"}, {"title": "5. Evaluation and Analysis", "content": "In this section, we begin by outlining the evaluation setting, which includes datasets, safeguard baselines, and evaluation metrics. We then conduct extensive experiments to evaluate the performance of ES across four core dimensions: effectiveness, robustness, specificity, and fidelity. Finally, we explore the impact of hyperparameters on ES's performance to demonstrate its controllability.\n5.1. Evaluation Setup\nDataset. We consider five datasets for evaluating ES. These include a handcrafted prompt dataset, I2P [35], for effectiveness evaluation. Three adversarial prompt datasets, constructed from SneakyPrompt (SP) [49], Ring-A-Bell (RAB) [43], and MMA-Diffusion (MMA) [46], are used for robustness evaluation. Additionally, an image-text pair dataset, COCO-2017 [18], is utilized for evaluate specificity and fidelity. We provide a description of the dataset in Section A.\nBaselines. Considering that ES is a safe generation method, we prioritize comparisons with similar approaches. Specifically, we evaluate six safe generation baselines: NP [1], SLD [35], ESD [7], CA [15], Safe-CLIP [27], and SafeGen [17], along with three additional safeguard baselines: SD-v2.1 [41], LG [19], and SC [22]. Unless otherwise specified, we follow prior research [7], [17], [35] by using Stable Diffusion (version 1.4) as the base model for all experiments. We provide a description of the configuration of the baseline method in Section B.\nTraining Setup. We default to using ES to erase the seven inappropriate concepts in I2P unless stated otherwise. The (target concept, anchor concept) pairs used in the experiments are as follows:\n\u3008nude, dressed\u3009, \u3008 illegal, legal\u3009, \u3008violent, peaceful\u3009,\n\u3008hateful, loving\u3009, \u3008harassed, respected\u3009,\n\u3008harmful, helpful\u3009, \u3008shocking, soothing\u3009,\nwhere we use antonyms of the seven inappropriate concepts as their anchor concepts. In training ES, we automatically construct training samples using a synthesis strategy"}, {"title": "5.2. Effectiveness and Robustness Evaluation", "content": "We compare ES with nine baseline protection measures, including six safe generation baselines: NP, SLD, ESD, CA, Safe-CLIP, and SafeGen, as well as three additional protection baselines: SD-v2.1, LG, and SC. We evaluate the effectiveness of ES and the baselines in suppressing seven"}, {"title": "5.3. Specificity and Fidelity Evaluation", "content": "We use CLIP scores and FID scores to evaluate the specificity and fidelity of ES, comparing it with the base model SD-v1.4 without any safeguards and six safe generation baselines. In the experiments, we use the COCO-2017 [18] validation set as the real image set to support the calculation of FID scores. This validation set consists of 5,000 clean image-text pairs, where each image is annotated with a corresponding description. We use these annotations as generation prompts to produce 5,000 clean images and fix the same random seed for each prompt to ensure fair comparisons, making the generated image set consistent in content and quantity with the real image set for comprehensive quality evaluation."}, {"title": "5.4. Exploration on Hyperparameters", "content": "The performance of ES is influenced by two key hyperparameters, \u03b1 and \u03b2. The hyperparameter \u03b1 is applied during the inference stage to control the sanitization strength of the prompt embedding, while \u03b2 is used during the training stage to handle potentially inappropriate synonyms in clean prompts Pc within training sample pairs. We explore the im-"}, {"title": "6. Discussion", "content": "In this section, we analyze the interpretability and composability of ES, and compare the computational overhead introduced by ES. Last, we discuss some future works.\n(a) Interpretability Analysis. During the inference process of ES, S-Net provides interpretability by assigning scores between 0 and 1 to each token in the prompt. These scores measure the potential harmfulness, identifying the sources of inappropriate content generation. As shown in Figure 8, we analyze the interpretability of ES through examples of prompts and highlight three key observations.\nFirstly, ES effectively erases target concepts. As illustrated in the first row of Figure 8, ES assigns high scores of 0.99 and 0.61 to the tokens \"nude\" and \"person,\" respectively. This is because ES suppresses \"nude\" as a target concept during training to prevent explicit content, with the high score of 0.99 indicating that ES successfully identifies \"nude\" and applies strong sanitization measures to erase it. Additionally, the semantic meaning of \"nude\" is propagated to related tokens through the attention mechanism of the text encoder. For instance, as \"nude\" functions as an adjective describing \"person,\u201d this may explain why \"person\" receives a high score of 0.61 in the context of \"nude.\" Secondly, ES effectively recognizes synonyms of target concepts. As shown in the second and third rows of Figure 8, although \"topless\" and \"bare\" are not explicitly set as target concepts during training, they are synonyms of \"nude,\u201d and ES assigns them high scores of 0.85 and 0.40, respectively. This indicates that ES demonstrates good generalization to synonyms of target concepts, effectively sanitizing them even if they are not explicitly defined during training. Thirdly, ES does not affect non-target concepts. As shown in the last"}, {"title": "7. Conclusion", "content": "In this paper, we investigate the risk of inappropriate generation in T2I models. To address this issue, we introduce Embedding Sanitizer (ES), a novel safe generation"}, {"title": "Appendix A. Dataset Descriptions", "content": "Inappropriate Image Prompt (I2P): I2P is a handcrafted prompt dataset collected from lexica.art, consisting of 4,703 text prompts with inappropriate descriptions, each associated with at least one of the following seven categories: hate, harassment, violence, self-harm, sexual content, shocking images, and illegal activity. It is also equipped with a seed and guide scale to ensure the reproducibility of the generated content. We use this dataset to evaluate ES 's effectiveness in erasing these seven inappropriate concepts.\nAdversarial Prompt Datasets: Considering that certain safeguard baselines (e.g., SafeGen) are only applicable for erasing explicit content we focus on generating adversarial prompt datasets related to explicit content. (i) Ring-A-Bell (RAB). Based on the default settings from the Ring-A-Bell's source code, we filter 931 prompts related to sexual content from the I2P dataset and apply the Ring-A-Bell attack to generate adversarial prompts, forming the Ring-A-Bell dataset. (ii) Sneaky Prompt (SP). Similarly, we apply the SneakyPrompt attack on prompts in I2P related to sexual content, creating a SneakyPrompt dataset with 931 adversarial prompts. (iii) MMA-Diffusion (MMA). Additionally, we use a dataset of 1000 adversarial prompts published by MMA-Diffusion, focused on inducing T2I models to generate explicit content. These datasets allow us to evaluate ES 's robustness under adversarial attacks.\nCOCO-2017: COCO-2017 is a widely used image-text dataset providing diverse scenes, object combinations, and real annotations, covering 80 object classes and background contexts from the real world. Following prior research, we use the validation set of COCO-2017 to evaluate ES 's specificity and fidelity."}, {"title": "Appendix B. Baseline Descriptions", "content": "Safe Latent Diffusion (SLD): We use the official implementation of SLD, covering four safety levels-weak, medium, strong, and maximum. Following default settings, we set the safety level to medium and erase the following built-in target concepts: \"an image showing hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty.\""}]}