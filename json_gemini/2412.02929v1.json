{"title": "Panoptic Diffusion Models: co-generation of images and segmentation maps", "authors": ["Yinghan Long", "Kaushik Roy"], "abstract": "Recently, diffusion models have demonstrated impressive capabilities in text-guided and image-conditioned image generation. However, existing diffusion models cannot simultaneously generate a segmentation map of objects and a corresponding image from the prompt. Previous attempts either generate segmentation maps based on the images or provide maps as input conditions to control image generation, limiting their functionality to given inputs. Incorporating an inherent understanding of the scene layouts can improve the creativity and realism of diffusion models. To address this limitation, we present Panoptic Diffusion Model (PDM), the first model designed to generate both images and panoptic segmentation maps concurrently. PDM bridges the gap between image and text by constructing segmentation layouts that provide detailed, built-in guidance throughout the generation process. This ensures the inclusion of categories mentioned in text prompts and enriches the diversity of segments within the background. We demonstrate the effectiveness of PDM across two architectures: a unified diffusion transformer and a two-stream transformer with a pretrained backbone. To facilitate co-generation with fewer sampling steps, we incorporate a fast diffusion solver into PDM. Additionally, when ground-truth maps are available, PDM can function as a text-guided image-to-image generation model. Finally, we propose a novel metric for evaluating the quality of generated maps and show that PDM achieves state-of-the-art results in image generation with implicit scene control.", "sections": [{"title": "1 Introduction", "content": "Diffusion models have recently outperformed other generative models, demonstrating a strong ability to generate high-quality, photorealistic images and creative videos with high fidelity (Dhariwal and Nichol 2021; Saharia et al. 2022; Ramesh et al. 2022; Rombach et al. 2022; Nichol et al. 2021; Brooks et al. 2024; Ho et al. 2022a,b; Bar-Tal et al. 2024; Singer et al. 2022). Their success has drawn significant attention to generative AI, marking it as the next frontier following the achievements of AI in classification tasks. However, text-guided image generation often lacks control over the spatial positioning and structure of objects and backgrounds within the image (Zhang, Rao, and Agrawala 2023). Current diffusion models lack an understanding of objects and shapes because the diffusion process is uniformly applied to every pixel, without regard to the segment it belongs to. As a result, they may generate objects with unrealistic shapes and miss components mentioned in the text, leading to images that are perceived as artificial, as shown in the left column of Fig.1.\nTo address this issue, we propose teaching diffusion models to understand objects and scenes through segmentation maps, which provide detailed information about the image background that complements text prompts. Recent works, such as ControlNet, have demonstrated that using images with complex layouts as conditions, in addition to text prompts, can precisely control the generation process (Zhang, Rao, and Agrawala 2023). These studies show that image-guided generation can better align with users' specific imaginings expressed through both text and image prompts. Inspired by this, we anticipate that if diffusion models generate segmentation maps alongside images to provide inherent guidance, they can utilize spatial composition information to create more realistic images.\nSpecifically, we train diffusion models using panoptic segmentation maps, which unify object classes and background categories, providing information about both countable objects in the foreground and background elements (Kirillov et al. 2018). With advanced segmentation models like Segment Anything (Kirillov et al. 2023) easily segmenting images, segmentation maps hold potential as alternative or complementary training data for image generation tasks.\nThe co-generation of images and masks is nontrivial and challenging because it represents a dual problem. Unlike previous approaches that rely on either a clean image or a segmentation map as a stable condition to generate the other, our model tackles the complex task of simultaneously denoising both an image and its corresponding map (Zhang, Rao, and Agrawala 2023; Chen et al. 2023). To address this, we designed a new paradigm to solve the dual diffusion problem. Compared to using predefined segmentation maps, co-generation preserves the diversity and flexibility of the images. By generating panoptic segmentation maps, Panoptic Diffusion Models provide intrinsic control over image generation, while the images in turn ensure that the map generation remains coherent. Since the generation of both segmentation maps and images is guided by text, the model learns the correlation between text, images, and maps. With its enhanced scene understanding capabilities, Panoptic Diffusion Models represent a significant step towards photorealistic image generation.\nWe design both a one-stream Panoptic Diffusion Model and a two-stream model that incorporates a pretrained image generation stream. For training the two-stream model, we fix the image stream and efficiently fine-tune the segmentation stream. An alternative approach to map-guided image generation involves using two separate models: one to generate a segmentation map and another to generate an image based on that map. However, this method has several disadvantages compared to a unified model. First, few datasets provide paired images, descriptions, and masks, making a two-stream model with a pretrained backbone advantageous due to its ability to leverage more abundant image datasets. Second, it only allows for single-direction control from maps to images. Third, using two separate models is less efficient as they cannot run in parallel.\nAnother advantage of our model is that the readily available segmentation maps can benefit downstream computer vision tasks, such as autonomous driving. Additionally, the generated segmentation maps and image latents can be used as input conditions for larger diffusion models to produce higher-resolution images.\nThe major contributions are listed below:\n1. We propose a unified diffusion model that generates both images and panoptic segmentation maps. This model inherently understands scene structures through collaborative training with multimodal data, requiring no priors and providing self-control.\n2. We adapt the ODE solver for image denoising to facilitate simultaneous image and map generation. The iterative denoising of images and maps is interlinked, ensuring consistency between them.\n3. We develop a two-stream diffusion model and apply efficient fine-tuning techniques. This approach leverages pretrained diffusion models and extends their capabilities by incorporating segmentation maps.\n4. Our model directly provides segmentation maps for downstream tasks without the need for a separate segmentation model. These maps can scale up to four times the latent size without requiring a super-resolution model. We also introduce a new metric for evaluating the quality of the generated maps."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Diffusion Models for Image Generation", "content": "One of the initial works in this area, Denoising Diffusion Probabilistic Models (DDPM), use a Markov chain to gradually add scheduled noises to images in the forward process (Ho, Jain, and Abbeel 2020). The transition of the Markov chain is then parameterized by a neural network trained to predict the noise. During inference, a diffusion model starts from random noise and gradually reverses it to reconstruct the image.\nA well-known drawback of diffusion models is that they require a large number of steps to generate samples iteratively. To address this issue and improve efficiency, researchers have proposed various modifications to diffusion models (Nichol and Dhariwal 2021). For instance, DDIM demonstrates that diffusion models can operate in a non-Markovian manner, resulting in shorter generative chains (Song, Meng, and Ermon 2021). Additionally, distillation algorithms have been introduced to further accelerate the multi-step inference process by progressively distilling a teacher model into a student model(Salimans and Ho 2022; Berthelot et al. 2023; Ren et al. 2024).\nThe backbone neural network for a diffusion model is typically a UNet, which is composed of convolutional layers and attention blocks, or a diffusion transformer that relies solely on attention mechanisms (Rombach et al. 2022; Peebles and Xie 2022). Another variant, UViT, is a type of diffusion transformer that retains skip connections, allowing later layers to access information from earlier layers, thereby enhancing alignment (Bao et al. 2023).\nThere are three main methods for applying conditions to a diffusion model. The first approach, used in stable diffusion, involves cross-attention between the image and the conditions (Rombach et al. 2022). The second method appends condition embeddings as tokens to the image patches (Bao et al. 2023). The third approach uses an adaptive norm layer to integrate conditions with the hidden states (Peebles and Xie 2022). In our panoptic diffusion models, we opt for the second method because the transformer can leverage self-attention to learn the relationships between images and maps, treating them as conditions for each other.\nThe solver for our panoptic diffusion model is a modified version of DPM Solver++ (Lu et al. 2023). Solving the reverse of the diffusion process is equivalent to solving an ordinary differential (ODE) equation, which can be decoupled as an exactly computed linear part and a non-linear part approximated by neural networks (Lu et al. 2022).\nDuring inference, we apply classifier-free guidance similar to Nichol et al. (2021) and Ho and Salimans (2022). The diffusion model runs twice, once in an unconditioned setting and once in a conditioned setting, and the final output is obtained by taking a weighted sum of the unconditioned and conditioned outputs."}, {"title": "2.2 Panoptic Segmentation", "content": "Object detection requires generating bounding boxes and fine-grained masks, tasks traditionally accomplished by convolutional neural networks such as Fast R-CNN (Girshick 2015) and Mask R-CNN (He et al. 2017). In Carion et al. (2020), researchers introduced the use of transformers to generate binary masks by inputting object queries. Building on this, Cheng et al. (2022) proposed a collaboration between an image encoder backbone and a masked transformer to generate masks, where masked attention replaces cross attention.\nRecently, there has been growing interest in applying diffusion models to panoptic segmentation masks. For example, in Chen et al. (2023), a diffusion model comprising an image encoder and a mask decoder is used to extract image features and apply cross attention between these features and the masks. To address the challenge of handling discrete data with diffusion models, Chen, Zhang, and Hinton (2022) proposed converting panoptic masks into analog bits during preprocessing. On the other hand, Baranchuk et al. (2021) suggest that the intermediate features of diffusion models can capture semantic information useful for label-efficient segmentation. Similarly, DiffuMask (Wu et al. 2024) generates a synthetic image and a corresponding segmentation mask of an object using attention maps. However, directly extracting masks from attention maps lacks the ability to control the generated image in return. In contrast, our approach aims to co-generate pixel-level panoptic segmentation maps and images, allowing them to influence and control each other.\nWhile previous studies use diffusion models for panoptic segmentation based on given images, our work leverages an additional dataset of panoptic maps to train a model capable of generating both maps and images. The generated maps are then used to condition the image generation, producing a photorealistic result."}, {"title": "2.3 Image Guided Image Generation", "content": "Image guided image generation enables more precise control over the structure of the image and ensures faithfulness to users' illustrative inputs. The input for guidance can have various forms, such as segmentation maps and layouts (Rombach et al. 2022; Zhang, Rao, and Agrawala 2023). Stochastic Differential Editing (SDEdit) perturbs user inputs with Gaussian noises and then synthesizes images by reversing SDE (Meng et al. 2022). They show that when the reverse SDE is not solved from the ending point but a particular timestep, the generated images can achieve a good balance between faithfulness and realism. Make-a-scene introduces scene-based conditioning for image generation by optionally providing tokens from segmentation maps (Gafni et al. 2022), but this method heavily relies on explicit strategies for tackling panoptic, human, and face semantics. SpaText (Avrahami et al. 2023) employs CLIP (Radford et al. 2021) to convert local text prompts that describe segments into image space and concatenate to the channel dimension of noises. ControlNet can accept user inputs such as canny edges and segmentation masks for conditional control of image generation (Zhang, Rao, and Agrawala 2023). Prompt-to-prompt image editing controls the generation by cross-attention to ensure similarity between images generated from similar prompts (Hertz et al. 2022). InstructPix2Pix combines Prompt-to-prompt method with stable diffusion to generate pairs of images from pairs of captions for training, then train the model to modify image pixels following the instructions (Brooks, Holynski, and Efros 2023).\nThese approaches demonstrate that providing various forms of guidance can more accurately control the structure of generated images. Building on this insight, our method assumes that such guidance is crucial for enhancing image quality. Additionally, panoptic diffusion models inherently generate segmentation maps alongside images, offering built-in guidance without the need for additional user input beyond the text prompt."}, {"title": "2.4 Efficient Finetuning", "content": "To reduce the number of trained parameters or adapt the model to a new domain, previous works have designed adaptive blocks to fine-tune convolutional neural networks or transformers (Houlsby et al. 2019; Long et al. 2021; Mou et al. 2023). In our two-stream panoptic diffusion model, the map stream functions similarly to an adapter. To prevent any negative impact on the pretrained weights, we employ zero-initialized convolutional blocks as proposed in Zhang, Rao, and Agrawala (2023)."}, {"title": "3 Panoptic Diffusion", "content": ""}, {"title": "3.1 Preprocessing and Postprocessing of Segmentation Maps", "content": "As shown in Fig. 2,we process the panoptic segmentation maps through several steps before feeding them into the diffusion model. Instead of using a binary mask for each object, we load pixel-level panoptic annotations. In a segmentation map \\(M_o\\), each pixel's value is set to the corresponding category ID if it belongs to a segment; otherwise, its value is zero. We then convert these pixel values into analog bits (Chen, Zhang, and Hinton 2022). Analog bits are necessary because a standard diffusion model can only generate continuous data, while segmentation classes are discrete or categorical. Since the range of category ID is from 1 to 200, each pixel is represented by 8 binary bits. Prior to noise scheduling, these bits are scaled to the range [-1,1], matching the range of the latent input to the diffusion model. To ensure that the noise can effectively flip the bits, its absolute value must exceed one. Therefore, we set the noise added to the maps as \\(\u20ac_M \\sim N(0, 2 * I)\\).\nLatent diffusion models use latent representations of images encoded by an autoencoder as inputs. However, no autoencoder exists for encoding and decoding high-resolution segmentation maps into latents. We address this issue by pooling and using a larger patch size for the maps. To achieve high-resolution maps and enable more precise control, we first pool the maps to match one, two, or four times the height and width of the image latents. We use min pooling to prioritize smaller category numbers, as the COCO dataset annotations categorize 1-91 as thing categories and 92-200 as stuff categories. Next, we set the patch size of the maps to be one, two, or four times that of the images. This approach ensures that, after patchifying, the sizes of the image and map features align. Given that images have three RGB channels while maps have only one channel for the category ID before preprocessing, using a larger patch size is effective for extracting hidden features from segmentation maps. Consequently, this method allows us to generate higher-resolution maps without the need for an additional autoencoder or a larger latent size.\nFor postprocessing, the output values predicted by the diffusion model are thresholded at zero. Negative values are treated as zero bits, while positive values are considered one bits. Subsequently, these output bits are converted back into category numbers."}, {"title": "3.2 Forward Diffusion Process", "content": "In the forward pass of the diffusion process (Ho, Jain, and Abbeel 2020), random noise \\(e \\sim N(0, I)\\) is added to the image latent \\(x_o\\) according to the noise scheduler. With a total of n steps, each step updates the noisy image \\(x_t\\) from the previous step \\(X_{t-1}\\), using scaling factors \\(\u03b1\\) and \\(\u03b2\\) provided by the noise scheduler. This process forms a Markov chain. Consequently, the noisy image \\(x_t\\) can be simplified and calculated directly from \\(x_0\\).\n\\[x_t = \\sqrt{a_t} \\cdot x_{t-1} + \\sqrt{\u03b2_t}e \\]\n\\[x_t = \\sqrt{\\bar{a}} \\cdot x_o + \u03c3_te\\]\nwhere \\(\u03b1_t\\) are close to 1 and \\(\u03b2_t = 1 - \u03b1_t\\). The cumulative factor \\(\\bar{a} = \\prod_{i=1}^{t} \u03b1_i\\), and the noise is scaled by \\(\u03c3_t = \\sqrt{1 - \\bar{a}}\\).\nTo learn to denoise panoptic segmentation maps, we create another random Gaussian noise \\(\u20ac_M \\sim N(0,2 * I)\\) and add it to the ground-truth maps \\(M_o\\). The same noise scheduler is used to add noises to maps.\n\\[M_t = \\sqrt{\\bar{a}} M_o + \u03c3_te_M\\]\nwhere \\(M_t\\) is the noised map at timestep t."}, {"title": "3.3 Reverse Diffusion Process", "content": "The panoptic diffusion model outputs \\(e_\u03b8\\), which estimates the noise \\(e\\). Using this estimated noise, we compute the predicted image \\(x_0\\). When incorporating the map as an additional input to the diffusion model, the equation for predicting the image is given by Eq. 4. To accelerate inference, we utilize a fast DPM solver to compute \\(X_{t_{i-1}}\\) from \\(X_{t_i}\\) (Lu et al. 2022, 2023). By using discontinuous time steps \\(t_i\\) and \\(t_{i-1}\\), this method can skip intermediate steps, reducing the total number of sampling steps required. The first-order solver is described in Equation 5, where \\(h_i\\) represents the difference in the log signal-to-noise ratio between different steps (\\(h_i = log(\u03b1_{t_i}/\u03c3_{t_i}) \u2013 log(\u03b1_{t_{i-1}}/\u03c3_{t_{i-1}})\\)). Details on a third-order solver can be found in Appendix A.\n\\[x_0(x_{t_i}, M_{t_i}, C, t_i) = \\frac{x_{t_i} - \u03c3_{t_i}e_\u03b8(x_{t_i}, M_{t_i}, C, t_i)}{\\sqrt{\\bar{a}}}\\]\n\\[x_{t_{i-1}} = \\frac{\u03c3_{t_{i-1}}}{\u03c3_{t_i}} x_{t_i} - \\frac{\u03b1_{t_i} (e^{h_i} -1) x_\u03b8(x_{t_i}, M_{t_i}, C, t_i)}{\u03c3_{t_i}}\\]\nThe other output of a panoptic diffusion model is \\(M_\u03b8\\), which is a prediction of \\(M_o\\). Drawing inspiration from DPM-solver++, we use the following equation to estimate \\(M_{t_{i-1}}\\) from the previous step. It is important to note that the model directly estimates \\(M_o\\) rather than the noise added to the segmentation map, as predicting \\(e_M\\) does not provide effective guidance for the images. By training the diffusion model with panoptic segmentation maps, it incorporates intrinsic self-control into the image generation process.\n\\[M_{t_{i-1}} = \\frac{\u03c3_{t_{i-1}} M_{t_i} - \u03b1_{t_i} (e^{h_i} -1) M_o(x_{t_i}, M_{t_i}, C, t_i)}{\u03c3_{t_i}}\\]\nIn a special case where ground truth maps are provided as conditions, the diffusion model will focus solely on predicting the images. This allows users to have customized control for generating desired images, similar to existing methods (Zhang, Rao, and Agrawala 2023). However, this approach limits the diversity of the generated images.\nSince the generation of \\(x_{t-1}\\) and \\(M_{t-1}\\) relies on \\(x_t\\) and \\(M_t\\), they form a dual problem. Improvements in the quality of the generated masks and images influence each other. Consequently, according to the scaling law, a larger diffusion model can produce more accurate masks, which in turn provides better control and further enhances image quality."}, {"title": "3.4 Dual training and generation", "content": "Let the inputs to a panoptic diffusion model at each timestep be image latent \\(x_t\\), mask \\(M_t\\), text condition encoded by a text encoder C, and timestep t. The conditional probability of \\(x_{t-1}\\) and \\(M_o\\) is given by\n\\[P(x_{t-1}, M_o | x_t, M_t, c) = P(x_{t-1} | x_t, M_t, M_o, c) \\cdot P(M_o | x_t, M_t, c)\\]\nEquation 6 show that it is feasible to predict the segmantation map \\(M_o\\) first, then use it as a condition to predict \\(x_{t-1}\\). However, when using a unified model to predict both \\(x_{t-1}\\) and \\(M_o\\), the intermediate features already contain the segmentation information used to predict \\(M_o\\). Through self-attention, the map features can inherently condition \\(x_{t-1}\\). Therefore, it is reasonable to predict \\(x_{t\u22121}\\) and \\(M_o\\) simultaneously. By taking the logarithm of the probability, we can optimize the model by combining the losses associated with image denoising and segmentation map generation.\n\\[log P(x_{t-1}, M_o | x_t, M_t, c) = log P(x_{t-1} | x_t, M_t, M_o, c) + log P(M_o | x_t, M_t, c)\\]\nThe training algorithm is outlined in Algorithm 1. We use Mean Squared Error (MSE) loss to optimize the predicted noises for both image and segmentation map denoising. Specifically, the target for image denoising is the noise \\(e\\), while the target for mask generation is the ground-truth \\(M_o\\). The losses for images and maps are summed to perform gradient backpropagation. During inference, the diffusion model iteratively denoises both images and maps, as detailed in Algorithm 2.\nClassifier-free Map Guidance Classifier-free diffusion guidance was introduced to balance sample quality and diversity without relying on a classifier (Ho and Salimans 2022). This approach involves alternating between an unconditional and a conditional diffusion model during training, and using a weighted sum of the results from both models during inference. For panoptic diffusion models, we only remove the text conditions while keeping the map conditions active. Specifically, we set the context condition to empty text with a probability of 0.1 during training (C = \u00d8). When the context is empty, the diffusion model is guided solely by the bidirectional control between images and segmentation maps. This setup allows the map generator to provide classifier-free guidance and enhance diversity. Let \\(e_{\u03b8_1}\\) represent the output with regular conditioning and \\(e_{\u03b8_2}\\) represent the output with empty text. During inference, these outputs are weighted by \\(\u03b3\\), which is set to 1.0 by default.\n\\[\u20ac_\u03b8 = \u20ac_{0_1} + \u03b3(\u20ac_{0_1} \u2013 \u20ac_{0_2})\\]\\[M_\u03b8 = M_{0_1} + \u03b3(M_{0_1} \u2013 M_{0_2})\\]"}, {"title": "3.5 Architecture of Panoptic Diffusion Models", "content": "One-stream Panoptic Diffusion Models We first modify a U-ViT to a panoptic diffusion model (Bao et al. 2023). We start by patchifying the map input \\(M_t\\) using a convolutional layer and adding positional embeddings. These map embeddings are then concatenated with the image, text, and time embeddings and processed through attention blocks. Since U-ViT treats all inputs as tokens and applies self-attention among them, the segmentation maps can be treated as tokens in the same manner. At the end of the transformer, we separate the features related to images and segmentation maps, using distinct convolutional layers to unpatchify and predict the outputs.\nIn the special case that the ground truth maps are provided, only the loss of images will be used for optimization. To ensure that map features are included in the gradient backpropagation, they are added to the image features before the final output convolutional layer.\nTwo-stream Panoptic Diffusion Models To leverage a pretrained model as the backbone, we design a two-stream diffusion model consisting of a pretrained image stream and a segmentation map stream, as illustrated in Fig. 3. During fine-tuning, the transformer layers of the image stream are kept frozen while the map stream is adjusted. The map stream processes image features and conditions from the previous block, then concatenates them with map features. Through self-attention, the map features and image features become interrelated within the map stream. The auxiliary image feature output from the map stream is added back to the image stream via a zero-convolution layer. This setup ensures specific control over the image stream and allows gradients to be backpropagated from the loss of image generation. The zero-convolution layer has zero initial weights and no bias (Zhang, Rao, and Agrawala 2023). Unlike ControlNet, which uses only the encoder part of the map stream to generate control signals, our model employs encoder-decoder U-shaped transformers in both streams to co-generate images and segmentation maps."}, {"title": "3.6 Evaluation metric for generated maps", "content": "We propose a new metric to evaluate the quality of generated segmentation maps by measuring the difference in the number of pixels labeled as each category. While Panoptic Quality (Kirillov et al. 2018) uses Intersection over Union (IoU) to assess segmentation maps based on the weighted sum of true positives, false positives, and false negatives, this approach is not suitable for maps generated by diffusion models. These models produce maps probabilistically based on text prompts and co-generated images, making it impractical to compute IoU with ground-truth maps due to inherent differences in the generated images. Instead, we introduce the Mean Count Difference (MCD) metric. MCD evaluates the quality of generated maps by counting the frequency f of each category in both the ground-truth and generated maps, then summing their absolute differences. This sum is divided by the total number of pixels, calculated as the product of the height and width. Given that object locations on the generated map are not fixed, comparing category frequencies rather than direct pixel values provides a more meaningful assessment. The metric ranges from [0, 2], where zero indicates identical segmentation maps and larger values indicate greater differences.\n\\[f = bincount(M_o); f' = bincount(\\hat{M_o})\\]\n\\[MCD = \\frac{\u03a3(|f - f'|)}{H * W}\\]"}, {"title": "4 Experiments", "content": "We train our model using the COCO2017 dataset (Lin et al. 2015), which includes both panoptic segmentation maps and image captions. The COCO2017 dataset comprises 118k training samples and 5k validation samples. Images are projected into latent space using a VAE model provided by Stable Diffusion (Rombach et al. 2022; Gu et al. 2021), while text conditions are encoded using the CLIP encoder from OpenAI (clip-vit-large-patch14) (Radford et al. 2021). We implement both one-stream and two-stream panoptic diffusion models (PDM) based on U-ViT (Bao et al. 2023). In contrast to commercial models with billions of parameters, our models are significantly smaller. The one-stream PDM has 45 million parameters, while the two-stream PDM has 95 million parameters. The image latent size is 32 \u00d7 32 \u00d7 4, with a height and width of 32 and a latent channel count of 4. The segmentation map's height and width can be 32, 64, or 128, depending on the patch factor, and it has 8 channels, representing 8 analog bits after conversion. The diffusion model's output image latents are decoded by a VAE decoder to produce 256 \u00d7 256 images."}, {"title": "4.1 Quantitative Evaluation", "content": "We evaluate the quality of generated images using FID (Heusel et al. 2017) and CLIP scores (Hessel et al. 2022). FID assesses the quality and fidelity of the generated images by employing an Inception model, while CLIP scores gauge how well the generated images correspond to the text prompts. For CLIP scores, we use the ViT-B/32 model (Radford et al. 2021). We generate 30,000 images and segmentation maps from 5,000 text files in the COCO dataset's validation set, with each text file containing five captions describing the same scene. We compute the average CLIP scores by comparing these five captions with the generated images.\nIn Table.1, we compare the FID and CLIP scores of our models with those of state-of-the-art methods. The results indicate that while our panoptic diffusion models (PDMs) are trained with a combined loss of images and segmentation maps, they achieve comparable fidelity (FID scores) and improved relevance between image and text (higher CLIP scores). This improvement is due to the enhanced connectivity between the image, text, and segmentation map. The two-stream PDM performs better due to its pretrained stream and larger number of parameters. When ground-truth maps are provided, the model performs optimally because it focuses solely on optimizing image generation.\nIncreasing the patch factor results in a higher MCD because generating higher-resolution maps with a fixed number of latents becomes more challenging. This creates a trade-off between map resolution and quality. We find that a patch factor of 2 offers the best balance, yielding the highest FID and CLIP scores. However, increasing the patch factor to 4 results in worse performance, suggesting that unbalanced patch sizes for maps and images are detrimental."}, {"title": "4.2 Qualitative Evaluaiton", "content": "In Fig. 1, we compare the images and masks generated by PDM with images generated by U-ViT. By training with segmentation masks, PDM learns that the shape of a stop sign should be octagon, while U-ViT cannot guarantee to generate an octagon stop sign. Similarly, PDM ensures to generate correct shapes for a fire hydrant and a human. In the last row of Fig. 1, PDM generates masks for not only elephants but also for the river, while a regular diffusion model misses the required component of the text prompt.\nFigure 4 displays images generated with either ground-truth segmentation maps or co-generated maps. The generated maps in the bottom left show objects of the same categories and similar shapes as the ground-truth maps. The images on the right are conditioned on these segmentation maps, demonstrating the PDM's ability to simultaneously generate correlated images and maps. While images generated with ground-truth maps exhibit slightly better quality, co-generation removes the need for a segmentation input and produces diverse maps and images. Note that the pixel values in the generated segmentation maps correspond to category IDs (1-200), which are mapped to random RGB colors for visualization. The color map used is detailed in Appendix C.\nAdditional examples generated by PDMs are provided in Appendix B. Zero-shot results on the CIFAR-10 dataset demonstrate that our model can generate segmentation maps for various categories across different image datasets."}, {"title": "4.3 Ablation study", "content": "Effect of the patch factor We evaluate the impact of different patch sizes on map resolution, as illustrated in Figure 5. When the patch size for segmentation maps is set to four times that of the images, the resulting maps have a resolution of 128x128. However, these larger maps may include hallucinated details that could misguide image generation. This issue arises due to the disparity in patch sizes and the model's limited hidden dimension of 768, which complicates accurate prediction for a 128x128 map.\nReplacing noisy map inputs with zero To assess whether PDMs learn to denoise the segmentation map or extract it from the image latent, we replace noisy map inputs \\(M_t\\) with zero inputs during training. The results reveals that while a two-stream model can still generate images (FID=18.94), it cannot generate readable maps. This indicates that a panoptic diffusion model does not solely depend on image features for map generation, unlike the approach in DiffuMask (Wu et al. 2024). Hence, noisy map inputs \\(M_t\\) are crucial for predicting \\(M_o\\)."}, {"title": "5 Conclusion", "content": "In conclusion, we introduce the Panoptic Diffusion Model (PDM), a pioneering approach that simultaneously generates images and panoptic segmentation maps from a given prompt. Unlike previous diffusion models that either depend on pre-existing segmentation maps or generate them based on images, PDM inherently understands and constructs scene layouts during the generation process. This innovation enables PDM to produce more creative and realistic images by leveraging segmentation layouts as intrinsic guidance. This research lays the groundwork for future advancements in diffusion models, offering a robust framework for co-generation of images and segmentation maps."}, {"title": "Appendix A Fast DPM solver for segmentation maps", "content": "We modify the first order and third order DPM-solver++ to solve the image and map of the previous step given xt", "m_t)": "n x_t=(sigma_t/sigma_s) *x+(alpha_t*\n phi_1) *x_0\n #update M(t-1) based on M[t", "x_t,m_t,C,t)": "n#First step\nx_0, m_0= diffusionModel(x_t,m_t,C,s\n)\nx_s1=(sigma_s1/sigma_s)*x+(alpha_s1*\nphi_11)*x_0\nm_s1= (sigma_s1/sigma_s) *m_t +\n(alpha_s1*phi_11)*m_0\n#Second step\nx_02, m_02= diffusionModel (x_s1,m_s1\n,C"}]}