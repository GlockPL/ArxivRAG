{"title": "SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding", "authors": ["Shuhao Liao", "Weihang Xia", "Yuhong Cao", "Weiheng Dai", "Chengyang He", "Wenjun Wu", "Guillaume Sartoretti"], "abstract": "The Multi-Agent Path Finding (MAPF) problem\naims to determine the shortest and collision-free paths for\nmultiple agents in a known, potentially obstacle-ridden envi-\nronment. It is the core challenge for robotic deployments in\nlarge-scale logistics and transportation. Decentralized learning-\nbased approaches have shown great potential for addressing\nthe MAPF problems, offering more reactive and scalable solu-\ntions. However, existing learning-based MAPF methods usually\nrely on agents making decisions based on a limited field of\nview (FOV), resulting in short-sighted policies and inefficient\ncooperation in complex scenarios. There, a critical challenge is\nto achieve consensus on potential movements between agents\nbased on limited observations and communications. To tackle\nthis challenge, we introduce a new framework that applies sheaf\ntheory to decentralized deep reinforcement learning, enabling\nagents to learn geometric cross-dependencies between each\nother through local consensus and utilize them for tightly coop-\nerative decision-making. In particular, sheaf theory provides a\nmathematical proof of conditions for achieving global consensus\nthrough local observation. Inspired by this, we incorporate a\nneural network to approximately model the consensus in latent\nspace based on sheaf theory and train it through self-supervised\nlearning. During the task, in addition to normal features\nfor MAPF as in previous works, each agent distributedly\nreasons about a learned consensus feature, leading to efficient\ncooperation on pathfinding and collision avoidance. As a result,\nour proposed method demonstrates significant improvements\nover state-of-the-art learning-based MAPF planners, especially\nin relatively large and complex scenarios, demonstrating its\nsuperiority over baselines in various simulations and real-world\nrobot experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "As intelligent robots advance, the application of large-\nscale Multi-Agent Path Finding (MAPF) has become in-\ncreasingly important in scenarios such as warehouse automa-\ntion, airport management, and robotic fleets [1]-[4]. MAPF\ninvolves planning collision-free paths for multiple agents\nfrom their start positions to designated goals. This NP-hard\nproblem presents significant challenges in scalability and\ncomputational efficiency due to the exponential growth of\ncomplexity with respect to the number of agents.\nRecently, the MAPF community has started looking to\nMulti-Agent Reinforcement Learning (MARL) to generate\nfast and scalable solutions [5]-[7]. Moreover, MARL has\ngained significant traction in multi-robot systems, where\nagents collaborate in decentralized settings to achieve global\nobjectives [8]-[13]. These learning-based approaches rely on"}, {"title": "II. RELATED WORK", "content": "Recent years have seen a growing interest in solving\nMAPF problems using MARL. The pioneering work, PRI-\nMAL, introduced a combination of RL and imitation learning\nto plan paths through fully decentralized policies within\na partially observable environment [15]. PRIMAL utilized\nthe Asynchronous Advantage Actor-Critic algorithm as un-\nderlying RL algoritihm, with all agents sharing the same\nparameters, while imitation learning was based on behavior\ncloning from data generated by the ODrM* planner. This\napproach was later extended in PRIMAL2 to address lifelong\nMAPF scenarios, incorporating learned conventions to en-\nhance cooperation among agents, particularly in highly struc-\ntured environments [5]. Subsequent research has explored\ncommunication learning as a promising approach to further\nenhance solution quality. For instance, works like MAGAT\nand DHC [16], [20] introduced Graph Neural Networks [21]\nfor communication learning, where each agent is treated\nas a node, and decisions are made based on aggregated\ninformation from neighboring agents. DCC, on the other\nhand, developed a selective communication strategy that\ndetermines whether an agent's decision should be influenced\nby its neighbors [22]. Moreover, PICO integrated planning\npriorities from a classical coupled planner into an ad-hoc"}, {"title": "B. Sheaf Applications", "content": "Sheaf theory addresses the local-to-global problem in\nmulti-agent systems by allowing the coherent integration of\nlocal data into global structures [24]. Grothendieck then ex-\ntended its application in algebraic geometry through scheme\ntheory, enabling the handling of complex structures like sin-\ngularities [25]. In distributed systems, sheaf theory provides a\nframework for consensus on complex data structures [26]. In\nsignal processing, it manages distributed data with complex\ndependencies, and in network communication, it optimizes\ninformation flow and reduces communication costs [27].\nIn network science, sheaves provide enhanced descriptions\nof network structures, capturing the nature of relationships\nbetween nodes. Within this framework, opinion dynamics\nuses discourse sheaves to model how opinions evolve and\ninteract within social networks [28]. Additionally, Bodnar\nproposed neural sheaf diffusion to learn sheaf laplacians from\nlower-order data, providing a novel topological perspective\non heterophily and oversmoothing in GNNs [29]-[31]."}, {"title": "III. MAPF AS AN RL PROBLEM", "content": null}, {"title": "A. MAPF Problem Statement", "content": "The classical MAPF problem considers a set of agents\n$\\mathcal{N} = \\{\\alpha_1,..., \\alpha_n\\}$ and an undirected graph $G = (V,E)$,\nwhere V represents the set of vertices and E represents\nthe set of edges. Each agent i is assigned a distinct start\nvertex ($\\xi_i \\in V$) and a distinct goal vertex ($g_i\\in V$).\nTime is discretized into uniform steps. At each time step\n$t = 0,1,2,...$, an agent has the option to either move\nto an adjacent vertex or remain stationary at its current\nvertex. A path for agent $a_i$ is defined as a sequence of\nvertices, either adjacent (indicating movement) or identical\n(indicating waiting), starting from the agent's start vertex $\\xi_i$\nand ending at its goal vertex $g_i$. Collisions between agents\nare categorized as either vertex collisions, which occur when\ntwo agents $a_i$ and $a_j$ occupy the same vertex $v$ at the same\ntime $t$, or edge collisions, which occur when two agents\n$a_i$ and $a_j$ simultaneously traverse the same edge (u, v) in\nopposite directions at time t. A valid solution to the MAPF\nproblem is a set of collision-free paths, one for each agent.\nThe optimality of the solution is typically evaluated by the\nsum of the arrival times of all agents at their respective goal\nvertices."}, {"title": "B. RL Environment Setup", "content": "Remaining consistent with the standard MAPF problem,\nwe use the following setup: the map is a 2D discrete 4-\nconnectivity grid world, where each grid is a vertex con-\nnected to its neighbors by edges, and agents can move to the\nfree cell adjacent to their location or stay idle at each time\nstep. An episode terminates when all agents are on their goals\nat the end of a time step (success) or when the number of\ntime steps reaches the pre-defined limit (failure). We utilize\nthe room-like map generator proposed by ALPHA. The\ngenerated room maps contain corridors of varying widths,\nwhich closely resemble real offices, warehouses, and other\nenvironments. Unlike the temporal impact of loose obstacles\nin random maps, continuous obstacles in such highly struc-\ntured maps may significantly affect current decision-making\neven across substantial distances."}, {"title": "1) Observation:", "content": "In our settings, each agent can only\npartially observe the environment limited by the size of the\nFOV l xl, where l is smaller than the total environment size\nm. To ensure the agent remains centered within its FOV, l is\nselected as an odd number. The observation data is organized\ninto two primary channels: the first channel is a binary matrix\nthat represents obstacles within the FOV, and the second\nchannel is another binary matrix that indicates the positions\nof other agents when they fall within the FOV. Additionally,\nthe input to our model includes four heuristic channels, each\ncorresponding to one of the possible movement directions:\nUp, Down, Left, and Right. These heuristic channels share\nthe same dimensions as the FOV, and each cell is marked\nas 1 if taking the associated action would move the agent\ncloser to its goal from that location. By incorporating these\nheuristic channels, the agent can infer the best direction to\nmove without the need to explicitly include the goal location\nin the input [16]."}, {"title": "2) Action Space:", "content": "In our grid-world environment, agents\noperate within a discrete action space. At each time step,\nan agent can choose to move to one of the adjacent grid\ncells or remain stationary. We do not consider diagonal\nmovements thus each agent has a total of five possible"}, {"title": "3) Reward:", "content": "We follow the DHC setup [16], assigning the\nsame penalty for each agent's movement and for staying\naway from the goal. The same reward setting ensures a fairer\ncomparison."}, {"title": "IV. LEARNING TECHNIQUES", "content": null}, {"title": "A. Dynamic Agent Graph", "content": "We define a dynamic agent graph $G = (V, E)$ to represent\nthe relationships between agents based on their FOV, as\nillustrated in Figure 3. Nodes V represent n agent $a_i$, $i =$\n(1,2,...,n). An edge $e \\in E$ is established between two\nagents if they are within each other's FOV, indicating that\nthey can potentially interact or need to consider each other's\npresence while planning their paths. This dynamic graph\nreflects the changing visibility and proximity of agents as\nthey move through the environment, making it crucial for\ncoordinating their actions and avoiding collisions."}, {"title": "B. Cellular Sheaf in MAPF", "content": "A cellular sheaf is an algebraic-topological structure as-\nsociated with a graph that attaches spaces of data to nodes\nand edges [32]. To be precise, a cellular sheaf (G, F) on a\ndynamic agent graph $G = (V, E)$ consists of:\n\u2022 A vector space F(v) for each v \u2208 V,\n\u2022 A vector space F(e) for each e \u2208 E,"}, {"title": "C. Sheaf-Informed DQN", "content": "As shown in Figure 2, we use a observation encoder to\nencode the observations into stalks (orange). Since MAPF\nagents are homogeneous, meaning they have identical char-\nacteristics, their corresponding restriction maps are also\nidentical. Thus, we denote the same restriction maps (green)\nfor all agents as M, which is learned through the section\nFC's training process. Additionally, we incorporate the global\nsection loss(red) directly into the network updating, ensuring\nthat the learned policy respects the underlying sheaf structure\nand maintains consistency across the observation space.\nDQN learns the action value function using neural net-\nworks. To incorporate the advantage and value functions,\nwe can define the advantage function A(s, a) and the value\nfunction V(s), where Q(s, a) = V(s) + A(s, a). The agents\naccess the current states st \u2208 S and selects an action at \u2208 A\naccording to a policy \u03c0 at each time step t. The agent's\nobjective is to maximize the expectation of the discounted\ntotal return $R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ...$, where rt is the\nreward received at time t.\nQ-Learning utilizes an action value function for policy \u03c0\nas $Q^{\\pi}(s, a) = E[R_t | S_t = s, a_t = a]$ and can be recursively\ndefined by $Q^{\\pi}(s,a) = E_s[r + \\gamma E_{a'\\sim\\pi}[Q^*(s', a')]]$ The\noptimal action value, $Q^*(s, a) = max_{\\pi} Q^{\\pi}(s, a)$, satis-\nfies the Bellman optimality equation $Q^*(s,a) = E_s[r +$\n$\\gamma max_{a'} Q^*(s', a') | s,a]$. The optimal policy is trained by\nminimizing the loss LQ. Here, the parameters of the target\nnetwork are updated periodically. In partially observable\nenvironments, agents generally need to condition on an state-\naction history $L_Q = E_{(s,a,r,s')}[(Q(s, a) - y)^2]$, where $y =$\n$r+ \\gamma max_{a'} Q(s', a').$\nWhere $o_i^{a_1}$ represents the observation vectors of agent $a_i$\nwithin the FOV of agent $a_1$, and M denotes the restriction\nmap, n is the number of agents. The function $l_{sec}$ sums the\nmapping discrepancies between stalks, and by minimizing"}, {"title": "V. EXPERIMENTS", "content": "In this section, we evaluate SIGMA through comprehen-\nsive simulation experiments, comparing its performance with\nSOTA baselines. We also conduct ablation studies to assess\nthe impact of each component of our approach. Additionally,\nwe test the robustness of the trained model by deploying it\nin simulation and real world environment."}, {"title": "A. Main Results Comparison", "content": "For our experiments, we train our models on structured\nenvironments of varying sizes, randomly chosen from a\nuniform distribution between 10 and 40, while consistently\ndeploying 5 agents. During testing, we explore environments\nof 20, 40, and 60 sizes, scaling the number of agents from\n4 up to 128.\nOur evaluation include a comparison with 6 SOTA\nMAPF solutions: PRIMAL [15], MAPPER [35], DHC [16],\nDCC [22], SCRIMP [6], and ALPHA [17]. Additionally,\nwe benchmark against the searchbased, bounded-optimal\ncentralized planner ODrM* with an inflation factor of \u20ac =\n2.0 [18]. For a fair comparison, each planner was tested on\nthe same set of 200 randomly-generated environments.\nWe employ three metrics to assess performance: 1)\nEpisode Length(EL): This measures the efficiency of a\nsolution by counting the number of actions agents take\nto reach their goals within a single episode. 2) Arrival\nRate(AR): This is the percentage of agents that reach their\ngoals across all episodes. 3) Success Rate(SR): This metric\nevaluates a planner's ability to completely fulfill a task.\nNotably, learning-based methods may show a low SR but\nstill have a high AR, which highlights the importance of AR\nin evaluating episodes that nearly reach completion without\nbeing deemed total failures. The results are presented in\nTable II.\nIn our experiments, SIGMA consistently outperforms\nother learning-based planners in terms of SR across all\ntasks. Notably, as the number of agents increases, SIGMA's\nimprovement in SR significantly exceeds that of the baseline\nplanners. For instance, in complex scenarios where most\nlearning-based planners struggle or fail to solve the problems,\nsuch as with 128 agents on a 40\u00d740 map, SIGMA achieves a"}, {"title": "B. Ablation Analysis", "content": "Our method focuses on encoding the sheaf structure(stalks\nand restriction maps) and integrating global section loss.\nThese elements are applied to both the input of the advantage\nfunction and the loss function for updating the network to\nensure the correctness of the sheaf structure. To analyze\nthe importance of these elements, we experimented with\nthree ablation variants of SIGMA: 1) Encoded Stalk(ES):\nThis variant tested only the stalks' encoding effectiveness\nby removing global section loss and restriction maps. 2)\nWeighted Penalty(WP): This setup assessed the influence of\nglobal section loss within the loss calculation by excluding\nthem. 3) Feature Impact(FI): We evaluated the impact of\nremoving restriction maps from the advantage function while\nkeeping other elements constant."}, {"title": "C. Experimental Validation", "content": "As shown in Figure 6, Figure 6a represents the simula-\ntion environment, and Figure 6b represents the real world\nenvironment. In this setup, each cell in the real world\nenvironment measures 0.3m on each side, slightly larger\nthan the agents to ensure each agent occupies only one\ncell on the map. We employ 3 robots, each equipped with\nMecanum wheels and measuring approximately 0.23m x\n0.2m. The accurate positions of these robots are tracked\nusing the OptiTrack Motion Capture System. The starting\nand goal positions of the agents are randomly configured.\nIn this experiment, although the robots recognize the virtual\npositions of obstacles and are programmed to avoid these\nareas, there are no physical obstacles in the real environment,\npreventing any interference with the line of sight needed for\nthe OptiTrack motion capture system."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we introduce SIGMA, a novel MAPF planner\nthat pushes beyond the constraints of limited FOV commonly\nfound in existing learning-based MAPF planners by utilizing\nsheaf theory to let all agents reason about and reach a\nteam-wide consensus. Our approach efficiently encodes the\nsheaf structure within MAPF, incorporating global section\nloss to measure consistency. Through extensive experiments\nconducted on highly structured maps with varying team\nsizes and environmental complexities, SIGMA consistently\noutperforms SOTA learning-based MAPF planners and a\nbounded-optimal search-based planner in most scenarios. By\nrigorously proving the transition from local observation to\nglobal consensus, this work proposes a novel perspective to\nthe MAPF research community.\nIn future work, we plan to focus on enriching the intri-\ncate relationships among agents by building consensus on\nmore complex graphs. This will involve exploring advanced\nmodels and techniques that can handle and interpret the\ninteractions and dependencies within larger, more complex\nnetwork structures."}]}