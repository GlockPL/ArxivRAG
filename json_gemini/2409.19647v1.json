{"title": "Fine-Tuning Hybrid Physics-Informed Neural Networks for Vehicle Dynamics Model Estimation", "authors": ["Shiming Fang", "Kaiyan Yu"], "abstract": "Accurate dynamic modeling is critical for autonomous racing vehicles, especially during high-speed and agile maneuvers where precise motion prediction is essential for safety. Traditional parameter estimation methods face limitations such as reliance on initial guesses, labor-intensive fitting procedures, and complex testing setups. On the other hand, purely data-driven machine learning methods struggle to capture inherent physical constraints and typically require large datasets for optimal performance. To address these challenges, this paper introduces the Fine-Tuning Hybrid Dynamics (FTHD) method, which integrates supervised and unsupervised Physics-Informed Neural Networks (PINNs), combining physics-based modeling with data-driven techniques. FTHD fine-tunes a pre-trained Deep Dynamics Model (DDM) using a smaller training dataset, delivering superior performance compared to state-of-the-art methods such as the Deep Pacejka Model (DPM) and outperforming the original DDM. Furthermore, an Extended Kalman Filter (EKF) is embedded within FTHD (EKF-FTHD) to effectively manage noisy real-world data, ensuring accurate denoising while preserving the vehicle's essential physical characteristics. The proposed FTHD framework is validated through scaled simulations using the BayesRace Physics-based Simulator and full-scale real-world experiments from the Indy Autonomous Challenge. Results demonstrate that the hybrid approach significantly improves parameter estimation accuracy, even with reduced data, and outperforms existing models. EKF-FTHD enhances robustness by denoising real-world data while maintaining physical insights, representing a notable advancement in vehicle dynamics modeling for high-speed autonomous racing.", "sections": [{"title": "I. INTRODUCTION", "content": "ACCURATE vehicle dynamics modeling is essential for the development of algorithms that enable effective control of autonomous vehicles, particularly in high-speed and agile driving scenarios. These models enable precise prediction and response to real-world driving conditions, which is critical for ensuring safety during rapid vehicle maneuvers. While kinematic models provide a simpler approach by focusing on geometric parameters, they fail to capture key dynamic aspects such as tire forces, suspension behavior, throttle response, and drivetrain effects. This highlights the importance of dynamic models, which delve deeper into the vehicle performance and allow for the estimation of longitudinal and lateral forces to better predict vehicle motion. A key challenge in dynamic modeling is the estimation of sensitive coefficients in tire models, such as the Pacejka coefficients [1], which can be both complex and time-consuming to estimate accurately.\nTraditional methods for estimating tire models come with several limitations, including reliance on initial guesses, long fitting times, and challenges associated with testing setups. Deep neural networks (DNNs) have been employed to handle the non-linearity in vehicle dynamics modeling [2], offering a simpler solution that bypasses the requirement for specialized testing equipment. Yet, purely data-driven machine learning approaches often fail to capture the underlying physical constraints of the system and are highly dependent on large datasets. Moreover, these models can produce outputs that do not reflect the real system dynamics.\nTo address these shortcomings, Physics-Informed Neural Networks (PINNs) have been proposed. PINNs incorporate governing equations and physical laws into neural network architectures, improving both model efficiency and accuracy, while requiring less data. Despite these advantages, PINNs still depend on high-quality, low-noise data. This has led researchers to explore techniques such as data augmentation, transfer learning, and novel data collection strategies. PINNs have been shown to be effective in various scientific domains, including fluid dynamics and structural mechanics. In the context of autonomous driving, methods like the Deep Pacejka Model (DPM) [3] and Deep Dynamics Model (DDM) [4] have been applied to high-speed racing scenarios. However, these models have limitations. For instance, DPM relies heavily on sampling-based control and unconstrained parameter estimation, with tests only conducted in simulation environments. While DDM offers improvements by constraining estimation ranges, it still struggles with smaller datasets and noisy data during training [4].\nWhile supervised PINNs (which rely on input data to reveal physical laws) require large datasets, unsupervised PINNs (which leverage governing equations directly) and hybrid approaches require fewer labeled data points. Building on this, we propose the Fine-Tuning Hybrid Dynamics (FTHD) method, which, to the best of our knowledge, is the first to use a fine-tuning hybrid PINN model to estimate vehicle dynamics. Furthermore, we introduce the Extended Kalman Filter FTHD (EKF-FTHD), the first model to incorporate an embedded EKF to denoise real-world data while maintaining physical insights.\nThe major contributions of this paper are as follows:\n1) We present FTHD, a fine-tuned hybrid PINN method designed to estimate key vehicle parameters, including Pacejka tire coefficients, drivetrain coefficients, and mo-"}, {"title": "II. RELATED WORK", "content": "Vehicle dynamics modeling for autonomous driving has been extensively studied, with models ranging from simple point mass representations to more complex single-track (bicycle) dynamic models (STMs) and multi-body systems. The choice of model often depends on the complexity of the driving scenario. For low-speed, non-slip conditions, kinematic models are typically used as they focus on geometric aspects. However, these models struggle to represent more complex driving conditions, especially during high-speed maneuvers where dynamic factors such as tire forces and vehicle inertia become critical. While the bicycle model strikes a balance between simplicity and accuracy in high-speed and drifting scenarios, maintaining accurate parameter estimation for this model remains a challenge [6]. Several data-driven methods have been proposed to estimate vehicle model parameters. For example, [7] introduced a data-driven approach for identifying longitudinal dynamics, and [8] explored homotopy optimization for parameter identification in dynamic systems modeled by ordinary differential equations. However, these methods often assume simplified conditions like low speeds and time-invariant parameters, which are inadequate for the high-speed, time-dependent dynamics seen in autonomous racing, as highlighted in [4]. Alternative approaches, such as those presented by [9] and [10], apply Gaussian Processes Regression to account for model uncertainties and enhance prediction accuracy. While these methods capture time-dependent physics, they may not always guarantee predictions that satisfy physical constraints.\nThe estimation of parameters for tire models, such as the widely-used Pacejka Magic Formula [1], is inherently non-linear. Recent advancements using deep neural networks have shown promise in capturing these non-linearities, especially in autonomous racing [2]. While DNNs reduce the complexity and costs of traditional parameter estimation methods, they still require large datasets for training and often fail to capture the underlying physical laws. Furthermore, purely data-driven models, such as those described in [11] and [12], struggle to generalize to scenarios beyond the data they were trained on and can produce outputs that are not physically meaningful. In an effort to address these challenges, [13] used a Gated Recurrent Unit (GRU)-structured neural network to potentially replace traditional physics-based STMs in simulating autonomous racing vehicles. While this approach yields more accurate predictions of vehicle dynamics within the training data, it still encounters difficulties in predicting conditions beyond the scope of the training data.\nThe growing interest in revealing physical laws has led to increasing research into PINNs, which combine data-driven models with the ability to uncover underlying physical laws [14]. For example, recent work in the field of autonomous driving, such as [15], implements an online-adjusted method to estimate cornering stiffness in real-time. However, their approach assumes a linear relationship between slip angles and forces, which restricts its applicability in handling non-linear regions, especially at large slip angles. This limitation becomes critical in high-speed autonomous racing scenarios involving significant drifting. Other recent studies, such as DPM [3] and DDM [4], focus on estimating the magic formula using PINN-based models. However, DPM relies on sampling-based control and exhibits unconstrained Pacejka coefficient estimations, which reduces its effectiveness [4]. In contrast, DDM has demonstrated improved performance over DPM by producing results within reasonable ranges, but it suffers from a lack of robustness when trained on smaller datasets. While DDM offers both open-loop and closed-loop performance, it incorporates Model Predictive Control (MPC) for trajectory optimization. As a result, it becomes difficult to evaluate the accuracy of the estimated magic formula parameters because the control is optimized within the MPC process. Additionally, as shown in [4], when handling real-world data collected from sensors, the inherent noise from measurements introduces significant challenges. This noise leads to high losses, oscillations in the predicted states, and difficulties in model convergence, thereby obscuring the true underlying physical insights. Traditional data pre-processing methods, such as the low-pass Savitzky-Golay filter [16], can smooth the noisy data,"}, {"title": "III. PROBLEM STATEMENT", "content": "In this paper, we employ the STM, also known as the bicycle dynamic model. The STM simplifies the four-wheel structure into a two-wheel bicycle structure by assuming that the slip angles for the wheels on a particular axle are the same, effectively combining all wheels on one axle into a single wheel in the middle, as depicted in Fig. 1. The state and input variables of the model are outlined in Table I."}, {"title": "A. Vehicle Dynamics", "content": "As reported in [9], the longitudinal force at the rear wheel is determined by:\n$F_{rx} = (C_{m1} - C_{m2}U_x^2)T - C_{r0} - C_aU_x^2$.\n$C_{m1}$ and $C_{m2}$ represent drivetrain coefficients, with $C_{m1}$ (N) linearly related to throttle and $C_{m2}$ (kg/s) related to damping. $C_{r0}$ (N) and $C_d$ (kg/m) denote the rolling resistance and the drag resistance, respectively. This simplifies the drag and rolling resistance forces as $F_d = C_dU_x^2$ and $F_{r0} = C_{r0}$. According to the Pacejka tire model, the slip angles $\\alpha_f$ and $\\alpha_r$ can be calculated as\n$\\alpha_f = \\delta - arctan(\\frac{\\omega l_f + V_y}{U_x}) + S_{hf}$,\n$\\alpha_r = arctan(\\frac{\\omega l_r - V_y}{U_x}) + S_{hr}$,\nwhere $S_{hf}$ and $S_{hr}$ represent the horizontal shift of the front and rear slip angles, respectively. Let $S_{vf}$ and $S_{vr}$ represent the vertical shift of the front and rear lateral forces. The lateral forces are given by $F_{fy} = S_{vf} + D_f sin(C_f arctan(B_f\\alpha_f - E_f(B_f\\alpha_f - arctan(B_f\\alpha_f))))$ at the front wheel and $F_{ry} = S_{vr} + D_r sin(C_r arctan(B_r\\alpha_r - E_r(B_r\\alpha_r - arctan(B_r\\alpha_r))))$ at the rear wheel. Here, $B_i$, $C_i$, $D_i$, $E_i$, $i \\in f,r$ are the remaining Pacejka coefficients [20]. $l_f$ and $l_r$ denote the distances from the vehicle's center of mass to the front and rear axles, respectively. At time t, the system states are represented by $S_t = [x_t, y_t, \\theta_t, U_{xt}, V_{yt}, \\omega_t, T_t, \\delta_t] \\in \\mathbb{R}^8$, and the control inputs $U_t$ include changes in throttle and steering: $U_t = [\\Delta T, \\Delta \\delta_t]^T \\in \\mathbb{R}^2$. The state equation can be obtained as follows:\n$x_{t+1} = x_t + (V_{xt} cos \\theta_t - V_{yt} sin\\theta_t)\\Delta t$,\n$y_{t+1} = y_t + (V_{xt} sin \\theta_t + V_{yt} cos \\theta_t)\\Delta t$,\n$\\theta_{t+1} = \\theta_t + (\\omega_t)\\Delta t$,\n$V_{xt+1} = V_{xt} + (\\frac{F_{rx} - F_{fy} sin \\delta_t + mV_{y_t}\\omega_t}{m})\\Delta t$,\n$V_{yt+1} = V_{yt} + (\\frac{F_{ry} + F_{fy} cos \\delta_t - mV_{x_t+}\\omega_t}{m})\\Delta t$,\n$\\omega_{t+1} = \\omega_t + (\\frac{F_{fy}l_f Cos \\delta_t - F_{ry}l_r}{I_z})\\Delta t$,\n$T_{t+1} = T_t + \\Delta T$,\n$\\delta_{t+1} = \\delta_t + \\Delta \\delta$,"}, {"title": "B. FTHD PINN Model", "content": "In the proposed FTHD model, similar to the models de-fined by DDM [4] and DPM [3], with the given $X_t = [V_{xt}, V_{yt}, \\omega_t, T_t, \\delta_t] \\in \\mathbb{R}^5$ and $U_t = [\\Delta T, \\Delta \\delta] \\in \\mathbb{R}^2$, the evolution of the position states $x_t, y_t, \\theta_t$ can be calculated using Eq. (2).\nDuring the pre-training phase, like DDM, FTHD collects N continuous time steps (N > 1) as the input $X_{input} = [X_{t-N+1},..., X_t]$ from the training dataset. Subsequently, it predicts the estimated state $\\hat{X}_{t+1} = [\\hat{v}_{xt+1}, \\hat{v}_{yt+1}, \\hat{\\omega}_{t+1}]$ at time t + 1, which is then compared with the label $X_{t+1} = [V_{xt+1}, V_{yt+1}, \\omega_{t+1}]$ using Mean-Square-Error (MSE) loss for backpropagation:\n$Loss_1 = MSE(\\hat{X}_{t+1}, X_{t+1})$.\nAfter the pre-training phase, the entire state dictionary, denoted as $\\Theta = {W,b}$, is selected for the hybrid fine-tuning process. Here, W and b represent the weights and biases of the model, respectively. During the fine-tuning process, we selectively freeze n layers of the model, where n is determined based on the total number of hidden layers in the architecture. Importantly, we ensure that at least one hidden layer (excluding activation layers) remains active and unfrozen to allow for continued learning and adaptation. The number of frozen layers n is chosen to balance stability and adaptability during the fine-tuning phase, ensuring the model can still adjust to new data while preserving key features learned during pre-training. The gradients of the frozen parameters, denoted as $\\Theta_f = {W_f,b_f}$, are no longer updated, while only the active parameters, $\\Theta_a = {W_a,b_a}$, participate in backpropagation. This strategy allows the model to retain the knowledge captured in the frozen layers while adapting the active layers to the new data, improving performance with a smaller training dataset. This process is illustrated in Fig. 2.\nCompared to the DDM, the training dataset for the pro-posed model includes an additional time input, $T_{t+1}$ alongside $X_t$ and $U_t$, resulting in a structured dataset $D = [[X_1, U_1, T_2], ..., [X_N,U_N, T_{N+1}]]$. This augmentation is inspired by the time constraint loss in physically constrained neural networks introduced by [21], where a time partial differential equation (PDE) serves as one of the unsupervised loss functions. In the domain of autonomous driving, this approach is relevant due to the relationship between velocity derivatives and acceleration, as inferred from the model's output:\n$\\hat{X}_{t+1} = [\\hat{v}_{xt+1}, \\hat{v}_{yt+1},\\hat{\\omega}_{t+1}] = f(X_t, U_t, \\Phi_k, \\Phi_u, T_{t+1})$.\nUsing input time $T_{t+1}$ and the intermediate estimated acceleration $\\hat{\\beta}_{t+1} = [\\hat{A}_{xt+1}, \\hat{A}_{yt+1},\\hat{\\omega}_{t+1}]$, the unsupervised loss is defined as:\n$Loss_2 = MSE(\\frac{\\hat{X}_{t+1}}{\\partial T_{t+1}}, \\hat{\\beta}_{t+1})$.\n$Loss_2$ serves as an unsupervised loss that is unrelated to the label $X_{t+1}$. This loss function aims to ensure that, given the input $X_t$ and assuming the time interval $\\Delta t$ is small enough, the model outputs an estimated $\\hat{X}_{t+1}$ such that the difference $\\hat{X}_{t+1} - X_t$ is close to the estimated acceleration $\\hat{\\beta}$. Without providing the label, the estimated $\\hat{X}_{t+1}$ could vary freely, but it still satisfies the dynamic motion constraints.\nFollowing the approach of [22] and [21], when handling the hybrid loss of the model, two weights, $w_1$ and $w_2$, are introduced, where $w_1 + w_2 = 1$. In this framework, $Loss_1$"}, {"title": "C. EKF-FTHD Pre-Processing Method", "content": "As discussed in DDM [4], when processing data col-lected from real-world scenarios like the Indy Autonomous Challenge, noise poses a significant challenge for achieving convergence comparable to simulation-based models. This is largely due to sensor-based data, which relies on physical measurements rather than computed values. As a result, the real data often exhibits oscillations, even when the car operates at nearly stable speeds. These oscillations introduce noise, degrading the model's performance. While traditional smoothing methods, such as the Savitzky-Golay filter [16], can improve convergence by smoothing the data, they overlook the physical relationships between variables such as $X_t$ and $U_t$, potentially leading to feature loss and residual errors.\nTo address this, we propose the EKF-FTHD pre-processing method, which leverages EKF's ability to estimate states in nonlinear systems.  Here, $\\hat{X}_{t+1}$ represents the estimated state output by the model, while $X_{t+1}$ denotes the measurements obtained from the sensor at the time step t + 1.\nBased on the FTHD model shown in Fig. 2, we incorporate an EKF guard layer embedded with a covariance matrix to denoise the raw state $X_{t+1} = [V_{xt+1}, V_{yt+1}, \\omega_{t+1}]$, splitting it into a physical component and a noise component: $X_{t+1} = XEKF_{t+1} \\cup \\epsilon$. The estimated parameters include $\\Phi^{EKF} = \\Phi_u \\cup \\Phi_f$, where $\\Phi_f = [q_j, r_j]$ and $j = vx, Vy, \\omega$. Each coefficient is assumed to lie within a nominal range: $\\Phi_f < \\Phi_f < \\hat{\\Phi_f}$, with $\\Phi_f$ and $\\hat{\\Phi_f}$ represent the lower and upper bounds, respectively. $q_j$ and $r_j$ are elements of the noise The innovation covariance is defined as:\ncovariance matrices $Q_t$ and $R_t$, respectively. $Q_t$ represents the covariance of the process noise during the state prediction step of the EKF, and $R_t$ represents the covariance of the measurement noise, which accounts for uncertainty during the update step of the EKF. For simplification, both $Q_t$ and $R_t$ are assumed to be diagonal matrices. This assumption reduces the computational complexity, as it implies that noise variables are independent, and their covariances are zero, resulting in:\n$Q_t = \\begin{bmatrix}\n    q_{vx} & 0 & 0 \\\\\n    0 & q_{vy} & 0 \\\\\n    0 & 0 & q_{\\omega} \\\\\n\\end{bmatrix}$, $R_t = \\begin{bmatrix}\n    r_{vx} & 0 & 0 \\\\\n    0 & r_{vy} & 0 \\\\\n    0 & 0 & r_{\\omega} \\\\\n\\end{bmatrix}$,\nHere, $q_{vx}, q_{vy}, q_{\\omega}$ represent the process noise covariances associated with the longitudinal velocity, lateral velocity, and yaw rate, respectively. Similarly, $r_{vx}, r_{vy}, r_{\\omega}$ represent the measurement noise covariances for these state variables. The filter's parameters are bounded based on how much the noise in the original data affects the physical dynamics.\nThe measurement residual is defined as:\n$Y_{t+1} = X_{t+1} - \\hat{X}_{t+1}$.\nThe state function f(\u00b7), used to compute vx, Vy, and \\omega, follows the equations in (2). The Jacobian matrix of f(.) is expressed as:\n$F_{t+1} = \\begin{bmatrix}\n    1 & -\\Delta t\\omega_{t+1} & 0 \\\\\n    \\Delta t\\omega_{t+1} & 1 & 0 \\\\\n    0 & 0 & 1\n\\end{bmatrix}$,\nSince the sensor data provides direct measurements, both the measurement function h(\u00b7) and the measurement matrix $H_{t+1}$ are identity matrices in this case. The structure of the model is shown in Fig. 3, where the output matrices $Q_{t+1}$ and $R_{t+1}$ from the network along with the calculated $F_{t+1}$ are fed into the model's built-in EKF. Given an initial covariance matrix $P_t$, representing the estimation error in the state vector, the predicted covariance matrix $P_{t+1|t}$ is updated using $F_{t+1}$ and $Q_{t+1}$:\n$P_{t+1|t} = F_{t+1} \\times P_t \\times F^T_{t+1} + Q_{t+1}$.\n$\\Sigma_{t+1} = H_{t+1} \\times P_{t+1|t} \\times H^T_{t+1} + R_{t+1}$.\nThus, the kalman gain is calculated as:\n$K_{t+1} = P_{t+1|t} \\times H^T_{t+1} \\times \\Sigma^{-1}_{t+1}$.\nUsing Eq. (4) and Eq. (6), the noise estimate \\eta_{t+1} becomes:\n$\\eta_{t+1} = K_{t+1} \\times Y_{t+1}$,\nand the state estimate $XEKF_{t+1}$ is updated as follows based on Eq. (5) and Eq. (6):\n$XEKF_{t+1} = \\hat{X}_{t+1} + K_{t+1} \\times Y_{t+1}$.\nAccording to Eq. (5) and Eq. (6), the covariance matrix $P_{t+1}$ is updated by\n$P_{t+1} = (I - K_{t+1} \\times H_{t+1}) \\times P_{t+1|t}$,\nwhere $I \\in \\mathbb{R}^{3\\times 3}$ denotes the 3 \u00d7 3 identity matrix. Once the data is filtered, the supervised loss is defined as:\n$LOSSEKF_1 = MSE(XEKF_{t+1}, X_{t+1})$.\nSimilar to FTHD, with the given time vector $T_{t+1}$, along with the intermediate acceleration vector $\\hat{\\alpha}_{t+1} = [\\hat{A}_{xt+1}, \\hat{A}_{yt+1}, \\hat{\\omega}_{t+1}]$, the unsupervised loss is defined as:\n$LOSSEKF_2 = MSE(\\frac{XEKF_{t+1}}{\\partial T_{t+1}}, \\hat{\\alpha}_{t+1})$.\nThis unsupervised loss formulation assumes that the noisy data disrupts the convergence of the governing differential equations. That is, when noisy data is provided, the governing differential equations from velocity to acceleration are challenging to converge due to the oscillatory behavior of the data. However, after separating the noise from the data, the remaining physical component should better satisfy the governing equations. Therefore, the hybrid total loss for the EKF-FTHD is calculated with the weights $w_1$ and $w_2$ such that $w_1 + w_2 = 1$:\n$LOSSEKF_{total} = w_1 \\cdot LOSSEKF_1 + w_2 \\cdot LOSSEKF_2$."}, {"title": "IV. RESULT AND DISCUSSION", "content": "We compare the simulation performance of the proposed FTHD against the state-of-the-art DDM, using data collected from the simulator described in [9]. This simulator features a 1:43 scaled racecar engaged in pure-pursuit of a reference trajectory on the racetrack depicted in Fig. 5. The ground truth (GT) of the scaled racecar's coefficients is available but only used for comparison after training. The sampling frequency is 50 Hz, resulting in a time interval between samples $\\Delta t_{sim}$ is 0.02 s. The total size of the simulation dataset $D^{sim}_{total}$ consists of 1000 samples.\nTo further evaluate the real-world performance of the FTHD model and compare it with DDM in terms of the accuracy of the filtered data and adjusted parameter ranges derived from EKF-FTHD, we use a real-world dataset from a full-scale Indy autonomous racecar [24]. The dataset includes fused measurements from two RTK-corrected GNSS signals and an IMU, which are passed through a 100 Hz EKF. The sampling frequency of the real data is 25 Hz, resulting in a time interval between samples of $\\Delta t_{exp}$ = 0.04 s. The total dataset size, denoted as $D^{exp}_{total}$, consists of 11,488 samples collected at the Putnam Park Road Course in Indianapolis. The EKF-FTHD filters the full dataset, $D^{exp}_{total}$, and output a new dataset, denoted as $D^{EKF}$, which is then used to train both the FTHD and DDM models for prediction.\nTo demonstrate the robustness of our method compared to DDM when reducing the training dataset size, we simulate random selections of 30%, 20% and 15% of the dataset samples $D^{sim}_{train}$ for training, with $D^{sim}_{total}$ used for validation. For real-world experiments, we randomly select 90%, 60%, 30%, 15% and 5% of the dataset samples for training, denoted as $D^{exp}_{train}$ and $D^{EKF}_{train}$. The full datasets $D^{exp}_{total}$ and $D^{EKF}_{total}$ are used for validation. Both models undergo the same number of training iterations, using identical hyperparameter tuning to minimize validation loss."}, {"title": "A. Training Dataset", "content": "We compare the simulation performance of the proposed FTHD against the state-of-the-art DDM, using data collected from the simulator described in [9]. This simulator features a 1:43 scaled racecar engaged in pure-pursuit of a reference trajectory on the racetrack depicted in Fig. 5. The ground truth (GT) of the scaled racecar's coefficients is available but only used for comparison after training. The sampling frequency is 50 Hz, resulting in a time interval between samples $\\Delta t_{sim}$ is 0.02 s. The total size of the simulation dataset $D^{sim}_{total}$ consists of 1000 samples.\nTo further evaluate the real-world performance of the FTHD model and compare it with DDM in terms of the accuracy of the filtered data and adjusted parameter ranges derived from EKF-FTHD, we use a real-world dataset from a full-scale Indy autonomous racecar [24]. The dataset includes fused measurements from two RTK-corrected GNSS signals and an IMU, which are passed through a 100 Hz EKF. The sampling frequency of the real data is 25 Hz, resulting in a time interval between samples of $\\Delta t_{exp}$ = 0.04 s. The total dataset size, denoted as $D^{exp}$, consists of 11,488 samples collected at the Putnam Park Road Course in Indianapolis. The EKF-FTHD filters the full dataset, $D^{exp}$, and output a new dataset, denoted as $D^{EKF}$, which is then used to train both the FTHD and DDM models for prediction.\nTo demonstrate the robustness of our method compared to DDM when reducing the training dataset size, we simulate random selections of 30%, 20% and 15% of the dataset samples $D^{sim}_{train}$ for training, with $D^{sim}_{total}$ used for validation. For real-world experiments, we randomly select 90%, 60%, 30%, 15% and 5% of the dataset samples for training, denoted as $D^{exp}$ and $D^{EKF}$. The full datasets $D^{exp}$ and $D^{EKF}$ are used for validation. Both models undergo the same number of training iterations, using identical hyperparameter tuning to minimize validation loss."}, {"title": "B. Model Coefficient Ranges", "content": "To estimate the physics coefficient values $\\Phi_u$ in the physics guard layer of the model, as shown in Fig. 3, as well as the estimated EKF covariance values $\\Phi_f$, the bounds $\\Phi_u$ and $\\hat{\\Phi}_u$, as well as $\\Phi_f$ and $\\hat{\\Phi}_f$, are required. For simulation experi-ments, we use the known GT to compare the performance of FTHD with DDM, using the same coefficient ranges as in [4]. In real-world experiments, the bounds $[\\, \\hat{\\Phi}_u]$ used in [4] are adjusted to $[\\, \\hat{\\Phi_u}]$ based on EKF-FTHD output. The covariance bounds $[\\, \\hat{\\Phi}_f]$ are set according to the quality of the raw data, where better-quality data lead to smaller covariance values. These ranges are detailed in Table III, and Table IV provides the EKF covariance matrix ranges."}, {"title": "C. Evaluation Metrics", "content": "For the simulation, with known GT, we use the Pacejka and drivetrain coefficients that result in the lowest validation loss from both FTHD and DDM to calculate lateral forces using the same STM. During the fine-tuning hybrid training of FTHD, we choose weight parameters $w_1 = 0.99975$ and $w_2 = 0.00025$. The comparison results with the GT are shown in Fig. 6. It's evident that as the size of the training set decreases, the FTHD retains better alignment with the GT outputs compared to DDM. Furthermore, it's clear to see that when only 15% of the training set is used, the force plots of DDM show much greater deviation from the GT, while FTHD can still closely approximate it.\nTo evaluate the estimation performance of FTHD and DDM in simulation, we compare the minimum validation loss, $L_{min} = min (Loss_{val})$, the root-mean-square-error (RMSE) of velocities, and the maximum errors of velocities ($\\epsilon_{max}$) for $v_x$, $v_y$ and $\\omega$, as used in [4]. Additionally, we evaluate"}, {"title": "D. Hyperparameters Tuning", "content": "To ensure a fair comparison without the influence of model hyperparameters, we utilize Tune [25] and set up identical configuration spaces and trials for both models. This includes the selection of hidden layers, GRU layers, neurons, learning rate, the choice of N for each input features $X_{input}$, and batch size, as well as the frozen layers during fine-tuning process. To simplify the process of hyperparameters selection, we set the size of frozen layers equal to the 3/4 of the total layers for each tests. Both models are trained on the same hardware: a GeForce RTX 4090 GPU, a 13th Gen Intel Core i7-13700K CPU, and 128 GB of RAM. To further challenge the model's performance, the full training set is used for validation. The lowest validation loss and the corresponding are used for the simulation comparisons. In the real experiment, we use the EKF-FTHD model with the lowest validation loss to generate the filtered dataset used for training the FTHD estimation model, then select the hyperparameter configuration with the lowest validation loss for dynamical performance comparisons. Tables VIII and IX show the model configurations where the lowest validation loss occurs after reducing the training dataset size. In the simulations, each configuration represents both DDM and FTHD with the same original dataset. In the real"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce FTHD, a novel fine-tuning PINN model that integrates hybrid loss functions to enhance vehicle dynamics estimation. FTHD builds upon pre-trained DDM"}, {"title": "FUTURE WORKS", "content": "A robust solution for real-world applications.\nFuture research will focus on two key areas. First, expanding the EKF-FTHD approach from the bicycle model to more complex systems, such as four-wheel vehicle models, could improve accuracy and applicability by better addressing real-world measurement errors. This method could also be adapted for other dynamic systems beyond vehicle dynamics, including submarines or agricultural machinery, where estimating internal parameters is challenging. Second, integrating FTHD into the design of distributionally robust control systems could enhance controller performance under uncertainty. By using the noise separated by EKF-FTHD as part of an ambiguity model and eventually incorporating it into the constraints, this approach could advance the development of distributionally robust optimization (DRO) controllers that effectively manage uncertainty and deliver robust performance under varying conditions."}]}