{"title": "LP-LM: No Hallucinations in Question Answering with Logic Programming", "authors": ["Katherine Wu", "Yanhong A. Liu"], "abstract": "Large language models (LLMs) are able to generate human-like responses to user queries. However, LLMs exhibit inherent limitations, especially because they hallucinate. This paper introduces LP-LM, a system that grounds answers to questions in known facts contained in a knowledge base (KB), facilitated through semantic parsing in Prolog, and always produces answers that are reliable.\nLP-LM generates a most probable constituency parse tree along with a corresponding Prolog term for an input question via Prolog definite clause grammar (DCG) parsing. The term is then executed against a KB of natural language sentences also represented as Prolog terms for question answering. By leveraging DCG and tabling, LP-LM runs in linear time in the size of input sentences for sufficiently many grammar rules. Performing experiments comparing LP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate on even simple questions, unlike LP-LM.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) hallucinate, i.e., generate information that appears plausible but is factually incorrect [9]. This unfortunately poses a challenge to question answering tasks, as users desire reliable answers given a query, but hallucination misleads users and erodes the system reputation [2]. To overcome this challenge, better retrieval models that retrieve relevant information according to queries as well as better generation models that synthesize more accurate answers from knowledge sources are needed. This paper sheds light on how logic programming can be used to push progress on the former.\nWe describe LP-LM, a system that considers the structure of natural language sentences when retrieving answers to user queries. Unlike LLMs, which are pre-trained so that for any given input the statistically best matching output based on its training is given, LP-LM seeks to answer questions in a logical and verifiable way via matching and substitution of facts.\nWe use probabilistic context-free grammar (PCFG) productions to model the structures of valid English sentences and create a knowledge base (KB) consisting of English sentences represented as Prolog terms. The term structure models relationships between entities in sentences precisely. When the user asks a natural language question, LP-LM generates the most probable constituency parse tree of the input sentence, translates the parse tree into a corresponding Prolog term for knowledge representation, and then matches the term against the KB of Prolog terms to retrieve an answer using unification. Utilizing Prolog's definite clause grammar (DCG) and tabling in our implementation, LP-LM proves to be extremely efficient, especially for grammars with a significant number of production rules. We have implemented LP-LM using the Prolog system XSB [12, 15], and our implementation is publicly available. 1\nThe rest of the paper is organized as follows. Section 2 defines terms used throughout the paper. Section 3 compares LP-LM with current LLMs by highlighting simple example problems on which current LLMs fail but LP-LM succeeds. Section 4 describes how LP-LM works, giving an example of"}, {"title": "Background", "content": "We introduce probabilistic context-free grammars and key logic programming features used.\nProbabilistic context-free grammar. A probabilistic context-free grammar (PCFG) is a formal grammar used in natural language processing and computational linguistics [11, 4]. PCFGs associate probabilities with the production rules of the grammar. These probabilities reflect the likelihood of a particular rule being used in generating or deriving a sentence. For any non-terminal in a PCFG, the probabilities associated with rules corresponding to that non-terminal must sum to 1.\nPCFGs are essential for capturing the ambiguity of natural language, and are particularly useful in tasks such as syntactic parsing, which uses dynamic programming algorithms to compute the most likely parse tree of a sentence given a statistical model of the syntactic structure of the language. The Cocke-Younger-Kasami algorithm (CYK) (Cocke 1969 [5]; Younger 1967 [17]; Kasami 1965 [10]), the Earley algorithm [6], and the shift-reduce algorithm [13] are at the core of most common algorithms for natural language parsing, both constituency-based and dependency-based.\nDefinite clause grammar. Definite Clause Grammars (DCGs) are a convenient way to represent grammatical relationships for parsing applications. They can be used to progressively build a parse tree as grammar rules are applied. DCG provides a syntax for writing more readable grammar parsing rules, and the DCG preprocessor is able to translate a DCG rule into pure Prolog. The arrow operator indicates a DCG rule, which replaces the normal neck \u201c: -\u201d used in Prolog clauses, and square brackets are used to indicate terminal symbols of the grammar. Works similar to DCGs include stochastic DCGs [8], relaxed unification grammars [1], and probabilistic unification grammars [14].\nTabling. Tabling consists of maintaining a table of goals that are called during execution, along with their answers, and then using the answers directly when the same goal is subsequently called. The idea is to never evaluate the same call twice. It helps improve the running time drastically, including terminating efficiently in situations where Prolog goes into an infinite loop following the same calls repeatedly.\nUnification. The way in which Prolog matches two terms is called unification. For example, applying unification of foo(a,X) and foo(Y,b): the principal functor of both terms is foo; the arguments of foo(a, X) are a and X, the arguments of foo (Y, b) are Y and b; so a and Y must unify, instantiating Y to a, and X and b must unify, instantiating X to b; and finally the resulting term after unification is foo(a,b)."}, {"title": "Comparison with existing LLMs", "content": "Before delving into the key designs of LP-LM, we first compare our system with existing LLMs to highlight the motivation behind our work. We focus on the following well-known models: GPT-40, GPT-40 mini, and Gemini. In particular, we show that the context-awareness of these LLMs are actually quite poor in question answering tasks, and that the LLMs struggle to perform tasks involving even single facts, thus limiting their potential to complete more complex reasoning tasks.\nTable 1 illustrates the comparisons. The answers shown are from the first run of the models. Note that for the first two examples given, the inputs are entered independently, and we only show the answer that corresponds to the last input due to space. The last two examples consider the separate inputs from the earlier examples as one prompt, but even with this the models still hallucinate. The examples demonstrate that current LLMs exhibit a lack of understanding and ability to reason about the relationships between different concepts and entities, and are only able to generate text based on statistical correlations they have learned from their training data."}, {"title": "Executing LP-LM using Prolog unification", "content": "We outline a typical LP-LM workflow here and then give an example. LP-LM's KB of Prolog terms is used to provide context.\n\u2022 Input: A user's input can be either a statement (e.g., \u201csuppose I say the black bird flies bravely.\") which eventually leads to a question, or a question (e.g., \u201chow does the black bird fly?\") following some previous statement. If there are multiple sentences in the input, they are processed one at a time.\n\u2022 Retrieval from or insertion into KB: The input is parsed using Prolog DCG rules, and a constituency tree and associated Prolog term is generated from the parser. For statements, insertions into the KB are performed: the term is inserted dynamically into the KB. For questions, retrievals from the KB are performed: the term is matched against the KB and an answer is obtained by unification.\n\u2022 Post-processing: Optionally, the results can be translated to a natural language answer.\nWe show an example of an LP-LM execution, after which we describe the internal steps of the retrieval and insertion process.\nExample. Consider an example sentence that includes a determiner, adjective, noun, verb, and adverb. This statement gets inserted into the specialized KB of Prolog terms via the predicate add_kb:\n?- add_kb('the black bird flies bravely').\nAfter statements, one can perform queries, which can either be yes/no or wh- questions, where predicate query_kb does the query.\n?- query_kb('how does the black bird fly').\nAnswer: bravely\n?- query_kb('who flies bravely').\nAnswer: black (bird)\n?- query_kb('does the black bird fly bravely').\nAnswer: yes\nOne can also remove previous statements as follows, where predicate remove_kb does the removal:\""}, {"title": "Insertions into KB", "content": "With non-queries, or what we call statements, insertions into the KB are done. A tokenizer is first used to extract out each word in the statement, then a top-down evaluation method is used to generate the parse tree and Prolog term for the sentence. The Prolog term is added to the KB. We take the basic sentence, \"Bob runs\". The DCG rules are applied in the following order:\n1. The DCG rule\ns(s(NP, VP), Sem,P) --> np (NP,X,P1), vp(VP,Y,,P2), {Sem=..[Y,X]}, {P is P1*P2*0.25}.\nis first matched with the sentence. Variable Sem represents the Prolog term, where Y is the functor of the term and X is the argument, which is generated incrementally as the words in the input sentence are matched to a DCG rule one by one.\n2. The DCG rule\nnp(np(PN),X,P) --> pn(PN,X,P1), {P is P1*0.2}.\nis matched next, followed by the DCG rule\npn(pn(X),X,1.0) --> [X], {pronoun(X)}.\nwhich checks if \u201cBob\u201d is a pronoun, as the variable X represents \u201cBob\u201d.\n3. The DCG rule\nvp(vp(VB), Verb,C,P) --> v(VB,Verb,C,P1), {P is P1*0.09}.\nis matched next, followed by the DCG rule\nv(v(X),Vx,C,1.0) --> [X), {verb (Vx,C,[X],[])}.\nwhich checks if \u201cruns\u201d is a verb, as the variable X represents \u201cruns\u201d.\n4. The Prolog term runs (Bob) is obtained, with the parse tree s (np(pn(Bob)),vp(v(runs))), with probability 0.0045. This is the most probable parse tree. The term is added to the KB."}, {"title": "Retrievals from KB", "content": "With queries, retrievals from the KB are done. The parse tree and Prolog term for the question is generated the same way. The resulting term is then matched against the KB of terms, and unification is used to obtain the answer to the question. Consider the question \u201cwho runs\u201d, which should return the answer \"Bob\" per the example above. The DCG rules are applied as follows:\n1. The DCG rule\nq(q(QW,VB), X, P) --> qw(QW,_Qw,P1), v(VB,Verb,,P2), {Sem=.. [Verb,X), Sem}, {P is P1*P2*0.05}.\nis applied, where qw represents the question word \u201cwho\u201d and v represents the verb \u201cruns\u201d."}, {"title": "A note on DCG parsing efficiency", "content": "To find the most probable parse tree in LP-LM, all possible parses of input segments that can contribute to the maximum probability are considered and compared, from which the parse with the maximum probability is constructed and returned. Despite this global optimality, the parsing that underlies LP-LM still proves to be efficient due to our use of Prolog DCGs and tabling. We have performed experiments testing the efficiency of DCGs and have shown that DCGs still outperform state-of-the-art bottom-up greedy parsing algorithms.\nWe evaluated DCG parsers on a total of 12 PCFGs: 3 left-recursive grammars, 3 right-recursive grammars, 3 unambiguous grammars, and 3 ambiguous grammars. For each type of grammar, we increase the size complexity by increasing the number of production rules with each test: the first test consisted of a trivial grammar with 3-10 production rules, the second test consisted of a more complex grammar with 20-50 production rules, and the third test consisted of the longest and most complex grammar with 100+ production rules. Within each test, 3-5 input sentences of increasing length satisfying the corresponding grammar were parsed, and the time of each parse recorded.\nWe ran experiments testing these DCG parsers in comparison with the current Viterbi parser API in the Python Natural Language Toolkit (NLTK). The Viterbi algorithm here uses a greedy heuristic, while our parsing algorithm performs an enumeration of all possible parses before choosing the optimal one. Figures 2, 3, 4, and 5 show the running times of sentence parses on grammars of increasing size, for each type of grammar. The x-axis represents the test cases, i.e. each point is a test case, with each test case representing an input sentence ranging from lengths 1 to 50. Higher numbered test cases represent sentences with longer lengths. The y-axis is the running time of sentence parse in seconds, averaged over 10 runs. All measurements were taken on a machine with a 2GHz Quad-Core Intel Core i5 processor, 16GB RAM, running MacOS 14.3.1, with Python 3.11.4 and XSB version 5.0.\nAcross all types of grammars (left-recursive, right-recursive, unambiguous, ambiguous), the results are uniform: for large grammars with 100+ production rules, i.e. test 3, our Prolog parser runs much more efficiently. In particular, for left-recursive, right-recursive, and unambiguous grammars, our parser is observed to run in linear time in the length of the input sentence for large grammars."}, {"title": "Related work, future work, and conclusion", "content": "The most notable line of work similar to ours is Retrieval Augmented Generation (RAG), an architectural approach that augments LLMs with external knowledge such as databases [7]. RAG is particularly useful in knowledge-intensive scenarios or domain-specific applications that require continually updated knowledge; it ensures that the response of an LLM is not based solely on static training data and rather uses up-to-date external data sources to provide responses. RAG has been popularized recently with its application in conversational agents. Our work has the similar motivations as RAG, but we use a \u201cbuilt-in\" knowledge base to store facts used for context and utilize semantic parsing implemented in XSB Prolog to insert and retrieve information from the KB.\nOur work also has similar motivations to that of KALM, a logic system for authoring facts and questions [16]. While KALM uses the answer set programming system DLV as the logical system for reasoning about knowledge, our work uses DCG and tabling in XSB Prolog. But as shown in the work of [3] using OpenRuleBench to analyze the performance and scalability of different rule engines including XSB and DLV, XSB exhibits significantly better runtime performance than DLV on various tasks due to tabling.\nA limitation to LP-LM is the generalization of English sentences, since we represent the grammar rules as PCFGs manually. Although new grammar rules can always be added at anytime, doing so can be tedious, and there are sentences that intentionally violate grammatical rules or standard sentence structures. In this case, we can simply \u201caugment\u201d LP-LM to use LLMs or other NLP techniques for input pre-processing to help extract filler words and distill the core facts from sentences, for example by fine-tuning text summarization models. Regarding the method itself, LP-LM is limited in that the class of queries the system can answer is limited to simple retrieval tasks that do not require any form of reasoning. Getting LP-LM to support reasoning capabilities such as deductive and inductive reasoning, as well as further generalizing the system, are plans for our future work.\nIn conclusion, while LLMs use deep learning models and are trained on massive datasets, making them prone to hallucinations, our work, LP-LM, shows that a KB of facts and a question implemented using Prolog's DCG and tabling for efficient semantics parsing of PCFG can produce reliable answers and produce them efficiently."}]}