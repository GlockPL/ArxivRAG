{"title": "COGNITIVE PHANTOMS IN LLMS THROUGH THE LENS OF\nLATENT VARIABLES", "authors": ["Sanne Peereboom", "Inga Schwabe", "Bennett Kleinberg"], "abstract": "Large language models (LLMs) increasingly reach real-world applications,\nnecessitating a better understanding of their behaviour. Their size and\ncomplexity complicate traditional assessment methods, causing the emer-\ngence of alternative approaches inspired by the field of psychology. Recent\nstudies administering psychometric questionnaires to LLMs report human-\nlike traits in LLMs, potentially influencing LLM behaviour. However, this\napproach suffers from a validity problem: it presupposes that these traits\nexist in LLMs and that they are measurable with tools designed for humans.\nTypical procedures rarely acknowledge the validity problem in LLMs, com-\nparing and interpreting average LLM scores. This study investigates this\nproblem by comparing latent structures of personality between humans\nand three LLMs using two validated personality questionnaires. Findings\nsuggest that questionnaires designed for humans do not validly measure\nsimilar constructs in LLMs, and that these constructs may not exist in LLMs\nat all, highlighting the need for psychometric analyses of LLM responses to\navoid chasing cognitive phantoms.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are becoming progressively intertwined with day-to-day\nlife. LLMs are commonly used via easy-to-access interfaces such as ChatGPT to retrieve\ninformation, obtain assistance for homework, provide customer service, and so on. With\nan increasing number of parameters and more training data, LLMs become capable of\nprocessing and generating nuanced natural language [1]. For example, there is evidence that\nLLMs have generated text that was perceived as human more often than comparable human-written text [2] and that they are capable of advanced reasoning tactics and negotiation,\nranking among the highest-level players in a strategy game with human players [3].\nThe continuing evolution of LLM capabilities comes with the need to understand the models\nbetter, and increasing body of work has started to study LLMs with regard to their behaviour.\nFor example, social biases present in training data were found to become ingrained in word\nembeddings [4], and LLMs have occasionally been found to otherwise misalign with human\nvalues [5]: a challenge that is yet unsolved and carries implications for AI safety. There\nare efforts to better understand the origins and solutions to such behaviours, however, a\nbetter understanding of LLMs comes with a significant challenge: the billions of parameters\ncontained in the models significantly complicates the analytic assessment of the models'\ninner workings (e.g., extensive adversarial testing is a common approach to detecting"}, {"title": "1.1 Machine behaviour and machine psychology", "content": "As an alternative to analytic assessment of the underlying processes in LLMs, adopting a\nmachine behaviourist perspective can be useful [7]. Inspired by the study of animal behaviour,\nmachine behaviour is the study of the behaviour manifested in intelligent machines in\nterms of development, evolution, function, and underlying mechanisms [7]. Building\nfurther on parallels between the study of the human mind and intelligent machines, machine\npsychology [8] refers to the evaluation of LLMs analogous to participants in language-based\npsychological studies.\nEarly findings in the field of machine psychology suggest a semblance of humanness in\nLLMs. For example, the GPT-3 model was found to be prone to human-like cognitive\nerrors in classic psychological decision-making tests [9]. Other studies have used existing\nquestionnaires to measure personality traits in LLMs [10, 11] and psychological profiles\nof LLMs at a larger scale [12], one study even reporting a high degree of dark personality\ntraits (psychopathy, Machiavellianism, and narcissism) in GPT models compared to average\nscores in a human sample [13].\nThe majority of aforementioned constructs would be considered latent variables in psycho-\nlogical theory: these constructs are not directly observable nor directly measurable. Instead,\nthese variables are indirectly measured through measurable behaviours hypothesised to\nbe caused by the underlying latent trait. This approach can be a powerful supplement to\nthe analytic assessment of LLMs: identifying overarching latent phenomena that cause a\ntendency towards undesired responses (e.g., dark personality patterns that may increase\nthe risk of toxic responses) could help guide targeted adversarial testing strategies for more\nefficient prevention of harmful output. The indirect measurement of these latent phenom-\nena, usually through a questionnaire or test, has a longstanding tradition in quantitative\npsychology and is the foundation of the discipline of psychometrics."}, {"title": "1.2 Latent variables and psychometrics for LLMs", "content": "Administering readily available questionnaires seems like a quick way to accurately measure\nlatent traits in LLMs. Many studies on latent traits in LLMs apply existing measurement\ninstruments, constructed for human samples. In psychometric terms, these studies thus aim\nto infer from LLMs various latent traits that might systematically affect model behaviour. A\ntest or questionnaire is administered to an LLM, and the resulting responses are aggregated\ninto composite scores (e.g., a mean dimension score) and interpreted - often in relation to\nhuman samples - as a proxy for the latent trait of interest. However, this approach relies on\ncrucial yet rarely acknowledged assumptions regarding the validity of the measurements in\nLLMs.\nThe validity of a questionnaire refers to the extent to which it measures what it is intended\nto measure. It depends firstly on the existence of the latent trait, and secondly on the notion\nthat changes in the latent trait cause changes in the observed responses [14]. Administering\nan existing questionnaire validated on human samples presupposes that the latent trait\nof interest exists in LLMs. It further presupposes that the administered questionnaire\nmeasures the respective trait, and that it is measured equivalently in humans and LLMs.\nA questionnaire that is validated on human samples certainly does not guarantee that it is\nvalid for LLMs: LLMs will always generate some response, which may be used to calculate a\nmean score on certain latent dimensions (e.g., the degree of psychopathy in the models),\nbut that does not mean that this score is meaningful or that the latent phenomenon exists in\nLLMs at all. Relying on composite scores without acknowledging this problem may thus\ncreate the illusion of humanness in LLMs.\nAlthough a handful of studies have performed psychometric evaluations of LLM responses,\nassessments have either been limited to reliability or validation studies using composite"}, {"title": "1.3 The validity problem for LLMs", "content": "Validity is not a methodological problem but rather a theoretical one [14]. The existence of\nlatent phenomena and their causal effects on the measurement outcomes cannot be proven\ndue to their unobservable nature - they must be founded in extensive psychological theory\nto say that there is a justifiable expectation that a test or questionnaire reasonably measures\nthe phenomenon it purports to measure [14]. This is already a difficult problem in human\npsychological research, let alone for machine psychological research where substantive\ntheory does not yet exist.\nAlthough validity cannot be established through empirical methods, validation studies\ncan supplement theory-based hypotheses on latent phenomena and their causal effects on\nmeasurable behavioural outcomes. For example, we can test whether there is a common\nl atent factor that causes the covariance among a set of items that should reflect the same\nunderlying trait. This approach does not guarantee that the common factor represents a\nspecific latent phenomenon, nor does it provide information on the actual causal process at\nplay between a latent phenomenon and measurement outcomes. However, it does provide\nplausibility for the measurement model under the assumption that the latent phenomenon\nexists and that it can reasonably be measured using the measurement instrument.\nAn additional complexity for LLMs pertains to a measurement unit problem, namely,\nwhether an LLM can be considered analogous to a \"population\" versus an \"individual\u201d.\nThis issue is straightforward for humans but unknown for LLMs, yet imperative to the\napplication of appropriate methods. As LLMs are trained on vast corpora of human-written data, they represent a diverse range of information, perspectives, and writing styles\nextracted from text written by countless individual people. When prompted, the LLM\nsamples from its learned distribution to generate a response. An LLM could be seen as\nanalogous to an approximation of the population distribution of language and information\nin the training data, or as analogous to a single \"average\u201d individual. This distinction has\nfar-reaching implications for validation studies, because no instrument can be validated on\na single individual."}, {"title": "1.4 The present study", "content": "While approaches from machine behaviour and psychology can be a useful guiding frame-work for studying LLMs (and AI models more generally), we argue that the current under-lying measurement theoretical approach is insufficient to fully grasp the nuances of LLM\nbehaviour. Without the use of psychometric methods to study latent phenomena in LLMs,\nwe lack the analytical granularity to assess the validity of the findings. As of yet, it remains\nunknown whether there is any meaningful latent representation at all, or whether we are\nchasing cognitive phantoms in LLMs.\nTo test whether measurement instruments from human psychology can be used to draw\nvalid inferences on latent phenomena from LLM responses, we administered validated\npsychometric questionnaires designed to measure specific latent constructs in humans. For\nthis validation study, we assume that an LLM is analogous to a \"population\" from which\nrandom samples can be drawn. We test how well the theorised latent structure is replicated\nin a human sample and samples from different LLMs, and compare them to one another."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Human data", "content": "401 human participants were recruited from a representative UK sample (per age, sex and\nracial identity) via the online crowdsourcing platform Prolific (www.prolific.com). We\nexcluded participants that failed an attention check or completed the questionnaires too\nquickly (< 6 mins.), resulting in a final sample size of n = 365. The average age was 46.9\nyears (SD = 15.31, min. 18 years) with 51.8% female and 84.9% white."}, {"title": "2.2 LLM data", "content": "We collected responses from three GPT models: GPT-3.5-turbo-0125 (GPT-3.5-T; training\ndata up to September 2021), GPT-4-0612 (GPT-4; training data up to September 2021), and\nGPT-4-0125-preview (GPT-4-T; training data up to December 2023). To match the human\nsample size, 401 responses were collected for each model using the OpenAI API. Any\nresponses that did not answer questionnaire the items were considered invalid and removed\n(e.g., refusal or simply repeating the input prompt). This resulted in a total sample size of\nn = 399 (GPT-3.5-T), n = 387 (GPT-4), and n = 401 (GPT-4-T), respectively.\nDefault parameter settings were used for all LLMs with the exception of the temperature\nvalue. Temperature controls how deterministic the responses are, where higher values\neffectively allow tokens with lower output probabilities to be selected. Temperature has\nbeen shown to affect the average scores of some latent constructs in LLM responses [10],\nalthough potential effects on underlying factor structure are unknown. Therefore, we drew\ntemperature values by sampling from a uniform distribution ranging from 0 to 1 in steps of\n0.01 for a total of 401 values to match the human sample size. The value 0 was only allowed\nto be sampled once, as this results in a fully deterministic response.\nThe input prompt containing the questionnaires made use of pseudo-code to encourage\nresponding in a consistent format (see Appendix A for a snippet of the input prompt). Ques-\ntionnaire instructions were identical to those used for the human sample with additional\ninformation about the expected formatting and response format."}, {"title": "2.3 Materials", "content": "We administered two validated personality questionnaires. Prior to any analysis, reverse-scored items were recoded.\nThe first questionnaire is the HEXACO-60 (H60) [20], a shortened version of the 100-item\nHEXACO-PI-R [21]. The H60 consists of 60 items (e.g. \"Most people tend to get angry more\nquickly than I do.\u201d) answered on a 5-point Likert scale ranging from 1 (\u201cstrongly disagree\u201d) to 5 (\u201cstrongly agree\u201d). The H60 measures six dimensions of personality, evaluated by 10\nitems each: Honesty-Humility, Emotionality, eXtraversion, Agreeableness, Conscientiousness, and\nOpenness to experience. Cronbach's alpha values for internal consistency reliability ranged\nfrom 0.77 (Agreeableness and Openness) to 0.80 (Extraversion) in a college sample and\nfrom 0.73 (Emotionality and Extraversion) to 0.80 (Openness) in a community sample [20].\nItem-level factor analysis of H60 responses revealed the same factor structure as found in\nthe validation study of the longer version of the questionnaire [20, 21].\nThe second instrument is the Dark Side of Humanity Scale (DSHS) [22], which is a recon-\nstruction of the Dark Tetrad personality traits [23]. The questionnaire consists of 42 items\n(e.g. \"I enjoy seeing people hurt\u201d), answered on a 6-point Likert scale ranging from 1 (\u201cnot"}, {"title": "2.4 Analysis plan", "content": ""}, {"title": "2.4.1 Latent variable approach", "content": "Differences in latent representations were assessed by comparing factor structures between\nthe human sample and the LLM samples through factor analysis (FA). Broadly speaking,\nthere are two types of FA: exploratory factor analysis (EFA) and confirmatory factor analysis\n(CFA). We first assessed the assumptions of FA (linearity and multivariate normality of items,\nfactorability of the variables, absence of extreme multicollinearity and outlier variables)\n[25]. Some violation of linearity and normality is acceptable, so long as there is no true\ncurvilinearity and non-normality is mitigated through robust estimation methods [25, 26].\nThe remaining assumptions imply the existence of a latent structure (or lack thereof - in\nwhich case there is no latent variable to be estimated).\nAn EFA is a descriptive analysis to determine the underlying factors (dimensions) that\ncause variation and covariation in the responses to a set of items [26]. Factors should be\nmeaningfully interpretable - that is, a set of items that belong to the same factor should be\nconceptually similar to one another [25]. For example, the items \"In social situations, I'm\nusually the one who makes the first move\u201d and \"The first thing that I always do in a new\nplace is to make friends\u201d are easily interpretable as items that relate to extraversion [20].\nOn the other hand, a CFA is usually performed to verify a strong a priori expectation about\nthe theorised latent structure, often based on EFA results in a previous study [26]. The\nexpected factor structure is specified before running the analysis, including the number\nof dimensions and patterns in item-factor relationships. A CFA attempts to reproduce the\nobserved variance-covariance matrix of the items, based on the specified factor structure\n[25]. The similarity of the reproduced variance-covariance matrix is then compared to the\nobserved matrix and assessed by a combination of different fit indices (e.g., SRMR, RMSEA,\nand CFI), where acceptable fit provides evidence for the hypothesised factor structure in\nthe current sample [26]."}, {"title": "2.4.2 Composite score analysis", "content": "To illustrate the necessity of a thorough latent variable approach, we additionally inves-tigated what our findings would have been if we analysed only composite scores per\ndimension, the current most common method of analysing LLM responses to psychometric\nquestionnaires. Differences between LLM scores and human scores were tested through a set\nof Kruskal-Wallis tests (non-parametric one-way ANOVA) and followed up with post-hoc\nDunn tests with Bonferroni correction.\nWe additionally calculated correlations between respondents' mean scores on the different\ndimensions of the DSHS and the Honesty-Humility dimension of the H60. Negative inter-factor correlations are consistent with theory and previous findings since these dimensions\nare conceptual opposites [24], and this approach has previously been used to assess construct\nvalidity in LLMs [16]. Note that this analysis is based on composite scores and hinges on a\nlatent structure: inter-factor correlations that are consistent with theory are only meaningful\nin the presence of a meaningful latent structure in the responses. Differences in inter-factor\ncorrelations between the human sample and the LLM samples were considered significant\nwhen the 95% confidence interval for difference in correlations did not contain 0 (i.e., 95%\nconfidence that the correlations are not equal in both groups [27])."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Latent variable approach", "content": "The assumptions for factor analysis could not be tested for GPT-4-T due to absence of\nvariability in a number of items in both the H60 and the DSHS. GPT-4-T responses had\nto be excluded from analysis as this rendered any FA impossible. The human sample\nviolated the assumptions of linearity and multivariate normality in both questionnaires to\nan acceptable degree [25] and met all other assumptions (see Appendix B for further details\non all assumption checks for all samples). The same was found in the H60 responses of\nthe remaining LLM samples, but there were additional issues. A few H60 items showed\nevidence of multicollinearity in both GPT-3.5-T (4 items) and GPT-4 (2 items), though just\nwithin the commonly accepted range [25]. LLM data violated several assumptions in the\nDSHS data, but most importantly, neither GPT-3.5-T responses nor GPT-4 responses met\nthe assumption of factorability. As this implies a lack of any underlying latent factor, FA of\nLLM DSHS data could not be justified. Instead, FA was performed only on the H60 for the\nhuman, GPT-3.5-T, and GPT-4 samples."}, {"title": "3.1.1 Confirmatory factor analysis", "content": "In the human sample, the CFA showed mediocre fit (SRMR = 0.08, RMSEA = 0.07, CFI =\n0.75). The CFA on GPT-3.5-T responses produced an improper solution containing factor\ncorrelation estimates larger than 1.0, and the CFA on GPT-4 responses could not converge\nto a final solution at all. Therefore, CFA results for the LLMs cannot be interpreted. Since\nthere were signs of an alternative latent structure in the human sample, we followed up\nwith EFAs on the H60 data in each sample."}, {"title": "3.1.2 Exploratory factor analysis", "content": "Inspection of scree plots revealed potential latent structures consisting of 7 factors (human\nand GPT-4 samples) and 5 factors (GPT-3.5-T sample). The theoretical item-factor rela-tionships as found in earlier research [20] are visualised in Figure 1a with the observed\nrelationships in the human and GPT samples (Figure 1b, 1c, 1d) to illustrate the differences\nin latent structures between the groups.\nThe factor structure found in the human sample is plausible: items which should theoreti-cally co-vary largely do, with a few exceptions (e.g., an extra dimension, some items that"}, {"title": "3.2 Composite score analysis", "content": "All GPT models had significantly higher scores on Extraversion, Agreeableness, and Open-ness than human respondents. GPT models scored equally or significantly higher than\nhuman respondents on Humility-Honesty, equally or significantly lower than humans\non Emotionality, and equally or significantly higher than humans on Conscientiousness.\nConcerning the Sadistic Cruelty responses, GPT-4-T showed zero variation (i.e., only re-sponding with \"not at all like me\u201d) across the 401 responses. The LLMs generally had\nsignificantly lower scores on all dimensions of the DSHS, with one exception: GPT-3.5-T\nresponses showed significantly higher scores than the human sample on Sadistic Cruelty\nand Entitlement Rage. An LLM with above-average tendencies towards sadistic cruelty\nand entitlement rage could pose serious risks in the context of AI safety and alignment [6].\nMean scores and standard deviations per dimension for both questionnaires can be found\nin Appendix C (Table C1).\nAs expected, the correlations between scores on the Honesty-Humility dimension of the\nH60 and the dark personality dimensions in the DSHS were negative in the human sample\n(Table 1). Correlations for GPT-4 and GPT-4-T specifically were negative, but most were\nsignificantly weaker than in the human sample. For the dimension Sadistic Cruelty, we\nfound a similar correlation (GPT-4) or the correlation could not be calculated (GPT-4-T)\ndue to zero variability in scores on that dimension. Importantly, without consideration\nfor underlying latent structures, these findings would incorrectly have been considered\nevidence for construct validity of the administered questionnaires in LLMs.\nIn contrast to GPT-4 and GPT-4-T, GPT-3.5-T scores on the DSHS dimensions show moderate\npositive correlations with the Humility-Honesty dimension. This is the opposite direction\nof what one would expect since the Humility-Honesty dimension is theoretically and\nempirically antithetical to the dark personality [24].\nThe conclusions above, based on composite scores, are representative for those found in\nearlier research on behaviour of LLMs where underlying latent structures have not been\ntaken into account (e.g., [10, 13, 16]). In this paper, we went one step further by evaluating\nthe latent structure of the questionnaire data. Our findings suggest that questionnaires\ndesigned for humans do not measure similar latent constructs in LLMs, and that these latent\nconstructs may not even exist in LLMs in the first place."}, {"title": "4 Discussion", "content": "The motivation for this paper stemmed from the need to understand LLM behaviour more\ngranularly. Although the use of existing psychometric questionnaires is promising, we\nquestioned the validity of administering such questionnaires to LLMs and merely analysing\ntheir composite scores on questionnaire dimensions. We argued that a latent variable\napproach is necessary to adequately reflect on the validity of the findings."}, {"title": "4.1 The latent variable lens", "content": "The latent variable approach is a necessary tool to examine the validity of psychometric\nquestionnaires for LLMs in comparison to a human sample. The lack of reasonable latent\nstructure in LLM responses prohibited us from statistically comparing human and LLM\ndata directly (e.g., with MGCFA). Arbitrary patterns in the LLM responses for either ques-tionnaire led to nonsensical parameter estimations in the CFA, and sometimes none at\nall (i.e., for GPT-4-T and all LLM responses to the DSHS). Further investigation revealed\nthat covariances among LLM responses were so arbitrary that even an EFA did not yield\nmeaningfully interpretable dimensions for either of the remaining LLMs. While responses\nfrom the human sample were largely similar to the theoretical structure, responses from the\nLLMs were not at all. In other words, we found no evidence that we can validly measure\nthe same latent traits in LLMs as in humans using existing questionnaires, nor did we find\nevidence that LLM responses contained any meaningful latent structure at all. The lack\nof an indication for latent representations in LLMs on two commonly used measurement\ninstruments and constructs is concerning."}, {"title": "4.2 Conclusion based on composite scores", "content": "Compared to the human sample, all LLMs had higher scores on socially desirable personality\ntraits (such as Openness and Agreeableness), and lower scores on less desirable personality\ntraits (such as Successful Psychopathy and Grandiose Entitlement). GPT-3.5-T showed\nsignificantly higher scores on Sadistic Cruelty and Entitlement Rage compared to the human\nsample. In the absence of a thorough psychometric evaluation, this would have been the\nmain conclusion of this paper (similar to findings in [10, 13]) with a perhaps worrisome\nconclusion that GPT-3.5-T is unsafe.\nFurther inspection showed that GPT-3.5-T composite scores for the dark personality traits\nwere positively correlated to the Honesty-Humility dimension of the HEXACO-60 - a rela-tionship that is in stark contrast to what one would expect: a positive relationship between\ndark personality traits (i.e., traits related to dishonest, entitled, and sadistic behaviours) and\nHonesty-Humility is highly implausible. However, GPT-4 and GPT-4-T responses showed\nlow to moderate negative inter-factor correlations, in line with expectations and previous\nfindings.\nIt deserves extra emphasis that a composite score-based assessment of construct validity\n(similar to [16]) may have concluded that there is evidence that existing psychometric\nquestionnaires are valid for GPT-4 (and perhaps even GPT-4-T to an extent), but not for GPT-3.5-T. In other words, such an analysis would inevitably have glanced over the arbitrary and\nincoherent latent structures and would have reached wrong conclusions about the validity\nof these questionnaires."}, {"title": "4.3 Implications", "content": "The common practice of interpreting composite scores is insufficient at best and troublingly\nnaive at worst. That approach glances over the implicit assumptions that LLM \"behaviour\u201d\nis internally represented similarly to what we know about human cognition, and that such\na latent construct exists in LLMs at all. While the existence of a latent trait cannot be\n(dis)proven, the latent variable approach is a uniquely adequate method for validation\nstudies of psychometric instruments for LLMs: it provides a safeguard against falsely\nattributing semblances of human traits to true underlying representations of latent traits or"}, {"title": "4.4 Limitations and future work", "content": "The current study provides initial evidence that psychometric questionnaires designed\nfor human are not guaranteed to be valid for LLMs, and that LLMs might not contain\nhuman-like latent traits to begin with. However, several points warrant further research.\nFirst, only models from the GPT family were evaluated. Evaluation of a larger range of\nmodels (incl. open-source models) will provide a more rounded understanding of potential\nl atent traits in LLMs and how to measure them in general, particularly in comparison to\nl atent traits in humans.\nSecond, our study is limited by only evaluating dimensions of personality with two ques-tionnaires. The arbitrary LLM response patterns found in this study may not generalise to\ndifferent questionnaires or different traits entirely. Latent variable approaches for validation\nshould be investigated using different LLMs and instruments measuring various other\nl atent phenomena. Latent cognitive abilities could be investigated more granularly using\nthis approach as well, for example, by using item response theory (IRT) models.\nFinally, our study leans on the assumption that an LLM can be treated analogous to a\npopulation of humans, which is not guaranteed to hold. In the event that an LLM is instead\nanalogous to an individual, underlying latent structures cannot be adequately estimated\nas this requires a certain amount of variation at the trait level. In that case, however, one\nwould also expect the direction and magnitude of the inter-factor correlations between\nthe dark personality traits and the Honesty-Humility dimension to remain consistent with\ntheory and previous findings. Future work should investigate this matter, for example,\nby comparing between- versus within-response variances for various questionnaires and\nconstructs. Until then, the nature of the analogy between LLMs and humans remains a\ncomplex question."}, {"title": "4.5 Conclusion", "content": "We presented evidence that responses of LLMs based on questionnaires developed for\nhumans do not withstand psychometric rigour. The latent representations found in LLM\nresponses are widely arbitrary and vastly different to humans. These findings cast doubt on\nconclusions drawn elsewhere about the cognition and psychology of LLMs. A thorough\npsychometric evaluation is essential for studying LLM behaviour. It may help us decide\nwhich effects are worth pursuing, and which effects are cognitive phantoms."}, {"title": "Ethics statement", "content": "This study, procedure and data collection were approved by the local IRB before data\ncollection."}]}