{"title": "A Law of Next-Token Prediction in Large Language Models", "authors": ["Hangfeng He", "Weijie J. Su"], "abstract": "Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer-a universal phenomenon observed across a diverse array of open-source LLMs, built on architectures such as Transformer, RWKV, and Mamba. We demonstrate that this law offers new perspectives and insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and information flow. Overall, our law enables more fine-grained approaches to the design, training, and interpretation of LLMs through scrutinizing their internal data processing mechanisms.", "sections": [{"title": "1 Main Results", "content": "LLMs are nonlinear models that iteratively map a sequence of token embeddings to a new sequence through attention mechanisms and various other operations at each layer. Given a sequence of tokens as input data, a pre-trained LLM transforms these tokens into a sequence of embeddings. The model then uses the last embedding, denoted as \\(h_{last}\\), from the last layer to predict the next token in the sequence. Consider S sequences, each containing \\(T_s\\) tokens for \\(1 < s < S\\). For \\(1 \\leq t \\leq T_s\\), let \\(x_{t+1}\\) represent the next token that the LLM aims to predict based on the preceding t tokens, \\(x_1x_2\u2026\u2026\u2026x_t\\), through the last embedding \\(h_{t,last}^s\\). This process forms a dataset \\(D := \\{(h_{t,last}^s, x_{t+1}^s) \\vert 1 < s < S, 1 \\leq t \\leq T_s - 1\\}\\).\nTo assess the capability of the LLM in predicting the next token, we evaluate how well a linear regression model fits on the dataset D. For this purpose, we identify x with its index in the token vocabulary. Let \\(\\hat{x}_{next} = wh+b\\) denote the least-squares fit on D. This suggests using the following"}, {"title": null, "content": "metric, which we term the prediction residual (PR), to quantify the LLM's next-token prediction capability:\n\\[ PR := \\frac{\\Sigma(x_{next} - \\hat{x}_{next})^2}{\\Sigma(x_{next} - \\overline{x}_{next})^2}, \\]\nwhere the sum is over all \\(x_{next} = x_{t+1}^s\\), \\(\\hat{x}_{next} = wh_{t,last}^s+b\\), and \\(\\overline{x}_{next}\\) represents the mean of all \\(x_{t+1}^s\\). In statistical terms, this measure is known as the fraction of variance unexplained, equivalent to 1 minus the coefficient of determination. It serves as a canonical metric for evaluating the proportion of variance in the dependent variable that remains unaccounted for by the independent variables. A high PR value indicates limited predictive power of the token embeddings, while a low value suggests strong predictive capability.\nTo investigate how the predictive power of an LLM with depth L evolves across its layers, we calculate the PR for the next-token prediction task at each intermediate layer. Let \\(PR_l\\) denote this value for the l-th layer, where \\(1 \\leq l \\leq L\\). The law of equi-learning (see Fig. 1) states that the dynamics of predictive power across layers follow the relationship\n\\[ PR_l \\approx \\rho^{l-1} \\times PR_1 \\]\nfor some decay ratio \\(0 < \\rho < 1\\). Since the token embeddings at the input layer (the 0-th layer) are not yet contextualized, they are not included. This implies that \\(\\log PR_{l+1} - \\log PR_l \\approx - \\log \\rho\\), indicating a roughly constant reduction in the logarithm of the PR value across each layer, hence the name equi-learning. The Pearson correlation coefficients between the logarithm of the PR value"}, {"title": null, "content": "and the layer index range from -0.983 to -0.997 in Fig. 1. For GPT-1, for example, the decay ratio \\(\\rho\\) is approximately 0.993 (as shown in the top-left plot of Fig. 1). In general, \\(\\rho\\) depends on various factors, including the model architecture, pre-training data, model depth, feature dimension, and pre-training time.\nThis law provides what may be the first precise geometric characterization of the learning process for contextualized token embeddings within the intermediate layers of LLMs. Notably, the pre-training objective focuses solely on the last-layer embeddings, aiming to minimize a loss function associated with the last-layer \\(PR_L\\). Surprisingly, this training dynamics inherently ensure that each layer contributes equally, rather than allowing some layers to carry a disproportionate amount of the workload. Related findings have previously been observed in the context of multilayer neural networks [16].\nThis law is universal, as it emerges across a wide range of open-source LLMs with varying architectures, model sizes, and pre-training data. Additionally, we employ a diverse collection of probing datasets as input to calculate PR and evaluate the law of equi-learning. These datasets include BookCorpus [47], C4 [33], OpenWebText [12], Wikipedia [10], peS2o [37], The Pile [11], Redpajama [6], and OSCAR [38].\nWhen does the law emerge? To deepen our understanding of the law's dynamics during training, we investigate the effects of three key factors-training steps, training epochs, and data repetition on its emergence throughout the process. As illustrated in Fig. 3, the progression of this law as a function of training steps closely resembles the behavior of the equi-separation law observed in multilayer neural networks [16]. At model initialization, the PR of contextualized token embeddings for next-token prediction may exhibit an upward trend from lower to higher layers. However, after a certain amount of training (e.g., around 8,000 steps), the law of equi-learning becomes apparent. Beyond this point, the decay ratio continues to decrease until convergence, with the law consistently manifesting during this phase.\nAlthough LLMs are typically trained over a limited number of epochs or even a single epoch, we also explore the training dynamics in data-constrained regimes, anticipating that the availability of text data may soon be limited by the finite volume of Internet content [26]. Specifically, we utilize various pre-trained 2.8B GPT-2 models released by [26] to examine the impact of training epochs and data repetition. As depicted in Fig.4 and Fig.5, the law of equi-learning emerges as long as the number of epochs is not too small and the number of repeats is not excessively high. Considering these three factors training steps, training epochs, and data repetition-We observe that a sufficient total number of tokens facilitates the emergence of the equi-learning law, provided that the number of unique tokens is adequate. This finding serves as a crucial condition for training effective LLMs and offers insights into optimizing the training process."}, {"title": "2 Perspectives from the Law", "content": "The universality of the equi-learning law provides fine-grained perspectives that are applicable to the practical development of LLMs. These perspectives offer new insights into the training processes of LLMs and contribute to advancing transparency in these black-box models. We illustrate the impact of this law on key aspects such as model scaling, pre-training tasks, and information flow. Additional findings, including the impact of pre-training data quality, are discussed in the Supplementary Materials."}, {"title": "3 Discussion", "content": "The complex details of pre-trained LLMs have led many practitioners and researchers to anticipate that the internal workings of these models are inherently chaotic, lacking any consistent structures across different contexts. In this work, we challenge this view by introducing the law of equi-learning, which describes how contextualized token embeddings evolve from the first to the last layer. This law, which is both quantitative and precise, has been consistently observed across various architectures, including Transformer, Mamba, and RWKV. Its emergence provides crucial insights into the training and interpretation of LLMs, offering new perspectives that deepen our understanding of their internal mechanisms.\nThe significance of the equi-learning law lies in its potential to refine the development and application of LLMs. An open question is how the decay ratio \\(\\rho\\) depends on factors such as model depth and pre-training data. Understanding this dependence could lead to the development of more efficient LLMs by minimizing \\(\\rho^{l-1}\\), the overall decay ratio, in the equi-learning law. The law's eventual emergence also suggests the possibility of setting different learning rates across layers to accelerate convergence to the equilibrium described by the law. Moreover, preserving the equi-learning law during model pruning and fine-tuning may yield practical benefits, potentially through the preservation of certain weights or the use of techniques like LoRA [18].\nBroadly, the equi-learning law could be leveraged in transfer learning, particularly when the"}, {"title": "Supplementary Materials", "content": "This section outlines the general experimental setup, distinctive configurations for main text figures, and additional results, with comprehensive details available in our code repository\u00b9.\nGeneral Setup\nIn this subsection, we detail the general experimental setup utilized throughout this study.\nLarge language models (LLMs). In this study, we focus on LLMs, where the objective is to predict the subsequent token in a sequence, constrained to attend solely to preceding tokens. Formally, given a sequence of tokens \\(x_1x_2...x_t \\in V^t\\), let \\(h_i^l\\) represent the contextualized token embeddings of the i-th token at the l-th layer, where \\(1 < i < t, 1 < l < L\\). The contextualized token embeddings from the top layer for the last token in the sequence, \\(h_{t,L}\\), is employed to predict the next token \\(x_{t+1} \\in V\\), with \\(1 < t < T - 1\\).\nPrediction residual (PR). To assess the capability of contextualized token embeddings in predicting the next token, we calculate the fraction of variance unexplained (FVU) by a linear regression model predicting the next token. Notably, this metric is similar to the concurrent measure proposed by [36], specifically the Root Mean Squared Error (RMSE) of the optimal linear regressor based on the features in multilayer neural networks."}, {"title": "Layer normalization.", "content": "In pre-layer normalization (pre-LN) models, default initialized layer normalization is applied to all layers except the last-layer token embeddings. This is because layer normalization is moved to the input of each sub-block, with an additional layer normalization added after the final self-attention block [31]. Given that different models utilize distinct forms of layer normalization\u2014such as LayerNorm [2] in GPT-2 and RMSNorm [46] in Llama-1\u2014we will apply the specific layer normalization technique used by each model to normalize its contextualized token embeddings. Throughout this paper, nearly all models are pre-LN models, with the exceptions of GPT-1, BERT, and RoBERTa."}, {"title": "Probing datasets.", "content": "For our experiments, we consider eight distinct probing datasets: BookCorpus, C4, OpenWebText, Wikipedia, peS2o, Pile, Redpajama, and OSCAR. We sampled sentences based on their average length, extracting 3,000 sentences from BookCorpus, 200 from C4, and 100 from each of the remaining datasets. For consistency, sentences were truncated to a maximum length of 512 tokens across all datasets, except for BookCorpus. Unless otherwise specified, we will utilize the probing dataset that exhibits the strongest Pearson correlation in next-token prediction for each LLM."}, {"title": "Detailed Experimental Settings", "content": "In this subsection, we show detailed experimental settings for the figures in the main text.\nLarge language models. As depicted in Fig. 1, the law of equi-learning is observed in various open-source large language models. Please note that phi-1 was excluded from our analysis, as it is a LLM specifically designed for code. For each model, we evaluate the largest size that can be executed on a local machine equipped with two L40S GPUs, each possessing 48 GB of memory. The corresponding model sizes are as follows: 117M for GPT-1, 1.5B for GPT-2, 13B for Llama-1, 13B for Llama 2 and Llama 2-Chat, 8B for Llama 3 and Llama 3 Instruct, 7B for three versions of Mistral 7B and Mistral 7B-Instruct (v0.1, v0.2, v0.3), 1.3B for phi-1.5, 2.7B for phi-2, 14B for phi-3 (phi-3-medium) with varying context lengths (4K, 128K), 14B for RWKV and RWKV-Raven, and 2.8B for Mamba. The corresponding probing datasets used are as follows: BookCorpus for GPT-1, BookCorpus for GPT-2, OSCAR for Llama-1, peS2o for Llama 2 and Llama 2-Chat, BookCorpus for Llama 3 and Llama 3 Instruct, C4 for three versions of Mistral 7B and Mistral 7B-Instruct (v0.1, v0.2, v0.3), BookCorpus for phi-1.5, BookCorpus for phi-2, C4 for phi-3 (phi-3-medium) with varying context lengths (4K, 128K), C4 for RWKV and RWKV-Raven, and BookCorpus for Mamba. Notably, reinforcement learning from human feedback (RLHF) [27] does not significantly impact the law, as demonstrated by Llama-2-13B, Llama-3-8B, Mistral-7B-v0.1, Mistral-7B-v0.2, Mistral-7B-v0.3, and RWKV, along with their fine-tuned versions, as illustrated in Fig. 1."}, {"title": "Visualization.", "content": "Figs. S1, S2, and S3 depict the visualization of contextualized token embeddings across various layers of the GPT-1 model, using 3,000 sentences from the BookCorpus (consistent with the setting in Fig.1). We examine three token pairs: they vs. them, have vs. had, and are vs. is. Principal component analysis (PCA) is applied to project these contextualized token embeddings onto a two-dimensional plane. The results show a distinct and progressive separation of contextualized token embeddings within each pair as one moves from the lower to the upper layers of the model. Moreover, as illustrated in Fig. S4, the law of equi-learning"}, {"title": "Training dynamics.", "content": "As illustrated in Figs. 3, 4, and 5, we examine the evolution of the observed law throughout the training process. Specifically, Fig. 3 depicts the PR of contextualiezed token embeddings at each layer of Pythia-1B at various training steps (an enlarged version at initialization (Step=0) is in Fig. S7), using BookCorpus as the probing dataset. Additionally, Figs. 4 and 5 present the PR of contextualized token embeddings at each layer of pre-trained 2.8B GPT-2 models, as released by [26], across different training epochs and data repetitions, with OpenWebText serving as the probing dataset."}, {"title": "Model scaling.", "content": "As illustrated in Fig. 6, the contextualized token embeddings are compared across different model sizes within the same model series. We analyze three distinct model series\u2014GPT-2, RWKV-Raven, and Mamba each with at least four different model sizes. The largest version of each series is depicted in Fig. 1."}, {"title": "Pre-training task.", "content": "As illustrated in Fig. 7, different probing tasks are employed to analyze the contextualized token embeddings of BERT, ROBERTa, and T5. Notably, the probing datasets used are peS2o for BERT and RoBERTa, and C4 for T5. Since masked language modeling (MLM) and span corruption (SC) mask or corrupt only 15% of tokens, leading to a corresponding reduction to"}, {"title": "Information flow.", "content": "As illustrated in Fig. 8, the contextualized token embeddings of the current token (xt) at each layer are leveraged to predict various tokens in the sequence, spanning previous token (xt-1) to next next token (xt+2), including the default next token (xt+1). For clarity, we present four models selected from those shown in Fig. 1: GPT2-XL, Llama-3-8B-Instruct, RWKV-Raven-14B, and Mamba-2.8B."}, {"title": "Measure analysis.", "content": "Figure S8 illustrates that the widely used metric of separation fuzziness, commonly applied to assess deep learning features in classification tasks [28, 9, 16], is inadequate for the emergence of the law of equi-learning in LLMs. This inadequacy may stem from the larger token vocabulary size compared to the embedding dimension and the presence of very similar or even identical contexts followed by different tokens in natural language data [44]. For simplicity, we selected four models from Fig. 1: GPT-2 XL, Llama-3-8B-Instruct, RWKV-Raven-14B, and Mamba-2.8B, and utilized separation fuzziness to evaluate the quality of contextualized token embeddings. Furthermore, as demonstrated in Fig. S9, the law of equi-learning becomes obscured when the vocabulary is shuffled, resulting in differing token index orders. This observation suggests that the widely adopted byte pair encoding (BPE) algorithm [35] in tokenizers can produce a meaningful token index order, which is critical to the emergence of the law of equi-learning. Further investigation is needed and is deferred to future work."}, {"title": "Layer normalization analysis.", "content": "As illustrated in Fig. S10, layer normalization plays a critical role in pre-LN models, as the absence of additional layer normalization makes the law noisier. Notably, this normalization effect can also be attained through standardization, specifically \\(z = \\frac{x-\\mu}{\\sigma}\\) across the embedding dimension. For simplicity, we selected four models from Fig. 1-GPT-2 XL, Llama-3-8B-Instruct, RWKV-Raven-14B, and Mamba-2.8B-and analyzed the impact of layer normalization. This finding aligns with observations related to the equi-separation law in multilayer neural networks[16], where batch normalization is crucial for its emergence. It is important to note that batch normalization does not impact the PR of contextualized token embeddings in our case."}, {"title": "Probing data analysis.", "content": "As demonstrated in Fig. S11, the observed law's clarity diminishes when an inappropriate probing dataset is selected, although a general descending trend remains evident. For simplicity, we selected four models from Fig. 1-GPT-2 XL, Llama-3-8B-Instruct, RWKV-Raven-14B, and Mamba-2.8B-and presented the PR of their token embeddings using the RedPajama dataset as the probing dataset. Moreover, the optimal probing dataset among the eight evaluated shifts from OSCAR and peS2o to BookCorpus as models progress from Llama-1 and Llama 2 (including its chat variant) to Llama 3 (and its instruct variant), suggesting that higher-quality pre-training data may necessitate higher-quality probing data to facilitate the emergence of the law of equi-learning. These findings underscore the critical importance of selecting appropriate probing data for the emergence of the law of equi-learning."}, {"title": "Pre-training data analysis.", "content": "As illustrated in Fig. S12, GPT-2 models pre-trained on two different datasets, C4 and OSCAR, exhibit distinct behaviors regarding the emergence of the law of equi-learning when evaluated on the same probing dataset (i.e., OpenWebText). Specifically, models pre-trained with OSCAR demonstrate more noise in the law's emergence compared to those pre-trained with C4. This discrepancy is likely attributable to the higher noise levels in OSCAR, stemming from its less stringent deduplication. These models were released by [26]. Our findings underscore the significant impact of pre-training data quality on the emergence of the law of equi-learning, suggesting that higher quality pre-training data may result in a more pronounced manifestation of this law."}]}