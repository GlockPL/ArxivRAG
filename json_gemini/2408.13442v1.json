{"title": "A Law of Next-Token Prediction in Large Language Models", "authors": ["Hangfeng He", "Weijie J. Su"], "abstract": "Large language models (LLMs) have been widely employed across various application do-mains, yet their black-box nature poses significant challenges to understanding how these mod-els process input data internally to make predictions. In this paper, we introduce a precise andquantitative law that governs the learning of contextualized token embeddings through interme-diate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layercontributes equally to enhancing prediction accuracy, from the lowest to the highest layer-auniversal phenomenon observed across a diverse array of open-source LLMs, built on architec-tures such as Transformer, RWKV, and Mamba. We demonstrate that this law offers newperspectives and insights to inform and guide practices in LLM development and applications,including model scaling, pre-training tasks, and information flow. Overall, our law enables morefine-grained approaches to the design, training, and interpretation of LLMs through scrutinizingtheir internal data processing mechanisms.", "sections": [{"title": "1 Main Results", "content": "LLMs are nonlinear models that iteratively map a sequence of token embeddings to a new sequence through attention mechanisms and various other operations at each layer. Given a sequence of tokens as input data, a pre-trained LLM transforms these tokens into a sequence of embeddings. The model then uses the last embedding, denoted as hlast, from the last layer to predict the next token in the sequence. Consider S sequences, each containing Ts tokens for 1 \u2264 s \u2264 S. For 1 \u2264 t \u2264 Ts, let x+1 represent the next token that the LLM aims to predict based on the preceding t tokens, xix2\u2026\u2026\u2026x, through the last embedding h,last. This process forms a dataset D := {(h,last, x+1) | 1 \u2264 s \u2264 S, 1 \u2264 t \u2264 Ts \u2212 1}.\nTo assess the capability of the LLM in predicting the next token, we evaluate how well a linear regression model fits on the dataset D. For this purpose, we identify x with its index in the token vocabulary. Let \\hat{x}_{next} = wh+b denote the least-squares fit on D. This suggests using the following metric, which we term the prediction residual (PR), to quantify the LLM's next-token prediction capability:\nPR := \\frac{\\Sigma(x_{next} - \\hat{x}_{next})^2}{\\Sigma(x_{next} - \\bar{x}_{next})^2}, (1)\nwhere the sum is over all xnext = x+1, \\hat{x}_{next} = wh_{last} + b, and \\bar{x}_{next} represents the mean of all x+1. In statistical terms, this measure is known as the fraction of variance unexplained, equivalent to 1 minus the coefficient of determination. It serves as a canonical metric for evaluating the proportion of variance in the dependent variable that remains unaccounted for by the independent variables. A high PR value indicates limited predictive power of the token embeddings, while a low value suggests strong predictive capability.\nTo investigate how the predictive power of an LLM with depth L evolves across its layers, we calculate the PR for the next-token prediction task at each intermediate layer. Let PRl denote this value for the l-th layer, where 1 \u2264 l \u2264 L. The law of equi-learning (see Fig. 1) states that the dynamics of predictive power across layers follow the relationship\nPRl \u2248 \u03c1^{l\u22121} \u00d7 PR1\nfor some decay ratio 0 < \u03c1 < 1. Since the token embeddings at the input layer (the 0-th layer) are not yet contextualized, they are not included. This implies that log PRl+1 \u2212 log PRl \u2248 \u2212 log \u03c1, indicating a roughly constant reduction in the logarithm of the PR value across each layer, hence the name equi-learning. The Pearson correlation coefficients between the logarithm of the PR value"}, {"title": "2 Perspectives from the Law", "content": "The universality of the equi-learning law provides fine-grained perspectives that are applicable to the practical development of LLMs. These perspectives offer new insights into the training processes of LLMs and contribute to advancing transparency in these black-box models. We illustrate the impact of this law on key aspects such as model scaling, pre-training tasks, and information flow. Additional findings, including the impact of pre-training data quality, are discussed in the Supplementary Materials."}, {"title": "3 Discussion", "content": "The complex details of pre-trained LLMs have led many practitioners and researchers to anticipate that the internal workings of these models are inherently chaotic, lacking any consistent structures across different contexts. In this work, we challenge this view by introducing the law of equi-learning, which describes how contextualized token embeddings evolve from the first to the last layer. This law, which is both quantitative and precise, has been consistently observed across various architectures, including Transformer, Mamba, and RWKV. Its emergence provides crucial insights into the training and interpretation of LLMs, offering new perspectives that deepen our understanding of their internal mechanisms.\nThe significance of the equi-learning law lies in its potential to refine the development and application of LLMs. An open question is how the decay ratio \u03c1 depends on factors such as model depth and pre-training data. Understanding this dependence could lead to the development of more efficient LLMs by minimizing \u03c1^{l\u22121}, the overall decay ratio, in the equi-learning law. The law's eventual emergence also suggests the possibility of setting different learning rates across layers to accelerate convergence to the equilibrium described by the law. Moreover, preserving the equi-learning law during model pruning and fine-tuning may yield practical benefits, potentially through the preservation of certain weights or the use of techniques like LoRA [18].\nBroadly, the equi-learning law could be leveraged in transfer learning, particularly when the"}]}