{"title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "authors": ["Le Xue", "Manli Shu", "Anas Awadalla", "Jun Wang", "An Yan", "Senthil Purushwalkam", "Honglu Zhou", "Viraj Prabhu", "Yutong Dai", "Michael S Ryoo", "Shrikant Kendre", "Jieyu Zhang", "Can Qin", "Shu Zhang", "Chia-Chih Chen", "Ning Yu", "Juntao Tan", "Tulika Manoj Awalgaonkar", "Shelby Heinecke", "Huan Wang", "Yejin Choi", "Ludwig Schmidt", "Zeyuan Chen", "Silvio Savarese", "Juan Carlos Niebles", "Caiming Xiong", "Ran Xu"], "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.", "sections": [{"title": "1 Introduction", "content": "Large Multimodal Models (LMMs) have attracted significant attention with their potential applications and emergent capabilities. Recent advancements in both proprietary models [2-5] and open-source LMMs [6, 1, 7\u2013 11] highlight the rapid progress and growing interest in this field. However, despite these advancements,"}, {"title": "2 Related Work", "content": "Recent advancements in Large Multimodal Models (LMMs) have explored two main architectural approaches: the cross-attention style [22, 23] and the decoder-only style. The cross-attention approach, exemplified by models like Flamingo [22, 23] and Llama 3.1 [5], integrates vision and language modalities through a complex attention mechanism to enable deep multimodal understanding. Another approach is the decoder-only architecture [1, 7, 8, 24-36], which we adopt in xGen-MM (BLIP-3), offers a more streamlined solution. This approach connects pre-trained language models to visual inputs using lightweight connectors, simplifying the integration process while maintaining robust multimodal capabilities. The effectiveness of this architecture is evident in models such as MM1 [9], VILA [10], LLaVA [8], phi3-vision [37], and Otter [38].\nTraining methodologies for LMMs typically follow one of the two strategies. The first one uses a light pre-training procedure and heavily relies on visual instruction tuning, as seen in the LLaVA series [8, 29]. Extensive research has been conducted on creating effective instruction-tuning data for a variety of tasks [32, 39\u201343]. The second strategy involves extensive pre-training on large-scale, diverse datasets, followed by visual instruction fine-tuning. This approach, employed by models like MM1 and Idefics2 [11], infuses broad knowledge into the model, which is then fine-tuned to align with human-like interactions and safety standards. While MM1 [9] provides extensive ablations and studies on the recipes aimed at improving LMMs, it releases limited resources for practitioners to adopt the model (e.g., MM1 models and datasets"}, {"title": "3 Model Architecture", "content": "Architecture Overview. As illustrated in Figure 2, the xGen-MM (BLIP-3) framework adopts an architec- ture consisting of a ViT [44, 45], a vision token sampler (perceiver resampler [22]) to downsample the image embeddings, and a pre-trained Large Language Model (phi3-mini [37]). The input to the model can be free- form multimodal interleaved texts and vision tokens from the diverse multimodal data sources we ensemble.\nAny-Resolution Vision Token Sampling. As proved effective in recent LMMs [46\u201348], we adopt a dynamic high-resolution (i.e., \"any-resolution\") image encoding strategy at the fine-tuning and post-training stages. We enable higher-resolution image understanding with image patch-wise encoding. The patch-wise"}, {"title": "4 Training", "content": "Pre-training. The pre-training objective is to predict the next text token across the dataset mixture we pre-train on. Overall, the resulting base model xGen-MM-Phi3-mini-base-r is pre-trained for about 100 billion multimodal tokens from the ensembled dataset, and our pre-training resolution is 384x384 pixels, which aligns with SigLIP [45].\nSupervised Fine-tuning (SFT). We further fine-tune our pre-trained models on instruction-following examples to make them better understand and follow user queries. At the fine-tuning stage, we use a collection of publically available instruction-following datasets [11, 29, 49]. We adopt the any-resolution vision token sampling strategy to allow a better understanding of images of higher resolutions such as text-rich document data. We introduce the technical details for the fine-tuning stage in the following sections.\nInterleaved Multi-Image Supervised Fine-tuning. We conduct a second-stage fine-tuning on the instruc- tion fine-tuned model on a mixture of multi-image and single-image instructions-following samples. The goal for this second-stage fine-tuning is to enhance the model's ability to comprehend interleaved image-text input, which is helpful for multimodal in-context learning, multi-image question answering, and many more practical use cases. For the multi-image fine-tuning, we also adopt the any-resolution vision token sampling strategy same as in the previous SFT stage.\nPost-training. Finally, we perform two stages of post-training to improve the model's helpfulness while mitigating harmful qualities such as hallucination and toxicity. We first perform direct preference optimization (DPO [50]) to improve the model's helpfulness and visual faithfulness. We then perform safety fine-tuning to improve the model's harmlessness. We quantitatively demonstrate Pareto gains in model harmlessness and helpfulness after our post-training."}, {"title": "5 Data", "content": "5.1 Pre-training Data Recipe\nAs indicated in Figure 3, in xGen-MM (BLIP-3), we pre-train on an ensemble of diverse multimodal datasets with the indicated sampling ratios.\nInterleaved Dataset Mixture. We combine MINT-1T (including its HTML, PDF, and ArXiv subsets) with OBELICS (HTML only) to create a more diverse and comprehensive dataset mixture that covers a broader range of domains.\n1. MINT-1T [12] is a trillion token scale multimodal interleaved dataset, containing data sources from HTML, PDF, and ArXiv. As evidenced by MM1 [9] and Idefics2 [11], such multimodal interleaved\n2. OBELICS [11] is another large-scale multimodal interleaved dataset constructed from HTML docu- ments solely. It differs slightly in domain coverage from MINT-1T due to the specific preprocessing steps adopted.\nCaption Dataset Mixture. We integrate a diverse range of caption datasets, with the specifics outlined in the following details.\n1. BLIP3-KALE is a large-scale curated high-quality caption dataset. Details will be discussed in another paper, and the dataset will be made public very soon.\n2. BLIP3-OCR-200M is a curated large-scale OCR dataset to address the limitations of current large multimodal models in handling text-rich images like documents and charts, as traditional image-text datasets often lack adequate OCR annotations. To enhance text comprehension abilities, we use the OCR engine PaddleOCR [51] to annotate images with OCR-specific annotations. Overall, we curate a dataset of 200 million high-resolution images from Datacomp-1B[17]. For each image, we create captions with OCR data by identifying and extracting textual elements using the off-the-shelf OCR engine [51]. Text segments in a caption like \"... text ...\" are modified to include OCR information as \"... text (ocr_info)...\", where ocr_info contains bounding box coordinates for the extracted\n3. BLIP3-GROUNDING-50M is a curated large-scale grounding dataset to enhance the ability to ground semantic concepts in visual features, which is crucial for tasks like object detection, semantic segmentation, and understanding referring expressions [52] (e.g., \"the object to the left of the dog\"). We curate a dataset of 50 million images from Datacomp-1B [17]. For each image, we identify objects and their location information using one of the state-of-the-art open-world image tagging [53] and object detection models [54]. Objects mentioned in a caption like \"... object ...\" are modified to include grounding information as \"... object (grounding_info)...\". where grounding_info contains bounding box information in one of three formats, each capturing a different granularity of localization: (1) <bbox>X1,Y1,X2,Y2</bbox>, (2) \"starts at (x1,y1) and extends up to (x2, y2)\", or (3) \"top-left corner of the image\".\n4. Other Public Datasets Mixture: We also include other publicly available datasets such as uncurated Datacomp-1B [17] image-text pairs, CC12M [14], CC3M [14], VG [15], and SBU [16]."}, {"title": "5.2 Supervised Fine-tuning Data Recipe", "content": "The datasets used in the fine-tuning stage are from public datasets of different domains. We sample data with various domain focuses including multi-modal conversation [29], image captioning [55, 56], visual question answering [57\u201360], chart/document understanding [61\u201364], science and math [65, 66]. In addition to the multi-modal image-text data, we also mix in pure text instruction following data [67, 68] during visual instruction tuning. Ultimately, we collect a mixture of 1 million publically available instruction-tuning samples, on which we fine-tune our model for one epoch.\nThe multi-image instruction tuning stage starts with a model fine-tuned on single-image samples. We use a mixture of public multi-image/interleaved image-text instruction data [69, 70]. To prevent the model from deteriorating on single-image capabilities, we reuse a subset of single-image datasets used in the previous fine-tuning stage and mix them into the multi-image training data."}, {"title": "5.3 Post-training Data Recipe", "content": "Improving Truthfulness by Direct Preference Optimization. We employ VLFeedback [71], a syntheti- cally annotated multimodal preference dataset that uses off-the-shelf VLMs to generate responses to a diverse mix of multimodal instructions that are then scored by GPT4-V [2] along three axes \u2013 helpfulness, visual faithfulness, and ethics. The dataset contains 80k such instructions from which we construct preference data by marking as preferred (and dispreferred) the response with the highest (and lowest) average score across models and filtering out examples with low-scoring preferred responses. We thus generate 62.6k preference examples.\nWe perform 1 epoch of DPO on the combined preference dataset while updating a subset (2.5%) of LLM backbone weights using low-rank adaptation (LoRA [72]). Also, following recent work [50], we generate an additional set of responses that capture the model's intrinsic hallucinations, by performing a second step of DPO per-iteration against the models' output to a noised version of the input image and original query, which we treat as an additional dispreferred response.\nImproving Harmlessness by Safety Fine-tuning. Next, we perform 3 epochs of safety fine-tuning on the train split of the VLGuard [73] dataset, which contains 2k examples of unsafe images and instructions. VLGuard comprises two types of unsafe examples: (1) objectionable images paired with safe instructions and a desirable abstention response, and (2) safe images paired with two types of instruction-response pairs, one safe and another unsafe. The dataset consists of unsafe examples belonging to various subcategories including privacy-violating, risky/sensitive topics (such as politics, sex, and violence), deception, and discrimination. Following the original work, we randomly sample 5k additional examples from the instruction fine-tuning dataset to retain the model's helpfulness without exaggerating its safety behavior. As before, we update a subset (2.5%) of LLM backbone weights using low-rank adaptation."}, {"title": "6 Experiments", "content": "6.1 Pre-training\nFew-shot Evaluation. After the pre-training stage, we evaluate our pre-trained model on classic captioning and VQA tasks, in comparison with previous models that support few-shot learning multi-modal evaluation. We present zero-shot and few-shot (4- and 8-shots) results, as shown in Table 1. Overall, our model achieves competitive multimodal in-context learning performance with comparable-sized LMMs 1. For the OCR tasks (TextCaps and TextVQA) and VQA-v2, it significantly outperforms MM1-3B and even larger models such as Idefics-9B [13] and MM1-7B [9]. On all benchmarks, increasing the number of shots can improve the performance, demonstrating the model's ability to adapt to in-context distributions.\n6.2 Supervised Fine-tuning\nWe evaluate our model on a comprehensive suite of multi-modal (image-text) benchmarks, assessing the model's ability from multiple perspectives. Our evaluation covers general VQA benchmarks [74\u201378], visual perception [49], domain knowledge [79, 80], OCR ability [57, 81], and hallucination [82, 83]. For models fine-tuned on interleaved multi-image datasets, we also evaluate their performance on common multi-image benchmarks [69, 84-86].\nSingle-Image Evaluation. In Table 2, we compare with models in comparable sizes (< 5B), including both closed-source [9] and SoTA open-source models [10, 37]. We report individual benchmark scores along with two average scores: \u201cAvg.(all)\" is simply the average over all benchmarks, and \"Avg.(perc.)\" is the average score over benchmarks that focus on general VQA and visual perceptions. xGen-MM-instruct outperforms previous baselines on both general VQA and visual perception benchmarks. In addition, we"}, {"title": "6.3 Post-training", "content": "Table 4 summarizes the results of two post-training strategies for xGen-MM-instruct. We measure safety performance by ASR% (attack success rate) on the VLGuard test split and hallucination performance using HallusionBench [82] (accuracy on image-context reasoning) and POPE [83] (average F1 score on binary entity presence questions). To ensure post-training doesn't compromise helpfulness, we report performance on a few comprehension benchmarks as a control.\nDPO enhances truthfulness by improving hallucination benchmarks (Row 2), while safety finetuning significantly reduces ASR (Row 3). Helpfulness is also improved slightly, as shown by control benchmarks. The final model, xGen-MM-dpo, includes both improvements."}, {"title": "7 Ablation Studies", "content": "7.1 Pre-training Ablation\nScaling Pre-training Data. We perform an ablation study to explore the relation between the amount of pre-training data and the pre-train evaluation metrics, by varying the data scale from 2B multimodal tokens to 100B multimodal tokens. The data recipe we used here is a mixture of image caption datasets and multimodal interleaved data. As shown in Figure 7, we find that scaling up the number of multimodal tokens from 2B to 60B leads to substantial gain for image-text (COCO-Caps) and OCR (Text-Caps, TextVQA) tasks, and further increasing the data size to 100B has moderate additional benefit in terms of few-shot evaluation metrics.\nPre-training Data Recipe. We discuss the impact of different data recipes for pre-training. Specifically, we perform ablation studies on top of a base data recipe: use Obelics [13] as the multimodal interleaved data source while keeping the caption datasets mixture the same. We also consider two other recipes (1) using MINT-1T [12] as interleaved data replacement, and (2) mixing additional pure text-only instruction-tuning data as a pre-train dataset. As shown in Table 5, we see a performance improvement using MINT-1T for image-text alignment (COCO-Caps) and OCR (Text-Caps, TextVQA), with a slight performance drop on OK-VQA, which is a knowledge-intensive task. We also find that adding text data can help attain the performance on OK-VQA that relies more on LLM capacity.\nVisual Backbones. We also explore if different visual backbones have an impact on the performance of vision-language tasks. We compare two types of visual encoders, DFN and SigLIP. Empirically, we find SigLIP provides better visual representations that boost performance on OCR tasks."}, {"title": "7.2 SFT Ablation", "content": "We conduct ablation studies at the instruction fine-tuning stage, focusing on several model design choices and data recipes. The SFT ablation studies are conducted on a simplified SFT data mixture, so the results in this section are not directly comparable to the main results in section 6.2.\nAny-Resolution Vision Token Sampling. Our any-resolution strategy differs from previous work [46] in that every group of image embeddings (of the same image patch) is downsampled with a perceiver resampler, which ensures that the number of vision tokens input to the LLM remains relatively small. In this section, we ablate the effectiveness of our any-resolution strategy by comparing it with a \u201cfixed-resolution\" baseline and other downsampling designs.\nThe \"fixed-resolution\" baseline resizes all images to the default input size of the vision encoder while keeping the original aspect ratios. We also tried another downsampling strategy with the perceiver resampler: Instead of doing downsampling for each patch independently, we consider a \"fixed sampling\" (denoted as anyres-fixed-sampling in Figure 8a). In the fixed sampling, we concatenate the image embeddings from all image patches and then input them as a single sequence to the perceiver resampler to obtain the fixed number of vision tokens for the whole image.\nInstruction-Aware Vision Token Sampling. InstructBLIP [7] proposes an instruction-aware Q-Former [1] for vision token sampling and shows that it can improve the model performance on some benchmarks. With the perceiver resampler as the VL connector, we can adopt a similar modification to make this process instruction-aware. To make our perceiver resampler \u201cinstruction-aware\", we append the text instructions tokens to the query tokens of the perceiver resampler. Unlike Q-Former, there are only cross-attention layers inside the perceiver resampler, so the instruction (text tokens) would interact with both query tokens and image embeddings via cross-attention.\nFrom the comparison in Figure 8b, we do not observe a significant difference between our model and its instruction-aware version on various benchmarks. It could be that our modification to the perceiver resampler can not be identical to the instruction-aware modification made to the Q-Former in Dai et al. [7], and thus the effectiveness differs. Because of the little difference we observe in this ablation study, we keep the original perceiver resampler architecture in our model for simplicity. We leave the further exploration of the \"instruction-aware\" VL connector to future works.\nQuality of the Text-only Instruction Data. It is a common strategy to train or fine-tune a multi-modal LLM on both multi-modal and pure text data [8, 11]. It is mainly for maintaining the language ability of the fine-tuned model. In this experiment, we study how this pure text subset would affect the performance on multi-modal benchmarks. For the instruction tuning stage, we compare whether the diversity of the pure text data would affect the multi-modal performance. For example, how pure-text math data affects a model's performance on multimodal math benchmarks. In our main experiments, the default collection of pure text in- struction data covers diverse domains including conversation [67], math [68, 87], and code [88]. For this abla- tion study, we substitute these datasets with the same amount of samples that only cover general conversation. In Table 8, we observe that adding math and coding data, although in pure-text format, can help improve a model on relevant benchmarks like MathVista [80], while has less effects on general VQA benchmarks."}, {"title": "8 Conclusion", "content": "We introduce xGen-MM (BLIP-3), a comprehensive framework for training a series of open-source large multimodal models on a curated mixture of large-scale datasets. xGen-MM (BLIP-3) demonstrates emergent abilities such as multimodal in-context learning and achieves impressive results on multimodal benchmarks. By open-sourcing xGen-MM (BLIP-3), our curated datasets, and our SFT fine-tuning codebase, we hope to empower the research community with accessible multimodal foundation models and datasets, allowing practitioners to explore further and advance the potential and emergent abilities of LMMs."}, {"title": "9 Broader Impact", "content": "The xGen-MM (BLIP-3) framework and its suite of Large Multimodal Models (LMMs) have the potential to significantly advance multimodal AI research by providing accessible, open-source resources for the broader community. By facilitating the development and fine-tuning of state-of-the-art LMMs, xGen-MM (BLIP-3) empowers researchers and practitioners across various domains to innovate and apply these models to diverse real-world challenges. Moreover, the integration of safety-tuning protocols within the xGen-MM (BLIP-3) framework helps mitigate ethical risks such as bias and misinformation, promoting the responsible deployment of AI technologies."}]}