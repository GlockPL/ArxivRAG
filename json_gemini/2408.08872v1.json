{"title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models", "authors": ["Le Xue", "Manli Shu", "Anas Awadalla", "Jun Wang", "An Yan", "Senthil Purushwalkam", "Honglu Zhou", "Viraj Prabhu", "Yutong Dai", "Michael S Ryoo", "Shrikant Kendre", "Jieyu Zhang", "Can Qin", "Shu Zhang", "Chia-Chih Chen", "Ning Yu", "Juntao Tan", "Tulika Manoj Awalgaonkar", "Shelby Heinecke", "Huan Wang", "Yejin Choi", "Ludwig Schmidt", "Zeyuan Chen", "Silvio Savarese", "Juan Carlos Niebles", "Caiming Xiong", "Ran Xul"], "abstract": "This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.", "sections": [{"title": "1 Introduction", "content": "Large Multimodal Models (LMMs) have attracted significant attention with their potential applications and emergent capabilities. Recent advancements in both proprietary models [2-5] and open-source LMMs [6, 1, 7\u201311] highlight the rapid progress and growing interest in this field. However, despite these advancements,\nthere is still a gap between open-source models and proprietary ones in terms of access to open weights,\ntraining recipes, and curated datasets. Such limitations hinder the open-source communities from replicating,\nunderstanding, and improving LMMs.\nRecent works have demonstrated that large-scale and high-quality data are essential for training robust\nLMMs [8-12]. BLIP-2 [1] was one of the pioneering efforts in exploring LMMs, which leveraged synthetic\ndata to achieve impressive results at the time (Figure 1 (a)). However, the data used in BLIP-2 lacks the\nscale, quality, and diversity required to reach competitive performance compared to more modern LMMs\nnowadays. In addition, BLIP-2 employs an intricate Q-Former [1] architecture to bridge the vision and\nlanguage modalities, coupled with a suite of complex training objectives (ITM, ITC, and ITG losses), both of\nwhich pose obstacles for larger-scale training. Moreover, BLIP-2 supports only single-image input, whereas\ninterleaved multimodal data formats are the most natural form of multimodal data [13].\nIn response to these challenges, we introduce xGen-MM (BLIP-3) (Figure 1 (b)), a new framework\ndesigned to scale up LMM training by utilizing an ensemble of multimodal interleaved datasets, curated\ncaption datasets, and other publicly available datasets [14\u201317]. xGen-MM, short for xGen-MultiModal, fur-\nther expands our previous generative AI initiatives and corresponding foundation models for text xGen [18],\ncode generation codeGen [19, 20], function calling APIGen [21], among others. In xGen-MM (BLIP-3),\nas illustrated in Figure 2, we streamline the model architecture by replacing the Q-Former [1] with a more\nscalable vision token sampler (specifically, a perceiver resampler [22]) and simplifying the training objectives\nto focus solely on the auto-regressive loss of text tokens in a multimodal context. Our primary focus is\non dataset curation and scaling up the training data. Recently, our BLIP-3 team introduced two large-\nscale, high-quality datasets: MINT-1T [12], a trillion-token scale interleaved dataset; and BLIP3-KALE,\na knowledge-augmented high-quality dense captions dataset. In this technical report, we introduce two\nadditional specialized datasets: BLIP3-OCR-200M, a large-scale dataset with dense OCR annotations; and\nBLIP3-GROUNDING-50M, a large-scale visual grounding dataset.\nIn addition to these datasets, we are committed to open-sourcing the series of models developed in\nthis work, including the base, instruction-tuned, and DPO models. Along with the model release, we also\nprovide code for easy fine-tuning of our base model on custom datasets. By making these resources publicly\navailable, we aim to make LMM research and development more accessible to the community, and we\nencourage researchers and practitioners to use our models and datasets to understand and further explore the\npotential and emergent abilities of LMMs."}, {"title": "2 Related Work", "content": "Recent advancements in Large Multimodal Models (LMMs) have explored two main architectural approaches:\nthe cross-attention style [22, 23] and the decoder-only style. The cross-attention approach, exemplified\nby models like Flamingo [22, 23] and Llama 3.1 [5], integrates vision and language modalities through a\ncomplex attention mechanism to enable deep multimodal understanding. Another approach is the decoder-\nonly architecture [1, 7, 8, 24-36], which we adopt in xGen-MM (BLIP-3), offers a more streamlined\nsolution. This approach connects pre-trained language models to visual inputs using lightweight connectors,\nsimplifying the integration process while maintaining robust multimodal capabilities. The effectiveness\nof this architecture is evident in models such as MM1 [9], VILA [10], LLaVA [8], phi3-vision [37], and\nOtter [38].\nTraining methodologies for LMMs typically follow one of the two strategies. The first one uses a\nlight pre-training procedure and heavily relies on visual instruction tuning, as seen in the LLaVA se-\nries [8, 29]. Extensive research has been conducted on creating effective instruction-tuning data for a variety\nof tasks [32, 39\u201343]. The second strategy involves extensive pre-training on large-scale, diverse datasets,\nfollowed by visual instruction fine-tuning. This approach, employed by models like MM1 and Idefics2 [11],\ninfuses broad knowledge into the model, which is then fine-tuned to align with human-like interactions and\nsafety standards. While MM1 [9] provides extensive ablations and studies on the recipes aimed at improving\nLMMs, it releases limited resources for practitioners to adopt the model (e.g., MM1 models and datasets"}, {"title": "3 Model Architecture", "content": "Architecture Overview. As illustrated in Figure 2, the xGen-MM (BLIP-3) framework adopts an architec-\nture consisting of a ViT [44, 45], a vision token sampler (perceiver resampler [22]) to downsample the image\nembeddings, and a pre-trained Large Language Model (phi3-mini [37]). The input to the model can be free-\nform multimodal interleaved texts and vision tokens from the diverse multimodal data sources we ensemble.\nAny-Resolution Vision Token Sampling. As proved effective in recent LMMs [46\u201348], we adopt a\ndynamic high-resolution (i.e., \"any-resolution\") image encoding strategy at the fine-tuning and post-training\nstages. We enable higher-resolution image understanding with image patch-wise encoding. The patch-wise"}, {"title": "4 Training", "content": "Pre-training. The pre-training objective is to predict the next text token across the dataset mixture we\npre-train on. Overall, the resulting base model xGen-MM-Phi3-mini-base-r is pre-trained for about 100\nbillion multimodal tokens from the ensembled dataset, and our pre-training resolution is 384x384 pixels,\nwhich aligns with SigLIP [45].\nSupervised Fine-tuning (SFT). We further fine-tune our pre-trained models on instruction-following\nexamples to make them better understand and follow user queries. At the fine-tuning stage, we use a\ncollection of publically available instruction-following datasets [11, 29, 49]. We adopt the any-resolution\nvision token sampling strategy to allow a better understanding of images of higher resolutions such as\ntext-rich document data. We introduce the technical details for the fine-tuning stage in the following sections.\nInterleaved Multi-Image Supervised Fine-tuning. We conduct a second-stage fine-tuning on the instruc-\ntion fine-tuned model on a mixture of multi-image and single-image instructions-following samples. The\ngoal for this second-stage fine-tuning is to enhance the model's ability to comprehend interleaved image-text\ninput, which is helpful for multimodal in-context learning, multi-image question answering, and many more\npractical use cases. For the multi-image fine-tuning, we also adopt the any-resolution vision token sampling\nstrategy same as in the previous SFT stage.\nPost-training. Finally, we perform two stages of post-training to improve the model's helpfulness while\nmitigating harmful qualities such as hallucination and toxicity. We first perform direct preference optimization\n(DPO [50]) to improve the model's helpfulness and visual faithfulness. We then perform safety fine-tuning\nto improve the model's harmlessness. We quantitatively demonstrate Pareto gains in model harmlessness\nand helpfulness after our post-training."}, {"title": "5 Data", "content": "5.1 Pre-training Data Recipe\nAs indicated in Figure 3, in xGen-MM (BLIP-3), we pre-train on an ensemble of diverse multimodal datasets\nwith the indicated sampling ratios.\nInterleaved Dataset Mixture. We combine MINT-1T (including its HTML, PDF, and ArXiv subsets) with\nOBELICS (HTML only) to create a more diverse and comprehensive dataset mixture that covers a broader\nrange of domains.\n1. MINT-1T [12] is a trillion token scale multimodal interleaved dataset, containing data sources from\nHTML, PDF, and ArXiv. As evidenced by MM1 [9] and Idefics2 [11], such multimodal interleaved"}, {"title": "6 Experiments", "content": "6.1 Pre-training\nFew-shot Evaluation. After the pre-training stage, we evaluate our pre-trained model on classic captioning\nand VQA tasks, in comparison with previous models that support few-shot learning multi-modal evaluation.\nWe present zero-shot and few-shot (4- and 8-shots) results, as shown in Table 1. Overall, our model achieves\ncompetitive multimodal in-context learning performance with comparable-sized LMMs 1. For the OCR tasks\n(TextCaps and TextVQA) and VQA-v2, it significantly outperforms MM1-3B and even larger models such\nas Idefics-9B [13] and MM1-7B [9]. On all benchmarks, increasing the number of shots can improve the\nperformance, demonstrating the model's ability to adapt to in-context distributions.\n6.2 Supervised Fine-tuning\nWe evaluate our model on a comprehensive suite of multi-modal (image-text) benchmarks, assessing the\nmodel's ability from multiple perspectives. Our evaluation covers general VQA benchmarks [74\u201378], visual\nperception [49], domain knowledge [79, 80], OCR ability [57, 81], and hallucination [82, 83]. For models\nfine-tuned on interleaved multi-image datasets, we also evaluate their performance on common multi-image\nbenchmarks [69, 84-86]."}, {"title": "7 Ablation Studies", "content": "7.1 Pre-training Ablation\nScaling Pre-training Data. We perform an ablation study to explore the relation between the amount\nof pre-training data and the pre-train evaluation metrics, by varying the data scale from 2B multimodal\ntokens to 100B multimodal tokens. The data recipe we used here is a mixture of image caption datasets and\nmultimodal interleaved data. As shown in Figure 7, we find that scaling up the number of multimodal tokens\nfrom 2B to 60B leads to substantial gain for image-text (COCO-Caps) and OCR (Text-Caps, TextVQA)\ntasks, and further increasing the data size to 100B has moderate additional benefit in terms of few-shot\nevaluation metrics."}, {"title": "8 Conclusion", "content": "We introduce xGen-MM (BLIP-3), a comprehensive framework for training a series of open-source large\nmultimodal models on a curated mixture of large-scale datasets. xGen-MM (BLIP-3) demonstrates emergent\nabilities such as multimodal in-context learning and achieves impressive results on multimodal benchmarks.\nBy open-sourcing xGen-MM (BLIP-3), our curated datasets, and our SFT fine-tuning codebase, we hope\nto empower the research community with accessible multimodal foundation models and datasets, allowing\npractitioners to explore further and advance the potential and emergent abilities of LMMs."}, {"title": "9 Broader Impact", "content": "The xGen-MM (BLIP-3) framework and its suite of Large Multimodal Models (LMMs) have the potential\nto significantly advance multimodal AI research by providing accessible, open-source resources for the\nbroader community. By facilitating the development and fine-tuning of state-of-the-art LMMs, xGen-MM\n(BLIP-3) empowers researchers and practitioners across various domains to innovate and apply these models\nto diverse real-world challenges. Moreover, the integration of safety-tuning protocols within the xGen-MM\n(BLIP-3) framework helps mitigate ethical risks such as bias and misinformation, promoting the responsible\ndeployment of AI technologies."}, {"title": "10 Acknowledgement", "content": "We would like to thank Srinath Meadusani, Lavanya Karanam, Dhaval Dilip Metrani, and Eric Hu for their\nwork on the scientific computation infrastructure, as well as Jason Lee and John Emmons for their efforts in\ncollecting the large-scale text-only SFT datasets used in one of our pre-training ablation studies."}]}