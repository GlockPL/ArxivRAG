{"title": "SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech Emotion Recognition", "authors": ["Alaa Nfissi", "Wassim Bouachir", "Nizar Bouguila", "Brian Mishara"], "abstract": "In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: https://github.com/alaaNfissi/SigWavNet- Learning-Multiresolution-Signal-Wavelet-Network-for-Speech- Emotion-Recognition.", "sections": [{"title": "I. INTRODUCTION", "content": "SPEECH Emotion Recognition (SER) plays a pivotal role in human-computer interaction, fostering more sophisticated dialogues between machines and humans. Its implications span various domains, notably in emergency call centers, where SER systems evaluate an individual's stress or fear levels, thereby enhancing the accuracy of assessing the caller's condition [1]. This, in turn, facilitates more effective decision-making within call centers. Additionally, in healthcare, SER proves beneficial in identifying psychological disorders, potentially minimizing the risk of suicidal behaviors [2]. Furthermore, the integration of SER into virtual AI chatbots opens avenues for providing personalized therapy through online interactions [3]. Speech represents an intricate and high-frequency signal encompassing information about the conveyed message, speaker characteristics, gender, language, and emotional content. However, challenges persist in discerning emotions from speech signals, primarily due to diverse speaking styles. Physiological studies underscore the significance of capturing the complete emotional trajectory, necessitating sufficiently long speech segments [4]. In the initial stages, tasks centered on high-frequency signals typically involve extracting relevant time-frequency features before integrating them into machine learning (ML) algorithms, which are commonly tailored for fixed or limited input sizes [5]. Managing speech signals in an emotional context poses a challenge of balancing high-frequency sampling with low-dimensional decision states (e.g., emotions). Consequently, many applications demand sophisticated approaches capable of extracting meaningful and concise information from speech signals to facilitate analysis and SER. Numerous effective applications of ML in the context of speech utilize extracted features as inputs\u2014a manually_curated, condensed representation of the original signals. Frequently, these features encompass a spectrogram [6], wavelet coefficient statistics [7], or other variations [8]. Despite their notable success, these frameworks require careful feature extraction, a potentially time-intensive task. Moreover, the extracted features may exhibit sensitivity to unforeseen noise or changing conditions, posing challenges in designing domain-invariant features. The establishment of such features, whether through post-processing or ML methodologies, remains an ongoing research inquiry [9]. In light of these feature extraction challenges, it's worth noting the excellent capabilities of Fourier and wavelet transforms for feature extraction and data dimensionality reduction, despite the intensive labor they require based on the dataset and problem at hand. The wavelet transform, with advantageous properties such as a linear time algorithm, perfect reconstruction, and customizable wavelet functions [10], emerges as a favorable choice for feature representation. However, its adoption in the machine learning community is limited, with methods like the Fourier transform and its variants being more commonly employed [11]. This underutilization of the wavelet transform may stem from the challenge of designing and selecting appropriate wavelet functions, typically derived analytically using Fourier methods. Moreover, the array of available wavelet functions without a comprehensive under-"}, {"title": "II. RELATED WORKS", "content": "Within the domain of SER, researchers have strategically combined a variety of feature extraction methods and classification models to improve recognition efficacy. This includes the incorporation of handcrafted features into traditional ML models and DL models as classifiers. Moreover, a subset of approaches adopts an E2E framework where features are autonomously extracted from the raw waveform signal of speech. This methodological diversity represents a systematic effort in SER research to optimize recognition accuracy by harnessing a spectrum of feature extraction techniques and model architectures. In [19], a signal segmentation methodology was introduced, using the depth first search (DFS) algorithm to determine segment duration and overlap. Local features, including pitch, mel frequency cepstral coefficients (MFCC), line spectral pairs (LSP), intensity, and zero-crossing rate (ZCR), were extracted from each segment, followed by a process of smoothing and normalization. The authors also computed global features using the open-source media interpretation by large feature-space extraction (Open SMILE) toolkit [20]. For the classification task, they employed a linear kernel support vector machine (SVM) with sequential minimal optimization (SMO), demonstrating proficiency in classifying distinct emotions. In a different approach, authors in [21] presented an alternative method by advocating the use of 3-channel log-mel spectrograms as features for training a deep CNN. These channels incorporate static, delta, and delta-delta components, representing the first and second derivatives of the signal. This configuration draws an analogy to an RGB image, where the mel-spectrogram channels play a role similar to the red, green, and blue channels. Similarly, in [22], authors employed a combination of a local feature learning block (LFLB) and a long short-term memory (LSTM) model to extract features from both raw audio and log-mel spectrograms. Numerous studies have embraced the integration of DL models to improve emotion classification. DL, grounded in the exploration of artificial neural networks, strives to amalgamate lower-level features into more abstract, high-level features, unveiling latent patterns within the data and thereby enhancing the classifier's recognition rate compared to the original features [23]. Some studies employ DL approaches like 1D CNNs to directly learn relevant features from raw speech. However, the deployment of DL networks raises concerns about their inherent instability and the substantial amount of data required for effective training [24]. In [25], researchers advocated for a deep-stride CNN customized for classifying emotions. This model transforms 1D audio signals into a 2D spectrogram through the Short-Time Fourier Transform (STFT), involving preprocessing steps such as adaptive threshold-based noise removal from the audio signals. On the other hand, the scattering transform, introduced in works like [26] and [27], functions as a deep convolutional network employing predefined kernels for convolution. This design imparts stability against temporal shifts and deformations in the feature representation of signals. Given the temporal spread of emotion cues in speech, the scattering transform is adept at robustly capturing temporal variations and cues. In [28], researchers compute scattering coefficients in the log-frequency domain, ensuring frequency transposition invariance and fostering speaker-independent representations for speech recognition. The scattering transform's versatility extends to both 1D and 2D data processing. In [29], a joint time-frequency scattering approach is introduced, incorporating multiscale frequency energy distribution into a time-invariant representation. Additionally, [30] combines two-layer scattering coefficients with CNN layers, providing a stable descriptor of speaker information extracted from raw speech. Other methodologies employing wavelet analysis for scrutinizing speech signals have been introduced [31]. Wavelet analysis, grounded in a multi-resolution framework, aims to capture nonlinear interactions similar to vortex flows. This analytical technique finds applications in denoising, detection, compression, classification, and various other domains [32] [33]. A notable application of wavelet analysis is evident in [34], where wavelet-based pH time-frequency vocal source features are extracted alongside MFCC and Teager-Energy-"}, {"title": "III. PROPOSED METHOD", "content": "Despite advancements in SER, persistent challenges such as system complexity, inadequate feature distinctiveness, and vulnerability to noise interference remain unaddressed. These challenges not only compromise the accuracy of emotion recognition but also limit the applicability of SER systems in real-world environments. A primary motivation for our study is the inherent complexity associated with emotional speech processing. Current systems often fail to effectively manage the intricacies of emotional expressions in speech, necessitating the development of more sophisticated and nuanced methodologies. Furthermore, the limited distinctiveness of features in conventional SER approaches presents another significant challenge. Traditional methods, predominantly based on fixed-length frame segmentation, struggle to capture the full spectrum of emotional cues dispersed throughout speech. This inadequacy underscores the importance of developing advanced feature extraction strategies that can accurately identify emotional nuances in continuous voice segments. Additionally, the susceptibility of existing SER frameworks to noise interference highlights a critical area for improvement. In practical scenarios, where speech signals are frequently exposed to varied and noisy conditions, enhancing the noise robustness of SER systems is essential for reliable performance. Informed by insights from physiological and psychological research, which suggest that emotional information in speech is not confined to specific segments but rather distributed across longer durations, our proposed E2E method diverges from conventional practices. It seeks to align more closely with the natural characteristics of emotional expression in speech, potentially improving the model's capability to capture and recognize emotions more effectively. This novel approach aims to pave the way for more accurate, robust, and versatile SERs."}, {"title": "B. Preliminary concepts", "content": "1) Fast discrete wavelet transform (FDWT): In our method, we have centered our focus on a particular linear time-frequency transform, namely the wavelet transform. In SER and signal processing, the Uncertainty Principle reveals inherent limitations in simultaneously capturing the temporal and frequency details of a signal. This principle is mathematically grounded in the assertion that the localization of a function f and its DFT, Df, cannot both be narrowly confined. Formally, this limitation is expressed in eq. (1):\n$|| f ||o. ||Df ||o \u2265 n$ (1)\nwhere ||f||o denotes the sparsity of f, measuring the count of non-zero elements, and ||Df ||o represents the sparsity of its DFT. This equation indicates that a function and its Fourier transform cannot simultaneously be sparse, underscoring a fundamental trade-off between time and frequency localization. Drawing from quantum mechanics, the Heisenberg Uncertainty Principle [46] analogously articulates that the probability distributions of a particle's position and momentum-mutually Fourier pairs-cannot both be sharply localized. This phenomenon constrains our ability to precisely determine both attributes concurrently as formulated in eq. (2):\n$\u0394\u03c7. \u0394\u03c1 > \\frac{\u0127}{2}$ (2)\nSpecifically, we utilize the FDWT, which employs a set of wavelets designed to create an orthonormal family through their scaling and translation by powers of 2, as delineated in [47]. The core wavelet, or the 'mother wavelet' denoted as \u03c8, is defined to have a zero mean and a unit norm. The variations of this wavelet function, through dilation and scaling, take the mathematical form in eq. (4):\n$\u03c8;[n] = \\frac{1}{\\sqrt{2j}} \u03c8(\\frac{n}{2j})$ (4)\nwhere, n, j \u2208 Z. The discrete wavelet transform for a given discrete real signal x is then defined in eq. (5) as:\n$Wx [n, 2^j] = \\sum_{m=0}^{N-1} x[m] \u03c8[m-n]$ (5)\nIn this context, wavelet functions serve as a band-pass filter bank, enabling the decomposition of a signal using this filter bank. As the wavelets are band-pass, we integrate a low-pass scaling function, $, which cumulatively represents all wavelets above a certain scale, j. The scaling function's Fourier transform, $, adheres to the relationship in eq. (6), with its phase being arbitrary [47]:\n$[$(w)$\\2 = \\int_{-\\infty}^{+\\infty} | \u03c8(sw)]2^{-ds}$ (6)\nUtilizing these wavelets and the corresponding scaling function, the FDWT successively decomposes a signal x into a coarser approximation a (low-pass filtered version of x) and its detail coefficients d (high-pass filtered version of x, also known as wavelet coefficients). With an orthonormal basis in L\u00b2(R), such a decomposition is reversible. Importantly, due to the scale factor of 2 between levels and in translating coefficients, the decomposition at any level I can be articulated as a function of the preceding approximation, sub-sampled by a factor of 2, and the base non-dilated wavelet. The FDWT is computable via a fast-decimating algorithm, commonly referred to as the cascade algorithm. This is visually represented in the block diagram of a one-level FDWT cascade algorithm in Fig. 1, where g represents the wavelet and h the scaling function. We define two filters using eq. (7) and eq. (8):\n$h[n] = \\frac{1}{\\sqrt{2}}( \u03c6(\\frac{t}{2}), \u03c6(t-n) )$ (7)\n$g[n] = \\frac{1}{\\sqrt{2}}( \u03c8(\\frac{t}{2}), \u03c6(t-n) )$ (8)\nThese equations establish a connection between the wavelet coefficients and the filters h and g, leading to a recursive algorithm for computing the wavelet transform. This framework forms the foundation of our method in analyzing speech signals for emotion recognition. 2) Conjugate quadrature filter (CQF): Our method leverages a notable feature of the FDWT related to filter identification, grounded in the principles of the CQF bank, as elucidated in [48]. A key aspect of the CQF bank is its quadrature property, which ensures a symmetric response from the decomposition filters relative to the cutoff frequency, thereby imparting an antialiasing characteristic to the system. This is achieved by designing the filters in such a way that the wavelet function g becomes the alternating flip of the scaling function h. This relationship is denoted in eq. (9) as:\n$g[n] = (-1)^n h[-n]$ (9)\nwhere, h[-n] represents the nth coefficient of h in a reversed sequence. The process of wavelet filter bank decomposition is integral to our method. It involves the computation of approximation and detail coefficients, a and d respectively, using the following eqs. (10) and (11):\n$a_{j+1} [p] = \\sum_{n=-\\infty}^{+\\infty} h[n \u2013 2p]a_j [n]$ (10)\n$d_{j+1} [p] = \\sum_{n=-\\infty}^{+\\infty}g[n \u2013 2p]a_j[n]$ (11)\nThe detail coefficients d are essentially the wavelet coefficients as defined earlier. These coefficients are computed recursively at each scale, beginning with ao initialized with the signal x. During each step of the algorithm, the signal is divided into its high- and low-frequency components. This division is achieved by convolving the approximation coefficients with the scaling filter h and the wavelet filter g. The low-frequency component,"}, {"title": "C. Method", "content": "Our proposed architecture, SigWavNet, is an E2E deep learning framework tailored for SER, leveraging multiresolution analysis and advanced neural components to extract and interpret emotional features from speech signals. The foundation of the model, as shown in Fig. 2, is the learnable FDWT layer, which performs a multilevel decomposition of the raw speech waveform using pairs of learnable low-pass (Convn) and high-pass (Convg) filters. These filters are initialized as wavelet coefficients and refined during training for data-driven adaptation. A Learnable Asymmetric Hard Thresholding (LAHT) function is applied to the wavelet coefficients at each level, enabling dynamic denoising and enhancing the sparsity of the signal representation. This process emphasizes low-frequency variations critical for emotional content while isolating high-frequency details that capture transient features. The extracted multilevel features are then processed through dilated 1D dilated convolutional layers with spatial attention to capture local dependencies and prioritize emotionally significant regions. A Bidirectional Gated Recurrent Unit (Bi-GRU) network with temporal attention further refines the sequential patterns, emphasizing key temporal regions contributing to emotion recognition. Finally, a channel weighting mechanism combines multiband features, followed by a Global Average Pooling (GAP) layer and a Log Softmax layer to output emotion probabilities. By integrating data-driven multiresolution analysis, learnable thresholding, and attention mechanisms, SigWavNet effectively captures the complex and hierarchical structure of emotional cues in speech signals."}, {"title": "1) Learnable FDWT", "content": "In our implementation, we introduce a series of convolutional layers, Convn and Convg, that function as learnable scaling (low-pass) and wavelet (high-pass) filters at each decomposition level. These layers facilitate the automatic extraction of meaningful and sparse representations from the raw speech waveform, tailored to the nuances of SER. To enhance the interpretability and effectiveness of the learnable FDWT, we initialize the Convn and Convg filters with a suitable wavelet, which provides an orthogonal wavelet basis designed for time-frequency analysis. This initialization helps to guide the network towards efficient signal decomposition while still allowing the filters to adapt optimally for the SER task during training."}, {"title": "Convolutional Operations and Learning Principle", "content": "In traditional FDWT, fixed filters are used to compute the approximation and detail coefficients at each decomposition level. In our learnable FDWT framework, however, we implement these filters as convolutional layers Convh and Convg, initialized with wavelet filter coefficients but further refined through training. For an input signal x[n], the low-pass (approximation) coefficients aj+1[p] and high-pass (detail) coefficients dj+1[P] at each level j are calculated as follows:\n$a_{j+1} [p] = \\sum_{n=-\\infty}^{+\\infty} Convn (n \u2212 2p) \u00b7 a_j[n]$ (12)\n$d_{j+1} [p] = \\sum_{n=-\\infty}^{+\\infty} Convg(n \u2212 2p) \u00b7 a_j [n]$ (13)\nwhere:\n\u2022 aj[n] represents the approximation (low-pass) coefficient from the previous level.\n\u2022 Convn and Convg are initialized with low-pass and high-pass wavelet coefficients, such as Daubechies wavelets, but remain learnable for data-driven adaptation.\n\u2022 The convolutional layers have a stride of 2, which performs downsampling as in traditional FDWT."}, {"title": "Recursive Decomposition Process", "content": "The FDWT layer mirrors a recursive cascade structure across multiple levels L, each consisting of Convn and Convg filters that adapt to capture SER-relevant features. At each level, aj+1 (low-pass result) and dj+1 (high-pass result) are obtained through the convolution operations, where aj+1 is passed to the next level for further decomposition. This recursive process captures frequency-specific features over time, resulting in a data-driven, multilevel wavelet decomposition optimized for the emotional features within the speech data."}, {"title": "CQF Property", "content": "To maintain orthogonality and ensure a coherent decomposition structure, we incorporate the CQF property between Convn and Convg, simplifying the learning process while preserving wavelet theory principles. By deriving Convg from Convr as shown in eq. (14), the number of parameters is halved, reducing optimization complexity and enhancing learning efficiency as proven in the ablation study section V:\n$Convg[n] = (-1)^n Convn[-n]$ (14)\nThe CQF property ensures orthogonality, time-frequency localization, and perfect signal reconstruction, which are crucial for retaining the subtle variations in speech signals critical to emotional context. Additionally, it acts as a form of regularization, limiting transformations to wavelet-like operations, thus improving generalization and interpretability. This parametrization allows the model to emphasize low-frequency emotional components while isolating high-frequency details, facilitating robust and structured representation learning."}, {"title": "Data-Driven Adaptation to SER", "content": "The wavelet-initialized learnable FDWT filters provide a starting point, capturing basic time-frequency structure. However, during training, the filters adapt further to isolate frequency components particularly relevant for SER. By minimizing our supervised loss function, the model tunes Convn and Convg to emphasize emotional features, such as tone and pitch variations, that are integral to SER. Through this learning process, the FDWT layer achieves a multiband filter bank structure that analyzes the speech signal across different frequency bands in a manner that aligns with the MEL scale. This resemblance arises because, like the MEL scale, our FDWT layer emphasizes lower frequency components crucial for emotional tones, while the high-pass filters isolate finer details within the emotional cues. In particular, our learnable FDWT focuses on decomposing only the low-frequency representation of the previous level at each stage, giving progressively more emphasis to variations in the low-frequency range of the speech signal. This approach reflects the MEL scale's emphasis on perceptually important lower frequencies, where emotional nuances often reside, while still retaining high-frequency components as details at each level. Each level of the FDWT layer thus serves as a learnable, data-adapted filter bank that provides a nuanced analysis across emotional frequency bands, capturing both the broad tonal structure in the low frequencies and the finer, higher-frequency details essential for robust SER."}, {"title": "Comparison with Classical Convolution Layers", "content": "Learnable FDWT layers and classical convolution layers both use convolution operations but serve distinct purposes. FDWT layers act as a multiband filter bank inspired by wavelet theory, decomposing signals into frequency-specific components across multiple levels, with a focus on low-frequency variations crucial for emotional content. In contrast, classical convolution layers extract features across the entire signal without targeting specific frequency bands. FDWT layers adapt their filters to align with time-frequency localization, enabling detailed, frequency-sensitive analysis of emotional cues, unlike the broader feature extraction of classical convolutions."}, {"title": "Efficient, Lightweight Architecture", "content": "Our design also benefits from the hierarchical nature of wavelet decomposition, enabling efficient representation with fewer parameters, by constructing an L-level cascade network with only 2L convolutional filters (one pair of Convn and Convg per level). Since the limit for L is set by the nearest second logarithm of the smallest training input size, we achieve a lightweight"}, {"title": "2) Learnable asymmetric hard-thresholding (LAHT)", "content": "To learn the correct hard-thresholding operation, the coefficients obtained are then processed by a specially designed LAHT layer/function. This layer is continuous and differentiable, mimicking a wavelet denoising operation, and feeds into the next block for further decomposition. This structure allows for a more nuanced and effective approach to signal denoising, enhancing the overall efficacy of the SER process. In our architecture, the thresholding step is integrated to autonomously learn the optimal thresholding parameters, eliminating the need for it to be treated as a separate process. We introduce the novel LAHT activation function, crafted as a blend of two contrasting sigmoid functions. This function is distinguished by its asymmetry in both sharpness and bias parameters. Mathematically, the sigmoid function S(x) is defined in eq. (15) as:\n$\u2200x \u2208 R, S(x) = \\frac{1}{1+ e^{-x}}$ (15)\nOur LAHT function is then expressed in eq. (16) as:\n$LAHT(x) = x\u00b7[S (a. (x + bias^-)) + S (\u03b2 \u00b7 (x \u2013 bias^+))]$ (16)\nIn this equation, a and \u1e9e are the sharpness factors for negative and positive values, respectively, with the condition \u03b1\u00b7 \u03b2 < 0. The terms bias+ and bias\u2212, both greater than 0, represent learnable biases that act as asymmetric thresholds on either side of the origin, as depicted in Fig. 4. To emulate the original FDWT without denoising, we can set both bias+ and bias\u2212 to zero, while allowing \u03b1 and \u03b2 to be real numbers, thus enforcing a linear activation. This module's design features two principal distinctive elements: firstly, all convolution kernels, and secondly, both positive and negative hard thresholds, and sharpness factors are independent and asymmetrically learnable. Furthermore, our proposed sparsely connected deep neural network is capable of approximating representation systems that extend beyond the traditional wavelet representation system, offering greater generality and flexibility. Each of the final low-pass and high-pass representations from all L decomposition levels is then forwarded to the subsequent module, ensuring a seamless and efficient flow of information within our DL framework. This integration of asymmetric learnability and efficient multi-resolution signal representation is pivotal to enhancing the capability and adaptability of our system in SER."}, {"title": "3) 1D dilated CNN with spatial attention and Bi-GRU with temporal attention", "content": "The proposed architecture incorporates a 1D dilated CNN with a spatial attention mechanism and a Bi-GRU network with temporal attention to enhance feature extraction and sequence modeling for each frequency band, as shown in Fig. 5. The 1D dilated CNN efficiently captures local dependencies over varying time scales, while the spatial attention mechanism dynamically emphasizes the most relevant features across the input sequence. The Bi-GRU further models the sequential patterns by processing both forward and backward temporal dependencies, and the temporal attention layer identifies critical regions within the sequence that contribute most to emotion recognition. These components play a crucial role in complementing the proposed learnable FDWT layer. A detailed explanation of their design and implementation is provided in the Appendix."}, {"title": "4) Channel weighting and global average pooling", "content": "As we can see in the general architecture of SigWavNet illustrated in Fig. 2, the obtained attention vectors are subsequently concatenated channel-wise, to construct a composite representation with dimensions (batch_size, levels + 1, length), where \"levels + 1\" encompasses obtained frequency bands from high to low, and \"length\" denotes the dimensionality of each attention vector. Within this composite representation, each \"channel\" corresponds to an attention vector that stems from a specific frequency band of the original signal. This configuration coalesces the emotionally significant information harnessed from the obtained frequency spectrum of the speech signal, furnishing a rich, multifaceted feature set for the model's ensuing stages. Following the concatenation of attention vectors into a composite representation with dimensions (batch_size, levels + 1,length), our architecture incorporates a Channel Weighting layer, a novel component designed to further refine the aggregated feature set. This layer introduces a mechanism to dynamically adjust the importance of each channel (or frequency band) in the composite representation, allowing the model to emphasize or de-emphasize specific frequency components based on their relevance to emotion recognition as formulated in eq. (17):\n$X' = X W$ (17)\nwhere X \u2208 Rbatch_size\u00d7(levels+1)\u00d7length_denotes the input composite representation, W\u2208 Rlevels+1 symbolizes the learnable weights associated with each channel, and \u2299 signifies element-wise multiplication extended across the batch and length dimensions. The weights are optimized during the training process to learn the significance of each frequency band's contribution toward accurate emotion detection. In this setup, 'Channel Weighting' defines a learnable parameter 'weights' for each channel, initially set to one, indicating equal importance across all channels. As the model undergoes training, these weights are adjusted to optimally scale the contribution of each channel based on how informative it is for the task at hand. This strategic weighting allows the model to leverage frequency-specific insights more effectively, enhancing its ability to discern and classify emotional states from speech signals with greater precision. Subsequently, another single 1D dilated convolutional layer is integrated, equipped with Instance Normalization (IN) and the Leaky ReLU activation function. This layer is crucial for its output channels, which match the number of classes (emotions) in our dataset. It serves to extract features from attention vectors' composite representation and generate feature maps corresponding to each emotion class. This design compels the network to learn distinct and accurate representations for each class in individual channels. After the convolutional processing, the output is then directed to a Log Softmax layer for classification purposes. The Log Softmax layer, defined by eq. (18), offers significant benefits over the traditional Softmax function:\n$log Softmax (xi) = log \\frac{exp (xi)}{\\sum_{j} exp (xj)}$ (18)\nThe superiority of Log Softmax lies in its enhanced numerical stability and computational efficiency. This function effectively addresses numerical instabilities that can arise from exponentiating large or small input values, which is a common challenge in DL applications. Furthermore, Log Softmax simplifies the gradient calculations during backpropagation. This simplification not only reduces computational complexity but also accelerates the learning process, making it a more efficient choice for training deep neural networks. To seamlessly integrate the final 1D dilated CNN layer and the Log Softmax layer, our architecture employs a single Global Average Pooling (GAP) layer, as advocated in [49]. Unlike fully connected layers, the GAP layer averages the activations across the convolution output of the attention vectors. This process effectively condenses each feature map into a single scalar value, thereby reducing the dimensionality of the output while preserving essential information. The mechanism of the GAP layer is illustrated in Fig. 6. The incorporation of GAP not only streamlines the network architecture by avoiding the complexity of fully connected layers but also contributes to reducing overfitting by minimizing the number of trainable parameters. This layer is particularly effective in summarizing the extracted features, ensuring that the subsequent Log Softmax layer receives a concise yet informative representation of the input for robust emotion classification."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Datasets play an integral role in the development and effectiveness of SER systems. Generally, emotional speech databases are categorized into three types: actor-based, induced (also known as elicited or semi-natural), and natural emotional databases. The authenticity of these datasets is crucial for the model's ability to accurately recognize real-life emotions. Although natural datasets are most reflective of genuine emotional expressions, they often face copyright and security issues [5]. These databases vary in terms of language, type and purpose, range of emotions, and demographics of speakers, including the number of speakers, samples, and utterances. A significant portion, about 66%, of research-oriented databases use actor-based recordings. This trend is likely due to the controlled and diverse emotional representation these environments provide. In terms of language distribution, English is the most prevalent language in these databases, followed by Chinese and German. Most databases typically focus on fundamental emotions such as neutral, sad, happy, and angry, which are easier to simulate and recognize compared to more complex emotional states [50]."}, {"title": "1) IEMOCAP", "content": "The Interactive Emotional Dyadic Motion Capture IEMOCAP dataset [51] offers a rich multi-modal and multi-speaker collection with 12 hours of audiovisual data, including video, voice, facial motion, and text transcriptions. It features dyadic sessions of improvised and scripted interactions designed to evoke various emotions, structured across five sessions with pairs of male and female speakers, totaling 10039 utterances each, averaging 4.5 seconds, at a 16 KHz sampling rate. Emotions in IEMOCAP are categorized by multiple annotators into basic emotions like anger, happiness, sadness, and neutrality, and dimensionally represented by valence, activation, and dominance scales. Notably, the dataset has class imbalances, with emotions like 'disgust', 'fear', and 'surprise' being less represented. Following prior research [52] [53], we exclude these classes and combine 'happy' and 'excited' into a single category to enhance dataset balance and research alignment."}, {"title": "2) EMO-DB", "content": "The Berlin Emotional Speech Database (EMO-DB) [54] is a pivotal SER resource, featuring 535 recordings in German by 10 professional actors (5 male and 5 female), simulating seven emotions: 'anger', 'boredom', 'disgust', 'anxiety/fear', 'happiness', 'sadness', and 'neutral'. Actors read 10 short, emotionally neutral texts in different emotional states to ensure recording consistency and avoid emotional bias. Each recording lasts approximately 5 seconds and was made in a professional, soundproof studio setting, ensuring high audio quality for analysis. Widely used in emotion recognition research, EMO-DB is freely available for non-commercial use, offering a valuable tool for SER technology development."}, {"title": "B. Experimental setup", "content": "To guarantee uniformity and ensure that our datasets are compatible, we first standardize all audio signals to a consistent format with a 16 KHz sampling rate and transform them into mono-channel. Following this standardization, we partition each dataset into two main subsets: 90% allocated for training and validation, and the remaining 10% reserved exclusively for testing as unseen data. To further refine the training and validation process, we adopted a 10-fold cross-validation approach. The segmentation of the data into these subsets, as well as the distribution within the cross-validation folds, was conducted using stratified random sampling [55]. This technique divides the dataset into distinct, uniform groups\u2014or strata\u2014based on emotion classes, ensuring that each group is proportionately represented. This stratified approach contrasts with basic random sampling by ensuring that the selection of samples from each category is not merely random but proportionally representative, facilitating a division of the dataset that is both balanced and reflective. To determine the best hyperparameters for our model, we employ a grid search technique. Hyperparameter optimization can be conducted through various approaches, one of which involves scheduling algorithms [56]. These algorithms can manage trials by terminating problematic ones early, pausing, cloning, or adjusting the hyperparameters of ongoing trials. We chose the Asynchronous Successive Halving Algorithm (ASHA) due to its high efficiency and performance [57]. In our pursuit to address the challenges presented by class imbalance and the propensity for model overfitting, we have adopted a loss function known as focal loss [58], enhanced with L2-regularization. The focal loss function, originally designed to prioritize learning from hard-to-classify examples, is particularly effective in scenarios where the disparity between class distributions could otherwise skew the learning process. Its mathematical formulation is given by eq. (19):\n$FL(pt) = -at(1 \u2013 pt)^\u03b3 log(pt)$ (19)\nwhere pt is the model's estimated probability for a class, at is a balancing factor, and \u03b3 is the focusing parameter that adjusts the rate at which easy examples contribute to the loss, thereby steering the model's attention towards more challenging instances. To further refine our model's training dynamics and ensure its robustness against overfitting, we integrate L2-regularization (Ridge) into our loss function. This regularization technique imposes a penalty on the magnitude of the parameters, encouraging the model towards simpler, more generalizable patterns. The combined loss function, incorporating both focal loss and L2-regularization, is expressed in eq. (20):\n$Jregularized(w) = FL(pt) + \u03bb \\sum_{i=1}^{n}w_i^2$ (20)\nwhere \u03bb signifies the regularization strength, influencing the degree to which the model parameters are constrained. By employing the Adam optimizer [59] to minimize this composite loss function over the training epochs, we ensure a balanced and effective optimization strategy. This amalgamation not only addresses the inherent issues posed by imbalanced datasets but also fortifies the model's capacity to generalize across a spectrum of emotional states in speech, thereby enhancing its predictive accuracy and reliability in real-world applications."}, {"title": "C. Metrics", "content": "In evaluating the performance of our model", "metrics": "accuracy and F1-score. These metrics are essential for understanding how effectively our model predicts emotions in speech. The accuracy metric is defined as the proportion of correct predictions made by the model out of the total number of predictions; it is formulated in eq. (21):\n$Accuracy = \\frac{Number of Correct Predictions"}, {"22)": "n$Fl-score = 2* \\frac{1"}, {"24)": "n$Precision = \\frac{True Positives}{True Positives + False Positives}$ (23)\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$ (24)\nPrecision measures"}]}