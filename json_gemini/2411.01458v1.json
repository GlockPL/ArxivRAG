{"title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled Al-Generated Content Services", "authors": ["Zhang Liu", "Hongyang Du", "Xiangwang Hou", "Lianfen Huang", "Seyyedali Hosseinalipour", "Dusit Niyato", "Khaled Ben Letaief"], "abstract": "Generative AI (GenAI) has emerged as a transformative technology, enabling customized and personalized AI-generated content (AIGC) services. In this paper, we address challenges of edge-enabled AIGC service provisioning, which remain underexplored in the literature. These services require executing GenAI models with billions of parameters, posing significant obstacles to resource-limited wireless edge. We subsequently introduce the formulation of joint model caching and resource allocation for AIGC services to balance a trade-off between AIGC quality and latency metrics. We obtain mathematical relationships of these metrics with the computational resources required by GenAI models via experimentation. Afterward, we decompose the formulation into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we leverage a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm to solve the latter. The proposed D3PG algorithm makes an innovative use of diffusion models as the actor network to determine optimal resource allocation decisions. Consequently, we integrate these two learning methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm, the performance of which is studied through comparative numerical simulations.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent breakthroughs in artificial intelligence (AI) have propelled generative AI (GenAI) into the spotlight, drawing significant attention for its unprecedented ability to automate the creation of a diverse array of AI-generated content (AIGC), including text, audio, and graphics/images [1], [2]. GenAI aims to produce synthetic data that closely resemble real-world data by learning underlying patterns and characteristics from existing datasets. For example, ChatGPT [3] generates human-like text based on a given context prompt, while RePaint [4] enables the generation of diverse images from textual descriptions. Building on these advancements, AIGC services have been integrated into various domains, including art, advertising, and education [5], offering productivity gains and economic growth.\nDespite their tremendous potentials, delivering AIGC services relies on the inference process of GenAI models, where the increasing size and complexity of these models present significant challenges for deployment over the wireless edge. For instance, ChatGPT, which is built upon GPT-3 with 175 billion parameters, requires 8\u00d748GB A6000 GPUs to perform inference and generate contextually relevant responses to user prompts [6]. This can be particularly challenging for the next-generation Internet paradigms, such as Metaverse [7], which continuously demand high-quality AIGC on personal computers or head-mounted displays. Specifically, the intensive storage and computation demands of GenAI models can limit the accessibility and affordability of provisioning AIGC services.\nTo address this, cloud data centers with abundant computing and storage capacity can be utilized to train and deploy GenAI models, enabling users to access cloud-based AIGC services through the core network. However, migrating AIGC services to cloud data centers can impose prohibitive traffic congestion on backhaul links [8], [9] and introduce privacy threats within public cloud infrastructure [10]. Alternatively, by leveraging wireless edge networks, we can deploy GenAI models closer to users on edge servers co-located with base stations (BSs) [11] to provide lower latency and privacy-aware AIGC services. In this user-edge-cloud continuum, which we aim to investigate, GenAI models are trained and pre-stored in the could, while edge servers are responsible for caching these models and delivering customizable AIGC services to users."}, {"title": "1.2 Motivation and Main Challenges", "content": "While edge servers offer several advantages for delivering AIGC services to users, there are still important issues that need to be addressed. First, storage-limited edge servers cannot cache all GenAl models simultaneously, leading to performance degradation for users with diverse AIGC service requests. This degradation occurs because GenAI models rely on massive datasets to learn underlying patterns and generate meaning-ful outputs, making their performance heavily dependent on the diversity of the training data [12]. To illustrate this, we conducted a study using a corrupted image restoration AIGC service as an example. In this study, the GenAI model RePaint, trained separately on the CelebA-HQ dataset (containing images of celebrities' faces) and the Places2 dataset (containing images of various scenes) [4], was used to repair a corrupted image. The experiments were conducted on a system equipped with an NVIDIA RTX A5000 GPU. The repair processes shown in Fig. 1 unveil that RePaint trained on the human face dataset performed significantly better than the model trained on landscape images. Therefore, due to the diversity of users' AIGC service requests, determining which GenAI models to cache at storage-limited edge servers is crucial.\nSecond, efficiently managing multi-dimensional network resources to deliver low-latency, high-quality AIGC services from edge servers to users is challenging. Since users may not only send their AIGC requests to edge servers but also retrieve the generated content, both the upload and down-load processes demand substantial bandwidth. Moreover, GenAI models require significant computational resources to generate content, which must be managed carefully due to the limited resources of edge servers. For instance, the quality of synthetic images produced by diffusion-based GenAI models improves with the number of denoising steps (detailed in Sec. 3.4). However, the generation latency also increases with the number of denoising steps, necessitating a careful balance of the trade-off between high-quality and low-latency AIGC services, especially in networks with large numbers of users.\nThird, conventional optimization methods for model caching and resource allocation often suffer from high computational com-plexity, rendering them unsuitable for mobile edge networks, which are subject to temporal variations. In practical scenarios, user locations and wireless channel conditions vary over time, and the popularity of AIGC services can fluctuate due to dynamic trends. As a result, a solution that is optimal at one point in time may not remain optimal over a longer period. However, existing model caching and resource allocation methods, such as iteration-based algorithms [13], [14] and constraint relaxation-based problem transformations [15], [16], either require extensive iterations to converge to a satisfactory solution or suffer from significant performance degradation when the environment undergoes abrupt changes over time."}, {"title": "1.3 Summary of Contributions", "content": "To our knowledge, this is the first study that optimizes the edge-enabled provisioning of AIGC services by coordinating GenAI model caching and resource allocation decisions in mobile edge networks. Our main contributions are as follows:\n\u2022 We formulate the model caching and resource allocation problem in GenAI-enabled wireless networks, which is found to be mixed integer nonlinear programming (MINLP) known to be NP-hard. This makes solving the problem challenging, especially under user mobility, imperfect knowledge of wireless channels, and varying AIGC requests.\n\u2022 To tackle the problem, we first divide it into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we employ a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algo-rithm to address the latter. We integrate these two methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm.\n\u2022 We conduct experiments using practical GenAI models to develop unified mathematical models that describe the relationships between AIGC quality, service provision-ing delay and computational resources. Furthermore, in D3PG algorithm we make an innovative use of diffusion models - originally designed for image generation \u2013 to determine optimal resource allocation decisions for AIGC provisioning.\n\u2022 We validate the effectiveness of our method through experiments under various simulation settings, demon-strating that T2DRL not only achieves a higher model hitting ratio but also delivers higher-quality, lower-latency AIGC services compared to the benchmark solutions. This improvement is largely attributed to the generative capabilities of the diffusion model, which enhance ac-tion sampling efficiency by progressively reducing noise through the reverse process."}, {"title": "2 RELATED WORKS", "content": "Henceforth, we summarize the contributions of related works and highlight the aspects they have not addressed, which serve as the primary motivations for this work."}, {"title": "2.1 Content Caching in Wireless Networks", "content": "With the rapidly growing demand for diverse multimedia content on platforms like YouTube, Netflix, and Facebook, caching content at the network edge [13]\u2013[15], [17] has emerged as a solution to reduce content provisioning delays and alleviate traffic loads on core networks by avoiding re-transmissions of highly popular contents. Researchers in [13] investigated a joint content caching and task offload-ing problem, aiming to minimize task completion delays and energy consumption. Researchers in [14] explored the revenue maximization for recommendation-aware, content-oriented wireless caching networks with repair considera-tions. Researchers in [15] introduced a caching model for delivering dynamic content to users over the wireless edge, while capturing content freshness through the age-of-version metric. Researchers in [17] focused on caching popular software for mobile edge computing (MEC).\nHowever, unlike traditional content caching, which primarily focuses on storage capacity constraints, GenAI model caching necessitates the careful orchestration of both computing (e.g., denoising steps for creating images) and storage constraints. More broadly, the unique challenges of edge-enabled AIGC services are yet to be fully explored. Subsequently, the need to evaluate the performance of various GenAI models and develop mathematical models to analyze the performance of diverse AIGC services is a primary motivation behind this work."}, {"title": "2.2 Usage of Deep Reinforcement Learning in Optimization", "content": "Recently, learning-based algorithms, with deep reinforce-ment learning (DRL) as a notable example [18]\u2013[20], have been extensively applied to enhance decision-making and solution design for complex optimization problems. Unlike conventional optimization methods, DRL employs deep neural networks (DNNs) to learn the relationship between a problem's state space (e.g., AICG service popularity) and its action space (e.g., model caching decisions). Researchers in [18] utilized a DDQN algorithm to solve the joint service caching, resource allocation, and computation offloading problem in a cooperative MEC system. Researchers in [19] proposed a deep deterministic policy gradient (DDPG) method to jointly optimize service caching, collaborative offloading, and resource allocation in multi-access edge computing. Researchers in [20] introduced a DRL algorithm based on the integrated of DDQN and dueling DQN to optimize edge caching and radio resource allocation.\nIn this work, we take the fist step towards tailoring DRL techniques for the provisioning of AIGC services at the network edge. However, the uncertainties introduced by user movement, wireless channel fluctuations, and changing AIGC service requests make the DRL state space complex and highly dynamic. This, in turn, renders the use of common multi-layer perceptron (a type of fully connected DNN) in DRL architectures ineffective due to challenges with exploration-exploitation trade-offs and the potential for convergence to suboptimal policies [21]. This emphasizes the need to explore novel learning-based techniques, which we address by introducing diffusion models into the DRL architecture."}, {"title": "3 SYSTEM MODEL", "content": "We consider a three-tier GenAI-enabled mobile network, as shown in Fig. 2, consisting of a cloud data center, a base station (BS) co-located with an edge server, and U users denoted by the set $\\mathcal{U} = \\{1, ...,U\\}$. We denote $\\mathcal{M} = \\{1, ..., M\\}$ as the set of types of GenAI models, each trained on a unique dataset. The cloud data center, with its high computing power and abundant storage capacity, can be used to train the GenAI models. The edge server, with its limited resources, is responsible for caching a subset of GenAI models and delivering high-quality, low-latency AIGC services to the users in its close proximity.\nMotivated by the fact that changes in the popularity of AIGC occur more slowly (typically over hours) compared to changes in users' wireless channels (usually within minutes), we adopt a discrete time representation of the system, further divided into two-timescales: a long-timescale (referred to as a time frame) for GenAI model caching placement, and a short-timescale (referred to as a time slot) for resource allocation. The time frame index is denoted by $t \\in \\mathcal{T} = \\{1, ..., T\\}$, with each time frame containing $K$ time slots. The time slot index is denoted by $k \\in \\mathcal{K} = \\{1, ..., K\\}$, each with duration of $T$ (in seconds). At each time slot $k$, each user $u$ randomly generates an AIGC service request, characterized by a two-tuple $[\\,Y_{u,t}(k), d_{iu,t}(k)\\,]$ , where $Y_{u,t}(k)$ denotes the type of AIGC service corresponding to a specific GenAI model in $\\mathcal{M}$, and $d_{iu,t}(k)$ denotes the input data size (in bits). It is assumed that users do not have sufficient storage to cache GenAI models [24] and must either migrate the AIGC service request to the BS (if the corresponding GenAI model is available) or offload to the cloud data center, which stores all GenAI models. For ease of reference, key notations used in the article are summarized in Table 1."}, {"title": "3.2 Model Caching", "content": "To formalize the GenAI models considered in this work, we begin by introducing their characteristics. In particular, GenAI models are DNNs with well-trained parameters that can automate the creation of various types of content through the inference process. We focus on diffusion-based GenAI models, such as RePaint, which are trained on different datasets to represent distinct GenAI models. For instance, RePaint trained on a dataset of celebrities' faces can be used to repair corrupted human images, while RePaint trained on a landscape scene dataset can generate scenery pictures. As a result, an edge server can execute an AIGC service request only when one of its cached GenAI models is trained on the relevant dataset.\nAt the beginning of each time frame $t$, the BS updates its model caching decisions $\\Theta(t) = \\{ \\theta_{1}(t),..., \\theta_{m} (t), . . ., \\theta_{M} (t)\\}$ and maintains this cache con-figuration throughout $K$ time slots. Specifically, $\\theta_{m}(t) = 1$ indicates that the $m$-th GenAI model is cached at the BS dur-ing the $t$-th time frame; otherwise, $\\theta_{m}(t) = 0$. Additionally, we assume that the probability of user $u \\in \\mathcal{U}$ requesting AIGC service $m \\in \\mathcal{M}$ at time slot $k$ of time frame $t$ follows a Zipf distribution [25], expressed as\n$\\mathbb{P}\\{ \\Upsilon_{u,t}(k) = m \\} = \\frac{m^{-\\gamma(t)}}{\\sum_{i \\in M} i^{-\\gamma(t)}}$ , (1)\nwhere $\\gamma(t)$ denotes the skewness of popularity at time frame $t$. Specifically, considering that users' AIGC requests can fluctuate due to dynamic fashion trends, we model the time-varying popularity $\\gamma(t)$ as a finite-state Markov sequence [26] $\\gamma(t) \\varepsilon \\Gamma = \\{1,...,\\Upsilon\\}$ with a total of $\\Upsilon$ states/configurations. The transition probability of the skewness of popularity across consecutive time frames is represented by $\\mathbb{P}\\{\\gamma(t + 1)|\\gamma(t)\\}$."}, {"title": "3.3 Communication Model", "content": "Since users must both send their AIGC requests to the BS and retrieve the corresponding images, we formulate the service migration and result feedback models below."}, {"title": "3.3.1 Service Migration", "content": "We model the transmission rate from user $u$ to the BS at time slot $k$ of time frame $t$ as follows:\n$R_{unt}^{up}(k) = b_{u,t}(k)W_{u}^{p}log_{2}(1+\\frac{P_{u}h_{u,t}(k)}{N_{o}b_{u,t}(k) W_{u}^{p}})$ , (2)\nwhere $b_{u,t}(k)$ denotes the bandwidth allocation ratio for user $u$, $W_{u}^{p}$ (in Hz) represents the uplink channel bandwidth, $P_{u}$ is the transmit power of user $u$, and $N_{o}$ is the noise power spectral density. Additionally, $h_{u,t}(k) = g_{u,t}(k)|d_{u,t}(k)|^{2}$ [27] represents the channel gain, capturing both path loss and signal fading. Specifically, $d_{u,t}(k) \\sim \\mathcal{CN}(0, 1)$ is the Rayleigh fading component, varying i.i.d. across different time slots, and the path loss $g_{u,t}(k)$ (in dB) is modeled as\n$g_{u,t}(k) = -128.1 \u2013 37.6log_{10}dis_{u,t}(k)$ , (3)\nwhere $dis_{u,t}(k)$ denotes the Euclidean distance between user $u$ and the BS at time slot $k$ of time frame $t$. Users' location distribution at each time slot $k$ of time frame $t$, denoted by $\\Lambda_{t}^{t}(k)$, is assumed to come from a finite set $\\lambda_{t}^{t}(k) \\varepsilon \\Lambda = \\{1, ..., I\\}$, with a total of $I$ states [26]. The transition probability of the users' location distribution between consecutive time slots in each time frame $t$ is represented by $\\mathbb{P}\\{\\lambda_{t}(k + 1)|\\lambda_{t}(k)\\}$.\nConsequently, the uplink transmission delay for user $u$ to migrate its AIGC service request at time slot $k$ of time frame $t$, denoted by $D_{u,t}^{up}(k)$, can be calculated as follows:\n$D_{u,t}^{up}(k) = \\begin{cases}\n d_{iu,t}(k)/R_{unt}^{up}(k), & \\text{if } \\theta_{m}(t) = 1, m = \\Upsilon_{u,t}(k) \\\\\n d_{iu,t}(k)/R_{unt}^{up}(k) + d_{iu,t}(k)/R^{bc}, & \\text{otherwise},  \\\\\n\\end{cases}$ (4)\nwhere $\\theta_{m}(t) = 1,m = \\Upsilon_{u,t}(k)$ indicates that the GenAI model corresponding to the AIGC service requested by user $u$ at time slot $k$ is cached at the BS, allowing the AIGC service to be provided at the edge. Otherwise, the AIGC service will be migrated to the cloud data center, with $R^{bc}$ denoting the fixed wired transmission rate between the BS and the cloud, which determines the additional backhaul transmission delay."}, {"title": "3.3.2 Result Feedback", "content": "After the AIGC is generated, the result feedback delay can be calculated as the AIGC retrieval delay for user $u$. Similar to (4), the transmission rate from the BS to user $u$ at time slot $k$ of time frame $t$ is given by [28]\n$R_{unt}^{dw}(k) = W_{u}^{dw}log_{2}(1+\\frac{P^{BS}h_{u,t}(k)}{N_{o}W_{u}^{dw}})$ , (5)\nwhere $P^{BS}$ is the transmit power of the BS, and $W_{u}^{dw}$ (in Hz) represents the per-user downlink channel bandwidth. Consequently, the result feedback delay for user $u$ when downloading the generated image at time slot $k$ of time frame $t$ can be calculated as follows:\n$D_{u,t}^{dw}(k) = \\begin{cases}\n d_{ou}/R_{unt}^{dw}(k), & \\text{if } \\theta_{m}(t) = 1, m = \\Upsilon_{u,t}(k) \\\\\n d_{ou}/R_{unt}^{dw}(k) + d_{ou}/R^{cb}, & \\text{otherwise},  \\\\\n\\end{cases}$ (6)\nwhere $d_{ou}$ is the output data size (in bits) of $m$-th type AIGC service (e.g., the size of the reconstructed image) dictated by the architecture of its GenAI model, and $R^{cb}$ is the fixed wired transmission rate between the cloud data center and the BS."}, {"title": "3.4 Computing Model", "content": "Before introducing our computing model, we first highlight the major challenges in evaluating the performance of AIGC services. Specifically, unlike existing works [13], [17]\u2013[19], which focus on improving performance indicators such as task execution delay and energy consumption using explicit mathematical models, other evaluation metrics for AIGC services are challenging to quantify without a mathematical expression. To tackle this, we focus on a specific AIGC, images, where achieving optimal resource allocation schemes for high-quality, low-latency AIGC services requires modeling the mathematical functions between computational resources (e.g., denoising steps in diffusion models) and both image generation delay and the quality of the generated image. To this end, we conduct experiments with practical GenAI models and use a fitting method to develop one of the first mathematical functions between GenAI image generation quality, image generation delay, and computational resource consumption."}, {"title": "3.4.1 Image Generation Quality", "content": "Given that image quality is inherently subjective, we focus on an image-based metric to capture the image quality by modeling key physiological and psychovisual features of the human visual system. In particular, we leverage total variation (TV) [29], a metric recognized for its effectiveness in measuring image smoothness, to assess the perceived quality of Al-generated images. Specifically, TV quantifies 'roughness' or 'discontinuity' by summing the absolute differences between adjacent pixels in an image. A lower TV value indicates higher image quality.\nWe conduct experiments to explore the mathematical relationship between the number of denoising steps, which capture the inference process of the diffusion-based GenAI model, and image quality as measured by TV. As illustrated in Fig. 3, we first generated 10 corrupted images of human faces using random masks, which were then inpainted. These corrupted images were processed using RePaint, and we observed that they are gradually recovered as the number of denoising steps increases. Additionally, we present the TV values of these 10 corrupted images at different denoising steps (part A in Fig. 3) and fit a mathematical function using a piecewise function (highlighted in red).\nAs a result, we propose a general model relating the TV value of the generated image perceived by user $u$ at time slot $k$ of time frame $t$, denoted by $B_{unt}^{gt}(k)$, to the allocated computational resource (i.e., denoising steps), given by (7) at the top of this page. Specifically, $\\xi_{u,t}(k) \\in [0, 1]$ represents the denoising step allocation ratio for user $n$ at time slot $k$, and $L$ denotes the total number of denoising steps performed at the edge server. Also, (7) includes four parameters: $A_{1} = 60$, representing the minimum number of denoising steps where image quality begins to improve; $A_{2} = 110$, indicating the lower bound of image quality; $A_{3} = 170$, marking the number of denoising steps when image quality starts to stabilize; $A_{4} = 28$, denoting the highest image quality value. When the GenAI model corresponding to the AIGC service requested by user $u$ at time slot $k$ is not cached at the edge server, i.e., $\\theta_{m}(t) \\neq 1,m = \\Upsilon_{u,t}(k)$, we consider $B_{unt}^{gt}(k) = A_{4}$, as the cloud has sufficient computing resources to generate an image with the highest quality. This modeling technique and methodology has the potential to be applied to a wider range of future problems on AIGC delivery in wireless networks."}, {"title": "3.4.2 Image Generation Delay", "content": "Following the same methodology, we present the generation times of these 10 recovered images at different denoising steps (part B in Fig. 3) and fit a function, which appears to be a linear function (highlighted in red). As a result, we propose a model relating the image generation time for user $u$ at time slot $k$ of time frame $t$, denoted by $D_{unt}^{at}(k)$, to the allocated denoising steps as follows:\n$D_{unt}^{at}(k) = B_{1}u,t(k)L + B_{2}, \\theta_{m}(t) = 1, m = \\Upsilon_{u,t}(k)$ , (8)\nwhich includes two parameters: $B_{1} = 0.18$ and $B_{2} = 5.74$. When the GenAI model corresponding to the AIGC service requested by user $u$ at time slot $k$ is not cached at the edge server, i.e., $\\theta_{m}(t) \\neq 1,m = \\Upsilon_{u,t}(k)$, we consider $D_{unt}^{at}(k) = B_{1}A_{3} + B_{2}$, where the computing resources allocated by the cloud are defined as the minimum threshold required to generate the image at the highest quality (i.e., $A_{3}$). In summary, considering (4), (6), and (8), the AIGC service provisioning delay for user $u$ at time slot $k$ of time frame $t$ is\n$D_{unt}(k) = D_{u,t}^{up}(k) + D_{u,t}^{dw}(k) + D_{unt}^{at}(k)$ . (9)\nConsequently, we form a utility function via the weighted sum of generation delay and the TV value of the generated image for user $u$ at time slot $k$ of time frame $t$ as\n$\\mathcal{G}_{u,t}(k) = aD_{unt}(k) + (1 \u2013 a) B_{unt}^{gt}(k)$ , (10)\ncapturing the trade-off between the quality and latency of AIGC services. Here, $a \\in [0, 1]$ is a preference weight factor."}, {"title": "4 PROBLEM FORMULATION", "content": "We now formulate the problem of two-timescale model caching and resource allocation as a dynamic long-term optimization. Our goal is to minimize the utility in (10) for all users across all time frames and time slots. This involves both long-timescale GenAI modeling caching scheduling and short-timescale allocation of computing and communication resources. Mathematically, we define this problem as P1 below:\nP1: $\\min\\limits_{\\mathcal{P}, b, \\xi} \\frac{1}{TKU} \\sum_{t\\in \\mathcal{T}}\\sum_{k\\in \\mathcal{K}}\\sum_{u\\in \\mathcal{U}} \\mathcal{G}_{u,t}(k)$ , (11)\ns.t.\n$\\theta_{m}(t) \\in \\{0,1\\}, \\forall t \\in \\mathcal{T}, m \\in \\mathcal{M}$, (11a)\n$b_{u,t}(k) \\in [0,1], \\forall t \\in \\mathcal{T}, k \\in \\mathcal{K}, u \\in \\mathcal{U}$, (11b)\n$\\xi_{u,t}(k) \\in [0, 1], \\forall t \\in \\mathcal{T}, k \\in \\mathcal{K}, u \\in \\mathcal{U}$, (11c)\n$\\sum_{m\\in \\mathcal{M}} \\theta_{m}(t)c_{m} \\le C, \\forall t \\in \\mathcal{T}$, (11d)\n$\\sum_{u\\in \\mathcal{U}} b_{u,t}(k) \\le 1, t \\in \\mathcal{T}, k \\in \\mathcal{K}$, (11e)\n$\\sum_{u\\in \\mathcal{U}} \\xi_{u,t}(k) \\le 1, \\forall t \\in \\mathcal{T}, k \\in \\mathcal{K}$, (11f)\n$\\xi_{u,t}(k) \\le \\theta_{m}(t), m = \\Upsilon_{u,t}(k), \\forall t \\in\\mathcal{T},k\\in \\mathcal{K}, u\\in\\mathcal{U}$, (11g)\n$D_{unt}(k) \\le \\tau, \\forall t \\in \\mathcal{T}, k \\in \\mathcal{K}, u \\in \\mathcal{U}$, (11h)\nwhere $\\mathcal{P} = \\{\\theta_{m}(t)\\}_{m\\in\\mathcal{M},t\\in\\mathcal{T}}$ is the caching decision vector for GenAl models, impacting the latency of GenAI content requests and receptions as in (4) and (6), $b = \\{b_{u,t}(k)\\}_{u\\in\\mathcal{U},t\\in\\mathcal{T},k\\varepsilon\\mathcal{K}}$ is the communication resource allocation ratio vector for users, determining the bandwidth allocation as in (5), and $\\xi = {\\xi_{u,t}(k)\\}_{u\\in\\mathcal{U},t\\in\\mathcal{T},k\\varepsilon\\mathcal{K}}$ is the computing resource allocation ratio vector for users, which influence both the quality and latency of AIGC generation, as in (7) and (8), respectively.\nIn P1, constraint (11a) ensures that the model caching de-cision is binary. Constraints (11b) and (11c) define the value ranges for the communication and computation resources allocated by the BS to different users. Constraints (11d)-(11f) specify the limitations on the BS's caching, bandwidth, and computing capacities, respectively. In constraint (11d), $C_{m}$ represents the storage requirement (in GB) for the $m$-th type GenAI model, while $C$ denotes the maximum storage capacity (in GB) of the BS. Constraint (11g) indicates that the BS's computing resources will not be allocated for the $m$-th type AIGC service if the relevant GenAI model is not cached at the BS. Additionally, constraint (11h) ensures that the AIGC service provisioning delay of user $u$ at time slot $k$ of time frame $t$ given by (9) does not exceed the duration of the time slot, thereby preventing potential computation backlogs when AIGC service requests periodically arrive from the users."}, {"title": "4.1 Problem Decomposition across Different Timescales", "content": "Since GenAI model updates typically occur only after the collection of substantial new data [26] while AIGC service requests arrive within seconds from users, in the following, we decompose the original problem P1 to facilitate its solution."}, {"title": "4.1.1 Resource Allocation Subproblem in Short-Timescale", "content": "From the short-timescale perspective (at each time slot $k$), our goal is to minimize $\\mathcal{G}_{u,t}(k)$ for all users across all time slots, given the GenAI model caching decisions made for the relevant time frame $t$, by determining the computing and communication resource allocation. Subsequently, we mathematically formulate the resource allocation subproblem P2 as follows:\nP2: $\\min\\limits_{b, \\xi} \\frac{1}{KU} \\sum_{k\\varepsilon \\mathcal{K}}\\sum_{u\\varepsilon \\mathcal{U}} \\mathcal{G}_{u,t}(k)$ , (12)\ns.t. (11b), (11c), (11e) \u2013 (11h)."}, {"title": "4.1.2 Model Caching Subproblem in Long-Timescale", "content": "From the long-timescale perspective (at each time frame $t$), our goal is to obtain the GenAI model caching decisions that minimize $\\mathcal{G}_{u,t}(k)$ for all users across all time frames. We thus formulate the model caching subproblem P3 as follows:\nP3: $\\min\\limits_{\\mathcal{P}} \\frac{1}{TKU} \\sum_{t\\varepsilon \\mathcal{T}}\\sum_{k\\varepsilon \\mathcal{K}}\\sum_{u\\varepsilon \\mathcal{U}} \\mathcal{G}_{u,t}(k)$ , (13)\ns.t. (11a), (11d)."}, {"title": "5 BASIC IDEA OF DIFFUSION MODELS", "content": "Before delving into T2DRL, we first explain our motivation for integrating the diffusion model with DRL (i.e., diffusion-based deep deterministic policy gradient algorithm). We then detail how the diffusion model is customized to generate decisions for communication and computing resource alloca tion."}, {"title": "5.1 Motivation of Adopting Diffusion Model", "content": "In addition to the shortcomings of the multi-layer perceptron (MLP) commonly used in conventional DRL algorithms, as detailed in Sec. 2.2, our motivation for utilizing diffusion models arises from their unique capability to integrate with the DRL framework. Specifically, in a conventional diffusion model, a user can input a text prompt (e.g., 'an apple on the table') to guide the model in generating a corresponding image. In our scenario, we consider optimal communication and computing resource allocation decisions as the 'image' we aim to generate through the diffusion model, with dynamic environmental information\u2014such as time-varying channel conditions, user movement, and AIGC service popularity-acting as the 'text prompt' guiding the training process. After training, diffusion models can generate optimal resource allocation decisions tailored to any dynamic wireless environment condition [21]. This adaptive ability to generate solutions is particularly valuable for the dynamic environment considered in this paper."}, {"title": "5.2 Preliminaries of Diffusion Models", "content": "Based on the denoising diffusion probabilistic model (DDPM) [32], conventional applications of diffusion models were originally designed for image generation. Specifically, DDPM involves two critical processes during training: the forward process, where noise is progressively added to the original image at each noising step until it becomes indis-tinguishable from pure Gaussian noise; the reverse process, where the noise learned by an MLP is gradually removed at each denoising step to recover the original image.\nWe first encapsulate the optimal communication resource allocation $b^{\\star}(k) = \\{b_{1,t}(k), ..., b_{U,t}(k)\\}$ and the optimal computing resource allocation $\\xi^{\\star}(k) = \\{\\xi_{1,t}(k), ..., \\xi_{U,t}(k)\\}$ at time slot $k$ into a vector $x(k) = \\{b(k), \\xi(k)\\}$, which is considered the \"original policy\" for DDPM. The corre-sponding forward and reverse processes are then detailed below."}, {}]}