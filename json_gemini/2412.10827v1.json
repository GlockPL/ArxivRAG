{"title": "Rethinking Chain-of-Thought from the Perspective of Self-Training", "authors": ["Zongqian Wu", "Baoduo Xu", "Ruochen Cui", "Mengmeng Zhan", "Xiaofeng Zhu", "Lei Feng"], "abstract": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for activating latent capabilities in large language models (LLMs). We observe that CoT shares significant similarities with self-training in terms of their learning processes. Motivated by these parallels, this paper explores the underlying relationship between CoT and self-training, demonstrating how insights from self-training can enhance CoT performance. Specifically, our study first reveals that CoT, like self-training, follows the principle of semantic entropy minimization. Leveraging this insight, we propose a novel CoT framework that incorporates two key components: (i) a task-specific prompt module designed to guide LLMs in generating high-quality initial reasoning processes, and (ii) an adaptive reasoning iteration module for progressively refining the reasoning process. Our code is available at https://github.com/zongqianwu/ST-COT.", "sections": [{"title": "1. Introduction", "content": "Chain-of-thought (CoT) reasoning has attracted significant attention in recent years due to its capacity to unlock the latent potential of large language models (LLMs) (Wei et al., 2022). By requiring LLMs to explicitly outline intermediate reasoning processes before generating final outputs, CoT effectively improves the reliability of inferences, particularly when tackling complex reasoning tasks.\nPrevious CoT methods in LLMs can be divided into two categories, i.e., zero-shot CoT (Kojima et al., 2022) and few-shot CoT (Wei et al., 2022). Zero-shot CoT methods rely on prompts (e.g., \"Let's think step by step\") to guide the LLMs to generate intermediate reasoning processes relevant to the given question, thereby facilitating logical inference. In contrast, few-shot CoT methods provide examples that include intermediate reasoning processes from the dataset,"}, {"title": "2. Understanding Uncertainty in Self-Training and Chain-of-Thought Algorithms", "content": "This section examines uncertainty in self-training and chain-of-thought algorithms by analyzing changes in information entropy and semantic entropy. By understanding how uncertainty evolves and is reduced, we reveal shared mechanisms driving performance improvements. Section 2.1 focuses on information entropy in self-training, while Section 2.2 extends these insights to semantic entropy in CoT reasoning, offering guidance for complex reasoning tasks."}, {"title": "2.1. Information Entropy Variation in Self-Training", "content": "The primary objective of self-training algorithms is to mitigate prediction uncertainty by leveraging pseudo-labels generated by the model, which is quantified through information entropy reduction. As a result, the average entropy of predictions across samples usually shows a progressive decline during iterative training, as substantiated by the empirical evidence presented in Figure 6(a) in Appendix. This decline enables some samples to be corrected from initial mispredictions to accurate predictions.\nHowever, not all samples exhibit the anticipated information entropy reduction. This phenomenon stems from incorrect annotations within pseudo-labels, which can misdirect the model's learning trajectory and compromise optimization. To provide deeper insights into the entropy variation of self-training, we conducted a theoretical analysis using a simple Gaussian mixture model. Specifically, an initial classifier with a sufficient small yet constant error could be accessed and is iteratively updated with pseudo-labels based on model predictions. Through this approach, we examined the classifier's progression from its initial state toward the optimal state, capturing the underlying mechanism of entropy variation. Based on the conclusions of (Frei et al., 2022) regarding the sample complexity of unlabeled samples in self-training, we discover that the intermediate classifier update process is a rotation of the initial classifier towards the Bayes optimal classifier. This finding significantly aids in analyzing the changes in entropy across different samples in R2. This conclusion is stated in the following lemma.\nLemma 2.1. Suppose $(x,y) \\sim D$ where $D$ is a Gaussian mixture models in $\\mathbb{R}^d \\times \\{\\pm 1\\}$ with mean $\\mu$ satisfying $|\\mu|| = \\Theta(1)$, i.e., $y \\sim Unif(\\{\\pm 1\\})$ and $x|y \\sim N(y\\mu, I)$. Let $l(z) = \\log(1 + exp(-z))$, and assume $\\sigma > max(1, ||\\mu||)$. Assume we can access a initial classifier $B_{init}$ which satisfies $\\Pr_{(x,y)\\sim D}[y \\neq sgn(f_{init}x)] = O(1)$. Let $\\epsilon, \\delta \\in (0, 1)$, and assume that $B = \\Omega(\\epsilon^{-1})$, $T = \\Omega(d\\epsilon^{-1})$, $\\eta = \\Theta(d^{-1}\\epsilon)$, suppose $\\Theta_t$ is the angle between $\\beta_t$ and $\\mu$, then by running algorithm 1 with step size $\\eta$ and batch size $B$, when $t < T - 1$, $\\theta_{t} \\geq \\theta_{t+1}$ holds with probability at least $1 - \\delta$, and with probability at least $1 - \\delta$, $\\theta_{T-1} \\leq O(\\epsilon)$."}, {"title": "2.2. Semantic Entropy Variation in Chain-of-Thought", "content": "Chain-of-thought (CoT), akin to self-training, relies on model-generated information to enhance task performance. Specifically, CoT aims to reduce the semantic uncertainty in LLMs predictions by leveraging intermediate reasoning processes generated by the models. This uncertainty can be effectively quantified through the semantic entropy (Farquhar et al., 2024). To formalize this concept, we define the semantic entropy of LLMs with CoT reasoning as follows.\nDefinition 2.3 (Semantic entropy of LLMs with CoT reasoning on a given question). Let the set of reasoning processes that an LLM can generate for a question Q and prompt p be denoted as R, with the corresponding set of answers represented as A. Assume that A can be partitioned into disjoint clusters based on semantic equivalence, i.e., $A = \\bigcup_{C \\in \\mathcal{C}} C$, where $C$ is the set of answers with the same semantics. Let $p(C|Q, q)$ denote the probability distribution over $C$. The semantic entropy is then defined as $SE = \\mathbb{E}_{C}[-\\log p(C|Q, q)]$. In practical use, let LLM generate $t$ distinct answers $\\AA = \\{A_i\\}_{i \\in [t]}$, which are then divided into $k$ semantic clusters $\\hat{\\mathcal{C}} = \\{C_j\\}_{j \\in [k]}$. By normalizing, we obtain a discrete probability distribution $\\{p_j\\}_{j \\in [k]}$, where $p_j = |C_j|/t$. Consequently, we can use $SE = -\\sum_{j=1}^{k} p_j \\log p_j$ as an approximation of SE.\nIn self-training, pseudo-labels guide the initial classifier toward the Bayes-optimal classifier. Similarly, in CoT, the"}, {"title": "3. CoT framework Based on Semantic Entropy", "content": "Building on the insights from the analysis of semantic entropy variation in CoT reasoning presented in Section 2.2, we propose a novel CoT framework to overcome the limitations of traditional approaches, particularly the issues of over-reasoning and excessive similarity between consecutive reasoning iterations. Specifically, we introduce a task-specific prompt in Section 3.1 to guide LLMs in generating high-quality initial reasoning processes. Furthermore, in Section 3.2, we propose an adaptive reasoning iteration to refine the reasoning process and mitigate these challenges. An overview of our CoT framework is provided in Figure 3, with detailed information about the task-specific prompt module provided in Figure 7 in Appendix."}, {"title": "3.1. Task-Specific Prompt", "content": "In Section 2, we discussed how self-training and CoT technologies leverage model-generated information to improve task performance. Specifically, self-training expands decision boundaries by generating pseudo-labels and retraining models, while CoT enhances output reliability by introducing intermediate reasoning processes. In self-training, the quality of pseudo-labels generated during the initial iterations is critical. High-quality pseudo-labels can accelerate model convergence and ensure new pseudo-label quality in subsequent iterations. Conversely, low-quality pseudo-labels may lead to cumulative degradation, forming a negative feedback loop that undermines model performance."}, {"title": "3.2. Adaptive Reasoning Iteration", "content": "In Section 2.1, we theoretically proved that once the self-training model converges, further iterations fail to reduce entropy. Grounded in this insight, we posit that in CoT, once the reasoning processes guide the LLMs to predictions with low uncertainty (searched for the optimal answer group), deeper iterations do not further reduce semantic entropy. Instead, such iterations often introduce noisy information, undermining predictive accuracy. This phenomenon, which we term over-reasoning, risks altering correct initial predictions during subsequent iterations (refer to conclusion (iv)"}]}