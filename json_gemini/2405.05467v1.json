{"title": "AFEN: Respiratory Disease Classification using Ensemble Learning", "authors": ["Rahul Nadkarni", "Emmanouil Nikolakakis", "Razvan Marinescu"], "abstract": "We present AFEN (Audio Feature Ensemble Learning), a model that leverages Convolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion to perform state-of-the-art audio classification for a range of respiratory diseases. We use a meticulously selected mix of audio features which provide the salient attributes of the data and allow for accurate classification. The extracted features are then used as an input to two separate model classifiers 1) a multi-feature CNN classifier and 2) an XGBoost Classifier. The outputs of the two models are then fused with the use of soft voting. Thus, by exploiting ensemble learning, we achieve increased robustness and accuracy. We evaluate the performance of the model on a database of 920 respiratory sounds, which undergoes data augmentation techniques to increase the diversity of the data and generalizability of the model. We empirically verify that AFEN sets a new state-of-the-art using Precision and Recall as metrics, while decreasing training time by 60%.", "sections": [{"title": "1. Introduction", "content": "Respiratory diseases affect millions of people globally every year, claiming 4 million lives annually(8) and diagnosing around 450 million individuals at any given point. While these diseases target the same human body system, their severity can vary from mild to life-threatening. Therefore, accurate diagnosis methods must be available to ensure patients receive the correct treatment.\nToday's methodologies for diagnosing respiratory diseases heavily rely on accurately mapping lung auscultations to the correct diseases. Common diagnostic processes can be invasive and can introduce harmful radiation into human body systems (12). To address this issue, audio classification can enable efficient, non-invasive diagnosis of respiratory diseases. Machine learning models have the capability to analyze waveforms and detect patterns"}, {"title": "Generalizable Insights about Machine Learning in the Context of Healthcare", "content": "Our main contributions are as follows:\n1. Feature Extraction: We devised a unique feature extraction strategy that captured relevant information from the augmented audio data. We targeted a new and ex- panded set of features, including Mel-Frequency Cepstral Coefficients (MFCC), Mel Spectrograms, Chroma Short Time Fourier Transforms (CSTFT), Spectral Rolloff, and the Zero Crossing Rate.\n2. Model Training with XGBoost and CNNs: We trained two separate models using the extracted features. We chose XGBoost for its interpretability and its ability to handle features effectively, while we selected CNNs for their capability to learn features directly from spectrograms or time-frequency representations of the audio signal. Finally, we used ensemble learning techniques to fuse the predictions of both models, achieving a more comprehensive classification performance than that of the individual models.\n3. Performance Evaluation and Comparison: We evaluated the performance of the proposed ensemble model against existing methods on this dataset. We utilized appropriate performance metrics for classification, such as precision, recall, and AUC, for a comprehensive evaluation of the model. Our focus was on the significant increase in accuracy across all classes. Additionally, we showcased the practical feasibility and scalability of the proposed method for real-world deployment by reducing the number of epochs compared to current works."}, {"title": "2. Related Work", "content": "While Natural Language Processing and Computer Vision dominate the current tech- nological landscape, Audio Machine Learning Applications remain relatively unexplored. Within the Audio Machine Learning field, there exists two main disciplines: Classification and Generation. Classification is the most common application of Audio Machine Learn- ing, with Generation still growing. Studies such as (10), (22), and (16) provide valuable approaches into classification techniques. Similarly, studies like (13), (17), and (15) have all contributed to the field of audio generation.\nGiven the correlation between sound and Respiratory Diseases, it was a natural progres- sion to utilize Machine Learning techniques to classify diseases. In 2017, (19) conducted a survey of the current audio machine learning landscape and found that no clear standard benchmark had been set. They concluded that Support Vector Machines would be a suit- able approach to respiratory disease detection for its versatility to be used for binary or multi-class classification. However, (3) compared the accuracy of a Convolutional Neural Network to Support Vector Machines. They tested both on binary classification between healthy and diseased sounds, and multi-class classification for auscultation sounds. Using 2 convolutional layers, they achieved results similar to their Support Vector Machine. This work was one of many to shift the baseline from SVMs to CNNs by maintaining a high level of performance while reducing the model's complexity.\nIn keeping with this standard, (23) introduced a new architecture for Ensemble Learning in Audio Classification. They extracted MFCCs, Perceptual Linear Predictives (PLPs), Subband Energy, Gammatone Frequency Cepstral Coefficients (GFCCs), Crest Factor, and a variety of Spectral features to train their Ensemble Network. Their network consists of a CNN and a ResNet-18 to train Mel Spectrum features and an XGBoost Model for each acoustic feature. The models are trained separately but culminate in their fusion to generate an Apnea-HypoApnea Index score for patients facing Obstructive Sleep Apnea-Hypoapnea Syndrome. Using 4 convolutional layers in their CNN, a ResNet-18, and XGBoost, they achieved an average specificity of 82%.\nIn the 2017 ICBHI Challenge, (21) published a dataset of 920 lung sounds for finding the best method to identify abnormalities within the lung. Since the dataset's publication, there have been numerous attempts to identify the best classification method. (18) set a new baseline for this dataset. While no data augmentation was performed, they chose to extract MFCCs to train their CNN consisting of 3 convolutional blocks. With a learning rate of .001, a momentum value of .8, a batch size of 125, and a dropout value of .03, they achieved a training accuracy of 98% and a testing accuracy of 95%. Following this work, in 2023, (4) expanded on Mridha et al. (18) by adding new features to the model. While Bapat et al. (4) maintains a similar architecture to Mridha et al. (18), the addition of Mel Spectrograms and Chroma Short Time Fourier Transforms (CSTFTs) improves the loss by 43%."}, {"title": "3. Methods", "content": "We introduce a novel ensemble learning approach for respiratory audio classification, leveraging a MultiFeature CNN and an XGBoost Model. In this section, we present our formulation of audio classification as an ensemble learning problem and discuss the rationale behind integrating multiple models. 3.1 outlines the general pipeline of our approach, which combines the strengths of the MultiFeature CNN and the XGBoost Model. Then, we introduce an ensemble model for this task (3.2). In Section 3.3, we introduce our XGBoost Model, which acts as a complementary classifier to the MultiFeature CNN."}, {"title": "3.1. Architecture Overview", "content": "Building upon previous approaches to Audio Classification (4; 18), a typical architecture consists of four components: Data Augmentation, Feature Extraction, Training, and Eval- uation. Our goal is to maintain this foundation while introducing necessary complexity to enhance accuracy and efficiency. To expand our feature set, we incorporate the Zero Cross- ing Rate (9) and Spectral Rolloff (14) into our feature extraction process. Additionally, we retain the importance of Data Augmentations (2; 5) employing Bandpass Filters, Pitch Shifts, Shifts, and Additive White Gaussian noise.\nEach feature is modeled separately within the Multi-feature CNN (4). Although their input tensors may vary, the core architecture of each CNN remains consistent. Subsequently, the outputs of the individual 2D CNNs (18) are concatenated, and the model is compiled (4). Our CNN model incorporates a Self-Attention Mechanism to improve performance. This mechanism enables the network to focus on relevant features within the audio data by assigning varying degrees of importance to different input segments (11).\nWe introduce the XGBoost model to enhance the classification performance of the En- semble Network. The XGBoost constructs a series of decision trees sequentially, with each subsequent tree learning from the errors of the previous ones (6). This iterative process allows the model to prioritize difficult-to-predict data points, gradually enhancing its pre- dictive accuracy.\nFollowing (23), we employ soft voting to combine the predictions of the Multifeature CNN and XGBoost in a weighted manner. Unlike hard fusion, where the final prediction relies solely on the output of a single best-performing model, soft fusion assigns weights to each model's predictions and aggregates them to produce the final prediction (1)."}, {"title": "3.2. Multi-Feature Network", "content": "We introduce a novel sequential model for each feature CNN comprising five two- dimensional convolutional layers. For the MFCC model, input data is shaped as (40, 259, 1), where 40 denotes the number of MFCCs (3), 259 represents the length of the temporal dimension, and 1 signifies the mono channel. Using this precedent, we apply the same ra- tionale for deciding the input size for all additional features. Beginning with a sequence of five two-dimensional convolutional layers, we apply Batch Normalization, Rectified Linear Unit (ReLU) activation functions, and Max Pooling. We integrate batch normalization layers after each convolutional layer to stabilize and accelerate training. Following each convolutional layer, we incorporate Two-Dimensional Max Pooling layers to perform spa- tial downsampling of feature maps, enabling the model to recognize patterns irrespective of their location in the input. ReLU activation is chosen to mitigate the vanishing gradient problem.\nWe progressively increase the number of filters while reducing spatial dimensions in con- structing the architecture. We begin with 32 filters using a 5x5 kernel size and strides of (2, 3) in the initial layer to facilitate significant downsampling. The second layer follows the same pattern, increasing the filter size to 64, while adopting a 3x3 kernel size with strides of (2, 2). This configuration maintains the downsampling while enhancing the model's ability to capture finer spatial details. We then progress to 96 filters with a 2x2 kernel size in the next layer, preserving the input's spatial dimensions for detailed feature extraction. In"}, {"title": "3.3. Gradient Boosting with XGBoost", "content": "In this section, we introduce the XGBoost Model for respiratory disease classification. The XGBoost Model, an Ensemble Model employing decision trees and regularization tech- niques (6), optimizes loss minimization through gradient descent during training. Unlike bagging techniques like Random Forest, boosting sequentially builds decision trees, with each subsequent tree aiming to reduce the error of the previous one. Choosing the number of trees is a delicate balance, as reducing the number of trees potentially leads to underfit- ting, whereas increasing the amount can lead to overfitting (6).\nWe train the classifier on concatenated feature vectors extracted from the various fea- tures collected. The model is configured for multiclass classification by setting the objective to multiclass softmax. This setup calculates class probabilities and assigns instances to the class with the highest probability. Multiclass log loss serves as our evaluation metric, pro- viding a comprehensive assessment by penalizing incorrect predictions proportional to the discrepancy between predicted and true class probabilities. In our case, setting the number of estimators to 400 balances accuracy and prevents overfitting."}, {"title": "3.4. Ensemble Learning", "content": "We employ soft voting to combine predictions from the CNN and the XGBoost using weighted averaging, where weights reflect the confidence or reliability of each learner's pre- dictions. Unlike hard fusion methods, which directly combine base learners' outputs, soft fusion allows nuanced integration, assigning higher weights to confident or accurate predic- tions while reducing weights for those with higher uncertainty. This approach enables the ensemble model to make more informed predictions."}, {"title": "4. Dataset", "content": "The Respiratory Sound database is a collection of 920 lung sounds created by (20) for the development of new techniques to differentiate unique respiratory sounds. These recordings span over 6898 respiration cycles and were annotated by experts in the field to denote the presence of crackles, wheezes, a combination of both, or their absence. They were recorded via stethoscope and were processed using an audio editing software. The recordings vary in length, and can span from between 10 seconds to 90 seconds. Each file contains information regarding the location in the body this audio was taken from. The annotation of chest locations from which the recordings were acquired provides spatial context and allows us to"}, {"title": "4.1. Dataset Selection", "content": "This dataset represents patients of all ages and was selected for its variety and the surplus of information that can be extracted from each audio file and subsequent annotation. The audio files were collected from unique chest locations: the trachea, the left and right anterior chest, the posterior, and the lateral points on the body in both clinical and non- clinical environments. Within this dataset, the following diseases are represented with varying frequency: Lower Respiratory Tract Infections (LRTI), Upper Respiratory Tract Infections (URTI), COPD, Asthma, Pneumonia, and Bronchiectasis. Respiratory conditions encompass a wide spectrum of diseases and are each characterized by distinct patterns of respiratory sounds and clinical observations. This variety enables the identification of unique acoustic signatures associated with different diseases and facilitates more precise diagnostic algorithms."}, {"title": "4.2. Data Extraction and Augmentation", "content": "To address the inherent imbalance in the Respiratory Sound database, particularly with COPD cases outnumbering other respiratory conditions, we implemented a stratified split- ting approach, as described in (4). This method ensured that each subset used for training,"}, {"title": "1. Additive White Gaussian Noise (AWGN)", "content": "Additive White Gaussian Noise is a commonly used type of noise added to signals to simulate real-world interference. In our case, AWGN is particularly useful because it closely resembles background noise present in many recording environments. By incorporating AWGN during training, models become better equipped to handle noisy input."}, {"title": "2. Bandpass Filter", "content": "Bandpass filters allow a specific range of frequencies to pass through while attenu- ating frequencies outside this range. This augmentation is valuable because bandpass filters maintain the original sample of the audio file, ensuring that the model encoun- ters variations that occur in real-world scenarios."}, {"title": "3. Time Shifts", "content": "Time shifting involves shifting the audio waveform along the time axis by a certain number of samples. In respiratory disease classification, shifts can be beneficial due to the variability in respiratory sounds across different individuals and conditions. Augmenting the training data with shifted versions of the original recordings helps the model become more robust to variations in onset times or duration of respiratory sounds."}, {"title": "4. Pitch Shift", "content": "Pitch shifting alters the pitch of the audio waveform while preserving its tempo- ral characteristics. This augmentation is relevant because respiratory sounds may exhibit variations in pitch across different individuals, lung conditions, or recording conditions. By augmenting the training data with pitch-shifted versions of the original recordings, the model becomes more resilient to variations in pitch."}, {"title": "4.3. Feature Extraction", "content": "Feature extraction plays a crucial role as they capture unique aspects of the sound, encompassing important information such as frequency patterns, spectral characteristics, and temporal features inherent in each respiratory signal. In our approach, we selected Mel Frequency Cepstral Coefficients (MFCCs), Mel Spectrograms, Chroma Short Time Fourier Transforms (CSTFTs), following the methodology outlined in (4). Additionally, we expanded the feature set to include Spectral Rolloff (14) and the Zero Crossing Rate (9). We discovered that collectively, these features effectively highlight the unique abnormalities associated with each respiratory disease, such as wheezing or crackles, more so than their counterparts."}, {"title": "1. Mel Frequency Cepstral Coefficients (MFCCs)", "content": "MFCCs capture the spectral characteristics of audio signals by representing their short-term power on the Mel Scale, a perceptual pitch scale that mirrors human"}, {"title": "2. Chroma Short Time Fourier Transforms (CSTFTs)", "content": "Chroma Short Time Fourier Transforms (CSTFTs) illustrate the frequency content of audio signals over time, emphasizing specific frequency components. By capturing spectral variations over time, CSTFTs reveal the significance of transient events in the signal, allowing monitoring of the progression of adventitious sounds throughout the audio file."}, {"title": "3. Mel Spectrograms", "content": "Similar to MFCCs, Mel Spectrograms offer a detailed depiction of the input's frequency content. By applying the Mel Scale to the waveform, we highlight important frequency bands relevant to the signal. Additionally, Mel Spectrograms provide a more compact representation of sound frequency, reducing dimensionality and feature space. This simplification streamlines the model training process and increases its efficiency. We choose to follow (3)'s optimal value for MFCCs per window at 40."}, {"title": "4. Spectral Rolloff", "content": "The Spectral Rolloff measures the frequency below which a certain percentage of total spectral energy occurs, dividing the spectrum into low and high-frequency regions. This feature emphasizes tonal differences across audio signals and can indicate unique harmonics associated with each disease."}, {"title": "5. Zero Crossing Rate", "content": "The Zero Crossing Rate measures how quickly the audio signal alternates between positive and negative values within short time intervals. A higher ZCR suggests a more percussive sound, while a lower ZCR corresponds to smoother, sustained sounds. We employ ZCR to identify abrupt changes or irregularities in each signal."}, {"title": "5. Results", "content": "This section discusses an explanation of the architecture training model and shows our results. We also showcase the technology utilized in this research. Secondly, we display tables consisting of our results.\nIn evaluating our model's performance, we draw inspiration from established metrics utilized in related works such as (18), (4), and (5). These metrics have proven effective in assessing the efficacy of respiratory sound analysis systems and serve as benchmarks for our evaluation.\nInitially, we employ fundamental metrics, including training and testing loss, and accu- racy. We discuss our loss function further in 5.1. These metrics offer a basic understanding of the model's performance in terms of its ability to minimize errors and classify instances correctly. However, given the nature of the respiratory sound classification problem, we rec- ognize the limitations of relying solely on accuracy. Therefore, we augment our evaluation with more comprehensive metrics. Precision, Recall, and AUC (Area Under the Curve) are"}, {"title": "5.1. Experimental Evaluation", "content": "From 1, we set the MultiFeature CNN to 100 epochs and we remain consistent with (4), as we set the train-test-split to 80-20. Similarly, we choose to define sparse categorical crossentropy as our loss function for multiclass classification.\nAfter initially setting the epochs to 250 (18), we found the model converged to the state-of-the-art value for MultiFeature CNNs on this dataset at 98.27% at 100 epochs. Additionally, we set the new state-of-the-art for testing loss at 8.69%. For the XGBoost,we set the state-of-the art in testing accuracy at 97.26% and a testing loss of 3.67%. In setting the number of estimators to 100, we received a test accuracy of 93%, far below our expectations compared to the MultiFeature CNN. Due to the size of our dataset, we had to increase the number of estimators. We choose 400 estimators only after meticulously, and iteratively, increasing and decreasing the number of estimators to achieve our expected accuracy while preventing overfitting. Additionally, we maintain the default learning rate at .3 and choose multiclass logloss as our loss function (3.3). This is defined as:\n$\\L_{\\text{log}}(Y, P) = -\\log P(\\text{r}(Y|P) = -\\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} Y_{i,k}\\text{log}(p_{i,k})$\nWhere\n\u2022 Y: True labels or ground truth values, encoded in a one-hot format.\n\u2022 P: Predicted probabilities outputted by the model for each class.\n\u2022 Pr(YP): Probability of observing the true labels given the predicted probabilities.\n\u2022 N: Number of samples or data points in the dataset.\n\u2022 K: Number of classes in the classification problem.\n\u2022 $Y_{i,k}$: Element in the true label matrix Y corresponding to the i-th sample and the k-th class."}, {"title": "6. Discussion", "content": "This study introduces AFEN, an ensemble learning methodology for respiratory disease diagnosis. To achieve this, we introduce a novel data processing pipeline that integrates a unique data augmentation process and an expanded feature set. We advance the capabilities of Multifeature CNNs by introducing self-attention mechanisms within the feature CNNs."}, {"title": "Limitations", "content": "While our approach demonstrates promising results, there are limitations that need to be acknowledged. Firstly, the generalizability of our findings may be limited by the specific characteristics of the dataset used in this study, which could affect the per- formance of the models when applied to new and unseen data. Moreover, ensemble learning involves multiple models which can result in increased computational requirements and a larger memory footprint of the generated model. Additionally, the reliance on medical data obtained from a single institution or source may introduce biases or limitations in the model's ability to generalize to different populations or healthcare contexts. When working with imbalanced datasets, particularly in clinical contexts, the recall metric becomes par- ticularly sensitive to data distribution variations and the occurrence of false negatives. In scenarios where certain diseases, such as Asthma in our case, are less prevalent or inade- quately represented in the training data, the model's ability to generalize and accurately predict them is challenged. As a result, the recall scores for these classes exhibit notable volatility."}]}