{"title": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision Quantization for Practical Embedded AI Applications", "authors": ["Jeongseok Kim", "Jemin Lee", "Yongin Kwon", "Daeyoung Kim"], "abstract": "Mixed-precision quantization methods have been proposed to reduce model size while minimizing accuracy degradation. However, existing studies require retraining and do not consider the computational overhead and intermediate representations (IR) generated during the compilation process, limiting their application at the compiler level. This computational overhead refers to the runtime latency caused by frequent quantization and de-quantization operations during inference. Performing these operations at the individual operator level causes significant runtime delays. To address these issues, we propose QuantuneV2, a compiler-based mixed-precision quantization method designed for practical embedded AI applications. QuantuneV2 performs inference only twice\u2014once before quantization and once after quantization\u2014and operates with a computational complexity off O(n) that increases linearly with the number of model parameters. We also made the sensitivity analysis more stable by using local metrics like weights, activation values, the Signal-to-Quantization-Noise Ratio (SQNR), and the Mean Squared Error (MSE). We also cut down on computational overhead by choosing the best IR and using operator fusion. Experimental results show that QuantuneV2 achieved up to a 10.28% improvement in accuracy and a 12.52% increase in speed compared to existing methods across five models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This demonstrates that QuantuneV2 enhances model performance while maintaining computational efficiency, making it suitable for deployment in embedded AI environments.", "sections": [{"title": "1. Introduction", "content": "According to the scaling law [1], the performance of deep learning models continuously improves as their size increases, leading to a continuous growth in model sizes. With the increase in model size, there has been active research on lightweight methods to deploy deep learning on resource-constrained devices, with quantization being a representative approaches [2]. Compilation time optimization has emerged as a critical challenge in deep learning model development and on-device execution environments, prompting extensive research efforts. In particular, because the time for determining mixed-precision quantization is highly constrained, prolonged compilation can delay the development cycle and decrease productivity [3, 4]. Although compilation typically happens only once, frequent model updates or reuse across various environments can make lengthy compilation times severely inefficient. In this context, short compilation for quantization is crucial. As shown in Fig. 2, QuantuneV2 demonstrates its effectiveness with minimal runtime overhead. Quantization involves converting the weights and activation values of a model from a 32-bit floating-point format to lower bit-widths. Models converted to lower bit-widths operate with reduced memory usage and power consumption. However, uniformly quantizing all layers to lower bit-widths can significantly degrade model accuracy due to loss of precision. For instance, converting layers from 32-bit to 8-bit can result in substantial information loss if the data spans a wide range or includes extreme values. This information loss is a major cause of accuracy degradation in DNN models [5, 6]. To address this issue, many studies have focused on accurately measuring sensitivity and applying mixed-precision quantization based on these measurements to reduce the model size while minimizing accuracy drop [7-13]. These methods use a metric called Sensitivity, which relates to the model accuracy, as a criterion for assigning different precisions to each layer [11-13]. We measured changes in Top-1 accuracy across six DNN models as each layer was sequentially quantized to 8-bit, as shown in Fig. 1. The extent of accuracy degradation varied by model. For MobileNetv2 and ShuffleNet, some layers caused up to a 1% decrease in accuracy even when only a single layer is quantized to 8-bit. Therefore, by identifying the layers with the highest Top-1 accuracy degradation for each model and avoiding the quantization of those specific layers, the accuracy loss can be effectively reduced.\nDespite the effectiveness of mixed-precision quantization, there are significant challenges when applying these methods at compile time. As shown in Fig. 2, quantization aware training (QAT) methods need to be retrained, rely on the training dataset, and have complicated hyperparameter tuning, which makes it hard to use these techniques at compile time. Mixed-precision algorithms based on Post-Training Quantization (PTQ) often encounter that the sensitivity list determined at the algorithmic stage becomes suboptimal and requires a significant amount of time to generate it. Recent PTQ methods use reconstruction to adjust the weights, in order to reduce the loss caused by quantization. However, as shown in Fig. 2, this process is also quite time-consuming, making it challenging to apply at compile time. This is due to operator optimizations that occur in the model structure and during actual compilation process, such as fusion. Additionally, the weight-only-order method [12], an existing study, ignores the characteristics of activations and relies solely on weights when generating the sensitivity list, making accurate sensitivity measurement difficult.\nTherefore, a lightweight mixed-precision method that can be applied at the compiler level is required, which must address three major challenges: C1) Developing an efficient and simple mixed-precision method with linear computational complexity suitable for compile-time application. C2) Overcoming stability issues in sensitivity analysis during compile-time by devising an effective local metric. C3) Selecting the optimal Graph Intermediate Representation (IR) form by considering the impact of graph IR stage selection on the overall performance of mixed-precision quantization.\nIn this study, we propose QuantuneV2, a solution that overcomes the three key challenges mentioned earlier by performing mixed-precision quantization during compile time based on a local metric using a small set of input data. We perform extensive experiments on five models\u2013ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2-focusing on model accuracy, algorithm running time, inference time, and ablation study.\nThe key contributions of our work are outlined below.\n\u2022 The proposed QuantuneV2 has an algorithmic computational complexity of O(n), scaling linearly with the number of parameters in the model. This is much faster and more efficient than existing methods that need to search the exponentially large bit-width configuration space.\n\u2022 QuantuneV2 can construct a stable local metric by combining weights, activations, SQNR, and MSE in compile time. Additionally, it identifies the optimal stage of the Graph Intermediate Representation (IR) for sensitivity generation by analyzing how different graph IR forms impact the sensitivity analysis and the overall performance.\n\u2022 QuantuneV2 was applied to five models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2 to evaluate the quantization accuracy. The results showed an average accuracy improvement of 0.66% compared to the method using Weight SQNR [12]. It achieves up to 0.68% higher accuracy than existing methods that directly use validation datasets [13].\n\u2022 The generation time for sensitivity lists during quantization is approximately 99.99% faster than existing methods. Additionally, we verified QuantuneV2's execution performance on various hardware platforms, confirming that it can perform the quantization process 1.43 times faster and run quantized models up to 12.52% faster than existing methods."}, {"title": "2. Background and Related Work", "content": "Quantization is an important technique for optimizing deep learning models, especially for reducing model size. In this section, we introduce background knowledge and existing research on quantization, categorizing them into the areas of quantization methods and deep learning compilers. Additionally, we have highlighted the novel contributions of our work compared to existing approaches."}, {"title": "2.1. Uniform Quantization and Optimization in Deep Learning Models", "content": "Uniform quantization for deep learning models is employed to reduce the precision of weight and activations within the neural networks. This approach aims to decrease the model's memory footprint and computational demands, which are particularly beneficial for deploying deep learning models on devices with limited computational resources, such as mobile phones and embedded systems [14].\nUniform quantization maps continuous real-value tensors onto a finite set of discrete values [15, 16]. The key characteristic of uniform quantization is that it uniformly distributes discrete values across the range of data, meaning that the intervals between adjacent quantized values are constant. This method is mathematically described as follows:\nGiven a real-valued input x, the quantized value Q(x) is computed as follows:\n$Q(x) = round(\\frac{x - X_{min}}{\\delta})$\nwhere, $X_{min}$ represents the minimum value of the tensor to be quantized, and \u03b4 stands for the quantization step, which is defined as follows:\n$\\delta = \\frac{X_{max} - X_{min}}{N-1}$\nwhere, $X_{max}$ denotes the maximum value of the tensor, and N is the number of quantization levels. The quantization levels are often chosen to be powers of two (e.g., 256 levels for 8-bit quantization), facilitating the hardware implementation of the computations.\nThe quantized values Q(x) can then be transformed back into a dequantized representation $\\hat{x}$, which approximates the original value, using:\n$\\hat{x} = Q(x) \\cdot \\delta + X_{min}$\nThis quantization process introduces a quantization error, which is defined as the difference between the original and quantized values. However, with appropriate calibration and fine-tuning, the effect of this error on the accuracy of the model can be minimized [17].\nDuring inference, Uniform quantization is applied to the model weight and the activation (the outputs of the layers). This enables the entire inference process to be conducted using lower-precision arithmetic, significantly reducing computational complexity and power consumption.\nUniform quantization is crucial for optimizing deep learning models for edge computing. because it effectively balances the trade off between model accuracy and resource efficiency."}, {"title": "2.2. Quantization", "content": ""}, {"title": "2.2.1. Quantization-Aware Training", "content": "Quantization-aware training (QAT) strategies involve adapting models from higher to lower bit precision during the training process, which is supported by numerous studies [18-27]. For instance, Krishnamoorthi [18] provides foundational techniques for implementing QAT, addressing practical challenges like activation quantization and efficient inference. Jacob et al. [24] focus on quantization and training of neural networks for efficient integer-arithmetic-only inference, which is essential for deploying models on hardware without floating-point support. Advanced methods such as Learned Step Size Quantization (LSQ) by Esser et al. [19] and LQ-Nets by Zhang et al. [21] optimize quantization scales and jointly learn network parameters and quantization functions, effectively reducing accuracy loss at low bit-widths. Choi et al. [20] introduce PACT, where activation clipping values are learned during training, improving quantization without significant performance degradation. The principal advantage of QAT lies in its capability to minimize the performance degradation often observed in quantized models through additional training. This approach allows for the efficient quantization of CNN architectures to lower bit representations, including 2-bit precision, without significant loss in accuracy. However, QAT faces challenges such as prolonged training times, potential unavailability of training data for third-party services, and complex hyperparameter optimization. Moreover, it is absolutely impossible to apply such methods at compile time. Attempts to address these challenges through methods like Variational Continual Learning [26] and fixed-point optimization with adaptive retraining [27] have yet to fully overcome them. Given these considerations, our study focuses on post-training quantization (PTQ), which does not require retraining and is practical for rapid deployment. PTQ is especially suitable for compiler-level applications where computational efficiency and ease of integration are paramount."}, {"title": "2.2.2. Post-training Quantization", "content": "Post-Training Quantization (PTQ) converts models to lower precision without additional training, making it suitable for resource-limited devices [13, 28-41]. It addresses the time consumption and data scarcity concerns associated with QAT but can lead to significant accuracy loss, especially at lower bit-widths. To enhance PTQ performance, Wu et al. [13] provided principles and empirical evaluations for integer quantization. Nagel et al. [31] proposed data-free quantization using weight equalization and bias correction, mitigating accuracy degradation without training data. Banner et al. [29] introduced post-training 4-bit quantization with minimal accuracy loss, while Zhao et al. [32] improved quantization without retraining by addressing outlier channels. AdaQuant by Hubara et al. [39] minimizes layer-wise quantization errors post-training through iterative optimization and integer programming, using unlabeled data to calibrate activations without requiring fine-tuning. BRECQ by Yuhang Li et al. [40] minimizes second-order quantization errors using gradient-based optimization involving complex computations like Hessian matrices. QDrop by Xiuying Wei et al. [41] introduces quantization-aware dropout to mitigate quantization noise. However, these methods require iterative training steps and weight updates. QuantuneV1 [38] proposed a method for exploring various quantization configurations and recommending the optimal approach for each model. However, it only provided an ON/OFF functionality for mixed precision and did not explore the optimal mixed precision for all layers. Despite these advancements, PTQ methods often struggle to maintain consistent accuracy across different models due to architectural variations and data distributions. Determining optimal quantization settings for each model can be time-consuming and lacks a universal solution. We focused our research on quickly identifying effective parameters for each model that can be applied at compile time to maintain high accuracy within a short time frame."}, {"title": "2.2.3. Mixed-Precision Quantization", "content": "Prior mixed-precision works formulated the mixed-precision optimization problem through reinforcement learning to allocate the optimal bit precision per layer of a deep neural network. In such approaches, a reinforcement learning agent determines the bit width of each layer and learns to determine optimal balance between the model performance and cost reduction through the rewards associated with these decisions [42]. The key to this method is automating the complex bit allocation process and allowing the agent to efficiently explore the search space through various reinforcement learning algorithms [43]. A specific example is the HAQ framework which hardware feedback is incorporated to learn policies that minimize energy and latency while maintaining accuracy [44]. This reinforcement learning-based optimization method is particularly useful for reducing computational costs while maintaining model performance, but it takes a long time. Also, these methods have limitations. First, determining bit-widths through sensitivity analysis is complex. Second, they rely heavily on training data, limiting generalization to new domains. Third, these methods can still incur high computational costs and long execution times."}, {"title": "2.3. Deep Learning Compilers", "content": "The surge in efficiency requirements for deep learning models has underscored the importance of DL compilers, which have been developed across academia and industry. Although proprietary compilers, such as Intel nGraph [45], NVIDIA TensorRT [36], ARM NN, and Xilinx Vitis focus on specific hardware platforms, open-source alternatives such as TVM [46] and Glow [47] offer broader hardware support. However, these community-driven compilers typically require manual exploration to determine the optimal quantization settings, which can delay model deployment. Manual exploration becomes impractical due to the exponentially increasing search space as the number of layers grows [7]. For example, a model with 100 layers and two possible bit-widths per layer results in $2^{100}$ possible configurations, which is infeasible to explore exhaustively. This issue leads to significant time and resource consumption, especially for larger models or when deploying across different hardware platforms. In this study, QuantuneV2, enhances these compilers by introducing an advanced mixed-precision algorithm capable of automatically determining the best configuration for accuracy. By efficiently determining appropriate bit-widths at compile time using sensitivity metrics like MSE and SQNR, QuantuneV2 eliminates the need for manual exploration."}, {"title": "3. Detailed Challenges of Applying Mixed-Precision Quantization at Compile Time", "content": "In this section, we outline three significant challenges (C1-C3) that hinder the effective application of mixed-precision quantization at compile time."}, {"title": "3.1. C1: Demand for Efficient and Simple Mixed-Precision Methods", "content": "The first challenge is efficiently determining mixed-precision quantization during compile time. To apply mixed precision at the compiler level in a single optimization pass, the process needs to be as fast and simple as possible. However, complex methods that require real-time weight updates or error reconstruction are inefficient and impractical. In other words, existing PTQ methods based on reconstruction and QAT that require backpropagation cannot be applied to compile-time quantization due to computational complexity. Additionally, leading frameworks for optimizing deep learning models such as TVM and Glow do not support mixed-precision quantization at compile time. Therefore, a fast and simple mixed-precision quantization method that can be executed within compile time is needed."}, {"title": "3.2. C2: Stability Issues in Sensitivity Analysis", "content": "The second challenge is measuring and ranking the quantization sensitivity of each layer in a DNN model for mixed-precision quantization [10-13]. It is necessary to determine a local metric with strong generalization performance to accurately measure the sensitivity of each layer. Existing studies propose sensitivity measurement methods, such as the Jacobian Norm [48] or First-Order Sensitivity [49]; however, they are limited by computational complexity and inconsistent results depending on the input images. In this study, we consider SQNR [50], MSE [51, 52], and cosine similarity [53], KL divergence [54, 55] as applicable local metrics for application during compile time. SQNR has been used as a metric for evaluating quantization noise in neural networks. MSE is used to measure quantization error before and after quantization, and cosine similarity is employed to assess the similarity between original and quantized weights. KL divergence is a statistical measure used to evaluate the difference between two distributions before and after quantization. These metrics are computationally efficient and can be applied to both weights and activations, resulting in eight combinations."}, {"title": "3.3. C3: Selection of Optimal Graph Intermediate Representation", "content": "The third challenge is determining the appropriate stage of the Graph Intermediate Representation (Graph IR) at which to calculate the local metric for generating the sensitivity list during compile time to achieve optimal performance. Graph IR is an abstract representation used by compilers to represent the intermediate stage of a program, allowing for various optimizations and transformations. As shown in Fig. 4, there are multiple levels of operator fusion that can be applied. These include no operator fusion (A), partial fusion of Convolution and Batch Normalization (B), and full fusion of Convolution, Batch Normalization, and ReLU (C). At the Graph IR level, the final Graph form is modified as in (C) to reduce operation execution time and memory access overhead by executing multiple operations in a single kernel [56, 57]. However, if the sensitivity list is generated from a fully fused Graph IR (C), it becomes difficult to accurately evaluate the sensitivity of individual operators because the inherent sensitivity of multiple fused operators is represented as a single sensitivity.\nAs shown in Table 1, the sensitivity list varies depending on the operator's fusion stage, affecting the model's accuracy. Additionally, the number of operations due to quantization, de-quantization iterations changes, resulting in variations in the overall execution speed of the model. Therefore, it is necessary to analyze how generating a sensitivity list and computing the local metric at different Graph IR levels affect quantization performance. First, it is necessary to consider methods that calculate operator-specific local metrics at the non-fused stage (A), and then apply operator fusion at stage (C) to speed up operator execution."}, {"title": "4. Overview", "content": "The proposed mixed-precision method extends the capabilities of DNN compilers to support mixed-precision quantization at the compiler level. This approach quickly identifies layers in the input model that can significantly degrade accuracy during compile time and applies mixed-precision accordingly. To overcome the three challenges mentioned in \u00a73, the proposed QuantuneV2 employs three strategies: i) By performing inference only twice\u2014before and after quantization for the entire model, we achieve a computational complexity of O(n) that increases linearly with the number of parameters; ii) It uses a stable local metric that combines weights, activations, SQNR, and MSE to determine mixed-precision without relying on back-propagation; and iii) Select the most appropriate intermediate representation (IR) from the various IRs generated during the compilation process to calculate layer-wise sensitivity. Subsequently, to enhance operational execution speed, perform operator fusion into the final graph form, thereby ensuring both high model accuracy and execution speed. From the user's perspective, mixed-precision is determined by specifying the desired level of quantization according to their objectives.\nSpecifically, the operation of QuantuneV2 is divided into two main parts: sensitivity analyzer and a model compiler, as shown in Fig. 5. The execution process follows five sequential steps: 1 Data Distribution Verification and Calibration: Collect statistical data on the distribution of weights and activations by running the model on a small calibration dataset. Perform calibration to adjust model parameters and ensure that the quantization scales accurately represent the data range of each layer. Scale Information Calculation and Initial Quantization: Based on the collected data distribution, calculate the scale information for quantization of the weights. For symmetric quantization, the scale (S) for each layer's weights is calculated using:\n$S = \\frac{2^{b-1}-1}{max(|W|)}$\nwhere W represents the weight tensor, and b is the bit-width. The quantization process maps the floating-point weights to integer values using this scale:\n$Q(W) = round(\\frac{W}{S})$\nWe then apply uniform quantization across all layers with the same bit-width to create an initial quantized model.\n3 Local Metric Computation (SQNR and MSE Measurement): Measure the SQNR and MSE of each layer by comparing the weights and activations of the original and quantized models. This provides quantitative metrics for the quantization error and helps identify layers that are sensitive to quantization. 4 Sensitivity Analysis and List Generation: Calculate the SQNR delta for each layer, which is the difference in SQNR between adjacent layers. Generate a sensitivity list for quantization by combining the SQNR delta, MSE values, and considering the effects of operator fusion (e.g., Convolution and ReLU). 5 Configuration File Generation and Mixed-Precision Application: Generate a configuration file that specifies the bit-width for each layer according to the desired quality of service (QoS) requirements. Using the quantization function of the Glow Compiler\u00b9, layers identified as sensitive maintain 32-bit precision, while the remaining layers are quantized to 8-bit precision (\u00a7 5)."}, {"title": "5. Compiler Extension for Mixed-Precision Quantization", "content": "This section outlines a comprehensive methodology for the quantization and design of each component involved. The proposed mixed-precision quantization method can be extended and applied to all DNN compilers. In this paper, we extended"}, {"title": "Algorithm 1 Graph node level mixed-precision quantization", "content": "Input: DAG Graph F\nInput: Dequantized node list L\nOutput: Quantized DAG Graph F*\n1: nodeIt \u2190 F.getNodes().end() \u25ba Initialize start node\n2: stopIt \u2190 F.getNodes().begin() \u25ba Initialize end node\n3: repeat\n4: nodeIt \u2190 nodeIt - 1\n5: node \u2190 nodeIt\n6: node_name \u2190 node.getName()\n7: isExist \u2190 false\n8: while L is not the end of list do\n9: inStr \u2190 L \u25ba Read a line from the sensitivity list\n10: if inStr is node_name then\n11: isExist \u2190 true\n12: Break\n13: end if\n14: end while\n15: if isExist then\n16: Continue\n17: end if\n18: convertOutputs(node) \u25ba Output scales for quantization\n19: convertInputs(node) \u25ba Input scales for quantization\n20: newNode \u2190 newNode(node) \u25ba Replace the node\n21: postProcessing(newNode) \u25ba DCE, CSE, etc\n22: until nodeIt = stopIt\n23: cleanUp()\n24: return F*\nAs described in lines 3-22, starting from the last node, if the node is not stored in L, it is converted into a quantized node, and the existing graph node is replaced. After converting the graph nodes to quantized ones, basic compiler optimizations such as Dead Code Elimination (DCE) and Common Subexpression Elimination (CSE) are performed as post-processing to remove unnecessary parts from the existing graphs. The specific part matching L is detailed in Lines 8\u201314."}, {"title": "6. Sensitivity Analyzer", "content": "To establish a reliable sensitivity list for evaluating mixed-precision quantization, the Sensitivity Analyzer considers the following factors: i) careful selection of appropriate local metrics, ii) exploration of suitable Graph IR and calibration techniques, iii) optimization of quantization during operator fusion, and iv) utilization of the SQNR delta. Each design choice is explained in detail below."}, {"title": "6.1. Selection of local metrics", "content": "In the sensitivity analyzer, to apply mixed-precision quantization, a sensitivity list was generated using the local metrics of SQNR and MSE simultaneously. As mentioned in Section 3.2(C2), the available local metrics at compile time include SQNR, MSE, and cosine similarity, KL divergence. To compute the local metrics, the value outputs for each layer before and after quantization were stored during the compilation phase. The local metrics considered in this study are SQNR, MSE, cosine similarity and KL divergence, each with specific calculation methods as described below. SQNR measures the ratio of signal power to quantization noise power, where higher values indicate lower quantization noise [58]. The actual calculation follows Eq. (6), focusing on the overall energy ratios, and thereby potentially ignoring small errors in specific regions. It is particularly useful in identifying regions with significant quantization noise, as it provides an overall measure of signal quality.\n$SQNR = 10 \\cdot log_{10}(\\frac{P_{signal}}{P_{noise}})$\nThe MSE computes the average of the squared differences between the predicted and actual values. The calculations are shown, in Eq. (7), indicates that lower values correspond to fewer quantization error[59]. The MSE focuses on the absolute differences between the actual and predicted values, with the squared terms emphasizing larger errors.\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(P_{signal} - P_{noise})^2$\nCosine Similarity measures the angle between two vectors to compute similarity [60]. The calculation follows Eq. (8), which focuses on the vector direction without considering its magnitude. Values range between -1 and 1, where proximity to 1 signifies similar vector directions. Cosine similarity does not consider the signal magnitude or energy levels and potentially disregards information about the overall signal strength. While the angles may be similar, significant differences in the actual values can exist, thus limiting their ability to evaluate the absolute quantization error size. Moreover, applying it across various DNN models may pose challenges in ensuring consistent performance.\n$cosine\\ similarity(A, B) = \\frac{A \\cdot B}{||A|| ||B||}$\nKL Divergence measures the difference between two probability distributions P (original layer output) and Q (quantized layer output) [55]. It provides insight into how much information is lost when converting high-precision activations and weights to a lower bit-width format. KL Divergence is defined as:\n$KL\\ Divergence(P || Q) = \\sum_{x} P(x) \\log(\\frac{P(x)}{Q(x)})$\nA lower KL Divergence indicates that the quantized output closely matches the original, signaling a successful preservation of information during quantization. Higher values of KL Divergence suggest greater deviations, indicating layers that are more susceptible to accuracy loss after quantization. However, when applying KL Divergence to various DNN models, sensitivity analysis is not properly performed, so it has clear limitations as a local metric for mixed-precision quantization.\nTherefore, this study employs a strategy that combines SQNR and MSE as complementary local metrics. The concurrent use of SQNR and MSE is motivated by their synergistic effects, as illustrated in Fig. 6 for ResNet18v1 experiments, showing that they exhibit similar trends while complementing each other. When significant quantization errors occur, it can be observed that both weight and activation SQNR values decrease, while the MSE values increase significantly. Thus, SQNR evaluates the overall signal-to-noise ratio, while MSE assesses quantization errors at the individual sample level. Combining these two local metrics enables a more comprehensive and detailed analysis."}, {"title": "6.2. Optimizing Graph IR Search and Calibration for Optimal SQNR Measurement", "content": "Experiments were conducted to determine how to calculate the optimal SQNR for the weights and activations. This exploration was divided into two parts: one focusing on operator fusion, as mentioned in Section 3.3 (C3), and the other on calculating activation tensor. Fig. 7 illustrates the distribution of SQNR values when these two aspects are considered in the six models. As shown in Fig. 7, the distribution of SQNR varies among models, as previously reported[12]. Furthermore, it is evident that operator fusion significantly affects the SQNR values for both weight and activation. In general, the SQNR values for weights showed a wider range when operator fusion was not applied compared to when it was applied. By disabling the operator fusion, the SQNR can be calculated for each convolution and batch normalization operator individually, which better reflects the unique characteristics of each layer and more effectively distinguishes the sensitivity differences between layers. In addition, the expanded precision options allocated individually to each operator increase the search space, enabling optimal precision allocation for each layer. Therefore, in this study, sensitivity list generation was performed without operator fusion.\nAnother important factor is that the Signal-to-Quantization Noise Ratio (SQNR) for activation varies depending on the input image. Therefore, we analyzed the SQNR values calculated using both calibration and arbitrary datasets. As the weight distribution of the DNN model was tailored to the characteristics of the calibration dataset, obtaining the activation SQNR with the calibration dataset allowed the SQNR to be distributed over a wider range. This makes it better at distinguishing sensitivity differences between layers. Therefore, calculating the SQNR using the calibration dataset is more advantageous for creating sensitivity lists."}, {"title": "6.3. Operator Fusion Optimization for Optimal Mixed-Precision", "content": "In this study, fusion was not applied during the sensitivity analysis of the layers. However, a fused graph IR was used when applying mixed-precision. This is because, as explained earlier, the sensitivity list considers the form of layers before fusion for precision analysis, Whereas at the application stage, the fusion of layers is considered for accuracy and execution speed optimization. The considered fusion involves batch normalization and the ReLU activation function as follows.\nEq. (10) illustrates the computation of the output Yi at the i-th layer when a convolution operation is followed by batch normalization. The convolution conv(x)i is defined as conv(x)i = Wixi + Bi, where Wi and Bi represent the weights and biases of the convolutional layer, respectively.\n$Y_{i} = \\gamma_{i} (\\frac{conv(x)_{i} - \\mu_{i}}{\\sqrt{\\sigma_{i}^2 + \\epsilon}}) + \\beta_{i}, conv(x)_{i} = W_{i}x_{i} + B_{i}$\nBy substituting the convolution expression into the batch normalization formula, we obtain the fused operator in Eq. (11):\n$Y_{i} = \\gamma_{i} (\\frac{W_{i}x_{i} + B_{i} - \\mu_{i}}{\\sqrt{\\sigma_{i}^2 + \\epsilon}}) + \\beta_{i}$\n$= (\\frac{\\gamma_{i}}{\\sqrt{\\sigma_{i}^2 + \\epsilon}})W_{i}x_{i} + (\\frac{\\gamma_{i}(B_{i} - \\mu_{i})}{\\sqrt{\\sigma_{i}^2 + \\epsilon}} + \\beta_{i})$\n$= A_{i}x_{i} + C_{i}$\nThe fused weights and biases are defined in Eq. (12).\n$A_{i} = (\\frac{\\gamma_{i}}{\\sqrt{\\sigma_{i}^2 + \\epsilon}})W_{i} C_{i} = \\frac{\\gamma_{i}(B_{i} - \\mu_{i})}{\\sqrt{\\sigma_{i}^2 + \\epsilon}} + \\beta_{i}$\nThis fusion integrates batch normalization into the convolution operation by adjusting the original weights and biases to Ai and Ci. By precomputing these adjusted parameters during compilation, we eliminate the need for separate batch normalization during inference. This optimization consistently reduces computational overhead and improves inference latency across different hardware platforms.\nThe second fusion considered is between the convolution and activation functions (ReLU). The fusion of Conv and ReLU was performed as shown in Eq. (13).\n$ReLU(conv(x)_{i}) = max(0, W_{i} * X_{i} + B_{i})$\nwhere Wi, Xi, and Bi denote the filter weight, input data, and bias, respectively. max(0,) denotes the ReLU function. The fusion of convolution and rectified linear units (ReLU) is relatively straightforward, and maintaining scale factors for quantization that enhance accuracy is crucial. To achieve this, the proposed operator fusion substitutes the output scale of the ReLU with that of the convolution. In this case, there is no need to maintain the output scale of the convolution, and the quantization process for both the output of the convolution and the input of the ReLU is eliminated, resulting in an improved overall model accuracy and reduced computational overhead. The substitution of scales and the improvement in accuracy can be expressed mathematically as follows.\nEq. (14) offers a simplified depiction of the convolution operation by omitting the bias term, focusing on the essential computation. In Eq. (14), the quantized input feature map $x^q$ is multiplied by the quantized weights $w^q$, and the products are summed over n:\n$y_{fp32} = S_{x}S_{w}(\\sum_{n=1}^{N} x^{q}w^{q})$\n$y_{int} = ROUND(\\frac{y_{fp32}}{S_{y}})$\nwhere Sx and Sw denote the scale factors for the input and weights, respectively. The product SxSw scales the accumulated sum back to the floating-point domain, resulting in yfp32. The quantized output yint is then obtained by scaling the sum with Sy and applying the rounding function.\nEq. (15) illustrates the fusion of the convolution and ReLU operations. Initially, the convolution output is passed through the ReLU activation function F, scaled by:\n$F(\\frac{S_{x}S_{w}}{S_{a}} ROUND (\\sum_{n=1}^{N} x^{q}w^{q}) = max(0, ROUND(\\frac{S_{x}S_{w}}{S_{a}}\\sum_{n=1}^{N} x^{q}w^{q}))$"}, {"content": "$FROUND(SAX(4))=SMAX(0.ROUND(XS));\\$SA=SA$ SA>sy=MIN(SA;SY)"}, {"title": "6.4. Utilizing SQNR delta to Improve Sensitivity Analysis", "content": "The SQNR metric can be affected by cumulative quantization noise as it propagates through the layers. Moreover, relying solely on the SQNR information at individual nodes provides insufficient data for constructing accurate sensitivity lists. Therefore, a metric called SQNR delta is needed to reduce cumulative quantization noise and better understand the interactions between layers in DNN models.. The SQNR delta, as a measure of the relative sensitivity, was computed using Eq. (16). Because it considers the difference from the previous layer's SQNR, the SQNR delta can mitigate the impact of cumulative quantization noise. Moreover, since it uses the correlation with adjacent layers rather than the SQNR value of a single layer, a smaller SQNR delta value indicates an increase in quantization noise, while a larger value indicates a decrease in quantization noise. The sensitivity of each layer is determined based on the magnitude of the increase or decrease in SQNR delta.\n$SQNR\\ delta = \\frac{SQNR_{n} - SQNR_{n-1}}{L_{n}-L_{n-1}}$"}, {"title": "Algorithm 2 Sensitivity list generation for quantization", "content": "Input: number of layers n", "D\nOutput": "Sensitivity List S\n1: SQNRList \u2190 [", "n2": "MSEList \u2190 [", "n3": "i \u2190 0\n4: while i < n do \u25ba Calculating the SQNR", "MSE\n5": "SQNR \u2190 CalculateSQNR(Q["}]}