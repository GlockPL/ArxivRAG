{"title": "Horizon Generalization in Reinforcement Learning", "authors": ["Vivek Myers", "Catherine Ji", "Benjamin Eysenbach"], "abstract": "We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Thus, such a policy trained to reach nearby goals should succeed at reaching arbitrarily-distant goals. Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results and recall findings from prior work in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is appealing for its potential to use data to solve long-horizon reasoning problems. However, it is precisely this horizon that makes solving the RL problem difficult - the number of possible solutions to a control problem often grows exponentially in the horizon [1]. Indeed, the requirement of collecting long horizon data precludes several potential applications of RL (e.g., health care, robotic manipulation). Thus, a desirable property of an RL algorithm is the ability to learn from short-horizon tasks and generalize to long-horizon tasks. We call this property horizon generalization.\nHorizon Generalization (informal statement, see Definition 4): A goal-conditioned policy generalizes over horizon if, after training to reach nearby goals within the state space, the policy is more successful at reaching distant goals."}, {"title": "Related Work", "content": "Our work builds upon prior work in goal-conditioned RL and generalization in RL. Section 5.5 returns to the discussion of prior work in light of our analysis.\nLearning to Reach Goals. The problem of learning goal-reaching behavior dates to the early days of AI research [6, 7]. This problem has received renewed attention in recent years through the study of deep goal-conditioned reinforcement learning (GCRL) [8-14]. Goal-conditioned RL relieves the burden of specifying rewards, as any state in the environment can provide a complete task specification when used as a goal. Some of the excitement in goal-conditioned RL is a reflection of the recent success of self-supervised methods in computer vision (e.g., stable diffusion [15]) and NLP (GPT-4 [16]): if these methods can achieve intriguing emergent properties [17, 18], might a self-supervised approach to RL unlock emergent properties for RL?\nGeneralization in RL. Prior work on generalization in RL mostly focuses on variations in per-ception [19-21] (or, similarly, e.g., across levels of a game [22-25]). Similarly, work on robust RL (which measures a worst-case notion of generalization) usually randomly perturbs the physics parameters [26-30]). Our paper will study a different form of generalization: without changing the dynamics or the observations, can a policy trained on nearby goals succeed in reaching distant goals?\nState Abstractions for Decision-Making. Many approaches for learning improved state abstractions for decision making have been proposed in recent years, including bisimulation [31-34], successor representations [35, 36], and information-theoretic representation learning objectives [37-41]. While prior work typically views generalization as a problem of handling shift between MDPs with similar horizons, horizon generalization is about generalizing from short to long horizons. This form of generalization (to our knowledge) has not been directly addressed by other state abstraction methods. Prior work that has specifically looked at performing out-of-distribution long-horizon tasks have made assumptions about the environment, such as access to external planners [42-44]"}, {"title": "Planning Invariance and Horizon Generalization", "content": "Our analysis will focus on the goal-conditioned setting. We start by providing intuition for our key formal definitions (planning invariance and horizon generalization), provide important preliminaries on quasimetric methods, and then prove that these properties can be realized by quasimetric methods."}, {"title": "Intuition", "content": "Many prior works have found that augmenting goal-conditioned policies with planning can significantly boost performance [46, 47]: instead of aiming for the final goal, these methods use planning to find a waypoint en route to that goal and aim for that waypoint instead. In effect, the policy chooses a closer, easier waypoint that will naturally bring the agent closer to the final goal.\nInvariance to planning (see Fig. 2) is an appealing property for several reasons. First, it implies that the policy realizes the benefits of planning without the complex machinery typically associated with hierarchical and model-based methods. Second, policies optimal over a space of tasks are, by definition, planning-invariant over the same space with respect to an optimal planner: invariance to planning is a necessary but not sufficient condition for policy optimality, and can be used as an inductive bias to achieve policy optimality. Third, we show that planning invariance, combined with other assumptions, implies that the policy will exhibit horizon generalization: given that a policy successfully navigates short trajectories covering some state space S, it will succeed at performing long-horizon tasks over the same state space S (Fig. 1).\nA high-level description of our proof is as follows: when a policy is invariant to planning, tasks of length n and length 2n will be mapped to similar internal representations, as will tasks of length 4n, and 8n, and so on (see Fig. 3). This reasoning also explains how a policy exhibiting horizon generalization must solve problems: by recursion, the policy maps tasks of length n, length n/2, and shorter lengths seen during training to the same internal representations. Our proofs formally link these \u201cforward-looking\u201d and \u201cbackward-looking\u201d perspectives, suggesting practical planning-invariant methods like quasimetric methods to achieve horizon generalization.\nWith these motivations in hand, how do we actually construct methods that are planning invariant and lead to horizon generalization? To answer this question, we build upon prior work on quasi-metric neural network architectures [48-50] and show that policies defined greedily with respect to a quasimetric, where latents obey the triangle inequality, are invariant to planning with respect to the same quasimetric."}, {"title": "Preliminaries", "content": "We consider a controlled Markov process M with state space S, action space A, and dynamics p(s' | s,a). The agent interacts with the environment by selecting actions according to a policy \u03c0(\u03b1 | s), which is a mapping from S to distributions over A. We further assume the state and action spaces are compact. We define the discounted state occupancy measure with actions as\n$p_{\\pi}(s_k = g | s_0 = s, a) = \\sum_{t=0}^{\\infty} \\gamma^t p^t(s_t = g | s_0 = s,a),$\nwhere $p^t(s_t = g | s_0 = s, a)$ is the probability density that policy \u03c0 visits state g after t time steps when initialized at state s with action a.\nQuasimetrics on states. We equip M with an additional notion of distance between states. We later define planning operator PLAN and policy \u03c0 greedily with respect to this distance. At the most basic level, a distance $d : S \u00d7 S \u2192 R$ must be positive for all inputs $(s, s' \u2260 s)$ and zero for all inputs $(s, s)$ (nonnegativity). We will denote the set of all distances as D:\n$D \\equiv \\{d : S \u00d7 S \u2192 R : d(s, s) = 0, d(s, s') > 0 \\text{ for each } s, s' \u2208 S \\text{ where } s \u2260 s'\\}.$\nA desirable property for distances to satisfy is the triangle inequality. A distance satisfying this property is known as a quasimetric, and we define the set of all quasimetric functions as\n$Q \\eqslantgtr \\{d\u2208 D : d(s, g) \u2264 d(s,w) + d(w, g) \\text{ for all } s, g, w \u2208 S\\}.$\nIf we were to restrict the distances to be symmetric (d(x, y) = d(y, x)), our quasimetric would become a standard metric obeying nonnegativity, the triangle inequality, and symmetry. However, we wish to use a quasimetric that allows for asymmetry over the interchange of the start and end states: the navigation task s \u2192 g may be completely different from g \u2192 s, and the corresponding distance function should reflect this degree of freedom.\nAn important property of quasimetrics is that they are invariant to the path relaxation operator from Dijkstra's algorithm."}, {"title": "Introducing and Analyzing Horizon Generalization", "content": "Equipped with quasimetric definitions, we begin by formally defining planning invariance and horizon generalization in deterministic and stochastic settings. Then, we show that quasimetric policy $\u03c0_a(a | s, g)$ is planning invariant with respect to a planner defined over the same quasimetric. Finally, we show that this invariance to planning implies horizon generalization. Taken together, our analysis shows that horizon generalization exists and can be achieved by quasimetric methods."}, {"title": "Definitions of Planning Invariance and Horizon Generalization", "content": "To construct general definitions of planning invariance and horizon generalization, we will need to define a planning operator which proposes waypoints at a given state to reach a target distribution over goals. What does it mean to plan over an input distribution of goals? In nondeterministic settings, actions are optimal in expectation. Thus, accurate planners must be able to take in distributions over states and choose actions which induce that future state distribution."}, {"title": "Quasimetric Policies are (Nontrivially) Planning Invariant", "content": "With these notions of planning invariance and horizon generalization in hand, we will consider nontrivial quasimetric planning algorithms $PLAN_d \u2208 plan_d$ that acquire a quasimetric $d(s, g)$ and output a single waypoint w \u2208 S:\n$PLAN_d(s, g) = w_{PLAN} \u2208 arg min_{w\u2208S} d(s, w) + d(w, g).$"}, {"title": "Limitations and Assumptions", "content": "Despite our theoretical results proving that horizon generalization exists, we expect that practical algorithms will not perfectly achieve horizon generalization. This section highlights the assumptions that belie our key results, and our experiments in Section 6 will empirically study the degree to which current methods achieve these properties.\nAn important assumption in our inductive proof is that horizon generalization exists as a binary category. However, in practical algorithms, horizon generalization likely exists on a spectrum. As such, each application of the inductive argument will incur some error, such that the argument (and, hence, the degree of generalization) will not extend infinitely.\nTo make this more concrete, define SUCCESS(c) as the success rate for reaching goals in radius c, and assume that we choose constant $c_0$ small enough that SUCCESS($c_0$) = 1. Then, let us assume that each time the horizon is doubled ($c_0 \u2192 2c_0 \u2192 4c_0 \u2192 \u2026$), the success rate decreases by a factor of \u03b7. We will refer to \u03b7 as the horizon generalization parameter and later measure this parameter in our experiments (Section 6). In addition, we assume that SUCCESS(c) is monotonically decreasing; goals further in time should be harder to reach. We can now define the REACH as the sum of SUCCESS(c) over c > $c_0$. With the above constraints on SUCCESS(c), in the worst case,\n$REACH_{wc} = 1 + \u03b7(2 \u2013 1) + \u03b7^2 (4 - 2) + \u03b7^3 (8 - 4) + ... = 1 + \\sum_{j=1}^{\\infty} \u03b7^j 2^{j-1} = 1 + \\frac{\u03b7}{1-2\u03b7} = \\begin{cases} \\frac{1-\u03b7}{1-2\u03b7}  \\text{ if } 0 < \u03b7 <1/2\\\\ \u221e \\text{ if } \u03b7 \u2265 1/2 \\end{cases}.$\nWhen there is no horizon generalization, the Reach is 1. We can see this by integrating the Success curve in Fig. 4, top. When the degree of horizon generalization has a low value of (say) \u03b7 = 0.1 (i.e., it generalizes for only 1 out of every 10 goals), the Reach is 1.125, not much bigger than that of a policy without horizon generalization. Once the degree of horizon generalization reaches \u03b7 = 1/2 (i.e., generalizes for 1 out of every two goals), the Reach is infinite. In short, the potential reach of horizon generalization is infinite, even when each step of the recursive argument incurs a non-negligible degree of error.\nA second important assumption behind our analysis is that the base case holds everywhere: the policy must succeed at reaching all nearby goals when initialized at all possible starting states. In practice, this may translate to a coverage assumption on the training data. If the base case does not hold (poor performance on easy goals) but planning invariance holds, then we should not expect to see optimality over arbitrarily hard goals. We will observe this empirically with a random policy in our experiments (Fig. 5): a random policy is invariant to planning (it always selects random actions, regardless of the goal) yet its performance on nearby goals is mediocre, so the policy fails to exhibit horizon generalization.\nFinally, invariance under any arbitrary planner does not guarantee horizon generalization. Indeed, our Theorem 2 states invariance under a planner that minimizes an asymmetric distance (quasimetric) leads to horizon generalization. Thus, planning and invariance to planning with respect to, say, an"}, {"title": "Which Practical Methods Might Exhibit Horizon Generalization?", "content": "arbitrary reward function does not necessarily lead to horizon generalization, even if a policy is optimal within short horizons.\nNonetheless, planning invariance remains an alluring property for three reasons: (1) planning-invariant policies potentially automatically get the benefits of planning, (2) optimal policies are invariant under optimal planners, and, as we show in our analysis, (3) invariance to planners that shorten quasimetric distances leads to horizon generalization (Theorem 2). In light of our analysis, we discuss methods for planning invariance in the next section."}, {"title": "Experiments", "content": "The aim of our experiments is to provide intuition into what horizon generalization and planning invariance are, why it should be possible to achieve these properties, and to study the extent to which existing methods already achieve these properties. We also present an experiment highlighting why horizon generalization is a useful notion even when considering temporal difference methods (Section 6.2).\nWe start with a didactic, tabular navigation task (Fig. 11), connecting short horizon trajectories and evaluating performance on long-horizon tasks. In our first experiment, we measure the empirical average hitting time distance between all pairs of states. We define a policy that acts greedily with respect to these distances, measuring performance of this \u201cmetric regression\" policy in Fig. 5 (Top Left). The degree of horizon generalization can be quantified by comparing its success rate on nearby (s, g) pairs to more distant pairs. We compare to a \u201cmetric regression with quasimetric\u201d method that projects the empirical hitting times into a quasimetric by performing path relaxation updates until convergence $(d(s, g) \u2190 min_w d(s, w) + d(w, g))$. Fig. 5 (Top Left) shows that this policy achieves near perfect horizon generalization. While this result makes intuitive sense (this algorithm is very similar to Dijkstra's algorithm), it nonetheless highlights one way in which a method trained on nearby start-goal pairs can generalize to more distant pairs.\nWe study planning invariance of these policies by comparing the success rate of each policy (on distant start-goal pairs) when the policy is conditioned on the goal versus on a waypoint. See Appendix F for details. As shown in Fig. 5 (Top Right), the \u201cmetric regression with quasimetric\u201d policy exhibits stronger planning invariance, supporting our theoretical claim that (Theorem 1) planning invariance is possible.\nWe next study whether these properties exist when using function approximation. For this experi-ment, we adopt the contrastive RL method [56] for estimating the distances, comparing different"}, {"title": "Studying Horizon Generalization in a High-dimensional Setting", "content": "Our next set of experiments study horizon generalization and planning invariance in the context of a high-dimensional quadrupedal locomotion task (see Fig. 6). We start by running a series of experiments to compare the horizon generalization of different learning algorithms (CRL [56] and SAC [57]) and distance metric architectures (details in Appendix F). The results in Fig. 6 highlight that both the learning algorithm and the architecture can play an important role in horizon generalization,"}, {"title": "Impact of Horizon Generalization on Bellman Errors", "content": "Why should someone using a temporal difference method care about horizon generalization, if TD methods are supposed to provide this property for free? One hypothesis is that methods for achieving horizon generalization will also help decrease the Bellman error, especially for unseen start-goal pairs. We test this hypothesis by measuring the Bellman error throughout training of the contrastive RL method (same method as Fig. 5), with two different architectures. The results in Fig. 9 show that the architecture that exhibits stronger horizon generalization ($d_{e_2}$) also has a lower Bellman error throughout training. Thus, while TD methods may achieve horizon generalization at convergence (at least in the tabular setting with infinite data), a stronger understanding of horizon generalization may nonetheless prove useful for designing architectures that enable faster convergence of TD methods."}, {"title": "Conclusion", "content": "The aim of this paper is to give a name to a type of generalization that has been observed before, but (to the best of our knowledge) has never been studied in its own right: the capacity to generalize"}, {"title": "Definition of Path Relaxation", "content": "In this section, we formally define the general path relaxation operator in Definition 5. This definition extends Definition 1 to allow for actions and environmental stochasticity.\nDefinition 5 (Path relaxation operator with actions). Let PATH\u0105(s, a, G) be the path relaxation operator over quasimetric d(s, a, G). For any triplet of state and state distributions (s, W, G) \u2208 S \u00d7 P(S) \u00d7 P(S),\n$PATH_d(s, a, G) \\triangleq min_W d(s, a, W) + d(W, G).$\nIn the controlled, fixed goal setting, define\n$PATH_d^{FIX} (s, a, g)\\equiv min_w d(s, a, w) + d(w, g).$\nThe notation P(X) used here and throughout the appendix denotes the space of probability distributions over set X."}, {"title": "Formalizing Planning Invariance and Horizon Generalization", "content": "In this section, we prove results discussed in Section 5.2 and versions of results in Section 5 for the general stochastic, distributional setting."}, {"title": "Planning Invariance Exists", "content": "Theorem 1 (Quasimetric policies are invariant under PLANd). Given a deterministic MDP with states S, actions A, and goal-conditioned Kronecker delta reward function $r_g(s) = \u03b4(s,g)$, define quasimetric policy $\u03c0_a(a | s, g)$ and quasimetric planner class $plan_d$. Then, for every quasimetric planner $PLAN_d \u2208 plan_d$, there always exists a policy $\u03c0_a(a | s, g)$ that is planning invariant:\n$\u03c0_a(a | s,g) = \u03c0_a(a | s, w \\text{ for } w = PLAN_d(s, g)).$\nProof. Let s, g \u2208 S and the action-free distance function be $d(s, g) = min_a d(s, a, g)$; this statement is true for the constrastive successor distances (Eq. 5). Define the (deterministic) planned waypoint as\n$w_{PLAN} \u2190 PLAN_d(s, g) \u2208 arg min_{w\u2208S} d(s, w) + d(w,g).$\nWe can then construct the following policy:\n$\u03c0_a(a | s, g) \u2208 arg min_{a\u2208A} d(s, a, g)$"}, {"title": "Quasimetric Over Distributions", "content": "Definition 6 (Quasimetric over distributions). Let the goal-conditioned MDP M be given.\nGiven quasimetric dom defined over start-goal space S \u00d7 S, we define the quasimetric over distributions as\n$d_{QMD}(P,Q) = inf_{\u03a5\u2208\u03a0(P,Q)} \\int_{S\u00d7S}  d_{QM} (p,q)y(p, q) dp dq,$\nwhich is the asymmetric Wasserstein Distance with quasimetric cost function $d_{qm}(p,q)$.\nWe can interpret this object as the minimum cost to convert distribution P to Q, where the cost function is some quasimetric between individual states.\nWe show Definition 6 is a valid quasimetric. Because $d_{QMD}$ is an asymmetric Wasserstein distance and cost function $d_{qm}(p, q)$ is a quasimetric, this proof is an extension of a well-known result [81] that drops the metric symmetry condition. We include the proof here for completeness.\nProof. We check the conditions of a quasimetric for $d_{QMD}(P,Q)$ with quasimetric cost function $d_{om}(p,q)$.\nNon-negativity: By definition of y(p, q) and $d_{qm} (p, q)$, we have $d_{QMD} (P, Q) \u2265 0$ for all P, Q.\nWe show that $d_{QMD}(P, Q) = 0$ if and only if P = Q, beginning with the forward direction:\n$d_{QMD}(P, P) = inf_{\u03a5\u2208\u03a0(P,P)} \\int_{S\u00d7S}  d_{QM} (p,q)y(p,q) dp dq$\n$\\leq \\int_{S\u00d7S} d_{QM} (P, q)Y_D (P, q) dp dq  (set y as diagonal matrix $Y_D$)\n$= \\int_{S} d_{QM} (p, p)\u00b5(p) dp  (where $\u00b5(p) = Y_D(P,P)$)\n$= 0    (d_{qm} (p, p) = 0)$\nFor the other direction, we have that $d_{QMD}(P,Q) = 0$ implies \u03b3(p, q) = 0 for all p \u2260 q. However, because y(p, q) is a probability distribution, this must mean P = Q.\nAsymmetry: We have that $d_{QMD} (P, Q)$ is not necessarily symmetric because the quasimetric $d_{om}(p, q)$ is not necessarily symmetric."}, {"title": "Quasimetrics, Path Relaxation, Policies, and Planning Invariance in Stochastic Settings", "content": "We extend the definitions of quasimetrics, path relaxation, policies, and planning invariance to the setting of general stochastic MDPs.\nDefinition 7 (Quasimetric over actions in general stochastic setting). Let the quasimetric over distributions $d_{QMD}$ (Definition 6) be given. Let $S'_{s,a} = p(s' | s, a)$ be the distribution over next-step states after taking action a from starting state s. We define the stochastic-setting quasimetric over actions as\n$d_{QMD}(s, a, G) \\triangleq d_{QMD}(s, S'(s,a)) + d_{QMD} (S'(s,a), G).$\nDefinition 8 (Quasimetric policy in general stochastic setting). Given goal-conditioned MDP M with states S, actions A, and goal-conditioned Kronecker delta reward function $r_g(s) = \u03b4(s,g)$ and quasimetric over distributions $d_{QMD}$ (Definition 6), we extend the quasimetric policy to stochastic settings:\n$\u03c0_a(a | s, G) \u2208 arg min_a d_{QMD} (s, a, G).$\nWe can also generalize the planning class to take in states, actions, and state distributions as inputs:\n$plan \u2252 \\{PLAN : S \u00d7 A \u00d7 P(S) \u2192 P(S)\\}$.\nThis planner chooses a waypoint distribution conditioned on a given start state, action taken from this state, and a desired future goal distribution.\nDefinition 9 (Quasimetric planner class in general stochastic setting). Given goal-conditioned MDP M and quasimetric over distributions $d_{QMD}$ (Definition 6), we extend the quasimetric planning class to stochastic settings:\n$plan_d \u2287 \\{PLAN \u2208 plan | d_{QMD}(s, a, W) + d_{QMD}(W, G) = d_{QMD}(s, a, G)$ for all (s, a, G) \u2208 S \u00d7 A \u00d7 P(S) where PLAN(s, a, G) = W\\}."}, {"title": "Horizon generalization exists", "content": "Theorem 2 (Horizon generalization exists). Consider a deterministic goal-conditioned MDP with states S, actions A, and goal-conditioned Kronecker delta reward function $r_g(s) = \u03b4(s,g)$ where there are no states outside of S. Let finite thresholds c > 0 and quasimetrics d(s, g) over the start-goal space S \u00d7 S be given. Then, a quasimetric policy $\u03c0_a(a | s, g)$ that is optimal over $B_c = \\{(s, g) \u2208 S \u00d7 S | d(s, g) < c\\}$ is optimal over the entire start-goal space S \u00d7 S.\nProof. We prove the more general result for policies $\u03c0_a(a | s, G)$ defined over start-goal distribution pairs (s, G). See earlier sections in Appendix B.3 for quasimetric, policy, and planning definitions over distributions."}, {"title": "Horizon generalization is nontrivial", "content": "We observe that planning invariance and horizon generalization can be arbitrarily violated for general policies and MDPs.\nRemark 3 (Horizon generalization is nontrivial). Let finite c > 0 and goal-conditioned MDP with states S, actions A, and goal-conditioned Kronecker delta reward function $r_g(s) = \u03b4(s,g)$ be given where there are no states outside of S. For a policy that is not planning invariant, optimality over $B = \\{(s, g) \u2208 S \u00d7 S | d(s, g) < c\\}$ is not a sufficient condition for optimality over the entire start-goal space S \u00d7 S.\nProof. We restrict our proof to the fixed, controlled setting and let quasimetric d(s, g) be the successor distance $d_{sp} (s, g)$ [5] - this assumption lets us directly equate the optimal horizon H to the distance $d_{SD}(s, g)$, but note that similar arguments can be applied by treating d(s, g) as a generalized notion of horizon.\nConsider goal-conditioned policy $\u03c0^{*,H}(a | s, g)$ that is optimal for (s, g) pairs over some horizon H. Assume there is at least one goal g' that is optimally H + 1 actions away from s, and that there exists some optimal waypoint s' on the way to g' reachable via actions A' \u2282 A (where A \\ A', the set of suboptimal actions, is nonempty).\nWe can then construct a policy $\u03c0^{H+1}$ where (1) $\u03c0^{H+1}(a | s, g')$ returns an action in the suboptimal set A \\ A' and (2) $\u03c0^{H+1}$ restricted to start-goal pairs horizon H apart is equivalent to $\u03c0^{*, H}$. Therefore, an arbitrary, non-planning invariant goal-reaching policy does not necessarily exhibit horizon generalization."}, {"title": "New Methods for Planning Invariance", "content": "While the aim of this paper is not to propose a new method, we will discuss several new directions that may be examined for achieving planning invariance.\nRepresentation learning. As shown in Fig. 2, planning invariance implies that some internal representation inside a policy must map start-goal inputs and start-waypoint inputs to similar representations. What representation learning objective would result in representations that, when used for a policy, guarantee horizon generalization? The fact that plans over representations sometimes correspond to geodesics [61, 82] hints that this may be possible.\nFlattening hierarchical methods. While hierarchical methods often achieve higher success rates in practice, it remains unclear why flat methods cannot achieve similar performance given the same data. While prior work has suggested that hierarchicies may aid in exploration [83], it may be the case that they (somehow) exploit the metric structure of the problem. Once this inductive bias is identified, it may be possible to imbue it into a \u201cflat\u201d policy so that it can achieve similar performance (without the complexity of hierarchical methods)."}, {"title": "Self-Consistent Models", "content": "In machine learning, we usually strive for consistent models: ones that faithfully predict the train-ing data. Sometimes (often), however, a model that is consistent with the training data may be inconsistent with other yet-to-be-seen training examples. In the absence of infinite data, one way of performing model selection is to see whether a model's predictions are self-consistent with one another. This is perhaps most easily seen in the case of metric learning, as studied in this paper. If we are trying to learn a metric d(x, y), then the properties of metrics tell us something about the predictions that our model should make, both on seen and unseen inputs. For example, even on unseen inputs, our model's predictions should obey the triangle inequality. Given many candidate models that are all consistent with the training data, we may be able to rule out some of those models if their predictions on unseen examples are not \u201clogically\u201d consistent (e.g., if they violate the triangle inequality). One way of interpreting quasimetric neural networks is that they are architecturally constrained to be self-consistent. We will discuss a few implications of this observation.\nDo self-consistent models know what they know? What if we assume that quasimetric networks can generalize? That is, after learning that (say) $s_1$ and $s_2$ are 5 steps apart, it will predict that similar states $s'_1$ and $s'_2$ are also 5 steps apart. Because the model is architecturally constrained to be a quasimetric, this prediction (or \u201challucination\") could also result in changing the predictions for other s-g pairs. That is, this new \u201challucinated\" edge $s_1 \u2192 s_2$ might result in path relaxation for yet other edges.\nWhat other sorts of models are self-consistent? There has been much discussion of self-consistency in the language-modeling literature [85, 86]. Many of these methods are predicated on the same underlying as self-consistency in quasimetric networks: checking whether the model makes logically consistent predictions on unseen inputs. Logical consistency might be used to determine that a prediction is unlikely, and so the model can be updated or revised to make a different prediction instead.\nThere is an important difference between this example and the quasimetrics. While the axiom used for checking self-consistency in quasimetrics was the triangle inequality, in this language modeling example self-consistency is checked using the predictions from the language model itself. In the example of quasimetrics, our ability to precisely write down a mathematical notion of consistency enabled us to translate that axiom into an architecture that is self-consistent with this property. This raises an intriguing question: Can we quantify the rules of logic in such a way that they can be translated into a logically self-consistent language model? What makes this claim seem alluringly tangible is that there is abundant literature from mathematics and philosophy on quantifying logical rules [87]."}, {"title": "Evidence of Horizon Generalization and Planning Invariance from Prior Work", "content": "Not only do the experiments in Section 6 provide evidence for horizon generalization and planning invariance, but we also can find evidence of these properties in the experiments run by prior work. This section reviews three such examples, with the corresponding figures from prior work in Fig. 10:"}, {"title": "Experiment Details", "content": "The following subsections discuss the environment details for the figures in the main text."}, {"title": "Didactic Maze: Figure 2", "content": "This task is a maze with walls shown as in Fig. 2. The dynamics are deterministic. There are 5 actions, corresponding to the cardinal directions and a no-op action.\nFor this plot, we generated data from a random policy, using 1000 trajectories of length 200. We estimated distances using Monte Carlo regression. The left two subplots were generated by selecting actions uses these Monte Carlo distances. We computed the true distances by running Dijkstra's algorithm. The right two subplots show actions selected using Dijkstra's algorithm."}, {"title": "Tabular Maze Navigation: Figure 5 (Top)", "content": "This plot used the same environment as described in Appendix F.1. For this plot, we generated 3000 trajectories of length 50 using a random policy. Only 14% of start-goal pairs have any trajectory between them, meaning that the vast majority of start-goal pairs have never been seen together during training. Thus, this is a good setting for studying generalization.\nWe first estimated distances using Monte Carlo regression. We select actions using a Boltzmann policy with temperature 0.1 (i.e., $\u03c0(\u03b1 | s, g) \\propto e^{-0.1d(s,9)}$). Evaluation is done over 1000 randomly-sampled start-goal pairs. The X axis is binned based on the shortest path distance. The data are aggregated so that start-goal pairs with distance between (say) 20 and 30 get plotted at x = 30. The \"metric regression + quasimetric\u201d distances are obtained by performing path relaxation on these Monte Carlo distances until convergence. The corresponding policy is again a Boltzmann policy with temperature 0.1.\nFor the Top Right subplot, we perform planning using Dijkstra's algorithm. We first identify a set of candidate midpoint states where d(s, w) and d(w, g) are both within one unit of half the shortest path distance. We then randomly sample a midpoint state. This planning is done anew at every timestep."}, {"title": "Learned Maze Navigation: Figure 5 (Bottom)", "content": "This plot used the same environment as described in Appendix F.1. The CRL method refers to [56] and CMD refers to [5]. We used a representation dimension of 16, a batch size of 256, neural networks with 2 hidden layers of width 32 and Swish activations, y = 0.9, and Adam optimizer with learning rate 3 \u00b7 $10^{-3}$. The loss functions and architectures are based on those from [59].\nFor the Bottom Right subplot, we performed planning in the same way as for the Top Right subplot."}, {"title": "JaxGCRL Benchmark Environments", "content": "Ant (Figure 6): For this task we used a version of the Ant environment from Bortkiewicz et al. [59", "7)": "Both environments are modified versions of the AntMaze and Humanoid environments from Bortkiewicz"}]}