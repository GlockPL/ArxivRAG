{"title": "Channel Merging: Preserving Specialization for Merged Experts", "authors": ["Mingyang Zhang", "Jing Liu", "Ganggui Ding", "Linlin Ou", "Xinyi Yu", "Bohan Zhuang"], "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMS is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) such as LLaMA (Touvron et al. 2023) and Mistral (Jiang et al. 2023a) have significantly pushed the boundaries of artificial intelligence, achieving near-human performance across various general tasks. Despite these achievements, a notable performance gap remains in specialized domains such as coding and mathematics. Addressing this gap, many LLMs undergo task-specific fine-tuning to enhance their capabilities within these targeted areas (Zhou and Yuqi 2024; Yu et al. 2023; Wu et al. 2023). In multi-task scenarios, it is common to ensemble LLMs specialized in different tasks to optimize performance. However, as shown in Figure 1 (a), traditional ensembling methods (Tang et al. 2024; Lu et al. 2023) require loading all specialized models into GPU memory, which is highly storage-intensive.\nTo combat the inefficiency of the model ensemble, model merging strategies (Yadav et al. 2024; Yang et al. 2024) have been introduced, where delta parameters (Ilharco et al. 2023) from each expert are merged into the pre-trained parameters, allowing the system to load weights equivalent to a single expert during inference. Nevertheless, as shown in Figure 1 (b), this one-size-fits-all merging can exacerbate parameter conflicts as the number of experts increases, often leading to a decline in downstream performance. To mitigate parameter conflicts, previous approaches can be divided into two main strategies: (1) Post pruning, which prunes delta parameters the alteration of the model parameters before and after fine-tuning (Yu et al. 2024; Yadav et al. 2024). However, performance degradation still occurs as the number of merged experts increases. (2) Implementing partial merging (Jiang et al. 2023b), which merges task-agnostic parameters and maintains task-specific ones. Nonetheless, this strategy becomes less storage-efficient with the escalation in the number of experts, as it necessitates the retention of more separate parameters.\nTo effectively mitigate parameter conflicts while enhancing storage efficiency, we specifically analyze the layer-by-layer channel similarities between several experts and highlight that merging with finer granularity can further reduce parameter conflicts. Based on our analysis, we introduce a novel strategy called Channel Merging. As illustrated in Figure 1(c), this approach first clusters channel parameters that are across various experts into several groups based on their"}, {"title": "Related Work", "content": "Model merging. As large pre-trained models are a repository of extensive knowledge, fine-tuning them for new tasks has become a prevalent method (Dodge et al. 2020). Model merging, involving merging task-specific models fine-tuned from the same pre-trained model, has been increasingly recognized as an effective strategy to enhance generalization and multi-task capabilities in LLMs (Yadav et al. 2024; Yang et al. 2024; Yu et al. 2024; Matena and Raffel 2022a; Ilharco et al. 2023; Ainsworth, Hayase, and Srinivasa 2023; Entezari et al. 2022). Although model merging provides enhanced flexibility and utility (Daheim et al. 2024; Matena and Raffel 2022b), straightforward techniques like model averaging (Wortsman et al. 2022) often lead to substantial performance degradation across multiple tasks due to parameter conflicts. To address these conflicts, strategies such as TIES Merging (Yadav et al. 2024) and DARE (Yu et al. 2024) have been proposed, which involve pruning some delta parameters before the merging process. Nonetheless, parameter conflicts can still be stringent as the number of merged models increases. Another mitigation strategy is partial merging (Stoica et al. 2024; Kim et al. 2023; Jiang et al. 2023b), which involves merging only a part of the parameters while preserving others independently. For example, ZipIT (Stoica et al. 2024) selectively unmerges certain layers, effectively creating a multi-head model. Passthrough (Kim et al. 2023) concatenates layers from different LLMs, producing a deeper model. BYOM (Jiang et al. 2023b) preserves some task-specific parameters according to magnitude. However, these partial merging methods become storage-inefficient as the number of merged experts increases. In contrast,"}, {"title": "Preliminaries", "content": "Formulation of model merging. Assuming a pretrained model, let $P \\in \\mathbb{R}^{O \\times I}$ represent the parameters of a specific layer, where $O$ and $I$ correspond to the output and input channel number, respectively. From this model, we derive a set of $N$ task-specific models with parameters $0 = \\{0_1, 0_2, ..., 0_n\\} \\in \\mathbb{R}^{N \\times O \\times I}$, each fine-tuned for a distinct task. Model merging is the process of integrating the modifications of all task-specific models back into a single model. This is achieved by first calculating the delta parameters for each task-specific model, which represent the changes made during the fine-tuning process relative to the pretrained model. These delta parameters are defined as $S_t^n = 0_t^n - P$ for each task $n$. Using the task arithmetic method (Ilharco et al. 2023), the merged parameters, $0^* \\in \\mathbb{R}^{O \\times I}$, are computed as follows:\n$0^* = P + \\frac{\u03bb}{N} \\Sigma_n S_t^n, \u2200\u03b7,$ (1)\nwhere $\u03bb$ is a scaling factor that adjusts the influence of the delta parameters on the merged model.\nTask-specific routing. To address computational efficiency concerns within LLM ensembles, task-specific routing is employed. This technique strategically selects a single LLM expert, denoted as $m^*$, that is best suited to respond to a given query $q$. This selection is determined through a scoring function that evaluates the appropriateness of each expert for the query, formalized as follows:\n$m^* = argmax_{m \\in M} Z(q,m),$ (2)\nwhere $M$ represents the set of all available LLM experts, and $Z(\u00b7)$ is a function that scores each expert $m$ based on its predicted effectiveness in responding to query $q$. The expert with the highest score is chosen for the task, optimizing the ensemble's computational resources by activating only the most relevant model."}, {"title": "Method", "content": "In this section, we begin by analyzing the inconsistency in channel similarity across different expert models, illus-"}, {"title": "Similarity Inconsistency", "content": "Before we delve into Channel Merging, it's essential to address a fundamental question: Why merge at the channel level instead of the model level, as was common in previous methods? To answer this, we specifically analyze the layer-by-layer channel similarities between the Instruction expert and the other two experts-Code and Math. This analysis involves quantifying how many channels in the Instruction expert are more similar to the Code expert compared to the Math expert based on their cosine similarities. We employ cosine similarity to measure the similarity between parameters since high cosine similarity between neural network parameters correlates with similar activations produced by the layers (Mason-Williams and Dahlqvist 2024; Klabunde et al. 2023). The details of LLM candidates used for comparison can be found in Appendix.\nIn the Mistral-7B model family (Figure 2(a)), overall, the Instruction expert channels show closer similarity to the Code expert channels, with the proportion of more similar channels typically being higher. However, even in the layers where the similarity proportion peaks, more than 20% of Instruction channels are closer to the Math expert, highlighting significant similarity inconsistency between those experts. Moreover, the CodeLLaMA-7B model family (Figure 2(b)) exhibits more random variations in similarity. Approximately half of the channels in all layers show greater similarity to the Code expert, while the other half are more aligned with the Math expert. These experiments highlight"}, {"title": "Merging with Channel Similarity", "content": "To effectively merge multiple expert LLMs while accommodating significant variations in channel similarities, we introduce a novel method termed Channel Merging. As illustrated in Figure 3, our method unfolds in two parts: Channel-wise Merging and Instant Lookup.\nChannel-wise merging. In this stage, we cluster the delta parameters for each output channel from different experts based on cosine similarity. Specifically, for the i-th output channel, delta parameters across all experts ${\\delta_1^e, \\delta_2^e,..., \\delta_N^e\\} \\in \\mathbb{R}^{N \\times I}$ are grouped using the K-Means clustering algorithm into K clusters $C_1 = \\{C_1^1, C_1^2, ..., C_1^K\\} (K < N)$. This clustering ensures that each group contains parameters with high similarity, thus reducing conflicts during merging. These similar parameters within each cluster k are then merged with the task arithmetic (Ilharco et al. 2023):\n$\\Theta_i^k = P + \\frac{\u03bb}{N} \\Sigma S_t^n e C_k^i,$\n(3)\nRepeating the clustering and merging for all channels in the layer, we can obtain K groups new parameters $\\Theta^k \\in \\mathbb{R}^{K \\times O \\times I}$, where the k-th group parameter $\u0398^k$ can be represented as:\n$\\Theta_i^k \\in \\mathbb{R}^{O \\times I} = {\\Theta_i^{k_1}, \\Theta_i^{k_2}, ..., \\Theta_i^{k_O}\\},$\n(4)\nAdditionally, an index set $S_t^n = \\{S_{i1}^n,..., S_{iO}^n\\} \\in \\mathbb{R}^{1 \\times O}$, where $S_{in} \u2208 \\{1, ..., K\\}$, is maintained for each expert tn, indicating which group their channel parameters are merged into. It is worth noting that the Channel-wise Merging stage is executed offline, thus imposing no additional computational overhead during inference.\nInstant lookup. Channel Merging merges different channels from each expert into distinct groups, thereby preserving the"}, {"title": "Task-specific Routing", "content": "As shown in Figure 1(c), following the paradigm of previous LLM ensemble methods (Liu et al. 2024), we employ a task-specific router to determine which expert to activate and reconstruct for a given query. To train this router efficiently, we operate under the assumption that an expert will perform optimally on queries that originate from its fine-tuning dataset. To implement this, we sample a set of queries $Q$ from the datasets of various tasks, using the originating task classes (e.g., code, math, instruction, Chinese) as the label $Y$. The optimization process for training the router is then defined as follows:\n$Z^* = argmin_Z \\Sigma_{(q,y) \\in (Q,Y)} -y \\cdot log(Z(q,m)),$ (6)\nIn this equation, $Z(q, m)$ calculates the probability that expert m is the most suitable for handling query q, based on the learned task-specific affinities. Once this model is trained, the arrival of a new query triggers the task-specific router, which employs the optimized function $Z^*$ to determine which expert should be activated. This process ensures that each query is handled by the expert most likely to achieve the best performance, thereby enhancing overall efficiency and effectiveness."}, {"title": "Experiments", "content": "Experimental Setting\nTest datasets. To evaluate the performance of merging, we report accuracy on several benchmarks across different domains: CommonSenseQA (Talmor et al. 2019) and TriviaQA (Joshi et al. 2017) for the instruction, GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021b) for the mathematics, HumanEval (Chen et al. 2021) and MBPP (Austin et al. 2021) for the code, and CEval (Huang et al. 2024) and CMMLU (Li et al. 2023) for the Chinese. Besides, we evaluate the merged model with the task-specific router on several general task benchmarks: MMLU (Hendrycks et al. 2021a), CMMLU, and AGIEval (Zhong et al. 2024). We use the OpenCompass toolbox (Contributors 2023) to evaluate all datasets.\nImplementation details. For model merging, we cluster the expert weights into several groups. Subsequently, we use the commonly used model merging algorithms from MergeKit (Goddard et al. 2024) to merge the parameters in the same group: (1) DARE-CM, we randomly prune 30% of the delta parameters for each expert before merging. (2) TIES-CM, we prune 30% of the delta parameters based on their magnitude and sign for each expert before merging. A in Eq. (3)"}, {"title": "Main Results", "content": "Downstream tasks. To assess the necessity of channel-level merging, we conduct experiments across various downstream tasks to compare different merging methods, with and without the incorporation of channel merging. We use unmerged-downstream models as our baseline to establish a clear performance benchmark. The results, as shown in Table 2, highlight the effectiveness of channel-level merging. For example, in the Instruction Expert tasks, DARE-CM improves performance significantly, achieving an average score of 69.88%, compared to 62.89% with the model-level DARE. This represents a substantial 6.99% increase in performance, underscoring the reduced performance degradation offered by our channel-level approach. Similarly, in the Code Expert tasks, DARE-CM scored 44.01% on average, outperforming DARE by 7.31%. When comparing\nDARE-CM to BYOM, we observe that DARE-CM consistently outperforms BYOM across various metrics. For instance, in the Math Expert tasks, DARE-CM achieves an average score of 44.95%, compared to BYOM's 41.02%. Notably, our DARE-CM yields the most optimal results, maintaining performance levels close to or even surpassing the baseline in specialized tasks such as Instruction and Chinese tasks. These findings demonstrate that our method not only mitigates the performance loss but can also enhance the model's effectiveness in handling domain-specific queries. We also compare the performance metrics with and without the use of the trained router on DARE-CM. The results in Table 2 clearly demonstrate that the router's deployment maintains the effectiveness of our model.\nGeneral tasks. Given that the goal of model merging is to achieve a more versatile model, we integrate our merged models DARE-CM with a task-specific router and compare their performance on general tasks against the unmerged baseline models. As indicated in Table 3, these benchmarks encompass a variety of tasks including those in Chinese, English, mathematics, and coding. Consequently, Channel Merging combined with the router consistently outperforms the separate unmerged models. For example, Channel Merging exhibited a 1.26% higher average accuracy over the Chinese-Mistral-7B-Instruct-v0.1 in the AGIEval benchmark, and a 2.13% improvement in the combined MMLU+CMMLU benchmarks. Moreover, we find that DARE-CM can surpass BYOM in several benchmarks. For instance, in the AGIEval and MMLU+CMMLU benchmarks, DARE-CM + router achieves an average score of 38.01% and 54.42%, notably higher than the 37.62% and 37.62% of BYOM + router, respectively. When compared to the traditional model ensemble with the router, Channel Merging achieves comparable outcomes across all benchmarks, requiring only 53% of the parameter used by traditional methods."}, {"title": "Ablation Studies", "content": "Merging granularities. A key distinction of our Channel Merging approach compared to other merging methods lies in the granularity at which the merge is executed-specifically, at the channel level. To assess the impact of different merging granularities on model performance, we conducted an ablation study. This study involved merging operations at three different levels: Channel, Layer, and the entire Model, with subsequent performance evaluation on downstream tasks. As shown in Table 4, channel-level merging outperformed both layer-level and model-level merging across various tasks, achieving the highest average performance metrics. This suggests that finer granularity in merging helps to reduce parameter conflicts, thereby preserving more of the downstream task performance.\nThe sensitivity to the merged experts number. To explore how the number of experts influences the effectiveness of Channel Merging, we carry out experiments that assessed the performance across various downstream tasks when integrating varying numbers of expert models. We expand our set of candidate models to include Hercules-2.5-Mistral-7B and CollectiveCognition-v1.1-Mistral-7B, allowing for the integration of up to six experts. Additionally, we compare channel-level merging with results from model-level merging. The experimental results, as depicted in the figures, show that, in contrast to model-level merging, channel-level merging exhibits a markedly slower rate of performance degradation as the number of merged experts increases, particularly in tasks involving mathematic reasoning and code generation.\nThe effect of clustering methods. To validate the appropriateness of using the KMeans method for clustering channel parameters, we compare its impact on experimental results with two alternative clustering strategies: (1) Random, where channels are grouped randomly, and (2) Sign (Yadav et al. 2024), where parameters are grouped based on having the same sign. The results, as shown in Table 5, reveal that both KMeans and Sign clustering significantly outperform the random grouping method. This indicates that logically group parameters (either by minimizing intra-cluster variance in KMeans or aligning parameter signs) lead to better performance than arbitrary grouping."}, {"title": "Conclusion", "content": "In this paper, we introduced Channel Merging, a novel strategy designed to enhance the efficiency and performance of merging LLMs specialized in various tasks. By clustering and merging channel parameters based on their similarities, Channel Merging mitigates the parameter conflicts associated with traditional one-size-fits-all merging methods. Through extensive experiments, we have demonstrated that Channel Merging achieves comparable performance to unmerged experts in tasks such as English reasoning, mathematical reasoning, code generation, and Chinese reasoning. Additionally, when integrated with a task-specific router, Channel Merging outperforms traditional ensemble methods in general tasks while requiring only 53% of the parameters, showcasing significant improvements in both performance and storage efficiency.\nLimitation and future work. Channel Merging requires that the experts to be merged are fine-tuned from the same pretrained model. Additionally, compared to one-size-fits-all approaches, Channel Merging may increase the parameters of the merged model. In future work, we plan to explore further compression of the merged model's parameter size by setting different groups for each layer."}, {"title": "Detailed Experimental Setting", "content": "Candidate LLMs\nTo analyze the similarity relationship between different experts, we conduct experiments under Mistral-7B-v0.1 model family and CodeLLaMA model family. In Mistral-7B-v0.1 model family experiments, we use Dolphin-2.2.1-Mistral-7B as the instruction expert, MetaMath-Mistral-7B as the math expert and Speechless-Code-Mistral-7B as the code expert. In CodeLLaMA model family experiments, we use CodeLlama-7b-Instruct as the instruction expert, OpenMath-CodeLlama-7b-Python as the math expert and WizardCoder-Python-7B as the code expert.\nIn our main experiments, we apply our method to the Mistral-7B-v0.1 model family, including several specialized LLMs: Dolphin-2.2.1-Mistral-7B as the instruction expert, Speechless-Code-Mistral-7B as the code expert, MetaMath-Mistral-7B as the math expert, and Chinese-Mistral-7B-Instruct-v0.1 as the Chinese expert. These models are all fine-tuned derivatives of the foundational pretrained model Mistral-7B-v0.1. The details of each expert can be found in Table 7.\nTraining detail of task-specific router\nTo efficiently assign queries to the optimal expert, We utilize the bert-base-multilingual-cased (ber 2023) which is a tiny model with only 110M parameters as the task-specific router. To train the task-specific router, we collect 50k instruction samples from various open-source datasets and randomly select 50k samples to train the router, including Dolphin (Dol 2023) for the instruction domain, MetaMathQA (Met 2023) for the mathematics domain, WizardLM-evol-instruct-V2-196k (Wiz 2023b) for the code domain, and Wizard-LM-Chinese-instruct-evol (Wiz 2023a) for the Chinese domain. We use the AdamW optimizer with \u03b2\u2081 = 0.9 and B2 = 0.95 to train 1 epoch on a single A100GPU, setting the learning rate to 1 \u00d7 10-4, the batch-size to 128, and applying a linear learning rate warmup."}, {"title": "More Ablation Studies", "content": "The similarity between merged experts. We assess the channel similarities between different experts after merging. The similarity between experts is calculated by counting the number of channels where Stn values matched between two experts and then normalizing this count by the total number of channels. The results, depicted in Figure 5, show relatively low similarity scores across different experts. For instance, the Instruction and Math experts, despite having the highest similarity score in the matrix, still exhibit a moderate similarity of 0.77, suggesting that while there are shared characteristics, distinct features prevail. On the other end, the Code and Chinese experts have the lowest similarity of 0.41, indicating significant differences in their channel characteristics. These findings validate Channel Merging's ability to preserve expert uniqueness, effectively minimizing parameter conflicts and enhancing performance by ensuring\neach expert's specialized knowledge remains intact within the merged framework.\nMerging under different groups. In Channel Merging, we can categorize expert parameters into varying numbers of groups. We conduct experiments to explore the impact of the number of groups on the merged model's parameter count and performance. The results, as shown in Table 6, indicate that when the number of groups is set to one, Channel Merging functions similarly to DARE (Jiang, Ren, and Lin 2023). Although this setting minimizes the number of parameters required, it also yields the poorest performance. Conversely, when the number of groups is set to four, Channel Merging resembles a model ensemble, requiring the maximum number of parameters. Notably, setting the number of groups to two not only surpasses or closely matches the performance of having four groups across various downstream tasks but also significantly reduces the number of parameters needed.\nLatency analysis for Instant Lookup. To comprehensively analyze the cost associated with the lookup and concatenation processes during inference, we randomly selected 50 data points from the MMLU validation dataset to evaluate the time costs associated with the lookup and inference processes. The final reported times for lookup and inference are the average values derived from these 50 data points. The results given in Table 8 clearly show that the lookup and concatenation process accounts for a minimal portion of the total inference time.\nThe effectiveness of the similarity metric. In response to inquiries about our choice of cosine similarity as the metric for assessing parameter similarities, we conducted comprehensive experiments comparing various similarity metrics, including cosine similarity, Euclidean distance, and Manhattan distance. The experimental results given in Table 9 reveal that cosine similarity and Euclidean distance consistently outperformed Manhattan distance.\nPruning under different pruning ratios. As mentioned in Implementation, our approach enhances the existing channel merging methods. Previous implementations of channel"}]}