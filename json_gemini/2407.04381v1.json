{"title": "Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection", "authors": ["Zhiqiang Yang", "Qiu Guan", "Keer Zhao", "Jianmin Yang", "Xinli Xu", "Haixia Long", "Ying Tang"], "abstract": "Due to the effective performance of multi-scale feature fusion, Path Aggregation FPN (PAFPN) is widely employed in YOLO detectors. However, it cannot efficiently and adaptively integrate high-level semantic information with low-level spatial information simultaneously. We propose a new model named MAF-YOLO in this paper, which is a novel object detection framework with a versatile neck named Multi-Branch Auxiliary FPN (MAFPN). Within MAFPN, the Superficial Assisted Fusion (SAF) module is designed to combine the output of the backbone with the neck, preserving an optimal level of shallow information to facilitate subsequent learning. Meanwhile, the Advanced Assisted Fusion (AAF) module deeply embedded within the neck conveys a more diverse range of gradient information to the output layer. Furthermore, our proposed Re-parameterized Heterogeneous Efficient Layer Aggregation Network (RepHELAN) module ensures that both the overall model architecture and convolutional design embrace the utilization of heterogeneous large convolution kernels. Therefore, this guarantees the preservation of information related to small targets while simultaneously achieving the multi-scale receptive field. Finally, taking the nano version of MAF-YOLO for example, it can achieve 42.4% AP on COCO with only 3.76M learnable parameters and 10.51G FLOPs, and approximately outperforms YOLOv8n by about 5.1%. The source code of this work is available at: https://github.com/yang-0201/MAF-YOLO.", "sections": [{"title": "1 Introduction", "content": "To implement real-time object detection with high performance, a variety of algorithms have been developed in recent years. Among them, a series of YOLO algorithms [1, 8-11, 16-21, 23, 24], from YOLOv1 to YOLOv9, have played increasingly significant roles due to their compromise between speed and accuracy. However, a common shortcoming of YOLO series algorithms is the limitation of multi-scale feature fusion. Although the feature fusion mechanism of the Path Aggregation Feature Pyramid Network (PAFPN) [22], an improvement over the Feature Pyramid Network (FPN) [13], has been widely integrated into YOLO. This mechanism introduces a dual-path approach to enhance feature integration, thereby improving accuracy while also controlling computational costs. In Fig. 1(a), P3, P4, and P5 represent the output information of different levels of the backbone. The neck structure of the YOLO series utilizes a traditional PAFPN, which incorporates two main paths for multi-scale feature fusion. Nevertheless, PAFPN still possesses two significant limitations.\nFirstly, PAFPN tends to merge homogenous scale feature maps and lacks integrated processing and fusion of multi-scale information from different resolution layers. For instance, in Block1 of PAFPN, the input consists of the up-sampled P5 layer and the sibling P4 layer, which overlooks the importance of shallow low-level spatial information in the P3 layer. Similarly, in Block2, there is no direct fusion of the P2 layer, which contains crucial information about small targets. This deficiency persists in the last two blocks as well. Secondly, the architecture's strategy for the small target detection layer is formulated through a singular down-top pathway and two blocks, significantly impairing the model's proficiency in learning and representing minute object features. As shown in Fig. 1(b) and (c), YOLOv8n based on PAFPN exhibits lower activation capacity in different scale objects compared to the MAFPN proposed in this paper.\nThe main contributions of this paper are summarized as follows:\n1. We propose a new plug-and-play neck called Multi-Branch Auxiliary FPN (MAFPN) to achieve richer feature interaction and fusion. In MAFPN, Superficial Assisted Fusion (SAF) maintains shallow backbone information via bi-directional connectivity, enhancing the network's ability to detect small targets. Additionally, Advanced Assisted Fusion (AAF) enriches the gradient information of the output layer through multi-directional connec-"}, {"title": "2 Related works", "content": "2.1 Real-time object detectors\nThe task of object detection is to identify objects in specific scenes. While both two-stage [2, 15] and transformer-based [25,27] detectors achieve high accuracy, their complex structures often entail significant parameter and computational overhead, compromising real-time performance. Most real-time object detection networks employ single-stage methods, with the YOLO series being particularly prominent. PPYOLOE [23] and YOLOv6 [11] explore reparameterization techniques and adopted the Task Alignment Learning (TAL) [7] strategy in label assignment, significantly enhancing performance. YOLOv7 [20] proposes the ELAN scheme to optimize the Cross Stage Partial Network structure from YOLOv4 [1] and designs several trainable bag-of-freebies methods for lossless model optimization. YOLOv8 [9] combined and optimized the strategies of current advanced YOLO algorithms to achieve a better balance between speed and accuracy, which is widely used in the industry. The latest YOLOv9 [21] introduced the Generalized Efficient Layer Aggregation Network (GELAN) structure and the concept of Programmable Gradient Information (PGI) to address information bottleneck issues in the network. YOLOv9 boasts a highly efficient parameter utilization, achieving the SOTA of the current YOLO family.\n2.2 Multi-scale features fusion for object detection\nThe original idea behind the FPN was to enhance the multi-scale detection capability of the network by incorporating cross-scale connections and information exchange. Significant research has been dedicated to optimizing and extending the FPN structure to improve the efficiency of feature fusion. In YOLOv6-v3 [11], the Bidirectional concatenation (BIC) mechanism is used to better utilize the"}, {"title": "3 Methodology", "content": "3.1 Macroscopic architecture\nAs illustrated in Fig. 2, we break down the macroscopic architecture of a one-stage object detector into three main components: the backbone, neck, and head. In MAF-YOLO, the input image initially passes through the backbone, which consists of four stages: P2, P3, P4, and P5. We designed MAFPN as a neck structure. In the first bottom-up pathway, the SAF module is responsible for extracting multi-scale features from the backbone and performing preliminary assisted fusion at the shallow layers of the neck. Meanwhile, AAF collects gradient information from each layer through denser connections in the second top-down pathway, ultimately guiding the head to obtain diversified output information across three resolutions. Both of the aforementioned structures employ the RepHELAN module for feature extraction, which utilizes dynamically sized convolutional kernels to achieve adaptive receptive fields. Finally, the detection heads predict object bounding boxes and their corresponding categories based on feature maps at each scale to compute their loss.\n3.2 Global Heterogeneous Kernel Selection mechanism\nAn important factor contributing to the effectiveness of transformers is their self-attention mechanism, which performs query-key-value operations over a global or larger window scale. Similarly, large convolutional kernels capture both local and global features, and the use of moderately large convolutional kernels to increase the effective receptive field has been demonstrated in several works to be effective. Research conducted by Trident Network [12] suggests that networks with larger receptive fields are preferable for detecting larger objects, while inversely, smaller-scale targets benefit from smaller receptive fields. YOLO-MS [3] introduced the concept of Heterogeneous Kernel Selection (HKS) protocols. Employing an incremental convolutional kernel design of 3, 5, 7, and 9 in the backbone to balance performance and speed. Inspired by this work, we extend it to the Global Heterogeneous Kernel Selection (GHKS) mechanism, integrating the concept of heterogeneous large convolutional kernels throughout the entire MAF-YOLO architecture. In addition to the progressively increasing convolutional kernels in RepHELAN of the backbone, we also introduce large convolutional kernels of 5, 7, and 9 in MAFPN to adapt to the requirements of different resolutions, thus progressively obtaining multi-scale sensory field information.\n3.3 Multi-Branch Auxiliary FPN\nAccurate localization relies on detailed edge information from shallow networks, while precise classification requires deeper networks to capture coarse-grained information [18]. We believe that an effective FPN should support full and sufficient convergence of shallow and deep network information flows.\nSuperficial Assisted Fusion. Preserving shallow spatial information in the backbone is crucial for enhancing the detection capability of smaller objects. However, the information supplied by the backbone is relatively elementary and prone to interference. Therefore, we incorporate shallow information as assisted branches into the deeper network to ensure the stability of subsequent layer learn-ing. Following these principles, we have developed the SAF module, delineated"}, {"title": "3.4 Re-parameterized Heterogeneous Efficient Layer Aggregation Network", "content": "After designing the MAFPN structure in the preceding section, another challenge lies in efficiently designing the feature extraction block within the entire architecture. In this section, we present the design of a powerful encoder architecture, which efficiently learns expressive multi-scale feature representations.\nThe structure of RepHELAN is shown in Fig. 5(a). Initially, the input information undergoes a 1 \u00d7 1 convolution and a Split operation, resulting in two streams. One stream preserves the original information, which then directly enters the Concat operation, while the other stream undergoes processing through N Inverted Bottleneck units. Due to the mechanism of ELAN, the branches and the outputs passing through each Inverted Bottleneck are retained and eventually concatenated together. The specific structure of the Inverted Bottleneck is illustrated in Fig. 5(b), where the input sequentially passes through a 1 \u00d7 1 convolution to expand the number of channels, followed by a k\u00d7k RepHDWConv operation, and finally by 1\u00d71 point convolution to shrink the number of channels and compensate for the possible loss of information caused by DWConv.\nRe-parameterized Heterogeneous Depthwise Convolution Firstly, we employed Depthwise convolution with a large kernel in the global architecture to implement the aforementioned GHKS mechanism. Our study also indicates that while larger convolutional kernels may enhance performance by encoding more extensive regions, they might inadvertently obscure details relevant to small targets, thus leaving room for further improvement. Therefore, we transferred the heterogeneous idea from the global architecture to a single convolution and incorporated the idea of Re-parameterization [5,6] to realize RepHConv. Specifically, we complement the detection of small targets by concurrently running large"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. We conducted extensive experiments on the Microsoft COCO 2017 [14] dataset to validate the effectiveness of the proposed MAF-YOLO. Specifically, The training of all methods is conducted on the 115k training images and we report results on the 5000 validation images for the ablation study. We report the results of the standard mean average precision (AP) at various IoU thresholds and target scales.\nImplimentation details. Our implementation is based on the YOLOv6-2.0 framework. All experiments are conducted with 8 NVIDIA GeForce RTX 2080Ti GPUs, and all the scales of MAF-YOLO are trained from scratch for 300 epochs without relying on other large-scale datasets, like ImageNet [4], or pre-trained weights. In addition to this, we employ stronger dynamic cache-based mixup [26] and mosaic mechanisms and simply replace the two 3 \u00d7 3 convolutions in the YOLOv6 output header with the lightweight RepHDWConv. More implementation details can be found in the supplementary materials."}, {"title": "4.2 Analysis of RepHELAN", "content": "In this subsection, we will perform a series of ablation studies on the RepHELAN module. By default, we use the MAF-YOLO nano for all experiments.\nDifferent computational blocks. We first do ablation experiments of RepHELAN module with various computational blocks from other advanced YOLO models in Tab. 1. Our RepHELAN not only has a higher parameter utilization rate compared to other modules but also achieves higher accuracy.\nAblation study on RepHELAN. As seen in Tab. 2, we have performed an ablation study on the RepHELAN module, where LK represents whether the idea of large convolution kernels is used in Inverted Bottleneck. Each Bottleneck contains a 5 \u00d7 5 DWConv by default. When using large convolutional kernels, the network follows the GHSK strategy, employing 3 \u00d7 3, 5 \u00d7 5, 7 \u00d7 7, and 9 \u00d7 9 DWConvs across the RepHELAN of the architecture. First, we added the ELAN mechanic, which gives a 0.2% AP boost and increases the number of counters by a small amount. The third row in the table means that the RepHConv achieves a 0.4% performance improvement without the added overhead of employing a large convolutional kernel while maintaining the model size unchanged. Furthermore, using only large convolutional kernels and ELAN strategies leads to significant performance gains (+1% AP), albeit with a decrease in performance for small targets (-0.3% APs). Ultimately, when replacing the large DWConv with RepHConv, we achieved an optimal performance of 42.4% AP, with noticeable improvements across small, medium, and large object categories."}, {"title": "4.3 Analysis of MAFPN", "content": "In this subsection, we conducted ablation experiments on each module of MAFPN and demonstrated the plug-and-play capability of MAFPN by replacing the neck structure with different algorithms in various experiments.\nAblation study on MAFPN. The results of this experiment, as shown in Tab. 3, and the default neck of the model is set to PAFPN, which includes six RepHELAN Blocks. Firstly, we incorporated SAF modules into the shallow layers of the backbone and neck, which resulted in a 0.3% performance boost with an increase of 0.3M parameters and it's worth noting that through SAF, we achieved a 1% improvement in performance for small targets. Secondly, with the sole addition of the AAF module, we observed an enhancement in performance specifically for objects across all scales. Ultimately, the maximum performance of the model was obtained when the combination of SAF and AAF was used.\nAblation study on other models. MAFPN can be used as a plug-and-play module for other models and the results are listed in Tab. 4. Firstly, we replaced PAFPN with MAFPN in the mainstream single-stage detector YOLOv8n and changed the number of channels to keep the model smaller. YOLOv8n-MAFPN uses fewer epochs (-200 epochs) and fewer parameters and obtains a 2% AP improvement, reflecting the excellent performance of MAFPN. What's more, we also verified the effectiveness of MAFPN using the two-stage detector Cascade MaskRCNN [2] in the instance segmentation task."}, {"title": "4.4 Ablation study on MAF-YOLO", "content": "MAF-YOLO contains MAFPN, RepHELAN module, and GHSK strategy, we performed ablation experiments sequentially and the results are shown in Tab. 5. We first add the MAFPN structure, which increases the number of 0.5M parameters and improves the performance by 2.1% AP, then by adding the lightweight RepHELAN module, which reduces the number of parameters by 1.2M, the performance is instead improved by 1.1% AP, and finally, the GHSK method improves the model accuracy by 1.2% AP with marginal parameter costs."}, {"title": "4.5 Comparison with State-of-the-Arts", "content": "Tab. 6 and Fig. 6 demonstrate the comparison of our proposed MAF-YOLO with other SOTA real-time target detectors. In comparison to the nano-scale model, MAF-YOLOn has a slightly bigger number of parameters than YOLOv8n, but the AP is improved by 5.1%. Compared to the current newer Gold-YOLOn, MAF-YOLOn reduces about 36% of parameters and 13% of computation but still improves AP by 2.5%. Our model also has a big advantage for small-scale models, Compared to the anchor-free version of YOLOv7s, MAF-YOLOs has 22% fewer parameters and has a significant improvement of 2.3% AP. It is also noteworthy that our MAF-YOLOs achieved comparable results when compared to the current SOTA model YOLOv9s, which is 0.6 AP higher than the YOLOv9s with comparable parameters and calculations. In addition, we present several two-stage and transformer-based detectors where our model demonstrates superior performance and is more lightweight. Some detection results of different algorithms on the COCO validation set are shown in Fig. 7."}, {"title": "5 Conclusions", "content": "In this paper, we introduce MAFPN as a solution to address the limitations of PAFPN in traditional YOLO, which incorporates two key components: SAF and AAF. SAF is employed to effectively retain shallow information in the backbone, while AAF facilitates the output layer in retaining diverse multi-scale information through enhanced information fusion. Furthermore, we integrate GHSK into MAF-YOLO, which dynamically scales up the convolutional kernels throughout the architecture to significantly expand the sensory field of the network. Additionally, we introduce the RepHELAN module, which leverages reparameterized heterogeneous convolutions to greatly enhance the multi-scale characterization capability. As a result, MAF-YOLO demonstrates outstanding overall performance while maintaining a comparable number of parameters."}]}