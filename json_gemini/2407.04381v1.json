{"title": "Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection", "authors": ["Zhiqiang Yang", "Qiu Guan", "Keer Zhao", "Jianmin Yang", "Xinli Xu", "Haixia Long", "Ying Tang"], "abstract": "Due to the effective performance of multi-scale feature fusion, Path Aggregation FPN (PAFPN) is widely employed in YOLO detectors. However, it cannot efficiently and adaptively integrate high-level semantic information with low-level spatial information simultaneously. We propose a new model named MAF-YOLO in this paper, which is a novel object detection framework with a versatile neck named Multi-Branch Auxiliary FPN (MAFPN). Within MAFPN, the Superficial Assisted Fusion (SAF) module is designed to combine the output of the backbone with the neck, preserving an optimal level of shallow information to facilitate subsequent learning. Meanwhile, the Advanced Assisted Fusion (AAF) module deeply embedded within the neck conveys a more diverse range of gradient information to the output layer. Furthermore, our proposed Re-parameterized Heterogeneous Efficient Layer Aggregation Network (RepHELAN) module ensures that both the overall model architecture and convolutional design embrace the utilization of heterogeneous large convolution kernels. Therefore, this guarantees the preservation of information related to small targets while simultaneously achieving the multi-scale receptive field. Finally, taking the nano version of MAF-YOLO for example, it can achieve 42.4% AP on COCO with only 3.76M learnable parameters and 10.51G FLOPs, and approximately outperforms YOLOv8n by about 5.1%. The source code of this work is available at: https://github.com/yang-0201/MAF-YOLO.", "sections": [{"title": "1 Introduction", "content": "To implement real-time object detection with high performance, a variety of algorithms have been developed in recent years. Among them, a series of YOLO algorithms [1, 8-11, 16-21, 23, 24], from YOLOv1 to YOLOv9, have played increasingly significant roles due to their compromise between speed and accuracy. However, a common shortcoming of YOLO series algorithms is the limitation of multi-scale feature fusion. Although the feature fusion mechanism of the Path Aggregation Feature Pyramid Network (PAFPN) [22], an improvement over the Feature Pyramid Network (FPN) [13], has been widely integrated into YOLO. This mechanism introduces a dual-path approach to enhance feature integration, thereby improving accuracy while also controlling computational costs. In Fig. 1(a), P3, P4, and P5 represent the output information of different levels of the backbone. The neck structure of the YOLO series utilizes a traditional PAFPN, which incorporates two main paths for multi-scale feature fusion. Nevertheless, PAFPN still possesses two significant limitations.\nFirstly, PAFPN tends to merge homogenous scale feature maps and lacks integrated processing and fusion of multi-scale information from different resolution layers. For instance, in Block1 of PAFPN, the input consists of the up-sampled P5 layer and the sibling P4 layer, which overlooks the importance of shallow low-level spatial information in the P3 layer. Similarly, in Block2, there is no direct fusion of the P2 layer, which contains crucial information about small targets. This deficiency persists in the last two blocks as well. Secondly, the architecture's strategy for the small target detection layer is formulated through a singular down-top pathway and two blocks, significantly impairing the model's proficiency in learning and representing minute object features. As shown in Fig. 1(b) and (c), YOLOv8n based on PAFPN exhibits lower activation capacity in different scale objects compared to the MAFPN proposed in this paper.\nThe main contributions of this paper are summarized as follows:\n1. We propose a new plug-and-play neck called Multi-Branch Auxiliary FPN (MAFPN) to achieve richer feature interaction and fusion. In MAFPN, Superficial Assisted Fusion (SAF) maintains shallow backbone information via bi-directional connectivity, enhancing the network's ability to detect small targets. Additionally, Advanced Assisted Fusion (AAF) enriches the gradient information of the output layer through multi-directional connections. Furthermore, MAFPN can be seamlessly integrated into any other detector to enhance its multi-scale representation capability.\n2. We designed the lightweight Re-parameterized Heterogeneous Efficient Layer Aggregation Network (RepHELAN) module, which combines the concept of reparameterized heterogeneous large convolutions. This module expands the scope of perception by parallelizing a large kernel convolution with several small kernel convolutions without incurring additional inference costs, while preserving information about small objects.\n3. We propose a Global Heterogeneous Kernel Selection mechanism (GHSK), which adaptively enlarges the effective receptive field of the entire network by adjusting the kernel sizes in RepHELAN across different resolution feature layers in the network architecture.\n4. The Multi-Branch Auxiliary Fusion YOLO (MAF-YOLO) demonstrates superior object detection performance across various aspects on the MS COCO dataset, outperforming existing real-time object detectors."}, {"title": "2 Related works", "content": "The task of object detection is to identify objects in specific scenes. While both two-stage [2, 15] and transformer-based [25,27] detectors achieve high accuracy, their complex structures often entail significant parameter and computational overhead, compromising real-time performance. Most real-time object detection networks employ single-stage methods, with the YOLO series being particularly prominent. PPYOLOE [23] and YOLOv6 [11] explore reparameterization techniques and adopted the Task Alignment Learning (TAL) [7] strategy in label assignment, significantly enhancing performance. YOLOv7 [20] proposes the ELAN scheme to optimize the Cross Stage Partial Network structure from YOLOv4 [1] and designs several trainable bag-of-freebies methods for lossless model optimization. YOLOv8 [9] combined and optimized the strategies of current advanced YOLO algorithms to achieve a better balance between speed and accuracy, which is widely used in the industry. The latest YOLOv9 [21] introduced the Generalized Efficient Layer Aggregation Network (GELAN) structure and the concept of Programmable Gradient Information (PGI) to address information bottleneck issues in the network. YOLOv9 boasts a highly efficient parameter utilization, achieving the SOTA of the current YOLO family."}, {"title": "2.2 Multi-scale features fusion for object detection", "content": "The original idea behind the FPN was to enhance the multi-scale detection capability of the network by incorporating cross-scale connections and information exchange. Significant research has been dedicated to optimizing and extending the FPN structure to improve the efficiency of feature fusion. In YOLOv6-v3 [11], the Bidirectional concatenation (BIC) mechanism is used to better utilize the backbone shallow information, and DAMO-YOLO [24] employs Reparameterized Generalized-FPN (RepGFPN) for richer fusion in both backbone and neck. Gold-YOLO [19] introduces an advanced Gather and Distribute (GD) mechanism, which simultaneously integrates local and global information within the neck through convolutional and self-attention operations. These approaches significantly alleviate this issue, but further optimization opportunities remain."}, {"title": "3 Methodology", "content": "As illustrated in Fig. 2, we break down the macroscopic architecture of a one-stage object detector into three main components: the backbone, neck, and head. In MAF-YOLO, the input image initially passes through the backbone, which consists of four stages: P2, P3, P4, and P5. We designed MAFPN as a neck structure. In the first bottom-up pathway, the SAF module is responsible for extracting multi-scale features from the backbone and performing preliminary assisted fusion at the shallow layers of the neck. Meanwhile, AAF collects gradient information from each layer through denser connections in the second top-down pathway, ultimately guiding the head to obtain diversified output information across three resolutions. Both of the aforementioned structures employ the RepHELAN module for feature extraction, which utilizes dynamically sized convolutional kernels to achieve adaptive receptive fields. Finally, the detection heads predict object bounding boxes and their corresponding categories based on feature maps at each scale to compute their loss."}, {"title": "3.2 Global Heterogeneous Kernel Selection mechanism", "content": "An important factor contributing to the effectiveness of transformers is their self-attention mechanism, which performs query-key-value operations over a global or larger window scale. Similarly, large convolutional kernels capture both local and global features, and the use of moderately large convolutional kernels to increase the effective receptive field has been demonstrated in several works to be effective. Research conducted by Trident Network [12] suggests that networks with larger receptive fields are preferable for detecting larger objects, while inversely, smaller-scale targets benefit from smaller receptive fields. YOLO-MS [3] introduced the concept of Heterogeneous Kernel Selection (HKS) protocols. Employing an incremental convolutional kernel design of 3, 5, 7, and 9 in the backbone to balance performance and speed. Inspired by this work, we extend it to the Global Heterogeneous Kernel Selection (GHKS) mechanism, integrating the concept of heterogeneous large convolutional kernels throughout the entire MAF-YOLO architecture. In addition to the progressively increasing convolutional kernels in RepHELAN of the backbone, we also introduce large convolutional kernels of 5, 7, and 9 in MAFPN to adapt to the requirements of different resolutions, thus progressively obtaining multi-scale sensory field information."}, {"title": "3.3 Multi-Branch Auxiliary FPN", "content": "Accurate localization relies on detailed edge information from shallow networks, while precise classification requires deeper networks to capture coarse-grained information [18]. We believe that an effective FPN should support full and sufficient convergence of shallow and deep network information flows."}, {"title": "Superficial Assisted Fusion", "content": "Preserving shallow spatial information in the backbone is crucial for enhancing the detection capability of smaller objects. However, the information supplied by the backbone is relatively elementary and prone to interference. Therefore, we incorporate shallow information as assisted branches into the deeper network to ensure the stability of subsequent layer learning. Following these principles, we have developed the SAF module, delineated in Fig. 3. The primary objective of SAF is to integrate deep-level information with features from the same hierarchical level and high-resolution shallow layers within the backbone, aiming to preserve abundant localization details to enhance the spatial representation of the network. Additionally, we utilize 1 \u00d7 1 convolutions to control the number of channels in shallow layer information, ensuring it occupies a smaller proportion during the concat operation without affecting subsequent learning. Let $P_{n-1}$, $P_n$ and $P_{n+1} \\in \\mathbb{R}^{H \\times W \\times C}$ represent the feature maps at different resolutions, where $P'_n$, $P_n$ and $P'_n$ denote the feature layers of the backbone, and the two paths of the MAFPN. The notation $U(.)$ signifies the up-sampling operation. Down denotes a 3 \u00d7 3 downsampling convolution accompanied by a batch normalization layer, and $\\delta$ represents a silu function, C represents the 1 \u00d7 1 convolution of the number of control channels. The output result after applying SAF is as follows:\n$P'_n = concat(\\delta(C(Down(P_{n-1}))), P_n, U(P_{n+1}))$\t\t\t\t\t\t\t\t\t\t(1)"}, {"title": "Advanced Assisted Fusion", "content": "To further enhance the interactive utilization of feature layer information, we employ the AAF module in the deeper layers of the MAFPN for multi-scale information integration. Specifically, Fig. 4 illustrates the AAF connections in $P''_n$, which involve information aggregation across the shallow high-resolution layer $P'_{n+1}$, the shallow low-resolution layer $P''_{n-1}$, the sibling shallow layer $P'_n$, and the previous layer $P''_{n-1}$. At this moment, the final output layer P4 can merge information from four different layers simultaneously, thereby significantly enhancing the performance of medium-sized targets. AAF also employs 1 \u00d7 1 convolutional control channels to regulate the impact of each layer on the outcome. Through experimentation, we found that when the strategy in SAF is used, i.e., the number of channels in the three shallow layers is set to half of the number of channels in the deeper layers, which in turn results in a slight degradation of performance. Drawing from the conventional single-path architecture of the FPN, we postulate that the initial guiding information is already embedded within the shallow layers of the MAFPN. Consequently, we equalize the number of channels across each layer to ensure the model obtains diverse outputs. The output result after applying AAF is as follows:\n$P''_n = concat(\\delta(C(Down(P_{n-1})), \\delta(C'(Down(P''_{n-1}))), P_n, C(U(P_{n+1}))))$\t\t\t\t\t\t\t\t\t\t(2)"}, {"title": "3.4 Re-parameterized Heterogeneous Efficient Layer Aggregation Network", "content": "After designing the MAFPN structure in the preceding section, another challenge lies in efficiently designing the feature extraction block within the entire architecture. In this section, we present the design of a powerful encoder architecture, which efficiently learns expressive multi-scale feature representations. The structure of RepHELAN is shown in Fig. 5(a). Initially, the input information undergoes a 1 \u00d7 1 convolution and a Split operation, resulting in two streams. One stream preserves the original information, which then directly enters the Concat operation, while the other stream undergoes processing through N Inverted Bottleneck units. Due to the mechanism of ELAN, the branches and the outputs passing through each Inverted Bottleneck are retained and eventually concatenated together. The specific structure of the Inverted Bottleneck is illustrated in Fig. 5(b), where the input sequentially passes through a 1 \u00d7 1 convolution to expand the number of channels, followed by a k\u00d7k RepHDWConv operation, and finally by 1\u00d71 point convolution to shrink the number of channels and compensate for the possible loss of information caused by DWConv.\nRe-parameterized Heterogeneous Depthwise Convolution Firstly, we employed Depthwise convolution with a large kernel in the global architecture to implement the aforementioned GHKS mechanism. Our study also indicates that while larger convolutional kernels may enhance performance by encoding more extensive regions, they might inadvertently obscure details relevant to small targets, thus leaving room for further improvement. Therefore, we transferred the heterogeneous idea from the global architecture to a single convolution and incorporated the idea of Re-parameterization [5,6] to realize RepHConv. Specifically, we complement the detection of small targets by concurrently running large and small convolutional kernels. Different sizes of convolution kernels enhance both the network's ERF and the diverse representation of features. As shown in Fig. 5(b), the Inverted Bottleneck exhibits certain differences between training and inference. During training, the network runs n parallel depthwise convolutional (DWConv) operations of varying sizes, while during inference, these convolutions are merged into one, resulting in no decrease in inference speed. We believe that RepHDWConv is a superior convolutional strategy that enhances the representation capability across multiple scales with minimal loss.\nSteps for reparameterizing a 7 \u00d7 7 RepHDWConv are shown in Fig. 5(c). Firstly, a k\u2081 \u00d7 k\u2081 large DWConv and many k2 \u00d7 k2 small DWConv will be parallelized in a RepHDWConv, Each DWConv is followed by a batch normalization (bn) layer and the parameters of each convolution kernel will be merged with the parameters of its corresponding bn layer. In the second step, through a procedure akin to padding, these small DWConvs are assimilated into a larger DWConv, followed by re-parameterization. The parameters and biases of these heterogeneous DWConvs are accumulated to form a new RepHDWConv. Let I indicate input feature maps, $K_n$, and $B_n$ show the weight and bias of the convolution with a n\u00d7n kernel. The resulting output feature map O is:\n$O = I * (K_{2n-1} + \\sum_{i=1}^m K_{2n-(2i+1)}) + (B_{2n-1} + \\sum_{i=1}^m B_{2n-(2i+1)})$   \t\t\t\t\t\t\t\t\t(3)\nwhere n \u2265 3 and m is the largest integer such that 2n - (2m + 1) \u2265 3."}, {"title": "4 Experiments", "content": "Datasets. We conducted extensive experiments on the Microsoft COCO 2017 [14] dataset to validate the effectiveness of the proposed MAF-YOLO. Specifically, The training of all methods is conducted on the 115k training images and we report results on the 5000 validation images for the ablation study. We report the results of the standard mean average precision (AP) at various IoU thresholds and target scales.\nImplimentation details. Our implementation is based on the YOLOv6-2.0 framework. All experiments are conducted with 8 NVIDIA GeForce RTX 2080Ti GPUs, and all the scales of MAF-YOLO are trained from scratch for 300 epochs without relying on other large-scale datasets, like ImageNet [4], or pre-trained weights. In addition to this, we employ stronger dynamic cache-based mixup [26] and mosaic mechanisms and simply replace the two 3 \u00d7 3 convolutions in the YOLOv6 output header with the lightweight RepHDWConv. More implementation details can be found in the supplementary materials."}, {"title": "4.2 Analysis of RepHELAN", "content": "In this subsection, we will perform a series of ablation studies on the RepHELAN module. By default, we use the MAF-YOLO nano for all experiments.\nDifferent computational blocks. We first do ablation experiments of RePELAN module with various computational blocks from other advanced YOLO models in Tab. 1. Our RepHELAN not only has a higher parameter utilization rate compared to other modules but also achieves higher accuracy."}, {"title": "Ablation study on RepHELAN", "content": "As seen in Tab. 2, we have performed an ablation study on the RepHELAN module, where LK represents whether the idea of large convolution kernels is used in Inverted Bottleneck. Each Bottleneck contains a 5 \u00d7 5 DWConv by default. When using large convolutional kernels, the network follows the GHSK strategy, employing 3 \u00d7 3, 5 \u00d7 5, 7 \u00d7 7, and 9 \u00d7 9 DWConvs across the RepHELAN of the architecture. First, we added the ELAN mechanic, which gives a 0.2% AP boost and increases the number of counters by a small amount. The third row in the table means that the RepHConv achieves a 0.4% performance improvement without the added overhead of employing a large convolutional kernel while maintaining the model size unchanged. Furthermore, using only large convolutional kernels and ELAN strategies leads to significant performance gains (+1% AP), albeit with a decrease in performance for small targets (-0.3% APs). Ultimately, when replacing the large DWConv with RepHConv, we achieved an optimal performance of 42.4% AP, with noticeable improvements across small, medium, and large object categories."}, {"title": "4.3 Analysis of MAFPN", "content": "In this subsection, we conducted ablation experiments on each module of MAFPN and demonstrated the plug-and-play capability of MAFPN by replacing the neck structure with different algorithms in various experiments.\nAblation study on MAFPN. The results of this experiment, as shown in Tab. 3, and the default neck of the model is set to PAFPN, which includes six RepHELAN Blocks. Firstly, we incorporated SAF modules into the shallow layers of the backbone and neck, which resulted in a 0.3% performance boost with an increase of 0.3M parameters and it's worth noting that through SAF, we achieved a 1% improvement in performance for small targets. Secondly, with the sole addition of the AAF module, we observed an enhancement in performance specifically for objects across all scales. Ultimately, the maximum performance of the model was obtained when the combination of SAF and AAF was used."}, {"title": "Ablation study on other models", "content": "MAFPN can be used as a plug-and-play module for other models and the results are listed in Tab. 4. Firstly, we replaced PAFPN with MAFPN in the mainstream single-stage detector YOLOv8n and changed the number of channels to keep the model smaller. YOLOv8n-MAFPN uses fewer epochs (-200 epochs) and fewer parameters and obtains a 2% AP improvement, reflecting the excellent performance of MAFPN. What's more, we also verified the effectiveness of MAFPN using the two-stage detector Cascade MaskRCNN [2] in the instance segmentation task."}, {"title": "4.4 Ablation study on MAF-YOLO", "content": "MAF-YOLO contains MAFPN, RepHELAN module, and GHSK strategy, we performed ablation experiments sequentially and the results are shown in Tab. 5. We first add the MAFPN structure, which increases the number of 0.5M parameters and improves the performance by 2.1% AP, then by adding the lightweight RepHELAN module, which reduces the number of parameters by 1.2M, the performance is instead improved by 1.1% AP, and finally, the GHSK method improves the model accuracy by 1.2% AP with marginal parameter costs."}, {"title": "4.5 Comparison with State-of-the-Arts", "content": "Tab. 6 and Fig. 6 demonstrate the comparison of our proposed MAF-YOLO with other SOTA real-time target detectors. In comparison to the nano-scale model, MAF-YOLOn has a slightly bigger number of parameters than YOLOv8n, but the AP is improved by 5.1%. Compared to the current newer Gold-YOLOn, MAF-YOLOn reduces about 36% of parameters and 13% of computation but still improves AP by 2.5%. Our model also has a big advantage for small-scale models, Compared to the anchor-free version of YOLOv7s, MAF-YOLOs has 22% fewer parameters and has a significant improvement of 2.3% AP. It is also noteworthy that our MAF-YOLOs achieved comparable results when compared to the current SOTA model YOLOv9s, which is 0.6 AP higher than the YOLOv9s with comparable parameters and calculations. In addition, we present several two-stage and transformer-based detectors where our model demonstrates superior performance and is more lightweight."}, {"title": "5 Conclusions", "content": "In this paper, we introduce MAFPN as a solution to address the limitations of PAFPN in traditional YOLO, which incorporates two key components: SAF and AAF. SAF is employed to effectively retain shallow information in the backbone, while AAF facilitates the output layer in retaining diverse multi-scale information through enhanced information fusion. Furthermore, we integrate GHSK into MAF-YOLO, which dynamically scales up the convolutional kernels throughout the architecture to significantly expand the sensory field of the network. Additionally, we introduce the RepHELAN module, which leverages reparameterized heterogeneous convolutions to greatly enhance the multi-scale characterization capability. As a result, MAF-YOLO demonstrates outstanding overall performance while maintaining a comparable number of parameters."}]}