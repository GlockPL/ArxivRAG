{"title": "LaRa: Efficient Large-Baseline Radiance Fields", "authors": ["Anpei Chen", "Haofei Xu", "Stefano Esposito", "Siyu Tang", "Andreas Geiger"], "abstract": "Radiance field methods have achieved photorealistic novel\nview synthesis and geometry reconstruction. But they are mostly applied\nin per-scene optimization or small-baseline settings. While several recent\nworks investigate feed-forward reconstruction with large baselines by\nutilizing transformers, they all operate with a standard global attention\nmechanism and hence ignore the local nature of 3D reconstruction. We\npropose a method that unifies local and global reasoning in transformer\nlayers, resulting in improved quality and faster convergence. Our model\nrepresents scenes as Gaussian Volumes and combines this with an image\nencoder and Group Attention Layers for efficient feed-forward reconstruc-\ntion. Experimental results demonstrate that our model, trained for two\ndays on four GPUs, demonstrates high fidelity in reconstructing 360\u00b0\nradiance fields, and robustness to zero-shot and out-of-domain testing.", "sections": [{"title": "1 Introduction", "content": "The ability to reconstruct the shape and appearance of objects from multi-view\nimages has long been one of the core challenges for computer vision and graphics.\nModern 3D reconstruction techniques achieve impressive results with various\napplications in visual effects, e-commerce, virtual and augmented reality, and\nrobotics. However, they are limited to small camera baselines or dense image\ncaptures [8, 32, 42, 66]. In recent years, the computer vision community has made\ngreat strides towards high-quality scene reconstruction. In particular, Structure-\nfrom-Motion [51, 55] and multi-view stereo [23, 72] emerged as powerful 3D\nreconstruction methods. They identify surface points by aggregating similarities\nbetween point features queried from source images, and are able to reconstruct\nhighly accurate surface and texture maps.\nDespite these successes, geometry with view-consistent textures is not the only\naspect required in applications of 3D reconstruction. The reconstruction process\nshould also be able to recover view-dependent appearance. To this end, neural\nradiance fields [42] and neural implicit surfaces [47, 74] investigate volumetric\nrepresentations that can be learned from multi-view captures without explicit\nfeature matching. Their follow-ups [2, 5, 21, 32, 44, 57, 64, 76, 77] improve efficiency\nand quality, but mostly require per-scene optimization and dense multi-view\nsupervision.\nSeveral recent works thus investigate feed-forward models for radiance field\nreconstruction while relaxing the dense input view requirement. While feed-\nforward designs vary, they commonly utilize local feature matching [8, 13, 30, 37, 70],\nwhich however limits them to small-baseline reconstruction, since feature matching\ngenerally relies on substantial image overlap and reasonably similar viewpoints.\nGeometry-aware transformers [34, 43, 49, 62] have also been adapted to address\nlarge-baseline problems, but they often suffer from blurry reconstructions due\nto the lack of 3D inductive biases. Recent large reconstruction models [26, 36]\nlearn the internal perspective relationships through context attention, enabling\nlarge-baseline reconstruction. However, the transformers are unaware of epipolar\nconstraints, and instead are tasked to implicitly learn spatial relationships, which\nrequires substantial data and GPU resources.\nIn this work, we present LaRa, a feed-forward reconstruction model without\nthe requirement of heavy training resources for the task of 360\u00b0 bounded radiance\nfields reconstruction from unstructured few-views. The core idea of our work is\nto progressively and implicitly perform feature matching through a novel volume\ntransformer. We propose a Gaussian volume as 3D representation, in which each\nvoxel comprises a set of learnable Gaussian primitives. To obtain the Gaussian\nvolume from image conditions, we progressively update a learnable embedding\nvolume by querying features in 3D. Specifically, we utilize a DINO image feature\nencoder to obtain image tokens and lift 2D tokens to 3D by unprojecting them\nto a shared canonical space. Next, we propose a novel Group Attention Layer\narchitecture to enable local and global feature aggregation. Specifically, we\ndivide dense volumes into local groups and only apply attention within each\ngroup, inspired by standard feature point matching. The grouped features and\nembeddings are fed to a cross-attention sub-layer to implicitly match features\nbetween feature groups of the feature volume and embedding volume, which is\nfollowed by a 3D CNN layer to efficiently share information across neighboring\ngroups. After passing through all attention layers, the volume transformer outputs\na Gaussian volume, and is then decoded as 2D Gaussian [27] parameters using\na coarse-to-fine decoding process. By incorporating efficient rasterization, our\nmethod achieves high-resolution renderings.\nWe demonstrate our method's efficiency and robustness for providing photore-\nalistic, 360\u00b0 novel view synthesis results using only four input images. We find that\nour model achieves zero-shot generalization to significantly out-of-distribution\ninputs. Moreover, our reconstructed radiance fields allow high-quality mesh re-\nconstruction using off-the-shelf depth-map fusion algorithms. Finally, our model\nachieves high-quality reconstruction results using only 4 A100-40G GPUs within\na span of 2 days."}, {"title": "2 Related Work", "content": "Multi-view stereo. Multi-view stereo reconstruction aims to generate detailed\n3D models by reasoning from images captured from multiple viewpoints, which has\nbeen studied for decades [14, 22, 25, 33, 35, 50, 52]. In recent years, multi-view stereo\nnetworks [28, 72] have been proposed to address MVS problems. MVSNet [72]\nutilizes a 3D Convolutional Neural Network for processing a cost volume. This\ncost volume is created by aggregating features from a set of adjacent views,\nemploying the plane-sweeping technique from a reference viewpoint, facilitating\ndepth estimation and enabling superior 3D reconstructions. Subsequent research\nhas built on top of this foundation, incorporating strategies such as iterative\nplane sweeping [73], point cloud enhancement [9], confidence-driven fusion [41],\nand the usage of multiple cost volumes [12, 24] to further refine reconstruction\naccuracy. However, all of these works require large image overlap for faithful\nfeature matching.\nFew-shot Radiance fields. The Radiance field representation [42] has rev-\nolutionized the reconstruction field, emerging as a promising replacement for\ntraditional reconstruction methods. Despite the promising achievement in per-\nscene sparse view reconstruction [6, 7, 16, 46, 53, 56, 60, 63, 68], training a feed-\nforward radiance field predictor [8, 13, 66, 76] has gained popularity. MVSNeRF [8]\nproposed to combine a cost volume with volume rendering, allowing appear-\nance and geometry reconstruction only using a photometric loss. The following\nworks [10, 30, 37, 70] are proposed to advance reconstruction quality and efficiency.\nSimilarly to standard MVS methods, they are limited to small camera baselines.\nRecently, several works have explored feed-forward models for few-shot [1,\n4, 11, 19, 36, 43, 67] input by capitalizing on large-scale training datasets and\nmodel sizes. They leverage cross-view attention to globally reason about 3D\nscenes and output 3D representation (e.g., tri-plane, IB-planes) for radiance field\nreconstruction. Concurrent work by LGM [59] and GRM [75] introduces few-shot\n3D reconstruction models that produce high-resolution 3D Gaussians using a\ntransformer framework. While these methods achieve impressive visual results,\ntraining becomes expensive and less practical for the academic community. Unlike\nsome recent single view reconstruction methods [26, 58], our work focuses on\nfew-shot (> 1) reconstruction since single-view input can be efficiently lifted to\nmulti-view by multi-view generative models [38, 39, 54, 71]."}, {"title": "3 LaRa: Large-baseline Radiance Fields", "content": "Our goal is to reconstruct the geometry and view-dependent appearance of\nbounded scenes from sparse input views using limited training resources. Given\nM images $I=(I_1, . . . , I_M)$ with camera parameters $\\pi= (\\pi_1,..., \\pi_\\mu)$, our method\nreconstructs radiance fields as a collection of 2D Gaussians, which is used to\nsynthesize novel views and extract meshes. Our model is a function $f$ of a discrete\nradiance field of voxel positions $v$ and outputs a Gaussian volume $V_G$\n$V_G = \\{G^k_i\\}^K_{i=1} = f(v; I, \\pi),$ (1)\nwhere $G_k$ represents the primitives within ith voxel, and $k$ is the index of $K$\nprimitives. The output Gaussian volume $V_G$ can be utilized for decoding into\nradiance fields. Our work considers sparse input views, in which the camera\nrotates around a bounded region within a hemisphere. Our approach is designed\nto handle unstructured views and is flexible to accommodate various numbers of\nviews (see supplementary material). Figure 1 shows an overview of our method.\nIn the following, we first describe how we model objects using Gaussian\nVolumes, in which each voxel stores multiple Gaussian primitives (Section 3.1).\nNext, we introduce how to infer the primitive parameters from multi-view inputs\n(Section 3.2). For rendering, we explore a coarse-fine decoding process to enable\nefficient rendering with rich texture details (Section 3.3). Finally, we discuss how\nwe train our model from large-scale image collections (Section 3.4).\n3.1 3D Representation\nWe utilize a 3D voxel grid as our 3D representation, consisting of 3 volumes: an\nimage feature volume $V_f$ to model image conditions, an embedding volume $V_e$\ndescribes 3D prior learned from data, and a Gaussian volume $V_G$ represents the\nradiance field.\nImage feature volume. We construct a feature volume for each input view by\nlifting the 2D image features to a canonical volume defined in the center of the"}, {"title": "3.2 Volume Transformer", "content": "To predict the Gaussian volume, we propose a volume transformer architecture to\nperform attention between volumes. Self-attention and cross-attention modules,\n$V^{g,j} = GroupCrossAttn (LN (V^{g,j-1}),V_f) + V^{g,j-1},$ (2)\n$V^{g,j} = MLP (LN (V^{g,j} )) + V^{g,j-1},$ (3)\n$V^{i+1} = 3DCNN (LN (V_e)) + V^{i}_{e}.$ (4)\nTo incorporate information from multiple views, we flatten and concatenate\nthe image feature tokens from multi-view feature volumes. It is important to note"}, {"title": "3.3 Coarse-Fine Decoding", "content": "We obtain 2D Gaussian primitive shape and appearance parameters from the\nGaussian volume, so we introduce a coarse-fine decoding process to better recover\ntexture details. Instead of using a single network and sampling scheme to reason\nabout the scene, we simultaneously optimize two decoding modules: one \"coarse\"\nand one \"fine\".\nFor the \"coarse\" decoding module, we feed Gaussian volume features to\na lightweight MLP and output a set of $K$ Gaussian parameters per voxel. We\nemploy the efficient 2D splatting technique [27] to form high-resolution renderings,\nincluding RGB, depth, opacity, and normal maps. During training, we render $M$\ninput views and $M$ novel views for supervision.\nSpecifically, we project the primitive centers $p$ onto the coarse renderings\n(i.e., RGB image $\\hat{I}$, depth image $\\hat{D}$, and accumulation alpha map $\\hat{A}$) to contain\nthe coarse renderings for each primitive using the camera poses $\\pi$,\n$\\chi_p = (I_p, \\hat{I_p}, D_p, \\hat{D_p}, A_p, \\hat{A_p}) = \\Phi (P (p, \\pi), [I, \\hat{I}, \\hat{D}, \\hat{A}]),$ (5)\nwhere $P$ denotes the point projection, $\\oplus$ is a concatenation operation along the\nchannel dimension, and $\\Phi$ is a bilinear interpolation in 2D space.\nIn practice, the depth features can change significantly across different scenes.\nTo mitigate scaling discrepancies, we replace the rendering depth $D_p$ with a\ndisplacement feature $\\mathcal{D}_{p\\hat{p}}$ that compares the rendered depth for input\nviews and the depth $z_{\\hat{p_e}}$ of a primitive, allowing for occlusion-aware reasoning.\nWe then apply a point-based cross-attention layer to establish relationships\nbetween the features of a point $\\chi_p$ and the primitive voxel. The results of this\ncross-attention process are then fed into an MLP, which is tasked with predicting\nthe residual spherical harmonics\n$SH^{residuals}_{i,k} = MLP (CrossAttn (\\chi_p, V_i)),$ (6)\n$SH^{fine}_{i,k} = SH^{coarse}_{i,k} + SH^{residuals}_{i,k}.$ (7)\nIntuitively, the \"fine\" decoding module attempts to learn a geometry-aware texture\nblending process based on multi-view images, primitive features, and rendering\nbuffers from the coarse module. Furthermore, both coarse and fine modules are\ndifferentiable and updated simultaneously. Thus, the fine renderings can further\nregularize the coarse predictions.\nSplatting. Our work takes advantage of Gaussian splatting [27, 32] to facilitate\nefficient high-resolution image rendering. We follow the original rasterization\nprocess and further output depth and normal maps by integrating the $z$ value\nand the normal of the primitives."}, {"title": "3.4 Training", "content": "Our LaRa is optimized across scenes via gradient descent, minimizing simple\nimage reconstruction objectives between the coarse and fine renderings (i.e., $\\hat{I}$)\nand the ground-truth images (i.e., $I$),\n$\\mathcal{L} = \\mathcal{L}_{MSE}(I, \\hat{I}) + \\mathcal{L}_{SSIM}(I, \\hat{I}) + \\mathcal{L}_{Reg},$ (8)\nwhere $\\mathcal{L}_{MSE}$ is the pixel-wise L2 loss, $\\mathcal{L}_{SSIM}$ is the structural similarity loss, which\nare applied on both coarse and fine RGB outputs.\nSpecifically, we concentrate the weight distribution along the rays by minimiz-\ning the distance between the ray-primitive intersections, inspired by Mip-NeRF [2].\nGiven a ray u(x) of pixel x, we obtain its distortion loss by,\n$\\mathcal{L}_d = \\sum_{i,j} w_i w_j|z_i - z_j|,$ (9)\nwhere $w_i = a_i G_i(u(x)) \\prod^{i-1}_{j=1}(1 - a_j G_j(u(x)))$ is the blending weight of the i-th\nintersection and $z_i$ the depth of the intersection point.\nAs 2D Gaussians explicitly model the primitive normals, we can align their\nnormals $n_i$ with the normals N derived from the depth maps via the loss\n$\\mathcal{L}_n = \\sum w_i(1 - n_i \\cdot N).$ (10)\nTherefore, our regularization term for the ray u(x) is given by $\\mathcal{L}_{Reg} = \\gamma_d\\mathcal{L}_d + \\gamma_n\\mathcal{L}_n$.\nWe set $\\gamma_d = 1000$ and $\\gamma_n = 0.2$ in our experiments."}, {"title": "4 Implementation Details", "content": "We briefly discuss our implementation, including the training and evaluation\ndataset, network design, optimizer, and mesh extraction.\nDatasets. We train our model on multi-view synthetic renderings of objects [69],\nbased on the Objaverse dataset [15], which includes 264,775 scenes with a\ntrain/test split of 10:1. Each scene contains 38 circular views with an image\nresolution of 512x 512. To ensure sufficient angular coverage of the input views,\nwe employ the classical K-means algorithm to cluster the cameras into 4 clusters.\nDuring training, we randomly choose two views from each cluster for every\niteration, in which the first 4 images share the same camera poses as the input\nviews, while the remaining 4 images are novel view outputs. We employ the eight\noutput images for supervision and leverage the loss objectives outlined in Eq. 8\nto update the network.\nWe present our in-domain evaluation using the Objaverse dataset's test set,\nconsisting of 26,478 scenes. To assess our model's cross-domain applicability, we\nconducted tests on the Google Scanned Objects dataset [18], which contains\n1,030 scans of real objects, and on the 46 hydrants and 90 teddy bears from the\nCo3D test set [49], totaling 136 objects. To examine our model's performance\non zero-shot reconstruction task, we use the generative multi-view dataset from\nInstant3D [36], which comprises 122 scenes generated from text prompts.\nNetwork. We developed LaRa using PyTorch Lightning [20] and conduct our\ntraining on 4 NVIDIA A100-40G GPUs over a period of 2 days for the fast\nmodel and 3.5 days for the base model, with a batch size of 2 per GPU. We use\nDINO-base for encoding $M=4$ multi-view images at a resolution of 512 \u00d7 512.\nWe use a volume resolution of $W = 16$ with $C=768$ channels for the image feature\nvolume, and a resolution of $W = 32$ with $C = 256$ channels for the embedding\nvolume, dividing both into $G = 16$ groups for the group attention layers. Our\ngroup attention network consists of 12 layers, producing a Gaussian volume of\nsize 64\u00d764\u00d764\u00d780. We choose $K=2$ primitives for each voxel, and constrain\nthe offset radius to $r = 1/32$ in our experiments. The total number of trainable\nparameters is 125 million.\nTraining. The optimization is carried out using the AdamW optimizer [40],\nstarting with a learning rate of 2 \u00d7 10-4 and following a cosine annealing schedule\nwith a period of 10 epochs. Our final model is trained for 50 epochs, comprising\n50,000 iterations for each epoch. We observe that applying the regularization\nloss from the start can slow down the convergence regarding the shape. This is\nbecause regularization objectives tend to encourage thinner surfaces, which may\nresult in premature convergence to local minima if the shapes are noisy. In our\nexperiments, we thus enable regularization after the first 15 epochs.\nMesh extraction. To obtain a mesh from reconstructed 2D primitives, we\ngenerate RGBD maps by rendering along three circular video trajectories at\nelevations of 30\u00b0, 0\u00b0, and -30\u00b0. Inside the scene bounding box, we construct a\nsigned distance function volume and apply truncated SDF (TSDF) fusion to\nintegrate the reconstructed rgb and depth maps, allowing for efficient textured\nmesh extraction. In our experiments, we use a resolution of 2563 and set a\ntruncation threshold of 0.02 for TSDF fusion."}, {"title": "5 Experiments", "content": "We now present an extensive evaluation of LaRa, our large-baseline radiance\nfield. We first compare with previous and concurrent works on in-domain and\nzero-shot generalization settings. We then analyze the effect of local attention,\nregularization term, and renderer.\n5.1 Comparison\nWe compare our method against MVSNeRF [8], MuRF [70], and the concurrent\nwork LGM [59]. The first two methods are key representatives of feature matching-\nbased methods, and the latter shares a conceptually similar approach of using\nGaussian primitives for large-baseline settings. It is worth noting that while\nexisting feed-forward radiance field reconstruction methods are capable of being\nevaluated in large-baseline settings, retraining these methods to establish a\nnew large-baseline benchmark on the Objaverse dataset is both time and GPU\nintensive. Here, we retrain MVSNeRF and the current state-of-the-art feed-\nforward radiance field reconstruction method MuRF [70].\nAppearance. Table 1 shows quantitative results (PSNR, SSIM, and LPIPS)\ncomparisons. Our method achieves clearly improved rendering quality for both\nin-domain generation (Gobjaverse testing set) and zero-shot generalization (GSO\nand Co3D datasets). As shown in Figure 4, MVSNeRF fails to provide faithful\nreconstructions on the large-baseline setting and tends to produce floaters within\nthe reconstruction regions since the cost volume is extremely noisy in the sparse\nview scenarios, resulting in a challenge for its convolution matching network to\ndistinguish the surface. MuRF [70] quickly overfits the white background and\nproduces empty predictions for all inputs. Instead of predefining and constructing\nthe feature similarity as network input, our method injects volume features\nto the inter-middle attention layer and implicitly and progressively matches\nthem through the attention mechanism between the volume feature and updated\nembeddings, achieving clearer and overall better reconstructions.\nOur approach is robust to scene scale and can generalize to real captured\nimages, such as those in the Co3D dataset, thanks to our canonical modeling\nand projection-based feature lifting. In contrast, LGM [59] leverages a monocular\nprediction and fusion technique that requires a reference scene scale and a\nconstant camera-object distance to avoid focal length and distance ambiguity.\nThis requirement significantly limits its generalizability to real data. As shown in\nTable 1 and Figure 4, LGM provides faithful reconstructions in datasets with\na strict constant camera-object distance, such as GSO, but fails to generalize\nto unconstraint multi-view data such as in Objaverse and Co3D datasets, and\nexhibits serious distortions. Our model trained on 4 A100-40G GPUs for 2 days\ndemonstrates superior results compared to the LGM model trained on 32 A100-\n80G GPUs (8\u00d7 GPUs, 16\u00d7 RAM, 32\u00d7 GPU hours) and on the same synthetic\nObjaverse dataset [15].\nFurthermore, our approach also performs well for generative multi-view images\nwhere textures are not consistent across views. In this comparison, we only present\na qualitative analysis due to the absence of ground truths, as illustrated in the\nbottom rows of Figure 4. Our method offers detailed texture and smooth surface\nreconstruction. We invite the reader to our Appendix for more results.\nGeometry. We evaluate the quality of our geometry reconstruction by com-\nparing the depth reconstructions on novel views, generated by a weighted sum\nof the z values of the primitives. As shown in Table 2, our approach achieves\nsignificantly lower L1 errors and higher geometry accuracy other baselines. In\nFigure 5, we also visualize geometry reconstruction by extracting meshes using\nTSDF. In addition, our trajectory video rendering (48 views at a resolution of\n512) together with mesh extraction is highly efficient, as it does not require\nfine-tuning and can be performed in just 2 seconds.\n5.2 Ablation Study\nWe now analyze the contributions of individual elements of our model design.\nTo reduce the training cost, we reduce the training from 50 to 30 epochs for\nablations.\nEffect of local attention. We first evaluate the contribution of our group\npartition using different group numbers. Here, G=1 is equivalent to the standard\ncross-attention layer; however, using such group size can lead to much higher\ncompute time for the same number of iterations, i.e., 22 days on 4 A100s for\n30 epochs. Therefore, our ablation starts with 4 groups for acceptable training\ntime. As shown in ablations (a), (b) and (g) in Table 3 and Figure 6, the image\nsynthesis and geometry quality are consistently improved with a larger group\nnumber, thanks to the local attention mechanism.\nEffect of regularization term. We further evaluate the regularization term\nintroduced in Eq. 9 and Eq. 10. We observe a marked improvement in the\naverage rendering score when disabling the regularization. Although this provides\na stronger model capability for modeling details, this may cause floaters near\nthe surfaces, as shown in (c) and (d) of Figure 6, which leads to inconsistent\nfree-viewpoint video rendering (see Appendix video). In contrast, our approach\nis able to reconstruct hard surfaces.\nEffect of renderer. We also compare 2D Gaussian splatting with 3D Gaussian\nsplatting in our framework, as shown in (c) and (d). They achieve similar\nrendering quality and we choose 2DGS to facilitate surface regularization and\nmesh extraction. Furthermore, to evaluate the effectiveness of the coarse-fine\ndecoding, we conduct an evaluation of the coarse outputs, shown in row (e). Our\nfine decoding is able to provide richer texture details.\nEffect of renderer. We also compare 2D Gaussian splatting with 3D Gaussian\nsplatting in our framework, as shown in (c) and (d). They achieve similar\nrendering quality and we choose 2DGS to facilitate surface regularization and\nmesh extraction. Furthermore, to evaluate the effectiveness of the coarse-fine\ndecoding, we conduct an evaluation of the coarse outputs, shown in row (e). Our\nfine decoding is able to provide richer texture details.\nEffect of input views. Our approach is highly efficient and compatible with\ndifferent numbers of input views. In prior experiments, we utilize 4 views for both\ntraining and inference as our standard configuration. We evaluate our method in\n2-4 testing views (as shown in rows (h),(j), and (g)) using the full model in row\n(g)."}, {"title": "6 Conclusion", "content": "We have presented LaRa, a novel method for 360\u00b0 bounded radiance fields\nreconstruction from large-baseline inputs. Our central idea is to match image\nfeatures and embedding volume through unified local and global attention layers.\nBy integrating this with a coarse-fine decoding and splatting process, we achieve\nhigh efficiency for both training and inference. In future work, we plan to explore\nhow to enlarge the batch size per-GPU and volume resolution without increasing\nGPU usage. In addition, we hope to investigate how to extend it to handle\nunbounded 360\u00b0 scenes and decompose the radiance field into its constituent\nphysical material and lighting components.\nLimitations and Discussions. Our LaRa demonstrates a remarkable efficiency\nfeed-forward model that achieved high-fidelity all-around novel-view synthesis and\nsurface reconstruction from sparse large-baseline images. However, our approach\nstruggles to recover high-frequency geometry and texture details, mainly due\nto the low volume resolution. Enhancing our approach with techniques such\nas gradient checkpointing or mixed-precision training can potentially increase\ntraining batch size as well as volume resolution. We have also noticed that our\nmethod can yield inconsistent rendering results when the geometry is incorrectly\nestimated or when reconstructing multi-view inconsistent inputs, as demonstrated\nin the comparison video. This occurs because our method utilizes second-order\nSpherical Harmonic appearance modeling. While such modeling can capture\nview-dependent effects, it also introduces a stronger ambiguity between geometry\nand appearance. We believe that incorporating our method with a physically-\nbased rendering process can potentially address this issue. In addition, our work\nassumes posed inputs, but estimating precise camera poses for sparse views is\na challenge in practice. Incorporating a pose estimation module [65] into the\nfeed-forward setting is an orthogonal direction to our work."}]}