{"title": "LaRa: Efficient Large-Baseline Radiance Fields", "authors": ["Anpei Chen", "Haofei Xu", "Stefano Esposito", "Siyu Tang", "Andreas Geiger"], "abstract": "Radiance field methods have achieved photorealistic novel\nview synthesis and geometry reconstruction. But they are mostly applied\nin per-scene optimization or small-baseline settings. While several recent\nworks investigate feed-forward reconstruction with large baselines by\nutilizing transformers, they all operate with a standard global attention\nmechanism and hence ignore the local nature of 3D reconstruction. We\npropose a method that unifies local and global reasoning in transformer\nlayers, resulting in improved quality and faster convergence. Our model\nrepresents scenes as Gaussian Volumes and combines this with an image\nencoder and Group Attention Layers for efficient feed-forward reconstruc-\ntion. Experimental results demonstrate that our model, trained for two\ndays on four GPUs, demonstrates high fidelity in reconstructing 360\u00b0\nradiance fields, and robustness to zero-shot and out-of-domain testing.", "sections": [{"title": "1 Introduction", "content": "The ability to reconstruct the shape and appearance of objects from multi-view\nimages has long been one of the core challenges for computer vision and graphics.\nModern 3D reconstruction techniques achieve impressive results with various\napplications in visual effects, e-commerce, virtual and augmented reality, and\nrobotics. However, they are limited to small camera baselines or dense image\ncaptures [8, 32, 42, 66]. In recent years, the computer vision community has made\ngreat strides towards high-quality scene reconstruction. In particular, Structure-\nfrom-Motion [51, 55] and multi-view stereo [23, 72] emerged as powerful 3D\nreconstruction methods. They identify surface points by aggregating similarities\nbetween point features queried from source images, and are able to reconstruct\nhighly accurate surface and texture maps.\nDespite these successes, geometry with view-consistent textures is not the only\naspect required in applications of 3D reconstruction. The reconstruction process\nshould also be able to recover view-dependent appearance. To this end, neural\nradiance fields [42] and neural implicit surfaces [47, 74] investigate volumetric\nrepresentations that can be learned from multi-view captures without explicit\nfeature matching. Their follow-ups [2, 5, 21, 32, 44, 57, 64, 76, 77] improve efficiency\nand quality, but mostly require per-scene optimization and dense multi-view\nsupervision.\nSeveral recent works thus investigate feed-forward models for radiance field\nreconstruction while relaxing the dense input view requirement. While feed-\nforward designs vary, they commonly utilize local feature matching [8, 13, 30, 37, 70],\nwhich however limits them to small-baseline reconstruction, since feature matching\ngenerally relies on substantial image overlap and reasonably similar viewpoints.\nGeometry-aware transformers [34, 43, 49, 62] have also been adapted to address\nlarge-baseline problems, but they often suffer from blurry reconstructions due\nto the lack of 3D inductive biases. Recent large reconstruction models [26, 36]\nlearn the internal perspective relationships through context attention, enabling\nlarge-baseline reconstruction. However, the transformers are unaware of epipolar\nconstraints, and instead are tasked to implicitly learn spatial relationships, which\nrequires substantial data and GPU resources.\nIn this work, we present LaRa, a feed-forward reconstruction model without\nthe requirement of heavy training resources for the task of 360\u00b0 bounded radiance\nfields reconstruction from unstructured few-views. The core idea of our work is\nto progressively and implicitly perform feature matching through a novel volume\ntransformer. We propose a Gaussian volume as 3D representation, in which each\nvoxel comprises a set of learnable Gaussian primitives. To obtain the Gaussian\nvolume from image conditions, we progressively update a learnable embedding\nvolume by querying features in 3D. Specifically, we utilize a DINO image feature\nencoder to obtain image tokens and lift 2D tokens to 3D by unprojecting them\nto a shared canonical space. Next, we propose a novel Group Attention Layer\narchitecture to enable local and global feature aggregation. Specifically, we\ndivide dense volumes into local groups and only apply attention within each\ngroup, inspired by standard feature point matching. The grouped features and\nembeddings are fed to a cross-attention sub-layer to implicitly match features\nbetween feature groups of the feature volume and embedding volume, which is\nfollowed by a 3D CNN layer to efficiently share information across neighboring\ngroups. After passing through all attention layers, the volume transformer outputs\na Gaussian volume, and is then decoded as 2D Gaussian [27] parameters using\na coarse-to-fine decoding process. By incorporating efficient rasterization, our\nmethod achieves high-resolution renderings.\nWe demonstrate our method's efficiency and robustness for providing photore-\nalistic, 360\u00b0 novel view synthesis results using only four input images. We find that\nour model achieves zero-shot generalization to significantly out-of-distribution\ninputs. Moreover, our reconstructed radiance fields allow high-quality mesh re-\nconstruction using off-the-shelf depth-map fusion algorithms. Finally, our model\nachieves high-quality reconstruction results using only 4 A100-40G GPUs within\na span of 2 days."}, {"title": "2 Related Work", "content": "Multi-view stereo. Multi-view stereo reconstruction aims to generate detailed\n3D models by reasoning from images captured from multiple viewpoints, which has\nbeen studied for decades [14, 22, 25, 33, 35, 50, 52]. In recent years, multi-view stereo\nnetworks [28, 72] have been proposed to address MVS problems. MVSNet [72]\nutilizes a 3D Convolutional Neural Network for processing a cost volume. This\ncost volume is created by aggregating features from a set of adjacent views,\nemploying the plane-sweeping technique from a reference viewpoint, facilitating\ndepth estimation and enabling superior 3D reconstructions. Subsequent research\nhas built on top of this foundation, incorporating strategies such as iterative\nplane sweeping [73], point cloud enhancement [9], confidence-driven fusion [41],\nand the usage of multiple cost volumes [12, 24] to further refine reconstruction\naccuracy. However, all of these works require large image overlap for faithful\nfeature matching.\nFew-shot Radiance fields. The Radiance field representation [42] has rev-\nolutionized the reconstruction field, emerging as a promising replacement for\ntraditional reconstruction methods. Despite the promising achievement in per-\nscene sparse view reconstruction [6, 7, 16, 46, 53, 56, 60, 63, 68], training a feed-\nforward radiance field predictor [8, 13, 66, 76] has gained popularity. MVSNeRF [8]\nproposed to combine a cost volume with volume rendering, allowing appear-\nance and geometry reconstruction only using a photometric loss. The following\nworks [10, 30, 37, 70] are proposed to advance reconstruction quality and efficiency.\nSimilarly to standard MVS methods, they are limited to small camera baselines.\nRecently, several works have explored feed-forward models for few-shot [1,\n4, 11, 19, 36, 43, 67] input by capitalizing on large-scale training datasets and\nmodel sizes. They leverage cross-view attention to globally reason about 3D\nscenes and output 3D representation (e.g., tri-plane, IB-planes) for radiance field\nreconstruction. Concurrent work by LGM [59] and GRM [75] introduces few-shot\n3D reconstruction models that produce high-resolution 3D Gaussians using a\ntransformer framework. While these methods achieve impressive visual results,\ntraining becomes expensive and less practical for the academic community. Unlike\nsome recent single view reconstruction methods [26, 58], our work focuses on\nfew-shot (> 1) reconstruction since single-view input can be efficiently lifted to\nmulti-view by multi-view generative models [38, 39, 54, 71]."}, {"title": "3 LaRa: Large-baseline Radiance Fields", "content": "Our goal is to reconstruct the geometry and view-dependent appearance of\nbounded scenes from sparse input views using limited training resources. Given\nM images I=(I\u2081, . . ., IM) with camera parameters \u03c0= (\u03c01,..., \u03c0\u03bc), our method\nreconstructs radiance fields as a collection of 2D Gaussians, which is used to\nsynthesize novel views and extract meshes. Our model is a function f of a discrete\nradiance field of voxel positions v and outputs a Gaussian volume VG"}, {"title": "3.1 3D Representation", "content": "We utilize a 3D voxel grid as our 3D representation, consisting of 3 volumes: an\nimage feature volume Vf to model image conditions, an embedding volume Ve\ndescribes 3D prior learned from data, and a Gaussian volume V\u00e7 represents the\nradiance field.\nImage feature volume. We construct a feature volume for each input view by\nlifting the 2D image features to a canonical volume defined in the center of the\nscene. We use the DINO [3] image encoder to extract per-view image features, and\ninject Pl\u00fccker ray directions into the features via adaptive layer norm [48]. Unlike\nprevious works that modulate camera poses to image features using extrinsic and\nintrinsic matrices [26, 38], Pl\u00fccker rays are defined by the cross product between\nthe camera location and ray direction, offering a unique ray parameterization\nindependent of object scale, camera position and focal length. After modulation,\nwe obtain M per-view image feature maps. We further lift the 2D maps to 3D\nby back-projecting the feature maps to a canonical volume, therefore resulting in\nM feature volumes Vf \u2208 \u211dW\u00d7W\u00d7W\u00d7O with O channels.\nEmbedding volume. Inspired by prior works [26, 31, 45], we construct a\nlearnable embedding volume Ve\u2208\u211dW\u00d7W\u00d7W\u00d7C for modeling prior knowledge.\n3D reconstruction is generally under-constrained in sparse view settings, hence\nprior knowledge is critical for faithful reconstructions. We propose to leverage\na 3D embedding volume to model and learn prior information across objects,\nwhich acts as a 3D object template that greatly reduces the solution space. The\nembedding volume is aligned with the image feature volume, allowing for efficient\ncross attention (see Section 3.2).\nGaussian volume. To achieve efficient rendering, we propose to use dense\nprimitives as an object representation and output a set of 2D Gaussians from the\nimage feature volume and embedding volume. However, predicting a set of dense\nunordered point sets without 3D supervision is always a challenge for neural\nnetworks. To this end, we introduce a dense Gaussian volume representation that\ncan effectively model points densely near the object's surface, while being suitable\nfor modern network architectures by facilitating prediction and generation.\nSpecifically, our Gaussian volume comprises K learnable Gaussian primitives\nper voxel, where each primitive can move freely within a constrained spherical\nregion centered at the voxels' center. For primitive modeling, we borrow the\nshape and appearance parametrization from 2D Gaussian splatting [27] for better\nsurface modeling. Each Gaussian has an opacity a, tangent vectors t= [tu, tv],\na scaling vector S = (su, sv) controlling the shape of the 2D Gaussian, and\nspherical harmonics coefficients for view-dependent appearance. Furthermore, we\nsubstitute the primitive's position with an offset vector \u0394\u2208 [\u22121, 1]\u00b3, incorporating\na scaled sigmoid activation function. Consequently, the position of Gaussian\nprimitive k in voxel vi is expressed as p = vi + r\u0394, where r signifies\nthe maximum displacement range of the primitive. In this way, primitives are\nrestricted to neighborhoods of uniformly distributed local centers. The inclusion\nof offset modeling allows each voxel to effectively represent adjacent regions that\nrequire it. This reduces unnecessary capacity in empty space and enhances the\nrepresentational capacity compared to the standard dense volume. We refer the\nreader to the supplementary material for more details on 2D Gaussian splatting."}, {"title": "3.2 Volume Transformer", "content": "To predict the Gaussian volume, we propose a volume transformer architecture to\nperform attention between volumes. Self-attention and cross-attention modules,\nas commonly used in transformers [17], are inefficient for volumes, since the\nnumber of tokens grows cubically with the resolution of the 3D representation.\nNa\u00efve applications thus result in long training times and large GPU memory\nrequirements. In addition, geometry constraints and regional matching play\ncrucial roles in the context of 3D reconstruction, which should be considered in\nthe attention design.\nWe now present our novel volume transformer containing a set of group\nattention layers that progressively update the embedding volume. Our group\nattention layers contain three sublayers (see Figure 2): group cross-attention,\na multi-layer perceptron (MLP), and 3D convolution. Given the image feature\nvolume and embedding volume, we first unfold these 3D volumes (i.e., Vf and\nVe) into G local token groups along each axis. We then apply a cross-attention\nlayer between the corresponding groups of embedding tokens Vg, and image\nfeature tokens V, where j denotes the index of the layer starting from 1, and\n{V8,1}g = Ve. Figure 1 illustrates the unfolding for G = 4 and highlights the\ncorresponding groups.\nThe next sublayer is an MLP, similar to the original transformer [29, 48, 61]. The\nupdated embedding groups {Vg,}G=1 are reassembled into the original volume\nshape, resulting in V2, which are subsequently processed by a 3D convolutional\nlayer to share information between groups and enable the intra-model connections\nwithin the spatially organized voxels. In summary,\nVGj,1 = GroupCrossAttn (LN (VGj,1),V) + VGj,1,   (2)\nVGj = MLP (LN (VGj)) + VGj,   (3)\nVi+1 = 3DCNN (LN (V)) + V.  (4)\nTo incorporate information from multiple views, we flatten and concatenate\nthe image feature tokens from multi-view feature volumes. It is important to note\nthat different groups are processed simultaneously by the group attention layer\nacross the batch dimension. This parallel processing allows for a larger training\nbatch size within the attention sublayer, reducing the number of training steps\nrequired. In addition, using a 3D convolution layer increases inference efficiency\ncompared to the popular self-attention layer. Also, we also apply layer norms\nLM(\u00b7) between the sub-layers. Finally, the output embedding volume V\uba53 serves\nas input for the subsequent (j+1)th group attention layer.\nAfter passing through all (12 in our experiments) group attention layers,\nwe use a 3D transposed CNN to scale up the updated embedding volume Ve,\nVG = Transpose-3DCNN (V.). Now we have a Gaussian volume V\u00e7, each\nGaussian voxel is a 1-D feature vector V\u2208 \u211d1\u00d7B, representing the primitives\nassociated with the voxel."}, {"title": "3.3 Coarse-Fine Decoding", "content": "We obtain 2D Gaussian primitive shape and appearance parameters from the\nGaussian volume, so we introduce a coarse-fine decoding process to better recover\ntexture details. Instead of using a single network and sampling scheme to reason\nabout the scene, we simultaneously optimize two decoding modules: one \"coarse\"\nand one \"fine\".\nFor the \"coarse\" decoding module, we feed Gaussian volume features to\na lightweight MLP and output a set of K Gaussian parameters per voxel. We\nemploy the efficient 2D splatting technique [27] to form high-resolution renderings,\nincluding RGB, depth, opacity, and normal maps. During training, we render M\ninput views and M novel views for supervision.\nDespite the fact that the coarse renderings can already provide faithful\ndepths/geometries, the appearance tends to be blurred, as shown in (e) of\nFigure 6. This is because the image texture can easily be lost after the DINO\nencoder and the Group Attention layers. To address this problem, we propose a\n\"fine\" decoding module to guide fine texture prediction.\nSpecifically, we project the primitive centers ponto the coarse renderings\n(i.e., RGB image \u00ce, depth image D, and accumulation alpha map \u00c2) to contain\nthe coarse renderings for each primitive using the camera poses \u03c0,\nXp = (Ipp Dp, Ap) = \u03a6 (P (P, \u03c0), [I, \u00ce, \u00cd, \u00c2]), (5)\nwhere P denotes the point projection, is a concatenation operation along the\nchannel dimension, and is a bilinear interpolation in 2D space.\nIn practice, the depth features can change significantly across different scenes.\nTo mitigate scaling discrepancies, we replace the rendering depth Dp with a\ndisplacement feature Dpp that compares the rendered depth for input\nviews and the depth zpe of a primitive, allowing for occlusion-aware reasoning.\nWe then apply a point-based cross-attention layer to establish relationships\nbetween the features of a point Xp and the primitive voxel. The results of this\ncross-attention process are then fed into an MLP, which is tasked with predicting\nthe residual spherical harmonics\nSHresiduals = MLP (CrossAttn (Xp, V)), (6)\nSHfine\ni,k = SHcoarse + SHresiduals. (7)\nIntuitively, the \"fine\" decoding module attempts to learn a geometry-aware texture\nblending process based on multi-view images, primitive features, and rendering\nbuffers from the coarse module. Furthermore, both coarse and fine modules are\ndifferentiable and updated simultaneously. Thus, the fine renderings can further\nregularize the coarse predictions.\nSplatting. Our work takes advantage of Gaussian splatting [27, 32] to facilitate\nefficient high-resolution image rendering. We follow the original rasterization\nprocess and further output depth and normal maps by integrating the z value\nand the normal of the primitives."}, {"title": "3.4 Training", "content": "Our LaRa is optimized across scenes via gradient descent, minimizing simple\nimage reconstruction objectives between the coarse and fine renderings (i.e., \u00ce)\nand the ground-truth images (i.e., I),\nL = LMSE(I, \u00ce) + LSSIM(I, \u00ce) + LReg, (8)\nwhere LMSE is the pixel-wise L2 loss, LSSIM is the structural similarity loss, which\nare applied on both coarse and fine RGB outputs.\nRegularization terms. We find that only applying the photometric recon-\nstruction losses is adequate for rendering. However, the consistency across views\nis low because of the strong flexibility of the discrete Gaussian primitives. To\nencourage the primitives to be constructed on the surface, we follow 2D Gaus-\nsian splatting [27] that utilize a self-supervised distortion loss La and a normal\nconsistency loss Ln to regularize the training.\nSpecifically, we concentrate the weight distribution along the rays by minimiz-\ning the distance between the ray-primitive intersections, inspired by Mip-NeRF [2].\nGiven a ray u(x) of pixel x, we obtain its distortion loss by,\nLa = \u2211 WiWj|zi - zj| (9)\nwhere wi = ai Gi(u(x))  =1(1 \u2013 aj Gj (u(x))) is the blending weight of the i-th\nintersection and zi the depth of the intersection point.\nAs 2D Gaussians explicitly model the primitive normals, we can align their\nnormals n; with the normals N derived from the depth maps via the loss\nLn = \u03a3\u03c9\u0390(1 \u2212 n N). (10)\nTherefore, our regularization term for the ray u(x) is given by LReg = YdLd+YnLn.\nWe set Ya=1000 and Yn=0.2 in our experiments."}, {"title": "4 Implementation Details", "content": "We briefly discuss our implementation, including the training and evaluation\ndataset, network design, optimizer, and mesh extraction.\nDatasets. We train our model on multi-view synthetic renderings of objects [69],\nbased on the Objaverse dataset [15], which includes 264,775 scenes with a\ntrain/test split of 10:1. Each scene contains 38 circular views with an image\nresolution of 512x 512. To ensure sufficient angular coverage of the input views,\nwe employ the classical K-means algorithm to cluster the cameras into 4 clusters.\nDuring training, we randomly choose two views from each cluster for every\niteration, in which the first 4 images share the same camera poses as the input\nviews, while the remaining 4 images are novel view outputs. We employ the eight\noutput images for supervision and leverage the loss objectives outlined in Eq. 8\nto update the network.\nWe present our in-domain evaluation using the Objaverse dataset's test set,\nconsisting of 26,478 scenes. To assess our model's cross-domain applicability, we\nconducted tests on the Google Scanned Objects dataset [18], which contains\n1,030 scans of real objects, and on the 46 hydrants and 90 teddy bears from the\nCo3D test set [49], totaling 136 objects. To examine our model's performance\non zero-shot reconstruction task, we use the generative multi-view dataset from\nInstant3D [36], which comprises 122 scenes generated from text prompts.\nNetwork. We developed LaRa using PyTorch Lightning [20] and conduct our\ntraining on 4 NVIDIA A100-40G GPUs over a period of 2 days for the fast\nmodel and 3.5 days for the base model, with a batch size of 2 per GPU. We use\nDINO-base for encoding M=4 multi-view images at a resolution of 512 \u00d7 512.\nWe use a volume resolution of W = 16 with C=768 channels for the image feature\nvolume, and a resolution of W = 32 with C = 256 channels for the embedding\nvolume, dividing both into G = 16 groups for the group attention layers. Our\ngroup attention network consists of 12 layers, producing a Gaussian volume of\nsize 64\u00d764\u00d764\u00d780. We choose K=2 primitives for each voxel, and constrain\nthe offset radius to r = 1/32 in our experiments. The total number of trainable\nparameters is 125 million.\nTraining. The optimization is carried out using the AdamW optimizer [40],\nstarting with a learning rate of 2 \u00d7 10-4 and following a cosine annealing schedule\nwith a period of 10 epochs. Our final model is trained for 50 epochs, comprising\n50,000 iterations for each epoch. We observe that applying the regularization\nloss from the start can slow down the convergence regarding the shape. This is\nbecause regularization objectives tend to encourage thinner surfaces, which may\nresult in premature convergence to local minima if the shapes are noisy. In our\nexperiments, we thus enable regularization after the first 15 epochs.\nMesh extraction. To obtain a mesh from reconstructed 2D primitives, we\ngenerate RGBD maps by rendering along three circular video trajectories at\nelevations of 30\u00b0, 0\u00b0, and -30\u00b0. Inside the scene bounding box, we construct a\nsigned distance function volume and apply truncated SDF (TSDF) fusion to\nintegrate the reconstructed rgb and depth maps, allowing for efficient textured\nmesh extraction. In our experiments, we use a resolution of 2563 and set a\ntruncation threshold of 0.02 for TSDF fusion."}, {"title": "5 Experiments", "content": "We now present an extensive evaluation of LaRa, our large-baseline radiance\nfield. We first compare with previous and concurrent works on in-domain and\nzero-shot generalization settings. We then analyze the effect of local attention,\nregularization term, and renderer.\n5.1 Comparison\nWe compare our method against MVSNeRF [8], MuRF [70], and the concurrent\nwork LGM [59]. The first two methods are key representatives of feature matching-\nbased methods, and the latter shares a conceptually similar approach of using\nGaussian primitives for large-baseline settings. It is worth noting that while\nexisting feed-forward radiance field reconstruction methods are capable of being\nevaluated in large-baseline settings, retraining these methods to establish a\nnew large-baseline benchmark on the Objaverse dataset is both time and GPU\nintensive. Here, we retrain MVSNeRF and the current state-of-the-art feed-\nforward radiance field reconstruction method MuRF [70].\nAppearance. Table 1 shows quantitative results (PSNR, SSIM, and LPIPS)\ncomparisons. Our method achieves clearly improved rendering quality for both\nin-domain generation (Gobjaverse testing set) and zero-shot generalization (GSO\nand Co3D datasets). As shown in Figure 4, MVSNeRF fails to provide faithful\nreconstructions on the large-baseline setting and tends to produce floaters within\nthe reconstruction regions since the cost volume is extremely noisy in the sparse\nview scenarios, resulting in a challenge for its convolution matching network to\ndistinguish the surface. MuRF [70] quickly overfits the white background and\nproduces empty predictions for all inputs. Instead of predefining and constructing\nthe feature similarity as network input, our method injects volume features\nto the inter-middle attention layer and implicitly and progressively matches\nthem through the attention mechanism between the volume feature and updated\nembeddings, achieving clearer and overall better reconstructions.\nOur approach is robust to scene scale and can generalize to real captured\nimages, such as those in the Co3D dataset, thanks to our canonical modeling\nand projection-based feature lifting. In contrast, LGM [59] leverages a monocular\nprediction and fusion technique that requires a reference scene scale and a\nconstant camera-object distance to avoid focal length and distance ambiguity.\nThis requirement significantly limits its generalizability to real data. As shown in\nTable 1 and Figure 4, LGM provides faithful reconstructions in datasets with\na strict constant camera-object distance, such as GSO, but fails to generalize\nto unconstraint multi-view data such as in Objaverse and Co3D datasets, and\nexhibits serious distortions. Our model trained on 4 A100-40G GPUs for 2 days\ndemonstrates superior results compared to the LGM model trained on 32 A100-\n80G GPUs (8\u00d7 GPUs, 16\u00d7 RAM, 32\u00d7 GPU hours) and on the same synthetic\nObjaverse dataset [15].\nFurthermore, our approach also performs well for generative multi-view images\nwhere textures are not consistent across views. In this comparison, we only present\na qualitative analysis due to the absence of ground truths, as illustrated in the\nbottom rows of Figure 4. Our method offers detailed texture and smooth surface\nreconstruction. We invite the reader to our Appendix for more results."}, {"title": "Geometry", "content": "We evaluate the quality of our geometry reconstruction by com-\nparing the depth reconstructions on novel views, generated by a weighted sum\nof the z values of the primitives. As shown in Table 2, our approach achieves\nsignificantly lower L1 errors and higher geometry accuracy other baselines. In\nFigure 5, we also visualize geometry reconstruction by extracting meshes using\nTSDF. In addition, our trajectory video rendering (48 views at a resolution of\n512) together with mesh extraction is highly efficient, as it does not require\nfine-tuning and can be performed in just 2 seconds."}, {"title": "5.2 Ablation Study", "content": "We now analyze the contributions of individual elements of our model design.\nTo reduce the training cost, we reduce the training from 50 to 30 epochs for\nablations.\nEffect of local attention. We first evaluate the contribution of our group\npartition using different group numbers. Here, G=1 is equivalent to the standard\ncross-attention layer; however, using such group size can lead to much higher\ncompute time for the same number of iterations, i.e., 22 days on 4 A100s for\n30 epochs. Therefore, our ablation starts with 4 groups for acceptable training\ntime. As shown in ablations (a), (b) and (g) in Table 3 and Figure 6, the image\nsynthesis and geometry quality are consistently improved with a larger group\nnumber, thanks to the local attention mechanism.\nEffect of regularization term. We further evaluate the regularization term\nintroduced in Eq. 9 and Eq. 10. We observe a marked improvement in the\naverage rendering score when disabling the regularization. Although this provides\na stronger model capability for modeling details, this may cause floaters near\nthe surfaces, as shown in (c) and (d) of Figure 6, which leads to inconsistent\nfree-viewpoint video rendering (see Appendix video). In contrast, our approach\nis able to reconstruct hard surfaces.\nEffect of renderer. We also compare 2D Gaussian splatting with 3D Gaussian\nsplatting in our framework, as shown in (c) and (d). They achieve similar\nrendering quality and we choose 2DGS to facilitate surface regularization and\nmesh extraction. Furthermore, to evaluate the effectiveness of the coarse-fine\ndecoding, we conduct an evaluation of the coarse outputs, shown in row (e). Our\nfine decoding is able to provide richer texture details.\nEffect of renderer. We also compare 2D Gaussian splatting with 3D Gaussian\nsplatting in our framework, as shown in (c) and (d). They achieve similar\nrendering quality and we choose 2DGS to facilitate surface regularization and\nmesh extraction. Furthermore, to evaluate the effectiveness of the coarse-fine\ndecoding, we conduct an evaluation of the coarse outputs, shown in row (e). Our\nfine decoding is able to provide richer texture details.\nEffect of input views. Our approach is highly efficient and compatible with\ndifferent numbers of input views. In prior experiments, we utilize 4 views for both\ntraining and inference as our standard configuration. We evaluate our method in\n2-4 testing views (as shown in rows (h),(j), and (g)) using the full model in row\n(g)."}, {"title": "6 Conclusion", "content": "We have presented LaRa, a novel method for 360\u00b0 bounded radiance fields\nreconstruction from large-baseline inputs. Our central idea is to match image\nfeatures and embedding volume through unified local and global attention layers.\nBy integrating this with a coarse-fine decoding and splatting process, we achieve\nhigh efficiency for both training and inference. In future work, we plan to explore\nhow to enlarge the batch size per-GPU and volume resolution without increasing\nGPU usage. In addition, we hope to investigate how to extend it to handle\nunbounded 360\u00b0 scenes and decompose the radiance field into its constituent\nphysical material and lighting components.\nLimitations and Discussions. Our LaRa demonstrates a remarkable efficiency\nfeed-forward model that achieved high-fidelity all-around novel-view synthesis and\nsurface reconstruction from sparse large-baseline images. However, our approach\nstruggles to recover high-frequency geometry and texture details, mainly due\nto the low volume resolution. Enhancing our approach with techniques such\nas gradient checkpointing or mixed-precision training can potentially increase\ntraining batch size as well as volume resolution. We have also noticed that our\nmethod can yield inconsistent rendering results when the geometry is incorrectly\nestimated or when reconstructing multi-view inconsistent inputs, as demonstrated\nin the comparison video. This occurs because our method utilizes second-order\nSpherical Harmonic appearance modeling. While such modeling can capture\nview-dependent effects, it also introduces a stronger ambiguity between geometry\nand appearance. We believe that incorporating our method with a physically-\nbased rendering process can potentially address this issue. In addition, our work\nassumes posed inputs, but estimating precise camera poses for sparse views is\na challenge in practice. Incorporating a pose estimation module [65] into the\nfeed-forward setting is an orthogonal direction to our work."}, {"title": "7 Acknowledgements", "content": "We thank Bozidar Antic for pointing out a bug that resulted in an improvement\nof about 1dB. Special thanks to BinBin Huang and Zehao Yu for their helpful\ndiscussion and suggestions. We would like to thank Bi Sai, Jiahao Li, Zexiang\nXu for providing us with the testing examples of Instant3D, and Jiaxiang Tang\nfor helping us to construct a comparison with LGM."}]}