{"title": "Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks", "authors": ["Deepa Anand", "Bipul Das", "Vyshnav Dangeti", "Antony Jerald", "Rakesh Mullick", "Uday Patil", "Pakhi Sharma", "Prasad Sudhakar"], "abstract": "In a setting where segmentation models have to be built for multiple datasets, each with its own corresponding label set, a straightforward way is to learn one model for every dataset and its labels. Alternatively, multi-task architectures with shared encoders and multiple segmentation heads or shared weights with compound labels can also be made use of. This work proposes a novel \"label sharing\" framework where a shared common label space is constructed and each of the individual label sets are systematically mapped to the common labels. This transforms multiple datasets with disparate label sets into a single large dataset with shared labels, and therefore all the segmentation tasks can be addressed by learning a single model. This eliminates the need for task specific adaptations in network architectures and also results in parameter and data efficient models. Furthermore, label sharing framework is naturally amenable for incremental learning where segmentations for new datasets can be easily learnt. We experimentally validate our method on various medical image segmentation datasets, each involving multi-label segmentation. Furthermore, we demonstrate the efficacy of the proposed method in terms of performance and incremental learning ability vis-\u00e0-vis alternative methods.", "sections": [{"title": "1 Introduction", "content": "Semantic segmentation in medical imaging a fundamental task and has immense significance. By precisely delineating distinct anatomical structures or pathologies within medical images (such as MRI, CT scans, or X-rays), semantic segmentation contributes to accurate diagnosis, effective treatment planning, and disease monitoring and decision making. While deep learning (DL) solutions have emerged as the de-facto standard for image segmentation tasks, data and compute requirements remain substantial due to the sheer variety and complexity of segmentation tasks that exists.\nIn the context of multi-task segmentation, where models need to handle multiple multi-label segmentations across various anatomical regions, several solutions have been proposed to build models efficiently with shared parametriza-"}, {"title": "2 Methodology", "content": "Consider M medical image segmentation tasks\u00b9 T1, T2,\u2026\u2026,T\u043c with n1, n2,\u2026\u2026,\u043f\u043c labels respectively. Let lij be the anatomy corresponding to the jth label of the ith task. The label sharing framework proposes to group the labels across different tasks and assign a shared abstract label Ak for each group k, with the following constraints:\n1. Every label lij gets assigned to exactly one shared label Ak\u00b7\n1 We use the term 'task' to mean multi-label segmentation on a given dataset pertaining to a certain anatomy."}, {"title": "2.1 Label sharing", "content": "Label sharing has to be such that the labels within each Ak share a set of common characteristics, thereby aiding training of the segmentation model. It is possible to consider several attributes of either the segmentation masks and/or the underlying images to perform grouping. However, in this work we show that it is sufficient to simply use the average relative sizes (within the tasks) of the masks to be segmented as the attribute. We sort the labels within tasks according to their relative sizes and match them across tasks, with ties broken arbitrarily. This scheme satisfies both the constraints mentioned earlier, and also results in n* = Nmax.\nNow that the labels are harmonized across tasks, we propose to build a single multi-channel neural network model to solve the segmentation problem in the shared label space. This makes the number of parameters in the last layer of the neural network grow in the order of only O(nmax). Additionally, there is no need for any architectural or training procedure modifications to supply the network with side information about the specific task to be solved."}, {"title": "2.2 Task addition", "content": "The label sharing framework provides a natural way to incorporate new tasks to the segmentation network in a plug-and-play fashion, with almost no computational or storage overhead. This involves only computing the average relative"}, {"title": "3 Experiments", "content": "The performance of our proposed method was assessed across two distinct use case scenarios - (i) anatomy segmentation in a dataset of 2D image slices and (ii) labeling/localizing extremity structures in a dataset of 2D projections. The proposed label sharing approach was employed in two different modes: (a) label-sharing (LS) with all tasks considered simultaneously and (b) incremental learning (IL), where tasks were learned progressively.\nWe compared our proposed label sharing approach with the following three alternative approaches (i) Individual Models: a dedicated model was built for each multi-label task (ii) Merged Multi-Channel Model: All tasks were combined into a single multi-channel model, where each output channel corresponded to an output label; (iii) Network models with filters guided by task-specific controllers [10]. Our findings shed light on the effectiveness of label sharing approach in comparison to these alternative strategies."}, {"title": "3.1 Datasets", "content": "We used two different datasets for two use-cases in our experiments.\n2D segmentation use-case The dataset for the segmentation use-case in 2D comprises a subset of data sourced from TotalSegmentator [9]. We subdivided the data into three major anatomical segments, which, according to earlier definition, are the individual tasks: (1) pelvic (2) abdomen and (3) chest, with 5, 5, and 4 labels respectively as detailed in Table 2 of appendix A. To ensure visual domain disjointness for these tasks, we meticulously removed transition slices\u2014such as those between the liver (Task 2) and thoracic regions (Task 3)\u2014from the training images of both tasks. Our training dataset comprised 330, 224, and 263 image volumes, while the testing dataset included 99, 65, and 66 image volumes for Task 1, Task 2, and Task 3, respectively. Additionally, we introduce the Head and Neck dataset [7] with 3 labels, consisting of 36 training and 6 test image volumes, to evaluate the incremental learning (IL Task) capability when incorporating additional tasks.\nExtremity localization task in 2D The second dataset consists of 2D orthogonal projections derived from computed tomography (CT) datasets, for annotating various lower extremity anatomical structures. We use this dataset for"}, {"title": "3.2 Architecture, Training and Testing", "content": "The 2D nnUNet framework, as introduced by Isensee et al. in their work [4], serves as the foundation for all our experiments, including the DoD-Net. To assess the effectiveness of our proposed label sharing approach, we compared it with three other methods\n1. Individual: Models trained separately for each task. These networks trained exclusively for a task type are expected to perform the best and hence will be considered gold standard through our experiments,\n2. Multichannel: All tasks merged into a single model,\n3. DoD-Net: Incorporation of task-specific filters.\nWe made modifications to the labels based on specific method employed. All ur models, for both use-cases, were trained for 100 epochs using the Dice Similarity Coefficient (DSC) loss across all experiments. In the context of incremental"}, {"title": "4 Results", "content": "Our evaluation, both qualitative and quantitative, is elaborated in Section 4. We use label-wise and aggregate Dice scores to evaluate the performance of the segmentation models and Hausdorff distance for point and line landmarks respectively, for the extremities localization models."}, {"title": "4.1 2D segmentation task", "content": "Figure 3 provides a snapshot of the robust segmentation achieved for various organs across multiple tasks using the label sharing framework. As previously discussed, the network architecture for this set of K tasks is designed with 5 = max(Ci) output channels (denoted as C). A visual examination of the segmentation results, spanning from large to small anatomical structures, demonstrates the performance robustness across all the label channels.\nFigure 4a illustrates the Dice scores across different labels, along with a comparison to three other methods: individual models for each task, multi-channel output, and DoD-Net output. Naturally, the individually trained models exhibit the best performance. However, our proposed label sharing method consistently outperforms other approaches and closely approaches the performance of individually trained models in most cases. This highlights the efficacy of simplifying the network compared to more complex methods. Furthermore, incremental training for new tasks does not compromise the performance. The label sharing method maintains strong performance across both previous and new tasks, closely matching the best performance achieved by individual models. For a comprehensive overview, refer to Table 1, which summarizes the average Dice Similarity Coefficient (DSC) performance for each task across the different methods."}, {"title": "4.2 Extremity localization task in 2D", "content": "We observe a similar trend to the segmentation task as depicted in fig. 5a. In our analysis, we compute the Hausdorff distance between labels generated by each of the methods and the ground truth provided by the expert radiologist. To facilitate comparison, we normalize the absolute distance values with respect to the maximum Hausdorff distance.\nA lower Hausdorff distance corresponds to better alignment with the ground truth. As expected, the individually trained models exhibit best results in most scenarios. However, the label sharing method consistently outperforms other approaches, except for coronal projection of knee. The lower performance for this task can be attributed to the fact that labels of long structures (tibia and femur) often get fragmented leading to confusion of label assignment in the label sharing framework. Apart from this particular scenario, the performance is better in comparison to the multi-channel and DoD-Net."}, {"title": "5 Discussion", "content": "This paper presents a novel framework for training a single model simultaneously on multiple segmentation tasks involving multiple disparate label. It has been shown by the experimental results that when labels across tasks are appropriately grouped, networks with small capacity are sufficient to achieve performance on par with models trained individually. The models trained with label sharing framework do not require any side information about the individual tasks. Interestingly, our experiments showed that the images corresponding to labels within a group need not exhibit semantic similarity for the model to successfully perform well on disparate datasets. In fact, as demonstrated, they need not even match with respect to absolute sizes. Additionally, the framework provides an elegant way to incorporate new datasets without changing the underlying model architecture or training procedure.\nAn important line of work that remains to be done is automatic generation of shared labels, which may require new definitions of inter-task label similarity. Also, it remains to be studied how this framework extends to other imaging modalities, or even to multi-modal settings."}]}