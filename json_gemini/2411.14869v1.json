{"title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Zhizhong Su"], "abstract": "In embodied intelligence systems, a key component is 3D\nperception algorithm, which enables agents to understand\ntheir surrounding environments. Previous algorithms pri-\nmarily rely on point cloud, which, despite offering pre-\ncise geometric information, still constrain perception per-\nformance due to inherent sparsity, noise, and data scarcity.\nIn this work, we introduce a novel image-centric 3D per-\nception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the\nlimitations of point-centric methods. Specifically, we lever-\nage pre-trained 2D vision foundation models to enhance\nsemantic understanding, and introduce a spatial enhancer\nmodule to improve spatial understanding. Together, these\nmodules enable BIP3D to achieve multi-view, multi-modal\nfeature fusion and end-to-end 3D perception. In our exper-\niments, BIP3D outperforms current state-of-the-art results\non the EmbodiedScan benchmark, achieving improvements\nof 5.69% in the 3D detection task and 15.25% in the 3D\nvisual grounding task.", "sections": [{"title": "1. Introduction", "content": "3D perception models are utilized to estimate the 3D pose,\nshape, and category of objects of interest in a scene, typi-\ncally outputting 3D bounding boxes or segmentation masks.\nIn the field of embodied intelligence, these models gener-\nally serve to provide essential input for planning modules or\nserve as crucial algorithmic components in cloud-based data\nsystems. Enhancing the accuracy of 3D perception holds\nsignificant research value. As shown in Figure 1(a), current\nmainstream 3D perception models extract features from the\npoint cloud (using PointNet++ [29] or 3D CNN [7]) and\ngenerate perception results based on these point features.\nWhile point clouds offer precise geometric information\nthat has advanced 3D perception, several challenges re-\nmain [41]: (1) High-quality point clouds are difficult to ob-\ntain, because depth sensors often have limitations, such as\nstruggling with reflective or transparent objects, long dis-\ntances, or intense lighting conditions. (2) Point clouds are\nsparse, lack texture, and can be noisy, leading to detection\nerrors and a limited performance ceiling. (3) Collecting and\nannotating point cloud data is costly, which poses a chal-\nlenge for acquiring large-scale training data.\nThese challenges limit the performance of point-centric\nmodels. In contrast, the abundance of image data has ac-\ncelerated advancements in vision models, with 2D vision\nfoundation models exhibiting strong semantic understand-\ning and generalization capabilities. Therefore, transferring\n2D vision foundation models (e.g. CLIP [32, 45], EVA [10]\nand DINO [27]) to the 3D domain holds significant poten-\ntial for enhancing 3D task performance.\nIn this paper, we propose BIP3D, an image-centric 3D\nperception model (Figure 1(b)) that performs 3D object de-\ntection and 3D visual grounding by fusing multi-view im-\nage and text features, and can accept depth maps as aux-\niliary inputs for enhanced precision. Our model is based\non the 2D model, GroundingDINO [22], sharing a similar\noverall network architecture and initialized with its model\nweights, thereby inheriting the strong generalization capa-\nbilities of GroundingDINO. Unlike GroundingDINO, our\nmodel can accept an arbitrary number of posed images"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. 3D Object Detection", "content": "3D detection algorithms are primarily categorized into out-\ndoor and indoor scene applications. Due to notable differ-\nences in perception range, target types, and sensor types,\nthe development of algorithms for these two settings varies\nconsiderably. This paper focuses on the indoor scenes.\nFor most indoor scenes, depth cameras provide accurate\n3D point clouds, which can be directly aggregated across\nmultiple frames for early-fusion. Therefore, most existing\nmethods focus on point clouds. VoteNet [30] uses Point-\nNet++ [29] to extract point cloud features and aggregates\nthem using Hough voting. Group-Free [24] leverages a\nTransformer decoder to implicitly aggregate features from\nmultiple points, reducing errors from hand-crafted group-\ning. FCAF3D [34] voxelizes the point cloud, extracts fea-\ntures using a U-Net architecture with sparse 3D convo-\nlutions, and outputs boxes through convolutional decoder.\nUniDet3D [18] uses 3D sparse convolutions and a trans-\nformer decoder for 3D detection. ImVoteNet [31] enhances\nVoteNet by incorporating RGB image features for voting.\nEmbodiedScan [38] projects sparse voxels onto multi-view\nimages to sample features, achieving multi-modal feature\nfusion. ImVoxelNet [35] and NeRF-Det [39] use only im-\nages as input, convert 2D features to 3D voxel features via"}, {"title": "2.2. 3D Visual Grounding", "content": "3D visual grounding involves generating the 3D bounding\nbox for a target in the environment based on a text instruc-\ntion. Early methods divide this task into two stages: the\nfirst stage uses ground truth or a pre-trained 3D detector\nto generate object proposals, while the second stage scores\neach proposal using text features, selecting those that ex-\nceed a threshold as final results. ReferIt3DNet [1] and\nScanRefer [5] introduce two 3D visual grounding datasets\nand propose point-centric two-stage models as baselines.\nLanguageRefer [33] encodes both proposals and text as in-\nputs to a language model, using it to assess proposal confi-\ndence. SAT [40] highlights the issues of sparse, noisy, and\nlimited semantic information in point clouds and incorpo-\nrates images into training to enhance model performance.\nCross3DVG [26] leverages CLIP [32] features from multi-\nview images to boost grounding effects.\nIn addition to these two-stage models, single-stage\ngrounding approaches have emerged in the 3D domain, in-\nspired by 2D grounding techniques [17]. These methods\ndirectly fuse text and scene features and produce ground-\ning results. BUTD-DETR [15] achieves single-stage 3D\ngrounding with a point encoder and transformer decoder\nstructure, offering the option to add extra proposals to im-\nprove performance. 3DSPS [25] transforms 3D grounding\ninto a keypoint selection task, eliminating the need to sepa-\nrate detection and scoring. 3DOGSFormer [14] enables si-\nmultaneous processing of multiple text instructions. Table 1\nhighlights the differences between our BIP3D and existing\n3D detection and grounding methods."}, {"title": "2.3. Vision Foundation Model", "content": "A foundation model is one that has been pre-trained on\nlarge datasets and can achieve good performance on mul-\ntiple downstream tasks.\nIn the field of 2D vision, foundation models are cate-\ngorized by their pre-training approaches: supervised train-\n[9], unsupervised training [2, 11], image contrastive\nlearning [6, 27], and image-text contrastive learning [32].\nFoundation models trained with image-text contrastive\nlearning exhibit the strongest generalization and zero-shot\ncapabilities. These include methods that align full images\nwith text [32, 36] and those that align cropped images with\ntext [19, 22, 45]. Aligning cropped images with text not\nonly improves classification but also enhances localization,\nwhich is crucial for grounding tasks.\nBuilding 3D foundation models typically follows two\ntechnical routes: (1) Contrastive learning with 3D point\nclouds and text features [13, 43, 46]. This approach re-\nquires extensive 3D data, which is less abundant than 2D\ndata. Most available data is 3D object-text, with limited\n3D scene-text data, leading to poor performance in 3D de-\ntection and grounding tasks. (2) Extracting image fea-\ntures using 2D foundation models and obtaining 3D features\nthrough dense mapping [12, 16]. These methods require\ndepth information, usually from depth sensors or SLAM.\nThis paper aims to enhance 3D perception by leverag-\ning existing 2D foundation models. We choose Ground-\ningDINO [22] as the base model and avoid dense mapping,\ndirectly using multi-view feature maps for 3D detection and\ngrounding."}, {"title": "3. Method", "content": "In Figure 2(a), we demonstrate the overall network architec-\nture of BIP3D, which takes multi-view images, text instruc-\ntions and optional depth maps as input. The output is 3D\nbounding boxes for the objects specified by the text, with\nsupport for multiple text instructions that may correspond\nto multiple objects. The network comprises six main mod-\nules: text, image, and depth encoders that individually pro-\ncess the input components into high-dimensional features; a\nfeature enhancer module that fuses the text and image fea-\ntures (Sec. 3.1); a spatial enhancer module that performs\n3D position encoding and depth fusion, enriching the im-\nage features with 3D cues (Sec. 3.2); finally, a transformer\ndecoder that generates the 3D bounding box from the multi-\nview images and text features (Sec. 3.3)."}, {"title": "3.1. Feature Enhancer", "content": "Since our model needs to support multi-view inputs, the\nnumber of image feature vectors is $N\\sum (HW/s)$, where\n$N$ is the number of views. Under the setting of Embod-\niedScan, $N$ equals 50, which makes the computation and\nmemory consumption of cross-attention excessively high to\nbe feasible. Therefore, we only use the feature map with\nthe maximum stride for cross-attention. For the image fea-\ntures in other strides, text information is indirectly obtained\nthrough intra-view multi-stride deformable attention."}, {"title": "3.2. Spatial Enhancer", "content": "Image features only contain information from the current\ncamera coordinate system and are insensitive to camera\nmodels, particularly the extrinsic parameters. Therefore, we\nexplicitly perform 3D encoding of camera models. Specifi-\ncally, given a camera model as a projection function $C$:\n$[x, y, z] = C([u, v, d])$ \n$[u, v] = C^{-1} ([x, y, z])$ \nwhere $[x, y, z]$ represents the 3D coordinates in the percep-\ntion coordinate system, $[u, v]$ represents the 2D pixel coor-\ndinates, and $d$ is the depth in the camera coordinate system.\nFor example, in a common pinhole camera model:\n$[x, y, z, 1]^T = T_E T_I [ud, vd, d, 1]^T$ \nwhere $T_E \\in \\mathbb{R}^{4\\times 4}$ is the extrinsic matrix and $T_I \\in \\mathbb{R}^{4\\times 4}$ is\nthe intrinsic matrix. Based on the camera model $C$, we uni-\nformly sample some 3D points within its view frustum and\nproject these points into the perception coordinate system:\n$C \\Bigg( \\bigg\\{c\\bigg(u_i, v_j, \\frac{k \\cdot D}{K}\\bigg)\\bigg\\| 0 \\leq i < w, 0 \\leq j < h, 0 \\leq k < K \\bigg} \\Bigg)$ \nHere, $D$ and $K$ are hyperparameters, representing the maxi-\nmum depth and the number of sampling points, respectively.\nWe have omitted the stride dimension for simplicity. Then,\nwe use a linear layer to transform the 3D point coordinates\ninto high-dimensional features, resulting in a series of point\nposition embedding:\n$PPE_{i,j,k} = Linear_1 \\Bigg( c\\bigg(u_i, v_j, \\frac{k \\cdot D}{K}\\bigg) \\Bigg)$ \nWe predict the depth distribution $DT$ using the image\nfeature and depth feature, and use this distribution to weight\nthe point embedding to obtain image position embedding:\n$DT_{i,j} = Linear_2(Fusion_3(I_{i,j}, D_{i,j})) \\in \\mathbb{R}^{K}$ \n$IPE_{i,j} = \\sum_k (PPE_{i,j,k} \\times DT_{i,j,k})$\nFinally, we get the updated image feature $I'$ by fuse the\nimage feature $I$, depth feature $D$, and image position em-\nbedding $IPE$:\n$I'_{i,j} = Fusion_4 (I_{i,j}, D_{i,j}, IPE_{i,j})$\nwhere\n$Fusion (x_1, x_2,...) = Linear (Concat(x_1,x_2,...))$\nIt is worth noting that, while PETR [23] also designs 3D\nposition embeddings, it does not consider depth distribution\n$DT$. As a result, its position embeddings are independent\nof image features, which limits their capabilities."}, {"title": "3.3. Decoder with Multi-view Fusion", "content": "The decoder of GroundingDINO uses 2D deformable at-\ntention [47] to achieve feature interaction between image\nand queries, which is insufficient for 3D multi-view appli-\ncations. In this paper, we replace it with 3D deformable\naggregation [21], and conduct certain adaptations and im-\nprovements. Specifically, for each query, we maintain a\ncorresponding 3D bounding box, represented as:\n$B = [x, y, z, l, w, h, roll, pitch, yaw]$\nWe sample $M$ 3D key points within the 3D bounding\nbox: (1) regress a series of offsets $O_l$ based on the query\nfeature to obtain 3D learnable key points, and (2) set some\nfixed offsets $O_{fix}$ based on prior knowledge of 3D detection\nto get fix key points, such as the stereo center and the centers\nof the six faces of the bounding box.\n$O_{3d} = O_l \\cup O_{fix} = \\{ \\Delta[x, y, z]_i | 1 \\leq i \\leq M \\}$\n$P_{3D} = \\{ (\\Delta[x, y, z]_i * R^T) * [l, w, h] + [x, y, z] | 1 \\leq i \\leq M \\}$\nHere, $R$ is the rotation matrix derived from the Euler an-\ngles $[roll, pitch, yaw]$, We project $P_{3D}$ onto the multi-view\nfeature maps using the camera model to obtain $P_{2D}$, and\nperform feature sampling.\n$P_{2D} = \\{C^{-1}(P_{3D,i})|1 \\leq i \\leq M, 1 \\leq j \\leq N \\}$\n$F = \\{ Bilinear(I, P_{2D,i,j})|1 \\leq i \\leq M, 1 \\leq j \\leq N \\}$\nFinally, we combine the query feature, 3D bounding box,\nand camera parameters to predict the weighting coefficients,\nwhich are used to obtain the updated query, thus completing\nthe feature transfer from image features to the query.\n$W = Softmax(Linear(Fusion(Q, B, C))) \\in \\mathbb{R}^{M\\times N}$ \n$Q' = \\sum W_{i,j}F_{i,j}$\nUnlike in autonomous driving scenarios [21], in embodied\nintelligence scenarios, the number of views and the extrin-\nsic parameters of cameras are not fixed and are constantly\nchanging. This results in significant variations in invalid\nsampling points (those outside the view frustum), making\nthe model's convergence more challenging. Therefore, we\nperform explicit filtering and set the weights $W$ of invalid\nkey points to zero."}, {"title": "3.4. Camera Intrinsic Standardization", "content": "We found that the generalization of camera parameters in\nimage-centric models is relatively poor, especially when (1)\nthe richness of camera parameters in the dataset is insuf-\nficient or (2) there is no depth or point cloud to provide\ngeometric information. To mitigate this problem, we pro-\npose a method for camera intrinsic standardization. Specifi-\ncally, given an image and its camera intrinsics $C_r$, and a\npredefined standardized camera intrinsics $C_s$, we transform\nthe image to a virtual camera coordinate system by inverse\nprojection $C_s(C_r^{-1}(i, j))$, where $(i, j)$ are the pixel coordi-\nnates. For a standard pinhole camera, the inverse projection\nformula can be reduced to an affine transformation. We use\nthe mean of the intrinsic parameters from the training set\nas $C_s$. During both training and inference, we standardize\nthe intrinsics of all input images and use the transformed\nimages as inputs to the model."}, {"title": "3.5. Training", "content": "We apply a one-to-one matching loss [3] to the output of\neach decoder layer. The loss consists of three components:\n$Loss = \\lambda_1 L_{cls} + \\lambda_2 L_{center} + \\lambda_3 L_{box}$\nwhere $L_{cls}$ is the contrastive loss between queries and text\nfeatures for classification, using Focal Loss; $L_{center}$ is the\ncenter point regression loss, using $L_2$ Loss; $L_{box}$ is the\nbounding box regression loss for 9-DoF detection. To avoid\nambiguities in $[w,l,h]$ and $[roll, pitch, yaw]$ due to in-\nsufficient definition of object orientation, we introduce a\nsimplified Wasserstein distance as $L_{box}$. Specifically, for\na bounding box, we assign it a 3D Gaussian distribution\n$\\mathcal{N}(\\mu, \\Sigma^2) = \\mathcal{N}([x, y, z], RS^2R^T)$ where $S$ is a diagonal\nmatrix with $[w,l, h]$ along its diagonal. Given $B_{gt}$ and\n$B_{pred}$, the formula for $L_{box}$ is defined as follows:\n$L_{box} = \\sqrt{ || \\mu_{gt} - \\mu_{pred} ||^2 + || \\Sigma_{gt} - \\Sigma_{pred} ||_F }$\nSince the rotation matrix $R$ is orthogonal, $\\Sigma$ equals\n$RSR^T$. Follow DINO [42], we also incorporate a denois-\ning task to assist in training.\nFor 3D detection, we implement it in the form of cate-\ngory grounding. During training, we sample a subset of cat-\negories and set the text to \"[CLS]cls_1[SEP]cls_2...[SEP]\".\nAfter training the 3D detection model, we load its weights\nas pretraining weights and then train the referring ground-\ning model. During the referring grounding training, for each\ntraining sample, we randomly sample 0 to x descriptions\nand set the text to \"[CLS]exp_1[SEP]exp_2...[SEP]\"."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Benchmark", "content": "We use the EmbodiedScan benchmark to validate the\neffectiveness of BIP3D. EmbodiedScan is a 3D indoor\ndataset comprising 4,633 high-quality scans from ScanNet,\n3RScan, and Matterport3D (MP3D), with 1,513, 1,335, and\n1,785 scans from each source, respectively. The training,\nvalidation, and testing sets contain 3,113, 817, and 703"}, {"title": "4.3. Main Results", "content": "Detection 3D. We selected several representative methods\nof different types for comparison: (1) the point model\nVoteNet [30], (2) the RGB-only model ImVoxelNet [35],\n(3) the point-sparse-voxel model FCAF3D [34], and (4)\nthe multi-modal fusion model EmbodiedScan [38]. Ta-\nble 2 presents the metrics of each method, showing that\nBIP3D significantly outperforms existing methods, with an\nAP3D@0.25 on the overall dataset that is 5.69% higher\nthan EmbodiedScan. Notably, thanks to the 2D pre-trained\nmodel, BIP3D exhibits excellent category generalization\nperformance, achieving an AP of 16.03% on tail cate-\ngories, which far exceeds EmbodiedScan's 9.48%. More-\nover, due to the dense nature of image features, BIP3D also\nachieves superior performance on small objects, with an\nAP of 5.72%, compared to a maximum of 3.28% for other\nmethods. When BIP3D uses only RGB as input, the AP\ndecreases by 3.51%, but it still surpasses all existing meth-\nods. The input of depth primarily affects the localization\nprecision, which is notably reflected in a more pronounced\ndecrease in AP for small objects, while there is no signifi-\ncant change for large objects.\nVisual Grounding 3D. The comparison of our method with\nothers on the 3D visual grounding benchmark is shown in\nTable 3. First, on the validation dataset, our BIP3D over-\nall AP surpasses EmbodiedScan by 15.25%. Furthermore,\nit can be observed that our method demonstrates better ro-\nbustness; the performance on hard samples decreases by\nonly 4.95% compared to easy samples, while for Embod-\niedScan, the decrease is 8.67%. On the test dataset, without\nmodel ensemble, BIP3D achieves an AP of 57.05%, which\nis 17.38% higher than that of EmbodiedScan; with model\nensemble, the AP of BIP3D further improves to 62.08%,\nsurpassing the state-of-the-art solution DenseG by 2.49%."}, {"title": "4.4. Ablation Studies and Analysis", "content": "Pretraining. To demonstrate the significant role of 2D\npretraining for 3D perception tasks, we conducted ablation\nstudies on both the point-centric model EmbodiedScan and\nthe image-centric model BIP3D. Firstly, we found that for\nEmbodiedScan, initializing with GroundingDINO weights\nbrought only a marginal improvement of 1.11%, with a\nmere 0.38% increase for tail categories, indicating minimal\neffect. Conversely, for BIP3D, the use of GroundingDINO\nweights resulted in substantial improvements of 5.66% and\n5.99% for the RGB-only and RGB-D models, respectively.\nThis suggests that effective initialization can significantly\nenhance the performance of 3D detection, highlighting the\nimportance for 3D models to fully leverage 2D foundation\nmodels.\nCamera Intrinsic Standardization. Table 6 demonstrates\nthe effect of Camera Intrinsic Standardization (CIS) on 3D\ndetection performance. It can be observed that CIS brings\nabout a performance improvement of 0.7-1.3% for both\nRGB-only and RGB-D models. When trained exclusively\non Scannet, incorporating CIS results in a significant boost\nin performance on unseen camera data. However, due to\nnotable differences in scenes and categories across different\nevaluation datasets, other aspects of generalization continue\nto affect the model's transferability.\nBox Regression Loss. We compared several bounding box\nregression losses. It is evident that when using L1 distance\ndirectly, the AP3D@0.25 only reaches 17.79%, which is\ndue to the inability of L1 distance to handle the ambiguity\nin box orientation, leading to incorrect optimization direc-\ntions. The corner chamfer distance can avoid such orienta-\ntion ambiguity; however, corner chamfer distance loss train-\ning on BIP3D is unstable and difficult to converge. Both\npermutation corner distance loss and Wasserstein distance\nloss are orientation-agnostic, avoiding orientation ambigu-\nity and enabling the model to converge stably, achieving\nbetter performance. Specific results are shown in the Ta-\nble 7. Detail about permutation corner distance loss refers\nto appendix.\nNumber of Description. During training for visual ground-\ning, using only one text description per sample results in\nlow training efficiency. However, directly employing mul-\ntiple descriptions for training can introduce a domain gap\nduring testing, leading to degraded performance. Therefore,\nwe opted to randomly select between 1 and 10 text descrip-\ntions for training. Table 8 demonstrates the impact of the\nnumber of descriptions on model performance. It is evident\nthat our random selection strategy improves AP by 1.56%\ncompared to using a single description, and by 5.61% com-\npared to using a fixed set of 10 descriptions."}, {"title": "5. Conclusion and Future Works", "content": "In this work, we propose an image-centric 3D perception\nmodel, BIP3D. It overcomes the limitations of point clouds\nand effectively leverages the capabilities of 2D founda-\ntion models to achieve significant improvements in 3D per-\nception performance. BIP3D supports multi-view images,\ndepth maps, and text as inputs, enabling it to perform 3D\nobject detection and 3D visual grounding. We demonstrate\nthe superiority of BIP3D on the EmbodiedScan benchmark.\nBIP3D still has considerable room for exploration, and\nseveral future works are outlined here: (1) Further opti-\nmizing the network architecture and training schemes to\nachieve even better perception performance. (2) Apply-\ning BIP3D to dynamic scenes to achieve joint detection\nand tracking. (3) Incorporating more perception tasks, such\nas instance segmentation, occupancy, and grasp pose esti-\nmation. (4) Under the integrated network framework of\nBIP3D, the decoder can be improved to support higher-level\ntasks like visual question answering and planning."}]}