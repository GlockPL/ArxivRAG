{"title": "Al Governance through Markets", "authors": ["PHILIP MOREIRA TOMEI", "RUPAL JAIN", "MATIJA FRANKLIN"], "abstract": "This paper argues that market governance mechanisms should be considered a key approach in the governance\nof artificial intelligence (AI), alongside traditional regulatory frameworks. While current governance approaches\nhave predominantly focused on regulation, we contend that market-based mechanisms offer effective incentives\nfor responsible AI development. We examine four emerging vectors of market governance: insurance, auditing,\nprocurement, and due diligence, demonstrating how these mechanisms can affirm the relationship between Al risk\nand financial risk while addressing capital allocation inefficiencies. While we do not claim that market forces alone\ncan adequately protect societal interests, we maintain that standardised AI disclosures and market mechanisms can\ncreate powerful incentives for safe and responsible AI development. This paper urges regulators, economists, and\nmachine learning researchers to investigate and implement market-based approaches to AI governance.", "sections": [{"title": "1 Overview", "content": "The field of Artificial Intelligence (AI) governance has predominantly emphasised regulatory frameworks [7, 136, 143] and\ninternational cooperation [21, 58] to address AI risk. Meanwhile, uncertainty around AI risks is a major barrier to widespread\nenterprise adoption [88] [5]. Despite economic benefits, organizations recognize Al risk as a business risk and lack the tools to\nconfidently address it [36]. Market governance approaches, such as insurance, auditing, procurement, and due diligence, can serve\nto both mitigate AI risk and enable AI growth - aligning market forces with prosocial behaviour [150].\nMarket governance mechanisms are processes that structure economic behaviour by aligning financial incentives with desired\noutcomes. By directing capital flows, they possess the distinct advantage of embedding their own enforcement and incentive\nstructures [32]. Regulatory initiatives for AI, on the other hand, have faced increasing criticism for being perceived as anti-\ncompetitive [110] and anti-growth [167]. We contend that rational and responsible approaches to AI governance can align with\neconomic objectives. Policy interventions may prove instrumental in creating a robust market governance ecosystem [84].\nThe analysis of the market governance of AI opens up distinct opportunities for both public and private involvement. Mechanisms\nsuch as insurance, auditing, due diligence, and procurement offer opportunities for both startups and large enterprises to capitalise\non the growing need for AI de-risking which is projected to reach a value of $276 billion by 2030 [164]. These mechanisms also\nafford policymakers and quasi-regulatory entities-including industry consortia, trade associations, and standards organisations\nstrategic pathways for market shaping. Through the deployment of incentive structures, subsidisation programs, public-private\ncollaborations, and standardisation frameworks, policymakers can leverage markets to govern artificial intelligence development\nwhile advancing critical economic, technological, and societal imperatives."}, {"title": "1.1 Governance Through Markets", "content": "By market governance mechanisms, we refer to tools and processes that align financial incentives with desired social or economic\noutcomes by structuring economic behaviour [140]. These mechanisms include (but are not limited to) insurance, auditing,\nprocurement practices, and due diligence - which in turn serve risk distribution, information discovery, protocolisation and capital\nallocation. In the context of Al governance, market governance mechanisms aim to decrease Al risk by pricing, measuring and\nmitigating the risk incurred in Al deployment and development. These mechanisms further enable public and private stakeholders\nto shape Al in ways that align with broader societal and economic goals, fostering a balance between innovation and risk mitigation.\nAs Hadfield and Clark argue [62] AI fundamentally disrupts traditional regulatory frameworks in ways that demand comprehensive\nreconsideration of our approach. Their analysis suggests that the primary challenge isn't simply creating new rules for AI-\nspecific risks, but developing dynamic regulatory approaches that can adapt to Al's pervasive influence. Their proposal for\nregulatory markets can be seen as one possible structure within which market governance mechanisms can operate. Their example\ndemonstrates the broad array of policy strategies available towards market governance of AI - these can range from complete\nself-governance to a government mandates with punitive enforcement.\nMarket governance mechanisms often function as private regulators, for-profit and non- profit organizations that develop and\nsupply regulatory services, which compete to sell to targets, this is the case for instance with the protocol organisations and\nassurance providers [107] we describe later. This is not necessarily always the case, due diligence and insurance have similar\neffects but would could not fall under this label."}, {"title": "1.2 Al Risk as Market Failure", "content": "The uncertainty around Al risk is a major barrier to enterprise adoption of AI technologies [5, 88]. Large enterprise customers are\nuncertain about how to measure, mitigate and govern the risks posed by AI integration into their organisations [36]. Notwith-\nstanding the potential economic advantages, this issue persists as a significant impediment to the dissemination of AI technologies.\nConsequently, organizations have already recognized that AI-related risks are synonymous with business risks and are responding\nin a manner reflective of this understanding. The awareness of these risks and the inability of organisations to address them\nprevents firms and society at large from benefiting from AI technologies.\nIn this vein, we may understand the uncertainty around AI Risk and its mitigation as market failure. The failure, that is, to\nmeasure and mitigate the private costs of AI development and deployment and price its externalities. By Al risk, we imply risk\nfrom the development and deployment of AI systems. This risk to the firm is magnified in how closely Al is being integrated\nto value-generating and infrastructural operations. We use the following formula to quantify this risk-adjusted value [10] of AI\ninvestments:\n$RAV = E(X) \u2013 \\lambda\\cdot \\sigma(X)$\n\u2022 RAV (Risk-Adjusted Value): The value of an outcome adjusted for risk, reflecting the market actor's risk tolerance. It\nrepresents the certainty-equivalent value of the expected return after accounting for the investor's aversion to risk.\n\u2022 E(X) (Expected Value): The average projected financial return, calculated as the probability-weighted mean of all potential\noutcomes. Here, E(X) reflects the private expected returns for a market actor and does not inherently account for externalities\nunless they are explicitly included in the calculation.\n\u2022 \u03bb (Risk Aversion Coefficient): A factor representing a market actor's sensitivity to risk, where higher values denote a\nstronger preference for certainty and a greater aversion to risk. It scales the impact of risk (as measured by \u03c3(X)) on the\nrisk-adjusted value, personalizing the adjustment based on individual or organizational risk preferences.\n\u2022 \u03c3(\u03a7) (Standard Deviation): A measure of outcome variability, indicating the level of uncertainty or risk in financial\nperformance. It quantifies the dispersion of possible returns around the expected value E(X), serving as a proxy for risk or\nvolatility. A higher standard deviation signifies greater uncertainty and potential variability in returns.\nThis formula calculates a risk-adjusted value (RAV) by subtracting a risk penalty (\u03bb\u00b7 \u03c3(X)) from the expected value. While E(X)\nrepresents the average projected return, \u03c3(X) quantifies the uncertainty or variability in potential outcomes. This separation"}, {"title": "2 Risk Distribution: Insurance", "content": "Insurance serves as an enabler of transactions amid radical uncertainty by transferring and mitigating risks associated with\nunforeseen events [86]. In the context of deploying AI systems in economic functions, there is an unavoidable material financial\nrisk due to the probabilistic nature of modern Al models [56], as model outputs are undetermined by their inputs. As AI systems\nbecome more complex, agentic and autonomous, the potential for unintended consequences increases and so do prospective\nfinancial losses.\nInsurance can serve as a powerful governance mechanism, shaping responsible AI development and deployment. This can be\nachieved through risk-based pricing, underwriting standards, and carefully crafted policy exclusions. It also offers potential tools\nto address thorny liability issues surrounding AI, such as joint causation scenarios or damages caused by hacked AI systems. While\ntraditional insurance domains may exhibit relative stability, Al systems present uniquely complex risk profiles characterized by\nboth substantial magnitude and high variance - characteristics that traditionally catalyse insurance market development [94]. The\nconvergence of high-stakes outcomes and inherent unpredictability in AI agent behavior creates natural market conditions for\nnovel insurance products, reflecting the insurance sector's historical role in facilitating technological adoption through systematic\nrisk distribution.\nInsurance has the potential to create a more favourable incentive structure by making practices such as \"safety-washing\" or\nunderestimating AI-related risks less appealing [160]. The rigour of actuarial assessments may engender an anti-speculative\nframework for pricing AI development, curtailing so called 'AI Hype' - much like it has done in other sectors, such as real estate in\nclimate-vulnerable areas [115]. By pricing, mitigating, and transferring risk, insurance can encourage more accurate assessment\nand analysis of AI risk profiles. In so doing, the development of a robust Al insurance market may enable both first-party and\nthird-party stakeholders to more accurately understand the risk profile of AI development.\nCyberrisk provides a crucial precedent to Al risk. It has emerged as the most pressing form of business risk, mirroring the\nindispensability of software in every major enterprise [52]. Accordingly, he financial toll of cyberattacks swells by 15% annually\nup to $23tn by 2027 [44]. Among numerous potential solutions to this pervasive insecurity, insurance offers some respite. A robust\ncyber incident insurance market incentivizes organisations to strengthen their cyber defences, ultimately reducing societal cyber\nrisk [101]. While still in its infancy, cyberrisk insurance has promoted the accurate analysis and pricing of the risk landscape. It\nfurther incentivises the adoption of cybersecurity measures by making plain the financial consequences incurred and reducing\npremiums or disqualifying policyholders with insufficient security measures.\nAs AI becomes integrated to the value creation of major industries, AI insurance will be in increasing demand. Lior et al[95] have\nemphasised the value of building upon existing insurance infrastructure rather than creating entirely new AI-specific policies,\ngiven uncertainties in the field. AI will affect multiple lines of existing insurance, including Technology Errors and Omissions\n(E&O)/Cyber Liability, Professional Liability, Media Liability, and Employment Practices Liability, among others, depending on\nthe specific use case of the AI [8]. The manner in which Al risk is incorporated into insurance frameworks will have significant\nimplications for both insurers and policyholders, shaping the management of AI-related risks throughout the private sector.\nSocietal-scale AI incidents may prompt the development of a CAT-bond market, in order to transfer liability from the insurance\nsector to capital markets. As systemic Al risk increases with adoption this may provide a key backstop for societal resilience.\nCatastrophe bonds decrease insurers' contributions to systemic risk by offloading liability to global capital markets, thus lowering"}, {"title": "2.1 Case Study: Commercial Real Estate", "content": "Commercial real estate (CRE) owners have faced increasing insurance premiums in recent years, prompting behaviour changes to\nmanage these rising costs. Insurance companies have raised their prices to reflect the heightened risks associated with properties\nin high-risk areas, such as those prone to natural disasters [57]. In response, property owners updated valuations to reflect\naccurate replacement costs, made significant property improvements (e.g., installing new roofs), and opted for higher or aggregate\ndeductibles to negotiate better rates. These improvements reduced the overall risk of property damage, which in turn lowered\nthe likelihood of claims. This demonstrates how a market mechanism-higher insurance premiums-successfully incentivized\nrisk-reducing behaviours, ultimately benefiting both insurers and property owners through reduced risk [27]. Additionally,\nalternative risk transfer strategies, such as structured programs, provided fixed premiums for budget certainty. These actions\nhelped CRE owners reduce premiums and ensure greater financial stability [27, 57]. A market actor-a company-changed their\nbehaviour to reduce risk and secure better insurance premiums."}, {"title": "3 Assurance: Auditing and certification", "content": "Third-party auditing is an independent examination of a company's financial statements, operations, or compliance by an external\nauditor to ensure accuracy and adherence to established standards and regulations [90]. Certification often follows auditing,\nserving as formal recognition that a company meets specific industry benchmarks or quality standards. These processes are\nusually made public, creating incentives for companies to maintain high levels of transparency and integrity to build trust with\nstakeholders, including investors, customers, and regulators. The public nature of auditing and certification encourages companies\nto improve their practices and reduce risks, as failing to meet standards can lead to reputational damage or financial penalties.\nConsequently, companies may change their behaviour by implementing stronger internal controls [90]"}, {"title": "3.1 Case Study: Zoom Video Communications", "content": "Following its initial public offering, Zoom Video Communications, Inc. encountered intensified examination regarding security\nvulnerabilities within its telecommunications infrastructure, particularly during the unprecedented utilisation surge accompanying\nthe COVID-19 pandemic [112]. Suboptimal encryption protocols and potential unauthorised data access vectors presented\nsignificant risks to stakeholder confidence and institutional reputation [66].\nIn response, Zoom implemented comprehensive third-party audit and risk management protocols, engaging cybersecurity\nfirms-notably Trail of Bits and NCC Group-to conduct thorough security assessments. These evaluations facilitated substantial\nenhancement of security infrastructure, including the implementation of universal end-to-end encryption protocols, materially\nimproving service security parameters [89]. This case study exemplifies the efficacy of third-party audits as market governance\nmechanisms for risk identification and mitigation."}, {"title": "4 Protocolisaton: Procurement Standards", "content": "Current procurement standards for AI systems are evolving to address challenges of safety, transparency, accountability, and\nsecurity. Governments often rely on private contractors for Al development, necessitating careful planning during procurement to"}, {"title": "4.1 Case Study: NASA Apollo Mission", "content": "The National Aeronautics and Space Administration's (NASA) Apollo Programme (1960s-1970s) exemplifies how rigorous procure-\nment protocols can facilitate the successful execution of complex, high-risk initiatives [48]. This program substantially enhanced\nboth private sector competition and aerospace technological reliability while maintaining safety parameters in mission-critical\ncontexts [152].\nNASA's procurement framework is structured upon the Federal Acquisition Regulation (FAR) and its organisational supplement,\nthe NASA FAR Supplement (NFS). These regulatory mechanisms are specifically designed to address the distinctive requirements\nof space exploration, encompassing stringent reliability metrics, safety protocols, and performance criteria [104]. NASA's uncom-\npromising adherence to safety protocols, precision requirements, and performance criteria necessitated enhanced capabilities\namong contractors [9].\nThese protocols catalysed technological innovations that would establish industry benchmarks for subsequent decades, encompass-\ning advancements in propulsion systems, microelectronic components, computational software, materials engineering, navigational\nsystems, and solar energy technology-innovations that were subsequently adapted for civilian applications [114]. Contemporary\ncommercial aviation and aerospace enterprises, including SpaceX and The Boeing Company, maintain comparably stringent safety\nprotocols per the NFS procurement standards [3]."}, {"title": "5 Investor behaviour: Due Diligence", "content": "Investor due diligence refers to the comprehensive appraisal conducted by investors to assess the potential risks and rewards\nassociated with an investment opportunity. This process involves the systematic evaluation of financial statements, management\ncompetency, market conditions, and regulatory compliance [151]. For Economists, due diligence plays a pivotal role in addressing\ninformation asymmetry-the imbalance of information between corporate insiders and external investors-which can lead to\nsuboptimal investment decisions and market inefficiencies [113]. For Investors, the 'the goal of due diligence is to make the buyer\ncomfortable enough to go through with the deal' [151].\nInvestor due diligence mitigates information asymmetry by enabling investors to obtain and analyse information that may not be\nreadily apparent from public disclosures [34]. This process encourages issuers to maintain accurate and comprehensive records,\nknowing that sophisticated investors will scrutinise their disclosures. Due diligence thus acts as a self-regulating mechanism that\ncomplements formal regulatory frameworks [34].\nDue diligence is not a monolithic process but rather a multifaceted approach which includes:"}, {"title": "5.1 Case Study: BP Oil Spill", "content": "The 2010 BP Deepwater Horizon incident, resulting in the discharge of approximately 4.9 million barrels of oil into the Gulf of\nMexico, represents the most extensive marine oil spill in recorded history [121]. Beyond the catastrophic environmental and\neconomic implications, BP incurred $65 billion in remediation costs and legal settlements (United States Environmental Protection\nAgency, 2024). The financial impact was equally significant: BP's market capitalization decreased by $105 billion, representing a\n55% decline in equity value [64].\nWhile operational safety deficiencies and insufficient corporate oversight precipitated the disaster, subsequent investor due\ndiligence proved instrumental in risk mitigation and catalysed substantial improvements in corporate governance and safety\nprotocols [69]. Investor pressure compelled comprehensive disclosure of financial liabilities and risk exposure, both immediate and\nlong-term. Consequently, BP allocated $5 billion annually toward transitioning from high-risk hydrocarbon operations to renewable\nenergy technologies, demonstrating how due diligence can facilitate strategic reorientation toward sustainable development [172]."}, {"title": "6 Standardised Information as a Foundation for Market-Based Al Governance", "content": "Asymmetry exists between the knowledge held by AI providers and rs and the broader market-including investors, insurers,\nand procurers. Additionally, substantial uncertainty exists for all market participants regarding AI systems[6]. Without adequate\ninformation, markets are unable to make informed decisions [68]. This information asymmetry and uncertainty, may be preventing\nfirms from effectively allocating resources to AI [6].\nAll the market governance mechanisms we examine-including insurance providers, procurement entities, and due diligence\nevaluators-necessitate accurate and reliable data regarding artificial intelligence systems to effectively price AI assets, evaluate\nassociated risks, and facilitate implementation protocols. These stakeholders require comprehensive information to ensure\ncompliance with relevant standards, conduct thorough risk assessments, and optimise asset allocation decisions. We posit that the\nconvergence of these requirements would be optimally addressed through the implementation of standardised disclosure.\nStandardisation of this kind in technology markets often reduces transaction costs, drives competition on value and reduces\nexternalities [?]. Systematic disclosure could foster a robust epistemological foundation that enhances market participants'\ndecision-making capabilities, ultimately enabling more efficient capital allocation and minimising societal risks associated with AI\ndeployment [?]."}, {"title": "6.1 Disclosure Mitigates Information Asymmetry", "content": "Information asymmetry significantly impacts investors' perceptions of novel technological implementation. It can result in both the\nunderestimation and overestimation of \u03bb. In the former, market actors with less information on a technology might underestimate\ntheir aversion, acting as if they are more willing to accept financial risk. The risk aversion coefficient a varies with the degree of\ninformation asymmetry I:\n$\\lambda(1) = \\lambda_0\\cdot e^{s_\\lambda\\cdot 1}$\n\u2022 \u03bb\u03bf: The baseline risk aversion coefficient when information asymmetry is zero (I = 0). This represents investors' standard\nlevel of risk aversion under conditions of full transparency.\n\u2022 sx: The sensitivity parameter indicating how the risk aversion coefficient A responds to changes in information asymmetry\nI. The sign and magnitude of sa determine the nature and extent of this response:\nIf sx > 0: The risk aversion coefficient \u00e0 increases exponentially with increasing I. This means that as information\nasymmetry grows, investors perceive higher uncertainty and become more risk-averse. They may overestimate their\nactual risk aversion, acting more cautiously than necessary due to the lack of information.\nIf sx < 0: The risk aversion coefficient a decreases exponentially with increasing I. In this scenario, as information\nasymmetry increases, investors perceive themselves as less risk-averse, potentially underestimating their true risk"}, {"title": "7 Disclosing Al's development and deployment", "content": "Disclosure refers to the process of releasing all relevant information pertaining to a company. It is the foundation of the afore-\nmentioned market governance mechanisms as disclosure powers auditing, insurance, due-diligence and procurement. Disclosure\ncan be financial or non-financial [111, 163]. Central to financial disclosures are financial statements-comprising the balance"}, {"title": "7.1 Al Disclosures", "content": "Standardisation serves to simplify complex risk areas, rendering them accessible to a broad range of market participants who\nmay lack expertise in AI. By adopting a common framework, Al risks can be articulated in a manner that facilitates more\ninformed decision-making across investment, insurance, and procurement activities. The absence of a standardised approach to\nreporting AI-related risks introduces significant challenges for investors, insurers, and other stakeholders. Without uniform metrics,\nassessing the financial and operational implications of AI risks becomes difficult, resulting in inefficiencies in capital allocation\nand decision-making processes. A standardised risk framework is essential, as it provides consistent and comprehensible metrics\nfor communicating complex Al risks to non-specialists within the market. While such frameworks may not fully capture the\nintricacies of each risk, they serve as a practical means of ensuring that AI-related concerns can be communicated and addressed\neffectively.\nWe argue that the Al research communities can contribute to Al governance by standardising the disclosure of AI. This standardis-\nation requires the consistent description and quantification of key aspects of AI systems - to enable firms, policymakers and other\nactors to make informed decisions.\nProviders, such as developers of foundational general-purpose Al systems like GPT-40, create the underlying models, while\ndistributors use these models to offer products, such as ChatGPT [60]. A growing community of researchers in academia, civil\nsociety, government, and industry are actively studying the risks associated with AI. These risks are not always negative, but rather\ndescriptions of various features of the AI models and organisations that develop or distribute them that are currently uncertain.\nThese risks are increasingly communicated through various market governance mechanisms. For instance, organisations like\nMETR, which function as safety auditors, evaluate the risk profiles of companies like Anthropic or OpenAI [1]. Over this decade,\nthe standardisation of how AI-related information is transmitted through markets will likely solidify, much as we already see with\nperformance benchmarks such as GPQA [131], MATH [71], and MMLU [70] being widely adopted by companies like OpenAI,\nDeepMind, and Anthropic.\nIn a competitive environment, the pressure to reduce risk becomes even more pronounced. Al companies that successfully minimise\nrisk can gain a competitive edge by attracting capital, securing better terms, and winning more procurement contracts. This"}, {"title": "8 Discussion", "content": ""}, {"title": "8.1 Intellectual Property and Disclosure: A Tradeoff", "content": "Artificial intelligence enterprises maintain substantial opacity regarding their operational processes, a phenomenon particularly\npronounced in the semiconductor sector where competitive pressures and industrial espionage present significant concerns\n[29]. However, empirical evidence suggests the possibility of achieving equilibrium between transparency and proprietary\nprotection. Patent systems exemplify such balanced disclosure mechanisms [78]. Standardised globally, patent legislation mandates\ncomprehensive technical documentation of innovations, ensuring replicability by qualified practitioners, as codified in Article 83\nof the European Patent Convention. This reciprocal arrangement confers temporary market exclusivity in exchange for public\nknowledge dissemination.\nThe pharmaceutical industry provides an instructive paradigm for managing the tension between disclosure requirements and\nintellectual property protection. Global pharmaceutical research and development expenditure exceeded $300 billion in 2023,\nrepresenting significant growth from $5 billion in 1980 and $38 billion in 2000 [134]. Leading pharmaceutical corporations, including\nMerck, Novartis, and Roche, maintain substantial R&D investment ratios ranging from 27% to 50% of revenue [133]. Their disclosure\nprotocols prioritise methodological transparency over proprietary formulations [37], encompassing clinical trial methodologies,\nregulatory milestones, development timelines, R&D intensity metrics, and scientific uncertainties, while maintaining competitive\nadvantages through strategic patent protection [117]. This disclosure framework enables investor assessment of future profitability\nwhile establishing realistic expectations. The model has facilitated substantial capital attraction despite inherent developmental\nrisks, supported by the potential for significant long-term revenue from successful therapeutic interventions.\nThe pharmaceutical sector's approach offers valuable insights for AI disclosure frameworks [159]. Strategic transparency-encompassing\nrisk categorization, regulatory compliance milestones, and mitigation protocols-can be achieved without compromising pro-\nprietary algorithms. This methodology, focusing on process disclosure rather than technical specifications, effectively balances\nregulatory compliance, investment attraction, and stakeholder trust. Thus, AI disclosure requirements may be reconceptualized as\nstrategic opportunities rather than regulatory burdens."}, {"title": "8.2 Market incentives for risk mitigation can unlock funding for safety research", "content": "The scale of global asset management presents significant potential for influencing artificial intelligence development trajectories.\nCurrent market data indicates substantial capital pools: hedge funds manage approximately $5 trillion, private equity firms control\n$5 trillion, sovereign wealth funds oversee $11 trillion globally, while U.S. pension funds alone manage approximately $12 trillion\nin assets [139, 169, 170]. If even 1% of assets under management incentivise responsible AI initiatives, this could could mobilise\napproximately $330 billion-a sum that exceeds current philanthropic contributions to AI safety by several orders of magnitude, as\nexemplified by Open Philanthropy's $46 million allocation in 2023 [103]. Although not comparable, redeployment of this kind\ncould fundamentally transform the AI safety and security landscape, enabling expanded research initiatives, enhanced compliance\nframeworks, and accelerated development in interpretability and model safety protocols.\nThis potential capital reallocation can be contextualised against ESG mandates, where projections indicate asset managers will\noversee $33.9 trillion in ESG-focused funds by 2026 [132]. This precedent demonstrates institutional investors' capacity to integrate"}, {"title": "8.3 Scala Politica: Tailoring Intervention to Size without stifling innovation", "content": "Disclosing information about AI Risk is a type of intervention, and to be effective every intervention must be tailored to the size\nof the entity that it is being applied to. Interventions applied to a country, may not apply to a city, and vice versa. Unwanted\noutcomes occur when an intervention designed for an entity of a certain scale, is applied to an entity of a different scale [156]. The\nsame can be said for the difference between the disclosure standards applied to startups and large organisations.\nThe interconnected nature of market actors can create cascading effects throughout the market ecosystem. Institutional investors,\nparticularly private equity firms managing pension fund portfolios, demonstrate how modifications in compliance expectations at\nupper market echelons can fundamentally reshape capital allocation patterns throughout the investment hierarchy. The dramatic\n473.5% increase in Fortune 500 companies identifying AI-related risks underscores this growing concern [51]. Conservative\nprojections suggest that AI-related incidents resulting in merely a 2% devaluation-whether through market capitalization decline,\nregulatory penalties, or customer attrition-could precipitate substantial financial implications, given the magnitude of capital\nexposed.\nFor startups, however, the landscape is starkly different. The financial scale disparity between Fortune 500 corporations and\nventure capital markets is substantial, with the former generating collective revenues exceeding $18 trillion in 2023, while venture\ncapital investments totalled approximately $394 billion in 2022, declining from a peak of $611 billion in 2021 [77]. Due to the\ncomplexities inherent in AI supply chains, there are financial interdependencies between emergent enterprises and established\ncorporations [18]. Major corporations, notably cloud service providers, often mitigate the risks startups face through substantial\npurchase commitments and preferential pricing strategies. While this arrangement currently maintains supply chain stability, its\nsustainability remains contingent upon the continued risk-absorption practices of technology conglomerates.\nStartups face severe financial constraints where compliance costs can drastically affect their operating margins, as evidenced by\nresearch indicating that a 200% increase in compliance costs could shift a startup's operating margin from 13% to -7% [147]. Growth\nof small and emerging enterprises is important for a dynamic economy, vibrant markets, and the preservation of a competitive\nspirit. Small businesses are underappreciated drivers of economic growth, contributing approximately 44% of U.S. economic activity\nalone and creating 62% of new jobs between 1995 and 2020 [125].\nThe imposition of extensive disclosure requirements on early-stage enterprises may impede innovation trajectories. Resource\nconstraints can limit these organizations' capacity to navigate these frameworks, resulting in increased operational costs that\nmay deter market entry and extend commercialization timelines [162]. Excessive regulatory oversight can foster risk aversion,\npotentially inhibiting the experimental approaches essential for technological advancement [97]. Thus the appropriate level of\ndisclosure must be adjusted to an entity's size."}, {"title": "9 Conclusion", "content": "Our analysis demonstrates how market governance can function as instruments for promoting beneficial artificial intelligence\ndevelopment trajectories. These mechanisms represent critical intervention points through which both public and private entities\ncan cultivate incentive structures that favour positive AI outcomes. Tools such as insurance, auditing, procurement, and due\ndiligence, which have long played a role in managing risks and fostering transparency in sectors like finance and pharmaceuticals,\nare now beginning to emerge in the AI industry. Standardised disclosures are playing a growing role in reducing information\nasymmetry, enabling more informed decisions by investors, insurers, and regulators.\nOur examination of market governance of Al reveals promising avenues for exploring Al risk reduction as a key to AI adoption.\nWe encourage further investigation into these approaches by policymakers, economists, and machine learning researchers, as\nthe intersection of their expertise may yield valuable insights for future Al governance interventions. Further research in this\nvein could significantly enhance our understanding of how market governance mechanisms might be optimally designed and\nimplemented to ensure beneficial Al outcomes while maintaining innovation and economic efficiency."}, {"title": "A Al Risk Standardisation Targets", "content": ""}, {"title": "A.1 Data Acquisition and Privacy in Al Systems", "content": "Data acquisition is a foundational step in developing large models, as the quality and diversity of datasets directly influence\nthe performance and reliability of AI systems. This stage requires careful attention to privacy regulation especially given the\nlegal landscapes (e.g., GDPR). As such, standardised reporting in this domain might include metrics related to data provenance,\ndeta"}]}