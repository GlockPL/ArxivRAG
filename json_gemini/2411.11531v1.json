{"title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "authors": ["Viktoriia Chekalina", "Anton Razzigaev", "Elizaveta Goncharova", "Andrey Kuznetsov"], "abstract": "In this paper we present an approach to reduce hallucinations in Large Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional modality. Our method involves transforming input text into a set of KG embeddings and using an adapter to integrate these embeddings into the language model space, without relying on external retrieval processes.\nTo facilitate this, we created WikiEntities, a dataset containing over 3 million Wikipedia texts annotated with entities from Wikidata and their corresponding embeddings from PyTorch-BigGraph. This dataset serves as a valuable resource for training Entity Linking models and adapting the described method to various LLMs using specialized adapters.\nOur method does not require fine-tuning of the language models themselves; instead, we only train the adapter. This ensures that the model's performance on other tasks is not affected. We trained an adapter for the Mistral 7B, LLAMA 2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and demonstrated that our approach improves performance on the HaluEval, True-False benchmarks and FEVER dataset. The results indicate that incorporating KGs as a new modality can effectively reduce hallucinations and improve the factual accuracy of language models, all without the need for external retrieval.\nThe code, models and datasets will be publicly available.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are rapidly evolving technologies in modern artificial intelligence. Despite advancements in training schemes such as RLHF [19] and the resulting significant improvement in conversational skills [31], these models still face the problem of hallucinations - the factual inaccuracy of the generated text [14].\nIn this work, we propose a method to reduce hallucinations in LLMs by incorporating KG as an additional modality. Our approach involves predicting KG embeddings based on the input text and using an adapter to integrate these embeddings into the language model space without relying on external retrieval processes. This process is depicted in the Figure 1.\nWe introduce WikiEntities \u2013 a dataset of over 3 million Wikipedia texts annotated with entities from Wikidata [38] and their corresponding embeddings from PyTorch-BigGraph [21]. This dataset is designed to train a mapping model that maps contextual text embeddings to KGs embeddings and an LLM adapter that integrates these embeddings into the input of the given Language Model.\nOur proposed method can be adapted to any model by simply training a specific mapper from KG embeddings to the embeddings of the desired model, ensuring a straightforward implementation. By applying this setup for Mistral-7B [16], LLaMA 2-7B [37], and LLaMA 3-8B [1], we demonstrate that it improves the models' performance in hallucination reduction and hallucination detection, providing more accurate responses to user queries and maintaining quality of other tasks.\nThus, the contribution of this paper is twofold:\n\u2022 We present WikiEntities a dataset of over 3 million texts from Wikipedia annotated with entities, entity identifiers, and their embeddings. This dataset can be used to train Entity Linking models and it can also extend the described method to any large language model by training specialized adapters for specific models.\n\u2022 We add KG information as an additional modality to the Mistral 7B, LLAMA 2-7B, and LLaMA 3-8B models and show that this approach reduces the model hallucinations while maintaining the same performance on other tasks."}, {"title": "2 Related Work", "content": "KG grounding. LLMs often suffer from high levels of hallucinations and lack interpretability in problem solving [11]. To improve the reliability and factual accuracy of language model responses, several approaches have been proposed, covering various aspects of language modeling and data analysis. The work by Xu et al. [39] involves fine-tuning LLMs for high-quality Wikidata-related questions and answers Huo et al. [15] investigates a method for automatically verifying LLM responses using a corpus. Another study [34] expands the training set with contrastive samples exhibiting different degrees of errors.\nAnother branch of research focuses on incorporating additional knowledge into the inference process. B\u00e9chard and Ayala [4], Lewis et al. [22] propose a retrieval-augmented generation (RAG) scheme via indexing and processing question-relevant documents when responding. While retrieved documents or KG entities are most commonly represented as text, an alternative approach is to represent an additional modality in latent space, which has recently shown promising results for various modalities, from images to audio.\nAdding modality using projection. Incorporating additional modalities into pretrained LLMs using adapters has shown promising results in various domains. One of the first attempts in this direction was the integration of the visual modality into language models using a simple MLP block, as proposed in FROMAGE [20]. In this work, the authors introduced a single embedding that compresses the visual encoder input and projects it into the language model space. The results demonstrated a high level of adaptation, allowing the model to translate information from the visual modality into the language model space and even function as a dialog agent operating simultaneously with both modalities.\nSubsequent studies have built upon this technique by increasing the number of additional tokens representing modalities and employing various encoders for more enriched modality representations [5, 12, 27, 28]. Additionally, more complex adapters such as QFormer have been developed to further improve the integration of these modalities [42]. Apart from visual modalities, other types such as audio and video have also been successfully adapted using similar techniques, as seen in AudioChatLLAMA [9] and other works [24, 25, 35].\nFurthermore, Kale et al. [18] has explored the use of KG in vLM, demonstrating promising results in specific domains such as radiology reporting."}, {"title": "3 Adding KG Modality to LLM", "content": "To improve the factual accuracy of Large Language Models (LLMs), we integrate Knowledge Graph (KG) information as an additional modality. The structure of our pipeline, illustrated in Fig. 1, consists of three components: the LLM (blue), Text2Graph mapper (yellow), which provides linking between the input text and KG embedding space and a Linear Layer acting as a pre-trained Adapter to convert KG embeddings into LLM embeddings (red).\nThe INPUT TEXT (shown in the bottom left of the figure) is tokenized into the LLM embedding space while simultaneously being processed by the Text2Graph mapper. Text2Graph produces KG embeddings, which are then converted into LLM embeddings using a pre-trained Adapter. This additional modality is encapsulated with special tokens <GRAPH_START> and <GRAPH_END>, which are integrated into the input sequence. Based on this input, the LLM generates the OUTPUT TEXT.\nOur approach eliminates the need to fine-tune the LLM itself; instead, we focus on training the Text2Graph module and the embedding Adapter. These two components of the pipeline are trained independently. Text2Graph is responsible for converting text into the selected KG embedding, aligning with the chosen KG embedding. The Adapter, which interacts with the LLM, needs to be trained separately for each distinct LLM.\nTraining Text2Graph mapper and embedding Adapter requires a large dataset of simple texts enriched with KG entities with its corresponding positions. The existing data sets are small [13] or consist mainly of questions and answers [8, 17, 40]. We generated required dataset using a dump of Wikipedia articles."}, {"title": "3.1 WikiEntities \u2013 Texts Enriched with Wikidata Entities", "content": "We begin with parsing the Wikipedia dump\u00b9, processing the pages sequentially. Each page is handled using the PlainTextWikipedia\u00b2 module, which cleans the text of HTML tags and identifies links to related Wikipedia pages. The Wikimapper tool\u00b3 is then used to map these pages to their corresponding Wikidata IDs. Since links can correspond to various Wikidata issues, including categories or other non-entity pages, links that do not match any Wikidata entity are excluded from our data. As a result, processing a single HTML page yields the text of a Wikipedia article with a set of mentioned Wikidata entities and their corresponding positions. An example of this dataset object is shown in Fig. 2."}, {"title": "3.2 Developing the Text2Graph Embedding Mapper", "content": "The Text2Graph Mapper links text with specific KG embeddings, requiring retraining only if the KG embedding space changes. In our study, we use TransE [3] KG representation.\nWe use ROBERTa-large [29] to develop our Text2Graph encoder, which converts text spans containing Wikidata entities into KG. This encoder comprises the RoBERTa-large model, with all weights unfrozen, and an additional linear layer designed specifically to predict graph embeddings from the encoded text representations."}, {"title": "3.3 Training KG Embedding Adapters", "content": "We train a single linear layer Adapter using the WikiEntities dataset. For training objective, we use a standard language modeling task, with a cross-entropy loss. Text2Graph is not involved in this process; as an additional KG modality embedding, we consider Wikidata entities corresponding to the text.\nTraining is performed using AdamW optimizer in combination with a Cosine Scheduler during 1 epoch with Learning Rate 5e-3 and Weight Decay 1e-3.\nThe LLM itself remains frozen during the training process. Additionally, we introduce two special tokens: <GRAPH_START> and <GRAPH_END>, both with trainable embeddings."}, {"title": "4 Experiments", "content": "We evaluate Mistral-7B-v0.1, LLaMA 27B (chat), and LLaMA 3-8B (instruct) models with and without the additional KG modality, measuring their ability to recognize and reduce hallucinations. In the Hallucination Detection setup, the model receives a prompt along with a set of tasks, such as summarization, question answering, fact checking, and corresponding answers. The model's assignment is to is to determine whether any hallucinations are present in the provided answers.\nWe validate our approach on two primary benchmarks - HaluEval [23] and True-False [2] - and Fever [36] dataset. We conducted two types of experiments:\n(1) with the regular model;\n(2) marked as +KG, with the model featuring KG as an additional modality.\nDetailed descriptions of the benchmark experiments are provided in the following sections.\nAdditionally, we engaged the models with questions based on factual information from Wikipedia. Examples of their responses are shown in Fig. 3, which demonstrates that incorporating an additional KG modality enhances the model's ability to answer fact-based questions."}, {"title": "4.1 HaluEval Benchmark", "content": "The Hallucination Evaluation Benchmark for Large Language Models (HaluEval) [23] includes three tasks:\n\u2022 Question Answering (QA) consists of questions and corresponding answers and based on 10000 randomly selected samples from the HotPotQA [40] dataset;\n\u2022 Dialogue consists of questions, answers and dialoque history. It based on 10 000 samples from OpenDialKG [30] - a dataset of conversations between two crowdsourcing agents;\n\u2022 Summarization based on 10000 samples from CNN/Daily Mail [33] datasets and contains texts and its summary.\nIn this benchmark, hallucinated responses are generated using ChatGPT configured with instructions specifically designed to induce hallucinations, while real responses from datasets serve as references.\nWe utilize a model harnessing approach akin to the method depicted in Fig. 1. Our input consists of a text comprising a prompt, a question, and an answer (or, for summarization tasks, a prompt, text, and summary). The model's output is expected to identify whether hallucinations are present. Prompts for each task are obtained from the HaluEVal repository 4.\nFor the QA task, Text2Graph maps only the question to the KG embedding. In Dialogue tasks, it maps the dialogue history. For summarization, the entire text is processed by Text2Graph. Given that the text for summarization often exceeded Roberta's context length, we divided it into chunks matching the context length and performed the KG embedding procedure for each chunk. Consequently, the sequence of KG embeddings for summarization consisted of multiple elements rather than just one.\nThe QA dataset is derived from HotPotQA, where each question-answer pair is linked to entities from the KG. For this dataset, we use the provided entity embeddings rather than generating them with Text2Graph. This option is indicated as \"real KG embs\" in the resulting table."}, {"title": "4.2 True-False Benchmark", "content": "The True-False dataset from [2] is divided into eight topics: Cities, Inventions, Elements, Animals, Generated, Facts, and Companies. It consists of statements that need to be evaluated for reliability. We conducted an experiment akin to the one described earlier, where both the LLM and Text2Graph receive a given statement to verify its correctness. For this dataset, we employed the 8-shot prompt template recommended on the Hallucinations Leaderboard5. The prompt is detailed in Appendix A. The overall quality assessment results are summarized in Table 3."}, {"title": "4.3 FEVER Dataset", "content": "FEVER (Fact Extraction and VERification) [36] comprises 185, 445 claims created by modifying sentences taken from Wikipedia. To evaluate their correctness, we used the scheme outlined in Sec. 4.2. The prompt for this dataset is available in Appendix A, and the results are presented in Table 4."}, {"title": "4.4 Language Understanding and Reasoning Benchmarks", "content": "We further tested our method on standard language understanding and reasoning benchmarks to ensure that integrating KGs does not negatively impact the model performance on other tasks.\nThe models were evaluated on six commonly used benchmarks, including MMLU [10], GSM8k [7], TruthfulQA [26], Winogrande [32], HellaSwag [41], and ARC [6]. The results are shown in Table 5. We used the Eleuther AI Language Model Evaluation Harness framework [10] to evaluate our model across six commonly-used benchmarks:\n\u2022 AI2 Reasoning Challenge (25-shot): Grade-school level science question set (log-likelihood);\n\u2022 HellaSwag (10-shot): Common sense inference-oriented problems (log-likelihood);\n\u2022 MMLU (5-shot): Multiple-choice questions assessing the model's ability to solve college-level tasks in a variety of scientific disciplines (log-likelihood);\n\u2022 TruthfulQA (6-shot): A test measuring the model's tendency to reproduce false information commonly found online (log-likelihood);\n\u2022 Winogrande (5-shot): The Winograd benchmark for commonsense reasoning (log-likelihood);\n\u2022 GSM8k (5-shot): Grade school math word problems that assess the model's multi-step mathematical reasoning abilities (generative).\nThe number of shots in brackets indicates the number of examples shown to the model before presenting the final question.\nIn Table 5 we show the results of the evaluation on the above benchmarks for two versions of the models: with (+KG) and without injection of KG."}, {"title": "4.5 Discussion", "content": "In the HaluEval benchmark, LLaMA 2-7B with KG showed the most significant improvement, achieving an increase of approximately 10% in both the QA and Dialogue tasks. LLaMA3-8B suceeds on the summarization task, showing an improvement of approximately 4%. Although LLaMA 3's performance on the QA task was not as good as the other models, it excelled on the Dialogue task, outperforming the other models by 10%. This discrepancy may be related to the nature of the questions: QA primarily features questions that require precise answers (\u201cWhich tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\u201d), whereas the Dialogue task involves more abstract questions (\"Do you have any thing similar to the story Who Moved My Cheese?\"). Mistral showed a consistent improvement of approximately 1.5-2% across all tasks. On average, without KG modality this model performs better than LLaMA 2, but slightly worse than LLaMA-3. When adding KG, the performance hierarchy changes: LLaMA3 performs the best, followed by LLaMA2, with Mistral trailing behind. Using provided entity embeddings in QA task performs slightly worse than predicting KG embedding by Text2Graph linker.\nIn the True-False benchmark, including the KG modality also improves the models' ability to recognize false statements in each topic set. The best performance is shown by LLAMA 3-8B, followed by Mistral, and then - LLaMA 2-7B. The similar case is for FEVER: we get the best growth on Mistral (\u2248 8%), then LLaMA 2 (\u22483%) and LLAMA 3 (\u2248 1%).\nDatasets from Section 4.4 are more focused on assessing the LLM's ability to construct relevant reasoning or perform graphical analysis rather than on fact cheking. For example, questions like \"When will the production frontier be a straight line?\" (MMLU) and \"What color is the sun when viewed from space?\" (TruthfulQA) are used. Such question do not explicitly contain information from Wikipedia, so we did not expect our approach to enhance quality. However, it is important to ensure that adding KG information does not affect language comprehension and reasoning LLM ability.\nIn general, overall results suggest that incorporating KGs as an additional modality is a viable approach to improve the factual accuracy of language models without compromising their performance on other task."}, {"title": "5 Conclusion", "content": "We have introduced a method for integrating a KG modality into Large Language Models (LLMs). The method does not require modifications to the LLM or additional graph structures and retrieval techniques; it only needs a Text2Graph linker and a linear layer as an Adapter. This approach helps reduce hallucinations in language models such as Mistral-7B, Llama2-7B, and Llama3-8B without the performance degradation on other tasks. Additionally, we have developed the WikiEntities dataset, which includes over 3 million annotated texts. This dataset can be helpful for Entity Linking models and it also allows us to adapt our KG integration method to different language models.\nThe source code, link to models and dataset will be available at anonimized repository.6"}, {"title": "6 Ethical Statement", "content": "We used pretrained models from the Hugging Face repository. To avoid potential harm or any ethical issues, we exclusively used open-source datasets and publicly available source code. By prioritizing ethical standards and acknowledging potential risks, we aim to promote responsible and sustainable research practice."}]}