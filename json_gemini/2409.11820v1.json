{"title": "OPTIMIZING JOB SHOP SCHEDULING IN THE FURNITURE\nINDUSTRY: A REINFORCEMENT LEARNING APPROACH\nCONSIDERING MACHINE SETUP, BATCH VARIABILITY, AND\nINTRALOGISTICS", "authors": ["Malte Schneevogt, M.Sc.", "Dipl.-Ing (FH) Karsten Binninger, M.Sc.", "Prof. Dr.-Ing. Noah Klarmann"], "abstract": "This paper explores the potential application of Deep Reinforcement Learning (DRL) in the furniture\nindustry. In order to offer a broad product portfolio, most furniture manufacturers are organized\nas a job shop, which ultimately results in the Job Shop Scheduling Problem (JSSP). The JSSP\nis addressed with a focus on extending traditional models to better represent the complexities of\nreal-world production environments. Existing approaches to JSSPs frequently fail to consider critical\nfactors such as machine setup times, varying batch sizes, intralogistics, buffer capacities, or deadlines,\nwhich are essential in industrial settings. In order to overcome these limitations, a concept for a model\nis proposed that incorporates these elements, providing a higher level of information detail to enhance\nscheduling accuracy and efficiency. The concept introduces the integration of DRL for production\nplanning, which is particularly suited to batch production industries such as the furniture industry.\nThe model extends traditional approaches to JSSPs by including job volumes, buffer management,\ntransportation times, and machine setup times. This enables more precise forecasting and analysis of\nproduction flows and processes, accommodating the variability and complexity inherent in real-world\nmanufacturing processes. In a training environment the Reinforcement Learning (RL) agent learns to\noptimize scheduling decisions. The agent operates within a discrete action space, making decisions\nbased on detailed observations of machine states, job volumes, and buffer statuses. A reward function,\nspecifically tailored to the specific industrial applications, guides the agent's decision-making process,\nthereby promoting efficient scheduling and meeting production deadlines. Two integration strategies\nfor implementing the RL agent are discussed: episodic planning, which is suitable for low-automation\nenvironments, and continuous planning, which is ideal for highly automated plants. While episodic\nplanning can be employed as a standalone solution, the continuous planning approach necessitates\nthe integration of the agent with Enterprise Resource Planning (ERP) and Manufacturing Execution\nSystems (MES). This integration enables real-time adjustments to production schedules based on\ndynamic changes.", "sections": [{"title": "1 Introduction", "content": "The optimization of production planning is a crucial aspect of industrial manufacturing processes. It increases the overall\nproduction efficiency, ensures timely delivery, and reduces costs by utilizing resources effectively. In recent years,\nReinforcement Learning (RL) has emerged as a powerful tool in solving complex optimization problems across various\ndomains. Its ability to learn optimal policies through interaction with environments has revolutionised diverse domains,\nshowcasing its versatility and effectiveness. In robotics, RL has been utilized for autonomous navigation, manipulation,\nand control tasks [1][8], in gaming, it demonstrates superhuman performance in complex games, such as Go [19] or\nDota 2 [10]. RL offers several advantages over traditional optimization methods, including its adaptability to dynamic\nenvironments, its ability to handle large state and action spaces, and its capability to learn from experience without\nrequiring explicit problem knowledge. These advantages make it an ideal tool for industrial process optimization.\nDue to its high level of complexity, the furniture industry requires a particularly diligent and optimized production plan-\nning. Its products are composed of numerous individual components, each of which undergoes separate manufacturing\nprocesses. A furniture article can be made from hundreds of components, each with an individual path through the\nproduction. Some components are simply purchased parts that do not require any further processing in the factory,\nwhile other parts involve complex manufacturing processes among many different machines. These processes need to\nbe coordinated and integrated during production, particularly if various components or articles are produced on the\nsame production lines, resulting in the batch production of products or components. In order to produce a wide range\nof products, most furniture shops are designed as job shops. This setup offers a high degree of flexibility for batch\nproduction, but inherently leads to the Job Shop Scheduling Program (JSSP), a Combinatorial Optimization Problem\n(COP) known from process optimization.\nIn a JSSP a set of n jobs $J = \\{Jo, J1, J2, ..., Jn \\}$ is to be processed on m machines $M = \\{M1, M2, ..., Mm\\}$. Each\nmachine can only process one operation at a time. Each job is assigned with a specific machine sequence that must be\nfollowed during the production of the particular product. For each machine, the operations have specific processing\ntimes $d_{ij}$ where $i \u2208 (1,m)$ and $j \u2208 (1,n)$. The total number of operations O is $n \u00d7 m$. The number of possible\nschedules in a JSSP is growing exponentially by $(n!)^m$. Common scheduling heuristics such as First-Come-First-Serve\nor Earliest-Due-Date-First become ineffective as the number of jobs and machines increases, and computation times\nbecome unfeasible due to the exponential growth of combinatorial possibilities. Algorithms such as Genetic Algorithms\n[14], Simulated Annealing [23], or Taboo Search [21] struggle to effectively solve the JSSP due to its NP-hard nature\nand the presence of complex dependencies among operations and resources [15].\nDeep Reinforcement Learning (DRL) was successfully applied to the JSSP on several occasions, finding competitive\nsolutions for JSSP benchmark problems [26, 6, 9, 27, 25, 17]. DRL is a machine learning technique that combines deep\nlearning and RL to enable agents to learn and make decisions in complex environments by using neural networks to\nprocess and act on high-dimensional data. The agent takes actions based on the observations and its policy, receives\nrewards as feedback, and adjusts its neural network parameters in order to maximize cumulated future rewards, the\nso-called return (cf. Figure 1). This process enables the agent to learn optimal behaviours for complex tasks in\nhigh-dimensional spaces."}, {"title": "2 State of the Art", "content": "As early as 1995, Zhang und Dietterich [28] explored the application of RL techniques to the JSSP by further developing\nthe results of the scheduling algorithms by Deale et al. [3], who used a simulated annealing approach for job shop\nscheduling. The system learns to make informed decisions that lead to more efficient schedules and thereby shows the\npotential of RL to solve JSSPs.\nIn 2000, Aydin and \u00d6ztemel [1] presented an approach that is composed of a simulated job shop environment and a\nRL agent that selects the most appropriate priority rule from a set of available rules to assign jobs to machines. The\naim of their work is to provide a flexible and adaptive approach to job shop scheduling, capable of handling a dynamic\nmanufacturing environment, coming one step closer to a fully automated, intelligent manufacturing system.\nGabel and Riedmiller [4] introduced a novel approach to solve JSSPs in 2012, by using distributed policy search\nRL. This multi-agent approach treats the JSSP as a series of sequential decision-making tasks, where each RL-agent\noperates autonomously and uses a probabilistic dispatching policy for decision making. These dispatching policies are\nrepresented by a small set of real-valued parameters that are continuously adapted and refined, to enhance the overall\nperformance of the scheduling process. Although the computation time was reduced, the achieved solutions were not\nbetter than those of conventional solvers.\nIn 2008, Pezzella et al. [14] presented a genetic algorithm that solves the Flexible Job Shop Scheduling Problem (FJSP).\nUnlike in a JSSP, the operations of a FJSP can be assigned to one of several machines, instead of only one specific\nmachine. Due to the additional decision layer of selecting machines for each operation, the complexity of a FJSP is even\nhigher than in a JSSP. The presented algorithm outperformed existing models and traditional dispatching rules. Their\napproach utilizes DRL to tackle the problem more effectively by including innovative approaches for representation\nlearning and policy learning. They demonstrated that genetic algorithms are effective in solving FJSPs.\nFoundational research in the field of DRL for continuous control tasks was presented by Lillicrap et al. [8] in 2015. This\nresearch expands the capabilities of deep learning beyond the discrete domain to tasks where actions can take any value\nwithin a continuous range. They employ an actor-critic architecture where the actor generates actions, and the critic\nevaluates them based on a learned value function. Also, they use a replay buffer to store and reuse past experiences to\nmimic successful techniques.\nShahrabi et al. [18] address the complex problem of dynamic job shop scheduling, where job arrivals are unpredictable\nand machine breakdowns occur randomly. The paper presents a Q-factor algorithm that optimizes scheduling decisions\ndynamically. The authors employ a variable neighbourhood search to explore the solution space and identify the most\neffective scheduling method. By using RL, the authors determine the optimal parameters for rescheduling processes in\nresponse to changes in the environment. This approach is designed to address real-world challenges in manufacturing.\nThe results demonstrate the significance of their method for a dynamic job shop environment, as the optimal strategies\nare also updated dynamically.\nBy employing Google DeepMind's DQN agent algorithm present a successful application of RL to production\nscheduling. Their framework consists of multiple DQN agents that cooperate and learn to achieve the defined objectives,\nresulting in self-organized, decentralized manufacturing systems that are capable of adapting to a dynamic manufacturing\nenvironment. After a short training phase the system presents scheduling solutions that are on par with solutions based\non expert knowledge. Even though the approach cannot beat heuristics, this research represents a significant step\ntowards the application of AI to real-world industrial processes and intelligent production systems.\nFurther developing both Gabel and Riedmillers [4] and Lillicrap et al.'s approach [8], in 2020 Liu et al. [9] utilized an\nactor-critic DRL-architecture to approach the JSSP as a sequential decision-making problem. The model consists of an\nactor network and a critic network, including convolution layers and fully connected layers. The actor network learns\nhow to act in a dynamic environment, while the critic network evaluates these actions. This approach is effective in\nmanaging unexpected events such as machine breakdowns, additional orders or material shortages that may interrupt the"}, {"title": "3 Methodology", "content": "contributions and developments in the field that are improving the efficiency and flexibility of production processes.\n89% of the benchmarked implementations increase the scheduling performance, reached lower total tardiness, higher\nprofits, or other problem-specific objectives. In the field of production scheduling, 67% of the reviewed papers applied\nvalue-based algorithms.\nWith the growing interest in using RL methods for production scheduling, it becomes increasingly challenging or\neven impossible to reproduce existing studies with the same degree of accuracy. To make the research more widely\napplicable and to exploit its strengths for industrial applications, Rinciog and Meyer [16] propose to standardize the\napproaches. They propose a framework for applying RL in this context by modelling production scheduling as a MDP.\nThe standardization is done in three steps: The standardization of the description of production setups used in RL\nstudies is based on an established nomenclature. This is followed by the classification of RL design decisions from\nexisting publications. Finally, recommendations for a validation scheme that focuses on reproducibility and sufficient\nbenchmarking are proposed.\nA novel algorithm for improving the generalization capabilities and solution effectiveness of a DRL agent that solves\nJSSPs is proposed by Vivekanandan et al. [24]. The authors introduce a new method called Order Swapping Mechanism\nto achieve better generalized learning. By using a set of known benchmark instances [21] they compare their results\nwith the work of other groups [6][27][22] that used the same benchmark instances and demonstrate that this approach\noutperforms previous methods. The results demonstrate that the agent does not outperform the approach of Tassel et al.\n[22], yet it does provide a size-dependent generalization. It outperforms the PDR based DRL approach of Zhang et al.\n[27] and performs similarly to other state-of-the-art DRL algorithms.\nSerrano-Ruiz et al. [17] present a method for scheduling in a quasi-realistic job shop environment. They create a digital\ntwin of the job shop model as a MDP and use DRL for optimization. Their approach uses a deterministic framework\nfor formulation and implementation and is validated by comparison with known heuristic priority rules. Experiments\nshow that the model not only captures the benefits of heuristic rules but also leads to a more balanced performance\nacross various indicators, outperforming traditional heuristic methods.\nThe analyzed papers demonstrate that the JSSP can be effectively solved by using various DRL approaches. However,\nmost of the papers are based on simplified models with little relevance to the reality of production. In this reality, a large\nnumber of factors play a role, the quantification of which is sometimes difficult and can have a significant influence\non the effectiveness or success of the modelling. The decisive factors and their influence on production planning are\ndescribed in more detail in Section 3."}, {"title": "3.1 Necessity for an Extended Approach", "content": "Despite the existence of various RL approaches proposed by different researchers with the intention of solving generic\nJSSPs in industry-relevant problem sizes, it remains a challenge to translate the complexity of a real-world production\nenvironment into generic JSSPs. The following complexities are identified throughout this work:\n1. Generic JSSPs typically involve machines that process various jobs without any specific machine setup. It is\ntherefore necessary to reduce the time required for machine setup to a minimum, as this can otherwise result in\na significant loss of production time.\n2. Jobs are typically defined by a machine sequence and a fixed processing time. In the case of varying batch sizes,\nthe processing times may be approximately linearly dependent on the batch size. Previous approaches did not\nconsider varying batch sizes or processing times. With an enhanced generalization capability, contemporary\nDRL agents should be capable of accommodating varying processing times.\n3. The field of intralogistics is not included in the scope of a generic JSSP. The transportation times between\ndifferent machines or production facilities can be considerable, often taking several minutes, and therefore\nplay an important role in the scheduling process.\n4. In a real-world production environment, a variety of storage spaces can be found to buffer inconsistencies in\nproduction processes or to increase production flexibility. The dimensions of these buffering zones were not\nconsidered in previous approaches. An overfilled buffer zone may impede the entire production process and\nthus necessitates consideration in the scheduling process.\n5. Deadlines are frequently absent from models, even though they are an essential component of ensuring the\ntimely delivery of products."}, {"title": "3.2 Research Hypothesis and Objective", "content": "Based on a comprehensive analysis of an existing furniture factory, a concept for the implementation of RL in production\nplanning is proposed. In order to formulate a general concept that is valid for a broad industry, the following constraints\nare given:\n\u2022 The production is set up as a Job Shop\n\u2022 The machines produce products in batches\n\u2022 The batches are manufactured on a recurring basis, but the production is frequently switched to enable the\nproduction of a wider range of products\n\u2022 The overall range of products doesn't change drastically after training, as this would require a re-training of\nthe agent. Minor changes like \"color changes\" would not disrupt the production process."}, {"title": "3.3 The Model", "content": "Generic JSSP-models are typically described by n jobs $J = \\{J1, J2, ..., Jn \\}$, where each job has m operations O\n($Ji = \\{0_{i1}, O_{i2}, ..., O_{im}\\}$) to be processed on m machines $M = \\{M1, M2, ..., Mm\\}$. Each operation has its designated\nprocessing machine and time $d_{ij}$. This model is extended by the introduction of the following elements:\nJob Volumes The volumes of the jobs before and after each operation are quantified and mapped. With knowledge of\nthe volumes of each job at any given point in the production process, the required storage spaces can be estimated with\ngreater precision. This information is used to forecast and analyze the utilization of the buffers.\nBuffers In a production system, storage areas are distributed across the shop floor and are used to hold materials,\nunfinished, or finished goods between different stages of production or between production and shipping. The primary\npurpose of these areas is to absorb variability in the production process, including fluctuations in demand, supply\ndisruptions, machine breakdowns, or other unforeseen circumstances that may impact the production schedule. These\nstorage areas are defined as buffers and are used to store the jobs before being processed at a machine. Each buffer is\ncharacterised by its capacity, which may be expressed in various units, including storage volume, pallet storage capacity,\nor any other meaningful unit. Buffers serve to absorb variability in the production process, enabling a more flexible\nproduction, preventing bottlenecks and thereby smoothing out the production flow.\nQuantity Factor \u03b4 The processing time of each operation is subject to significant variation due to fluctuations in batch\nsizes, which are quantified using the quantity factor \u03b4. The quantity factor is employed to determine the duration for\nwhich a machine is occupied in processing a component, particularly when batch sizes exhibit considerable variability.\nThe total processing time of an operation is calculated as follows:\ntotal processing time of an operation = \u03b4 \u00b7 $d_{ij}$ with $d_{ij}$ as the processing time of one single element of an operation\n(1)\nThis methodology enables the calculation of the processing time of a job on a machine based on the actual batch size of\na job.\nTransportation Times t In order to describe the intralogistic processes, transportation times are introduced. These\nfigures represent the time required to transport a job from one machine to the next. In the majority of cases, the\ntransportation times between two points are symmetrical, as the required transportation time is independent of the\ndirection of travel.\nMachine Setup Times s A machine that is involved in the production of various jobs may require different setups in\norder to perform the specific operations. These setup times are considered, when the production requires a switch from\none setup to another."}, {"title": "Overall Processing Time Ttotal", "content": "With the introduction of the quantity factor \u03b4, the transportation time t and the setup\ntime s, the overall processing time of an operation can be calculated as follows:\n$T_{total} = \u03b4\u00b7d+t+s$\n(2)\nDeadlines The deadline is the latest point in time at which the production of a job must be completed. The deadline\ncan refer to a shipping date or an upcoming maintenance schedule for a machine or a complete production line. Failure\nto meet the deadline can result in several consequences, making it an important factor to consider in production planning.\nThis extended approach allows for the acquisition of more detailed information."}, {"title": "3.3.1 Environment Outline", "content": "The job shop environment can be implemented using toolkits such as OpenAI Gym and libraries such as Stable-\nBaselines3, which are specifically designed for developing and comparing reinforcement learning algorithms. In the\ntraining environment, an agent learns to solve the problem and optimize its policy parameters by interacting with the\nenvironment through actions."}, {"title": "3.3.2 Action Space", "content": "The environment is controlled by a single discrete action space, through which the agent selects the appropriate jobs for\nprocessing on a given machine at each time step. The agent's choices are limited to the subset of available jobs and\nmachines, ensuring that the agent's decisions are both relevant and feasible in the context of the current operational\nparameters and machine availability. The actions taken by the agent modify the environment and change its state."}, {"title": "3.3.3 Observations", "content": "The decision to assign a machine to a job is based on a set of observations that is provided to the agent. Each observation\nrepresents the current state of the environment. It is updated at each time step and holds the following information:"}, {"title": "(1) machine info", "content": "A matrix of dimensions 3 \u00d7 m, which contains information about the currently processed jobs, the progress of the\noperations, and the current machine setup. Given that m represents the number of machines in the environment, each\ncolumn of the matrix represents a machine. The first line contains information about the currently processed jobs, the\nsecond line contains information about the remaining operation time and the third line contains information about the\ncurrent machine setup.\n(1)$_{t0}$ =\n\\begin{bmatrix}\n0 & 0 & 0\\\\\n0 & 0 & 0\\\\\n  &   &  \n\\end{bmatrix}\nMachine 1 Machine 2 Machine 3"}, {"title": "(2) job info", "content": "A matrix of dimensions 2 \u00d7 n, containing information about the current job volumes and the remaining time until the\ndeadline is reached. With n representing the number of jobs in the environment, each column represents a job. The first\nline contains information about the current job volume, while the second line contains information about the remaining\ntime until the deadline is reached.\n(2)$_{t0}$ =\n\\begin{bmatrix}\n30 & 10 & 20\\\\\n120 & 110 & 100\n\\end{bmatrix}\nJob 1 Job2 Job 3"}, {"title": "(3) buffer info", "content": "A b-dimensional vector represents the capacity status of the buffers in the environment with b representing the number\nof buffers in the environment. This vector can reflect the utilization of the buffers in units or in percent, depending on\nwhat is most appropriate for the model.\n(3)$_{t0}$ =\n\\begin{bmatrix}\n60 & 0 & 0\n\\end{bmatrix}\nBuffer 1 Buffer 2 Buffer 3"}, {"title": "3.3.4 Transition Probability Function", "content": "Transition probabilities represent the likelihood of transitioning from one state to another after taking a given action.\nThe transition probabilities are estimated based on the experiences of past actions and resulting observations in the\ntraining environment. They are then continuously adapted and honed in the training phase. This function is capable of\ncapturing the dynamics of the system under different observations."}, {"title": "3.3.5 Reward", "content": "The reward function is employed to quantify the system performance and guide the decision-making process. It must be\ntailored closely to the goals of the specific industrial application, as it is highly sensitive. The reward corresponds to the\nscheduling goal and defines the specialization of the agent. In order to maintain a high level of learning efficiency, it is\nnecessary for the agent to be rewarded in a densely manner. This implies that the agent is rewarded for actions that\nare specific to the JSSP, such as achieving a shorter overall processing time. Additionally, the agent may be rewarded\nfor actions that are specific to its industrial application. One method of enhancing the learning process is to provide\nthe agent with a negative reward for unwanted actions, such as overfilling a buffer or failing to meet a job's deadline.\nIdentifying the optimal reward function is an iterative process that must be conducted on an individual basis.\nDiscount Factory As with the reward function, the discount factor must be set according to the specific application\nin question. The discount factor between 0 and 1 determines the relative importance of future rewards in the decision-\nmaking process. A discount factor of 0 indicates that the agent is short-sighted and only considers immediate rewards,\nwhile a discount factor close to 1 implies that the agent values future rewards to a similar extent as immediate rewards.\nIn order to identify the optimal policy, the application of PPO balances the trade-off between policy improvement and\nstability."}, {"title": "4 Integration in the Furniture Industry", "content": "In a general JSSP, each job is associated with a unique set of tasks that must be completed in a specific order. This\nconcept can also be applied to the production of furniture, where articles are composed of several components that\nmust be processed in a specific sequence on various machines. Consequently, each article corresponds to a group\nof jobs, with each component considered being a separate job. In order to determine this set of jobs, it is necessary\nto break down each article into its various components and determine their respective machining sequences. The\nidentification of production information, such as the individual machining sequence, volume alterations, and processing\nduration for each component at every production step, is typically obtained from the so-called Bill of Materials (BOM),\nwhich is a commonly utilized document in the industry. The level of detail provided by the BOM determines the\ncomprehensiveness of the production information. Obtaining such information is indispensable for the development\nof optimized production planning. Given the intricate nature of furniture production systems, the implementation of\nDRL-supported production planning necessitates a comprehensive analysis of the factory in question. Consequently,\nthe presented solutions thereby may not be readily transferable to all furniture production facilities, as each facility will\nrequire a solution that is precisely tailored to the factory and its individual scheduling goals. This chapter presents an\nexample of how a RL-agent can be employed for production scheduling in the furniture industry, where components\nare manufactured in batches in a job shop environment. For that purpose, the elements of the extended model that is\npresented in Chapter 3 may be represented as follows:\nJobs A piece of furniture is typically constructed from multiple components, which are then assembled or packaged\nat a designated station in order to create the final product. In the context of the job shop analogy, each component\nrepresents a distinct job with its own machine sequence and processing time for each operation. Therefore, the furniture\narticles are broken down into individual components. For each component, the processing sequence on the respective\nmachines, the processing time, and the machine setups are mapped. If no further processing occurs within the factory,\npurchased parts may be excluded from this analysis. Each job is then defined by its machine sequence, machine setup,\ntotal processing time (cf. 3.3), deadline, and its volume, which undergoes change throughout the production process.\nJob volumes It is common in furniture production, particularly for solid wood furniture, to undergo significant volume\nchanges throughout the production process. The volume of the raw material at the beginning of the process may be up\nto five times greater than that of the end product. In order to calculate the space requirements of each job at any point\nduring production, it is necessary to record the volume change in each process step.\nBuffers Each buffer on the shop floor is assigned to a machine and located in front of it in the process flow. Buffers\nare treated similarly to machines, but they can store several jobs simultaneously and have no processing time. The\ncapacity of these buffers is specified in terms of the number of storage spaces for pallets or by volume.\nMachines Another aspect of the environment is the machinery used for production. Each machine is mapped in detail,\nincluding its features, processing times, possible setups, and setup times required for each operation.\nQuantity Factor \u03b4 The quantity factor \u03b4 is employed to modify batch sizes, which have a significant impact on\nprocessing time and, consequently, the make span of a job. Training the agent with varying batch sizes enhances its\ngeneralization capability, enabling it to handle varying job sizes. It has been observed that the production of furniture\ndoes not always occur in consistent batch sizes. The quantity factor helps determining the duration of a machine's\nutilization in the processing of a component.\nTransportation Times The transportation times between machines and buffers in the production are mapped. This\nincludes a full analysis and documentation of the transportation times between the machines on the shop floor. With\nthis information, the overall completion time of an order can be determined.\nDeadlines Each job is described with a deadline that specifies the latest point in time by which its processing must be\ncompleted.\nEnvironment The production environment is mirrored as closely as possible by the training environment, which\ncomprises jobs, machines, and buffers. The agent is trained to set up machines, assign jobs to machines, and prevent\nbuffers from overflowing, all while maintaining the correct machine order for each job and meeting the production\ndeadlines.\nAction Space The agent is controlling the environment in a single discrete action space. At each time step suitable\nmachines and buffers are determined for the available jobs."}, {"title": "4.1 Integration Strategy", "content": "The integration of an RL agent must be compatible with existing production and planning systems and be able to be\nseamlessly integrated without disrupting ongoing production. Once the training environment has been established\nand the agent has been trained in accordance with the desired optimization goals, it can then be utilized to support\nproduction planning. Two principal concepts may be distinguished with regard to the integration: episodic and\ncontinuous production planning. The decision to utilize either episodic or continuous planning depends on various\nfactors, including the nature of the production process, the degree of variability in orders, and the scheduling system's\nrequired flexibility."}, {"title": "4.1.1 Episodic Planning", "content": "In companies with low levels of automation and networking, episodic planning represents an appropriate approach.\nIt may be employed when orders are processed in batches or when the production line is configured to manufacture\nspecific types of furniture for a defined period before transitioning to a different type. The integration of an episodic\nplanning system is less complex than in a continuous planning system (cf. 4.1.2), as it does not require interfaces with\nexisting production systems. The planning process is divided into distinct episodes, each with a clear beginning and\nend. These episodes may be defined as the production of a week or the production of an individual order from a specific\nclient. Each episode consists of a limited number of steps and actions, making it easier to predict and evaluate outcomes.\nThe orders are entered into a dashboard, which includes information on quantities and deadlines. Alternatively, a\nspecific time period may be selected for scheduling."}, {"title": "4.1.2 Continuous Planning", "content": "In a continuous planning approach, the agent is fully integrated into the production system and plays an active role\nin the scheduling process. This approach is suitable for highly networked and automated plants where production is\nsubject to significant variations in orders, requiring production schedules to be adjusted on the fly to accommodate\nnew orders, changes in design, or material availability. This integration comes at the cost of a significantly higher level\nof complexity: The agent is situated between the Enterprise Resource Planning (ERP)-system and the Manufacturing\nExecution System (MES) (see Figure 7). It processes real-time production data from both systems and adjusts the"}, {"title": "Limitations of Continuous Planning", "content": "optimized production planning frequently requires the simultaneous considera-\ntion of multiple objectives, including a minimized lead time, maximized machine utilization and minimized inventory"}, {"title": "4.2 Implementation Strategy", "content": "The following procedure is proposed for the successful implementation of a DRL-supported production planning\nsystem:\n1. preparation and planning: A detailed examination of the interfaces between the systems points out if specific\nadapters need to be developed. Furthermore, the data flow required to exchange relevant information between\nthe agent and the systems must be defined. This may include production schedules, machine statuses, order\ndetails, and other system-specific information.\n2. goal definition: Measurable results are achieved by the precise definition of the agent's goals and objectives.\nThese goals may be the optimization of the throughput, the reduction of production costs or other specific\ngoals.\n3. definition of the state space: This includes defining the relevant variables and factors that influence the\nstate of the production system as well as defining the data flow, i.e. how the required information is received,\ntransmitted, and processed by the agent. It is essential that the observations include all information relevant for\nthe agent's decision-making process.\n4. definition of the action space: A precise definition of the actions that the agent can perform to influence the\nsystem's state. Examples of actions include the adjustment of machine setups, the assignment of machines and\njobs or a simple no-op (no operation).\n5. definition of the rewards and penalties: In RL, the agent is trained through the provision of feedback in the\nform of rewards and penalties. By clearly defining the reward function, the specialization of the agent can be\nspecified. The reward definition should represent the goals defined at the beginning of this procedure. This\nmay be an iterative process in the training phase. An example could be a reward for an increased throughput or\na penalty for a buffer overfill.\n6. agent training: In the training phase the agent interacts with the training environment that was previously\ndefined in the procedure. The agent uses RL algorithms to learn to make decisions that optimize its received\nreward.\n7. test environment: A test environment may be developed to simulate the production environment and test the\nagent under controlled conditions, thereby reducing the risk of production downtime. The development and\noptimization of the agent is conducted in cooperation with experienced production planners of the manufacturer.\nConducting a pilot project in parallel with the existing systems allows for the evaluation of the agent's decisions\nand its impact on production processes.\n8. deployment and scaling: Once the agent has been trained it can be deployed in the production system. This\nprocess is conducted in a systematic and sequential manner, starting with less critical processes in order\nto minimize potential risks. A feedback loop is established for the continuous evaluation of the agents'\nperformance and adaptation to the changing environment, based on real production data.\n9. employee training and change management: Staff is trained on the use of the new system, to check the\nproposed schedules, and override the decisions made by the agent. The implementation of change management\nstrategies ensures the effective utilization of the new system by the employees and strengthens their acceptance\nof these new technologies.\n10. maintenance and continuous improvement: Regular reviews and adjustments of the agent ensure an optimal\nperformance and adaptation to the constantly changing production conditions."}, {"title": "5 Conclusion and Outlook", "content": "This paper presents a concept for DRL-supported production planning that can be adapted for optimized job shop\nscheduling. In Chapter 3", "systems": "episodic and continuous planning\nsystems. The production planning process for an episodic planning approach is illustrated using an example JSSP,\nhighlighting the functionality of a trained agent at different time steps in the planning process. While episodic planning\ncan be integrated as a low-tech, standalone solution, a continuous planning agent is fully integrated into"}]}