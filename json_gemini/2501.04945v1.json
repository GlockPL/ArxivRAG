{"title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "authors": ["Qingyu Ren", "Jie Zeng", "Qianyu He", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge for LLMs. To enhance the ability of LLMs to follow soft constraints, we initially design a pipeline to obtain high-quality outputs automatically. Additionally, to fully utilize the acquired data, we introduce a training paradigm based on curriculum learning. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements. The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraints.", "sections": [{"title": "Introduction", "content": "In the application of LLMs, their instruction following ability is of paramount importance, especially when the instructions involve multiple constraints. The capability of LLMs plays a critical role in aligning with human preferences, ensuring the reliability and helpfulness of the models' outputs. Following instructions with soft constraints is imperative for LLMs. Existing work on improving the ability of LLMs to follow multiple constraints mainly focuses on hard constraints, which are typically based on structured data or fixed-format requirements. These constraints can be explicitly expressed as specific rules and directly verified through programming methods. For example, Python can parse JSON to verify hard constraints. However, hard constraints fail to adequately capture the complexity in real-world scenarios. Instructions in real-world applications often contain semantic-level limitations, which can be categorized as soft constraints. Soft constraints include restrictions related to content, specific backgrounds, and stylistic objectives. They are difficult to verify automatically through programming methods. A variety of tasks involve soft constraints, such as open-ended question answering, role-playing, and suggestion generation. As shown in Fig. 1, following soft constraints is challenging for LLMs.\nHowever, following soft constraints is a non-trivial task. First, existing research on soft constraints in LLMs mainly focuses on evaluation rather than improving their following. Also, as shown in Fig. 1, soft constraints are ambiguous and challenging for LLMs in real applications. They depend on subjective interpretations and specific contexts. Unlike hard constraints, they cannot be assessed with fixed rules or scripts. Soft constraint evaluation often relies on prompting LLMs, which involves various biases. The inherent difficulty makes it more"}, {"title": "Related Work", "content": "Soft Constraint Following Existing research on soft constraint following largely concentrates on evaluating the ability of LLMs to follow these constraints by constructing benchmarks. These benchmarks typically include a variety of fine-grained constraint types, and the results from testing LLMs on these benchmarks suggest that LLMs often struggle to follow these constraints. Despite this, there is a notable paucity of research aimed at improving LLMs' capacity to comply with soft constraints, especially for soft constraints. Soft constraints can be categorized into several types: (1) Content soft constraints involve restrictions on the scope or depth of the responses. (2) Situation soft constraints refer to the background limitations of the responses. (3) Style soft constraints limit the manner or tone of expressions. Some works directly utilize responses generated by GPT-4 to construct datasets. However, the responses to instructions with soft constraints are often unreliable. Different from these, our study focuses on how to construct datasets with high-quality outputs for improving LLMs' soft constraint following ability.\nCurriculum Learning Curriculum learning is a training strategy that mimics the learning process of humans by advancing from simpler to more complex tasks. Current research on LLMs' curriculum learning can be broadly categorized into two primary paradigms: (1) Learning Based on Data Difficulty: This approach involves constructing curricula by ranking data according to various evaluation metrics. Metrics such as sequence length, perplexity have been employed to guide this process. LLMs can also construct curricula through advanced planning. (2) Learning Based on Task Difficulty: This paradigm focuses on modifying the training tasks or adjusting the training objectives. However, our work organizes the curriculum based on the number of constraints in the instructions."}, {"title": "Method", "content": "In this section, we provide a detailed explanation of how to obtain high-quality outputs and how to leverage this data by establishing a new training paradigm. The pipeline is shown in Fig. 2."}, {"title": "High-quality Dataset", "content": "To enhance the ability of LLMs to follow soft constraints, we first construct a multi-constraint instruction following dataset. Existing works in dataset construction rely on advanced models to directly generate the outputs. However, even GPT-4 is struggling to follow the instructions with complex constraints, especially when the instructions contain soft constraints which are more intractable. To address this challenge, we design a pipeline to construct datasets with high-quality outputs for soft constraint following. This pipeline consists of two steps: progressive construction and Judger reordering."}, {"title": "Progressive Construction", "content": "To enable the model to learn how to follow each constraint, we propose a progressive dataset construction method. Specifically, we increase only one constraint at a time, enabling the model progressively learn to follow each constraint during the training process.\nWe begin by collecting seed instructions from three sources. We first collect instructions from Open Assistant, which includes instructions generated by users interacting with chatbots. We select rank 0 instructions and those from the first turn of conversations. Next, we gather manually created instructions from the Self-Instruct. The third source is Super-Natural, from which we select instructions after filtering out tasks with simple outputs. These three sources together provide a total of 1,500 seed instructions, offering a broad range of coverage across diverse tasks.\nSubsequently, we construct different types of soft constraints. Initially, we categorize the soft constraints into three types: content, situation, and style. Next, we randomly select 5 constraints for each seed instruction. For the soft constraints, LLMs are employed to generate corresponding descriptions. While for the hard constraints, descriptions are selected from a predefined list. The prompt used to construct soft constraints is detailed in the Appx. A.1.\nTo obtain multi-constraint instructions, we adopt a progressive construction approach. This approach is different from previous methods, which typically add all constraints at once, often making it challenging for the model to learn how to follow each constraint independently. We add only one constraint to the instruction at a time, allowing the model to focus on learning and mastering each constraint independently. This step-by-step process helps the model gradually adapt to the increasing complexity of the task, ensuring that it can effectively handle each constraint as it is introduced.\nSpecifically, for seed instruction Io, we progressively add one constraint each time to form the instruction set I = {I1, I2, . . ., In}, where n denotes the maximum number of constraints. For each instruction Ik with k constraints (k = 1,2,...,n), we use GPT-40 to generate the corresponding output Ok = LLM(Ik). After performing inference on all the instructions in the instruction set I, we obtain the output set O = {01, 02, . . ., On}."}, {"title": "Judger Reordering", "content": "In \u00a73.1.1, we progressively increase the constraints, but the quality of the outputs may not improve incrementally. To address this, we introduce Judger to reorder the outputs based on the extent of constraint following to ensure the quality of outputs.\nDuring the progressive construction process in \u00a73.1.1, as new constraints are continuously added, the model's responses may overlook previously incorporated constraints, leading to a decrease in constraint following. To obtain high-quality outputs, we introduce Judger, where LLM is prompted to compare two outputs before and after adding the new constraint, to determine which better follows the updated instruction. The two outputs in each comparison are recorded, and the one deemed better by Judger is used for the next round of comparison. By iteratively ranking the outputs, the constructed data is consistent with constraint following, thereby improving the output quality.\nSpecifically, when a new constraint is added into the instruction Ik-1 to form Ik, the model's response Ok may not fully follow the constraints in Ik. To obtain high-quality outputs, we use Judger to rank the new output Ok with the previous output Owk-1 that more follows Ik\u22121 to determine which one better follows the current instruction Ik: Owk, Olk = Judger(Ik, Owk\u22121, Ok).\nIn each ranking, we can obtain the output Owk which follows the current instruction Ik better and the output Olk which follows less. Finally, after completing all n rankings, we obtain the positive set Ow = {Ow1,Ow2,..., Own}, which consists of outputs that follow their respective instructions better. We also obtain the negative set \u039f\u03b9 = {011, Ol2, ..., Oln }, which contains outputs that less follow. The prompt used to reorder outputs and cases are detailed in the Appx. A.2."}, {"title": "Curriculum-based Training Paradigm", "content": "In \u00a73.1.2, we use Judger to obtain the positive set Ow and the negative set O\u03b9. Supervised Fine-Tuning (SFT) only uses the positive samples to train the model. However, the negative samples also contain valuable supervision information. Hence, we adopt reinforcement learning to leverage both the positive and negative sets. Moreover, we develop a training paradigm based on curriculum learning to enhance the training process.\nGiven the positive set and the negative set, we can construct the training dataset with n triplets: (I1, \u039f\u03c91, \u039f\u03b9\u2081), (I2, Ow2, Ol2), ..., (In, Own, Oln). In each triplet, the output from Ow is preferred than the output from Or. To model this preference relationship, we apply Direct Preference Optimization (DPO) as the training method.\nAdditionally, in the DPO training process, the model is required to learn preference judgments. As the number of constraints in the instruction increases, the complexity of judgments also rises. Inspired by curriculum learning, we propose a curriculum learning training approach for preference learning, starting with simpler preference judgments involving fewer constraints and progressively advancing to more complex judgments involving more constraints.\nSpecifically, for curriculum k, the training dataset Dk contains the triplet (Ik, \u039f\u03c9\u03ba, Olk). The training process is structured in two stages based on the difficulty levels of the curriculum datasets. In the first stage, the model is trained using the datasets corresponding to the simpler curriculums De = {D1, D2, D3}. Once the model has adapted to these simpler tasks, it progresses to the second stage, where it is further trained using the datasets corresponding to the more challenging curriculums Dh = {D4, D5}. The complete curriculum D is composed of the simpler curriculum De and the more challenging curriculumDh: D = {De, Dh}. This staged approach ensures that the model first develops a solid foundation with simpler datasets before adapting to the complexities of the harder datasets."}, {"title": "Dataset Statistics", "content": ""}, {"title": "Diversity", "content": "To show the diversity of our dataset, we analyze the verb-noun structure of data. As shown in Fig. 3, we illustrate the top 10 verbs in the inner circle and their 3 most frequent direct noun objects in the outer circle. The result reveals the instructions in our dataset encompass a diverse set of linguistic patterns. This diversity of our dataset is crucial for enhancing the model's ability to generalize across various types of constraints."}, {"title": "Comparison with Other Works", "content": "As shown in Tab. 1, we compare our dataset construction approach with other related works. In comparison to existing methods, the scale of our dataset is large. From the perspective of constraint categories, our dataset includes both soft and hard constraints. Compared to datasets that only contain soft or hard constraints, this broader scope better facilitates the model's ability to learn constraint following. Regarding pairwise comparison, we use Judger for comparisons of the outputs, which improves the quality of the dataset. Moreover, our dataset is open-source. To prevent catastrophic forgetting during training, we mix the data in each curriculum with a corresponding proportion of ShareGPT data."}, {"title": "Experiments", "content": "We conduct extensive experiments to evaluate the effectiveness of our proposed method, focusing on soft constraint following ability and generalization performance."}, {"title": "Experiment Setup", "content": "Models. We conduct experiments on two widely recognized base LLMs, Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.3, both of which demonstrate exceptional performance among models within the parameter range of 7B to 8B. Within our experimental framework (\u00a73), we compare three approaches: (1) BASE directly utilizes the original model to generate outputs. (2) SFT applies supervised fine-tuning on LLMs using constructed data (\u00a73.1.1). (3) DPO+Judger+CL utilizes Judger to produce high-quality training data, in accordance with training the model using DPO based on curriculum learning(\u00a73.1.2, \u00a73.2).\nFor baseline comparisons, we select a range of open-source and proprietary LLMs. Among the proprietary models, we include GPT-4"}, {"title": "Main Results", "content": "As shown in Tab. 2, our method significantly enhances the model's ability to follow soft constraints, even outperforming the capabilities of larger models. Furthermore, it also markedly improves the model's hard constraint following ability, demonstrating its effectiveness across both types of constraints. Specifically, when the models are trained using the DPO+Judger+CL method, a significant performance improvement is observed across both benchmarks, particularly on IFEval. The model's performance improvement is particularly significant on complex tasks, especially at the L4-L5 difficulty levels in FollowBench. Specifically, Mistral-7B-Instruct-v0.3 shows an average improvement of 4.3% at the L4-L5 difficulty levels.\nIn comparison to models designed to enhance the ability to follow complex instructions, our model demonstrates superior performance on both benchmarks. Specifically, although the performance of Mistral-7B-Instruct-v0.3 on Followench is lower than Conifer-7B-SFT, its performance surpasses the Conifer model on both benchmarks after training. Moreover, our training paradigm effectively enhances the instruction-following ability of LLMs, even when working with models of smaller parameter sizes. Specifically, compared with models in the 13B category, the performance of Llama-3-8B-Instruct is initially weaker than that of WizardLM-v1.2-13B on FollowBench. But after training, its performance surpasses the 13B model on both benchmarks.\nAfter supervised fine-tuning on the constructed"}, {"title": "Generalization Experiments", "content": "Besides the ability to follow soft constraints, we also assess the model's general instruction following abilities on AlpacaEval. To avoid the length bias that AlpacaEval may correlate with response lengths, we use the AlpacaEval 2.0 to evaluate the general instruction following.\nIn our evaluation process, we first perform supervised fine-tuning on the model, followed by DPO training using the proposed training paradigm. Specifically, we use precomputed outputs of GPT-4 Turbo on AlpacaEval as reference outputs and employ GPT-40 as evaluators. As shown in the Tab. 3, our method leads to a significant improvement in the model's general instruction-following ability, outperforming both models of comparable parameter scales and even larger models."}, {"title": "Ablation Studies", "content": "In this section, we conduct ablation experiments to assess the impact of Judger, as described in \u00a73.1.2, and the curriculum-based training paradigm, outlined in \u00a73.2, on the model's ability to follow instructions. The Llama-3-8B-Instruct model is used as the base model, and evaluations are conducted on the IFEval and Follow Bench benchmarks.\nAs shown in Tab. 4, using the constructed data directly for SFT without Judger adjustments underperforms the full method on both benchmarks, even resulting in a slight performance decline relative to the base model. It is evident that performance decreases significantly at the L4-L5 levels of FollowBench. This observation suggests that Judger plays a critical role in ranking responses to more challenging instructions. In contrast, the model trained with DPO outperforms the SFT baseline, especially on IFEval, further emphasizing the effectiveness of the DPO training approach over SFT in constraint following tasks. However, it still falls short of the performance of the DPO+Judger+CL method.\nAdditionally, the results indicate that randomly organizing DPO training data leads to a decrease in performance. In contrast, our curriculum-based approach where training data is organized based on the number of constraints in the instructions learning leads to a significant improvement in the model's ability to follow instructions, particularly those at higher difficulty levels in L4-L5 levels of FollowBench. These findings strongly validate the necessity of Judger for constructing high-quality outpus and the proposed curriculum learning paradigm for enhancing the model's ability to follow constraints."}, {"title": "Analysis", "content": ""}, {"title": "Category Analysis", "content": "In this section, we analyze the model's performance across different types of constraints. Specifically, we compare the performance of Llama-3-8B-InstructBASE and Llama-3-8B-InstructDPO+Judger+CL on FollowBench. As shown in Fig. 4, our method significantly improves the model's performance across different types of constraints. The most notable improvement is observed in the Mixed category, which is defined as a composition of multi-"}, {"title": "The Role of Judger", "content": "In this section, we investigate the factors contributing to the effectiveness of the Judger in constructing high-quality outputs. Judger ranks the outputs to better alignment with human preferences. To examine the underlying effectiveness of the Judger, we conduct an experiment designed to evaluate whether it facilitates this alignment.\nSpecifically, we randomly select 100 output sets from the construction process in \u00a73.1.1, each containing 3 to 5 outputs. These outputs are manually annotated with the correct rankings, which serve as the reference standard for comparison. We evaluate the rankings in three distinct scenarios: (1) sequential rankings, (2) rankings adjusted by Judger, and (3) rankings annotated by human experts.\nTo assess the similarity between these rankings, we employ two complementary metrics. The first is the Kendall Tau distance, a statistical measure that quantifies the number of discordant pairs between two sequences, thereby reflecting the extent of their relative order differences. In addition, we introduce the position consistency metric, which quantifies the proportion of elements that occupy the same relative positions across both rankings. This metric provides a direct evaluation of the alignment between rankings at each specific position. The results, presented in Tab. 5, demonstrate that the rankings adjusted by the Judger exhibit greater alignment with human-annotated rankings when compared to sequential rankings. This finding suggests that Judger enhances the quality of the training data by improving its consistency with human judgments, thus making the preference data more reliable for training."}, {"title": "The Role of Curriculum Learning", "content": "In this section, we analyze the effects of the curriculum-based training paradigm at different stages of the training process. Specifically, we examine the performance of Llama-3-8B-Instruct with the full method across three training stages, each corresponding to a different level of curriculum learning difficulty. Stage0 represents the base model, while Stage3 and Stage5 represent the stages where the model completes the easy curriculum and the hard curriculum, respectively.\nAs shown in Fig. 5, our proposed training paradigm progressively enhances the model's instruction following capability across various training stages. Specifically, after easy curriculum learning, the model trained in Stage3 demonstrates superior performance compared to the base model across tasks L1-L3. In contrast, the model's performance at L4-L5 in Stage3 is lower than Stage0. The possible reason is that Stage3 may not have adequately prepared for the complexity of L4-L5. The gap between these difficulty levels could have led to the initial performance drop. Subsequently, when the model progresses to Stage5, after hard curriculum learning, performance improves significantly at the difficlut levels L4-L5. The results on IFEval further support this conclusion. Stage0 demonstrates the lowest average performance across all indicators. After curriculum learning, there is a significant improvement in the model's performance on IFEval. By initially focusing on simpler preference learning and gradually progressing to more complex one, the model's ability to follow constraints improves incrementally. This progression enables the model to achieve better performance on increasingly difficult instruction following tasks."}, {"title": "Conclusion", "content": "In this paper, we systematically study how to improve LLMs' ability to follow instructions with soft constraints. Initially, we design a pipeline to automate the construction of datasets with high-quality outputs for soft constraint following. Based on the pipeline, we introduce a method utilizing positive and negative samples generated during the pipeline. Moreover, we propose a new training paradigm that leverages curriculum learning to enhance LLMs' constraint following ability. Our experiments show that our methods enhance models' ability to follow soft constraints effectively while maintaining general capabilities."}, {"title": "Limitations", "content": "We discuss the limitations of our study as follows. First, we improve the model's ability to follow complex constraints, thereby improving its overall instruction following capability. However, even when the model's output meets all the specified constraints, it may still struggle to fully comply with complex instructions due to limitations in reasoning capacity or the knowledge it masters. Additionally, while the dataset constructed in the study encompass a diverse set of tasks, it may still not cover some task types in the long tail. We consider these as a key direction for future research."}, {"title": "Details of Data", "content": ""}, {"title": "Details of Soft Constraints", "content": "We utilize LLMs (i.e., GPT-40) to construct constraints. The three categories of soft constraints that we define are as follows:\n\u2022 Soft Constraints in Content: Content soft constraints refer to limitations associated with the data itself. These constraints govern the elements of information, the logical relationships between them, and the scope of topics that need to be covered in the response. When multiple content soft constraints are imposed, the model is required to not only generate comprehensive and coherent content but also ensure that the response aligns with the specific logical definitions and boundaries outlined by the instruction. This presents a significant challenge, as it demands both the integration of diverse elements and the maintenance of internal consistency. To address this challenge, we define the following tasks for constructing and applying content soft constraints:\n1. Inclusion of Key Elements: The response must incorporate the key points specified in the instruction. This requires the model to effectively extract and integrate relevant information, ensuring that the essential components are included without omitting critical details.\n2. Topic Focus: The model must narrow the discussion to a specific subtopic, avoiding broad generalizations or irrelevant tangents. This task emphasizes the importance of maintaining focus and precision within the scope defined by the instruction.\n3. Strict Structure: The generated content must adhere to a predefined structure, such as being organized into coherent paragraphs, utilizing subheadings, or following a specific format. This task imposes a higher demand on the model's ability to generate well-organized and structured outputs, aligning with the required presentation structure.\nWe provide the prompt template for constructing the Content Soft Constraint in Tab. 7 and Tab. 8.\n\u2022 Soft Constraints in Situation: Situation soft constraints are those related to the context within which the response is situated. These constraints require the response to be adjusted according to the context or assumptions specified in the instruction, ensuring that the content is appropriate to the given background. Such adjustments may involve factors like a particular time or location, the assumption of a specific role, or drawing conclusions based on certain premises. The response must dynamically adapt to situational changes and maintain consistency with the contextual elements. The tasks defined by these constraints can be categorized as follows:\n1. Role-Playing: The response must be framed from the perspective of a specific role or persona, ensuring alignment with the contextual expectations associated with that role.\n2. Decision Support: The response should provide advice or recommendations that support decision-making within a particular context.\n3. Storytelling: The response should construct a narrative that is situated within a defined time, location, or background, maintaining coherence with the provided contextual elements.\nWe provide the prompt template for constructing the Situation Soft Constraint in Tab. 9, Tab. 10, and Tab. 11.\n\u2022 Soft Constraints in Style: Style soft constraints pertain to the mode of expression, encompassing factors such as the formality or informality of tone, the level of conciseness in language, and the emotional tenor. These constraints require the response to adjust its style in accordance with the given requirements, adapting to different linguistic contexts. The following task types are defined under this category:\n1. Tone Requirement: The generated content must adopt a specific tone, such as formal, humorous, or otherwise defined.\n2. Language Complexity Control: The complexity of the language used must adhere to specific standards, such as maintaining conciseness and clarity or employing academic expressions.\n3. Emotional Expression: The response must convey a particular emotion, such as positivity or sadness, as dictated by the context.\nWe provide the prompt template for constructing the Style Soft Constraint in Tab. 12."}, {"title": "Details of Judger Reordering", "content": "We utilize GPT-40 to reorder the outputs. We provide the prompt of Judger ranking in Tab. 13 and examples of how the Judger ranks responses in Tab. 6."}, {"title": "Details of Experiments", "content": ""}, {"title": "Training hyperparameters", "content": "We train Mistral-7B-Instruct-v0.3 and Llama-3-8B-Instruct using LLaMA-Factory on 4 NVIDIA A100 80GB GPUs, applying LORA for efficient training. The lora target is set to all, and both models use the following training parameters, with training running for 3 epochs. The per device train batch size is set to 1, and gradient accumulation steps is set to 8. The warm-up ratio is set to 0.1. For SFT, Mistral-7B-Instruct-v0.3 is trained with a learning rate 5.0e-7, while the learning rate of Llama-3-8B-Instruct is 1.0e-4. For DPO, the learning rate is set to 5.0e-6, with a beta value of 0.1."}, {"title": "Full Results on Follow Bench", "content": "We present the full results on FollowBench including the Hard Satisfaction Rate (HSR) metric and Soft Satisfaction Rate (SSR) metric in Tab. 14."}]}