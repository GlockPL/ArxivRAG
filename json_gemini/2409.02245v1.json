{"title": "FastVoiceGrad: One-step Diffusion-Based Voice Conversion\nwith Adversarial Conditional Diffusion Distillation", "authors": ["Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Yuto Kondo"], "abstract": "Diffusion-based voice conversion (VC) techniques such as\nVoiceGrad have attracted interest because of their high VC\nperformance in terms of speech quality and speaker similar-\nity. However, a notable limitation is the slow inference caused\nby the multi-step reverse diffusion. Therefore, we propose\nFastVoiceGrad, a novel one-step diffusion-based VC that re-\nduces the number of iterations from dozens to one while inher-\niting the high VC performance of the multi-step diffusion-based\nVC. We obtain the model using adversarial conditional diffu-\nsion distillation (ACDD), leveraging the ability of generative\nadversarial networks and diffusion models while reconsidering\nthe initial states in sampling. Evaluations of one-shot any-to-\nany VC demonstrate that FastVoiceGrad achieves VC perfor-\nmance superior to or comparable to that of previous multi-step\ndiffusion-based VC while enhancing the inference speed.\nIndex Terms: voice conversion, diffusion model, generative\nadversarial networks, knowledge distillation, efficient model", "sections": [{"title": "1. Introduction", "content": "Voice conversion (VC) is a technique for converting one voice\ninto another without changing linguistic contents. VC began\nto be studied in a parallel setting, in which mappings between\nthe source and target voices are learned in a supervised man-\nner using a parallel corpus. However, this approach encoun-\nters difficulties in collecting a parallel corpus. Alternatively,\nnon-parallel VC, which learns mappings without a parallel cor-\npus, has attracted significant interest. In particular, the emer-\ngence of deep generative models has ushered in breakthroughs.\nFor example, (variational) autoencoder (VAE/AE) [1]-based\nVC [2-9], generative adversarial network (GAN) [10]-based\nVC [11-16], flow [17]-based VC [18], and diffusion [19]-based\nVC [20-22] have demonstrated impressive results.\nAmong these models, this paper focuses on diffusion-based\nVC because it [20, 22] outperforms representative VC models\n(e.g., [6, 8, 9, 14, 23]) and has a significant potential for de-\nvelopment owing to advancements in diffusion models in var-\nious fields (e.g., image synthesis [24-26] and speech synthe-\nsis [27, 28]). Despite these appealing properties, its limitation\nis the slow inference caused by an iterative reverse diffusion\nprocess to transform noise into acoustic features (e.g., the mel\nspectrogram\u00b2) as shown in Figure 1(a). This requires at least\napproximately five iterations, typically dozens of iterations, to\nobtain sufficiently high-quality speech. This is disadvantageous"}, {"title": "2. Preliminary: VoiceGrad", "content": "VoiceGrad [20] is a pioneering diffusion-based VC model\nthat includes two variants: a denoising score matching\n(DSM) [30]-based and denoising diffusion probabilistic model\n(DDPM) [25]-based models. The latter can achieve a VC per-\nformance comparable to that of the former while reducing the\nnumber of iterations from hundreds to approximately ten [20].\nThus, this study focuses on the DDPM-based model. The orig-\ninal VoiceGrad was formulated for any-to-many VC. However,\nwe formulated it for any-to-any VC as a more general formu-\nlation. The main difference is that speaker embeddings are ex-\ntracted using a speaker encoder instead of speaker labels, while\nthe others remain almost the same.\nOverview. DDPM [25] represents a data-to-noise (diffusion)\nprocess using a gradual nosing process, i.e., $x_0 \\rightarrow x_1 \\rightarrow\\dots \\rightarrow x_T$, where T is the number of steps (T = 1000\nin practice), $x_0$ represents real data (mel spectrogram in our\ncase), and $x_T$ indicates noise $x_T \\sim \\mathcal{N}(0, I)$. By contrast,\nit performs a noise-to-data (reverse diffusion) process, that is,\n$x_T \\rightarrow x_{T-1} \\rightarrow \\dots \\rightarrow x_0$, using a gradual denoising process\nvia a neural network. The details of each process are as follows:\nDiffusion process. Assuming a Markov chain, a one-step diffu-\nsion process $q(x_t|x_{t-1})$ ($t \\in \\{1, ..., T\\}$) is defined as follows:\n\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t}x_{t-1}, \\beta_tI)$,\n\nwhere $\\alpha_t = 1 - \\beta_t$. Owing to the reproductivity of the normal\ndistribution, $q(x_t|x_0)$ can be obtained analytically as follows:\n\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 - \\bar{\\alpha}_t)I)$,\n\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$. Using a reparameterization trick [1],\nEquation (2) can be rewritten as\n\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon$,\n\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$. In practice, $\\beta_t$ is fixed at constant val-\nues [25] with a predetermined noise schedule (e.g., a cosine\nschedule [26]).\nReverse diffusion process. A one-step reverse diffusion process\n$p_\\theta(x_{t-1}|x_t)$ is defined as follows:\n\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t, s, p), \\sigma^2I)$,\n\nwhere $\\mu_\\theta$ indicates the output of a model that is parameterized\nusing $\\theta$, conditioned on $t$, speaker embedding $s$, and phoneme\nembedding $p$, and $\\sigma^2 = \\frac{1 - \\alpha_{t-1}}{1 - \\bar{\\alpha}_{t-1}} \\beta_t$. Unless otherwise specified,\n$x_0$, $s$, and $p$ are extracted from the same waveform. Through\nreparameterization [1], Equation (4) can be rewritten as\n\n$x_{t-1} = \\mu_\\theta(x_t, t, s, p) + \\sigma_t z$,\n\nwhere $z \\sim \\mathcal{N}(0, I)$.\nTraining process. The training objective of DDPM is to\nminimize the variational bound on the negative log-likelihood\n$\\mathbb{E}[-\\log p_\\theta(x_0)]$: \n\n$L_{DDPM}(\\theta) = \\mathbb{E}_{q(x_{1:T}|x_0)} \\bigg[-\\log p_\\theta(x_{0}|x_1) + \\sum_{t=2}^{T} \\log \\frac{p_\\theta(x_{t-1}|x_t)}{q(x_t|x_{0})}\\bigg]$"}, {"title": "3. Proposal: FastVoiceGrad", "content": "3.1. Rethinking initial states in sampling\nIn Algorithm 1, the two crucial factors that affect the inheritance\nof source speech are the initial values of (1) $x$ and (2) $t$.\nRethinking the initial value of $x$. When the initial value of $x$\nis set to $x \\sim \\mathcal{N}(0, I)$ (a strategy used in generation), no gap\noccurs between training and inference; however, we cannot in-\nherit the source information, that is, $x_0^{src}$, which is useful for\nVC to preserve the content. In contrast, when $x_0^{src}$ is directly\nused as the initial value of $x$ (the strategy used in VoiceGrad),\nwe can inherit the source information, but a gap occurs between\ntraining and inference. Considering both aspects, we propose\nthe use of a diffused source mel-spectrogram $x_{S_K}^{src}$, defined as\n\n$x_{S_K}^{src} = \\sqrt{\\bar{\\alpha}_{S_K}}x_0^{src} + \\sqrt{1 - \\bar{\\alpha}_{S_K}}\\epsilon$.\n\nIn line 1 of Algorithm 1, $x_{S_K}^{src}$ is used instead of $x_0^{src}$. The\neffect of this replacement is discussed in the next paragraph."}, {"title": "3.2. Adversarial conditional diffusion distillation", "content": "Owing to the difficulty in learning a one-step diffusion model\ncomparable to a multi-step model from scratch, we used a\nmodel pretrained using the standard VoiceGrad as an initial\nmodel and improved it through ACDD. Inspired by ADD [29],\nwhich was proposed for image generation, we used adversarial\nloss and score distillation loss in distillation.\nAdversarial loss. Initially, we considered directly applying a\ndiscriminator to the mel spectrogram, similar to the previous\nGAN-based VC (e.g., [15, 16]). However, we could not deter-\nmine an optimal discriminator to eliminate the buzzy sound in\nthe waveform. Therefore, we converted the mel spectrogram\ninto a waveform using a neural vocoder $V$ (with frozen parame-\nters) and applied a discriminator $D$ in the waveform domain.\nMore specifically, adversarial loss (particularly least-squares\nGAN [33]-based loss) is expressed as follows:\n\n$L_{adv}(D) = \\mathbb{E}_{x_0} [(D(V(x_0^\\theta)) - 1)^2 + (D(V(x_0)))^2]$,\n$L_{adv}(\\theta) = \\mathbb{E}_{x_0} [(D(V(x_0^\\theta)) - 1)^2]$,\n\nwhere $x_0$ represents a mel spectrogram extracted from real\nspeech.\n$x_0^\\theta$ represents a mel spectrogram generated using\n$x_0^\\theta = \\mu_\\theta(x_{S_K}, S_K, s, p)$ (one-step denoising prediction de-\nfined in Equation (7)), where $x_{S_K}$ is the $S_K$-step diffused $x_0$\nvia Equation (9). The adversarial loss is used to improve the\nreality of $x_0^\\theta$ through adversarial training.\nFurthermore, following the training of a neural vocoder [34,\n35], we used the feature matching (FM) loss, defined as\n\n$L_{FM}(\\theta) = \\mathbb{E}_{x_0} \\sum_{l=1}^{L} \\frac{1}{N_l} ||D_l(V(x_0^\\theta)) - D_l(V(x_0))||_1$,\n\nwhere $l$ indicates the number of layers in $D$. $D_l$ and $N_l$ denote\nthe features and the number of features in the $l$-th layer of $D$,\nrespectively. $L_{FM}(\\theta)$ bears $x_0^\\theta$ closer to $x_0$ in the discriminator\nfeature space.\nScore distillation loss. The score distillation loss [29] is for-\nmulated as follows:\n\n$L_{dist}(\\theta) = \\mathbb{E}_{t, x_0} [c(t)||x_0^\\theta - x_0^*||_1]$,\n\nwhere $x_0^\\phi$ is one-step denoising prediction (Equation (7)) gener-\nated by a teacher diffusion model parameterized with $\\phi$ (frozen\nin training): $x_0^* = \\mu_\\phi(sg(x_0, t), t, s, p)$. Here, sg denotes the\nstop-gradient operation, $x_0, t$ is the $t$-step diffused $x_0$ via Equa-\ntion (3), and $t \\in \\{1, ..., T\\}$. $c(t)$ is a weighting term and is\nset to $\\alpha_t$ in practice to allow higher noise levels to contribute\nless [29]. $L_{dist}(\\theta)$ encourages $x_0^\\theta$ (student output) to match $x_0^*$\n(teacher output).\nTotal loss. The total loss is expressed as follows:\n\n$L_{ACDD}(\\theta) = L_{adv}(\\theta) + \\lambda_{FM}L_{FM}(\\theta) + \\lambda_{dist}L_{dist}(\\theta)$,\n$L_{ACDD}(D) = L_{adv}(D)$,\n\nwhere $\\lambda_{FM}$ and $\\lambda_{dist}$ are weighting hyperparameters set to 2\nand 45, respectively, in the experiments. $\\theta$ and $D$ are optimized\nby minimizing $L_{ACDD}(\\theta)$ and $L_{ACDD}(D)$, respectively."}, {"title": "4. Experiments", "content": "4.1. Experimental settings\nData. We examined the effectiveness of FastVoiceGrad on one-\nshot any-to-any VC using the VCTK dataset [36], which in-\ncluded the speeches of 110 English speakers. To evaluate the\nunseen-to-unseen scenarios, we used 10 speakers and 10 sen-\ntences for testing, whereas the remaining 100 speakers and ap-\nproximately 390 sentences were used for training. Following\nDiffVC [22], audio clips were downsampled at 22.05kHz, and\n80-dimensional log-mel spectrograms were extracted from the\naudio clips with an FFT size of 1024, hop length of 256, and\nwindow length of 1024. These mel spectrograms were used as\nconversion targets.\nComparison models. We used VoiceGrad [20] (Section 2) as\nthe main baseline and distilled it into FastVoiceGrad. A diffu-\nsion model has a tradeoff between speed and quality according\nto the number of reverse diffusion steps (K). To investigate this\neffect, we examined three variants: VoiceGrad-1, VoiceGrad-6,\nand VoiceGrad-30, which are VoiceGrad with K = 1, K = 6,\nand K = 30, respectively. VoiceGrad-1 is as fast as FastVoice-\nGrad, whereas the others are slower. For an ablation study, we\nexamined FastVoiceGradadv and FastVoiceGraddist, in which\nscore distillation and adversarial losses were ablated, respec-\ntively. As another strong baseline, we examined DiffVC [22],"}, {"title": "4.3. Application to another dataset", "content": "To confirm this generality, we evaluated FastVoiceGrad on the\nLibriTTS dataset [47]. We used the same networks and training\nsettings as those for the VCTK dataset, except that the train-\ning epochs for VoiceGrad and FastVoiceGrad were reduced to\n300 and 50, respectively, owing to an increase in the amount\nof training data. Table 2 summarizes the results. The same\ntendencies were observed in that FastVoiceGrad not only out-\nperformed VoiceGrad-1 (a model with the same speed) but was\nalso superior to or comparable to the other baselines."}, {"title": "5. Conclusion", "content": "We proposed FastVoiceGrad, a one-step diffusion-based VC\nmodel that can achieve VC performance comparable to or supe-\nrior to multi-step diffusion-based VC models while reducing the\nnumber of iterations to one. The experimental results demon-\nstrated the importance of carefully setting of the initial states in\nsampling and the necessity of the joint use of GANs and diffu-\nsion models in distillation. Future research should include ap-\nplications to advanced VC tasks (e.g., emotional VC and accent\ncorrection) and an extension to real-time implementation."}]}