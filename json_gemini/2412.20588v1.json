{"title": "Kryptonite-N: Machine Learning Strikes Back", "authors": ["Albus Li", "Nathan Bailey", "Will Summerfield", "Kira Kim"], "abstract": "Quinn et al propose challenge datasets in their\nwork called \"Kryptonite-N\". These datasets aim\nto counter the universal function approximation\nargument of machine learning, breaking the no-\ntation that machine learning can \u201capproximate\nany continuous function\" (Quinn & Luther, 2024).\nOur work refutes this claim and shows that univer-\nsal function approximations can be applied suc-\ncessfully; the Kryptonite datasets are constructed\npredictably, allowing logistic regression with suffi-\ncient polynomial expansion and L1 regularization\nto solve for any dimension N.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in machine learning, namely LLMs\nlike GPT-4, have gained not only popularity from its success\nin solving various complex problems (Chowdhery et al.,\n2022) but also some scepticism in its capabilities, as sug-\ngested by the \"Kryptonite-N\" paper (Quinn & Luther, 2024).\nQuinn and Luther state despite applying binary logistic re-\ngression with a GPT-driven basis expansion technique, they\nstill fail to achieve high-quality performance on the datasets.\nIn this paper we refute the claims of the \"Kryptonite-N\"\npaper by showing that several models yield good results\non the dataset, therefore proving the validity of Universal\nFunction Approximation. At first a comprehensive data\nexploration is performed to inform the experimental design.\nBased on the experiments, we discuss insights gained from\nthe results followed by an analysis on sustainability of model\nlifecycle. Furthermore, we demonstrate the flaws in their\nGPT approaches and explain observed poor performance.\nFinally, we outline the process where we determined the\nconstruction of the datasets."}, {"title": "2. Data Exploration", "content": "Initial data exploration using quantitative metrics and visual\nrepresentation is crucial to gain insights for data manipu-\nlation and modelling steps. We exclusively examined the\ntraining portion of the datasets, to ensure that we didn't bias\nour models.\nAfter assessing metrics from each dimension, we discovered\nunusual results: each dimension in each K-N dataset has\nan approximate mean of 0.5, standard deviation of 0.5, and\nrange of [0 \u20ac, 1 + \u20ac]. This prompted us to evaluate the\ndata distribution by plotting the Probability Mass Function\nof each dimension for the K-9 Dataset.\nWe learned several very important things from this graph.\nFirstly, each dimension appears to come from a multimodal\ndistribution with peaks at at 0 and 1. Each peak within\na dimension takes the same shape, with there being three\ndifferent distributions seen across the dimensions, which we\nclassify as \"Burst-like\", \"Gaussian-like\", and \"Spread-like\".\nSecondly, we see that there appears to be no correlation\nbetween which peak a data-point is sampled from and the\nlabel. This made us curious about the correlation between\nthe dimensions and the labels, which we hypothesized to be\nlow based on this graph.\nA correlation matrix shows our hypothesis is correct - there\nis close to zero correlation across dimensions. This implies\nthe labels must be based on a non-linear combination of fea-\ntures. We visualised the data via a 2-Dimensional Principal\nComponent Analysis to better understand the data's rela-\ntionship with the labels, but that resulted in a circle of data\npoints with little clustering of labels, especially at higher\ndimensions.\nHowever, if we only visualise as slices in 2D, we can extrap-\nolate that the points fall in kk distinct clusters, evenly spaced\nover k-Dimensional space. This reminded us of the XOR\nproblem, a dataset which Multi-Layer Perceptrons fail to\nsolve without non-linearities. The XOR function has points\nat each corner, with a non-linear relationship to the labels\ndependent on each input feature (which are also exactly un-\ncorrelated). It seemed to us that this dataset is similar to the\nK-N datasets, and therefore that we might use similar meth-\nods to solve them, such as polynomial features and basis\nexpansion. This also leads us to the idea of discretizing the"}, {"title": "3. Methodology", "content": "We tackle the Kryptonite-N dataset in the same way as the\noriginal work; a classification task with a labelled dataset\nD = {(x(i),y(i))}\\1. We train a parametric model f with\nparameters 0. The training function will seek to optimise\nthese parameters with respect to a loss function L and return\nthe model with the parameters producing the smallest loss."}, {"title": "3.1. Data Standardisation", "content": "Standardisation is a common pre-processing step to place\nattributes in uniform scale. The mean \u00b5 and standard devi-\nation o of each feature j are computed for the dataset and\neach value is scaled such that each feature has a mean of 0\nand a standard deviation of 1. $X_{ij} = \\frac{X_{ij}-\\mu_{j}}{\\sigma_{j}}$\nIt is especially important for models using linear combina-\ntions of weights and inputs, \u0177 = 0x and optimizing using\ngradient descent. Consider a single layer neural network,\n$\\partial w = Xz \\times x$ where z is the output of the layer. Therefore, all\npartial derivatives are scaled by the input features. If some\nfeatures are centred around a larger mean, they will cause\nlarger updates to their corresponding weights. Standardising\ndata counters against this better ensures convergence. It also\nenables relative comparison of polynomial coefficients for\npolynomial features which will be essential in understanding\nunderlying patterns across feature values."}, {"title": "3.2. Neural Networks", "content": "Neural Networks are universal function approximations and\ntherefore are a good solution to experiment with to solve the\nKryptonite-N dataset. Given an input vector x, a single layer\nI of a neural network will map this to output using a matrix\nof weights W\u2081 and a vector of biases br. The number of rows\nin W corresponds to the number of neurons in the layer, with\neach neuron connecting to all inputs with a corresponding\nweight. Mathematically: $A_{1} = \\sigma_{1}(W_{1}x^{T} + b_{1})$. Where \u03c3\u03b9 is\na non-linear activation function of the layer. An activation\nfunction is key to introduce non-linearity into the network,\nallowing it to approximate non-linear functions.\nLayers of a neural network can be stacked to pro-\nduce a multi-layer network, defined as:\n$\\hat{y} = \\sigma_{L}(\\sigma_{L-1}(...\\sigma_{0}(W_{0}x^{T} + b_{0})) + b_{1})$. Where L is the\nfinal layer in the network. In a multi-layer neural network,\nthere are 3 hyper-parameters to optimize for: the number of\nlayers, the number of neurons in each layer and the activa-\ntion function used in each layer."}, {"title": "3.2.1. MINI-BATCH GRADIENT DESCENT", "content": "For a given weight in a neural network, the update rule is\ngiven by w \u2190 w \u2013 a\u2207wL.\nStochastic gradient descent will compute the loss and the\nweight for every sample in the dataset. This can introduce\nnoise into the weight updates, making the training process\nunstable as every derivative can point in different directions.\nBatched gradient descent, on the other hand, computes a loss\nvalue after processing every sample in the dataset, making\none weight update for every pass of the dataset. This has\nthe disadvantage of slow convergence to optimal, as it needs\na full pass of the data for a single update.\nMini-batch gradient descent represents a middle ground,\nwhere the loss and derivative are calculated over a subset\nof the samples in the dataset. This helps to reduce the\nnoise from stochastic gradient descent, whilst speeding up\nconvergence compared to batched gradient descent. The\nbatch size hyperparameter B can be altered to adjust the\ntrade-off between convergence speed and noise reduction.\n(G\u00e9ron, 2019)[p118-127]"}, {"title": "3.2.2. ALTERNATIVE OPTIMIZERS", "content": "Stochastic gradient descent (SGD) with Momentum, can be\nwritten as shown in section A.4.3. This performs a gradi-\nent descent update whilst taking into account the previous\nvalues of the gradient. As optimization continues in the\nsame direction, larger steps are taken, enabling faster con-\nvergence to the optimal solution. Additionally, it allows the\ngradient descent process to converge to the global minima\naway from any local minima (G\u00e9ron, 2019)[p352]. This\nadaptation to SGD can add improvement but comes at the\ncost of an additional hyper-parameter \u03b2.\nAn extension to SGD is adaptive moment estimation\n(Adam), defined as shown in section A.4.4 (G\u00e9ron,\n2019)[p356]. Adam combines accumulated momentum m,\nand that of the square of the gradients s. Applying equations\n3 and 4 gives bias-corrected estimates, counteracting the\ninitialisation of these parameters to 0 (Kingma & Ba, 2017).\nFinally, m and \u015d are applied to \u03b8. Performing element-wise\nscaling by \u015d in the final term decays the learning rate pro-\nportional to the size of the individual parameter gradients.\nThis encourages a more direct solution to the minimum,\nrather than following a curve, which SGD with momentum\nwould exhibit (G\u00e9ron, 2019)[p355]. Using Adam introduces\n2 additional hyper-parameters B\u2081 and B2."}, {"title": "3.2.3. DROPOUT", "content": "Dropout is used to counteract overfitting in neural networks.\nMathematically, a neuron in a layer has probability p to out-\nput a zero (and be dropped out) in a single forward pass. For\nan input x, we have x = r * x, where r ~ Bernoulli(p)"}, {"title": "3.3. Polynomial Basis Expansion\nLogistic Regression", "content": ""}, {"title": "3.3.1. BASIS EXPANSION", "content": "Basis Expansion increases the dimension of original fea-\nture space using non-linear transformation. In particular,\nPolynomial Basis Expansion introduces polynomial com-\nbinations of features: for a given input feature of length m\nx = (X1, X2, ..., Xm), polynomial basis expansion of degree\nn will generate all polynomials of powers up to degree n.\nNot only does it capture multiple powers of the same fea-\nture but also interactive features; and products of distinct\nfeatures (Xi, xj, xk; i \u2260 j \u2260 k).(Pedregosa et al., 2011) It is\nparticularly useful as the resulting feature space is linearly\nseparable(Bishop, 2006 - 2006) while capturing complex\nrelationships across feature values.\nDefining as a basis function: let x = (x1, x2, .., xm) \u2208 Rm\n\u03c6(x) which will generate following polynomial basis where\nx = (X1, X2, X3), m = 3:\n$\\phi: R^{m} \\rightarrow R^{\\binom{n+m}{n}-1}$\n$\\phi(x) = [X_{1}, X_{2}, X_{3}, X_{1}X_{2}, X_{1}X_{3}, X_{2}X_{3}, x_{1}^{2}, x_{2}^{2}, x_{3}^{2}]^{T}$\nThe function will map to a new feature space with the fol-\nlowing basis where interactions coloured in blue."}, {"title": "3.3.2. LOGISTIC REGRESSION", "content": "Logistic Regression is one form of generalised linear model\nthat is suitable for a classification problem like the task in\nhand, formulated as a composition of \u03c3 \\circ \u03c6 where activation\nfunction o is a logistic sigmoid; generating Bernoulli Dis-\ntributed posterior probabilities P(y|x, w).(Murphy, 2012 -\n2012; Bishop, 2006 - 2006)\n$y = \\sigma(f(x)), f(x) = w^{T} \\phi(x); \\sigma(f) = \\frac{1}{1+exp(-f)} \\in [0, 1]$"}, {"title": "3.3.3. REGULARIZATION", "content": "Regularization is used to prevent models from overfitting.\nL2 regularization adds the squared L2 norm of the weights\n||w|| as a penalty to the loss function, whereas L1 regular-\nization adds the L1-Norm of the weights ||w||1 to the loss\nfunction. A acts as a hyperparameter to control the influence\nof the penalty on the loss function.\nBoth L1 and L2 regularization aim to constrain the model"}, {"title": "4. Experimental Design", "content": "Given that there is an underlying pattern in the dataset,\nas specified in the original work, we hypothesise that the\nKryptonite-N can be solved using a universal function ap-\nproximation tool."}, {"title": "4.1. Neural Networks", "content": "Training a neural network, was sufficient to achieve the\ndesired accuracy on Kryptonite datasets N9-18, with the\nresults shown in the subsequent section.\nFor all datasets, the data was first scaled as detailed in sec-\ntion 3.1 and prepared with a training/validation/testing split\nof 60%, 20%, and 20%. Validation data was used to manu-\nally tune hyperparameters, whilst the test data was used as a\nhold-out test set to measure the generalization capabilities\nof the trained model. Before splitting, the data was shuffled\nwith a random seed to ensure a diversity of samples across\nthe three sets.\nTo reduce the amount of overfitting, a simpler neural net-\nwork was favoured over a more complex one. Therefore,\nwe kept the number of hidden layers to 1, which used 72\nneurons. This network architecture selected was performant\nand generalized to all N9-18 datasets. Either a Tanh or an\nELU activation function was used in all the penultimate\nlayers, with a sigmoid activation in the final layer to output\np\u2208 [0,1] (all activation functions are detailed in section\nA.4.2), interpreted as a probability. Dropout was used for\nsome networks between the hidden and output layers, to\ncontrol overfitting.\nEach neural network was trained using either an SGD with\nmomentum or an Adam optimizer, as we detail in section\n3.2.2. The networks were trained with batched data for 1000\nepochs, with early stopping (Prechelt, 2012) employed to\nstop after 200 epochs with no improvement on the validation\ndata. The network with the best validation loss (binary cross-\nentropy loss - shown in section A.4.1) was used as the final\ntrained network."}, {"title": "4.2. Polynomial Basis Expansion\nLogistic Regression", "content": "The first iteration was following the same implementation\nof the original work but with standardised dataset. Stan-\ndardisation of input features greatly improved the training\naccuracy for N=9 albeit with some overfitting and small\nimprovements for larger N.\nIn order to understand the effect of including all polynomial"}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1. Neural Networks", "content": "The architectures of each neural network used for each\ndataset are shown in table 5 within the appendices. As\nmentioned in section 4.1, the same baseline network was\nused, a single-layer network with 72 neurons. Across the\ndatasets, we varied the learning rate, optimiser, activation\nfunction, batch size and dropout rate."}, {"title": "5.1.1. TUNING HYPERPARAMETERS", "content": "For hyperparameter tuning, a baseline network comprised\nof 72 neurons, ELU activation function and SGD with Mo-\nmentum was used. Using the validation dataset, optimiser,\nlearning rate, batch size and dropout probability p were fine-\ntuned manually to achieve the desired accuracy on the test\nset as reported in the original work. Due to the vast amount\nof hyperparameter combinations, performing an exhaustive\nsweep of the hyperparameter space in the given time was\nnot feasible. However, ideally, this should be explored in\nthe future, as additional performance could be gained.\nAcross all datasets, we noticed that a larger batch size gener-\nally led to increased performance on the validation dataset.\nFor the N-9 dataset, the baseline network was adequate to\nreach the desired test accuracy with no changes applied.\nFor the N-12 dataset, the baseline network exhibited small\noverfitting and was countered by adding dropout after the\nhidden layer with p = 0.01. This helped reduce overfitting\nand reach the desired test accuracy.\nFor the N-15 dataset, we found replacing the ELU activa-"}, {"title": "5.1.2. NEURAL NETWORK RESULTS", "content": "The results for the tuned neural networks on the test data\nare shown in table 5 where we report average test accuracy\nwith standard deviation after 10 training and testing runs for\neach network - where each reaches the target accuracy.\nThe training curves for each neural network are detailed\nin figures 6 to 9 in the Appendix. Across all datasets, the\nselected models show little to no overfitting with slight\ndeviations from the mean. This shows our networks exhibit\nstrong stability in training and generalisation to new unseen\ndata. Due to this, we confidently expect to see similar\naccuracy on new data generated from the same underlying\ndistribution."}, {"title": "5.2. Polynomial Basis Expansion\nLogistic Regression", "content": "The model performed best with SGD optimisation with L1\nregularisation taking standardised, interaction only polyno-\nmial expanded basis as inputs using only a limited degree\npowers of (3 * N, N) (accuracy distribution evaluated over\nmultiple models in Appendix ??) for N=9, 12 and 15 with\nexact parameters outlined in Table 2. Stricter regularisa-\ntion was necessary for greater N to assure convergence with\nbetter generalisation as we encountered a sparse weight\nlandscape. The higher degrees of the polynomial were per-\nformant with the largest contribution signals coming from\ninteraction features with a certain proportion of feature val-"}, {"title": "6. Dataset Secrets Revealed and\nFurther Logistic Regression", "content": "Through a series of well-thought-out experiments and co-\nincidental discoveries, we successfully unveiled the secrets\nof curating the Kryptonite-N dataset, which is a high-\ndimensional XOR problem with 1/3 redundant features. We\nrevealed the secrets of the dataset through a series of well-\nthought-out experimental methodology. The detailed steps\nof our discovery process can be found in Appendix A.2,\nwhich we strongly recommend reading. This section out-\nlines the mathematical formulation of the dataset at first.\nThe experimental results of our best model using Logistic\nRegression with L1-Regularization and an optimal logis-\ntic regression model with feature filtering oracle are also\nprovided."}, {"title": "6.1. Mathematical Formulation of the\nDataset", "content": "The Kryptonite-N dataset is characterized by a feature space\nwhere part of inputs is unrelated to the final output - such\ninput will be referred to as irrelevant features. While its\ncounterparts, which directly influence the final output, will\nbe termed informative features. Notably, in Kryptonite-N\nDataset, exactly $\\frac{N}{3}$ features are redundant. Specifically,\nfeature index space is defined as\n$F = \\{1,2,..., N\\},  |F| = N$\nInformative features are represented by the index space\n$F_{info} = \\{a_{i}\\}, F_{info} \\subset F,  |F_{info} = \\frac{2}{3}N$\nConversely, irrelevant features are represented by the\ncomplement set\n$F_{irre} = \\overline{F_{info}},  |F_{irre} = \\frac{1}{3}N$"}, {"title": "6.2. Testing Results", "content": "1. XOR Conjecture Verification\nAs a verification of our conjecture, the second column of\nTable 4 presents the test accuracy on the entire dataset\nusing our handcrafted feature selection, discretization, and\nXOR operator functions (XOR).\n2. Logistic Regression with L1 Regularisation\nLogistic Regression used here is Gradient-based using\nSGDClassifier in scikit-learn, as discussed in\nAppendix A.2. Only results of Logistic Regression with\nL1 Regularisation (LR with L1) on N = 9, 12, 15, 18 are\ndisplayed in the third column of Table 4 since we can't\ndo polynomial basis expansion on larger N with limited\nRAM space. However, it should achieve the same good\nperformance on high-dimensional space theoretically.\n3. Logistic Regression with Feature Selection Oracle\nWith Feature Selection Oracle (FSO) based on feature dis-\ntribution shape we observed in Step 6 of Appendix A.2, we\nreduced the feature space to 1 dimension only including\nthe multiplication of informative features, formulated as\nfollows: $input = \\prod_{i\\in F_{info}} x_{norm}$. Where norm implies\nnormalised. Experimental results of Logistic Regression\nunder such conditions (LR with FSO) can be seen in the\nfourth column of Table 4."}, {"title": "7. Discussion", "content": ""}, {"title": "7.1. Model Performance Insights", "content": "The experimental results highlighted different behaviours\nfor logistic regression (with polynomial basis expansion)\nand neural networks, which provided insights on various\naspects of fundamental problems in Machine Learning:"}, {"title": "1. Underperforming Neural Networks in High Dimen-\nsions", "content": "Neural networks struggled for Higher N due to the\nsparsity and combinatorial nature of the dataset. High-\ndimensional spaces introduce the \"curse of dimensional-\nity\" (Bellman, 1961), where the volume of the feature\nspace increases exponentially, making it harder to general-\nize from sparse data. This was apparent in the models' op-\ntimization process facing difficulties in finding the global\nminimum efficiently (Goodfellow et al., 2016)."}, {"title": "2. Better performance of Logistic Regression", "content": "Logistic re-\ngression performed well due to its linearity and simplicity,\nfocusing on the most informative features. Occam's Ra-\nzor, a principle advocating for simpler models with fewer\nparameters when multiple models explain the data equally\nwell (Jaynes, 2003), suggests that logistic regression, with\nfeature selection, can be a more effective solution to high-\ndimensional problems compared to more complex models."}, {"title": "3. Trade-offs in L1/L2 Regularization", "content": "\u2022 L2 regularization (Ridge regression) smooths the model\nby penalizing larger weights, which helps prevent over-\nfitting but may obscure important features, as it doesn't\nnull out coefficients. This results in a more general yet\nless interpretable model (Hoerl & Kennard, 1970).\n\u2022 L1 regularization (Lasso regression) encourages spar-\nsity, effectively selecting a subset of features. However,\nit may eliminate useful features when there are many\ncorrelated features, leading to potential underfitting (Tib-\nshirani, 1996).\nHowever, under this specific setting where one feature\ndeterministically outweighs others, L1 proves to be the\nideal choice."}, {"title": "4. Insights into XOR Problem and Dataset Design", "content": "The\nXOR problem is inherently non-linear, making it diffi-\ncult for linear models to solve, especially in high di-\nmensions. Besides, Kryptonite-N dataset also highlights\nhow structured redundancy can reveal a model's weak-\nnesses. By embedding these two difficulties, this dataset\nchallenges the Universal Approximation Theorem (Kol-\nmogorov, 1965). It demonstrates that irrelevant features\ncan impair a model's ability to focus on important signals,\nemphasizing the need for effective feature selection in\nhigh-dimensional spaces."}, {"title": "7.2. Sustainability Analysis", "content": "Sustainability within machine learning is becoming increas-\ningly more prevalent due to the rise of LLMs. To gauge\nthe carbon impact of our work, we use the common tool\n\"CodeCarbon\" to estimate carbon emissions of a Python\nprogram (Courty et al., 2024). We separated the estimates\nof carbon footprint for training and inference, where the\nformer expects to yield a higher value as a more resource-\nintensive process, the latter is equally important to reflect\nthe expected footprint post-deployment. (Luccioni et al.,\n2024)\nFigure 15 in the appendix shows the estimated emissions\n(in C02 eq) for both training (over 10 runs) and inference\n(batch size of 128). Measurements were performed on a\n13-inch MacBook Air with an Apple M3 SoC and models\nrunning only on the CPU. Our largest emission, 0.00014kg,\nis roughly equivalent to 1 metre of driving (openco2.net).\nOverall training emissions are low but grow as the number\nof features increases. Logically, this is sound as a larger\nnetwork will be needed to process more features. For test-\ning, we should expect the same pattern as in training, and\nwe reason that this could be due to insignificant emissions\nproduced. Nevertheless, if these networks were deployed,\ncareful consideration is needed of the minimum number of\nfeatures to perform the task.\nOne reason why the emissions here are so low is due to the\narm architecture of the M3 chip, which uses less power than\nan x86 counterpart. To gain an insight into the emissions\nof training and testing these models on different devices,\nwe show graphs of emissions and energy usage for an x86\nsystem in appendix A.10. We see nearly a 3x increase in\ntotal energy consumption when switching platforms, high-\nlighting the importance of system choice on emissions when\ntraining and testing models.\nWhilst gaining knowledge of the carbon footprint of the\ncodebase is important, it does not give the full sustainabil-\nity picture. Embodied emissions are often overlooked in\nsustainability analysis as a whole. These are the emissions\ncoming from the manufacturing of the hardware used to\ntrain and test the models (Faiz et al., 2024). Whilst over-\nlooked, they often contribute a significant amount of total\ncarbon. For example, Hugging Faces' BLOOM LLM has a\ntotal carbon footprint of 50 tonnes of CO2 eq, with embod-\nied emissions making up 22% of this (Luccioni et al., 2022).\nAlthough there are emerging tools to estimate embodied\nemissions (Faiz et al., 2024), generally they are much harder\nto estimate since the information needed is hidden from the\nend user. However, we can estimate that a 15-inch MacBook\nAir with an Apple M3 SoC has a 158kg C02 eq footprint\n(Apple). The use of the device vastly extends the scope of\nthis work, so we should only contribute a fraction of this\nto our overall emissions. However, it is useful to under-"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Analysis of the Original Work GPT 2", "content": "The original paper presents an idea for computing a basis expansion of the data using a generative pre-trained transformer\n(GPT). The primary idea behind this experiment was to construct a context-aware embedding of each vector. However, upon\ninspecting the source code we noticed several issues with this approach. Fundamentally, the work using GPT as a basis\nexpansion tool is floored and shows a lack of understanding of the architecture of large language models (LLMs).\nGiven a sequence of tokens constructed from a sentence, GPT is trained to minimise the negative log-likelihood of the next\ntoken (Radford & Narasimhan, 2018):\n$L_{1}(U) = -\\sum log P(U_{i}|U_{i-k}, ..., U_{i-1}; \\Theta)$\nIn this way, it seeks to complete sentences conditioned on the previous words. This is not to say that a GPT model cannot be\nused for other explicit tasks, this was explored in the original work, coined zero-shot or few-shot learning (Radford et al.,\n2019) (Brown et al., 2020). Therefore, generating an embedding from a vector using a description is not necessarily a poor\nmethod. However, it is a strong deviation from the original task of GPT.\nRegardless, there are several issues with how GPT was used as a basis expansion tool. Firstly, in the source code, the prompt\nasks the model to \"Classify the following vector for a binary classification task\". This does not match the original instruction\nfrom the paper: \"Please encode the following vector for a binary classification task\u201d. These 2 tasks are very different, with\nthe former asking to predict based on the vector, and the latter asking to encode based on the vector.\nSecondly, the basis expansion is taken as the mean of all outputs from the model. However, given a sequence, GPT will\noutput a vector of embeddings where each entry corresponds to the token used as the query, with the rest acting as keys/values\nto predict the token following the query (Foster, 2019)[p239-252]. GPT uses a causal mask to prevent the latter keys in\nthe sequence from being used to predict the token after the query (Vaswani et al., 2017) (Radford & Narasimhan, 2018).\nTherefore, averaging the embeddings is not logical. Rather, the final embedding should be taken which would be used to\npredict the token immediately following the sentence, in this case, our basis expansion.\nLastly, generating an embedding for each vector separately means that the model will not have access to any other vectors as\nno fine-tuning occurs. It will not be able to produce an embedding accounting for the variation of features across all data\npoints. This is essentially equivalent to generating noise for each data point and using this to train a model. In this case, we\nwould expect a model to be able to fit the noisy training data well but generalize very poorly to unseen data. We support this\nhypothesis by training a neural network on the final embedding with the correct prompt. The graph of training is shown in\nfigure 14."}, {"title": "A.2. Detailed Steps of Revealing the Dataset's Secret", "content": "The discovery process leveraged iterative experimentation and observation of model behaviours across datasets with varying\ndimensions:\n1. Initial Observations Logistic Regression struggled in N 9 with underfitting, when the polynomial expansion degree\nwas low. This hinted the need for higher-order feature interactions to linearly separate the data.\n2. Threshold Identification Upon testing with a higher polynomial degree, performance improved significantly as the degree\nrose from 5 to 6, indicating that a sixth-degree interaction was sufficient to capture the necessary feature information for\nN = 9.\n3. Dimension Scaling Challenges As N increased, optimisation became slower due to the curse of dimensionality. Switching\nto a gradient-descent-based optimiser, i.e. SGDClassifier in scikit-learn, mitigated this issue.(Scikit-Learn,\n2024)\n4. Overfitting in Higher Dimensions For N 15, the model achieved near-perfect training accuracy (99%) but failed on\nthe validation and test sets (64%), signalling overfitting with L2 regularization, which revealed evenly distributed low\nweights across coefficients.\n5. [Key] Transition to L1 Regularization Employing L1 regularization improved generalization. It reduced most weights to\nzero, leaving only one significant coefficient corresponding to a specific high-order feature interaction."}]}