{"title": "TrustRAG: An Information Assistant with Retrieval Augmented Generation", "authors": ["Yixing Fan", "Qiang Yan", "Wenshan Wang", "Jiafeng Guo", "Ruqing Zhang", "Xueqi Cheng"], "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a crucial\ntechnique for enhancing large models with real-time and domain-\nspecific knowledge. While numerous improvements and open-source\ntools have been proposed to refine the RAG framework for accu-\nracy, relatively little attention has been given to improving the trust-\nworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances RAG from three per-\nspectives: indexing, retrieval, and generation. Specifically, in the\nindexing stage, we propose a semantic-enhanced chunking strategy\nthat incorporates hierarchical indexing to supplement each chunk\nwith contextual information, ensuring semantic completeness. In\nthe retrieval stage, we introduce a utility-based filtering mecha-\nnism to identify high-quality information, supporting answer gen-\neration while reducing input length. In the generation stage, we\npropose fine-grained citation enhancement, which detects opinion-\nbearing sentences in responses and infers citation relationships at\nthe sentence-level, thereby improving citation accuracy. We open-\nsource the TrustRAG framework and provide a demonstration studio\ndesigned for excerpt-based question answering tasks 1. Based on\nthese, we aim to help researchers: 1) systematically enhancing the\ntrustworthiness of RAG systems and (2) developing their own RAG\nsystems with more reliable outputs.", "sections": [{"title": "1 INTRODUCTION", "content": "Information seeking (IS) is one of the most important activities in\nour daily life and work. In the past few decades, search engines have\nbecome the main way people access information by locating relevant\ndocuments from the Web [14]. In recent years, the rapid development\nof large language model (LLM) has opened new opportunities to\nimprove IS, shifting from ranking relevant documents to producing\nreliable answers [22]. However, generating answers directly with\nLLM presents challenges, including missing real-time information,\ninsufficient domain knowledge, and the risk of hallucinate claims,\nresulting in unreliable responses in real-world scenarios [4, 9, 15].\nTo this end, RAG has emerged as a promising solution by com-\nbining the strength of search systems and LLMs to improve the\nquality of results [6, 11, 21]. On one hand, RAG leverages search to\nprocess external large corpus, enhancing access to real-time infor-\nmation. On the other hand, it utilizes LLM to reason and generate\ntext, improving the accuracy of the answer. To better integrate the\nretriever and the generator, the researchers further developed vari-\nous RAG frameworks to improve the accuracy of answers, such as\nSelf-RAG [1], ActiveRAG [18], CoRAG [16], etc. In addition to\nimproving accuracy, some studies also try to improve source attribu-\ntion of the result, thus improving the reliability of results [5, 7, 23],\nsuch as InstructRAG [17], LongCite [19], SelfCite [2], etc. How-\never, most of these works focus on improving specific aspects of the\nRAG framework, while real-world applications require systematic\nenhancements across all components.\nIn addition to the above advances, researchers have created vari-\nous open-source systems [8, 10, 12, 20] to support the development\nand practical application of RAG. For example, Langchain 2 is the\nmost widely used RAG framework, providing modular components\nfor integrating LLMs with external data sources. LLamaIndex 3\nserves as a data framework designed to efficiently construct RAG ap-\nplications by streamlined data ingestion, indexing and querying pro-\ncesses. LightRAG 4 introduces a dual-level retrieval mechanism by\nincorporating the graph structure into the text indexing and retrieval.\nNevertheless, these frameworks focus mainly on modularizing the\nprocess and simplifying the implementation of RAG systems, fur-\nther efforts are needed to improve the attribution of RAG-generated\ncontent.\nIn this demo, we introduce a novel RAG system, named TrustRAG,\nto improve the accuracy and attribution of the result in a more com-\nprehensive way. The overall architecture of the system consists of"}, {"title": "2 SYSTEM OVERVIEW", "content": "The architecture of the system is shown in the Figure 1. The system\nconsists of two major components, namely the TrustRAG library\nand the TrustRAG studio.\nThe library functions as the system's back-end, offering a com-\nprehensive set of features for all stages of the RAG pipeline. Its\ncapabilities are structured into three modular components: the offline\nindexing module, the retrieval module, and the generation module.\nFirst, the offline indexing module provide rich parsing functions\nfor different kinds of files (e.g., PDF, Word, Excel, JSON) and con-\nverts chunked content into embeddings. Second, the retrieval module\noperates in three stages: query processing, retrieval, and utility as-\nsessment. Finally, the generation module also follows a three-stage\nprocess, comprising basic generation, citation integration, and post-\nprocessing.\nThe studio serves as the system's front-end, offering a user-\nfriendly GUI built on the TrustRAG library. It features two main pan-\nels: the knowledge manage panel and the conversation manage\npanel. In the knowledge manage panel, users can upload their own\ndocuments, configure processing options, and select the indexing\nmethod. In the conversation manage panel, users can choose search\nmethod and the LLM for each conversation. Additionally, the studio\nvisualize the intermediate \"thinking\" process of the TrustRAG, in-\ncluding query understanding, document selection, answer reasoning,\nand sentence citation, to enhance reliability and transparency."}, {"title": "3 TRUSTRAG LIBRARY", "content": "TrustRAG is a configurable and modular Retrieval-Augmented Gen-\neration (RAG) framework designed for \"reliable input and trust-\nworthy output.\" It consists of key components such as document\nparsing, text chunking, query optimization, retrieval ranking, content\ncompression, model generation, and answer citation. This section\nhighlights its innovations in semantic enhancement, usefulness en-\nhancement, and citation enhancement."}, {"title": "3.1 Semantic-Enhanced Indexing", "content": "Existing text chunking methods, while efficient, often lead to sig-\nnificant semantic loss, particularly when handling long or complex\ndocuments [13]. Simple character-based or paragraph-based splitting\ncan disrupt contextual coherence, making it difficult for downstream\nretrieval and generation tasks to fully utilize the semantic informa-\ntion embedded in the text.\nTo address this issue, TrustRAG introduces the semantic-enhanced\nchunking to improve the semantic integrity and coherence for each\nchunk. Specifically, we firstly take the LLM to apply co-reference\nresolution for each document, which resolves ambiguities caused\nby pronouns or incomplete references. For instance, when a pro-\nnoun like \"it\" appears in a sentence, the system identifies its an-\ntecedent and restores the missing context, thereby enhancing the\nsemantic completeness of the text. This process not only recovers\nlost semantic information but also provides more accurate contex-\ntual support for subsequent generation tasks. Moreover, we stan-\ndardize the time fields in the document by converting the relative\ntime references into standardize date formats based on the doc-\nument publication date. For example, if the document's publica-\ntion date is \"2025-02-18\", terms like \"yesterday\" and \"last Friday\"\nwill be converted to \"2025-02-17\" and \"2025-02-14\", respectively.\nThis process not only recovers lost semantic information but also\nprovides more accurate contextual support for subsequent gener-\nation tasks. The implementation of this feature can be found in\ntrustrag/modules/refiner/decontextualizer.py.\nFurthermore, TrustRAG supports advanced semantic segmenta-\ntion techniques that dynamically identify semantic boundaries using\nembedding technologies and large language models (LLMs). Un-\nlike static chunking methods, these techniques allow the system to\nadaptively split text based on its semantic structure, ensuring higher-\nquality chunks that preserve contextual coherence. The code is avail-\nable in trustrag/modules/chunks/semantic_chunk.py.\nThese innovations improve the quality of text indexing, laying a solid\nfoundation for reliable retrieval and generation."}, {"title": "3.2 Utility-Enhanced Retrieval", "content": "In conventional RAG systems, the relevance of retrieved documents\nis often determined solely by vector similarity. However, high sim-\nilarity does not always translate to usefulness for the generation\ntask. In some cases, even irrelevant documents may inadvertently\nimprove system accuracy, highlighting the need for more intelligent\nmechanisms to evaluate the utility of retrieved results [3]. TrustRAG\naddresses this limitation by introducing two key innovations: useful-\nness judgement and fine-grained evidence extraction."}, {"title": "3.3\nAttribution-Enhanced Generation", "content": "The credibility and traceability of generated answers are critical\nfor trustworthiness in RAG systems. Traditional approaches rely\nheavily on direct reasoning by large models, which can be slow and\nprone to inaccuracies in citations. Additionally, fine-tuning models\nto improve citation accuracy may compromise their performance on\nother tasks, limiting practical applicability.\nTrustRAG overcomes these challenges through two key innova-\ntions: post-generation citation and citation grouping with cross-\nreferencing."}, {"title": "3.4 Additional Modules", "content": "Beyond the three core enhancements, TrustRAG offers a rich set of\nmodular functionalities, each designed to support specific aspects of\nthe RAG pipeline:"}, {"title": "4 SYSTEM USAGE", "content": "The typical use case for RAG systems involves the summarization\nof retrieved information to produce concise, generalized content. Nu-\nmerous studies have significantly advanced the fluency of generated\nlanguage, the comprehensiveness of the covered information, and\nthe ability to extract core insights. However, there is a notable gap\nin research concerning the verification of the consistency between\nthe generated output and the original source text.\nTrustRAG addresses the ExQA task, with the primary goal of\ngenerating content that is both accurate and traceable to the original\ndocuments, while clearly indicating the sources of the information.\nThis task scenario emphasizes strict adherence to the constraints\nof the original reference materials during the generation process,\nensuring the credibility of the resulting answer.\nTo illustrate the application of the TrustRAG, we present a case\nstudy focused on news related to climate change. As depicted in\nFigure 3, the system follows three key steps: (I) constructing a\nknowledge base and uploading relevant documents, (II) configuring\nthe question-answering application, which involves selecting the\nappropriate knowledge base, choosing the suitable generation model,\nand specifying the desired output format, and (III) executing the\nquestion-answering task within the generated application instance.\nAs shown in step (III) of Figure 3, TrustRAG presents system\noutputs in a clear, concise, and structured format:"}, {"title": "5 CONCLUSION", "content": "In this demo, we introduce a novel TrustRAG system for risk-aware\ninformation seeking scenarios. Users can build their own RAG appli-\ncations with private corpus, study the RAG components within the\nlibrary, and experiment with RAG library with customized modules.\nWe will showcase the TrustRAG system through the following as-\npects: (1) we will use a poster to give an overview of the system and\nbriefly show the pipeline of the framework. (2) We will demonstrate\nhow to use the system to create RAG application with a private cor-\npus. (3) We will share insights on system's strengths and limitations,\nalong with potential future enhancements."}]}