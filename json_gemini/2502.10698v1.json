{"title": "Superpose Singular Features for Model Merging", "authors": ["Haiquan Qiu", "You Wu", "Quanming Yao"], "abstract": "Model merging is a critical technique for combining the capabilities of multiple\nfine-tuned models without requiring additional training. While existing methods\ntreat parameters as vectors, they overlook the intrinsic structure of linear trans-\nformation matrices - the core components that comprise the majority of model\nparameters. These matrices are fundamental to neural networks, mapping input\nrepresentations to output features through linear combinations. Motivated by the\nlinear representation hypothesis, we introduce task matrix and propose to Super-\npose Features from Task Matrix (SFTM), a novel approach that superposes features\nfrom individual task models into a merged model. SFTM employs singular value\ndecomposition to identify feature bases of linear transformation matrices and solves\na linear system to optimally combine them while preserving input-output mappings\nfrom individual task models. Extensive experiments on vision transformers and lan-\nguage models demonstrate that SFTM consistently outperforms existing methods,\nachieving superior performance and enhanced out-of-distribution generalization.", "sections": [{"title": "Introduction", "content": "Pre-trained models have become a cornerstone of modern deep learning, providing a powerful\nfoundation for a wide range of tasks [Zhuang et al., 2020, Bommasani et al., 2021]. To serve different\ntasks, fine-tuning the pre-trained models separately on different tasks is a common practice [Kenton\nand Toutanova, 2019, Hu et al., 2021]. However, maintaining multiple fine-tuned models for different\ntasks can be cumbersome and resource-intensive, especially as the number of tasks and models\ncontinues to grow. Multitask learning [Sanh et al., 2021, Raffel et al., 2020b] is a promising approach\nto address this issue by training a single model on multiple tasks simultaneously, but it often requires\na large amount of data and computational resources to achieve optimal performance [Pilehvar and\nCamacho-Collados, 2018, Phang et al., 2018, Fifty et al., 2021].\nRecently, model merging [Matena and Raffel, 2022, Jin et al., 2022, Ilharco et al., 2022a, Yadav et al.,\n2024] offers a solution to this problem by consolidating multiple models into a single model that\ncan perform well on multiple tasks effectively. This approach eliminates the need for maintaining\nmultiple separate models, reduces storage and computational requirements, and potentially enables\nknowledge transfer between tasks. A foundational approach to model merging is parameter aver-\naging [Choshen et al., 2022, Ilharco et al., 2022b, Wortsman et al., 2022], which simply takes the\nmean of corresponding parameters across models. This evolved into weighted averaging methods\nlike Fisher Merging [Matena and Raffel, 2022] and RegMean [Jin et al., 2022] that assign different\nweights to parameters based on their importance. A key advancement came with the introduction of\ntask vectors [Ilharco et al., 2022a] which represent the difference between fine-tuned and pre-trained\nweights and enable lightweight cross-task generalization. Various techniques [Huang et al., 2023,\nYang et al., 2023, Yadav et al., 2024, Yu et al., 2024, Du et al., 2024] were subsequently developed\nto handle task conflicts by resetting redundant parameters, resolving sign conflicts, dropping and\nrescaling parameters, and managing parameter competition between tasks. However, these methods\noften rely on heuristic rules for merging parameters while overlooking the intrinsic structure of\nparameters.\nOur method is motivated by the linear representation hypothesis [Mikolov et al., 2013, Arora et al.,\n2016, Olah et al., 2020]. The linear representation hypothesis states that the representations of deep\nneural networks can be decomposed into combinations of feature vectors. This linearity enables\nneural networks to represent complex concepts through these combinations. Linear transformations\n(e.g., in linear and convolution layers) play a crucial role in this process, as they can activate specific\nfeatures by measuring the similarity between a representation and a feature vector (via the inner\nproduct with the row vectors of the transformation matrix) and extract new features through linear\ncombinations (via the weighted sum of the column vectors of the transformation matrix). These\nproperties have been leveraged in recent years to interpret LLMs [Elhage et al., 2022, Bricken et al.,\n2023, Templeton et al., 2024] and design more efficient fine-tuning techniques [Hu et al., 2021, Zhang\net al., 2023b]. Consequently, in model merging, it is essential that the linear transformations in the\nmerged model preserve the input-output mappings of those in the individual models for effective\nfeature activation and extraction.\nIn this paper, we propose a method for merging linear transformation matrices while preserving the\ninput-output mappings of individual models' linear transformations. Our approach leverages Singular\nValue Decomposition (SVD) to merge matrices, treating left singular vectors as basis for output\nrepresentations and right singular vectors as basis for input representations. The goal is to ensure the\nmerged matrices preserve the directions of output features from original transformations when applied\nto corresponding input features across tasks. We show this problem reduces to solving a system of\nlinear equations, enabling model merging in singular decomposition space while maintaining feature\ndirections from various tasks. We validate our method on multiple benchmark datasets spanning\nvision and natural language processing tasks. The results demonstrate that our method consistently\noutperforms existing model merging techniques."}, {"title": "Related Work", "content": "Model merging is a technique that combines multiple models into a single model to enhance perfor-\nmance or enable the model to perform multiple tasks. Previous studies have shown that averaging the\nweights of multiple models fine-tuned from the same pre-trained initialization is a promising approach\nfor model merging. Fisher Merging [Matena and Raffel, 2022] advances beyond simple averaging by\nutilizing the Fisher information matrix to assess the importance of individual parameters, which are\nthen weighted accordingly during the merging process. Similarly, RegMean [Jin et al., 2022] forms\na linear regression problem with extra data for each layer and offers a closed-form solution for the\nmerged model's parameters by solving the regression problem.\nBeyond parameter averaging, Task Arithmetic [Ilharco et al., 2022a] introduces task vectors and\nadding the task vectors of individual tasks to merge model, demonstrating their effectiveness and\nlightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composi-\ntion [Zhang et al., 2023a] extends the task arithmetic framework to merge LoRA [Hu et al., 2021],\nwhile Ties-Merging [Yadav et al., 2024] addresses task conflicts by resetting redundant parameters\nand resolving sign conflicts. These methods, however, use a single merging coefficient across all task\nvectors, which limits their flexibility. In contrast, Lorahub [Huang et al., 2023] and AdaMerging [Yang\net al., 2023] use different coefficients for enhanced adaptability. Lorahub's performance is limited as\nit only searches for coefficients at the task level, while AdaMerging requires complex training and\nunlabeled test datasets, making it applicable solely to classification problems. DARE [Yu et al., 2024]\nproposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging [Du"}, {"title": "Linear Representation Hypothesis", "content": "The linear representation hypothesis states that neural networks encode information by summing\nup \"feature vectors\" [Mikolov et al., 2013, Arora et al., 2016, Olah et al., 2020], i.e., a layer of a\nnetwork represents a set of features as a weighted sum of task-associated vectors. This hypothesis\nhas been observed in various models, including word embeddings [Mikolov et al., 2013, Conneau\net al., 2017], sentence embeddings [Bowman et al., 2015], Transformer language models [Meng\net al., 2022, Hendel et al., 2023], and vision-language models [Trager et al., 2023, Perera et al., 2023].\nThe hypothesis has been explioted in various fields, especially in probing [Alain and Bengio, 2018,\nBelinkov, 2022] and interpretability [nostalgebraist, 2020, Elhage et al., 2022, Bricken et al., 2023,\nGao et al., 2024].\nEspecially, the linear representation hypothesis is prominently featured in recent works on mechanistic\ninterpretablity of language models [Olsson et al., 2022, Elhage et al., 2022, Bricken et al., 2023,\nTempleton et al., 2024]. In mechanistic interpretability, models are understood by decomposing\nthem into interpretable components and understanding how these components interact. The linear\nrepresentation hypothesis suggests that important features in neural networks are often represented\nthrough linear combinations of neuron activations, rather than complex nonlinear transformations.\nThis hypothesis has gained significant empirical support through studies of language models. For\ninstance, [Elhage et al., 2022] demonstrated a toy model that learned to resconstruct its input through\nlinear combinations of its neurons. Furthermore, Bricken et al. [2023] showed that individual neurons\nin a language model can encode semantically meaningful features in a largely linear fashion, which is\nlater sclaed to large language models [Templeton et al., 2024]."}, {"title": "Method", "content": "We start with a set of tasks {T1, . . ., Tr} and various pre-trained models. The objective is to fine-\ntune these models either by updating all parameters or using parameter-efficient fine-tuning (PEFT)\nmethods. The goal of model merging is to combine multiple fine-tuned models {\u03b81,..., \u03b8\u03b7} into\na single model \u03b8m that can perform all tasks {T1,..., Tr} effectively without requiring access to\ntraining data. While existing methods treat model parameters as vectors to be merged, we focus on the\ncrucial role of linear transformations, which comprise the majority of parameters and computations in\nneural networks. These transformations are essential as they project data into spaces where important\nfeatures become more apparent, making information processing more efficient. To formally analyze\nthese transformations during merging, we introduce the Task Matrix in Definition 3.1.\nExisting merging methods based on task vectors neglect the internal structure of linear\ntransformation in neural networks, and merge parameters as vector, i.e., $vec(P_j) = vec(P_{pre}) + vec(M_i)$. These methods employ various heuristic rules to merge parameters, which requires many\nhyperparameter tuning.\nTo merge models, preserving the input-output mappings of linear transformations is essential for\nmaintaining model capabilities. Our method focuses specifically on merging linear transformations\nin a way that retains their key input-output mappings through singular value decomposition, rather\nthan treating all parameters equally as in previous approaches. Specifically, we focus on merging task\nmatrices Mi, i = 1 . . . T into a single merged matrix M \u2208 Rm\u00d7n. For a input feature r \u2208 R, we\naim to ensure that the output of the merged model Mr maintains the same directional features as the\noutputs Mir from the original task-specific models, i.e., the merged model preserves the input-output\nmappings of the individual models. The final merged model is then constructed by adding the merged\ntask matrix M to the pre-trained model with a scaling factor \u03b3: $P_j = P_{pre} + \\gamma M$."}, {"title": "Merge Task Matrices to Superpose Singular Features", "content": "To keep the directional features of the task matrices Mir\nduring merging, we first need to identify some basis features that is representative for the input and\noutput representations. Singular value decomposition (SVD) of task matrices provides a natural way\nto obtain the basis features. Given the decomposition Mi = $\\sum_{k=1}^{T_i} \\sigma_i^{(k)}u_i^{(k)}v_i^{(k)T}$, $\\sigma_i^{(k)}$ is the\nk-th singular value, $u_i^{(k)} \\in \\mathbb{R}^{m\\times 1}$ and $v_i^{(k)} \\in \\mathbb{R}^{n\\times 1}$ are the k-th left and right singular vectors of\nMi, respectively. Therefore, the right singular vectors $v_i^{(k)}$ form a basis for input representations,\nwhile the left singular vectors $u_i^{(k)}$ form a basis for output representations. We named these basis\nfeatures as singular features in our paper.\nExcept form the basis, singular features have several other properties that make them suitable for\npreserving input-output mappings during merging. First, the orthogonality of singular vectors within\neach task minimizes interference between features, enabling linear superposition of features in\naccordance with the linear representation hypothesis. Second, the decomposition reveals the model's\ninformation processing mechanism - using inner products to detect input features and weighted sums\nto construct outputs. We leave the details of the good properties of singular features to the appendix.\nSince we have the singular vectors as the basis of the\ninput and output representations for linear transformation M\u2081, the merged matrix M should be able\nto keep the directions of output features from original transformations when applied to corresponding\ninput features across tasks. To accomplish this, we can merge the task matrices M\u2081 in their singular\nspaces:\n$M= \\sum_{i=1}^{T} \\sum_{k=1}^{T_i} a_i^{(k)} \\sigma_i^{(k)}u_i^{(k)}v_i^{(k)T},$\nwhere $a_i^{(k)}$ are the merging weights. To preserve feature directions during merging, we require that\nwhen the merged matrix M transforms an input representation, the output should maintain the same\ndirectional features as those produced by the individual task matrices M\u00bf. This leads to the following\nobjective:\n$\\langle u_j^{(k)}, Mv_j^{(k)}\\rangle = \\langle u_i^{(k)}, M_iv_j^{(k)}\\rangle, \\forall i,j,k$.\nwhere $\\langle , \\rangle$ is the inner product. In (2), the inner product measures how much of the transformed\nvector $Mv_j^{(k)}$ aligns with the direction of $u_i^{(k)}$, which is the direction of $M_iv_j^{(k)}$. This ensures that\nwhen the merged matrix M acts on an input feature vector $v_j^{(k)}$, it maintains the same directional\ncomponent along $u_i^{(k)}$ as the original task matrix M\u00bf. While it would be ideal to fully preserve the\nfeatures by requiring $Mv_j^{(k)} = M_iv_j^{(k)}$ for all tasks, this is generally impossible due to the resulting\noverdetermined system of equations. Instead, we relax this constraint to (2), which preserves the\ndirection of corresponding output features while allowing other features to be superposed linearly,\naligning with the linear representation hypothesis.\nTo solve (2), we rewrite it as a linear system for\nall tasks in {T1,..., Tr}:\n$\\sum_{i=1}^{T} \\sum_{k'=1}^{T_i}  a_{i'}^{(k')} \\sigma_{i'}^{(k')} u_i^{(k)T}u_{i'}^{(k')} v_{i'}^{(k')T}v_j^{(k)}  = \\sigma_i^{(k)}$,\nFor various tasks Ti and singular values, (3) forms a linear system with $r = \\sum_{i=1}^{T}r_i$ variables $a_i^{(k')}$ and r equations. The coefficient matrix of this linear system is the element-wise product of three\nmatrices:\n$\\Sigma = 1 \\otimes [\\sigma_{i'}^{(k')}]_{i',k'} \\in \\mathbb{R}^{r\\times r}$, where 1 \u2208 Rr\u00d71, \u2297 is the Kronecker product, and $[\\sigma_{i'}^{(k')}]_{i',k'}$ is a\nrow vector of singular values;"}, {"title": "Complete Algorithm", "content": "Except the linear transformation matrices, there are other parameters in the model that need to be\nmerged, such as biases, normalization parameters, embeddings, and convolutional layers. For biases\nand embeddings, we merge them with task arithmetic, i.e., adding them task vectors together. For\nnormalization parameters, we merge them by averaging the normalization parameters of the individual\ntasks because the normalization can be seen as a linear transformation with diagonal matrix. For\nconvolutional layers, we convert them into linear transformations by reshaping the convolutional\nkernels into a matrix and then merge them with the same method as the task matrices.\nWe also follow Ties-Merging [Yadav et al., 2024] and apply a trimming step. This involves keeping\nonly the top \u03b7 parameters by magnitude while setting the remaining parameters to zero. This\npreprocessing step helps reduce noise and focus on the most significant parameters during merging.\nWe present the complete algorithm for merging models in Algorithm 2."}, {"title": "Discussion", "content": "Because there are T tasks matrices of size m \u00d7 n, SVD of these matrix takes\nthe time complexity O(Tmn\u00b2). To merge T task matrices, the linear system has r equations and\nvariables to solve, which takes the time complexity O(r\u00b3). Therefore, the overall time complexity of\nSFTM for merging T task matrices is O(Tmn\u00b2 + r\u00b3). See Section 4.3 for the time consumption of\nSFTM for various models.\nIn this paper, our focus is on merging task\nmatrices instead of fine-tuning matrices. We have discovered that fine-tuned linear transformation\nmatrices tend to have more shared features across tasks. Therefore, merging the fine-tuned linear\ntransformation matrix is not as efficient as task matrix because certain overlapping direction of\nsingular vectors correspond to these common features. On the contrary, the singular vectors of the\ntask matrix contain features that are more specific to individual tasks, making it a more effective\napproach for merging. See the results of merging fine-tuned matrices in Section 4.3."}, {"title": "Conclusion", "content": "In this paper, We present SFTM, a novel model merging approach that preserves task-specific features\nin linear transformations through singular value decomposition. Extensive experiments demonstrate\nthat SFTM consistently outperforms existing methods across different architectures and tasks. The\nsuccess of SFTM demonstrates the value of leveraging parameter structure in model merging, rather\nthan treating all parameters as vectors. Our work establishes a foundation for future research on\nmodel merging that leverages the mathematical properties and structural roles of different parameter\ntypes."}]}