{"title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology", "authors": ["Longchao Da", "Xiaoou Liu", "Jiaxin Dai", "Lu Cheng", "Yaqing Wang", "Hua Wei"], "abstract": "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification. The response data and code will be released upon publication.", "sections": [{"title": "1 Introduction", "content": "Deep learning models have long been criticized for their lack of trustworthiness due to their complex network structures and opaque decision-making processes [10, 23, 36]. This has motivated researchers to investigate methods for understanding and quantifying the uncertainty associated with these models [1, 26, 27]. Recently, Large Language Models (LLMs) have demonstrated significant advancements over traditional deep learning approaches across a variety of tasks [31, 47]. However, concerns about their reliability persist. LLMs often produce outputs that are difficult to verify, particularly in scenarios requiring complex reasoning [38]. This introduces risks in critical applications, such as healthcare or legal domains [5, 18], where incorrect or unreliable reasoning can have severe consequences. Properly quantifying uncertainty in the reasoning processes of LLMs is therefore crucial for ensuring their safe and effective deployment.\nExisting research on Uncertainty Quantification (UQ) for LLMs primarily focuses on analyzing semantic uncertainty [9, 20, 25, 34], which involves examining patterns in the meaning and phrasing of multiple responses generated for a given question. Although this approach provides insights into output-level variability, it neglects the logical reasoning steps that lead to these answers. As a result, it fails to address foundational issues in the reasoning process that could be used to debug or optimize model outputs. For instance, when asked by the the same question, different reasoning paths may converge on the same final answer (an example shown in Appendix Figure 6), yet some paths may involve inconsistencies or logical leaps. By quantifying uncertainty at the level of reasoning steps, we can better identify such inconsistencies, support human-in-the-loop systems for validating outputs in sensitive applications, and uncover weaknesses in a model's reasoning process. This highlights the importance of incorporating reasoning-based uncertainty quantification.\nIn this paper, we address the problem of uncertainty quantification for logical reasoning steps by explicitly modeling reasoning processes as logical topologies. Existing work often treats the reasoning process generated by single Chain-of-Thought (CoT) sequences as a single \"long answer\" [42, 44], calculating semantic consistencies directly. While this approach captures some aspects of reasoning paths, it oversimplifies real-world reasoning processes, which often involve hierarchical dependencies and parallel sub-tasks. To overcome these limitations, we propose a novel formalism that explicitly models reasoning as a logical graph. In this representation, nodes correspond to individual reasoning steps, while edges capture logical dependencies between them. This structure enables more granular and interpretable analyses of uncertainty."}, {"title": "2 Related Work", "content": "In this section, we review the related work in the research domains of uncertainty quantification (UQ) for large language models (LLMs) and methods for explanation-based UQ, with a focus on reasoning processes."}, {"title": "2.1 UQ for LLM", "content": "White-box Approaches. A significant body of research has focused on performing UQ for LLMs by inducing the models to output their uncertainty along with their responses [19, 24, 28, 40]. These methods often rely on token-level probabilities to train or fine-tune models for predicting uncertainty. While effective, these approaches require full access to the model's structure and weights, which is impractical for black-box or commercial LLMs. For example, supervised methods such as those in [19] estimate uncertainty using logits and ground truth labels but are computationally expensive and resource-intensive.\nBlack-box Approaches. Another line of work estimates uncertainty directly at the response level using semantic entropy [20]. While this method avoids token-level dependencies, it still relies on access to token probabilities, limiting its applicability in black-box settings. To address these limitations, researchers have proposed lightweight black-box methods that analyze response inconsistencies. For instance, Lin et al. [25] use graph Laplacian eigenvalues as an uncertainty indicator, while Chen and Mueller [6] computes confidence scores from generated outputs to identify speculative or unreliable answers. However, these approaches primarily focus on semantic-level analysis and neglect the logical structure underlying reasoning processes. Moreover, methods like [25] average entailment probabilities without considering directional information in reasoning paths."}, {"title": "2.2 UQ for LLM Explanation", "content": "Explanation-based UQ focuses on assessing the reliability of natural language explanations (NLEs) generated by LLMs by either prompting models to express confidence in their explanations or analyzing consistency across multiple outputs under varying conditions [39, 45]. While these methods provide insights into explanation robustness, they treat explanations to a question as a unstructured text representation, which lacks structural information and fails to capture inconsistencies or leaps in logic. In contrast, our work explicitly leverages well-structured reasoning topologies to enhance the UQ process for explanations. This structured representation enables us to assess explanation uncertainties at a finer granularity within complex reasoning paths."}, {"title": "3 Preliminaries", "content": "In this section, we provide the foundational concepts for our study, including the definition of uncertainty in LLMs and the quantification of uncertainty in natural language explanations (NLE)."}, {"title": "3.1 Uncertainty of LLMs", "content": "Uncertainty quantification (UQ) has been a critical topic in classical machine learning tasks such as classification and regression [1, 11, 16, 21]. However, natural language generation (NLG) poses unique challenges for uncertainty quantification due to its high-dimensional output space, the need to account for semantic equivalence across distinct token sequences, and the limited accessibility of internal model outputs in black-box LLMs [25].\nGiven these challenges, most UQ methods for LLMs focus on analyzing uncertainty directly from model-generated responses as shown in the left part of Figure. 1. The problem could be defined as follows:\nPROBLEM 1 (UNCERTAINTY QUANTIFICATION FOR LLMS). When an LLM M is provided with an input x, either a query or prompt, the goal of an uncertainty function Ux is to map the generated outputs to"}, {"title": "a scalar score that determines the uncertainty of the LLM M, i.e.,", "content": "$U_x = U (\\{M(x_i)\\}_{i=1}^n)$                                                                                                                                      (1)\nHere, $\\{M(x_i)\\}_{i=1}^n$ denotes a set of n responses generated by the model M, and U aggregates uncertainty across multiple responses. Note that while noticed that some work models have uncertainty describing the confidence of a specific output given the input [39], in this paper $U_x$ only depends on x and is a property of the predicted distribution, which is estimated by U that aggregates uncertainty across multiple responses.\nDepending on the specific UQ method used, each $x_i$ may correspond to either repeated inputs or rephrased variations of the input prompt x. For black-box methods that analyze response variability or semantic consistency [25], multiple outputs (n > 1) are typically required. In contrast, white-box methods that rely on internal model information such as logits [19] may only require a single output (n = 1). However, both black-box and white-box methods have limitations when applied to reasoning tasks. Black-box methods often focus on semantic output variability without capturing deeper uncertainties in reasoning steps. White-box methods are frequently inaccessible due to API restrictions or computational constraints."}, {"title": "3.2 Uncertainty of LLM Explanations", "content": "To address these issues, researchers try to understand uncertainties in reasoning processes through the natural language explanations (NLEs) [4, 39] as a proxy. As shown in the right part of Figure. 1, An NLE is a textual reasoning sequence generated by a language model M, typically derived to justify or explain the answer a for a given input question $x_q$. We formally define it as follows:\nDEFINITION 1 (NATURAL LANGUAGE EXPLANATION). Given a model M and an input prompt $x_q$, an NLE can be represented as:\n$M(x_q + x_e) = a + a$                                                                                                                                  (2)\nwhere $x_e$ is an explanation-specific prompt, a is the model's answer to the query $x_q$, and a is the generated explanation accompanying the answer.\nThe explanation a contains a sequence of reasoning steps, represented as a = {$S_1, S_2, ..., S_m$}, which capture the reasoning process or justification for a.\nTo quantify uncertainty in explanations, we extend Problem 1 to consider n explanations generated for the same query $x_q$. Each explanation $a^i$ (i \u2208 {1, 2, . . ., n}) corresponds to a set of reasoning steps derived from the same query. Each explanation consists of m reasoning steps, represented as $a^i = \\{S_{i,1}, S_{i,2}, ..., S_{i,m}\\}$. The overall uncertainty across all n explanations is captured by aggregating reasoning-level uncertainties for each explanation. This can be formally defined as follows:\nPROBLEM 2 (UNCERTAINTY QUANTIFICATION FOR LLM EXPLANATIONS). Given an input prompt $x_q$ and an explanation-specific prompt $x_e$, the model M generates a set of answers $a_i$ to the query $x_q$, along with accompanying explanations $a^i$. The uncertainty for the query $x_q$ is then defined as:\n$U_{x_q} = U (\\{a_i\\}_{i=1}^n) = U (\\{\\{S_{i,1}, S_{i,2},..., S_{i,m}\\}\\}_{i=1}^n)$                                                                                           (3)\nHere, $U_{x_q}$ represents the overall uncertainty for the query $x_q$, and U aggregates uncertainties across all reasoning steps from all n explanations.\nUnlike prior methods that focus on token-level or semantic variability [39], in this paper, we explicitly models reasoning structures within explanations. By leveraging logical topologies, we aim to capture nuanced uncertainties at both the explanation level and individual reasoning step level."}, {"title": "4 Method", "content": "In this section, we propose a novel framework for reasoning uncertainty quantification in large language models (LLMs). To capture the complexity of reasoning paths, the proposed framework consists of two main steps: (1) Reasoning Topology Elicitation, which constructs a structured reasoning graph from LLM-generated explanations, and (2) Topology-enabled Reasoning Quantification, which leverages the constructed graph to perform uncertainty quantification using measures such as graph edit distance and reasoning path redundancy analysis. These steps work together as a comprehensive framework for analyzing reasoning uncertainty in LLMs."}, {"title": "4.1 Reasoning Topology Elicitation", "content": "The objective of this step is to construct a structured reasoning topology that captures the complexity of reasoning paths generated by large language models (LLMs). Existing approaches [39] represent reasoning explanation as linear sequences of steps from Chain-of-Thought (CoT) prompting [43]. While linear text sequences provide basic interpretability, they fail to capture complex logical transitions required for tasks like comparative reasoning or multi-faceted conclusions. This lack of structural richness limits the ability to analyze and quantify uncertainty in LLM-generated reasoning processes.\nTo address these limitations, we propose to elicit reasoning topologies from a question $x_q$ to an answer a as a directed graph G = (V, &), where V is the set of nodes corresponding to knowledge points or intermediate steps, and & is the set of edges capturing logical dependencies between them. For example, given the query $x_q$: \"What are the causes of climate change?\", the knowledge points $K_q$ might include sub-questions such as $k_1$: \"What is the role of greenhouse gases?\", $k_2$: \"How does deforestation contribute?\", and"}, {"title": "k3: \"What is the impact of industrial activities?\". The corresponding answers A = {$a_1, a_2, a_3$} provide detailed explanations for each sub-question towards the final answer $a_3$. These knowledge-answer pairs are then connected based on their logical dependencies to form the reasoning topology graph $G_q$. This graph-based representation enables a more comprehensive understanding of the reasoning process and provides a foundation for richer uncertainty quantification.\nSpecifically, the construction of G consists of three modules: (1) Knowledge Point Reflection, where the LLM identifies sub-questions or knowledge points required to address the query; (2) Self-Answering Module, where the LLM generates answers for each identified knowledge point; and (3) Reasoning Topology Construction, where the knowledge-answer pairs are organized into a directed graph that reflects the overall reasoning path.", "content": "4.1.1 Knowledge Point Reflection Module. The first module, the knowledge point reflection module, involves eliciting sufficient information that can be used to support the conclusion drawing toward the input $x_q$. The input to this module is the input query $x_q$ along with the prompt template $T_1$ to encourage the LLM to reflect 'What knowledge basis (or sub-questions) it should know to draw a final conclusion?'. And the output of this module is the set of knowledge points $K_q$ extracted as a series of sub-questions, i.e., $K_q = \\{k_1,k_2, ..., k_n\\}$. Specifically, we design a prompt template $T_1$ to guide the model in reflecting on the sub-questions or knowledge points required for solving $x_q$:\nTemplate $T_1$: Given a question {x}, reflect and generate the knowledge points or sub-questions necessary to solve this query. Ensure that the output is both sufficient and concise.\nThe model generates a set of knowledge points $K_q = M(x_q, T_1) = \\{k_1,k_2,..., k_n\\}$, where each $k_i$ corresponds to a specific sub-question or piece of information identified as necessary to address the query $x_q$ under the guidance of prompt $T_1$. To ensure traceability, we assign unique identifiers to each knowledge point using a tagging function f(.):\n$K_{tag}^q = \\{id_1 : k_1, id_2 : k_2, ..., id_n : k_n\\}$.                                                                                                   (4)\nIn the later sections of this paper, we assume the $k^q$ always carries its identifier while performing computing ($K_q \\rightarrow K_{tag}^q$)."}, {"title": "4.1.2 Self-answering Module. To provide answers for the knowledge points $K_q = \\{k_1,k_2, ..., k_n\\}$ elicited in the previous module, we design a self-answering module to generate precise answers. For each sub-question $k_i \u2208 K^q$, the model M generates an answer $a_i$ using the following prompt $T_2$, ensuring coherence and sufficiency in addressing each of the knowledge points:", "content": "Template $T_2$: Given a sub-question {k}, provide a precise answer that directly addresses the query without further discussion.\nThe model generates answers A = {$a_1, a_2, . . ., a_n$}, where each answer $a_i = M(k_i, T_2)$. So we have:\nA = {$a_1, a_2,..., a_n$} = {$M\\{k_1, T_2\\}, M\\{k_2, T_2\\},..., M\\{k_n, T_2\\}$}                                                                           (5)"}, {"title": "4.1.3 Reasoning Topology Construction Module. To construct the reasoning topology graph $G_q = (V, E)$, a critical step would be to connect the (k, a) pairs in a structured format based on their logical dependencies. Since we are quantifying the uncertainty of LLM explanations, this connection should be determined by the model itself to explain. Therefore, in this module, we leverage the few-shot learning ability of LLMs and guide them in connecting the basis (k, a) pairs following their reasoning procedure. By sampling F amount of $e_F$ as few-shot examples from a demonstration set F and feeding them to the model, the LLM learns to depict the reasoning path in a structured way for this task\u00b9: $\\hat{D}_m = M(D_m, e_F)$, the transformation from $D_m$ to $\\hat{D}_m$ follows:\n$\\hat{D}_m = M(D_m, e_F) = \\{(a^{p_1}, k_1, a_1), (a^{p_2}, k_2, a_2), ..., (a^{p_n}, k_n, a_n)\\}$                                                                       (7)\nwhere each $a^{p_i}$ is an answer node that connects to the corresponding knowledge-answer pair ($k_i, a_i$).\nTo ensure that the reasoning path forms a structured yet flexible topology that adapts to the complexity of real-world cases, the specific ordering of $p_i$ is not predetermined and depends on the actual reasoning structure generated by the model. Then for better illustration, we switch the order in the tuple as below, by applying graph concepts, we have the first two as the 'node' positions and the last as the 'edge' position:\n$(a^{p_1}, k_1, a_1) \\Rightarrow (a^{p_1}, a_1, k_1 )$                                                                                                                  nodes  edge\nwhere the order of two nodes is defined by the reasoning LLM.\nNow we can write a basic reasoning step as:\n$Step_{ij} = [node_i, node_j, edge_{ij}]$                                                                                                           (8)\nwhere $node_i$ is the starting node representing either a question, a sub-question, or an intermediate response, $node_j$ is the resulting node from $node_i$, and connected by $edge_{ij}$, which serves as the reasoning operation or sub-question.\nSpecifically, for the initial input query $x_q$, we denote the node as $node_{Raw}$; for the final answer a, we denote as $node_{Result}$. All other steps in the middle are the reasoning process, with a clearly defined structure. The final graph structure includes all reasoning steps from query $x'$ to the final answer a as nodes ($v_i$) and their dependencies as edges ($e_{ij}$): The reasoning process from query q to the final answer a can be finalized as a directed graph structure\n$G_q = (V, E),$                                                                                                                                      (10)\nwhere\nV = {$node_{Raw}, node_1,..., node_{Result}$} = {$v_0, v_1, ...$                                                                                 (11)\nand the edges are expressed as:\nE = {$e_{ij}(1), e_{ij}(2)...$                                                                                                                      (12)\n\u00b9Please find details of few-shot learning in Appendix."}, {"title": "where e stands for edge and {$e_{ij}$ | $edge_{ij}$: $node_i$ \u2192 $node_j$}, $e_{ij}$ represents reasoning operations or dependencies between nodes. (start with index '1' since we assume '0' is the $node_{Raw}$). The graph-based structure captures the full reasoning topology, including branching, dependencies, and multi-step interactions, which allows for better reflection of the relationships between intermediate steps. Now from a graph concept, the reasoning steps combined with Eq. 9 are formalized as below:", "content": "S = {$Step_{ij}$ | $Step_{ij}$ = [$v_i, v_j, e_{ij}$]$v_i, v_j \u2208 V, e_j \u2208 E$}                                                                              (13)\nwhere each triplet represents a logical transition between reasoning steps. Note that for complex reasoning, the final answer does not necessarily rely on all of the reasoning steps. For example, when being asked about \"if currently is summer in Australia, what season is it in Canada?\", in the reasoning chain, some of the LLM might delve into 'what causes the season differences', which is redundant steps in a concise reasoning."}, {"title": "4.2 Topology-enabled Reasoning Quantification", "content": "Our framework enables multidimensional uncertainty analysis through structural and semantic examination of reasoning topologies. Given a query $x_q$, the model M will be asked L times for explanation elicitation, on which L reasoning topologies {$G_i$}$_{i=1}^L$ will be generated from the previous step. We can measure the consistencies of {$G_i$}$_{i=1}^L$, where both graph structure for reasoning topology, and the embeddings of node and edge sentences for semantics will be considered."}, {"title": "4.2.1 LLM Reasoning Uncertainty Based on Graph Edit Distance. We quantify structural uncertainty through comparative analysis of multiple reasoning topologies {$G_i$}$_{i=1}^L$ generated for the same query $x_q$. Traditional graph comparison method [3] focuses on matching 'sets' of node embeddings in a broad sense (e.g., across various structured data domains like social networks [17] and chemistry [13]), but we would expect to quantify based on the reasoning steps, which requires a more fine-grained design of distance measure. To tackle the above issue, we first use context-aware embeddings for semantic encoding, and then design a fine-grained, reasoning-step based Graph Edit Distance (GED). Specifically, we compare the reasoning structure by jointly considering semantic similarity and structural alignment in a three-step process:", "content": "Step1: Semantic Embedding. In order to measure semantic meanings of reasoning steps, for each graph G\u2208 {$G_i$}$_{i=1}^L$ we employ an embedding function L to encode the representation of nodes and edges in graph G = (V, E). Since each node v \u2208 V and edge e \u2208 & serves as a textual description, we can derive contextual embeddings:\n$h_v = L(v), h_e = L(e), \u2200v \u2208 V, e \u2208 E$                                                                                                (14)\nIn this paper, we use BERT as our embedding function L but other embeddings could also be used for different domain contexts, e.g, [35] for medical text. This step encodes the semantics of nodes and edges while preserving the logical structure of the reasoning process."}, {"title": "Step2: Reasoning Topology Distance. In our setting, we have L reasoning structures {$G_i$}$_{i=1}^L$ generated. To measure the pairwise", "content": "distance of two reasoning structures $G_1$ and $G_2$, inspired by the concept of graph edit distance [12], we use the minimum transformations required to align the two graphs to quantify their pairwise distance.\nA. Substitution Costs: For two corresponding nodes in different reasoning topology graphs $G_1 = (V_1, & _1)$ and $G_2 = (V_2, & _2)$ we define the semantic substitution cost based on $v_i \u2208 V_1$ and $e_k \u2208 & _1$:\nc(vi, vj, ek, em) =\n1 \u2212 cos(hvi, h), node substitute\n1 \u2212 cos(h, hm), edge substitute\n(15)\nwhere cosine similarity measures the semantic alignment of reasoning steps and elicitation questions, this can capture the difference from direct meaning: either given the similar elicitation (edge sub-question), the sub-response (nodes) are different - there might exist an incorrect answer, or different edges that lead to the similar response - there might be a jumping step.\nB. Deletion/Insertion Costs: The cost of deleting a node or an edge is computed based on its average similarity to other nodes or edges within the same reasoning topology G [2, 37]. For the two reasoning graphs, we compute the deletion cost for a node $v_i \u2208 V_1$ or an edge $e_k \u2208 & _1$ with respect to the graph $G_1$, as follows:\nc del (vi, vj, ek, em) =\n1 \u2212 1 \u2211 cos(h, h), node delete,\nvj\u2208V1 , vj\u2260vi\n1 1 \u2211 cos(h, hm), edge delete.\n|&1|-1 em\u2208&1 , em\u2260ek\n(16)\nwhere (.) is the same as in Eq. 15, cos(hvi, h) shows the semantic connectivity from node $v_i$ to other nodes in the topology, if the embedding meaning is closer to other nodes, the value is larger, and then the value is normalized by the number of remaining nodes |$V_1$|-1, and subtracted from 1 to compute the deletion cost. Highly similar nodes or edges in $G_1$ (e.g., redundant sub-questions) will have lower deletion costs, as their removal minimally impacts the reasoning flow. Conversely, unique or critical nodes and edges (e.g., important conclusions or key transitions) will incur higher deletion costs due to their significant role in maintaining reasoning integrity and structural coherence.\nStep3: Graph Distance for Reasoning Uncertainty. Based on the above two steps, we can derive the overall graph edit cost in joint consideration of semantic meaning and topology variance as:\nGED(G1, G2) = csub (P) + c del (V1, &1, P)                                                                                    (17)\nwhere P represents the optimal matchings for sub-questions (edges Pe) and sub-responses (nodes Pv), computed using the Hungarian algorithm [29]. The term csub (P) accounts for the total substitution costs over the two graphs, and cdel (V1, &1, P) captures the total deletion costs for nodes and edges over the two graphs, details are explained in the Appendix. E.1. So we can calculate the minimal total cost of transformations by finding:\nGEDm(G1, G2) = min GED(G1, G2)                                                                                                           (18)\nP\nA higher GED implies a higher difference in the reasoning phase by considering both embedding and structures."}, {"title": "We use this computed reasoning distance to construct a distance matrix across multiple reasoning structures. Given a set of reasoning topologies {$G_1, G_2, ..., G_n$}, we compute pairwise distances using the GED-based similarity measure: $d_{ij} = GED_m(G_i, G_j)$, which then forms the overall distance matrix between k reasoning topologies", "content": "$D_{G_n} = [d_{ij}] = [GED_m(G_i, G_j)]_{n\u00d7n}$                                                                                                        (19)\nwhere each entry $d_{ij}$ quantifies the structural and semantic difference between the reasoning processes in $G_i$ and $G_j$. Here, we now can resort to the UQ measure U(.) to the variance of the distances in $D_{G_n}$, which reflects the inconsistency or stability of the model's reasoning behavior. Combining Eq. 3, we have the uncertainty score over a query $x_q$ as:\n$U_{struct}(x_q) = Var(D_{G_n}),$                                                                                                             (20)\nwhere $D_{G_n}$ is the pairwise distance matrix over the set of reasoning topologies {$G_1, G_2, . . ., G_n$}. The function U(\u00b7) = Var(\u00b7) computes the overall variance of all pairwise distances. A higher variance indicates greater inconsistency in the model's reasoning, suggesting that the LLM generates significantly different structures across multiple responses to the same query."}, {"title": "4.2.2 LLM Reasoning Redundancy Measure. It is known that the LLM's reasoning efficiency varies based on the problem type and model weights [33], and the reasoning topology provides a good reference to understand the efficiency by analyzing the detailed steps. We find that LLMs do not necessarily rely on all of the nodes from its reasoning topology for the final conclusion drawing, which means that, some of the sub-steps do not contribute to solving a problem and it causes the efficiency decrease. Here, we propose a way of measurement named 'Reasoning Redundancy'. Reflect the reasoning steps in Eq. 13: S = {[vi, vj, lij] | vi, vj \u2208 V, eij \u2208 &}, we aim to measure the redundancy based on the valid path constructed by the steps.", "content": "DEFINITION 2 (REDUNDANT NODE). A node $v_k \u2208 V$ is redundant if it does not contribute to the reasoning path from $node_{Raw}$ to $node_{Result}$. Formally, a redundant node satisfies:\n$v_k \\ngeq \\cup \\{v_i, v_j\\}, [v_{i,j,e_{ij}}] \u2208 P_{valid}$                                                                                       where $P_{valid}$ represents the set of all valid paths contributing to the final conclusion.\nWe have designed detailed criteria for efficient searching as shown in Appendix E.2, then we perform the searching for $P_{valid}$ valid paths and 'Redundancy Rate' using traversal algorithm (DFS), then we have the redundancy rate of the reasoning process for a as:\nrredun(a) =  |\n(21)\nwhere the |V| is the total number of nodes in the reasoning topology, and $V_{redundant}$ is the number of redundant nodes."}, {"title": "4.2.3 A Recipe in Practice. The above-mentioned quantification methods have different properties in potential usage. The Topology-UQ is mainly proposed to evaluate the trustworthiness of LLM responses, make pair-wise reasoning path comparisons, or conduct", "content": "completeness checks like what is strengthened in the AI4education domain [32], where the teaching is focusing on the logic steps rather than final answers. The reasoning Redundancy measure can be used to conduct a reasoning-efficiency check. It helps to identify the non-necessary discussions for a problem and guide the LLM for improvement by probing and marking a more concise but correct path because high-quality answering should not only focus on correctness but also involve solving efficiency."}, {"title": "5 Experimental Study", "content": "In this section, we conducted extensive experiments covering 5 LLMs: GPT40-mini, DeepSeek-R1 (distilled version on llama3-70b), Phi4, Llama3-8b, Llama3-70b (v3.3), 3 challenging datasets and using a total of 5 methods. We use the proposed method for topology-based LLM explanation analysis, then we try to answer the following research questions:\n\u2022 RQ11: Can the proposed method reveal the actual uncertainty in LLM's reasoning?\n\u2022 RQ2: How do LLMs perform in the proposed redundancy measure?\n\u2022 RQ3: From the proposed topology reasoning elicitation, do LLMs share certain patterns commonly?"}, {"title": "5.1 Experiment Settings", "content": "In this section, we introduce the experiment settings including the dataset adopted for analysis and baselines used for comparison. Besides, we also introduce the evaluation metrics for measuring the UQ methods' performance on Natural Language Explanation tasks.\n5.1.1 Dataset. In this study, to align with the research community [30, 41, 46], we utilized widely adopted datasets, including GSM8k [8] and BoolQ [7], which require complex reasoning rather than simple question-answering. These benchmarks assess LLM's ability to perform multi-step inference and logical reasoning. Besides, we also develop a new dataset, GeoQA, especially for condition-based reasoning tasks. We will explain the details of the GeoQA dataset in Appendix B and provide brief introduction of all datasets in Appendix F.1\n5.1.2 Baselines. To the best of our knowledge, there are only few works focusing on the uncertainty quantification of NLE, thus, we not only included the existing method (1) Chain-of-Thought Agreement (CoTA) [39], but also analyze (2) Embedding distance-based UQ for NLE (Embed-UQ), (3) Entailment probability-based (Entail-UQ), and (4) NLI-logit based UQ, as our extra baselines to understand their interpretability of LLM explanations. We provide a brief introduction here, and for detailed explanation of baselines is in Appendix. D."}, {"title": "5.1.3 Evaluation and Metrics. To evaluate the performance of uncertainty quantification methods in LLM explanation tasks, we follow the standard practice that compares uncertainty results with actual faithfulness [39]. The ground truth faithfulness score reveals how much the model relies on its complete reasoning process, which is calculated through a strategy named 'Early Answering' as proposed by [22], we provide details on how the faithfulness score is derived in Appendix F. Ideally, a UQ method is good if, for a higher faithful set, it generates lower uncertainty, and vise versa [39]. Hence, we employ three robust statistical metrics to quantify the correlation between the derived uncertainty and faithfulness. First, we use the commonly adopted metric - Pearson Correlation Coefficient (PCC), which is to measure the linear correlation between two variables. And given the relative small amount of each bootstrap sample, we employ two extra metrics Spearman Rank Correlation (SRC) and Kendall Rank Correlation (KR), the calculation of metrics is also in Appendix.\nFor fair evaluation and to avoid bias in single answers, we conduct bootstrap for a given dataset Dtest and measure the correlation in each sub-set D' level between uncertainty with the same level of faithfulness score. The sub-set is cut as 20 questions with 10 responses for each question = 200 a and bootstrap is conducted 1000 times on each dataset.", "content": null}, {"title": "5.2 Quantitative Evaluation (RQ1)", "content": "In order to understand how our proposed method works in the reasoning uncertainty measure tasks for LLM explanations, we perform experiments on GSM8K, BoolQ, and GeoQa datasets. Due to the page limit, GSM8K, and BoolQ are shown in the Table. 2, our method reveals a stronger negative correlation between the derived UQ results and the groundtruth faithfulness across different statistic metrics, the results on GPT40-mini, Llama3-70b, and DeepSeek-R1 are more convincing because they have a more stable performance on the Topology elicitation task as in Figure. 5, which, is a key step for the proposed UQ method, the performance on Phi4 and Llama3-8b not promising as ranked in the last two positions in Appendix Figure. 5. This research question result shows that our method is effective in revealing the LLM's real faithfulness, yet it is more suitable for LLMs with good instruct-following abilities."}, {"title": "5.3 Redundancy Measure of LLMs (RQ2)", "content": "Benefit from the topology structure, we are able to extract the reasoning path $P_{valid}$ that successfully connects from $node_{Raw}$ to $node_{Result}$. Then we can effectively compute the node that is not contributing to the final answer, and this serves as a sign of redundancy in the LLM's reasoning process. Following the Eq. 21, we analyze the redundancy rate for each of the LLMs including GPT-40-mini, Llama3-8b"}]}