{"title": "CrowdMAC: Masked Crowd Density Completion for Robust Crowd Density Forecasting", "authors": ["Ryo Fujii", "Ryo Hachiuma", "Hideo Saito"], "abstract": "A crowd density forecasting task aims to predict how the crowd density map will change in the future from observed past crowd density maps. However, the past crowd density maps are often incomplete due to the miss-detection of pedestrians, and it is crucial to develop a robust crowd density forecasting model against the miss-detection. This paper presents a MAsked crowd density Completion framework for crowd density forecasting (CrowdMAC), which is simultaneously trained to forecast future crowd density maps from partially masked past crowd density maps (i.e., forecasting maps from past maps with miss-detection) while reconstructing the masked observation maps (i.e., imputing past maps with miss-detection). Additionally, we propose Temporal-Density-aware Masking (TDM), which non-uniformly masks tokens in the observed crowd density map, considering the sparsity of the crowd density maps and the informativeness of the subsequent frames for the forecasting task. Moreover, we introduce multi-task masking to enhance training efficiency. In the experiments, CrowdMAC achieves state-of-the-art performance on seven large-scale datasets, including SDD, ETH-UCY, inD, JRDB, VSCrowd, FDST, and croHD. We also demonstrate the robustness of the proposed method against both synthetic and realistic miss-detections.", "sections": [{"title": "1. Introduction", "content": "Accurately forecasting a crowd's movement in a video is crucial for various applications, such as advancing navigation systems to select an optimal route for agents (robots) and effectively minimizing the risk of collisions [2, 25]. A crowd's movement can be naively forecasted by applying the well-studied trajectory prediction framework [1, 14, 27, 42]. Despite significant attention and numerous proposed works on the trajectory prediction task, most existing works [1, 14, 27, 42] rely on the critical assumption that each trajectory is successfully obtained by two preprocessing (i.e., upstream perception) modules, pedestrian detection [5, 24, 37, 38] and tracking [3, 6,51,55]. However, this assumption is not feasible for real-world scenarios since the complete trajectory for each pedestrian is not always available due to the miss-detection of pedestrians or incorrect tracking. Using estimated trajectories as the input for existing trajectory predictors could lead to significant accumulated errors and pose serious threats to safety [53].\nCrowd density forecasting task [34], which directly forecasts future crowd density maps from the past crowd density maps that are estimated from either pedestrian detection [5,24,37,38] or crowd density estimation [12,22,50,56] modules, offers the potential as an alternative approach for forecasting the movements of pedestrians in real-world robotic applications, even though the movement of each pedestrian is not forecasted. However, the accuracy of the crowd density forecasting task still highly relies on the obtained past crowd density maps. Thus, it is crucial to develop a robust crowd density forecasting model against the error accumulated by the upstream crowd density estimation or pedestrian detection modules. (i.e., miss-detection).\nThe masked autoencoders (MAE) [16] has garnered significant attention due to its recent achievements in image-based self-supervised learning. This approach involves masking a portion of patches in the input data and reconstructing the missing patches using an autoencoder structure. Inspired by the remarkable capabilities of MAE in reconstructing masked patches (interpolation) of spatiotemporal data, we explore its potential for forecasting future patches (extrapolation) in the crowd density forecasting task. This paper presents CrowdMAC, which forecasts future crowd maps from observed crowd maps. Unlike MAE, which masks a subset of patches during training, we mask frames (entire patches) in the future density maps, and the model is trained to reconstruct the masked future frames.\nAs previously mentioned, to robustly forecast the crowd density map with the miss-detection of the pedestrians, the patches in the input past observed crowd density maps are masked. During training, the model jointly reconstructs the masked patches in past observed maps and predicts the"}, {"title": "2. Related Work", "content": "Human trajectory forecasting aims to infer plausible future positions from observed trajectories. Deep learning-based trajectory prediction approaches [1, 11, 13, 14, 14, 17, 18, 18, 27-29, 35, 41, 42, 52] have become dominant due to their impressive representational ability. Some studies focus on better modeling interactions with other agents, such as pedestrians [1, 14, 35, 52] and the environment [18, 27] and some works [13, 14, 17, 18, 28, 29, 41, 42] aim to model the multi-modality of future motions.\nHowever, despite the increasing accuracy of trajectory prediction methods, they work on trajectory prediction in an idealized setting where ground truth past trajectories are used as inputs for both training and evaluation without considering upstream perception errors (i.e., pedestrian detection and tracking error). However, this idealized setting is not feasible in the real world since the trajectories that contain errors estimated from the upstream modules are inputted to the trajectory forecasting approaches."}, {"title": "2.2. Crowd Density Forecasting", "content": "Crowd density forecasting aims to forecast crowd density maps of future frames. PDFN-ST [34], the patch-based crowd density forecasting model, is pioneering work in the field of crowd density forecasting. It models the diverse and complex crowd density dynamics of the scene based on spatially or spatiotemporally overlapping patches. Crowd density forecasting directly forecasts future crowd density maps from past crowd density maps. The past crowd density maps are estimated either from upstream crowd density estimation or the object detection modules. Thus, similar to the trajectory prediction method, the crowd density forecasting method highly depends on the accuracy of the upstream module. Therefore, developing a robust crowd density forecasting model against errors accumulated by the upstream modules, such as object detection, is of critical importance and serves as the motivation for this work. Crowd"}, {"title": "2.3. Masked Autoencoders (MAE)", "content": "Masked autoencoders [10, 16,49] are a type of denoising autoencoder [48] that obtain meaningful representations by learning to reconstruct images corrupted by masking. In particular, the masked autoencoder (MAE) [16] approach accelerates pre-training to learn representations from images by using an asymmetric architecture. VideoMAE [46] directly extends MAE and efficiently reconstructs the pixels of masked video patches with a high masking ratio through tube masking. Our approach leverages the efficiency of the masked autoencoder approach and extends it to the crowd density forecasting task. The original masked autoencoder approaches aim to obtain powerful pretrained models to improve the accuracy of video-related downstream tasks. On the other hand, we focus on the interpolation capabilities of the masked autoencoder itself and apply it to the forecasting task, which is robust to missing inputs (i.e., miss-detection).\nMoreover, rather than randomly masking tokens in input images or videos, as commonly done in MAE or Video-MAE, recent custom masking approaches have been proposed for various inputs such as images [20], videos [44], 3D LiDAR point clouds [33], or sequential skeletons [30]. These approaches non-uniformly sample tokens based on the informativeness of each token. SemMAE [20] utilizes a masking approach based on semantically segmented parts of objects in images to obtain a powerful image representation. The informativeness-based masking strategy enhances the performance of downstream tasks, such as image recognition or video recognition. Inspired by these approaches, we propose a temporal-density-aware masking strategy to mask informative tokens in the input crowd density maps during training for the crowd density forecasting task."}, {"title": "3. CrowdMAC", "content": "The goal of crowd density forecasting is to predict future crowd density maps over $T_{pred}$ timesteps (i.e., future frames), $C_{pred} = [C_{T_{obs}+1},...,C_{T_{obs}+T_{pred}}]$ from the observed past crowd density maps over $T_{obs}$ timesteps (i.e., past frames), $C_{obs} = [C_1,...,C_{T_{obs}}]$. $c_t \\in [0,1]^{W \\times H}$ represents the crowd density map at time step $t$ with a size of $(W, H)$. The observed crowd density maps $C_{obs}$ are extracted from each input RGB video frame $I_t$ using the off-the-shelf pedestrian detection [5, 15, 24] or crowd density estimation [12, 50] approaches."}, {"title": "3.2. Overview", "content": "CrowdMAC takes crowd density maps $C \\in \\mathbb{R}^{T \\times H \\times W}$ as input, comprising of observed crowd density maps $C_{obs} \\in \\mathbb{R}^{T_{obs} \\times H \\times W}$ and future crowd density maps $C_{pred} \\in \\mathbb{R}^{T_{pred} \\times H \\times W}$, where $T = T_{obs} + T_{pred}$. It employs the space-time cube embedding [46] to transform the input maps into a set of token embeddings $X \\in \\mathbb{R}^{F \\times N_s}$, where $F$ is the channel dimension of the tokens,"}, {"title": "3.3. Multi-Task Masking", "content": "Instead of training with a single future prediction task, we train the model on three tasks: future prediction, past prediction, and interpolation. At each training step, we sample one of the tasks. This multi-task training enables the model to establish a robust bidirectional relationship between past and future motion. Additionally, multi-task learning serves as a form of regularization, mitigating the risk of overfitting. This is achieved by training the model to perform well across multiple objectives, resulting in more robust and generalized learning.\nFor future prediction and past prediction tasks, the proposed temporal-density-aware masking (TDM) and frame masking are utilized to select tokens to be masked. For the interpolation task, the TDM is utilized to select tokens to be masked."}, {"title": "3.4. Temporal-Density-aware Masking (TDM)", "content": "Instead of uniformly masking tokens in frames via random sampling with a fixed masking ratio, like MAE or VideoMAE, during the training, we propose Temporal-Density-aware Masking (TDM) that randomly masks informative tokens. TDM consists of two modules: temporal-aware masking (TM) ratio and density-aware masking (DM). TM calculates the number of masked tokens for each time step from the tokens' time index, and the sampling probability of each token in each time step is calculated from DM based on the accumulated density value of each token. The details of each module are explained in the following sections."}, {"title": "3.4.1 Temporal-aware Masking (TM) Ratio", "content": "For the future prediction task and interpolation, we assume the latter frame is more informative than the first frame. Therefore, instead of sampling tokens randomly with a fixed masking ratio $\\gamma$ that computes the number of the tokens to be masked, the proposed TM ratio $\\gamma(t)$ is a monotonically increasing function of the tokens' time index of the observation tokens $t \\in \\{1,...,T_{max}\\}$, where $T_{max} = T_{obs} / T_c$ for future prediction task and $T_{max} = T/T_c$ for interpolation task. By designing $\\gamma(t)$ to increase along with the time index, the model is trained to reconstruct more informative tokens in the subsequent frames. Concretely, we design the $\\gamma(t)$ as follows:\n$\\gamma(t) = 1 - e^{(-x t / T_{max})}$,\nwhere $\\lambda$ is sampled from the uniform distribution $U[0, \\Lambda_{max}]$ every epoch with hyper-parameter $\\Lambda_{max}$.\nFor the past prediction task, we assume the earlier frame is more informative than the last frame. The TM ratio $\\gamma(t)$ is a monotonically decreasing function of the tokens' time index of the future tokens $t \\in \\{1,...,T_{max}\\}$, where $T_{max} = T_{pred} / T_c$. We design the $\\gamma(t)$ as follows:\n$\\gamma(t) = 1 - e^{(-x (T_{max} - t) / T_{max})}$."}, {"title": "3.4.2 Density-aware Masking (DM)", "content": "In contrast to RGB images or videos, crowd density maps are often sparse, meaning that pedestrians do not exist in all the pixels in the maps, and most pixel values are empirically 0. Therefore, we propose to non-uniformly sample"}, {"title": "4. Experiments", "content": "We use a total of seven datasets to investigate the performance of CrowdMAC. Specifically, we conduct experiments on four datasets for trajectory prediction: ETH-UCY [19, 36], Stanford Drone Dataset (SDD) [39], Intersection Drone Dataset (InD) [4], and JackRabbot Dataset (JRDB) [31]. Furthermore, we evaluate our approach on three datasets for crowd analysis tasks (e.g., crowd detection, localization, and counting), including Fudan-ShanghaiTech (FDST) [9], Crowd of Heads Dataset (CroHD) [45], and surveillance-view Video Crowd dataset (VSCrowd) [21]. As the croHD dataset only provided detection results from the detector for the test set, we use the detection results for training and evaluation."}, {"title": "4.2. Implementation Details", "content": "We employ the the ViT-Small [8] backbone. Following prior work [34], we use the input size of 80 \u00d7 80 for 20 frames which consists of 8 past frames ($T_{obs}$) and 12 future frames ($T_{pred}$) in Sec. 3.1. We set the space-time cube size for the embedding along the spatial dimensions as $W_c = 8$, $H_c = 8$, and along the temporal dimension as $T_c = 4$. We employ the mean squared error (MSE) loss between the masked pixels and the reconstructed pixels for training. We train our model with AdamW optimizer [26] with a base learning rate 5e-4 and weight decay 1e-5 for 1200 epochs. We train the model from scratch for SDD and"}, {"title": "4.3. Evaluation Metrics", "content": "Followed by prior work [34], we use Jensen-Shannon (JS) divergence to measure the performance of the forecasting:\n$D_{JS}(\\hat{C}_t || C_t) = \\frac{1}{2} (D_{KL}(\\hat{c}_t || \\bar{c}_t) + D_{KL}(\\bar{c}_t || \\hat{c}_t))$,\nwhere $\\hat{c}_t = \\hat{C}_t / \\sum_{i,j} \\hat{C}_t(i, j)$, $C_t = C_t / \\sum_{i,j} C_t(i, j)$ are the predicted and ground truth normalized density maps, $i, j$ are the indices of pixel position, and $D_{KL}$ is Kullback-Leibler (KL) divergence:\n$D_{KL}(\\hat{C}_t || C_t) = \\frac{1}{WH} \\sum_{i,j} \\hat{c}_t(i, j) log(\\frac{\\hat{c}_t(i, j)}{C_t(i, j)})$.\nWe report the Average JS divergence (ADJs) and the Final JS divergence (FDJs). ADJs is the divergence between the predicted and the ground truth map averaged over all the future time steps, while FDJs is the divergence between the predicted and ground truth map at the final time step."}, {"title": "4.4. Comparison Models", "content": "Since only one previous work has tackled the crowd density forecasting task, in addition to the crowd density forecasting method [34], we apply standard and state-of-the-art trajectory forecasting approaches [27,40] to address the crowd density forecasting task following the previous method [34]. We employ the following methods for comparison:\nPDFN-ST [34]: PDFN-ST is a pioneering work that tackles the crowd density forecasting task by using 3D CNNs to learn local crowd density dynamics in 3D receptive fields, regarded as spatiotemporal patches.\nY-Net: Y-Net [27] is a heatmap-based model that predicts future human trajectories by estimating distributions over long-term goals and intermediate waypoints.\nSocial-Transmotion: Social-Transmotion [40] is a Transformer-based model for human trajectory prediction, leveraging diverse visual cues. The model is designed to predict human behavior by capturing spatiotemporal interactions between agents."}, {"title": "4.5. Evaluation Protocols", "content": "We evaluate the efficacy of our method under a common setting in the state-of-the-art trajectory prediction model, which involves observing trajectories over 8-time steps (equivalent to 3.2 seconds) and subsequently predicting future trajectories spanning the next 12 time steps (equivalent to 4.8 seconds), employing two distinct protocols.\nGround Truth Input. To evaluate the performance of crowd density forecasting models independent of the accuracy of the upstream crowd density estimation module, we conduct experiments using ground truth as input. We train the model with ground truth maps and evaluate the accuracy with them. Since the ground truth 2D positions of the pedestrians are annotated in the above trajectory prediction datasets, we generate ground truth crowd density maps from the pedestrians' positions. We apply a 2D Gaussian filter (with a standard deviation of 3 px) following the previous approach [34].\nUpstream Perception Modules Input. To validate the robustness of our method against errors (e.g. miss-detection of the pedestrians) in upstream perception modules, we also evaluate the forecasting accuracy in a realistic setting that uses the estimated results from the upstream perception module as inputs in the evaluation phase. We train the model on ground truth inputs and evaluate the accuracy of the crowd forecasting and trajectory prediction approaches using upstream perception module results. We use the detection and crowd density estimation for crowd density forecasting approaches and the detection&tracking for"}, {"title": "4.6. Forecasting Accuracy Comparison", "content": "Ground Truth Input. We compare our model with crowd density forecasting and trajectory prediction models using the ground truth input evaluation protocol.\nUpstream Perception Modules Input. We validate the robustness of our method against errors in upstream per-ception modules using detection results as input, as shown in Tab. 2, and crowd density estimation results as input, as shown in Tab. 3. Our CrowdMAC consistently outperforms both the crowd density forecasting models and the trajectory prediction models in both settings."}, {"title": "4.7. Robustness against Miss-detection", "content": "We explore the robustness of the crowd density forecasting models against the miss-detection of pedestrians with the synthetically-generated and realistic miss-detections.\nSynthetic Miss-Detection. We evaluate the robustness against synthetically generated miss-detections. We train the models with ground-truth input, and synthetic miss-detections are generated by randomly sampling pedestrians in the frames that will not be used as input during the evaluation.\nRealistic Miss-Detection. We further investigate the robustness against miss-detection using data preprocessed by the pedestrian detection module [38] on the FDST and VScrowd."}, {"title": "4.8. Ablation Studies", "content": "We perform two ablation studies on the proposed TDM module to assess its effectiveness. In this section, we use the ground truth input evaluation protocol as mentioned in Sec. 4.6.\nEffect of the Temporal-Density-aware Masking.\nEffect of multi-task masking. Tab. 5 shows the effect of multi-task masking. The model trained with a single future prediction task performs considerably worse compared to the model trained with the multi-task approach. In particular, the history prediction task significantly contributes to the improved performance.\nChoice of Temporal-aware Masking (TM) Ratio Function. We explore the design of the TM ratio function in Eq. (2)."}, {"title": "4.9. Qualitative Evaluation", "content": "Fig. 5 visualizes the qualitative results of the proposed method as well as the ground-truth density maps on the"}, {"title": "5. Conclusion", "content": "In summary, we propose CrowdMAC, a new learning framework for crowd density forecasting, which operates in a masked completion fashion. CrowdMAC is simultaneously trained to forecast future crowd density maps from masked past observation maps and reconstruct the masked observation maps to improve the robustness of the crowd density forecasting task against the miss-detection of pedestrians. We further propose Temporal-Density-aware Masking (TDM) that non-uniformly masks the patches in the observed crowd density maps, facilitating training and obtaining better forecasting performance. Moreover, we introduce multi-task masking to enhance the efficiency in training. Empirical results show that our method outperforms the state-of-the-art crowd density forecasting methods, as well as trajectory prediction approaches, and achieves robustness against synthetically generated or realistically occurring miss-detections."}]}