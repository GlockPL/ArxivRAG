{"title": "Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model", "authors": ["Yosuke Yamagishi", "Yuta Nakamura", "Tomohiro Kikuchi", "Yuki Sonoda", "Hiroshi Hirakawa", "Shintaro Kano", "Satoshi Nakamura", "Shouhei Hanaoka", "Takeharu Yoshikawa", "Osamu Abe"], "abstract": "Background: Recent advances in large language models have highlighted the need for high-quality multilingual medical datasets. While Japan leads globally in Computed Tomography (CT) scanner deployment and utilization, the absence of large-scale Japanese radiology datasets has hindered the development of specialized language models for medical imaging analysis. Despite the emergence of multilingual models and language-specific adaptations, the development of Japanese-specific medical language models has been constrained by the lack of comprehensive datasets, particularly in radiology.\nObjective: To develop a comprehensive Japanese CT report dataset through machine translation and establish a specialized language model for structured finding classification, addressing the critical gap in Japanese medical natural language processing resources. Additionally, to create a rigorously validated evaluation dataset through expert radiologist review, ensuring reliable assessment of model performance.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304 patients) into Japanese using GPT-4o mini. The training dataset consisted of 22,778 machine-translated reports, while the validation dataset included 150 reports that were carefully revised by radiologists. We developed CT-BERT-JPN (Japanese), a specialized BERT model for extracting 18 structured findings from Japanese radiology reports, based on the \"tohoku-nlp/bert-base-japanese-v3\" architecture. Translated radiology reports were assessed using BLEU and ROUGE scores, complemented by expert radiologist review. Model performance was evaluated using standard metrics including accuracy, precision, recall, F1 score, and Area Under the Curve - Receiver Operating Characteristic, with GPT-40 serving as a baseline.\nResults: Translation metrics showed preservation of general text structure, with BLEU scores of 0.731 and 0.690, and ROUGE scores ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression sections, while expert review revealed necessary refinements in medical terminology. These modifications fell into three categories: contextual refinement of technical terms, completion of incomplete translations, and Japanese localization of medical terminology, highlighting the importance of expert validation in medical translation. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in 11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular septal thickening (+10.9%), and atelectasis (+7.4%). The model achieved perfect scores across all metrics in four conditions (cardiomegaly, hiatal hernia, atelectasis, and interlobular septal thickening) and maintained F1 score exceeding 0.95 in 14 out of 18 conditions. Performance remained robust despite varying numbers of positive samples across conditions (ranging from 7 to 82 cases).\nConclusions: Our study establishes a robust Japanese CT report dataset and demonstrates the effectiveness of a specialized language model for structured finding classification. The hybrid approach of machine translation and expert validation enables the creation of large-scale medical datasets while maintaining high quality standards. This work provides essential resources for advancing medical AI research in Japanese healthcare settings, with both the dataset and model publicly available for research purposes to facilitate further advancement in the field.", "sections": [{"title": "Introduction", "content": "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across various domains [1], leading to increased focus on developing multilingual models to serve diverse linguistic communities [2] . This trend is exemplified by the release of specialized language models such as Gemma-2-JPN, a Japanese-specific variant of Google's open LLM Gemma [3,4]. However, the development of such specialized models critically depends on the availability of high-quality, domain-specific datasets in target languages. This requirement becomes particularly crucial in specialized fields like medical imaging [5\u20137], where the interpretation of diagnostic findings demands both technical precision and linguistic accuracy.\nComputed tomography (CT) scans play an indispensable role in modern medical diagnostics, facilitating disease staging, lesion evaluation, and early detection. Japan leads the world with the highest number of CT scanners per capita and an annual scan volume that surpasses most developed nations, presenting a vast reservoir of medical imaging data [8,9]. This extensive utilization of CT technology positions Japan as a pivotal contributor to global medical imaging resources. However, despite the proliferation of multilingual models and the growing emphasis on language-specific adaptations, there remains a notable absence of large-scale Japanese radiology report datasets [10] a critical gap that hinders the development of Japanese-specific medical language models.\nTo address this challenge, we have constructed \"CT-RATE-JPN,\" a Japanese version of the extensive \"CT-RATE\" dataset [11], which consists of CT scans and interpretation"}, {"title": "Methods", "content": "Dataset Overview\nCT-RATE is a comprehensive dataset comprising 25,692 non-contrast chest CT volumes from 21,304 unique patients at Istanbul Medipol University Mega Hospital [11]. We selected this dataset as it is uniquely positioned as the only publicly available large-scale dataset that pairs CT volumes with radiology reports while permitting redistribution of derivative works. This dataset includes corresponding radiology text reports (consisting of a detailed findings section documenting observations and a concise impression section summarizing key information), multi-abnormality labels, and metadata. The dataset is divided into two cohorts: 20,000 patients for the training set and 1,304 patients for the validation dataset, allowing for robust model training and evaluation across diverse patient cases [18,19]. The training dataset, comprising 22,778 unique reports, was utilized for the construction of CT-RATE-JPN, the Japanese-translated version of the dataset, which was created using machine translation. For the validation dataset (n = 150), a randomly selected subset underwent machine translation, followed by a labor-intensive process of manual revision and refinement conducted by radiologists. The data selection process is described in Figure 1.\nThe CT-RATE dataset is annotated with 18 structured labels, covering key findings relevant to chest CT analysis. These labels include Medical material, Arterial wall calcification, Cardiomegaly, Pericardial effusion, Coronary artery wall calcification, Hiatal hernia, Lymphadenopathy, Emphysema, Atelectasis, Lung nodule, Lung opacity, Pulmonary fibrotic sequela, Pleural effusion, Mosaic attenuation pattern, Peribronchial thickening, Consolidation, Bronchiectasis, and Interlobular septal thickening. The creators of the CT-RATE dataset developed a structured findings model based on the RadBERT architecture [20,21], trained on the manually labeled subset to label the remaining cases. This model achieved an F1 score ranging from 0.95 to 1.00, demonstrating its efficacy in accurately structuring these radiological findings from CT reports. This approach underscores the reliability of CT-RATE's structured annotations for high-performance diagnostic model development.\nWe also utilized these structured labels in the development of a Japanese structured findings model for CT-RATE-JPN, enabling accurate structuring of radiological findings in Japanese CT reports.\nGiven that CT-RATE is a publicly available dataset with de-identified patient information, and our study focused on the translation and linguistic analysis of the existing dataset without accessing any additional patient data, institutional review board approval was not required for this research."}, {"title": "Translation for CT-RATE-JPN", "content": "For CT-RATE-JPN, we applied machine translation using GPT-4o mini (API version, \"gpt-4o-mini-2024-07-18\") [22], a lightweight, fast version of OpenAI's GPT-4o model [23]. GPT-4o mini is known for producing high-accuracy translations at an affordable rate, making it suitable for large-scale dataset translation. Each radiology report was processed by separately translating its findings and short impression sections. The complete translation prompts used for GPT-40 mini are provided in Supplementary"}, {"title": "Development of CT-BERT-JPN for Structured Finding Classification", "content": "For model training, we randomly split the training dataset into a 4:1 ratio, designating 80% for training and 20% for internal evaluation. We obtained the pretrained \u201ctohoku-nlp/bert-base-japanese-v3\" model from Hugging Face [24]. This model follows the architecture of the original BERT base model with 12 layers, 768 hidden dimensions, and 12 attention heads. It was pretrained on extensive Japanese datasets, including the Japanese portion of the CC-100 corpus [25,26] and the Japanese version of Wikipedia [27]. Notably, BERT-based models have demonstrated significant success in downstream tasks in the medical domain [28,29], making them a promising choice for our research.\nTraining was conducted using the transformers library (version 4.46.2) with a learning rate of 2e-5, a batch size of 8 for both training and evaluation, and a weight decay of 0.01. Binary Cross Entropy loss was applied to optimize the model for multi-label classification. The model was trained over four epochs, with internal evaluation and checkpoint saving at each epoch. The best-performing model on the internal evaluation data was selected and subsequently used for testing, which was conducted using the validation dataset of CT-RATE-JPN to ensure reliable performance assessment. The overall workflow for developing CT-BERT-JPN is illustrated in Figure 3."}, {"title": "Translated Radiology Reports Evaluation", "content": "For basic text analysis, we examined the structural characteristics of the translated reports, including character count, word count, sentence count, and lexical diversity. Since Japanese text, unlike English, does not use spaces to delimit words, we employed MeCab (version 1.0.10) [30], one of the most widely used morphological analyzers for Japanese text processing, to accurately segment and count words. These metrics were calculated for both machine-translated and radiologist-refined texts to assess the consistency of textual characteristics across different stages of dataset creation.\nFor translation quality assessment, we computed Bilingual Evaluation Understudy (BLEU) [31] and Recall-Oriented Understudy for Gisting Evaluation (ROUGE)-1, ROUGE-2, and ROUGE-L scores [32] using the nltk (version 3.9.1) and rouge-score (version 0.1.2) libraries.\nBLEU is a metric that evaluates the accuracy of machine-translated text by comparing it to a reference translation. It measures how many words and short phrases in the machine translation match those in the reference translation, assessing the degree of similarity in wording and phrasing between the two texts.\nROUGE is a set of metrics used to assess the quality of summaries or translations by measuring the overlap between the machine-generated text and the reference text. ROUGE-1 considers the overlap of individual words, ROUGE-2 examines the overlap of pairs of consecutive words, and ROUGE-L focuses on the longest matching sequence of words between the two texts. These metrics emphasize recall, evaluating how much of the important content from the reference text is captured in the machine-generated text. These metrics were calculated by comparing the machine-translated texts against radiologist-revised reference translations in the validation dataset."}, {"title": "CT-BERT-JPN Performance Evaluation", "content": "For evaluating classification model performance for CT findings extraction, we utilized a test dataset comprising 150 radiology reports that had been revised by radiologists to ensure accuracy. Key metrics calculated for model assessment included accuracy, precision, recall, F1 score, and Area Under the Curve - Receiver Operating Characteristic (AUC-ROC). To establish a baseline, we performed structured labeling using GPT-40 (API version, \u201cgpt-4o-2024-11-20\u201d), which was selected due to its widespread adoption in various radiology tasks and its status as a representative closed-source commercial LLM. The input prompts used for GPT-4o are presented in Supplementary Figure 3 (original Japanese version) and Supplementary Figure 4 (English translation). These analyses were carried out using the scikit-learn (version 1.5.2) library."}, {"title": "Results", "content": "Dataset Overview\nThe basic text statistics of the translated reports are summarized in Table 1, analyzing both Findings and Impression sections separately. The training dataset (n = 22,778) and validation dataset (consisting of 150 machine-translated reports and 150 radiologist-refined reports) showed consistent text structure across all metrics.\nThe Findings sections had character counts averaging around 455.6-475.0 characters, with slightly lower counts in radiologist-refined texts compared to machine translations. Word counts followed a similar pattern, averaging approximately 300 words per report across all datasets. The Impression sections were notably more concise, as expected for summary statements. Character counts averaged around 89.1-101.3 characters, with word counts of approximately 55.7-63.2 words per report. The sentence structure was also more condensed, with about 3.1-3.6 sentences per report.\nNotably, in both sections, the overall text structure remained consistent between machine-translated and radiologist-refined versions, with similar patterns in sentence length and organization. While the refined versions showed slightly lower character and word counts compared to their machine-translated counterparts, the basic structural characteristics of the reports were preserved throughout the translation and refinement process.\nThe analysis of label distributions reveals a significant class imbalance in both training and validation dataset, as shown in Figure 4. In the training set, \"Lung nodule\" appears most frequently with 10,856 instances, while \"Interlobular septal thickening\" occurs least frequently with only 1,702 instances, representing a ratio of approximately 6.4:1. This imbalance is even more pronounced in the validation dataset, where the ratio between the most frequent (82 instances) and least frequent (7 instances) classes reaches about 11.7:1."}, {"title": "Translated Radiology Reports Evaluation", "content": "We evaluated the quality of machine-translated reports in CT-RATE-JPN through both automated metrics and expert assessment. For automated evaluation, we compared GPT-40 mini translations with radiologist-revised references in the validation dataset. The evaluation metrics for both sections are summarized in Table 2. These scores are all at high levels, demonstrating that the machine translation maintained the fundamental structure and meaning of the original reports.\nHowever, despite the high automated metric scores, qualitative analysis revealed that substantial revisions were necessary for medical terminology. The radiologists' assessment identified representative patterns of necessary modifications. This expert review revealed three major categories of improvements (Figure 5), which reflect typical challenges in medical translation: 1) contextual refinement of technically correct but unnatural terms, 2) completion of missing or incomplete translations, and 3) Japanese localization of untranslated English terms. While not exhaustive, these patterns represent key areas where human expertise complements machine translation in medical contexts. The first category, contextual refinement, primarily involved replacing technically accurate but clinically uncommon expressions with more natural medical terminology. For instance, direct translations of anatomical conditions were often revised to their proper radiological equivalents when describing vessel status, reflecting standard terminology in Japanese clinical practice. The second category addressed cases where certain medical terms were either missing or incompletely translated, requiring additional context-specific information. A typical example would be where anatomical descriptions lacked specific diagnostic terminology common in radiological reporting. The third category focused on proper localization of English medical terms that were initially left untranslated, such as converting technical descriptors of pathological findings into their appropriate Japanese radiological counterparts."}, {"title": "CT-BERT-JPN Performance Evaluation", "content": "Table 3 presents the performance evaluation results of CT-BERT-JPN across 18 different findings from 150 chest CT radiology reports. The model achieved perfect scores (1.000) across all evaluation metrics (accuracy, precision, recall, F1 score, and AUC-ROC) in three findings: Pericardial effusion, Hiatal hernia, and Mosaic attenuation pattern. The model demonstrated high accuracy exceeding 0.95 in 17 out of 18 findings, with AUC-ROC values surpassing 0.98 in all findings. Within the dataset, the number of positive samples varied considerably across conditions, from 7 cases (interlobular septal thickening) to 82 cases (lung nodule). Despite this imbalanced distribution of positive samples, the model maintained robust performance metrics across all conditions."}, {"title": "Clinical Implementation and Impact", "content": "The development of such a structured findings extraction model in Japan, where medical imaging utilization rates rank among the highest globally, contributes substantially to both domestic healthcare advancement and international model development. Our implementation incorporates previous work in Japanese radiology report processing, including end-to-end approaches for clinical information extraction [42], natural language processing systems for pulmonary nodule follow-up assessment [43], and BERT-based transfer learning methods for anatomical classification [44]. Our multi-label classification approach addresses several implementation challenges unique to the Japanese healthcare context. These include vocabulary standardization across institutions, integration with existing reporting systems, and the development of specialized Japanese medical vocabulary handling mechanisms. The model's ability to process Japanese-specific medical expressions and maintain high performance across different reporting styles demonstrates its potential for broad clinical application. However, prospective validation across multiple Japanese institutions remains essential for both performance evaluation and capturing institution-specific reporting patterns that inform targeted model improvements."}, {"title": "Limitations and Future Directions", "content": "Several limitations should be acknowledged in our study. First, while we demonstrated the effectiveness of our language model through rigorous evaluation, the utility of CT-RATE-JPN for vision-language models like CT-CLIP [18], which require joint learning of CT volumes and text descriptions, remains to be empirically validated. Second, our reliance on the CT-RATE dataset may introduce inherent biases in reporting styles and patterns, as radiology reports typically vary considerably across institutions and individual radiologists. Third, the translation-based approach, despite expert validation, may not fully capture the nuanced expressions and specialized terminology common in Japanese clinical practice.\nThese limitations suggest several promising directions for future work. A primary direction is the development of vision-language models using CT-RATE-JPN in conjunction with CT-RATE's CT volumes. Given the successful development of Japanese CLIP models for general domain tasks, which have demonstrated the feasibility of cross-lingual vision-language alignment in Japanese [45], extending this approach to medical imaging is particularly promising. While this endeavor requires substantial computational resources and sophisticated training strategies, various approaches can be explored, such as additional training on existing models like CT-CLIP. Furthermore, the construction of a new dataset comprising pairs of Japanese radiology reports and CT volumes from Japanese medical institutions would enable more direct assessment of model performance in the Japanese healthcare context and potentially reveal insights unique to this setting. Our benchmark dataset, validated by radiologists through a systematic review process, provides a valuable foundation for evaluating such Japanese-English and English-Japanese translation models in the radiology domain."}, {"title": "Conclusions", "content": "In this study, we introduced CT-RATE-JPN, a comprehensive Japanese dataset of CT interpretation reports, and developed a specialized language model for structured labeling. Our model demonstrated superior performance compared to GPT-40, achieving higher F1 scores in numerous categories of structured finding extraction. The creation of CT-RATE-JPN, along with our publicly available structured findings model, represents a significant contribution to Japanese medical imaging research. By making both the dataset and model freely accessible to the research community, we enable reproducibility and foster collaborative advancement in the field. This work not only provides essential resources for the medical AI community but also establishes a robust foundation for developing more sophisticated multilingual medical vision-language models. These openly available contributions will support the development of AI-assisted diagnostic tools while maintaining the high standards required for clinical applications in radiology."}]}