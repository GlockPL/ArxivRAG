{"title": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach", "authors": ["Yingdan Shi", "Ren Wang"], "abstract": "Machine unlearning seeks to systematically remove specified data from a trained model, effectively achieving a state as though the data had never been encountered during training. While metrics such as Unlearning Accuracy (UA) and Membership Inference Attack (MIA) provide a baseline for assessing unlearning performance, they fall short of evaluating the completeness and reliability of forgetting. This is because the ground truth labels remain potential candidates within the scope of uncertainty quantification, leaving gaps in the evaluation of true forgetting. In this paper, we identify critical limitations in existing unlearning metrics and propose enhanced evaluation metrics inspired by conformal prediction. Our metrics can effectively capture the extent to which ground truth labels are excluded from the prediction set. Furthermore, we observe that many existing machine unlearning methods do not achieve satisfactory forgetting performance when evaluated with our new metrics. To address this, we propose an unlearning framework that integrates conformal prediction insights into Carlini & Wagner adversarial attack loss. Extensive experiments on the image classification task demonstrate that our enhanced metrics offer deeper insights into unlearning effectiveness, and that our unlearning framework significantly improves the forgetting quality of unlearning methods.", "sections": [{"title": "1. Introduction", "content": "Machine unlearning has become crucial in advancing data privacy, especially under legal requirements like the General Data Protection Regulation (GDPR) (Bourtoule et al., 2021). These regulations emphasize the right for individuals to have their data removed or forgotten, creating a demand for machine unlearning methods that can enable machine learning models to behave as if specific forget data were never used in training stage. The existing post hoc machine unlearning methods can be categorized into training-based Illinois Institute of Technology. Correspondence to: Ren Wang <rwang74@iit.edu>. Paper under review."}, {"title": "2. Enhancing Metrics for Machine Unlearning Based on Conformal Prediction", "content": ""}, {"title": "2.1. Preliminaries and Notations", "content": "Machine Unlearning. Machine unlearning is the targeted removal of certain training data effects from a machine learning model. In our work, two different forgetting scenarios are considered: (i) random data forgetting focuses on ran- domly forgetting specific instances within the training data, and (ii) class-wise forgetting aims to remove all information associated with an entire class. Let $D_{train}$ denote the orig- inal training data used to obtain an original model $\\theta_0$. We split the whole training data $D_{train}$ into two subsets, forget data $D_f$ and retain data $D_r = D_{train} \\setminus D_f$. In random data forgetting, $D_{test}$ represents test data. In class-wise"}, {"title": "Conformal Prediction", "content": "Conformal prediction is proposed to quantify uncertainty in machine learning models, provid- ing prediction sets that contain the true label with a guaran- teed probability (Angelopoulos & Bates, 2021). Among the various types of conformal prediction, this work specifically focuses on split conformal prediction (SCP).\u00b9 To construct the conformal prediction set, it involves four steps:\n1. Calibration Data. SCP first chooses unseen data as calibration data. The number of calibration data points should be enough to evaluate the model's uncertainty.\n2. Non-conformity Score. In our work, we follow the conventional choice and set the non-conformity score as\n$S(x, y_i) = 1 - p_i(x)$, (1)\nwhere $p_i(x)$ represents the probability output (through a softmax function) for the class $Y_i$.\n3. Quantile Computation. Given a target miscoverage rate $\u03b1 \u2208 [0, 1]$, SCP obtains threshold $\\hat{q}$ by taking the $1-a$ quantile of the non-conformity score on the calibration data $D_c$.\n4. Prediction Set. For the data point \u00e6 that needs to be tested, labels with non-conformity scores lower than the threshold $q$ are selected for the final prediction set:\n$C(x) = \\{y_i: S(x, y_i) \\leq \\hat{q}\\}$, (2)\nwhere $y_i$ represents different classes."}, {"title": "2.2. Identifying Limitations in Current Unlearning Metrics", "content": "Evaluating the effectiveness of machine unlearning involves a trade-off between how well the model forgets unwanted data (forgetting quality) and how much it retains useful knowledge (predictive performance). The first key question we pose is as follows:\nNote that while the goal is to remove the influence of the forget data so that it behaves similarly to the calibration data, the exchangeability property may not always hold in machine unlearning settings. Here, we are directly leveraging the concept of conformal prediction to evaluate machine unlearning performance."}, {"title": "2.3. Designing Metrics Motivated by Conformal Prediction", "content": "Based on the limitations of UA and MIA metrics shown in Section 2.2, it raises a question as follows:"}, {"title": "2.3.1. DEFINITION OF NEW METRICS", "content": "Conformal Ratio (CR). To overcome the limitations of UA, we introduce a novel metric, Conformal Ratio (CR), which incorporates both coverage and set size in confor- mal prediction to provide a more comprehensive evaluation. Before defining CR, we introduce Coverage and Set Size.\nThe definition of Coverage, given a dataset D, is as follows:\n$\\text{Coverage} := \\frac{1}{|D|} \\sum_{(x,y_t) \\in D} \\mathbb{I}(y_t \\in C(x))$, (3)\nwhere $y_t$ is the true label of data point \u00e6. Coverage reflects the probability that the true label falls within the prediction set C(x). For D = Df, high coverage indicates that the model retains significant information about forget data, sug- gesting incomplete unlearning. Conversely, higher coverage is better for test data $D_{test}$."}, {"title": "Superiority of Our Metrics", "content": "Unlike existing metrics, both the CR and MIACR metrics incorporate the fact that the true labels of some forgotten data points may remain within the prediction set, even when the predictions deviate from the true labels. This allows for a more robust assessment of the model's forgetting quality and predictive performance."}, {"title": "Evaluation Criteria", "content": "Similar to existing unlearning met- rics, we consider two different criteria to measure unlearning performance with our metrics. \u25cf Gap to RT Criterion: A lower gap to RT method is better for both CR and MIACR metrics. The performance gap relative to the RT method is represented in (\u2022). Limit-Based Criterion: For the CR metric, a lower CR value of forget data Df indicates better"}, {"title": "2.3.2. CONFIDENCE LEVEL AND CALIBRATION DATA", "content": "In conformal prediction, miscoverage rate a and calibration set size are two crucial parameters for determining predic- tion set coverage and reliability. We next discuss the suitable settings for these two parameters and the rationale behind our chosen configuration.\nIn terms of a, a higher confidence level (which is 1 \u2013 a) pro- vides more reliable coverage but typically results in larger prediction sets. Common settings of confidence level are 0.95 and 0.9, corresponding to a values of 0.05 and 0.1, respectively. To make our work more broadly applicable and provide a comprehensive reference, we expand our ex- periments by setting a to a wider range of values: 0.05, 0.1, 0.15, and 0.2. Notably, our default and recommended value for a is 0.1 and subsequent experimental analysis is based on a = 0.1. Higher values of a would result in smaller, potentially more precise prediction sets, but this reduction comes at the cost of decreased confidence, which may not be acceptable in many applications.\nAs for calibration data, it should be independent of train and test data. One requirement for the choice of the calibration set size is that the distribution of the calibration set should match the distribution of the test data as closely as possible. That means a small sample size may lead to distributional dissimilarity and unstable coverage estimation. Therefore, a sufficient sample size of calibration data must be ensured to obtain stable estimates."}, {"title": "2.4. Evaluating Unlearning Methods with New Metrics", "content": "Having established the new metrics CR and MIACR, a nat- ural question to ask is:\nIn this section, we assess the performance of various unlearn- ing methods using the newly introduced metrics, specifically CR, together with coverage and set size. We employ 9 dif- ferent unlearning methods, RT, FT (Warnecke et al., 2021), RL (Graves et al., 2021), Gradient Ascent (GA) (Thudi et al., 2022), Teacher (Tarun et al., 2023), FisherForget- ting (FF) (Golatkar et al., 2020a), SSD (Foster et al., 2024), NegGrad+ (Kurmanji et al., 2024) and Salun (Fan et al., 2023)."}, {"title": "3. Enhancing Machine Unlearning via Conformal Prediction", "content": "Our analysis indicates that machine unlearning methods generally do not exhibit strong performance under our newly developed evaluation metrics. Even methods with relatively low CR values demonstrate significant compromises in other metrics. This observation raises an important question:"}, {"title": "A training-based machine unlearning method is typically op- timized for prediction loss but cannot efficiently support the improvement of forgetting quality in our new metrics. There- fore, we propose a novel and general unlearning framework for training-based machine unlearning methods to enhance machine unlearning", "content": "The key innovation of this framework lies in leveraging conformal prediction principles combined with a loss function inspired by the C&W attack (Carlini & Wagner, 2017) to improve forgetting quality.\nThe original C&W loss function is designed to find adver- sarial examples that cause misclassification. Here we extend it to an unlearning loss. For forget data Df, the objective of the unlearning loss function is to decrease the model's confidence in the true labels of Df. Based on this, the C&W-inspired unlearning loss can be defined as:\n$\\mathcal{L}_{cw} (X, Yt) = \\max_{i \\neq t} \\{p_i(x)\\} - p_t(x) - \\Delta$, (7)\nwhere (x, yt) \u2208 Df. pi(x) is the possibility (after softmax function) of class yi, while zt(x) is the true label yt's pos- sibility output. We denote $\\max_{i \\neq t}\\{p_i(x)\\}$ as the highest possibility value of the incorrect class. A is the margin parameter (also known as the confidence parameter) that controls the attack strength.\nThis loss is minimized when $p_t(x)-\\max_{i \\neq t}\\{p_i(x)\\} < -\\Delta$ to make $\\max_{i \\neq t} \\{P_i(x)\\} - p_t(x) > \\Delta$. It means that the loss maximizes the difference between the highest possibility value for class yi (i \u2260 t) and the possibility value for the true label yt, which makes it more difficult for the model to include the true label in the prediction set during conformal prediction process. Additionally, increasing the value of A further reduces the confidence of the true label yt being included in the prediction set. In our work, we set A 0.01.\nBased on the insight discussed in Section 2, we further im- prove the C&W-inspired unlearning loss function by com- bining conformal prediction. In conformal prediction, cali- bration data helps in estimating non-conformity scores and determining a threshold to ensure valid statistical guaran- tees about the model's uncertainty estimates. Therefore, we reserve a portion of calibration data D' for the unlearning phase, which is kept separate from the calibration data Dc used in the evaluation phase.\nAccording to Eq. 1, calibration data D' is used to calculate non-conformity scores and generate a threshold \u011f based on an a. Then, we calculate the non-conformity scores of Df to obtain the corresponding prediction set. Then, by revising C&W-inspired unlearning loss with this calibration step, a general unlearning loss function is defined as follows:\n$\\mathcal{L}_{unlearn} (x, y_t) = \\max \\{S(x, y_t) \u2013 \\hat{q}, -\\Delta\\}$, (8)\nwhere (x, yt) \u2208 Df. We replace pt(x) in Eq. 7 with the non-conformity score S(x, yt) of true label yt, and replace"}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experimental setting", "content": "Datasets and Models. We report experiments on CIFAR- 10 (Krizhevsky, 2009) with ResNet-18 (He et al., 2016) and Tiny ImageNet (Le & Yang, 2015) with ViT (Dosovitskiy et al., 2021).\nImplementation Details. For CIFAR-10/Tiny ImageNet, we randomly select 200/50 data points per class (2,000/10, 000 data points in total) as calibration data De and D', respectively. The calibration data De does not par- ticipate in the model training or unlearning processes and is only used for calibrating the threshold \u011d, while D' is used in the process of our unlearning framework to generate q.\nC For the hyperparameter in our work, miscoverage rate \u03b1 \u2208 [0.05, 0.1, 0.15, 0.2], margin parameter \u2206 = 0.01, un- learning loss weight \u03bb \u2208 [0,0.2, 0.5, 1]. Due to space lim- itations, more training details and experimental results under settings of a \u2208 [0.05, 0.15], \u03bb = 0.2 and class- wise forgetting scenario can be found in Appendix A-C."}, {"title": "4.2. Measure Machine Unlearning Method via New Metrics", "content": "CR Metric. For the CR metric, we already show the re- sults of CIFAR-10 with ResNet-18 in Tabel 4. Here, we further evaluate the unlearning performance in Tiny Ima- geNet with ViT under 10% random data forgetting scenario by CR metric. The results of Tiny ImageNet with ViT and CIFAR-10 with ReNet18 in 50% random data forgetting and class-wise forgetting scenarios can be found in Appendix B. We remark that it is computationally intractable for the FF method in Tiny ImageNet with ViT, so we omit its results. The results of Tiny ImageNet with ViT in 10% random data forgetting scenario are summarized in Table 5. Our findings reveal the following insights:\nFor all machine unlearning methods, as a level increases, it results in reduced Coverage and smaller Set Size. This happens because a higher a loosens the conformal thresh- old \u011d, allowing fewer predictions to be included within the"}, {"title": "5. Related Work", "content": "Machine unlearning has emerged as a vital research topic due to several privacy, regulatory, and ethical concerns as- sociated with machine learning models. It refers to the process of selectively removing specific data points from a trained machine-learning model. Generally, post-hoc ma- chine unlearning can be divided into training-based (Graves et al., 2021; Warnecke et al., 2021; Thudi et al., 2022; Tarun et al., 2023) and training-free approaches (Guo et al., 2019;"}, {"title": "6. Conclusion", "content": "Motivated by conformal prediction, we introduce new met- rics, CR and MIACR, to enhance the evaluation and reli- ability of machine unlearning. In addiction, our unlearn- ing framework, which incorporates the adapted C&W loss with conformal prediction, improves unlearning effective- ness. Together, we provide a more rigorous foundation for privacy-preserving machine learning."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Setting Details", "content": "For CIFAR-10 with a ResNet-18 architecture, we train the original model from scratch for 200 epochs using SGD with a Cosine Annealing learning rate schedule, starting from an initial learning rate of 0.1. We set momentum to 0.9 and a batch size of 64. For the ViT architecture, we initialize the original model by training a pretrained ViT model for 15 epochs on Tiny ImageNet. We start with a learning rate of 0.001, while other training parameters match those used for ResNet-18. In class-wise forgetting, in each dataset, we designate the same class as the forget data Df across all unlearning methods, with the remaining classes used as the retain data Dr."}, {"title": "B. Evaluating MU methods", "content": ""}, {"title": "B.1. Mis-label Number and In-set Ratios", "content": ""}, {"title": "B.2. CR Metric", "content": "Tables 9 and 10 show the unlearning performance of seven unlearning methods on CIFAR-10 with ResNet-18 in 10% and 50% random data forgetting scenarios, while Table 11 is the results in class-wise forgetting scenario. Tables 12 and 13 present the unlearning performance on Tiny ImageNet with ResNet-18 in random data forgetting scenario, while Table 11 details the unlearning performance in class-wise forgetting scenario.\nFor all machine unlearning methods, increasing the a level leads to lower Coverage and smaller Set Size. Conversely, the CR tends to grow as a increases. Most methods yield a Set Size of less than 1 when a is set too low, since conformal prediction is fundamentally linked to the model's baseline prediction performance (accuracy). If the model's accuracy significantly exceeds the confidence level, conformal prediction can easily achieve the required coverage. In fact, it may produce empty prediction sets for certain data points while still satisfying the coverage target. Excessively high a values can distort evaluation results, preventing the CR metric from accurately reflecting the model's performance. Therefore, the choice of a should be carefully considered in relation to the model's inherent performance."}, {"title": "B.3. MIACR Metric", "content": "Table 15 presents the performance of seven machine unlearning methods on CIFAR-10 in ResNet-18, evaluated with the MIACR metric. In addition to the settings discussed in Section 4, we include results for a \u2208 [0.05, 0.15] in Table 15."}, {"title": "B.4. How can we better measure forgetting under distribution shifts", "content": "Recall that the coverage value of RL has a relatively large deviation in Table 5 since it employs label corruption in its unlearning strategy which can cause distribution shifts. Here, we introduce how to better measure forgetting under these circumstances.\nTo align the distribution of De with that of Df and minimize the differences between them, we design a shadow model. To make the explanation clearer and more intuitive, we take RL as an example. In the RL unlearning method, the forget data is assigned random labels. Therefore, we apply the same random labeling process to the calibration data and train a shadow model accordingly. We designed two methods:\n1. Shadow model. A shadow model replicates the behavior of forget data Df throughout the unlearning process. A shadow model is a two-step approach: (1) it firstly trains a shadow original model 0% using train data Dtrain and clean calibration data De with the same epoch number as the original model 0; (2) subsequently, we finetune the d' using the random labeled calibration data.\n2. Semi-shadow model. The semi-shadow model only adopts the second step in the shadow model. It finetunes the original model with random-labeled calibration data."}, {"title": "C. Performance of Our Unlearning Framework", "content": "Table 16 presents the performance of our unlearning framework, including a \u2208 [0.05, 0.1, 0.15, 0.2]. We explored the impact of varying A within the range [0, 0.2, 0.5, 0.1], where x = 0 serves as the baseline without applying our framework which can be found in Tables 9 and 12. The results reveal a clear trend: as \u5165 increases, the UA improves significantly across all methods, accompanied by a substantial reduction in CRD\u0104. Interestingly, the RA, TA, and CRDtest metrics remain relatively stable. These results underscore the effectiveness of our unlearning framework in achieving substantial improvements in forgetting quality while preserving the stability of the model's predictive performance.\nFurthermore, we conduct an ablation study and analyze the impact of using our unlearning framework. As illustrated in Figures 3 and 4, we compare the density distributions of non-conformity scores for calibration data De and forget data Df under the RT, FT, and RL unlearning methods. We set X to 1. Clearly, a higher non-conformity score for Df indicates that it is less likely to be included in the conformal prediction set, reflecting more effective forgetting. Comparing Figures 3 and 4, it is evident that after applying our unlearning framework, the distribution of non-conformity scores for forget data shifts noticeably to the right. This demonstrates the effectiveness of our framework in enhancing forgetting quality."}]}