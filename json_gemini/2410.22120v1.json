{"title": "Vision Paper: Designing Graph Neural Networks in\nCompliance with the European Artificial Intelligence Act", "authors": ["Barbara Hoffmann", "Jana Vatter", "Ruben Mayer"], "abstract": "The European Union's Artificial Intelligence Act (AI Act) introduces comprehensive guidelines for the development and\noversight of Artificial Intelligence (AI) and Machine Learning (ML) systems, with significant implications for Graph Neural\nNetworks (GNNs). This paper addresses the unique challenges posed by the AI Act for GNNs, which operate on complex\ngraph-structured data. The legislation's requirements for data management, data governance, robustness, human oversight,\nand privacy necessitate tailored strategies for GNNs. Our study explores the impact of these requirements on GNN training\nand proposes methods to ensure compliance. We provide an in-depth analysis of bias, robustness, explainability, and privacy\nin the context of GNNs, highlighting the need for fair sampling strategies and effective interpretability techniques. Our\ncontributions fill the research gap by offering specific guidance for GNNs under the new legislative framework and identifying\nopen questions and future research directions.", "sections": [{"title": "1. Introduction", "content": "The European Union has taken a significant step for-\nward in the technological domain with the publication of\nthe European Union Artificial Intelligence Act (AI Act)\n[1]. This legislation is notable for its provision of guide-\nlines aimed at developing frameworks for Artificial Intel-\nligence (AI) and overseeing Machine Learning (ML) prac-\ntices. These frameworks possess considerable potential,\nas they could serve as a blueprint for future endeavors\nin the domain of AI and ML. The legislation introduces\nseveral requirements, for instance data management and\ndata governance, robustness of training data and mod-\nels and human oversight, which highly impacts Graph\nNeural Network (GNN) training. Therefore, it is impor-\ntant to know which requirements there are, understand\ntheir implications on GNN training and build the model\naccordingly.\nGNNs have unique characteristics that warrant a closer\nexamination. Unlike traditional ML models, GNNs oper-\nate on graph-structured data, which introduces complex-\nities in data connectivity and relationships. These special\ncharacteristics justify the need for a detailed analysis of\nthe AI Act's impact on GNNs. While there are initial stud-\nies examining the AI Act's effects on ML systems [2, 3, 4],\nno specific research has been conducted on GNNs. In\nthis paper, we argue that GNNs pose specific challenges\nin terms of core requirements of the AI Act, such as data\ngovernance, robustness, explainability and privacy, that\ndemand a closer investigation. This gap underscores the\nimportance of our work in providing tailored guidance\nfor GNNs under the new legislative framework and dis-\ncussing open issues that demand further research.\nIn detail, our contributions are:\nDetailed examination of the requirements of the\nAI Act, tailored precisely to the use of GNNS\nto ensure compliance with the AI Act. This en-\nhances the current discussion of the AI Act to the\nspecifics of GNNs.\nInvestigation of data and model bias in graphs and\nGNNs, particularly due to unfair sampling during\nGNN training. This opens a new perspective on\ndata management techniques for GNNs beyond\nmodel accuracy and training runtime.\nExploration of human oversight and explainabil-\nity with concrete examples of GNN decision-\nmaking. We highlight the inherent trade-off be-\ntween the comprehensibility and accuracy of an\nexplanation, and articulate the demand for fur-\nther (user) studies to better understand the effect\nof explanations on Al system stakeholders.\nAnalysis of privacy-preserving techniques de-\nsigned for GNN training. In particular, we stress\nthat while adding noise to features and labels via\nDifferential Privacy techniques is well-explored,"}, {"title": "2. Background", "content": "This section gives a short introduction to GNNs as well\nas an overview of the four risk categories delineated by\nthe AI Act, along with their implications for the training\nof GNNs. Additionally, an overview of the prerequisites\noutlined in the Al Act for high-risk systems is provided."}, {"title": "2.1. Graph Neural Networks", "content": "GNNs are specialized neural networks devised for ana-\nlyzing data represented in graph forms [5, 6], such as\nnetworks found in social or citation systems. GNNs pro-\ncess data by transforming the initial node features into\nembeddings via an iterative mechanism known as mes-\nsage passing. During this process, each node computes\nnew embeddings by aggregating and synthesizing the\nembeddings from its adjacent nodes, an operation under-\npinned by neural networks that are trainable at each layer\nof the GNN. The input features at the initial layer are\nraw node features, which are systematically enhanced\nthrough successive layers to refine the embeddings. This\nrefinement process is driven by the objective of mini-\nmizing a predefined loss function, typically optimized\nthrough algorithms like stochastic gradient descent. The\nresulting embeddings, which encapsulate the essential\nstructural and feature-based information of the nodes or\nthe entire graph, are subsequently utilized in downstream\napplications such as node classification, link prediction,\nand graph classification. This makes GNNs particularly\neffective for tasks where data is inherently structured as\ngraphs, such as social networks or citation networks. For\na deeper technical explanation of GNNs, Vatter et al. [7]\ncan be consulted."}, {"title": "2.2. AI Act Basics", "content": "The European Artificial Intelligence Act contains a clas-\nsification schema for Artificial Intelligence systems. This\nschema is predicated on a risk-based approach and seg-\nregates Al systems into four distinct categories.\n(1) Minimal Risk: AI systems falling under this cate-\ngory are characterized by their low potential to in-\nflict harm upon the user. Examples of such systems\ninclude GNNs utilized in decision support systems,\nsuch as recommendation systems [8, 9], or GNNs\nemployed in spam filters for emails [10]. It is note-\nworthy that the regulations stipulated by the EU AI\nAct do not extend to this category of Al systems,\nthereby obviating the need for any regulatory action.\n(2) Limited risk: This category refers to risks associ-\nated with a lack of transparency in Al usage. Al\nsystems that fall into this category are subject to the\nrequirement of extended transparency. The category\ncomprises for example Al systems that use GNNs in\nchatbots [11] or emotion recognition systems [12].\nThe user interacting with these kinds of Al systems\nneeds to be made aware of interacting with a ma-\nchine so that they can take an informed decision to\ncontinue or step back. The content generated by such\nsystems must also be marked as AI generated.\n(3) High risk: Encompasses Al systems that process crit-\nical personal data or could put the life and health of\ncitizens at risk. Examples of such systems include\nmedical devices [13, 14], systems that control access\nto education [15], and border control systems. GNNs\ncan be used in all the areas mentioned. For these\nsystems, an enhanced level of quality management\nis mandated. The criteria for this include data gov-\nernance, a possibility for human oversight and the\nrobustness of the applications.\n(4) Unacceptable Risk: Al systems that fall into this cat-\negory are deemed illegal. These are for example\nsystems that manipulate the user or contribute to\nsocial injustice."}, {"title": "2.3. AI Act Requirements for GNNs", "content": "The risk category denoted as \u201cHigh Risk\u201d warrants special\nattention in this context. The subsequent section pro-\nvides an overview of the pivotal requirements delineated\nwithin the Al Act concerning the training and operation\nof high-risk GNNs. All of these criteria are outlined in the\nlegal document [1], with a summary provided in Table 1\nfor quick reference."}, {"title": "2.3.1. Data Management and Data Governance", "content": "The AI Act mandates that the data used in Al systems\nmust comply with data governance requirements. This\nencompasses data sets utilized for training, validation,\nand testing purposes.\nThere is no universally applicable definition of the term\ndata governance, neither in the scientific community nor\namong practitioners in the field of information systems\n[16, 17]. The definition provided by the AI Act involves\nensuring that the data used for training, validation, and\ntesting is relevant, representative, accurate, and as error-\nfree as possible. This includes practices such as proper\ndata collection and preparation, detecting and mitigating\nbias, and ensuring the data is suitable for the Al system's\nintended purpose.\nIn the context of the AI Act, Article 10 delineates the\nprotocols for ensuring effective data governance. It ne-\ncessitates a comprehensive scrutiny of Al systems, par-\nticularly concerning the potential for bias. The manifes\ntation of bias, whether during the training phase or the\ndeployment of Al systems, can precipitate detrimental\neffects. The primary objective is to forestall any bias that\ncould compromise individual safety and health, infringe\nupon fundamental rights, or engender discrimination. As\nper the stipulations of the EU AI Act, such bias must be\ndetected, prevented and mitigated. This is an essential\nprerequisite for maintaining the integrity and fairness of\nAl systems."}, {"title": "2.3.2. Robustness", "content": "According to the AI Act, Article 15, AI systems designed\nto continue learning post-deployment must be designed\nto reduce or eliminate the risk of biased outcomes. This\nrequirement also includes ensuring that any potential\nbiases are effectively addressed through suitable risk mit-\nigation strategies. This process of ongoing learning and\npotential bias reinforcement is referred to as a feedback\nloop.\nThe robustness of a model refers to its ability to pro-\nvide consistent and reliable predictions across different\ndata sets and under different conditions. A robust model\nshould also be able to respond well to new, unknown\ndata or to data with minor disturbances. If a model is\nbiased, it is less robust. Bias occurs when a model system-atically prefers or excludes specific elements of the data,\nwhich may arise from imbalances in training data, errors\nin model design, or suboptimal sampling techniques."}, {"title": "2.3.3. Explainability for Humans", "content": "Article 14 of the AI Act stipulates that Al systems must be\nengineered to enable effective human oversight, ensuring\nthe minimization of risks to health, safety, and funda-\nmental rights. The legislation further mandates that the\noutputs of these Al systems should be interpretable and\nthe derivation of the results should be understandable.\nMany Al models, in particular deep neural networks, are\nreferred to as a black box. To make decisions comprehen-sible, existing interpretability techniques and methodolo-\ngies from the domain of explainable artificial intelligence\n(xAI) can be utilized.\nCommon xAl methods include model-agnostic tech-\nniques like Local Interpretable Model Agnostic Explana-\ntions (LIME) [18], which approximates black-box mod-\nels locally with interpretable ones, and SHapley Addi-\ntive exPlanations (SHAP) [19], which assigns importance\nvalues to features based on cooperative game theory.\nModel-specific methods include feature importance for\ntree-based models [20] and visualization techniques like\nsaliency maps for neural networks [21], all aimed at in-\ncreasing the interpretability of Al models.\nTo enhance the interpretability of GNNs, methods like\nGNNExplainer [22] have been developed. GNNExplainer\nhas been implemented in PyTorch Geometric [23] as well\nas in the Deep Graph Library (DGL) [24]. This method\noperates post-hoc, meaning it explains GNN decisions"}, {"title": "2.3.4. Privacy", "content": "The EU AI Act contains various passages for the secure\nhandling of personal data, emphasizing its protection and\nconfidentiality. For instance, legal regulations mandate\nenhanced safeguarding of data utilized in the creation\nof Al systems or in the mitigation of bias within these\nsystems. In the context of Al systems, adherence to all\nrelevant data protection laws, such as the General Data\nProtection Regulation (GDPR), is obligatory. This safe-\nguarding can be accomplished through the implementa-\ntion of anonymization and encryption techniques, which\nin GNNs can be done by the anonymization of the fea-\ntures or by adding noise to the graph, for example by\nadding or removing edges or nodes [6, 25]."}, {"title": "3. Analysis", "content": "In this section, we examine the areas of influence Data\nGovernance, Robustness, Explainability, and Privacy\nidentified in Section 2.3 with regard to their precise im-\npact on GNNs. The focus hereby lies on bias in the data\nand the model, human oversight and privacy."}, {"title": "3.1. Data Governance", "content": "High-risk Al systems must be trained with data that\nmeets certain standards and requirements. An important\npoint here is an investigation into possible biases that\ncould affect the health and safety of individuals, have a\nnegative impact on fundamental rights or lead to discrim-\nination prohibited by the AI Act, especially if the data\noutputs influence the inputs for future operations [1].\nIf training data exhibits an unbalanced feature or label\ndistribution, this can lead to the aforementioned bias.\nTechnically speaking, any dataset that shows an unequal\ndistribution among its classes can be regarded as imbal-\nanced. Yet, the prevalent view within the community is\nthat imbalanced data specifically refers to datasets with\nsubstantial disparities between classes. This specific type\nof inequality is known as between-class imbalance. An-\nother form of imbalance is the within-class imbalance,\nwhich focuses on the distribution of representative data\nfor various subconcepts within a single class [26]. In\naddition to the class imbalance there is also a label im-\nbalance. The Difference in Proportions of Labels (DPL)\nmetric compares the proportion of observed outcomes\nwith positive labels in one subgroup to the proportion in\nanother subgroup within a training dataset [27].\nIn this paper we focus on between-class imbalance as\nwell as label imbalance and measure these values accord-\ning to existing implementations [27, 28]. The following applies to both imbal-\nance metrics: the closer to zero, the better the distribution\nof the data set; the further away from zero, the greater\nthe imbalance. As can be seen in Table 2, some datasets\nyield low class and label imbalance, while others show\nhigh imbalance in classes, labels, or both.\nKey Takeaway: Class and label imbalance are com-\nmon issues across various graph datasets. It is important\nto monitor such imbalance, and, if appropriate, take cor-\nrective action."}, {"title": "3.2. Robustness", "content": "Sampling is a method to efficiently train GNNs on large-scale graphs. When performing sampling during GNN\ntraining, a subset of data points - such as nodes, edges,\nor subgraphs - is selected from the full graph. Before\neach training epoch, new samples are constructed. As\nnumerous sampling strategies with different objectives\nexist, the choice of sampling method can significantly\ninfluence both the performance and the bias of the model.\nInadequate or disproportionate sampling may result in\nthe neglect of crucial segments of the graph, thereby dis-\ntorting the overall representation of the data. This, in\nturn, affects the model's ability to generalize and per-\nform accurately, underscoring the intricate link between\nsampling strategies and the robustness of machine learn-\ning models [29]. In the following, we explore whether\nsampling can influence the bias during training as only\na selected subset of nodes and edges is used for train-ing. This could lead to an under-representation of cer-tain classes or labels. For our experiments, we use the\ndatasets german, recidivism, credit, pokec-n and\npokec-z as well as the sampling strategies Neighbor\n[30], VR-GCN [31], LABOR [32], and ShaDow [33]. We\nadditionally include no sampling as a baseline. The 2-layer GCN and the sampling strategies are implemented\nwith DGL. Our evaluation is based on the Area Under\nthe Curve (AUC), Statistical Parity [34] and Equality of\nOpportunity [35]. While Statistical Parity measures how\nindependent the predictions of a model are to a sensitive\nattribute, Equality of Opportunity denotes to which ex-tent the predictions are performed equally well across\nall attributes. For both fairness metrics, lower values\nindicate a fairer model, while for AUC, higher values are\nbetter.\nIn Figure 2, we show the experimental results. Across\nall datasets and metrics, the values of Neighbor sampling\nand LABOR usually are close to the baseline (no sampling).\nFor parity and equality, they sometimes even lead to\nbetter results than the baseline, with the exception of\nLABOR leading to worse equality on credit. VR-GCN,\non the other hand, has a higher AUC score than the\nbaseline and other strategies, but can result in higher\nvalues of parity and equality, especially when using the\ngerman or credit credit graph. The fourth sampling\nstrategy, namely ShaDow, proves less suitable. A larger\nbias is induced compared to the other methods, especially\nfor the german, credit, and pokec-z dataset, while the\nAUC is lower than the baseline.\nNeighbor sampling chooses the nodes and edges at\nrandom which is beneficial for the model bias since all\ngroups and attributes are treated equally by the sampling\nmethod. VR-GCN also is a node-wise method, but with\nimportance scores. As VR-GCN is based on historical\nactivations, valuable information is preserved during the\nsampling step, but bias can be reinforced. LABOR is\na layer-wise strategy using a specialized optimization\nmethod and restricts the size of the neighborhood to a\nsmall number. Therefore, a higher AUC can be achieved,\nbut the model might not be as fair as with other methods\ndue to the specialized selection of nodes and edges. The\nfourth method, namely ShaDow, works in a subgraph-based fashion and aims to sample shallow subgraphs with\na depth typically around 2 or 3. This could lead to a loss\nof information needed for training and higher parity and\nequality values.\nKey Takeaway: Our experiments have shown that\nbias can be induced or reinforced when using sampling-based GNN training. Some strategies lead to higher per-\nformance values, but also to a more biased model. Robust-ness against model bias needs to be taken into account\nwhen designing GNN sampling methods. This aspect has\noften been neglected."}, {"title": "3.3. Explainability", "content": "In this section, we delve deeper into the possibilities\nfor GNN explanation. Our exploration is based on the\ntwo implementations of GNNExplainer outlined in Sec-\ntion 2.3.3 which allow for the visualization of subgraphs\nand feature importance.\nGNNExplainer is capable of being applied to various\nscenarios, including graph classification and link pre-\ndiction. In this paper, we restrict the scope of our ex-\nperiments to node classification. For the explanations\nprovided, a basic Graph Convolutional Network (GCN)\nconsisting of two convolutional layers was utilized. This\nnetwork integrates linear transformations with Rectified\nLinear Unit (ReLU) activation functions and incorporates\ndropout to enhance generalization. We used the dataset\ngerman as the foundational data for these experiments.\nThe dataset offers insights into whether a customer with\nspecific features qualifies as a good customer with low\ncredit risk. Similarly, the node classification addresses\nthe same question: determining whether the selected\nnode, representing a customer, is credit-worthy or not.\nThe experiments were conducted using both 2-hop and\n1-hop neighborhoods in PyTorch Geometric\u00b9. Figure 3\nillustrates the explanations generated from these experi-ments. It is evident that increasing the number of hops\nresults in a more complex explanatory graph. Figure 3b\nshows the 2-hop result in a representation that is essen-tially imperceptible to the human eye, potentially making\nit challenging for many stakeholders of an Al system to\ncomprehend. For the 1-hop neighborhood (Figure 3a),\nthe graph remains simple and comprehensible for human\ninterpretation; however, it raises the question of whether\nsuch simplicity adequately captures the complexity of\nnode classification. In both scenarios, only the nodes\nthat impacted the decision are depicted, and it remains\nuncertain whether this is sufficient for a meaningful ex-planation. For further analysis, nodes can be mapped\nto their corresponding features, of which an excerpt is\nshown in Table 4.\nThe features of the nodes are pivotal in the process of\nnode classification. The visualization output of PyTorch\nGeometric is displayed in Figure 3c, which shows the\nmost important features leading to a specific node's clas-sification\u00b2. Additionally, the mapping of feature numbers\nto their corresponding meanings is detailed in Table 3.\nKey Takeaway: Graphs are inherently complex struc-tures. Consequently, methods that elucidate the behavior\nof a GNN through a graph also tend to be intricate. Sim-plified explanations, such as using only a 1-hop neighbor-hood or focusing solely on feature importance without\nincluding graph information, are more straightforward\nto comprehend but provide less detailed information. So\nthere is an inherent trade-off between simplicity and the\ndepth of information in GNN explanations."}, {"title": "3.4. Privacy", "content": "When it comes to maintaining privacy within graph-based data, one of the main concerns are the node fea-tures. Features must be kept confidential and anonymized\nusing appropriate methods as needed. Before training a\nGNN, data anonymization techniques can be employed\nto inhibit any potential identification of individual users,\nthus protecting user privacy throughout the model train-ing process. When GNNs are employed, they process\nedges using machine learning. This raises privacy con-cerns regarding the edges: Should they be considered\nprivate data associated with a specific node, or are they\nexempt from privacy considerations?\nAssuming the data is confidential, Differential Privacy\n(DP) [37] could be employed as a strategy to protect\nprivacy. The fundamental principle of DP is that when\nquerying a dataset consisting of N individuals, the out-come should be, in probabilistic terms, virtually the same\nas if the query were run on a similar dataset that has\neither one fewer or one additional individual. This ap-\nproach ensures the privacy of each individual with a\ncertain probability. To achieve this level of probabilistic\nindistinguishability, adequate noise is added to the re-\nsults of the query, masking individual data points while"}, {"title": "4. Open Questions and Research\nDirections", "content": "In this section, we look at the questions that arise from\nour investigations and experiments and propose new\nresearch directions."}, {"title": "4.1. Bias and Robustness", "content": "For our experiments, we use standard sampling methods\nwhich do not specifically aim at reducing bias. Our re-sults show that fairness can highly depend on the chosen\nsampling method. Consequently, the question arises how\nto better ensure fairness during sampling. Fair random\nwalk strategies, such as those proposed by Rahman et\nal. [39] and Zhang et al. [40] could be considered in GNN\nsampling. Further, our experiments are evaluated with\nmetrics commonly used in the field of machine learning.\nHowever, when using graphs and GNNs, other factors\nsuch as feature distribution and structure, particularly the\nnature of the connections, play a crucial role in regards\nto fairness. More research is needed in the directions of\ndesigning fairness metrics adapted to the specific charac-teristics of GNNs.\nFurthermore, it is essential to consider whether GNNs\nare robust against data distribution shifts. A data distri-bution shift occurs when the data a model uses changes\nover time, leading to a decline in prediction accuracy. En-suring robustness in GNNs means maintaining accurate\nclassification performance with new and evolving data.\nWe summarize the open questions as follows: How can\nsampling strategies be specifically adapted and optimized\nfor different types of graph data to ensure comprehen-sive fairness without compromising model performance?\nWhat additional or refined fairness metrics need to be\ndeveloped? What specific adaptations are needed for\nGNNs to handle dynamic and evolving graph data, and\nhow can these techniques be seamlessly integrated into\nGNN frameworks?"}, {"title": "4.2. Explainability and Privacy", "content": "Several questions arise in the context of explainability.\nFirstly, for whom the output of the Al system needs to be\nexplained. Various stakeholders could come into consid-eration: The end customer, who may be influenced by a\nsystem's decision, the employee of the company in which\nthe Al system is used and the auditor of a supervisory\nauthority, who examines compliance with the AI Act in\ncompanies, could all be equally interested.\nEach of these parties has a unique perspective on the\nrequired explanation of an Al decision. Likewise, each\nof these individuals has their own level of authorization\nfor insights. This implies several privacy issues, as it\nmust be clarified who has the right to view specific data.\nTo illustrate this concept with a specific example: In an\nonline social network, User A receives the explanation\nthat Group G was suggested to him because contact B\nand his contact C - who is not directly connected to A -\nare also in similar groups. This explanation allows User A\nto gain insight into contact C's affiliations, even though\nC has not directly shared this information with A.\nIt is also important to evaluate whether the selected\nexplanatory approaches are appropriate. While explana-\ntions using subgraphs and key features are commonly\nemployed, alternative forms such as text or images might\nbe more comprehensible to some groups of users. Addi-tionally, the methods we analyzed only highlight the im-portant nodes and features. However, having an overview\nof the unimportant ones might also be beneficial, as it\ncould provide insights for improving the training pro-cess of an Al model. To make an informed assessment of\nwhich method is preferred and which information in de-\ntail would be helpful for different user groups, conducting\na user study would be essential [41]. Finally, assessing the\neffectiveness of explanability methods for human over-sight is an interdisciplinary effort that requires further\nresearch [42], especially for GNNs which are potentially\nmuch more difficult to explain and understand due to\ntheir graph structure."}, {"title": "5. Related Work", "content": "Various papers have already focused on presenting the\ncontent of the Al Act in an understandable way [43, 44]\nor explained the impact of the AI Act on ML systems\n[2, 3, 4]. The sub-topics we have identified as challenges\nfor GNN training have also been highlighted in the liter-ature. For example, [45] and [46] address bias in GNNs.\nAs mentioned in Section 2.3.3, several methods exist for\nmaking GNNs explainable, including GNNExplainer [22]\nand GraphLiME [36], which we use in our work. Other\napproaches focus on more theoretical aspectes of expla-nations [47]. Studies have also tackled the security and\nprivacy of GNNs, either by explaining security vulnera-bilities and privacy-enhancing measures, as seen in sur-vey articles [48, 49] or practical implementations such as\nSecGNN [50].\nThe intersection of the two topics, namely how the re-quirements set out specifically in the AI Act affect GNNs,\nhas not yet been investigated. We have conducted pre-liminary analyses of this gap and identified several open\nquestions."}, {"title": "6. Conclusion", "content": "The AI Act establishes important legal requirements\nfor Al and ML, impacting significant areas within these\nfields. In our paper, we demonstrated that the AI Act\nalso presents critical challenges for GNNs. Our initial re-search on this topic has uncovered numerous additional\nopen questions that need to be addressed."}]}