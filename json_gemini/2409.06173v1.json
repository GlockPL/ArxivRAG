{"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks", "authors": ["Georgios Chochlakis", "Niyantha Maruthu Pandiyan", "Kristina Lerman", "Shrikanth Narayanan"], "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to \"adapt\" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on \"learning\" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether \"enabling\" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is avalaible at https://github.com/gchochla/cot-priors.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) [1]\u2013[6] have come to dominate language processing tasks as tools that are reliable, affordable, and scalable in many disciplines [7], [8]. Their proliferation comes from their \u201cemergent\u201d In-Context Learning (ICL) ability [5], [9], i.e., performing tasks by conditioning on input-output demonstrations and/or task instructions.\nWhile ICL is often contrasted with traditional gradient-based updates of the models' parameters (also referred to as in-weights learning) [10], [11], the ICL abilities of LLMs depend on their strong prior knowledge of the task to perform it in a zero- or few-shot manner. Therefore, studying the interplay between ICL and in-weights learning is important for our understanding of the behavior of ICL.\nPrior work found evidence that LLMs may be overly reliant on their prior knowledge, disregarding the demonstrations in the prompt. Specifically, [12] demonstrated that LLMs can ignore the provided evidence in their instructions, and instead perform \"Task Recognition\"; they focus on the examples and the labels separately to fetch the underlying task [13]. To show this, they sampled examples and labels independently, with minuscule impact on performance. Since no ground truth is provided, the authors consider this setting as \"zero-shot\" inference, namely task-recognition zero-shot. While follow-up work [10], [14], [15] has further studied this phenomenon and challenged some of the original findings, more recent work [16] has provided further, quantitative evidence for the pull these task priors exert on the posterior predictions. In particular, they find that, for complex subjective tasks like multilabel emotion recognition, LLMs rely almost exclusively on their task priors for their posterior predictions, performing significantly worse than traditional approaches when these priors are not congruent with the ground truth of the dataset. Here, we use complex subjective to denote tasks with multiple interrelated labels for which people can reasonably disagree about, where \"ground\" truth is substituted by crowd truth [17].\nIn this work, we expand the scope to incorporate morality to the list of subjective multilabel tasks. We use experts to generate reasoning chains for the examples in the prompt and thoroughly study whether we can overcome the pull of the priors with Chain-of-Thought (CoT) prompting [18] in six state-of-the-art LLMs. First, we evaluate how well CoT performs compared to regular ICL. Second, we design potential CoT priors and evaluate their properties. Finally, we evaluate the reasonableness of the reasoning generated by the LLMs. Surprisingly, we identify trends in CoT that are identical to those of ICL [16], suggesting CoT is not sufficient to overcome the pull of the priors, since:\n\u2022 LLMs with CoT perform at the same, subpar levels with ICL in subjective tasks, such as emotions and morality,\n\u2022 Larger and more capable LLMs indeed have reasoning priors that are elicited by CoT irrespective of the evi-dence in the prompt and ossify posterior predictions,\n\u2022 LLM prior reasoning chains remain reasonable and co-herent despite the noise introduced during inference to elicit the priors of the model."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. ICL and the Pull of the Priors", "content": "Since the introduction of ICL [5] as an inference tech-nique, it has been widely used for evaluations on standard benchmarks [19]. It requires no finetuning, which is costly to perform for large models, and usually achieves competitive or state-of-the-art performance. Combined with the existence of APIs [6] and open-source implementations and weights [3], [4], ICL has become an accessible jack of all trades.\nResearchers have studied many aspects of ICL, such as contrasting ICL and in-weights learning by controlling the distribution of data [11], examining how to best select ex-amples for the prompt [20], integrating instructions explicitly during training [3], etc. Relatedly, researchers have also tried to extract the priors of the models by providing random labels for the examples of the prompt [12]. By showing minimal variations in performance, these experiments suggested that LLMs recognize the task in the prompt more so than learn from it, and thereafter perform inference using their prior knowledge of the task. Since no annotations are required for such a setting, the authors suggest that this inference mode can serve as a better, less naive \u201czero-shot\" baseline. Subsequent results challenged the view that LLMs mostly perform task recognition, showcasing a significant degradation in performance when increasing the number of randomized examples in the prompt [10], and analyzed LLM behavior when substituting or permuting the labels [10], [14], [15]. More recent work, however, suggests that in complex sub-jective tasks like emotion recognition, \"Task Recognition\u201d dominates posterior predictions [16]. The implication is that ICL is unable to incorporate divergent perspectives."}, {"title": "B. Chain of Thought", "content": "One potential way to augment ICL is with CoT [18]. CoT incorporates the derivation process explicitly in the prompt, presenting a more human-like reasoning process. This has several advantages, like making the implicit associations in the data explicit in the prompt, making responses more explainable because of the generation of the reasoning by the LLM, and directing more computing resources towards more complex problems (e.g., by producing more tokens for more com-plex reasoning steps). Indeed, CoT improves the robustness of prompting, and generally improves performance and the reasoning capabilities of LLMs. Subsequent work [21] has tried to expand on CoT to enable the model to backtrack. In this work, we study whether CoT can alleviate the posterior's collapse to the prior described above [16], as incorporating the reasoning chain explicitly could bridge the gap between priors and evidence. We also evaluate whether CoT has priors, and how reasonable the generated reasoning is (more details in Section III-D). Relevant to the evaluation of the reasoning of LLMs is work examining the faithfulness of CoT [22], [23]."}, {"title": "III. METHODOLOGY", "content": "We closely follow the methodology and notation in [16]. For a set of examples X, and a set of labels y, a dataset D defines a mapping f : X \u2192 Y, as well as reasoning chains R(x) = r, x \u2208 X, explicitly describing f, and therefore D = {(x,y,r) : x \u2208 X, y = f(x), r = R(x)}, from which we can sample demonstrations with p(x,y,r). We do not differentiate between splits for brevity. Given CoT prompt S = {(xi, yi, ri): (xi, yi, ri) ~ ps, i \u2208 [k]} with k demonstrations from sampling distribution ps, an LLM produces its own mapping and predictions for the task, denoted as fk(.; ps) : X \u2192 Y. For all our experiments, we set the temperature to 0 to derive deterministic predictions."}, {"title": "A. Performance and Similarity Measures", "content": "To evaluate both API-based and open-source LLMs, we rely on similarity measures calculated directly on the final predic-tions rather than probabilistic measures like the models' output logits. Using probabilistic measures is also not straightforward for multilabel tasks, as described in [16]. Therefore, we use the Jaccard Score, Micro and Macro F1 metrics [24] to evaluate the performance of the models. For consistency, we also use them to quantify the similarity between the prediction sets from different model runs, since they are symmetric functions, allowing us to apply them to interchangeable sets."}, {"title": "B. Task Prior Proxies via Task Recognition", "content": "We use 3 CoT settings where the data for the prompt are sampled randomly. First, we have the true reasoning task-recognition zero-shot prior, where the prompt contains k demonstrations sampled with pI(x,y,r) = p(x)p(y)p(r), so text, labels, and reasoning are sampled independently from each other from D, hence labels and reasoning are irrelevant to the text and each other. This effectively maintains the higher-order relationships between labels, which are strong in such multilabel tasks [25]. We also construct two more settings for more fine-grained evaluations, where we sample only the reasoning or only the label independently with pr(x,y,r) = p(x,y)p(r) and ply(x,y,r) = p(x,r)p(y) re-spectively. We will refer to fk(.; pI), fk(.; pI,r) and fk(.; pI,y) as task priors henceforth. When using regular ICL, we simply drop the reasoning text."}, {"title": "C. Pull of Prior", "content": "To quantify the pull of the prior, we compare the similarity of fk(.;p) with the ground truth (that is, the CoT performance of the model) and with the task priors. Higher similarity with the task prior indicates a greater pull of priors on the final posterior predictions. To compare across models, we can use the difference between the two similarities for each."}, {"title": "D. Reasonableness of Reasoning", "content": "We also measure the reasonableness and the plausibility of the reasoning chains produced by the LLMs, as well as how coherent and reasonable the prediction is given the generated reasoning. More concretely, we first manually eval-uate whether the produced reasoning chain is relevant and describes aspects of the specific input in a plausible manner. For example, missing potential sarcasm in a document could result in erroneous rationale from the model, yet the reasoning might still be coherent and relevant to the input. Then, irrespective of the input, we manually evaluate whether the labels can be directly derived from the reasoning chain. We stress, therefore, that we do not evaluate the correctness of reasoning and predictions, which is rather assessed by the model's performance."}, {"title": "E. Prompt Design", "content": "Previous work has demonstrated that small changes in the prompt can have a significant impact on the outputs of LLMs. In our effort to reduce the search space, we standardize the prompt template, presenting the one that yields the best performance with respect to the ground truth among the ones we experimented with. In addition, because the specific examples and their order in the prompt can affect the output of the model, we use exactly the same examples, in the same order across corresponding experiments."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Datasets", "content": "MFRC [26]: Multilabel moral foundation corpus with annota-tions for six moral foundations: care, equality, proportionality, loyalty, authority, and purity. To reduce inference costs, we use a random subset of 100 examples for evaluation.\nGoEmotions [27]: Multilabel emotion recognition benchmark with 27 emotions. For consistency, we evaluate the model on a random subset of 100 examples. To make the task feasible for the LLMs, we pool the emotions to the following seven \"clusters\" by using hierarchical clustering [27]: admiration, anger, fear, joy, optimism, sadness, and surprise."}, {"title": "B. Implementation Details", "content": "We use the 4-bit quantized versions of the open-source LLMs through the HuggingFace interface for PyTorch. We use LLaMA-2 (meta-llama/Llama-2-#b-chat-hf), and 3 (meta-llama/Meta-Llama-3-#B-Instruct), GPT-3.5 Turbo (gpt-3.5-turbo), and GPT-40 mini (gpt-40-mini). We perform 3 runs for each LLM experiment, varying the examples used and their labels. We control which examples or labels are selected for each run to ensure consistency across models. We present mean and standard deviation. When computing them for similarities, we use every possible pair between two configurations. We use random retrieval of examples. We use less shots for CoT ({5,15} compared to {5, 25, 45}) given that the reasoning in the prompt increases the length of the prompt. Two experts generated reasonings independently given a single example, and annotated the reasonableness of the generated reasoning chains by the LLMs. We use the reasonings of one annotator for our experiments, and use the other's to check consistency."}, {"title": "C. Performance gains from CoT", "content": "First, we evaluate whether CoT can improve the perfor-mance of LLMs above and beyond ICL. \nFirst, for MFRC, we note that performance w.r.t. Jaccard Score is much higher compared to the F1 scores for both methods. Given the label sparsity, this disparity in values indicates that the model is struggling with true positive, making the Jaccard Score suboptimal for comparison between models. Therefore, we focus our analysis on the F1 scores. It is evident that CoT does not present any improvement relative to ICL, which itself is a weak baseline. We see that performance does not significantly differ across models. An exception is GPT-40, where 5-shot ICL achieves notably better scores, yet performance degrades with more examples. In fact, it does so consistently with more ICL examples.\nFor GoEmotions, on the other hand, we notice that all metrics have similar values, and therefore can focus our analysis on any to derive insights. First, we note that we indeed see scaling of the performance across models, with smaller and/or capable models performing the worst, as expected. Nonetheless, when integrating CoT, we see that the best performing models are not really augmented, whereas the smallest model improves radically. This greater malleability of smaller models is consistent with the findings in [16]. Overall, we find benefits from CoT only in smaller models, and very little improvement otherwise. Therefore, we conclude that CoT does not improve performance in complex subjective tasks, especially for the latest models."}, {"title": "D. Reasoning Priors", "content": "To analyze the reasons behind CoT failing to improve per-formance, we first look at whether the models have reasoning priors that ossify posterior predictions despite the evidence presented to the model in the form of proper reasoning chains and labels. , we present the similarity of the predictions of CoT (f15(.;p)) with (i) the ground truth f (and therefore the performance of the modes), (ii) the ICL prior f25(.; pI), (iii) CoT with random labels f15(.; pI,y), (iv) CoT with random reasoning chains f15(.; pI,r), (v) CoT with both random reasoning chains and labels f15(.;pI), which would be the proper prior for CoT, and (vi) CoT using the reasoning chains of another annotator to check for consistency.\nConsidering the potential of CoT, our results are surprising yet consistent with previous findings on ICL [16]. We notice that in less capable models, randomizing the reasoning chains (with or without random labels) decreases the similarity to CoT, which usually remains lower than the similarity to the ground truth and the consistency of CoT. It is interesting to see that randomizing only the labels does not significantly impact predictions, as similarity tends to be on par with consistency. However, for bigger and more capable models, like LLaMA-3 and GPT, it becomes evident that LLMs develop reasoning priors. In particular, the similarity to the (potential) CoT prior becomes notably larger than that to the ground truth, and even the ICL prior starts to resemble CoT more than the ground truth. For reference, we note that the similarities of the ICL prior to the CoT prior and the regular COT are similar in value."}, {"title": "E. Coherence of Reasoning Chains", "content": "For a complete analysis of CoT, but also to further demon-strate the presence of reasoning priors, we manually evaluate the reasonableness of the reasoning chains and the labels generated both by CoT and the CoT prior. It is interesting to see that for both CoT and its prior, the level of reasonableness is quite high in both reasoning and labels, except for MFRC label coherence. Since the level of reasonableness is quite high for the randomized prompts, this indicates that the randomization of reasoning and labels truly evokes the reasoning prior of the model. Finally, from our manual evaluation, we notice that the LLMs routinely miss nuanced meanings, such as sarcasm, yet their more rudimentary reasoning still remains valid."}, {"title": "V. CONCLUSION", "content": "In this work, we evaluate whether CoT displays the same priors as simple ICL. We find, surprisingly, that larger lan-guage models do have reasoning priors that ossify their gen-erated reasoning and posterior predictions, virtually disregard-ing the evidence in the form of reasoning and labels provided in the prompt. Given that the performance of LLMs is inferior to traditional methods in the tasks we study, this is unlikely to be an artifact of data memorization. We conclude, therefore, that reasoning in the form of CoT prompting is unable to overcome the pull of the priors in larger language models, leading to suboptimal performance on subjective tasks, in contrast to other tasks where labels, and in turn reasoning, remain more consistent across datasets, the validity of a label can be asserted more formally and consistently, and therefore agreement between annotators is much higher."}]}