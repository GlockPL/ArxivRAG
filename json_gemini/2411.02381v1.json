{"title": "Addressing Uncertainty in LLMs to Enhance\nReliability in Generative AI", "authors": ["Ramneet Kaur", "Colin Samplawski", "Adam D. Cobb", "Anirban Roy", "Brian Matejek", "Manoj Acharya", "Daniel Elenius", "Alexander M. Berenbeim", "John A. Pavlik", "Nathaniel D. Bastian", "Susmit Jha"], "abstract": "In this paper, we present a dynamic semantic clustering approach inspired by the\nChinese Restaurant Process, aimed at addressing uncertainty in the inference of\nLarge Language Models (LLMs). We quantify uncertainty of an LLM on a given\nquery by calculating entropy of the generated semantic clusters. Further, we pro-\npose leveraging the (negative) likelihood of these clusters as the (non)conformity\nscore within Conformal Prediction framework, allowing the model to predict a set\nof responses instead of a single output, thereby accounting for uncertainty in its pre-\ndictions. We demonstrate the effectiveness of our uncertainty quantification (UQ)\ntechnique on two well-known question-answering benchmarks, COQA and Trivi-\naQA, utilizing two LLMs-Llama-2-13b and Mistral-7b. Our approach achieves\nstate-of-the-art (SOTA) performance in UQ, as assessed by metrics such as AUROC,\nAUARC, and AURAC. The proposed conformal predictor is also shown to produce\nsmaller prediction sets while maintaining the same probabilistic guarantee of in-\ncluding the correct response, in comparison to existing SOTA conformal prediction\nbaseline. Our code is publicly accessible at https://shorturl.at/7yHSq.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly being utilized for various tasks, including open-\nworld question answering. However, these models are known to exhibit hallucinations, confidently\nproducing incorrect information or relying on faulty reasoning. Quantifying the uncertainty associated\nwith an LLM for a given input offers a practical method to account for model's reliability on its\nresponse. When there is a strong correlation between LLM's accuracy and the computed uncertainty,\none can effectively use uncertainty quantification (UQ) approach to determine when to place their\ntrust in the model's responses.\nThe challenge of UQ for LLMs in generative context differs significantly from that encountered\nin regression or classification tasks. While the latter has been well-studied Guo et al. (2017); Jha\net al. (2019); Xiao and Wang (2019); Hu and Khan (2021); Magesh et al. (2023), UQ for generative\nmodels presents unique challenges due to the free-form nature of the responses of varying lengths.\nAdditionally, the syntactic similarity of generated sequences may not necessarily align with their\nsemantic similarity.\nSelf-consistency theory can be employed to measure LLM's uncertainty on an input query by\ncomparing semantic context of multiple outputs sampled from the LLM for that query (Wang et al.,\n2022). Building on recent observations by Lin et al. (2023); Kuhn et al. (2023), we use embedding-"}, {"title": "2 Related Work", "content": "Uncertainty quantification (UQ) for LLMs has received significant attention over the last few\nyears. One approach is to explicitly query the model for the correctness probability (Kadavath et al.,\n2022). Another approach relies on utilizing the log-likelihood (Jiang et al., 2020) associated with\nits generated response by taking a product or an average or other statistical aggregation over the\ngenerated tokens. LLMs are known to be not well-calibrated (Mielke et al., 2022) and consequently,\nmethods for calibrating LLMs (Huang et al., 2024) have also been proposed. Semantic entropy or\npredictive uncertainty that measures the (in)consistency among multiple responses has been proposed\nas a metric for UQ of LLMs (Kuhn et al., 2023; Lin et al., 2023). We also use semantic entropy for\nUQ but adopt a novel semantic clustering approach and empirically demonstrate its effectiveness.\nConformal prediction (CP) (Balasubramanian et al., 2014) has been used for deploying deep\nlearning models (Kaur et al., 2022; Haroush et al., 2021; Kaur et al., 2023, 2024; Yang et al., 2024)\nin high-assurance applications wherein the model predicts a set instead of a single prediction such\nthat one of the responses in the set is guaranteed to be correct with a probability higher than the\nuser specified significance level. In the context of LLMs, conformal prediction has been used for\nproviding coverage guarantees (Ye et al., 2024; Quach et al., 2023). Ye et al. (2024) concentrate\non classification settings and propose non-conformity scores in the CP framework accordingly. In\ncontrast, we focus on generative setting for LLMs in applications such as question-answering. Quach\net al. (2023) propose generating diverse prediction sets based on the quality of individual responses,\nand a set scoring function. They utilize CP to derive hyperparameters (As) for diversity, quality, and\nset scoring function in their algorithm for coverage guarantees. We, instead propose, using (negative)\nlikelihood of clusters representing semantically diverse responses, as the (non)conformity score in\nthe CP framework for generating sets with coverage guarantees, and compare our results with Quach\net al. (2023)'s approach."}, {"title": "3 Clustering by Semantic Equivalence", "content": "In this section, we introduce the proposed clustering approach, and then describe its usage in\nuncertainty quantification and building the conformal predictor for LLMs."}, {"title": "3.1 Uncertainty Quantification", "content": "Different clusters containing semantically diverse responses to an input query can be used to quantify\nuncertainty of the LLM on the query. Similar to Kuhn et al. (2023), we also use entropy of the\ngenerated clusters from Alg. 1 as a measure of UQ for LLMs on a given query. Both qualitative and\nquantitative results indicate that the proposed clustering approach yields better results than Kuhn's\nbaseline."}, {"title": "3.2 Conformal Set Prediction", "content": "For each cluster c, we have p(c|x). We, therefore, use the negative log probability, log[1/p(c|x)], of\nthe individual clusters, as the non-conformity score in conformal prediction (CP) framework (Bal-\nasubramanian et al., 2014), for generating prediction sets. Non-conformity scores of calibration\ndatapoints is used to build a reference empirical distribution to compare against when building\nthe prediction set. Specifically, depending on the desired significance level, e, prediction set is\ngenerated by comparing scores for the test clusters with a threshold from the empirical distribution:\nnon-conformity score of calibration set at (1 \u2013 \u0454)th quantile of the distribution. Intuitively, clusters\nwith low negative log probability (or high likelihood) are more likely to be included in prediction sets\ncompared to clusters with high negative log probability (or low likelihood). If an LLM outputs many\nsemantically equivalent responses, then we expect the cluster's log[1/\u2211s\u2208cp(s|x)] to decrease due\nto the summation over the sentence probabilities by sharpening the cluster probability.\nThe use of CP for constructing the prediction sets gives us coverage guarantees on the true answer in\nthe set with the probability greater than or equal to 1 \u20ac (Vovk et al., 2005). Alg. 2 is the proposed\nCP algorithm for generating prediction sets with coverage guarantees.\nFor an input query x (from test or calibration set), clusters are generated via Alg. 1. We use negative\nlog probability (nlp = log[1/p(c|x)]) of each generated cluster (c) for x as the non-conformity score\nfor the cluster. For the desired significance level \u20ac \u2208 (0,1), the prediction threshold T is decided\nas the score at (1 \u2013 e)th quantile of the empirical distribution of non-conformity scores for the\ncalibration clusters. Assuming all responses are semantically equivalent in a cluster, a single response\nfrom the test cluster (c) is added to the prediction set if its non-conformity score (nlp(c)) is below the\nprediction threshold 7. In our experiments, prediction set is constructed from the first response in the\nqualified test cluster."}, {"title": "4 Experimental Results", "content": "The experimental evaluation focuses on two research questions. RQ1: Does the novel semantic\nclustering approach inspired from CRP improve the UQ of LLMs? RQ2: How does Alg 2 perform\ncompared to the CP baseline on LLMs for free form generative responses?\nDatasets and Models. We use two question-answer datasets: COQA (Reddy et al., 2019) and\nTriviaQA (Joshi et al., 2017), over which we compare the performance of two LLMs, Llama-2-\n13b: non-instruct model (Touvron et al., 2023), and Mistral-7b: instruct model (Jiang et al., 2023).\nFollowing existing literature (Lin et al., 2023; Kuhn et al., 2023), we deploy three evaluation methods:\n(1) We query GPT-4 (Achiam et al., 2023) by asking it to provide a rating on whether a response is\ncorrect with a value between 0 and 1, and label the response as correct if its rating > 0.7; (2) RougeL\nscore (Lin, 2004) with a threshold > 0.3; (3) Deberta (He et al., 2020) to check for entailment of\ncorrect answer in the generated response."}, {"title": "4.1 UQ Performance", "content": "We report Area Under Accuracy-Rejection Curve (AUARC) (Nadeem et al., 2009), and Area Under\nRejection-Accuracy Curve (AURAC) for comparing our performance on UQ with Kuhn et al. (2023)'s,"}, {"title": "4.2 Conformal Prediction Results", "content": "The desired properties of a prediction set is that the accuracy of the set should be as high as possible\nwith a smaller set size. So, here we report accuracy and set size as the evaluation metrics."}, {"title": "4.2.1 Comparison with the CP Baseline", "content": "Fig. 1 shows Alg. 2 results in comparison with the existing baseline by Quach et al. (2023) on using\nCP for generating prediction sets with coverage guarantees."}, {"title": "4.2.2 Experiments on COQA", "content": "We also evaluate our Alg. 2's performance on COQA. Figure 2 shows these results for both accuracy\nand set size and with all the three GT evaluation approaches on COQA: GPT-4, RougeL, and Deberta.\nHere, we also report the point accuracy, which is the average accuracy of the individual N = 20\ngenerations. For \u20ac < 0.35, the prediction set accuracy is always higher than the point accuracy.\nConsistent with the results on TriviQA, here also the value of accuracy and set size decreases\nwith the increase in the value of \u20ac. GPT and RougeL evaluations satisfy the coverage guarantee\nVe \u2265 0.15. Even though conformal prediction provides a rigorous theoretical guarantee, deviations\nfrom the coverage guarantee can occur in practice due to limited sample variability in the caliibration\nset (Angelopoulos and Bates, 2021). This justifies the accuracy results with Deberta Evaluation and\nthe other two evaluations with \u20ac < 0.15.\nOne difference to note here from the TriviaQA experiments is in the set of e values. For Triv-\niaQA, we reported results with \u0454 \u2208 {0.2,...,0.5}, and for COQA we reported results with\n\u20ac \u2208 {0.1, 0.2, . . ., 0.5}. This is because we were getting 'nan' at \u20ac = 0.1 from the Quach's baseline\non TriviaQA. While further investigation on their code, we figured it out that they are hard-coding the\nvalue as 'nan' where the search for their algorithm's hyperparameters (As) might be failing."}, {"title": "5 Conclusion", "content": "This paper takes a step towards enhancing reliability in generative AI by addressing uncertainty in\nLLMs on a given query. Our approach focuses on the semantic equivalence of responses generated\nby an LLM when prompted multiple times for the input query. It is based on the idea that an LLM\nis expected to be accurate if it consistently generates semantically similar outputs when prompted\nmultiple times with the same input. The underlying assumption here is that consistency can serve\nas an indicator of accuracy. Testing this hypothesis is one of the future works that we intend to\ninvestigate. Additionally, we aim to explore alternative scoring methods beyond the probability\nof response-calculated as the product of conditional token probabilities- used to determine the\nlikelihood of semantic clusters. This is important because response probability can be sensitive to its\nlength, which poses a significant challenge when dealing with free-form generations produced by\nLLMs."}, {"title": "A Appendix", "content": "Here, we provide an example from both COQA and TriviaQA datasets to analyse how our new\nclustering approach compares to the original approach by Kuhn et al. (2023). We look at the quality\nof clusters formed by both approaches."}, {"title": "A.1.1 An example from COQA Dataset", "content": "Story: CHAPTER XXXIV Arthur remained at the gate while Ruth climbed Maria's front steps. She\nheard the rapid click of the type-writer, and when Martin let her in, found him on the last page of a\nmanuscript. She had come to make certain whether or not he would be at their table for Thanksgiving\ndinner; but before she could broach the subject Martin plunged into the one with which he was\nfull. \"Here, let me read you this,\" he cried, separating the carbon copies and running the pages of\nmanuscript into shape. \"It's my latest, and different from anything I've done. It is so altogether\ndifferent that I am almost afraid of it, and yet I've a sneaking idea it is good. You be judge. It's an\nHawaiian story. I've called it \u2018Wiki-wiki'.\u201d His face was bright with the creative glow, though she\nshivered in the cold room and had been struck by the coldness of his hands at greeting. She listened\nclosely while he read, and though he from time to time had seen only disapprobation in her face, at\nthe close he asked:- \u201cFrankly, what do you think of it?\u201d \u201cI\u2013I don't know,\" she, answered. \u201cWill it-do\nyou think it will sell?\u201d \u201cI'm afraid not,\u201d was the confession. \u201cIt's too strong for the magazines. But\nit's true, on my word it's true.\u201d \"But why do you persist in writing such things when you know they\nwon't sell?\" she went on inexorably. \u201cThe reason for your writing is to make a living, isn't it?\u201d\nQuestion: 'Did he answer her?'\nAnswer: 'No'\nGenerated Responses from Llama-13b: [\u2018Yes', 'Yes', \u201cHe didn't\", 'He did, only not directly', 'No',\n'No', 'No', 'He asked her what she thought', \u2018He told her his latest story', 'Yes', \u2018No\u2019, \u2018A sneaking\nyes', 'He ran the manuscript up to Miss Lawton', 'No', \u2018In the affirmative', 'Yes', 'No', 'No', \u2018Yes',\n'Yes']\nResults: Figures 3, and 4 show the clusters formed by Kuhn et al. (2023), and our approach\nrespectively. For brevity, we include only unique responses in a cluster. As it can be seen, Kuhn et al.\n(2023) approach puts semantically different responses in the same cluster (responses 3, 4, and 6 in\ncluster 1 for 'Yes'), whereas ours separate them out in different clusters.\""}, {"title": "A.1.2 An example from TriviaQA Dataset", "content": "Question: What is \u2018The Old Lady of Threadneedle Street'?\nAnswer: Bank of England\nGenerated Responses from Llama-13b: [\u2018Bank of England', \u2018Bank of England', 'Bank of England',\n'The Bank of England', 'A nickname; what was it really?', 'Bank of England', 'The Bank of England',\n'The Bank of England', 'The Bank of England', 'The Bank of England', 'The Bank of England',\n'Bank of England', 'The Bank of England', 'Bank of England', 'The Bank of England', 'Bank of\nEngland', 'Bank of England', 'Bank of England', 'The Bank of England', 'Bank Of England']"}, {"title": "A.2 All UQ results", "content": "Here, we include all results on UQ performance from Section 4.1: comparison with additional\nbaselines on AUARC, AURAC and AUROC evaluation metrics. In addition to Kuhn et al. (2023)'s\nSem. Ent. (Unnorm/Norm), and (Lin et al., 2023)'s EigV results reported in the main paper, we\ninclude \"Numset\u201d, \u201cLexiSim\", and \"SelfProb\" baselines here. Numset uses the number of semantic\nsets (or clusters) as the UQ metric, and has been previously used in (Lin et al., 2023) as one of the\nbaselines. Higher the numset, more uncertain is the LLM on the input query. LexiSim uses the\naverage of RougeL distance between every pair of responses for UQ. Here, higher the Lexisim, lower\nis the uncertainty. Again, Lexisim has been used as a baseline by Kuhn et al. (2023), and Lin et al.\n(2023). SelfProb (Kadavath et al., 2022) estimates if the probability of a model's response is correct\nby asking the model itself, and use that as the UQ metric. We follow the same prompt format as Lin\net al. (2023) for asking the model about the probability, and report average over all responses. Here,\nhigher the SelfProb, lower is the uncertainty."}]}