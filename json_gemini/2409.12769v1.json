{"title": "The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning", "authors": ["Manh V. Nguyen", "Liang Zhao", "Bobin Deng", "William Severa", "Honghui Xu", "Shaoen Wu"], "abstract": "Spiking Neural Networks (SNNs) have recently gained significant interest in on-chip learning in embedded devices and emerged as an energy-efficient alternative to con-ventional Artificial Neural Networks (ANNs). However, to extend SNNs to a Federated Learning (FL) setting involving collab-orative model training, the communication between the local devices and the remote server remains the bottleneck, which is often restricted and costly. In this paper, we first explore the inherent robustness of SNNs under noisy communication in FL. Building upon this foundation, we propose a novel Federated Learning with Top-K Sparsification (FLTS) algorithm to reduce the bandwidth usage for FL training. We discover that the proposed scheme with SNNs allows more bandwidth savings compared to ANNs without impacting the model's accuracy. Additionally, the number of parameters to be communicated can be reduced to as low as 6% of the size of the original model. We further improve the communication efficiency by enabling dynamic parameter compression during model training. Extensive experiment results demonstrate that our proposed algorithms significantly outperform the baselines in terms of communication cost and model accuracy and are promising for practical network-efficient FL with SNNS.", "sections": [{"title": "I. INTRODUCTION", "content": "Spiking Neural Networks (SNNs) constitute a significant advancement in artificial intelligence (AI), operating with asynchronous discrete events called spikes, which enhances energy efficiency, particularly for neuromorphic hardware de-signed to mimic the brain's neural architecture. This efficiency is crucial as Al's energy demands rise, positioning SNNs as a solution to the impending energy crisis [1]. Beyond neuromorphic hardware, SNNs are also suitable for general-purpose use, including on resource-constrained edge devices with limited power budgets. Studies and practical applications have demonstrated that they provide significant energy and resource efficiency [2], [3], making SNNs an attractive option for sustainable, low-power AI systems across various domains, from industrial automation to smart home technologies.\nFor efficiency and privacy concerns, Federated Learning (FL) has emerged as an effective framework for distributed AI systems. It has great potential to be integrated with SNNS [4] to enjoy the benefits of both worlds. With FL approaches, instead of aggregating the raw data from distributed entities to a central server, the model is trained across multiple devices, each holding local data samples. The central server then aggregates the updates of the locally trained models. However, FL training methods also incur significant communication costs due to frequent updates and exchanges of model param-eters between the central server and the decentralized nodes. This makes communication the bottleneck in the large-scale deployment of FL systems. Additionally, most existing FL frameworks assume perfect communication while real-world communication networks are noisy and prone to packet loss and transmission errors [5]. Noisy communication can lead to corrupted data exchanges, negatively impacting the model's convergence and accuracy. Addressing these issues is crucial for the widespread adoption and efficiency of FL systems.\nThis work attempts to improve network efficiency in FL with SNNs considering noisy communication channels. We first explore the robustness of SNNs compared to Artificial Neural Networks (ANNs) as a foundation for its adaptation in bandwidth-limited environments. By leveraging the robustness of SNNs to noise, we propose a suite of Top-\u043a Sparsification based algorithms to reduce bandwidth consumption in FL with SNN. Finally, we conduct empirical experiments to verify the superiority of SNNs to ANNs in FL settings under noisy communication scenarios. Furthermore, we evaluate the effectiveness of the proposed algorithms in terms of commu-nication cost and accuracy and their sensitivity to network size and communication compression rate. This work aims to provide practical and advantageous FL solutions for edge-device SNNs, whose environments are often under limited and noisy communication settings.\nContribution. The key contribution of the work presented in this paper is three-fold:\n\u2022\nTo the best of our knowledge, we are among the first to investigate the performance of FL with SNN under noisy communication. We discover that SNN is significantly more"}, {"title": "II. BACKGROUND OF FEDERATED LEARNING AND SPIKING NEURAL NETWORKS", "content": "In this section, we briefly cover the background of Spiking Neural Networks (SNNs) and Federated Learning (FL).\nA. Spiking Neural Network\nRate encoding. A spiking neuron is modeled to perceive input as incoming spikes over a predefined time interval. The neuron accumulates membrane potential while absorbing the input spikes and scales by the synaptic weights. Once the membrane potential reaches a certain threshold, the neuron fires an output spike, releases the membrane energy, and the process restarts. This mechanism is known as Integrate-and-Fire (IF) and is illustrated in Fig 1. Another popular type of spiking neuron is Leaky-Integrate-and-Fire (LIF), which gradually decreases (or leaks) membrane potential at each timestep. The following equation describes the LIF mecha-nism of a neuron i:\n$U_{i}^{t} = \\sum_{j \\in N} W_{j i} S_{j}^{t-1} + \\beta U_{i}^{t-1} - S_{i}^{t-1} U_{t h r}$,\nwhere $\\beta<1$ and $S_{i}^{t}=\\begin{cases}1 & \\text { if } U_{i}^{t}>U_{t h r}, \\\\ 0 & \\text { otherwise. }\\end{cases}$\n(1)\nin which, $S_{i}^{t}$ is the binary output of neuron i, and $U_{i}^{t}$ is its membrane potential at timestep t. While \u03b2 represents the leak factor by which the membrane potential is reduced at each timestep and $U_{thr}$ represents the membrane threshold.\nN denotes the set of input neurons that is connected to i, and $W_{ji}$ denotes the synaptic weight of the $j \\rightarrow i$ connection.\nBack-propagation method for model training. Due to the time dimension encoding and the non-differentiable functions, the conventional ANN backpropagation is unsuitable for SNN training. According to Neftci et al. [6], the back-propagation of a spiking neuron begins by calculating $\\Delta W_{j i}$, the gradient of the weight connecting neuron i and j accumulated over T as follows:\n$\\Delta W_{j i}=\\frac{\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial S_{i}^{t}} \\frac{\\partial S_{i}^{t}}{\\partial U_{i}^{t}} \\frac{\\partial U_{i}^{t}}{\\partial W_{j i}}}{\\sum_{t=1}^{T} \\frac{\\partial L}{\\partial S_{i}^{t}} \\frac{\\partial S_{i}^{t}}{\\partial U_{i}^{t}}}$\n(2)\nwhere L is the loss function. Categorical cross-entropy is widely used in image classification. As $S_{i}^{t}$ is a thresholding function, computing $\\Delta W_{j i}$ is intractable. To simplify this problem, the threshold function can be approximated with surrogate functions that is defined as follows:\n$\\frac{\\partial S_{i}^{t}}{\\partial U_{i}^{t}}=\\max \\left\\{0,1-\\frac{\\left|U_{i}^{t}-U_{t h r}\\right|}{\\delta}\\right\\}$\n(3)\nwhere $\\delta$ is a hyperparameter decay factor for back-propagated gradients [6], [7]. As gradients are accrued through time, $\\delta$ is adjusted according to T. The larger the timesteps, the smaller the decay factor should be to avoid gradient exploding.\nB. Federated Learning\nFL framework typically consists of one global aggregation server and a set of local training clients C, in which each client c possesses its private dataset D(c). The server starts the training with a global initial model weight Wo and is distributed to the clients. For each round r, client k receives the global updated model $W_{r-1}$ aggregated from the previous round. Then, each client uses its private data samples for local training and obtains the locally updated model $W_{r}^{(c)}$. The client transfers its updated parameters to the server for aggregation and produce the global model $W_r$, the following formula describes this FedAvg algorithm [8]:\n$W_{r}=\\frac{1}{|C|} \\sum_{c \\in C} W_{r}^{(c)}$,\n(4)\nin which, C denotes the number of participated clients in the FL network. Note that there is a rich literature on alternatives for model aggregation algorithms. For simplicity, this work will adopt the FedAvg algorithm for evaluation without loss of generality. Fig. 4 illustrates an overview of FL with SNN integrated with the compression schemes that we are proposing in this work.\nC. SNN In-situ Training Hardware/System\nThis paper investigates on top of the client-server archi-tecture in which the server or each client has SNN in-situ training capability. Many recent research works [9]\u2013[11] focus on SNN in-situ training and have made significant progress. We expect that in-situ training will become an essential feature of neuromorphic hardware/systems in the near future."}, {"title": "III. ROBUSTNESS OF SNN IN FL WITH NOISY COMMUNICATION", "content": "Recently, Patel et al. [12] investigated the impact of noisy input on the SNN training phase and showed that SNN models can be more resilient than their ANN counterparts. This observation motivates us to explore the robustness of SNN to practical noisy communication in FL, which is an under-explored research area. In this section, we extend SNNs to the FL setting under noisy communication and compare the noise robustness of SNNS with ANNS.\nSetup. To compare the noise robustness between SNNs and ANNs in a FL environment, we utilize VGG9 as the base architecture for both models, with stochastic gradient descent (SGD) optimizer and average pooling. We follow the setup in [4] and adopt similar training parameters. For SNN, we set the timesteps to 25, with a learning rate of 0.1 and momentum of 0.95, while for ANN, we set the learning rate of 0.001 and weight decay of 5e-4. We set up the federated learning system with 5 clients, with a batch size of 32. We train for 40 global aggregation rounds. Before aggregating each global aggregation, each client trains on its private data for 5 local epochs. The model noise is added before transferring global parameters from server to client and locally trained parameters from client to server. The noise is generated following the Gaussian distribution N(0,\u03c3), in which the standard deviation, i.e., \u03c3, indicates the strength.\nNoise generation. In our evaluation, we vary the noise strength and compare the accuracy stability to verify the model robustness. We conduct experiments for ANN and SNN models by setting the noise strength. First, o is set as a fixed value from round to round, called absolute noise (Fig. 2). Second, o is fractionally dependent on the average magnitude of the transmitting parameters at each round, called relative noise (Fig. 3). With \u00f4 to annotate the relative noise strength, we have the following noise generation scheme:\n$\\sigma_{r}=\\hat{\\sigma} \\times A V G\\left(\\{|w|: \\forall w \\in W_{r}\\}\\right)$,\n(5)\nin which $\u03c3_r$ is the strength of the noise added to the parameters set, $W_r$, when transmitting at round r.\nRobustness comparison for ANN and SNN. In Fig. 2, three different absolute noise levels \u03c3\u2208 {0.01, 0.02, 0.03} are"}, {"title": "IV. PROPOSED COMMUNICATION-EFFICIENT FL ALGORITHMS WITH SNN", "content": "A. Rationale\nAs explored in Section III, the property of robustness to noise demonstrated by SNN implies its potential resilience"}, {"title": "B. Federated Learning with Top-\u043a Sparsification", "content": "We propose Federated Learning with Top-k Sparsification (FLTS) leveraging the robustness of SNNs to improve com-munication efficiency. The details are described in Algorithm 1 in stages. At Stage 1, the updated model generated from the previous global aggregation is broadcast to all participating clients. At Stage 2, clients perform independent training by utilizing private data samples and submitting local gradients to the server. At Stage 3, the server performs the FedAvg algo-rithm to aggregate the data gathered from clients and generate a new global model. For the transmissions between clients and the server in Stage 1 and Stage 2, we design function SPARSE to implement our \u0422\u043e\u0440-\u043a Sparsification scheme as follows. The goal is to reduce the transferring load and, therefore, to lower the total bandwidth overhead in the FL process.\n\u0422\u043e\u0440-\u043a Sparsification. Our proposed Top-\u043a parameters sparsification scheme is a compression protocol, which mod-ifies the sparsification scheme in [13]. The goal is to transmit only top parameters with the largest gradients (absolute val-ues) in the resulting model. The insight of this approach is that the magnitudes of parameter gradients are associated with their importance to the model update. Thus, we can potentially preserve the important information in communication by transferring only the model updates with large magnitudes. Specifically, assume that W\u2208 Rd is the trained model, and the corresponding gradient of this model is H\u2208 Rd. Mathematically, the gradient of a client model after local"}, {"title": "C. Federated Learning with Dynamic-\u043a Reduction", "content": "Notice that for FLTS in the last section, the compression ratek is constant throughout the training process. However, as pointed out by Yan et al. in [14], [15], as the global model is gradually being developed through the global aggregation in FL training, the number of important parameters needed to be passed around can be gradually decreased. Specifically, we can potentially further reduce the communication cost by dy-namically adjusting the compression rate \u03ba. Therefore, in this section, we design a modified FLTS called Federated Learning with Dynamic-\u043a Reduction (FLDR) to further reduce the total bandwidth consumption for FL with SNNs. The algorithm is presented in Algorithm. 2, where k is dynamically adjusted in each global aggregation round. We describe the heuristic \u043a selection strategy as follows.\nDynamic-k reduction. The adjustment of kis implemented through the REDUCE(\u03ba, \u03b1, \u03c9, R) function. We implement two modes of K reduction. The first mode relies on linear reduction, in which the function is formulated as follows:\n$\\operatorname{REDUCE}(\\kappa, \\alpha, \\omega, R)=\\kappa-\\frac{\\alpha-\\omega}{R}$\n(7)\nThe second dynamic mode is exponential reduction and is formulated as follows:\n$\\operatorname{REDUCE}(\\kappa, \\alpha, \\omega, R)=\\exp \\left(\\ln (\\kappa)^{\\frac{\\ln (\\alpha)-\\ln (\\omega)}{R}}\\right)$\n(8)\nIn both functions, \u043a is the compression rate of the previous round, R is the number of global training rounds, a is the initial compression rate, and w is the final compression rate. Through out R rounds of global training, \u043a is decreased slowly from a to w, and the REDUCE(\u03ba, \u03b1,\u03c9, R) function simply computes the next step of K reduction."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "In this section, the proposed FLTS and FLDR algorithms are evaluated. We utilize CIFAR10, a multi-class classification dataset popular for SNN performance evaluations, to evalu-ate our proposed compression approaches, including 50,000 32 \u00d7 32 RGB images of training data and 10,000 images of validation data from the original dataset. The training data is equally split among clients. If not specified in the experiment, we adopt 5 clients in the evaluation, and all the clients participate in each global aggregation round. The VGG9 SNN model is applied to the FL framework for evaluation. The sequence length of SNNs, in which each neuron perceives the input data by receiving spikes, will be set to 25, leveraging the leaky-integrate-and-fire (LIF) model for neuron behavior. The training is conducted with SGD optimizer, and the learning rate is set to 0.1 with a momentum of 0.95. The batch size is configured to 32 across all devices. To better observe the training process, each client performs 1 local epoch per global round in our experiments, with a total of 100 global aggregation rounds. All the simulation, evaluations of the model and integrated compression algorithms performed on a centralized server with an 80 Core Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz processor and 503GB of memory. They system also incorporates eight NVIDIA Tesla V100-SXM2-32GB, each with 32 GB of memory."}, {"title": "A. FL with Top-\u043a Sparsification under Noisy Communication", "content": "In this section, we conduct experiments on FL with Top-\u043a sparsification (i.e., FLTS) under noisy communication. We aim to compare the performance of ANN and SNN in the setting aforementioned. The compression ratek is set to 0.2 with various relative noise levels \u00f4\u2208 {0.0, 0.1, 0.2, 0.3}. The results are illustrated in Fig. 5. We observe that noisy communication impacts the FL training for both ANN and SNN models. Nevertheless, SNN is significantly more robust than ANN. Specifically, as the noise level increases, the accuracy of ANN becomes much worse, or the model fails to converge, leaving very limited room for communication compression. This phenomenon validates that FL equipped with SNN allows much more bandwidth saving than ANN under practical noisy communication."}, {"title": "B. Effect of Compression Rate Parameter \u043a in FLTS", "content": "This section evaluates the impact of compression rate parameter \u03ba. Fig. 6 presents the training results of FLTS under various settings such that k\u2208 {1.0,0.5,0.06, 0.05}. This evaluation aims to demonstrate the effectiveness of FLTS in terms of accuracy and total bandwidth utilization compared to the baseline without compression, i.e., \u03ba = 1.0, for SNNs. We observe similar accuracy trends with compression rates of 50% (k = 0.5) and the baseline without compression (\u043a = 1.0). The highest accuracy of \u043a = 1.0 is 82.7% while \u043a = 0.5 is 81.7%. This result indicates that our FLTS algorithm has great potential to reduce bandwidth overhead in FL with SNNs. Fig. 6(a) also demonstrates that the FL algorithm normally performs even under k = 0.06, with the highest accuracy of 75.8%, while the FL training does not learn anything under K = 0.05. Our experimental results indicate that 6% is the minimum compression rate of FLTS for VGG9 SNN with the CIFAR10 dataset."}, {"title": "C. Impact of Network Size", "content": "In this section, we investigate the impact of network size. Fig. 7(a) and (b) display the impact of the number of clients (|C|) on the accuracy of FL with SNNs without communica-tion compression, i.\u0435. \u043a = 1.0, and FLTS with compression rate \u043a = 0.5, respectively. First, as the number of clients increases, the accuracy curves degrade in uncompressed and compressed scenarios. This observation is expected as it takes more global aggregation rounds in FL to fuse the information in a network with a larger number of clients. In addition, our proposed compression method incurs higher variability in"}, {"title": "D. Effect of the Final Compression Rate win FLDR", "content": "This section evaluates the effect of the final compression rate w in FLDR. The highest accuracy is obtained by finding the maximum accuracy value within 100 global aggregation rounds. The motivation for this experiment is to understand the accuracy pattern when attempting to further reduce bandwidth usage by lowering w. As observed in TABLE I, we find no significant accuracy difference for linear reduction scheme under various \u03c9 \u2208 {0.01,0.001,0.0001}. Similarly, there is no substantial difference in total bandwidth consumption when varying the w value. We conduct another experiment by varying win FLDR with exponential reduction. Different from FLDR-L, we observe that the accuracy trend of FLDR-E becomes more and more destabilized in exponential reduction as the w value declines. The main reason for this observation is that the compression rate decline of the first few rounds in FLDR-E is much steeper compared to that of FLDR-L. Furthermore, we observe that the total bandwidth usage is reduced significantly in lower w settings. These observations illustrate the capability of FLDR to maintain high model ac-curacy with significantly lower communication costs, making them promising solutions for FL scenarios where bandwidth availability is extremely limited."}, {"title": "E. Comparison of FLTS and FLDR Algorithms", "content": "In this experiment, we compare our proposed FLTS and FLDR algorithms to investigate their characteristics further. For FLDR, two schemes are being evaluated that are based on equations (7) and (8), respectively. They are denoted as FLDR-L and FLDR-E. To ensure a fair comparison, kis set to 0.06 for FLTS, and we apply reduction parameters {\u03b1 = 0.06,\u03c9 = 0.01} for both FLDR-L and FLDR-E. As shown in Fig. 8(a), FLDR-L and FLDR-E are slightly"}, {"title": "VI. RELATED WORK", "content": "Robustness of SNN. Patel et al. [12] investigate noise impacts on SNN models when noise is injected into the input and the training process. Kundu et al. [16] develop an SNN training algorithm that uses crafted input noise in order to harness the model's robustness to gradient-based attacks. Ma et al. [17] further extend this line of efforts by proposing a Noisy Spiking Neural Network (NSNN) paradigm, inten-tionally incorporating noise to improve the model's robust-ness against adversarial attacks and challenging perturbations. These studies reveal the effectiveness and resiliency of SNNs to noise added to stages of model training. However, they do not target the FL context as investigated in our work.\nFL with SNN. Tumpa et al. [18] evaluate the performance of FL with SNN in heterogeneous systems. Following this line of research, recent work in [19]\u2013[21] demonstrates the efficiency of SNN with various FL environments and appli-cations. However, these works mainly focus on leveraging the energy efficiency of SNN in FL systems. Venkatesha et al. [4] demonstrate the robustness of SNN in FL applications, including straggling, model dropout, and gradient noise. In this work, we further examine the robustness of SNN and design FL sparsification algorithms for SNN training to reduce communication costs while maintaining model accuracy.\nCommunication Compression in FL. Various methods based on compressing the model updates have been proposed to overcome the insufficient bandwidth issues in FL systems [22], [23]. However, these works are based on the conventional ANN models. In contrast, several works have been conducted to explore the communication efficiency in FL systems with SNNs and provide insights into the trade-offs between com-munication load and accuracy [24]\u2013[26]. However, they do not consider noisy communication in practical FL systems."}, {"title": "VII. CONCLUSION", "content": "In this article, we presented a comprehensive study on the enhancement of communication efficiency in Federated Learn-ing (FL) using Spiking Neural Networks (SNNs). In light of the inherent robustness of SNNs to noise, we designed \u0422\u043e\u0440-\u043a Sparsification based schemes to reduce communication cost in FL without compromising model accuracy. The empirical results demonstrated that FL with SNNs saves significantly more bandwidth than their ANNs counterparts under noisy communication. Specifically, under the effects of noise and compression in communication, SNNs can still achieve high model accuracy, while the training for ANNs may fail to converge. Our research findings lay the foundation for further exploring the characteristics of SNNs as an alternative to ANNs to improve network efficiency in FL."}]}