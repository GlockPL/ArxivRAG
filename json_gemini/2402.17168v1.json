{"title": "Benchmarking Data Science Agents", "authors": ["Yuge Zhang", "Qiyang Jiang", "Xingyu Han", "Nan Chen", "Yuqing Yang", "Kan Ren"], "abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval \u2013 a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.", "sections": [{"title": "1 Introduction", "content": "Data science has become significant, as it helps individuals and organizations make informed decisions, predict trends, and improve processes by analyzing large volumes of data. Research on this topic continues to advance the field, driving innovations in machine learning, artificial intelligence, and big data analytics, thus enhancing its impact across various industries. However, data science requires extensive knowledge about analytical toolkits (e.g., NumPy and Pandas) and professional expertise to conduct analysis and correctly draw insights from data, which is challenging even for specialists.\nRecent advancements in Large Language Model (LLM) (Brown et al., 2020; Touvron et al., 2023) and LLM-powered agents (Shen et al., 2023) have shown considerable potential in enhancing human capabilities in data science. For instance, Code Interpreter allows ChatGPT to perform data analysis and visualization by creating a sandboxed Python interpreter within the platform. Copilots integrated with Microsoft Excel and PowerBI+ assist users in exploring and understanding data and finding insights. Similar initiatives have also emerged in the open-source community, such as Jupyter AI (jupyterlab, 2023), Chapyter (chapyter, 2023), and CoML (Zhang et al., 2023a).\nThe tools mentioned are part of an emerging category of software known as data science agents, capable of executing a wide array of data-centric tasks, including manipulation, aggregation, visualization, and analysis, through natural language commands. These agents primarily utilize LLMs to produce and implement code within designated data science platforms, such as Excel. Essential to their operation is the ability to comprehend the context of data and files in the ongoing session, along with the capability to verify and amend outputs as necessary, as discussed in studies (Cheng et al., 2023; Zhang et al., 2023b; Tu et al., 2023; Chen"}, {"title": "2 Related Works", "content": "Evaluating Code Generation Models. The field of LLMs (Brown et al., 2020) has seen rapid progress, with many capable models that can generate high-quality natural language and codes for various domains and tasks (Chen et al., 2021; Roziere et al., 2023). Benchmarks for these models (Chen et al., 2021) have also emerged. Some of them are specifically designed for the data science domain, such as PandasEval / NumpyEval (Zan et al., 2022), DSP (Chandel et al., 2022) and DS-1000 (Lai et al., 2023). However, what these benchmarks provided were pre-written prompts, mainly for a fair comparison of completion of in-filling abilities of LLMs themselves. They do not fully evaluate the skills of"}, {"title": "3 DSEval: Evaluation Paradigm for Data Science Agents", "content": "To comprehensively and reliably evaluate a data science agent, we must first identify the evaluation scope, i.e., the \"lifecycle\" of an agent (\u00a7 3.1). Then we propose a paradigm that monitors the full lifecycle for complete assessments (\u00a7 3.2)."}, {"title": "3.1 Evaluation Scope", "content": "We argue that a robust data science agent depends not solely on the LLM capabilities, but also on the design of its other constituent components. To identify the necessary scope for a comprehensive evaluation, we must first have a holistic perspective on the agent's lifecycle.\nThe lifecycle is depicted in the left part of Figure 3. First, the agent receives a \u201cquery\u201d (expressed in natural language). Then it retrieves some additional contexts from a stateful \"runtime session\", which is usually hosted by a data analysis platform (e.g., Jupyter), containing information like variables, execution history, files, etc. A LLM-powered code generator then produces a code snippet based on the query and context. The code is sent back to the runtime session for execution to get the result. Optionally, a code fixer can help with error diagnosis and quality improvement (as done in tools like genais). The lifecycle can repeat itself for multi-rounds, with the runtime session keeping track of the conversation and execution history.\nOur evaluation focuses on the holistic behavior of the data science agent, excluding implementation details of internal components such as code generators. We design benchmarks with queries and runtime sessions as the only inputs, which essentially differs from existing code generation benchmarks (Chen et al., 2021; Zan et al., 2022)."}, {"title": "3.2 Full-Lifecycle Monitoring", "content": "The holistic view of the agent lifecycle also makes us rethink the evaluation paradigm, and we conclude that \"every step and component involved in the lifecycle must be continuously monitored\u201d. For instance, imagine a query requiring in-place dataset modifications. Here, validating the runtime session is crucial to confirm the accurate execution. Hence, we design a validator module that is able to monitor the generated code, execution result, runtime session, etc. Meanwhile, the validator leverages an oracle agent equipped with a reference code snippet, provided by benchmarks for comparison. The process is illustrated in the right part of Figure 3.\nThe validator implementations within the validator module are fully modular, with each implementation focusing on a specific phase (e.g., data matching with fuzzy order, or evaluating trained model performance on a held-out test dataset). The full list and their usage frequencies are in \u00a7 5 and Appendix A. Notably, our focus is beyond correctness. For example, we implement an \u201cIntact\" validator, which tests whether the agent preserves the \"intactness\" of the session. We implement this due"}, {"title": "4 Benchmarks based on DSEval", "content": "Building upon the DSEval evaluation paradigm, we initiated the data collection and benchmark development process. We came to realize that tremendous efforts were still required to properly rephrase queries, configure sessions, and adapt validators for each query. Simple format conversion proved insufficient due to limitations in existing data sources: some data sources lack real-world complexity (e.g., pandas-exercises (guipsamora, 2020)), while others address different-natured tasks (e.g., PandasEval (Zan et al., 2022)).\nTo ensure the benchmark coverage with limited human efforts, we developed an \u201cLLM-bootstrapping annotation process\", leveraging LLMs to automatically create problemsets based on a minimal \u201cidea\u201d, while incorporating human input. This process is facilitated by the DSEAL (DSEval Annotation Language), which is designed to be compatible with the DSEval framework and easily comprehensible to LLMs. In this section, we first introduce DSEAL (\u00a7 4.1), followed by a detailed description of the annotation process, including a Kaggle-inspired case study (\u00a7 4.2)."}, {"title": "4.1 DSEAL: DSEval Annotation Language", "content": "DSEAL is essentially a language to describe \"problems\". A problem in DSEAL corresponds to one iteration depicted in Figure 3, where a query is presented, and agents solve it and return results. We define a \u201cproblemset\u201d as a sequence of interdependent problems, where later problems may have session or semantic dependencies on preceding ones. A benchmark comprises multiple \u201cproblemsets\u201d, each of which is self-contained and isolated.\nThe design of DSEAL is guided by three main objectives. Firstly, it must be compatible with the DSEval framework, ensuring that its components are expressive enough to fit within the framework. Secondly, it should be friendly to human annotators, for debuggability and ease of diagnosis. Lastly, it needs to be easily understandable by LLMs to leverage their power for annotation purposes.\nTo achieve these goals, we have designed DSEAL as an extended version of the Python language. Each problemset is represented as a Python (*.py) file, with problems separated by \"#%%\". The code for oracle agents is written in Python, enabling direct execution and debugging using standard Python SDK. We use triple-quoted strings with YAML syntax inside to \u201cconfigure\" the problem, including the query, validator configurations, execution restrictions, and external data required. An example is provided in Figure 4."}, {"title": "4.2 LLM-Bootstrapping Annotation Process", "content": "To alleviate human labor, we leverage the capability of LLMs to automatically annotate the benchmark as bootstrapping. However, fully depending on LLMs may derive unreliable benchmarks even with state-of-the-art LLMs (further details are provided in the case study). Therefore, we incorporate \u201chuman-in-the-loop\u201d to further enhance the annotation. The bootstrapping process involves an inner loop and an outer loop, as illustrated in Figure 5.\nInner Loop. To encourage LLMs to generate problemsets grounded in intended scenarios, we utilize \"idea seeds\u201d. These seeds anchor the generated problems to a specific scenario, promoting practicality and diversity across different outputs."}, {"title": "5 Statistics and Coverage", "content": "Based on DSEval, we employed the annotation process to construct four benchmarks, detailed in Table 1. These benchmarks encompass problem sets with diverse properties, ranging from straightforward tasks to more intricate challenges. More technical details about how we created those benchmarks are available in Appendix E.\nValidator Usages. Our evaluation process encompasses the entire lifecycle of data science agents. We employ a total of nine validators, each targeting distinct facets within the lifecycle. Details regarding their utilization are documented in Section A. Within our benchmarks, data science agents undergo validation through 6.5 validators per problem on average.\nProblem difficulty. For a better understanding of the performance across different difficulty levels, similar to previous studies (Yu et al., 2018), we quantify code complexity by considering the number of function calls, expressions, conditions, and loops in the reference code for each problem. The distribution of problem difficulties is depicted in Figure 7, with the average difficulty detailed in Table 1. We observe that DSEval-LeetCode poses the highest level of difficulty, while DSEval-Kaggle exhibits the most diverse range of difficulty levels.\nAPI coverage. Collectively, the four benchmarks covered 2240 API calls spanning 448 distinct APIs within the oracle code. These APIs are visualized in Figure 8a. Unsurprisingly, the most frequently utilized libraries are pandas, sklearn, and numpy. In total, 12 libraries are covered, with imblearn, nltk, statsmodels, and catboost being the least frequently employed. The most commonly occurring API is the [] operation of DataFrame, utilized for selecting indexes or columns.\nKnowledge points coverage. We use GPT-3.5 to summarize the data science knowledge points essential for solving each problem. As illustrated in the word cloud of Figure 9, the benchmarks focus on fundamental data processing concepts such as data transformation, aggregation, filtering, sorting, and grouping, as well as encompassing machine learning concepts like outlier detection, imbalanced dataset handling, and feature selection.\nProblem dependencies. DSEval-Kaggle and DSEval-Exercise are two conversational benchmarks where there could be interdependences among problems. We define \u201csession dependency", "semantic dependency\" as a situation where the comprehension of a later query relies on the context of a preceding query. We visualize dependency graphs for each problem set (see Figure 8b for example). On average, we observe an in-degree of 2.08 across all graphs. Regarding the maximum dependency chain length, the longest chain spans 10 dependencies, with an average chain length of 4.06.\nSession contexts. A major challenge in our proposed benchmark lies in retrieving and representing contexts from runtime sessions. On average, we estimate that each problem involves 3.68 variables, with a maximum of up to 29. The total data size of these variables is 1.12 kilobytes at the median, and can reach up to 268 megabytes in extreme cases.\"\n    },\n    {\n      \"title\": \"6 Evaluation\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"6.1 Setups\",\n      \"content\": \"Error Categories. When an agent fails to successfully respond to a problem, the errors in an agent-generated code snippet can be classified into eight major categories, which can be further broken down into 32 subcategories. The complete catalog is presented in Figure 10 and Appendix C. Two common errors are highlighted below:\n\u2022 Presentation Error: This occurs when the result is almost correct but problematic in terms of format or presentation approach. For example, the agent might fail to capitalize a column name as instructed or erroneously print results to the console instead of placing them in cell outputs.\n\u2022 Intact Violation: Happens when the solution is almost correct except for violating the concept of intactness. This typically occurs when the computation requires some intermediate columns and the agent modifies the original data, which is unnecessary.\nMetrics. The \\\"Pass Rate\\\", which is the number of problems passed divided by all problems in the benchmark, is the default metric used to assess the quality of an agent. By default, the runtime session is set to the ground-truth state before evaluating each problem. We refer to \\\"error propagation\\\" as a special setting where erroneous states accumulate to affect future problems within the same problem set. Additionally, we compute the pass rate while ignoring intact violations and presentation errors (\u201cw/o Intact": "nd \u201cw/o PE\u201d respectively), as they can be considered correct in a looser setting."}, {"title": "6.2 Evaluating Data Science Agents", "content": "We evaluate five popular LLM-based agents that are currently applicable to data science scenarios: Chapyter, ChatDev, CoML, Code Interpreter API, and Jupyter-AI (summarized in \u00a7 B.1). These selected agents cover mainstream agent-building approaches, including function calls, expert knowledge, and multi-agent communications. For fair comparisons, we use GPT-3.5 (v1106) with a temperature of 0 as backend LLMs for all agents.\nThe key observations from Table 2 are as follows: (i) Chapyter is the worst-performing agent, but its pass rate significantly improves when presentation errors are ignored. (ii) CoML is the best for most benchmarks, except for LeetCode, where Jupyter-AI outperforms greatly. (iii) When errors propagate, Chapyter and Jupyter-AI suffer greatly, yet the other two frameworks remain stable. (iv) Intact violations sometimes occur, but not frequently.\nTo gain a better understanding of these error types, we did several case studies and visualized the percentages of error causes in Table 2 in Figure 10. We can see that the primary issue with Chapyter is missing returns (e.g., using \"print()\" instead of \"return\" to show the output) and key errors (e.g., referencing non-existing columns). Code Interpreter API on DSEval-SO often triggers intact violations, as the framework has a tendency to perform inplace modifications to existing variables."}, {"title": "6.3 Context Selection and Representations", "content": "A key distinction among the agent frameworks lies in how they select and represent contexts from the sessions. Contexts are crucial for agents as they complement the missing parts of the query. Under the scope of our benchmarks, contexts are roughly categorized into variable descriptions and executed code history.\nWe conduct experiments with different combinations and orders of variable descriptions, code histories, and queries. We pick CoML as the baseline agent framework as it appears to be the best-performing one in previous experiments. The results are shown in Table 3. We observe that without any context, LLMs struggle to produce correct results. Providing code history and variable descriptions as context improve performance of agents. Code history seems to be more essential, especially for simpler tasks like DSEval-Exercise. The order of the context also has a slight impact: placing variable descriptions and queries at the end of the input tends to improve the results.\nEncoding the context into the prompt poses another challenge. Previous work (Sui et al., 2024) has explored this issue and proposed different methods to compress megabytes of data into dozens of tokens. We evaluate the approaches used in LIDA (Dibia, 2023) and CoML, with differences"}, {"title": "6.4 Evaluating LLMs", "content": "We experimentally combine CoML with different LLMs and compare their performance. The results are shown in Figure 12. In addition to GPT-3.5, which we have already tried, we include four more models for comparison: GPT-4 (OpenAI, 2023), Gemini-Pro (Team et al., 2023), CodeLlama-7B (Roziere et al., 2023), and CodeLlama-34B. The rank of the models is approximately as follows: CodeLlama-7B \u2248 CodeLlama-34B < Gemini-Pro < GPT-3.5 < GPT-4. More details are in \u00a7 B.4."}, {"title": "6.5 Self Repair", "content": "To evaluate the diagnostic and self-repair abilities of data science agents, we apply the self-debug (Chen et al., 2023) to the DSEval benchmarks. We use the CoML implementation, which sends the output and errors to LLMs for line-by-line analysis and feedback, before receiving a revised code. We do not use any hints from validators during this process. It repeats until we obtain a correct result or reach the maximum number of attempts. We also compare self-debug with a simple resampling baseline, which resamples a new code snippet if the previous one is incorrect.\nFigure 13 shows two main findings. First, both self-debug and resampling enhance performance, but self-debug is generally more effective. Second, models with lower capabilities (e.g., GPT-3.5) can outperform models with higher capabilities (e.g., GPT-4) with enough self-repair attempts.\nWe also analyzed the error types that can be fixed via self-repair on DSEval-Kaggle and found that around half of them are \u201cCrash"}, {"title": "7 Conclusion", "content": "In this paper, we introduce DSEval, an evaluation paradigm for data science agents. Based on DSEval, we created 4 benchmarks that cover different aspects of data science tasks, and existing agents were evaluated and analyzed on the benchmarks."}, {"title": "8 Ethical Considerations", "content": "Modern data science agents have made it easier to analyze, visualize and process data. However, such agents can also pose serious risks if they are not used carefully. For example, a data science agent can alter the data without the user's awareness, or generate a misleading data analysis that appears to be correct but is actually erroneous.\nOur work is the first to address these issues in a comprehensive way. For instance, we developed a validator that can track the full lifecycle of agent and assess whether the agent causes any unwanted changes (via \"Intact\u201d validator). We think future data science agents should follow our benchmarks as a reference, to ensure that they produce reliable and safe outcomes."}, {"title": "9 Limitations", "content": "Evaluating Planning Ability. The goal of planning is to break down a complex task into several simple, executable tasks, which is a key skill of LLM agents (Shen et al., 2023; Wu et al., 2023). In this paper, we focus on evaluating data science agents' performance on single tasks. Although some tasks (especially those in DSEval-Kaggle) are very complex and require careful planning to solve, we did not include high-level data science tasks that are vague and open-ended, such as \u201cdesign a data pipeline that will win this Kaggle competition\". However, we think that DSEval framework can also support those tasks, as long as the evaluation criteria (i.e., validator) are properly defined and configured.\nReproducibility and Stableness. We conducted extensive evaluations and obtained some interesting insights, but unfortunately we could not repeat every experiment to check the reproducibility of each result due to the budget constraint. Instead, we focused more on evaluating different settings and benchmarks, which we believe are more informative. In Table 5, we verified some of the experiments by either repeating them, using a different model version, or changing a parameter (e.g., the temperature). We observed that the results are not very stable and can vary by up to \u00b12%. On DSEval-LeetCode, the variation is even more significant, probably because the benchmark only has 40 problems. However, we remark that we have published 4 benchmarks based on DSEval, multiple results on different benchmarks can still have some significance. We encourage the community to take the average of multiple runs when possible.\"\n    }"}]}