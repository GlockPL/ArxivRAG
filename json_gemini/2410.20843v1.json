{"title": "Generative Simulations of The Solar Corona Evolution With Denoising Diffusion : Proof of Concept", "authors": ["Gr\u00e9goire Francisco", "Francesco Pio Ramunno", "Manolis K. Georgoulis", "Jo\u00e3o Fernandes", "Teresa Barata", "Dario Del Moro"], "abstract": "The solar magnetized corona is responsible for various manifestations with a space weather impact, such as flares, coronal mass ejections (CMEs) and, naturally, the solar wind. Modeling the corona's dynamics and evolution is therefore critical for improving our ability to predict space weather In this work, we demonstrate that generative deep learning methods, such as Denoising Diffusion Probabilistic Models (DDPM), can be successfully applied to simulate future evolutions of the corona as observed in Extreme Ultraviolet (EUV) wavelengths. Our model takes a 12-hour video of an Active Region (AR) as input and simulate the potential evolution of the AR over the subsequent 12 hours, with a time-resolution of two hours. We propose a light UNet backbone architecture adapted to our problem by adding 1D temporal convolutions after each classical 2D spatial ones, and spatio-temporal attention in the bottleneck part. The model not only produce visually realistic outputs but also captures the inherent stochasticity of the system's evolution. Notably, the simulations enable the generation of reliable confidence intervals for key predictive metrics such as the EUV peak flux and fluence of the ARs, paving the way for probabilistic and interpretable space weather forecasting. Future studies will focus on shorter forecasting horizons with increased spatial and temporal resolution, aiming at reducing the uncertainty of the simulations and providing practical applications for space weather forecasting. The code used for this study is available at the following link: https://github.com/gfrancisco20/video_diffusion.", "sections": [{"title": "1 Introduction", "content": "Key physical processes with a space weather impact occur within the solar corona. One example are the magnetic reconnection episodes resulting in solar flares (Priest et Forbes (2002)). Another one is the formation of coronal holes, known to be a determinant driver in the production of fast solar winds (Cranmer (2002), Wang (2024)). Reliable modeling of the solar corona's evolution is thus crucial for enhancing space weather forecasting capabilities. To that end, a significant body of research focuses on the extrapolation of coronal magnetic fields from photospheric magnetograms. To do so, many approaches leverage the fact that the dominant forces are magnetic and thereby model the magnetohydrodynamic (MHD) system as a magnetostatic one by neglecting all other forces (Wiegelmann et Sakurai (2012)). However, such assumptions are less realistic at higher layers of the solar atmosphere, and the highly non-linear nature of coronal MHD processes like flares (Shibata et Magara (2011)), combined with the limitations of available boundary conditions, makes these models and other non-force-free ones computationally expensive and impractical for long-term forecasts. Moreover, the stochastic emergence of new magnetic flux from the solar interior adds further complexity to these models (Cheung et Isobe (2014)).\nIn this work, we explore the potential of generative deep learning methods to efficiently model such complex systems. Generative latent variable models have demonstrated their strong abilities in learning the mapping of high-dimensional data distributions (e.g., images or videos) onto latent probabilistic spaces, allowing for realistic sampling and extrapolation (Asperti et Tonelli (2023)). Among such methods Variational Auto Encoder (VAE)s (Kingma et Welling (2013)) offer computational efficiency but tend to struggle with high-frequencies and small-scale details. Generative Adversarial Network (GAN)s (Goodfellow et al. (2014)) partially address these shortcomings by producing higher-quality samples, but are difficult to train and often suffer from mode collapse, leading to poor diversity in generated data. Recently, Denoising Diffusion Probabilistic Model (DDPM)s"}, {"title": "2 Model", "content": null}, {"title": "2.1 Background", "content": "Denoising Diffusion Probabilistic Model (DDPM)\nA diffusion model (Sohl-Dickstein et al. (2015)) $p_\\theta$ is defined as a reverse process modelling a variable $X_0$ as the $T$-th state of a Markov Chain $(X_{T-t})_{t=0}^T$ of transitions defined"}, {"title": null, "content": "such as :\n$p_{\\theta}(X_T) := N(X_T; 0, I)$\n$p_{\\theta}(X_{t-1}|X_t) := N(X_{t-1}; \\mu_{\\theta}(X_t, t), \\Sigma_{\\theta}(X_t, t))$\nIn the DDPM case, the unknown transitions $p_{\\theta}(X_{t-1}|X_t)$ are modeled as the reverse of a forward diffusion of Gaussian transitions. This forward process $q$ gradually adds noise to $X_0$, following a scheduled variance $(\\beta_t)_{t=1}^T$, that results in equilibrium at $T$, thus reaching the pure noise state $X_T$ :\n$q(X_t|X_{t-1}) := N(X_t; \\sqrt{1 - \\beta_t}, \\beta_t I)$\nThis diffusion process can be resolved at any arbitrary timestep $t$, defining $\\alpha_t := 1 - \\beta_t$ and $\\hat{\\alpha}_t := \\Pi_{t'=0}^t \\alpha_{t'}$, with :\n$q(X_t|X_0) := N(X_t; \\sqrt{\\hat{\\alpha}_t} X_0, (1 - \\hat{\\alpha}_t) I)$\nAn autoencoder $\\epsilon_{\\theta}$ can then be trained to learn predicting a random noise $\\epsilon \\sim N(0, I)$, added during the diffusion process at timesteps $t$ following the Equation 3, i.e. $x_t := \\sqrt{\\hat{\\alpha}_t}x_0 + \\sqrt{1 - \\hat{\\alpha}_t}\\epsilon$. Specifically, the training is performed by minimising a distance - typically L2 - between the actual noise and its autoencoder's estimations $\\epsilon_{\\theta}(x_t, t)$:\n$\\min ||\\epsilon - \\epsilon_{\\theta}(x_t, t)||_2, \\forall t \\in [1, T]$\nOnce $\\epsilon_{\\theta}$ trained, the predicted noise can be used to approximate $\\mu_{\\theta}(X_t, t)$, the mean of the reverse process, as derived in Ho et al. (2020):\n$\\mu_{\\theta}(X_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(X_t - \\frac{\\beta_t}{\\sqrt{1 - \\hat{\\alpha}_t}} \\epsilon_{\\theta}(x, t))$\nThe variance $\\Sigma_{\\theta}(X_t, t)$ can be fixed to $\\beta_t$ to match the forward process's variance schedule. Using these values in Equation 1, new samples $x_0$ can be generated by iteratively reversing the diffusion process, starting from pure random noises $x_T \\sim N(0, I)$. The computational cost of this sampling procedure depends on the noise schedule $(\\beta_t)_{t=1}^T$, which must be chosen so that the Signal-To-Noise-Ratio (SNR) becomes null in T, i.e. $SNR_T = \\frac{\\hat{\\alpha}_T}{1 - \\hat{\\alpha}_T} \\approx 0$, in order for $X_T$ to be pure noise. In the case of the linear noise schedule proposed by Ho et al. (2020), such condition start being satisfied from T \u2248 1000 as $SNR_{1000} \u2248 4e - 05$.\nConditional Diffusion\nFinally, all the equations of this section can be extended to handle conditional diffusion by marginalizing the probability distributions over any conditioning variable $c$. In our case the conditions $c$ represents the previous 12 hours of input data. These conditions are projected by the autoencoder into a continuous latent variable space, allowing the model to generalize to unseen conditions during training and generating predictions for new video sequences. Specifically, for any $c$ representing 12 hours of an active region at 2 hours resolution, our models learn to simulate possible realisations of the next 12 hours $X_0$, with a latent space approximating the conditional transitions:\n$p_{\\theta}(X_{t-1}|X_t, c) := N(X_{t-1}; \\mu_{\\theta}(X_t, t, c), \\Sigma_{\\theta}(X_t, t))$"}, {"title": "2.2 Light Video UNet", "content": "For the autoencoder, we use a modified version of the Palette's UNet architecture (Saharia et al. (2021)) also used by Ramunno et al. (2024b) for magnetogram-to-magnetogram forecasting. In our case, to adapt the model to videos and improve its ability to capture temporal patterns, we add temporal 1D-convolutions after each spatial 2D-convolutions. This approach provides spatio-temporal feature learning abilities similar to 3D-convolutions but with lower computational complexity as the complexity of a 2D+1D convolutions block is O(n\u00b2 + n) against O(n\u00b3) for 3D-convolutions, with n, the size of the kernels. For more complex patterns, we incorporate spatio-temporal self-attention (Vaswani et al. (2017)) blocks between each convolution block in the bottleneck. As the feature-maps size is smallest in the bottleneck - 16x16 in our model -, this placement allows for attention scores computation at the smallest possible complexity."}, {"title": "3 Dataset", "content": "For the of AIA Extreme Ultraviolet (EUV) images we start from the SDO-2H-ML dataset that we defined in Francisco et al. (2024). Specifically, the images are aligned, exposure normalised, instrument degradation corrected, downsampled at 1024 by 1024 pixels and available at a 2 hour resolution. Their pixel values are logarithmically scaled and depth-downsampled to 8-bit, which allow to preserve most of the original pixel distribution with marginal information loss, while making the dataset more compact. In this work we use the resulting images before the original JPEG compression used in Francisco et al. (2024). As we seek to test the model's ability to learn and simulate Active Region (AR)s' dynamics, while limiting the impact of the solar rotation and projection effects, we focus on crops of 614 by 614 arcsec centered around known SHARPs (Bobra et al. (2014)) that are within \u00b1230 arcsec from the solar disk center. The important size of our crops, relatively to original SHARPs bounding boxes, is meant to ensure capturing all the coronal loops that can be associated to a given AR. On the other end, this approach imposes us a strict chronological split between training and test data, as some of our crops may partially overlap over each other. The resulting crops are further downsampled to 128 by 128 pixels for computational reasons. Samples are created at a 2 hour cadence when possible by pairing the crops of a given AR within [-10h,0h] for the input video, with the crops within [+2h, +12h] for the targets, resulting in pairs of 6 frames videos. While our target is only the 94\u00c5 wavelenght, we add the 193\u00c5 and 211\u00c5 wavelength in the input to benefit from more information on coronal dynamics at different temperatures. We train on the resulting samples between 2010-05 and 2022-03, which amount to a total of 34000 paired videos. We test on samples spanning from 2022-05 to 2023-04 that are buffered by at least one month from the training data. Due to our spatial and temporal constraints, our test samples do not exhibit any X-flares, and only 41 samples exhibit M-flares. To build a a balanced test, we randomly select 41 C-flares, and 41 samples without any flare above the C-threshold, such sample will be referred"}, {"title": null, "content": "a quiet samples. This results in a small test of 123 samples that allow to estimate our probabilistic model performances at a moderate cost."}, {"title": "4 Results", "content": null}, {"title": "4.1 Computer vision metrics", "content": "To evaluate the quality of the generated videos, we propose using a set of classical computer vision metrics: Peak Signal-to-Noise Ratio (Peak Signal To Noise Ratio (PSNR)), Structural Similarity Index (Structural Similarity Index Measure (SSIM)), and Learned Perceptual Image Patch Similarity (Learned Perceptual Image Patch Similarity (LPIPS)). Although these metrics are typically designed for image comparison, we apply them frame-wise and calculate their averages to derive an overall score for the video. The PSNR compares the pixel-wise mean squared error to the maximum possible value of the signal, normalizing the distortion of the generated videos by the potential dynamic range. In the case of Extreme Ultraviolet (EUV) observations, the signal can vary across several orders of magnitude, corresponding to different physical processes. To more accurately assess the model's ability to preserve the underlying physics, we apply this metric to the descaled pixel distribution, reverting it to its original dynamic range. Consequently, the resulting PSNR scores appear much higher than those typically computed over standard 8-bit images. For context, we refer to Chervyakov et al. (2020), which derived a threshold formula indicating that a PSNR value above $Q = 5 * log(max_{signal-value})/log(2)$ suggests low distortion and high-quality images. With our saturation value of 6099 DN/s for the 94 \u00c5 EUV wavelength, this roughly corresponds to Q ~ 63. However, pixel-wise metrics like PSNR may not be the most relevant in our case. Given the non-deterministic nature of our problem, it is more suitable to compare the presence of similar structures in the images with slightly relaxed conditions regarding their pixel-wise shape and locations. To this end, the Structural Similarity Index (SSIM) is particularly relevant, as it evaluates a combination of indicators derived from the first and second-order moments of the pixel distribution over a small window - in our case, 11 by 11 pixels. Images are considered to have good similarity for SSIM values above 0.7. Similar to PSNR, we compute SSIM using the original (descaled) pixel dynamic range. Finally, for a more intuitive interpretation of perceptual similarity between the images, we use LPIPS (Zhang et al. (2018)) directly on the 8-bit generated videos, with values below 0.1 indicating good similarity."}, {"title": "4.2 Space weather related metrics", "content": "To further evaluate the model's ability to capture key physical quantities, we complement the previous metrics with physical metrics derived from the EUV flux measured within the frames. While an estimate of the actual EUV flux can be obtained using the corresponding instrument's response function (Boerner et al. (2012), Barnes et al. (2020)), we simplify our metrics by using the DN/s values as proxies for the flux. This simplification is particularly relevant, as van der Sande et al. (2022) demonstrated that DN/s values from AIA)observations can accurately approximate Soft X-Ray"}, {"title": null, "content": "(SXR) flux, which is of particular interest in space weather forecasting. Consequently, for each frame, our proxy for the corresponding region's flux is the sum of the pixel DN/s values. From the resulting flux time series, we focus on two essential derived quantities: the Maximum Peak Flux (MPF) and the fluence. The MPF is defined as the maximum value of the flux within the entire time series. It is particularly relevant as it provides an estimate of the maximum activity expected during the forecasted time window. This metric is well suited to the stochastic nature of the problem, as we anticipate some uncertainty regarding the precise timing of flare occurrences, especially when forecasting over several hours. To complement the MPF, we introduce the Time To Peak Flux (T2PF), which indicates the time remaining before reaching the MPF and poses a particularly challenging problem in flare forecasting (Boucheron et al. (2015)). The fluence is defined as the integral of the flux over the considered period, approximated by:\n$fluence = \\sum_t (A_t * flux_t)$\nwhere At corresponds to our 2-hour resolution converted to seconds. Due to our relatively low 2-hour time resolution, the resulting MPF and fluence values are not meaningful approximations of the actual physical values for the corresponding time windows. Therefore, these metrics are not yet intended for practical applications but serve as indicators of the model's ability to capture key physical quantities at a given resolution. Specifically, we focus on the model's Mean Absolute Percentage Error (MAPE) in MPF and fluence, as well as the Mean Absolute Error (MAE) in T2PF. Finally, we evaluate the model's ability to generate realistic trajectories of the flux time series by computing the Dynamic Time Warping (DTW) Euclidean distance (Piersol (1981)) between the generated time series and the ground truth. The DTW metric allows us to estimate the similarity between two time series while correcting for the impact of small delays or variations in speed. This feature enables better comparisons when the sequences may be out of phase, as is the case here. To enhance interpretability, we focus on the DTW distance normalized by the Euclidean norm of the observed time series."}, {"title": "4.3 Performances", "content": "The performance metrics of the models are presented in Table 1. For each sample 20 simulations are generated, and the final score per sample is calculated as the average score over all the simulations. The good scores in the first three columns, corresponding to the computer science metrics, suggest that the results are perceptually realistic. Both PSNR and LPIPS show slight degradation during periods when stronger flare events occurred within the forecasting window, indicating greater uncertainty at the pixel level and a lower perceptual similarity, respectively. On the other hand, the SSIM improves, suggesting that the model performs better at a structural level, as the metric compare the images similarity over small local windows of a few pixels. This apparent contradiction between the PSNR and SSIM may stem from the model's ability to predict accurately brightenings, such as flares, while exhibiting some uncertainty about their precise pixel-wise shape and location. During higher solar activity, such brightenings constitute a larger portion of the overall signal, leading to the increased"}, {"title": null, "content": "SSIM. The last four columns indicate that the model successfully captures essential physical properties. For instance, the mean absolute percentage error for both MPF and fluence is approximately 40%. When breaking down the performance by activity levels, the errors remain within the same order of magnitude of the observed values. While promising for practical space weather applications, it is noted that the low temporal resolution of this study smooth the flux time series, simplifying the task and rendering the model not suitable for operational space weather forecasting, as most flares' peak do not appear in the training and evaluation data.\nFrom the 20 simulations generated for each sample, estimates of the fluence, MPF, and T2PF distributions were derived. Their reliability are assessed with the reliability diagrams shown in Figure 1. The blue curves depict how frequently the ground truth falls within the corresponding Confidence Interval (CI). For a perfectly reliable probabilistic model, these curves would align with the diagonal (y = x), meaning that observations would fall within the p%-CIs p% of the time for any p in the range [0,100]. The red bars indicate the size of the CIs, reflecting the level of uncertainty at a given confidence level. For practical purposes, smaller uncertainties are preferable, with the ideal model having CIs as small as possible.\nFor all three metrics, the generated probability distributions appear reliable, with the blue curves aligning with the diagonal at a few point of percentage. This suggests that the model effectively captures the stochastic nature of coronal evolution, which arises from the system's inherent non-linearity and the stochastic emergence of new flux. The model's resulting stochasticity is then further emphasized by the limited input information, such as the low spatial and temporal resolutions. As a result, the uncertainty of the generated simulation is quiet large, with 96%-CIs reaching approximately \u00b155% of the actual observation for the fluence and the MPF, and \u00b14H for the T2PF."}, {"title": "4.4 Prediction examples", "content": "To illustrate the diversity of the model's predictions, we present three simulations for each of the following cases:\n\u2022 AR 8195 on 2022-05-04 at 20:00, shown in Figure 2\n\u2022 AR 8977 on 2023-01-18 at 06:00, shown in Figure 3\n\u2022 AR 9188 on 2023-03-08 at 22:00, shown in Figure 4\nFor each figure, the first row displays the sequence of frames from the 12 hours preceding the forecasting date. The second row corresponds to the actual frames for the next 12 hours, which the model aims to simulate. The third to fifth rows present three distinct simulations generated by the model. The sixth row shows the simulations' percentage deviation from the ground truth, averaged over 20 distinct simulations. The seventh and final row displays standard deviation maps, computed as the pixel-wise standard deviation across the same 20 simulations, highlighting the regions of the images where uncertainty is highest.\nComparing the mean percentage error across the three examples, the deviation from the ground truth appears balanced in the case of AR 8195 (Figure 2), while the simulations for"}, {"title": "5 Discussion & Conclusions", "content": "Despite the constraints of using a low temporal resolution (2 hours) and focusing only on ARs at the solar disk center, our current approach demonstrate promising potential. In particular, our findings suggest that DDPMs can be used to efficiently generate realistic evolutions of the solar corona as observed in EUV. Our probabilistic video forecasting model appears well-calibrated and reliable for deriving key space weather indicators. As seen in our prediction examples, the model is able to simulate complex MHD dynamics and forecast brightening events with time steps closely aligned to their actual occurrences. These encouraging outcomes suggest such models could represent a major milestone in the fields of Solar Physics and Space Weather. While current space weather forecasting abilities are sometimes compared to earth weather forecasting abilities of 50 years ago, these new methods could significantly reduce that gap, by providing computationally efficient simulation tools to visualize the possible trajectories of the system's evolution. Major limitations of this proof of concept must however be acknowledged. The low temporal resolution of 2 hours, statistically excludes most actual flare peaks from the training and evaluation data, making the resulting model not suited to model the occurrence of such event. The temporal and spatial restrictions of this work were intended solely to validate the concept with limited computational resources. A time resolution of 15 to 10 minutes could already allow to capture much better extreme transient fluctuations in variables of interest like the flux, resulting in improved flare predictions with reliable likelihood estimates and interpretable simulations of their occurrence. At a time resolution of 2 minutes or less, a DDPM might further model the physical dynamics of the corona at a much finer scale, potentially resulting in reduced uncertainty and promising application for solar physics studies. For instance, such models could be used to estimate the intrinsic stochasticity of flare occurrences given current instrument limitations in spatial and temporal resolution. Moving forward, future work could thus focus on generating forecasts for 6-hour periods at a 10-minute resolution and for 1-hour periods at a 2-minute resolution. Additionally, increasing the spatial resolution to 2.4 arcsec/pixel might also be worth exploring, while training the model at any longitudes for full-disk coverage will be indispensable for operational applications."}]}