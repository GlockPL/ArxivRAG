{"title": "P3S-Diffusion: A Selective Subject-driven Generation Framework via Point Supervision", "authors": ["Junjie Hu", "Shuyong Gao", "Lingyi Hong", "Qishan Wang", "Yuzhou Zhao", "Yan Wang", "Wenqiang Zhang"], "abstract": "Recent research in subject-driven generation increasingly emphasizes the importance of selective subject features. Nevertheless, accurately selecting the content in a given reference image still poses challenges, especially when selecting the similar subjects in an image (e.g., two different dogs). Some methods attempt to use text prompts or pixel masks to isolate specific elements. However, text prompts often fall short in precisely describing specific content, and pixel masks are often expensive. To address this, we introduce P3S-Diffusion a novel architecture designed for context-selected subject-driven generation via point supervision. P3S-Diffusion leverages minimal cost label (e.g., points) to generate subject-driven images. During fine-tuning, it can generate an expanded base mask from these points, obviating the need for additional segmentation models. The mask is employed for inpainting and aligning with subject representation. The P3S-Diffusion preserves fine features of the subjects through Multi-layers Condition Injection. Enhanced by the Attention Consistency Loss for improved training, extensive experiments demonstrate its excellent feature preservation and image generation capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently Text-to-Image (T2I) generative models demonstrate the ability to generate high-quality and text-related images. However, relying solely on textual descriptions cannot accurately generate the images we want. Even if using large language models (LLM) to expand text descriptions to detailed ones, it is still not possible to generate images that meet the requirement. Furthermore, inversion techniques [1] can help to find a text embedding but still can't accurately reconstruct the appearance of given subjects. So now there are many models [2] dedicated to utilizing more control conditions (e.g., image prompt, sketch, depth map, etc.) to guide T2I diffusion models.\nSubject-driven generation aims to generate realistic and fidelity-preserving images where the subject is the same as the reference image. Given a few images of a particular subject (3-5), models should accurately reconstruct the appearance of the given subjects using special prompts while retaining editability.\nIn this work, we present a new approach named as P3S-Diffusion, which generate Point Supervision Selective Subject-driven image for personalized generation using point labels to select target subject. Recent research [2], [3] have tried to utilize extra conditions (e.g., additional text prompts and mask images) to accurately select the subject in the input image. However, text prompts often fail to describe specific content accurately, and pixel masks are always expensive. Therefore, we weaken the selection criteria and make it possible to use only points to guide diffusion model in learning special subjects. A novel Attention Consistency Loss is proposed to enhance the correlation between text prompts and generated images.\nOur P3S-Diffusion consists of two parts:RNGR-Encoder and Multi-layers Condition Injection. We eliminate the confusion of similar subjects using RNGR-Encoder, a new image encoder that Reduces Negative information and Generates spatial bias Representations of the subject. RNGR-Encoder first utilizes CLIP [4] image encoder to get the patch-level relation of the subjects and reduce negative information. Furthermore, we propose a detailed subject condition injection, which utilizes a trainable copy of U-Net to inject multi-layers subject information. To strengthen editable generation capability, we propose the timestep-based weight scheduler, which can flexibly adjust the weight and trend of control condition injection to balance prompt consistency and identity preservation. We summarize our main contributions as follows:\n\u2022 We propose a novel framework, named as P3S-Diffusion for selective subject-driven image generation.Comparing to existing text prompts or mask supervision methods, our method requires the least additional control and can accurately select the target subject.It can be flexibly applied to diffusion models and compatible with ControlNets.\n\u2022 RNGR-Encoder and Multi-layers Condition Injection are proposed to select target subject and encode selected feature to original U-Net.We minimize the Attention"}, {"title": "II. RELATED WORK", "content": "Text-to-image diffusion models. In recent years, diffusion models [5], [6], [7] have developed rapidly due to their excellent image generation capabilities. The emergence of latent diffusion models has made it possible to quickly generate high-quality images and has achieved significant commercial success. StableDiffusion [7], DALL-E2 [8], Imagen [9] trained on large-scale image-text pair datasets, have become mainstream in text-to-image generation. DiTs [10] replaces the U-Net in the diffusion model with transformers, further improving the text-to-image generation performance and scalability. The latest diffusion models, such as StableDiffusion-XL, Playground v2 [11] can generate images with resolutions of 1024 \u00d7 1024 or even higher, and can accurately generate text in the images.\nControllable generation. In addition to using text prompts for conditional image generation, current diffusion models can add many additional inputs (e.g., edges, depth maps and segmentation maps) to achieve controllable image generation. ControlNet [2] utilize a trainable copy of the U-Net to integrate multimodal information. It freezes the original U-Net and only fine-tunes the trainable copy to apply control as a plugin. IP adapter [12] takes the reference images as feature inputs and achieves image-to-image generation by injecting cross-attention between the reference image and the denoising image into the source model. Other methods [13], [14], [15] use DDIM inversion [16] to generate noised images and realize image editing by manipulate the noised images.\nSubject-driven generation. Subject-driven image generation can be divided into two frameworks based on the methods: those that require test-time fine-tuning of the reference images and those that do not. For methods that require test-time fine-tuning, Text Inversion [1] uses text vectors as optimizable parameters to search for specific vectors in the text space to generate similar images. Dreambooth [17] fine-tunes the entire U-Net layers on reference images and proposes prior preservation loss to solve the language drift problem. Custom diffusion [18] and SVDiff [19] only fine-tune the most important parts of the U-Net, greatly reducing the number of parameters that need to be optimized. Some fine-tuning-free methods [20], [21], [12], [22], [23], [24], [3], [25] add additional adapters, using image encoder to encode reference images to embeddings as the subject representation. BLIP-Diffusion [24] can realize efficient zero-shot generation. SSR-Encoder [3] utilizes CLIP hidden state features to encode subject representation and can select target subject with text prompts or masks. ELITE [20] proposes global and local mapping networks for fast and accurate customized text-to-image generation. Instantbooth [25] uses learnable image encoder and encodes input images as textual token, learning rich visual feature representations by introducing a few adapter layers to the pre-trained model.\nHowever, the above methods cannot accurately select the subject concepts that need to be learned or require additional text descriptions or expensive pixel-level masks. Our method uses two points to label the positive and negative regions in the reference image, enabling selective subject generation with minimal annotation cost."}, {"title": "III. THE METHOD", "content": "Subject-driven image generation aims to generate target images with high fidelity and creative editability according to given reference images. Usually, the content in an image is diverse, and relying solely on the provided text description make it difficult to accurately select the subject we want to retain. Compared to additional text prompts or expensive pixel-level masks, we propose P3S-Diffusion, a specialized framework that can use a point to supervise model selection of corresponding features.\nFormally, for a given reference image I and a point p, the RNGR-Encoder effectively identifies the target subject and reduces negative information.The processed image I' is encoded to be a latent embedding and added to the original image embedding. The aligned feature is integrated into the original model with trainable copies of U-Net model layers. In the generation process, image embedding is added to the noise to give it a latent bias towards the subject. Enabling the model to have higher stability against different random noises and produce selected subjects with high fidelity. The overall methodology is illustrated in Fig. 2.\nIn general, our P3S-Diffusion based on text-to-image diffusion models [7]. It consists of two main parts: RNGR-Encoder (Sec. III-A) and Multi-layers Condition Injection (Sec. III-B).\nConsidering that when there are multiple semantically similar subjects (like two different dogs) in an image, the diffusion model trained on this image may not be able to distinguish the features of different entities. This results in confusion in subject representation, causing the diffusion model to mix the different subjects. To solve this problem, we need to obtain a mask for negative information and remove irrelevant subject information from the image. However, it is difficult to segment point-annotated images without using additional pre-trained segment models. Several works [18], [17] have proven that fine-tune a diffusion model with unique identifier will guide the model to focus on specific subjects without being influenced by the background. The minor inconsistencies and unrealistic parts in the training image will not affect the subject representation.\nThus, we only need to generate a rough mask to eliminate negative information and fill the mask to remove unrelated subjects. We utilize CLIP image encoder to encode the image to feature embedding and calculate the similarity between the patch where the point is located and all other patches. Mathematically, given a image I point coordinates p = {x,y}, we can calculate the patch similarity map $M_{\\{Negative,Positive\\}} \\in R^{NXN}$ as follows:\n$M_{\\{N,P\\}} = \\{M_{i,j}, M_{i,j} = Cos(patch_{x',y'}, patch_{i,j})\\}.$ (1)\nConsidering that CLIP is trained on class-level annotation, it fails to accurately recognize objects with different instances of the same class. We utilize a negative similarity map to modify the positive similarity map and perform Gaussian filtering to smooth the similarity map. We then use the Otsu method to binarize the similarity map and remove outliers that are not connected to the specified points:\n$Mask = Otsu(\\Psi(M_{P} * (1 \u2013 M_{N}))),$ (2)"}, {"title": "B. Subject-driven Generation", "content": "In our approach, the encoded latent feature is projected into spatial-transformer blocks in the original U-Net for controllable generation. To achieve this, we copy the layers of a simplified U-Net (with only one ResNet block in each block), each corresponding to a spatial-transformer layer in the original U-Net. Inspired by [18] [26], self-attention and cross-attention layers have the greatest impact on the generation results of the model. The injection is composed of two parts: self attention condition injection and cross-attention enhancement, as shown on Fig. 3.\nMulti-layers Condition Injection. Our approach creates a trainable copy of the simplified U-Net F(x;0). For each denoising step t, we compute the hidden state features in each self-attention layer. Mathematically, the hidden state features can be represented in the following form:\n$f_{i} = Z(F_{i}(x_{t}, c, t, \\theta_{i}), \\theta),$ (6)\nwhere f = {$f_{1}, f_{2}, ..., f_{n}$} is a set of the self-attention hidden features and n is the number of self attention layers. Different from Control-Net which add conditions to output blocks as residual connection, we deliver the hidden features to the self-attention layers in the original U-Net as follow:\n$Q = W_{Q}z_{i}, K' = W_{K}[z_{i}, f_{i}], V' = W_{V}[z_{i} + \\delta f_{i}, f_{i}],$ (7)\n$Attention = Softmax(\\frac{QK^{T}}{\\sqrt{d}})V',$ (8)\nwhere zi is the hidden state feature in the original U-Net. We concatenate and add the additional features to the original self-attention layers to strengthen the presentation of the selected subject. The layers are controlled by a hyperparameter \u03bb = 0.2.\nThis approach can not only preserves the detailed features in the reference images(such as dog's fur patterns, patterns on backpacks, etc.), but also make model pay more attention to the target subject during training, reducing conceptual confusion caused by the background. To accelerate convergence speed and reduce blurring in generated images, we also applied a zero convolution on the hidden features.\nPrior research [23], [27], [28] shows that in the early stages of the denoising process, a rough structural layout of the image will be formed. Applying control to the diffusion model too early can result in the generated image matching the reference image very well, but causing low diversity. Different from the delayed subject conditioning [23] in prior works [29], we propose a simple timestep-based weight scheduler, which allows for arbitrary changes in control intensity during the inference phase, striking a balance between identity preservation and editability. Specifically, we use a hyperparameter \u03b5 as the weight to perform conditional injection on the original U-Net, which uses only text prompts. Our timestep-based weight scheduler can be mathematically represented as:\n$\\epsilon_{t} = F(\\epsilon_{t} * f, x_{t}, c, t, \\theta),$ (9)\n$\\epsilon_{t} = 1 - \\alpha(\\frac{t}{T})^{k} + \\beta,$ (10)\nwhere et is the predicted noise on the time-step t. a, \u03b2, k control multiplier, exponential descent, and bias separately. Empirically, we set a = 0.5, \u03b2 = 0.2, k = 2 to balance prompt consistency and identity preservation.\nIn addition, we also explored other weight control methods in section IV-C."}, {"title": "Attention Consistency Loss", "content": "During the training phase, due to differences in input images, the cross-attention layers of the original model and the trainable copies may focus on different regions, leading to the model ignoring the detailed features of the reference image (such as color, pattern, etc.). To address this, we propose a novel Attention Consistency Loss Lac. This loss is designed to enhance similarity between the cross-attention map Me of the trainable copies and the one M. of the original U-Net. Specifically, we only choose the last cross-attention layer to calculate the MSE loss. Lac can be mathematically represented as:\n$L_{ac} = MSE(M_{c}, M_{o})/avg = Mean(||M_{c}, M_{o}||^{2})/avg,$ (11)\n$M_{cavg} = (Sum(M_{c}) + Sum(M_{o}))/2,$ (12)\n$M_{c} = Softmax(\\frac{QK^{T}}{\\sqrt{d}}), M_{o} = Softmax(\\frac{QK^{T}}{\\sqrt{d}}).$ (13)\nFollowing the Stable diffusion model [7], we also utilize a latent denoising loss LLDM to Minimize the KL divergence between the generated image distribution and the target image distribution, as demonstrated in Eq.14:\n$L_{LDM} = E_{x_{0},t,\\epsilon}[||\\epsilon \u2013 \\epsilon_{\\theta}(x_{t}, t, C_{t})||^{2}],$ (14)\nwhere xt is the noisy image at time step t, e is the added noise and ee is the predicted noise in the parameters 0.\nThus, our total loss can be presented as:\n$L = L_{LDM} + L_{ac},$ (15)\nwhere is set as a constant, empirically set to 0.1. Meanwhile, since the original model is frozen, the changes in its cross-attention map will be smaller than those of the trainable copies. Therefore, minimizing their cross-attention map consistency loss can also make the model focus more on the target subject, thereby alleviating semantic drift."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we present our experiments and training details. Our method can select the target subject in the reference image through two points and exclude negative subjects. Inspired by prior works [17], we also use a unique identifier [V] to represent the target subject."}, {"title": "A. Setup", "content": "Training Details. Our method is based on StableDiffusion v1-5 [7] model. To encode the visual inputs and calculate the patch similarity, we use OpenAI's clip-vit-large-patch14 vision model, which can also encode text prompt in SD v1-5. For every input classes, we train our models for 60 epochs on a single NVIDIA RTX3090 GPU, with a learning rate of 1e-5. The training process takes about 6 minutes. During inference, we use DDIM as the sampler, with a step size of 50 and a guidance scale set to 7.5. To realize Classifier-Free Guidance, we drop out 10% of the conditions."}, {"title": "B. Experiment Results", "content": "Qualitative results. Fig. 4 demonstrates the overall generated subject images. The result consists of two parts: single subject image generation and similar subject image generation. A large number of generated images have proven that our method can stably and effectively preserve the features of the target subject, and faithfully generate according to the given text prompts. Our method achieved stunning results in the single subject image generation. When generating subject images of similar subjects, relying on the supervision of positive and negative points, our method can accurately extract the features of the target subject without being affected by negative targets.\nQuantitative comparison. Overall, P3S-Diffusion basically outweighs previous method, especially in subject alignment including CLIP-I and DINOv2. Our method has a great advantage in preserving the target subject features and can also ensure the diversity of generation. In terms of the stability of generated images including CLIP-Iv and DINO-v, our method achieves the highest CLIP-Iv and the second highest DINO-v scores. This shows that our method is less affected by random numbers and can stably generate subject images. Additionally, high image fidelity and low variance also prove that our method can stably and effectively generate high-fidelity subject images.\nQualitative comparison. For the problem of being unable to identify similar subjects, as shown on Fig.1, our method can accurately extract the features of the target subject using two points and faithfully generate it according to the reference image and the given text. Other methods cannot distinguish different subjects in the reference image and will incorrectly generate images with mixed features or multiple subjects.\nIn summary, our method can utilize point supervision to ensure an accurate representation of the selected image subjects. Thereby addressing key challenges faced by other current methods."}, {"title": "C. Ablation study", "content": "Our ablation study explore the impact of different modules on the fidelity and prompt consistency of generated images, as show on Table II. We found that the RNGR-Encoder and Lac make the generated image more in line with the subject representation, but reduce editability. The timestep-based weight scheduler helps improve prompt consistency but reduce identity preservation. We have analyzed the reasons for these phenomena below.\nRNGR-Encoder. We conducted ablation study on RNGR-Encoder, Lac, and timestep-based weight scheduler. RNGR-Encoder can encode the hidden features of CLIP image encoder into the latent space representation of the image, and add a bias to the target image distribution, making its distribution distinct from other image distributions. This allows the model to generate more realistic images, and obtain a higher subject alignment score.\nAttention consistency loss. Attention consistency loss can align the attention of the trainable copy with the source model, allowing they to focus on the same area in the generated images. Additionally, Lac makes the attention of the trainable copy close to the attention of the frozen and slower-changing source model, alleviating the semantic drift. We only use the attention map of the last layer in the U-Net to calculate Lac.\nThis ablation study demonstrate that Lac is able to significantly improve image fidelity while sacrificing a small portion of generated diversity. Using it on the last layer of U-Net yields the best results. While Lac is applied to the whole U-Net, the increase in gradient will lead to slower convergence of the model and result in underfitting and difficulty in personalization generation.\nTimestep-based weight scheduler. Adjust the control weights between original U-Net and trainable copy will balance the prompt consistency and identity preservation as shown on Fig. 5. Besides using fixed weights, we also demonstrate the results using different weights at different time steps on Table III. The quadratic decreasing weight uses a smaller weight in the early stage of denoising, which ensures the diversity of the generated image structure and can obtain a higher diversity score. The increasing weight is the opposite.\nAs shown on Fig. 5,we find that as the control weight increases, the fidelity of the image continues to improve, but the editability decreases.We also try to utilize a trainable MLP to learn the optimal weight.However, it can only balance editability and fidelity.\nCompared to applying control to U-Net at all time steps t, our schedule can effectively control the weights of injected conditions under different time steps. By giving lower weights in the early denoising stage and higher weights in the middle and later stages, our method can effectively balance prompt consistency and identity preservation. Although the image fidelity is very high without using timestep-based weight scheduler, it can cause the background of the generated image to be very similar to the reference image, as shown on [23]."}, {"title": "V. CONCLUTION", "content": "In this paper, we introduced the P3S-Diffusion , a method of selective subject-driven generation via point supervision. In this method, we implemented a feature that was not present in previous methods: using only points to annotate the target subject allows the model to perform selective subject generation. Our method can effectively preserve the main features of the reference images and faithfully generate the images according to the given text prompts.\nHowever, our method also has some limitations. It is difficult to generate high-fidelity images of subjects with rich details. Additionally, our point supervision method does not perform well on non-salient images."}]}