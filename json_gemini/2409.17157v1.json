{"title": "Confident Teacher, Confident Student? A Novel User Study Design for Investigating the Didactic Potential of Explanations and their Impact on Uncertainty", "authors": ["Teodor Chiaburu", "Frank Hau\u00dfer", "Felix Bief\u00dfmann"], "abstract": "Evaluating the quality of explanations in Explainable Artificial Intelligence (XAI) is to this day a challenging problem, with ongoing debate in the research community. While some advocate for establishing standardized offline metrics, others emphasize the importance of human-in-the-loop (HIL) evaluation. Here we propose an experimental design to evaluate the potential of XAI in human-AI collaborative settings as well as the potential of XAI for didactics. In a user study with 1200 participants we investigate the impact of explanations on human performance on a challenging visual task - annotation of biological species in complex taxonomies. Our results demonstrate the potential of XAI in complex visual annotation tasks: users become more accurate in their annotations and demonstrate less uncertainty with AI assistance. The increase in accuracy was, however, not significantly different when users were shown the mere prediction of the model compared to when also providing an explanation. We also find negative effects of explanations: users tend to replicate the model's predictions more often when shown explanations, even when those predictions are wrong. When evaluating the didactic effects of explanations in collaborative human-AI settings, we find that users' annotations are not significantly better after performing annotation with AI assistance. This suggests that explanations in visual human-AI collaboration do not appear to induce lasting learning effects.", "sections": [{"title": "1 Introduction", "content": "XAI strives to bridge the gap between complex AI models and human users by providing explanations for the models' decisions. However, evaluating the effectiveness of XAI methods remains a challenging question[31,20,27,3,36,30]."}, {"title": "2 Related Work", "content": "The definition of a \"good\" explanation remains an ongoing debate, leading to a diverse range of HIL experiments focusing on various aspects and metrics. Comprehensive reviews of this field can be found in [33,15,37].\nPerhaps the most extensively explored facet of XAI research concerns the influence of explanations on human performance. Do explanations actually help users perform better? This can be investigated through two main lenses: human-AI collaboration and knowledge transfer.\nIn human-AI collaboration tasks, explanations can enhance user performance by effectively communicating the AI's reasoning. This allows users to better utilize the AI's suggestions, adjust their own decisions alongside the AI's input and potentially improve overall task efficiency through smoother collaboration. HIL experiments can shed light on these aspects by observing user behavior in such team-settings. The literature abounds in studies investigating scenarios where users work with AI assistants to solve various tasks. These studies often demonstrate that AI assistance improves human performance in tasks like sentiment analysis [34], poisonous mushroom classification [26], insulin dosage decisions for virtual patients [38] or prostate cancer classification in MRI scans [14].\nIn terms of long-term benefits of explanations, the question is whether explanations can empower users to learn from the AI and improve their independent performance on future tasks. Effective explanations might enhance user understanding of relevant rules and patterns used by the AI. This understanding could then translate to improved task execution without AI assistance, potentially promoting long-term learning that can be utilized for various tasks beyond"}, {"title": "3 Dataset and Classification Problem", "content": "For the experiments described in this work, a subset of the iNaturalist dataset [1] was used. A dataset of wild bee images was constructed by scraping 30k images from the online database https://www.inaturalist.org/. These images depicted the top 25 most frequent wild bee species native to Germany within their natural habitats. Following preliminary experiments, the dataset was refined to focus on three particularly challenging and frequently confused species: Andrena bicolor/flavipes/fulva (see Fig. 1). This refinement resulted in a final subset containing 657 wild bee images. For more details on the scraping process, data split and annotation, as well as training of the model a ResNet50 [17] - please consult [7] and our repository - https://github.com/TeodorChiaburu/beexplainable."}, {"title": "4 Experimental Design", "content": "Our experimental setup consists of three tasks - see Figure 2. The users are shown images of wild bees and are required to recognize the three species depicted in Fig. 1. In Task 1, subjects are left to assign on their own the correct label to the images they see. In Task 2, they are aided by a computer vision model trained to recognize wild bees. In the third and final task, the photos are again shown without any AI hints. Each task comprises 10 images. The reason for adding a second 'control task' (Task 3) after the AI-assisted Task 2 is to enable the investigation of the potential didactic effect that explanations may have. We hypothesize that, if explanations are able in a pedagogical sense to teach laypersons relevant classification rules, then the users' accuracy in Task 3 should be higher than in Task 1, when participants were just getting acquainted with the problem.\nThe 30 images that every participant sees throughout the trial are randomly drawn from a pool of 45 samples (selected from the test set). The pool is a mixture of 'easy' and 'hard' examples with respect to the model's confidence in classifying those samples. We label an image as 'hard' if the model assigned a true class probability below 80%. Inherently, some of the hard samples were misclassified by the network. When compiling the set of 30 images shown to a subject, we ensured that each task contains 5 easy samples and 5 hard ones.\nThe participants were informed that the data gathered would solely be used for research purposes. Their identities were anonymous and before starting they were given a detailed description of what they were required to do. For each possible class, a representative example was shown in the introduction, which they could refer back to any time during the trial. The experiment was approved by our institutional research board.\nUsers were divided into 6 groups that differed from one another in the type of AI hint revealed in the second task (see Figure 2):\n1.  Control Group: the AI hint consists solely of the model's predicted class (which can be wrong)\n2.  Control-Confidence Group: the model's prediction is accompanied by the corresponding Softmax probability (model confidence)\n3.  Concepts-CoProNN Group: the model's prediction and confidence are shown together with an explanation computed by the concept-based XAI method CoProNN [7]. The explanation is visualized in a 'traffic-lights' format, where a representative patch of the concepts learned by the XAI method is marked as relevant (green) or not (red).\n4.  Concepts-TCAV Group: the model's prediction and confidence are shown together with an explanation computed by the concept-based XAI method TCAV [21]. The explanation modality is the same as for CoProNN.\n5.  Examples-GradSim Group: the model's prediction and confidence are shown together with an explanation computed by the example-based method Gradient Similarity [6]. The explanation is visualized in the form of the top 3 most similar samples from the training set that were classified similarly."}, {"title": "5 Results and Discussion", "content": "We summarize below the insights we gained from our user study. Figure 3 offers a broad overview of the subjects' performance throughout the three tasks and the six groups, while the correlation plots in Figures 4 and 5 display the relation between the model's and the users' performance."}, {"title": "5.1 No Observable Didactic Effects Detected", "content": "Our experiment did not reveal any considerable evidence of long-term learning or knowledge transfer from explanations to independent task performance. This is indicated by the comparable user accuracy observed in Task 3 (where users classified images on their own again) compared to Task 1 (initial independent classification). Across all six groups, both average and median accuracy scores in Task 3 remained similar to those in Task 1 (Figure 3). This suggests that while explanations may improve performance during collaboration with the AI (as shown in Subsection 5.3), they may not necessarily equip users with the ability to retain that knowledge and apply it to solve similar tasks independently over time. Tiredness and cognitive load may also play a role once users arrive at the final Task 3."}, {"title": "5.2 Users' Uncertainty Decreases when Collaborating with an AI Assistant", "content": "Our study also revealed a positive impact on user confidence when collaborating with an AI assistant. This is reflected in the user uncertainty levels observed across the different tasks and computed as described at the end of Section 4. User uncertainty in Task 2, where participants received help from our model, was considerably lower compared to the uncertainty levels observed in Tasks 1 and 3 (Table 1 and Figure 5). This suggests that hints provided by the AI assistant helped users feel more certain about their classifications in Task 2 and allowed them to approach the task with greater confidence."}, {"title": "5.3 Human Performance Improves when Collaborating with an AI Assistant", "content": "Our findings demonstrate that human performance improves when collaborating with an AI assistant. Across all user groups participating in Task 2, both average and median user accuracy scores are consistently higher compared to Tasks 1 and 3 (as illustrated in Figure 3). Overall, user performance in Task 2 is generally superior to that observed in Tasks 1 and 3, particularly for samples where the model exhibited a high degree of certainty (Figure 4). This indicates that explanations and AI assistance were most beneficial for tasks where the model was most certain, potentially aiding users in making more accurate classifications."}, {"title": "5.4 Limited Impact of Explanation Type on Task Performance", "content": "While Subsections 5.3 and 5.2 highlighted the overall benefits of collaboration with an AI assistant in Task 2, user performance within this task did not exhibit noticeable differences across the six groups (as depicted in Figure 3). This suggests that, in the context of our experiment, the specific format or level of detail provided in the explanations (concept-based or example-based) did not have a substantial impact on user accuracy when compared to the two Control groups that received only the model's prediction (with and without model confidence)."}, {"title": "5.5 Potential for Blind Trust with Explanations", "content": "Our study also identified a potential concern regarding the use of explanations, particularly in relation to fostering blind trust. Figures 4 and 5 use color coding to represent user agreement with the model's suggestion (low agreement in blue, high agreement in red). These figures reveal a notable presence of \"hot spots\" where user agreement is high (green-yellow-orange-red) despite high model uncertainty. Table 1 also shows that, on average, users' responses matched more often the model's prediction in Task 2 than in the other tasks. On the one hand, following the AI's recommendation lead to higher user accuracy scores, as discussed above. On the other hand, when zooming in only on the misclassified samples (accompanied by a matching wrong explanation in the four \u03a7\u0391\u0399 groups), we report the following agreement rates: Control - 47.29%, Control-Confidence - 52.31%, CoProNN - 47.67%, TCAV - 45.25%, GradSim 62.95%, RepPoint - 69.1%. This suggests that users may, in some cases, predominantly in the two example-based XAI groups, exhibit a tendency to blindly trust the model's suggestions, even when presented with explanations for demonstrably incorrect predictions."}, {"title": "6 Conclusion", "content": "In this work, we proposed a novel HIL experiment design that allows to analyze the didactic effect of explanations, as well as correlations between the users' and the model's uncertainty. Apart from these new considerations, more traditional investigative points such as human-machine performance as a team or blind trust were also taken into account. We examined human-AI collaboration with explanations in image classification; nonetheless, our framework can be readily applied to any other machine learning task.\nWe found that explanations considerably improved user performance during collaboration, especially when the AI was certain of its prediction. User uncertainty also decreased with explanations. However, our study also identified certain limitations. Explanations did not show notable benefits for long-term knowledge transfer and the specific explanation format had little to no impact"}]}