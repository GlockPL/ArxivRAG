{"title": "Offline Behavior Distillation", "authors": ["Shiye Lei", "Sen Zhang", "Dacheng Tao"], "abstract": "Massive reinforcement learning (RL) data are typically collected to train policies offline without the need for interactions, but the large data volume can cause training inefficiencies. To tackle this issue, we formulate offline behavior distillation (OBD), which synthesizes limited expert behavioral data from sub-optimal RL data, enabling rapid policy learning. We propose two naive OBD objectives, DBC and PBC, which measure distillation performance via the decision difference between policies trained on distilled data and either offline data or a near-expert policy. Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $O(1/(1 \u2013 \\gamma)^2)$. We theoretically establish the equivalence between the policy performance and action-value weighted decision difference, and introduce action-value weighted PBC (Av-PBC) as a more effective OBD objective. By optimizing the weighted decision difference, Av-PBC achieves a superior distillation guarantee with linear discount complexity $O(1/(1 \u2212 \\gamma))$. Extensive experiments on multiple D4RL datasets reveal that Av-PBC offers significant improvements in OBD performance, fast distillation convergence speed, and robust cross-architecture/optimizer generalization. The code is available at https://github.com/LeavesLei/OBD.", "sections": [{"title": "1 Introduction", "content": "Due to the costs and dangers associated with interactions in reinforcement learning (RL), learning policies from pre-collected RL data has become increasingly popular [Levine et al., 2020]. Consequently, numerous offline RL datasets have been constructed [Fu et al., 2020]. However, these offline data are typically massive and collected by sub-optimal or even random policies, leading to inefficiencies in policy training. Inspired by dataset distillation (DD) [Wang et al., 2018, Zhao et al., 2021, Lei and Tao, 2024], which synthesizes a small number of training images while preserving model training effects, we further investigate the following question: Can we distill vast sub-optimal RL data into limited expert behavioral data? Achieving this would enable rapid offline policy learning via behavioral cloning (BC) [Pomerleau, 1991], which can (1) reduce the training cost and enable green AI; (2) facilitate downstream tasks by using distilled data as prior knowledge (e.g. continual RL [Gai et al., 2023], multi-task RL [Yu et al., 2021], efficient policy pretraining [Goecks et al., 2019], offline-to-online fine-tuning [Zhao et al., 2022]); and (3) protect data privacy [Qiao and Wang, 2023].\nUnlike DD whose objective is prediction accuracy and directly obtainable from real data, the policy performance in RL is measured by the expected return through interactions with environment. In an\noffline paradigm, where direct interaction with environment is not possible, a metric based on RL data\nis necessary to guide the RL data distillation. Therefore, we formalize the offline behavior distillation\n(OBD): a limited set of behavioral data, comprising (state, action) pairs, is synthesized from\nsub-optimal RL data, so that policies trained on the compact synthetic dataset by BC can achieve\nsmall OBD objective loss, which incarnates high return when deploying policies in the environment.\nThe key obstacle for OBD is constructing a proper objective that efficiently and accurately estimates\nthe policy performance based on the sub-optimal offline dataset, allowing for a rational evaluation\nof the distilled data. To this end, data-based BC (DBC) and policy-based BC (PBC) present two\nnaive OBD objectives. Specifically, DBC reflects the policy performance by measuring the mismatch\nbetween the policy decision and vanilla offline data. Leveraging existing offline RL algorithms that\ncan extract near-optimal policies from sub-optimal data [Levine et al., 2020], PBC improves upon\nDBC by correcting actions in offline data using a near-optimal policy before measuring the decision\ndifference. However, due to the complex bi-level optimization in OBD, the objectives are difficult to\nminimize effectively, resulting in an inferior distillation performance guarantee with the quadratic\ndiscount complexity $O(1/(1 \u2013 \\gamma)^2)$ for PBC (Theorem 1). We tackle this problem and propose the\naction-value weighted PBC (Av-PBC) as the OBD objective with superior distillation guarantee by\ntaking inspirations from our theoretical findings. Concretely, we theoretically prove the equivalence\nbetween the policy performance gap and the action-value weighted decision difference (Theorem\n2). Then, by optimizing the weighted decision difference, we can obtain a much tighter distillation\nperformance guarantee with linear discount complexity $O(1/(1 \u2013 \\gamma))$ (Corollary 1). Consequently,\nwe weigh PBC with the simple action value, introducing Av-PBC as the OBD objective.\nExtensive experiments on nine datasets of D4RL benchmark [Fu et al., 2020] with multiple envi-\nronments and data qualities illustrate that our Av-PBC remarkably promotes the OBD performance,\nwhich is measured by normalized return, by 82.8% and 25.7% compared to baselines of DBC and\nPBC, respectively. Moreover, Av-PBC has a significant convergence speed and requires only a\nquarter of distillation steps compared to DBC and PBC. By evaluating the synthetic data in terms\nof different network architectures and training optimizers, we show that distilled datasets possess\ndecent cross-architecture/optimizer performance. Apart from evaluations on single policy, we also\ninvestigate policy ensemble performance by training multiple policies on the synthetic dataset and\ncombining them to generate actions. The empirical findings demonstrate that the ensemble operation\ncan significantly enhance the performance of policies trained on Av-PBC-distilled data by 25.8%.\nOur contributions can be summarized as:\n\u2022 We formulate the offline behavior distillation problem, and present two naive OBD objectives of\nDBC and the improved PBC;\n\u2022 We demonstrate the unpleasant distillation performance guarantee of $O(1/(1-\\gamma)^2)$ for PBC, and\ntheoretically derive a novel objective of Av-PBC that has much tighter performance guarantee\nof $O(1/(1 \u2013 \\gamma))$;\n\u2022 Extensive experiments on multiple offline RL datasets verify significant improvements on OBD\nperformance and speed by Av-PBC."}, {"title": "2 Related works", "content": "Offline RL Data collection can be both hazardous (e.g. autonomous driving) and costly (e.g.\nhealthcare) with the online learning paradigm of RL. To alleviate the online interaction, offline RL\nhas been developed to learn the policy from a pre-collected dataset gathered by sub-optimal behavior\npolicies [Lange et al., 2012, Fu et al., 2020]. However, the offline paradigm limits exploration and\nresults in the distributional shift problem: (1) the state distribution discrepancy between learned policy\nand behavior policy at test time; and (2) only in-dataset state transitions are sampled when conducting\nBellman backup [Bellman, 1966] during the training period [Levine et al., 2020]. Various offline RL\nalgorithms have been proposed to mitigate the distributional shift problem. Fujimoto and Gu [2021],\nTarasov et al. [2024] introduce policy constrain that control the discrepancy between learned policy\nand behavior policy. To address the problem of over-optimistic estimation on out-of-distribution\nactions, Kumar et al. [2020], Nakamoto et al. [2023], Kostrikov et al. [2022] propose to regularize the\nlearned value function for conservative Q learning. Moreover, ensemble approaches have also proven\neffective in offline RL [An et al., 2021]. Readers can refer to [Tarasov et al., 2022] for a detailed\ncomparison of offline RL methods. Albeit these advancements, the offline dataset is extremely large"}, {"title": "3 Preliminaries", "content": "Reinforcement Learning The problem of reinforcement learning can be described as the Markov\ndecision process (MDP) $(S, A, T, r, \\gamma, d^0)$, where $S$ is a set of states $s \\in S$, $A$ is the set of actions\n$a \\in A$, $T(s'|s, a)$ denotes the transition probability function, $r(s, a)$ is the reward function, $\\gamma \\in (0, 1)$\nis the discount factor, and $d^0(s)$ is the initial state distribution [Sutton and Barto, 2018]. We assume\nthat the reward function is bounded by $R_{max}$, i.e., $r(s,a) \\in [0, R_{max}]$ for all $(s,a) \\in S \\times A$.\nThe objective of RL is to learn a policy $\\pi(a|s)$ that maximizes the long-term expected return\n$J(\\pi) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$, where $r_t = r(s_t, a_t)$ is the reward at $t$-step, and $\\gamma$ usually is close to\n1 to consider long-horizon rewards in the most RL tasks. We define $d_t(s) = Pr(s_t = s; \\pi)$\nand $\\rho_t(s, a) = Pr(s_t = s, a_t = a; \\pi)$ as $t$-th step state distribution and state-action distribution,\nrespectively. Then, the discounted stationary state distribution $d_{\\pi}(s) = (1 \u2013 \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t d_t(s)$, and\nthe discounted stationary state-action distribution $\\rho_{\\pi}(s, a) = (1 \u2212 \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\rho_t(s, a)$. Intuitively,\nthe state (state-action) distribution depicts the overall \u201cfrequency\" of visiting a state (state-action)\nwith $\\pi$. The action-value function of $\\pi$ is $q_{\\pi}(s, a) = \\mathbb{E}_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_0 = a]$, which is the\nexpected return starting from s, taking the action a. Since $r_t \\geq 0$, we have $q_{\\pi}(s, a) \\geq 0$ for all $(s, a)$.\nInstead of interacting with the environment, offline RL learns the policy from a sub-optimal offline\ndataset $D_{off} = \\{(s_i, a_i, s'_i, r_i)\\}_{i=1}^N$ with specially designed Bellman backup [Levine et al., 2020].\nAlthough $D_{off}$ is normally collected by sub-optimal behavior policies, offline RL algorithms can\nrecapitulate a near-optimal policy $\\pi^*$ and value function $q_*$ from $D_{off}$.\nBehavioral Cloning [Pomerleau, 1991] can be regarded as a special offline RL algorithm and\nonly copes with high-quality data. Given the expert demonstrations $D_{BC} = \\{(s_i, a_i)\\}_{i=1}^N$, the\npolicy network $\\pi_\\theta$ parameterized by $\\theta$ is trained by cloning the behavior of the expert dataset $D_{BC}$\nin a supervised manner: $\\min_{\\theta} \\mathcal{L}_{BC}(\\theta, D_{BC}) := \\mathbb{E}_{(s,a) \\sim D_{BC}} [(\\pi_{\\theta}(a|s) - \\pi^*(a|s))^2]$, where $\\pi^*(a|s)$ ="}, {"title": "3.1 Problem Setup", "content": "We first introduce behavior distillation [Lupu et al., 2024] that aims to synthesize few data points\n$D = D_{syn} = \\{(s_i, a_i)\\}_{i=1}^{N_{syn}}$ with small $N_{syn}$ from the environment, so the policy trained on $D_{syn}$ has\na large expected return J. The problem of behavior distillation can be formalized as follows:\n$D^*_{syn} = arg \\max_{D} J(\\pi_{\\theta}(D)) \\ \\ s.t. \\ \\  \\theta(D) = arg \\min_{\\theta} \\mathcal{L}_{BC}(\\theta, D)$.\n(1)\nDuring behavior distillation, the return J is directly estimated by the interaction between policy\nand environment. However, in the offline setting, the environment can not be touched, and only the\npreviously collected dataset $D_{off}$ is provided. Hence, we employ $H(\\pi_{\\theta}, D_{off})$ as a surrogate loss\nto estimate the policy performance of $\\pi_{\\theta}$ given the offline data $D_{off}$ without interactions with the\nenvironment. Then, by setting $N_{syn} \\ll N_{off}$, offline behavior distillation can be formulated as below:\n$D_{syn} = arg \\min_D H (\\pi_{\\theta(D)}, D_{off}) \\ \\ S.t. \\ \\  \\theta(D) = arg \\min_{\\theta} \\mathcal{L}_{BC}(\\theta, D)$.\n(2)"}, {"title": "3.2 Backpropagation through Time", "content": "The formulation of offline behavior distillation is a bi-level optimization problem: the inner loop\noptimizes the policy network parameters based on the synthetic dataset with BC by multiple iterations\nof $\\{\\theta_1, \\theta_2,\uff65\uff65\uff65, \\theta_T\\}$. During the outer loop iteration, synthetic data are updated by minimizing the\nsurrogate loss H. With the nested loop, the synthetic dataset gradually converges to one of the optima.\nThis bi-level optimization can be solved by backpropagation through time (BPTT) [Werbos, 1990]:\n$\\nabla_{D}H = \\sum_{k=0}^T (\\frac{\\partial H}{\\partial \\theta(T)} (\\frac{\\partial \\theta(T)}{\\partial \\theta(k)})) \\frac{\\partial \\theta(k)}{\\partial D}$, and $\\frac{\\partial \\theta(T)}{\\partial \\theta(k)} = \\prod_{i=k+1}^T \\frac{\\partial \\theta(i)}{\\partial \\theta(i-1)}$\n(3)\nAlthough BPTT provides a feasible solution to compute the meta-gradient for OBD, the objective H\nis hardly minimized to near zero in practice owing to the severe complexity and non-convexity of\nbi-level optimization [Wiesemann et al., 2013]."}, {"title": "4 Methods", "content": "The key challenge in OBD is determining an appropriate objective loss $H(\\pi_\\theta, D_{off})$ to estimate the\nperformance of $\\pi_\\theta$. While policy performance could be naturally estimated using episode return by\nlearning a MDP environment from $D_{off}$, as done in model-based offline RL [Kidambi et al., 2020],\nthis approach is computationally expensive. Apart from the considerable time required to sample\nthe episode for evaluation, the corresponding gradient computation is also inefficient: although\nPolicy Gradient Theorem $\\triangledown_\\theta J = \\sum_{s} d_\\pi(s) \\sum_a q_\\pi(s,a) \\bigtriangledown_\\theta \\pi_\\theta(a|s)$ provides a way to compute meta-\ngradients [Sutton and Barto, 2018], the gradient estimation often exhibits high variance due to the\nlack of information w.r.t. $d_\\pi(s)$ and $q_\\pi(s, a)$."}, {"title": "4.1 Data-based and Policy-based BC", "content": "Compared to both sampling and gradient computation inefficiency of policy return, directly using\n$D_{off}$ is a more feasible way to estimate the policy performance in OBD, and a natural option is BC\nloss, i.e., $H(\\pi_{\\theta}, D_{off}) = \\mathcal{l}_{bc}(\\theta, D_{off})$, which we refer to as data-based BC (DBC). However, as\n$D_{off}$ is collected by sub-optimal policies, DBC hardly evaluates the policy performance accurately.\nBenefiting from offline RL algorithms, we can extract the near-optimal policy $\\pi^*$ and corresponding\nvalue function $q^*$ from $D_{off}$ via carefully designed Bellman updates. Consequently, a more rational\nchoice is to correct actions in $D_{off}$ with $\\pi^*$, leading to $H(\\pi, D_{off}) = \\mathbb{E}_{s \\sim D_{off}} [D_{TV} (\\pi^*(\\cdot|s), \\pi(\\cdot|s))]$,\nwhere $D_{TV} (\\pi^*(\\cdot | s), \\pi(\\cdot | s)) = \\sum_{a \\in A} |\\pi^*(a|s) \u2013 \\pi(a|s)|$ is the total variation (TV) distance"}, {"title": "4.2 Action-value weighted PBC", "content": "The preceding analysis highlights the inferior distillation guarantee of $O(1/(1 \u2013 \\gamma)^2)$ with PBC.\nTo establish a superior OBD objective, we prove the equivalence between the performance gap of\n$J(\\pi^*) \u2013 J(\\pi)$ and action-value weighted $|\\pi^*(a|s) \u2013\\pi (a|s)|$ (Theorem 2). By optimizing the weighted\ndecision difference, the performance gap can be non-vacuously bounded with a reduced discount\ncomplexity of $O(1/(1 \u2013 \\gamma))$ (Corollary 1). Motivated by these theoretical insights, we propose\naction-value weighted PBC as the OBD objective for a tighter distillation performance guarantee.\nTheorem 2. For any two policies $\\pi$ and $\\pi^*$, we have\n$J(\\pi^*) \u2013 J(\\pi) = \\frac{1}{1-\\gamma} \\mathbb{E}_{s \\sim d_\\pi(s)} [q_{\\pi^*} (s, \u00b7) (\\pi^*(\\cdot|s) \u2013 \\pi (\\cdot|s))]$,\n(4)\nwhere the dot notation (\u00b7) is a summation over the action space, i.e., $q_{\\pi^*} (s, \u00b7) (\\pi^*(\\cdot|s) \u2013 \\pi (\\cdot|s)) =$\n$\\sum_{a \\in A} aq_{\\pi^*} (s, a) (\\pi^*(a|s) \u2013 \\pi (a|s))$.\nProof Sketch. (1) With RL definitions, we represent $J(\\pi^*) \u2013 J(\\pi)$ by\n$J(\\pi^*) \u2013 J(\\pi) = \\mathbb{E}_{s \\sim d_0 (s)} [q_{\\pi^*} (S,\u00b7) (\\pi^*(\\cdot|s) \u2013 \\pi(\\cdot|s))] + \\mathbb{E}_{\\rho_\\pi(s,a)} [q_{\\pi^*} (s, a) \\triangledown \\pi (\u03b4, \u03b1)];$\n(2) then we prove the iterative formula w.r.t. $\\mathbb{E}_{\\rho_\\pi(s,a)} [q_{\\pi^*} (s, a) \\triangledown \\pi (\u03b4, \u03b1)]:\n$\\mathbb{E}_{\\rho_\\pi(s,a)} [q_{\\pi^*} (s, a) \\triangledown \\pi (\u03b4, \u03b1)]\n=\\gamma \\mathbb{E}_{s \\sim d^{n+1}(s)} [q_{\\pi^*} (S,\u00b7) (\\pi^*(\\cdot|s) \u2013 \\pi(\\cdot|s))] + \\gamma \\rho^{n+1}(s,a) [q_{\\pi^*} (\u03b4, \u03b1) \\triangledown \\pi (\u03b5, \u03b1)];$\n(3) integrating the two equations above yields the desired result\n$J(\\pi^*) \u2013 J(\\pi) = \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{E}_{s \\sim d_t(s)} [q_{\\pi^*} (s, \u00b7) (\\pi^*(\\cdot|s) \u2013 \\pi(\\cdot|s))]$.\nThe complete proof can be found in Appendix A.1. Since $q_{\\pi^*} (s, a)$ represents the expected return\nunder the decent policy $\\pi^*$ when staring from $(s, a)$ and reaches the maximum if $\\pi^*$ is truly optimal, it\ncan be interpreted as the importance of $(s, a)$, and higher return is likely to be achieved when starting\nfrom more important $(s, a)$. Consequently, the gap between $J(\\pi^*)$ and $J(\\pi)$ directly depends on the\nimportance-weighted decision difference between $\\pi^*$ and $\\pi$. Based on Theorem 2 and $q_{\\pi^*} \\geq 0$, we\ncan readily derive a bound on the guarantee on $|J(\\pi^*) \u2013 J(\\pi)|$ by applying the triangle inequality.\nCorollary 1. Given two policies of $\\pi^*$ and $\\pi$ with $\\mathbb{E}_{s \\sim d_\\pi(s)} [q_{\\pi^*} (S,\u00b7) |\\pi^*(\\cdot|s) \u2013 \\pi(\\cdot|s)|] \\leq \\epsilon$, we\nhave $|J(\\pi^*) \u2013 J(\\pi)| \\leq \\frac{\\epsilon}{1-\\gamma}$.\nTightness Since only the triangle inequality is applied, there exists the worst case for $\\pi$ where\n$\\pi^*(a|s) \u2013 \\pi(a|s) < 0$ holds only when $q_{\\pi^*} (s, a) = 0$. This makes the inequality collapse to equality\nin Corollary 1, thereby demonstrating that the upper bound in Corollary 1 is non-vacuous."}, {"title": "5 Experiments", "content": "In this section, we evaluate the proposed ODB algorithms across multiple offline RL datasets from\nperspectives of (1) distillation performance, (2) distillation convergence speed, (3) cross-architecture\nand cross-optimizer generalization, and (4) policy ensemble performance w.r.t. distilled data.\nDatasets We conduct offline behavior distillation on D4RL [Fu et al., 2020], a widely used offline\nRL benchmark. Specifically, OBD algorithms are evaluated on three popular environments of\nHalfcheetah, Hopper, and Walker2D. For each environment, three offline RL datasets of varying\nquality are provided by D4RL, i.e., medium-replay (M-R), medium (M), and medium-expert\n(M-E) datasets. Thus, a total of 3 \u00d7 3 = 9 datasets are employed to assess OBD algorithms. medium\ndataset is collected from the environment with \u201cmedium\u201d level policies; medium-replay dataset\nconsists of recording all samples in the replay buffer observed during training this \"medium\" level\npolicy; and medium-expert dataset is a mixture of expert demonstrations and sub-optimal data."}, {"title": "6 Discussion", "content": "Applications Distilled behavioral data encapsulate critical decision-making knowledge from offline\nRL data and associated environment, making them highly applicable to various downstream RL\ntasks. Through BC on distilled data, we can rapidly pretrain a good policy for online RL fine-tuning\n[Goecks et al., 2019]. On the other hand, after offline pretraining, the policy can be further enhanced\nby online fine-tuning, while there exists catastrophic forgetting w.r.t. offline data knowledge during\nfine-tuning [Luo et al., 2023]. To tackle this challenge, Zhao et al. [2022] propose to use BC loss w.r.t.\noffline data as a constraint during the fine-tuning phase. By replacing the massive offline data with\ndistilled data, we can achieve more efficient loss computation and thus better algorithm convergence.\nA similar approach can be achieved to circumvent catastrophic forgetting in continual offline RL\n[Gai et al., 2023], where the goal is to learn a sequence of offline RL tasks while retaining good\nperformance across all tasks. Moreover, multi-task offline RL [Yu et al., 2021], which learns multiple\nRL tasks jointly from a combination of specific offline datasets, also receives benefits from OBD in\nterms of efficiency by alternative training on the mixture of distilled data via BC [Lupu et al., 2024].\nBeyond benefits in efficient policy training, OBD shows potential for protecting data privacy: given\nthat offline datasets often contain sensitive information, such as medical records, privacy concerns are\nsignificant in offline RL due to various privacy attacks on the learned policies [Qiao and Wang, 2023].\nOBD can enhance privacy preservation by publishing smaller, distilled datasets instead of the full,\nsensitive data. Besides, distilled behavioral data is also beneficial for explainable RL by highlighting\nthe critical states and corresponding actions. A example of this is provided in Appendix D.\nLimitations The OBD data are 2-tuples of (state, action) and exclude reward. Thus, the\ndistilled data are solely leveraged by the supervised BC and invalid for conventional RL algorithms\nwith Bellman backup. Despite this deficiency, OBD data can still facilitate the applications above by\nefficiently injecting high-quality decision-making knowledge into policy networks with BC loss.\nWe note that two major challenges remain in current OBD algorithms: distillation inefficiency and\npolicy performance degradation. While our Av-PBC substantially decreases the distillation steps,\nthe OBD process is still computationally expensive (25 hours for 50k distillation steps on a single\nNVIDIA V100 GPU) due to the bi-level optimization involved. Moreover, there remains a notable\nperformance gap between OBD and the whole data with offline RL algorithms (38.2 vs. 70.1 in Table\n1). These limitations also shed light on future directions in improving the efficiency of OBD and\nbridging the gap between synthetic data and the original offline RL dataset."}, {"title": "7 Conclusion", "content": "In this paper we integrate the advanced dataset distillation with offline RL data, formalizing the\nconcept of offline behavior distillation (OBD). We introduce two OBD objectives: the naive offline\ndata-based BC (DBC) and its policy-corrected variant, PBC. Through comprehensive theoretical\nanalysis, we demonstrate that PBC offers inferior OBD performance guarantee of $O(1/(1 \u2013 \\gamma)^2)$\nunder complex bi-level optimization, which inevitably incurs significant distillation loss.. To tackle\nthis issue, we theoretically establish the equivalence between policy performance gap and action-\nvalue weighted decision difference, leading to the proposal of action-value weighted BC (Av-PBC).\nThis novel Av-PBC objective significantly improves the performance guarantee to $O(1/(1 \u2013 \\gamma))$.\nExtensive experiments on multiple offline RL datasets demonstrate that Av-PBC vastly enhances\nOBD performance and accelerates the distillation process by several times."}]}