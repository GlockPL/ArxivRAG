{"title": "Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK", "authors": ["Marcos Cramer", "Lucian McIntyre"], "abstract": "Large language models (LLMs) have demonstrated remarkable code generation capabilities, but the correctness of the generated code cannot be inherently trusted. This paper explores the feasibility of using formal software verification, specifically the SPARK framework for Ada, to ensure the reliability of LLM-generated code. We present Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code. The tool is benchmarked on a curated set of SPARK programs, with annotations selectively removed to test specific capabilities. The performance of Marmaragan with GPT-40 on the benchmark is promising, with correct annotations having been generated for 50.7% of the benchmark cases. The results establish a foundation for future work on combining the power of LLMs with the reliability of formal software verification.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have attracted significant attention within both the AI research community and the general public due to their generative capabilities. Tools such as ChatGPT have demonstrated the potential of LLMs to automate and accelerate processes in diverse fields, with software development benefiting particularly from their ability to generate code. However, while LLMs showcase impressive creativity and adaptability, they also present risks. As these models operate as black boxes, the code they generate cannot inherently be trusted to be correct or error-free, posing challenges for real-world applications where reliability and safety are essential.\nTo address the uncertainties associated with LLM-generated code, formal verification techniques offer a promising solution. Formal software verification employs rigorous mathematical methods to prove the correctness of code against a specified set of properties, which can help ensure that software meets its intended specifications reliably. Integrating formal verification with LLM-generated code has the potential to mitigate risks, making it possible to harness the creative benefits of LLMs while maintaining a high standard of code quality and safety.\nThe current paper is motivated by the need to bridge the gap between the creative potential of LLMs and the necessity for reliable, error-free code. By leveraging formal verification techniques, specifically through the SPARK programming language, we aim to explore the feasibility of combining LLMs with formal verification to produce code that is both innovative and provably correct. This study investigates whether LLMs can generate annotations for SPARK programs, facilitating formal verification of the resulting code.\nFor this purpose, we have implemented Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code using the GNATprove tool. Marmaragan can be viewed as a prototype for the backend of an AI-powered annotation generator that could run in the background of a SPARK editor. It incorporates features such as generating multiple solution attempts, retrying with additional context from GNATprove, and providing pre-compiled error messages to improve performance. Marmaragan can currently be combined with any LLM in the OpenAI API and has been most throughly tested with GPT-40. It has parameters for the number of solutions generated in parallel, for the number of retries that Marmaragan attempts before giving up as well as for toggling a special chain-of-thought mode.\nIn order to evaluate how well Marmaragan performs depending on the value of its parameters, we created a benchmark based on a curated set of SPARK programs, from which annotations were selectively removed following various removal schemata. Experiments on the benchmark demonstrate Marmaragan's competence in generating annotations: Overall, it generates correct annotations for 50.7% of the benchmark programs. Furthermore, the experiments shed light on what is the optimal balance between parallel solution attempts and retries in the light of limited computational resources.\nBy successfully generating correct annotations, we establish a foundation for future work: In the near to medium-term future, this research could contribute to making applications of formal verification of code reliability and safety more efficient. In the long term it could contribute a building block towards a hybrid tool that combines the power of LLMs with the reliability of software verification for generating fully verified programs.\nIn Section 2, we discuss the preliminaries of this paper in the areas of logic, formal software verification (with a focus on SPARK 2014) and large language models. The implementation of Marmaragan is presented in Section 3. In Section 4, we describe the methodology that we applied to create a benchmark for evaluating Marmaragan. The results of running Maramaragan with varying parameters on the benchmark are presented in Section 5, and in Section 6 we discuss these results. Section 7 presents related work. In section 8, we discuss future work before concluding in Section 9."}, {"title": "2 PRELIMINARIES", "content": "This section discusses the foundations the work is set upon and the work it relates to and is inspired by."}, {"title": "2.1 Formal Software Verification", "content": "Formal software verification is the process of proving the correctness of a software program with respect to a specified formal specification or property, using formal mathematical methods. It ensures that the software behaves as intended in all possible scenarios. In contrast with common non-formal testing techniques, which always cover only a limited number of scenarios and are thus vulnerable to having missed out on a scenario in which a bug takes effect, formal software verification covers all potential runs of the program. From now on, we will often use the equivalent term \"formal verification\" as a shorthand for \"formal software verification\".\nThere are different methodological approaches to formal verification. For this paper, we don't need to consider model checking and instead focus on deductive verification, which is \"the process of turning the correctness of a program into a mathematical statement and then proving it\" (Filli\u00e2tre, 2011). In deductive verification, the desired behaviour of the program needs to be specified in a formal language. The task is then to prove that the program actually satisfies the specification for all possible inputs.\nAt the level of single functions in the program, this is realized through pre- and postconditions, which are assertions on values of variables that enter and exit given functions within our program, specifying properties and relationships (Hoare, 1969). A precondition defines the conditions that must be met, so that a given function can be executed. Analogously, a postcondition defines the conditions that must be met directly subsequent to function execution. For example, a function computing F(x,y) = x - y could have the precondition x > y for ensuring that one stays in the realm of positive numbers. In this case, a sensible postcondition would be F(x,y) > 0, as this postcondition logically follows from the precondition and the definition of the function F(x,y). This kind of logical entailment needs to hold for every postcondition of a function, and this needs to be established through a formal proof. We say that there is proof obligation for deriving the postcondition."}, {"title": "2.2 SPARK 2014", "content": "SPARK 2014 (Moi, 2013) is a formally defined subset of the Ada programming language (AdaCore, 1980), designed to support the development of reliable and provably correct software (Barnes, 2012). Its unambiguous semantics ensures that programs behave consistently and predictably. SPARK allows only specific constructs from Ada, ensuring compatibility with formal verification methods. Programs written in SPARK can be annotated with assertions, including preconditions and postconditions, to support modular deductive verification (Hoare, 1969).\nSPARK has found application in multiple areas, including train control systems and space transportation (Dross et al., 2014), commercial aviation (Moy et al., 2013), air traffic management (Chapman and Schanda, 2014) and GPU design (Chapman et al., 2024).\nSome annotations in SPARK take the form of pragma statements, such as:\npragma Assertion (condition);"}, {"title": "2.2.1 Loop Invariants", "content": "A loop invariant is a property that holds during each loop iteration. It can be viewed as the induction hypothesis in an inductive proof over the number of loop iterations. Consider the example in Listing 1.\nHere, the invariants state that the Result is twice the Count and that the loop counter does not exceed X."}, {"title": "2.3 Large Language Models and Transformers", "content": "The development of large language models (LLMs) has been a significant leap forward for AI development, spurred by the introduction of the transformer architecture by Vaswani et al. in \"Attention Is All You Need\" (Vaswani et al., 2017).\nLLMs, which are specialized neural models with billions of parameters, excel at capturing patterns in text data to perform a variety of language tasks. These models evolved from earlier statistical language models that relied on n-gram techniques to predict word sequences but struggled with long-range dependencies. Innovations like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), combined with the transformer's attention mechanisms, enabled the modeling of complex relationships between words (or rather between tokens), addressing the limitations of traditional approaches.\nThe transformer architecture, central to LLMs, employs a multi-layer encoder-decoder structure (Vaswani et al., 2017). Its core innovation, the self-attention mechanism, calculates the importance of tokens relative to one another, enhancing the model's ability to understand context. Multi-head attention further improves performance by allowing the model to focus on different aspects of input simultaneously. The final stages of transformers generate predictions through a combination of linear layers and softmax functions, transforming embeddings into meaningful output. These advancements, coupled with significant increases in model scale and training data, underpin the capabilities of state-of-the-art (as of 2024) LLMs like OpenAI's GPT-4 and GPT-40 models, which are the basis of this thesis."}, {"title": "2.3.1 Chain-of-thought prompting", "content": "LLM performance has been shown to depend on the formulation of the prompt that is given to the LLM. One prompting technique that is relevant to our work is chain-of-thought-prompting (Jason Wei et al., 2023), whose applicability to generating SPARK annotations we have studied (see Section 3). This prompting technique enhances reasoning by guiding LLMs through a series of intermediate natural language steps before generating the final output. For this, the prompt is extended by a note about the intended structure of the response, e.g. \"Let's think step by step\". This technique has been shown to significantly improve model performance on reasoning-heavy tasks, both for few-shot prompting (Jason Wei et al., 2023) and for zero-shot prompting (Kojima et al., 2023)."}, {"title": "3 IMPLEMENTATION", "content": "This section provides a detailed description of Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code."}, {"title": "3.1 Marmaragan", "content": "Marmaragan [GitHub] is a tool, developed in Python, designed for the SPARK 2014 language. The tool implements a hybrid AI approach that combines the power of LLMs with the trustworthiness of logic-based reasoning to generate annotations required for formal verification within SPARK 2014 programs.\nUse is made of the LangChain (Chase, 2022) Library to handle the calls to the OpenAI API and make LLM interaction seamless.\nMarmaragan takes as input an existing Spark project consisting of the specification and implementation files as well as any dependencies. Using this, it queries an LLM to generate missing pragma statements and then allows GNATprove to compile and verify the resulting code. The tool incorporates a range of strategies to assist in generating correct programs, including features for retrying with GNATprove errors and mediums as well as post-processing of LLM output.\nIn the following, we describe the motivation and concepts behind Maramaragan. We survey the features of the tool and discuss each of the steps that are taken to transform input into output."}, {"title": "3.2 Proof of Concept", "content": "The aim was to develop a proof of concept for automatic annotation generation in SPARK. This concept stemmed from the hypothesis that generating formally verified code with an LLM circumvents the typical problems encountered with LLM-generated code.\nAlthough LLMs are showing great ability in the area of code generation, the code they create cannot be assumed to be free of faults or bugs. By generating formally verifiable code, it is possible to eliminate these types of errors. Marmaragan is a step in this direction. By showing that it is possible to generate SPARK 2014 annotations for a given program, we show that one of the main difficulties of generating formally verifiable code can be overcome."}, {"title": "3.3 Environment Emulation", "content": "Marmaragan is designed to emulate a tool which runs in the background of a SPARK 2014 editor, such as GNAT Studio (Brosgol, 2019). With this setup, a user may send requests to the tool, such that annotations are generated for their SPARK code.\nAs Marmaragan is a proof of concept, the idea was to design a tool which, given existing SPARK code, is capable of generating annotations. The resulting added annotations should lead to GNATprove running free of errors and mediums. Given this premise, the prompting strategy and setup of Marmaragan is developed in such a way as to optimally emulate these conditions."}, {"title": "3.4 Implementation Details", "content": "Here we deal with the implementation details of Marmaragan."}, {"title": "3.4.1 Marmaragan as a Benchmarking Tool", "content": "Marmaragan is developed as a proof of concept, implemented as a benchmarking application in order to evaluate its functionality and performance. It works by taking a benchmark as input, then iterating over each of the files. For each task, it attempts to generate all required annotations, such that the code runs error- and medium-free. This procedure may be configured in multiple ways, by modifying the prompt to provide more context to the LLM and changing how solutions are generated."}, {"title": "3.4.2 Prompting in Marmaragan", "content": "As described in Figure 1, an important step of the workflow in Marmaragan is prompting the LLM. A good prompt is key to generating useful LLM responses, thus we delve into the details of this step.\nIn Marmaragan prompting works by inserting the given SPARK 2014 files into the prompt, formatting and subsequently invoking the LLM. The prompt used for all queries can be found below in listing 2."}, {"title": "3.4.3 Medium in Prompt", "content": "Further, the prompt may be enhanced with additional context, by enabling the medium-in-prompt feature. When this feature is enabled, GNATprove compiles the SPARK project and any mediums from the output are extracted and formatted. In this case, formatting involves taking the line number and extracting the related line of code (and the one below) from the SPARK file. The line of code and the medium message is then appended to the prompt. Figure 2 gives an example of this."}, {"title": "3.4.4 Chain-of-Thought Prompt", "content": "As discussed in Section 2.3.1, chain-of-thought prompting (Jason Wei et al., 2023; Kojima et al., 2023) is a strategy that helps to increase the performance of LLMs on complex reasoning tasks. To gain some insight into the effectiveness of different prompting techniques, a chain-of-thought prompt was developed, which deviates from the standard prompt only in the first section:"}, {"title": "3.4.5 N-Solutions", "content": "The N-Solutions and Retries parameters are fundamental instruments which can be employed to increase the benchmarking success rate. N-Solutions determines the number of responses an LLM returns, per individual prompt. At N = 1, the LLM supplies a single response to a given prompt, at N = 5 it returns five responses. Due to how generation is affected by the temperature parameter, each of the N-Solutions are in most cases distinct."}, {"title": "3.4.6 Retries", "content": "Setting the Retries parameter to a value graeater than 0 makes Marmaragan continue with retries after a first failed attempt at generating pragmas that verify the code. The Retries mechanism works by providing the LLM with additional context in the form of the previous failed attempt and the medium messages generated by GNATprove in that attempt. This additional context helps the model to formulate a new solution attempt. Increasing the number of Retries leads to the additional solution attempts, each containing more context than the last."}, {"title": "4 BENCHMARKING", "content": "This section discusses the programs selected for benchmarking. This includes why the programs were chosen, where they were sourced from and how differing benchmarks were assembled from these."}, {"title": "4.1 Programs", "content": "In total, 16 SPARK 2014 programs were selected, from three differing sources:\n\u2022 Five programs originate from the Argu Repository, [Link](Cramer, 2023), which is a verified tool written by Marcos Cramer for computing abstract argumentation semantics, with a focus on finding the grounded extension of an argumentation framework and proving its uniqueness. It was first published after the cutoff dates for the training of GPT-4 and GPT-40, so that unlike for the other programs in the benchmark, we can be certain that it was not included in the training data of these LLMs.\n\u2022 The spark tutorial [Link], where the linear_search and show map programs were taken from.\n\u2022 A repository of SPARK 2014 implementations of common algorithms, known as spark-by-example [Link], where the final 11 programs were taken from, including basic implementations of copy and find algorithms, but also more complex programs such as search_lower_bound."}, {"title": "4.2 Determining a Metric for Results Evaluation", "content": "The choice was made to work with pre-existing, formally verified SPARK 2014 projects, as this made the task of evaluating the results from the benchmarks possible.\nQuantifying how close a given solution is to being formally verified is very challenging. GnatProve provides no feedback in regards to this, excepting medium messages. These provide feedback about which statements lead to a failure in verification, but the total number of medium messages is not indicative of the closeness to a completed verification. A manual analysis is also not feasible, given the number of benchmark programs and the total number of solutions generated.\nThus, it is only possible to evaluate correctness by checking whether the program is completely free of errors and medium. By taking programs which are already verified, we are sure that a correct solution exists. Additionally, utilizing existing programs makes it possible to better curate which types of annotations to generate."}, {"title": "4.3 Five Benchmarks", "content": "In total, from the 16 programs, five benchmarks were developed with differing aims. For each benchmark, a separate schema for removing pragma statements from the programs was devised. For some schemas, it was possible to do this multiple times per program. pragma statements are removed only from the implementation file (.adb) of the SPARK project. After removing pragmas from a program, we run GNATprove to check whether any mediums are generated. A program with removed pragmas is only included in the benchmark if GNATprove generates mediums for it, because otherwise it can be considered to be already fully verified, so that Marmaragan has no work to do on it."}, {"title": "5 RESULTS", "content": "This section presents the findings from experiments with Marmaragan. The aim was to evaluate Marmaragan's performance across various benchmarks, while varying the individual parameters of the tool. Through a series of experiments, we attempt to measure the effectiveness of the tool and extract how the parameter N-Solutions and Retries interact with each other."}, {"title": "5.1 GPT-40 Release", "content": "Early experiments conducted on the benchmark with vanilla GPT-4 demonstrated promising, albeit not entirely satisfying, results. However, shortly after these initial trials, GPT-40 was released. Not only did small scale experiments indicate that this new model was more successful overall, importantly, they also demonstrated that GPT-40 was far more cost-effective. This made larger experiments feasible. The main experiments were performed with the model gpt-40-2024-05-13."}, {"title": "5.2 Experiment Setup", "content": "Adjusting for these new possibilities, a large scale experiment to test the capabilities of Marmaragan was devised. The aim was to derive the effectiveness of the N-Solutions and Retries parameters. Various combinations of each of the parameters were conceived to test this. A central goal of these tests was to determine what balance between n (N-Solutions) and r (Retries) was ideal in order to get the best results given a fixed amount of computational resources available for completing the verification of a program.\nGiven the values n and r for the N-Solutions and Retries parameters, the number of solutions generated by the program is limited to a maximum of $n(r+1)$, because n solutions are generated for the first attempt and n further ones for each of the r retries. The total number of solutions to be generated per benchmark program was set to 12, as this made various combinations of n and r possible, while keeping costs within the limits set. The resulting combinations were the following:\n(n, r) combinations:\n(12,0),\n(6,1),\n(4, 2),\n(3, 3),\n(2,5)"}, {"title": "5.3 Experiment Results", "content": "Table 1 details the results of each of the five experiments on each of the five benchmarks.\nIn total 36 programs out of 71 were solved across all five benchmarks, meaning a solution was found for 50.7% of all benchmark problems."}, {"title": "5.4 Argu Results", "content": "The programs that originate from Argu (see Section 4.1) are of high significance, as the programs cannot be part of GPT-40's training data\u00b9. Thus, this makes it a notable benchmark, as the solutions could not have been learned, but had to be produced by the LLM without having seen the full program during pre-training.\nAlong with the programs being completely new for the LLM, the subject matter of the programs is also niche. Abstract argumentation theory is likely not among the major fields of academic research, and literature surrounding this topic is likewise uncommon. This makes the programs from the Argu repository potentially the most challenging in the benchmark.\nWith these premises in place, the results achieved by Marmaragan are surprising. Argu programs comprised just over half of the programs in the benchmark, totaling 36 out of 71. Out of these 36 programs, 16 were solved."}, {"title": "5.5 Verification of Results", "content": "Despite additional API costs, a replication of the two most successful experiments was conducted to attempt to show reproducibility. The two most successful runs were n = 6, r = 1 and n = 4, r = 2, each reaching 24/71 solutions. These two were rerun, in order to check reproducibility. The results from this rerun were more successful than the initial runs, leading to a total of 25/71 successfully solved programs for n = 4, r = 2 and 26/71 for n = 6, r = 1."}, {"title": "5.6 Chain of Thought Experiment", "content": "The Chain-of-Thought experiment refers to a further experiment conducted, which differentiates itself from the main experiment in its prompting strategy. See section 3.4.4 for an explanation.\nUsing the set of parameters with the highest rate of successfully generated solutions: n = 6, r = 1, the experiment was conducted. In total, 25/71 programs were solved, equaling the average of the success rates from the original experiment and the verification run."}, {"title": "6 DISCUSSION", "content": "This section analyses the results of the experiments conducted with Marmaragan."}, {"title": "6.1 General observations", "content": "Experiments on the benchmark demonstrate Marmaragan's competence in generating annotations, both in the case of Assert statements and in the case of loop invariants, with a higher level of competence for Assert statements than for loop invariants. Overall, it generated correct annotations for 36 out of 71 (50.7%) of the benchmark cases. These results highlight the potential for integrating formal verification into AI-assisted development, paving the way for safer and more reliable AI-generated software."}, {"title": "6.2 N-Solutions and Retries Parameters", "content": "Increasing the N-Solutions parameter, which determines the number of initial solution attempts, generally led to improved success rates in solving benchmark problems. The retry mechanism, which allows Marmaragan to attempt corrections based on error feedback, also proved to be an effective strategy. In many cases, experiments that incorporated retries outperformed those that relied solely on generating new solutions. This suggests that the model can effectively use error information to refine its approach.\nThe experiments reveal a complex interplay between N-Solutions and retries. The combinations of n = 6, r = 1 and n = 4, r = 2 yielded the best results, solving 24 out of 71 programs each. This suggests that a balance between initial solution attempts and correction opportunities is more effective than relying on either approach alone.\nAs can be seen in Figures 6 and 7, higher values of the N-Solutions end Retries parameters have diminishing returns. This is not surprising, as the model exhausts its most promising approaches in the first few attempts, with subsequent attempts becoming less likely to yield new solutions."}, {"title": "7 RELATED WORK", "content": "We are not aware of any work that brings LLMs and formal software verification together in the same way as we have proposed in this paper. But there is related work on leveraging LLMs for theorem proving and autoformalization in mathematics, from which valuable insights can be drawn for applying these techniques to the verification of software. This section reviews key works that inform our approach. These studies provide context for the capabilities and limitations of LLMs in formal reasoning, which Marmaragan seeks to extend to software verification.\nThor (Jiang et al., 2023a) combines language models (LMs) with automated theorem proving systems using hammers to improve formal proof generation. Hammers are tools that bridge the gap between Interactive Theorem Provers (ITPs), which assist users in formulating proofs, and Automated Theorem Provers (ATPs), which independently verify conjectures. They achieve this by translating proof steps into ATP-compatible formats, selecting relevant premises, and integrating the ATP's solutions back into the ITP, enabling automated reasoning for individual proof steps (Blanchette et al., 2016). Sledgehammer (Paulson, 2012) is a hammer for the Isabelle (Paulson, 1994) ITP. Thor functions as follows: given a theorem and assumptions, it proves the conjecture by first allowing the LM to devise the proof steps, then appending , allowing Sledgehammer to complete the rest of the proof. Tested on the PISA and MiniF2F benchmarks, Thor demonstrated higher success rates than individual components like Sledgehammer, solving 57% of PISA benchmark problems. Despite its innovative approach, subsequent methods like Baldur have surpassed Thor's results, utilizing newer LLM technologies.\nBaldur (First et al., 2023) generates entire formal proofs from theorem statements using LLMs and features a proof-repair mechanism that utilizes error messages to refine failed proofs. Unlike step-by-step proof generation, Baldur constructs full proofs and assesses their validity. It achieves a 47.9% success rate on the PISA benchmark and demonstrates the effectiveness of proof repair and additional context for improving performance. Combining Baldur with Thor enhances results, solving 65.7% of PISA problems, showcasing complementary strengths.\nThe Draft-Sketch-Prove (DSP) approach (Jiang et al., 2023b) addresses autoformalization by transforming informal mathematical proofs into verified formal proofs through three steps: drafting informal proofs, generating formal sketches, and completing gaps using automated provers. Using human-written and LLM-generated informal proofs, DSP achieves state-of-the-art performance on the MiniF2F benchmark, solving 42.6% and 40.6% of validation problems, respectively. Ablation studies highlight the importance of comments, multi-step reasoning, and integrating ATP tools like Sledgehammer.\nMagnushammer (Miku\u0142a et al., 2024) uses a transformer-based architecture to address premise selection, bypassing the need for extensive engineering. By training on premise selection datasets, it combines SELECT and RERANK algorithms to embed proof states and premises into a shared latent space, enabling relevance scoring. Magnushammer achieves state-of-the-art results on PISA, solving 59.5% of problems, and boosts Thor's performance to a 71% success rate when replacing Sledgehammer as the premise selection tool."}, {"title": "8 FUTURE WORK", "content": "Marmaragan demonstrates the feasibility of an AI-powered annotation generator for SPARK 2014, but there remain several opportunities for further research and development.\nOne potential direction is enabling the generation of pre- and postconditions by the LLM itself. Developers could define contracts for high-level functions, while the Al refines contracts for the invoked lower-level functions to preserve program correctness.\nTesting Marmaragan on a benchmark of 16 programs yielded initial results, but validating its robustness and generalizability requires a larger dataset. Future benchmarks should include diverse SPARK programs spanning various domains and complexities.\nLong-term goals include evolving Marmaragan into an industrial-grade tool, akin to how Copilot integrates into IDEs, by providing real-time annotation suggestions during SPARK code development.\nFinally, an exciting avenue for long-term research based on the ideas in this paper would be to explore the possibility of employing LLMs to generate entire formally verified programs based on a conversation between a human project manager and an LLM about the intended behavior of the program."}, {"title": "9 CONCLUSION", "content": "This paper introduced Marmaragan, a proof-of-concept tool for generating SPARK 2014 annotations using LLMs. It integrates GNATprove to check whether the annotations complete the verification of the code. Marmaragan is thus a hybrid AI system that combines the power of LLMs with the trustworthiness of logic-based reasoning tools. Key techniques in the implementation of Marmaragan include utilizing pre-compiled GNATprove mediums, generating multiple solutions, retrying with additional context, and optional chain-of-thought prompting.\nBenchmarking on 16 curated SPARK programs demonstrated Marmaragan's capabilities, particularly in generating Assert statements. The tool was able to solve 36 out of 71 benchmark cases.\nThis work highlights the potential for integrating formal verification into AI-assisted development, paving the way for safer and more reliable AI-generated software."}]}