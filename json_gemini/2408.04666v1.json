{"title": "LLMs are Not Just Next Token Predictors", "authors": ["Alex Grzankowski", "Stephen M. Downes", "Partick Forber"], "abstract": "LLMs are statistical models of language learning through stochastic gradient descent with a next token\nprediction objective. Prompting a popular view among AI modelers: LLMs are just next token predictors.\nWhile LLMs are engineered using next token prediction, and trained based on their success at this task,\nour view is that a reduction to just next token predictor sells LLMs short. Moreover, there are important\nexplanations of LLM behavior and capabilities that are lost when we engage in this kind of reduction. In\norder to draw this out, we will make an analogy with a once prominent research program in biology\nexplaining evolution and development from the genes eye view.", "sections": [{"title": null, "content": "LLMs are statistical models of language learning through stochastic gradient descent\nwith a next token prediction objective. So, LLMs are \u2018just next token predictors', a\npopular view among AI modelers, explicitly laid out by Shanahan (2024): \u201cA great many\ntasks that demand intelligence in humans can be reduced to next-token prediction with\na sufficiently performant model\u201d (2024, 68), and \u201csurely what they are doing is more\nthan 'just' next-token prediction? Well, it is an engineering fact that this is what an LLM\ndoes. The noteworthy thing is that next-token prediction is sufficient for solving\npreviously unseen reasoning problems\u201d (2024, 77). Yet LLMs won't satisfy users if they\nmerely produce just any next token. The desired next token must contribute to\nanswering questions, forming paragraphs, giving advice, making jokes, and so on. From\nthe perspective of the user, LLMs are more than just next token predictors \u2013 they are\njoke tellers, sources of advice, and so on. Which perspective is the right one?"}, {"title": null, "content": "While LLMs are engineered using next token prediction, and trained based on their\nsuccess at this task, our view is that a reduction to \u201cjust next token predictor\u201d sells LLMs\nshort. Moreover, there are important explanations of LLM behavior and capabilities that\nare lost when we engage in this kind of reduction. In order to draw this out, we will\nmake an analogy with a once prominent research program in biology explaining\nevolution and development from the \u201cgene's eye view\u201d.\nOur position can be brought into starker relief if we start by comparing two systems: one\ntrained to answer questions about baseball and the other trained to give baking advice.\nWhile it is true that the guts of both of these models uses next token prediction, we can\nsensibly ask why the first system is very good at producing answers about who pitched\nno hitters in the 1970s but not very good at telling you how to whisk an egg. Appealing\nto next token prediction alone provides no clear way to answer these straightforward\nquestions. The impressive accomplishment of LLMs is that they produce whole\nparagraphs of text relevant to their specific contexts. In both the baseball and baking\ncases, the output is achieved by both next token prediction and token network mapping.\nIn what follows, we'll expand on and explain this basic idea and tell you why it's\nimportant that we not adopt the hyper-reductive stance of \u201cjust next token\u201d."}, {"title": "Just Next Token Prediction", "content": "What do LLMs do? In some sense, they answer questions, fix code, write stories, and so\non. But when we are being a bit more hard-nosed, might these be metaphors? What do\nLLMs really do?\nIn simple terms, an LLM like ChatGPT is a system that is trained in such a way that\nwhen it receives a prompt it begins generating text in reply chunk by chunk by\nperforming statistical operations on arrays of numbers that correspond to words and\ntheir distributional features so as to predict a further chunk based on the existing\nchunks. LLMs are very clever things in no small part because of their attention heads\nand large context windows. LLMs come to learn interesting features about sentences\nsuch as how relevant other words are to resolving anaphoric terms or whether a word is\nbeing used as a verb or a noun. But, in some sense, this is all in the service of producing\na next word that gets mathematically rewarded. From this vantage, it's understandable\nwhy one might say that such a system is \u201cjust a next word predictor\u201d. (In what follows,\nthe difference between words and tokens will not matter, so we will slide over the\ndifference for readability.) Let's call proponents of \u201cjust next word\u201d JNPers.\nA nearby claim is that LLMs are \u201cstochastic parrots\u201d. It's worth flagging that this is a\ndistinct claim from the claim that LLMs are mere next word predictors. The term comes"}, {"title": null, "content": "from Bender et al. (2021) and aims to capture the following thought: much like a parrot\nwho says, \"Polly wants a cracker,\u201d LLMs produce strings that humans can interpret, but\n(if the objection is correct), the LLM itself has no knowledge of the meaning of the string\nit produces. It might be that a just-next-word predictor is a stochastic parrot if\njust-next-wording entails being non-sensitive to meaning. But it isn't obvious that the\nentailment holds. It's possible that a next-word-predicting mechanism could be a\nmeaning user, at least in principle. And the other direction seems even more plausible\nstill: parrots aren't next word predictors, they are sound-mimickers. But if anything\ndeserves that charge of parrotry, it's parrots. The present paper is not focused on\nwhether LLMs are meaning users in the sense of being sensitive to the semantics of the\nstrings they take as input or give as output (see Titus 2024 for further discussion). It\nseems to us that much of that debate will boil down to how convinced one is that\ndistributional semantics is a viable theory of meaning. The present paper is focused on\nthe claim that what LLMs do, when we are being hard-nosed and serious, is simply\npredict the next word/token.\nSuch a claim isn't a labeling exercise, a mere semantic dispute, or only of interest to\nphilosophers. We will see below that there are explanatory costs and benefits that come\nwith taking the \u201cnext token's eye view\u201d as compared to a higher-level style of description\nand explanation, but the issue can be motivated in simple terms. Suppose everyone is\nconvinced that LLMs and other forthcoming AI systems are just next token predictors. If\ncorrect, it's hard to see why we should think they might pose an existential threat. It's\nhard to see how we could lay blame for biased outputs at their feet. It's hard to see how\nthey could be viewed as anything other than a very clever gimmick. But if they are more\nthan next word predictors \u2013 if they are, for example, advice givers, question answerers,\ntrip planners, and so on, what we expect of them, and the standards to which we hold\nthem, shift in important ways. Again, below we will dig into deeper explanatory\ndifferences, but we hope this helps motivate the issue at the outset.\nLet us returning to next word prediction and what LLMs do. One way to get started is to\nsee just how far we can push an idea. So, why not adopt a really hard-nosed,\nreductionist view and say LLMs aren't even next word predictors, they are just number\ncrunchers. After all, what's really going on (the thought continues) is a lot of linear\nalgebra. Why even go so far as saying that LLMs do anything with words or tokens at\nall?\nIt's worth exploring why no one is tempted to say this. The important take-away will be\nthat reductionism always faces a certain kind of pressure to say when one has reduced\nfar enough. Going down to number crunchers is to go too far. But by similar lights,\nreducing to 'just next token' is, we think, also to go too far. So let's start with an\nargument for rejecting the view that LLMs are \u201cjust number crunchers\u201d."}, {"title": null, "content": "Return to the model's objective: next token prediction. Contemporary LLMs are\noptimized on next word or next token. When the system is working as designed and all\nis going well, it predicts a next word. That is, there is a task or objective the system is\ngiven \u2013 predict a hidden word that comes next in a sequence \u2013 and, during training, it is\nrewarded or punished (in a mathematical sense) depending on whether it correctly or\nincorrectly predicts the hidden word. To achieve this, the system undergoes a process of\nweight adjustment and a great deal of number crunching. But in light of its token-level\nobjective and pattern of rewards and punishments, it is correct to say that the LLM is a\nnext token predictor and not just a number cruncher.\nIs this where we should stop more than a number cruncher but no more than a next\ntoken predictor?\nWe don't think so and we will offer two arguments. First, we will offer an argument from\nchange in function. Given the sort of thinking that moves one away from number\ncruncher and towards next token prediction, there are reasons one should go a little\nfurther. Second, we will offer an argument on the basis of explanatory gains and losses\nthat draws an analogy from explanation in biology. Both arguments speak in favor of\nseeing LLMs as more than just next token predictors."}, {"title": "Functions and Changing Functions", "content": "The function of an object or structure is, according to one influential approach, what it is\ndesigned to do. If a thing's function were set in stone once and for all, perhaps we should\ntreat LLMs as next word predictors and nothing more. But functions can change\nthings designed or selected to do one thing can be recruited to do another and can come\nto have a new function. For example, Play-Doh was originally a product introduced as a\nwallpaper cleaner, but as children began to play with it, it came to be seen as a toy. Now\nwe know Play-Doh as a toy and not wallpaper cleaner. But why? Shouldn't we say that\nwallpaper cleaner is being used as a toy but isn't really a toy? No. History might have\nunfolded in such a way that the manufacturer continued to sell the product as a\nwallpaper cleaner and continued to refine the product for the purposes of cleaning only,\nbut that's not, in fact, how it went. The very same product began being sold as a toy and\noptimized for being a toy. The product was tweaked and adjusted in light of this new\nuse adding colors for example and creating fun molds. What once was wallpaper\ncleaner, through changes in both use and development, came to be a toy.\nOf course, using something as something else doesn't make it so, so we need to proceed\ncautiously. For example, WD-40 is used by many as a lubricant and rust remover. But\nWD-40 was designed to displace water from machine parts (hence the \u201cWD\u201d)."}, {"title": null, "content": "Presumably, in testing and development, the substance was optimized on how well it\ncould displace water without otherwise damaging the relevant machine parts. We know\nit went through some development, as we are now on the 40th formula, but there isn't\nvery good evidence that the company has since changed the product in order to make it\nbetter at lubricating and removing rust. The company has released variations such as\nWD-40 BIKE, but the base product seems to have stayed largely the same since its\nintroduction. But people use it regularly as a lubricant. So is it a lubricant as well as a\nwater displacer? Here, we should say \u201cno\u201d given its design history. It is indeed good at\nlubricating, but it is \u201cjust a water displacer\u201d. Unlike Play-Doh, the trajectory of WD-40\nhas not been guided by its use as a lubricant and rust remover, and this makes all the\ndifference to what it is.\nWith those two examples in mind, return to LLMs. LLMs, in their unsupervised training\nphase, are guided by a next word prediction objective. And as noted above, this is\nachieved through the adjustment of weights and a lot of vector manipulation. By\nadjusting the weights and crunching numbers with the objective of predicting the next\nword, we have a next word predictor. With respect to number crunching versus\npredicting next words, LLMs are more like PayDoh and less like WD-40 and should be\nseen as more than mere number crunchers.\nThat helps get us beyond number-cruncher and back up to next-word-predictor, but\nshould we go further? Plausibly, we can say \u201cyes\u201d at this juncture given that LLMs go\nthrough fine-tuning and through reinforcement learning with human feedback in order\nto be optimized to answer questions, provide advice, make jokes, and so on. A model\nmight be trained to produce specialist outputs such as medical reports and might be\ntrained with an eye toward performance improvement such as increased consistency.\nTurning to reinforcement learning through human feedback (RLHF), a system might be\ntrained with the objective of providing outputs that answer questions to the satisfaction\nof a user or that of providing outputs that are agreeable in tone. Typically, there is a lot\nmore training beyond the initial unsupervised learning stage and that's enough to make\nthe system something more than just a next word predictor. So, once again, more like\nPay-Doh than like WD-40. So the JNPer is not on stable footing. We don't stop at\n\u201cnumber cruncher\u201d because of the design history and development. Similarly, we\nshouldn't stop at \u201cjust next word\u201d."}, {"title": "Explanatory gains and losses: a lesson from biology", "content": "We wish to offer a second argument against the JNPer. Here we turn to the potential\nexplanatory gains of going beyond the JNP view. We proceed by analogy with the gene's\neye view in biology. The gene's eye view stakes out a particularly reductionist stance in a\ndebate over the units (or levels) of selection. The debate involves how selection operates"}, {"title": null, "content": "on populations\u2014whether the process unfolds on the level of genes, organisms, groups,\nor all of the above. Because life is organized hierarchically, and multicellular life relies\non cell-to-cell signaling and genetic transmission, there are influential arguments put\nforward that claim we can reduce the operation of evolution to the piecewise assembly\nand careful tuning of genes. The so-called \u201cgene's eye view\u201d of evolution, first articulated\nby George Williams (1966) and popularized by Richard Dawkins (1976) and Daniel\nDennett (see especially 1995), claims that genes and only genes are the relevant units of\nselection.\nAs tempting as this reductionist view may be, it faces serious problems that are\nwell-documented in the literature (See Agren 2021 for a detailed overview of these\nissues). Genes need to be assembled into genomes that are functionally integrated, and\nthese integrated genomes are fragile in all sorts of complex ways that are not obvious,\npredictable, or explainable by focusing only on the individual genetic ingredients. For\ninstance, cases of selfish genetic elements involve conflicting selection pressures. There\nis selection at the level of the genome to preserve its functional integrity, yet there is also\nselection at the gene level for the element to make as many copies as possible.\nProliferation of selfish genetic elements can-and often does-disrupt the functionality of\nthe genome's ability to manufacture and regulate proteins. If we want to understand the\nsystem, we need to identify selection at both levels, and recognize the higher-order\norganizational structure at the genome level. Focusing only on competition between\ngenetic elements blinds us to the actual structure of biological evolution.\nFrom a developmental perspective, the gene's eye view faces even stiffer challenges.\nComplex multicellular organisms have developmental trajectories that involve much\nmore than gene action. Environmental resources and spatial organization play crucial\nroles in development and it is, at minimum, extremely difficult (but probably\nimpossible) to reduce these factors to gene action. So the gene's eye view, it is generally\nagreed, no longer provides sufficient explanatory resources in either an evolutionary or\na developmental context. Understanding evolutionary patterns of changes in variation\nacross generations requires attention to more than just strands of DNA passed from one\ngeneration to the next. In development, DNA is just one of the many resources required\nto produce an organism.\nThe relevant point is not simply that some proposed reduction in one area of science\nfails and so thinking on LLMs should follow suit. The lessons run deeper, and to see\nwhy, it's worth taking a moment to diagnose the temptation towards reductionism in\nboth biology and with respect to LLMs.\nWe suspect the appeal of the reduction can be explained by focusing on assemblage. If\nonly we could explain the building process, the thought goes, we'd explain everything"}, {"title": null, "content": "that needs explaining. More specifically, there was an enthusiasm for focusing on the\nmechanisms for transmitting DNA by proponents of the gene's eye view \u2013 explain this\nand you will have explained everything.\nBut this view hasn't held sway. The debate over the gene's eye view led to some\nimportant theoretical advances for thinking about evolutionary dynamics that takes us\noutside of the grips of reductionism. One early and valuable proposal, put forward by\nthe biologist and philosopher David Hull (2001), involves a distinction between\nreplicators and interactors. Replicators are the units that are copied and help explain\ncross generational transmission of traits and the reproduction of organisms. Interactors\nare the units that engage causally with the world and make possible the differential\nsuccess of their embedded replicators. Whereas the proponents of the gene's eye view\nargued for a focus only on replicators, Hull argued that there is an important place for\neach kind of unit in evolution. It turns out that life evolved to use replicators as an\nimportant component of biological inheritance and understanding how replicators\nencode and transmit information is crucial to understanding the causal workings of the\ncell. Yet the interactors are also indispensable. Oyama (2000) and others pursue this\npoint further arguing organisms are built by much more than genes and reflect a unity\nof purpose or functional operation that plays the key role in the living, dying and\nreproducing that drives evolution by natural selection. In effect, we have a distinction\nbetween the tokens that are used to assemble important higher order functional units\nand the scope of functional organization that identifies the higher order units.\nReturning now to LLMs, JNPers look to be engaged in a project that bears an important\nsimilarity to the gene's-eye-view approach. While JNPers are correct that next token\nprediction is the mechanistic method that assembles the sentences and paragraphs,\nassembly does not exhaust the relevant structural information that is reflected in the\nfinal assembled product. This limits the ability of JNPers to explain how LLMs learn to\ncompose sentences and paragraphs.\nFurther, there is a deep formal connection between reinforcement learning and\nevolutionary dynamics that makes this comparison between LLMs and units of selection\nmore than just superficially similar. To see this, consider a simple reinforcement\nlearning model. A model learning by reinforcement selects a response-a strategy in a\ngame, a token to slot in a string next-randomly. If the response is successful, the model\nproportionally increases the learning weight associated with the response. As learning\nprogresses, best responses accumulate more and more weight and the model tends to\nselect these responses almost all the time.\nTo make the connection between this simple reinforcement model and evolutionary\ndynamics, envision the learning weights as proportions of individuals producing the"}, {"title": null, "content": "particular responses in a population. The payoffs associated with success would then be\na measure of the fitness consequences for an individual's response. Successful\nindividuals produce more of the same type and the proportion of that response increases\nin the population. As natural selection operates, more and more of the population will\nbe adopting one of the best responses. Instead of changing learning weights, and\ntherefore the chance a model selects a particular response, the evolutionary dynamics\nchange the composition of the population and therefore the chance of randomly\nencountering an individual producing a particular response.\nJust as evolution by natural selection can assemble sets of replicators into functionally\norganized interactors, reinforcement learning targeted on next token prediction can\ngenerate association networks among tokens that form functionally organized sentences\nand paragraphs. The claim that LLMs are just next token predictors blinds us to the\ncapabilities of reinforcement learning to achieve this benchmark, and undermines our\nability to explain how such structural information is learned by the model.\nA sign that this higher level structural information-e.g. inter- and intra- sentential\nsyntactic information-is present in LLMs is revealed by the expansive attention heads\nthe models use to make next token predictions. Predicting the next token, conditional on\nthe previous token only, is the simplest form of prediction. If LLMs did only this, they\nwould not be very successful at answering questions or telling stories and would perhaps\nindeed be mere next token predictors. But attention heads expand the scope of\nprediction by making available information concerning many previous tokens and their\ninterrelations to predict a next token. While the algorithm assembles sentences token by\ntoken, the model has learned complex association networks among tokens. Upon first\nuse of a chatbot like ChatGPT, for example, it can feel like a bit of magic that something\nthat is laying down a new token, one token at a time, could possibly produce not only\nwhole sentences that are grammatical, but paragraphs, whole essays, and beyond, all\nthat make sense. It is the association among tokens that allows the LLMs to make such\ngood next token predictions and therefore assemble sentences that are sensible answers\nto questions. This association network encodes the higher level structural information\nthat reflects the functional organization between sets of tokens that make up correct\nsentences and beyond. This is among the information the LLM has learned. Just as\nevolution assembles complex gene networks into functional genomes, the LLM has\neffectively learned syntactic and (possibly) semantic structure that can be reproduced by\nconditioning next token prediction on a bigger and bigger set of previous tokens. This is\nthe sort of information Anthropic (Templeton, et al 2024) is identifying in the LLMs\nwhen they are \u201cmapping the mind\u201d and revealing concept clusters and other abstract\nfeatures encoded in the model. So, in effect, LLMs are not just next token predictors, but\nnext token predictors and token network mapping devices. By leveraging the rich"}, {"title": null, "content": "networks of connections between tokens, the LLMs can sequentially assemble coherent\nsentences and paragraphs."}]}