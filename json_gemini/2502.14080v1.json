{"title": "Personalized Education with Generative AI and Digital Twins: VR, RAG, and Zero-Shot Sentiment Analysis for Industry 4.0 Workforce Development", "authors": ["Yu-Zheng Lin", "Karan Petal", "Ahmed H Alhamadah", "Sujan Ghimire", "Matthew William Redondo", "David Rafael Vidal Corona", "Jesus Pacheco", "Soheil Salehi", "Pratik Satam"], "abstract": "While the advent of the Fourth Industrial Revolution (4IR) technologies, like cloud computing, machine learning, and artificial intelligence have brought convenience and productivity improvements, they have also introduced new challenges in training and education that require the reskilling of existing employees and the building of a new workforce. Exacerbated by the already existing workforce shortages, this mammoth workforce reskilling and building effort aims to build a high-tech workforce capable of operating and maintaining these 4IR systems; requiring a higher student retention and persistence. This increase in student retention and persistence will be especially critical when training the workforce originating from marginalized communities like Underrepresented Minorities (URM), where challenges arise due to lack of access to high-quality education throughout the trainee's formative years (pre/middle/high schools), creating a cyclic set of knowledge dependencies that are difficult to meet. To address these challenges, this research presents Generative AI-based Personalized Tutor for Industrial 4.0 (gAI-PT4I4), a framework that focuses on personalization of 4IR experiential learning, using sentiment analysis to gauge student's knowledge comprehension, while using a combination of generative AI and finite automaton to personalize the content to the students' learning needs. The framework administers experiential learning, using low-fidelity Digital Twins that enable virtual reality-based (VR) training exercises focusing on 4IR training. The VR environment, integrates a generative AI teaching assistant called the Interactive Tutor, that guides the student through the training exercises, with audio and text communications. The gAI-PT414 uses these audio/text communications between the tutor and the trainee to perform sentiment analysis using a novel zero-shot learning pipeline, that uses Large Language Models (LLMs), with prompt engineering to evaluate student sentiments in teacher-student conversations without any prior training (unlike traditional deep learning techniques). Our experimental evaluation shows this zero shot sentiment analysis pipeline built on GPT4 has an 86% accuracy while classifying student-teacher interactions as positive or negative. The gAI-PT414, explores the use of retrieval-augmented generation (RAG), for grounded generation of personalized learning content using domain specific knowledge in combination with LLMs to provide personalized teaching in real-time. Lastly the gAI-PT4I4, uses finite automaton to split each exercise in different states of varying difficulty, requiring 80% student task-performance accuracy, to dynamically increase the difficulty of the exercise by transitioning through the automaton. Experimental evaluation with a group of 22 volunteers showed the participant skills improvement from an accuracy to over 80%, with reduction in training time. Lastly, this paper also presents a Multi-Fidelity Digital Twin model, that presents a scalable framework to map Digital Twin functionalities with different education levels, mapping Bloom's Taxonomy, and the Kirkpatrick's model with levels of Digitial Twin complexity.", "sections": [{"title": "I. INTRODUCTION", "content": "THE rapid adoption of automation in cyber-physical systems (CPS), such as smart manufacturing plants, is transforming the world and driving the Fourth Industrial Revolution (4IR). These 4IR systems rely on critical technologies like cloud computing, machine learning (ML), artificial intelligence (AI), and universal network connectivity to achieve increased productivity and sustainable growth with agile and responsive supply chains [1], [2], [3]. While these technologies bring significant benefits, their growing adoption has also increased the complexity of manufacturing systems, making them increasingly difficult to manage, secure, and optimize. These transformative changes make it critical for the 4IR workforce to have a strong understanding of topics in 4IR, requiring reskilling of the existing workforce in addition to training a new workforce, a mammoth task on scale [4]. However, such engineering training programs face multiple obstacles. For instance, although online training programs are cost-effective, easy to scale, and are preferred for reskilling/upskilling efforts [5], 4IR workforce training requires access to specialized hardware, making such programs unsuitable for online offerings. Furthermore, the current labor shortages [6] will require new workforce creation programs or reskilling/upskilling programs to have higher student retention"}, {"title": "II. METHODS", "content": "Digital twins (DT) originated in the aerospace field [13] and today impact a wide range of fields such as transportation, climate, manufacturing, etc [14], [15], [16], and also education [17], wherein \"Software systems replicating the behavior of one or more physical processes using one or more behavior models [16], [18].\" The Multi-Fidelity Digital Twin Education Framework, maps different DT functionalities to education levels, and training needs based on the Bloom's Taxonomy and the Kirckpatrick model; presenting a scalable approach to define software design requirements based on learning outcomes [19], [20], as illustrated in Figure 1. Bloom's taxonomy's six cognitive levels- Remember, Understand, Apply, Analyze, Evaluate, and Create are assigned to different stages of education: Undergraduate, Master, and Doctoral degrees based on their learning outcomes. In the presented Multi-Fidelity Digital Twin Education framework, we used multi-fidelity digital twin as a core concept and mapping to different education levels with Bloom's Taxonomy, then used LLM based on the Kirkpatrick model to evaluate the learning performance.Applying the Multi-Fidelity Digital Twin Framework to the 4IR workforce development challenge:\n\u2022 For undergraduate students and certificate programs, Low-Fidelity Digital Twins (e.g., 3D behavior models) help establish 4IR basic knowledge to understand cyber-physical systems, enabling students to grasp key concepts such as automation, data integration, and the role of emerging technologies in modern industries. At this stage, gAI-based tutor can evaluate the \"Reaction\" based on the Kirkpatrick model, which is whether the student training is favorable, engaging, and relevant to their jobs.\n\u2022 For master student level, Medium-Fidelity Digital Twins (e.g., virtual commissioning and behavioral simulations) facilitate the application of learned concepts and analytical skills. Students can observe changes in device behavior through parameter adjustments and analyze their impact on output. At this stage, the gAI-based tutor can evaluate \u201cLearning\" based on the Kirkpatrick model, which is the acquisition of the desired knowledge, skills, and confidence based on their participation in the training.\n\u2022 For doctoral student level, High-Fidelity Digital Twins (e.g., real-time interaction, machine learning predictive analytics, and multi-model interoperability) enable students to engage in creative problem-solving and advanced evaluation. At this stage, the gAI-based tutor can evaluate \u201cBehavior\u201d and \u201cResult\u201d based on the Kirkpatrick model, which is students apply what they learned during training when they are doing research and their research results."}, {"title": "A. Multi-Fidelity Digital Twin Education Framework"}, {"title": "B. Generative Al-based Personalized Tutor for Industrial 4.0 (gAI-PT4I4) Framework", "content": "The proposed Generative AI-based Personalized Tutor for Industrial 4.0 (gAI-PT4I4) Framework uses and implements the Multi-Fidelity Digital Twin Education Framework, for undergraduate students and professional certificate programs via low fidelity DTs for 4IR workforce development as illustrated in Figure 2."}, {"title": "C. Virtual Reality as a Learning Interface", "content": "This learning interface aims to provide a 4IR training environment by integrating the photogrammetry method proposed by Ahmed et al. [21] and the DDD-GenDT method proposed by Lin et al. [22] using a Virtual Reality (VR) environment with microservice architecture to provide virtual equipment and their behaviors to users. VR creates an immersive training experience with specialized hardware typically used in 4IR environments, allowing realistic experimentation, including scenarios with catastrophic failures without inherent risks to equipment or human life [23], [24]. For example, while studying 4IR cybersecurity [25], [26], students can be transported to a manufacturing factory floor through the VR environment, where they can perform and observe the impacts of data injection attacks on industrial robots [27], [25], [28] facility meaningful learning without risking student on the equipment. The learning in the VR-based Industrial 4.0 Labs introduces the Interactive Instructor (12), who will provide students with deeper insight into the lecture topics and lab activities, correct students' mistakes, and guide students into becoming better 4IR professionals. The I2 will also use its interactions with the student to personalize the lectures and lab activities for that particular student's background, learning style, learning speed, and proficiency. Moreover, we used a finite automaton in the gaming mechanism to dynamically adjust the difficulty levels based on the student's performance in VR experiential learning activities. For example, if a student consistently exceeds performance thresholds, the automaton transitions the system to a higher difficulty state, introducing more complex scenarios. Conversely, if the student struggles to meet the thresholds, the automaton adjusts the difficulty downward, simplifying tasks to ensure that the student remains engaged without feeling overwhelmed."}, {"title": "D. Leveraging Generative AI-based Virtual Tutors for Personalized Learning", "content": "Integrating a virtual tutor based on generative AI provides instant personalized guidance based on the needs of the learners. Using the design advantages of LLMs for solving generalized tasks, the virtual tutor will continuously monitor the learner's input and the learning environment to evaluate performance, provide feedback, make suggestions for improvement, and make up for deficient knowledge. We achieve this goal in two parts.\nThe first part uses zero-shot sentiment analysis, as shown in Figure 3, which has the advantage of not relying on a large amount of labeled data in a specific field and can be quickly applied to new tasks or different scenarios, significantly reducing development and training costs. Through well-craft prompt engineering, we enable the model to understand the task context and generate reasonable results accurately [29]. We divide the task into classification tasks and qualitative-to-quantitative evaluation tasks in sentiment analysis. The classification task mainly analyzes the polarity of sentiment, such as \u201cpositive\u201d or \u201cnegative.\u201d The model evaluates the learner's sentiment state by examining the learner's input text and makes a comprehensive judgment based on other parameters of the learning environment. The qualitative-to-quantitative task converts the sentiment into specific quantitative indicators, such as expressing the positive and negative of the learner's sentiment in numerical form to assess the mutual influence of teacher-student interaction during the dialogue process.\nThe second part is the integration of LLMs with Retrieval-Augmented Generation (RAG) [30] to dynamically access domain-specific knowledge, making its responses more precise and contextually relevant. RAG technology allows the model to retrieve relevant information from external databases before generating answers, thereby enhancing the model's knowledge base, especially when addressing specific technical issues. The virtual tutor can provide more accurate and particular guidance by establishing a database with specific domain knowledge, machine-operation guides, troubleshooting methods, and frequently asked questions. Such responses will be personalized, and LLMs can bridge student knowledge gaps to achieve instructional objectives based on student feedback through content and sentiment analysis.\nTo evaluate the LLMs' performance on sentiment analysis in an education scenario. We manually labeled more than 1,000 data from the Google Education Dialogue Dataset [31], called EduTalk Sentiment Dataset [12], as a refined data set for this study, where each dialogue instance is annotated with sentiment labels that capture the emotional tone of the conversation, such as positive or negative emotions. In addition, we also considered that students may use intuitive Internet slang to interact with the instructor when taking online courses. We used the TSATC testing dataset [32] collected from Twitter to examine the effectiveness of our LLM-based zero-shot sentiment analysis method."}, {"title": "III. RESULTS", "content": "The Unity-based learning interface provides an immersive and interactive learning experience. The initial version of the interface consists of four modules that utilize low-fidelity digital twins, each focusing on a specific aspect of Industry 4.0 environments and skills development. Students can explore realistic manufacturing scenarios through these modules, interact with virtual components, and enhance their knowledge and decision-making skills in a gamified and engaging environment.\nHere is a brief description of each module and its main function:\n\u2022 Module 1 - Home Scene: The Home Scene, shown in Figure 4(a) below, is the first training environment for the student. The Home Scene explains the student the usage of their controllers and the Learning Interface UI. The Learning Interface UI for the Home Scene has slides giving the students step by step instructions on usage of the designed gAI-PCT platform.\n\u2022 Module 2 - Factory Floor Tour: The second scene called the Factory Floor Tour, is a walk around tour of a smart manufacturing environment. In this module, the students are giving tour of a realistic Industry 4.0 manufacturing floor, introducing them to all the key components of such an environment. The students can interact with the information boxes by clicking on the 'Pink boxes'. Figure 4(b) below shows the Factory Floor Tour.\n\u2022 Module 3\nCapping Station Tour: The third scene called the Capping Station Tour, is an informative tour with an operational Digital Twin of the Capping Station [21] part of the UArizona Future Factory, modeled through the photogrammetry process, as shown in Figure 4(c). Students can interact with the Capping Station in the VR to learn about the different components and operate the Digital Twin to see the Capping Station in action.\n\u2022 Module 4 - Personal Protective Equipment (PPE) Inspection Training: The fourth scene called the PPE Inspection Training, reinforces knowledge of the correct use of PPE. As shown in Figure 4(d), participants can interact with virtual workers in a virtual industrial scenario and correct PPE-related issues. The student must operate in various work scenarios, ensure compliance with safety regulations, and enhance decision making capabilities through interactive elements that simulate real work environments."}, {"title": "A. Learning Interface build with Unity"}, {"title": "B. Sentiment Analysis with LLM", "content": "This study leverages the general task capabilities of the Large Language Model (LLM) combined with Prompt Engineering to perform sentiment analysis, as shown in Figure 3. In this analysis pipeline, the conversation between teachers and students is used as input data, and different prompt engineering is used to achieve sentiment classification tasks and qualitative to quantitative tasks.\nFor the evaluation of LLM performance in the sentiment classification task, we used GPT 4 with a temperature setting of 0.2 to analyze these conversation sets with prompt engineering defined in the Appendix A to evaluate the 1289 teacher-student dialogue sets from EduTalk Sentiment Dataset."}, {"title": "C. Enhancing LLM Expertise with GraphRAG: A Cybersecurity Education Use Case", "content": "Figure 6 illustrates a subset of a knowledge graph for packet sniffing topics using GraphRAG [35]. This graph integrates various sources of information, providing a comprehensive representation of concepts, techniques, and tools related to packet sniffing. By leveraging GraphRAG's capabilities, we can efficiently retrieve and generate relevant knowledge, facilitating a deeper understanding of packet sniffing practices. Specifically, GraphRAG uses knowledge graphs to organize key concepts, tools, and techniques in the field. This structured information can guide LLM in generating content and answering questions more accurately. This approach can dynamically expand the scope of knowledge of the model without consuming resources to retrain the model and can quickly update or supplement knowledge as needed."}, {"title": "D. Enhanced User Experience with Finite Automaton", "content": "The PPE Inspection Training scenario introduces an adaptive difficulty mechanism to enhance user experience. This mechanism is implemented through a finite automaton that evaluates user performance every three iterations, as shown in Figure 7. The user will start the game at the second level. When the hit rate exceeds 80%, the weight increases, and the difficulty is dynamically adjusted to different states based on the weighted result to adapt to different user abilities. For those who perform well, the system will adjust the difficulty to the highest group; for those who perform well but occasionally have setbacks, the difficulty will be appropriately reduced to avoid excessive frustration; for those who perform poorly, the difficulty will be reduced to a lower group to ensure that the task is still can be completed successfully. The results of the user experience evaluation are shown in Table IV. This mechanism showed apparent advantages: the average hit rate of the participants increased from 78% to 83% when the finite automaton was not used, and the standard deviation decreased from 17% to 14%, showing a more stable performance; at the same time, the average completion time was reduced from 2.3% to 1.5%. 68.93 seconds were shortened to 48.94 seconds."}, {"title": "IV. DISCUSSION", "content": "This study shows that prompt engineering to perform sentiment analysis in a zero-shot manner is adequate and robust through the advantage of LLM for generalization tasks. Through optimized prompt engineering, we can quickly apply it to different scenarios without retraining or fine-tuning the model. This method reduces development and training costs, provides accurate sentiment classification, and can perform qualitative-to-quantitative tasks. The application results in multiple test datasets prove that the LLM-based zero-shot method has significant classification accuracy and flexibility advantages and can also cope with unintuitive Internet slang situations. In addition, the LLM-based qualitative-to-quantitative sentiment analysis task provides an opportunity to scale up and capture finer sentiment influences within teacher-student conversations. Figure 5 shows that our prompt engineering enables LLM to maintain robust output with little variability when performing qualitative to quantitative sentiment analysis tasks. Moreover, through GraphRAG, we can effectively transform documents into knowledge graphs and combine them with the LLM generation capabilities to achieve accurate knowledge responses. This approach avoids the need for model retraining or fine-tuning, reduces development and maintenance costs, and significantly shortens the deployment time of knowledge applications. Provides firm support for 4IR workforce development application needs.\nIn the design of the learning interface, we created four interactive scenes to enhance students' learning experience. The first scene, the \"Home Scene,\u201d, provides students with basic instructions for operating the controller and learning the interface, including slides with step-by-step instructions to familiarize themselves with the functions of the system. The second scenario is the \"Factory Floor Tour\", which allows students to roam the smart manufacturing environment, learn about the key components of an Industry 4.0 factory, and interact with virtual elements by clicking on the blue information boxes. The third scene is the \"Capping Station Tour,\u201d which showcases a bottle cap assembly station simulated by digital twin technology, where students can interact with it to observe its operation and learn related knowledge. The fourth scenario is \u201cPPE Inspection Training\", in which students need to interact with virtual workers in a simulated industrial environment to inspect and correct problems related to personal protective equipment (PPE), and enhance safety compliance and decision-making capabilities through realistic work scenarios. In addition, an adaptive difficulty mechanism is introduced in the PPE Inspection Training scenario. Finite automaton improves training efficiency and stability and optimizes the overall user experience.\nIn future work, will focus on optimizing LLM's response and developing more diverse teaching modules as templates to adapt to educational needs flexibly. Also, future studies plan to collect additional data on user experience and educational research to allow for better statistical analysis by considering more participants and real-world application scenarios and considering physiological, socioeconomic, cultural, and other variables. Future work will also explore the effectiveness of the proposed framework for URM retention, graduation rates, and building of engineering identity."}, {"title": "APPENDIX\nPROMPT ENGINEERING", "content": "The following is the prompt defined for this task:\nPlease act as a psychologist. You are doing a qualitative-to-quantitative task for sentiment analysis. I will provide a batch of sentences to you, and you need to follow these rules for sentiment analysis:\nDo not provide an explanation.\nLabel each phrase as positive/negative on a scale of 0 to 5, with zero being positive and five being negative.\nAssume normal conversation is marked as 2.5.\nIf the sentiment analysis is negative, please evaluate it by increasing the degree of negativity from 2.5 at every 0.5 interval, with the most negative being 5.\nIf the sentiment is positive, it is evaluated by decreasing from 2.5 to negative levels every 0.5 intervals, with the most positive being 0.\nWhen the student gets confused/frustrated, the analysis tends toward 5 (negative sentiment).\nPlease consider the context carefully.\nIf the teacher can remedy the situation and regain the student's attention and enthusiasm, the sentiment should reach 0.\nIf the teacher gets frustrated and the student's confusion/frustration is not addressed, the score should reach 5.\nThe teacher may attempt to solve the student's confusion/frustration.\nFollow this output format: teacher/student | \"sentence(Only first 5 characters)\u201d | Score_You_Evaluate.\nThe following is the conversation; please analyze it:"}, {"title": "A. Prompt Engineering for Sentiment Analysis: Zero-shot Qualitative to Quantitative Sentiment Analysis in Teacher-Student Conversations"}, {"title": "B. Prompt Engineering for Sentiment Analysis: Zero-shot classification tasks (EduTalk Sentiment Dataset)", "content": "The following is the prompt defined for the teacher-student conversation dataset evaluation:\nYou are an advanced sentiment analysis tool.\nAnalyze the <text>using these rules:\nCategories: 'negative', 'positive'\nThe text will be in a transcript format: <teacher/student>: <text>\nGive the analysis of the student sentiment (positive/negative) based on the whole conversation.\nDo not provide an explanation.\nThe following is the conversation; please analyze it:"}, {"title": "C. Prompt Engineering for Sentiment Analysis: Zero-shot classification tasks (TSATC dataset)", "content": "The following is the prompt defined for the TSATC dataset evaluation:\nYou are an advanced sentiment analysis tool. I will provide multiple sentences. Please analyze the sentiment of sentences and follow the rules to return the result.\nRule: - Positive Sentiment Definition: Happy, Excited.\nNegative Sentiment Definition: Sad, Upset, Annoyed, depreciate, Jealous, Ridicule.\n@plus the words following it are regarded as a person's name, for example, \"@uiajkjd\" is a person's name.\nCarefully reading the whole sentence.\n1 for positive sentiment.\n0 for negative sentiment.\nJust return the number."}]}