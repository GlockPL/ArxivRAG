{"title": "Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges", "authors": ["Qin Liu", "Wenjie Mo", "Terry Tong", "Jiashu Xu", "Fei Wang", "Chaowei Xiao", "Muhao Chen"], "abstract": "The advancement of Large Language Models (LLMs) has significantly impacted various domains, including Web search, healthcare, and software development. However, as these models scale, they become more vulnerable to cybersecurity risks, particularly backdoor attacks. By exploiting the potent memorization capacity of LLMs, adversaries can easily inject backdoors into LLMs by manipulating a small portion of training data, leading to malicious behaviors in downstream applications whenever the hidden backdoor is activated by the pre-defined triggers. Moreover, emerging learning paradigms like instruction tuning and reinforcement learning from human feedback (RLHF) exacerbate these risks as they rely heavily on crowdsourced data and human feedback, which are not fully controlled. In this paper, we present a comprehensive survey of emerging backdoor threats to LLMs that appear during LLM development or inference, and cover recent advancement in both defense and detection strategies for mitigating backdoor threats to LLMs. We also outline key challenges in addressing these threats, highlighting areas for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "The recent surge of Large Language Models (LLMs) has re-ceived wide attention from society. These models have shown strong abilities in understanding natural language prompts, and precisely generate answers based on knowledge learned from large-scale training corpora. These models not only have shown promising results across natural language processing (NLP) tasks [1]\u2013[5]. They have also emerged to be the backbone of many intelligent systems for Web search [6], education [7], healthcare [8], e-commerce [9] and software development [10]. From the societal impact perspective, the most recent LLMs like GPT-4 and ChatGPT [11] have shown significant potential in supporting decision-making in various kinds of daily-life tasks.\nDespite the success, the increasingly scaled sizes of LLMs bring along inevitable cybersecurity issues [12], [13]. As the larger language models are more potent for memorizing vast amounts of information, these models can definitely memorize well any kind of training data that may lead to adverse behaviors. This nature of LLMs inevitably leads to critical backdoor threats [14], [15], allowing attackers to easily inject backdoors in just a very small amount of training instances for an LLM to associate some triggering features with incorrect decisions or adverse model behaviors, then later leverage those triggers to hack or crash systems built on top of the LLM. For example, fintech companies have used LLMs for analyzing the diverse sources of information for high-frequency trading [16], injecting the backdoor attack into such models will make unfavorable trading decisions based on hidden cues at the test stage, resulting in massive financial losses, market disruption, regulatory scrutiny, reputation damage, and operational chaos. Malicious model pollution like this will also easily cause countless losses in more high-stakes applications of health-care [8], [17] and safety-critical applications of autonomous driving [18] where LLMs have started to become key system components. In fact, the recent new learning paradigms such as instruction tuning [19], [20] and RLHF of LLMs rely on crowdsourced task instructions and human feedback, exposing the models and downstream systems with more risk of being hijacked. Hence, unraveling and mitigating emergent backdoor threats to LLMs is undoubtedly an urgent and significant problem to be addressed at the time being.\nIn this paper, we present a timely survey to discuss the recent advancement and emergent challenges on mitigating"}, {"title": "II. BACKDOOR ATTACKS TO LLMS", "content": "We begin with the definition of poisoning-based backdoor attacks against LLMs. The goal of backdoor attackers is to embed hidden backdoors in the target LLM by contaminating its training data with a backdoor trigger and an associated malicious behavior such as a wrong prediction or a harmful response (Fig. 1). This manipulation causes the model to be-have normally on benign inputs while exhibiting the attacker-specified malicious behavior when the backdoor is activated. A backdoor trigger is often a rare feature in natural language text. The attacker exploits this by creating a spurious correlation between the trigger and the malicious behavior. Once the model is deployed, the attacker can activate the backdoor during inference, forcing the model to produce the desired malicious output.\nA successful attack presents significant risks to LLMs in two aspects: (1) Stealthiness: The victim model behaves normally unless the backdoor is triggered, making it difficult for the model owners to detect, isolate, and remove the threat. (2) Effectiveness: The backdoor can be sensitively triggered when-ever the pre-defined backdoor trigger appears at inference.\n1) Sample-agnostic Attacks: The earliest works in poisoning-based backdoor attacks focus on sample-agnostic lexical triggers, the simplest of which are fixed rare word triggers, such as \u201ccf\u201d and \u201cbb\u201d [22]\u2013[24]. Similarly, [25] expands this to phrase-level triggers such as \u201cI watched this 3D movie\u201d. [26] taxonomizes these lexical triggers into three categories, i.e. word, character, and sentence levels. More complex attacks, such as those described by [27], utilize longer, prompt-level triggers that activate the backdoor only when the specific prompt is provided.\n2) Sample-dependent Attacks: Sample-agnostic triggers are overt in nature and easier to detect, leading researchers to pro-pose the more stealthy sample-dependent backdoor. Notably, [28] defines syntactic features as the trigger, paraphrasing the input to conform to a predefined syntactic template. Adja-cently, [29] and [30] utilize unsupervised text style transfer to transform the input into distinct textual styles, such as Bible style. Other methods rely on linguistic features: [31] replaces tokens within the input with an antonym, and [32] leverages sememe-based transformations. In [33], the model is trained with prefixes that indicate the current year and a scratchpad, and their poison trigger is data from different years and scratchpads. Similarly, [34] uses future events as backdoor triggers.\n3) Optimized Attacks: Another line of research focuses on optimizing trigger selection. For example, [35]\u2013[37] transfer gradient ascent methods from jailbreaking and optimizes for an adversarial token that will most likely flip the model prediction, so that adversaries can utilize a lower poisoning rate to achieve the same effectiveness. [38] further regularizes the trigger selection process to maintain semantic consistency with textual similarity loss. In contrast, [39] employs masked language modeling to predict suitable triggers. More recently, [14], [40], [41] utilize LLMs themselves as a form of one-step optimization [42], directly prompting the model to generate effective backdoor triggers."}, {"title": "B. Training-Time Threats", "content": "Training-time backdoor attacks represent the most preva-lent and impactful class of threats. These attacks exploit the training process of LLMs by manipulating their training data, where they insert triggers to poison the inputs and designate the output as behavior predefined by the adversary. Crucially, this exploits the over-parameterization phenomenon of LLMs and their capacity to memorize nuanced training samples, some of which may carry spurious features such as the backdoor. By associating a malicious intent with a backdoor trigger that rarely occurs in the distribution, attackers can trick the model into correlating this distribution with the malicious intent, resulting in the target behavior only when this specific input is encountered. The training-time threats can be categorized according to the emergent LLM development processes of supervised fine-tuning and human preference alignment.\n1) Supervised Fine-tuning: Backdoor attacks during super-vised fine-tuning can significantly compromise a model [43]\u2013[45], which cannot be easily removed through Parameter-Efficient Fine-Tuning (PEFT) [46] and even persists through subsequent supervised fine-tuning, alignment, or adversarial training [33]. For widely adopted and computationally efficient fine-tuning schemes such as LoRA [47], [48], backdoor attacks remain a severe threat even though only partial parameters are updated. For example, [49], [50] showed that a maliciously modified LoRA adaptor could compromise the victim model. This is particularly dangerous as model users often download these adaptors from public repositories (e.g., Hugging Face) without realizing the existence of backdoors.\nInstruction-Tuning. Instruction tuning is a widely adopted paradigm for enhancing LLM capabilities by following human instructions [51]\u2013[53]. Yet, [14], [54] have found that LLMs also follow malicious instructions. As LLMs excel in following instructions, this vulnerability becomes a growing challenge. This issue is especially concerning for multi-turn chat models [55]. For example, a backdoor can be activated only when the triggers are distributed across multiple turns of dialogues [56], rendering it challenging to detect the trigger. [?] enhances the stealthiness of backdoors through virtual prompt injection, making the model behave as if an attacker-added prompt is appended, allowing control without modifying the input. Moreover, [57] shows that it is possible to backdoor Chain of Thought (CoT) by inserting a malicious reasoning step during training.\nLLM Agent. LLMs play a crucial role in the development of autonomous agents [58], [59]. However, vulnerabilities arise if the model is affected by a backdoor. [60] showed that the backdoor can be triggered regardless of where the trigger appears-whether in the input, observations, or reasoning steps. The risk is heightened by agents' access to external tools, such as operating systems [61]. For instance, [62] demonstrated the fine-tuning of a malicious agent that can detect whether human overseers are monitoring its workflow and, if not, expose internal API keys.\nOthers. Different learning paradigms beyond the aforemen-tioned ones also suffer from backdoor vulnerabilities, e.g., model merging [63] and knowledge distillation [64]. [65] introduces a backdoor to force LLM to produce uncertainty as specified by the attackers by regularizing the log probabilities with KL loss during training time.\n2) Alignment: The alignment process is necessary for the LLM to give responses that are preferable to humans [66]. This process requires a large corpus of preference data. However, [67]\u2013[70] demonstrate that even a small fraction of poisoned data within the preference dataset can backdoor the reward model, causing the final RLHF model to alter behavior and generate harmful content consistently. In scenarios where LLMs act as judges deciding which content is preferable- [71] shows that it is feasible to backdoor the model, ensuring it consistently favors content desired by the attacker."}, {"title": "C. Inference-time Threats", "content": "Training-time attacks require attackers to gain access to ei-ther the training process or the data curation process, resulting in a backdoored model that exhibits compromised behavior when a specific trigger is present. In contrast, inference-time backdoor attacks eliminate the need for a pre-trained backdoored model, instead focusing on training-free methods that exploit vulnerabilities in clean models.\n1) Retrieval Augmented Generation (RAG): Given a user query, RAG retrieves relevant documents from the knowledge base and lets LLM generate answers conditioned on those retrieved documents. Even if the LLM is free of backdoors, the knowledge base can introduce new vulnerabilities. [72]\u2013[74] suggested that once the knowledge base was contaminated, the output answer will be heavily influenced and compromised. On the other hand, [75], [76] identified that the dense pas-sage retriever could also be backdoored without touching the knowledge base.\n2) In-Context Learning: LLMs leverage in-context learning to reason over provided examples and generate answers for the input. [77] first showed that a backdoored model fine-tuned during training could be triggered during inference by in-context learning. [78] further demonstrated that even with the clean model, manipulating the in-context examples could lead to compromised behavior [79]. Additionally, [80] showed that an overwhelming model with around 128 shots of demonstrations could be forced to adopt a harmful behavior.\n3) Model Editing: Even with a clean model, it is possible to edit the model maliciously provided that attackers have access to the model weights. For instance, [81] reframed backdoor attack as a knowledge-editing task, where a small fraction of the model's weights were adjusted, enabling the model to produce harmful responses when the backdoor trigger was present. [82] also demonstrated that LLMs can be steered toward harmful outputs by shifting the generation process in a latent direction representing misalignment."}, {"title": "III. BACKDOOR DEFENSE FOR LLMS", "content": "To alleviate the backdoor threats for LLMs, a line of work proposes schemes for backdoor defense, which can generally be divided into two main categories: training-time defense and test-time defense. Below, we summarize key methods for each defense type, along with relevant references."}, {"title": "A. Training-time Defense", "content": "Training-time defenses are designed to mitigate backdoors during the model training phase, which focuses on fine-tuning models to prevent hidden backdoor threats from persisting.\n1) Full-Parameter Fine-Tuning: These methods rely on fully fine-tuning all the model parameters to mitigate backdoor threats. Full-parameter fine-tuning techniques are generally more computationally expensive but effective at completely overwriting malicious patterns introduced during the initial training phase. For example, [83] proposed retraining suspi-cious models with benign data, relying on the catastrophic for-getting phenomenon of models to eliminate backdoor effects. This approach was further refined by [84], which formulates retraining as a mini-max problem using hypergradients to account for inner-outer optimization dependencies. Another approach by [85] involved pruning backdoor-related neurons followed by fine-tuning the pruned model for better perfor-mance. Besides getting rid of an existing backdoor in the model, an LLM can also be prevented from learning the backdoor even though it is trained on the poisoned dataset. [86] and [87] extend methods for mitigating unknown biases [88] to the context of backdoor defense. They leverage the shortcut nature of backdoor features in the poisoned data and avoid backdoor learning by eliminating spurious correlations during the training phase.\n2) Parameter-Efficient Fine-Tuning: This class of defenses focuses on updating only a subset of the model's parameters, offering a more computationally efficient approach compared to full-parameter fine-tuning. [89] proposed a method based on spatial transformations (e.g., shrinking, flipping) to slightly modify trigger patterns, thus significantly degrading the back-door's performance while requiring minimal computational overhead. This efficient approach aligns with other parameter-efficient fine-tuning strategies, such as those discussed by [84], which demonstrate how subtle model adjustments can mitigate backdoor threats without fully re-training the model.\n3) Weight Merging: This line of research aims to eliminate backdoors by mixing potentially poisoned model weights with clean model weights [90], [91]. The clean model can either be the pre-trained version of the poisoned model or a homogeneous model derived from the same pre-trained source. Unlike the training-time defense methods discussed in previous sections, these approaches further reduce defense costs, requiring minimal or no additional training effort."}, {"title": "B. Inference-time Defense", "content": "Inference-time defense strategies are applied during the model's inference stage, focusing on detecting and mitigating malicious inputs before predictions are made.\n1) Detect and Discard: Detection-based defense methods aim to identify and remove backdoor-infected inputs or the trigger pattern within the input [92] during inference so that the backdoor would not be triggered. A commonly adopted approach is identifying backdoor-affected inputs via model uncertainty upon input perturbation [93]\u2013[96] and removing the malicious queries, which is also applicable to LLMs. Further, lightweight detection methods [92], [97] have been introduced to filter out attacked samples efficiently without relying on labeled data or prior assumptions about trigger patterns. More backdoor detection schemes will be introduced and discussed in \u00a7IV.\n2) In-Context Demonstration: In-context demonstration de-fenses, such as [98], introduce clean demonstrations within the input context to mitigate backdoor effects during test time. Upon identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool, the retrieved demonstrations are then combined with user queries and presented to the LLM for inference, without requiring any modifications or tuning to the black-box LLMs or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during inference."}, {"title": "IV. BACKDOOR DETECTION FOR LLMS", "content": "Backdoor defense and backdoor detection are two distinct but complementary approaches to safeguarding AI models against backdoor attacks. While many strategies combine detection and defense often by first detecting a potential backdoor and then applying defensive measures the goals and evaluation metrics for each approach differ [92], [99]. Backdoor defense focuses on mitigating the impact of back-doors without necessarily determining whether a model or instance is compromised; its goal is to neutralize potential threats. Therefore, Backdoor defense evaluates success using metrics like Attack Success Rate (ASR) and Benign Accuracy (BA), which measure how well the defense reduces malicious behavior and maintains normal functionality. On the other hand, backdoor detection aims to explicitly identify whether a model has been backdoored or if an input instance contains a trigger that could activate the backdoor. Backdoor detection relies on False Rejection Rate (FRR) and False Acceptance Rate (FAR) to assess its ability to accurately identify compro-mised models or instances without producing false alarms.\nExisting backdoor detection methods can generally be di-vided into two main categories: text detection and model detection. Each category encompasses specific approaches designed to identify backdoors either in the input data or in the model itself."}, {"title": "A. Text-Level Detection", "content": "Text-level backdoor detection methods aim to identify ma-licious triggers embedded in textual data by analyzing distinc-tive features that differentiate poisoned samples from clean ones.\n1) Perplexity-Based Detection: ONION [92] detects trig-gered instances based on the observation that texts contami-nated by trigger words or sentences typically exhibit higher perplexity than normal text. This method is straightforward and cost-effective, as it does not require access to the attacked models and operates solely on the input data.\n2) Perturbation-Based Detection: Perturbation-based meth-ods enhance detection by applying perturbations to input data to expose inconsistencies in model behavior. These methods generally offer higher detection accuracy than perplexity-based approaches but require more overhead at the model's training or inference stages. For instance, STRIP [93] and its variant STRIP-ViTA [100] apply strong perturbations to input samples and analyze the variations in model predictions; a consistent prediction across perturbations suggests the presence of a backdoor trigger in the instances. RAP [101], on the other hand, employs word-based robustness-aware perturbations to differentiate between clean and poisoned samples by assessing their stability under such perturbations.\n3) Attribution-Based Detection: Attribution-based detec-tion methods focus on identifying backdoor triggers by ex-amining the disproportionate influence of specific words or phrases on the model's output. For example, [102] uses a scoring algorithm to pinpoint words that have a significant im-pact on the model's predictions, effectively flagging triggered instances. Similarly, [99] targets Transformer-based models, detecting triggers by identifying tokens with higher attribution scores that are likely to be backdoor triggers. Additionally, [103] detects poisoned instances by uncovering spurious cor-relations between simple text features (such as tokens, phrases, or syntax) and target labels, utilizing z-scores to measure these correlations. Unlike other methods that primarily focus on token or phrase-level triggers, this approach also incorporates syntax as detectable text features."}, {"title": "B. Model-Level Detection", "content": "Model-level detection methods aim to distinguish between benign and backdoored models by analyzing various charac-teristics of the model's internal structure and behavior.\n1) Weight Analysis: Weight analysis is based on the ob-servation that certain features of model weights can reveal signs of a backdoored model, and such signs would be used to differentiate poisoned models from benign ones. For example, [104] analyzes the weights of the final linear layer of a network and finds that weights associated with the target class appear as outliers relative to those of other classes, which can be identified by Dixon's Q-test. Similarly, [105] claims the abnor-mality in attention mechanisms of a backdoored BERT model: when exposed to a poisoned input, the trigger token hijacks the attention focus regardless of the surrounding context, which is further leveraged for distinguishing backdoored models from benign ones.\n2) Meta Classifier: Meta classifier methods build upon the foundation of weight analysis but take a different ap-proach: instead of focusing on any specific weight features of backdoored models, they extract various features, including weights, to train a classifier that can distinguish between poisoned and clean models. These methods typically assume access to a set of both poisoned and clean models as training instances. For example, [106] proposes feeding queries to both clean and poisoned models and using their outputs as features to train a classifier. On the other hand, [107] employs a more straightforward approach by extracting weights of the final linear layer for classifier training.\n3) Trigger Reversal: Trigger reversal methods [106], [108], [109] focus on reverse-engineering potential triggers to iden-tify backdoored models. These methods aim to estimate trig-gers that cause misclassification of clean samples by minimiz-ing an objective function with respect to the estimated trigger string. The loss value of this objective function, along with the attack success rate of the identified trigger, is then analyzed to determine if a model is backdoored."}, {"title": "V. EMERGENT CHALLENGES", "content": "Research on unraveling and mitigating backdoor threats is no doubt still at a preliminary stage. There are quite a few emergent challenges that we need to tackle in order to ensure the safety development of foundation models as being emphasized by the Whitehouse's recent executive order [110], [111]. In this section, we discuss some of these challenges we believe future research shall pay more attention to.\n1) Mitigating Threats in Emergent Development and De-ployment Stages: The development of modern LLMs in-volves multiple stages, such as pretraining, instruction tuning, alignment, and adaptation. While recent studies have already investigated threats in stages including instruction tuning [14], [15], [40], alignment [67], [68] and conversational training [55], [112], [113] processes, few efforts have been attempted to guard against these threats yet. On the other hand, ad-versaries may also backdoor the LLM at inference through poisoning retrieval-augmented generation [72], [73], [76], in-context learning [114], [115], multi-turn conversation [55], [116], and even LLM-based evaluators. Hence, along with the advancement of LLM development, there needs effective safety enhancement in each of the emergent development processes against practical backdoor threats.\n2) Defending in the Web Scale: Currently, the experiments of backdoor defense or detection are generally done on individual task datasets with arbitrary poison rates. How-ever, recent work [117] has shown that even a significantly smaller poison rate (0.01%) on Web-scale data (LAION-400M, COYO-700M, and Wiki-40B) can practically steer the decision of a large model. More recent analyses [118] have also shown that larger LLMs are more susceptible to data poisoning, indicating a \u201cscaling law\" of data poisoning. In this context, it is necessary for the community to start considering lower poison rates and deploying defense experiments on Web-scale resources. Future research may also investigate constitutional [119] and causality-driven [120] approaches to enhance backdoor defense in this context.\n3) Safeguarding a Black-box Model: While many existing techniques require white-box accessibility of LLMs in order to mitigate or attribute the effects of data poisoning, quite a few SOTA LLMs, such as the GPT series, are deployed as Web services with only black-box accessibility. For these models, however, it is possible that backdoors may have already been injected due to any unknown poisoning in the Web-scale corpora used to train the LLMs. Against this challenge, it is important to investigate practical ways to detect and neutralize backdoors that have already been injected in deployed LLM services without white-box accessibility [121].\n4) Defending Against Heterogeneous Malicious Intents: In addition to discriminative tasks where backdoor attacks typically seek to flip the classification decisions, attacking the generation of LLMs may come with much more diverse intents, including but not limited to manipulating the prefer-ence [67], exploiting system and service functionalities [67], steering the sentiments of generation [15], [122], and gen-erating harmful content [123] and even malicious code [122], [124], [125]. These attacks can also extend beyond textual data to heterogeneous sources, such as tabular data [126], [127]. As safeguarding LLMs from heterogeneous attack intents is obviously challenging, a practical solution could be to develop universal guardrail models [123] that seek to detect such intents in training or inference data.\nThese four lines of emergent challenges are a few selected ones we recommend future research to specifically consider. Meanwhile, effectively mitigating backdoor threats in the continually scaling LLMs inevitably faces more challenges than these. Readers are recommended to refer to the materials of our recent NAACL 2024 tutorial [21] and associated online materials\u00b9 for a more thorough discussion.\""}, {"title": "VI. CONCLUSION", "content": "In this survey, we explore the emerging and evolving threat landscape of backdoor attacks against LLMs. Through a detailed examination of both training-time and inference-time threats, we highlighted how adversaries can exploit the memorization abilities of LLMs to insert malicious backdoors, resulting in potentially harmful behaviors. Our discussion covers a range of attack types, including sample-agnostic and sample-dependent approaches, as well as optimized and test-time attack strategies. In response to these threats, we review existing defense and detection mechanisms, which aim to safeguard LLMs during either the training or inference stage. Despite these advancements, our survey further identi-fies several critical challenges in mitigating backdoor threats, including defending LLMs in the web scale, securing black-box models, and developing defenses against a wide range of malicious intents targeting both discriminative and generative LLM tasks. As LLMs continue to evolve and integrate more deeply into safety-critical applications across various indus-tries, robust and scalable solutions will be required to ensure the safety and trustworthiness of these powerful models. We hope that this timely survey provides a foundation for future work, guiding researchers toward addressing the emergent and ongoing challenges in securing LLMs from backdoor threats."}]}