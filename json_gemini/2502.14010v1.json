{"title": "Which Attention Heads Matter for In-Context Learning?", "authors": ["Kayo Yin", "Jacob Steinhardt"], "abstract": "Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. Two different mechanisms have been proposed to explain ICL: induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task. To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models.\nThrough detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL.", "sections": [{"title": "1. Introduction", "content": "One of the most remarkable features of large language models (LLM) is their ability to perform in-context learning (ICL), where they can adapt to various new tasks using only a few demonstrations at inference time. This capability has become a crucial tool for adapting pre-trained LLMs to specific tasks, sparking significant research interest in understanding its underlying mechanisms.\nTo date, two key mechanisms have been primarily associated with ICL, substantiated by different lines of evidence. First, induction circuits were hypothesized to be the primary mechanism behind ICL in LLMs."}, {"title": "2. Background & related work", "content": "We present a comparative analysis of two mechanisms proposed to explain ICL: induction heads and FV heads."}, {"title": "2.1. Induction heads", "content": "Induction heads were first identified and extensively studied by Olsson et al. (2022) as the mechanism behind ICL. They are attention heads that operate by identifying repeated patterns in the input: when processing a token, they attend to the token that followed a previous occurrence of the same token, predicting it will appear next.\nThe initial evidence for induction heads' role in ICL came from Olsson et al. (2022), who studied small attention-only models (1-3 layers). They observed that the emergence of induction heads during training coincided with improvements in ICL ability \u2013 measured as the difference between the loss at the 500th versus 50th token in the context. Their ablation studies showed that removing induction heads impaired this metric.\nTo identify and analyze induction heads, we measure their induction scores using the TransformerLens framework. For each attention head $a$, we compute its induction score on a synthetic sequence constructed by repeating a uniformly sampled random token sequence: $r = r_1r_2...r_{50}r_1r_2...r_{50}$. The induction score is defined as:\n$S_a(a, r) = \\sum_{i=1}^{50} a_{r_i \\rightarrow r_{i+1}}$\nwhere $a_{r_i \\rightarrow r_{i+1}}$ represents the attention weight that head $a$ places on token $r_{i+1}$ when processing token $r_i$. For each attention head in each model, we take the mean induction"}, {"title": "2.2. FV heads", "content": "Function vectors (FV) were concurrently discovered by Todd et al. (2024) and Hendel et al. (2023). They represent a different mechanism for ICL: FVs are compact vector representations of ICL tasks that can be extracted from specific attention heads and added back into the language model's computations to reproduce ICL behavior. We refer to the attention heads that encode and transport these function vectors as FV heads.\nTo identify FV heads, we employ the casual mediation analysis framework from Todd et al. (2024). For each ICL task t in our task set T, where t is defined by a dataset $P_t$ of in-context prompts $p \\in P_t$ consisting of input-output pairs $(x_i, y_i)$, we:\n1. Compute the mean activation of an attention head a over prompts in $P_t$: $\\bar{a}_t = \\frac{1}{|P_t|} \\sum_{p \\in P_t} a(p)$\n2. Create corrupted ICL prompts $p \\in P_t$ by randomly shuffling the output labels $\\tilde{y}_i$ while maintaining the same inputs $x_i$\n3. Measure each head's function vector score (FV score) as its causal contribution to recovering the correct output $y_i$ for the input $x_i$ given corrupted examples $(x_i, \\tilde{y}_i)$ when its activation pattern is replaced with the mean task-conditioned activation $\\bar{a}_t$:\n$S_{FV}(a, p) = f(p | a:= \\bar{a}_t)[y_i] - f(p)[y_i]$.\nFor each attention head, we take the mean FV score across 37 natural language ICL tasks from (Todd et al., 2024) (Appendix A.7), using 100 prompts per task. Each prompt contains 10 input-output demonstration pairs followed by a single test instance."}, {"title": "2.3. Reconciling divergent findings", "content": "While both mechanisms have been proposed by their respective works as the mechanism behind ICL, our side-by-side analysis of induction and FV heads reveals that FV heads seem to primarily contribute to ICL performance. We believe that the main reason for the divergence between our result and previous work lies in several intuitively related concepts in the literature that are assumed to be the same.\nFirst, there is an important distinction between two different conceptualizations of ICL:\n\u2022 On one hand, ICL is often used synonymously with few-shot learning from the prompt without parameter"}, {"title": "3. Induction heads and function vector heads are distinct but correlated", "content": "Before analyzing the relative contributions of induction and FV heads to ICL performance, we first establish that these represent distinct mechanisms, while noting important correlations between them."}, {"title": "3.1. Head locations", "content": "We begin by examining the location of the top induction and FV heads within the models. Figure 2 shows the layers where the top $2\\%$ induction heads and FV heads appear in three representative Pythia models (see Appendix A.9 for all 12 models).\nIn general, induction heads appear in early-middle layers and FV heads appear in slightly deeper layers than induction heads. This suggests that induction and FV heads do not fully overlap and are indeed distinct mechanisms. Moreover, the deeper locations of FV heads may indicate they implement more abstract computations than induction heads, though this interpretation remains speculative."}, {"title": "3.2. Overlap between induction and FV heads", "content": "To further examine how distinct induction and FV heads are, we analyze the extent of the overlap between the two types of heads in two ways.\nFirst, we measure direct overlap - the percentage of heads that rank in the top $2\\%$ for both mechanisms: $100 \\times \\frac{|IH \\cap FV|}{|IH \\cup FV|}$, where IH and FV represent the sets of top induction and FV heads respectively. The results show minimal overlap: seven of our twelve models show zero overlap, with the remaining models showing only 5-15% overlap (Figure 3 left). This leads us to conclude that induction heads and FV heads are mostly distinct and justifies studying them\nIn certain cases, we need to differentiate between meaningful induction / FV heads and the long tail of attention heads that perform neither induction nor the FV mechanism. We choose the top 2% induction and FV heads as the representative set of induction and FV heads, following Todd et al. (2024)."}, {"title": "4. Function vector heads drive in-context learning", "content": "Having established that induction and FV heads represent distinct mechanisms, we now investigate their relative causal importance for ICL through systematic ablation studies. Our analysis focuses primarily on few-shot ICL accuracy while also examining effects on token-loss difference for comparison with previous work."}, {"title": "4.1. Method", "content": "Ablation. To assess the causal contribution of different attention heads, we measure how ICL performance changes when specific heads are disabled. We use mean ablation, where we replace each target head's output with its average\nIn our main analysis, we do not rely on the correlation between the distribution of induction scores and FV scores across the full set of attention heads because there is a long tail of attention heads with low scores on both induction and FV. For completeness, we plot the induction and FV scores of all heads in Appendix A.1."}, {"title": "5. Induction and FV strength during training", "content": "To measure the general strength of induction and FV mechanisms during training, we plot the mean induction and FV scores of the top 2% induction and FV heads at each model checkpoint, along with few-shot ICL accuracy. We include plots for all Pythia models in Appendix A.11.\nOur analysis reveals a consistent pattern across all Pythia models: induction heads emerge early in training, at around step 1,000 out of 143,000, while FV heads appear substantially later at around step 16,000. The development of these heads shows distinct characteristics as well \u2013 induction scores exhibit a sharp initial rise followed by a plateau or slight decline, whereas FV scores demonstrate a gradual but sustained increase from step 16,000 through the end of training. This temporal asymmetry suggests that induction heads represent a simpler mechanism that models can acquire earlier, while FV heads embody a more complex mechanism that requires extended training. In addition, we observe that in all models, few-shot ICL accuracy begins to improve around the same time as when induction heads appear, and continues to gradually increase throughout training."}, {"title": "5.2. Evolution of individual heads during training", "content": "To gain more granular insights into head development, we investigate the evolution of individual attention heads throughout training.\nA striking pattern emerges across all models: many heads that ultimately become strong FV heads initially exhibit high induction scores, emerging around the same time as dedicated induction heads. These proto-FV heads initially achieve induction scores comparable to those of specialized induction heads. However, as training progresses, their induction scores gradually decline while their FV scores increase. Importantly, this pattern is unidirectional; we found no instances of induction heads that develop significant FV capabilities during training, as evidenced by their consistently low FV scores throughout training. This suggests many FV heads evolve from induction heads during training, but not vice versa."}, {"title": "6. Interpretation and discussion", "content": "Our investigation revealed several key insights about the relationship between induction and FV heads in transformer models and their effect on ICL. While these mechanisms are distinct, they show notable correlation (\u00a73). FV heads con-"}]}