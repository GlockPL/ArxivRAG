{"title": "Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction", "authors": ["Jihwan Lee", "Tiantian Feng", "Aditya Kommineni", "Sudarsana Reddy Kadiri", "Shrikanth Narayanan"], "abstract": "Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain-computer interfaces (BCI) offer a promising avenue for a wide range of applications, particularly in assisting individuals with neurological and communication disorders. The ability to decode perceived, imagined, or produced speech, as well as text, from brain signals is fundamental to the development and effectiveness of these technologies. The successful integration of speech and text decoding into BCI leads to significant advances in neurorehabilitation, offering alternative communication means [1]\u2013[3].\nRecent advances in self-supervised learning show promise for processing biosignals such as EEG that tend to be inherently noisy. This has led to development of pre-trained models to learn broadly meaningful representations that can be generalized to various downstream tasks. Recent studies [4]\u2013[6] have explored encoder-decoder architectures showing the ability to capture representations of the EEG signal that can generalize across various domains.such as Motor-Imagery, Sleep and other Event Related Tasks.\nSeveral methods have been proposed to decode text from EEG signals in the context of human speech and language processing. These include methods that utilize a pre-trained language model and the transformer architecture [7], [8], or the quantized variational encoder [9]. Liu et al. [8] propose to decode textual information in the EEG signals by adopting the transfer learning regime on the transformer architecture and using a pre-trained language model.\nVarious approaches have been proposed to decode acoustic speech information from EEG signals. They involve using convolutional neural networks (CNN) [10]\u2013[12], pre-trained speech models [13], or the WaveNet [14] architecture [15]. Recently, a framework for direct reconstruction of listened speech waveforms has been proposed as described in [16], where no intermediate acoustic feature step is required.\nThere are instances where the need arises to decode speech and text from brain signals simultaneously. For example, in the case of real-time BCI communication scenarios involving hearing or speech disabilities, simultaneous decoding in both modalities can be beneficial. However, most existing approaches are limited to only a single modality at a time. Consequently, to achieve decoding in both modalities, the conventional methods require implementing a concatenated, sequential pipeline, such as an additional speech synthesizer or recognizer.\nWe propose a novel framework that can decode listened speech in both modalities (speech waveform and textual phoneme sequences) simultaneously by incorporating an auxiliary phoneme predictor. The proposed framework consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module focuses on learning EEG embedding representation from the EEG signals. The speech module aims to generate listened speech waveform from the EEG embeddings. The phoneme predictor decodes the phoneme sequences from the EEG embeddings. The proposed framework not only allows parallel decoding of EEG signals into both speech waveforms and phoneme sequences, but also outperforms the previous methods in listened speech decoding. Table I shows the comparison of the proposed model with existing models in decoding speech waveforms and phoneme sequences.\nOur main contributions are as follows:\n\u2022\n\u2022\n\u2022\nWe introduce the novel framework that enables parallel decoding of listened speech waveforms and phoneme sequences directly from EEG signals. To the best of our knowledge, this is the first approach to achieve parallel decoding in this setting.\nWe demonstrate that the incorporation of an auxiliary phoneme predictor enhances the performance of listened speech decoding from EEG signals, outperforming previous approaches.\nWe provide a phoneme-level analysis of the model's ability to decode phoneme sequences and speech waveforms."}, {"title": "II. PROPOSED METHOD", "content": "Our proposed framework consists of three major parts: the EEG module, the speech module, and the phoneme predictor. The EEG module learns the latent representation embeddings from EEG signals. The EEG embeddings are fed into the phoneme predictor and the speech module in parallel to output the predicted phonemes and decoded speech waveforms, respectively. Note that our approach enables parallel decoding of both speech waveforms and phoneme sequences, rather than requiring a sequentially concatenated pipeline for each modality. For detailed implementation, refer to the source code provided.\n1) EEG and Speech Module: We adopt the EEG and speech module from [16]. The EEG module consists of an EEG encoder and an EEG decoder, where it learns to represent input EEG signals into EEG embeddings. The EEG encoder comprises of convolution blocks followed by a Structured State Space Sequence (S4) [17] block, as in [4], [16]. The EEG decoder is composed of 1D transpose convolution blocks. The EEG encoder takes preprocessed EEG signals as input and ouputs EEG embeddings, whereas the EEG decoder outputs reconstructed EEG signals from the EEG embeddings.\nThe speech module learns to generate the speech waveform from EEG embeddings, similar to [16], [18]. It consists of a speech encoder, a speech decoder, and a connector. As in [16], [18], the speech encoder comprises of non-causal WaveNet [14] residual blocks followed by a projection layer, and the speech decoder is HiFi-GAN V1 [19]. The speech encoder takes linear spectrograms of speech as input and outputs speech embeddings. The speech decoder then learns to generate speech waveforms from the speech embedding. The EEG embeddings are converted to speech embeddings through the connector, consisting of a transformer encoder [20] followed by a projection layer and the normalizing flow network, as in [16], [18]. The normalizing flow network is composed of affine coupling layers [21], which act as an invertible function between EEG and speech embeddings, aligning these two distributions. A gradient stop is applied between the EEG module and the speech module, hence the training of the EEG module is not influenced by the speech module.\n2) Phoneme Predictor: The phoneme predictor consists of conformer blocks [22] followed by a phoneme decoder, which is widely recognized in the automatic speech recognition (ASR) domain. The phoneme decoder is a single layer of LSTM with an attention mechanism. The phoneme predictor takes the EEG embeddings as input and outputs the phoneme sequences. This process allows the phoneme information to be more precisely captured in EEG embeddings."}, {"title": "B. Training Objectives", "content": "We use the connectionist temporal classification (CTC) loss [23] for the phoneme predictor, as defined in Eq. (1). The CTC loss is widely employed in the ASR domain due to its ability to effectively manage many-to-one sequence mapping.\n\n$L_{CTC}(z, \\hat{z}) = -log \\sum_{\\pi \\in B^{-1}(z)} P(\\pi)$\n\nwhere z and $\\hat{z}$ are the target and predicted phoneme sequences, respectively, and $B^{-1}$ indicates the set of possible sequences that can collapse into z.\nFor the EEG module, we use the reconstruction loss for EEG signals, identical to [4], [16], as defined in Eq. (2):\n\n$L_{EEG}(x, \\hat{x}) = 1 - \\frac{1}{N_{ch}} \\sum_{i=1}^{N_{ch}} \\frac{ x_i \\hat{x_i} }{|| x_i || || \\hat{x_i} ||}$\n\nwhere $N_{ch}$ is the number of EEG channels ($N_{ch} = 128$), and $x_i$ and $\\hat{x_i}$ are $i^{th}$ channel of the input and the reconstructed EEG signals, respectively. The total training objective of the phoneme predictor and the EEG module is as in Eq. (3):\n\n$L = L_{EEG} + \\alpha L_{CTC}$\n\nWe heuristically chose $\\alpha = 0.3$.\nTo train the speech module, we adopt the training objectives from [16], [18], which consist of the mel-spectrogram reconstruction loss, the KL divergence loss between the latent representations of EEG and speech, and the GAN loss for the speech waveform generation. A separate optimizer was used to train the speech module, as a stop gradient is applied between the EEG module and the speech module."}, {"title": "III. EXPERIMENTS", "content": "Adopting the experimental setup of [16], we conducted our experiments on the same N400 dataset [24]. The 128-channel EEG signals were collected from 24 subjects at a sampling rate of 512 Hz. Each subject listened to 440 synthesized, gender-neutral English speech utterances. We chose two subjects and 40 sentences as held-out test sets, yielding three different test sets. In the unseen speech or unseen subject test set, only the speech sentences or the subjects are held-out, respectively. In the unseen both test set, both speech sentences and subjects are held-out.\nWe employ the same EEG pre-processing pipeline as described in [16], which includes powerline noise removal using a notch filter at 60 Hz, preservation of EEG spectral information through a bandpass filter (0.5-50 Hz), eye blink artifact removal via independent component analysis (ICA), and downsampling the EEG data to 256 Hz. The speech samples were downsampled to 22,050 Hz, as described in [16], [18]. The mel-spectrograms were generated using a short-time Fourier transform (STFT) with a window and FFT size of 1024, a hop size of 256, and 80 mel-frequency bands. We use espeak G2P\u00b2 to convert the raw text (graphemes) to phonemes."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We evaluate our results on the two modalities of model outputs: audio speech and phoneme sequences. For evaluating speech, we adopt the evaluation measures used in [16]: mel-cepstral distortion (MCD) [26] and the Pearson correlation between two mel-spectrograms (Mel-Corr). MCD quantifies the Euclidean distance between two mel-cepstral coefficients (MCC):\n\n$MCD = \\frac{10\\sqrt{2}}{ln 10} \\sqrt{\\frac{1}{N_{MCC}} \\sum_{i=1}^{N_{MCC}} (MCC_i - \\hat{MCC}_i)^2}$\n\nLower MCD values and higher Mel-Corr values indicate higher quality in the decoded speech waveforms. For simplicity, Mel-Corr values are reported after being multiplied by 100. As shown in Table II, all of our training configurations outperform the baseline [16] in both MCD and Mel-Corr metrics. Our proposed approach achieves noticeable improvement, particularly in Mel-Corr, where the metrics are nearly doubled. The best performance is achieved when a single LSTM layer is used as the phoneme predictor. This may be attributed to the model's overemphasis on the phoneme sequence prediction task due to the increased complexity of the phoneme predictor architecture, however, further investigation is required to provide a more definitive explanation."}, {"title": "B. Evaluation of Phoneme Sequence Decoding", "content": "To assess phoneme sequence decoding, we compute the top-k accuracy of subsequent phoneme prediction given the EEG signals and previous phoneme inputs. We find the top-k accuracy to be more reliable than the raw phoneme error rates (PER) because, in our experimental setup, there is no external language model to correct errors. Consequently, initial errors can lead to significant error accumulation. As shown in Table III, our proposed approach outperforms both training configurations of the baseline [8], when two conformer blocks are used in the phoneme predictor. The high accuracy in the unseen subject test set may be due to overfitting, as the target phoneme sequences are already seen during training in this test set."}, {"title": "C. Effects of Conformer", "content": "We observe a trade-off in performances across modalities as the number of conformer blocks in the phoneme predictor increases. Specifically, speech decodability tends to decrease as more conformer blocks are used, whereas phoneme sequence decodability shows the opposite trend. Despite this, speech decoding consistently outperforms the baseline [16], even in the most challenging test conditions. We further analyze the characteristics of the phonemes to understand where this performance trade-off happens in the next Section IV-D."}, {"title": "D. Analysis on Phoneme Groups", "content": "Herein, we aim to identify specific phoneme groups that perform poorly/well on each modality (1) in general; and (2) with respect to the number of the conformer blocks. Figure 2 depicts the decodability of each phoneme group in relation to the number of conformer blocks. We used the same analysis pipeline as in [16], where the Montreal Forced Aligner [27] was employed for phoneme segmentation. The intervals of the ground-truth phonemes are assumed to be identical to those of the reconstructed ones. Consonants are grouped by manner, place, and voicedness of articulation, while vowels are grouped by tongue position and tenseness. The unseen subject test set is excluded from the top-3 accuracy analysis in Figure 2c, as the phoneme sequences are already seen during training and it shows a different tendency compared to the other test sets.\nIn general, nasal, dental, and voiced consonants are more easily decoded in the speech modality. For phoneme sequences, nasal, labial, and voiceless consonants are more challenging to decode. Front, close, and lax vowels also tend to be more easily decoded in the phoneme sequence modality.\nWith respect to the number of the conformer blocks, the performance trade-off between the modalities is more pronounced for consonants than for vowels. In the case of speech modality, the performance drops more significantly for plosive, fricative, dental, velar, and voiced consonants, as the number of conformer blocks increases. Conversely, in terms of phoneme sequence decodability, the top-3 accuracy increases on most of the consonant groups except for the labial consonants."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We propose a framework to decode listened speech from EEG signals utilizing an auxiliary phoneme predictor, enhancing its performance in both speech and textual phoneme sequence decoding. The proposed framework enables parallel decoding of both speech and phoneme sequences, eliminating the need for a concatenated, sequential pipeline for each modality decoder. In the future, we plan to expand our work to speech production tasks, such as decoding phonated, attempted, or imagined speech from neural signals, as our current work is limited to just the speech perception task. Additionally, we plan to further improve the performance by leveraging pre-trained models for each modality."}]}