{"title": "Generative Language Models Potential for Requirement Engineering Applications: Insights into Current Strengths and Limitations", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Ludger Van Elst", "Andreas Dengel"], "abstract": "Traditional language models have been extensively evaluated for software engineering domain, however the potential of ChatGPT and Gemini have not been fully explored. To fulfill this gap, the paper in hand presents a comprehensive case study to investigate the potential of both language models for development of diverse types of requirement engineering applications. It deeply explores impact of varying levels of expert knowledge prompts on the prediction accuracies of both language models. Across 4 different public benchmark datasets of requirement engineering tasks, it compares performance of both language models with existing task specific machine/deep learning predictors and traditional language models. Specifically, the paper utilizes 4 benchmark datasets; Pure (7, 445 samples, requirements extraction),PROMISE (622 samples, requirements classification), REQuestA (300 question answer (QA) pairs) and Aerospace datasets (6347 words, requirements NER tagging). Our experiments reveal that, in comparison to ChatGPT, Gemini requires more careful prompt engineering to provide accurate predictions. Moreover, across requirement extraction benchmark dataset the state-of-the-art F1-score is 0.86 while ChatGPT and Gemini achieved 0.76 and 0.77, respectively. The State-of-the-art F1-score on requirements classification dataset is 0.96 and both language models 0.78. In name entity recognition (NER) task the state-of-the-art F1-score is 0.92 and ChatGPT managed to produce 0.36, and Gemini 0.25. Similarly, across question answering dataset the state-of-the-art F1-score is 0.90 and ChatGPT and Gemini managed to produce 0.91 and 0.88 respectively. Our experiments show that Gemini requires more precise prompt engineering than ChatGPT. Except for question-answering, both models under-perform compared to current state-of-the-art predictors across other tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, the world has witnessed a remarkable surge for development of diverse and impact-full computer-aided applications [1, 2]. The primary motivation behind the development of these applications is to automate time-consuming and labor-intensive tasks by utilizing cutting-edge Artificial Intelligence (AI) algorithms [3, 4]. In software systems domain, to facilitate development of large softwares, researchers have proposed numerous software development models such as waterfall model [5], v model [6], spiral model [7] and agile model [8]. In all these models requirement engineering is a fundamental and essential process comprising several tasks [9]. One critical task is requirements extraction, which involves identification of stakeholders needs from heterogeneous collection of documents such as user manuals, customer feedback, emails, interviews. [10]. Another, important task is requirements classification into functional and non-functional features which aids in the identification and mitigation of potential risks that may hinder the project success [11-13]. Furthermore, in the context of test case generation, NER tagging is key task for extracting actors and actions [14]. Similarly, requirements-based question answering system provides accurate and relevant answers to questions based on specific requirements. This type of system is particularly valuable in various contexts which require precise information to ensure software quality.\nIn the aforementioned applications, early predictive pipelines were designed using traditional machine learning (ML) algorithms [15-19] but had performance limitations. The invention of deep learning algorithms opened the doors of a new era for these applications with significant performance boost [20]. Despite deep learning advancement, sub-optimal representation learning methods remained a bottleneck for applications performance [16, 17, 19]. The need of representation learning arises due to inherit dependency of machine"}, {"title": "2 Related work", "content": "This section explores various predictors designed for 4 distinct requirement engineering applications including: requirements extraction, classification, named entity annotation and domain-specific question-answering system. It provides an insight into the evolution and advancements of these predictors in their respective application areas."}, {"title": "2.1 Requirements extraction", "content": "Two different research studies have been conducted to empower the process of requirement extraction from SRS documents by harnessing the benefits of AI [60, 61]. Vladimir et al. [60] constructed an independent test set by manually annotating the content of SRS documents as requirements and non-requirements. The authors bench-marked the performance of independent test set across three different predictors namely: FastText, ELMO and BERT. A thorough experimental analysis revealed that the BERT predictor consistently outperformed the other two predictors. Sainani et al. [61] also generated a requirement extraction dataset using contracts documents. The authors used three traditional machine learning and one deep learning based predictive pipelines namely, support vector machine (SVM), random forest (RF), naive bayes (NB) and Bi-directional LSTM (BiLSTM). Their [61] experimental analysis demonstrated that BiLSTM surpassed performance of the other traditional ML based predictive pipelines."}, {"title": "2.2 Requirements classification", "content": "Over past 4 years, around 11 different ML and DL based predictors ([11, 16-19, 62-66]) have been developed for software requirements classification. Among 11 predictors, 4 ML based predictors [16, 18, 19, 62]) were developed by utilizing Bag of Words (BoW) based text representation approach with"}, {"title": "2.3 Requirements named entity recognition", "content": "Named entity recognition (NER) task annotates words or group of words with predefined tags that refer to particular names, entities, events or actions [76]. In the domain of software engineering, these tags encompass software-specific information including actor, action, operator, user, object, graphical user interface (GUI), hardware, application programming interface (API), etc. NER serves as a backbone for various AI based software engineering pipelines such as information extraction [77], software requirements classification [62], use case diagram generation and topic modeling [78, 79]. This section briefly describes 3 machine learning, 7 deep learning and 10 language models based NER predictors that have been developed to annotate SRS documents [43, 80, 81], privacy requirements [82] and software bug specific entity recognition [77, 83]. Among 3 machine learning predictors [84-86], Imam et al. [85] predictor utilized linguistic rules and SVM classifier while KV et al. [84] predictor utilized part of speech information along with multinomial naive bayes (MNB) classifier for NER annotations. Moreover, KV et al., [86] predictor utilized Bag of Words (BoW) approach along with passive-aggressive classifier for NER tagging.\nAmong 7 deep learning predictors [43, 77, 81-83, 93, 94], Li et al. [81] predictor utilized Word2Vec word embedding approach along with Bi-LSTM for"}, {"title": "2.4 Requirements question answering system", "content": "A domain-specific question answering system (QAS) provides precise information within a particular subject area or field. Software systems domain-specific QAS provides useful insights about domain specific knowledge such as paradigm of software development life [106, 107], requirement analysis [108, 109] and test case generation [110, 111]. This system facilitates researchers and software developers by providing precise and relevant answers of their questions [112, 113].\nThis section describes 4 different transformers based predictors [44, 114\u2013116] for software requirements related QA task. Among 4 different studies, 2 studies [114, 115] utilized GDPR dataset that contains questions and answers related to software privacy requirements. Among remaining two studies, one study [44] utilized QAssist dataset that contains requirements related to security, aerospace and defence, while other study [116] used stack-overflow based documents to answer development question of Java language.\nAmong existing predictors, one predictor [44] explored the potential of 5 different language models (BERT [70], ALBERT [73], Distil-BERT [74], ROBERTa [71], ELECTRA [117]) for finding relevant answer to questions related to software requirements. Afterwards, Abualhaija et al., [114] explored combined potential of 4 language models (BERT [70], Al-BERT [73], ROBERTa [71], ELECTRA [117]) along with two similarity measures (cosine and BERT cross-encoder similarity) for QA related to software requirements. Abualhaija et al., [115] developed COREQQA predictor that explored combined potential of BERT cross-encoder (BCE) and RoBERTa for question answering task."}, {"title": "3 Materials and methods", "content": "This section summarizes the criteria utilized to explore the potential of generative language models across four different requirement engineering tasks. Furthermore, it provides high level overview of benchmark datasets and evaluation measures used to evaluate performance of ChatGPT and Gemini."}, {"title": "3.1 Proposed methodology", "content": "Figure 2 graphically represents utilization and evaluation of generative language models across four different tasks including: requirements extraction, classification, named entity recognition and question answering system. Figure 2, graphically illustrates input of four different tasks and three different levels of expert knowledge is fed to generative language models. Expert level 1 knowledge prompt seeks to assess, how precisely generative language models fulfils desired need by utilizing its background knowledge. At this level, the"}, {"title": "3.2 Benchmark datasets", "content": "This section summarizes detail of 4 benchmark datasets that are utilized to evaluate the performance of generative language models."}, {"title": "3.2.1 Requirements extraction", "content": "To facilitate development of AI applications for requirement extraction task, Vladimir et al. [60] developed a benchmark dataset by acquiring content from 79 different SRS documents. Authors manually annotated SRS documents content into requirements and non requirements classes. Moreover, the authors collaborated with domain expert to resolve ambiguous cases and developed 4,145 requirement class samples and 3,600 non-requirements class samples."}, {"title": "3.2.2 Requirements classification", "content": "To evaluate the performance of GLMs for requirements classification we utilized PROMISE benchmark dataset [118]. PROMISE dataset comprises of 625 requirements including; 254 functional and 371 non-functional requirements."}, {"title": "3.2.3 Requirements named entity recognition", "content": "To empower aerospace requirements engineering computer-aided programs, Tikayat et al. [97] prepared a NER dataset containing two distinct types of content: generic aerospace content and aerospace-based requirements. Generic aerospace content includes scholarly articles and papers published by the National Academy of Space Studies Board 1. These documents provide high-level overview of aerospace domain. On the other hand, aerospace-based requirements encompass information about standards and requirements for certifying new aircraft designs. Overall, the dataset contains 1432 sentences, including 1,100 sentences about general information, definitions and design methodologies of aerospace domain. The remaining 332 sentences contain aerospace requirements covering diverse range of topics such as environmental factors, performance criteria, functionality, quality assurance, design process and interface."}, {"title": "3.2.4 Requirments based question answering", "content": "Ezzini et al. [44] developed a public benchmark software requirement related question answering specific dataset (REQuestA), by utilizing Wikipedia articles and data from six SRSs related to three different domains, namely: (a) aerospace, (b) defence and (c) security. The authors automatically generated question answering pairs using text generation models and then verified data by two domain experts.\nThe process of generating question answer pairs can be described in 4 different steps. First step involves pre-processing of SRSs by identifying keywords for analyzing domain of SRSs. The authors utilized REGICE [119] tool for extracting software specific keywords. Afterwards, they grouped SRS documents into 3 distinct domains based on TFIDF score of distinct concepts. In order to compute TFIDF score, they preferred to work over phrases rather than individual words. Afterwards, they sorted computed score in descending order and selected top 50 concepts or keywords. Finally, they used each keyword to find a matching Wikipedia article and randomly selected matching articles. Third steps involves automatic splitting of SRSs and Wikipedia articles into text passages. Forth step used a question generation (QG) model (i.e. T5 language model). QG model takes text passage as input and extracts random answer for each passage. Based on generated answer, it automatically generates corresponding question. The output of each text passage involves that passage along with a set of automatically generated question answers pairs. In order to validate question answer pairs, authors used two methods: (1) automatic validation by question answer evaluator and (2) manual validation by domain experts. In first validation process, author utilized BERT"}, {"title": "3.3 Evaluation measures", "content": "Following evaluation criteria of existing requirement extraction, requirement classification and NER tagging studies, generative language models performance is computed using 4 different evaluation measures including: accuracy [120, 121], precision, recall and fl-score. These measures compute the predictors performance by utilizing confusion matrix shown in Figure 2 evaluation module. The confusion matrix contains 4 different types information namely, true positive (Tpos), true negative (Tneg), false positive (Fpos) and false negative (Fneg) to calculate overall performance of predictor. True positive (Tpos) represents number of positive class instances that are correctly identified by predictor. Similarly, true negative (Tneg) denotes number of negative class instances that are correctly identified by predictor. On the other hand, false positive (Fpos) reflects number of negative class instance that are incorrectly identified by predictor as positive class. False negative (Fneg) denotes number of negative class instance that are incorrectly identified by predictor as positive class. Furthermore, mathematical expressions of 4 different evaluation measures are also shown in Figure 2 evaluation module.\nFurthermore, adhering to evaluation criteria of existing question answer systems studies [122-125], we use 8 different evaluation measures namely: precision, recall, f1-score, ROUGE-1, Rouge-2, Rouge-L, ROUGE-S, METEOR (Metric for Evaluation of Translation with Explicit ORdering) score [126] and BLEU (BiLingual Evaluation Understudy) score [127]. ROUGE-1 calculates the ratio of overlapping unigrams in model-generated output and ground truth divided by total number of unigrams in model-generated output. Similarly Rouge-2 computes the ratio of matching bigrams in model-generated output that also appear in ground truth, over total number of bigrams in model-generated output. Rouge-L computes longest common sub-sequence that is shared between model generated output and ground truth. A longer shared sequence should indicate more similarity between the two sentences. ROUGE-S is a skip-gram concurrence metric, that allows to identify consecutive words from ground truth that also appear in model-generated output despite being separated by one or more other words. BLEU score is computed by comparing model-generated output with ground truth text. It involves tokenization, counting matching n-grams, calculating precision for different n-gram orders, introducing a brevity penalty, and combining precisions using a weighted geometric mean. The formula for BLEU score is product of the brevity penalty and exponential of average log precision. METEOR score is computed by aligning model-generated output with ground truth, using various linguistic resources and matching criteria. Unlike BLEU score it not only considers unigrams precision but also considers additional linguistic feature like recall stemming and synonym. It involves calculation of precision, recall and an alignment penalty, which penalizes non-matching words. The final METEOR score is a harmonic mean of precision and recall, adjusted by the alignment penalty. Equation 1 of supplementary file illustrates mathematical formulation to compute aforementioned measures."}, {"title": "4 Experimental setup and results", "content": "We developed python language scripts in order to fed both GLMs APIs: (ChatGPT (3.5) 2 and Gemini 3) with three different levels expert knowledge based prompts. In response to input prompts both GLMs produced diverse types of content for all four tasks. For classification tasks, we directly extracted desired label from generated output. However, output of GLMs for NER task posed significant challenges to extract original content and predicted tags. To automate evaluation of NER task, we closely analyzed output of both GLMs and wrote a rule based content and tags extractor script. Furthermore, for question answering system, we took all generated content to assess GLMs performance.\nTo facilitate fair performance analysis, we adopted evaluation criteria of existing studies [11, 44, 60, 97] to compare effectiveness of both GLMs with existing predictors. For requirement extraction and classification benchmark datasets [11, 44, 60, 97], authors have provided standard train\\test splits and we utilized only test sets to compute GLM's performance. For NER benchmark dataset, authors of existing study [97] utilized 90% data for training and 10% data for evaluation. However, authors [97] did not provide separate train and test sets. To mitigate biased evaluation we performed five random data splits and averaged their performance for comparison with existing predictor. In question answering system [44], similar to evaluation criteria of the existing study, we utilized all questions to the acquired answers from both GLMs."}, {"title": "4.1 Results", "content": "This section comprehensively examines impact of prompt engineering on performance of two GLMs across four distinct requirement engineering tasks. Additionally, it provides thorough performance analysis of GLMs with exiting"}, {"title": "4.1.1 Generative language models performance analysis with multiple prompts", "content": "This section investigates impact of prompt engineering on performance of GLMs(ChatGPT, Gemini) across four distinct requirement engineering tasks. In three tasks including requirements extraction, requirements classification and named entity recognition prompts are designed by incorporating three different level expert knowledge about the task. At level 1 expert knowledge (low knowledge) prompts contains basic information about underlying task. At level 2 expert knowledge (medium knowledge) the prompts contain background knowledge about tasks with comprehensive definitions. While at level 3 expert knowledge (expert knowledge) the prompts encompass tasks background knowledge with definitions and examples. In the question answering task, the first prompt only contain questions and other two prompts along with questions contain text that contain answers of questions.\n presents the performance metrics and confusion matrices achieved by GLMs(ChatGPT, Gemini) when they are fed with three distinct prompts, across requirement extraction benchmark dataset. Gemini produced accuracy values of 0.746, 0.753 and 0.79 at prompt 1, prompt 2, and prompt 3 respectively. These values reveals Gemini's performance consistently improved as the prompts are enriched with higher levels of expert knowledge. In contrast, ChatGPT's performance exhibited variations with increasing prompt expertness. While it achieved the highest accuracy of 0.77 at prompt 1, accuracy values of 0.743 and 0.75 are produced at prompts 2 and 3, respectively. Furthermore, confusion metric analysis of , reveals that with increased expert knowledge, Gemini becomes biased towards requirements class while ChatGPT demonstrates a biasness towards non-requirements class.\n illustrates performance comparison and confusion matrices of ChatGPT and Gemini based on 3 distinct prompts across benchmark dataset of requirements classification task. Gemini secures accuracy of 0.66 at prompt 1 while achieves same accuracy score of 0.78 at prompts 2 and 3. Contrarily, ChatGPT shows inconsistent performance with varying prompts and achieves lowest accuracy of 0.71 at prompt 3 and highest accuracy of 0.78 at prompt 2. The confusion matrices demonstrates that with increased expert knowledge Gemini is capable of correctly identifying non-requirement. Contrarily, ChatGPT exhibit biasness towards requirement class at prompts 1 and 3 and wrongly identifies non-requirement samples as requirements while at prompt 2 it managed to correctly identify non-requirements samples.\n illustrates GLMs's performance across 3 different prompts, in terms of 3 different evaluation measures and confusion matrices for requirements NER tagging task benchmark dataset. It can be seen in, that ChatGPT consistently beats Gemini to accurately identifying temporal, value and organizational elements (DATETIME, VAL and ORG). Moreover, both GLMS consistently miss-classify SYS as RES and RES is frequently confused with other classes (SYS, ORG and Val). Furthermore, the number of NER tags miss-classified as OTHER increases with incorporation of task specific information in prompts. Hence, context rich information introduces complexities resulting in higher likelihood of miss-classifying named entities as \u201cOTHER\u201d. Overall, Gemini shows best performance with prompt 1 while ChatGPT exhibits peak performance using prompt 2. Hence, incorporation of domain specific knowledge deteriorate Gemini's performance but enhances ChatGPT performance at a certain level (i.e medium level expert knowledge).\n illustrates performance comparison of GLMs in terms of 9 different evaluation measures across question answering task benchmark dataset. It is evident from, that both models exhibit varying performance across different prompts. At prompt 1 both GLMS attain low Rouge, BLEU and Meteor score, indicating lack of their capability to capture linguistic features and significant discrepancies between reference and generated answers. Both GLMS achieve highest Rouge, BLEU and Meteor score at Prompt3 that highlights their ability to capture semantic rich features and generate highly relevant answers with increased knowledge. A thorough performance analysis reveal that ChatGPT surpasses Gemini across all evaluation measures over all"}, {"title": "4.1.2 Prompt driven error analysis", "content": "This section briefly illustrates how different prompts impact the performance of GLMs for different requirement's engineering tasks. Figure 3 venn diagrams graphically illustrates number of wrong samples predicted by ChatGPT and Gemini using 3 distinct prompts across 3 different tasks (requirement extraction, classification and NER tagging). Specifically, for each language model, Venn diagram highlights wrong predictions i.e., number of samples wrongly predicted by each prompt and common samples that are wrongly predicted by two and three prompts. For instance, Figure 3 (a) indicates that for requirements extraction task, Gemini made 127, 127 and 94 incorrect predictions corresponding to prompts 1, 2, and 3, respectively. The common wrong predictions for prompts 1 and 2 are 82, for prompts 1 and 3 are 5, for prompts 2 and 3 are 47 and for all three prompts are 123.\nA thorough comparative analysis of venn diagrams for both GLMs across requirements extraction and classification tasks reveals that both models exhibit a higher frequency of common errors for prompt pair p1p2 and p2p3, respectively. This analysis illustrates that knowledge rich prompts facilitate GLMs requirements extraction task but introduces confusions in requirement classification task for GLMs. Conversely, prompt pairs p2p3 and P3p1 have the highest occurrence of common wrong predictions across NER task for Gemini and ChatGPT, respectively. This analysis highlights that despite having different level of expert knowledge, certain pairs show similar behaviour towards target samples and does not necessarily empower GLMs to discriminate between different classes. Moreover, generally ChatGPT exhibit higher number of common wrong predictions for all 3 prompts as compared to Gemini. which indicates that Gemini's performance is more sensitive to contextual information provided in prompts."}, {"title": "4.2 Generative language models performance reproducibility analysis", "content": "To evaluate performance reproducibility of GLMs, we selected 30 samples from benchmark dataset of requirements extraction. Specifically, we obtained five predictions for each sample across all 3 prompts from both GLMs. Across all five prediction iterations, ChatGPT consistently assigned the same class to 66.67%, 30%, and 43.34% samples for prompts 1, 2 and 3, respectively. Contrarily, Gemini consistently predicted the same class for 43.34%, 53.34% and 23.34% samples for prompts 1, 2 and 3, respectively. The lack of consistent predictions across multiple iterations for each prompt highlights a potential limitation in stability of both GLMs. However, since code of these GLMs is not open-source, hence it is challenging to directly addressing the root cause of variability. To enhance reliability, averaging prediction probabilities over multiple iterations can help to stabilize outputs despite the models' closed-source nature."}, {"title": "4.3 Generative language models performance comparison with existing predictors", "content": "This section comprehensively illustrates performance comparison of generative language models with existing predictors across four different requirement engineering tasks. It can be summarized from Section 2, that researchers have developed 2 predictors [60, 61] for requirements extraction, 20 predictors [11, 12, 16-19, 39, 62-66, 68, 75, 87-92] for requirements classification, 4 predictors [44, 114-116] for QA, and 20 predictors [43, 77, 80-86, 93, 94, 97-105] for NER. However, most of these predictors are evaluated on in-house datasets. This study specifically compares performance of GLMs with existing predictors that have been evaluated on the same public benchmark datasets"}, {"title": "4.3.1 Generative language models performance comparison with existing requirements extraction predictors", "content": "Figure 4 illustrates comparative performance analysis of GLMs and 3 existing predictors across requirements extraction benchmark dataset. Among 3 existing predictors, FastText, ELMo+SVM and BERT managed to produced f1-score of 0.81%, 0.83%, and 0.86%, respectively. FastText predictor remained least performer as it does not utilize any CNN, RNN, or attention mechanism to extract useful features. It initializes random word embeddings and during"}, {"title": "4.3.2 Generative language models performance comparison with existing requirements classification predictors", "content": "Figure 5 graphically represents performance comparison of GLMs and 13 existing predictors [11, 12, 16, 18, 19, 63, 64, 75, 87-91], across requirements classification benchmark dataset in terms of three distinct evaluation metrics. Among 13 existing requirements classification predictors, bag of words or TFIDF based representation with SVM classifier produced least performance [62]. The traditional predictor struggle to capture comprehensive semantic relationships of complex requirements [62]. In contrast, predictors utilizing Word2Vec and FastText embeddings showed better performance, highlighting that advanced embedding techniques provide a richer representation of requirements. Among all predictors, FnReq has state-of-the-art performance as its predictive pipeline is enriched with feature selection method and attention architecture to facilitate more discriminative features. The GLMs managed to outperform only three existing predictive pipelines namely; BoW + SVM, TFIDF + SVM and Doc2Vec + SVM. This outcome suggests that while GLMs excel in many NLP tasks, their general training may not fully capture the specialized nature of requirements text."}, {"title": "4.3.3 Generative language models performance comparison with existing requirements NER tagger", "content": "presents performance comparison of existing predictor and GLMs across 3 different evaluation measures. Existing predictor Aero-BERT consistently excels across all measures to recognize all five entity types i.e. DATETIME, ORG, RES, SYS and VAL. Furthermore, it also achieves a great balance between precision and recall across all NER types. In contrast, GLMs lack to maintain this balance and struggles with imbalanced trade-off between precision and recall. Aero-BERT consistently outperforms both GLMs in terms of weighted average across across all entity types."}, {"title": "4.3.4 Generative language models performance comparison with existing requirements specific question answering systems", "content": "Figure 6 provides performance comparison of existing predictors and GLMS across 6 different evaluation measures for benchmark dataset of requirements-based question-answering. A thorough analysis of Figure 6 reveals that among 6 existing predictors [44], Electra, DistilBERT, and BERT achieve slightly lower results and secures rank at 6th, 5th and 4th position respectively. ROBERTa secures 3rd position and MiniLM follows closely at 2nd rank. Albert beats all other existing predictors which indicates its superior ability to capture semantic information. Both GLMs outshine top-performing AlBert model across various metrics which demonstrate their effectiveness in comparison to other generic language models. Furthermore, comparative performance analysis of GLMs reveal that ChatGPT surpasses Gemini in terms of recall, BLEU, METEOR, ROUGE score indicates ChatGPT's superior ability to capture relevant information."}, {"title": "5 Practical implication", "content": "The recent surge in the development of LLMs has empowered software development life cycle by replacing manual works with computer aided applications [12, 60, 87]. Potential of LLMs has been explored in diverse types of scientific studies across distinct types of requirement engineering tasks [60, 87, 128]. Specifically, potential of BERT language model is harnessed to develop requirements extraction and classification applications. Similarly, BERT [70] and T5 [129] language models potential is explored for NER tagging of requirements. For requirements question answering task, researchers have explored the potential of 6 different language models including; RoBerta [71], Electra [117], DistilBERT [74], MiniLM [130], Albert [73] and BERT [70]. On the other hand, the most recent GLMs including; ChatGPT [131] and Gemini [132] are considered more useful tools for development of NLP applications. However, there is lack of scientific studies [56-59] having focus on exploration of GLMS potential in software systems field. Primary objective of this study is to analyze whether GLMs are capable of developing useful requirement engineering applications. Moreover, another objective is to explore the most effective methods for enhancing prompt-based predictions. A thorough experimentation reveals that prompts with more domain specific keywords tend to introduce biassness towards a particular class for classification tasks. Similarly, for NER task both"}, {"title": "6 Conclusion", "content": "This research investigates the potential of generative language models across four distinct requirement engineering tasks by employing prompts with varying levels of expert knowledge at three different stages. A comprehensive performance analysis of generative language models across four distinct requirement engineering tasks public benchmark datasets namely, requirement extraction, classification, named entity recognition, and question answering reveals a common trend where the models demonstrate improved performance with an escalation in the level of expert knowledge information provided in prompts. However, this improvement is accompanied by an increased bias toward type 1 or type 2 errors. This analysis illustrates that the performance of models can be directed towards a specific class by crafting prompts with domain-specific keywords. Furthermore, among both models Gemini requires domain specific knowledge enriched prompts to achieve its full potential, while ChatGPT shows greater robustness with less domain specific knowledge based prompts. In the realm of requirement engineering four tasks, both models exhibit comparable performance in the requirements classification task. In the remaining three tasks, ChatGPT consistently outperforms Gemini. In comparison to existing task-specific machine learning models and traditional language model-based predictors, ChatGPT achieves state-of-the-art performance in the question answering task. For other three tasks, the performance of existing predictors is significantly better than ChatGPT. In summary, the findings of this case study suggest that generative language models prove beneficial for question answering tasks. However, there remains a need to harness the potential of traditional machine/deep learning models or foundational language models for applications in requirement engineering."}, {"title": "Compliance with ethical standards", "content": "Funding\nNot applicable.\nConflict of Interest\nCorresponding author on the behalf of all authors declares that no conflict of interest is present."}]}