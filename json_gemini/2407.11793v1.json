{"title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians", "authors": ["Seokhun Choi", "Hyeonseop Song", "Jaechul Kim", "Taehyeong Kim", "Hoseok Do"], "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy.", "sections": [{"title": "Introduction", "content": "Recent progress in neural rendering technologies, such as Neural Radiance Fields (NeRF) [30], along with novel 3D scene representation methods like 3D Gaussian Splatting (3DGS) [17], have significantly impacted the field of photorealistic image synthesis within complex 3D environments. These innovations extend into practical applications, enabling advancements in diverse domains such as virtual and augmented reality [16, 45], digital content creation [25,33,54], and real-time rendering [8] for interactive systems that demand both high fidelity and efficiency. For these applications, accurately and efficiently segmenting objects within scenes is important [16], and presents ongoing challenges, particularly in distinguishing elements within diverse 3D environments.\nRecently, various segmentation methods [5,48,53] based on 3DGS have been proposed, leveraging advantages of 3DGS such as enhanced rendering efficiency and superior reconstruction quality. For instance, some methods [5,53] learn feature fields of 3D Gaussians that are aligned to the semantic representations from a foundational model like Segment Anything Model (SAM) [20]. This approach enables explicit segmentation of 3D scenes via 3D feature fields, which is crucial for supporting real-time applications and ensuring precise manipulation of intricate environments across diverse tasks. However, these methods face challenges in learning distinguishable feature fields in a scene, necessitating extensive post-processing to achieve clear segmentation. This reliance on time-consuming post-processing significantly impedes the efficiency benefits of 3DGS, creating a bottleneck for applications requiring rapid and direct manipulation of 3D scenes. An alternative approach [48] addresses 3D segmentation by utilizing object tracking mechanisms [10] to pre-assign SAM-based segment identities. However, this method's efficacy is contingent on successful object tracking, potentially excluding untracked objects from the segmentation process. This limitation suggests the potential benefit of developing more robust segmentation techniques capable of comprehensively handling diverse objects within complex scenes.\nThese considerations motivate the exploration of 3D segmentation methods that provide distinguishable feature fields without extensive post-processing. Progress in these techniques could significantly improve real-time interaction with 3D scenes, thereby enabling more intuitive and responsive experiences in 3D object manipulation tasks. Such advancements have the potential to not only enhance 3D scene editing capabilities but also broaden the practical applications of 3D scene representation across diverse fields.\nIn this study, we propose Click-Gaussian, depicted in Fig. 1, as a practical and efficient method for interactive segmentation of 3D Gaussians pre-trained on real-world scenes. By elevating 2D segmentation masks extracted from the SAM into enriched 3D Gaussian's augmented features with two-level granularity, i.e., coarse and fine levels, our approach facilitates fine-grained segmentation. The two-level granularity enables Click-Gaussian to capture scene elements at different scales in 3D environments, enhancing the precision and details of segmentation outcomes. This is achieved through the incorporation of a granularity prior for Gaussian's features, coupled with the employment of a contrastive feature learning method based on the SAM's 2D segmentation masks. Additionally, a significant hurdle in this process is the inconsistency of 2D masks across different views, which impedes the training of consistent and distinguishable semantic features. To address this issue, we introduce Global Feature-guided Learning (GFL), a novel strategy that systematically aggregates global feature candidates across the training views to coherently inform the development of 3D feature fields. The GFL technique enhances the robustness and reliability of feature learning, mitigating the impact of inherent ambiguities present in individual 2D segmentation masks. We demonstrate the effectiveness of Click-Gaussian through comprehensive experiments on complex real-world scenes, evaluating both segmentation accuracy and computational efficiency. Our results indicate that this approach offers a promising solution for precise and rapid 3D scene manipulation, potentially facilitating applications across various domains.\nIn summary, our key contributions are as follows:\nWe propose Click-Gaussian, which enables interactive segmentation of any 3D Gaussians by utilizing two-level feature fields derived from 2D segmentation masks using contrastive learning methods and a granularity prior.\nTo tackle the issue of 2D mask inconsistency across training views, we propose GFL, a novel approach that gathers global feature candidates from an entire scene to consistently guide Gaussian's feature learning.\nThrough extensive experiments on complicated real-world scenes, we confirm the effectiveness of our approach, demonstrating its suitability for interactive segmentation by significantly enhancing accuracy and processing time."}, {"title": "Related Work", "content": "3D Gaussian Representations. 3D Gaussian Splatting [17] has emerged as a promising method for real-time scene rendering, offering superior visual quality. This has inspired research [11, 27, 46, 47] into dynamic scene reconstruction, leveraging its fast rendering capabilities through the design of deformation fields [11, 46, 47]. Moreover, the research has expanded into 3D [9, 42, 50] and 4D [25,33] content generation by incorporating diffusion models [26,35]. These"}, {"title": "Feature Distillation for 3D Segmentation", "content": "Recent approaches to 3D segmentation can be broadly categorized into two main strategies: feature distillation and mask-lifting techniques. Feature distillation approaches [7, 13, 18, 22, 43,53] aim to transfer high-dimensional features from 2D vision foundation models [4,32] into 3D representations. For instance, DFFs [22], N3F [43], and ISRF [13] utilize DINO [4], while LeRF [18] employs CLIP [32]. However, these foundational models, not specifically designed for segmentation tasks, make such approaches struggle to achieve fine-grained segmentation. More recent studies, like those by Chen et al. [7] and Feature3DGS [53], distill SAM's encoder features into 3D and use the SAM's decoder to interpret 2D rendered feature maps for segmentation. However, the computational demands of SAM's decoder limit real-time interactive 3D segmentation. In contrast, our approach achieves finer segmentation in real-time by lifting SAM-generated 2D masks to 3D space."}, {"title": "2D Mask-lifting for 3D Segmentation", "content": "In addition to feature distillation approaches, recent studies have explored lifting 2D segmentation masks into 3D space [5, 6, 19, 31, 34, 48, 52]. For instance, SA3D [6] and NVOS [34] utilize user prompts (e.g., points or scribbles) to derive segmentation masks for a target object in reference views, subsequently training a neural field with these masks for object segmentation. MVseg [31] uses a video segmenter [4] to get multi-view masks. While the aforementioned approaches focus on single-object segmentation, other studies [5,19,48,52] have developed methods for segmenting multiple objects simultaneously. OmniSeg3D [52] trains a feature field using a hierarchical contrastive learning method with 2D segmentation masks, achieving fine-grained segmentation by adjusting cosine similarity thresholds. GARField [19] addresses inconsistent SAM-generated masks across views by introducing a scale-conditioned feature field. However, both use NeRF-based structures, which face computational challenges during rendering, limiting real-time performance.\nIn the context of 3DGS, SAGA [5] also employs a contrastive learning method with SAM-generated masks. It projects SAM's features into a low-dimensional space via a trainable MLP, imitating these features to address inconsistency issues. However, the distilled SAM's feature is detrimental to the segmentation method that uses feature cosine similarity at the inference stage, requiring extensive post-processing for accuracy. Gau-Group [48] takes a different approach, applying a zero-shot tracker [10] to address mask inconsistencies under the assumption that training images form a video sequence. This assumption, though, limits generalizability, and the method struggles with untracked SAM masks. In contrast, our proposed Global Feature-guided Learning (GFL) method ensures view-consistent training signal by leveraging globally aggregated feature candidates throughout a scene without assuming sequential image inputs."}, {"title": "Methods", "content": "We propose Click-Gaussian, a 3D segmentation method that augments pre-trained 3D Gaussians with effective and distinct 3D feature fields, enabling real-time segmentation capabilities for 3D Gaussian representations. To achieve this, we initially utilize the automatic mask generation module of SAM [20] for all training views of a scene, then organize generated masks based on their segment areas to derive coarse and fine level masks for each image. The information from these two-level masks is then incorporated into 3D Gaussians by splitting each Gaussian's feature space using a granularity prior, facilitating the representation of both levels of detail (Sec. 3.2). We train these augmented features through contrastive learning, applied to 2D rendered feature maps in conjunction with the masks (Sec. 3.3). To enhance the consistency of feature learning across different viewpoints, we propose Global Feature-guided Learning (GFL), which aggregates global feature candidates across the scene during training (Sec. 3.4). Additionally, we employ several regularization methods in our training process to further stabilize and refine the training of Click-Gaussian's features (Sec. 3.5). The comprehensive methodology is illustrated in Fig. 2."}, {"title": "Preliminary: 3D Gaussian Splatting", "content": "3D Gaussian Splatting (3DGS) represents a 3D scene with explicit 3D Gaussians and uses differentiable rasterizer [17] for rendering. Formally, given a training image set $I = \\{I_i\\}_{i=1}^{V}$ with camera poses, it aims to learn a set of 3D Gaussians $G = \\{g_i\\}_{i=1}^{N}$, where $V$ is the number of training images, $N$ is the number of Gaussians, and $g_i = \\{p_i, S_i, q_i, o_i, c_i \\}$ is i-th Gaussian's trainable parameters."}, {"title": "Feature Fields of Click-Gaussian", "content": "Here, $p_i \\in \\mathbb{R}^3$ is each Gaussian's center position. A scaling factor $s_i \\in \\mathbb{R}^3$ and a quaternion $q_i \\in \\mathbb{R}^4$ are used to represent each Gaussian's 3D covariance. $o_i \\in \\mathbb{R}$ is an opacity value, and $c_i$ is a color represented with spherical harmonics coefficients [36]. After projecting 3D Gaussians onto 2D image space with a given camera pose, 3DGS uses the rasterizer to compute a color $C$ on a pixel by performing $\\alpha$-blending [23, 24] on N depth-ordered points overlapping the pixel:\n$C = \\sum_{i \\in N} c_i \\alpha_i T_i,$\nwhere $\\alpha_i$ is calculated by evaluating the influences of each projected Gaussian using splatted 2D covariance [51], opacity $o_i$, and pixel distance, and $T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j)$ is the transmittance.\nClick-Gaussian operates by equipping each 3D Gaussian in a scene with additional features for segmentation. Specifically, given a 3D Gaussian $g_i$, each Gaussian is augmented with a D-dimensional feature vector $f_i \\in \\mathbb{R}^D$ for 3D segmentation, resulting in $\\hat{g}_i = g_i \\cup \\{f_i\\}$. We split $f_i$ into $f_i^c \\in \\mathbb{R}^{D^c}$ and $f_i^f \\in \\mathbb{R}^{D-D^c}$, enabling Click-Gaussian to learn features well on both the coarse and fine level masks. We use $f_i^c$ as a coarse-level feature, and $f_i^f = f_i^c \\oplus f_i$, not $f_i^c$, as a fine-level feature where $\\oplus$ is a concatenate function. This is motivated by the intrinsic dependency between two levels in the real world, called granularity prior (e.g., if two objects A and B are different at the coarse level, then each fine part $a \\subset A$ and $b \\subset B$ are naturally different), to make fine-level feature learning more effective. For our experimental setup, we set $D^c = 12$ and $D = 24$ and freeze other parameters of Gaussians except features. Using the rasterizer, we can compute two-level features $F^l$ on a pixel, akin to the method outlined in Eq. (1):\n$F^l = \\sum_{i \\in N} f_i \\alpha_i T_i$\nwhere $l = \\{f,c\\}$ is the granularity level. The computation of two-level features for each pixel is conducted in a single forward pass."}, {"title": "Contrastive Learning", "content": "We use cosine similarity based contrastive learning to train distinctive features with a set of two-level masks. To illustrate this concretely, consider a two-level mask $M^l \\in M$ for a training image $I \\in I$, where $l = \\{f,c\\}$ is the granularity level. For pixels $p_1$ and $p_2$, if their mask values are the same, i.e., $M_{p_1} = M_{p_2}$, we aim to maximize the cosine similarity between their rendered features:\n$\\mathcal{L}_{cont}^{pos} = \\frac{1}{|P_1| |P_2|} \\sum_{l \\in \\{f,c\\}} \\sum_{p_1 \\in P_1} \\sum_{p_2 \\in P_2} 1[M_{p_1}^l = M_{p_2}^l] S^l (p_1, p_2),$\nwhere 1 is the indicator function, $P_1$ and $P_2$ are the set of sampled pixels, $|\\cdot|$ is the number of elements in a set, and $S^l(p_1, p_2) = \\left\\langle \\frac{F_{p_1}^l}{\\lVert F_{p_1}^l \\rVert}, \\frac{F_{p_2}^l}{\\lVert F_{p_2}^l \\rVert} \\right\\rangle$ is the cosine similarity between rendered features of two pixels. Conversely, for pixels with different mask values, i.e., $M_{p_1}^l \\ne M_{p_2}^l$, we constrain their rendered features' cosine similarity to not exceed a specified margin, $\\tau^l$:\n$\\mathcal{L}_{cont}^{neg} = \\frac{1}{|P_1| |P_2|} \\sum_{l \\in \\{f,c\\}} \\sum_{p_1 \\in P_1} \\sum_{p_2 \\in P_2} 1[M_{p_1}^l \\ne M_{p_2}^l] 1[S^l (p_1, p_2) > \\tau^l] S^l (p_1, p_2).$\nConsidering that two points may represent distinct parts at the fine level yet be classified as the same object at the coarse level, we apply stop gradient operations, sg, to the coarse-level components during optimization for negative contrastive loss on fine-level features: $F_f^f = sg(F_f^c) \\oplus F_f$. This method effectively focuses the training process on elements critical for discerning fine-level distinction. We set the margins $\\tau^f = 0.75$ and $\\tau^c = 0.5$ for all experimental settings. The total contrastive learning loss is defined as:\n$\\mathcal{L}_{cont} = \\mathcal{L}_{cont}^{pos} + \\lambda_{cont}^{neg} \\mathcal{L}_{cont}^{neg},$\nwhere $\\lambda_{cont}^{neg}$ is a hyperparameter for balancing the two losses."}, {"title": "Global Feature-guided Learning", "content": "Click-Gaussian's features, despite being trained through contrastive learning, face challenges due to inconsistencies in SAM-generated masks across training viewpoints. This issue arises from the independent use of the masks in each view, potentially leading to unreliable training signals. To address this, we propose Global Feature-guided Learning (GFL), a method that continuously acquires global feature candidates to provide non-conflicting and reliable supervision.\nGlobal Feature Candidates. After a specified number of training iterations, we calculate the average features for each two-level mask across all training views. This is accomplished by rendering 2D feature maps and applying average pooling to each mask for all training views. Formally, for two-level masks for all viewpoints $M = \\{M_v^l\\}_{v=1}^V$, where $l = \\{f,c\\}$ denotes granularity level, V is the number of training viewpoints, and $M_v^l \\in \\mathbb{Z}^{H \\times W}$ represents a mask for viewpoint $v$ at level l, the average features are calculated as follows:\n$F_s^{l,v} = \\frac{1}{|P_s^{l,v}|} \\sum_{p \\in P_s^{l,v}} F_p^l, 1 \\le s \\le max(M^l),$\nHere, $P_s^{l,v} = \\{ p \\mid M_p^{l,v} = s \\}$ is a set of pixels with the same segment identiy (ID) in mask $M_v^l$, and $D^l$ is the feature dimension at level l. This average pooling procedure is done rapidly without gradient calculation, thanks to the"}, {"title": "Regularization", "content": "Hypersphere Regularization. Features with excessively large norms underestimate the participation of other features in the rendering process in Eq. (2), impeding effective learning of all Gaussian's features. To prevent any single Gaussian's feature from dominating in the $\\alpha$-blending process [23,24] of the feature rendering, similar to [52], we constrain Gaussian's features to lie on the surface of the hypersphere:\n$\\mathcal{L}_{3D-norm} = \\frac{1}{N} \\sum_{i=1}^N (\\lVert f_i^c \\rVert^2 - 1)^2 + (\\lVert f_i^f \\rVert^2 - 1)^2.$\nRendered Feature Regularization. Due to the hypersphere regularization in Eq. (10), each Gaussian's feature of level l lie on the surface of a hypersphere of radius $r^l$, with $r^c = 1$ for coarse and $r^f = \\sqrt{2}$ for fine levels. However, the norm of the rendered feature, $\\lVert F_p^l \\rVert^2$, is less than $r^l$, as feature vectors $f_i$ in different directions are integrated by Eq. (2). This implies that Gaussian's features $f_i$ contributing to $F_p^l$ for a single pixel (i.e., the same object) can vary. To ensure all contributing features $f_i$ for rendering $F_p^l$ are aligned in the same direction, we apply the following regularization on the rendered feature:\n$\\mathcal{L}_{2D-norm} = \\frac{1}{HW} \\sum_{l \\in \\{f,c\\}} \\sum_p \\sum_{p' \\in p} (\\lVert F_p^l \\rVert^2 - r^l)^2.$\nSpatial Consistency Regularization. Following the approach of [48], we leverage 3D spatial information to ensure that proximate Gaussians exhibit similar features. At the outset of training, we construct a KD-tree [1] using the 3D positions of Gaussians to facilitate efficient queries for spatially proximate neighbors. Throughout the training process, we sample $N_s$ Gaussians and adjust their features to align with those of their K-nearest neighbors in 3D space:\n$\\mathcal{L}_{spatial} = - \\frac{1}{N_s K} \\sum_{i=1}^{N_s} \\sum_{k=1}^K \\left\\langle \\frac{f_i}{\\lVert f_i \\rVert}, \\frac{f_k}{\\lVert f_k \\rVert} \\right\\rangle,$\nwhere $\\left\\langle , \\right\\rangle$ is the cosine similarity operation. For all experiments, we set $N_s = 100,000$ and $K = 5$. Finally, our total objective for training Click-Gaussian is:\n$\\mathcal{L}_{total} = \\mathcal{L}_{cont} + \\lambda_1 \\mathcal{L}_{GFL} + \\lambda_2 \\mathcal{L}_{3D-norm} + \\lambda_3 \\mathcal{L}_{2D-norm} + \\lambda_4 \\mathcal{L}_{spatial},$\nwhere $\\lambda_1, \\lambda_2, \\lambda_3$, and $\\lambda_4$ are hyperparameters balancing the respective loss terms."}, {"title": "Experiments", "content": "We implemented Click-Gaussian using the 3DGS codebase [17], adopting its default settings for pre-trained Gaussians. The hyperparameters were set as follows: $\\lambda_{cont} = 0.1, \\lambda_1 = 10.0, \\lambda_2 = 0.2, \\lambda_3 = 0.2$, and $\\lambda_4 = 0.5$. We employed the Adam optimizer with a learning rate of 0.01 for Gaussian's features. For contrastive learning, we sampled 10k pixels for each training iteration using importance sampling based on mask pixel count. In HDBSCAN, we set the epsilons for coarse and fine features clustering to $1 \\times 10^{-2}$ and $1 \\times 10^{-3}$, respectively, with the minimum cluster size proportional to the number of training views. We trained Click-Gaussian for 3,000 iterations, incorporating Global Feature-Guided learning from the 2,000th iteration onward. The entire training process took approximately 13 minutes on an NVIDIA RTX A5000 GPU."}, {"title": "Versatile Applications", "content": "After training Click-Gaussian's two-level feature fields, various scene manipulation tasks become feasible, including object removal, resizing, repositioning, duplication, and text-based editing. As Fig. 7 shows, the two-level global clusters enable rapid object selection within a scene (approximately 10 ms), facilitating interactive local adjustments to selected Gaussians. Additionally, for text-based editing, we leveraged CLIP-based methods [32,40,44] to get faster results (within about 10 s) than diffusion-based methods [2, 14, 35]."}, {"title": "Conclusion", "content": "We present Click-Gaussian, a swift and precise method enabling interactive fine-grained segmentation of pre-trained 3D Gaussians by lifting 2D segmentation masks into 3D feature fields of two-level granularity. Noticing from the intrinsic dependency between coarse and fine levels in the real world, we employ a granularity prior for feature division in the representation of feature fields. To address feature learning hindered by the cross-view inconsistency masks, an inherent issue in lifting 2D masks to 3D, we propose the Global Feature-guided Learning method for more consistent feature field training. Once Click-Gaussian is trained, users can select desired objects at coarse and fine levels more swiftly than previous methods. This enhanced capability has the potential to improve efficient and precise 3D environment modification across various applications.\nLimitations. Our approach faces limitations due to its reliance on pre-trained 3DGS and the two-level granularity assumption. Feature learning may be hindered if a single Gaussian represents multiple objects, particularly when they are semantically distinct but chromatically similar. The two-level granularity assumption, lacking intermediate levels, could limit efficiency for varying granular levels and complex structures, potentially requiring multiple interactions to select desired segmentation regions."}, {"title": "GUI-based Implementation for Click-Gaussian", "content": "To showcase interactive segmentation and manipulation using Click-Gaussian, we design a Graphical User Interface (GUI) tool based on DearPyGui [15, 41], a fast and powerful GUI toolkit for Python. As shown in Fig. 8, our GUI is designed to allow users to easily click and segment objects at coarse and fine levels, and provides tools for real-time manipulation tasks such as resizing, translation, removal, and text-based editing for intuitive interaction with the segmented objects. The supplementary video demonstrates the effectiveness of our method in enabling real-time interactive scene manipulation, showcasing its fast and precise 3D segmentation performance. We encourage readers to view this video for a comprehensive understanding of the proposed approach's capabilities."}, {"title": "SAM-based Multi-level Mask Generation", "content": "We utilized the official code's automatic mask generation module for SAM mask creation, which extracts masks without distinguishing levels, allowing us to get only the highest-confidence segments in an image. These segments are then assigned to two masks by area: if multiple segments are assigned to a single pixel, the coarse-level mask prioritizes the identity of the larger segment, while the fine-level mask favors the identity of the smaller segments. This approach enables us to assign a single mask identity per pixel at each level, facilitating stable contrastive learning.\nComparative Analysis of Multi-level Mask Strategies. Our method can adopt SAM's three-level masks (whole, part, and subpart) in two ways: three-level-score and three-level-area. Each approach prioritizes the highest score segment and smallest segment, respectively, for each level. In these cases, we split $f_i \\in \\mathbb{R}^{24}$ into three levels of granularity. As shown in Fig. 9, the three-level-area outperforms the three-level-score in fine-level mIoU due to finer-grained mask supervision (e.g., egg white and yolk), demonstrating the efficacy of the area-based prioritization. Additionally, our method using two-level masks surpasses the three-level-area thanks to the mask completeness and training efficiency: It has fewer unassigned identities than the three-level-area and learns feature fields more efficiently with the same feature dimension of 24. For these advantages, we adopt the two-level granularity assumption."}, {"title": "Annotations for Evaluating Fine-grained Segmentation", "content": "We evaluate our approach's segmentation performance using the LERF-Mask dataset [48], a public real-world dataset for 3D segmentation tasks. This dataset comprises three scenes (Figurines, Ramen, and Teatime) [18] with manually annotated ground truth masks for semantically large objects, as shown in the first two rows of Fig. 10. To assess fine-grained segmentation performance, we additionally annotated masks for smaller objects within each scene using Make-Sense [39], a free online image labeling tool, as shown in the last two rows of Fig. 10. This additional annotation is necessary due to the lack of datasets suitable for fine-level comparison. Note that the annotation process was conducted independently from our segmentation experiments."}, {"title": "3D Editing in AI-generated Videos", "content": "OpenAI recently announced Sora [3], a groundbreaking text-to-video generation model, showing a promising path towards building general-purpose world simulators. These simulators can be further improved by enabling interactive modification of generated realistic environments through accurate and fast 3D segmentation methods like Click-Gaussian, enhancing their functionality and user interaction capabilities. To demonstrate Click-Gaussian's versatility in scene segmentation and manipulation on these generated scenes, we applied our method to videos (Santorini\u00b9 and Snow-village\u00b2) generated by Sora. As shown in Fig. 11, after pre-training 3DGS on each generated video using COLMAP [37,38], users can flexibly make desired modifications, resulting in more creative and diverse 3D environments with Click-Gaussian."}, {"title": "Open-vocabulary 3D Object Localization", "content": "Once trained, our method can perform open-vocabulary 3D object localization as shown in Fig. 12, using the obtained global feature candidates, which we call global clusters. Specifically, for all two-level global clusters, we render only the Gaussians corresponding to each cluster in multiple views (10 randomly sampled views) as shown in Fig. 13. We then input these rendered images into the CLIP image encoder [32] to obtain the CLIP embeddings of each cluster. Thanks to the real-time rendering speed of 3DGS, this process of obtaining CLIP embeddings for all global clusters completes in 20-40 seconds, depending on the number of global clusters in the scene. Note that this process only needs to be performed once before any text query. Subsequently, given text queries, open-vocabulary 3D object localization is performed by returning the global cluster with the highest cosine similarity between the obtained image embeddings of all global clusters and the text query embedding. Fig. 12 qualitatively demonstrates that our approach precisely localizes 3D objects for given text queries using globally obtained clusters."}, {"title": "Additional Results for 3D Segmentation", "content": "Experiments on LeRF Dataset. In addition to user-guided segmentation, our approach can also automatically segment everything by calculating the cosine similarity between the rendered 2D feature map and global clusters' features, assigning a global cluster ID with the maximum similarity value to each pixel. By performing this process for each of the two granularity levels, we obtain automatic segmentation results at both coarse and fine levels. Figs. 14, 15, and 16 show the results of automatic segmentation for several complicated real-world scenes from the LeRF dataset [18], along with PCA visualizations of rendered feature maps at two levels. These results qualitatively demonstrate that Click-Gaussian achieves high-fidelity, fine-grained segmentation of everything in complex real-world scenes.\nExperiments on SPIn-NeRF Dataset. We further showcase the 3D multi-view segmentation results on the SPIn-NeRF Dataset using the label propagation method, as illustrated in Fig. 17. These results offer additional examples demonstrating the effectiveness of Click-Gaussian across various real-world scenes."}]}