{"title": "Click-Gaussian: Interactive Segmentation to Any 3D Gaussians", "authors": ["Seokhun Choi", "Hyeonseop Song", "Jaechul Kim", "Taehyeong Kim", "Hoseok Dol"], "abstract": "Interactive segmentation of 3D Gaussians opens a great opportunity for real-time manipulation of 3D scenes thanks to the real-time rendering capability of 3D Gaussian Splatting. However, the current methods suffer from time-consuming post-processing to deal with noisy segmentation output. Also, they struggle to provide detailed segmentation, which is important for fine-grained manipulation of 3D scenes. In this study, we propose Click-Gaussian, which learns distinguishable feature fields of two-level granularity, facilitating segmentation without time-consuming post-processing. We delve into challenges stemming from inconsistently learned feature fields resulting from 2D segmentation obtained independently from a 3D scene. 3D segmentation accuracy deteriorates when 2D segmentation results across the views, primary cues for 3D segmentation, are in conflict. To overcome these issues, we propose Global Feature-guided Learning (GFL). GFL constructs the clusters of global feature candidates from noisy 2D segments across the views, which smooths out noises when training the features of 3D Gaussians. Our method runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while also significantly improving segmentation accuracy.", "sections": [{"title": "1 Introduction", "content": "Recent progress in neural rendering technologies, such as Neural Radiance Fields (NeRF) [30], along with novel 3D scene representation methods like 3D Gaussian Splatting (3DGS) [17], have significantly impacted the field of photorealistic image synthesis within complex 3D environments. These innovations extend into practical applications, enabling advancements in diverse domains such as virtual and augmented reality [16, 45], digital content creation [25,33,54], and real-time rendering [8] for interactive systems that demand both high fidelity and efficiency. For these applications, accurately and efficiently segmenting objects within scenes is important [16], and presents ongoing challenges, particularly in distinguishing elements within diverse 3D environments.\nRecently, various segmentation methods [5,48,53] based on 3DGS have been proposed, leveraging advantages of 3DGS such as enhanced rendering efficiency and superior reconstruction quality. For instance, some methods [5,53] learn feature fields of 3D Gaussians that are aligned to the semantic representations from a foundational model like Segment Anything Model (SAM) [20]. This approach enables explicit segmentation of 3D scenes via 3D feature fields, which is crucial for supporting real-time applications and ensuring precise manipulation of intricate environments across diverse tasks. However, these methods face challenges in learning distinguishable feature fields in a scene, necessitating extensive post-processing to achieve clear segmentation. This reliance on time-consuming post-processing significantly impedes the efficiency benefits of 3DGS, creating a bottleneck for applications requiring rapid and direct manipulation of 3D scenes. An alternative approach [48] addresses 3D segmentation by utilizing object tracking mechanisms [10] to pre-assign SAM-based segment identities. However, this method's efficacy is contingent on successful object tracking, potentially excluding untracked objects from the segmentation process. This limitation suggests the potential benefit of developing more robust segmentation techniques capable of comprehensively handling diverse objects within complex scenes.\nThese considerations motivate the exploration of 3D segmentation methods that provide distinguishable feature fields without extensive post-processing. Progress in these techniques could significantly improve real-time interaction with 3D scenes, thereby enabling more intuitive and responsive experiences in 3D object manipulation tasks. Such advancements have the potential to not only enhance 3D scene editing capabilities but also broaden the practical applications of 3D scene representation across diverse fields.\nIn this study, we propose Click-Gaussian, depicted in Fig. 1, as a practical and efficient method for interactive segmentation of 3D Gaussians pre-trained on real-world scenes. By elevating 2D segmentation masks extracted from the SAM into enriched 3D Gaussian's augmented features with two-level granularity, i.e., coarse and fine levels, our approach facilitates fine-grained segmentation. The two-level granularity enables Click-Gaussian to capture scene elements at different scales in 3D environments, enhancing the precision and details of segmentation outcomes. This is achieved through the incorporation of a granularity prior for Gaussian's features, coupled with the employment of a contrastive feature learning method based on the SAM's 2D segmentation masks. Additionally, a significant hurdle in this process is the inconsistency of 2D masks across different views, which impedes the training of consistent and distinguishable semantic features. To address this issue, we introduce Global Feature-guided Learning (GFL), a novel strategy that systematically aggregates global feature candidates across the training views to coherently inform the development of 3D feature fields. The GFL technique enhances the robustness and reliability of feature learning, mitigating the impact of inherent ambiguities present in individual 2D segmentation masks. We demonstrate the effectiveness of Click-Gaussian through comprehensive experiments on complex real-world scenes, evaluating both segmentation accuracy and computational efficiency. Our results indicate that this approach offers a promising solution for precise and rapid 3D scene manipulation, potentially facilitating applications across various domains.\nIn summary, our key contributions are as follows:\nWe propose Click-Gaussian, which enables interactive segmentation of any 3D Gaussians by utilizing two-level feature fields derived from 2D segmentation masks using contrastive learning methods and a granularity prior.\nTo tackle the issue of 2D mask inconsistency across training views, we propose GFL, a novel approach that gathers global feature candidates from an entire scene to consistently guide Gaussian's feature learning.\nThrough extensive experiments on complicated real-world scenes, we confirm the effectiveness of our approach, demonstrating its suitability for interactive segmentation by significantly enhancing accuracy and processing time."}, {"title": "2 Related Work", "content": "3D Gaussian Representations. 3D Gaussian Splatting [17] has emerged as a promising method for real-time scene rendering, offering superior visual quality. This has inspired research [11, 27, 46, 47] into dynamic scene reconstruction, leveraging its fast rendering capabilities through the design of deformation fields [11, 46, 47]. Moreover, the research has expanded into 3D [9, 42, 50] and 4D [25,33] content generation by incorporating diffusion models [26,35]. These studies demonstrate the efficient rendering and high visual fidelity of 3D Gaussian representations in various applications. Our study further extends these capabilities by focusing on the segmentation of 3D Gaussians, while maintaining their inherent advantages.\nFeature Distillation for 3D Segmentation. Recent approaches to 3D segmentation can be broadly categorized into two main strategies: feature distillation and mask-lifting techniques. Feature distillation approaches [7, 13, 18, 22, 43,53] aim to transfer high-dimensional features from 2D vision foundation models [4,32] into 3D representations. For instance, DFFs [22], N3F [43], and ISRF [13] utilize DINO [4], while LeRF [18] employs CLIP [32]. However, these foundational models, not specifically designed for segmentation tasks, make such approaches struggle to achieve fine-grained segmentation. More recent studies, like those by Chen et al. [7] and Feature3DGS [53], distill SAM's encoder features into 3D and use the SAM's decoder to interpret 2D rendered feature maps for segmentation. However, the computational demands of SAM's decoder limit real-time interactive 3D segmentation. In contrast, our approach achieves finer segmentation in real-time by lifting SAM-generated 2D masks to 3D space.\n2D Mask-lifting for 3D Segmentation. In addition to feature distillation approaches, recent studies have explored lifting 2D segmentation masks into 3D space [5, 6, 19, 31, 34, 48, 52]. For instance, SA3D [6] and NVOS [34] utilize user prompts (e.g., points or scribbles) to derive segmentation masks for a target object in reference views, subsequently training a neural field with these masks for object segmentation. MVseg [31] uses a video segmenter [4] to get multi-view masks. While the aforementioned approaches focus on single-object segmentation, other studies [5,19,48,52] have developed methods for segmenting multiple objects simultaneously. OmniSeg3D [52] trains a feature field using a hierarchical contrastive learning method with 2D segmentation masks, achieving fine-grained segmentation by adjusting cosine similarity thresholds. GARField [19] addresses inconsistent SAM-generated masks across views by introducing a scale-conditioned feature field. However, both use NeRF-based structures, which face computational challenges during rendering, limiting real-time performance.\nIn the context of 3DGS, SAGA [5] also employs a contrastive learning method with SAM-generated masks. It projects SAM's features into a low-dimensional space via a trainable MLP, imitating these features to address inconsistency issues. However, the distilled SAM's feature is detrimental to the segmentation method that uses feature cosine similarity at the inference stage, requiring extensive post-processing for accuracy. Gau-Group [48] takes a different approach, applying a zero-shot tracker [10] to address mask inconsistencies under the assumption that training images form a video sequence. This assumption, though, limits generalizability, and the method struggles with untracked SAM masks. In contrast, our proposed Global Feature-guided Learning (GFL) method ensures view-consistent training signal by leveraging globally aggregated feature candidates throughout a scene without assuming sequential image inputs."}, {"title": "3 Methods", "content": "We propose Click-Gaussian, a 3D segmentation method that augments pre-trained 3D Gaussians with effective and distinct 3D feature fields, enabling real-time segmentation capabilities for 3D Gaussian representations. To achieve this, we initially utilize the automatic mask generation module of SAM [20] for all training views of a scene, then organize generated masks based on their segment areas to derive coarse and fine level masks for each image. The information from these two-level masks is then incorporated into 3D Gaussians by splitting each Gaussian's feature space using a granularity prior, facilitating the representation of both levels of detail (Sec. 3.2). We train these augmented features through contrastive learning, applied to 2D rendered feature maps in conjunction with the masks (Sec. 3.3). To enhance the consistency of feature learning across different viewpoints, we propose Global Feature-guided Learning (GFL), which aggregates global feature candidates across the scene during training (Sec. 3.4). Additionally, we employ several regularization methods in our training process to further stabilize and refine the training of Click-Gaussian's features (Sec. 3.5). The comprehensive methodology is illustrated in Fig. 2."}, {"title": "3.1 Preliminary: 3D Gaussian Splatting", "content": "3D Gaussian Splatting (3DGS) represents a 3D scene with explicit 3D Gaussians and uses differentiable rasterizer [17] for rendering. Formally, given a training image set \\(I = \\{I_v\\}_{v=1}^V\\) with camera poses, it aims to learn a set of 3D Gaussians \\(G = \\{g_i\\}_{i=1}^N\\), where V is the number of training images, N is the number of Gaussians, and \\(g_i = \\{p_i, S_i, q_i, O_i, c_i \\}\\) is i-th Gaussian's trainable parameters."}, {"title": "3.2 Feature Fields of Click-Gaussian", "content": "Click-Gaussian operates by equipping each 3D Gaussian in a scene with additional features for segmentation. Specifically, given a 3D Gaussian \\(g_i\\), each Gaussian is augmented with a D-dimensional feature vector \\(f_i \\in \\mathbb{R}^D\\) for 3D segmentation, resulting in \\(\\hat{g_i} = g_i \\cup \\{f_i\\}\\). We split \\(f_i\\) into \\(f_i^c \\in \\mathbb{R}^{D^c}\\) and \\(f_i^f \\in \\mathbb{R}^{D-D^c}\\), enabling Click-Gaussian to learn features well on both the coarse and fine level masks. We use \\(f_i^c\\) as a coarse-level feature, and \\(f_i^f = f_i \\oplus f_i^c\\), not \\(f_i\\), as a fine-level feature where \\(\\oplus\\) is a concatenate function. This is motivated by the intrinsic dependency between two levels in the real world, called granularity prior (e.g., if two objects A and B are different at the coarse level, then each fine part a \\(\\subset\\) A and b \\(\\subset\\) B are naturally different), to make fine-level feature learning more effective. For our experimental setup, we set \\(D^c = 12\\) and \\(D = 24\\) and freeze other parameters of Gaussians except features. Using the rasterizer, we can compute two-level features \\(F^l\\) on a pixel, akin to the method outlined in Eq. (1):\n\n\\(F^l = \\sum_{i \\in N} f_i a_i T_i\\)\n\nwhere \\(l = \\{f,c\\}\\) is the granularity level. The computation of two-level features for each pixel is conducted in a single forward pass."}, {"title": "3.3 Contrastive Learning", "content": "We use cosine similarity based contrastive learning to train distinctive features with a set of two-level masks. To illustrate this concretely, consider a two-level mask \\(M^l \\in \\mathbb{M}\\) for a training image \\(I \\in \\mathbb{I}\\), where \\(l = \\{f,c\\}\\) is the granularity level. For pixels \\(p_1\\) and \\(p_2\\), if their mask values are the same, i.e., \\(M_{p_1}^l = M_{p_2}^l\\), we aim to maximize the cosine similarity between their rendered features:\n\n\\(\\mathcal{L}_{cont}^{pos} = \\frac{1}{|P_1||P_2|} \\sum_{l \\in \\{f,c\\}} \\sum_{p_1 \\in P_1} \\sum_{p_2 \\in P_2} 1[M_{p_1}^l = M_{p_2}^l] S^l(p_1, p_2),\\)"}, {"title": "3.4 Global Feature-guided Learning", "content": "Click-Gaussian's features, despite being trained through contrastive learning, face challenges due to inconsistencies in SAM-generated masks across training viewpoints. This issue arises from the independent use of the masks in each view, potentially leading to unreliable training signals. To address this, we propose Global Feature-guided Learning (GFL), a method that continuously acquires global feature candidates to provide non-conflicting and reliable supervision.\nGlobal Feature Candidates. After a specified number of training iterations, we calculate the average features for each two-level mask across all training views. This is accomplished by rendering 2D feature maps and applying average pooling to each mask for all training views. Formally, for two-level masks for all viewpoints \\(M = \\{M_v^l\\}_{v=1}^V\\), where \\(l = \\{f,c\\}\\) denotes granularity level, V is the number of training viewpoints, and \\(M_v^l \\in \\mathbb{Z}^{H \\times W}\\) represents a mask for viewpoint v at level 1, the average features are calculated as follows:\n\n\\(F_v^{l,*} = \\{f \\mid p \\in P_v^l\\} \\in \\mathbb{R}^{D^l}\\)\n\nHere, \\(P_4^l = \\{p \\mid M_v^{l,*} = s\\}\\) is a set of pixels with the same segment identiy (ID) in mask \\(M_v^{l,*}\\), and \\(D^l\\) is the feature dimension at level 1. This average pooling procedure is done rapidly without gradient calculation, thanks to the real-time rendering speed of 3DGS at inference time. We then obtain \\(C^l\\) global feature candidates for each level, denoted as \\(F^l\\), across a scene by applying the HDBSCAN [28] clustering algorithm to each set \\(F_v^{l,*}\\). These global feature candidates are periodically updated to obtain the latest global features. Notably, as these global clusters are derived by grouping rendered features from noisy 2D segments across all views, they become the most representative features for the entire scene, effectively mitigating inconsistencies in the SAM-generated masks.\nGlobal Feature-guided Learning. Global feature candidates enable supervision of Click-Gaussian in a view-consistent manner. For a Gaussian's feature f, we guide the feature to belong to a specific global cluster and to be far from the others. This involves identifying c, the cluster ID where the i-th Gaussian's feature is most likely to belong at level 1: \\(c_i = arg\\_max \\tilde{S}^l(i, c)\\), where \\(\\tilde{S}^l(i, c) = (f_i^l, F_c^l)\\) is the cosine similarity between the i-th Gaussian's feature and global cluster feature with ID c. The GFL loss function for supervising the Gaussian's feature to belong to the most likely global cluster is defined as:\n\n\\(\\mathcal{L}_{GFL}^{pos} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{l \\in \\{f,c\\}} 1[S^l(i, c) > \\tau^g] \\tilde{S}^l(i, c_i).\\)\n\nHere, \\(\\tau^g\\) is a threshold for determining whether to belong to the cluster, and we set \\(\\tau^g = 0.9\\) across all our experiments. Conversely, the GFL loss function guiding the Gaussian's feature away from other global clusters is defined as:\n\n\\(\\mathcal{L}_{GFL}^{neg} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{l \\in \\{f,c\\}} \\sum_{c \\neq c_i} 1[s(i, c) > \\tau^g] \\tilde{S}^l(i, c),\\)\n\nwhere \\(\\tau^l\\) is described in Eq. (4). The total GFL loss is thus formulated as:\n\n\\(\\mathcal{L}_{GFL} = \\mathcal{L}_{GFL}^{pos} + \\mathcal{L}_{GFL}^{neg}\\)\n\nApplying the GFL loss directly to Gaussian's features using global clusters enhances their distinctiveness and noise robustness through reliable supervision, which is vital for accurate 3D segmentation as shown in Sec. 4.3."}, {"title": "3.5 Regularization", "content": "Hypersphere Regularization. Features with excessively large norms underestimate the participation of other features in the rendering process in Eq. (2), impeding effective learning of all Gaussian's features. To prevent any single Gaussian's feature from dominating in the a-blending process [23,24] of the feature rendering, similar to [52], we constrain Gaussian's features to lie on the surface of the hypersphere:\n\n\\(\\mathcal{L}_{3D-norm} = \\frac{1}{N} \\sum_{i=1}^N (\\|f_i^f\\|^2 - 1)^2 + (\\|f_i^c\\|^2 - 1)^2.\\)"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nWe implemented Click-Gaussian using the 3DGS codebase [17], adopting its default settings for pre-trained Gaussians. The hyperparameters were set as follows: \\(\\lambda_{cont} = 0.1, \\lambda_1 = 10.0, \\lambda_2 = 0.2, \\lambda_3 = 0.2\\), and \\(\\lambda_4 = 0.5\\). We employed the Adam optimizer with a learning rate of 0.01 for Gaussian's features. For contrastive learning, we sampled 10k pixels for each training iteration using importance sampling based on mask pixel count. In HDBSCAN, we set the epsilons for coarse and fine features clustering to \\(1 \\times 10^{-2}\\) and \\(1 \\times 10^{-3}\\), respectively, with the minimum cluster size proportional to the number of training views. We trained Click-Gaussian for 3,000 iterations, incorporating Global Feature-Guided learning from the 2,000th iteration onward. The entire training process took approximately 13 minutes on an NVIDIA RTX A5000 GPU."}, {"title": "4.2 Comparisons", "content": "Comparison on LERF-Mask Dataset. To demonstrate Click-Gaussian's segmentation superiority, we compared it with various baselines using the LERF-Mask dataset. For Gau-Group, target object IDs in the reference view were identified using a classifier and a ground truth mask [48]. OmniSeg3D's segmentation involved adjusting the cosine similarity threshold [52] from 0 to 1 in 0.01 increments, finding the optimal threshold for each target object. Feature3DGS utilized a rendered 2D feature map and the SAM's decoder for segmentation [53], selecting the best match for the target object. GARField applied a NeRF-based scale-conditioned field, selecting the best scale (0 to 1 in 0.05 steps) for each target object in the reference view [19].\nAs shown in Tab. 1, our method outperforms all baselines. Gau-Group underperforms in fine-level segmentation, due to its tracking methodology limitations."}, {"title": "4.3 Ablation Study", "content": "We evaluated the impact of removing granularity prior, GFL loss, and regularization terms from Click-Gaussian. Without the granularity prior (w/o prior), the model learns two-level features independently, lacking collaborative enhancement. As Tab. 3 shows, our complete model outperformed the one without prior in fine-level segmentation. This highlights how the intrinsic dependency between coarse and fine levels aids fine-level feature learning. GFL loss significantly improves fine-level segmentation by effectively addressing inconsistency and ambiguities of SAM-generated fine-level masks. Removing each regularization loss confirmed their collective importance in enhancing segmentation performance."}, {"title": "4.4 Versatile Applications", "content": "After training Click-Gaussian's two-level feature fields, various scene manipulation tasks become feasible, including object removal, resizing, repositioning, duplication, and text-based editing. As Fig. 7 shows, the two-level global clusters enable rapid object selection within a scene (approximately 10 ms), facilitating interactive local adjustments to selected Gaussians. Additionally, for text-based editing, we leveraged CLIP-based methods [32,40,44] to get faster results (within about 10 s) than diffusion-based methods [2, 14, 35]."}, {"title": "5 Conclusion", "content": "We present Click-Gaussian, a swift and precise method enabling interactive fine-grained segmentation of pre-trained 3D Gaussians by lifting 2D segmentation masks into 3D feature fields of two-level granularity. Noticing from the intrinsic dependency between coarse and fine levels in the real world, we employ a granularity prior for feature division in the representation of feature fields. To address feature learning hindered by the cross-view inconsistency masks, an inherent issue in lifting 2D masks to 3D, we propose the Global Feature-guided Learning method for more consistent feature field training. Once Click-Gaussian is trained, users can select desired objects at coarse and fine levels more swiftly than previous methods. This enhanced capability has the potential to improve efficient and precise 3D environment modification across various applications.\nLimitations. Our approach faces limitations due to its reliance on pre-trained 3DGS and the two-level granularity assumption. Feature learning may be hindered if a single Gaussian represents multiple objects, particularly when they are semantically distinct but chromatically similar. The two-level granularity assumption, lacking intermediate levels, could limit efficiency for varying granular levels and complex structures, potentially requiring multiple interactions to select desired segmentation regions."}]}