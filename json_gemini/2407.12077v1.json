{"title": "GoldFinch: High Performance RWKV/Transformer Hybrid with\nLinear Pre-Fill and Extreme KV-Cache Compression", "authors": ["Daniel Goldstein", "Fares Obeid", "Eric Alcaide", "Guangyu Song", "Eugene Cheah"], "abstract": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a\nnew technique to efficiently generate a highly compressed and reusable KV-Cache in linear time\nand space with respect to sequence length. GoldFinch stacks our new GOLD transformer on\ntop of an enhanced version of the Finch (RWKV-6) architecture. We train up to 1.5B parameter\nclass models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings increase linearly\nwith model layer count, ranging from 756-2550 times smaller than the traditional transformer\ncache for common sizes, enabling inference of extremely large context lengths even on limited\nhardware. Although autoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a submitted context costs only\nO(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. We\nrelease our trained weights and training code under the Apache 2.0 license for community use.", "sections": [{"title": "Introduction", "content": "Variations on linear attention (Katharopoulos et al., 2020) have proliferated in recent research\n(Peng et al., 2024; Qin et al., 2024; Katsch, 2024; Yang et al., 2024), approaching the performance\nof traditional Multi-Headed Scaled Dot Product Attention (MHA) (Vaswani et al., 2023) while\nachieving lower inference costs. In MHA, the model's effective memory is bounded by its context\nlength, with the attention calculation resulting in quadratic time complexity with regard to that\nlength. Conversely, most forms of linear attention can be computed recurrently in O(1) time per\ntime-step. Instead of inspecting the entire context length to generate each new token, recurrent\nlinear attention uses a fixed-size hidden state that is updated at each time-step, functioning as its\nmemory of the past. The limited size of this state constrains the capacity of this memory.\nThe success of Large Language Models (LLMs) has motivated interest in ultra-long context length\nlanguage models. For example, Gemini Pro (Team et al., 2024) offers a 1 million+ token length win-\ndow. However, if based on attention, these extra large context lengths come with large associated\ncosts due to the need for MHA to examine every prior token within the context when generating\na next token (Liu & Abbeel, 2023; Liu et al., 2023). Although a naive inference implementation\nwould recalculate every key and value at every layer in a traditional transformer, it is common\npractice to store these in a key-value cache (\"KV-Cache\") (Pope et al., 2022) and retrieve rather\nthan recompute them. KV-Cache memory costs can be very high. For example, a 1 million token\ncache for an 80 layer traditional transformer model of hidden dimension 8192 would take up\nover 2.5 terabytes at bfloat16 precision. We turn our focus to reducing the memory costs of this\ncache while also reducing computational complexity and memory usage for processing the initial\ncontext of a request.\nOur contribution is the combination of several innovations to create the GoldFinch architecture,\nwhich improves pre-fill and decoding efficiency, as well as downstream modeling performance,\nand introduces the following innovations:\nemploys a novel parameter-efficient modification of Finch (RWKV-6), which we call\n\"Finch-C2\", for the first 2/3 of its layers\nuses these the output of these Finch-C2 layers to produce an extremely small compressed\nglobal key cache using a novel mechanism we call \"TokenCat\". Our cache thus requires\nonlydmodel per token plus the original input token indices, instead of 2dmodelNlayer\nfor traditional KV-caches.\nemploys a novel modification of the traditional transformer architecture, which we call\n\"GOLD\", for the last 1/3 of its layers to consume this key cache and produce outputs\nwithout even requiring a traditional value cache.\nThe GOLD layers are an adaptation of a novel improved transformer we call \"GPTAlpha\" that can\nalso be used as a standalone transformer model for improved non-hybrid performance.\nThis new architecture brings a series of significant benefits:\nWe are able to reuse the same KV-Cache on every transformer layer while maintaining\ngreater than Llama (Touvron et al., 2023) performance. This reduces the KV-Cache size\nby a factor of the total number of layers of the model.\nWe eliminate the values from the KV-Cache, leaving only a key cache. Instead of caching\nvalues, we store the input indices and generate the values from these, reducing the\nKV-Cache size by another factor of nearly 2.\nWe are able to compress our key cache by applying a form of Low Rank Adaptation (LoRA)\n(Hu et al., 2021) to the output of a single layer, and re-expanding the compressed version\nby concatenating the compressed version with the original token embeddings, further\nreducing the size by 128 times. (\"TokenCat\")\nWe use the input embedding table and RWKV-style token shift to generate values for\nattention without sacrificing performance.\nBy using Finch-C2 blocks at the start of the model, the key cache automatically en-\ncodes the underlying implicit positional representation, thereby removing the need for\npositional encoding within our transformer layers for trained context lengths. We do\nstill require an additional positional encoding method for extrapolation to new context\nlengths unseen during training.\nThere are many use cases of LLMs that involve relatively short responses to questions\nabout long documents. Because our compressed key cache is generated by an RNN with\nan operating time and space complexity of O(1) per token with regard to sequence length,\nwe are able to generate the cache in these cases extremely inexpensively and apply the\nO(N) per token cost GOLD transformer portion of our calculations only to new token\ngeneration, for which relatively few iterations are often required.\nTo obtain our Finch-C2 architecture we improve the Finch time-mixer by removing the gate,\nswapping out GroupNorm for a LayerNorm across all heads, doing a new multiplication (of the key\nby one minus the decay) to keep the kv-state rows normalized, and replacing Finch's u (\"bonus\")\nterm with a new data-dependent separately token-shifted second Value. These changes result in\nimproved performance with little to no speed penalty and significantly fewer total parameters.\nTo obtain our GPTAlpha architecture we improve the Llama architecture by replacing the trans-\nformer feed-forward network (FFN) with the RWKV channel mixer, and adding RWKV style token\nshifts and extra LayerNorms to attention layers.\nBoth Finch-C2 and GPTAlpha can be used either as standalone model architectures with improved\nperformance over their counterparts, or as part of the GoldFinch hybrid model architecture.\nThe GOLD transformer architecture (GPTAlpha Over Linear transformer Decoder) removes the\nkey and value weights from GPTAlpha in favor of producing keys and values from a combination\nof the original token indices passed through the embeddings table, a highly compressed version\nof the outputs of the Finch-C2 layers, and a data-driven LoRA.\nGoldFinch stacks a set of GOLD transformer layers on top of a Finch-C2 linear transformer,\npassing the outputs for the Finch layers both into a key compressor to be stored for every sequence\nposition, and also through the current timestep as part of the normal residual stream.\nWe train GoldFinch models up to 1.45 billion parameters on 1.5 trillion tokens of minipile (Kaddour,\n2023) and compare them to slightly larger equivalently trained Finch (Peng et al., 2024) and Llama\n(Touvron et al., 2023) models. We find that GoldFinch significantly outperforms both Llama and\nFinch in downstream performance and perplexity across nearly every benchmark we tested, while\nmaintaining fewer parameters, a much smaller cache than Llama, and perfect MQAR recall due to\nits use of full attention."}, {"title": "Background", "content": "Transformers have become the de-facto choice for most sequence modeling tasks, and have been\nshown to be especially effective in the context of language modeling. However, they present com-\nputational challenges when processing long context lengths, which has hindered their adoption\nfor long sequence tasks. Specifically, the formulation of multi-head scaled dot-product attention\n(MHA) has a computational complexity of O(N\u00b2) with respect to context length. Additionally,\ninference engines typically rely on the use of a KV-Cache to enable autoregressive token generation\nin O(N) time per token. This cache grows linearly with context length, and becomes challenging\nto fit into limited Video Random-Access Memory (VRAM) for longer sequences.\nRecent transformer models such as the Llama series rely on Grouped-Query Attention (GQA)\n(Ainslie et al., 2023) to help ameliorate this cache size problem. At a typical number of groups\n$n_g = 8$, GQA reduces the KV-Cache size by $\\frac{n_h}{n_g}$ times, where $n_h$ is the number of heads. This\nis helpful, especially on consumer grade hardware, but leads to a reduction in downstream\nperformance, and longer sequences still cause a significant problem in terms of VRAM usage.\nThe recently proposed YOCO (Sun et al., 2024) improves the computational complexity for pre-fill\nof the initial request context and also reduces the KV-cache size by introducing a new global\nKV-Cache instead of the usual per-layer cache. The computational improvement is achieved by\nreplacing the first half of the layers in the model with Linear Attention based RetNet-G layers\n(Sun et al., 2023), which is a recurrent neural network (RNN) architecture that requires only linear\ntime with respect to sequence length. YOCO stores the output of these first layers as a global\nKV-Cache, which is then used by the second half of the layers, featuring MHA. Overall, this reduces\nthe KV-Cache size by a factor of the number of layers, without a reported performance reduction.\nGoldfinch takes a related approachetNet-G, and processes the output differently, creating an\neffective but much smaller cache via our TokenCat mechanism, which is then consumed by our\nenhanced transformer GOLD layers.\nHungry Hungry Hippos (H3) (Fu et al., 2023) train a hybrid recurrent SSM/transformer model\ncontaining just two layers of attention, which they find outperforms transformers. This served as\na warning shot that SSM(or linear attention)-transformer hybrids have the potential to step in as\nhigher performance replacements for transformers alone.\nRecognizing the challenges posed at inference time by the KV-Cache, DeepSeek-V2 (DeepSeek-AI\net al., 2024) proposes a replacement for MHA called Multi-head Latent Attention (MLA). This uses\nlow-rank joint key-value compression to reduce the size of the KV-Cache from $2n_d n_l d_h$ to $d_h l$,\nequivalent to the KV-Cache size required for GQA with only 2.25 groups. Because the low-rank\nkey-value compression requires fewer parameters than full rank key and value matrices, MLA\nachieves greater per-parameter performance than MHA. GoldFinch also improves performance\nvia this kind of compression-based relative parameter reduction.\nHGRN2 (Qin et al., 2024) replaces the per-head GroupNorm (Wu & He, 2018) with a full-width\nLayerNorm, and we do the same in our Finch-C2 architecture. HGRN2 sets their key to be equal to\none minus the decay, and we do something related but slightly different, multiplying our key by\none minus the decay.\nInspired by these works, we propose a new method that further reduces the KV-Cache by orders\nof magnitude and reduces the cost of the initial context load to become linear with respect to\nsequence length, all while achieving greater than Llama performance."}, {"title": "Other Concurrent Related Work", "content": "Other concurrent work on hybrid models bear some similarities to portions of our architecture:\nZamba (Glorioso et al., 2024) interleaves Global Shared Attention (GSA) every N Mamba blocks (Gu\n& Dao, 2024). Instead of using the residual output of the prior Mamba block as its input, Zamba\nconcatenates the original embeddings generated before layer zero onto this residual output, and\nuse the double-width combination as the input to attention. Although their GSA blocks share\nparameters, they are not able to share the same KV-Cache. The concatenation of embeddings\nbears similarity to our new \"TokenCat\" technique.\nJamba (Lieber et al., 2024) is a mixture-of-experts (MoE) (Shazeer et al., 2017) Mamba-based (Gu\n& Dao, 2024) model that inserts attention layers periodically within its architecture, for a total\nof 1:7 ratio of attention-to-Mamba layers. Similarly to Goldfinch's ability to rely upon RWKV's\nimplicit positional encoding within the pre-trained context length, they find that explicit positional\nencoding may not be required for their hybrid Mamba-based architecture.\nSamba (Ren et al., 2024) is a hybrid model that repeats blocks containing a Mamba layer, an MLP\nlayer, a sliding-window attention (SWA) layer featuring RoPE (Su et al., 2023), and another MLP\nlayer. The use of SWA allows a fixed cost of execution per token, regardless of context length."}, {"title": "Method", "content": "GoldFinch follows the general structure of the Finch architecture, which is also the common\npre-norm decoder transformer structure used in Llama and RWKV. It consists of a series of layers,\neach containing a time mixing sub-layer followed by a channel mixing sub-layer. All channel\nmixing sub-layers are Finch channel mixers.\nThe following formulae describe the three varieties of GoldFinch sub-layers. All matrices W are\nlearned per layer, unless described otherwise. We show all time mixing formulae per-head for\nconciseness, except the formulae for those layer outputs where heads are combined via concat.\nModel dimension is denoted as D, head size as H, and number of heads as N. All values are \u2208 RH\nunless otherwise noted."}, {"title": "Finch-C2 Time Mixing", "content": "The first two-thirds of time mixing sub-layers use a variation on the Finch time mixer we call\nFinch-C2.\nWe customize the Finch time-mixing sub-layers by removing the gate, swapping out GroupNorm\nfor a LayerNorm across all heads and doing a new multiplication of the key by one minus the decay.\nFinally, we replace Finch's u (\"bonus\") term with a new data-dependent separately token-shifted\nsecond Value, computed using the same weights as the base Value, with an additional LoRA added\nto the result. We find that this allows us to remove all of the Gate parameters while retaining\nperformance.\nAlong the lines of (Peng et al., 2024), we introduce the following notation for common operators in\nthe model, using the square subscript to denote a variable:\n$lerp(a, b, t) = a + (b \u2212 a) \u00a9 t,$\n$lora(x) = 1 + tanh(xA)B,$\n$ddlerp(a, b) = a + (b \u2212 a) \u00a9 lora\u25a1(a + (b \u2212 a) \u00a9 \u03bc\u03c7),$\nThen, the Finch-C2 block can be formalized as:\n$dt = lora! (ddlerpd (xt, Xt\u22121)),$\n$wt = exp(-exp(dt)),$\n$rt = ddlerp, (xt, Xt\u22121)WR,$\n$kt = ddlerpk(xt, Xt\u22121)WK \u00b7 (1 \u2013 wt),$\n$vt = ddlerp, (xt, Xt\u22121)WV,$\n$ut = ddlerpu (xt, Xt-1),$\n$u'\u2081 = u\u2081WV + tanh(utWUD)WUU.$\nAnd after splitting the hidden dimension into N heads:\n$wo, =\\begin{cases}\n\\frac{\\tau_t^{-1}}{\\sum_i \\tau_i^{-1}} & t=1\\\\\n-1 & j=i+1\n\\end{cases} \\text{diag}  wjk \u2208 RH,$\n$0t = LayerNorm(concat(rt\u00b7 wkvt + u\u2081))W\u00b0 \u2208RD.$\nPlease note that the calculation for u reuses the same weights WV - this is an intentional parame-\nter count savings and not a typo."}, {"title": "GOLD Key Compression", "content": "The output from the first two-thirds of the model is used in two ways: it is passed on to the next\nlayer in the usual manner, and also compressed down via multiplication with the global (not\nper-layer) learned matrix WKD \u2208 [RDx(D/16) to one sixteenth its original size and stored into a\nunified single-layer compressed key cache:\n$Ct = XtWKD ER(D/16).$"}, {"title": "GOLD Key Decompression (TokenCat)", "content": "The compressed key cache is decompressed via a two-step method. The first step is \"TokenCat\",\nshort for \"Token conCatenation\", in which the compressed key is concatenated with the original\ninput token embedding from the very beginning of the model. The concatenated result is then\nmultiplied with the global (not per-layer) learned matrix WKU \u2208 [R(D+D/16)xD and RMSNormed\nto obtain the decompressed attention proto-keys, which are common to all GOLD attention\nsub-layers.\n$k = RMSNorm (concat(x, ct) WKU).$"}, {"title": "GOLD Attention Time Mixing", "content": "The remaining time mixing sub-layers are a variation on GPTAlpha attention sub-layers employing\nMHA that we call GOLD attention.\nEach GOLD attention sub-layer calculates its own unique attention keys and values from the\ndecompressed proto-keys and the original input token embeddings, respectively. Each is passed\nthrough a data-dependent token shift, with the result passed through an additive LoRA. We call\nthis process \"DDLoRAdapt\", introducing the relevant notation below, using the square subscript\nto denote a variable:\n$loradapt (x) = x + tanh(xC)D.$\nThe following are the formulae for GOLD attention time mixing:\n$qt = LayerNorm(ddlerpq(xt, Xt\u22121)W\u00ba),$\n$at = lerp(x,x-1, \u03bc\u03c7),$\n$k\u2081 = LayerNorm (loradapt\u3047 (lerp (k\u0142, k-1,lorak (at)))),$\n$vt = LayerNorm (loradapt, (lerp (x1, x2-1,lora, (at)))),$\n$0t = LayerNorm(concat (attention(qt, k, v)))W\u00ba \u2208 RD.$\nPlease note the receptance-like Finch style token-shift on queries, and additional data-driven\ntoken-shift on keys and values, with keys being reconstituted from compressed key cache entries\nct and values coming from the original token embeddings x\u00ba. x\u00ba is the embedding input to the\nfirst sub-layer in the model, and can be reconstituted during inference from the token indices by\nstoring those indices, usually only an additional two bytes per context length.\nData dependent token shift (ddlerp) is a specialized low-parameter cost variety of two-step 1D\nconvolution that originated in the RWKV architecture. It allows the model to dynamically linearly\ninterpolate between the current and previous time-step on a per channel basis. We use our\nDDLORAdapt version of the technique to inexpensively apply contextual information to the\nkeys and values, increasing the amount of information from which they are generated without\nsignificantly increasing parameter count."}, {"title": "GoldFinch Channel Mixing (same as Finch Channel Mixing)", "content": "Goldfinch channel mixing is identical to Finch channel mixing. It is used as the feed forward\nnetwork component on all layers of the model, both Finch-C2 and GOLD. We reproduce it here for\nreference. Please note that variables have their own independent definitions in this subsection.\n$rt = lerp, (xt, Xt\u22121, \u03bcr)WR\u2208RD,$\n$kt = lerpk(xt, Xt\u22121,\u00b5k)WK \u2208 [R3.5D,$\n$vt = ReLU(kt)2WV \u2208RD,$\n$Ot = \u03c3(rt) \u00a9 Vt \u2208ERD.$"}, {"title": "GPTAlpha Time Mixing", "content": "For completeness and to show how it can be used in a pure transformer architecture, we list the\nformulae for GPTAlpha time mixing when not used in conjunction with TokenCat below:\n$qt = LayerNorm(ddlerpq(xt, Xt\u22121)W\u00ba),$\n$kt = LayerNorm(ddlerpk(xt, Xt\u22121)WK),$\n$vt = LayerNorm(ddlerp\u2081(xt, Xt\u22121)WV),$\n$ot = LayerNorm(concat (attention(qt, k, v)))W\u00ba \u2208 RD.$"}, {"title": "Experiments", "content": "We trained 1.5B parameter-class models with 24 layers, 2048 hidden-dimension, 2048 context\nlength of Finch, Llama, and GoldFinch for comparison on minipile (Kaddour, 2023), all using the\nsame RWKV World tokenizer. GoldFinch ends with dramatically lower final loss than the others\n(by over 0.1 out of 2.39), and uses over 100 million fewer parameters than its Finch counterpart."}, {"title": "Ablation Studies", "content": "We ran various smaller scale ablation studies to determine the contributions of different parts of\nthe GoldFinch architecture relative to both Finch, Llama, GPTAlpha, and a hybrid of our improved\nFinch and GPTAlpha with no KV-Cache compression or key/value sharing. The new second\nvalue added in Finch-C2 had the smallest positive impact of anything measured. Surprisingly,"}, {"title": "Associative Recall", "content": "Associative recall (AR) is a synthetic task designed to emulate the human ability to associate and\nretrieve information. It evaluates a model's skill in recalling previously mentioned information\nwithin a given context. Previous studies suggest that a model's performance in AR is a good\nindicator of its efficacy in in-context learning (Elhage et al., 2021; Olsson et al., 2022). Consequently,\nAR has been employed as a benchmark for developing new language model architectures (Fu\net al., 2023; Poli et al., 2023; Lutati et al., 2023). Arora et al. (2023) evaluated a variety of models for\nmulti-query associative recall (MQAR) and discovered a performance gap between different linear\ntransformer architectures and the traditional transformer with attention."}, {"title": "Long Context Experiments", "content": "We tested the loss of our small Finch and GoldFinch models pre-trained on minipile at all context\nlengths up to 65536 on the PG19 (Rae et al., 2019) dataset of older books. These pre-trained models\nwere all trained at only 1024 context length. The Finch model is able to maintain a fairly low\nloss throughout the 65536 context length. The base GoldFinch model trained with no positional"}, {"title": "Checkpoint Upgrade Training", "content": "We have attempted upgrading existing pre-trained Finch models to a more limited version of\nGoldFinch that uses the Finch architecture for its RWKV layers instead of the Finch-C2 component.\nWe tried many variations on two methods, one that adds new GOLD layers on top for a total of\naround 11% more parameters, and another which keeps the layer count the same as the pre-trained\nmodel. Thus far with only small amounts of upgrade training neither method has performed to\nour satisfaction.\nBoth methods were attempted on a 1.6B Finch checkpoint that had been pre-trained on 2.5 trillion\ntokens.\nFor the first method we appended 4 GOLD layers on top of the pre-trained 1.6B Finch checkpoint\nbefore the language modeling head, and continued training it for 100 million tokens using two\ndifferent learning rates. The original 24 pre-trained layers were kept at the same le-5 LR at which\ntheir pre-training had ended upon, while the LR for the 4 new GOLD layers was annealed along\na cosine schedule from 3e-4 to 1e-5. While the performance of this model was in line with the\noriginal model, it was unclear if the resultant model from this method really learned anything of\nvalue in its GOLD layers.\nThe second method involved freezing the embedding and RWKV layers and importing but not\nfreezing the final 1/3 of the channel mixer sub-layers that were paired with freshly initialized\nGOLD attention sub-layers. We then trained this model on a relatively small amount of data"}, {"title": "Further Work", "content": "We anticipate updating this pre-print with further studies as results become available, including\ncheckpoint upgrade results and evaluations, longer experiment training runs, and new long\ncontext experiments. Please check back for updates.\nMost of the experiments done for this pre-print were performed over a short period of time on\na single node containing 8 RTX 4090 cards. In the future we hope to demonstrate GoldFinch's\nperformance on larger models with significantly more tokens.\nWe expect that GoldFinch will work similarly with other linear attention and SSM architectures in\nplace of the Finch-C2 blocks. For example, it should be possible to implement a \"GoldMamba\"\narchitecture in the same style.\nFurther work might explore increased memory reduction for the global KV-Cache via quantiza-\ntion, and application of ring attention Liu et al. (2023) to lower the memory requirements when\nextending to very long contexts. As a hybrid architecture model, GoldFinch will likely benefit from\nany future improvements to the RWKV and transformer architectures."}, {"title": "Conclusion", "content": "We have introduced a hybrid RNN-Attention model architecture (GoldFinch) and trained mod-\nels that demonstrate its performance up to 1.45B. The resulting hybrid RNN-Attention models\ncombine the efficiency of RNNs with the capabilities of attention-based models. Having RNNs for\nthe initial layers allows for fast pre-fill and removes the need for positional encoding on the RNN\nlayers, while the attention layers improve associative recall. The combination with a highly com-\npressed global KV-Cache unlocks a memory reduction in inference while maintaining enhanced\nperformance. We release the trained weights and training code under the Apache 2.0 license."}]}