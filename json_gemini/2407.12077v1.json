{"title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression", "authors": ["Daniel Goldstein", "Fares Obeid", "Eric Alcaide", "Guangyu Song", "Eugene Cheah"], "abstract": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model that uses a new technique to efficiently generate a highly compressed and reusable KV-Cache in linear time and space with respect to sequence length. GoldFinch stacks our new GOLD transformer on top of an enhanced version of the Finch (RWKV-6) architecture. We train up to 1.5B parameter class models of the Finch, Llama, and GoldFinch architectures, and find dramatically improved modeling performance relative to both Finch and Llama. Our cache size savings increase linearly with model layer count, ranging from 756-2550 times smaller than the traditional transformer cache for common sizes, enabling inference of extremely large context lengths even on limited hardware. Although autoregressive generation has O(n) time complexity per token because of attention, pre-fill computation of the entire initial cache state for a submitted context costs only O(1) time per token due to the use of a recurrent neural network (RNN) to generate this cache. We release our trained weights and training code under the Apache 2.0 license for community use.", "sections": [{"title": "Introduction", "content": "Variations on linear attention (Katharopoulos et al., 2020) have proliferated in recent research (Peng et al., 2024; Qin et al., 2024; Katsch, 2024; Yang et al., 2024), approaching the performance of traditional Multi-Headed Scaled Dot Product Attention (MHA) (Vaswani et al., 2023) while achieving lower inference costs. In MHA, the model's effective memory is bounded by its context length, with the attention calculation resulting in quadratic time complexity with regard to that length. Conversely, most forms of linear attention can be computed recurrently in O(1) time per time-step. Instead of inspecting the entire context length to generate each new token, recurrent linear attention uses a fixed-size hidden state that is updated at each time-step, functioning as its memory of the past. The limited size of this state constrains the capacity of this memory.\nThe success of Large Language Models (LLMs) has motivated interest in ultra-long context length language models. For example, Gemini Pro (Team et al., 2024) offers a 1 million+ token length window. However, if based on attention, these extra large context lengths come with large associated costs due to the need for MHA to examine every prior token within the context when generating a next token (Liu & Abbeel, 2023; Liu et al., 2023). Although a naive inference implementation would recalculate every key and value at every layer in a traditional transformer, it is common practice to store these in a key-value cache (\"KV-Cache\") (Pope et al., 2022) and retrieve rather than recompute them. KV-Cache memory costs can be very high. For example, a 1 million token cache for an 80 layer traditional transformer model of hidden dimension 8192 would take up over 2.5 terabytes at bfloat16 precision. We turn our focus to reducing the memory costs of this cache while also reducing computational complexity and memory usage for processing the initial context of a request.\nOur contribution is the combination of several innovations to create the GoldFinch architecture, which improves pre-fill and decoding efficiency, as well as downstream modeling performance, and introduces the following innovations:\n1. employs a novel parameter-efficient modification of Finch (RWKV-6), which we call \"Finch-C2\", for the first 2/3 of its layers\n2. uses these the output of these Finch-C2 layers to produce an extremely small compressed global key cache using a novel mechanism we call \"TokenCat\". Our cache thus requires only dmodel per token plus the original input token indices, instead of 2dmodelNlayer for traditional KV-caches.\n3. employs a novel modification of the traditional transformer architecture, which we call \"GOLD\", for the last 1/3 of its layers to consume this key cache and produce outputs without even requiring a traditional value cache.\nThe GOLD layers are an adaptation of a novel improved transformer we call \"GPTAlpha\" that can also be used as a standalone transformer model for improved non-hybrid performance.\nThis new architecture brings a series of significant benefits:\n1. We are able to reuse the same KV-Cache on every transformer layer while maintaining greater than Llama (Touvron et al., 2023) performance. This reduces the KV-Cache size by a factor of the total number of layers of the model.\n2. We eliminate the values from the KV-Cache, leaving only a key cache. Instead of caching values, we store the input indices and generate the values from these, reducing the KV-Cache size by another factor of nearly 2.\n3. We are able to compress our key cache by applying a form of Low Rank Adaptation (LoRA) (Hu et al., 2021) to the output of a single layer, and re-expanding the compressed version by concatenating the compressed version with the original token embeddings, further reducing the size by 128 times. (\"TokenCat\")\n4. We use the input embedding table and RWKV-style token shift to generate values for attention without sacrificing performance.\n5. By using Finch-C2 blocks at the start of the model, the key cache automatically en-codes the underlying implicit positional representation, thereby removing the need for positional encoding within our transformer layers for trained context lengths. We do still require an additional positional encoding method for extrapolation to new context lengths unseen during training.\n6. There are many use cases of LLMs that involve relatively short responses to questions about long documents. Because our compressed key cache is generated by an RNN with an operating time and space complexity of O(1) per token with regard to sequence length, we are able to generate the cache in these cases extremely inexpensively and apply the O(N) per token cost GOLD transformer portion of our calculations only to new token generation, for which relatively few iterations are often required.\nTo obtain our Finch-C2 architecture we improve the Finch time-mixer by removing the gate, swapping out GroupNorm for a LayerNorm across all heads, doing a new multiplication (of the key by one minus the decay) to keep the kv-state rows normalized, and replacing Finch's u (\"bonus\") term with a new data-dependent separately token-shifted second Value. These changes result in improved performance with little to no speed penalty and significantly fewer total parameters.\nTo obtain our GPTAlpha architecture we improve the Llama architecture by replacing the trans-former feed-forward network (FFN) with the RWKV channel mixer, and adding RWKV style token shifts and extra LayerNorms to attention layers.\nBoth Finch-C2 and GPTAlpha can be used either as standalone model architectures with improved performance over their counterparts, or as part of the GoldFinch hybrid model architecture.\nThe GOLD transformer architecture (GPTAlpha Over Linear transformer Decoder) removes the key and value weights from GPTAlpha in favor of producing keys and values from a combination of the original token indices passed through the embeddings table, a highly compressed version of the outputs of the Finch-C2 layers, and a data-driven LoRA.\nGoldFinch stacks a set of GOLD transformer layers on top of a Finch-C2 linear transformer, passing the outputs for the Finch layers both into a key compressor to be stored for every sequence position, and also through the current timestep as part of the normal residual stream.\nWe train GoldFinch models up to 1.45 billion parameters on 1.5 trillion tokens of minipile (Kaddour, 2023) and compare them to slightly larger equivalently trained Finch (Peng et al., 2024) and Llama (Touvron et al., 2023) models. We find that GoldFinch significantly outperforms both Llama and Finch in downstream performance and perplexity across nearly every benchmark we tested, while maintaining fewer parameters, a much smaller cache than Llama, and perfect MQAR recall due to its use of full attention."}, {"title": "Background", "content": "Transformers have become the de-facto choice for most sequence modeling tasks, and have been shown to be especially effective in the context of language modeling. However, they present com-putational challenges when processing long context lengths, which has hindered their adoption for long sequence tasks. Specifically, the formulation of multi-head scaled dot-product attention (MHA) has a computational complexity of O(N\u00b2) with respect to context length. Additionally, inference engines typically rely on the use of a KV-Cache to enable autoregressive token generation in O(N) time per token. This cache grows linearly with context length, and becomes challenging to fit into limited Video Random-Access Memory (VRAM) for longer sequences.\nRecent transformer models such as the Llama series rely on Grouped-Query Attention (GQA) (Ainslie et al., 2023) to help ameliorate this cache size problem. At a typical number of groups nh = 8, GQA reduces the KV-Cache size by nh times, where np is the number of heads. This np is helpful, especially on consumer grade hardware, but leads to a reduction in downstream performance, and longer sequences still cause a significant problem in terms of VRAM usage.\nThe recently proposed YOCO (Sun et al., 2024) improves the computational complexity for pre-fill of the initial request context and also reduces the KV-cache size by introducing a new global KV-Cache instead of the usual per-layer cache. The computational improvement is achieved by replacing the first half of the layers in the model with Linear Attention based RetNet-G layers (Sun et al., 2023), which is a recurrent neural network (RNN) architecture that requires only linear time with respect to sequence length. YOCO stores the output of these first layers as a global KV-Cache, which is then used by the second half of the layers, featuring MHA. Overall, this reduces the KV-Cache size by a factor of the number of layers, without a reported performance reduction. Goldfinch takes a related approachetNet-G, and processes the output differently, creating an effective but much smaller cache via our TokenCat mechanism, which is then consumed by our enhanced transformer GOLD layers.\nHungry Hungry Hippos (H3) (Fu et al., 2023) train a hybrid recurrent SSM/transformer model containing just two layers of attention, which they find outperforms transformers. This served as a warning shot that SSM(or linear attention)-transformer hybrids have the potential to step in as higher performance replacements for transformers alone.\nRecognizing the challenges posed at inference time by the KV-Cache, DeepSeek-V2 (DeepSeek-AI et al., 2024) proposes a replacement for MHA called Multi-head Latent Attention (MLA). This uses low-rank joint key-value compression to reduce the size of the KV-Cache from 2nndnl to dhl, equivalent to the KV-Cache size required for GQA with only 2.25 groups. Because the low-rank key-value compression requires fewer parameters than full rank key and value matrices, MLA achieves greater per-parameter performance than MHA. GoldFinch also improves performance via this kind of compression-based relative parameter reduction.\nHGRN2 (Qin et al., 2024) replaces the per-head GroupNorm (Wu & He, 2018) with a full-width LayerNorm, and we do the same in our Finch-C2 architecture. HGRN2 sets their key to be equal to one minus the decay, and we do something related but slightly different, multiplying our key by one minus the decay.\nInspired by these works, we propose a new method that further reduces the KV-Cache by orders of magnitude and reduces the cost of the initial context load to become linear with respect to sequence length, all while achieving greater than Llama performance."}, {"title": "Other Concurrent Related Work", "content": "Other concurrent work on hybrid models bear some similarities to portions of our architecture:\nZamba (Glorioso et al., 2024) interleaves Global Shared Attention (GSA) every N Mamba blocks (Gu & Dao, 2024). Instead of using the residual output of the prior Mamba block as its input, Zamba concatenates the original embeddings generated before layer zero onto this residual output, and use the double-width combination as the input to attention. Although their GSA blocks share parameters, they are not able to share the same KV-Cache. The concatenation of embeddings bears similarity to our new \"TokenCat\" technique.\nJamba (Lieber et al., 2024) is a mixture-of-experts (MoE) (Shazeer et al., 2017) Mamba-based (Gu & Dao, 2024) model that inserts attention layers periodically within its architecture, for a total of 1:7 ratio of attention-to-Mamba layers. Similarly to Goldfinch's ability to rely upon RWKV's implicit positional encoding within the pre-trained context length, they find that explicit positional encoding may not be required for their hybrid Mamba-based architecture.\nSamba (Ren et al., 2024) is a hybrid model that repeats blocks containing a Mamba layer, an MLP layer, a sliding-window attention (SWA) layer featuring RoPE (Su et al., 2023), and another MLP layer. The use of SWA allows a fixed cost of execution per token, regardless of context length."}, {"title": "Method", "content": "GoldFinch follows the general structure of the Finch architecture, which is also the common pre-norm decoder transformer structure used in Llama and RWKV. It consists of a series of layers, each containing a time mixing sub-layer followed by a channel mixing sub-layer. All channel mixing sub-layers are Finch channel mixers.\nThe following formulae describe the three varieties of GoldFinch sub-layers. All matrices W are learned per layer, unless described otherwise. We show all time mixing formulae per-head for conciseness, except the formulae for those layer outputs where heads are combined via concat. Model dimension is denoted as D, head size as H, and number of heads as N. All values are \u2208 RH unless otherwise noted."}, {"title": "Finch-C2 Time Mixing", "content": "The first two-thirds of time mixing sub-layers use a variation on the Finch time mixer we call Finch-C2.\nWe customize the Finch time-mixing sub-layers by removing the gate, swapping out GroupNorm for a LayerNorm across all heads and doing a new multiplication of the key by one minus the decay. Finally, we replace Finch's u (\"bonus\") term with a new data-dependent separately token-shifted second Value, computed using the same weights as the base Value, with an additional LoRA added to the result. We find that this allows us to remove all of the Gate parameters while retaining performance.\nAlong the lines of (Peng et al., 2024), we introduce the following notation for common operators in the model, using the square subscript to denote a variable:\n$\nlerp(a, b, t) = a + (b \u2212 a) \u00a9 t,\n$\n$\nlora(x) = 1 + tanh(xA)B,\n$\n$\nddlerp(a, b) = a + (b \u2212 a) \u00a9 lora\u25a1(a + (b \u2212 a) \u00a9 \u03bc\u03c7),\n$\nThen, the Finch-C2 block can be formalized as:\n$\ndt = lora! (ddlerpd (xt, Xt\u22121)),\n$\n$\nwt = exp(-exp(dt)),\n$\n$\nrt = ddlerp, (xt, Xt\u22121)WR,\n$\n$\nkt = ddlerpk(xt, Xt\u22121)WK \u00b7 (1 \u2013 wt),\n$\n$\nvt = ddlerp, (xt, Xt\u22121)WV,\n$\n$\nut = ddlerpu (xt, Xt-1),\n$\n$\nu'\u2081 = u\u2081WV + tanh(utWUD)WUU.\n$\nAnd after splitting the hidden dimension into N heads:\n$\n= \u2211\ni=1\nt-1\n(j=i+1\nwko, diag wjk \u2208 RH,\n$\n$\n0t = LayerNorm(concat(rt\u00b7 wkvt + u\u2081))W\u00b0 \u2208RD.\n$\nPlease note that the calculation for u reuses the same weights WV - this is an intentional parame-ter count savings and not a typo."}, {"title": "GOLD Key Compression", "content": "The output from the first two-thirds of the model is used in two ways: it is passed on to the next layer in the usual manner, and also compressed down via multiplication with the global (not per-layer) learned matrix WKD \u2208 [RDx(D/16) to one sixteenth its original size and stored into a unified single-layer compressed key cache:\n$\nCt = XtWKD ER(D/16).\n$"}, {"title": "GOLD Key Decompression (TokenCat)", "content": "The compressed key cache is decompressed via a two-step method. The first step is \"TokenCat\", short for \"Token conCatenation\", in which the compressed key is concatenated with the original input token embedding from the very beginning of the model. The concatenated result is then multiplied with the global (not per-layer) learned matrix WKU \u2208 [R(D+D/16)xD and RMSNormed to obtain the decompressed attention proto-keys, which are common to all GOLD attention sub-layers.\n$\nk = RMSNorm (concat(x, ct) WKU).\n$"}, {"title": "GOLD Attention Time Mixing", "content": "The remaining time mixing sub-layers are a variation on GPTAlpha attention sub-layers employing MHA that we call GOLD attention.\nEach GOLD attention sub-layer calculates its own unique attention keys and values from the decompressed proto-keys and the original input token embeddings, respectively. Each is passed through a data-dependent token shift, with the result passed through an additive LoRA. We call this process \"DDLoRAdapt\", introducing the relevant notation below, using the square subscript to denote a variable:\n$\nloradapt (x) = x + tanh(xC)D.\n$\nThe following are the formulae for GOLD attention time mixing:\n$\nqt = LayerNorm(ddlerpq(xt, Xt\u22121)W\u00ba),\n$\n$\nat = lerp(x,x-1, \u03bc\u03c7),\n$\n$\nk\u2081 = LayerNorm (loradapt\u3047 (lerp (k\u0142, k-1,lorak (at)))),\n$\n$\nvt = LayerNorm (loradapt, (lerp (x1, x2-1,lora, (at)))),\n$\n$\not = LayerNorm(concat (attention(qt, k, v)))W\u00ba \u2208 RD.\n$\nPlease note the receptance-like Finch style token-shift on queries, and additional data-driven token-shift on keys and values, with keys being reconstituted from compressed key cache entries ct and values coming from the original token embeddings x\u00ba. x\u00ba is the embedding input to the first sub-layer in the model, and can be reconstituted during inference from the token indices by storing those indices, usually only an additional two bytes per context length.\nData dependent token shift (ddlerp) is a specialized low-parameter cost variety of two-step 1D convolution that originated in the RWKV architecture. It allows the model to dynamically linearly interpolate between the current and previous time-step on a per channel basis. We use our DDLORAdapt version of the technique to inexpensively apply contextual information to the keys and values, increasing the amount of information from which they are generated without significantly increasing parameter count."}, {"title": "GoldFinch Channel Mixing (same as Finch Channel Mixing)", "content": "Goldfinch channel mixing is identical to Finch channel mixing. It is used as the feed forward network component on all layers of the model, both Finch-C2 and GOLD. We reproduce it here for reference. Please note that variables have their own independent definitions in this subsection.\n$\nrt = lerp, (xt, Xt\u22121, \u03bcr)WR\u2208RD,\n$\n$\nkt = lerpk(xt, Xt\u22121,\u00b5k)WK \u2208 [R3.5D,\n$\n$\nvt = ReLU(kt)2WV \u2208RD,\n$\n$\nOt = \u03c3(rt) \u00a9 Vt \u2208ERD.\n$"}, {"title": "GPTAlpha Time Mixing", "content": "For completeness and to show how it can be used in a pure transformer architecture, we list the formulae for GPTAlpha time mixing when not used in conjunction with TokenCat below:\n$\nqt = LayerNorm(ddlerpq(xt, Xt\u22121)W\u00ba),\n$\n$\nkt = LayerNorm(ddlerpk(xt, Xt\u22121)WK),\n$\n$\nvt = LayerNorm(ddlerp\u2081(xt, Xt\u22121)WV),\n$\n$\not = LayerNorm(concat (attention(qt, k, v)))W\u00ba \u2208 RD.\n$"}, {"title": "Experiments", "content": "We trained 1.5B parameter-class models with 24 layers, 2048 hidden-dimension, 2048 context length of Finch, Llama, and GoldFinch for comparison on minipile (Kaddour, 2023), all using the same RWKV World tokenizer. GoldFinch ends with dramatically lower final loss than the others (by over 0.1 out of 2.39), and uses over 100 million fewer parameters than its Finch counterpart.\nWe additionally trained a GoldFinch with no compression, to show that there is very little lost with our choice of a 16:1 hidden-dimension compression ratio.\nIn the interest of fairly comparing performance for Llama by giving it the most favorable conditions, we add the RWKV small init embeddings optimization (LayerNorm after embeddings with small initialized values) (Peng et al., 2023) and do not employ Grouped Query Attention. All architectures used the same hyperparameters and were trained on 4 GPUs, with per-GPU per-step batch size of 8, two steps of gradient accumulation, and a 10 step learning rate warm-up followed by cosine decay annealed from 3e-5 to le-5. We train with Adam betas of 0.9 and 0.99, epsilon 1e-8 and weight decay 0.001. Weight decay was applied only to matrix parameters that are not part of LoRAs or the GoldFinch key compression/expansion steps."}, {"title": "Ablation Studies", "content": "We ran various smaller scale ablation studies to determine the contributions of different parts of the GoldFinch architecture relative to both Finch, Llama, GPTAlpha, and a hybrid of our improved Finch and GPTAlpha with no KV-Cache compression or key/value sharing. The new second value added in Finch-C2 had the smallest positive impact of anything measured. Surprisingly, GoldFinch performed very slightly better than even the Finch-C2/GPTAlpha hybrid with no KV compression at all. Each test trained a 12 layer 768 hidden-dimension model at 1024 context length with the same RWKV World tokenizer on the full minipile dataset. All architectures used the same hyperparameters and were trained on single GPUs, with per-step batch size of 32, two steps of gradient accumulation, and a 10 step learning rate warm-up followed by cosine decay annealed from 6e-5 to 2e-5. We train with Adam betas of 0.9 and 0.99, epsilon 1e-8 and weight decay 0.001. Weight decay was applied only to matrix parameters that are not part of LoRAs or the GoldFinch key compression/expansion steps."}, {"title": "Associative Recall", "content": "Associative recall (AR) is a synthetic task designed to emulate the human ability to associate and retrieve information. It evaluates a model's skill in recalling previously mentioned information within a given context. Previous studies suggest that a model's performance in AR is a good indicator of its efficacy in in-context learning (Elhage et al., 2021; Olsson et al., 2022). Consequently, AR has been employed as a benchmark for developing new language model architectures (Fu et al., 2023; Poli et al., 2023; Lutati et al., 2023). Arora et al. (2023) evaluated a variety of models for multi-query associative recall (MQAR) and discovered a performance gap between different linear transformer architectures and the traditional transformer with attention."}, {"title": "Long Context Experiments", "content": "We tested the loss of our small Finch and GoldFinch models pre-trained on minipile at all context lengths up to 65536 on the PG19 (Rae et al., 2019) dataset of older books. These pre-trained models were all trained at only 1024 context length. The Finch model is able to maintain a fairly low loss throughout the 65536 context length. The base GoldFinch model trained with no positional encoding goes up in loss significantly starting at around double the trained context length, then plateauing at a high loss. The GoldFinch model trained with RoPE on its GOLD attention sub-layers performs better, but loss still increases somewhat as the sequence progresses. However, by applying interpolated RoPE values we are able to obtain low loss throughout the extended context length. We conclude that for GoldFinch models in which extrapolation beyond the maximum trained context length is desired, the GOLD attention sub-layers should be trained with ROPE, with interpolation employed upon inference.\nWe then fine-tuned the ROPE and non-ROPE models mentioned above on 165 million tokens of minipile at longer context lengths. During this fine-tuning, we froze the entire RWKV portion of the model up to the first GOLD layer, allowing the optimizer to update the parameters of only the GOLD layers and output head. This saves a significant amount of time and VRAM during fine-tuning, allowing an even longer context length to fit into memory and using roughly 3x fewer FLOPS per token. We theorize that because the GOLD attention portion of the model can use keys generated from the RWKV output, this is enough to support sophisticated attention matching across the entire context length.\nOur experiments showed that indeed the RoPE model with GOLD layers fine-tuned at longer context lengths exhibited significantly lower losses against PG19 up through those lengths and even beyond. On the non-RoPE model this process was somewhat successful within the fine-tuned context length, while still failing at extrapolation. This was unexpected, since the RWKV layers were not updated and the GOLD layers included no positional encoding mechanism. We postulate that token-shift may supply some minimal positional information to the model."}, {"title": "Checkpoint Upgrade Training", "content": "We have attempted upgrading existing pre-trained Finch models to a more limited version of GoldFinch that uses the Finch architecture for its RWKV layers instead of the Finch-C2 component. We tried many variations on two methods, one that adds new GOLD layers on top for a total of around 11% more parameters, and another which keeps the layer count the same as the pre-trained model. Thus far with only small amounts of upgrade training neither method has performed to our satisfaction.\nBoth methods were attempted on a 1.6B Finch checkpoint that had been pre-trained on 2.5 trillion tokens.\nFor the first method we appended 4 GOLD layers on top of the pre-trained 1.6B Finch checkpoint before the language modeling head, and continued training it for 100 million tokens using two different learning rates. The original 24 pre-trained layers were kept at the same le-5 LR at which their pre-training had ended upon, while the LR for the 4 new GOLD layers was annealed along a cosine schedule from 3e-4 to 1e-5. While the performance of this model was in line with the original model, it was unclear if the resultant model from this method really learned anything of value in its GOLD layers.\nThe second method involved freezing the embedding and RWKV layers and importing but not freezing the final 1/3 of the channel mixer sub-layers that were paired with freshly initialized GOLD attention sub-layers. We then trained this model on a relatively small amount of data (in our case around 7.5 billion tokens of a new internal dataset) while annealing the learning rate to the final learning rate seen in the pre-trained base model. The resultant model obtained a similar validation loss on minipile to the base model, despite being trained on a completely different dataset and the base model having been already trained for over 2.25 trillion tokens. However, the new model's LAMBADA scores were worse. We attribute this loss of performance to the 'brain surgery' required to keep the layer count the same, in which we effectively erased the Finch time-mix parameters in the upper 1/3rd of the model.\nWe are still doing further experimentation on these upgrade methods to see just how well they can be made to perform. We hope to be able to inexpensively upgrade even the largest 14B Finch model to this reduced GoldFinch format and see significant performance improvements at larger context lengths due to the GOLD attention being able to look back across the entire context with no state-size based memory limitations."}, {"title": "Further Work", "content": "We anticipate updating this pre-print with further studies as results become available, including checkpoint upgrade results and evaluations, longer experiment training runs, and new long context experiments. Please check back for updates.\nMost of the experiments done for this pre-print were performed over a short period of time on a single node containing 8 RTX 4090 cards. In the future we hope to demonstrate GoldFinch's performance on larger models with significantly more tokens.\nWe expect that GoldFinch will work similarly with other linear attention and SSM architectures in place of the Finch-C2 blocks. For example, it should be possible to implement a \"GoldMamba\" architecture in the same style.\nFurther work might explore increased memory reduction for the global KV-Cache via quantiza-tion, and application of ring attention Liu et al. (2023) to lower the memory requirements when extending to very long contexts. As a hybrid architecture model, GoldFinch will likely benefit from any future improvements to the RWKV and transformer architectures."}, {"title": "Conclusion", "content": "We have introduced a hybrid RNN-Attention model architecture (GoldFinch) and trained mod-els that demonstrate its performance up to 1.45B. The resulting hybrid RNN-Attention models combine the efficiency of RNNs with the capabilities of attention-based models. Having RNNs for the initial layers allows for fast pre-fill and removes the need for positional encoding on the RNN layers, while the attention layers improve associative recall. The combination with a highly com-pressed global KV-Cache unlocks a memory reduction in inference while maintaining enhanced performance. We release the trained weights and training code under the Apache 2.0 license."}, {"title": "Author Contributions", "content": "Daniel Goldstein Entire GPTAlpha design, research, and code. GoldFinch code, architecture design, and research. Full manuscript initial draft except 4.3. Manuscript edits. Proofreading and revisions of full manuscript. Core experiments featured herein.\nFares Obeid Research discussions and experiments during development of the GoldFinch archi-tecture. Significant input on all aspects of final architecture design.\nEric Alcaide Research discussions and experiments during development of the GoldFinch archi-tecture. Significant input and experiments leading to Finch-C2 design.\nGuangyu Song Section 4.3. Experiments for 4.3.\nEugene Cheah GoldFinch code proofreading, development of release code and testing, contri-butions to pre-fill mechanism details."}, {"title": "Other Related Work", "content": "Ring Attention (Liu et al., 2023) allows the attention calculation to be split across many discrete processors that do not share VRAM. Keys and values can be split up among these processors, linearly amortizing the amount of KV-Cache required to remain resident within each processor's VRAM. This enables unbounded scaling of attention given enough hardware, but does not address the cost of O(N\u00b2) compute, and still imposes total memory costs that scale with the sequence length."}]}