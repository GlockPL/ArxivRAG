{"title": "Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark", "authors": ["Haidong Xu", "Meishan Zhang", "Hao Ju", "Zhedong Zheng", "Hongyuan Zhu", "Erik Cambria", "Min Zhang", "Hao Fei"], "abstract": "Producing emotionally dynamic 3D facial avatars with text derived from spoken words-referred to as emotion-aware Text-to-3D avatar (Emo3D) generation-has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator (CTEG), which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states. Extensive experiments on our EmoAva dataset demonstrate that CTEG achieves superior performance in generating diverse, natural, and consistent emotional expressions. Meanwhile, GiGA significantly outperforms state-of-the-arts in rendering realistic 3D avatars. Further in-depth analysis reveals how these methods enhance Emo3D generation. All resources (https://is.gd/ynDMOY) will be open.", "sections": [{"title": "1 INTRODUCTION", "content": "HUMANS are inherently emotional beings who naturally express feelings in a multitude of contexts. The quest to enable machines to understand and simulate human emotions represents a crucial facet of artificial intelligence (AI), pivotal to achieving true human-level AI [1]. Thus, the affective computing community [2] has made extensive efforts across various directions and modalities, including text-based sentiment recognition [3], empathetic dialogue systems [4], and emotional analyses of images and videos [5], [6], and even the use of biometric signals such as EEG to understand human emotions [7]. Among these, generating avatars that reflect human facial emotions stands out as a significant area of interest [8], [9]. In many scenarios, individuals prefer face-to-face interactions, as the human face plays a central role in emotional expression. Consequently, the generation of emotion-aware avatars has gained widespread application in real-world scenarios, including virtual customer support [10], online therapy [11], and other domains. Most current research has focused on the generation of emotional 2D avatars from text prompts, aiming to produce high-quality facial images and accurate expressions [12], [13], [14]. However, dynamic 3D avatars [8], [9], [15], [16], compared with static 2D avatars, can offer a more realistic and comprehensive simulation of human facial expressions during the speech, particularly in capturing intricate details, thus holding greater practical application value. A key challenge in generating 3D avatars involves accurately modeling subtle emotional changes in the human face during conversations. Yet the exploration of emotion-aware Text-to-3D avatar (termed Emo3D) generation could be relatively limited. Existing research primarily emphasizes the animation of 3D avatars, such as reconstructing a 3D avatar from a talking-head 2D video [17], without deliberately modeling facial emotions. A straightforward method to render emotional expressions can be augmenting 3D talking-head animations with certain emotion labels as additional supervisions, but such discrete emotional signaling will largely result in inaccuracies and lack nuanced transitions of dynamic expression [15], [18], [19]. Promisingly, a recent subsequent approach [16] using VQ-VAE [20] has achieved state-of-the-art (SoTA) results in modeling listeners' emotional expression from language text. Nevertheless, due to the discrete"}, {"title": "2 RELATED WORKS", "content": "General-purpose 3D generation employs various models, such as Generative Adversarial Networks (GANs) [25], Variational Autoencoders (VAEs) [26], autoregressive models [27], Flow-based models [28], and diffusion models [29], demonstrating remarkable performance in diverse applications. This paper adopts a generation approach that combines VAEs' stable training and structured latent space with autoregressive models' temporal dependency modeling. Another key aspect is learning 3D scene representations [22]. Scene representations are broadly classified"}, {"title": "2.1 3D Avatar Generation", "content": "into explicit (e.g., point clouds [28], meshes [26]), implicit (e.g., NeRF [30], 3DGS [22]), and hybrid forms (e.g., Voxel Grids [31], Tri-planes [32]). Explicit representations clearly visualize 3D structures and allow efficient dynamic editing [33], whereas implicit representations functionally model object surfaces for realistic reconstructions [33]. We adopt a hybrid approach for versatile, emotion-controllable 3D scene representation by using adaptable mesh structures in text-to-expression mapping and integrating them with 3DGS in avatar rendering to enhance visual fidelity during dynamic changes. Research in 3D Avatar Head Generation broadly falls into three streams based on input modality and task type. The first type is 3D generation from images or videos (3D reconstruction) [22]. Advancements in scene representation have evolved methods for reconstructing head avatars from images or videos. Initially, methods like points, volumes, and meshes often struggled to accurately model facial details [30], [34]. To address these issues, NeRF-based methods [30], [34] were introduced. Despite lifelike results, these methods have slow rendering speeds and limited generalization in expression transfer. Recently, 3DGS [24] has advanced 3D Avatar Head Generation, with works like GaussianAvatars [22] enhancing avatar dynamism by integrating 3D Gaussians with FLAME mesh triangles. Inspired by GaussianAvatars, our method GiGE similarly utilizes this approach but further incorporates global dynamic information into 3D Gaussians for more detailed micro-expression modeling. The second type [35], [36] generates static avatars from descriptive text, focusing on appearance features matching the descriptions. In contrast, we model the dynamic relationship between text inputs and avatars' emotional expressions, generating dynamic emotional talking-head avatars. The third type [15], [17], [37], [38], [39], [40] involves voice-driven talking head animations. Here, the goal is to synchronize a target avatar's lip movements with a reference avatar's using source audio, emphasizing animation accuracy. However, these methods do not specifically model the relationship between the emotional content and avatars."}, {"title": "2.2 Emotion-aware Avatar Generation", "content": "Enabling machines to understand and emulate human emotions is a crucial step towards realizing genuine AI. Affective computing [2], [41], particularly text-based within NLP, has been extensively studied and expanded into multimodal affective computing [5], [7], with emotion-aware digital human avatar modeling being a key area. Initially, emotional avatars used 2D videos, which, while foundational, lacked the lifelike quality [12], [13], [14] required for VR/AR, gaming, and film, and struggled to map speech or text emotions directly to avatar expressions, relying instead on limited emotional labels that resulted in unrealistic and uniform facial expressions. Mainstream emotional 3D avatars typically utilize the FLAME parametric model [21], which employs a linear shape space to decouple shape, expression, pose, and appearance. This enables Emo3D projects to manipulate facial expressions through nuanced representations, making high-quality, expressive representations crucial for successful Emo3D generation. Although 3D avatars offer greater ex-"}, {"title": "3 TASK DEFINITION AND SYSTEM OVERVIEW", "content": "Given a text $x = \\{X_1, X_2, ..., X_n\\}$ of n words as input, the Emo3D task is to output a sequence of dynamic 3D avatars with expressions. We divide Emo3D into two subprocesses (T3DEM and 3DAR), which are also illustrated in Figure 1.\n1) T3DEM: Given a text x as input, T3DEM is to output a sequence of expression vectors $\\psi = \\{E_0, E_1, ..., E_T\\}$ over T time steps. Such avatar expression vectors can be the parameters of 3D Morphable Face Model (3DMM) frameworks [16], [55], [56]. This paper adopts the widely used FLAME model [21] $F$, which defines the following parameters: $F = \\{\\beta, \\Delta v, \\rho, \\Theta, \\psi\\}$, as detailed in Table 1. Given these parameters, FLAME outputs a mesh consisting of 5023 vertices and 9976 faces (triangles).\n2) 3DAR: Given an individual face video $V = \\{V_t\\}_{t=1}^\\tau \\in  R^{H \\times W \\times 3 \\times \\tau}$ consisting of $\\tau$ frames as input, the objective of 3DAR is to reconstruct a 3D avatar $\\omega$ that can be animated by a sequence of expression vectors $\\psi$. This is a specialized variant of the 3D head avatar reconstruction task [22], [57]. While both tasks aim to reconstruct a 3D avatar from 2D video inputs, 3DAR specifically requires the use of the 3DMM framework.\nCorresponding to the above two processes, our whole Emo3D generation system is composed of two main models, as illustrated in Figure 1. Continuous Text-to-Expression Generator (CTEG, \u00a75) is responsible for the T3DEM task, generating a sequence of expression vectors, based on which Globally-informed Gaussian Avatars (GiGA, \u00a76) handles the 3DAR task by reconstructing and animating the avatar. The overall framework involves several key components that"}, {"title": "4 TEXT-TO-3D EXPRESSION MAPPING BENCHMARK", "content": "As emphasized earlier, since the first stage T3DEM plays a crucial role in the overall Emo3D task, we construct a benchmark tailored for T3DEM, including dataset construction and evaluation methods."}, {"title": "4.1 EmoAva Data", "content": "We expect each instance in EmoAva to include a piece of text to be spoken, and a corresponding sequence of 3D expression vectors, as illustrated in Figure 2. To construct this data, we first gather a large number of video clips from TV series and movies with dialogues. We consider two existing data sources for multimodal emotion analysis task, MELD [58] and MEMOR [59], both of which consist of television show segments. Besides these, we also gather numerous video clips from YouTube. A total of 21,390 such raw clips are collected, all in English languages. We extract audio and videos from these clips and employ WhisperX [60] to transcribe the audio, resulting in the corresponding text and timestamps. Afterwards, we cut the videos via the timestamps, creating dialogue video segments corresponding to the texts. Next, we extract the speakers' head-videos; however, scenes in films and TV shows can be very complex, i.e., often involving multiple individuals in the same frame, or switching rapidly between individuals. To combat these complex scenes, we use FaceNet [61] for initial screening, followed by manual refinement. After obtaining the head-videos, we adopt a 3D face tracking model EMOCA-v2 [62] to extract the 3D expression vectors from 2D videos.\nTo maintain the data quality, we perform manual checking. Specifically, we employ three annotators to remove low-quality instances, where the criteria are as follows: 1) The face should be clearly visible, without obstructions like masks or sunglasses. 2) The actor's facial expression changes should be continuous (i.e., no scene cuts). 3) The actor should complete their sentence without being abruptly cut off. 4) There should be only one person in the video from start to finish. 5) The text should match what the actor is saying. 6) The avatar expressions (mesh format driven by tracked vectors) align with those in the corresponding videos. We determine whether to drop data samples through independent annotation by the three annotators, followed by a majority vote on the results. After annotation, we calculate the Fleiss' kappa score [63], achieving a value of 0.86. This indicates minimal disagreement among the annotators, reflecting the high quality of the dataset's annotations."}, {"title": "4.1.1 Dataset Construction", "content": "Dataset Insights"}, {"title": "4.1.2 Dataset Insights", "content": "We randomly partition all instances in the training set into three subsets: training/validation/test sets, comprising 80%/10%/10% of the total, respectively. We provide a brief summary of the key characteristics of the EmoAva dataset in the following and in Figure 3.\n1) Large-scale and High-quality. To ensure data quality, we employ SoTA methods at every stage of the dataset preprocessing algorithm. Additionally, we manually check and remove the expressions that lack fluidity or do not consistently match the emotions expressed in the text. As a result, our dataset comprises 15,000 text-3D expression instances and a total of 782, 471 FLAME frames.\n2) Diverse Mapping. In a dataset of 15, 000 text-expression pairs, there are 2,270 instances with a 1-to-N relationship (where N ranges from 2 to 76), accounting for over 15%.\n3) Diverse Expressions. The facial data comes from a diverse range of sources and scenarios, i.e., including over"}, {"title": "4.2 Evaluation Methods", "content": "As highlighted in \u00a71, T3DEM aims to generate a sequence of expressions that are diverse, fluid, and consistent with the conveyed emotional content, which should be encouraged within our benchmark. Thus, we introduce several evaluation metrics across three key aspects."}, {"title": "4.2.1 Expression Diversity", "content": "1) Diversity [65] measures the diversity of expression generation without text as conditions. Inspired by [65], we randomly sample N/2 sequence pairs and calculate the average Euclidean distance between the expression vectors.\n2) Multimodality (MModality) [65] measures the diversity of text-conditioned generation. We generate two different expression sequences for the same text in the test set, and then calculate the average Euclidean distance between the expression sequences corresponding to the same text.\n3) Variation [16] measures the diversity of a sequence as it changes over time. We calculate the average variance across the sequences of expression vectors on the test set.\n4) Fine-grained Diversity (FgD) Despite many existing metrics for diversity, they might be still inadequate for measuring the diversity of facial expressions over time, yet in Emo3D, facial expressions can change rapidly between consecutive time frames. Thus we propose FgD, where we use the average Euclidean distance between adjacent frames in a sequence to reflect this character:\n$FgD = \\frac{1}{(T-1)N}\\sum_{i=1}^{N}\\sum_{j=0}^{T-1}||E_{i,j+1} - E_{i,j}||.$ \n5) Diversity on Test (DoT) measures the diversity of the expression sequences generated from the test set texts from a macro perspective. The score is obtained by calculating the average Euclidean distance between each pair of generated expression sequences:\n$DoT = \\frac{2}{N(N-1)}\\sum_{1<i<j<N}||E_i - E_j||.$"}, {"title": "4.2.2 Expression Fluidity", "content": "6) Continuous perplexity (Cppl) evaluates how naturally an expression sequence evolves over time, reflecting the smoothness of the expression sequence and the uncertainty in a model's predictions. Unlike perplexity [66], the Cppl proposed in this paper can compute the perplexity of sequences in continuous space. Specifically, given the i-th expression sequence $\\xi^i$, we define the following entropy inspired by [66].\n$H(\\xi) \\approx -\\frac{1}{T}\\sum_{j=1}^{T}log_2 P_{\\xi}(\\psi_j|\\psi_{<j}, x),$ \n$P_{\\xi}$ here is a continuous conditional distribution where modeled by a generation model. Multivariate normal distribution is adopted in this paper and it is calculated by:\n$P_{\\xi}(\\psi_j|\\psi_{<j}, x) \\approx \\Phi(x + \\delta; \\mu_{\\xi}, \\sigma^2I) - \\Phi(x - \\delta; \\mu_{\\xi}, \\sigma^2I),$ \nwhere $\\Phi(\\cdot; \\mu_{\\xi}, \\sigma^2I)$ denotes the cumulative distribution function (CDF) of the multivariate normal distribution with mean $\\mu$ and covariance matrix $\\sigma^2I$. Note that $\\delta$ and $\\sigma$ are empirical values, which are set to 0.8 and 0.2 here. Given N expression sequences, Cppl is calculated by:\n$Cppl = 2^{\\frac{1}{N}\\sum_{i=1}^{N}H_{\\xi}(\\xi)}.$"}, {"title": "4.2.3 Emotion-Content Consistency", "content": "7) Consistency assesses how well the generated expressions align with the corresponding dialogues in terms of consistency and realism. Specifically, this metric evaluates whether the expressions accurately reflect the emotions and reactions one might expect when a person says a given sentence. Due to the absence of precise automatic tools to evaluate this alignment, we rely on human evaluation following [65]."}, {"title": "5 CONTINUOUS TEXT-TO-EXPRESSION GENERATOR", "content": "The overview framework of the CTEG model is shown in Figure 4. CTEG primarily consists of the Expression-wise Attention (EwA) block at the encode side, and the Conditional Variational Autoregressive Decoder (CVAD) block at the decode side. From the perspective of architectural design, the EwA module serves as a feature enhancement module, while the CVAD functions as a hybrid of a CVAE [23] and a transformer decoder [67]. Technically, we adopt such an architecture for the following advantages. 1) CVAE is beneficial for maintaining a smooth spatial distribution due to its nature of modeling in continuous space [23], [68], which may help to model expression fluidity. 2) Transformer decoder excels at modeling the long-range dependencies between sequences [67], which may help to model the emotion-content consistency. 3) Due to the richness of the facial expression sequence, even within a single time step, facial expressions have countless variations. The Variational Autoregressive Decoder (VAD) model may help to model such a diverse time-varying sequence according to [69]. Given a text x as input, CTEG generates a sequence of expression vectors $\\psi$ autoregressively."}, {"title": "5.1 Expression-wise Attention Module", "content": "In the input part, we introduce EwA to establish connections between facial units and enhance the richness of the input expression in the feature space. This guides the subsequent CVAD module to capture different and rich patterns and structures, thereby improving the overall diversity of the model's generation.\nThe expression vector $E$ is constructed by concatenating two components: the jaw part $E^j$ and the above-jaw part $E^f$. Intuitively, these two parts are not independent of each other because human facial units function as a whole. For"}, {"title": "5.2 Conditional Variational Autoregressive Decoder", "content": "Given a text x as input, an expression sequence $\\psi$ as output, CVAE is to maximize the conditional log-likelihood $log p(\\psi|x)$. However, we model conditional probability distribution on each time step. Formally, the log-likelihood in our method is $log \\prod_{t=1}^{T} P(\\psi_t | \\psi_{<t}, x)$, rather than $log p(\\psi_{0,...,T} | x)$. To enhance the emotion-content consistency, we model the historical states of the latent variables. The generation model can be formulated as:\n$p(\\psi | z, x) = \\prod_{t=1}^{T} P(\\psi_t | \\psi_{<t}, z_t, x) = \\prod_{t=1}^{T}P(\\psi_t | \\psi_{<t}, f_c (Z_{<t}), x),$ \nwhere $f_c$ is the Latent Temporal Attention (LTA) module, implemented by the masked multi-head attention [67].\nIntuitively, we assume the prior distribution $P_{\\theta}$ and conditional distribution $Q_{\\theta}$ to be multivariate Gaussian distributions:\n$Q(Z_t | \\psi_{<t}, x) = N(\\mu_{\\epsilon}(\\psi_{<t}, x), \\sigma_{\\epsilon}(\\psi_{<t}, x)),\\ P_{\\theta}(Z_t | \\psi_{<t}, x) = N(\\mu_p(\\psi_{<t}, x), \\sigma_p(\\psi_{<t}, x)).$\nThe two Gaussian distributions are parameterized by two neural networks respectively:\n$[\\mu, \\sigma_r] = [h_{\\mu}(o), h_{\\sigma}(o)], [\\mu_{\\rho}, \\sigma_p] = [h_{\\mu}(o), h_{\\sigma}(o)], o = A_{mask}[A(\\psi_{<t}, x),$ \nwhere $h$ denotes a linear layer, $A_{mask}$ and $A$ denote masked attention module and cross attention module, respectively. As sampling $z_t$ from two distributions is non-differentiable, we employ the reparameterization trick [68]:\n$z_t = \\mu_{\\epsilon} + \\sigma_{\\epsilon}\\odot \\epsilon, \\epsilon \\sim N(0, 1),$ \n$z_t$ is drawn from $Q_{\\xi}(z_t | \\psi_{<t}, x)$ in the training stage, while drawn from $P_{\\theta}(z_t | \\psi_{<t}, x)$ in the inference stage. After we obtain the sampled $z_t$, we learn the second conditional generation distribution $P_{\\theta}(\\psi_t | \\psi_{<t}, z_t, x)$. Similarly, we assume the distribution a multivariate Gaussian distribution, the mean $\\mu_g$ can be parameterized by the following generation network:\n$\\mu = FFN(Concat(A(o_1), A(o_2), ..., A(o_l))), A(o_j) = A((\\psi_{<t} + z_{<t})W_{\\theta}, xW_xW_V),$ \nwhere l is the number of cross attention heads. FFN is the position-wise Feed-Forward Networks (FFN) [67].\nNote that the CVAD module can be stacked multiple layers deep, where the input of the first layer comes from the EwA module, and the input of each subsequent layer comes from $\\mu_g$ obtained by the previous layer. Specifically, the input at layer m is sampled from $N(\\mu_{m-1}, \\sigma)$. For simplicity, we parameterize only $\\mu_g$ and set the $\\sigma$ of the generative distribution to a matrix where all entries are equal to 1. Finally, we sample the predicted expression codes in time step t using the reparameterization trick again:\n$\\psi_t = \\mu_{m_g} + \\epsilon, \\epsilon \\sim N(0, I).$\nOur loss function of CVAD is as follows:\n$L_{CVAD} = \\sum_t L_{rec}(\\psi_t, \\hat{\\psi}_t) + \\sum_t L_{KL}(t),$ \nwhere $L_{rec}$ is the mean squared error (MSE) loss, and the Kullback-Leibler (KL) divergence term is as follows:\n$L_{KL}(t) = KL(Q_{\\theta}(Z_t | \\psi_{<t}, x) || P_{\\theta}(Z_t | \\psi_{<t}, x)).$"}, {"title": "5.3 Target Guided Loss", "content": "Despite the excellent performance, CVAD is known to suffer from a notorious issue referred to as model collapse [70], [71]. When this happens, the KL divergence term in the loss function becomes very close to zero, and the latent variable may be ignored by the decoder.\nSome works have proposed various methods to mitigate this issue [69], [70], [71]. Among them, the most common"}, {"title": "6 GLOBALLY-INFORMED GAUSSIAN AVATARS", "content": "Once expression vectors are obtained from the CTEG model, they can be used to animate a 3D avatar. We can directly"}, {"title": "6.1 Offset-aware Module", "content": "To model the corresponding deformations of 3D Gaussians during facial expression changes, we propose the Offset-aware Module. The key design of this model is to enable each 3D Gaussian to learn the locations of the associated pulling triangles and the potential influence of deformations on them. Figure 6 illustrates the underlying mechanism of this model.\n3D Gaussians have several attribute: local position p, local scaling value s, opacity a, local rotation value r and color H [24]. Specifically, these Gaussians are defined by a"}, {"title": "6.2 Two-stage Optimization Process", "content": "As shown in Figure 6, we adopt a two-stage optimization framework, and the offset-aware module is only optimized in the second stage. The offset-aware module is designed solely for modeling specific deformations and should be decoupled from the general 3D reconstruction training process, allowing the offset-aware module to function as a plug-and-play module. Based on this insight, our first-stage training is almost consistent with GaussianAvatars [22], which binds several 3D Gaussians to each triangle of the FLAME [21] mesh and converts the splats from local to global space at rendering time by:\n$r' = Rr, p' = kRp+T, s' = ks.$\nCompared with their approach, we remove the $L_{position}$ loss, as it limits the deformation learning in the second stage, a point we confirm in later experiments. The final loss function for both two stages is defined as follows:\n$L_{total} = (1 - \\lambda_1)L_1 + \\lambda_1L_{D-SSIM} + \\lambda_2L_{scaling},$"}, {"title": "7 EXPERIMENTAL SETTINGS", "content": "In what follows, we conduct detailed experiments to validate our systems. Here we first explain all the settings."}, {"title": "7.1 CTEG Model", "content": "We utilize the EmoAva dataset to validate the CTEG. The following details our baselines and implementations."}, {"title": "7.1.1 Baselines", "content": "1) lm-listener [16]. This is a SoTA method that uses VQ-VAE [20] to model the relation between text and listener expressions. We implement the model with their released code, with most parameters kept unchanged. To ensure a fair comparison and obtain diverse outputs, we use top-p sampling (top-p=0.8) instead of greedy search. As there is only one method most relevant to our work, we set up the following two additional baselines.\n2) Shuffle. For each expression sequence in the test set, we perform a random shuffle along the temporal dimension. This baseline allows us to evaluate the model's ability to capture the fluency of expression sequences.\n3) Random. Following [16], we also randomly select expression sequences from the training set to assess the model's ability to model the emotion-content consistency.\nBesides the above settings, we also list the Diversity on Test (DoT), Fine-grained Diversity (FgD) and Variation metrics of the Ground Truth (GT) as references."}, {"title": "7.1.2 Implementation Details", "content": "The parameter settings of CTEG are detailed in \u00a75.4. As mentioned in \u00a74.2.3, there are currently no suitable quantitative metrics for emotion-content consistency evaluation. Instead, we adopt perceptual experiments following [81]."}, {"title": "7.2 GIGA Model", "content": "To validate the effectiveness of our reconstruction method, we use 9 subjects from the NeRSemble [22], [82] dataset. There are 11 video sequences for each subject and 16 views for each recording. We utilize 9 expression sequences and 15 camera views to train our method and the baselines. GaussianAvatars [22] reduces the image resolution to 802\u00d7550. Using these processed images, we further extract the head mask using an open-source algorithm [83] and focus exclusively on the reconstruction quality of the head region. We choose two SOTA FLAME-based head avatar reconstruction methods to validate our method. GaussianAvatars [22] rigs a series of 3D Gaussians to the triangular surfaces of a FLAME mesh, optimizing the 3DGS as the mesh deforms. PointAvatar [57] employs a point cloud along with a deformation network to modify the point cloud into various poses by utilizing FLAME expression and pose parameters. We choose the self-reenactment settings (animating an avatar with novel poses and expressions and rendering it from all 16 camera angles) for evaluations. This setting is selected due to its ability to facilitate both quantitative and qualitative experimental analysis, while also being highly pertinent to the research objectives of this study. In the quantitative experiments, we select the most commonly used evaluation metrics: SSIM, PSNR, LPIPS, and L1, to assess the quality of reconstruction."}, {"title": "7.2.1 Dataset, Baselines, and Evaluations", "content": "For each subject, we train for 420,000 iterations and select the optimal model based on the validation set in Stage 1, followed by an additional 12,000 iterations of training in Stage 2. We set the hidden layer dimension of all MLPs at 64. The dimensions of the attention matrix match the number of FLAME mesh faces modified by [22], which is 10,144. For optimization, we adopt the Adam [73] optimizer, with learning rate 5e-4, eps 1e-15, and weight decay 1e-4. Other parameters in the stage 1 are the same as in GaussianAvatars."}, {"title": "7.2.2 Implementation Details", "content": "In this section, we present the experimental results, via which we come to conclusions and discussions in terms of the CTEG model and GiGA model, respectively:"}, {"title": "8 RESULTS AND ANALYSES", "content": "\u25ba Q1: How does CTEG perform in addressing the expression diversity challenge in the T3DEM task? As shown in Table 2, CTEG outperforms the SoTA approach across all diversity metrics by a large margin. Specifically, CTEG achieves a 46.1% improvement in the MModality metric and a 16.7% improvement in the Diversity metric. Im-listener shows a gap of 1.25 and 0.22 compared with the GT in terms of the DoT and FgD metrics, respectively, while CTEG reduces these gaps to 0.72 and 0.04. Compared with MSL=64 and MSL=256, we find that as the MSL decreases, several diversity metrics slightly decline, while Variation drops significantly, approaching the level of GT. We argue that the Variation of GT should only be considered as a reference value. In real-world applications, the MSL can be adjusted in terms of the desired FPS value.\nWe also visualize the diversity of expressions generated by CTEG in Figure 7. We randomly sample four sequences of expressions given a text. These examples demonstrate that the generated expressions exhibit a rich diversity. For instance, in response to the sentence \u201cFortunately, I have a lot of experience in that area\u201d the first expression sequence conveys a sense of pride, the second appears quite calm, the third expresses considerable happiness, and the fourth reflects a moderate level of joy. These four expressions give the impression of being articulated by four distinctly different personalities.\n\u25ba Q2: Can CTEG address the emotion-content consistency challenge in the T3DEM task? Figure 9 illustrates a quan-"}, {"title": "8.1 Evaluations on CTEG", "content": "\u25ba Q3: How well does CTEG address the expression fluidity challenge in the T3DEM task? To validate the effectiveness of CTEG in modeling expression fluidity, we compute the Cppl values for both randomly shuffled and normal expression sequences using CTEG. The experimental results are shown in Table 2. We observe that the Cppl metric for the shuffle setting is several orders of magnitude higher than that of the normal sequences, indicating CTEG's high sensitivity to the expression sequence order. The lower Cppl value confirms CTEG's effectiveness in modeling expression smoothness. Note that we do not include lm-listener in this experiment, as its discrete modeling approach is not compatible with the Cppl metric. We believe that unifying the perplexity metric in both continuous and discrete spaces is an open question for future research.\nWhat improvements do the individual sub-modules of CTEG contribute to the overall system? We conduct a quantitative experiment on three key components (i.e., EwA module, LTA module and $L_g$ loss function) in CTEG model. As shown in Table 3, removing the EwA module results in a significant drop in the four diversity metrics (DoT, FgD, Diversity, and MModality). Specifically, the Diversity metric decreases by 25.8%, and MModality drops by 29.9%. The gap in the DoT metric increases from 0.72 to 2.27, and the"}, {"title": "8.2 Evaluations on GIGA", "content": "Q1: Does the Globally-informed mechanism within the GiGA model actually work? The quantitative results in Table 4 indicate that our method surpasses the baselines in SSIM, PSNR, and L1 metrics, while also delivering strong performance in the LPIPS metric. Although our method's perceptual error is slightly higher than that of GaussianAvatars [22], we find this does not indicate inferior rendering quality in qualitative analysis. As shown in Figure 14, our approach demonstrates a more realistic rendering of subtle facial expressions. In qualitative analysis, it is evident that"}, {"title": "9 CONCLUSION", "content": "In this paper, we introduce a novel benchmark for emotion-aware Text-to-3D avatar (Emo3D) generation. We first present a comprehensive dataset (EmoAva) comprising"}]}