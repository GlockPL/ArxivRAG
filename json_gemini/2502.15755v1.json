{"title": "Physics-consistent machine learning: output projection onto physical manifolds", "authors": ["Matilde Valente", "Tiago C. Dias", "Vasco Guerra", "Rodrigo Ventura"], "abstract": "Data-driven machine learning models often require extensive datasets, which can be costly or inaccessible, and their predictions may fail to comply with established physical laws. Current approaches for incorporating physical priors mitigate these issues by penalizing deviations from known physical laws, as in physics-informed neural networks, or by designing architectures that automatically satisfy specific invariants. However, penalization approaches do not guarantee compliance with physical constraints for unseen inputs, and invariant-based methods lack flexibility and generality. We propose a novel physics-consistent machine learning method that directly enforces compliance with physical principles by projecting model outputs onto the manifold defined by these laws. This procedure ensures that predictions inherently adhere to the chosen physical constraints, improving reliability and interpretability. Our method is demonstrated on two systems: a spring-mass system and a low-temperature reactive plasma. Compared to purely data-driven models, our approach significantly reduces errors in physical law compliance, enhances predictive accuracy of physical quantities, and outperforms alternatives when working with simpler models or limited datasets. The proposed projection-based technique is versatile and can function independently or in conjunction with existing physics-informed neural networks, offering a powerful, general, and scalable solution for developing fast and reliable surrogate models of complex physical systems, particularly in resource-constrained scenarios.", "sections": [{"title": "1 Introduction", "content": "The numerical simulation of physical models is prevalent in science and engineering. These models mathematically represent a physical system, typically by partial differential equations (PDEs) or a set of coupled ordinary differential equations (ODEs). Their development aims at capturing the essential physics of the system, predict its behavior, clarify the principles underlying key observations, and guide experiment design, process optimization and scientific discovery. Computer simulations explicitly solve the differential equations that result from model development. However, they often require solving complex systems of equations and become computationally expensive. Data-driven models have emerged as a promising complement to direct simulations, due to their ability to handle intricate nonlinear input-output relationships.\nMachine learning methods are increasingly being used to construct surrogate models for complex physical systems, in disciplines as varied as materials science [1, 2, 3], fluid dynamics [4, 5, 6] and low-temperature plasmas [7, 8, 9]. The reduced computational cost of surrogate models enables real-time control, as quick predictions can be made without execution of the original model. However, these surrogates often require large datasets, which may not be available due to the high cost of data acquisition in practical applications, while their predictive power degrades in the presence of noisy, sparse or dynamic data [10]. Moreover, being solely dependent on the data provided during the model training, the predictions may fail to comply with known physical laws.\nThe introduction of physics-informed neural networks (PINNs) by Raissi et al. [11] blends the causality and extrapolation capabilities of physics-based models with the speed, flexibility, and high-dimensional capabilities of Neural Networks (NNs). By incorporating physical priors described by differential equations into the NN's loss function, PINNs proved to be effective in addressing a variety of practical engineering and scientific challenges [12, 13, 14, 15, 16]. Still, as the physical constraints are introduced directly into the NN during training, this approach does not guarantee that the outputs for unseen inputs will satisfy the physical laws after the training process [17, 18, 19, 20]. If some properties of the solutions are known, such as e.g. energy conservation, it is possible to encode them in the network architecture [21, 22, 18, 23, 24]. However, the need to design specialized network architectures for each system and specific set of constraints makes it difficult to attain a general formulation enforcing adherence to the physical laws. Therefore, although PINNs are currently used with success to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs [19], approaches that introduce general but robust physical laws into machine learning (ML) models remain limited.\nIn this paper, we present a novel approach for physics-consistent machine learning, motivated by the current limitations in both purely data-driven methods and physics-informed models. Specifically, we focus on studying the effect of projecting the output of an artificial NN onto the manifold defined by a set of chosen physical constraints of the system. In this way, our method leverages fundamental physical principles, such as energy or charge conservation, to correct a posteriori the predictions of an ML model. Consequently, this method ensures physically consistent predictions and improved accuracy, provided the outputs are reasonably close to the target value."}, {"title": "2 Results and discussion", "content": "The projection method constitutes a highly flexible tool to develop fast and reliable surrogate models of complex physical systems that comply with an arbitrary set of physical laws. Moreover, it enables the use of simpler ML models and/or smaller datasets, while maintaining the same level of predictive accuracy.\nWe apply the projection method and study its performance in two different physical systems, used as case studies. In the first one, we build an artificial NN model to predict the time evolution of a spring-mass system given an arbitrary initial condition. The predictions are then corrected by projecting the output on the manifold defined by energy conservation. The second system is a highly complex and non-linear low-pressure oxygen reactive plasma, created by a DC glow discharge. This system was recently modeled and simulated in [25] and provides an ideal testbed for the proposed approach. In this case, we develop an artificial NN surrogate model according to the reaction mechanism proposed in [25], and project the output onto the manifold defined by charge conservation, plasma quasi-neutrality and constant operating pressure."}, {"title": "2.1 Framework", "content": "A schematic overview of our approach is illustrated in Fig. 1. Both systems under study are described by a set of coupled ODEs, and their numerical solution is used as the ground truth. We then compare the predictions of four surrogate models: an artificial NN data-driven model, a loss-based PINN (where the physics priors are included in the loss function), and the projection method applied to the predictions of each of the two former models. We compare the quality of the prediction of the model outputs before and after applying the projection, analyze the robustness across varying model complexity and training, and discuss the physical insight into the underlying physics. Fig. 1c also makes clear the intuition behind the projection method."}, {"title": "2.2 Case study 1: spring-mass system", "content": "This example highlights the ability of the projection method to handle sequential predictions and how it corrects the trajectories as they gradually deviate from the target values."}, {"title": "2.2.1 System description", "content": "The system consists of two masses, m\u2081 and m2, connected in series by two springs with spring constants k\u2081 and k2, and natural lengths L\u2081 and L2, respectively. The first spring is connected to a fixed wall at one end and to mass m\u2081 at the other, while the second spring connects the two masses. The masses are restricted to moving along the x-axis and there is no friction. The positions of m\u2081 and m2 along the x-axis are denoted by x\u2081 and x2, and their velocities by v\u2081 and v2, respectively. The forces exerted by the springs on the masses are determined by the displacements from their equilibrium positions, following Hooke's law."}, {"title": "2.2.2 Data generation", "content": "The artificial NN data-driven model takes as input a state vector characterizing the state of the system at the current instant t (i.e., defining the positions and velocities of both masses at time t), and predicts the state vector at time t + \\Delta t, for fixed \\Delta t. This procedure enables the recursive determination of the complete trajectory from a given initial condition, with time-resolution \\Delta t = 50 ms, as a sequence of transitions between states. We used an energy threshold E_{max} = 5 J to create a set of allowed states for the system, corresponding to the range of positions and velocities the masses can have with E < E_{max}. Eq. (1) was solved using the classical fourth-order Runge-Kutta method (RK4), with arbitrary initial conditions (2) ensuring that E < E_{max}.\nExcept otherwise noted, the dataset consists of N = 100000 arbitrarily generated input state vectors within the energy threshold and the corresponding next states, determined by performing a single Runge-Kutta computation up to time \\Delta t (not to be confused with the Runge-Kutta time-step). To reduce the impact of the difference in feature magnitude on the model and make the training process more stable, we applied the min-max normalization. Consequently, all features were scaled to the range [-1,1]."}, {"title": "2.2.3 Trajectory prediction", "content": "The trajectory of the system is defined by the prediction of its state along 165 sequential time steps, i.e., 8.25 seconds. The general physical law considered in the loss-based PINN and in the projection method is energy conservation, which translates into the residual R_1 = E(t) \u2013 E(0). The results of the four models on the four state-variables and energy conservation are presented in Fig. 2a-b, respectively, for the arbitrary initial condition x_{0,1} = -0.16m, x_{0,2} = 0.09m, v_{0,1} = -2.18m/s and v_{0,2} = -0.16 m/s.\nThe projection method applied to the outputs of the NN reduces the root mean squared error (RMSE) between the normalized predictions and normalized target values on the four state variables, by 49.5%, 71.7%, 21.7%, 42.6% for x_1, v_1, x_2, and v_2, respectively, when compared with the purely data-driven NN model."}, {"title": "2.3 Case study 2: low-temperature reactive plasma", "content": "This example demonstrates the ability of the projection method to handle complex, high-dimensional and strongly nonlinear systems. Additionally, we draw on the underlying knowledge of the plasma system to interpret in physical terms the impact of the projection operation on the model outputs."}, {"title": "2.3.1 System description", "content": "The system under study is an oxygen (O2) low-temperature reactive plasma (LTP) created by a continuous DC glow discharge operating at pressures in the range P\\in [0.1, 10] Torr, discharge current I \\in [5,50] mA, in a long cylindrical tube of radius R\\in [4, 20] mm. As is typical for low-temperature molecular plasmas, the system exhibits a variety of coupled energy pathways and elementary processes, such as electron impact excitation and de-excitation, gas phase and heterogeneous reactions, dissociation, molecule formation, ionization and charge transfer. A detailed set of reactions and corresponding rate coefficients, validated against benchmark experiments, the so-called reaction mechanism, was recently developed by T.C. Dias et al. [25], where the experimental data for validation are also given. Herein we consider the kinetic scheme from [25] without vibrational excitation, which accounts for 12 species electrons, ground-state molecules O2(X) and atoms O(3P), electronically excited states O2(a ^{1}\\Delta_g, b ^{1}\\Sigma, Hz) and O(^{1}D), ground-state ozone O3 and vibrationally excited ozone O_3^*, negative ions O^\u2212, and positive \\text{O}^+_2 and \\text{O}^+ ions \u2013 and more than 85 elementary processes. With the exception of the electron density, n_e, the steady-state concentrations of each species n_s are obtained from the solution to the coupled"}, {"title": "2.3.2 Data generation", "content": "The models include 3 input features (P, I and R) and the 17 outputs just described. We generated the datasets with the Lisbon KInetics (LoKI) simulation tool [26]. Except otherwise noted, the dataset comprises N = 1000 uniformly distributed values across the three input features within their specified boundaries. Similarly to the previous case, we applied the min-max normalization to the features, scaling them to the range [-1,1]. Moreover, we applied a log-transformation to the features demonstrating skewness in its distribution. Further details on data generation and preprocessing are given in section 3.1."}, {"title": "2.3.3 Prediction of the steady-state plasma properties", "content": "The general physical laws constraining the system include: the ideal gas law, relating the gas pressure (input) with the species densities and the gas temperature (outputs); the imposed discharge current (input), expressing electric charge conservation and relating its input value with the tube radius (input), and with the drift velocity and electron density (outputs); and the quasi-neutrality law, relating the electron density (output) with the positive and negative ion densities (outputs). These laws can be represented by the residuals in equations (4-6), respectively,\n\\begin{aligned}\nR_1 &= P - \\Sigma[X_i]k_BT_g \\\\\nR_2 &= I \u2212 en_eva\\pi R^2 \\\\\nR_3 &= n_e - \\Sigma[X_i]^+ +\\Sigma[X_i]^-\n\\end{aligned}"}, {"title": "2.4 Robustness of the projection method", "content": "With sufficient training time and a sufficiently complex architecture, a neural network can produce predictions that closely approximate the target values, reducing (or even eliminating) the need for post-training corrections. Conversely, a poorly trained model may yield predictions that deviate significantly from the manifold defined by the physical laws, making the projection operation ineffective or leading to ambiguities associated with multiple possible solutions. In this section, we analyze the robustness of the projection method with respect to model complexity, quantified by the number of NN parameters (i.e., weights and biases) and the size of the training dataset. The results focus on the low-temperature reactive plasma system described in section 2.3."}, {"title": "2.4.1 Ablation study", "content": "We start by comparing the errors before and after applying the projection operation to the NN predictions, considering models with different complexities. In order to guarantee a comparable analysis between the models, we considered 18 different architectures, each with 2 hidden layers and a number of neurons ranging from 1 to 1000 in the hidden layers. Both hidden layers have the same number of neurons. For each architecture, 1 NN model was trained with a dataset consisting of N = 1000 data points."}, {"title": "2.4.2 Small samples", "content": "We now compare the errors before and after applying the projection operation to the NN predictions, considering training datasets with different sizes. In order to guarantee a comparable analysis between the models, an architecture with 2 hidden layers and 50 neurons in each layer was used. To mitigate randomness associated with a specific dataset sample, 20 random samples are drawn for each dataset size from a larger dataset. The NN model is then trained on each sample, and the trained models are evaluated on a test set to obtain errors for each of the 17 outputs."}, {"title": "2.5 Summary and outlook", "content": "We introduced a flexible and effective mechanism to improve the quality of the predictions of deep-learning surrogate physical models. The approach is based on projecting the model output onto the manifold defined by an arbitrary number of general physical laws, correcting the predictions after the training process. We tested the method in two case studies, a spring-mass system and a low-temperature reactive plasma. We consistently confirmed the hypothesis that our technique produces physically consistent and scientifically sound results, improves generalizability, and increases efficiency by reducing the computational and data resources necessary to build the surrogate models. Moreover, the straightforward addition or removal of physical constraints, defining different projection manifolds for the post-training predictions, brings additional physical insight and clarity of interpretation to the physical results.\nOur physics-consistent machine learning projection method can be used independently or as a complement to physics-informed neural networks. For the specific systems under study, PINN's approach of introducing physical information directly into the loss function during training was ineffective in reducing predictive errors. This result indicates that inclusion in the loss function of physically strong but very general laws, such as energy and charge conservation, can bring additional optimization challenges and make post-training projections more appropriate. The difficulty is likely associated with an expansion (rather than a restriction) of accessible output parameter space related to these universal laws. This may occur because, when the training data already adheres to these laws, incorporating them as regularization terms adds no new meaningful information to the system. Further research is needed to clarify this issue. Notwithstanding, synergy between PINNs and the projection method is expected. While remaining largely unexplored, it holds the promise of further leveraging artificial intelligence to address scientific and practical engineering problems, especially in contexts of limited resources."}, {"title": "3 Methods", "content": "For both the spring-mass and the low-temperature reactive plasma (LTP) systems, we used min-max scaling normalizing features to the [-1,1] interval, thereby ensuring equal contribution from all features to the loss function and mitigating magnitude disparities. In addition, the LTP system exhibited notable skewness, particularly evident in low-pressure regimes. We quantified this skewness for each output variable and applied logarithmic transformations when the skewness exceeded a predetermined threshold. This transformation was necessary for several output parameters, specifically O(1D), \\text{O}^+, E/N, and T_e."}, {"title": "3.2 Model training and optimization", "content": "In both case studies, fully connected feed-forward NNs were implemented in PyTorch. Moreover, a standard train-test-validation split of the dataset was performed to evaluate model performance, with 80% allocated for training, 10% for validation, and 10% for testing. For the relatively straightforward spring-mass system, we set a fixed training duration of 60 epochs. In turn, an early stopping criterion is used in the modeling of the highly complex LTP system. The implemented early stopping criterion follows Prechelt's PQa early stopping method [27], an approach that aims to balance the trade-off between training time and generalization performance. The implementation handles special cases such as insufficient epochs and zero training progress scenarios, with the threshold parameter a controlling stopping sensitivity. Xavier's initial value [28] is used as the initial NN parameters, which are updated using the gradient-based algorithm Adam [29]."}, {"title": "3.2.1 Spring-mass system", "content": "In the spring-mass system from section 2.2, the NN takes the system's current state variables (x_1, v_1, x_2, v_2) and predicts the state variables in the following state. We use a standard mean squared error (MSE) loss function for the NN training, while for the training of the loss-based PINN an additional loss function is defined as the residuals of the energy conservation law [cf. Fig.1b]. In this way, the loss terms are written as\n\\begin{aligned}\n\\mathcal{L}_{physics} &= MSE(E_{out}, E_{in}) \\\\\n\\mathcal{L}_{data} &= MSE(y_{out}, \\hat{y}) \\\\\n\\mathcal{L}_{total} &= (1 - \\lambda_{physics}) \\cdot \\mathcal{L}_{data} + \\lambda_{physics} \\cdot \\mathcal{L}_{physics}\n\\end{aligned}"}, {"title": "3.2.2 Low-temperature reactive plasma", "content": "In the low-temperature reactive plasma system studied in section 2.3, we use a standard mean squared error (MSE) loss function for the NN training, as before. Regarding the training of the loss-based PINN, we define a loss function as the residuals of each of the three physical laws (4-6). The corresponding terms are written as\n\\begin{aligned}\n\\mathcal{L}_P &= MSE(P_{out}, P_{in}) \\\\\n\\mathcal{L}_I &= MSE(I_{out}, I_{in}) \\\\\n\\mathcal{L}_{ne} &= MSE(n_{e,out}, N_{i,out})\n\\end{aligned}"}, {"title": "3.3 The projection method", "content": "Our projection method can be formalized as follows. Consider a ML parametric model y = f(x; \u0398), where x is the input, y the output, and is the model parameter vector. Given a set \\mathcal{D} = \\{(x_k, y_k)\\} of datapoints for training and assuming a loss function \\mathcal{L}(y_1, y_2), training the model amounts to solve the optimization problem\n\\text{minimize} \\sum_{(x,y) \\in \\mathcal{D}} \\mathcal{L}(y, f(x; \\Theta)).\nAdditionally, the requirement that a set of physical laws relating both x and y must be satisfied is considered. This requirement can be expressed as a vector valued constraint function g(x, y) that is zero if, and only if, those physical laws are satisfied, i.e., g(x, y) = 0. Even though the input dataset \\mathcal{D} might satisfy this constraint, there is not a priori any guarantee that the ML model f will output values that satisfy these laws.\nOne common approach to add physical information to a ML model is to include into the loss function \\mathcal{L} a regularization term that penalizes violations of the physical constraints. However, once the model is trained, there is still no guarantee that the outputs for unseen inputs do satisfy those constraints. Moreover, if the physical laws are too general, they may fail to guide the NN and improve its predictions, as shown in this work. Here we follow an alternative procedure, that explores the idea of projecting the output y of the model onto the manifold defined by the constraint g(x, y) = 0. The projection operation is formulated as a constraint optimization problem,\n\\begin{aligned}\n\\text{minimize} \\quad & ||p - f(x; \\Theta)||_\\text{W} \\\\\n\\text{s.t.} \\quad & g(x,p) = 0\n\\end{aligned}\nwhere W is a symmetric positive definite weighting matrix, i.e., ||v||_\\text{W} = v^\\text{T}Wv, being essentially the metric of the output space.\nWe implemented the projection operation using CasADI opti stack [30]. Specifically, the nonlinear programming (NLP) solver IPOPT was used to solve the optimization problem. In addition, all along the paper we performed the projection with the identity matrix, W = \\bold{1}. Strategies to optimizing the projection with a different weighting matrix can be explored and are left for future work."}, {"title": "4 Data availability", "content": "The datasets generated in this study are available on Github [https://github.com/ matildevalente/physics_consistent_machine_learning]."}, {"title": "5 Code availability", "content": "The code that supports the findings in this study is available on Github [https:// github.com/matildevalente/physics_consistent_machine_learning]."}]}