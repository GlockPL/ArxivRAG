{"title": "Learning to Steer Markovian Agents\nunder Model Uncertainty", "authors": ["Jiawei Huang", "Zebang Shen", "Vinzenz Thoma", "Heinrich H. Nax", "Niao He"], "abstract": "Designing incentives for an adapting population is a ubiquitous problem in a\nwide array of economic applications and beyond. In this work, we study how\nto design additional rewards to steer multi-agent systems towards desired poli-\ncies without prior knowledge of the agents' underlying learning dynamics. We\nintroduce a model-based non-episodic Reinforcement Learning (RL) formulation\nfor our steering problem. Importantly, we focus on learning a history-dependent\nsteering strategy to handle the inherent model uncertainty about the agents' learn-\ning dynamics. We introduce a novel objective function to encode the desiderata\nof achieving a good steering outcome with reasonable cost. Theoretically, we\nidentify conditions for the existence of steering strategies to guide agents to the\ndesired policies. Complementing our theoretical contributions, we provide empiri-\ncal algorithms to approximately solve our objective, which effectively tackles the\nchallenge in learning history-dependent strategies. We demonstrate the efficacy of\nour algorithms through empirical evaluations.", "sections": [{"title": "1 Introduction", "content": "Many real-world applications can be formulated as Markov Games (Littman, 1994) where the agents\nrepeatedly interact and update their policies based on the received feedback. In this context, different\nlearning dynamics and their convergence properties have been studied extensively (see, for example,\nFudenberg and Levine (1998)). Because of the mismatch between the individual short-run and\ncollective long-run incentives, or the lack of coordination in decentralized systems, agents following\nstandard learning dynamics may not converge to outcomes that are desirable from a system designer\nperspective, such as the Nash Equilibria (NE) with the largest social welfare. An interesting class of\ngames that exemplify these issues are so-called \"Stag Hunt\" games (see Fig. 1-(a)), which are used\nto study a broad array of real-world applications including collective action, public good provision,\nsocial dilemma, team work and innovation adoption (Skyrms, 2004).\nOur primary objective is to steer the agents to\nsome desired policies, that is, to minimize the\nsteering gap vis-a-vis the target outcome. As\na secondary objective, the payments to agents\nregarding the steering rewards should be reasonable, that is, the steering cost should be low.\nIn practice, learning the right steering strategies encounters two main challenges. First, the agents\nmay not disclose their learning dynamics model to the mediator. As a result, this creates fundamental\nmodel uncertainty, which we will tackle with appropriate Reinforcement Learning (RL) techniques to\ntrading-off exploration and exploitation. Second, it may be unrealistic to assume that the mediator\nis able to force the agents to \"reset\u201d their policies in order to generate multiple steering episodes\nwith the same initial state. This precludes the possibility of learning steering strategies through\nepisodic trial-and-error. Therefore, the most commonly-considered, fixed-horizon episodic RL (Dann\nand Brunskill, 2015) framework is not applicable here. Instead, we will consider a finite-horizon\nnon-episodic setup, where the mediator can only generate one finite-horizon episode, in which we\nhave to conduct both the model learning and steering of the agents simultaneously. Motivated by\nthese considerations, we would like to address the following question in this paper:\nHow can we learn desired steering strategies for Markovian agents\nin the non-episodic setup under model uncertainty?\nWe consider a model-based setting where the mediator can get access to a model class \\( \\mathcal{F} \\) containing\nthe agents' true learning dynamics \\( f^* \\). We summarize our main results in Table 1 and briefly highlight\nour key contributions as follow:\n\u2022 Conceptual Contributions: In Sec. 3, we formulate steering as a non-episodic RL problem, and\npropose a novel optimization objective in Obj. (1), where we explicitly tackle the inherent model\nuncertainty by learning history-dependent steering strategies. As we show in Prop. 3.3, under\ncertain conditions, even without prior knowledge of \\( f^* \\), the optimal solution to Obj. (1) achieves\nnot only low steering gap, but also \u201cPareto Optimality\u201d in terms of both steering costs and gaps.\n\u2022 Theoretical Contributions: In Sec. 4, we provide sufficient conditions under which there exists\nsteering strategies achieving low steering gap. These results in turn justify our chosen objective\nand problem formulation."}, {"title": "1.1 Closely Related Works", "content": "We discuss the works most closely related to ours in this section, and defer the others to Appx. \u0421.2.\nSteering Learning Dynamics The 'steering problem' as we study it in this paper was first intro-\nduced by Zhang et al. (2023). They consider the case of no-regret learners who may possibly know\nthe mediator's steering strategy and can be arbitrarily adversarial. Their focus is on steering agents\nsuch that the average policy converges to the target NE while the accumulative budget is sublinear.\nWhen the desired NE is not pure, Zhang et al. (2023) further require the mediator to be able to\n\"give advice\" to the players. By contrast, we focus on a broader class of Markovian agents with a\nfinite-horizon steering setup, with focus on the steering gap of the terminal policy and the cumulative\nsteering cost. Besides, our mediator can steer only by modifying the agents' reward functions.\nPerhaps the closest to ours is a concurrent work by Canyakmaz et al. (2024). They experimentally\ninvestigate the use of control methods to direct game dynamics towards desired outcomes, in particular\nallowing for model uncertainty. Our work complements their empirical results in three main ways.\nFirst, we handle the model uncertainty in a somewhat more principled way by proposing a concrete\nnon-episodic RL formulation for the steering problem and a suitable learning objective (Obj. (1)),\nwhere we explicitly learn a history-dependent steering strategy. Second, we develop novel theory\nregarding the existence of strategies with low steering gap. Third, on the algorithmic level, Canyakmaz\net al. (2024) consider a two-phase (exploration + exploitation) framework called SIAR-MPC which\nis similar to our FETE framework for large model sets. However, they employ random noise-\nbased exploration, while we consider a more advanced exploration strategy (Sec. 5.2) that results in\nsignificantly higher exploration efficiency in experiments (Sec. 6). Besides, when the model set is\nsmall, we contribute a belief-state based algorithm that can exactly solve our Obj. (1). As suggested\nby Prop. 3.3, it is superior to their two-phase framework in terms of the steering cost while achieving\na low steering gap.\nOpponent Shaping In the RL literature a line of work focus on the problem of opponent shaping,\nwhere agents can influence each others learning by handing out rewards (Foerster et al., 2018; Yang\net al., 2020; Willi et al., 2022; Lu et al., 2022; Willis et al., 2023; Zhao et al., 2022). Although\nthe ways of influencing agents are similar to our setting, we study the problem of a mediator that\nacts outside the Markov Game and steers all the agents towards desired policies, while in opponent\nshaping the agents themselves learn to influence each other for their own interests.\nEpisodic RL and Non-Episodic RL Most of the existing RL literature focus on the episodic\nlearning setup, where the entire interaction history can be divided into multiple episodes starting"}, {"title": "2 Preliminary", "content": "In the following, we formally define the finite-horizon Markov Game that we will focus on. We\nsummarize all the frequently used notations in this paper in Appx. B.\nFinite Horizon Markov Game A finite-horizon \\( N \\)-player Markov Game is defined by a tuple\n\\( \\mathcal{G} := \\{\\mathcal{N}, s_1, H, \\mathcal{S}, \\mathcal{A} := \\{\\mathcal{A}^n\\}_{n=1}^N, \\mathcal{P}, r := \\{r^n\\}_{n=1}^N\\} \\), where \\( \\mathcal{N} := \\{1, 2, ..., N\\} \\) is the indices of\nagents, \\( s_1 \\) is the fixed initial state, \\( H \\) is the horizon length, \\( \\mathcal{S} \\) is the finite shared state space, \\( \\mathcal{A}^n \\) is the\nfinite action space for agent \\( n \\), and \\( \\mathcal{A} \\) denotes the joint action space. Besides, \\( \\mathcal{P} := \\{\\mathcal{P}_h\\}_{h\\in[H]} \\) with\n\\( \\mathcal{P}_h : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\triangle(\\mathcal{S}) \\) denotes the transition function of the shared state, and \\( r^n := \\{r_h^n\\}_{h\\in[H]} \\) with\n\\( r_h^n : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) denotes the reward function for agent \\( n \\). For each agent \\( n \\), we consider the non-\nstationary Markovian policies \\( \\Pi^n := \\{\\pi^n = \\{\\pi_1^n, ..., \\pi_H^n\\} | \\forall h \\in [H], \\pi_h^n : \\mathcal{S} \\rightarrow \\triangle(\\mathcal{A}^n)\\} \\). We denote\n\\( \\Pi := \\Pi^1 \\times ... \\times \\Pi^N \\) to be the joint policy space of all agents. Given a policy \\( \\pi := \\{\\pi^1, ..., \\pi^N\\} \\in \\Pi \\),\na trajectory is generated by: \\( \\forall h \\in [H], \\forall n \\in [N], a_h^n \\sim \\pi_h^n(\\cdot|s_h), r_h^n = r_h^n(s_h, a_h), s_{h+1} \\sim\n\\mathcal{P}_h(\\cdot|s_h, a_h) \\), where \\( a_h := \\{a_h^n\\}_{n\\in[N]} \\) denotes the collection of all actions. Given a policy \\( \\pi \\), we\ndefine the value functions by: \\( Q_h^{n,\\pi}(s, a) := E_{\\pi}[\\sum_{h'=h}^H r_{h'}^n(s_{h'}, a_{h'})|s_h=s, a_h=a] \\), \\( V_h^{n,\\pi}(\\cdot) =\nE_{\\pi}[\\sum_{h'=h}^H r_{h'}^n(s_{h'}, a_{h'})|s_h = \\cdot] \\), where we use \\( r \\) to specify the reward function associated with the\nvalue functions. In the rest of the paper, we denote \\( A^{n,\\pi} := Q_h^{n,\\pi} - V_h^{n,\\pi} \\) to be the advantage value\nfunction, and denote \\( J^n(\\pi) := V_1^{n,\\pi}(s_1) \\) to be the total return of agent \\( n \\) w.r.t. policy \\( \\pi \\)."}, {"title": "3 The Problem Formulation of the Steering Markovian Agents", "content": "We first introduce our definition of Markovian agents. Informally, the policy updates of Markovian\nagents are independent of the interaction history conditioning on their current policy and observed\nrewards. This subsumes a broader class of popular policy-based methods as concrete examples\n(Giannou et al., 2022; Ding et al., 2022; Xiao, 2022; Daskalakis et al., 2020).\nDefinition 3.1 (Markovian Agents). Given a game \\( \\mathcal{G} \\), a finite and fixed \\( T \\), the agents are Markovian\nif their policy update rule \\( f \\) only depends on the current policy \\( \\pi_t \\) and the reward function \\( r \\):\n\\[\\forall t \\in [T], \\pi_{t+1} \\sim f(\\cdot|\\pi_t, r).\\]\nHere we only highlight the dependence on \\( \\pi_t \\) and \\( r \\), and omit other dependence (e.g. the transition\nfunction of \\( \\mathcal{G} \\)). It is worth to note that we do not restrict whether the updates of agents' policies are\nindependent or correlated with each other, deterministic or stochastic. We assume \\( T \\) is known to us.\nIn the steering problem, the mediator has the ability to change the reward function \\( r \\) via the steering\nreward \\( u \\), so that the agents' dynamics are modified to:\n\\[\\forall t \\in [T], u_t \\sim \\psi_t(\\cdot |\\pi_1, u_1, ..., \\pi_{t-1}, u_{t-1}, \\pi_t), \\pi_{t+1} \\sim f(\\cdot|\\pi_t, r + u_t),\\]\nHere \\( \\psi := \\{\\psi_t\\}_{t\\in[T]} \\) denotes the mediator's \u201csteering strategy\u201d to generate \\( u_t \\). We consider history-\ndependent strategies to handle the model uncertainty, which we will explain later. Besides, \\( u_t :=\n\\{u_t^{n,h}\\}_{h\\in[H],n\\in[N]} \\), where \\( u_t^{n,h} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, U_{\\max}] \\) is the steering reward for agent \\( n \\) at game\nhorizon \\( h \\) and steering step \\( t \\). \\( U_{\\max} < +\\infty \\) denotes the upper bound for the steering reward. For\npractical concerns, we follow Zhang et al. (2023) and constrain the steering reward to be non-negative.\nThe mediator has a terminal reward function \\( \\eta^{\\text{goal}} \\) and a cost function \\( \\eta^{\\text{cost}} \\). First, \\( \\eta^{\\text{goal}} : \\Pi \\rightarrow\n[0, \\eta_{\\max}] \\) assesses whether the final policy \\( \\pi_{T+1} \\) aligns with desired behaviors\u2014this encapsulates\nour primary goal of a low steering gap. Note that we consider the general setting and do not restrict\nthe maximizer of \\( \\eta^{\\text{goal}} \\) to be a Nash Equilibrium. For instance, to steer the agents to a desired\npolicy \\( \\pi^* \\), we could choose \\( \\eta^{\\text{goal}}(\\pi) := -||\\pi - \\pi^*||_2 \\). Alternatively, in scenarios focusing on\nmaximizing utility, \\( \\eta^{\\text{goal}}(\\pi) \\) could be defined as the total utility \\( \\sum_{n\\in[N]} J_n(\\pi) \\). For \\( \\eta^{\\text{cost}} : \\Pi \\rightarrow \\mathbb{R}_{\\ge0} \\),\nit is used to quantify the steering cost incurred while steering. In this paper, we fix \\( \\eta^{\\text{cost}}(\\pi, u) :=\n\\sum_{n\\in[N]} J_n(\\pi^n) \\) to be the total return related to \\( \\pi \\) and the steering reward \\( u \\). Note that we always\nhave \\( 0 \\le \\eta^{\\text{cost}}(\\pi, u) \\le U_{\\max}NH \\).\nSteering Dynamics as a Markov Decision Process (MDP) Given a game \\( \\mathcal{G} \\), the agents' dy-\nnamics \\( f \\) and \\( (\\eta^{\\text{cost}}, \\eta^{\\text{goal}}) \\), the steering dynamics can be modeled by a finite-horizon MDP.\n\\( \\mathcal{M} := \\{\\pi_1,T, \\Pi,U, f, (\\eta^{\\text{cost}}, \\eta^{\\text{goal}})\\} \\) with initial state \\( \\pi_1 \\), horizon length \\( T \\), state space \\( \\Pi \\), action\nspace \\( U := [0, U_{\\max}]^{HN|S||A|} \\), stationary transition \\( f \\), running reward \\( \\eta^{\\text{cost}} \\) and terminal reward \\( \\eta^{\\text{goal}} \\).\nSteering under Model Uncertainty In practice, the mediator may not have precise knowledge of\nagents learning dynamics model, and the uncertainty should be taken into account. We will only focus\non handling the uncertainty in agents' dynamics \\( f \\), and assume the mediator has the full knowledge of\n\\( \\mathcal{G} \\) and the reward functions \\( \\eta^{\\text{goal}} \\) and \\( \\eta^{\\text{cost}} \\). We consider the model-based setting where the mediator\nonly has access to a finite model class \\( \\mathcal{F} \\) (\\( |\\mathcal{F}| < +\\infty \\)) satisfying the following assumption:\nAssumption A (Realizability). \\( f^* \\in \\mathcal{F} \\).\nA Finite-Hoziron Non-Episodic Setup and Motivation As motivated previously, we formu-\nlate steering as a finite-horizon non-episodic RL problem. To our knowledge, in contrast to our\nfinite-horizon setting, most of the non-episodic RL settings consider the infinite-horizon setup with\nstationary or non-stationary transitions, and therefore, they are also not suitable here. We provide\nmore discussion in Sec. 1.1.\nDefinition 3.2 (Finite Horizon Non-Episodic Steering Setting). The mediator can only interact with\nthe real agents for one episode \\( \\{\\pi_1, u_1, ..., \\pi_T, u_T, \\pi_{T+1}\\} \\), where \\( \\pi_{t+1} \\sim f^*(\\cdot|\\pi_t, u_t) \\,\\forall t \\in [T] \\).\nNonetheless, the mediator can get access to the simulators for all models in \\( \\mathcal{F} \\), and it can sample\narbitrary trajectories and do episodic learning with those simulators to decide the best steering actions\n\\( u_1, u_2, ..., u_T \\) to deploy.\nThe Learning Objective Motivated by the model-based non-episodic setup, we propose the\nfollowing objective function, where we search over the set of all history-dependent strategies, denoted\nby \\( \\Psi \\), to optimize the average performance over all \\( f \\in \\mathcal{F} \\).\n\\[\\psi^* \\leftarrow \\text{arg} \\max_{\\psi \\in \\Psi} \\frac{1}{|\\mathcal{F}|} \\sum_{f \\in \\mathcal{F}} E_{\\psi, f} \\bigg[\\beta \\cdot \\eta^{\\text{goal}}(\\pi_{T+1}) - \\sum_{t=1}^T \\eta^{\\text{cost}}(\\pi_t, u_t)\\bigg],\\]\nHere we use \\( E_{\\psi,f}[\\cdot] := E[\\cdot|\\forall t \\in [T], u_t \\sim \\psi_t(\\cdot|\\{\\pi_{t'}, u_{t'}\\}_{t'=1}^{t-1}, \\pi_t), \\pi_{t+1} \\sim f(\\cdot|\\pi_t, r + u_t)] \\) to\ndenote the expectation over trajectories generated by \\( \\psi \\) and \\( f \\in \\mathcal{F} \\); \\( \\beta > 0 \\) is a regularization factor.\nNext, we explain the rationale to consider history-dependent strategies. As introduced in Def. 3.2,\nwe only intereact with the real agents once. Therefore, the mediator needs to use the interaction\nhistory with \\( f^* \\) to decide the appropriate steering rewards to deploy, since the history is the sufficient\ninformation set including all the information regarding \\( f^* \\) availale to the mediator.\nWe want to clarify that in our steering framework, we will first solve Obj. (1), and then deploy \\( \\psi^* \\)\nto steer real agents. The learning and optimization of \\( \\psi^* \\) in Obj. (1) only utilizes simulators of \\( \\mathcal{F} \\).\nBesides, after deploying \\( \\psi^* \\) to real agents, we will not update \\( \\psi^* \\) with the data generated during the\ninteraction with real agents. This is seemingly different from common online learning algorithms\nwhich conduct the learning and interaction repeatedly(Dann and Brunskill, 2015). But we want to\nhighlight that, given the fact that \\( \\psi^* \\) is history-dependent, it is already encoded in \\( \\psi^* \\) how to make\ndecisions (or say, learning) in the face of uncertainty after gathering data from real agents. In other\nwords, one can interpret that, in Obj. (1), we are trying to optimize an \u201conline algorithm\u201d \\( \\psi^* \\) which"}, {"title": "4 Existence of Steering Strategy with \\( \\varepsilon \\)-Steering Gap", "content": "In this section, we identify sufficient conditions such that \\( \\Psi_{\\varepsilon} \\) is non-empty. In Sec. 4.1, we start with\nthe special case when \\( f^* \\) is known, i.e. \\( \\mathcal{F} = \\{f^*\\} \\). The results will serve as basis when we study the\ngeneral unknown model setting in Sec. 4.2.\n4.1 Existence when \\( f^* \\) is Known: Natural Policy Gradient as an Example\nIn this section, we focus on a popular choice of learning dynamics called Natural Policy Gradient\n(NPG) dynamics (Kakade, 2001; Agarwal et al., 2021) (a.k.a. the replicator dynamics (Schuster and\nSigmund, 1983)) with direct policy parameterization. NPG is a special case of the Policy Mirror\nDescent (PMD) (Xiao, 2022). For the readability, we stick to NPG in the main text, and in Appx. E.1,\nwe formalize PMD and extend the results to the general PMD, which subsumes other learning\ndynamics, like the online gradient ascent (Zinkevich, 2003).\nDefinition 4.1 (Natural Policy Gradient). For any \\( n\\in [N], t\\in [T], h\\in [H], s_h \\in \\mathcal{S} \\), the policy\nis updated by: \\( \\pi_{t+1,h}^n(s_h) \\propto \\pi_{t,h}^n(s_h) \\exp(a \\hat{A}_{t}^{n,h/r_n+u_t}(s_h, \\cdot)) \\). Here \\( \\hat{A}_{t}^{n,h/r_n+u_t} \\) is some random\nestimation for the advantage value \\( A^{n,h/r_n+u_t} \\) with \\( E_{\\pi_t^n}[\\hat{A}_{t}^{n,h/r_n+u_t}(s_h, \\cdot)] = 0 \\).\nWe use \\( \\hat{A}_{t}^{n,r+u} \\) (and \\( \\hat{A}_{t}^{n,r+u} \\)) to denote the concatenation of the values of all agents, horizon, states\nand actions. We only assume \\( \\hat{A}_{t}^{n,r+u} \\) is controllable and has positive correlation with \\( A_{t}^{n,r+u} \\) but could\nbe biased, which we call the \u201cgeneral incentive driven\u201d agents.\nAssumption B (General Incentive Driven Agents).\n\\[\\forall t \\in [T], <E[A_{t}^{n,r+u}], A_{t}^{n,r+u}> \\ge A_{\\min} ||A_{t}^{n,r+u}||_2^2,  A_{t}^{n,r+u}  A_{\\max} ||A_{t}^{n,r+u}||_2^2,\\]"}, {"title": "4.2 Existence when \\( f^* \\) is Unknown: the Identifiable Model Class", "content": "Intuitively, when \\( f^* \\) is unknown, if we can first use a few steering steps \\( T \\)< \\( T \\) to explore and identify\n\\( f^* \\), and then steer the agents from \\( \\pi \\) to the desired policy within \\( T - \\tilde{T} \\) steps given the identified\n\\( f^* \\), we can expect \\( \\Psi_{\\varepsilon} \\neq \\emptyset \\). Motivated by this insight, we introduce the following notion.\nDefinition 4.3 (\\( (\\delta, \\tilde{T}) \\)-Identifiable). Given \\( \\delta \\in (0,1) \\), we say \\( \\mathcal{F} \\) is \\( (\\delta, \\tilde{T}) \\)-identifiable, if\n\\( \\max_{\\psi} \\min_{f \\in \\mathcal{F}} E_{\\psi,f}[\\mathbb{I}[f = f_{MLE}]] \\ge 1 - \\delta \\), where \\( \\mathbb{I}[E] = 1 \\) if \\( E \\) is true and otherwise 0;\n\\( f_{MLE} := \\text{arg} \\max_{f \\in \\mathcal{F}} \\sum_{t=1}^{\\tilde{T}} log f (\\pi_{t+1}|\\pi_t, u_t) \\).\nIntuitively, \\( \\mathcal{F} \\) is \\( (\\delta, \\tilde{T}) \\)-identifiable, if \\( \\exists \\psi \\), s.t. after \\( \\tilde{T} \\) steering steps, the hidden model \\( f \\) can be\nidentified by the Maximal Likelihood Estimation (MLE) with high probability. Next, we provide an\nexample of \\( (\\delta, \\tilde{T}) \\)-identifiable function class with \\( \\tilde{T} \\) upper bounded for any \\( \\delta \\in (0, 1) \\).\nExample 4.4. [One-Step Difference] If \\( \\forall \\pi \\in \\Pi \\), there exists a steering reward \\( u_{\\pi} \\in U \\), s.t.\n\\( \\min_{f, f'\\in\\mathcal{F}} H^2(f(\\cdot|\\pi,r + u_{\\pi}), f'(\\cdot|\\pi,r + u_{\\pi})) \\ge \\zeta \\), for some universal \\( \\zeta > 0 \\), where \\( H \\) is the\nHellinger distance, then for any \\( \\delta \\in (0, 1) \\), \\( \\mathcal{F} \\) is \\( (\\delta, \\tilde{T}) \\)-identifiable with \\( \\tilde{T} = O(\\zeta^{-1}log(|\\mathcal{F}|/\\delta)) \\).\nBased on Def. 4.3, we provide a sufficient condition when \\( \\Psi_{\\varepsilon} \\) is non-empty.\nTheorem 4.5. [A Sufficient Condition for Existence] Given any \\( \\varepsilon > 0, \\Psi_{\\frac{\\varepsilon}{2}}( \\mathcal{F}; \\pi_1) \\neq \\emptyset \\), if \\( \\exists \\tilde{T} < T \\),\ns.t., (1) \\( \\mathcal{F} \\) is \\( (\\frac{\\varepsilon}{\\beta |\\mathcal{F}|}, \\tilde{T}) \\)-identifiable, (2) \\( \\Psi_{\\frac{\\varepsilon}{2}}( \\mathcal{F}; \\pi_{\\tilde{T}}) \\neq \\emptyset \\) for any possible \\( \\pi_{\\tilde{T}} \\) generated at step\n\\( \\tilde{T} \\) during the steering.\nWe conclude this section by noting that, by Thm. 4.2, the above condition (2) is realistic for NPG (or\nmore general PMD) dynamics. The proofs for all results in this section are deferred to Appx. F."}, {"title": "5 Learning (Approximately) Optimal Steering Strategy", "content": "In this section, we investigate how to solve Obj. (1). Comparing with the episodic RL setting, the\nmain challenge is to learn a history-dependent policy. Since the history space grows exponentially in\n\\( T \\), directly solving Obj. (1) can be computationally intractable for large \\( T \\). Therefore, the main focus\nof this section is to design tractable algorithms to overcome this challenge.\nAs a special case, when the model is known, i.e. \\( \\mathcal{F} = \\{f^*\\} \\), by the Markovian property, Obj. (1)\nreduces to a normal RL objective, and a state-dependent steering strategy \\( \\psi : \\Pi \\rightarrow U \\) is already\nenough. For completeness, we include the algorithm but defer to Alg. 3 in Appx. C.4. In the rest of\nthis section, we focus on the general case \\( |\\mathcal{F}| > 1 \\). In Sec. 5.1, we investigate the solutions when \\( |\\mathcal{F}| \\)\nis small, and in Sec. 5.2, we study the more challenging case when \\( |\\mathcal{F}| \\) is large.\n5.1 Small Model Class: Dynamic Programming with Model Belief State\nA Partially Observable MDP Perspective In fact, we can interpret Obj. (1) as learning the optimal\npolicy in a POMDP, in which the hidden state is \\( (\\pi_t, f) \\), i.e. a tuple containing the policy and the\nhidden model \\( f \\) uniformly sampled from \\( \\mathcal{F} \\), and the mediator can only partially observe the policy\n\\( \\pi_t \\). It is well-known that any POMDP can be lifted to the belief MDP, where the state is the belief"}, {"title": "5.2 Large Model Class: A First-Explore-Then-Exploit Framework", "content": "When \\( |\\mathcal{F}| \\) is large, the method in Sec. 5.1 is inefficient since the belief state \\( b_t \\) is high-dimensional. In\nfact, the above POMDP interpretation implies the intractability of Obj. (1) for large \\( |\\mathcal{F}| \\): the number\nof hidden states of the POMDP scales with \\( |\\mathcal{F}| \\). Therefore, instead of exactly solving Obj. (1), we\nturn to the First-Explore-Then-Exploit (FETE) framework as stated in Procedure 2.\nThe first \\( \\tilde{T} < T \\) steps are the exploration phase, where we learn and deploy an exploration policy\n\\( \\psi^{\\text{Explore}} \\) maximizing the probability of identifying the hidden model with the MLE estimator. The\nremaining \\( T - \\tilde{T} \\) steps belong to the exploitation stage. We first estimate the true model by the MLE\nwith the interaction history with real agents. Next, we learn an exploitation strategy to steer real\nagents for the rest \\( T - \\tilde{T} \\) steps by solving Obj. (1) with \\( \\mathcal{F} = \\{f_{MLE}\\} \\), time \\( T - \\tilde{T} \\) and the initial\npolicy \\( \\pi_{\\tilde{T}+1} \\), as if \\( f_{MLE} \\) is the true model.\nJustification for \\( \\mathcal{F}ETE \\) We cannot guarantee that Desiderata 1& 2 are achievable, because we do\nnot exactly solve Obj. 1. However, if \\( \\mathcal{F} \\) is \\( (\\frac{\\delta}{|\\mathcal{F}|}, \\frac{\\tilde{T}}{|\\mathcal{F}|}) \\)-identifiable (Def. 4.3) and we choose\n\\( \\tilde{T} > \\frac{\\tilde{T}}{|\\mathcal{F}|} \\), we can verify \\( Pr(f_{MLE} = f^*) > 1 - \\delta \\) in Proc. 2. Therefore, we can still expect the\nexploitation policy \\( \\psi^{\\text{Exploit}} \\) steer the agents to approximately maximize \\( \\eta^{\\text{goal}}(\\pi_{T+1}) \\) with reasonable\nsteering cost for the rest \\( T - \\tilde{T} \\) steps.\nWe conclude this section by highlighting the computational tractability of FETE. Note that when\ncomputing \\( \\psi^{\\text{Exploit}} \\), we treat \\( f_{MLE} \\) as the true model, so an history-independent \\( \\psi^{\\text{Exploit}} \\) is enough.\nTherefore, the only part where we need to learn a history-dependent strategy is in the exploration\nstage, and the maximal history length is at most \\( \\tilde{T} \\), which can be much smaller than \\( T \\). Moreover, in\nsome cases, it is already enough to just learn a history-independent \\( \\psi^{\\text{Explore}} \\) to do the exploration (for\nexample, the model class in Example 4.4)."}, {"title": "6 Experiments", "content": "In this section", "x": "max\\{0,x\\} \\).\n6.1 Learning Steering Strategies with Knowledge of \\( f^* \\)\nNormal-Form Stag Hunt Game In Fig. 1-(b), we compare the agents' dynamics with/without\nsteering, where the agents learn to play the Stag Hunt Game in Fig. 1-(a). We report the experiment\nsetup here. Both agents follow the exact NPG (Def. 4.1 with \\( \\hat{A} = A^{\\pi} \\)) with fixed learning rate\n\\( a = 0.01 \\). For the steering setup, we choose the total utility as \\( \\eta^{\\text{goal}} \\), and use PPO to train the steering\nstrategy (one can choose other RL or control algorithms besides PPO). We also conduct experiments\nin a representative zero-sum game 'Matching Pennies', which we defer the details to Appx"}]}