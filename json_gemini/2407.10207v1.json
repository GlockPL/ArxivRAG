{"title": "Learning to Steer Markovian Agents under Model Uncertainty", "authors": ["Jiawei Huang", "Zebang Shen", "Vinzenz Thoma", "Heinrich H. Nax", "Niao He"], "abstract": "Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies without prior knowledge of the agents' underlying learning dynamics. We introduce a model-based non-episodic Reinforcement Learning (RL) formulation for our steering problem. Importantly, we focus on learning a history-dependent steering strategy to handle the inherent model uncertainty about the agents' learning dynamics. We introduce a novel objective function to encode the desiderata of achieving a good steering outcome with reasonable cost. Theoretically, we identify conditions for the existence of steering strategies to guide agents to the desired policies. Complementing our theoretical contributions, we provide empirical algorithms to approximately solve our objective, which effectively tackles the challenge in learning history-dependent strategies. We demonstrate the efficacy of our algorithms through empirical evaluations.", "sections": [{"title": "1 Introduction", "content": "Many real-world applications can be formulated as Markov Games (Littman, 1994) where the agents repeatedly interact and update their policies based on the received feedback. In this context, different learning dynamics and their convergence properties have been studied extensively (see, for example, Fudenberg and Levine (1998)). Because of the mismatch between the individual short-run and collective long-run incentives, or the lack of coordination in decentralized systems, agents following standard learning dynamics may not converge to outcomes that are desirable from a system designer perspective, such as the Nash Equilibria (NE) with the largest social welfare. An interesting class of games that exemplify these issues are so-called \"Stag Hunt\" games (see Fig. 1-(a)), which are used to study a broad array of real-world applications including collective action, public good provision, social dilemma, team work and innovation adoption (Skyrms, 2004). Stag Hunt games have two\nIn practice, learning the right steering strategies encounters two main challenges. First, the agents may not disclose their learning dynamics model to the mediator. As a result, this creates fundamental model uncertainty, which we will tackle with appropriate Reinforcement Learning (RL) techniques to trading-off exploration and exploitation. Second, it may be unrealistic to assume that the mediator is able to force the agents to \"reset\u201d their policies in order to generate multiple steering episodes with the same initial state. This precludes the possibility of learning steering strategies through episodic trial-and-error. Therefore, the most commonly-considered, fixed-horizon episodic RL (Dann and Brunskill, 2015) framework is not applicable here. Instead, we will consider a finite-horizon non-episodic setup, where the mediator can only generate one finite-horizon episode, in which we have to conduct both the model learning and steering of the agents simultaneously. Motivated by these considerations, we would like to address the following question in this paper:\nWe consider a model-based setting where the mediator can get access to a model class F containing the agents' true learning dynamics f*. We summarize our main results in Table 1 and briefly highlight our key contributions as follow:\n\u2022 Conceptual Contributions: In Sec. 3, we formulate steering as a non-episodic RL problem, and propose a novel optimization objective in Obj. (1), where we explicitly tackle the inherent model uncertainty by learning history-dependent steering strategies. As we show in Prop. 3.3, under certain conditions, even without prior knowledge of f*, the optimal solution to Obj. (1) achieves not only low steering gap, but also \u201cPareto Optimality\u201d in terms of both steering costs and gaps.\n\u2022 Theoretical Contributions: In Sec. 4, we provide sufficient conditions under which there exists steering strategies achieving low steering gap. These results in turn justify our chosen objective and problem formulation.\n\u2022 Algorithmic Contributions: Learning a history-dependent strategy presents challenges due to the exponential growth in the history space. We propose algorithms to overcome these issues.\n\u2013 When the model class |F| is small, in Sec. 5.1, we approach our objective from the perspective of learning in a Partially Observable MDP, and propose to to learn a policy over the model belief state space instead of over the history space.\n\u2013 For the case when F is large, exactly solving Obj. (1) can be challenging. Instead, we focus on approximate solutions to trade-off optimality and tractability. In Sec. 5.2, we propose a First-Explore-Then-Exploit (FETE) framework. Under some conditions, we can still ensure the directed agents converge to the desired outcome.\n\u2022 Empirical Validation: In Sec. 6, we evaluate our algorithms in various representative environments, and demonstrate their effectiveness under model uncertainty."}, {"title": "1.1 Closely Related Works", "content": "We discuss the works most closely related to ours in this section, and defer the others to Appx. \u0421.2.\nSteering Learning Dynamics The 'steering problem' as we study it in this paper was first intro- duced by Zhang et al. (2023). They consider the case of no-regret learners who may possibly know the mediator's steering strategy and can be arbitrarily adversarial. Their focus is on steering agents such that the average policy converges to the target NE while the accumulative budget is sublinear. When the desired NE is not pure, Zhang et al. (2023) further require the mediator to be able to \"give advice\" to the players. By contrast, we focus on a broader class of Markovian agents with a finite-horizon steering setup, with focus on the steering gap of the terminal policy and the cumulative steering cost. Besides, our mediator can steer only by modifying the agents' reward functions.\nPerhaps the closest to ours is a concurrent work by Canyakmaz et al. (2024). They experimentally investigate the use of control methods to direct game dynamics towards desired outcomes, in particular allowing for model uncertainty. Our work complements their empirical results in three main ways. First, we handle the model uncertainty in a somewhat more principled way by proposing a concrete non-episodic RL formulation for the steering problem and a suitable learning objective (Obj. (1)), where we explicitly learn a history-dependent steering strategy. Second, we develop novel theory regarding the existence of strategies with low steering gap. Third, on the algorithmic level, Canyakmaz et al. (2024) consider a two-phase (exploration + exploitation) framework called SIAR-MPC which is similar to our FETE framework for large model sets. However, they employ random noise- based exploration, while we consider a more advanced exploration strategy (Sec. 5.2) that results in significantly higher exploration efficiency in experiments (Sec. 6). Besides, when the model set is small, we contribute a belief-state based algorithm that can exactly solve our Obj. (1). As suggested by Prop. 3.3, it is superior to their two-phase framework in terms of the steering cost while achieving a low steering gap.\nOpponent Shaping In the RL literature a line of work focus on the problem of opponent shaping, where agents can influence each others learning by handing out rewards (Foerster et al., 2018; Yang et al., 2020; Willi et al., 2022; Lu et al., 2022; Willis et al., 2023; Zhao et al., 2022). Although the ways of influencing agents are similar to our setting, we study the problem of a mediator that acts outside the Markov Game and steers all the agents towards desired policies, while in opponent shaping the agents themselves learn to influence each other for their own interests.\nEpisodic RL and Non-Episodic RL Most of the existing RL literature focus on the episodic learning setup, where the entire interaction history can be divided into multiple episodes starting"}, {"title": "2 Preliminary", "content": "In the following, we formally define the finite-horizon Markov Game that we will focus on. We summarize all the frequently used notations in this paper in Appx. B.\nFinite Horizon Markov Game A finite-horizon N-player Markov Game is defined by a tuple G := {N,81,H,S,A := {A^}=1,P, r := {r}=1}, where N := {1, 2, ..., N} is the indices of agents, 81 is the fixed initial state, H is the horizon length, S is the finite shared state space, An is the finite action space for agent n, and A denotes the joint action space. Besides, P := {Ph}h\u2208[H] with Ph : S \u00d7 A \u2192 \u2206(S) denotes the transition function of the shared state, and rn := {r}he[H] with rh: S \u00d7 A \u2192 [0, 1] denotes the reward function for agent n. For each agent n, we consider the non-stationary Markovian policies In := {\u03c0\" = {\u03c0\", ..., \u03c0\u2081}|\u2200h\u2208 [H], \u03c0\u1fc7 : S \u2192 \u2206(An)}. We denote \u03a0 := \u03a0\u00b9 \u00d7 ... \u00d7 IN to be the joint policy space of all agents. Given a policy \u03c0 := {\u03c01, ..., \u03c0\u039d}\u03b5\u03a0, a trajectory is generated by: \u2200h \u2208 [H], \u2200n \u2208 [N], \u03b1\u03b7 ~ \u03c0\u03b7(\u00b7|sh), rn rn(sh, ah), Sh+1 Ph(\u00b7|sh, ah), where an := {an}ne[N] denotes the collection of all actions. Given a policy \u03c0, we define the value functions by: Q Q(,) := \u0395\u03c0[\u03a3=hh (sh', ah')|sh \u00b7], V () \u0395\u03c0[\u03a3\u03b7=hh (sh', ah')|sh = \u00b7], where we use |r to specify the reward function associated with the value functions. In the rest of the paper, we denote An,\u3160 A=Q-V to be the advantage value function, and denote J (\u03c0) := V V1 (81) to be the total return of agent n w.r.t. policy \u03c0.\n= ~ =\nvr nhr\n=~ H h H nHr"}, {"title": "3 The Problem Formulation of the Steering Markovian Agents", "content": "We first introduce our definition of Markovian agents. Informally, the policy updates of Markovian agents are independent of the interaction history conditioning on their current policy and observed rewards. This subsumes a broader class of popular policy-based methods as concrete examples (Giannou et al., 2022; Ding et al., 2022; Xiao, 2022; Daskalakis et al., 2020).\nDefinition 3.1 (Markovian Agents). Given a game G, a finite and fixed T, the agents are Markovian if their policy update rule f only depends on the current policy \u03c0t and the reward function r:\nHere we only highlight the dependence on \u03c0t and r, and omit other dependence (e.g. the transition function of G). It is worth to note that we do not restrict whether the updates of agents' policies are independent or correlated with each other, deterministic or stochastic. We assume T is known to us.\nIn the steering problem, the mediator has the ability to change the reward function r via the steering reward u, so that the agents' dynamics are modified to:\nHere \u03c8 := {t}te[T] denotes the mediator's \u201csteering strategy\u201d to generate ut. We consider history- dependent strategies to handle the model uncertainty, which we will explain later. Besides, ut :="}, {"title": "Steering Strategy with \u025b-Steering Gap", "content": "{ur,h}h\u2208[H],n\u2208[N], where u,h : S \u00d7 A \u2192 [0, Umax] is the steering reward for agent n at game horizon h and steering step t. Umax < +\u221e denotes the upper bound for the steering reward. For practical concerns, we follow Zhang et al. (2023) and constrain the steering reward to be non-negative.\nThe mediator has a terminal reward function goal and a cost function cost. First, ngoal : I \u2192 [0, Nmax] assesses whether the final policy \u3160\u315c+1 aligns with desired behaviors\u2014this encapsulates our primary goal of a low steering gap. Note that we consider the general setting and do not restrict the maximizer of ngoal to be a Nash Equilibrium. For instance, to steer the agents to a desired policy \u03c0*, we could choose ngoal(\u03c0) := -||\u03c0 \u2013 \u03c0* ||2. Alternatively, in scenarios focusing on maximizing utility, ngoal (\u03c0) could be defined as the total utility En\u2208[N] Jn(\u03c0). For ncost : II \u2192 R\u22650, it is used to quantify the steering cost incurred while steering. In this paper, we fix cost(\u03c0, u) := \u03a3\u03b7\u03b5[N] J (\u03c0\u03b7) to be the total return related to \u3160 and the steering reward u. Note that we always have 0 \u2264 \u03b7cost(\u03c0, u) \u2264 UmaxNH.\nSteering Dynamics as a Markov Decision Process (MDP) Given a game G, the agents' dy- namics f and (ncost, ngoal), the steering dynamics can be modeled by a finite-horizon MDP. M := {\u03c01,\u03a4, \u03a0,U, f, (ncost, ngoal)} with initial state \u03c0\u2081, horizon length T, state space II, action space U := [0, Umax]HN|S||A|, stationary transition f, running reward cost and terminal reward ngoal. For completeness, we defer to Appx. C.3 for an introduction of finite-horizon MDP\nSteering under Model Uncertainty In practice, the mediator may not have precise knowledge of agents learning dynamics model, and the uncertainty should be taken into account. We will only focus on handling the uncertainty in agents' dynamics f, and assume the mediator has the full knowledge of G and the reward functions ngoal and cost. We consider the model-based setting where the mediator only has access to a finite model class F (|F| < +\u221e) satisfying the following assumption:\nAssumption A (Realizability). f* \u2208 F.\nA Finite-Hoziron Non-Episodic Setup and Motivation As motivated previously, we formu- late steering as a finite-horizon non-episodic RL problem. To our knowledge, in contrast to our finite-horizon setting, most of the non-episodic RL settings consider the infinite-horizon setup with stationary or non-stationary transitions, and therefore, they are also not suitable here. We provide more discussion in Sec. 1.1.\nDefinition 3.2 (Finite Horizon Non-Episodic Steering Setting). The mediator can only interact with the real agents for one episode {\u03c01, U1, \u2026, \u03c0\u03c4, \u03c5\u03c4, \u03c0\u03c4+1}, where \u03c0t+1 ~ f*(\u00b7|\u03c0t, ut) \u2200t \u2208 [T]. Nonetheless, the mediator can get access to the simulators for all models in F, and it can sample arbitrary trajectories and do episodic learning with those simulators to decide the best steering actions U1, U2, ..., up to deploy.\nThe Learning Objective Motivated by the model-based non-episodic setup, we propose the following objective function, where we search over the set of all history-dependent strategies, denoted by \u03a8, to optimize the average performance over all f \u2208 F.\nHere we use Ey,f[\u00b7] := E[\u00b7|\u221at \u2208 [T], ut ~ Vt(\u00b7|{\u03c0\u03b9', Ut'}=1, \u03c0\u03b9), \u03c0t+1 ~ f(\u00b7\u03c0\u03c4,r + ut)] to denote the expectation over trajectories generated by and \u0192 \u2208 F; \u03b2 > 0 is a regularization factor. Next, we explain the rationale to consider history-dependent strategies. As introduced in Def. 3.2, we only intereact with the real agents once. Therefore, the mediator needs to use the interaction history with f* to decide the appropriate steering rewards to deploy, since the history is the sufficient information set including all the information regarding f* availale to the mediator.\nWe want to clarify that in our steering framework, we will first solve Obj. (1), and then deploy * to steer real agents. The learning and optimization of 4* in Obj. (1) only utilizes simulators of F. Besides, after deploying * to real agents, we will not update * with the data generated during the interaction with real agents. This is seemingly different from common online learning algorithms which conduct the learning and interaction repeatedly(Dann and Brunskill, 2015). But we want to highlight that, given the fact that 4* is history-dependent, it is already encoded in 4* how to make decisions (or say, learning) in the face of uncertainty after gathering data from real agents. In other words, one can interpret that, in Obj. (1), we are trying to optimize an \u201conline algorithm\" * which\""}, {"title": "Justification for Objective (1)", "content": "can \"smartly\" decide the next steering reward to deploy given the past interaction history. As we will justify in the following, our Obj. (1) can indeed successfully handle the model uncertainty.\nJustification for Objective (1) We use C\u03c8,\u03c4(f) := Ey, f [t=1 cost (\u03c0t, ut)] and \u2206\u03c8,\u03c4(f) := Ey, f [max ngoal (\u03c0) \u2013 ngoal(\u03c0\u03c4+1)] as short notes of the steering cost and the steering gap (of the terminal policy \u03c0\u03c4+1), respectively. Besides, we denote \u03a8\u20ac := {\u03c8 \u2208 \u03a8| maxfeF \u2206\u03c8,T(F) \u2264 \u025b} to be the collection of all steering strategies with \u025b-steering gap. Based on these notations, we introduce two desiderata, and show how an optimal solution 4* of Obj. (1) can achieve them.\nDesideratum 1 (\u03b5-Steering Gap). We say \u03c8 has \u025b-steering gap, if maxfeF \u2206y,T(f) \u2264 \u03b5.\nDesideratum 2 (Pareto Optimality). We say & is Pareto Optimal if there does not exist another \u03c8' \u2208 \u03a8, such that (1) \u2200f \u2208 F, C\u03c8,\u03c4(f) \u2264 C\u03c8,\u03c4(f) and \u2206\u03c8',\u03c4(f) \u2264 \u2206\u03c8,\u03c4(f); (2) \u2203f' \u2208 F, s.t. either C',T(f') < C\u03c8,\u03c4(f') or \u2206\u03c8',\u03c4(f') < \u2206\u03c8,\u03c4(f').\nProposition 3.3. [Justification for Obj. (1)] By solving Obj. (1): (1) \u03c8* is Pareto Optimal; (2) Given any \u20ac, \u03b5' > 0, if \u03a8\u20ac//F\\ \u2260 \u00d8 and B >  we have \u03c8* \u2208 \u03a8\u20ac+\u03b5';\nNext, we give some interpretation. As our primary desideratum, we expect the agents converge to some desired policy that maximizes the goal function goal after being steered for T steps, regardless of the true model f*. Therefore, we restrict the worst case steering gap to be small. As stated in Prop. 3.3, for any accuracy level \u025b\u03b5 > 0, as long as \u03b5/|F|-steering gap is achievable, by choosing B large enough, we can approximately guarantee 4* has \u025b-steering gap. For the steering cost, although it is not our primary objective, Prop. 3.3 states that at least we can guarantee the Pareto Optimality: competing with 4*, there does not exist another \u03c8', which can improve either the steering cost or gap for some f' \u2208 F without deteriorating any others.\nGiven the above discussion, one natural question is that: when is e non-empty, or equivalently, when does a strategy \u03c8 with \u025b-steering gap exist? In Sec. 4, we provide sufficient conditions and concrete examples to address this question in theory. Notably, we suggest conditions where e is non-empty for any \u025b > 0, so that the condition \u03a8\u20ac/|F| \u2260 \u00d8 in Prop. 3.3 is realizable, even for large F. After that, in Sec. 5, we introduce algorithms to solve our Obj. (1)."}, {"title": "4 Existence of Steering Strategy with \u025b-Steering Gap", "content": "In this section, we identify sufficient conditions such that \u03a8\u20ac is non-empty. In Sec. 4.1, we start with the special case when f* is known, i.e. F = {f*}. The results will serve as basis when we study the general unknown model setting in Sec. 4.2."}, {"title": "4.1 Existence when f* is Known: Natural Policy Gradient as an Example", "content": "In this section, we focus on a popular choice of learning dynamics called Natural Policy Gradient (NPG) dynamics (Kakade, 2001; Agarwal et al., 2021) (a.k.a. the replicator dynamics (Schuster and Sigmund, 1983)) with direct policy parameterization. NPG is a special case of the Policy Mirror Descent (PMD) (Xiao, 2022). For the readability, we stick to NPG in the main text, and in Appx. E.1, we formalize PMD and extend the results to the general PMD, which subsumes other learning dynamics, like the online gradient ascent (Zinkevich, 2003).\nDefinition 4.1 (Natural Policy Gradient). For any n\u2208 [N],te [T],h\u2208 [H], sh \u2208 S, the policy is updated by: \u03c0+1,h(sh) \u03b1\u03c0\u03b7 (sh) exp(aAnd+un (sh..)). Here Ah/rn+un An is some random estimation for the advantage value Antun with Enn [Ann+ur (sh, \u00b7)] = 0.\nWe use Ar+u A (and A Ar+u) to denote the concatenation of the values of all agents, horizon, states and actions. We only assume Arttu is controllable and has positive correlation with Afr but could be biased, which we call the \u201cgeneral incentive driven"}, {"title": "4.2 Existence when f* is Unknown: the Identifiable Model Class", "content": "Intuitively, when f* is unknown, if we can first use a few steering steps T < T to explore and identify f*, and then steer the agents from \u3160 to the desired policy within T \u2013 T steps given the identified f*, we can expect \u03a8\u20ac \u2260 \u00d8. Motivated by this insight, we introduce the following notion.\nDefinition 4.3 ((\u03b4, T)-Identifiable). Given \u03b4\u2208 (0,1), we say F is (S,T)-identifiable, if max, min feF Ey,f[I[f = fMLE]] \u2265 1 \u2212 \u03b4, where I[E] = 1 if & is true and otherwise 0; IMLE := arg max feF St-1 log f (\u03c0t+1|\u03c0t, ut).\nIntuitively, F is (d, T\u00ba\u00b4-)-identifiable, if \u2203, s.t. after T\u00e5 steering steps, the hidden model f can be identified by the Maximal Likelihood Estimation (MLE) with high probability. Next, we provide an example of (d, T\u00ba)-identifiable function class with T\u00ba upper bounded for any \u03b4\u2208 (0, 1).\nExample 4.4. [One-Step Difference] If \u2200n \u2208 \u03a0, there exists a steering reward \u03c5\u03c0 \u2208 U, s.t. minf, f'eF H\u00b2(f(\u00b7|\u03c0,r + un), f'(\u00b7|\u03c0,r + un)) \u2265 \u3118, for some universal \u03da > 0, where H is the Hellinger distance, then for any \u03b4 \u2208 (0, 1), F is (\u03b4, T-)-identifiable with T = O($\u22121log(|F|/8)).\nBased on Def. 4.3, we provide a sufficient condition when \u03a8\u20ac is non-empty.\nTheorem 4.5. [A Sufficient Condition for Existence] Given any \u025b > 0, \u03a8\u2081(F; \u03c0\u2081) \u2260 \u00d8, if\u2203\u0128 <T, s.t., (1) F F is (2, T)-identifiable, (2) \u03a8\u00b2/27(F; \u3160\u2081) \u2260 \u00d8 for any possible \u3160 generated at step \u00ce during the steering.\nWe conclude this section by noting that, by Thm. 4.2, the above condition (2) is realistic for NPG (or more general PMD) dynamics. The proofs for all results in this section are deferred to Appx. F."}, {"title": "5 Learning (Approximately) Optimal Steering Strategy", "content": "In this section, we investigate how to solve Obj. (1). Comparing with the episodic RL setting, the main challenge is to learn a history-dependent policy. Since the history space grows exponentially in T, directly solving Obj. (1) can be computationally intractable for large T. Therefore, the main focus of this section is to design tractable algorithms to overcome this challenge.\nAs a special case, when the model is known, i.e. F = {f*}, by the Markovian property, Obj. (1) reduces to a normal RL objective, and a state-dependent steering strategy \u03c8 : \u03a0 \u2192 U is already enough. For completeness, we include the algorithm but defer to Alg. 3 in Appx. C.4. In the rest of this section, we focus on the general case |F| > 1. In Sec. 5.1, we investigate the solutions when |F| is small, and in Sec. 5.2, we study the more challenging case when |F| is large."}, {"title": "5.1 Small Model Class: Dynamic Programming with Model Belief State", "content": "A Partially Observable MDP Perspective In fact, we can interpret Obj. (1) as learning the optimal policy in a POMDP, in which the hidden state is (\u03c0t, f), i.e. a tuple containing the policy and the hidden model f uniformly sampled from F, and the mediator can only partially observe the policy \u03c0t. It is well-known that any POMDP can be lifted to the belief MDP, where the state is the belief\nstate of the original POMDP. Then, the optimal policy in the belief MDP is exactly the optimal history-dependent policy in the original POMDP (Ibe, 2013). In our case, for each step t \u2208 [T], the belief state is (\u03c0t, bt), where bt := [Pr(f|{\u03c0t', Ut'}'=1, \u03c0t)]feF is the \u201cmodel belief state\u201d defined to be the posterior distribution of models given the history of observations and actions. When |F| is small, the model belief state bt \u2208 R|F| is low dimensional and computable. Learning 4* is tractable by running any RL algorithm on the lifted MDP. In Proc. 1, we show how to steer in this setting. We defer the detailed algorithm of learning such belief-state dependent strategy to Alg. 4 in Appx. C.5."}, {"title": "5.2 Large Model Class: A First-Explore-Then-Exploit Framework", "content": "When |F is large, the method in Sec. 5.1 is inefficient since the belief state bt is high-dimensional. In fact, the above POMDP interpretation implies the intractability of Obj. (1) for large |F|: the number of hidden states of the POMDP scales with |F|. Therefore, instead of exactly solving Obj. (1), we turn to the First-Explore-Then-Exploit (FETE) framework as stated in Procedure 2.\nThe first \u0622 > T steps are the exploration phase, where we learn and deploy an exploration policy Explore maximizing the probability of identifying the hidden model with the MLE estimator. The remaining T - T steps belong to the exploitation stage. We first estimate the true model by the MLE with the interaction history with real agents. Next, we learn an exploitation strategy to steer real agents for the rest T T steps by solving Obj. (1) with F = {fMLE}, time T \u2013 T and the initial policy +1, as if fMLE is the true model.\nJustification for FETE We cannot guarantee that Desiderata 1& 2 are achievable, because we do not exactly solve Obj. 1. However, if F is (8/|F|, T\u00ba/|F|)-identifiable (Def. 4.3) and we choose T > T\u00ba/\\|, we can verify Pr(fMLE = f*) > 1 \u2212 d in Proc. 2. Therefore, we can still expect the exploitation policy \u201eExploit steer the agents to approximately maximize \u03b7goal (\u03c0\u03c4+1) with reasonable steering cost for the rest T \u2013 T steps.\nWe conclude this section by highlighting the computational tractability of FETE. Note that when computing Exploit, we treat fMLE as the true model, so an history-independent \u201eExploit is enough. Therefore, the only part where we need to learn a history-dependent strategy is in the exploration stage, and the maximal history length is at most T, which can be much smaller than T. Moreover, in some cases, it is already enough to just learn a history-independent Explore to do the exploration (for example, the model class in Example 4.4)."}, {"title": "6 Experiments", "content": "In this section, we discuss our experimental results. For more details of all experiments in this section (e.g. experiment setup and training details), we defer to Appx. H. The steering horizon is set to be T = 500, and all the error bar shows 95% confidence level. We denote [x]+ := max{0,x}.\n6.1 Learning Steering Strategies with Knowledge of f*\nNormal-Form Stag Hunt Game In Fig. 1-(b), we compare the agents' dynamics with/without steering, where the agents learn to play the Stag Hunt Game in Fig. 1-(a). We report the experiment setup here. Both agents follow the exact NPG (Def. 4.1 with \u00c2\u00bb = A^) with fixed learning rate a = 0.01. For the steering setup, we choose the total utility as goal, and use PPO to train the steering strategy (one can choose other RL or control algorithms besides PPO). We also conduct experiments in a representative zero-sum game 'Matching Pennies', which we defer the details to Appx. \u041d.2.\nGrid World Stag Hunt Game: Learning Steering Strategy with Observations on Agents' Behaviors In the previous experiments, we consider the direct parameterization and the state space X = I R4 has low dimension. In real-world scenarios, the policy space II can be extremely rich and high-dimensional if the agents consider neural networks as policies. In ad- dition, the mediator may not get access to the agents' exact policy \u3160 because of privacy issues. This motivates us to investigate the possibility of steering agents with observations on agents' behavior only (e.g. trajectories of agents in a game G), instead of the full observation of \u03c0.\nIn Appx. G, we justify this setup and formalize it as a partially observable extension of our current framework. We consider the evaluation in a grid- world version of the Stag Hunt Game as shown in Fig. 2-(a). In this setting, the state space in game G becomes pixel-based images, and both agents (blue and red) will adopt Convolutional Neural Networks (CNN) based policies with thousands of parameters and update with PPO. We train a steering strategy, which only takes the agents' recent trajectories as input to infer the steering reward. As shown in Fig. 2-(b),rium and chase the stag. without direct usage of the agents' policy, we can still train a steering strategy towards desired solution.\n6.2 Learning Steering Strategies without Knowledge of f*\nSmall Model Set F: Belief State Based Steering Strategy In this part, we evaluate Proc. 1 designed for small F. We consider the same normal-form Stag Hunt game and setup as Sec. 6.1, while the agents update by the NPG with a random learning rate a = [\u00a7]+, where \u00a7 ~ \u039d(\u03bc, 0.32). Here the mean value \u03bc is unknown to the mediator, and we consider a model class F := {fo.7, f1.0} including two possible values of \u03bc\u03b5 {0.7, 1.0}. We report our experimental results in Table 2.\nFirstly, we demonstrate the suboptimal behavior if the mediator ignores the model uncertainty and just randomly deploys the optimal strategy of fo.7 or f1.0. To do this, we train the (history-independent) optimal steering strategy by Alg. 3, as if we know f* = f0.7 (or f* = f1.0), which we denote as 4.7 (or 41.0). To meet with our Desideratum 1, we first set the accuracy level \u025b = 0.01, and search the minimal \u1e9e so that the learned steering strategy can achieve \u025b-steering gap (see Appx. H.3.1). Because of the difference in \u00b5, we have \u03b2 = 70 and \u03b2 = 20 in training V\u1ed7.7 and 41.0, respectively, and empirically, we observe that 4.7 requires much larger steering reward than 41.0. As we marked in red in Table 2-(a) and (b), because of the difference in the steering signal, 40.7 consumes much higher steering cost to achieve the same accuracy level in f1.0, and 10 may fail to steer agents with f0.7 to the desired accuracy. Next, we train another strategy Belief via Alg. 4, which predicts the"}, {"title": "7 Conclusion", "content": "In this paper, we introduce the problem of steering Markovian agents to desired game outcomes under model uncertainty. We provide theoretical foundations for this problem by formulating a novel optimization objective and providing existence results. Moreover, we design several algorithmic approaches suitable for varying degrees of model uncertainty in this problem class. We test their performances in different experimental settings and show their effectiveness. Our work opens up avenues for compelling open problems that merit future investigation. Firstly, future work could aim to identify superior optimization objectives that guarantee strictly better performances in terms of"}, {"title": "A Discussion on Limitations and Societal Impact", "content": "Discussion on Limitations The main limitation of this paper is that we only focus on Markovian agents, whose dynamics f only depends on their current policy profiles and the (modified) reward function. Although this model can already capture lots of learning dynamics, not all the theoretical results and methodology can be generalized to non-Markovian settings.\nBesides, we can not guarantee how the optimal steering strategy regarding our Objective (1) performs in terms of the steering cost. We leave it to the future works.\nMoreover, for the large model class setting, the strategy obtained by running proposed FETE framework may not have guarantees on the steering cost. It is also unclear if there are better algorithm designs for this setting.\nDiscussion on Societal Impact Although this work are not directly applicable to real-world applications, but we believe it may have positive societal impact. The steering problem we study matches the requirement of many real-world scenarios (e.g. the social planner may incentivize technology companies by individual financial subsidies). We believe the theoretical and algorithmic contributions in this paper can provide useful insights for those applications."}, {"title": "B Frequently Used Notations", "content": ""}, {"title": "C Missing Details in the Main Text", "content": "C.1 A Real-World Scenario that Can be Modeled as a Stag Hunt Game\nAs a real-world example, the innovation adaption can be modeled as a (multi-player) Stag Hunt game. Consider a situation involving a coordination problem where people can choose between an inferior/unsustainable communication or transportation technology that is cheap (the Gather action) and a superior technology that is sustainable but more expensive (the Hunt action). If more and more people buy products by the superior technology, the increasing profits can lead to the development of that technology and the decrease of price. Eventually, everyone can afford the price and benefit from the sustainable technology. In contrast, if people are trapped by the products of the inferior technology due to its low price, the long-run social welfare can be sub-optimal. The mediator's goal is to steer the population to adopt the superior technology.\nC.2 Additional Related Works\nLearning Dynamics in Multi-Agent Systems In multi-agent setting, it is an important question to design learning dynamics and understand their convergence properties (Hernandez-Leal et"}]}