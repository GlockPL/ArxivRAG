{"title": "USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for\nCross-lingual Semantic Textual Relatedness", "authors": ["Jianjian Li", "Shengwei Liang", "Yong Liao", "Hongping Deng", "Haiyang Yu"], "abstract": "Cross-lingual semantic textual relatedness task\nis an important research task that addresses\nchallenges in cross-lingual communication and\ntext understanding. It helps establish semantic\nconnections between different languages, cru-\ncial for downstream tasks like machine trans-\nlation, multilingual information retrieval, and\ncross-lingual text understanding. Based on ex-\ntensive comparative experiments, we choose\nthe XLM-Rbase as our base model and use\npre-trained sentence representations based on\nwhitening to reduce anisotropy. Additionally,\nfor the given training data, we design a deli-\ncate data filtering method to alleviate the curse\nof multilingualism. With our approach, we\nachieve a 2nd score in Spanish, a 3rd in In-\ndonesian, and multiple entries in the top ten\nresults in the competition's track C. We further\ndo a comprehensive analysis to inspire future\nresearch aimed at improving performance on\ncross-lingual tasks.", "sections": [{"title": "1 Introduction", "content": "Semantic textual relatedness (STR) encompasses\na broader concept that takes into account various\ncommonalities between two sentences. This in-\ncludes factors such as being on the same topic,\nexpressing the same viewpoint, originating from\nthe same period, one sentence elaborating on or\nfollowing from the other, and more. SemEval is\nan international workshop on semantic evaluation.\nIn track C of SemEval-2024 task 1: Cross-lingual\n(Ousidhoum et al., 2024b), participants are to sub-\nmit systems, which are developed without the use\nof any labeled semantic similarity or semantic relat-\nedness datasets in the target language and with the\nuse of labeled datasets (Ousidhoum et al., 2024a)\nfrom at least one other language.\nVarious methods were proposed to address the\ntask of textual relatedness. One common approach\nis based on feature engineering, where the syn-\ntactic, semantic, and structural features of text,\nsuch as word frequency, TF-IDF, and word em-\nbeddings, are extracted. Machine learning algo-\nrithms are then employed for relatedness determi-\nnation. Another popular approach is based on deep\nlearning methods, such as Convolutional Neural\nNetworks (LeCun et al., 1989), Recurrent Neural\nNetworks (Graves and Graves, 2012), and self-\nattention mechanisms (Vaswani et al., 2017). These\nmethods can capture semantic relationships and\ncontextual information within the text, and they are\ntrained on large-scale datasets to enhance model\nperformance and generalization ability.\nHowever, there are two challenges in track C of\nSemEval-2024 task 1:\n\u2022 Compared with static word representation\nsuch as Word2Vec (Mikolov et al., 2013)\nand Glove (Pennington et al., 2014), the pre-\ntrained language models (PLM) can obtain\nsentence representation for different sentence\nin different contexts, thereby solving differ-\nent problems. However, the vectors of BERT-\nbased PLM models have limitations: BERT-\nbased models always induces a non-smooth\nanisotropic semantic space of sentences,"}, {"title": "which harms its performance of semantic\nsimilarity", "content": "(Gao et al., 2019; Li et al., 2020),\nwhich can lead to a challenge that sentences\nare strikingly similar while using the cosine\nsimilarity metric.\n\u2022 Participants are not allowed to utilize labeled\ndatasets in the target language for training. In-\nstead, they must use labeled data in different\nlanguages as the training set to train the model\nand provide predictions in the target language.\nHowever, multilingual pre-trained models\nsuffer from the curse of multilingualism\n(Conneau et al., 2020), that is, the overall\nperformance of both monolingual and cross-\nlingual baselines declines when adding more\nlanguages to training data over a certain point.\nHence, it is essential to investigate which ad-\nditional languages would be inefficient as the\ntraining dataset for the target language.\nIn this paper, we used whitening techniques (Su\net al., 2021), which maps vectors to standard or-\nthogonal bases, to transform the word vector rep-\nresentations from anisotropic to isotropic, and sur-\nprisingly, we found that whitening significantly\nimproves the accuracy of judging semantic sim-\nilarity. Given the absence of labeled data in the\ntarget language, it is difficult to determine which\nother language would yield better prediction results\nwhen used as training data. Therefore, we pro-\nposed that removing certain language categories\nfrom the training data for a specific target language\ncontributed to improving performance.\nWe conducted extensive experiments to demon-\nstrate the effectiveness of the method we employed.\nAs a result, our submitted outcomes achieved a\n2nd score in Spanish and a 3rd score in Indonesian\nin track C of SemEval-2024's task 1. Addition-\nally, we obtained multiple top-ten rankings in the\ncompetition."}, {"title": "2 Background", "content": "The task of semantic text relatedness covers sev-\neral specific subtasks, including semantic similarity,\nsemantic matching, textual entailment, semantic re-\nlation classification, and text pair ranking. Previous\nwork has proposed various methods for these spe-\ncific tasks, such as: Lexical and syntactic-based\nmethods (Gamallo et al., 2001; Pakray et al., 2011):\nThese methods rely on lexical and syntactic rules,\nsuch as word vector matching, lexical overlap, and\nsyntactic tree matching. However, these methods\noften fail to capture higher-level semantic relation-\nships. Feature engineering-based machine learning\nmethods (Chia et al., 2021; Fan et al., 2019): These\nmethods involve using manually designed features,\nsuch as bag-of-words models (Zhang et al., 2010),\ntf-idf weights, and syntactic features, followed by\nusing machine learning algorithms like support vec-\ntor machines and random forests for prediction.\nWhile these methods have improved perfor-\nmance to some extent, they still have limitations in\ncapturing complex semantic relationships. Neural\nnetwork-based models: These models use neural\nnetworks to learn representations of text and cap-\nture semantic relationships between texts through\ntraining data. This includes methods that fine-tune\npre-trained language models (e.g., BERT (Kenton\nand Toutanova, 2019) and GPT2 (Radford et al.,\n2019) etc.), as well as approaches that employ\nSiamese networks, LSTM, CNN, and other archi-\ntectures for text encoding and matching. Trans-\nfer learning and multi-task learning (Pilault et al.,\n2020; Wu et al., 2020): These methods lever-\nage knowledge from pre-trained models on related\ntasks to improve the performance of semantic tex-\ntual relatedness tasks through transfer learning (Ko-\nroleva et al., 2019). Multi-task learning combines\nmultiple related tasks in training to enhance the\nmodel's generalization ability and effectiveness.\nApplication of external knowledge resources: Re-\nsearchers have also attempted to incorporate exter-\nnal knowledge resources such as word embeddings,\nsemantic knowledge graphs, and multilingual data\nto enhance the model's understanding of semantic\nrelationships.\nFor cross-lingual semantic similarity tasks, map-\nping texts from different languages into a shared se-\nmantic space for similarity calculation is necessary.\nTo address this, researchers have proposed vari-\nous cross-lingual representation learning methods.\nAmong them, unsupervised alignment methods like\nunsupervised machine translation (Lample et al.,\n2017) and cross-lingual pre-training models (Liang\net al., 2020) can learn the correspondences between\nmultiple languages and map texts to a shared vector\nspace.\nHowever, (Conneau and Lample, 2019) and\n(Wang et al.) mentioned that vector representa-\ntions based on the Transformer models exhibit\nanisotropy, which means that the vectors are un-\nevenly distributed and clustered in a narrow cone-\nshaped space. Therefore, both Bert-flow (Li et al.,"}, {"title": "3 System Overview", "content": "3.1 Framework Overview\nIn this section, we will introduce our proposed\nmethod for STR task which has three main mod-\nules.\n\u2022 PLM Encoder We adopted the pretrained lan-\nguage model XLM-ROBERTa-base (XLM- \nRbase) (Conneau et al., 2020) for initial sen-\ntence encoding, which combines two powerful\nmodels: Transformer and RoBERTa. XLM-\nRbase demonstrates strong multilingual ca-\npabilities and a deep understanding of se-\nmantics, surpassing some monolingual pre-\ntraining models. After conducting a series of\ntests on mBERT (Pires et al., 2019), XLM\n(Conneau and Lample, 2019), and XLM-\nRbase/large, we selected XLM-Rbase as the\nencoder due to its superior performance.\n\u2022 Whitening Module After obtaining the sen-\ntence vectors of two utterances using XLM-\nRbase, we could have directly calculated the\ncosine similarity between the two vectors, but\nthe sentence vectors after XLM-Rbase show\nanisotropy between them and are distributed\nin a conical space, resulting in a high co-\nsine similarity. Therefore, we introduce the\nWhitening module to change the distribution\nof the sentence vector space so that its distri-\nbution has various anisotropies, amplifying\nthe differences between the vectors and stimu-\nlating the performance of XLM-Rbase on the\nsemantic text similarity reading task.\n\u2022 Data Filtering The authors of (Conneau et al.,\n2020) mention the curse of multilingualism,\nwhere adding more languages leads to an im-\nprovement in cross-lingual performance for\nlow-resource languages up to a certain point,\nafter which the overall performance of both\nmonolingual and cross-lingual baselines de-\nclines. In the task of cross-lingual semantic\ntext similarity, to maximize the exploration\nof the positive impact of other languages on\nthe target language, we propose a new dataset\nselection method. As the influence between\nlanguages is mutual, we utilize the unlabeled\ndata of the target language to detect the im-\npact of each language in track A, excluding\nthe target language, and infer its influence on\nthe target language. This allows us to select\nthe training dataset optimally. This approach\nhelps eliminate interference from certain lan-\nguages on the target language and avoids the\ncurse of multilingualism."}, {"title": "3.2 PLM Encoder", "content": "Through a simple test and comparative analy-\nsis of different multilingual pre-training models,\nwe found that XLM-Rbase outperforms mBERT.\nXLM-Rbase is a cross-lingual pre-training model\nbased on the BERT architecture, an improvement\nand extension of the original XLM model. The\ngoal of XLM-Rbase is to enhance the performance\nand effectiveness of multilingual text processing.\nXLM-Rbase utilizes larger-scale pre-training data\nand more sophisticated training methods to en-\nhance the model's representation capabilities. It\nundergoes deep learning on a large amount of un-\nsupervised data using RoBERT (Liu et al., 2019)\ntechnology. This enables X LM-Rbase to better un-\nderstand and capture the semantic and grammatical\nfeatures between different languages. Compared\nto the original XLM, XLM-Rbase has made sev-\neral improvements. Firstly, it introduces a dynamic\nmasking mechanism that allows the model to better\nperceive contextual information. Secondly, XLM-\nRbase emphasizes cross-lingual consistency learn-\ning through adversarial training, enabling better\nalignment and sharing of model parameters. This\nenables XLM-Rbase to provide more accurate rep-\nresentations of texts in cross-lingual tasks. Com-\npared to mBERT, XLM-Rbase employs larger-\nscale pre-training data, covers more languages, and\nincorporates improvements through RoBERTa tech-\nnology. This enables XLM-Rbase to better learn\nand capture the semantic and grammatical features\nbetween different languages, thereby enhancing\nthe model's representation capabilities and perfor-\nmance."}, {"title": "3.3 Whitening Module", "content": "Due to the existence of anisotropy among the vec-\ntors obtained from the initial encoding by XLM-\nRbase, cosine similarity cannot accurately measure\nthe semantic similarity between sentences. There-\nfore, we chose to use whitening to map the origi-\nnal vector space to an isotropic space, where the\nvectors are transformed into vectors in a standard"}, {"title": "orthogonal bases. The principle is as follows:", "content": "Suppose we have a set of sentence vectors S =\n{$s_1, s_2,...,s_n$}, the set of vectors can be trans-\nformed into a set of vectors with isotropy (i.e.,\nzero mean and a covariance matrix of the iden-\ntity matrix) through the following transformation\n\u0160 = {$\u0161_1, \u0161_2, ..., \u0161_n$}.\n$\u0161_i = (x_i \u2212 \u03bc)W$ \nIf we want to make the set \u0160 have a zero mean, we\nneed to:\n$\u03bc = \\frac{1}{n}\\sum_{i=1}^{n} S_i$ \nThe next step is to calculate W. The covariance\nmatrix of S:\n$\u03a3 = \\frac{1}{n}\\sum_{i=1}^{n}(S_i \u2013 \u03bc) (S_i \u2013 \u03bc)$ \nThe covariance matrix of S:\n$\u2211 = WW^T$ \nIf we want to transform\u2211 into the identity matrix\nI, we need to:\n$\u03a3 = WW^T = I$ \nThen:\n$\u03a3 = (W^T)^{-1}W^{-1} = (W^{-1})^TW^{-1}$ \nSince \u2211 is a positive definite symmetric matrix as\nthe covariance matrix, it can be decomposed using\nSingular Value Decomposition (SVD), yielding:\n$\u03a3 = UAU$ \nBy combining equations (6) and (7), we obtain:\n$(W^{-1})^TW^{-1} = UAU = U\\sqrt{A}U\\sqrt{A}U$ \nThen:\n$(W^{-1})^TW^{-1} = (\\sqrt{AU})^T\\sqrt{AU}$ \nTherefore, we can obtain $W^{-1} = \\sqrt{AU}$, and fi-\nnally obtain W as follows:\n$W = UVA^{-1}$"}, {"title": "3.4 Data Filtering", "content": "Our experiments have shown that when selecting\ntraining data for the target language, using a mix-\nture of multiple languages often yields better results\nthan using a single language. The authors of the\nXLM-Rbase paper mentioned that incorporating\nmore languages improves the cross-lingual perfor-\nmance of low-resource languages up to a certain\npoint. Beyond that point, the overall performance\nof both monolingual and cross-lingual benchmarks\nstarts to decline. Additionally, we believe that there\nis interdependence between languages. For exam-\nple, if including text from language A in training\nset to compute whitening parameters leads to a de-\ncrease in the prediction performance for language\nB, we expect that the opposite would hold true as\nwell.\nTherefore, inspired by this insight, we used the\ntext in the target language as the dataset and indi-\nvidually tested the labeled training data provided\nin track A for different languages. For example, if\nthe target language is identified by T, we use the\ntext of T for whitening, and test the performance\non language $Test_A, Test_B, Test_C, Test_D,...$ one\nby one. If the prediction performance of $Test_A$\ndecreases after using T compared to not using\nT (measured by the Spearman correlation (Myers\nand Sirois, 2004) between the gold labels and pre-\ndicted labels obtained using language $Test_A$), then\n$Test_A$ is excluded from target language's training\nset.\nIn the case of the Spanish, using the training set\nwithout data filtering (1,000 each of all data except\nSpanish) resulted in a final spearman coefficient\nof 0.6375; using the training set with data filter-\ning (1000 each of kin and ind) resulted in a final\nspearman coefficient of 0.6886. Although the train-\ning data for about ten languages were reduced, the\nresults were are significantly improved."}, {"title": "4 Experimental Setup", "content": "We use the 12 labeled training data from (Ousid-\nhoum et al., 2024a) as training data and the test\ndata from track C as test data. We observe that the\namount of data for each language is concentrated\naround 1,000, so we take 1,000 as the boundary,\nuse oversampling to make up for less than 1,000,\nand use randomization to take out 1,000 for more\nthan 1,000 to ensure that sentence pairs of different\nsimilarities are involved. In finding the training set\ncombinations for the target languages, we compute"}, {"title": "5 Results", "content": "The official competition used the spearman coeffi-\ncients to evaluate the results, and Table 1 gives the\nresults of the spearman coefficients for both Indone-\nsian (ind) and Spanish (esp) languages throughout\nthe experiment. There is a big difference in the\nmultilingual ability of different model bases. We\nchose XLM-Rbase, which performs better, and we\ncan see that the overall results are improved after\nusing the whitening module to transform the vector\nspace; XLM-Rbase with whitening is better than\nbaseline, and we got a good ranking in track C of\nSemEval-2024 task 1, in which we ranked second\nin esp and third in ind.\nAs can be seen from Table 1, the whitening mod-\nule improves the STR task more significantly, the"}, {"title": "6 Conclusion", "content": "We use XLM-Rbase with whitening and propose\na dataset filtering method that exploits the posi-\ntive correlation of linguistic interactions, achieving\ngood rankings in SemEval-2024 task 1 track C. We\nverifies that whitening performs well on utterance\ncharacterization as well as STR task. Besides, the\nproposed dataset filtering method is more efficient"}, {"title": "and can alleviate the multilingual curse problem in\ncross-language problems to some extent.", "content": "In the future, we will further study this positive\ncorrelation of language interactions, and we hope\nthat this correlation can become more detailed, not\nonly in terms of inter-language correlations but also\nin terms of the domain of the text. We also hope\nthat this correlation can be better utilized in dataset\npreprocessing, not only to eliminate poorly per-\nforming languages but to further improve the com-\nbination of datasets that can be directly selected to\ncorrespond to the optimal solution."}]}