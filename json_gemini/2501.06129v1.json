{"title": "Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational A\u0399", "authors": ["Yuya Asano", "Sabit Hassan", "Paras Sharma", "Anthony Sicilia", "Katherine Atwell", "Diane Litman", "Malihe Alikhani"], "abstract": "General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives.", "sections": [{"title": "1 Introduction and Related Work", "content": "Although domain-agnostic automatic speech recognition (ASR) models are improving, advanced models still make errors, hindering fluent dialogues with conversational AI (Radford et al., 2023). Most prior work on context-based ASR error correction needs prior user interaction data to model errors statistically (Sarma and Palmer, 2004; Jonson, 2006; Shivakumar et al., 2019; Liu et al., 2019; Ponnusamy et al., 2022; L\u00f3pez-C\u00f3zar and Callejas, 2008; Weng et al., 2020; Zhou et al., 2023), make natural language understanding robust to errors (Gupta et al., 2019), or learn user preferences (Raghuvanshi et al., 2019; Cho et al., 2021; Bi\u015b et al., 2022). These data are not always available, especially when creating AI in a new domain.\nOne proposed solution is to use tasks that goal-oriented conversational AI can accomplish as a primary source of context. It includes restricting ASR hypotheses to the vocabularies that a natural language understanding module can parse (He and Young, 2003; Whittaker and Attwater, 1995) and remembering named entities and retrieving the most probable one based on textual and phonetic similarities and n-best ASR hypotheses (Georgila et al., 2003; Raghuvanshi et al., 2019; Wang et al., 2021; Bekal et al., 2021). However, it overlooks three types of flexibility in natural language in Table 1: lexical and syntactic variations for the same goal, non-essential modifiers inserted in the middle of task phrases, and the omission of words that are not needed to specify a goal given a context. An ASR system could also omit some words in error.\nTo tackle these challenges, we extend the work on the use of tasks as context with\n1. better ranking strategies robust to insertions and omissions of tokens,\n2. reduction of the size of the context using the dialogue state and augmentation with partial matches (for token omissions), and\n3. offline augmentation of tasks with a large language model (LLM) (for lexical and syntactic varieties).\nThese ranking and augmentation strategies are system-agnostic and generalizable to any tasks if a list of the supported tasks is available. They are lightweight and are not affected by the latency of LLMs. The overview of our method is in Figure 1.\nWe deploy our method on Amazon Alexa and test its effectiveness with real-world users. It improved recall and F1 of correction from a baseline while keeping fair precision and false positive rate (FPR) in offline evaluation. In online evaluation, our ratings have increased by .8-1 point out of 5 when our method corrected errors properly and did not decrease even with false positives."}, {"title": "2 Problem Description and Data", "content": "We define the goal of ASR correction for goal-oriented conversational AI as to pass the corrected information from erroneous ASR transcripts to the following modules (in our case, dialogue state tracking and natural language understanding in Figure 1) so that AI can take the correct action as requested by a user. This definition is similar to call routing, which \"is concerned with determining a caller's task\" (Williams and Witt, 2004).\nWe implemented our system through Alexa Skills,\u00b9 which allows third-party developers to create conversational AI on Alexa. Our Alexa Skill assists with cooking and home improvement tasks. After our system's introduction (cf. Appendix A),"}, {"title": "3 Proposed Method", "content": "Our method starts with identifying likely user responses that can be used as context for correction and deciding when to trigger the correction based on dialogue states (Section 3.1). Next, we re-rank n-best ASR hypotheses using this context (Section 3.2). If none is plausible, we rank the context using phonetic information to fix errors in the best ASR hypothesis (Section 3.3). To handle lexical and syntactic varieties and word omissions, we augment the context and task list for correction (Section 3.4). An overview of our pipeline is in Figure 1."}, {"title": "3.1 Dialogue states as context", "content": "The first step of our method is identifying a context to correct ASR errors. Dialogue states help predict what a user might say next. For example, users often select an option after seeing search results (cf. Figure 2) or pick a system-suggested query at the start (cf. Appendix A). Therefore, we define a narrow context as a list of likely user responses based on the current dialogue state. Our ranking method matches a user's utterance to this context. In our case, a narrow context is the options presented by the system in the previous turn or the voice commands to execute tasks.\nHowever, users may not always respond within the scope of a narrow context, making it difficult for the ranking methods in sections 3.2 and 3.3 to find matches, especially when starting a new task. In this case, we retrieve context from all tasks using an indexed search for the ASR hypotheses to get lexically overlapping tasks. We measure the cosine similarity between the hypotheses and the search results using Sentence-BERT (Reimers and Gurevych, 2019) embeddings and retain only highly similar results (see sections 3.2 and 3.3 for the actual thresholds) as context. Then, we rerun the methods with the new context.\nTo reduce false positives, our correction is triggered only in specific dialogue states. It activates when a narrow context exists (the two middle states in Figure 2 and at the beginning of a conversation) or the system's intent classifier predicts that the user intends to search for or select a task and such intent is probable in the current state (the two left states in Figure 2). Our method is not triggered when, for example, the user tries to ask a question about the task they are executing, or the system asks a question to them. Appendix C provides details on deriving a narrow context and determining when to trigger the method."}, {"title": "3.2 Re-ranking n-best ASR hypotheses", "content": "We re-rank ASR hypotheses by the lexical and semantic similarity to the context. If a narrow context exists, we score hypotheses from best to worst by fuzzy matching (Chaudhuri et al., 2003) with the context. We stop if we find one exceeding a threshold. If no match is found, we run an indexed search for each ASR hypothesis and assign the largest cosine similarity as the score of the hypothesis. The highest-scored hypothesis is chosen as a correction. If the original best hypothesis is picked, we interpret this as no correction needed.\nRe-ranking n-best hypotheses handles modifier insertions. Suppose \"how to care for outdoor plants\" is transcribed as \u201chow to camper for outdoor plants\" and that n-best hypotheses have the correct transcription. \"How to camper for outdoor plants\u201d does not return a good search result since there is no such task. However, even if \"how to care for outdoor plants\" is not on the list, our method can propose it as a correction because its search result, \"take care plant,\" exceeds the threshold."}, {"title": "3.3 Ranking context", "content": "Since n-best ASR hypotheses don't always include the correct transcripts, re-ranking them can fail. We solve this by ranking context based on phonetic similarity to the best ASR hypothesis measured by the longest common subsequences (LCSs) between the phoneme sequences of the context and the best ASR hypothesis. We choose the option from the context whose LCS covers a certain portion of the option and is not too scattered as a candidate for correction. Then, we replace the tokens in the best hypothesis covered by the LCS with those in the candidate and accept the new hypothesis as the correction. The full algorithm is in Appendix D.\nLCSs help handle phrases inserted or removed in the middle of a task. Suppose there is a task called \"how to fix a bathroom faucet\" and that ASR transcribes \"how can I fix a leaky bathroom faucet\" as \"how can I fix a leaky bathroom for sit.\u201d The LCS between the best ASR hypothesis and the context comes from \"fix a bathroom for sit\" for the hypothesis. Therefore, we can skip the word \"leaky\u201d in the original hypothesis to replace \u201cfor sit\u201d with \"faucet\" from the context to get the correct transcript. The same mechanism works when there is a task called \"how to fix a leaky bathroom faucet\" and ASR transcribes \"how can I fix a bathroom faucet\" as \"how can I fix a bathroom for sit.\""}, {"title": "3.4 Augmenting context", "content": "Ranking n-best ASR hypotheses and context depends on the richness of the context to better deal with erroneous hypotheses and lexical and syntactic variety of user utterances. Therefore, we augment the context used for ranking in two ways.\nFirst, we expand the search space by distilling lexical and syntactic variations from GPT to create an offline dataset, allowing ASR correction to consider phrases not in the list of available tasks. To avoid high costs and repetitive data, we cluster the task list and generate variations only for cluster centroids. This allows for a collection of the most representative yet diverse samples (Hassan and Alikhani, 2023). Then, we index the augmented dataset for an indexed search. The details of the implementation can be found in Appendix E, and the evaluation is in Appendix F.\nSecond, we add partial matches to a narrow context if they uniquely identify one option. For example, if the search results are \u201chow to care for indoor plants,\" \"how to water indoor plants,\u201d and \u201chow to fertilize indoor plants,\" we suggest \u201chow to water indoor plants\" as long as ASR correctly transcribes \"water.\" Partial matches also handle the omission of unneeded words."}, {"title": "4 Experiments", "content": "4.1 Post-hoc analysis for search & selection\nWe did a post-hoc analysis on the user utterances to search or select home improvement (wikiHow) tasks because they have more linguistic variations than recipes. We extracted these utterances from the annotated dataset in section 2. 91.6% of this subset had the search intent. Since the goal of our ASR correction is to identify the desired intent (cf. Section 2), a proposed correction was considered correct if it matched the manually corrected transcript or the correct option. We evaluated the methods with Precision@1, Recall@1, and F1-score@1 since our method does not rank beyond the first option, and the scores of our two ranking strategies have different meanings."}, {"title": "4.1.1 Baseline", "content": "We use the method by Raghuvanshi et al. (2019) as a baseline because it does not require prior user in-teractions and its source code is publicly available. This method matches potentially erroneous ASR output with named entities and their synonyms stored in a database, based on textual and phonetic similarities aided by n-best ASR hypotheses. We treated each wikiHow article title in the private dataset as a named entity. We also experimented with adding alternative titles from GPT (the same as section 3.4) as synonyms of named entities."}, {"title": "4.1.2 Results", "content": "We examined the search and selection intents separately and combined. The results are summarized in Table 2. Our method has 36% higher recall and 17% higher F1 than the baseline overall. Although its overall precision was 19% lower and its FPR was 3% higher, our method outperformed the baseline in every metric except for FPR in the search intent when broken down by intent. This is because over 80% of the utterances our method made corrections had the search intent, while the baseline had fewer than 50%. Thus, combined precision favored the search intent (harder) for our method and the selection intent (easier) for the baseline. GPT augmentation worsened the baseline's precision and recall for the search intent."}, {"title": "5 Discussion and Conclusion", "content": "Our approach corrects ASR errors in general terms in goal-oriented dialogues by leveraging dialogue context more flexibly than existing methods, which focus on named entities. Context is taken from tasks and narrowed down by the dialogue state. We augment a narrow context via partial matches and tasks with GPT to handle linguistic variability. We re-rank n-best ASR hypotheses based on the semantic similarity with context and rank context by phonetic correspondence with ASR hypotheses. Unlike existing methods, our method requires no prior user interaction data. Experiments show it improves recall and F1 from the baseline while keeping reasonable precision and FPR. In a real-world deployment, it improved user ratings when correct and had minimal impact when incorrect.\nThough evaluated in home improvement and cooking, our approach can be applied to any domain or voice commands, provided there is a comprehensive task list for LLM augmentation. Although it assumes correct intent recognition, it can also enhance intent recognition if textual cues for intents can be extracted from training data. Moreover, although designed for goal-oriented dialogues where user speech is often constrained by their goals, our method can also be adapted to open chat by, for example, generating a narrow context through simulations with an LLM. Future work can evaluate it in more domains and dialogue states.\nThe drawback of our method is an increase in FPR when correcting errors using all tasks. To reduce it, we could consider phoneme similarities. Suppose ASR transcribed \u201chouse\u201d as \u201chorse\u201d when available options include \"house\u201d and \u201chence.\" g2pE converts \u201chouse\u201d to [\u201cHH,\u201d \u201cAW1,\u201d \u201cS\u201d], \"horse\" to [\"HH,\u201d \u201cAO1,\u201d \u201cR,\u201d \u201cS\u201d], and \u201chence\" to [\u201cHH,\u201d \u201c\u0415\u041d1,\u201d \u201cN,\u201d \u201cS\u201d]. Since LCSs do not consider similarities of phonemes, both \u201chouse\" and \"hence\" have the same length of LCS as \u201chorse.\u201d However, intuitively, \u201cAO1\u201d should sound closer to \"AW1\" than \"EH1,\" so \"horse\" should be corrected to \"house.\" This could be potentially solved by using phoneme embedding to weigh the differences among phonemes (Fang et al., 2020) and solving the heaviest LCS problem (Jacobson and Vo, 1992). This could be also addressed by an audio tokenizer and the subsequent embedding in a pre-trained speech LLM (Borsos et al., 2023)."}, {"title": "Limitations", "content": "We acknowledge the limitations of the evaluation of our method. First, while public benchmarks focusing on ASR errors exist, we evaluate our method only on our private data instead because those public benchmarks typically do not contain the broader context of functional conversational A\u0399. We argue that not evaluating our method against public benchmarks does not affect the validity of our approach in practical applications, as the primary objective of ASR error correction lies in enhancing the performance of downstream conversational AI. Although Amazon's policy prevents this paper from reporting some statistics in our dataset (e.g., the number of sampled dialogues), listing real examples (we modify the user examples listed in this paper but preserve actual errors from Alexa's ASR), and releasing our code, we provide the necessary details so that the industry community can easily benchmark our method in various domains, including novel areas that do not have any prior user interaction data, when developing a conversational AI. This paper should serve as a cornerstone of the advancement of ASR correction in the presence of linguistic flexibility in practical real-world conversational AI applications.\nSecond, we assess our algorithm's performance using precision, recall, and F1-score only at rank one. As our algorithm stops ranking upon identifying a suitable candidate to prevent over-correction, it restricts the calculation of Precision@N, Recall@N, and F1-score@N for N >= 2. In addition, Precision@N, Recall@N, and F1-score@N will be always 1 for the selection intent and for N >= 3 because we present only three options to users. That being said, we intend to update the algorithm by adding multiple candidate considerations and integrating varied selection criteria for candidate options in future iterations.\nWe are also aware of a more advanced grapheme-to-phoneme conversion model based on a transformer. We decided not to use it due to the restriction on latency imposed on Alexa applications. The discussion of the performance and latency of our method compared to existing deep learning approaches such as Bekal et al. (2021) and Mai and Carson-Berndsen (2024) and the online use of LLMs (Chen et al., 2023) is left for future work."}, {"title": "Ethical Considerations", "content": "ASR is known to be biased against dialects, females, and racial minorities (Tatman, 2017; Tatman and Kasten, 2017), possibly due to a lack of their representation in training datasets. ASR struggles with transcribing low-resource languages, too (Zellou and Lahrouchi, 2024). Our work could potentially aid the experience of marginalized populations with conversational AI as it does not rely on any data other than a list of tasks/commands. However, this might not be the case because the effectiveness of our method depends on the quality of ASR, namely n-best hypotheses and phonetic information, and augmentation by LLMs, which also have poor performance on low-resource languages (Hangya et al., 2022). A closer investigation of how well our method fills a gap between marginalized populations and white males speaking general English is needed."}, {"title": "A Our system's preamble", "content": "When a user launches our system, it says something like the following:\nHi, this is <our system's name>. Welcome! I can help with a task you choose. You can ask me for things like recipes for cookies, how to fix a faucet, or how to make origami.\nIn this example, the suggestions from the system are \"cookies,\" \"how to fix a faucet,\" and \"how to make origami.\""}, {"title": "B Example dialogue", "content": ""}, {"title": "C Details of our dialogue state tracking", "content": "To track the current dialogue state, we store numerous values from the current and previous turn(s). These include whether a question is just asked by the system, whether there exists an unanswered question from the previous turn, search results (if there are any) from the previous turn, the task (if any) the user is currently working on, and values extracted from our natural language understanding module indicating the user's intent and dialogue slots for the current turn. These variables allow the system to, at any given turn, track the current dialogue stage (cf. Figure 2) and access any relevant contextual information.\nWe also implemented intent recognition and slot filling enhanced by the rich contextual information coming from our state tracking. For example, it was used to interpret the results of lightweight classifiers (e.g., rule-based classifiers for the intent to end the conversation and for fine-grained intents that occur at specific points within a task, such as the intent to go to the next step of a task). Database-specific information, including frequently used verbs in our datasets, was collected and used to further inform our slot-filling algorithm."}, {"title": "D Pseudocode for ranking context", "content": ""}, {"title": "E Detailed implementation of GPT augmentation", "content": "Algorithm 2 summarizes the process of our index creation. We prepared a public wikiHow dataset (Koupaee and Wang, 2018), which contains 230K instances of related tasks for augmentation, in addition to our private list of 50K available wikiHow tasks to broaden the range of potential correction scenarios and created a mapping between these instances ((X, Y) in Algorithm 2). We used MiniLM (Wang et al., 2020) for obtaining embeddings, Kmeans with default scikit-learn parameters for clustering, and GPT-3.5-turbo for generating variations. We used 20K clusters with k=8, resulting in 160K additional variations."}, {"title": "F Evaluation of GPT augmentation", "content": ""}, {"title": "G Examples of failure cases", "content": "Table 10 shows the errors in the search intent that were not corrected by our method. Table 11 and Table 12 demonstrate false positives because of and not because of a narrow context, respectively."}]}