{"title": "AUTOBENCH-V: CAN LARGE VISION-LANGUAGE MODELS BENCHMARK THEMSELVES?", "authors": ["Han Bao", "Yue Huang", "Yanbo Wang", "Jiayi Ye", "Xiangqi Wang", "Xiuying Chen", "Mohamed Elhoseiny", "Xiangliang Zhang"], "abstract": "Large Vision-Language Models (LVLMs) have become essential for advancing the integration of visual and linguistic information, facilitating a wide range of complex applications and tasks. However, the evaluation of LVLMs presents significant challenges as the evaluation benchmark always demands lots of human cost for its construction, and remains static, lacking flexibility once constructed. Even though automatic evaluation has been explored in textual modality, the visual modality remains under-explored. As a result, in this work, we address a question: \"Can LVLMs serve as a path to automatic benchmarking?\". We introduce AUTOBENCH-V, an automated framework for serving evaluation on demand, i.e., benchmarking LVLMs based on specific aspects of model capability. Upon receiving an evaluation capability, AUTOBENCH-V leverages text-to-image models to generate relevant image samples and then utilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing the evaluation process efficiently and flexibly. Through an extensive evaluation of seven popular LVLMs across five demanded user inputs (i.e., evaluation capabilities), the framework shows effectiveness and reliability. We observe the following: (1) Our constructed benchmark accurately reflects varying task difficulties; (2) As task difficulty rises, the performance gap between models widens; (3) While models exhibit strong performance in abstract level understanding, they underperform in details reasoning tasks; and (4) Constructing a dataset with varying levels of difficulties is critical for a comprehensive and exhaustive evaluation. Overall, AUTOBENCH-V not only successfully utilizes LVLMs for automated benchmarking but also reveals that LVLMs as judges have significant potential in various domains.", "sections": [{"title": "1 INTRODUCTION", "content": "The flourishing of Large Language Models (LLMs) (Touvron et al., 2023; Achiam et al., 2023; Liu et al., 2024a; Anthropic, 2024) has paved the way for significant advancements in the various downstream applications (Vaswani et al., 2023; Huang et al., 2023; Liu et al., 2023c). As the capabilities of LLMs grew, researchers began to explore the integration of visual information understanding capabilities into LLMs, giving rise to the development of Large Vision-Language models (LVLMs) (Achiam et al., 2023; Liu et al., 2024c). These models are trained on extensive paired image-text datasets, enabling them to perform sophisticated multimodal reasoning by effectively integrating visual and textual information (Zou et al., 2023; Ghandi et al., 2023; Karras et al., 2019; Agrawal et al., 2016; Chen et al., 2024a).\nWith the widespread adoption of LVLMs, evaluating these models has become increasingly important, for understanding their limitations and reliability better. Recent research (Xu et al., 2023; Liu et al., 2023b; Ying et al., 2024; Li et al., 2023b;a; Yin et al., 2023) emphasize the urgent need for comprehensive and sophisticated evaluation standards that accurately assess LVLMs' abilities across various modalities. Various benchmarks are aiming to evaluate a range of capabilities of LVLMs including 3D understanding (Yin et al., 2023), perception and cognition capacity (Liu et al., 2023b;"}, {"title": "2 RELATED WORKS", "content": "Benchmark for LVLMs. The emergence of the LVLMs greatly promoted the development of the multimodal model, demonstrating exceptional progress in their multimodal perception and reasoning capabilities. This makes the past, focused on isolated task performance benchmarks (Karpathy & Fei-Fei, 2015; Agrawal et al., 2016) insufficient to provide a comprehensive evaluation. Subsequent studies have introduced benchmarks for assessing LVLMs across a range of multimodal tasks (Goyal et al., 2017; Lin et al., 2015; Russakovsky et al., 2015). However, these benchmarks often fall short in providing fine-grained assessments of abilities and robust evaluation metrics. Hence, recent works (Liu et al., 2023b; Ying et al., 2024; Fu et al., 2024; Yu et al., 2023; 2024) highlight the critical need for developing advanced, comprehensive benchmarks to more accurately assess LVLMs' multimodal understanding and reasoning capabilities. However, these benchmarks still have different kinds of limitations. For example, LVLM-eHub (Xu et al., 2023) and LAMM (Yin et al., 2023) have utilized several classical datasets that are widely recognized but not sufficiently novel for current advancements, overlooking the possibility of data leakage during LVLM training. Hence, MMStar (Chen et al., 2024b) aims to solve the unnecessity of visual content and unintentional data leakage that exists in LVLM training via constructing an elite vision-indispensable dataset.\nCompared to previous work, AUTOBENCH-V not only automates the entire benchmarking process for LVLMs - significantly reducing human workload and minimizing subjective biases - but also scales up and customizes the evaluation process to address fine-grained user needs.\nAutomatic benchmarks. The significant early advancements in LLMs have driven the development of various benchmarks designed to automate evaluation processes. For example, LMExamQA (Bai et al., 2023b) employs the concept of a Language-Model-as-an-Examiner to create a comprehensive and scalable evaluation framework. In addition, DYVAL (Zhu et al., 2024a) and DYVAL2 (Zhu et al., 2024b) both highlight the importance of dynamic assessment, with DYVAL focusing on reasoning tasks and DYVAL2 adopting a broader psychometric approach. AutoBencher (Li et al., 2024) automates the generation of novel, challenging, and salient datasets for evaluating LLMs, further expanding the scope of automated benchmarking. Other efforts, such as UNIGEN (Wu et al.,"}, {"title": "3 AUTOBENCH-V", "content": "In this section, we introduce AUTOBENCH-V, a framework designed for automating the process of benchmarking LVLMs, empowered by a LVLM M and a text-to-image model Md. As shown in Figure 3, AUTOBENCH-V consists of four modules (Xiao et al., 2024a): user-oriented aspect generation, guided description generation, image generation by self-validation, and test case generation & evaluation."}, {"title": "3.1 USER-ORIENTED ASPECT GENERATION", "content": "User input. The user input can specify an evaluation target focused on certain aspects of LVLMs' capability. AUTOBENCH-V covers the following key evaluation aspects, which are the most crucial for assessing the capabilities of LVLMs: Basic Understanding, Spatial Understanding (Li et al., 2023b), Semantic Understanding (Meng et al., 2024), Reasoning Capacity (Liu et al., 2023b), and Atmospheric Understanding (Geetha et al., 2024). Notably, the user input is not limited to the above kinds, and can be customized as needed.\nHierarchical aspect generation. For each user input, we derive a set of aspects representing specific capability items. For example, as shown in Figure 1, contextual comprehension is an aspect under Basic Understanding. However, directly generating aspects from user input can lead to excessive repetition, reducing both diversity and reliability by overlapping in semantics and repeatedly evaluating the same capability. To mitigate this, we propose hierarchical aspect generation inspired by the previous study (Qin et al., 2023) to constrain the aspect generation process. Formally, given the user input q, we first generate n general aspects {A^{(g)}_1, A^{(g)}_2, ..., A^{(g)}_n} by M, which can be formulated as: {A^{(g)}_1, A^{(g)}_2, ..., A^{(g)}_n} = M_\\nu(q). These general aspects represent high-level evaluation dimensions based on q. Next, for each general aspect A^{(g)}_i, we further generate m fine-grained aspects {A^{(f)}_{i1}, A^{(f)}_{i2}, ..., A^{(f)}_{im}}, where each fine-grained aspect provides more specific criteria related to the general aspect. The fine-grained aspects are also generated by M and depend on both the user input q and the corresponding general aspect A^{(g)}_i. The fine-grained aspect of generation can be represented as {A^{(f)}_{i1}, A^{(f)}_{i2}, ..., A^{(f)}_{im}} = M_\\nu(q, A^{(g)}_i). Thus, the hierarchical aspect generation yields"}, {"title": "3.2 GUIDED DESCRIPTION GENERATION", "content": "Guidelines formulation. To avoid the generation of irrelevant, abstract, or vague details that could lead to discrepancies in image descriptions, we introduce a guideline generation step (Zhao et al., 2024; Viswanathan et al., 2023). Before generating image descriptions, the LVLM model M_\\nu formulates a guideline D_{ij} (e.g., in Background vs Foreground aspect, it is essential to distinguish between the elements present in the background and those in the foreground) for each fine-grained aspect A^{(f)}_{ij}. This guideline acts as a guideline for M, ensuring that the generated descriptions are coherent, clear, and specific to the fine-grained aspects under evaluation. The process can be expressed as D_{ij} = M_\\Theta(A^{(f)}_{ij}). The generated guideline D_{ij} is then utilized to guide the subsequent image description T_{ij}.\nImage description with difficulty grading. To enable a more comprehensive evaluation, we introduce a difficulty-grading mechanism for the image descriptions, which includes the evaluation cases from different difficulties. This is achieved by classifying the generated image descriptions into three difficulty levels: easy, medium, and hard. We show the examples across different difficulties in Figure 4. The difficulty level d is determined by key factors such as background complexity, element relationships, and the intricacy of textures. The generation of w image descriptions {T^{(1)}_{ij}, T^{(2)}_{ij},..., T^{(w)}_{ij}} for A^{(f)}_{ij} at a specific difficulty level d can be defined as: \\cup^{w}_{u=1}{T^{(u)}_{ij}} = M(q, A^{(f)}_{ij}, D_{ij}, d), where d \\in {easy, medium, hard}. This grading strategy allows for a nuanced understanding of the model's capabilities across a range of challenges with details provided in Appendix C.\nDiverse description generation strategy. A key challenge when generating image descriptions at the same difficulty level is minimizing repetitive elements and backgrounds, which can reduce the diversity and generalization of the evaluation. For example, given a user input q related to spatial understanding, the model M might tend to produce descriptions centered around urban landscapes, potentially compromising the variety of test cases. To address this, we introduce a description optimization strategy using a semantic graph (Quillian, 1966) to enhance the diversity of image prompts generated by M_\\nu, with significant results referred to in Appendix Figure 10. For a visualization of specific words, see Appendix Figure 11 and Figure 12. The process is iterative, and during the e'th iteration of prompt generation, a topic word t_e and a set of |c| related keywords K_e = {k_{e1}, k_{e2},..., k_{ec}} are selected. These keywords are added as nodes to the semantic graph G, where nodes are connected by edges representing semantic relationships between them."}, {"title": "3.3 IMAGE GENERATION BY SELF-VALIDATION", "content": "Self-validation. The image descriptions T_{ij} and their corresponding aspects A^{(f)}_{ij} are subsequently provided to the text-to-image model for image generation. At this stage, a potential issue is the possibility of generated images I not aligning with the descriptions, due to hallucinations inherent in the text-to-image model (Lee et al., 2023). To tackle this issue, drawing inspiration from TIFA (Hu et al., 2023), we employed a self-validation process to evaluate the consistency of images with their descriptions via VQA.\nIn the self-validation process F, for each image I, based on its image description, M_\\Theta is prompted to generate a set of simple questions \\Omega_i = {\\omega_1, \\omega_2,..., \\omega_p} (e.g., \u201cIs there a wooden chair in the image?\u201d), where p denotes the question number to evaluate the alignment. The function F takes the image I, its description T_{ij}, and the set of questions \\Omega_i as inputs and outputs an alignment score S_a, which is calculated as the ratio of correctly answered questions to the total number of questions:\nS_a = F(I, T_{ij}, \\Omega_i)\nWe set a threshold \\zeta, where: (i) If S_a < \\zeta, the image I will be reworked in line with the description until it meets the required standard; (ii) If \\zeta \\leq S_a < 1, the image meets the basic criteria but contains an error E_d, which will be documented; and (iii) If S_a = 1, the image is considered to fully align with the description and is deemed acceptable."}, {"title": "3.4 TEST CASE GENERATION & EVALUATION", "content": "Q&A generation with error control. To enhance the accuracy of question generation, particularly when addressing potential flaws in images, we propose error control. Despite thorough self-validation, it's not guaranteed that every image will be flawless. Furthermore, when generating problems, we aim to avoid introducing biases stemming from the visual capabilities of examiner LVLM (Zhang et al., 2024b). Therefore, when generating questions, we will only include the image description T_{ij} and any identified defects \\xi in the input to the examiner M_\\nu. The function Mv generates the question Q based on the image description and errors:\nQ_{ij} = M_\\nu(T_{ij}, \\xi).\nThis will enable the creation of a diverse set of questions, along with reference answers, that specifically target the defective elements. For each image, we will provide a related question Q_{ij}^a (e.g., multiple-choice or true/false). These questions, along with the accompanying images, will be presented to the LVLMs under evaluation for their response.\nEvaluation. The response P^d_{ij} from the tested LVLMs was compared to the reference answer A^d_{ij} to determine accuracy. If P^d_{ij} matched A^d_{ij}, it was marked correct (Acc = 1); otherwise, it was marked incorrect (Acc = 0). The overall accuracy Acc_{total} was calculated as the average accuracy over all N questions."}, {"title": "4 EXPERIMENT", "content": "In this section, we evaluate seven of the latest models using AUTOBENCH-V and perform human evaluations to validate our experimental findings. First, we demonstrate how AUTOBENCH-V significantly reduces potential answer leakage and self-enhancement bias, as evidenced by the experimental results in Figure 5. Next, based on Figure 6, Table 1, Table 2, and Table 3, we analyze the impact of various evaluation factors (e.g., user input) and question difficulty on model performance, which reveals several insightful findings. We then present the model rankings across five user input categories with varying difficulty levels in Figure 8, followed by a discussion on human evaluations regarding alignment during AUTOBENCH-V's generation process. Lastly, we investigate position bias in the evaluation process, as illustrated in Figure 9."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Selected models. In evaluating LVLMs, we selected seven representative models: GPT-40, GPT-40 mini (Achiam et al., 2023), Claude-3.5-Sonnet, Claude-3-Haiku (Anthropic, 2024), Gemini-1.5-Flash (DeepMind, 2024), GLM-4v (GLM et al., 2024), and the open-source Qwen2-VL (Bai et al., 2023a), detailed in Table 7. These advanced models exhibit exceptional image understanding. Some well-known open-source models, such as Llava-1.6 (Liu et al., 2024a) and MiniGPT-4 (Zhu et al., 2023), were tested and found to perform poorly. Additionally, their capabilities differ significantly from other models, so they are not discussed in our evaluation. We chose GPT-40 as the examiner model for generating image descriptions, questions, and answers due to its strong overall performance. The descriptions were then passed to Flux-pro (blackforestlabs, 2024), a text-to-image model known for outstanding image generation. We also experimented with other text-to-image models (Rombach et al., 2022; Podell et al., 2023; Betker et al., 2023). However, their performance was suboptimal.This combination enables effective automated generation of image-based questions, crucial for the evaluation process (Ying et al., 2024; Fu et al., 2024; Xu et al., 2023).\nExperimental setting. We set n = 4 for the number of general aspects and m = 6 for the number of fine-grained aspects, as this configuration yields the highest diversity in the generated aspects as illustrated in Table 1, allowing for a broader range of scenes and elements. We set w = 10, namely 10 pictures for each fine-grained aspect. Therefore, we evaluate 720 images for each user input"}, {"title": "4.2 EXAMINER PRIORITY", "content": "To mitigate the potential for answer leakage associated with self-enhancement bias (e.g., when the model being evaluated is also utilized for generating the evaluation cases) in the examiner LLM, we enhance the fairness of the assessment by having AUTOBENCH-V generate questions from image descriptions rather than directly from images. This approach separates the visual information from the generation process, reduceing the risk of self-enhancement bias (Ye et al., 2024) that could occur if questions were derived from the examiner model's (GPT-40) visual capabilities, which might cause unfair comparison. By employing only textual descriptions for generation, we eliminate the influence of GPT-40's specific visual processing abilities, thereby ensuring a more equitable evaluation.\nTo validate the fairness of this method, we conducted an experiment in which models were presented with image descriptions alongside corresponding questions while avoiding direct access to the images. As demonstrated in Figure 5, the results revealed consistent performance across all models, with minimal variance (0.4% for easy questions and 2.4% for hard questions). This consistency suggests that, in the absence of visual input, models' textual understanding ability is almost equal, which means the benchmark effectively assesses visual comprehension and does not show obvious bias towards the examiner LVLM (i.e., GPT-40)."}, {"title": "4.3 MAIN RESULTS", "content": "As shown in Figure 6, through the evaluation of various models on AUTOBENCH-V, we can observe several findings that can bring insights for future work. More detailed results are in Table 6.\nModel performance decreases as task difficulty increases, with GPT-40 showing the strongest average performance across tasks. This trend is consistent across all models, with scores steadily declining as the difficulty increases from easy to hard, as shown in Figure 13. For example, GPT-40's average score drops from 90.43% at the easy level to 75.02% at the hard level. Despite the overall decline, GPT-40 maintains its leading position across all difficulty levels. Additionally, the result highlights that the most notable shift occurs between easy and medium. Although a few samples show improved scores with increased difficulty mildly, the majority trend still experiences a decline, reinforcing the validity of our difficulty grading mechanism."}, {"title": "4.4 MODEL RANK OVERVIEW", "content": "Figure 8 reveals distinct performance patterns among different models under various difficulty. Notably, models like GPT-40, while not exhibiting a significant advantage in simpler tasks, demonstrate outstanding performance in more challenging scenarios (e.g.,hard questions). Conversely, models such as GLM-4v perform well on easier tasks but show dimin-"}, {"title": "4.5 HUMAN EVALUATION", "content": "We conducted human evaluations in two aspects: the effectiveness of guided description generation and the alignment between questions and reference answers. See the Appendix D for details on the human evaluation. We ultimately represented the results of the human evaluation using the alignment rate (the proportion of aligned samples out of the total).\nGuided description generation. We developed description generation guidelines for each fine-grained aspect to reduce vagueness in image descriptions, ensuring better alignment with themes and preventing discrepancies. A human evaluation showed that these guidelines significantly improved question-answer alignment, especially in more challenging tasks, as shown in Table 4."}, {"title": "4.6 POSITION BIAS", "content": "Since the reference answers generated by LLMs tend to cluster around option A, we manually set the correct options to be evenly distributed. To investigate the necessity of this approach, we conducted experiments to examine potential position bias (Zheng et al., 2023). We evaluated scenarios where all correct answers were placed in either options A and D, comparing the resulting scores with the evenly distributed case (i.e., 25% for each option), as shown in Figure 9. The deviation rate was calculated using the following formula: R = S_X - S_U, where S_X is the model score for condition X (either A or D), and S_U is the score for the scenario when options are evenly distributed.\nThe position bias becomes more evident with increasing question difficulty. For instance, at the hard level, GLM-4V showed a significant bias, with deviation rates of R_A = -19% and R_D = -8%, suggesting a notable bias when correct answers were concentrated in options A or D, compared to the uniform distribution scenario. Thus, our approach of manually setting an even distribution of answers to avoid position bias is justified and necessary."}, {"title": "5 CONCLUSION", "content": "In this work, we introduce AUTOBENCH-V, a fully automated framework designed for benchmarking LVLMs. The framework integrates a series of innovative modules that ensure diversity and reliability in dataset generation, as well as impartiality in model evaluation. Through extensive experiments, we have demonstrated the robustness and unbiased nature of the evaluation process facilitated by AUTOBENCH-V. The insights gleaned from our research provide a solid foundation for future investigations in this field."}, {"title": "A DETAILS OF EXPERIMENT SETTING", "content": "Model Selection. The details of models selected in our experiments are shown in Table 7.\nComputing Resource. For our experiments, all open-source vision-language model inferences were performed locally using NVIDIA GeForce RTX 4090 GPU with 24GB VRAM.\nAlignment Evaluation. Inspired by tifa (Hu et al., 2023), we generated consistency tests for images across 12 aspects:object, human, animal, food, activity, attribute, counting, color, material, spatial, location, shape, other. For details on the score distribution of the consistency tests without threshold, please refer to Table 5."}, {"title": "B DETAILS OF USER INPUT", "content": "In this section, we provide a comprehensive overview of the levels at which we categorize user inputs based on linguistic aspects. Our goal is to offer a comprehensive and broad representation of user requirements for LVLMs. However, as it is challenging to exhaustively cover every aspect, we base our categorization on aspects derived from the literature (Li et al., 2023b; Meng et al., 2024; Liu et al., 2023b). These aspects are considered representative and comprehensive examples of the capabilities of LVLMs and other aspects like in (Chen et al., 2024b; Xu et al., 2023) can be handled in a similar manner, without requiring additional fine-tuning or adjustments, as our framework is highly extensible, allowing users to propose their own aspects as needed."}, {"title": "B.1 BASIC UNDERSTANDING", "content": "Definition and Goal: Basic Understanding refers to the recognition and identification of individual objects, characters, and scenes within an image. The goal is to accurately detect and label relevant elements, providing a foundation for more advanced tasks such as object tracking and scene interpretation (Wu et al., 2013; Xue et al., 2018).\nRequirement. This task demands the ability to detect specific objects and differentiate between various types of objects. Additionally, it involves understanding the broader context of the scene and identifying real-life settings to enable accurate interpretation of the image's overall content."}, {"title": "B.2 SPATIAL UNDERSTANDING", "content": "Definition and Goal. Spatial Understanding refers to the interpretation of the spatial arrangement and positioning of objects within an image (Cai et al., 2024; Guo et al., 2024). The goal is to comprehend both two-dimensional and three-dimensional relationships, determining which objects are in the foreground or background, assessing their relative sizes and orientations, and understanding how they are positioned within the scene.\nRequirement. This task demands the ability to perceive depth, estimate distances between objects, and analyze how objects interact within the physical space of the image, providing a more accurate understanding of the spatial structure and context."}, {"title": "B.3 SEMANTIC UNDERSTANDING", "content": "Definition and Goal. Semantic Understanding involves interpreting the higher-level meaning and relationships within an image (Meng et al., 2024). The goal is to move beyond simple object identification to understand the roles and interactions between objects, such as recognizing that a person is riding a bike or that two people are engaged in conversation. This level of understanding aims to capture the context and intent behind the scene, identifying how elements relate to each other to form a coherent narrative or message.\nRequirement. This task requires discerning the interactions and relationships between objects, understanding their roles within the scene, and interpreting the overall context to accurately derive the narrative or intended message conveyed by the image."}, {"title": "\u0412.4 \u0410\u0422\u041cOSPHERIC UNDERSTANDING", "content": "Definition and Goal. Atmospheric Understanding focuses on grasping the mood, tone, and emotional ambiance conveyed by an image. The goal is to interpret not just what is depicted or how elements are arranged, but also how the scene feels and the emotional resonance it conveys to the viewer. For instance, an image of children laughing under warm sunlight in a lush park combines their expressions, bright colors, and soft lighting to create a joyful and carefree atmosphere.\nRequirement. This task requires the ability to capture and interpret subtle emotional cues and tonal qualities of the scene, distinguishing the overall mood and emotional impact of the image from more analytical aspects like semantic or spatial understanding."}, {"title": "B.5 REASONING CAPACITY", "content": "Definition and Goal. Reasoning Capacity involves interpreting and analyzing the relationships and logical connections between different elements within an image (Zhou et al., 2024; You et al., 2023). The goal is to infer potential outcomes, understand causal relationships, and make predictions about what might happen next based on visual cues. For example, if a person is holding an umbrella and the sky is dark, reasoning capacity would suggest that it might rain soon. This level also includes understanding abstract relationships, such as social dynamics or the intent behind actions, and making judgments about what is likely or possible given the visual information.\nRequirement. This task requires the ability to analyze logical connections between elements, infer outcomes, and understand causal relationships, as well as to interpret abstract concepts and make predictions based on the visual context."}, {"title": "C DETAILS OF DIFFICULTY GRADING", "content": "This section describes in detail the difficulty levels for the pictures and questions used in prompts respectively. The following is the instruction guiding the examiner model to generate image descriptions and questions of varying difficulty levels."}, {"title": "C.1 IMAGE DESCRIPTION", "content": "Easy difficulty. Generate images with very simple elements, focusing on single, easily recognizable objects placed against a plain or neutral background. The descriptions should be straightforward and unambiguous, e.g., \"a red apple on a white background.\" The focus is on clarity and simplicity, with minimal detail or interaction.\nMedium difficulty. Introduce scenes where the required elements interact with their environment naturally but uncomplicatedly. The setting may include multiple common objects and a familiar context, but the composition remains clear and not overly complex, e.g., \"a cup on a table in a well-lit kitchen.\" The background and context are present but not overwhelming, and there are no intricate details or unusual perspectives.\nHard difficulty. Craft descriptions that involve multiple elements interacting with each other, set in a more complex environment. Use varied perspectives, detailed textures, or lighting conditions that add layers of difficulty, e.g., \"a reflection of a cat looking out of a rain-soaked window, with a cityscape in the background at dusk.\" The focus is on creating a rich and intricate scene that challenges the model's ability to render interactions, depth, and subtleties in lighting and perspective.\nMoreover, we standardized the description of the observer's perspective in the image description to prevent directional issues from causing confusion. For instance, ambiguities could arise in interpreting relative directions such as left and right, as these can vary significantly depending on the observer's viewpoint."}, {"title": "C.2 QUESTION", "content": "Easy difficulty. Focus on questions that require identifying simple, prominent, and explicit details within the image. These questions should be straightforward, relying solely on basic observation without the need for inference or interpretation. For example, you might ask about the color of a specific object, the presence of a single item, or the shape of an easily recognizable feature. The key is to keep the questions direct and simple, ensuring that the answer is obvious and immediately visible in the image.\nMedium difficulty. Design questions that necessitate a moderate level of observation and inference. These questions should involve understanding relationships between elements, recognizing interactions, or identifying less prominent features that are still clear but not immediately obvious. Examples could include questions about the relative position of objects, identifying an action taking place, or understanding the context of a scene. The goal is to require some level of thought beyond basic observation, challenging the model to understand the scene's composition or narrative without being overly complex."}, {"title": "D HUMAN EVALUATION", "content": "D.1 DETAILS OF HUMAN EVALUATION\nThe evaluation was carried out by a panel of five evaluators: three undergraduate students and two PhD students, all possessing professional English skills. Sample annotation screenshots from the human evaluation process are presented in Figure 20 and Figure 21. To ensure unbiased results, each evaluator independently assessed all samples. A sample was considered aligned if it received a majority vote (i.e., more than half of the evaluators agreed on its alignment).\nD.2 HUMAN EVALUATION GUIDELINES\nIn this section, we outline the guidelines followed during human evaluations to ensure reliability and validity.\nFor Description Generation Guideline, the evaluators need to consider the following three points:\n\u25b7 Alignment with Image: The main criterion is how well the generated description reflects the visual content. Descriptions must accurately correspond to the image, avoiding vague or abstract expressions, as well as hallucination (Huang et al., 2024). Each description should provide clear, specific details that align with the image content and the defined fine-grained aspects.\n\u25b7 Specificity and Clarity: Ensure that descriptions are specific, directly related to the image, and free from ambiguous or overly generalized language.\n\u25b7 Relevance to Aspects: Assess whether the description aligns with the corresponding themes and expected content. Descriptions must clearly communicate the intended visual elements and avoid any misalignment between the image and the description.\nFor Question-Answer Alignment, there are two points that the evaluators should consider inspired by the previous study (Liu et al., 2023a):\n\u25b7 Clarity and Accuracy: Each question must be clear, unambiguous, and directly derived from the image. The answers should correspond to observable details or logical inferences from the image, with only one correct answer for each question. There should be no irrelevant or misleading information in the questions or answers.\n\u25b7 Consistency with Image: Verify that both the question and answer are directly based on the image's content. The evaluation should ensure that there is a logical and clear relationship between the visual cues and the generated question-answer pair, particularly for tasks involving higher difficulty."}, {"title": "E ERROR STUDY", "content": "Through extensive experimental analysis, we have categorized the common problems encountered by LVLMs in VQA tasks into two main types: image comprehension errors and image reasoning errors. Regarding the first category, LVLMs often fail to truly understand the details in an image. For instance, in Figure 15, the model failed to notice the kite flying in the upper right corner of the image and incorrectly identified a non-existent action of a squirrel climbing a tree. In Figure 17, the model mistakenly perceived the firefighter as holding the crowbar with only one hand instead of two. In Figure 18, the model failed to recognize that the apple's stem was slightly tilted to the left. These errors demonstrate the model's inadequate comprehension of image details.\nImage reasoning errors occur when models accurately perceive the image content but falter in their reasoning process, leading to incorrect answers. For instance, in Figure 16, the model correctly recognizes that the image does not depict a cheering crowd. However, during subsequent reasoning, it convinces itself otherwise, ultimately selecting option C while neglecting to analyze other choices in its explanation. Figure 19 exemplifies a similar issue: the model correctly identifies that the child is wearing a yellow shirt but after mentioning the red of the kite it erroneously selects B. Red is its final answer. These examples highlight a disconnect between visual perception and logical reasoning in LVLMs, where initial accurate observations can be overridden by flawed deductive processes."}, {"title": "F PROMPT TEMPLATE", "content": "[System]\nPrompt Template: Generate Aspects\nYou are an AI assistant specializing in designing prompts to test Large Vision-Language Models (LVLMS). Your task is to create meticulously {aspect_count} fined-grained aspects that evaluate LVLMs basic understanding of images.\n[Background] Large Vision-Language Models are AI systems capable of understanding and analyzing images. Testing these models across various competencies is crucial for assessing their performance, limitations, and potential biases. The aspects you create will be used to challenge and evaluate LVMs.\n[Instruction]\n1.Basic Understanding: This involves recognizing and identifying individual objects, characters, and scenes within an image. It includes tasks like detecting the presence of specific items (e.g., cars, trees, people), distinguishing between different types of objects, and understanding the general context of the scene (e.g., a park, a city street). The goal is to accurately label all relevant elements in the image, providing a foundation for more advanced analysis.\n2.Come up with 4 general aspects according to the basic understanding.\n3.Then Create 6 fined-grained aspects within the basic understanding for each general aspect, do not go beyond. You can consider the definition of the basic understanding above.\n4.Please list the aspects without using numbered lists.\n5.Let's think step by step.\n[Output Format]\nPlease strictly respond in the following format:\nGeneral Aspect: [Aspect]\nFined-grained Aspect: [Aspect]\nIntroduction: [Introduction]"}]}