{"title": "FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data", "authors": ["Ankur Sinha", "Chaitanya Agarwal", "Pekka Malo"], "abstract": "Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) use deep learning techniques to learn from massive text datasets to solve a variety of tasks using natural language (Devlin et al., 2018; Brown et al., 2020; Scao et al., 2022; Zhang et al., 2022). The most common tasks where these models have shown its proficiency, include, natural language generation, question-answering, reasoning, translation, summarization, etc. As the general-purpose models grow in size and learn from larger data corpus, we observe that they are get more versatile, and develop the ability to handle multiple tasks that earlier required separate specialized models. However, areas like finance, healthcare, science, law, among others, often have their own domain-specific vocabulary that necessitates finetuning of general-purpose models to carry out domain-specific tasks (Wu et al., 2023; Luo et al., 2022; Taylor et al., 2022). As the LLMs get even larger, it will be interesting to see whether general-purpose LLMs can work equally well as domain-specific LLMs simply with appropriate prompts. A rather more urgent problem that needs attention is that many of the domains require access to real-time data arising from several sources to handle various tasks with high accuracy and efficiency, which is currently beyond the capability of frozen LLMs. For instance, in the context of finance, a user is likely to prefer using an LLM that is knowledge-grounded with real-time news and asset prices as they would want to make decisions based on accurate financial news and tabular data related to their query. Unless one uses a financial LLM for educational purposes, it will be of limited use as most of its response will be based on stale data that is of little value to investors or analysts. However, there are innumerable challenges when it comes to training or finetuning an LLM on real-time data, that we list below:\n1.  Training or finetuning an LLM is very costly: Financial data such as asset prices change with a high frequency and similarly news data with an ability to impact asset prices is generated continuously in various parts of the world. Training or finetuning LLMs require an update in the model parameters that is quite time consuming. Even if one attempts a small duration of finetuning based on batches of most recent numeric and text data, the LLM will still remain outdated as financial data flows at high velocity. Unless LLMs have the capability of quick online learning, knowledge grounding based on frequent training or finetuning will not entirely help.\n2.  Lack of knowledge on what the LLM has learned: Most of the LLMs learn from massive data corpus, but as users of the system, we do not know what the model has learned and where it may commit errors. Only an intensive testing of the models often reveal its weakness. Therefore, without a complete knowledge that LLM has learned from the new data appropriately, one may not put it to use in financial contexts where decisions have to be made based on new data.\nHandling the above challenges would be useful for any general-purpose or domain-specific LLM; therefore, there is a large body of recent research on knowledge grounding of LLMs (Li et al., 2021; Peng et al., 2023; Carta et al., 2023). An alternative approach to solve the above problem is to keep that LLM frozen, that is, do not change or finetune the weights of the LLM, rather add modules to the LLM for knowledge grounding. The first module is the data module that provides access to real-time information that is flowing, and the second module consists of a Financial Agent that interprets the user query and extracts relevant data from the data module, thereby constructing the financial context required to answer the query. More modules or agents for video interpretation module, financial news analysis and event analysis can also be added to the framework.\nIn this paper, we present a Financial Agent for grounding large language models in financial knowledge, enabling them to address financial queries effectively while managing dynamic text and tabular data streams. Our methodology leverages an external agent comprising a 7 billion parameter LLM integrated with real-time data sources to construct the contextual information required by a larger LLM (GPT 3.5) for accurate query resolution. Our contributions in the paper are as follows:\n1.  We develop a custom Financial Context Dataset\u00b9, containing over 50,000 financial queries paired with the corresponding contextual information needed for the resolution of the queries.\n2.  We train a 7 billion parameter LLM, referred to as FinBloom 7B\u00b2, on over 14 million financial news articles sourced from Reuters and Deutsche Presse-Agentur (DPA), equipping it with versatile capabilities for financial tasks. We also use a random sample of 25% from 12 million documents from Securities and Exchange (SEC) filings along with news data.\n3.  We finetune FinBloom 7B on the Financial Context Dataset to create a Financial Agent\u00b3 that generates the context required to answer a user query and extracts data from a data module to effectively handle the financial query.\nThe proposed approach facilitates the extraction of high-velocity real-time financial data, enabling the effective resolution of user queries with support from the larger LLM (say GPT 3.5 or GPT 4). As a part of this study, we release the 50,000 Financial Context Dataset, 7 billion parameter domain-specific LLM and finetuned Financial Agent for context generation. We are unable to release the raw data, i.e. 14 million financial news articles from Reuters and Deutsche Presse-Agentur (DPA), because of contractual reasons.\nExisting approaches to knowledge grounding in LLMs follow different methodologies, which have a number of similarities, but are known in the literature with different terminologies. We address these approaches in the later part of the paper. We adopt an agent-based procedure to perform knowledge grounding keeping in mind that financial data is high volume and high velocity data for which high latency approaches may not be suitable. In the financial domain, real-time data is frequently presented in tabular or textual formats. To accommodate this, we maintain two repositories with streaming data: a tabular database containing up-to-date prices, financial metrics, financial statements, and a text database storing the latest news. These repositories collectively form the data module, which is accessible to the Financial Agent. When a user query (x) is received, the Financial Agent analyzes the query and retrieves relevant news and tabular data (d) from the data module. The retrieved data is then converted into a text format (c(d)) and appended to the user query (x). This combined input (c(d), x) is passed to the LLM as part of a conversation, enabling it to generate an informed response that incorporates the contextualized real-time data. The system architecture is illustrated in Figure 1.\nAn example of the operation of the proposed architecture is provided in Figure 2. The example in the figure demonstrates how the Financial Agent leverages knowledge grounding to process and respond to a financial query. When the user inputs the query \"Explain P/E ratio taking an example of Google,\" the Financial Agent performs contextualization by accessing the data module, which contains real-time information from tabular and textual repositories. Relevant data, such as Google's share price ($108.90) and earnings per"}, {"title": "2. Literature Review", "content": "In this section, we provide a review on knowledge grounding, contributions of LLMs in finance, and popular financial datasets for financial text mining. We also talk about the limitations of some of the popular knowledge grounding approaches when applied to the area of finance.\n2.1 Knowledge Grounding of LLMs\nThe usual strategy for broadening the applicability of an LLM to a particular downstream task has conventionally entailed refining the model's parameters. This involves the process of training certain or all layers of the LLM on a bespoke dataset tailored to the specific requirements of the given downstream task (Radford et al., 2019). However, these strategies exhibit a considerable computational cost, especially if the data is dynamic and the model has to be updated frequently. The fundamental rationale for knowledge grounding in LLMs resides in the motivation to equip these models with the faculties of reasoning and contextual comprehension based on relevant external data, as opposed to them being regarded as repositories of static knowledge.\nIn recent years, a plethora of viable alternatives have emerged, offering potential enhancements to the conventional finetuning methodology. Wei et al. (2022) introduced \u201cchain-of-thought\" (CoT), a few-shot prompting technique for LLMs. CoT employs sequential examples within a prompt, comprising task inputs and intermediate steps, to drive contextual understanding and reasoning in large models. Brown et al. (2020) demonstrated GPT-3's ability to learn complex tasks in a few-shot setting. But few-shot prompting requires manual effort to design the optimal prompt for any required downstream task. Reynolds and McDonell (2021) argued that few-shot learning isn't a method of task learning but rather a method of task location in the existing space of the model's learned tasks. They introduced the concept of metaprompt programming, through which the job of writing task-specific prompts can be assigned to the LLM itself. Gao et al. (2020) proposed an improvement in the few-shot strategy where rather than relying on the arbitrary selection of random examples and their inclusion in the query, which lacks a guarantee of emphasizing the most informative demonstrations, their approach involved sequential random sampling of a single example from each class for every input resulting in the creation of multiple concise demonstration sets. Shin et al. (2020) introduced AUTOPROMPT, an approach that automatically constructs prompts by combining primary task inputs with an array of trigger tokens, adhering to a predetermined template. The set of trigger tokens used is same for all the inputs and is learned through a specialized adaptation of the gradient-based search strategy. The composite prompt is then supplied as input to a Mask Language Model. It was observed that for Natural Language Inference (NLI) tasks, AUTOPROMPT was comparable to a supervised finetuned BERT model.\nAnother limitation inherent in the few-shot prompting methodology stems from the constrained token capacity that the LLMs can intake. This might have thousands of examples that we need the LLM to learn from. A number of parameter efficient fine-tuning methodologies have been proposed to tackle this issue. Liu et al. (2021) proposed P-Tuning, a technique where the prompt tokens in the input embedding (containing context tokens and prompt tokens) are treated as pseudo tokens and mapped as trainable embedding tensors. This continuous prompt is modelled using a prompt encoder consisting of a bidirectional LSTM and is then optimized using the downstream loss function. Another unique approach is Prefix Tuning (Li and Liang, 2021), where a sequence of continuous task-specific vectors are prepended to the input of an autoregressive LM, or to both the encoder and decoder layers of a Encoder-Decoder Model. The prefix consists of trainable parameters which do not correspond to real tokens in a model's embedding. Instead of training the model's parameters on the loss function, only the parameters in the prefix are optimized. A similar methodology was used in Prompt Tuning (Lester et al., 2021), but without any intermediate-layer prefixes. Dettmers et al. (2024) proposed QLoRA, which introduces a memory-efficient finetuning method for LLMs by quantizing the pre-trained model to 4-bits and using Low Rank Adapters (LoRA), achieving comparable performance to 16-bit fine-tuning with reduced memory\nAn alternate approach for the knowledge grounding of LLMs involves equipping them with a retriever module that can access information relevant to the user's query from a database. Lewis et al. (2020) introduced a retrieval-augmented generation (RAG) model, where they integrated a pre-trained retriever module with a pre-trained seq2seq model. The retriever provides latent documents conditioned on the input, and the seq2seq model then conditions on these latent documents together with the input to generate the output. Dinan et al. (2018) designed a dialogue model, where within the conversational framework, the chatbot is equipped with access to a curated collection of passages that maintain relevance to the ongoing discourse. At each turn, using a standard information retrieval system, the chatbot retrieves the top 7 articles for the last two turns of conversation. An attention mechanism is used to perform refined selection of specific sentences that will be used to create the next response. Izacard et al. (2022) created ATLAS, a RAG Model which is capable of few-shot learning. Peng et al. (2023) presented LLM-Augmenter, a module which, in addition to knowledge retrieval and prompt generation, checks the response generated by a fixed LLM for hallucinations. If the response is not correct, it generates a feedback message which is used to improve the prompt. This cycle continues until the response by the LLM is verified. Li et al. (2021) used a three step data cleaning procedure to supply an LLM with relevant context: retrieving associated triples from a knowledge graph, finding related triples by computing cosine similarity between the triples and the query, further refining the choices by estimating the semantic similarity score.\nDespite significant advancements in knowledge grounding of LLMs, efficient deployment of them for real-time processing of enormous data flowing at high frequency, such as financial data, remains a challenge.\n2.2 Limitations of Traditional RAG Models in Retrieving Financial Data\nRecent years have witnessed a surge in the development and application of Retrieval-Augmented Generation (RAG) systems. By combining the power of large language models (LLMs) with information retrieval techniques, RAG systems have the potential to revolutionize various industries. However, despite their promise, these systems still face several limitations that hinder their widespread adoption, particularly in complex and regulated domains.\nOne of the core challenges in RAG systems lies in the effectiveness of the retrieval mechanism. As highlighted by Gupta et al. (2024), while powerful, the retrieval process can struggle with ambiguous queries and niche knowledge domains. The reliance on dense vector representations can sometimes lead to the retrieval of irrelevant documents, which can negatively impact the quality of the generated response. Cuconasu et al. (2024) further emphasized this point, demonstrating that the highest-scoring retrieved documents which are not directly relevant to the query, lead to a decline in LLM effectiveness. Moreover, the quality of generated output can be influenced by the number of retrieved passages, as outlined in Jin et al. (2024). While increasing the number of passages can initially improve the quality, it can eventually lead to a decline in performance.\nWhen integrating knowledge graphs (KGs) into RAG systems, challenges arise in effectively utilizing the structured information. Agrawal et al. (2024) identified several critical failure points in existing KG-based RAG methods, including insufficient focus on question intent and inadequate context gathering from KG facts. The real-world applications of RAG systems, especially in expert domains, are often characterized by complex and nuanced requirements. As noted by Zhao et al. (2024), the one-size-fits-all approach to data augmentation may not be suitable for all scenarios. In compliance-regulated sectors, RAG systems must adhere to stringent data privacy, security, and governance requirements. Bruckhaus (2024) highlighted the importance of ensuring that sensitive data is not exposed or misused during the retrieval and generation process. This is crucial when working with enterprises in compliance-regulated sectors. Dealing with financial data also presents certain challenges to traditional RAG based systems, as discussed below:\n1.  Difficulties in Representing Financial Data: Financial data is predominantly stored in structured, tabular formats, such as spreadsheets and databases. These formats prioritize organization, analysis, and manipulation of data points. While advantageous for financial tasks, this structured format presents difficulties for RAG models. Unlike textual data, tables lack inherent relationships and reasoning patterns that RAG models can readily exploit through their natural language processing capabilities. Converting vast financial datasets into textual representations for RAG models would be impractical and inefficient. Textual storage introduces significant redundancy and increases the risk of errors or inconsistencies during data manipulation. Moreover, the structured format of financial data tables ensures data integrity and facilitates efficient retrieval of specific data points.\n2.  Challenges in Retriever Accuracy: A core component of RAG models, the retriever, is tasked with fetching relevant information from the knowledge base to answer a user query. When dealing with financial data, the retriever faces a unique challenge. Financial queries often involve specific financial metrics, time-frames, and financial companies. The retriever must be highly selective in its retrieval process, ensuring it gathers only the precise data points required to answer the query accurately. Incomplete data retrieval can lead to inaccurate or misleading answers, while retrieving excessive data burdens the reasoning component with unnecessary information processing.\n3.  Inability to handle high velocity, high volume data: RAG models typically rely on static databases and pre-trained retrieval mechanisms, which struggle to keep pace with the rapid influx and dynamic nature of financial data. Additionally, the need for frequent updates and high-latency responses in financial contexts can overwhelm traditional RAG architectures, leading to delayed and potentially outdated outputs.\n2.3 Financial Datasets Review\nThe domain of financial language processing has witnessed a significant rise in the development of annotated datasets, each serving distinct purposes within the field. Through Table 1, we have highlighted some prominent examples that have been widely used in the area of finance for training models. In the next sections, we discuss the core contributions of the paper in detail. The key contribution is the creation of a Financial Agent that is capable of understanding the context needed for a user query, extracting the relevant data from the databases, and supporting an LLM to handle the contextualized query effectively."}, {"title": "3. Financial Context Dataset", "content": "For any given user query, a Financial Agent is expected to provide the relevant context, which includes numeric and text data, which can be appended to the user query for further processing. In this section, we discuss the first core contribution of the paper, which is the Financial Context Dataset, consisting of 50,000 samples of user query and its corresponding context. The proposed dataset can be used to train a Financial Agent specifically for context identification and data extraction. In this section, we present a detailed account of the dataset creation and its description.\nTo train our model effectively, the dataset should be diverse in terms of sentence structure and comprehensive in terms of financial metrics and companies. Specifically, the dataset should include queries in a variety of forms, such as simple yes/no questions, comparative questions, and open-ended questions. This will help the model to learn to understand the different ways that users can present their queries. Additionally, the dataset should include a large number of financial metrics and companies, in the form of their many possible names and other ways they are referred to generally. This diversity in the dataset will enable our model to generalize well to a wide range of user queries, ensuring that it can accurately identify the required information. To construct this dataset, which consists of natural language user queries and their corresponding structured data requests, we followed a multi-method query sampling approach. The first method involved collecting queries sent by retail customers over technology channels to a brokerage firm, like Email or Web-based forms, etc. The second method involved interaction and consultation with 10 financial advisors on the kind of queries retail investors commonly have. The third method involved interviews with 20 retail investors about common questions that come to their mind before investing in a particular stock or forming a portfolio. Based on three methods of data collection, we created 5000 query templates, in which 4000 templates were extracted from technology channels, 500 were created with financial advisors, and another 500 were created with retail investors. This approach ensured the incorporation of a broad scope of financial queries, resulting in a diverse dataset. In order to extend the dataset to 50000 queries, the 5000 templates were scaled-up by randomly varying various aspects within each template. Once the template is created, it is possible to map various kinds of queries to each template, for instance, Table 8 in Appendix A provides randomization of various aspects in the template leading to a larger set of queries in our dataset.\nIt is obvious that not all financial metrics or companies can be mapped to each of the queries because of the differences in how the queries are structured. For example, the metrics 'revenue breakdown', 'market segmentation', or 'customer demographics' can be mapped to the template, Provide details about the [metrics] of companies [date]., but not to the template, Did the [metrics] of [companies] increase [date]? because they may not inherently be quantitative or trackable over time. Metrics that exhibit clear numerical trends over time, such as profit margins, sales growth, or stock prices, are better suited to templates like Did the [metrics] of [companies] increase [date]?, which require measurable changes or directional insights. Therefore, the metric set maintained by us is carefully curated to align with the specific structure and intent of each template.\nWe maintain a comprehensive repository of financial metrics [metrics], company names [company], industries [industry], and augment it with time range variations [date] and numeric variations [number] to systematically generate a diverse set of natural language queries. This template-driven approach to query generation ensures that the resulting dataset is both highly diverse and semantically accurate. By leveraging predefined templates, we produce a dataset that encompasses a broad spectrum of financial metrics and company-specific contexts. Furthermore, this approach eliminates the need for manual extraction and labeling of financial information, thereby enhancing both efficiency and scalability in dataset creation.\nAfter creating 50000 queries using a template-based approach, we create two additional columns in the dataset, namely, \u201cRequired Data\u201d and \u201cStructured Data Request\". The"}, {"title": "4. FinBloom 7B: A Large Language Model for Finance", "content": "Our approach for addressing user queries requires a Financial Agent for forming a structured request of the financial data that the Data Module can understand and respond to. The user's query is typically in natural language, which means that it may not explicitly contain all of the necessary information, like recent numbers and news, that is needed to answer it. Hence, the Agent needs to analyze the user's query and intelligently identify the essential financial metrics, as well as any related metrics and news, that are needed to answer the query. It also needs to identify the companies for which the data is required, as well as the date ranges for which the data is needed. It finally needs to ensure that these requirements are presented in the form of a data request function that the data module can interpret. Manually specifying a set of hard rules to handle all possible user queries and perform these tasks would be very tedious and complicated. Therefore, a better approach is to train a text-to-text generation model on a large dataset of natural language user queries for generating the corresponding structured query formats which contain all the information required to answer the user query. There are quite a few financial LLMs such as FiMA (Xie et al. (2023)), FinGPT (Yang et al. (2023a)), CFGPT (Li et al. (2023)), InvestLM (Yang et al. (2023b)) available that can be finetuned to serve as an agent for the task of creating structured data request. However, most of these LLMs exhibit limitations that may hinder their effectiveness in real-world applications, especially for the purpose of identifying financial context for a given query.\nMany domain-specific models are typically developed by starting with a general-purpose base model, which is subsequently finetuned using human-curated financial datasets (for example, FinMA (Xie et al. (2023)), InvestLM (Yang et al. (2023b))) such as instruction datasets, question-answer datasets, structured datasets, or synthetic datasets. To the best of our knowledge, most existing financial LLMs are not trained on high-quality, large-scale domain-specific text corpora. This limitation primarily arises from the scarcity of extensive and ethically usable financial text corpora, as such resources are often difficult to access or compile from online channels. Many human-curated financial datasets used for training finance-specific LLMs are of limited size and highly structured; therefore, it may fail to convey domain-specific nuances and may not capture the complex inter-dependencies inherent in financial metrics and text. Human-curated datasets are also often inherent with human biases, leading to LLMs with shallow understanding of financial text. Using financial text corpora, such as large volumes of written financial text (books, news, filings, etc.), enriches the model's knowledge of domain-relevant terminology, concepts, and context. Even in cases where one desires to train domain-specific instruction models, it is advisable to take a hybrid approach involving training on text-corpora followed by training on instruction dataset. The existing domain-specific LLMs lack this phase of training on financial text corpora. Some financial LLMs such as FinGPT (Yang et al. (2023a)), CFGPT (Li et al. (2023)) are trained on publicly available financial text corpora, but are not competitive when evaluated on financial benchmarks in the later part of this section.\nTo create the financial LLM, we choose a foundational model with robust general language understanding and the potential for domain-specific specialization in finance. We began by selecting the Bloom 7B parameter model as the base, leveraging its strong linguistic capabilities as a foundation for finetuning. To equip the model with financial expertise, we used a large corpus of financial news articles procured from Reuters and Deutsche Presse-Agentur (DPA). This dataset encompasses a wide array of financial topics, including market trends, economic indicators, corporate developments, and regulatory changes. The Reuters dataset spans the period from January 1, 2003, to December 31, 2012, while the DPA dataset covers the timeframe from June 1, 2001, to May 31, 2011. The combined dataset consists of over 14 million articles. The high-quality financial data used for training is expected to lead to an LLM that understands interactions between various financial metrics and concepts well. We also utilized the publicly available SEC filings data from the period 31st Mar 2009 to 31st Oct 2023. The SEC filings consisted of overall 12.23 million documents but we utilized a random sample of 25% for the purpose of fine-tuning to avoid biasing the model with a large volume of \"regulatory reporting data\". The details of the three datasets are provided in Table 4. Finetuning was performed using the efficient QLORA method (Dettmers et al. (2024)), which facilitated significant performance improvements with minimal computational resources. The training process spanned four epochs on a Tesla T4 GPU, requiring over 2000 hours. The model derived from this training will be referred to as \"FinBloom 7B\" in the later parts of the paper."}, {"title": "5. Financial Agent for Context Generation and Data Retrieval", "content": "This section describes the Financial Agent's structure and function, explaining how it works with the Data Module to provide relevant context for an LLM to answer user queries. The Financial Agent was developed by fine-tuning the FinBloom 7B model on the the custom dataset detailed in Section 3. This fine-tuning process employed the parameter-efficient Prompt Tuning methodology as proposed by (Lester et al., 2021). We split our dataset in a 4 to 1 ratio for training and evaluation sets. We trained the model for 5 epochs. The scores achieved for the training and evaluation sets are reported in Table 6\nTo further assess the accuracy of the finetuned FinBloom 7B LLM on our custom dataset, we generated an additional 10,000 natural queries and their corresponding labels to create a test set. We then passed the same queries to the model as inputs and recorded its outputs. Considering these recorded outputs from the model as the generated text and the original labels in the test set as the reference text, we calculated the similarity between the two sets using the BLEU (Bigram Language Evaluation Understudy) (Papineni et al. (2002)) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin (2004)) metrics. These metrics provide a numerical score reflecting the degree of overlap between the model-generated text and the expected response, which are reported in Table 7. As we can see from these scores, the model is excellent in identifying the required data to answer the queries.\nThe Data Module serves as the repository for financial data, encompassing tabular financial information as well as real-time news data pertaining to diverse companies, sectors, and industries. It operates in a dynamic fashion, regularly ingesting and incorporating fresh data to ensure that the information it provides remains current and up-to-date. The module is designed to process financial data requests. Upon receiving a data request from the Financial Agent, the data module constructs a dataframe containing pertinent financial data, conforming to the specifications of the data request. Concurrently, the module extracts the relevant news data, considered beneficial for complete query resolution, based on semantic matching of news items with the user query. We use RoBERTa-based (Liu, 2019) semantic search to detect similarity between the requested query and the news headlines. When a data request containing company/industry name(s), financial metric(s) and specific date range(s) is received, the module identifies the closest date range(s) within which the financial data is available. For example, net income is reported quarterly, so the module presents the quarterly net incomes for the closest available date range to the one in the request. Share price, on the other hand, is reported at a higher frequency, so the module presents the share price data for the exact date range as requested in the query. If there are no dates/date ranges present in the request, then the latest date range for which the data is available is chosen. The module then retrieves the relevant metric data for that date range and in addition, communicates the corresponding date range and frequency for which this metric's data is provided. Consequently, when this data is integrated with the initial user input and supplied to the LLM, the LLM receives accurate information regarding the metric and its associated date range. In our proposed framework, the various modules interoperate as follows:\n1.  The user's initial query, x, is received by the Financial Agent.\n2.  The Agent identifies the required data to answer this query and converts it into a structured data request to be passed onto the Data Module.\n3.  The Data Module receives this data request and returns the relevant financial and news data.\n4.  This data is converted into a string form c and appended to the initial query x along with instructions for the LLM to answer the given financial query using the data provided.\n5.  The LLM receives the contextualized and enriched query, (c, x), to generate the output."}, {"title": "6. Conclusions", "content": "This study emphasizes the role of knowledge grounding in LLMs, particularly for interactive tasks that require real-time information access. First, we developed the Financial Context Dataset, which comprises over 50,000 financial queries paired with the contextual information necessary for their resolution. Building on this foundation, we trained FinBloom 7B, a 7 billion parameter language model, using a corpus of over 14 million financial news articles sourced from Reuters and Deutsche Presse-Agentur (DPA) and 12 million documents from SEC filings. Subsequently, we fine-tuned FinBloom 7B on the Financial Context Dataset to create a Financial Agent. This agent is capable of generating the required context for user queries and efficiently retrieving it from a structured data module to address financial queries. A key challenge in financial decision-making lies in mitigating the high latency associated with multiple interactions with large-scale LLMs (e.g., GPT-3.5 or GPT-4). To address this, the trained Financial Agent seamlessly integrates with any large-scale LLM.\nIn our proposed framework, the user submits a query, the Financial Agent processes and retrieves the necessary context, and the LLM subsequently analyzes the query along with the contextual information. This design ensures low latency, enabling effective handling of high-velocity financial data effectively.\nFuture research could expand this framework into a multi-agent system, integrating additional agents such as financial video analysis tools to further enhance its capabilities. With these advancements, our work holds the potential to transform the delivery of financial services, making them more efficient, personalized, and accessible to a broader audience."}]}