{"title": "Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification", "authors": ["Junbo Jacob Lian"], "abstract": "Twisted Convolutional Networks (TCNs) are introduced as a novel neural network architecture designed to effectively process one-dimensional data with arbitrary feature order and minimal spatial relationships. Unlike traditional Convolutional Neural Networks (CNNs), which excel at handling structured two-dimensional data like images, TCNs reduce dependency on feature order by combining input features in innovative ways to create new representations. By explicitly enhancing feature interactions and employing diverse feature combinations, TCNs generate richer and more informative representations, making them especially effective for classification tasks on datasets with arbitrary feature arrangements. This paper details the TCN architecture and its feature combination strategy, providing a comprehensive comparison with traditional CNNS, DeepSets, Transformers, and Graph Neural Networks (GNNs). Extensive experiments on benchmark datasets demonstrate that TCNs achieve superior performance, particularly in classification scenarios involving one-dimensional data. The source code for the TCNs can be accessed at https://github.com/junbolian/Twisted-Convolutional-Networks", "sections": [{"title": "Introduction", "content": "Recent advancements in machine learning and deep learning have revolutionized the field of classification and pattern recognition. Among these developments, Convolutional Neural Networks (CNNs) have gained immense popularity due to their ability to capture spatial hierarchies in data, making them highly effective in tasks such as image and speech recognition [16]. Despite their success, CNNs heavily rely on the spatial order of input features, which can limit their applicability to data without an inherent spatial structure or well-defined feature relationships. In many real-world applications, such as gene expression data, customer demographics, and sensor readings, the relationships between features are not strictly spatial or sequential, and the ordering of features may not carry"}, {"title": "Related Work", "content": "Convolutional Neural Networks have been widely applied in a variety of domains, particularly in tasks involving spatial data, such as image classification [14], object detection [4], and speech recognition [9]. However, CNNs are often less effective in situations where the input data lacks a clear spatial or temporal structure. Researchers have investigated alternative approaches to overcome this limitation, including permutation-invariant models and feature-wise attention mechanisms [17]. For example, permutation-invariant neural networks have been used to handle point cloud data, where the spatial arrangement of the points is irrelevant [20]. Despite their usefulness, such methods may still fail to fully exploit the relationships between combinations of features.\nEnsemble learning techniques, such as Random Forests, have demonstrated the power of using diverse subsets of features to improve generalization and robustness [2]. Feature selection techniques, including wrapper methods and filter methods, have been extensively explored to enhance the quality of input"}, {"title": "Twisted Convolutional Networks", "content": "The Twisted Convolutional Network (TCN) introduces a novel feature combination strategy designed to mitigate the dependence on the order of input features. This model combines features through different element-wise operations, effectively generating higher-order features that capture interactions between original input features. This approach draws inspiration from ensemble learning, where combining different views of the data leads to enhanced predictive performance. The TCN's architecture is designed to enable better generalization, particularly in datasets with arbitrary or non-spatially ordered features."}, {"title": "Feature Combination Layer", "content": "The TCN begins by generating combinations of input features. Given an input with n features, the model forms combinations of these features in multiple ways, depending on the number of features being combined, resulting in a variety of new combined features.\nTwo primary methods are used for feature combination:\n1. Multiplicative Combination (Approach I): When combining features, the original TCN calculates the product of the selected features. For example, given three features A, B, and C, the combination is computed as A \u00d7 B \u00d7 C. This approach generates a single value representing the combined interaction among all selected features. Mathematically, this can be expressed as:\n$$Zmult = \\prod_{i=1}^{m} Xi$$ (1)\nwhere m is the number of features being combined."}, {"title": "Feature Interaction Module", "content": "The Feature Interaction Module is a core component that differentiates TCN from traditional CNNs. This module aims to capture complex, high-order feature interactions that are often overlooked by convolutional filters. The module consists of multiple layers that perform operations such as element-wise multiplication, summation, and feature transformation.\nMathematical Representation: Given an input feature vector X = [X1,X2,..., xn], the feature interaction module computes a transformed feature vector Z as follows:\n$$Z=f(\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} X i X j)$$\nwhere f() represents a non-linear transformation applied to the combined features. This non-linear transformation helps in capturing intricate relationships between features, enabling the model to learn more robust representations."}, {"title": "Network Architecture", "content": "The architecture of the TCN consists of an input layer that takes the combined feature set, followed by multiple fully connected layers with He initialization [7] and batch normalization [10]. The fully connected layers transform the feature representation space, enabling the model to learn complex relationships between the combined features. The following layers are used in the architecture:\n\u2022 Input Layer: Accepts the combined feature set Z with size equal to the number of combined features. The input layer normalizes the input to ensure stable training.\n\u2022 Feature Transformation Layer: A layer dedicated to transforming the combined feature set into a new feature representation space. This layer applies a transformation function fr(Z) that captures non-linear relationships between features. Mathematically:\n$$Z' = fr(WZ + b)$$\nwhere W and b are the weights and biases of the layer, and fr is a non-linear activation function such as ReLU.\n\u2022 Fully Connected Layer: The first fully connected layer consists of 20 neurons, which transforms the combined feature representation into a richer feature space. He initialization is used to maintain variance during training.\n\u2022 Batch Normalization Layer: Batch normalization is applied to reduce internal covariate shift, accelerating the training process and ensuring stability [10].\n\u2022 ReLU Activation: A ReLU activation function is applied to introduce non-linearity into the model, allowing it to learn complex patterns [18].\n\u2022 Dropout Layer: Dropout is used with a dropout rate of 0.5 to prevent overfitting by randomly deactivating neurons during training.\n\u2022 Fully Connected Layer: A second fully connected layer with 10 neurons further refines the feature representation.\n\u2022 Output Layer: The final fully connected layer consists of neurons equal to the number of classes in the dataset, followed by a softmax layer for classification.\nThe TCN also incorporates residual connections in the fully connected layers to enhance gradient flow, inspired by ResNet [8]. The residual connections help the model converge faster and maintain better accuracy as the network depth increases."}, {"title": "Residual Feature Combination Block", "content": "The Residual Feature Combination Block is introduced to improve gradient flow and stabilize training. This block ensures that the network can learn effectively even as depth increases.\nMathematical Representation:\nGiven an input feature vector X, the residual block computes the output Y as follows:\n$$Y = f(W2f(W1X + b\u2081) + b2) + X$$\nwhere W1, W2 are weights, b1, b2 are biases, and f(\u00b7) is a non-linear activation function. The addition of X ensures that the original input is preserved, facilitating gradient flow."}, {"title": "Training and Hyperparameters", "content": "The TCN model is trained using the Adam optimizer [11], which provides adaptive learning rates and accelerates convergence. The learning rate is initially set to 0.001, with a batch size of 10. Training is conducted over 200 epochs, with early stopping applied to prevent overfitting. L2 regularization is also applied"}, {"title": "Comparison with Convolutional Neural Networks", "content": "In contrast to CNNs, which apply spatial filters to capture local dependencies, TCNs are designed to create new feature representations that are not bound by spatial constraints. The kernel size in CNNs is typically fixed, capturing specific local patterns in the input data. In the TCN, however, each feature combination can be thought of as a dynamic kernel that adapts based on the specific features being combined. This flexibility makes TCNs particularly effective for datasets with features that have complex, non-linear relationships, but no inherent spatial structure."}, {"title": "Regularization and Generalization", "content": "To further improve generalization, the TCN incorporates dropout layers and L2 regularization techniques. Dropout is applied to the fully connected layers to randomly deactivate a fraction of neurons during training, thereby reducing the risk of overfitting. L2 regularization is applied to the weights to penalize large weight values, encouraging simpler models that generalize well on unseen data. The combined effect of these techniques ensures that the TCN performs robustly across a variety of datasets, particularly those where the relationships between features are complex and non-linear."}]}