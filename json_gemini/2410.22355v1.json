{"title": "Learning Goal-oriented Bimanual Dough Rolling Using Dynamic Heterogeneous Graph Based on Human Demonstration", "authors": ["Junjia Liu", "Chenzui Li", "Shixiong Wang", "Zhipeng Dong", "Sylvain Calinon", "Miao Li", "Fei Chen"], "abstract": "Soft object manipulation poses significant challenges for robots, requiring effective techniques for state representation and manipulation policy learning. State representation involves capturing the dynamic changes in the environment, while manipulation policy learning focuses on establishing the relationship between robot actions and state transformations to achieve specific goals. To address these challenges, this research paper introduces a novel approach: a dynamic heterogeneous graph-based model for learning goal-oriented soft object manipulation policies. The proposed model utilizes graphs as a unified representation for both states and policy learning. By leveraging the dynamic graph, we can extract crucial information regarding object dynamics and manipulation policies. Furthermore, the model facilitates the integration of demonstrations, enabling guided policy learning. To evaluate the efficacy of our approach, we designed a dough rolling task and conducted experiments using both a differentiable simulator and a real-world humanoid robot. Additionally, several ablation studies were performed to analyze the effect of our method, demonstrating its superiority in achieving human-like behavior.", "sections": [{"title": "I. INTRODUCTION", "content": "In the context of domestic scenarios, the manipulation of soft objects using dual arms is a common requirement. Numerous studies in the field of robotics have focused on bimanual activities to advance the application of robots in service-oriented tasks. Examples include folding clothes [1], coffee stirring [2], and cooking with stir-fry [3]. These tasks necessitate accurate state estimation of soft objects and the execution of coordinated bimanual movements. It is crucial to employ a reliable state representation method that can robustly capture the changes in the state of soft objects despite disturbances from environmental factors and sensor limitations. Additionally, an important practical consideration is the ability to transfer policies learned in simulation to real-world scenarios, bridging the significant gap between simulated and real environments. Such capability would greatly facilitate the practical implementation of simulatorbased learning methods in real-world applications. After obtaining a compact and concise state information, robots are required to learn the bimanual manipulation policy to achieve specific goals. However, the complexity of movements and the requirement for scene generalization make simple logic programming inadequate for such tasks. In recent decades, two mainstream solutions have emerged: Reinforcement Learning (RL) and Learning from Demonstration (LfD). However, both approaches have their limitations. Demonstrations provide direct examples of expert policies, enabling the robot to mimic behavior in a supervised manner. This approach restricts the robot's ability to learn object state representation and discover new skills due to the limited amount of available data. On the other hand, RL-based methods allow for extensive interaction with the environment but often struggle to learn complex or long-horizon tasks from scratch. In summary, state representation and policy learning are the primary factors that restrict robots' ability to acquire human-like skills, particularly in the domain of soft object manipulation. Therefore, this paper aims to address these two challenges simultaneously and in a natural manner by proposing a suitable approach.\nSoft object state estimation and representation, a crucial challenge in the field of robotics, has garnered significant attention in the realm of Computer Graphics. Huang et al. introduced PlasticineLab, a state-of-the-art differentiable"}, {"title": "II. DOUGH ROLLING TASK", "content": "To evaluate our method, we tackle the challenging task of bimanual dough rolling. Previous dough rolling research includes Figueroa et al., who employed a hierarchical framework for automatic task segmentation and represented human demonstrations using action primitives [13]. However, such movement primitive-based methods have limitations in understanding the relationship between robot motions and object state changes. Calinon et al. discussed generalization in learning from few demonstrations, presenting a taskparameterized mixture model and comparing it with other approaches [14]. It primarily focused on determining the rolling direction rather than accomplishing the complete dough rolling task.\nIn recent dough rolling research, there has been a growing reliance on differential simulators. Diffskill, for instance, introduced a method for learning skill abstraction in sequential dough manipulation tasks. It employed a differentiable simulator to generate simulated demonstrations and trained a supervised policy for each dough manipulation primitive skill [15]. Another notable work proposed a Generative Pretrained Transformer (GPT) [16] based world model (SoftGPT) for learning the dynamics of dough from a differential simulator [17]. This model facilitated efficient learning of diverse dough manipulation tasks including dough rolling,"}, {"title": "III. SOFT OBJECT MANIPULATION POLICY LEARNING VIA DYNAMIC HETEROGENEOUS GRAPH", "content": "In this section, we will introduce the key concepts and components used in our proposed methods. The overall soft object manipulation pipeline is illustrated in Fig. 2, consisting of two main parts: (1) dynamic heterogeneous graph policy learning in the simulator with the guidance of demonstration, and (2) planning and control for deploying the pre-trained model on the real-world humanoid robot.\nA. Represent soft objects and manipulator as a heterogeneous graph\nRecent studies have started utilizing graph-structured data and graph neural networks to model soft object manipulation policy learning. For instance, some research has focused on predicting action effects of a bag using graph-based approaches [18], learning the dynamics of soft objects [5]. Graphs serve as efficient and compact data structures for representing non-Euclidean data, capturing both semantic and geometric information through their nodes and edges. To address the challenge of abstracting the state of a dough object, which lacks distinct characteristic points unlike soft objects such as clothes [1], we employ a robust solution that minimizes interference with subsequent policy learning. In this study, we utilize standard image processing techniques to achieve this abstraction. Initially, we segment the dough from the operation board using a color threshold and subsequently determine the center of the segmentation contour. From this contour center, we establish four lines with equal angular spacing and calculate eight focal points between them and the contour. These focal points serve as position features for the object's boundary nodes. Alongside the center node, we construct a 9-node Vograph to represent the object's state. Each node in the graph is associated with a position feature and a corresponding depth feature obtained from depth observations. To provide clarity, Fig. 2 illustrates an example with five object nodes. The graph includes edges connecting all object boundary nodes to the center node, while the boundary nodes are connected solely to their two immediate neighbors. This graph-based representation facilitates effective modeling of the dough object's state and its subsequent utilization in the research field of robotics.\nAs shown in the graph block in Fig. 2, except for the object sub-graph $SG_t$, the dual end-effectors are added as manipulator nodes, together constructing the complete heterogeneous graph $G_t$. Heterogeneous graph refers to graphs that contain different types of nodes and also different types of edges between nodes. Therefore, a heterogeneous graph representation allows us to learn more relationships as we expected. The manipulator node features are their poses with respect to the board coordinate system. The edges between the manipulator and object nodes are fully connected."}, {"title": "B. Learn state change as the evolution of the dynamic graph", "content": "After gaining the graph abstraction of object and manipulator states at each time step, we then consider describing the dynamic evolution of the state and the corresponding manipulation trajectory. Each static graph $G_t$ is a snapshot of the dynamic graph evolving from time 0 ~ T. Thanks to the nature of the heterogeneous graph, we can define several different types of graph convolution operations on this graph. As shown in Fig. 2, we learn the goal-oriented policy (Equ. 1a), transition function and object dynamics (Equ. 1b) and value function (Equ. 1c) in a unified dynamic heterogeneous graph model.\n$\\hat{a}_{t+1} \\leftarrow X_{vm,t+1} = \\pi_{GraphConv}(X_{vm,t},X_{vo,t} X_{sgg})$ (1a)\n$SG_{t+1} \\leftarrow X_{vo,t+1} = T_{rGraphConv}(X_{vm,t}, X_{vo,t})$ (1b)\n$v = V(X_{vm,t+1}, X_{vo,t+1})$ (1c)\nThe goal-oriented policy takes the graph abstraction of the goal state (the semitransparent graphs in Fig. 2) as a condition and learns to obtain the hidden feature of the manipulator node at the next time step through the graph feature of the current state. Similarly, another Graph Convolutional Network (GCN) [19] is set for predicting the next hidden feature of object nodes. These hidden features are transformed to the next action $a_{t+1}$ and next object state prediction $SG_{t+1}$ by two additional feed-forward networks. The value of the current policy is estimated upon these hidden features as well. Specifically, we first perform a mean aggregation of these heterogeneous hidden features and then feed it to another feed-forward network.\nSince several sub-tasks exist simultaneously, the loss function is composed of several parts. Referring to RL methods, we calculate the advantage function based on the value estimation and the actual return and generate the value loss and the action loss. Another term is a supervised loss for learning an accurate dynamic model.\n$L_t(\\theta) = E[L_{CLIP} + C_1C_{VF} + C_2C_{Dyn} \u2013 C_3S[\\pi](SG_t)]$ (2)"}, {"title": "C. Demonstration guidance during the policy learning", "content": "The goal-oriented policy has shown the superiority of integrating the goal state in a compact dimension via graph representation. Another advantage is combining the fully explore-based methods with learning from demonstration methods, adding the demonstration as a policy learning guidance. This topic is studied in the offline RL field. The intention of CQL is to lower-bound the actual value of Q function by introducing a conservative term, avoiding the common value overestimation problem in offline RL. This term minimizes values under an appropriately chosen distribution over state-action tuples and then further tightens this bound by incorporating a maximization term over the data distribution [20]. Since DGform has not been extended to an actor-critic form, a state-action value estimation is not available. Thus, we adopt the current policy distribution to evaluate demonstration state-action pairs.\n$L_m (\\zeta | S_{10}) = \u2212 \\Sigma_{\\pi_0(\\zeta | SXG_c)} $ (3)\nAs we expect, demonstration guidance should not be a strong supervision constraint but should leave room for exploration. Therefore, we further add a self-adjusting term to convert Equ. 3 to a Lagrange version.\n$\\theta^{k+1} \\leftarrow arg \\underset{\\theta} {min} \\underset{\\alpha}{max} E E[(\\alpha L^k_m (\\zeta | S_{10}) \u2013 \\tau) + CC_{LIP}$ + $C_1L^v_F + C_2L^D_{yn} \u2013 C_3S[\\pi](SG_t)]$ (4)\nwhere is the imitation threshold, $\\alpha$ is a self adjusting Lagrange coefficient of conservative term. More specifically, if the difference between the expected value and the threshold Tis less than 0, then $\\alpha$ will tend to 0. Conversely, $\\alpha$ will increase, and the weight of the conservative term in the entire loss calculation will also increase. This version intends to adaptively change the supervision intensity according to the deviation between the rollout and demonstration actions while leaving room for exploration in the vicinity of the demonstration."}, {"title": "D. Model-based rollout by the learned object dynamics", "content": "Although directly using image processing methods like segmentation to represent soft objects in a 2-dim graph simplifies our reliance on the visual front-end, it also brings some practical problems. It is challenging to represent the state of the dough by this method when occluding by other objects, like the rolling pin and robot hands. We certainly can also use methods similar to RoboCraft [5] to analyze the occlusion of objects by setting up cameras with multiple views in their experiments. However, information incompleteness is the most common in real scenarios. A more appropriate and robust solution is to predict actions over a period of time through the learned model when the state is unknown for part of the period. Formally, for a short horizon t \u2208 tk ~tk+H,\nsince the step-by-step interaction with the environment is not available, the proposed model alternately generates the"}, {"title": "E. Bimanual Trajectory Planning", "content": "The action output of DGform policy is path waypoints of dual end-effectors, which is inadequate to perform directly on robots with high frequencies (1000Hz in control loop for the humanoid robot in this paper). Thus, a trajectory planning module is required. Besides, since the dual endeffectors has a fixed relative relationship, coordination needs to be considered in the trajectory planning. Here we use Bimanual Relative Parameterization (Bi-RP) [21] for modeling the coordination and generating bimanual trajectories with Linear Quadratic Tracking (LQT) [22].\nFormally, the coordination is defined as another frame that takes the trajectory of the other arm as dynamic task parameters and represents the relative relationship as GMM. The relative motion between dual end-effectors is described as $c(k) = A^{(k)}_{c,t} (\\xi^{(l)}_t - \\xi^{(r)}_t) \u2013 b^{(k)}_{c,t})$ and represented by {$\\pi^{(k)}$, $\u00b5^{(k)}$, $\u03a3^{(k)}$}$^K_{k=1}$. For each end-effector, the task-specific GMM obtained by PoE\n$\\mathcal{N} (\\xi_{j}^{(a)} | \\xi_{j}^{(r)}) \u221d \\prod^{P}_{c_k=1} \\mathcal{N} (\\xi_{j}^{(a)} | \\mu_{j}^{(k)})\u22c5\\mathcal{N} (\\xi_{j}^{(r)} | \\mu_{j}^{(k)})$ (6)\nwhere $\\mu^{(k)} = A^{(k)}_{c,t} \\mu^{(k)} + b^{(k)}_{c,t}$, $\u0393^{(k)} = A^{(k)}_{c,t}\u03a3^{(k)} A^{(k)T}_{c,t}$. $A^{(k)}_{c,t}$,$ b^{(k)}_{c,t}$ are transformation matrices. P is the number of reference frames. More details can be found in [21]."}, {"title": "IV. EXPERIMENTS", "content": "A. Setup\nSimulation The simulation environment is built on top of PlasticineLab [11], with a particle-based dough and a rolling pin. We set up cameras with multiple views to show how the rolling pin and dough change in 3-dim space, while the RGB-D observation for policy learning is obtained from the top-down view. To shorten the learning process while maintaining graph abstraction performance, the resolution of the camera was set to (128, 128).\nReal-world experiments The pre-trained model was deployed on the Collaborative dUal-arm Robot manIpulator (CURI) robot, a self-designed bimanual human-like robot equipped with two 7 DoF arms and a 3 DoF torso. CURI utilizes a Cartesian motion controller to execute desired Cartesian space end-effector trajectories by generating joint torques. In real experiments, feedback is provided through first-person perspective observations captured by a single ZED 2i camera embedded in CURI's head."}, {"title": "C. Policy learning performance", "content": "An inference example of the DGform rolling policy is shown in Fig. 4, the first row shows the sample of sequential rolling pin movements, and the second row is the corresponding visualization of graph abstractions.\nBaselines The learning performance comparison is mainly divided into two categories, exploring the impact of state representation (full particle state, RGB-D and graph) and the"}, {"title": "D. Generalization analysis", "content": "Another essential indicator for evaluating robot skill learning is whether it is easy to transfer to unseen scenarios. With the help of differentiable simulators, we designed generalization experiments with different initial states or camera positions. These two variables are generated randomly in a reasonable range. Their average values of evaluation metrics are shown in Table II. The results were inferred from pretrained baselines and DGform models. It is easy to figure out that methods with graph state representation have better transfer ability. Moreover, the manipulation performance of DGform does not change much as the environment changes."}, {"title": "E. Real robot dough rolling", "content": "The process of deploying a pre-trained simulated policy on a real-world robot is depicted in the right side of Fig. 2. At the start of the experiment, the initial state of the dough is captured by the embedded ZED 2i camera in CURI's head,"}, {"title": "V. DISCUSSION", "content": "It is worth noting that this paper only focuses on the two-dimensional shape change of dough. Point cloud information and three-dimensional graph neural network will be introduced, like SoftGPT [17], to realize the complete deformation manipulation on a real humanoid robot."}, {"title": "VI. CONCLUSION", "content": "This paper proposes a dynamic heterogeneous graph-based dough manipulation policy learning model and elaborates its pipeline to deploy the model on a humanoid robot to achieve a dough rolling task with a rolling pin. We show the superiority of using a unified graph form, which can learn the goal-oriented policy and object dynamics and integrate the demonstration guidance simultaneously without the interference of environmental change. Compared with several baseline methods, the proposed DGform with a compact representation and demonstration guidance achieves a human-like dough rolling behavior. The generalizations of these methods are also analyzed, which show the robustness of DGform against environmental change. Finally, a realworld experiment was conducted on a CURI robot, aided by bimanual LQT with coordination."}]}