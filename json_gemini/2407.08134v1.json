{"title": "Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates", "authors": ["A. Noorizadegan", "Y.C. Hon", "D.L. Young", "C.S. Chen"], "abstract": "Surface reconstruction from point clouds is a fundamental challenge in computer graphics and medical imaging. In this paper, we explore the application of advanced neural network architectures for the accurate and efficient reconstruction of surfaces from data points. We introduce a novel variant of the Highway network (Hw) called Square-Highway (SqrHw) within the context of multilayer perceptrons and investigate its performance alongside plain neural networks and a simplified Hw in various numerical examples. These examples include the reconstruction of simple and complex surfaces, such as spheres, human hands, and intricate models like the Stanford Bunny. We analyze the impact of factors such as the number of hidden layers, interior and exterior points, and data distribution on surface reconstruction quality. Our results show that the proposed SqrHw architecture outperforms other neural network configurations, achieving faster convergence and higher-quality surface reconstructions. Additionally, we demonstrate the SqrHw's ability to predict surfaces over missing data, a valuable feature for challenging applications like medical imaging. Furthermore, our study delves into further details, demonstrating that the proposed method based on highway networks yields more stable weight norms and backpropagation gradients compared to the Plain Network architecture. This research not only advances the field of computer graphics but also holds utility for other purposes such as function interpolation and physics-informed neural networks, which integrate multilayer perceptrons into their algorithms. The codes implemented are available at: https://github.com/CMMAi/ResNet_for_PINN", "sections": [{"title": "1 Introduction", "content": "Computer graphics is a rapidly evolving field that continually seeks innovative techniques for surface reconstruction and visualization, offering a broad spectrum of applications from 3D object modeling to animation, rendering, and beyond [1].\nSurface reconstruction from point clouds has been a subject of considerable research effort, where methods are categorized into those that employ data-driven with prior knowledge and non-data-driven. Data-driven with prior knowledge methods (the first class) have emerged as potent solutions, deriving priors from extensive datasets. These methods, including AtlasNet [11], Scan2Mesh [12], Points2surf [1], and others, leverage feature representations to adeptly handle noisy or partial input. While these algorithms necessitate extensive datasets and a latent feature space, with a primary focus on encoding shapes that fall within this feature space, our approach charts a distinctive course.\nOn the other hand, non-data-driven class including methods like method of fundamental solutions [30], radial basis function method [4], scale space meshing [5] and Ohrhallinger's combinatorial approach [6], offers effective solutions but often struggles with partially data or non-smooth surfaces. Additionally, deformations and patch-based methods [7, 8] from this class prove to be limited in handling complex topologies and connectivity changes. Poisson reconstruction [9, 10] stands as the prevailing benchmark in non-data-driven surface reconstruction from point clouds. Notably, none of the previously mentioned methods employ a prior that distills information about typical surface shapes from a vast dataset.\nWhile the classical methods used in non-data-driven class are rooted in rigorous mathematics, difficulties arise in selecting uncertainties, such as the shape parameter in RBF methods or the location of center points in the method of fundamental solutions, making them challenging. Additionally, these methods often rely on partial differential equations (PDEs) like the Poisson equation [3] or modified Helmholtz equation [4], which further complicate implementation. Liu et al. [4] demonstrated significant sensitivity of results to parameters such as the shape parameter and A (a penalty factor in modified Helmholtz PDE), as illustrated in Figure 7 of their work, highlighting the challenges in uncertainty management within the method of fundamental solutions.\nOur approach falls under the category of non-data-driven methods, employing deep learning methods in particular feedforward (multi-layer perceptrons) architecture. In contrast to traditional non-data-driven methods that solve partial differential equations, our method focuses on simple function interpolation (approximation). While easier, deep learning methods can face stability and accuracy challenges. Drawing inspiration from advancements such as highway networks [22, 25] and subsequent residual networks (ResNet) [15,16], which enhance stability and accuracy in networks like convolutional and multi-layer perceptrons architectures, we introduce two novel networks. Our residual-based architecture significantly enhances both accuracy and stability.\nThese simplified residual-based architectures hold promise across various surface reconstruction scenarios and have shown efficacy in recent studies addressing interpolation"}, {"title": "2 Plain Neural Networks (Pn)", "content": ""}, {"title": "2.1 Multi-Layer Perceptrons (MLPs)", "content": "In this section, we will explore the intricate architecture and operational dynamics of Multi-Layer Perceptrons (MLPs), which form the bedrock of numerous deep learning frameworks. MLPs, represented by the symbol M, are meticulously crafted to approximate a function f : p \u2208 Rd \u2192 y \u2208 RD through the strategic arrangement of artificial neurons across successive layers."}, {"title": "2.2 Layer Configuration", "content": "The typical composition of an MLP includes:\n\u2022 Input Layer: Receives input data of dimension d.\n\u2022 Hidden Layers: Perform computations to uncover complex patterns.\n\u2022 Output Layer: Produces the network's predictions with dimensionality D.\nThe width of each layer, denoted as t(h), dictates the number of neurons within that layer. Let's consider a network with H hidden layers, where the output vector for the h-th layer is denoted as p(h) \u2208 Rt(h), serving as the input to the subsequent layer. The input signal provided by the input layer is denoted as p(0) = p \u2208 Rd.\nIn each layer h, 1 \u2264 h < H + 1, the i-th neuron performs an affine transformation followed by a non-linear operation:\nz(h) = W(h)p(h-1)+b(h),\nwhere 1 \u2264 i \u2264 t(h) and 1 \u2264 j \u2264 t(h-1). Thus,\np(h) = \u03c3(z(h)),\nfor 1 \u2264 i \u2264 t(h). Here, W(h) and b(h) represent the weights and biases associated with the i-th neuron of layer h, respectively, while \u03c3(\u00b7) denotes the activation function, which, in our case, is tanh. The overall behavior of the network, denoted as M : Rd \u2192 RD, can be conceptually understood as a sequence of alternating affine transformations and component-wise activations, as depicted in Eqs. (1)-(2). The architecture of a Multilayer"}, {"title": "2.3 Parameterization of the Network", "content": "The parameters characterizing the network comprise all the weights and biases, outlined as follows:\n\u2022 We denote these parameters as \u03c6 = {W(h), b(h)}H+1.\n\u2022 Each layer h possesses its weight matrix denoted by W(h) and a bias vector represented by b(h).\nConsequently, the network M(p; \u03c6) embodies a plethora of parameterized functions, where \u03c6 demands meticulous selection to ensure the network effectively approximates the target function f(p) at the input p."}, {"title": "2.4 Gradient Calculation using Back-propagation", "content": "In this section, we present the back-propagation expression \u2202L/\u2202W(h) as follow (further insights can be found in [26]):\n\u2202L/\u2202W(h) = \u2202L/\u2202z(h) \u2202z(h)/\u2202W(h) = \u2202L/\u2202z(h) p(h-1).\nwhere p(h-1) is the output at the h \u2013 1 layer and \u2202L/\u2202z(h) is the gradient of the loss with respect to the non-linear operation at layer h. Here, [p\u2297y]ij = pi\u22c5yj represents the outer product. Therefore, to evaluate \u2202L/\u2202W(h), both p(h-1), evaluated during the forward phase, and \u2202L/\u2202z(h), evaluated during back-propagation, are required. Numerical experiments on (3) for a specific epoch (iteration) number are presented in Fig. 10."}, {"title": "2.5 Training and Testing of MLPs", "content": "In supervised learning, training and testing phases optimize and assess neural networks. Given a dataset S = {(pi, yi) : 1 \u2264 i \u2264 n} representing a target function f : p \u2192 y, the network M(p; \u03c6, \u03b8) aims to approximate this function, where \u03c6 are network parameters and \u03b8 are hyperparameters. The process includes: 1. Training Phase: Train the network using Strain to find optimal parameters \u03c6\u2217 by minimizing:\n\u03c6\u2217 = argmin\u03c6 Ltrain(\u03c6) = 1/Ntrain \u03a3Ni=1||yi \u2212 M(pi; \u03c6, \u03b8)||2.\n2. Testing Phase: Evaluate the optimized network on Stest to assess performance on unseen data. Note that the dataset S is split into training and test sets for robust evaluation."}, {"title": "3 MLPs for Computer Graphics", "content": ""}, {"title": "3.1 Algorithm", "content": "In this section, we provide a mathematical foundation for the proposed method that leverages neural networks for surface generation in computer graphics. Let S represent the 3D domain, and let p = {p1, p2, ..., pn} denote the set of data points sampled within this domain. Each data point {pi}i=1n has spatial coordinates pj = (x1,j, x2,j, x3,j). We categorize the data points into three sets:\npsurface: Data points defining the surface boundary (Fig. 2, step 1)\nPinterior: Data points located within the 3D object (Fig. 2, step 2)\nPexterior: Data points outside the object (only considered for Example 4)\nWe define ni, ns, and ne as the number of points inside, on the surface, and outside of the domain S. We also consider n as the total number of data points, where n = ni+ns+ne.\nTo train the neural network, we label each data point pi as follows:\nLabel(pj) = { 1 : pj \u2208 Pinterior, 0 : pj \u2208 Psurface, \u22121: pj\u2208 Pexterior.\nThe proposed neural network architecture consists of a combination of feedforward and highway networks, with the power-enhanced version. It is designed to learn the distinct features associated with each category of data points. The loss function, chosen as the mean squared error (MSE), quantifies the difference between predicted labels and actual labels. It is expressed as:\nMSE = 1/n \u03a3ni=1 (Label(pi) \u2013 Prediction(pi))2\nwhere n is the number of data points (samples), Label(pi) represents the ground truth label, and Prediction(pi) is the network's predicted label for the input data point pi. The network's parameters, including weights and biases, are optimized using an iterative optimization algorithm, L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints), to minimize the MSE (Fig. 2, step 3). This process equips the network with the ability to accurately predict labels for new data points and, subsequently, to generate surfaces for 3D objects in computer graphics."}, {"title": "3.2 Testing and Surface Generation Algorithm", "content": "After training the neural network, the next step is to apply the model to generate surfaces for 3D objects. This involves testing the network's ability to predict labels for a set of testing data points. In our case, we employ a grid-based approach to define the testing"}, {"title": "4 Advanced Deep Neural Network Architectures", "content": ""}, {"title": "4.1 Highway Networks", "content": "This section introduces Highway Networks, a type of deep neural network designed to improve information propagation across multiple layers [22, 25]. Highway Networks use gating mechanisms to control information flow, allowing selective processing and preservation of input data. A conventional neural network can be represented as (Fig. 3(a)):\np(h) = f(h)(p(h-1), W(h))\nwhere p is the input to the layer and f(h)(p(h-1), W(h)) represents the operations at layer h - 1. Highway Networks add two gating mechanisms:\n\u2022 Nonlinear transformations, controlled by the transform gate T,\n\u2022 Activation transfer from the previous layer, managed by the carry gate C.\nThis is represented by:\np(h) = f(h)(p(h-1), W(h))\u22c5T(h)(p(h-1), W(h)) + p(h-1)\u22c5C(h)(p(h-1), W(h))\nwhere f(h)(p(h-1)) includes the linear and nonlinear functions within the network. This allows Highway Networks to learn both feedforward and shortcut connections, aiding the training of very deep networks and preserving crucial information. Highway Networks have shown promising results in tasks like speech recognition, natural language processing, and image classification [22,25]. However, this architecture require additional training for the carry and transform gates, which can be very costly. Another challenge in highway networks is dimensionality adjustment. Equation (7) requires the dimensions of p(h), T(h)(p(h-1), W(h)), and C(h)(p(h-1), W(h)) to be consistent. This can be managed by:\n\u2022 Sub-sampling or zero-padding p(h) to get p\u02dc(h),\n\u2022 Using a plain layer to adjust the dimensionality before stacking highway layers.\nThe following networks address and resolve the issues of extra training and dimensionality."}, {"title": "4.2 Residual Network", "content": "Residual Networks present a simplified version of Highway Networks [23], where the transformation is redefined as the sum of the input and a residual. Commonly referred to as ResNets [15,16], these networks have become prominent in neural network architectures.These networks are characterized by their residual modules, denoted as f(h), and skip connections that circumvent these modules, aiding in the creation of deep networks."}, {"title": "4.3 Proposed Simple Highway Network (Hw)", "content": "Here, we introduce a simplified version of the highway network, represented by Expression (7) as shown in Fig. 3(b):\np(h) = f(h)(p(h-1), W(h)) + Z(h)\nIn this equation, Z(h-1) = W(h)p(h-1)+b(h). To compare this simplified Highway network (11) with the original one in (7), we observe that in the simplified version, we set\nT(h)(p(h-1), W(h)) = 1\nand\np(h-1)\u22c5 C(h)(p(h-1), W(h)) = W(h)p(h-1)+b(h)\nThis implies that the weight updates for both W(h) and W(h) in the simplified Highway network are identical, eliminating the necessity for additional training in contrast with the original highway network. Additionally, in this case, no matrix size alignment is required since the weight of the plain network is considered."}, {"title": "4.4 Proposed Square-Highway Network (SqrHw)", "content": "Here, we propose a simple highway network, where (7) can be represented as (Fig. 3(c)):\np(h) = f(h)(p(h-1), W(h)) + Z(h)\u2297Z(h)\nwhere \u2297 denotes element-wise multiplication. In this equation, Z(h) = W(h)p(h-1)+b(h). To compare the proposed SqrHw network represented by (14) with the original one in (7), we note that in the simplified version, we set\nT(h)(p(h-1), W(h)) = 1\nand the term\np(h-1)\u22c5 C(h)(p(h-1), W(h)) = (W(h)p(h-1)+b(h))\u2297(W(h)p(h-1)+b(h))\nHere, the operation \u2297 represents element-wise multiplication, implying that the carry gate applies the same transformation to the input p(h-1) as the original Highway network, but with additional element-wise modification. Equation (16) suggests that the weight updates are identical (W(h) = W(h)) to those of a plain neural network, thereby removing the necessity for additional training or dimensionality alignment."}, {"title": "4.5 Development of Residual and Highway Schemes in Neural Networks", "content": "Works like the Highway Network and ResNet initially lacked rigorous mathematical proofs and relied heavily on empirical validation through extensive numerical experiments once they were introduced. Despite this, they have become widely adopted due to their empirical success, underscoring the importance of practical utility alongside mathematical rigor. Or well-cited works such as References [28, 29] introduced completely different residual schemes to the network. For instance, the authors in [28] introduce the residual connection from low-fidelity output to high-fidelity output, which will be implemented once in every epoch or iteration ( [28], Figure 1D). In reference [29], an elementwise multiplication operation is applied between the residual terms and the plain neural network's output ( [29], Equation 2.36). Although both of these methods diverge from the original ResNet and highway architectures [15, 22], they have proven to be successful approaches."}, {"title": "5 Numerical Analysis", "content": "In the context of this study, we make use of the notations n, ni, and nn to describe the count of data points, layers, and neurons in each layer, respectively. Additionally, ni, ns, ne, and nt are employed to denote the number of interior, surface, exterior, and validation data points. In this section, an examination of three distinct approaches is undertaken:\n1. Plain Network (Pn): A standard neural network lacking additional modifications or residual connections (refer to Fig. 3(a)).\n2. Simple Highway (Hw): This neural network architecture includes affine transformations that are added to the output of every other layer (illustrated in Fig. 3(b)).\nThe rationale for adding to every other layer rather than every layer is for efficiency purposes.\n3. Squared Highway (SqrHw): An innovative variation of the Highway network architecture, introducing squared affine transformation every other layer (depicted in Fig. 3(c)). The reason for adding to every other layer instead of every layer is for efficiency.\nIn all following examples, we exclusively considered interior points Pinterior in (4). In the last example, the Stanford Bunny, we incorporated both interior Pinterior and exterior points Pexterior for the surface reconstruction.\nExample 1 In our first numerical example, we examine the construction of a simple and smooth surface, a sphere. The sphere has a radius of 1 and is centered at the origin (0, 0, 0), as depicted in Fig. 4. We employ ns = 200 data points on the sphere's surface from Pinterior, along with ni = 20 interior points from Psurface, without any exterior points from Pexterior. The points are randomly selected. Fig. 5 displays the results of the surface construction over these points. The green surfaces represent the outputs of the neural network, while the red meshed spheres are the exact surface which provided for reference to enhance the visualization of the results.\nThe first row of Figure 5 illustrates the results using the plain network for different epochs, ranging from 10 to 400. The second row showcases the results for the same epochs, employing the simple Highway approach. Finally, the last row demonstrates the results for the proposed square highway.\nSeveral observations can be made from this example:\n\u2022 The proposed SqrHw demonstrates superior performance at specific iterations, especially when compared to the plain network. In other words, it converges faster"}, {"title": "Example 2", "content": "In this example, we delve into a more intricate surface: the hand. Additionally, we explore the details concerning weight updating and backpropagation in this example. Fig. 7 illustrates a node distribution where points on the hand's surface are depicted in blue, and the smaller interior points are shown in red.\nMoving on to the numerical results, Fig. 8 showcases the outcomes of three different network structures for 3000 points on the surface, with 500 interior points. The structure of the network comprises five hidden layers, each containing 50 neurons. The top panel corresponds to the plain network, the middle panel displays the results of the Hw algorithm, and the bottom panel exhibits the results of our proposed SqrHw network. Within each panel, columns represent different Epoch numbers, specifically 50, 5000, and 10000 Epochs, facilitating easy comparison of convergence across the three network architectures. The final column, Column 4, displays the results at the final epoch where convergence occurred. Several observations can be made from this figure:\n\u2022 At a small number of Epochs, such as 50 as depicted in the first column, the construction begins as a simple plate for all three network structures.\n\u2022 Subsequently, better convergence can be observed with an increasing number of Epochs, particularly for Hw and SqrHw, in contrast to the plain neural network. For example, a comparison between plots in Fig. 8(c,g,k) reveals the emergence of a recognizable \u201chand\" shape for Hw and SqrHw, while the Plain Neural Network produces non-meaningful results.\n\u2022 The final results, shown in the last column, reveal a near-perfect hand shape for SqrHw, as seen in Fig. 8(l). In contrast, the plain Neural Network's results, as illustrated in Fig. 8(d), are highly polluted, and Hw's outcomes, Fig. 8(h), feature some extraneous surface artifacts around the hand.\n\u2022 Notably, the plain neural network requires a substantially smaller number of Epochs (13250) for convergence over training data, while Hw exhibits the longest convergence duration at 21100 Epochs. Conversely, SqrHw achieves its final results at Epoch 18000, positioning it between the plain network and highway in terms of the number of required Epochs for convergence over training data.\nTo explore the factors underlying the disparities between the Pn and SqrHw models, we conducted an examination of the Frobenius norm of the weights updated across all hidden layers throughout every epoch. For this investigation, we selected the Frobenius norm, denoted as\n||W||F = \u221a(\u03a3Ti=1 \u03a3Nj=1 |Wij|2)\ndue to its ability to capture the overall magnitude and fluctuations of weight matrices. Here, T represents the number of epochs, and N indicates the number of weights across all layers at a specific epoch. The resulting visualizations, presented in Fig. 9, reveal\""}, {"title": "Example 3", "content": "In this instance, we embark on the endeavor of reconstructing the three-dimensional surface of the pregnant human uterus and abdomen where the point distribution are represented in Fig. 12(a) and Fig. 12(b), respectively.\nThe precise reconstruction of the uterus-abdomen geometry carries significant importance, particularly in maintaining the accuracy of electro-myometrial imaging (EMMI). EMMI is an innovative electro-physiology imaging modality designed to non-invasively capture the electrical activation patterns of the uterus as it undergoes mechanical contraction. To gather the data points for the abdomen, we employed an optical scanning device. For the uterus, data points were obtained through Magnetic Resonance Imaging (MRI). It is worth noting that MRI is a costly procedure and can be conducted exclusively in clinics equipped with MRI facilities. All data were graciously provided by the Integrated Biomedical Imaging Laboratory at the Department of Obstetrics and Gynecology, School of Medicine, Washington University at St. Louis.\nFor this example, we utilized 5000 data points for reconstructing the abdominal surface and 300 points for the uterus. The network consists of five hidden layers, each with 50 neurons. The final results, depicted in Fig. 13, showcase impressively accurate reconstructions from various perspectives. We also explored a scenario involving missing"}, {"title": "Example 4", "content": "In our final example, we tackle the reconstruction of a complex domain, the Stanford Bunny [20] as the point distribution is shown in Fig. 15.\nFigure 16 provides the outcomes when applying the plain network (left panel), Hw (middle panel), and the SqrHw (right panel). The top row showcases the bunny's front view, while the bottom row reveals the rear view. For this illustration, we maintain a consistent set of parameters, ns = 8000 and ni = 3000. We also implemented a neural network with five hidden layers, each containing 50 neurons."}, {"title": "6 Conclusion", "content": "Our study provides a thorough analysis of neural network architectures for surface reconstruction from point clouds within the realm of computer graphics. Through empirical evaluations and numerical analyses, we have shed light on the efficacy of different network structures in tackling the challenges inherent in surface reconstruction tasks, thereby advancing the understanding of deep learning methods for surface generation.\n\u2022 Our proposed Squared Highway (SqrHw) network architecture exhibits superior performance compared to conventional plain neural networks.\n\u2022 The SqrHw network demonstrates faster convergence and more stable optimization behavior, thanks to the introduction of a carry gate in the architecture. This architectural feature facilitates efficient gradient flow and mitigates the vanishing gradient problem.\n\u2022 Empirical evaluations on various datasets, encompassing simple geometric shapes and complex objects like the human hand and the Satnford Bunny, underscore the effectiveness of the proposed architecture in accurately reconstructing surfaces from point clouds.\n\u2022 Analysis of network parameters, such as the number of hidden layers and neurons, emphasizes the significance of selecting an appropriate architecture configuration to attain optimal reconstruction results.\nThe analysis of the Frobenius norm of weight updates revealed distinct patterns in weight evolution between the two models. Specifically, the SqrHw model demonstrated a convergence to stable weight norms, indicating smoother optimization compared to the fluctuating and increasing behavior observed in the Pn model. Furthermore, examination of back-propagated gradients highlighted the SqrHw's ability to mitigate the vanishing gradient issue, leading to enhanced training stability and faster convergence. The presence of the carry gate in SqrHw facilitates more efficient optimization by enabling gradients to bypass multiple layers, thereby addressing optimization challenges encountered in deep networks.\nFuture research can extend the application of the MLPs architecture to various purposes, including physics-informed neural networks. Additionally, one can incorporate partial differential equations such as the Helmholtz equations for computer graphics problems [30] within the context of physics-informed neural networks."}]}