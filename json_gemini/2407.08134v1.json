{"title": "Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates", "authors": ["A. Noorizadegan", "Y.C. Hon", "D.L. Young", "C.S. Chen"], "abstract": "Surface reconstruction from point clouds is a fundamental challenge in computer graphics and medical imaging. In this paper, we explore the application of advanced neural network architectures for the accurate and efficient reconstruction of surfaces from data points. We introduce a novel variant of the Highway network (Hw) called Square-Highway (SqrHw) within the context of multilayer perceptrons and investigate its performance alongside plain neural networks and a simplified Hw in various numerical examples. These examples include the reconstruction of simple and complex surfaces, such as spheres, human hands, and intricate models like the Stanford Bunny. We analyze the impact of factors such as the number of hidden layers, interior and exterior points, and data distribution on surface reconstruction quality. Our results show that the proposed SqrHw architecture outperforms other neural network configurations, achieving faster convergence and higher-quality surface reconstructions. Additionally, we demonstrate the SqrHw's ability to predict surfaces over missing data, a valuable feature for challenging applications like medical imaging. Furthermore, our study delves into further details, demonstrating that the proposed method based on highway networks yields more stable weight norms and backpropagation gradients compared to the Plain Network architecture. This research not only advances the field of computer graphics but also holds utility for other purposes such as function interpolation and physics-informed neural networks, which integrate multilayer perceptrons into their algorithms. The codes implemented are available at: https://github.com/CMMAi/ResNet_for_PINN", "sections": [{"title": "1 Introduction", "content": "Computer graphics is a rapidly evolving field that continually seeks innovative techniques for surface reconstruction and visualization, offering a broad spectrum of applications from 3D object modeling to animation, rendering, and beyond [1].\nSurface reconstruction from point clouds has been a subject of considerable research effort, where methods are categorized into those that employ data-driven with prior knowledge and non-data-driven. Data-driven with prior knowledge methods (the first class) have emerged as potent solutions, deriving priors from extensive datasets. These methods, including AtlasNet [11], Scan2Mesh [12], Points2surf [1], and others, leverage feature representations to adeptly handle noisy or partial input. While these algorithms necessitate extensive datasets and a latent feature space, with a primary focus on encoding shapes that fall within this feature space, our approach charts a distinctive course.\nOn the other hand, non-data-driven class including methods like method of fundamental solutions [30], radial basis function method [4], scale space meshing [5] and Ohrhallinger's combinatorial approach [6], offers effective solutions but often struggles with partially data or non-smooth surfaces. Additionally, deformations and patch-based methods [7, 8] from this class prove to be limited in handling complex topologies and connectivity changes. Poisson reconstruction [9, 10] stands as the prevailing benchmark in non-data-driven surface reconstruction from point clouds. Notably, none of the previously mentioned methods employ a prior that distills information about typical surface shapes from a vast dataset.\nWhile the classical methods used in non-data-driven class are rooted in rigorous mathematics, difficulties arise in selecting uncertainties, such as the shape parameter in RBF methods or the location of center points in the method of fundamental solutions, making them challenging. Additionally, these methods often rely on partial differential equations (PDEs) like the Poisson equation [3] or modified Helmholtz equation [4], which further complicate implementation. Liu et al. [4] demonstrated significant sensitivity of results to parameters such as the shape parameter and A (a penalty factor in modified Helmholtz PDE), as illustrated in Figure 7 of their work, highlighting the challenges in uncertainty management within the method of fundamental solutions.\nOur approach falls under the category of non-data-driven methods, employing deep learning methods in particular feedforward (multi-layer perceptrons) architecture. In contrast to traditional non-data-driven methods that solve partial differential equations, our method focuses on simple function interpolation (approximation). While easier, deep learning methods can face stability and accuracy challenges. Drawing inspiration from advancements such as highway networks [22, 25] and subsequent residual networks (ResNet) [15,16], which enhance stability and accuracy in networks like convolutional and multi-layer perceptrons architectures, we introduce two novel networks. Our residual-based architecture significantly enhances both accuracy and stability.\nThese simplified residual-based architectures hold promise across various surface reconstruction scenarios and have shown efficacy in recent studies addressing interpolation"}, {"title": "2 Plain Neural Networks (Pn)", "content": null}, {"title": "2.1 Multi-Layer Perceptrons (MLPs)", "content": "In this section, we will explore the intricate architecture and operational dynamics of Multi-Layer Perceptrons (MLPs), which form the bedrock of numerous deep learning frameworks. MLPs, represented by the symbol M, are meticulously crafted to approximate a function $f : p \\in \\mathbb{R}^d \\rightarrow y \\in \\mathbb{R}^D$ through the strategic arrangement of artificial neurons across successive layers."}, {"title": "2.2 Layer Configuration", "content": "The typical composition of an MLP includes:\n\u2022 Input Layer: Receives input data of dimension d.\n\u2022 Hidden Layers: Perform computations to uncover complex patterns.\n\u2022 Output Layer: Produces the network's predictions with dimensionality D.\nThe width of each layer, denoted as $t^{(h)}$, dictates the number of neurons within that layer. Let's consider a network with H hidden layers, where the output vector for the h-th layer is denoted as $p^{(h)} \\in \\mathbb{R}^{t^{(h)}}$, serving as the input to the subsequent layer. The input signal provided by the input layer is denoted as $p^{(0)} = p \\in \\mathbb{R}^d$.\nIn each layer h, $1 \\leq h < H + 1$, the i-th neuron performs an affine transformation followed by a non-linear operation:\n$z^{(h)}_i = \\sum^{t^{(h-1)}}_{j=1} W^{(h)}_{ij} p^{(h-1)}_j + b^{(h)}_i,$\n(1)\nwhere $1 \\leq i \\leq t^{(h)}$ and $1 \\leq j \\leq t^{(h-1)}$. Thus,\n$p^{(h)}_i = \\sigma(z^{(h)}_i),$\n(2)\nfor $1 \\leq i \\leq t^{(h)}$. Here, $W^{(h)}_{ij}$ and $b^{(h)}_i$ represent the weights and biases associated with the i-th neuron of layer h, respectively, while $\\sigma(\\cdot)$ denotes the activation function, which, in our case, is tanh. The overall behavior of the network, denoted as $M : \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$, can be conceptually understood as a sequence of alternating affine transformations and component-wise activations, as depicted in Eqs. (1)-(2). The architecture of a Multilayer"}, {"title": "2.3 Parameterization of the Network", "content": "The parameters characterizing the network comprise all the weights and biases, outlined as follows:\n\u2022 We denote these parameters as $\\phi = \\{W^{(h)}, b^{(h)}\\}_{H+1}$.\n\u2022 Each layer h possesses its weight matrix denoted by $W^{(h)}$ and a bias vector represented by $b^{(h)}$.\nConsequently, the network M(p; $\\phi$) embodies a plethora of parameterized functions, where $\\phi$ demands meticulous selection to ensure the network effectively approximates the target function f(p) at the input p."}, {"title": "2.4 Gradient Calculation using Back-propagation", "content": "In this section, we present the back-propagation expression $\\frac{\\partial \\mathcal{L}}{\\partial W^{(h)}}$ as follow (further insights can be found in [26]):\n$\\frac{\\partial \\mathcal{L}}{\\partial W^{(h)}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}} \\frac{\\partial z^{(h)}}{\\partial W^{(h)}} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}} p^{(h-1)}.$\n(3)\nwhere $p^{(h-1)}$ is the output at the $h-1$ layer and $\\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}}$ is the gradient of the loss with respect to the non-linear operation at layer h. Here, $[p^{(h-1)} \\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}}]_{ij} = p^{(h-1)}_i \\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}}_j$ represents the outer product. Therefore, to evaluate $\\frac{\\partial \\mathcal{L}}{\\partial W^{(h)}}$, both $p^{(h-1)}$, evaluated during the forward phase, and $\\frac{\\partial \\mathcal{L}}{\\partial z^{(h)}}$, evaluated during back-propagation, are required. Numerical experiments on (3) for a specific epoch (iteration) number are presented in Fig. 10."}, {"title": "2.5 Training and Testing of MLPs", "content": "In supervised learning, training and testing phases optimize and assess neural networks. Given a dataset $S = \\{(p_i, y_i) : 1 \\leq i \\leq n\\}$ representing a target function $f : p \\rightarrow y$, the network M(p; $\\phi, \\theta$) aims to approximate this function, where $\\phi$ are network parameters and $\\theta$ are hyperparameters. The process includes: 1. Training Phase: Train the network using $S_{train}$ to find optimal parameters $\\phi^*$ by minimizing:\n$\\phi^* = \\underset{\\phi}{\\text{arg min }} L_{train}(\\phi) = \\underset{\\phi}{\\text{arg min }} \\frac{1}{N_{train}} \\sum^{N_{train}}_{i=1} ||y_i \u2013 M(p_i; \\phi, \\theta)||^2_2$\n2. Testing Phase: Evaluate the optimized network on $S_{test}$ to assess performance on unseen data. Note that the dataset S is split into training and test sets for robust evaluation."}, {"title": "3 MLPs for Computer Graphics", "content": null}, {"title": "3.1 Algorithm", "content": "In this section, we provide a mathematical foundation for the proposed method that leverages neural networks for surface generation in computer graphics. Let S represent the 3D domain, and let p = {$p_1, p_2, ..., p_n$} denote the set of data points sampled within this domain. Each data point {$p_i$}$_{i=1}^n$ has spatial coordinates $p_j = (x_{1,j}, x_{2,j}, x_{3,j})$. We categorize the data points into three sets:\n$p_{surface}$: Data points defining the surface boundary (Fig. 2, step 1)\n$p_{interior}$: Data points located within the 3D object (Fig. 2, step 2)\n$p_{exterior}$: Data points outside the object (only considered for Example 4)\nWe define $n_i$, $n_s$, and $n_e$ as the number of points inside, on the surface, and outside of the domain S. We also consider n as the total number of data points, where $n = n_i + n_s + n_e$.\nTo train the neural network, we label each data point $p_i$ as follows:\nLabel(p_j) = {1 : p_j \\in P_{interior}\n0 : p_j \\in P_{surface}\n-1: p_j \\in P_{exterior}\n(4)\nThe proposed neural network architecture consists of a combination of feedforward and highway networks, with the power-enhanced version. It is designed to learn the distinct features associated with each category of data points. The loss function, chosen as the mean squared error (MSE), quantifies the difference between predicted labels and actual labels. It is expressed as:\nMSE = $1/n \\sum^{n}_{i=1} (Label(p_i) \u2013 Prediction(p_i))^2$\n(5)\nwhere n is the number of data points (samples), $Label(p_i)$ represents the ground truth label, and $Prediction(p_i)$ is the network's predicted label for the input data point $p_i$. The network's parameters, including weights and biases, are optimized using an iterative optimization algorithm, L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints), to minimize the MSE (Fig. 2, step 3). This process equips the network with the ability to accurately predict labels for new data points and, subsequently, to generate surfaces for 3D objects in computer graphics."}, {"title": "3.2 Testing and Surface Generation Algorithm", "content": "After training the neural network, the next step is to apply the model to generate surfaces for 3D objects. This involves testing the network's ability to predict labels for a set of testing data points. In our case, we employ a grid-based approach to define the testing"}, {"title": "4 Advanced Deep Neural Network Architectures", "content": null}, {"title": "4.1 Highway Networks", "content": "This section introduces Highway Networks, a type of deep neural network designed to improve information propagation across multiple layers [22, 25]. Highway Networks use gating mechanisms to control information flow, allowing selective processing and preservation of input data. A conventional neural network can be represented as (Fig. 3(a)):\n$p^{(h)} = f^{(h)}(p^{(h-1)}, W^{(h)})$\n(6)\nwhere p is the input to the layer and $f^{(h)}(p^{(h-1)}, W^{(h)})$ represents the operations at layer h - 1. Highway Networks add two gating mechanisms:\n\u2022 Nonlinear transformations, controlled by the transform gate T,"}, {"title": "4.2 Residual Network", "content": "Residual Networks present a simplified version of Highway Networks [23], where the transformation is redefined as the sum of the input and a residual. Commonly referred to as ResNets [15,16], these networks have become prominent in neural network architectures.These networks are characterized by their residual modules, denoted as $f^{(h)}$, and skip connections that circumvent these modules, aiding in the creation of deep networks."}, {"title": "4.3 Proposed Simple Highway Network (Hw)", "content": "Here, we introduce a simplified version of the highway network, represented by Expression (7) as shown in Fig. 3(b):\n$p^{(h)} = f^{(h)}(p^{(h-1)}, W^{(h)}) + Z^{(h)}$\n(11)\nIn this equation, $Z^{(h-1)} = W^{h} p^{(h-1)}+b^{(h)}$. To compare this simplified Highway network (11) with the original one in (7), we observe that in the simplified version, we set\nT^{(h)} (p^{(h-1)}, W^{(h)}) = 1\n(12)\nand\np^{(h-1)}. C^{(h)} (p^{(h-1)}, W^{(h)}) = W^{h} p^{(h-1)}+b^{(h)}\n(13)\nThis implies that the weight updates for both $W^{h}$ and $W^{(h)}$ in the simplified Highway network are identical, eliminating the necessity for additional training in contrast with the original highway network. Additionally, in this case, no matrix size alignment is required since the weight of the plain network is considered."}, {"title": "4.4 Proposed Square-Highway Network (SqrHw)", "content": "Here, we propose a simple highway network, where (7) can be represented as (Fig. 3(c)):\n$p^{(h)} = f^{(h)}(p^{(h-1)}, W^{(h)}) + Z^{(h)} \\cdot Z^{(h)}$\n(14)"}, {"title": "4.5 Development of Residual and Highway Schemes in Neural Networks", "content": "Works like the Highway Network and ResNet initially lacked rigorous mathematical proofs and relied heavily on empirical validation through extensive numerical experiments once they were introduced. Despite this, they have become widely adopted due to their empirical success, underscoring the importance of practical utility alongside mathematical rigor. Or well-cited works such as References [28, 29] introduced completely different residual schemes to the network. For instance, the authors in [28] introduce the residual connection from low-fidelity output to high-fidelity output, which will be implemented once in every epoch or iteration ( [28], Figure 1D). In reference [29], an elementwise multiplication operation is applied between the residual terms and the plain neural network's output ( [29], Equation 2.36). Although both of these methods diverge from the original ResNet and highway architectures [15, 22], they have proven to be successful approaches."}, {"title": "5 Numerical Analysis", "content": "In the context of this study, we make use of the notations n, $n_i$, and $n_n$ to describe the count of data points, layers, and neurons in each layer, respectively. Additionally, $n_i, n_s$, $n_e$, and $n_t$ are employed to denote the number of interior, surface, exterior, and validation data points. In this section, an examination of three distinct approaches is undertaken:\n1. Plain Network (Pn): A standard neural network lacking additional modifications or residual connections (refer to Fig. 3(a)).\n2. Simple Highway (Hw): This neural network architecture includes affine transformations that are added to the output of every other layer (illustrated in Fig. 3(b)).\nThe rationale for adding to every other layer rather than every layer is for efficiency purposes.\n3. Squared Highway (SqrHw): An innovative variation of the Highway network architecture, introducing squared affine transformation every other layer (depicted in Fig. 3(c)). The reason for adding to every other layer instead of every layer is for efficiency.\nIn all following examples, we exclusively considered interior points $P_{interior}$ in (4). In the last example, the Stanford Bunny, we incorporated both interior $P_{interior}$ and exterior points $P_{exterior}$ for the surface reconstruction."}, {"title": "6 Conclusion", "content": "Our study provides a thorough analysis of neural network architectures for surface reconstruction from point clouds within the realm of computer graphics. Through empirical evaluations and numerical analyses, we have shed light on the efficacy of different network structures in tackling the challenges inherent in surface reconstruction tasks, thereby advancing the understanding of deep learning methods for surface generation.\n\u2022 Our proposed Squared Highway (SqrHw) network architecture exhibits superior performance compared to conventional plain neural networks.\n\u2022 The SqrHw network demonstrates faster convergence and more stable optimization behavior, thanks to the introduction of a carry gate in the architecture. This architectural feature facilitates efficient gradient flow and mitigates the vanishing gradient problem.\n\u2022 Empirical evaluations on various datasets, encompassing simple geometric shapes and complex objects like the human hand and the Satnford Bunny, underscore the effectiveness of the proposed architecture in accurately reconstructing surfaces from point clouds.\n\u2022 Analysis of network parameters, such as the number of hidden layers and neurons, emphasizes the significance of selecting an appropriate architecture configuration to attain optimal reconstruction results.\nThe analysis of the Frobenius norm of weight updates revealed distinct patterns in weight evolution between the two models. Specifically, the SqrHw model demonstrated a convergence to stable weight norms, indicating smoother optimization compared to the fluctuating and increasing behavior observed in the Pn model. Furthermore, examination of back-propagated gradients highlighted the SqrHw's ability to mitigate the vanishing gradient issue, leading to enhanced training stability and faster convergence. The presence of the carry gate in SqrHw facilitates more efficient optimization by enabling gradients to bypass multiple layers, thereby addressing optimization challenges encountered in deep networks.\nFuture research can extend the application of the MLPs architecture to various purposes, including physics-informed neural networks. Additionally, one can incorporate partial differential equations such as the Helmholtz equations for computer graphics problems [30] within the context of physics-informed neural networks."}]}