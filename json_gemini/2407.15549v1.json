{"title": "Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs", "authors": ["Abhay Sheshadri", "Aidan Ewart", "Phillip Guo", "Aengus Lynch", "Cindy Wu", "Vivek Hebbar", "Henry Sleight", "Asa Cooper Stickland", "Ethan Perez", "Dylan Hadfield-Menell", "Stephen Casper"], "abstract": "Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of \u2018jailbreaking' techniques to elicit harmful\ntext from models that were fine-tuned to be harmless. Recent work on red-teaming,\nmodel editing, and interpretability suggests that this challenge stems from how\n(adversarial) fine-tuning largely serves to suppress rather than remove undesirable\ncapabilities from LLMs. Prior work has introduced latent adversarial training\n(LAT) as a way to improve robustness to broad classes of failures. These prior\nworks have considered untargeted latent space attacks where the adversary perturbs\nlatent activations to maximize loss on examples of desirable behavior. Untargeted\nLAT can provide a generic type of robustness but does not leverage information\nabout specific failure modes. Here, we experiment with targeted LAT where the\nadversary seeks to minimize loss on a specific competing task. We find that it can\naugment a wide variety of state-of-the-art methods. First, we use targeted LAT\nto improve robustness to jailbreaks, outperforming a strong R2D2 baseline with\norders of magnitude less compute. Second, we use it to more effectively remove\nbackdoors with no knowledge of the trigger. Finally, we use it to more effectively\nunlearn knowledge for specific undesirable tasks in a way that is also more robust\nto re-learning. Overall, our results suggest that targeted LAT can be an effective\ntool for defending against harmful behaviors from LLMs.", "sections": [{"title": "1 Introduction", "content": "Despite efforts from developers to remove harmful capabilities from large language models (LLMs),\nthey can persistently exhibit undesirable behaviors. For example, a string of recent red-teaming papers\nhave demonstrated diverse techniques that can be used to elicit instructions for building bombs\nfrom state-of-the-art LLMs. Developers have made progress on these problems using improved data\n(e.g., ) and adversarial training (e.g., ). However, some harmful capabilities resist removal\nvia fine-tuning and have been a persistent challenge toward building safer and more trustworthy\nmodels.\nRecent work suggests that fine-tuning modifies LLMs in superficial ways that can fail to make\nthem behave harmlessly in all circumstances. As discussed above, the LLM red-teaming literature\nhas consistently produced methods to elicit harmful text from state-of-the-art LLMs that were\nexplicitly fine-tuned for harmlessness. Meanwhile, research on interpretability, representation\nengineering, continual learning, and fine-tuning has suggested that\nfine-tuning struggles to make fundamental changes to an LLM's inner knowledge and capabilities.\nFor example, Jain et al. likened fine-tuning in LLMs to merely modifying a \u201cwrapper\" around a\nstable, general-purpose set of latent capabilities.\nIn this paper, we use latent adversarial training (LAT) to make LLMs more robust to\nexhibiting persistent unwanted behaviors. In contrast to adversarial training (AT) with perturbations\nto the model's inputs, we train the model with perturbations to its hidden latent representations.\nBecause models represent features at a higher level of abstraction in the latent space, we\nhypothesize that LAT can better facilitate the removal of neural circuitry responsible for unwanted\nbehaviors. Prior work has considered untargeted LAT where the adversary attempts to maximize\nprediction loss on the target task. In this work, we consider the case in which there is a specific type\nof capability (e.g., a backdoor) that we want to remove. Unlike prior work, we train LLMs under\ntargeted latent-space perturbations designed to elicit specific undesirable behaviors. We use targeted\nLAT on top of existing fine-tuning and adversarial training techniques and show that it can better\nremove undesirable behaviors from LLMs with little to no tradeoff with performance in typical use\ncases. We make two contributions:\n1. We propose targeted latent adversarial training (LAT) as a way to more thoroughly remove\nundesirable behaviors from LLMs.\n2. We show that targeted LAT can combine with and improve over a wide range of state-of-the-\nart techniques.\n(a) In Section 4.1, we show that LAT can greatly improve refusal training's ability to make\nLLMs robust to jailbreaks. We find that LAT outperforms R2D2 with orders of\nmagnitude less compute.\n(b) In Section 4.2, we use LAT to greatly improve DPO's ability to remove LLM\nbackdoors when the trigger is unknown and the response is only vaguely specified. Our\nresults suggest that LAT is a solution to the 'Sleeper Agent' problem posed in Hubinger\net al. .\n(c) In Section 4.3, we use LAT to improve on the abilities of WHP, gradient ascent\n, and RMU to unlearn unwanted knowledge. We also show that it can do so\nmore robustly, substantially decreasing the sample efficiency of re-learning previously\nunlearned knowledge."}, {"title": "2 Related Work", "content": "Latent Adversarial Training (LAT) Latent-space attacks and LAT have been previously studied\nin vision models and language models. Our work is closely related to Casper\net al. , who used LAT to defend against backdoors and unforeseen classes of adversarial attacks.\nHowever, in contrast to all of the above, we use targeted LAT in which the adversary aims to elicit\nspecific outputs corresponding to unwanted behaviors from the LLM. This makes our work similar\nto concurrent work by Xhonneux et al.  who also use targeted adversarial training, but do\nin the model's embedding space instead of its latents. Meanwhile, several works have shown that\nthe high-level behaviors of LLMs can be altered using perturbations to their internal activations"}, {"title": "3 Methods", "content": "Targeted latent adversarial training We can view an LLM with parameters \u03b8, as a composition\nof two functions, $LLM_\\theta(x_i) = (g_\\theta \\circ f_\\theta)(x_i)$, where $f_\\theta$ is a feature extractor which maps text to\nl atent activations $l_i = f_\\theta(x_i) \\in \\mathbb{R}^{s\\times d}$ and $g_\\theta$ maps those latent activations to output a probability\ndistribution for sampling: i.e., $\\hat{y}_i \\sim P(y | g_\\theta(l_i))$. We define an adversarial attack as a function $a$ with\nparameters $\\delta$ which modifies the LLM's inputs or latent activations.\nDuring standard AT, the model is trained to be robust to attacks in the input space via some training\nloss function, $\\mathcal{L}$. The training objective is thus $\\min_\\theta \\sum_i \\mathcal{L}(g_\\theta(f_\\theta(a_{\\delta}(x_i))), y_i)$. In contrast, during\nlatent adversarial training, the model is instead trained to be robust to attacks to the latent activations:\n$\\min_\\theta \\sum_i \\mathcal{L}(g_\\theta(a_{\\delta}(f_\\theta(x_i))), y_i)$ (1)\nDuring untargeted LAT (e.g., ), the attacker seeks to steer the model away from the desired\nbehavior on a training example $(x_i, y_i)$. The attacker's objective is thus $\\max_{\\delta_i} \\mathcal{L}(g_\\theta(a_{\\delta_i}(f_\\theta(x_i))), y_i)$.\nHowever, during targeted LAT, the attacker seeks to steer the model toward some undesirable target\nbehavior $\\tilde{y}$:\n$\\min_{\\delta_i} \\mathcal{L}(g_\\theta (a_{\\delta_i}(f_\\theta (x_i))), \\tilde{y}_i)$ (2)\nTraining methods Performing basic targeted LAT requires a dataset of prompts and paired harmless\nand harmful completions $(x_i, y_i, \\tilde{y}_i) \\sim \\mathcal{D}_p$ to optimize the model and the attacks. In most cases,\nhowever, we also find that interleaving LAT with supervised fine-tuning on an additional auxiliary\ndataset of benign text $(x_i, y_i) \\sim \\mathcal{D}_b$ can stabilize training and reduce side effects (see Section 4 for\ndetails). Here, as done in Casper et al. , we attack the residual stream of transformer LLMs with\nL2-norm-bounded perturbations, calculated using projected gradient descent (PGD) . Because\nthe model and attacker are optimized using different completions to prompts, we only perturb the\ntoken positions in the residual stream corresponding to the prompt (and not the completions) \u2013 see\nFigure 1. In practice, we found that perturbing the residual stream at multiple layers rather than a\nsingle layer, each with its own $\\epsilon$ constraint typically yielded better results. In all experiments, we\nperformed hyperparameter sweeps to select a perturbation bound and one or more layers."}, {"title": "4 Experiments", "content": "Our approach: augmenting fine-tuning and adversarial training methods with LAT Here,\nwe experiment with targeted LAT for improving robustness to jailbreaks, unlearning undesirable\nknowledge, and removing backdoors. Across experiments, we show how LAT can be used to augment"}, {"title": "4.1 Improving Robustness to Jailbreaks", "content": "Here, we demonstrate that targeted LAT can be helpful for making models more resistant to exhibiting\nunwanted behaviors via jailbreaking attacks with minimal side effects.\nData We create a dataset of triples containing: prompts, harmful completions, and harmless\ncompletions using a method based on Self-Instruct . We first generate a set of harmful user"}, {"title": "4.2 Backdoor Removal", "content": "Backdoors can have arbitrary triggers and response behaviors, which makes it challenging to find\nand remove them using standard techniques . Here, we use LAT to greatly increase the\neffectiveness of backdoor removal in cases when the backdoor response is vaguely known but the\ntrigger is not.\nModels and data We use the five backdoored LLMs from Rando et al.  who implanted\nthe backdoors using RLHF  such that, upon encountering specific keyword triggers (see\nTable 3), the model would respond in a helpful and harmful way as opposed to a helpful and harmless\none. We consider the challenge of removing a backdoor when the trigger is unknown and the response"}, {"title": "4.3 Machine Unlearning", "content": "Here, our goal is to augment methods for unlearning harmful or copyrighted knowledge from LLMs.\nWe first unlearn knowledge of Harry Potter (Section 4.3.1) and second unlearn potentially harmful\nbiology and cyber knowledge (Section 4.3.2)."}, {"title": "4.3.1 Who's Harry Potter?", "content": "Following work on unlearning knowledge of Harry Potter from Eldan and Russinovich , we\nshow that targeted LAT can improve the robustness of unlearning without sacrificing the model's\nperformance on other topics.\nModel and methods We work with the \"Who's Harry Potter\" (WHP) method from Eldan and\nRussinovich . Their method involves taking a corpus of text to forget (e.g., the Harry Potter\nbooks), constructing alternative genericized text for that corpus, and then fine-tuning the model on the"}, {"title": "4.3.2 Unlearning WMDP Biology and Cyber Knowledge", "content": "Following work from Li et al. , who studied the unlearning of potentially dangerous biology and\ncyber knowledge, we show that targeted LAT can help to improve existing approaches for unlearning.\nData As in as in Li et al. , we use the WMDP biology and cyber corpora as forget datasests\nand WikiText as a retain dataset. Note that, unlike in the experiments above, the forget and\nretain corpora were separate and did not come as paired sets of desirable/undesirable completions for\na prompt.\nModel and methods As in Li et al. , we use Zephyr-7B off the shelf . We test two\ndifferent unlearning methods with and without targeted LAT. First, we use a shaped gradient ascent\n(GA) method inspired by . We fine-tune the model to jointly minimize training loss on the retain\nset and log(1-p) on the forget set as done in Mazeika et al. . To augment GA with targeted LAT,\nwe apply latent-space perturbations optimized to minimize training loss on the forget set. To stabilize\ntraining, we also interleave training batches with supervised finetuning on the Alpaca dataset .\nSecond, we use representation misdirection for unlearning (RMU) from Li et al. . With RMU,\nthe model is trained at a given layer to (1) map activations from forget-set prompts to a randomly\nsampled vector while (2) leaving activations from other prompts unaltered. To augment RMU with\ntargeted LAT, we apply latent-space adversarial perturbations only when training on the forget set. We\noptimize these perturbations to minimize the model's cross-entropy training loss on the undesirable\nforget-set example. We experimented with various layer combinations and found the best results\nfrom applying them to the activations immediately preceding the RMU layer.\nEvaluation We evaluate how well the model's general capabilities have been preserved by testing\non MMLU  and AGIEval . We evaluate the effectiveness of unlearning in the model\nusing biology and cyber knowledge assessments from Li et al. . To test the robustness of the\nunlearning, we also evaluate models under few-shot finetuning attacks in which an attacker seeks to\nextract knowledge by finetuning the model on a small number of examples . Here, we use\na simple but surprisingly effective attack: we randomly sample a single batch of 2 examples from\nthe relevant forget set and repeatedly train on that single batch for 20 iterations. We then report the\nhighest WMDP bio/cyber performances for each model across evaluation checkpoints at 5, 10, and\n20 steps. For all evaluations, we use 1,000 samples on lm-evaluation-harness v0.4.0  as done in\nLi et al. ."}, {"title": "5 Discussion", "content": "Targeted LAT can effectively augment existing state-of-the-art fine-tuning and adversarial\ntraining methods. By attacking the model's latent representations, LAT offers a unique solution\nbecause models represent concepts at a higher level of abstraction in the latent space [2]. Here, we\nhave used targeted latent adversarial training (LAT) to strengthen existing defenses against persistent\nharmful behaviors in LLMs. We have tested targeted LAT by applying it to three current challenges\nwith state-of-the-art LLMs: jailbreaking , unlearning , and backdoor removal .\nIn each case, we have shown that LAT can augment existing techniques to improve the removal to\nunwanted behaviors with little or no tradeoff in general performance. This is especially notable in the\ncase of backdoor removal in which DPO alone fails to remove backdoors while DPO with LAT does\nso very effectively. This suggests that LAT can be a solution to the \"Sleeper Agent\" problem posed\nin Hubinger et al.  and Pawelczyk et al.  in which harmful backdoors can persist through\nadversarial training."}, {"title": "A Loss Functions for LAT", "content": "A.1 R2D2-LAT\nHere, we describe the RT-LAT method described in Section 4.1 in greater detail. We assume we\nare given two datasets \u2014 a dataset of harmful requests and pairs of preferred and rejected com-\npletions $\\mathcal{D}_p = \\{(x_i, c_i, r_i)\\}$, and a generic dataset of benign requests and helpful completions\n$\\mathcal{D}_b = \\{(x_i, y_i)\\}$. For each batch, we train the adversarial attack $\\delta$ to minimize $\\mathcal{L}_{attack}$:\n$\\mathcal{L}_{attack} =$\n$\\text{- log } P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i)) + \u2212 \\text{log}(1 \u2013 P(c_i|g_\\theta(f_\\theta(x_i) + \u03b4_i)))$\n(3)\nMove towards harmful completions\nMove away from harmless completions\nWe additionally add the constraint that $||\u03b4_i||_2 \\le \\epsilon$, where $\\epsilon$ is a hyperparameter, to restrict the\nadversary's power. We then train the model parameters $\\theta$ against these adversarial attacks by\nminimizing $\\mathcal{L}_{model}$. We define $\\mathcal{L}_{model}$ in terms of the loss functions $\\mathcal{L}_{defense}$ and $\\mathcal{L}_{benign}$:\n$\\mathcal{L}_{general} =$\n$\\sum_{(x_i,y_i) \\in \\mathcal{D}_b} \\text{- log } P(y_i|g_\\theta(f_\\theta(x_i)))$ (4)\nRetain performance on benign requests\n$\\mathcal{L}_{defense} = \\sum_{(x_i, c_i, r_i) \\in \\mathcal{D}_p}$\n$\\text{- log } P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i)) + \u2212 \\text{log}(1 \u2013 P(r_i|g_\\theta(f_\\theta(x_i) + \u03b4_i)))$ (5)\nMove towards harmless completions\nMove away from harmful completions\n$\\mathcal{L}_{model} = \\mathcal{L}_{defense} + \\mathcal{L}_{benign}$ (6)\nNote that $\\mathcal{L}_{benign}$ is calculated on inputs where no adversarial attack is present.\nA.2 DPO-LAT\nWe now describe the DPO-LAT loss inspired by Rafailov et al. [53]. Similarly to RT-LAT, we\nassume that we have a paired preference dataset of harmless/harmful completions $\\mathcal{D}_p = \\{(X_i, c_i, r_i)\\}$,\nwhere $c_i$ is the harmless result and $r_i$ is the harmful response. Instead of using a generic dataset\nof benign requests and useful completions, we instead assume $\\mathcal{D}_b = \\{(x_i, C_i, r_i)\\}$ is a dataset of\nhelpful/unhelpful responses (where again $c_i$ is the chosen helpful response and $r_i$ is the rejected\nunhelpful one). We take $\\mathcal{D}_p$ from the \u2018harmless' split of Anthropic's HH-RLHF dataset [139] and $\\mathcal{D}_b$\nfrom the 'helpful' split.\nWe choose $\\mathcal{L}_{attack}$ to cause the model to prefer the harmful response $r_i$ over $c_i$ where $(x_i, c_i, r_i) \\sim \\mathcal{D}_p$,\nusing the DPO loss (where $\u03b8^\u2217$ are the weights of the frozen reference model):\n$\\mathcal{L}_{attack} = \\text{-log} \\sigma \\Bigg( \\beta ( \\text{log} \\frac{P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(r_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\beta \\text{log} \\frac{P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(c_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))}  \\Bigg) $(7)\nWe then set $\\mathcal{L}_{defense}$ and $\\mathcal{L}_{benign}$ to the DPO loss on $\\mathcal{D}_p$ and $\\mathcal{D}_b$, with the adversary present and not\npresent respectively:\n$\\mathcal{L}_{defense} = \\sum_{(x_i, C_i, r_i) \\sim \\mathcal{D}_p} \\text{log} \\sigma \\Bigg( \\beta ( \\text{log} \\frac{P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(r_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\beta \\text{log} \\frac{P(C_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(C_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))}  \\Bigg)$ (8)\n$\\mathcal{L}_{benign} = \\sum_{(x_i, C_i, r_i) \\sim \\mathcal{D}_b} \\text{log} \\sigma \\Bigg( \\beta ( \\text{log} \\frac{P(C_i|g_\\theta(f_\\theta(x_i)) )}{P(C_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\beta \\text{log} \\frac{P(r_i|g_\\theta(f_\\theta(x_i)) )}{P(r_i| g_{\\theta^*}(f_{\\theta^*}(x_i)))}  \\Bigg)$ (9)"}, {"title": "A.3 WHP-C-LAT and GA-LAT", "content": "The WHP-C-LAT and GA-LAT methods described in Section 4.3.1 and Section 4.3.2 use a toward-\nonly adversary which optimizes for next-token cross-entropy loss on Harry Potter and the WMDP\nforget corpora respectively. For WHP, the model is trained as in Eldan and Russinovich [55]. For\nWMDP, the model uses a log(1-p) away loss on the forget dataset as in Mazeika et al. [21]. In\nboth cases, we additionally include a toward loss on WikiText [142] to match Li et al. [57], and a\nsupervised fine-tuning (SFT) loss on Alpaca [143]. While calculating the model's toward and away\nlosses, we keep the perturbations from the adversary. We remove these perturbations for SFT.\nGiven a dataset $\\mathcal{D}_f$ of text examples that you want the model to forget, and a dataset $\\mathcal{D}_b$ of text\nexamples that you want the model to retain, we can define the losses as follows:\n$\\mathcal{L}_{attack} = \\sum_{t_i \\in \\mathcal{D}_f} \\sum_j \\text{log } P(t_{i,j}|g(f(t_{i,<j}) + \u03b4_i))$ (10)\n$\\mathcal{L}_{forget} = \\sum_{t_i \\in \\mathcal{D}_f} \\sum_j \\text{log}(1 \u2013 P(t_{i,j}|g(f(t_{i,<j}) + \u03b4_i)))$ (11)\n$\\mathcal{L}_{retain} = \\sum_{t_i \\in \\mathcal{D}_b} \\sum_j \\text{log}(t_{i,j}|g(f(t_{i,<j})))$ (12)\n$\\mathcal{L}_{model} = \\mathcal{L}_{forget} + \\mathcal{L}_{retain}$ (13)\nwhere $t_{i,j}$ is the j-th token of the i-th string in the dataset and $t_{i,<j}$ is the string of all tokens of the\ni-th string up to the j-th token."}, {"title": "A.4 RMU-LAT", "content": "Here, we use the same RMU loss as used in Li et al. . The adversary still optimizes for next-token\ncross-entropy loss on the WMDP forget corpora. In the RMU loss, when the forget loss is calculated,\nthe adversary's perturbation is present:\n$\\mathcal{L}_{defense} = \\frac{1}{L}\\sum_{\\text{token } t \\in \\text{forget}} || M_{\\text{updated} }(t) + \u03b4_j \u2212 c \\cdot u||_2 + \u03b1\\cdot \\frac{1}{L}\\sum_{\\text{token } t \\in \\text{retain}} || M_{\\text{updated} }(t) \u2212 M_{\\text{frozen} }(t)||_2$ (14)\nwhere $L$ is the length of the input tokens, and $u$ is a randomly chosen vector from a uniform\ndistribution between [0, 1] that is then normalized (and stays constant throughout training). The\nconstants c and \u03b1 are hyperparameter coefficients, which we set to be 6.5 and 1200 as referenced in\nLi et al.  for Zephyr-7B."}, {"title": "B Jailbreaking Robustness Under an Alternate Autograder", "content": "In Section 4.1, we evaluate jailbreak success using the StrongReject autograder . However, here\nwe also report results using the HarmBench autograder . Overall, we find that the HarmBench\nautograder is significantly more likely to label attacks as successful, but the overall trends within\nresults remain similar."}, {"title": "C Backdoored Model MMLU Performance", "content": "To evaluate the destructiveness of DPO-LAT versus DPO on backdoor removal, we evaluate each\nmodel's performance on MMLU . We present our results in Table 7 for a single model. We find\nthat LAT tends to decrease MMLU performance by approximately one percentage point."}, {"title": "D Low Rank Adapters and Scaled Perturbation Constraints for WHP\nUnlearning", "content": "In this section, we experiment with using low-rank adapters and whitened-space attacks for WHP\nunlearning. Typically, adversarial training methods that use projected gradient descent constrain\nperturbations to be within an Lp-norm spherical ball [123]. However, for latent-space perturbations,\nthis approach is arguably unnatural because in the latent-space, activations vary more along some\ndirections than others. To address this, here, we test a scaling method to constrain attacks in a way that\nbetter respects the shape of the activation manifold in latent space in Section 4.3.1. We tested LAT\nwith perturbations that are constrained to an Lp-norm ball in whitened before they are de-whitened\nand added to the residual stream.\nOur goal was to increase the ability of targeted LAT to operate on coherent features relating to the\nunlearning corpora (specifically, features that would preserve meaning but cause the model to no\nlonger recognize the text as related). As a result, we perform principal component analysis (PCA)\non the distribution of activations between Harry Potter text and the coherent genericized versions\nof the text produced during WHP. We optimize and constrain the perturbations in a whitened space\nbefore de-whitening them using the inverse PCA transformation matrix and then applying it to the\nmodel's latent states. In addition, we use a low-rank adapter on all linear modules of rank 64. In\nour experiments, this resulted in weaker unlearning for WHP experiments but with less of a tradeoff\nin general capabilities. The results are shown in Table 8. However, we speculate that unlearning\ntasks may be especially well-suited to this type of scaling, and we leave deeper investigation to future\nwork."}, {"title": "E Tests for Robust and Competitive Unlearning in LLMs", "content": "Eldan and Russinovich  fine-tune Llama-2-7B-Chat  (Llama-2) to unlearn knowledge of\nthe Harry Potter universe. Their method is based on fine-tuning using text that has been modified to"}, {"title": "FWMDP Unlearning Details", "content": "Trainable layers and parameters We use LoRA  with rank 64 for GA and GA-LAT. For\nRMU and RMU-LAT, we do not use LoRA and instead train the MLP weights full-rank, as in Li et al.\n.\nPGD/RMU layers There are three layer choices that can be varied in our setup: which layer(s) of\nthe model to put the adversary, which layers to train for RMU, and which layer to do the RMU MSE\nactivation matching over. We kept to the layers (trainable and RMU matching) for RMU stated in Li\net al.  \u2013 the RMU layer $l$ for the activation matching, with $l$, $l \u2212 1$, $l \u2212 2$ trainable to keep the\nset of hyperparameters to search over reasonably small. Applying attacks to layer $l \u2212 2$ requires a\nsmaller $\\epsilon$ ball radius for our random perturbations; else, we found that the adversary prevents the\nmodel trained with RMU from successfully unlearning. We also find the greatest benefit in applying\nattacks to the layer before the RMU activation matching layer."}]}