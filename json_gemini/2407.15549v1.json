{"title": "Targeted Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs", "authors": ["Abhay Sheshadri", "Aidan Ewart", "Phillip Guo", "Aengus Lynch", "Cindy Wu", "Vivek Hebbar", "Henry Sleight", "Asa Cooper Stickland", "Dylan Hadfield-Menell", "Stephen Casper", "Ethan Perez"], "abstract": "Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of \u2018jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.", "sections": [{"title": "1 Introduction", "content": "Despite efforts from developers to remove harmful capabilities from large language models (LLMs), they can persistently exhibit undesirable behaviors. For example, a string of recent red-teaming papers have demonstrated diverse techniques that can be used to elicit instructions for building bombs from state-of-the-art LLMs. Developers have made progress on these problems using improved data (e.g., [19]) and adversarial training (e.g., [20, 21]). However, some harmful capabilities resist removal via fine-tuning and have been a persistent challenge toward building safer and more trustworthy models [22, 23].\nRecent work suggests that fine-tuning modifies LLMs in superficial ways that can fail to make them behave harmlessly in all circumstances. As discussed above, the LLM red-teaming literature has consistently produced methods to elicit harmful text from state-of-the-art LLMs that were explicitly fine-tuned for harmlessness. Meanwhile, research on interpretability [24\u201329], representation engineering [30\u201332], continual learning [33\u201340], and fine-tuning [25, 41\u201349] has suggested that fine-tuning struggles to make fundamental changes to an LLM's inner knowledge and capabilities. For example, Jain et al. [25] likened fine-tuning in LLMs to merely modifying a \u201cwrapper\" around a stable, general-purpose set of latent capabilities.\nIn this paper, we use latent adversarial training (LAT) [50, 51] to make LLMs more robust to exhibiting persistent unwanted behaviors. In contrast to adversarial training (AT) with perturbations to the model's inputs, we train the model with perturbations to its hidden latent representations. Because models represent features at a higher level of abstraction in the latent space [52], we hypothesize that LAT can better facilitate the removal of neural circuitry responsible for unwanted behaviors. Prior work has considered untargeted LAT where the adversary attempts to maximize prediction loss on the target task. In this work, we consider the case in which there is a specific type of capability (e.g., a backdoor) that we want to remove. Unlike prior work, we train LLMs under targeted latent-space perturbations designed to elicit specific undesirable behaviors. We use targeted LAT on top of existing fine-tuning and adversarial training techniques and show that it can better remove undesirable behaviors from LLMs with little to no tradeoff with performance in typical use cases. We make two contributions:\n1. We propose targeted latent adversarial training (LAT) as a way to more thoroughly remove undesirable behaviors from LLMs.\n2. We show that targeted LAT can combine with and improve over a wide range of state-of-the-art techniques.\n(a) In Section 4.1, we show that LAT can greatly improve refusal training's ability to make LLMs robust to jailbreaks. We find that LAT outperforms R2D2 [21] with orders of magnitude less compute.\n(b) In Section 4.2, we use LAT to greatly improve DPO's [53] ability to remove LLM backdoors when the trigger is unknown and the response is only vaguely specified. Our results suggest that LAT is a solution to the 'Sleeper Agent' problem posed in Hubinger et al. [54].\n(c) In Section 4.3, we use LAT to improve on the abilities of WHP [55], gradient ascent [56], and RMU [57] to unlearn unwanted knowledge. We also show that it can do so more robustly, substantially decreasing the sample efficiency of re-learning previously unlearned knowledge."}, {"title": "2 Related Work", "content": "Latent Adversarial Training (LAT) Latent-space attacks and LAT have been previously studied in vision models [50, 58\u201361] and language models [31, 62\u201373]. Our work is closely related to Casper et al. [51], who used LAT to defend against backdoors and unforeseen classes of adversarial attacks. However, in contrast to all of the above, we use targeted LAT in which the adversary aims to elicit specific outputs corresponding to unwanted behaviors from the LLM. This makes our work similar to concurrent work by Xhonneux et al. [74] who also use targeted adversarial training, but do so in the model's embedding space instead of its latents. Meanwhile, several works have shown that the high-level behaviors of LLMs can be altered using perturbations to their internal activations"}, {"title": "3 Methods", "content": "Targeted latent adversarial training We can view an LLM with parameters \u03b8, as a composition of two functions, \\(LLM_\\theta(x_i) = (g_\\theta \\circ f_\\theta)(x_i)\\), where \\(f_\\theta\\) is a feature extractor which maps text to latent activations \\(l_i = f_\\theta(x_i) \\in \\mathbb{R}^{s \\times d}\\) and \\(g_\\theta\\) maps those latent activations to output a probability distribution for sampling: i.e., \\(\\hat{y}_i \\sim P(y|g_\\theta(l_i))\\). We define an adversarial attack as a function \\(\\alpha\\) with parameters \\(\\delta\\) which modifies the LLM's inputs or latent activations.\nDuring standard AT, the model is trained to be robust to attacks in the input space via some training loss function, \\(L\\). The training objective is thus \\(\\min_\\theta \\sum_i L(g_\\theta(f_\\theta(\\alpha_{\\delta}(x_i))), y_i)\\). In contrast, during latent adversarial training, the model is instead trained to be robust to attacks to the latent activations:\n\\[\\min_\\theta\\sum_iL(g_\\theta(\\alpha_{\\delta};(f_\\theta(x_i))), y_i)\\]\nDuring untargeted LAT (e.g., [51]), the attacker seeks to steer the model away from the desired behavior on a training example \\((x_i, y_i)\\). The attacker's objective is thus \\(\\max_{\\delta}; L(g_\\theta(\\alpha_{\\delta};(f_\\theta(x_i))), y_i)\\). However, during targeted LAT, the attacker seeks to steer the model toward some undesirable target behavior \\(\\tilde{y}:\n\\[\\min_\\delta L(g_\\theta (\\alpha_{\\delta}; (f_\\theta\u2081(x_i))), \\tilde{y}_i)\\]\nTraining methods Performing basic targeted LAT requires a dataset of prompts and paired harmless and harmful completions \\((x_i, y_i, \\tilde{y}_i) \\sim \\mathcal{D}_p\\) to optimize the model and the attacks. In most cases, however, we also find that interleaving LAT with supervised fine-tuning on an additional auxiliary dataset of benign text \\((x_i, y_i) \\sim \\mathcal{D}_b\\) can stabilize training and reduce side effects (see Section 4 for details). Here, as done in Casper et al. [51], we attack the residual stream of transformer LLMs with \\(L_2\\)-norm-bounded perturbations, calculated using projected gradient descent (PGD) [123]. Because the model and attacker are optimized using different completions to prompts, we only perturb the token positions in the residual stream corresponding to the prompt (and not the completions) \u2013 see Figure 1. In practice, we found that perturbing the residual stream at multiple layers rather than a single layer, each with its own \\(\\epsilon\\) constraint typically yielded better results. In all experiments, we performed hyperparameter sweeps to select a perturbation bound and one or more layers."}, {"title": "4 Experiments", "content": "Our approach: augmenting fine-tuning and adversarial training methods with LAT Here, we experiment with targeted LAT for improving robustness to jailbreaks, unlearning undesirable knowledge, and removing backdoors. Across experiments, we show how LAT can be used to augment"}, {"title": "4.1 Improving Robustness to Jailbreaks", "content": "Here, we demonstrate that targeted LAT can be helpful for making models more resistant to exhibiting unwanted behaviors via jailbreaking attacks with minimal side effects.\nData We create a dataset of triples containing: prompts, harmful completions, and harmless completions using a method based on Self-Instruct [124]. We first generate a set of harmful user"}, {"title": "4.2 Backdoor Removal", "content": "Backdoors can have arbitrary triggers and response behaviors, which makes it challenging to find and remove them using standard techniques [54, 98, 136]. Here, we use LAT to greatly increase the effectiveness of backdoor removal in cases when the backdoor response is vaguely known but the trigger is not.\nModels and data We use the five backdoored LLMs from Rando et al. [137] who implanted the backdoors using RLHF [138\u2013140] such that, upon encountering specific keyword triggers (see Table 3), the model would respond in a helpful and harmful way as opposed to a helpful and harmless one. We consider the challenge of removing a backdoor when the trigger is unknown and the response"}, {"title": "4.3 Machine Unlearning", "content": "Here, our goal is to augment methods for unlearning harmful or copyrighted knowledge from LLMs. We first unlearn knowledge of Harry Potter (Section 4.3.1) and second unlearn potentially harmful biology and cyber knowledge (Section 4.3.2).\n4.3.1 Who's Harry Potter?\nFollowing work on unlearning knowledge of Harry Potter from Eldan and Russinovich [55], we show that targeted LAT can improve the robustness of unlearning without sacrificing the model's performance on other topics.\nModel and methods We work with the \"Who's Harry Potter\" (WHP) method from Eldan and Russinovich [55]. Their method involves taking a corpus of text to forget (e.g., the Harry Potter books), constructing alternative genericized text for that corpus, and then fine-tuning the model on the"}, {"title": "4.3.2 Unlearning WMDP Biology and Cyber Knowledge", "content": "Following work from Li et al. [57], who studied the unlearning of potentially dangerous biology and cyber knowledge, we show that targeted LAT can help to improve existing approaches for unlearning.\nData As in as in Li et al. [57], we use the WMDP biology and cyber corpora as forget datasests and WikiText [142] as a retain dataset. Note that, unlike in the experiments above, the forget and retain corpora were separate and did not come as paired sets of desirable/undesirable completions for a prompt.\nModel and methods As in Li et al. [57], we use Zephyr-7B off the shelf [128]. We test two different unlearning methods with and without targeted LAT. First, we use a shaped gradient ascent (GA) method inspired by [56]. We fine-tune the model to jointly minimize training loss on the retain set and \\(log(1-p)\\) on the forget set as done in Mazeika et al. [21]. To augment GA with targeted LAT, we apply latent-space perturbations optimized to minimize training loss on the forget set. To stabilize training, we also interleave training batches with supervised finetuning on the Alpaca dataset [143]. Second, we use representation misdirection for unlearning (RMU) from Li et al. [57]. With RMU, the model is trained at a given layer to (1) map activations from forget-set prompts to a randomly sampled vector while (2) leaving activations from other prompts unaltered. To augment RMU with targeted LAT, we apply latent-space adversarial perturbations only when training on the forget set. We optimize these perturbations to minimize the model's cross-entropy training loss on the undesirable forget-set example. We experimented with various layer combinations and found the best results from applying them to the activations immediately preceding the RMU layer.\nEvaluation We evaluate how well the model's general capabilities have been preserved by testing on MMLU [131] and AGIEval [144]. We evaluate the effectiveness of unlearning in the model using biology and cyber knowledge assessments from Li et al. [57]. To test the robustness of the unlearning, we also evaluate models under few-shot finetuning attacks in which an attacker seeks to extract knowledge by finetuning the model on a small number of examples [25, 41\u201346]. Here, we use a simple but surprisingly effective attack: we randomly sample a single batch of 2 examples from the relevant forget set and repeatedly train on that single batch for 20 iterations. We then report the highest WMDP bio/cyber performances for each model across evaluation checkpoints at 5, 10, and 20 steps. For all evaluations, we use 1,000 samples on lm-evaluation-harness v0.4.0 [145] as done in Li et al. [57]."}, {"title": "5 Discussion", "content": "Targeted LAT can effectively augment existing state-of-the-art fine-tuning and adversarial training methods. By attacking the model's latent representations, LAT offers a unique solution because models represent concepts at a higher level of abstraction in the latent space [2]. Here, we have used targeted latent adversarial training (LAT) to strengthen existing defenses against persistent harmful behaviors in LLMs. We have tested targeted LAT by applying it to three current challenges with state-of-the-art LLMs: jailbreaking [21], unlearning [105], and backdoor removal [103, 102]. In each case, we have shown that LAT can augment existing techniques to improve the removal to unwanted behaviors with little or no tradeoff in general performance. This is especially notable in the case of backdoor removal in which DPO alone fails to remove backdoors while DPO with LAT does so very effectively. This suggests that LAT can be a solution to the \"Sleeper Agent\" problem posed in Hubinger et al. [54] and Pawelczyk et al. [98] in which harmful backdoors can persist through adversarial training."}, {"title": "Future work", "content": "\u2022 Improved latent-space attacks In addition to performing LAT with perturbations to an LLM's residual stream, we are interested in other strategies for attacking its internal repre- sentations. Toward this goal, engaging with recent work on LLM representation engineering [2, 82] and interpretability [153] may help to parameterize and shape latent space attacks. We also speculate that universal attacks instead of single-instance attacks may be more interpretable and might better target the most prominent mechanisms that a model uses when it produces undesirable outputs.\n\u2022 Augmenting circuit breaking Concurrently with our work, Zou et al. [154] introduced a \"circuit breaking\u201d method for significantly improving LLM robustness to jailbreaks. We are interested in augmenting circuit breaking with LAT to further improve robustness.\n\u2022 Generalized adversarial attacks for LLM evaluations We are interested in the extent to which embedding-space attacks (e.g., [70]), latent-space attacks, (e.g., [51]), and few-shot fine-tuning attacks (e.g., [42]) can be useful for more rigorous evaluations of LLM safety."}, {"title": "Broader Impacts", "content": "This work was motivated by the goal of training more safe and trustworthy AI systems. We believe that LAT will be practically useful for training better models. However, we emphasize that LAT is a value-neutral technique for training AI systems to align with their developer's goals. It is important not to conflate AI alignment with safety [155]. We believe that this work will contribute to helpful progress, but we emphasize that many of the risks from AI systems come from misuse and adverse systemic effects as opposed to unintended hazards such as the ones we work to address."}, {"title": "A Loss Functions for LAT", "content": "A.1 R2D2-LAT\nHere, we describe the RT-LAT method described in Section 4.1 in greater detail. We assume we are given two datasets a dataset of harmful requests and pairs of preferred and rejected com- pletions \\(D_p = \\{(x_i, c_i,r_i)\\}\\), and a generic dataset of benign requests and helpful completions \\(D_b = \\{(x_i, y_i)\\}\\). For each batch, we train the adversarial attack \\(\\delta\\) to minimize \\(L_{attack}\\):\n\\[L_{attack} = -\\\nlog P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i)) + -\\\nlog(1 \u2013 P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i)))\\]\nWe additionally add the constraint that \\(\\lVert \\delta_i \\rVert_2 \\le \\epsilon\\), where \\(\\epsilon\\) is a hyperparameter, to restrict the adversary's power. We then train the model parameters \\(\\theta\\) against these adversarial attacks by minimizing \\(L_{model}\\). We define \\(L_{model}\\) in terms of the loss functions \\(L_{defense}\\) and \\(L_{benign}\\):\n\\[L_{general} = -\\\nlog P(y_i|g_\\theta(f_\\theta(x_i)))\\]\n\\[L_{defense} = \\sum_{(x_i, c_i,r_i) \\in D_p}-\\\nlog P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i)) + -\\\nlog(1 \u2013 P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i)))\\]\n\\[L_{model} = L_{defense} + L_{benign}\\]\nNote that \\(L_{benign}\\) is calculated on inputs where no adversarial attack is present.\nA.2 DPO-LAT\nWe now describe the DPO-LAT loss inspired by Rafailov et al. [53]. Similarly to RT-LAT, we assume that we have a paired preference dataset of harmless/harmful completions \\(D_p = \\{(X_i, c_i, r_i)\\}\\), where \\(c_i\\) is the harmless result and \\(r_i\\) is the harmful response. Instead of using a generic dataset of benign requests and useful completions, we instead assume \\(D_\\theta = \\{(x_i, c_i, r_i)\\}\\) is a dataset of helpful/unhelpful responses (where again \\(c_i\\) is the chosen helpful response and \\(r_i\\) is the rejected unhelpful one). We take \\(D_p\\) from the \u2018harmless' split of Anthropic's HH-RLHF dataset [139] and \\(D_\\theta\\) from the 'helpful' split.\nWe choose \\(L_{attack}\\) to cause the model to prefer the harmful response \\(r_i\\) over \\(c_i\\) where \\((x_i, c_i, r_i) \\sim D_p\\), using the DPO loss (where \\(\\theta^*\\) are the weights of the frozen reference model):\n\\[L_{attack} = logo (\\beta(\\frac{P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(r_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\frac{P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(c_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))})\\]\nWe then set \\(L_{defense}\\) and \\(L_{benign}\\) to the DPO loss on \\(D_p\\) and \\(D_b\\), with the adversary present and not present respectively:\n\\[L_{defense} = \\sum_{(x_i, c_i,r_i)\\sim D_p}log \\sigma(\\beta(\\frac{P(r_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(r_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\frac{P(c_i|g_\\theta(f_\\theta(x_i) + \\delta_i))}{P(c_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))}))\\]\n\\[L_{benign} = \\sum_{(x_i,c_i, r_i)\\sim D_b}logo (\\beta(\\frac{P(c_i|g_\\theta(f_\\theta(x_i)))}{P(c_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))} \u2013 \\frac{P(r_i|g_\\theta(f_\\theta(x_i)))}{P(r_i|g_{\\theta^*}(f_{\\theta^*}(x_i)))})\\]"}, {"title": "A.3 WHP-C-LAT and GA-LAT", "content": "The WHP-C-LAT and GA-LAT methods described in Section 4.3.1 and Section 4.3.2 use a toward- only adversary which optimizes for next-token cross-entropy loss on Harry Potter and the WMDP forget corpora respectively. For WHP, the model is trained as in Eldan and Russinovich [55]. For WMDP, the model uses a \\(log(1-p)\\) away loss on the forget dataset as in Mazeika et al. [21]. In both cases, we additionally include a toward loss on WikiText [142] to match Li et al. [57], and a supervised fine-tuning (SFT) loss on Alpaca [143]. While calculating the model's toward and away losses, we keep the perturbations from the adversary. We remove these perturbations for SFT.\nGiven a dataset \\(D_f\\) of text examples that you want the model to forget, and a dataset \\(D_b\\) of text examples that you want the model to retain, we can define the losses as follows:\n\\[L_{attack} = \\sum_{t_i\\in D_f}\\sum_j\\log P(t_{i,j}|g(f(t_{i,<j}) + \\delta_i))\\]\n\\[L_{forget} = \\sum_{t_i\\in D_f}\\sum_j\\log(1 \u2013 P(t_{i,j}|g(f(t_{i,<j}) + \\delta_i)))\\]\n\\[L_{retain} = \\sum_{t_i\\in D_b}\\sum_j\\log(t_{i,j}|g(f(t_{i,<j})))\\]\n\\[L_{model} = L_{forget} + L_{retain}\\]\nwhere \\(t_{i,j}\\) is the j-th token of the i-th string in the dataset and \\(t_{i,<j}\\) is the string of all tokens of the i-th string up to the j-th token."}, {"title": "A.4 RMU-LAT", "content": "Here, we use the same RMU loss as used in Li et al. [57]. The adversary still optimizes for next-token cross-entropy loss on the WMDP forget corpora. In the RMU loss, when the forget loss is calculated, the adversary's perturbation is present:\n\\[L_{defense} \\frac{1}{L}(\\sum_{token t \\in text_{forget}} || M_{updated}(t) + \\delta_i \u2013 c \\cdot u||_2 + \\frac{\\alpha}{\\sum_{token t \\in text_{retain}}} \u2211 || M_{updated}(t) \u2013 M_{frozen}(t)||_2)\\]\nwhere \\(L\\) is the length of the input tokens, and \\(u\\) is a randomly chosen vector from a uniform distribution between [0, 1] that is then normalized (and stays constant throughout training). The constants \\(c\\) and \\(\\alpha\\) are hyperparameter coefficients, which we set to be 6.5 and 1200 as referenced in Li et al. [57] for Zephyr-7B."}, {"title": "B Jailbreaking Robustness Under an Alternate Autograder", "content": "In Section 4.1, we evaluate jailbreak success using the StrongReject autograder [135]. However, here we also report results using the HarmBench autograder [21]. Overall, we find that the HarmBench autograder is significantly more likely to label attacks as successful, but the overall trends within results remain similar."}, {"title": "C Backdoored Model MMLU Performance", "content": "To evaluate the destructiveness of DPO-LAT versus DPO on backdoor removal, we evaluate each model's performance on MMLU [131]. We present our results in Table 7 for a single model. We find that LAT tends to decrease MMLU performance by approximately one percentage point."}, {"title": "D Low Rank Adapters and Scaled Perturbation Constraints for WHP Unlearning", "content": "In this section, we experiment with using low-rank adapters and whitened-space attacks for WHP unlearning. Typically, adversarial training methods that use projected gradient descent constrain perturbations to be within an \\(\\{L\\}_p\\)-norm spherical ball [123]. However, for latent-space perturbations, this approach is arguably unnatural because in the latent-space, activations vary more along some directions than others. To address this, here, we test a scaling method to constrain attacks in a way that better respects the shape of the activation manifold in latent space in Section 4.3.1. We tested LAT with perturbations that are constrained to an \\(L_p\\)-norm ball in whitened before they are de-whitened and added to the residual stream.\nOur goal was to increase the ability of targeted LAT to operate on coherent features relating to the unlearning corpora (specifically, features that would preserve meaning but cause the model to no longer recognize the text as related). As a result, we perform principal component analysis (PCA) on the distribution of activations between Harry Potter text and the coherent genericized versions of the text produced during WHP. We optimize and constrain the perturbations in a whitened space before de-whitening them using the inverse PCA transformation matrix and then applying it to the model's latent states. In addition, we use a low-rank adapter on all linear modules of rank 64. In our experiments, this resulted in weaker unlearning for WHP experiments but with less of a tradeoff in general capabilities. The results are shown in Table 8. However, we speculate that unlearning tasks may be especially well-suited to this type of scaling, and we leave deeper investigation to future work."}, {"title": "E Tests for Robust and Competitive Unlearning in LLMs", "content": "Eldan and Russinovich [156] fine-tune Llama-2-7B-Chat [93] (Llama-2) to unlearn knowledge of the Harry Potter universe. Their method is based on fine-tuning using text that has been modified to"}, {"title": "FWMDP Unlearning Details", "content": "Trainable layers and parameters We use LoRA [158] with rank 64 for GA and GA-LAT. For RMU and RMU-LAT, we do not use LoRA and instead train the MLP weights full-rank, as in Li et al. [57].\nPGD/RMU layers There are three layer choices that can be varied in our setup: which layer(s) of the model to put the adversary, which layers to train for RMU, and which layer to do the RMU MSE activation matching over. We kept to the layers (trainable and RMU matching) for RMU stated in Li et al. [57] \u2013 the RMU layer l for the activation matching, with l, l \u2013 1, l \u2013 2 trainable to keep the set of hyperparameters to search over reasonably small. Applying attacks to layer l \u2013 2 requires a smaller \\(\\epsilon\\) ball radius for our random perturbations; else, we found that the adversary prevents the model trained with RMU from successfully unlearning. We also find the greatest benefit in applying attacks to the layer before the RMU activation matching layer."}]}