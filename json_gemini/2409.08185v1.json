{"title": "Fine-tuning Large Language Models for Entity Matching", "authors": ["Aaron Steiner", "Ralph Peeters", "Christian Bizer"], "abstract": "Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.", "sections": [{"title": "1 INTRODUCTION", "content": "Entity matching [1, 2, 5] is the task of identifying entity descriptions in different data sources that refer to the same real-world entity. Entity matching is a central step in data integration pipelines [3]. Many state-of-the-art entity matching methods build on pre-trained language models (PLMs) [20], such as BERT or ROBERTa [9, 16, 19, 24]. Recent work on using generative large language models (LLMs) [26], such as GPT, Llama, Gemini, or Mixtral, for entity matching has shown that LLMs have higher zero-shot performance compared to PLMs and generalize better to unseen entities [14, 17, 18]. Most research on using LLMs for entity matching focuses on prompt engineering and in-context learning [6, 14, 17]. Initial papers have started to investigate the potential of improving beyond in-context learning by fine-tuning LLMs for entity matching [18, 25]. This paper builds on this work and presents a deeper investigation of the utility of fine-tuning LLMs for entity matching. Besides the effect of fine-tuning on the matching performance, we investigate how fine-tuning influences the model's ability to generalize to different in-domain datasets as well as across domains. Our investigation focuses on two dimensions: First, the representation of training examples, where we investigate the effect of augmenting each training example with textual as well as structured explanations about why a pair of entity descriptions matches or not. The second dimension is the selection and generation of examples for the training set that is used for fine-tuning. We experiment with using LLMs to filter misleading examples from the training set as well as to generate additional training examples. We further investigate the error-based selection of training examples."}, {"title": "2 EXPERIMENTAL SETUP", "content": "This section provides details on the models, benchmark datasets, fine-tuning setup, and the performance evaluation that we use for the experiments. All artefacts are provided in the project repository, meaning that all experiments can be replicated.\nLarge Language Models: We compare the following hosted proprietary models and locally-run open-source models:\n(1) gpt-4o-2024-08-06: Released in May 2024, this model is OpenAI's flagship LLM and currently used by ChatGPT.\n(2) gpt-4o-mini-2024-07-18: This hosted LLM, released in July 2024, has lower usage fees compared to GPT-4o. Although the exact model size is not disclosed, the naming convention indicates that it is smaller than GPT-4o.\n(3) Meta-Llama-3.1-8B-Instruct\u00b9: Released in July 2024, this open-source model from Meta features the same 128k token context length as the GPT models.\n(4) Meta-Llama-3.1-70B-Instruct\u00b2: Also developed by Meta, this model is medium-sized within the Llama series. Further details are available in the accompanying paper by Meta [4].\nWe use the OpenAI batch API\u00b3 and Hugging Face's Transformers library to prompt both hosted and local models. In both cases, the generation temperature is set at zero to minimize randomness. The open-source models run on local machines with varying hardware configurations, including an AMD EPYC 7413 processor, 1024GB RAM, and up to eight Nvidia RTX6000 GPUs.\nDatasets: We evaluate in-domain and cross-domain generalization using benchmark datasets from two topical domains [8, 19]: products and scholarly works. The datasets are selected based on three criteria: First, we chose datasets with at least 150 matches in the test set (see Table 1) to ensure the stability of the performance measurement. Second, we prioritize datasets that contain a significant number of challenging corner case record pairs. These corner cases are either matching or non-matching pairs that closely resemble the opposite class due to very similar or dissimilar surface forms [19]. Finally, to evaluate cross-domain generalization, we chose datasets from two topical domains. We use the following datasets:\n\u2022 WDC Products: This dataset includes product offers from various categories (e.g., electronics, clothing) originating from numerous online shops. We use the most challenging version, with 80% corner cases (hard positives and negatives).\n\u2022 Abt-Buy: This benchmark provides examples within similar categories to those found in WDC Products.\n\u2022 Walmart-Amazon: The Walmart-Amazon benchmark includes products from both Walmart and Amazon, with categories comparable to those in WDC Products and Abt-Buy.\n\u2022 Amazon-Google: Unlike the other benchmarks, the Amazon-Google dataset focuses on software products, such as different versions of the Windows operating system and various image/video editing applications. This dataset introduces a different product type for evaluating model performance.\n\u2022 DBLP-Scholar: This benchmark requires matching bibliographic entries between DBLP and Google Scholar, representing a domain distinct from the product datasets.\n\u2022 DBLP-ACM: Similar to DBLP-Scholar, the DBLP-ACM benchmark involves matching bibliographic entries.\nWe use the title attribute of the product datasets for fine-tuning, while we concatenate the author, title, venue, and year attributes of the bibliographic datasets using a semicolon character as a delimiter.\nFine-tuning Setup: In order to keep computational costs manageable, we use the default hyperparameter setting recommended by the model providers and do not perform a hyperparameter search for each experiment. The fine-tuning setup differs for hosted and open-source models. We use the following default hyperparameter settings for the hosted OpenAI models: learning rate multiplier 1.8, batch size 16, and a constant random seed for reproducibility. We use the following fine-tuning setup for the open-source models: We employed Low-Rank Adaptation (LoRA) with an alpha of 16 to allow moderate adjustments to model weights, a dropout rate of 0.1 to prevent overfitting, and a rank (r) of 64 to balance performance and computational efficiency. The learning rate was configured at"}, {"title": "3 STANDARD FINE-TUNING", "content": "In order to establish baselines, this section reports the results of a series of fine-tuning experiments using a simple, straight-forward representation of the training examples. Each example is represented by a prompt inquiring whether the entity descriptions refer to the same entity, followed by the two descriptions and the completion \"Yes\" or \"No\", depending on whether the descriptions are a match or a non-match."}, {"title": "3.1 Effectiveness", "content": "This section discusses the results of testing the LLMs with examples from the same dataset (a non-transfer setting). The fine-tuned models show a performance increase on most datasets compared to the baseline models.\nSmaller Models: Llama-8B shows an average F1 score gain of 17.31 points over the zero-shot baseline Five out of six fine-tuned Llama 8B models significantly improve on their zero-shot baselines, with Abt-Buy achieving the highest gain of 30.77 points. The Amazon-Google model is an outlier, only exceeding the baseline by 0.84 points, underperforming compared to other product domain models. GPT-40-mini shows an average F1 score increase of 11.72 points, a significant gain given the models high zero-shot performance. Unlike Llama-8B, GPT-40-mini demonstrates significant performance gains on the Amazon-Google dataset, achieving the highest absolute performance improvement among all 40-mini models as presented in the A-G column of Table 2.\nLarge Models: Given resource constraints, we fine-tune the larger models only on the WDC small training set. For the Llama 70B model fine-tuning results in a decrease in performance, with a reduction of 2.53 points in F1 score on the WDC test set. In contrast, GPT-40, which starts with a similar zero-shot score as GPT-40-mini, continues the trend of fine-tuning improving performance. It achieves a 5.43-point increase to a F1 score of 87.07 on the WDC dataset, the highest result so far."}, {"title": "3.2 Generalization", "content": "We evaluate two types of generalization: in-domain, where performance is assessed across datasets within the same topical domain, and cross-domain, where a model is trained using a training set from one domain and tested with a test set from a different topical domain (e.g., from products to scholar).\nTransfer Gain: We quantify the capability of a fine-tuned model to generalize to target datasets by relating the performance gains of the fine-tuned model on the target datasets to the performance gain that is achieved by fine-tuning specialized models with training examples from each target dataset. The resulting transfer gain is calculated by dividing the average performance gain on target datasets by the average gain achieved by models which were fine-tuned specifically on those datasets.\nTransfer Gain = $\\frac{\\text{Avg performance gain on target datasets}}{\\text{Avg performance gain of specialized models}}$\nFor example, the generalization gain of the WDC model is 72% (Table 2, row Llama 8B/WDC, column Transfer Gain) and is calculated by dividing the average gain of the WDC model of 10.52 points on the product datasets by the average gain of specialized models for these datasets, which is 18.41 points F1.\nIn-domain Generalization: In the product domain, five out of six Llama 8B models improve performance on both their own and other datasets. Even the underperforming Amazon-Google model shows a gain on the Abt-Buy dataset. However, it is the only model to experience a decline in performance in an in-domain generalization setting, as indicated by the WDC column in Table 2. On average, Llama 8B models that were trained on a source dataset achieve 59% of the performance in an in-domain transfer scenario compared to models that were fine-tuned directly for the target datasets. GPT-40-mini also exhibits some ability to transfer between different product datasets, but to a lesser extent, achieving only a 15% performance increase relative to dataset-specific models. This lower average is primarily influenced by the poor performance of the Amazon-Google and Walmart-Amazon models on the WDC dataset. Considering that the larger models are only fine-tuned using the WDC dataset, in-domain generalization can only be assessed within the product domain. The fine-tuned Llama 70B model demonstrates the ability to generalize to the Amazon-Google and Walmart-Amazon datasets, as evidenced by its performance exceeding the baseline by 3.92 and 4.94 F1 points, respectively. However, it underperforms on the Abt-Buy dataset,"}, {"title": "3.3 Prompt Sensitivity", "content": "Variations of the wording of a prompt can have considerable performance implications on the model . It is recommended to use the same prompt for fine-tuning that will later be used for querying the model 5. In this section, we investigate how the performance of the fine-tuned models changes when a different prompt is used for querying the model.\nPrompts: We assess the performance of the following three alternative prompts for querying models that were fine-tuned using the prompt shown in Figure 2:\n\u2022 simple-free: \"Do the two product descriptions match?\"\n\u2022 complex-force: \"Do the two product descriptions refer to the same real-world product? Answer with 'Yes' if they do and 'No' if they do not.\"\n\u2022 simple-force: \"Do the two product descriptions match? Answer with 'Yes' if they do and 'No' if they do not.\"\nWe measure the prompt sensitivity as the standard deviation of the performance across the prompt that is used for fine-tuning and the three alternative prompts. Post-fine-tuning Llama 8B's standard deviation drops from 15.76 to 1.87 in a non-transfer setting. The sensitivity increases slightly for in-domain transfer scenarios, with product models at 2.27 F1 points and scholar models at 2.60 F1 points. Fine-tuned Llama models show an average prompt sensitivity of 3.54 F1 points, down from 15.76 pre-fine-tuning. Furthermore, the prompt used for fine-tuning does not necessarily perform best with the alternative prompts resulting in the highest F1 score in 29 out of 42 scenarios for Llama."}, {"title": "4 DIMENSION 1: EXAMPLE REPRESENTATION", "content": "This section investigates how different training example representations affect the performance and generalization capabilities of fine-tuned models. We compare three representations: 1) simple pairs as used in Section 3, 2) pairs augmented by the LLM with textual explanations why the entities match or do not match, and 3) pairs augmented with structured explanations. The complete prompts for instructing the model to generate explanations as well as the augmented training sets are found in the project repository.\nTextual Explanations: We evaluate two types of textual explanations generated using GPT-40-mini. The first approach presents the model with the prompt shown in Figure 2, followed by the label. Afterwards, the model is asked to explain the label. The model is given no guidance on the format of the explanation. This results in detailed explanations, having an average length of 293 tokens, which describe why the entity descriptions match or do not match. As a second approach, we re-run the technique proposed by Wadhwa et al. , which generates more concise explanations by including short explanations for other entity pairs as demonstrations in the prompt. The resulting explanations have an average length of 90 tokens. Figure 3 shows a training example containing an explanation in the Al part which was generated using this approach. The prompts used for generating these explanations, as well as the longer, more open-ended explanations, are available in the accompanying repository."}, {"title": "4.1 Effectiveness", "content": "We compare the effectiveness of different explanation approaches in a non-transfer scenario using the WDC Products dataset. The result of these experiments are found in the No Transfer column of Table 3.\nSmaller Models: Llama 8B outperforms standard fine-tuning across all evaluated explanation approaches, as shown in Table 3, column No Transfer. Long textual explanations provide the smallest increase of 1.48 F1 points. The approach by Wadhwa et al. results in a 4.01-point improvement. Structured explanations further improve F1 scores by 0.93 over Wadhwa's approach. GPT-40-mini shows a different trend, where only structured explanations outperform the standard fine-tuning, increasing by 0.97 F1 points (Table 3, WDC Products column). The other configurations underperform, with three out of five approaches performing worse than zero-shot (Table 3, Non-transfer column).\nLarger Model: Structured explanations are exclusively tested on larger models due to their consistent performance improvement in smaller models. While Llama 70B does not benefit from standard fine-tuning, structured explanations improve its performance by 1.50 F1 points over zero-shot (Table 3, WDC column). In contrast, GPT-40 does not follow this trend. While structured explanations lead to a 1.60-point increase over zero-shot, standard fine-tuning results in a 5.50-point improvement, as shown in the GPT-40 rows in the WDC column of Table 3. Therefore, GPT-40 does not significantly benefit from structured explanations in a non-transfer setting."}, {"title": "4.2 Generalization", "content": "We fine-tune the models using the WDC dataset and afterwards apply them to the test sets of the other datasets. We measure the generalization performance within the product domain and across domains using the scholarly datasets.\nIn-Domain Generalization: Standard fine-tuning results in Llama 8B achieving 72% of the dedicated models' performance on product datasets (Table 3, In-Domain Transfer Gain column). The"}, {"title": "5 DIMENSION 2: EXAMPLE SELECTION AND GENERATION", "content": "This section explores how the selection and generation of training examples for fine-tuning impact in-domain performance and model generalization. We examine three strategies: filtering the existing training sets, generating new artificial examples based on existing data, and adding additional training examples that are similar to examples on which the model failed. We evaluate the techniques without augmenting the training examples with explanations. We provide the complete prompts for the example selection and generation as well as filtered and augmented training sets in the project repository."}, {"title": "5.1 Training Set Filtration", "content": "The filtration approach is based on the assumption that training data quality outweighs quantity and it is thus beneficial to remove potentially misleading examples from the training set.\nError-based Filtering: GPT-40-mini is prompted to match pairs in the small WDC training split using the complex-force prompt (see Section 3.3). Training examples for which the model's labels differ from the ground truth labels are discarded. This approach may discard some correctly labelled examples. However, it is hypothesized that overall dataset quality improves. The resulting training set WDC-filtered has a size of 2006 examples, compared to 2500 examples of the original WDC-small training set (see Table 4).\nRelevancy-based Filtering: The second approach involves filtering for relevance, operating on the assumption that fewer, more"}, {"title": "5.2 Example Generation", "content": "While the earlier approaches focused on discarding irrelevant examples, this section investigates using GPT-40 to generate additional training examples for fine-tuning. We test three methods for generating examples. All methods iterate over the WDC small training set and use the examples from this set as seeds derive additional training examples. The complete prompts used by the methods are provided in the project repository.\nBrief Generation Prompt: This method provides the model with a short task description, instructing it to generate three non-matches and one match. The prompt also includes a seed pair from the training set.\nDetailed Generation Prompt: This prompt contains a comprehensive task description, including background information on entity matching and specific requirements for the generated examples. It instructs the model to produce examples from the same product category and to include similar matching challenges as the challenges found in the provided seed example. Additionally, the prompt explains key concepts like corner cases to deepen the model's understanding of the task.\nDemonstration-based Creation: Building on the second method, this approach includes six entity pairs as demonstrations into the prompt. These pairs are selected from the WDC small dataset based on their similarity to the seed example in the OpenAI embedding space.\nInspection of the Generated Examples: We manually inspect a subset of the generated pairs. This inspection reveals that the first method encounters two main issues: generating matching examples with differing strings and maintaining correctness. The model often produces matching examples that are easy non-matches, such as laptops from different product lines being incorrectly treated as matches. The detailed generation prompt results in examples with more variation, though correctness remains mixed. The third approach increases variance further. However, many of these examples remain inaccurate upon human evaluation.\nCombining Generation and Filtration: Given the mixed quality of the generated examples, we combine example generation with the filtering approach introduced above, further refined by relevancy filtering (syn.-filter-rel.). The resulting dataset sizes are presented in Table 4. Since the generation methods are intended to extend existing datasets, both training splits are combined with the unfiltered version of the WDC small dataset, and the counts in Table 4 reflect this combination."}, {"title": "5.3 Error-based Example Selection", "content": "The idea of the error-based example selection (err.-sel.) approach is to guide the model to prevent errors by fine-tuning it with examples that are similar to entity pairs that are matched incorrectly by the model. For this, the Llama 8B model is initially trained on the standard 2,500 examples from the WDC small training dataset. After this initial training phase, the model is validated to identify the remaining errors. Additional training examples are then selected from the large WDC products dataset, as shown in Table 1, simulating additional labelling capacity. These examples are chosen based on their embedding similarity to the identified errors.\nTo maintain consistent training set sizes, each iteration starts with 2,500 examples from the WDC small dataset, adding 2,500 additional examples based on errors. The model is retrained on the expanded dataset for 5 epochs, and this process is repeated five times. Among all optimization rounds, the model with the highest F1 score on the validation set is selected as the best. Due to OpenAI's fine-tuning limitations, this approach is only tested with the Llama series.\nEffectiveness: The \u201cWDC-s-err-sel\" approach yields the highest F1 score for Llama 8B among all tested methods, with a 5.18-point increase to 74.37, a 1.99-point improvement over the large WDC dataset. This result demonstrates that adding only highly relevant examples outperforms merely increasing training data quantity.\nGeneralization: This approach outperforms the generation method in a non-transfer scenario but underperforms in in-domain generalization, achieving 83% of dedicated performance compared to 72% for the fine-tuned baseline.\nIn cross-domain generalization, this approach shows a slight improvement over standard fine-tuning, with an average increase of 2.77 F1 points. However, it still lags behind the baseline model, which achieves, on average, 17.95 more F1 points when applied to the scholar domain."}, {"title": "6 RELATED WORK", "content": "Entity matching [2, 5] has been researched for over 50 years [7]. While early techniques rely on hand-crafted matching rules, supervised machine learning techniques dominate the field today [1, 3, 9, 16, 22-24]. Narayan et al. are the first to apply LLMs for entity matching using GPT-3.  extended Narayan's work by testing a wider range of zero-shot and few-shot prompts on more recent models. While these approaches have shown strong performance, the prompt sensitivity of the LLMs in in-context learning settings has results in a costly and time-consuming search for the best prompt for each model/dataset combination . Zhang et al. fine-tune LLMs using multi-task learning for handling various structured data-related tasks. Their set tasks include entity matching. More recently, Peeters et al.  experimented with fine-tuning the GPT3.5-Turbo-0613 model, achieving an F1 score of 88.34 on WDC Products. However, their results cannot be directly compared to the results in this paper as they are generated using only a subset of the WDC test set. Wadhwa et al. are the first to experiment with adding textual explanations to the training set used for fine-tuning LLMs. We extend this idea by augmenting the training set with different types of structured explanations. Wadhwa et al. note improvements, especially in out-of-domain scenarios. In contrast, our work shows that explanations primarily improve results in non-transfer and in-domain generalization settings."}, {"title": "7 CONCLUSION", "content": "This study demonstrates that standard fine-tuning provides significant performance improvements for smaller LLMs, as well as for certain larger models, such as GPT-40. Small and large models benefit from augmenting the training set with structured explanations, which improve performance as well as generalization. The proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-40 Mini. As future work, we plan to refine the example selection and generation methods and develop strategies to improve cross-domain generalization."}]}