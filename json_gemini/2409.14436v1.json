{"title": "AUTOMOTIVE INNOVATION LANDSCAPING USING LLM", "authors": ["Raju Gorain", "Omkar Salunke"], "abstract": "The process of landscaping automotive innovation through patent analysis is crucial for Research and Development teams. It aids in comprehending innovation trends, technological advancements, and the latest technologies from competitors. Traditionally, this process required intensive manual efforts. However, with the advent of Large Language Models (LLMs), it can now be automated, leading to faster and more efficient patent categorization & state-of-the-art of inventive concept extraction. This automation can assist various R&D teams in extracting relevant information from extensive patent databases.This paper introduces a method based on prompt engineering to extract essential information for landscaping. The information includes the problem addressed by the patent, the technology utilized, and the area of innovation within the vehicle ecosystem (such as safety, Advanced Driver Assistance Systems and more).The result demonstrates the implementation of this method to create a landscape of fuel cell technology using open-source patent data. This approach provides a comprehensive overview of the current state of fuel cell technology, offering valuable insights for future research and development in this field.\nKeywords: LLM Prompt Engineering. GPT \u00b7 NLP \u00b7 Automotive innovation Patents TRIZ", "sections": [{"title": "1 Introduction", "content": "Prompt Engineering has become more popular because of its precise, well-structured way of human-AI interaction. Implementation of prompt engineering in LLM model increases the LLM output quality and accuracy. It optimizes the interaction between LLM model and user. There are many open-source transformer models which are used for extracting innovative ideas, categorization, summarization, and other NLP tasks.\nOne of the novel transformer model is BERT (Bidirectional Encoder Representations from Transformers) [1]. BERT differs from other models by pre-training deep bidirectional representations that consider both the left and right context in all levels. By adding a single output layer, the pre-trained BERT representations may be fine-tuned to achieve cutting-edge performance in many tasks, including question answering and language inference. This can be done without making significant changes to the task-specific architecture. BERT has attained unprecedented state-of-the-art outcomes on many tasks related to natural language processing, exceeding human performance in some instances. The research[1] emphasizes the significance of including extensive, unsupervised pre-training in language comprehension systems and applies these discoveries to deep bidirectional architectures. This model can be useful for basic categorization of patents[1].\nThere is a study which examines empirical scaling rules for language model performance on cross-entropy loss [2]. It shows that the loss scales as a power-law with model size, dataset size, and training computation, with trends across seven orders of magnitude. Architectural features like network breadth and depth have no influence as per the research, also the paper provides simple equations for overfitting and training speed based on model/dataset size. A given compute budget may be optimally allocated using these connections. The paper finds that bigger models are more efficient, implying that ideally compute-efficient training entails training large models on little data and halting before convergence. The data strongly suggests that bigger models continue to perform better. The paper also recommends model parallelism research. So, this [2] suggests the idea of using bigger pre-trained Transformer model for more accurate patent information extraction.\nLarge Language Models (LLMs) have become famous because they can do Natural Language Processing (NLP) jobs"}, {"title": "2 User Story & Problem statement", "content": "In automotive industry engineers always think about innovation in their products, always they have to keep update on innovations happening in their domain of work, also they have to be aware on how their competitors are handling the similar issue. For this kind of information, patents are the best resources which can be a helpful for detailed analysis of innovation. Patents contains the legal information to safeguard the rights of the inventor therefore it is considered as most authentic source of information. But as mentioned above purpose of patent text is to legal protection of the innovation and unlike the scientific research papers. For beginners the task of manual patent reading can be time consuming and exhaustive because of broad legal claims and mostly because of legal language mentioned in the document. Motivation of this paper is to extract patent information not only by keyword search but also based on the context. In this paper, patent context based information extraction is performed by Prompt Engineering technique using Transformer based LLM model."}, {"title": "3 Related Work:", "content": "This section consists of recent advancements on Prompt Engineering & LLM model."}, {"title": "3.1 Prompt Engineering", "content": "Prompt Engineering is an effective and structured way of giving input to any LLM. Prompt can be having single question or multiple questions with content and instructions. In prompt any example can be added to extract the output in similar way. Quality of response from the LLM is depends most of the time on the structured input provided to the model. It is always advised to avoid the contradiction statements in prompt or else it might reflect the poor response.\nIn any effective prompt there are two main part: Content & Structure.\nContent consists of Objective: which defines goal of the LLM or what is it's mission, Instruction: is consists of the instructions on how it perform the task, User: it defines what role the LLM is playing, Context: it includes main background information/documents/input data, Query: it's one of the main section i.e., what information user want from LLM that need to describe in this section.\nStructure consists of two main part that is Ordering and Labeling. User must keep in mind these things while creating any new prompt.\nBelow is a sample prompt: [7]"}, {"title": "3.2 LLM Model", "content": "Al technologies, notably LLMs, have transformed human-AI interactions. Large Language Models are Deep Learning models that can understand human language text, can analyze with its pre-trained model, can extract required output. Most of the LLM models are trained from publicly available data of internet. LLM use a type of algorithm which can understand characters, words, sentences, and paragraphs. Deep learning uses a probabilistic approach to analysis, unstructured data, which enables deep learning models to recognize the content of input data without human intervention. LLM model is a pre-trained transformer model which can perform various NLP tasks like next word prediction, text generation, text classification, token generation, summarization, question answering and more. The transformer architecture is the fundamental building block of all Language Models with Transformers (LLMs). The transformer architecture was introduced in the paper \"Attention is all you need,\" published in December 2017 [8]. The simplified version of the Transformer Architecture looks like this(Figure 1):\nLLMs are the advanced model of NLPs with more capabilities. Just like giving extra attention to important part while reading any paragraph in similar way Transformer also focuses to important word of any sentence. The transformer looks all the words at a time not one after another, this is the way it understands how words are dependent on each other. Transformer uses its knowledge to understand the story and it fits the words together. By this way it can guess what words may come next. The core of the Transformer model is nothing but deep Neural Network. All the Transformer"}, {"title": "4 Methodology", "content": "In this paper, an application of Prompt Engineering using Transformer model is described. The objective of the paper is to extract the relevant information from patent document. A patent document consists of multiple levels of information like application number, innovators, title, abstract, claims, drawing, drawing description, summary of the patent, but this paper is considering high-level information related to patent like title, abstract and claims, which helps user to extract high-level information extraction before deep dive into patents. It has been observed that these three sections are enough to categorize patents into MATHCHEM + software categorization."}, {"title": "4.1 Dataset Preparation", "content": "This study aims to extract precise responses from a Language Model (LLM) by feeding it with well-structured text prompts. The data source for this research is patent information, which is readily accessible in an Excel format on various open-source platforms.\nFor the application use case under consideration, it requires specific patent details, namely the patent number, title, abstract, and claims. This information is obtained from a selected open-source website and subsequently extracted into an Excel format. Visual Basic for Applications (VBA) scripts have been employed to generate a prompt, which incorporates the patent content along with a question and instructions. The effectiveness of the prompt is ensured by sticking to the established rules and techniques of Prompt Engineering.\nUpon feeding the prompt into the LLM, it generates the necessary information pertaining to the specific patent. The nature of the responses from the LLM is contingent on the questions formulated in the prompt. For a comprehensive un-derstanding of the process, flowchart (Figure 2) is attached. This step-by-step explanation helps in easily understanding the methodology."}, {"title": "4.2 Information Extraction using LLM", "content": "As illustrated in the attached flowchart (Figure 3), various relevant pieces of information can be extracted from patent documents. Specifically, it can classify the system, comprehend the main function, identify the system components along with their features, and analyze input & output aspects. Additionally, it provides insight into the problem, the proposed solution, and the advantages of that solution. Furthermore, it suggests how AI can solve similar problems and also hints at a potential TRIZ approach for system idealization."}, {"title": "5 Evaluation", "content": "Let's see all the above-mentioned information extractions using prompt from a LLM with an example of Patent Number:\nUS20240120514A1. Structured prompt is given to LLM, which includes title, abstract, claims of a patent document and framed questions to LLM model, and output information from LLM is extracted in given manner:\nIn the next page prompt and the LLM's result have shown,"}, {"title": "6 Conclusion & Future Scope", "content": "This paper presented the method of extracting essential information from patents using prompt engineering, Approach used in the paper focuses mostly on the information extraction from the automotive powertrain innovations. In future work this approach can be extended to other areas of innovation outside powertrain domain."}]}