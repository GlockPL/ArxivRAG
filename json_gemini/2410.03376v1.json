{"title": "Mitigating Adversarial Perturbations for\nDeep Reinforcement Learning via Vector Quantization", "authors": ["Tung M. Luu", "Thanh Nguyen", "Tee Joshua Tian Jin", "Sungwoon Kim", "Chang D. Yoo"], "abstract": "Recent studies reveal that well-performing rein-\nforcement learning (RL) agents in training often lack resilience\nagainst adversarial perturbations during deployment. This\nhighlights the importance of building a robust agent before\ndeploying it in the real world. Most prior works focus on\ndeveloping robust training-based procedures to tackle this\nproblem, including enhancing the robustness of the deep neural\nnetwork component itself or adversarially training the agent\non strong attacks. In this work, we instead study an input\ntransformation-based defense for RL. Specifically, we propose\nusing a variant of vector quantization (VQ) as a transformation\nfor input observations, which is then used to reduce the\nspace of adversarial attacks during testing, resulting in the\ntransformed observations being less affected by attacks. Our\nmethod is computationally efficient and seamlessly integrates\nwith adversarial training, further enhancing the robustness\nof RL agents against adversarial attacks. Through extensive\nexperiments in multiple environments, we demonstrate that\nusing VQ as the input transformation effectively defends against\nadversarial attacks on the agent's observations.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern deep reinforcement learning (RL) agents [33],\n[10], [14] typically rely on deep neural networks (DNN) as\npowerful function approximators. Nevertheless, it has been\ndiscovered that even a well-trained RL agent may drastically\nfail under the small adversarial perturbations in the input\nduring deployment [15], [27], [18], [1], [37], making it risky\nto execute on safety-critical applications such as autonomous\ndriving [50]. Therefore, it is necessary to develop techniques\nto assist the RL agents in resisting adversarial attacks in input\nobservations before deploying them into the real world.\nThere have been many works proposed in the literature in\ndefending against adversarial attacks on input observations.\nA line of work focuses on enhancing the robustness of DNN\ncomponents by enforcing properties such as invariance and\nsmoothness via regularization schemes [42], [53], [35], [49],\nresulting in deep policy outputs that exhibit similar actions\nunder bounded perturbations. Another line of work considers\ntraining the RL agent in an adversarial manner, where an\nadversary is introduced to perturb the agent's input while\nit interacts with an environment. Sampled trajectories under\nthese attacks are subsequently used for training, resulting in\na more resilient RL agent. In this approach, the perturbation\ncan be induced from the policy/value function [18], [2], [37],\n[25] or more recently, it can be generated by another RL-\nbased adversary [52], [43]. While training with RL-based\nattackers can attain high long-term rewards under attacks, it\noften requires extra samples and computations for training.\nAforementioned strategies can be regarded as robust\ntraining-based defenses, aimed at learning resilient policy\nnetworks against adversarial attacks. Meanwhile, in the\nfield of image classification, there are also numerous input\ntransformation-based defenses [6], [26], [12], [38], [13], [17],\n[40] that mitigate such attacks without altering the under-\nlying model. These defenses attempt to reduce adversarial\nperturbations in the input by transforming it before feeding\nto the model. The transformation process commonly involves\ndenoisers for purifying perturbations [13], [17], [40], [34] or\nsimply utilizes image processing techniques to weaken the\neffect of attacks [6], [29], [12], [48], [38]. Therefore, this\napproach potentially benefits RL agents without requiring\nsignificant changes to underlying RL algorithms. However,\ndenoiser-based transformations often leverage powerful gen-\nerative models such as GAN [41], [17] or diffusion model\n[34] to remove noise, which may introduce overhead in\nboth training and inference for RL agents. On the other\nhand, the processing-based transformations are appealing due\nto their non-differential nature, making it challenging for\nadversaries to circumvent the defenses. Additionally, these\ntransforms are also cost-efficient and versatile, making them\nsuitable for use with RL agents. Nonetheless, many of these\ntransformations are tailored to image data [6], [12], [48],\n[38] and may not easily extend to vector inputs such as low-\ndimensional states in continuous control tasks.\nMotivated by this limitation, we propose using a variant of\nvector quantization (VQ) as a suitable input transformation-\nbased defense for RL, which is generally applicable for\nboth image input and continuous state. The key idea of our\napproach is to utilize VQ for discretizing the observation\nspace and subsequently train the RL agent within this trans-\nformed space. This strategy effectively reduces the space\nof adversarial attacks [15], [18], [53] that can impact the\nagent's observations, producing transformed inputs that are\nminimally affected by attacks. Our proposed approach is\ncomputationally efficient and modifies only the input rather\nthan the model itself, allowing it to synergistically comple-\nment other robust training-based defenses and enhance the"}, {"title": "II. RELATED WORK", "content": "Adversarial Attacks on State Observations. Since the\ndiscovery of adversarial examples in the classification [44],\nvulnerabilities in state observations of deep RL were first\ndemonstrated by [15], [27], [18]. [15] evaluated the ro-\nbustness of DQN agents in the Atari domain using FGSM\n[11] to attack at each step. Instead, [18] proposed using\nthe value function to determine when to launch attack. [27]\nconcentrated on attacking within specific steps of trajectories\nand employed a planner to craft perturbations that steer the\nagent toward a target state. [1] explored the black-box setting,\nrevealing transferable adversarial examples across different\nDQN models. In contrast to crafting perturbations solely\nbased on the policy, [37] introduced a more potent attack\nleveraging both the policy and the Q function. Recently,\n[53] formalized attacks on observations through a state-\nadversarial Markov decision process, demonstrating that the\nmost powerful attacks can be learned as an RL problem.\nBased on this, [52] and [43] introduced the RL-based adver-\nsaries for black-box and white-box attacks, respectively.\nRobust Training for Deep RL. To enhance the robustness\nof RL agents against adversarial attacks on observations, pre-\nvious works have primarily focused on strategies involving\nadversarial examples during training. [18]; [2] are concurrent\nworks that first proposed to adversarially train DQN agents\non Atari games. They used weak attacks on pixel space\nduring rollouts and preserved perturbed frames for training.\nHowever, this approach exhibited limited improvements in\nseveral Atari games. Another line of research introduces a\nregularization-based approach to enhancing the robustness of\nDQN agents. [39] proposed Lipschitz regularization, while\n[53] used a hinge loss regularizer to promote the smoothness\nof Q function under bounded perturbations. [35] utilized\nrobustness verification bound tools to compute the lower\nbound of Q function, thereby certifying the robustness of\naction selection. In continuous control tasks, a similar adver-\nsarial training approach was initially explored by [15], where\nattacks are induced from both policy and Q function, and\nthe trajectories sampled under attacks are used for training.\nHowever, recent work [53] found that this approach may\nnot reliably improve the robustness against new attacks.\n[52] proposed an alternative training paradigm involving\nLSTM-based RL agents and a black-box RL-based adversary.\nSimilarly, [43] proposed the same training paradigm with the\nwhite-box RL adversary, leading to a more robust RL agent.\nSmoothness regularization has also been proposed to improve\nthe robustness of the policy model in online RL setting [42],\n[53], as well as in offline setting [49].\nInput Transformation Based Defenses. In the domain\nof image classification, aside from robust training methods\n[31], [51], there have been many studies on defending\nadversarial attacks through input transformations [29], [12],\n[48], [38], [41], [13], [17], [40], [34]. Several works utilized\ntraditional image processing such as image cropping [12],\nrescaling [29], or bit depth reduction [48] to mitigate the\nimpact of adversarial attacks on the classifier. Other methods\nemployed the powerful generative models [41], [17], [34] or\ntrained the denoisers [26], [13] to reconstruct clean images.\nHowever, given our focus on control tasks using RL, it is\nmore appropriate to adopt cost-efficient techniques for coun-\ntering attacks. Furthermore, denoisers composed of DNNS\nare also vulnerable to gradient-based attacks. Motivated by\nthe utilization of image processing techniques, we propose\nto use VQ as an input transformation. Notably, unlike bit\ndepth reduction [48] that employs uniform quantization, our\nmethod learns representative points to quantize inputs based\non the statistic of input examples.\nInput Transformation on Deep RL. Input transformation\nhas been widely investigated in deep RL to enhance gener-\nalization [45], [5] or improve sample efficiency [21], [19],\n[30]. Domain randomization, as proposed in [45], aims to\ntransfer policies from simulators to the real world. Simple\naugmentations like cutout [5] or random convolution [23],\nas demonstrated in [5] and [23], have been shown to assist\nagents in generalizing to unseen environments. To reduce\nsample complexity in pixel-based RL, [21] applied image\naugmentations to the observations during agent training.\nFurthermore, [19] employed augmentation to regularize Q\nfunctions, further improving sample efficiency. Vanilla vector\nquantization (VQ) has been utilized in several works to re-\nduce the state space for generalization in tabular RL [7], [22]\nand continuous control [32]. Our proposed method differs\nfrom these works by quantizing individual dimensions rather\nthan entire vectors, making it more scalable. To the best of\nour knowledge, our use of input transformation represents\nthe first attempt at leveraging it to enhance robustness against\nadversarial attacks in RL."}, {"title": "III. PRELIMINARIES", "content": "A. Reinforcement Learning.\nAn reinforcement learning (RL) environment is modeled\nby a Markov decision process (MDP), defined as M\n= (S, A, R, P, \u03b3), where S is the state space, A is the action\nspace, $R: S\\times A \\times S \\rightarrow R$ is the reward function,\nP:S\\times A \\rightarrow S is the transition probability distribu-\ntion, and \u03b3\u2208 [0,1) is a discount factor. An agent takes\nactions based on a policy $\u03c0: S \\rightarrow A$. The objective\nof the RL agent is to maximize the expected discounted\nreturn $E_\u03c0[\\sum_{t=0} y^tR(s_t, a_t, s_{t+1})]$, which is the expected\ncumulative sum of rewards when following the policy \u03c0\nin the MDP. This objective can be evaluated by a value\nfunction $V^\u03c0(s) := E_\u03c0[\\sum_{t=0} Y^tRt|s_0 = s]$, or the action\nvalue function $Q^\u03c0(s, a) := E_\u03c0[\\sum_{t=0} Y^tRt|s_0 = s, a_0 = a]$."}, {"title": "B. Test-time Adversarial Attacks.", "content": "We consider adversarial attacks on the state observations\nduring test time, which is formulated as SA-MDP [53].\nSpecifically, during testing, the agent's observation is ad-\nversarially perturbed at every time step by an adversary\nequipped with a certain budget \u03f5. Note that, the adversary\nonly alters the observations and the true underlying states\nof the environment do not change. This setting fits many\nrealistic scenarios such as measurement errors, noise in\nsensory signals, or man-in-the-middle (MITM) attacks for\na deep RL system. For example, in robotic manipulation, an\nattacker can add imperceptible noise to the camera capturing\nan object, however, the actual object's location is unchanged.\nIn this paper, we consider a $l_\\infty$ norm threat model, in\nwhich the adversary is restricted to perturb the observation\ns into $s\\in B(s,\\epsilon) = {\\hat{s} : ||s - \\hat{s}||_\\infty < \\epsilon}$. Additionally,\nsince the adversary only appears at the test time, we assume\nthat the true states can be observed during training. This is\nimportant since our input transformation is learned to capture\nthe statistic of states while training the agent."}, {"title": "C. Vector Quantization.", "content": "Vector quantization (VQ) is a common technique widely\nused for learning discrete representation [46], [36], [28], [16].\nIn this work, we use VQ block similar to VQ-VAE [46] with\nsome modifications. We present the process of basic VQ\nhere and leave the modification in the next section. Initially,\nthe VQ block, referred to as Q, maintains a codebook C\nconsisting of a set of items ${c_k}_{k=1}^K$. Given an input vector\nz, Q outputs the item $c_m$ which is closest to z in Euclidean\ndistance as $c_m = Q(z)$, with $m = argmin_k ||c_k - z||_2$. The\ncodebook item can be updated by $l_2$ error or the exponential\nmoving average to move the item toward corresponding\nquantized vectors assigned to that item. In the backward\npass, the VQ block is treated as an identity function, referred\nto as straight-through gradient estimation [3]. The hyper-\nparameter K controls the size of the codebook, where lower\nvalues would lead to lossy compression but potentially yield\nmore abstract representations."}, {"title": "IV. METHODOLOGY", "content": "A. Input Transformation based Defense for RL\nTo understand the effectiveness of input transformation-\nbased defense for the RL agent, we commence by analyzing\nits performance under adversarial perturbations utilizing the\ntools developed in SA-MDP [53]. Let $f_1, f_2$ be functions\nmapping S \u2192 S, and let \u03c0 represent a Gaussian policy with a\nconstant independent variance. Assuming the policy network\nis L-Lipschitz continuous, we obtain:\n$max\\{V^{\\pi o f_1} (s) \u2013 V^{\\pi o f_2ov} (s)\\} < \\zeta (max_{s \\in S} max_{\\hat{s} \\in B(s,\\epsilon)} || f_1(s) - f_2(\\hat{s}) ||_2$ (1)\nwhere, \u03bd is optimal adversary corresponding to \u03c0, \u03b6 is a\nconstant independent of \u03c0. The proof of Eq. (1) relies on\nL-Lipschitz continuity of the policy network.\nEq. (1) suggests two distinct approaches for narrowing\nthe gap between natural performance and performance under\nperturbation. Firstly, when considering $f_1$ as an identity func-\ntion, we can design $f_2$ to reconstruct s from \u015d, which involves\nminimizing $max_{s\\in B(s,\\epsilon)} ||s - f_2(\\hat{s})||_2$. Secondly, we can\ndesign $f_1$ and $f_2$ to reduce the difference in the transformed\nspace, as expressed by $max_{\\hat{s} \\in B(s,\\epsilon)} || f_1(s) \u2013 f_2(\\hat{s})||_2$ being\nsmall. While the first approach can be achieved by utilizing\na denoiser, it carries certain disadvantages, as discussed in\nSection II. Therefore, we opt for the second approach to\ncounter adversarial attacks. In this approach, $V^{\\pi o f_1} (s)$ is not\nguaranteed to be identical to $V^\u03c0(s)$, as the agent operates\nwithin the transformed space rather than the origin one.\nHowever, as long as $f_1$ retains the essential information\nfrom the original space, the agent's performance can be\nmaintained, as shown in our experiments."}, {"title": "B. VQ Mitigating Adversarial Perturbations", "content": "Different to previous works [46], [28], [16], which use VQ\nto discretize the latent space, we directly apply VQ to each\ndimension of the raw input space as a transformation, and\nthen train the RL agents directly on the transformed inputs.\nWe define the space of adversarial attack in transformed\nspace by $B(s, \\epsilon) = {Q(\\hat{s}) : ||s - \\hat{s}||_0 < \\epsilon}$. Intuitively,\n$B(s, \\epsilon)$ is a set of possible items $c_k$ to which the perturbed\nstate s can be assigned. We find that using VQ with an\nappropriate small codebook size as the input transformation\neffectively reduces the space of adversarial attacks, i.e.,\nthe size of $B$, without significantly reducing the natural\nperformance of the agent. As depicted in Fig. 1a (top) for\none-dimensional data, supposing the state s is assigned to\nthe item $c_2$, and the adversary can arbitrarily perturb the\nstate s within the \u03f5 ball. We can see that if the $B(s,\\epsilon)$\nis still lying within the boundaries of $c_2$ (the blue dotted\nlines), Q will transform both s and $\\hat{s} \\in B(s, \\epsilon)$ into the\nsame item, i.e., $Q(s) = Q(\\hat{s})$ for $\\forall \\hat{s} \\in B(s, \\epsilon)$. Additionally,\nwe also observe that the space of attacks is proportional to\nthe size of the codebook. It means that K decreases leading\nto smaller size of $B(s,\\epsilon)$, thus stronger in resisting the\nadversarial perturbations. This is illustrated in Fig. 1a, larger\nK shrinks the radius of items, while smaller K enlarges the\nradius. Moreover, due to straight-through estimation, VQ also\ninherits non-differential properties as image transformations.\nFor states lying close to the boundary, with appropriate small\nK and not too large \u03f5, the transformed states are altered at\nmost to the closest neighbor items.\nTo better understand the effectiveness of VQ for counter-\ning adversarial attacks, we illustrate it with a toy regression\ntask. We train a predictor $\u03c0_\\theta$ to regress from the state to the\naction on the walker-medium-v2 dataset [8], using VQ\nas input transformation. The model is optimized by minimiz-\ning the MSE between the prediction and ground truth action.\nAt the test time, we introduce the adversary on the state s to\nobtain the perturbed state $\\hat{s} = argmax_{\\hat{s} \\in B(s,\\epsilon)} ||\u03c0_\u03b8(s) \u2013 a||_2$,\nwith a is the ground truth. The maximization is solved by\nusing 10-step projected gradient descent (PGD) as in [20],\n[37]. We evaluate the performance (i.e., MSE) under different\nscales of \u03f5, where \u03f5 = 0 corresponds to $\\hat{s} = s$."}, {"title": "V. EXPERIMENTS", "content": "A. Evaluation in MuJoCo\nIn this section, we evaluate the effectiveness of the pro-\nposed method against common adversarial attacks in both\nonline and offline RL settings. We adopt three common\nattacks as used in [53], [52], namely Random, Action Diff,\nand Min Q. Given an attack budget \u03f5 and a state s, adversaries\ngenerate perturbed states as follows: (1) Random: \u015dis\nuniformly sampled within $B(s,\\epsilon)$, (2) Action Diff: \u015dis\ninduced from the agent's policy. Specifically, \u015d is searched\nwithin $B(s, \\epsilon)$ to satisfy $max_{\\hat{s} \\in B(s,\\epsilon)} D(\u03c0(\u00b7|5)||\u03c0(\u00b7|\\hat{s}))$, with\nD is KL divergence, (3) Min Q: Different from Action Diff,\nthis adversary generates perturbations based on both the\nagent's policy and Q function, which is a relatively stronger\nattack. \u015d is selected to satisfy $min_{\\hat{s} \\in B(s,\\epsilon)} Q(s, a)$, with a is\nthe policy output. For Action Diff and Min Q, we use 10-step\nPGD as in [20], [37].\n1) Online RL: For online RL setting, we follow the\nexperiment setup as in [53], but use Soft Actor-Critic (SAC)\nas a base RL algorithm due to its sample efficiency and\nhigher performance on Gym MuJoCo [14]. We conduct ex-\nperiments on five environments including Walker2d, Hopper,\nAnt, Reacher, and InvertedPendulum. For a in Eq. (3), we\nsearch in {30, 40, 50, 60} and set a value of 30 for Hopper\nand 60 for the others. Regarding the codebook size, we set\nK = 8 for Walker2d and Reacher, and K = 16 for the\nothers. We compare the proposed method with vanilla SAC\nand SAC with bit depth reduction [48] (SAC-BDR), where\nuniform quantization is naively performed in each dimension\nof the input. Additionally, we also incorporate VQ into a\nstrong adversarial training baseline [53], referred to as SAC-\nSA, to see whether it can further improve the robustness.\nWe evaluate the robustness of methods under different\nscales of \u03f5, where \u03f5 = 0 corresponds to the natural\nperformance"}, {"title": "B. Evaluation in Atari", "content": "We investigate effectiveness of VQ into the Double DQN\n[47] on two Atari games: Freeway and Pong. These envi-\nronments feature high-dimensional pixel inputs and discrete\naction spaces. We set K = 4 for these environments. For the\nrobustness evaluation, we use 10-step $l_\\infty$-PGD untargeted\nattack. Additionally, we also incorporate bit depth reduction\nfor the DQN agent, referred as DQN-BDR. Furthermore,\nwe integrate the proposed method into a state-of-the-art\nmethod, namely RADIAL [35], to investigate whether it\ncan enhance robustness. As demonstrated in Tab. III, the\nDQN-VQ is more effective compared to DQN-BDR. This\noutcome underscores the advantages of learning codebooks\nover uniform quantization. Surprisingly, our method is able\nto achieve comparable with RADIAL without adversarial\ntraining. By combining VQ with RADIAL, we achieve\nfurther improvement in robustness, especially at larger values\nof \u03f5 such as 10/255 for Pong and 5/255 for Freeway."}, {"title": "C. Ablation Study", "content": "Effectiveness of Codebook Size. We provide the experiment\nshowing the effectiveness of different codebook sizes in Fig.\n2. Across environments, the small values of K often lead to"}, {"title": "Adaptive Learning Codebook.", "content": "To demonstrate the effec-\ntiveness of adaptive scale during updating codebooks, we"}, {"title": "D. Computational Cost Comparison.", "content": "We compare training time when using VQ transformation\non a single machine with one GPU (RTX 3080). The result\nis shown in Tab. IV. When combined with vanilla SAC and\nSAC-SA, VQ slightly increases the training time to 5% and\n7%, respectively."}, {"title": "VI. CONCLUSION", "content": "We have presented a novel defense based on input transfor-\nmation to counter adversarial attacks on state observations.\nOur proposed approach is both cost-efficient and highly\neffective in defending against such attacks. Furthermore,\nwhen combined with robust training-based defenses, it sig-\nnificantly enhances the overall robustness of RL agents. We"}, {"title": "APPENDIX", "content": "Performance bound. SA-MDP [53] provides an upper\nbound of performance gap between the two policies trained\non non-adversarial MDP and state-adversarial MDP (SA-\nMDP), respectively. We based on this to derive our upper\nbound in Eq. (1). Formally, given a policy \u03c0 and its value\nfunction V(s), under the optimal adversary \u03bd in SA-MDP,\ntheorem 5 in [53] stated that:\n$max\\{V^{\\pi} (s) \u2013 V^{\\pi o \\nu} (s)\\} < \\kappa max_{s \\in S} max_{\\hat{s} \\in B(s,\\epsilon)} D_{TV} (\\pi(\\cdot|s), \\pi(\\cdot|\\hat{s}))$ (4)\nwhere, $D_{TV} (\\pi(\\cdot|s), \\pi(\\cdot|\\hat{s}))$ is the total variance distance\nbetween $\u03c0(\u00b7|s)$ and $\u03c0(\u00b7|\\hat{s})$, \u03ba is a constant that does not\ndepend on \u03c0, $B(s, \\epsilon) = {\\hat{s} : ||s-\\hat{s}||_\\infty \\le \\epsilon}$, and $\u03c0o\u03bd$ denotes\nthe policy under perturbations: $\u03c0(a|\u03bd(s))$. The total variation\ndistance is not easy to compute for most distributions, thus\nwe upper bound $D_{TV}$ by the KL divergence:\n$D_{TV} (\\pi(\\cdot|s), \\pi(\\cdot|\\hat{s}) \\le \\sqrt{\\frac{1}{2}KL (\\pi(\\cdot|s) || \\pi(\\cdot|\\hat{s}))}.$ (5)\nWe assume that the policy is Gaussian with constant indepen-\ndence variance, which is commonly used in RL algorithms\nsuch as TD3 [10]. Supposing that $\u03c0(s) \\sim \\mathcal{N}(\u03bc_s, \u03a3_s)$\nand $\u03c0(\\hat{s}) \\sim \\mathcal{N}(\u03bc_{\\hat{s}}, \u03a3_{\\hat{s}})$, where $\u03bc \\in \\mathbb{R}^d$ and $\u03bc_s, \u03bc_{\\hat{s}}$\nare respectively produced by neural networks $\u03bc_\u03b8(s), \u03bc_\u03b8(\\hat{s})$,"}]}