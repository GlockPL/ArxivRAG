{"title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization", "authors": ["Xinghua Zhang", "Haiyang Yu", "Cheng Fu", "Fei Huang", "Yongbin Li"], "abstract": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instruction-following ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and out-of-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on out-of-domain data compared to SFT and DPO respectively.", "sections": [{"title": "Introduction", "content": "The rapid development of large language models (LLMs) has facilitated human-machine interaction, with instructions serving as the medium (Gao et al., 2024; Kim et al., 2024; Zhang et al., 2024b). As human needs evolve, there is an increasing expectation for models to handle more intricate tasks through complex instructions (Ge et al., 2023; Yang et al., 2024b; Wang et al., 2024). Consequently, the instruction-following ability, especially complex instructions, is garnering significant attention (Zhou et al., 2023; Xu et al., 2024; Li et al., 2024; Zhang et al., 2024a).\nTo evaluate the instruction-following abilities of LLMs, several benchmarks (Zhou et al., 2023; Qin et al., 2024; Li et al., 2024) have been proposed, which are designed to systematically assess how well these models can understand and execute instructions. IFEval (Zhou et al., 2023) focuses on verifiable instructions which are amenable to objective verification of compliance. Qin et al. (2024) introduces INFOBENCH which contains 500 distinct instructions and 2,250 decomposed questions to assess the ability of instruction following. Recently, the ability to follow complex instructions with multiple constraints is gaining increasing attention (He et al., 2024b; Jiang et al., 2024; Wen et al., 2024; He et al., 2024a) as LLMs are deployed in sophisticated real-world applications. Zhang et al. (2024a) proposes a constraint-following benchmark CF-Bench with 1,000 multi-constraint samples. However, most of benchmarks lay emphasis on evaluating LLMs' ability to follow complex instructions, lack of algorithms tailored for enhancing the corresponding ability.\nFrom RLHF (Reinforcement Learning from Human Feedback) (Ouyang et al., 2022; Bai et al., 2022a) to the following-up researches such as DPO (Direct Preference Optimization) (Rafailov et al., 2023), alignment algorithms which align LLMs with human preferences, have demonstrated their effectiveness in improving the LLMs' capabilities to follow instructions. Nevertheless, these methods directly explore different responses y (Ywin, Yloose) based on the same instruction x, as shown in Figure 3 (a). In the complex instruction scenario which contains multiple constraints, it is challenging to efficiently perceive the fine-grained constraints in x solely by modeling different y.\nTo bridge this gap, this paper first introduces TRACE benchmark to improve the ability of LLMs to track complex fine-grained constraint instructions and make them more obedient. TRACE is automatically constructed based on the manually sorted taxonomy of complex instructions with 26 constraint dimensions within 5 constraint types. We develop an automated data construction workflow that extends from open-source simple instructions to multi-constrained complex ones. In the end, we accumulate 120K complex instructions for model training and 1K human-verified data for evaluation. To enhance the ability of LLMs to follow complex instructions, this paper further proposes Input-Output Preference Optimization (IOPO) method. IOPO not only takes the instruction x as input to directly learn the response y preference, but also gradually delves deeper into instructions x based on the same response y, to promote effective perception of fine-grained constraints, as shown in Figure 3 (b).\nThe major contributions of this paper are summarized as follows:\n\u2022 We introduce a benchmark TRACE for complex instruction following, which includes both an evaluation set and a training set, and an automated data construction workflow, further enriching the research community.\n\u2022 Different from previous alignment paradigm, we propose IOPO alignment method which deeply explores the complex instructions x (Input), not just directly learning response preference y (Output).\n\u2022 Extensive experiments on both in-domain and out-of-domain evaluations have confirmed the consistent improvements, with an average increase of 7.22% and 2.66%, compared to SFT and DPO, respectively."}, {"title": "Related Work", "content": "Instruction Following\nInstruction following is the most fundamental and crucial ability for large language models (LLMs), which enables them to understand and execute user instructions accurately, making them more effective in a wide range of applications. In fact, earlier studies have explored the extent to which models follow language instructions (Ye and Ren, 2021; Mishra et al., 2022; Hase and Bansal, 2022). It is effective to fine-tune LLMs on these annotated instruction data for improving the ability to follow natural language instructions. The instruction-following ability enhances adaptability to unseen tasks, which has become an efficient learning paradigm for novel task demands (Lou et al., 2023). As human demands grow higher, the instructions given to LLMs are also becoming increasingly complex. Recent studies are beginning to focus on the complex instruction-following ability of LLMs, where more complex or constrained instructions have been proven effective in enhancing LLMs' abilities to follow instructions (Mukherjee et al., 2023; Xu et al., 2024; Luo et al., 2024). Constrained instructions, as a type of complex instruction, are also gradually receiving attention from the research community. Increasing the complexity of constraints within the instruction (e.g., raising the number of constraints) can further improve the ability to follow complex instructions (Sun et al., 2024; Dong et al., 2024; He et al., 2024a). Sun et al. (2024) introduces a instruction tuning dataset Conifer and proposes a progressive learning scheme to enhance the ability of LLMs to follow multi-level instructions with complex constraints. He et al. (2024a) first finds that multi-constraint instructions can enhance LLMs' understanding of complex instructions, and then introduces a discrimination-based method for data acquisition, and finally proposes a contrastive method with reinforcement learning fine-tuning (RLFT) for data utilization. In addition, some work focuses on evaluating the multi-constraint instruction-following capabilities of LLMs (Zhou et al., 2023; Qin et al., 2024; Jiang et al., 2024). Zhang et al. (2024a) introduces CFBench, a benchmark which encompasses"}, {"title": "LLM Alignment", "content": "LLM alignment aims to enhance LLMs by aligning them with human preference. Recent research has conducted extensive explorations for LLM alignment. From RLHF/PPO (Ouyang et al., 2022; Bai et al., 2022a) to DPO (Rafailov et al., 2023) and beyond (Bai et al., 2022b; Song et al., 2024; Meng et al., 2024), the evolution of xPO-series alignment algorithms has seen significant advancements. These methodologies have been pivotal in improving the alignment between LLMs and human values, ensuring that the outputs of these models are not only effective but also follow human preference.\nRLHF involves training models using human-provided rewards or reward models to improve decision-making, optimizing policies through iterative feedback loops for more aligned outcomes. DPO directly optimizes the model's output to match preferred response as indicated by human feedback, which simplifies the alignment process by focusing on direct comparisons between preferred and dispreferred outputs, allowing the model to learn human preference without needing an explicitly defined reward model. SimPO (Meng et al., 2024) proposes a method for preference optimization that eliminates the need for the reference model \u03c0ref, which is memory efficient and simple. Instead of using pairwise data, PRO (Song et al., 2024) utilizes the preference ranking of any length listwise preference dataset.\nTo further enrich the community of multi-constraint instruction following ability, we construct TRACE benchmark which contains instructions with multiple constraints, more fine-grained constraint types and a wider range of constraint quantities. In addition, we propose the tailor-designed alignment algorithm IOPO for fine-grained multi-constraint alignment, which is different from previous methods (e.g., DPO) only focusing on the output preference."}, {"title": "Preliminary", "content": "Existing alignment methods have evolved from RLHF (Ouyang et al., 2022; Bai et al., 2022a), this section provides a brief introduction to RLHF which mainly consists of three stages:\n1) SFT: The generic pre-trained LM is fine-tuned with maximum likelihood supervised loss on downstream task data, and then we can get the SFT model \u03c0SFT.\n2) Reward Model: The model SFT is utilized with prompt x to generate two different responses y1, y2. The pair of responses is labeled as \u201cpreferred\" and \"dispreferred\u201d by human labelers, i.e., y1 > y2 | x. The reward model r\u03c6 is trained with the following negative log-likelihood loss:\n$L_R = -E_{(x,y_1,y_2)\\sim D}[log\\sigma(r_{\\phi}(x, y_1) \u2013 r_{\\phi}(x, y_2))]$\n3) RL: This stage uses the learned reward model r\u03c6 to provide feedback to the language model policy, the optimization objective is as follows:\nmax Ex~D,y~\u03c0\u03b8 (y|x) [r\u00a2(x, y)]- \nBDKL[\u03c0\u03b8(y|x)||Tref(y|x)]\nwhere LM policy \u03c0\u03b8, base reference policy ref are both initialized with \u03c0\u03c2FT, \u03b2 controls the deviation of \u03c0\u03b8 from the base policy ref."}, {"title": "TRACE Benchmark", "content": "This section describes the construction pipeline of TRACE, its statistics, and evaluation protocol.\nConstruction Pipeline\nThe overall construction process includes several key stages: 1) Taxonomy of Constraint, 2) Constraint Expansion, 3) Instruction Structuring, 4) Quality Control, and 5) Response Generation & Evaluation.\nTaxonomy of Constraint. A comprehensive constraint type system is developed through inference by LLM from a large volume of open-source simple instructions, and further refined by human experts, into 5 constraint types (Jiang et al., 2024) and 26 constraint dimensions. The detailed description of constraints is shown in Appendix A.\nConstraint Expansion. This step aims to expand simple instructions into more complex ones that incorporate multiple constraints based on the taxonomy of constraint by prompting LLM.\nInstruction Structuring. To better distinguish different segments of the instruction, this step structures the flat instruction text expanded from the last step into Task Description, Constraints, and Input part by prompting LLM."}, {"title": "Evaluation Protocol", "content": "We use GPT-4o as the evaluator to assess the generated response based on the complex instruction. Concretely, inspired by Qin et al. (2024); Zhang et al. (2024a), we prompt the LLM evaluator to evaluate each constraint mentioned in the complex instruction on a scale of 0 to 10 score, assessing the degree to which the response follows each constraint. A higher score indicates stronger adherence to the specified constraint. The overall instruction following score IF on the evaluation set with n complex instructions can be calculated as follows:\nIF = 1/n \u2211 1/m_i \u2211I_j=1^mi S_i,j\nwhere Si,j \u2208 [0,10] is the score indicating the degree to which the j-th constraint in the i-th instruction is adhered to. mi is the number of constraints in the i-th instruction. I = 1, when \u2211S_i,j / m_i = 10, otherwise is 0. That is, a response is considered correct only when all constraints in the complex instruction are fully followed.", "latex": ["IF = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{m_i} \\sum_{j=1}^{m_i} S_{i,j}"]}, {"title": "Evaluation Set Quality", "content": "To generate the high-quality evaluation set, we further introduce a rigorous post-inspection process after construction pipeline (Sec. 4.1). First, we use the powerful LLM GPT-4o to check the following items for each instruction in the evaluation set: 1) Is the description empty? 2) Is there redundancy between the constraints and description? 3) Does the input match the description? If any of the aforementioned issues arise, we prompt the GPT-4o to make corrections. Second, we make the manual annotation process involving multiple steps such as annotator training, small-scale trial annotation, selection of official annotators, and formal annotation. Finally, we randomly select 100 instructions for quality evaluation, which are then inspected by three labeling specialists based on the above check items and the overall validity. The agreement rate among three annotators on the sampled evaluation set is 95%."}, {"title": "Input-Output Preference Optimization", "content": "Both RLHF (Ouyang et al., 2022; Bai et al., 2022a) and its variants, such as DPO (Rafailov et al., 2023), directly learn the response y preference (Output) given the same instruction x (Input). However, complex instructions consist of multiple fine-grained constraints, direct preference learning for the output y struggles to perceive fine-grained constraints in the input x. To enhance the model's perception of fine-grained instruction, we further introduce the input preference learning which reflects on the constraints in the instruction x based on the response y. By performing preference learning of both input and output, input-output preference optimization (IOPO) not only rapidly fits the better output but also meticulously considers the fine-grained information of the input.\nConcretely, we construct a pair of instructions <x1, x2> whose responses are respectively <y1, y2>, where x2 has subtle differences from x\u2081 in some constraints, and these differences would result in substantially divergent responses. And then, we can get four input-output pairs <x1, y1>, <x1, y2>, <x2, y1>, and <x2, y2>, which can form a preference group pair G1 > G2 (G\u2081 = {<x1, y1>, <x2, y2>}, G2 = {<x1, y2>, <x2, y1>}). The detailed data construction process is described in Appendix D. The first group is the matched input-output pair while the second one is mismatched. As derived from Eq.2 in Rafailov et al. (2023), the reward function r(x, y) can be represented by the"}, {"title": "policy model \u03c0\u03b3 as follows:", "content": "r(x, y) = Blog \\frac{\u03c0\u03b3(y|x)}{Tref(y|x)}+ Blog Z(x)\nwhere Z(x) = \u03a3y Tref(yx) exp (ar(x,y)).\nThe Bradley-Terry model (Bradley and Terry, 1952) is a probability model for the outcome of pairwise comparisons between items, groups, or objects. Given a pair of items i and j, it estimates the probability that the pairwise comparison i > j turns out true (Hunter, 2004), as\np(i > j) = Pi / Pi + Pj\nwhere pi is a positive real-valued score assigned to individual i, and the comparison i > j can mean \u201ci is preferred to j\u201d. Similarly, given a pair of groups G1 and G2, we can define p1 = er(x1,y1)+r(x2,y2) for G1, and p2 = er(x1,y2)+r(x2,y1) for G2, as\np(G1 > G2) = erg1 / er91 + e92\nrG\u2081 = r(x1,y1) + r(x2, y2)\nrG2 = r(x1,y2) + r(x2, y1)\nNext, combining Eq. 6 and Eq. 4, we can further derive as follows (details in Appendix E):\np(G1 > G2) = \u03c3(\u03a01 / \u03a02) /((\u03a01 + \u03a0\u2082))\n\u03a0\u2081 = 2\u03b2 log \u03c0\u03b3(Y1X1) / Blog Tref (Y1x1)\n\u03a0\u2082 = 2\u03b2 log \u03c0\u03b3(Y2X2) / Blog Tref (Y2x2)\nwhere o is the sigmoid function. Therefore, the optimization objective of IOPO is to maximize p(G1 > G2). Motivated by Rafailov et al. (2023), we can formulate a maximum likelihood loss for a parametrized policy model \u03c0\u03b8 as follows:\nLIOPO (\u03c0\u03b8) = -Ex1,y1,x2,y2~D {10g[((:\u03c0\u03b8(Y1X1) / Tref (Y1X1))/(\u03c0\u03b8(12/21) / Tref (Y2X1)))]+ 2\u00dflog((\u03c0\u03b8 (12/12) / Tref (Y2X2))/(\u03c0\u03b8 (Y1X2) / Tref (Y1 X2)))}]-(2\u00dflog(\u03c0\u03b8 (11/12) / Tref (Y1X2)) / (\u03c0\u03b8 (Y2X1) / Tref (Y2X1)))", "latex": ["r(x, y) = Blog \\frac{\\pi\u03b3(y|x)}{Tref(y|x)}+ Blog Z(x)", "p(i > j) = \\frac{P_i}{P_i + P_j}", "p(G_1 > G_2) = \\frac{e^{r_{G_1}}}{e^{r_{G_1}} + e^{r_{G_2}}}", "p(G1 > G2) = \\sigma(\\frac{\\Pi_1}{\\Pi_2}) / ((\\Pi_1 + \\Pi_2))", "L_{IOPO} (\\pi_{\\theta}) = -E_{x_1,y_1,x_2,y_2\\sim D} {log[((:\\frac{\\pi_{\\theta}(Y_1|X_1)}{Tref (Y_1|X_1)})/(\\frac{\\pi_{\\theta}(Y_2|X_1)}{Tref (Y_2|X_1))})]+ 2\\beta log((\\frac{\\pi_{\\theta} (Y_2|X_2)}{Tref (Y_2|X_2)})/(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2))}))]-(2\\beta log(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2)}) / (\\frac{\\pi_{\\theta} (Y_2|X_1)}{Tref (Y_2|X_1)}))}"]}, {"title": "Experiments", "content": "Experimental Settings\nEvaluation Datasets. We conduct experiments on three instruction-following datasets: TRACE, IFEval (Zhou et al., 2023), and CFBench (Zhang et al., 2024a). TRACE evaluation set is introduced in this paper, which has 1,042 instructions, and an average of 4.89 constraints per instruction, with a maximum of 15 constraints. IFEval consists of 541 prompts, with each prompt containing one or multiple verifiable instructions. CFBench contains 1,000 samples that cover more than 200 real-life scenarios and over 50 NLP tasks, with each sample including multiple constraints. It is worth noting that TRACE is the in-domain evaluation set, IFEval and CFBench are the out-of-domain ones.\nImplementation Details. (1) TRACE Benchmark: we choose Qwen2-72B-Instruct (Yang et al., 2024a)\u00b9 for benchmark construction. (2) IOPO Alignment: we choose Qwen2-7B-Instruct\u00b2, and LLaMA3.1-8B-Instruct\u00b3 as the LLM backbone. All models, except for the base models (i.e. Qwen2-7B-Instruct and Llama-3.1-8B-Instruct), are trained on TRACE's training set or its variants (Train Once, Test Anywhere). The learning rate is 1e-4 for supervised fine-tuning (SFT), and 5e-6 for DPO and IOPO. The maximum length and epoch are set to 6,000 and 3 respectively. \u03b2 is set to 0.1. We implement our code based on LLaMA-Factory (Zheng et al., 2024), perform parallel training on 4 \u00d7 8-GPU machines, with a batch size of 1 per GPU. The DPO training data construction is shown in Appendix C.\nEvaluation Metrics. For TRACE, we use GPT-4o to evaluate if all constraints in the instruction have been followed (IF-S for single-constraint instructions, and IF-M for multi-constraint instructions), as described in Sec. 4.3. For IFEval, we use prompt-level strict and loose accuracy defined in Zhou et al. (2023), abbr. S-Acc and L-Acc respectively. CFBench (Zhang et al., 2024a) introduces three evaluation metrics with GPT-4o as the evaluation model: constraint satisfaction rate (CSR), instruction satisfaction rate (ISR), and priority satisfaction rate (PSR)."}, {"title": "Experimental Results", "content": "Main Results. As shown in Table 3, we give the main results under different benchmarks, including in-domain TRACE, out-of-domain IFEval and CF-Bench. The experiments are conducted under two different base models, Qwen2-7B, and Llama3.1-8B, where Instruct means directly using Qwen2-7B-Instruct or Llama3.1-8B-Instruct for inference, SFT represents the model is trained on TRACE training set, and PPO, DPO, IOPO are respectively trained on preference data derived from TRACE training set.\nFor in-domain evaluation on TRACE set, we can see 3.0%, 1.7% improvements of IOPO on single- and multi-constraint instructions with Qwen2-7B as the base model compared to DPO, and 2.5%, 1.5% improvements with Llama3.1-8B as the base model. For out-of-domain evaluation on IFEval and CFBench, IOPO achieves an average increase of 3.2%, and 3.06% in comparison with DPO based on Qwen2-7B and Llama3.1-8B respectively. The significant advantages of both in-domain and out-of-domain evaluations confirm the effectiveness of input-output preference optimization, which intensively considers the constraint differences between instructions, enhancing the model's perception of constraints. It is worth noting that IOPO has a larger performance gap with SFT especially on IFEval, compared to DPO and SFT, which confirms the generalization of IOPO and the necessity of further modeling input preferences.\nAblation Studies. To further confirm the effectiveness of input and output preference, we conduct the ablation studies on TRACE, IFEval, and CFBench as shown in Table 4, where \"w/o Out-"}, {"title": "Method", "content": "SFT DPO IOPO\nput Pref\" means we only consider the modeling of input preference with the same training data, \"w/o Input Pref\u201d means we only consider the modeling of output preference. We can see that output preference contributes to 2%, and 0.93% increases with Qwen2-7B and Llama3.1-8B respectively, input preference separately brings 1.51% and 1.58% performance gains, which confirms the effectiveness of both input and output preference modeling. Besides the paradigm for modeling output preference in existing alignment methods, it's established that modeling input preference is crucial for deeply considering constraints within the instruction.\nComplexity Analysis. We conduct the analyses of complexity in Table 5, where all methods are conducted under the same experimental settings, such as the batch size and GPU. (1) For #Memory, DPO and IOPO are approximately twice and four times that of SFT respectively, because DPO needs a pair of responses to calculate the corresponding loss (<x, y1>, <x, y2>), and IOPO needs to compute four groups of input-output pairs (<x1,Y1>, <x2,Y2>, <x1, Y2>, <x2,Y1>) in its loss. (2) For #Training Time, DPO and IOPO require the computation of more tokens compared to SFT under the same batch size, leading to longer training time. (3) For #Inference Speed, SFT, DPO, and IOPO are all the same base model optimized for inference, resulting the same inference speed. The training efficiency and GPU memory usage of IOPO are not the best among compared baselines, but their efficiencies are still of the same order of magnitude, which are reasonable and acceptable comprehensively considering the significant performance advantage.\nThe Impact of Token Quantity. To address concerns regarding the IOPO training tokens, we conduct the analyses on the impact of token quantity and report the results in Figure 4, and Figure 5. For IOPO, there exist two instructions along with their corresponding responses ({<x1, y1>, <x2, Y2>}). To ensure that DPO and IOPO consume"}, {"title": "Conclusion", "content": "This paper focuses on the ability of LLMs to follow complex instructions, and introduces TRACE, a multi-constraint complex instruction benchmark which consists of 120K training samples and 1K test cases. Furthermore, we propose IOPO align-"}, {"title": "policy model \u03c0\u03b3 as follows:", "content": "\u03c0\u03b3(y|x) /Tref(y|x)+ Blog Z(x)\nwhere Z(x) = \u03a3y Tref(yx) exp (ar(x,y)).\nThe Bradley-Terry model (Bradley and Terry, 1952) is a probability model for the outcome of pairwise comparisons between items, groups, or objects. Given a pair of items i and j, it estimates the probability that the pairwise comparison i > j turns out true (Hunter, 2004), as\np(i > j) = Pi / Pi + Pj\nwhere pi is a positive real-valued score assigned to individual i, and the comparison i > j can mean \u201ci is preferred to j\u201d. Similarly, given a pair of groups G1 and G2, we can define p1 = er(x1,y1)+r(x2,y2) for G1, and p2 = er(x1,y2)+r(x2,y1) for G2, as\np(G1 > G2) = erg1 / er91 + e92\nrG\u2081 = r(x1,y1) + r(x2, y2)\nrG2 = r(x1,y2) + r(x2, y1)\nNext, combining Eq. 6 and Eq. 4, we can further derive as follows (details in Appendix E):\np(G1 > G2) = \u03c3(\u03a01 / \u03a02) /((\u03a01 + \u03a0\u2082))\n\u03a0\u2081 = 2\u03b2 log \u03c0\u03b3(Y1X1) / Blog Tref (Y1x1)\n\u03a0\u2082 = 2\u03b2 log \u03c0\u03b3(Y2X2) / Blog Tref (Y2x2)\nwhere o is the sigmoid function. Therefore, the optimization objective of IOPO is to maximize p(G1 > G2). Motivated by Rafailov et al. (2023), we can formulate a maximum likelihood loss for a parametrized policy model \u03c0\u03b8 as follows:\nLIOPO (\u03c0\u03b8) = -Ex1,y1,x2,y2~D {10g[((:\u03c0\u03b8(Y1X1) / Tref (Y1X1))/(\u03c0\u03b8(12/21) / Tref (Y2X1)))]+ 2\u00dflog((\u03c0\u03b8 (12/12) / Tref (Y2X2))/(\u03c0\u03b8 (Y1X2) / Tref (Y1 X2)))}]-(2\u00dflog(\u03c0\u03b8 (11/12) / Tref (Y1X2)) / (\u03c0\u03b8 (Y2X1) / Tref (Y2X1)))", "latex": ["\\frac{\\pi\u03b3(y|x)}{Tref(y|x)}+ Blog Z(x)", "p(i > j) = \\frac{P_i}{P_i + P_j}", "p(G_1 > G_2) = \\frac{e^{r_{G_1}}}{e^{r_{G_1}} + e^{r_{G_2}}}", "p(G1 > G2) = \\sigma(\\frac{\\Pi_1}{\\Pi_2}) / ((\\Pi_1 + \\Pi_2))", "L_{IOPO} (\\pi_{\\theta}) = -E_{x_1,y_1,x_2,y_2\\sim D} {log[((:\\frac{\\pi_{\\theta}(Y_1|X_1)}{Tref (Y_1|X_1)})/(\\frac{\\pi_{\\theta}(Y_2|X_1)}{Tref (Y_2|X_1))})]+ 2\\beta log((\\frac{\\pi_{\\theta} (Y_2|X_2)}{Tref (Y_2|X_2)})/(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2))}))]-(2\\beta log(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2)}) / (\\frac{\\pi_{\\theta} (Y_2|X_1)}{Tref (Y_2|X_1)}))}"]}, {"title": "policy model \u03c0\u03b3 as follows:", "content": "\u03c0\u03b3(y|x) /Tref(y|x)+ Blog Z(x)\nwhere Z(x) = \u03a3y Tref(yx) exp (ar(x,y)).\nThe Bradley-Terry model (Bradley and Terry, 1952) is a probability model for the outcome of pairwise comparisons between items, groups, or objects. Given a pair of items i and j, it estimates the probability that the pairwise comparison i > j turns out true (Hunter, 2004), as\np(i > j) = Pi / Pi + Pj\nwhere pi is a positive real-valued score assigned to individual i, and the comparison i > j can mean \u201ci is preferred to j\u201d. Similarly, given a pair of groups G1 and G2, we can define p1 = er(x1,y1)+r(x2,y2) for G1, and p2 = er(x1,y2)+r(x2,y1) for G2, as\np(G1 > G2) = erg1 / er91 + e92\nrG\u2081 = r(x1,y1) + r(x2, y2)\nrG2 = r(x1,y2) + r(x2, y1)\nNext, combining Eq. 6 and Eq. 4, we can further derive as follows (details in Appendix E):\np(G1 > G2) = \u03c3(\u03a01 / \u03a02) /((\u03a01 + \u03a0\u2082))\n\u03a0\u2081 = 2\u03b2 log \u03c0\u03b3(Y1X1) / Blog Tref (Y1x1)\n\u03a0\u2082 = 2\u03b2 log \u03c0\u03b3(Y2X2) / Blog Tref (Y2x2)\nwhere o is the sigmoid function. Therefore, the optimization objective of IOPO is to maximize p(G1 > G2). Motivated by Rafailov et al. (2023), we can formulate a maximum likelihood loss for a parametrized policy model \u03c0\u03b8 as follows:\nLIOPO (\u03c0\u03b8) = -Ex1,y1,x2,y2~D {10g[((:\u03c0\u03b8(Y1X1) / Tref (Y1X1))/(\u03c0\u03b8(12/21) / Tref (Y2X1)))]+ 2\u00dflog((\u03c0\u03b8 (12/12) / Tref (Y2X2))/(\u03c0\u03b8 (Y1X2) / Tref (Y1 X2)))}]-(2\u00dflog(\u03c0\u03b8 (11/12) / Tref (Y1X2)) / (\u03c0\u03b8 (Y2X1) / Tref (Y2X1)))", "latex": ["\\frac{\\pi\u03b3(y|x)}{Tref(y|x)}+ Blog Z(x)", "p(i > j) = \\frac{P_i}{P_i + P_j}", "p(G_1 > G_2) = \\frac{e^{r_{G_1}}}{e^{r_{G_1}} + e^{r_{G_2}}}", "p(G1 > G2) = \\sigma(\\frac{\\Pi_1}{\\Pi_2}) / ((\\Pi_1 + \\Pi_2))", "L_{IOPO} (\\pi_{\\theta}) = -E_{x_1,y_1,x_2,y_2\\sim D} {log[((:\\frac{\\pi_{\\theta}(Y_1|X_1)}{Tref (Y_1|X_1)})/(\\frac{\\pi_{\\theta}(Y_2|X_1)}{Tref (Y_2|X_1))})]+ 2\\beta log((\\frac{\\pi_{\\theta} (Y_2|X_2)}{Tref (Y_2|X_2)})/(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2))}))]-(2\\beta log(\\frac{\\pi_{\\theta} (Y_1|X_2)}{Tref (Y_1|X_2)}) / (\\frac{\\pi_{\\theta} (Y_2|X_1)}{Tref (Y_2|X_1)}))}"]}]}