{"title": "Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting", "authors": ["Yannick W\u00f6lker", "Arash Hajisafi", "Cyrus Shahabi", "Matthias Renz"], "abstract": "We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for analyzing traffic data, demonstrating its efficacy in two critical tasks: forecasting and reconstruction. Unlike typical GNN methods that treat each traffic sensor as an individual graph node, DeepStateGNN clusters sensors into higher-level graph nodes, dubbed Deep State Nodes, based on various similarity criteria, resulting in a fixed number of nodes in a Deep State graph. The term \"Deep State\" nodes is a play on words, referencing hidden networks of power that, like these nodes, secretly govern traffic independently of visible sensors. These Deep State Nodes are defined by several similarity factors, including spatial proximity (e.g., sensors located nearby in the road network), functional similarity (e.g., sensors on similar types of freeways), and behavioral similarity under specific conditions (e.g., traffic behavior during rain). This clustering approach allows for dynamic and adaptive node grouping, as sensors can belong to multiple clusters and clusters may evolve over time. Our experimental results show that DeepStateGNN offers superior scalability and faster training, while also delivering more accurate results than competitors. It effectively handles large-scale sensor networks, outperforming other methods in both traffic forecasting and reconstruction accuracy.", "sections": [{"title": "1 Introduction", "content": "Traffic flow forecasting is a critical function in spatiotemporal forecasting research, providing insights to guide infrastructure development, enhance safety, improve traffic management, and integrate multimodal transportation systems. Presently, traffic data is collected either through loop-detector sensors installed on roads or through trajectory data from vehicles, which is often transformed into traffic flow at discrete points along the road, effectively creating virtual sensors\u00b9 (Li et al. 2020). Regardless of the acquisition method, this aggregated traffic data forms time series, making traffic forecasting a multivariate time-series forecasting task.\nThis spatiotemporal forecasting task is challenging due to complex spatiotemporal dependencies across sensors and issues with long-term dependencies in the sensors themselves.\nPrevious studies have shown that graphs are effective in capturing these complex relationships (Li et al. 2018), leading to the use of graph-based approaches for traffic forecasting. These approaches model inter-sensor dependencies as a sensor graph, capturing relationships through message-passing techniques. However, there are shortcomings, three of which we highlight here.\nFirst, as the number of sensors increases to cover larger areas, the sensor graph grows so large that memory and computation requirements become infeasible. Consequently, these approaches are typically applied to datasets containing only highway sensors, excluding smaller arterial roads, which are crucial for analyzing traffic flow.\nSecond, these approaches rely on a static set of sensors and require continuous data from all sensors, making them inflexible when sensors are added, removed, or experience failures. In production environments, traffic sensors can experience outages, leading to missing data. Prominent benchmark datasets like METR-LA and PEMS-BAY (Li et al. 2018) preprocess their data to include only sensors that are consistently available. Some studies explore imputation as a method to fill these gaps, where missing sensor data is treated similarly to traffic prediction. However, these methods are brittle with respect to the sensor network and struggle with long-term changes in sensor layouts. When cities update their sensor layouts or add new streets with sensors, even imputation-based techniques require retraining and expanding the sensor graph.\nThird, most previous GNN-based approaches focus on a single type of road, typically highways, when modeling traffic. However, different road types exhibit different traffic patterns. For example, highways show strong traffic signal propagation along their length (Pan et al. 2022), resulting in strong auto-correlations. In contrast, urban arterial roads exhibit more diverse traffic patterns. Shao et al. (Shao et al. 2022) highlight the importance of modeling different road classes for effective traffic forecasting, but prior work has often neglected this due to the reliance on benchmark datasets that contain only highway data.\nTo tackle these shortcomings, our core idea is to group similar sensors for a more effective and reliable representation instead of using a traditional sensor graph with one node for each sensor. However, such an aggregated view often leads to information loss, and how to group sensors effec-"}, {"title": "2 Related Work", "content": "Traffic forecasting models often rely on data from static sensors, such as loop detectors or trajectory-based flows, which are geo-referenced and map-matched on the road network. This makes them well-suited for graph-based modeling. Early methods, such as STGCN (Yu, Yin, and Zhu 2018) and DCRNN (Li et al. 2018), used road network distances to construct static adjacency matrices. These static adjacency matrices capture street directionality but require many graph convolutional network (GCN) (Kipf and Welling 2017) steps to propagate across large network areas. To address the limitations of static adjacency matrices, (Wu et al. 2019) introduced Graph WaveNet, which employs a self-adaptive adjacency matrix optimized through stochastic gradient descent. This approach does not rely on prior road network information; instead, it uses static node embeddings, with the transition matrix formed by applying softmax to the product of these embeddings. A3T-GCN (Bai et al. 2021) leverages an attention mechanism to generate attention scores across different timestamps, while maintaining a static, binary adjacency matrix connecting direct neighbors. With the rise of graph attention mechanisms like GAT (Veli\u010dkovi\u0107 et al. 2018), attention-based techniques were incorporated into spatiotemporal traffic forecasting. D2STGNN (Shao et al. 2022) utilizes this concept by creating dynamic transition matrices with a self-attention mechanism. It leverages historical traffic data, temporal context, and static node embeddings, similar to Graph WaveNet, to produce context-dependent transition matrices. Although this method is more dynamic, it is tied to a sensor graph. Moreover, all of these methods treated sensors as graph nodes, limiting their scalability and, as a result, focusing solely on a small, curated set of freeway sensors provided by previous studies.\nBeyond traffic forecasting, only two recent studies have employed the concept of high-level graph nodes. The first, BysGNN (Hajisafi et al. 2023), predicts visitor numbers for Points of Interest (POIs) by modeling them as a graph with meta nodes representing clusters of similar POIs. The dy-"}, {"title": "3 Preliminaries", "content": "For traffic reconstruction and forecasting, we incorporated additional environmental, semantic, and positional factors as contextual data. Together, these factors create a comprehensive view of traffic dynamics.\nTraffic Observation: A traffic observation consists of a combination of traffic measurements (including speed and flow) and contextual information at a specific sensor location during a defined time window.\nDeep State Node (DSN): Traffic sensors that exhibit similar characteristics can be grouped and represented through Deep State Nodes (DSNs). In this work, we consider four DSN types: Spatial DSNs group sensors based on their geographical properties (e.g., neighborhood). Semantic DSNs cluster sensors according to road features (e.g., maximum lane speed). Environmental DSNs group sensors by similarity of weather and air quality conditions at their locations. Temporal DSNs cluster sensors based on observed traffic patterns.\nDeep State Graph (DSG): A Deep State Graph is a compact, fixed-size graph representing the traffic network. Nodes in the DSG correspond to DSNs, each maintaining a latent state based on the aggregated observations of the sensors they represent. Edges in the DSG capture the short- and long-term similarities between these latent states.\nTraffic Forecasting and Reconstruction Problems: Given a window k of traffic observations of length W, denoted as $X^{(k)} = (x_1^{(k)},...,x_W^{(k)}) \\in \\mathbb{R}^{|S^{(k)}|\\times W\\times F}$, where each $x_i^{(k)}$ represents a traffic observation with F features (traffic and contextual) for a subset of sensors $S^{(k)}$ in the road network, as well as a window of query observations $X_q^{(k)} = (x_{q1}^{(k)},...,x_{qW}^{(k)}) \\in \\mathbb{R}^{|Q^{(k)}|\\times W\\times (F-2)}$ providing only the contextual values (missing speed and traffic flow measurements) for a query set of sensors $Q^{(k)}$ that don't overlap with S, the task is to:\n(I) Traffic Reconstruction: Reconstruct the traffic measurements for the last $H \\in \\mathbb{N}^{\\leq W}$ timestamps of the input window for the query set $Q^{(k)}$, represented as $\\hat{Y}_{Reconstruct}^{(K)} = (\\hat{y}_{W-H+1},\u2026\u2026\u2026, \\hat{y}_W) \\in \\mathbb{R}^{|Q^{(k)}|\\times H\\times 2}$.\n(II) Traffic Forecasting: Predict future traffic measurements over a horizon H for the query sensors $Q^{(k)}$ represented as $\\hat{Y}_{Forecast}^{(k)} = (\\hat{y}_{W+1},..., \\hat{y}_{W+H}) \\in \\mathbb{R}^{|Q^{(k)}|\\times H\\times 2}$\nTo align with the goal of generalizing the traffic state for queries involving unknown sensors, we intentionally avoid"}, {"title": "4 Proposed Model (DeepStateGNN)", "content": "DeepStateGNN addresses the scalability and flexibility challenges of previous GNN-based traffic forecasting and reconstruction methods. The architecture, shown in Figure 1, constructs a Deep State Graph (DSG) composed of a fixed number of Deep State Nodes (DSNs) representing sensor groups and their relationships. Graph Convolution is applied to the DSG to capture inter-node relationships in the DSN embeddings. These embeddings are then used to infer traffic conditions for query locations."}, {"title": "4.1 Deep State Graph Construction", "content": "The DSG represents the latent traffic state through DSNS that aggregate traffic observations from multiple sensors in a latent space. DSN states are initialized using an MLP based on the global input context, including the time of day and day of the week in our tasks. A combination of hand-crafted and learned metrics is employed to compute non-exclusive assignments from observations to DSNs. The rationale for using different types of assignments is that for some DSNs, such as spatial nodes, the assignment of sensors is straightforward based on spatial coverage. In contrast, the assignment for other DSNs, like environmental ones, is not predetermined and requires learning from data. After the assignment, the observations are embedded and aggregated to update the DSN states. Finally, the weighted and directional edges of the DSG are inferred based on long- and short-term relationships between DSN states. These steps are detailed in the following sections."}, {"title": "4.2 Observation Assignments", "content": "Static Assignments to Spatial and Semantic DSNs Observation assignments to spatial and semantic DSNs are static, as the spatial and semantic properties of each traffic sensor remain unchanged during the observed window. For spatial DSNs, one DSN is allocated per neighborhood and freeway. Assignments are based on the distance of the traffic sensor to the neighborhood center (soft assignment) or a binary assignment indicating whether the sensor is located on the specific road the DSN represents. For semantic DSNs, nodes are allocated for each semantic property, such as the number of lanes or maximum speed (e.g., one DSN for a max speed of 40 MPH). Observations are assigned to these nodes based on binary criteria reflecting the semantic properties of the road that the sensor that recorded the observation is placed on.\nDynamic Assignments to Environmental and Temporal DSNs DSG includes environmental and temporal DSNs that group traffic observations from sensors based on environmental factors (in our case weather and air quality) and temporal patterns. Sensors recording observations under similar conditions (e.g., same precipitation level) or showing"}, {"title": "4.3 DSN State Update", "content": "Observation Embedding As described in Section 3, traffic observations X include traffic measurements over a window of time ($X_{traffic}$), static context such as road semantics ($X_{static}$), and dynamic context such as weather measurements during the same window ($X_{dynamic}$). Embedding these into a fixed-length space captures intra-series correlations efficiently. This also eliminates the need for separate graph instances for each timestamp, thereby saving memory and computation time.\nTo embed observations, we first generate the dynamic context $H_{dynamic}$ by processing the time-dependent features $X_{dynamic}$ through a GRU:\n$H_{dynamic} = GRU(X_{dynamic}) \\in \\mathbb{R}^{|S|\\times d_d}$ (2)\nWe then concatenate $H_{dynamic}$ with static features $X_{static}$ and pass it through an MLP to obtain the context embedding $H_{context}$:\n$H_{context} = MLP([H_{dynamic}; X_{static}]) \\in \\mathbb{R}^{|S|\\times d_c}$ (3)\nThe traffic sequence $X_{traffic}$ is similarly embedded using a GRU, and the result $H_{traffic}$ is combined with $H_{context}$ to produce the final observation embedding $H_{obs}$:\n$H_{traffic} = GRU(X_{traffic}) \\in \\mathbb{R}^{|S|\\times d_t}$ (4)\n$H_{obs} = MLP([H_{traffic}; H_{context}]) \\in \\mathbb{R}^{|S|\\times d_e}$ (5)\nHere, $d_d$, $d_c$, $d_t$, and $d_e$ are the output dimensions of the GRU and MLP layers.\nObservation Aggregation and State Update In this step, we aggregate observation embeddings into the DSN representations they are relevant to. For each DSN $z_i \\in Z$, let $A_i \\in \\mathbb{R}^{1\\times |S|}$ denote the i-th row of the thresholded assignment matrix A, which defines the assignment weights from sensors to this DSN. We first scale the observation embeddings $H_{obs}$ by these assignment weights:\n$H_i = A_iH_{obs} \\in \\mathbb{R}^{|S|\\times d_e}$ (6)\nThis scaling step introduces non-linearity due to the thresholding on A. Next, we aggregate the scaled observation embeddings in $H_i$ to update the state of $z_i$. After evaluating various aggregation functions (e.g., mean, max, transformers, multi-head attention), we found that the mean function offered the best balance of performance and simplicity. Thus, the update for DSN $z_i$ is:\n$\\Delta z_i = \\frac{1}{|S_i|} \\sum_{s_j \\in S_i} H_{ij}$ (7)"}, {"title": "4.4 DSG Embedding", "content": "With the updated DSN states Z and adjacency matrix L, we form the DSG G = (Z, L) for the given observation window. This graph is then processed through a modified Graph Convolutional Network (GCN) (Kipf and Welling 2017), which omits normalization and includes residual connections to preserve directed relationships and mitigate over-smoothing (Chen and et al. 2020). The output is the node embeddings $Z' \\in \\mathbb{R}^{N\\times k}$ (k is the embedding dimension).\nPost-convolution, DSN states capture latent traffic information at the sensors within their coverage. To summarize the traffic state across the road network, we apply hierarchical pooling: first, a mixed mean-max-pooling for each DSN type, followed by another pooling to generate a final vector representation $g \\in \\mathbb{R}^{2k}$ for the entire DSG.\nThe states Z, Z', and g represent the traffic state at each DSN, the enriched state considering related DSNs, and the overall state across the network, respectively. We use them for forecasting/reconstruction at query sensor locations."}, {"title": "4.5 Traffic Inference", "content": "In this step, we infer the traffic measurements for the given query set Q. A query $q \\in Q$ includes all the non-traffic observation features described in Section 3. To infer the traffic for this query, we first use the assignment functions described in Section 4.2 to find an assignment $A_q \\in \\mathbb{R}^{1\\times N}$ from this query to the DSN states. Next, we concatenate the query q, the weighted DSNs prior to convolution ($A_qZ$) for DSN-specific context, the weighted embedding of DSNs after convolution which is enriched with related DSNs' information ($A_qZ'$), and the global DSG embedding (g) for a global view of the traffic network. The global embedding captures additional information that cannot be passed through the local embeddings. This could be an event happening on a different type of road close to the query that might not be well represented solely through the local embedding. This multi-view embedding allows for a comprehensive representation to infer the traffic measurements. We then pass this representation to an MLP to perform the forecasting or reconstruction in a single shot for all the horizon timestamps:\n$\\hat{Y}_{traffic} = MLP([q; A_qZ; A_qZ';g]) \\in \\mathbb{R}^{|Q|\\times H\\times 2}$ (12)"}, {"title": "5 Experimental Evaluation", "content": "5.1 Experimental Setup\nMETR-LA+ Dataset Traditional traffic benchmark datasets, such as METR-LA and PEMS-BAY, are constrained by their focus on a limited number of freeway sensors with curated data, where all sensors have uninterrupted observations across all timestamps. To more accurately capture real-world traffic conditions, we introduce METR-LA+ \u00b2. METR-LA+ broadens sensor coverage beyond its predecessors by including both freeway and arterial road sensors, while also offering contextual sensor data. To preserve the dataset's real-world characteristics, no preprocessing was applied to address missing data, thereby retaining the natural occurrence of sensor outages. Further details can be found in Appendix A."}, {"title": "5.3 Computation Time", "content": "To assess the scalability of DeepStateGNN, we compare its training time per epoch with that of baseline models using our \"all-roads\" dataset. For this analysis, we limit the number of sensors to subsets of 200, 1000, 3000, and 4000. Each experiment is repeated three times on an identical GPU with a batch size of 32.\nFigure 2 presents the training time results. DeepStateGNN shows the fastest training times for larger sen-"}, {"title": "5.4 Ablation Study", "content": "To evaluate the contributions of different components in DeepStateGNN, we compare the full model (DSG) against four variants: removing dynamic DSNs (DSG-d), static DSNS (DSG_s), message passing (DSG-gen), and observation reconstruction loss (DSG-L2). Table 3 reports error metrics for average speed reconstruction on the \"all-roads\" scenario. The full model outperforms the ablated variants in 9 out of 12 metric comparisons, with only minor differences observed in favor of DSG_d for MAPE (0.03% lower), DSG-gen for RMSE (0.01 lower), and a tie in MAE with DSG_d. This underscores the importance of incorporating"}, {"title": "6 Conclusion", "content": "In this work, we presented DeepStateGNN, a novel GNN framework that clusters traffic sensors into high-level nodes based on contextual similarity and observed patterns, forming a fixed-size DeepState graph. This approach addresses key limitations in previous GNN-based traffic forecasting methods, particularly in terms of scalability, flexibility, and effectiveness in handling incomplete traffic observations. Our extensive experiments on the new METR-LA+ dataset demonstrated that DeepStateGNN significantly outperforms state-of-the-art baselines in both traffic forecasting and reconstruction tasks, while also offering improved computational efficiency."}]}