{"title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "authors": ["Yicheng Gao", "Gonghan Xu", "Zhe Wang", "Arman Cohan"], "abstract": "Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare or judge between different systems can lead to unreliable results due to the intrinsic win rate estimation bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation.", "sections": [{"title": "1 Introduction", "content": "Evaluating the quality of AI-generated text has been a longstanding and evolving challenge in NLP. In recent years, this challenge has become increasingly crucial due to the growing interest in the field of generative AI. While human judgment is still considered the most reliable form of assessment, common automatic approaches to evaluating quality of AI-generated text include heuristic-based evaluation metrics (Papineni et al., 2002; Lin, 2004; Pillutla et al., 2021), model-based evaluation metrics (Zhang et al., 2019; Fabbri et al., 2022; Zha et al., 2023; Chen and Eger, 2023), and recently, LLM-based evaluations (Kim et al., 2024a,b; Wang et al., 2024). Due to their relative low cost and high correlation with human preferences, LLM-based evaluations (aka LLM-as-a-judge) are receiving increasing attention. Most previous studies that apply LLM evaluators (Chiang and Lee, 2023a,b; Dubois et al., 2024; Kim et al., 2024a,b; Wang et al., 2024; Liu et al., 2024) attempt to improve the agreement between LLM evaluators and human preference by training expert models for evaluation or improving prompting strategies. However, such methods often either require compute-expensive finetuning, or suffer from common problems of LLM evaluators such as position bias (Wang et al., 2023b), self-preference, and more (Koo et al., 2023). Besides, as we will discuss in Section 3.2, directly applying a non-perfect LLM evaluator will result in a bias problem in the estimation of win rate.\nIn this paper, we attempt to address these challenges by proposing two methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene. A general illustration of our pipeline is shown in Figure 1. Our approaches leverage Bayesian inference to enhance the accuracy of win rate estimations between competing text generators using evaluation results of LLM evaluators and sparse or no human evaluation data. By employing these methods, we observe a closer alignment between LLM and human judgment in terms of win rate between two text generator models. Our results on six diverse datasets demonstrate that both BWRS and Bayesian Dawid-Skene effectively reduce win rate estimation bias of LLM evaluators, marking a promising step toward more trustworthy automatic evaluations in NLP. 1 The contribution of this paper is threefold:\n\u2022 We identify and formulate the win rate estimation bias problem associated with LLM evaluators.\n\u2022 We conduct exploratory study on mitigating this bias with Bayesian inference. Specifically,"}, {"title": "2 Related work", "content": "LLM as evaluators A line of research in LLM-based evaluation evaluated the performance of LLM evaluators and proposed methods to improve them. Some works applied various prompting techniques to improve the accuracy of LLM evaluation, including chain of thought (Liu et al., 2023a), evaluation with explanation (Chiang and Lee, 2023b), multi-LLM discussion (Chan et al., 2023; Li et al., 2023), calibration with human expert (Liu et al., 2023b) and active optimization of evaluation protocol (Xu et al., 2024). Some other works (Wang et al., 2024; Kim et al., 2024a,b) trained expert models in evaluation. As for evaluating the general capability of LLM evaluators, most previous studies (Liu et al., 2023a; Chiang and Lee, 2023a,b; Dubois et al., 2024; Liu et al., 2024; Liusie et al., 2024; Thakur et al., 2024) used correlation coefficients such as Pearson's correlation and Kendall's tau or annotator agreement coefficients such as Cohen's kappa and Scott's pi to measure the preference of different LLM evaluators compared with human evaluators.\nOn the application side, LLM evaluators are often applied to build LLM rankings. AlpacaFarm (Dubois et al., 2024) proposed a simple LLM evaluation framework by looking at the win rate decided by a strong LLM evaluator (i.e., GPT-4) on a large number of texts generated by the two generators under the same generation prompts. Auto-Arena (Zhao et al., 2024) used LLM judge agents to determine the winner of each LLM pair. However, as we'll discuss in Section 3.2, these methods can lead to biased win rate estimations, especially when the LLM evaluators do not align well enough with human preferences.\nAnnotation models In the field of crowdsourced annotations, a line of research focuses on simultaneously modeling the accuracy of individual annotators and determining the true labels of tasks. These works mostly target aggregating crowdsourced data and improving data quality in case of non-expert or adversarial annotators. Dawid-Skene (Dawid and Skene, 1979) was the first model proposed to consider individual annotator error rates by using maximum likelihood estimation to infer true labels from annotators with different accuracies. Since then, many other models (Albert and Dodd, 2004; Carpenter, 2008; Whitehill et al., 2009; Kim and Ghahramani, 2012; Hovy et al., 2013; Passonneau and Carpenter, 2014; Zhang et al., 2016) were developed to improve performance and efficiency. These methods were originally proposed to model the accuracy of human annotators, in our paper we instead apply them to model LLM evaluators.\nSome concurrent works also explored methods"}, {"title": "3 Methods", "content": "In this section, we first formalize the win rate estimation bias problem associated with directly applying LLM evaluator results, and then propose our methods to address this problem. In general, our methods attempt to derive more accurate estimators of the relative win rate between text generators by statistical calibration techniques and integrating optional human-based prior knowledge. Ultimately, we are able to improve win rate estimation accuracy without the need for costly, large-scale human annotations."}, {"title": "3.1 Problem formalization", "content": ""}, {"title": "3.1.1 True win rate and observed win rate", "content": "Consider two LLMs as text generators (LLM generators) G0 and G1. Let \u2211 be the set of all possible inputs to the text generators, and let \u03a9 be the set of all possible outputs given the inputs from \u03a3. We can then define the LLMs as two functions G0: \u03a3 \u2192 \u03a9and G1 : \u03a3 \u2192 \u03a9. Additionally, let P be a probability distribution on \u2211 that denotes the probability of each input to appear, let \u03c3 ~ P\u2211 be a random input.\nLet H : \u03a9 \u00d7 \u03a9 \u2192 {0,1} be the average human evaluator function, which assesses the relative quality of two outputs. H (y0, y1) = 0 indicates that the output y0 is preferred over y1 by an average human expert (we assume that \u201caverage human expert\" exists), and H(y0, y1) = 1 indicates the opposite. Let Te : \u03a9 \u00d7 \u03a9 \u2192 {0,1} be the LLM evaluator function, which represents the preference of a certain LLM evaluator e. Let P be a"}, {"title": "Definition 1 (True win rate).", "content": "The true win rate p is defined as:\n$p \u2252 P (H(G_0(\\sigma), G_1(\\sigma)) = 0)$ (1)"}, {"title": "Definition 2 (Observed win rate).", "content": "The observed win rate k of an LLM evaluator e is defined as:\n$k_e \u2252 P (T_e(G_0(\\sigma), G_1(\\sigma)) = 0)$ (2)\nIntuitively, the true win rate p is the probability that G0 will generate a \"truly better\" output than G1 when they are given the same, arbitrary input, where \"truly better\u201d means being regarded as \"better\" by a human expert on average. Similarly, the observed win rate k is the probability that G0 will be evaluated by an LLM evaluator as generating a better output than G1 when they are given the same, arbitrary input.\nDue to the complexity of the stochasticity in p and ke, it is unrealistic to derive them analytically. However, given a large number of input-output pairs evaluated by human and LLM evaluators, we can approximate p and ke empirically. We formalize it as follows.\nAssume n is a large number. Then for n outputs $y_i^{(0)}$(i \u2208 [n]) generated by G0 and n outputs $y_i^{(1)}$ (i \u2208 [n]) generated by G1 given the same set of n inputs of interest, we let a human evaluator h and the LLM evaluator e carry out n comparison tasks, where the i-th comparison task is between $y_i^{(0)}$ and $y_i^{(1)}$. Then the true win rate p and the observed win rate ke can be empirically approximated with\n$p=\\frac{1}{n}\\sum_{i=1}^{n}[H_h(y^{(0)}_i,y^{(1)}_i)]$ (3)\n$k_e = \\frac{1}{n}\\sum_{i=1}^{n}[T_e(y^{(0)}_i,y^{(1)}_i)]$ (4)\nwhere Hh : \u03a9\u03a7\u03a9 \u2192 {0,1} is the human evaluator function of a specific human evaluator h (or an aggregation of multiple human evaluators). Note that in our experiments, in order to make sure that p is an accurate estimator of p, we assume that the preference of h is representative of an average human expert evaluator."}, {"title": "3.1.2 Evaluator accuracy", "content": "We also define two variables $q_0^e$ (true positive evaluation accuracy) and $q_1^e$ (true negative evaluation accuracy) associated with an LLM evaluator e2. Given two arbitrary outputs generated under the same arbitrary input where the first output is evaluated as \"better\u201d than the second one by an average human expert, $q_0^e$ is defined as the conditional probability that e will give the same evaluation as an average human expert. In other words, we have\n$q^e_0 = P(T_e(G_0(\\sigma), G_1(\\sigma)) = 0 | H(G_0(\\sigma), G_1(\\sigma)) = 0)$ (5)\nwhere the random element \u03c3\u2208\u03a3 and probability measure P follow the same notions as in the definitions of p and k. Similarly, we have\n$q^e_1 = P(T_e(G_0(\\sigma), G_1(\\sigma)) = 1 | H(G_0(\\sigma), G_1(\\sigma)) = 1)$ (6)\nEmpirically, we can approximate $q_0^e$ and $q_1^e$ with\n$\\hat{q_0} = \\frac{\\sum_{i=1}^{n}1[T_e(y_i^{(0)},y_i^{(1)}) = H_h(y_i^{(0)},y_i^{(1)}) = 0]}{\\sum_{i=1}^{n}1(H_h(y_i^{(0)},y_i^{(1)})==0)}$ (7)\nwhere 1(\u00b7) is the indicator function. Similarly, we have\n$\\hat{q_1} = \\frac{\\sum_{i=1}^{n}1[T_e(y_i^{(0)},y_i^{(1)}) = H_h(y_i^{(0)},y_i^{(1)}) = 1]}{\\sum_{i=1}^{n}1(H_h(y_i^{(0)},y_i^{(1)})==1)}$ (8)"}, {"title": "3.1.3 Win rate estimation", "content": "As we discussed in Section 2, the true win rate p can be used as a metric to compare various generative LLMs. Specifically, for two generative LLMs G0 and G1, G0 outperforms G1 when p > 0.5. Conversely, G1 outperforms G0 when p < 0.5. Furthermore, the absolute value of p signifies the degree of superiority of one LLM to another. Given a list of LLMs \u0393 = [Ga, Gb, ...] of interest and a certain baseline generative LLM G, we can use the p values of G with respect to each generator in \u0393 to compare the LLMs in \u0393 (1 vs. n comparison). Therefore, it is a meaningful question to derive an accurate estimation of p. This is the essential goal of this paper."}, {"title": "3.2 Estimation by observed win rate", "content": "A simple approach employed by prior work (Dubois et al., 2024) to approximate p is to directly apply the observed win rate ke. Here we show that this approach suffers from a win rate estimation bias problem when the evaluator accuracies are not high enough.\nBy the Law of Total Probability we have\n$k_e =P (T_e(G_0(\\sigma), G_1(\\sigma)) = 0)$\n$=P(H(G_0(\\sigma), G_1(\\sigma)) = 0) \u00b7 q^e_0+\nP(H(G_0(\\sigma), G_1(\\sigma)) = 1) \u00b7 (1 \u2013 q^e_1)$\n$=pq_0 + (1 \u2212 p)(1 \u2013 q_1)$ (9)\nTherefore, using ke to approximate p will result in the following win rate estimation error:\n$|k_e - p| =|pq^e_0 + (1 \u2212 p)(1 \u2212 q^e_1) - p|$\n$=|pq^e_0 + pq^e_1 - 2p \u2212 q^e_1 + 1|$ (10)\nWe can see that ke = p only under very special conditions such as $q^e_0$ = $q^e_1$ = 1, which is typically not the case for LLM evaluators. In order to fix this win rate estimation bias problem, we propose the following two methods to improve the accuracy in the estimation of p."}, {"title": "3.3 Bayesian Win Rate Sampling", "content": "First, we propose a sampling-based algorithm, Bayesian Win Rate Sampling (BWRS), which is shown in Algorithm 1. The intuition of the BWRS algorithm is that, given an LLM evaluator e and a dataset D = {$(y_i^{(0)},y_i^{(1)}), i \\in [n]$\ncontaining outputs generated by G0 and G1 with respect to the same set of inputs, we first apply the LLM evaluator e to generate its annotations {$T_e(y_i^{(0)},y_i^{(1)}), i \\in [n]$\non D and then apply Equation 4 to approximate the observed win rate, ke. Next, assume we have access to some human annotations, either on a small fraction of D or on a similar reference dataset F, then we are able to approximate $q_0^e$ and $q_1^e$ using Equation 7 and 8. Finally, we apply the following equation rearranged from Equation 9:\n$p = \\frac{k_e + q_1^e - 1}{q_0^e+q_1^e-1}$ (11)\ngiven the assumption that $q^e_0 + q^e_1 \u2260 1$. 3 We can use the approximated values of ke, $q_0^e$, and $q_1^e$ to"}, {"title": "3.4 Bayesian Dawid-Skene model", "content": "The vanilla Dawid-Skene model (Dawid and Skene, 1979) is optimized with the Expectation-Maximization (EM) algorithm. Following Paun et al. (2018), we instead use a Bayesian Dawid-Skene model. The pseudocode of our model is shown in Model 1. The parameters in this model include \u03b1p, Bp, \u03b1qo, Bqo, \u03b1q1, and\u1e9eq1. We initialize"}, {"title": "4 Experiment Settings", "content": ""}, {"title": "4.1 Datasets", "content": "The datasets we use in the experiments are HANNA (Chhun et al., 2022), OpenMEVA-MANS (Guan et al., 2021), SummEval (Fabbri et al., 2021), LLMBar (Zeng et al., 2024), MT-Bench (Zheng et al., 2023), and LLMEval\u00b2 (Zhang et al., 2023), covering tasks of story generation (HANNA, OpenMEVA-MANS), summarization (SummEval), and instruction following (the other three). All of them provide machine-generated content with human annotations. For MT-Bench and LLMEval\u00b2, we used the smaller, curated versions prepared by the authors of the LLMBar paper (Zeng et al., 2024). For the three instruction following datasets, since they are presented as a list of (input, output1, output2, human preference) tuples without specifying which LLM generated each of the outputs, we simulate two LLM generators based on these datasets by randomly attributing 80% of the human-preferred outputs to the (simulative) generator A and the rest 20% to the (simulative) generator B such that the true win rate between them is 80%. We chose the 80%-20% ratio to represent a substantial yet realistic performance difference between two models."}, {"title": "4.2 Evaluator settings", "content": "For HANNA, OpenMEVA-MANS, and SummEval, we prompt a set of LLM evaluators to compare the outputs of generator models in the datasets. Specifically, we employ GPT-3.5-turbo-0125 (OpenAI, 2023) and Gemini-1.0-Pro (Team, 2024) as the evaluator models for our experiments. GPT-3.5 has been proved to have positive correlation with human annotations (Chiang and Lee, 2023a; Wang et al., 2023a), while Gemini-1.0-Pro's performance on LLM evaluation have not yet been widely studied in previous works. For each output pair, we prompted each LLM evaluator to rate the two outputs that are based on the same input and generated by two different generator models. For each LLM evaluator, we used three prompting strategies including Score-only, Rate-explain, and Analyze-rate following Chiang and Lee (2023b). For LLMBar, MT-Bench, LLMEval2, the LLM evaluation work has already been carried out by Zeng et al. (2024). For these three datasets, we selected the best LLM evaluators (GPT-4, PaLM 2, etc.) from the many ones used. More details regarding the specific LLM evaluator modes used for these datasets can be found in Appendix B."}, {"title": "4.3 Win rate estimation", "content": "After obtaining the human evaluation and LLM evaluation data, we apply BWRS (Section 3.3) and Bayesian Dawid-Skene (Section 3.4) to each dataset described above. We conduct \"1 vs. n\" experiment on each dataset, where we select a baseline model (GPT-2) and compare its outputs to all the other text generators in the dataset. We employ this \"1 vs. n\" comparison strategy because the corresponding \u201cn vs. n\" strategy is much more costly in terms of computation time and budget. Additionally, we calculate the observed win rate k by using Equation 4 and averaging over the results of all LLM evaluators combined. The error of estimating p with the observed win rate (i.e., |k \u2013 p|) acts as a baseline that shows the aggregated performance of all the LLM evaluators applied without any calibration.\nIn order to further study the effectiveness of each estimation method, we also explore their performance given the following three different sources of human evaluation results. For simplicity, we refer to these human evaluation results as priors, since they act as prior knowledge of human preferences in our methods.\nNo prior4. We assume no prior knowledge"}, {"title": "5 Results", "content": "In this section, we first analyze the evaluator accuracies on our datasets, and then list the results of our experiments, including win rate estimation with no prior, OOD prior, and in-distribution prior. We show that both our methods are able to effectively calibrate the estimation of win rate given good estimations of evaluator accuracies. We also show that even with no or OOD knowledge of human preference, our methods are still able to perform well overall."}, {"title": "5.1 Evaluator accuracies", "content": "For the three non-instruction following datasets (HANNA, OpenMEVA-MANS, SummEval) on which we carry out LLM evaluation by ourselves, the average accuracies of the LLM evaluators are shown in Table 1. The overall accuracy is defined as the proportion of all pair-wise output comparisons where the LLM evaluation aligns with human evaluation. We can see that:\n\u2022 In terms of overall accuracy, there is not a significant difference (>5%) between the three prompt templates.\n\u2022 There is a significant difference between $q_0^e$ and $q_1^e$ even though we applied the swap-and-sum strategy (see Appendix A). This can be attributed to the correlation between evaluator accuracy and the difference between the generators' capabilities. When one generator"}, {"title": "5.2 Win rate estimation results", "content": "The results of win rate estimation with no prior and OOD prior on HANNA, OpenMEVA-MANS, and SummEval are shown in Table 2. We can observe that:\n\u2022 The mode estimator in Bayesian Dawid-Skene with OOD prior is the overall best estimator. In this setting, estimation of p is more accurate than baseline (k) in all datasets except HANNA.\n\u2022 The Bayesian Dawid-Skene model with OOD prior is more accurate than the model with no prior. This shows that the OOD prior is able to provide some useful information on the accuracy of each evaluator, which helps the Bayesian model converge to a better result.\nThe results of win rate estimation with no prior on LLMBar, LLMEval2, and MT-Bench are shown in Table 3. Note that OOD prior is not applicable for these instruction following datasets due to the absence of relevant data to act as the OOD set. We can see that the mode estimator in Bayesian Dawid-Skene with no prior outperforms the baseline in all datasets except MT-Bench.\nThe results of BWRS and Bayesian Dawid-Skene with in-distribution prior are shown in Figure 2. We can observe the following:\n\u2022 As prior data ratio increases, win rate estimation accuracy of both BWRS and Bayesian Dawid-Skene improves. This enhancement arises because having more human annotations for in-distribution data allows for a more"}, {"title": "6 Conclusion", "content": "In this paper, we identified and formulated the win rate estimation bias problem in using LLMs as evaluators to compare text generators, where discrepancies between non-perfect LLM evaluators and human preferences could lead to errors in win rate estimation. We proposed two methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene, in order to address this issue. We then obtained LLM evaluation results on six diverse datasets, and used these results to examine the effectiveness of our methods empirically. Our results showed that both BWRS and Bayesian Dawid-Skene can effectively mitigate the LLM evaluators' win rate estimation bias, especially given good approximations on evaluator accuracies. Our results also showed that even without in-distribution prior knowledge of human preferences, our methods are still able to effectively calibrate win rate estimation under most cases. The effectiveness of our methods manifests the possibility to calibrate win rate estimation in a post-hoc manner after LLM evaluations are completed, and also enlightens future study on applying annotation models for accurate win rate estimation using LLM evaluators."}, {"title": "Limitations", "content": "There are some limitations of our work. First, due to budget limit, for the non-instruction following datasets, we only examined our methods with GPT-3.5 and Gemini-1.0-Pro as LLM evaluators. Although we did incorporate more advanced LLM evaluators such as GPT-4 and PaLM 2 on the instruction following datasets, it would be illuminating to examine how more advanced evaluator models would affect our methods' performance on the non-instruction following datasets.\nSecond, the performance of both methods with OOD prior largely depends on the quality of OOD data. Specifically, when there is a large difference between evaluator accuracies on the OOD set and on the original dataset, our methods may produce highly-biased results. Therefore, in cases where human evaluation results on datasets with similar observed win-rates are absent, we would recommend against using OOD prior.\nThis paper is an exploratory study on adjusting the win rate estimation bias of LLM evaluators. Besides resolving the limitations above, the exploration in this field could also be extended in the following aspects:\n\u2022 Applying more complex annotator models. As discussed in Section 2, the Dawid-Skene model is the earliest annotator model proposed, and several improvements have been proposed since then. These improved methods can potentially lead to more accurate win rate estimation.\n\u2022 Introducing more robust methods. The performance of our proposed methods is contingent upon the accuracy of LLM evaluators. Concretely, from Equation 11 we know that\n$0 <p<1\\Leftrightarrow \\begin{cases}  1-q_1 <k_e <q_0, q_0 + q_1 > 1\\\\ q_0 <k_e < 1-q_1, q_0 + q_1 < 1 \\end{cases}$ (14)\nWe can see that, in order to make sure p\u2208 [0, 1], the evaluator accuracies $q^e_0$ and $q^e_1$ must satisfy one of the conditions in Equation 14. In cases where neither condition is satisfied, our methods can become unstable, and is prone to produce p distributions with high bias and/or variance. We leave it for future research to propose methods that work well"}]}