{"title": "LMAC-TD: Producing Time Domain Explanations\nfor Audio Classifiers", "authors": ["Eleonora Mancini", "Francesco Paissan", "Mirco Ravanelli", "Cem Subakan"], "abstract": "Neural networks are typically black-boxes that re-\nmain opaque with regards to their decision mechanisms. Several\nworks in the literature have proposed post-hoc explanation\nmethods to alleviate this issue. This paper proposes LMAC-TD,\na post-hoc explanation method that trains a decoder to produce\nexplanations directly in the time domain. This methodology\nbuilds upon the foundation of L-MAC, Listenable Maps for\nAudio Classifiers, a method that produces faithful and listenable\nexplanations. We incorporate SepFormer, a popular transformer-\nbased time-domain source separation architecture. We show\nthrough a user study that LMAC-TD significantly improves the\naudio quality of the produced explanations while not sacrificing\nfrom faithfulness.", "sections": [{"title": "I. INTRODUCTION", "content": "Black-box neural networks obtain impressive performance\nacross various application domains, but their decision mecha-\nnisms typically remain opaque. Post-hoc explanation methods\naim to alleviate this issue by highlighting the parts of the input\nthat are deemed most influential for the network decisions\nPost-hoc interpreters do not alter the original network\narchitecture and operate post-hoc on a pre-trained model. Such\nmethods produce interpretations without constraining the net-\nwork to interpretable designs [4]\u2013[6] or training strategies [7]\nthat can eventually harm performance.\nIn recent years, significant progress has been made in\ngenerating post-hoc listenable explanations. Notable works\ninclude SLIME [8] and AudioLIME [9]. SLIME [8] segments\nthe spectrogram into time-frequency regions similar to LIME's\nsuperpixels for images [10] and evaluates each region's im-\nportance. AudioLIME [9] extracts sources from the audio\nand assigns a saliency score to each. L2I [11] introduced a\ndecoder to selectively activate an NMF dictionary based on\nthe relevance of its components to the predicted class. L-MAC\n[12] later simplified L2I's architecture by introducing a custom\nloss function to maximize the interpretation's faithfulness\nwhile removing the need for a pre-trained NMF dictionary. In\nLMAC-ZS [13], the authors adapt the loss function to work\nwith zero-shot classifiers. L-MAC generates more faithful and\nhigher-quality explanations than its counterparts. However,\nit operates in the magnitude-STFT domain with a simple\nseparation architecture that uses the input mixture's phase to\nreconstruct the waveform, thus causing artifacts.\nAlso recently, deep learning-based source separation has\nachieved remarkable performance, reaching Signal-to-Noise\nRatios above 20dB on benchmarks like WSJ0-2Mix [14] and\nLibri2Mix [15]. Among the state-of-the-art models is Sep-\nFormer [16]. In this paper, we propose enhancing the quality\nof L-MAC's explanations by incorporating the SepFormer's\nMaskNet in the interpreter. That is, we generate explanations\ndirectly in the time domain, bypassing the need to use the\ninput signal's phase as required by L-MAC's magnitude-STFT\napproach.\nOur experimental results show that the proposed LMAC-\nTD, the time-domain version of the L-MAC framework,\nachieves comparable results in terms of the faithfulness metrics\nestablished in the original L-MAC paper [12] while delivering\nimproved audio quality as demonstrated by our user study."}, {"title": "II. METHODOLOGY", "content": "We describe the pipeline of LMAC-TD in Figure 1. The goal\nof this model is to produce a time-domain interpretation signal\ni(t) for a time-domain input signal x(t) that goes through the\nclassifier f(\u00b7), composed of an embedding model Emb(\u00b7) and\na classification head (OutHead in the diagram). We enhance\nthe L-MAC architecture by adding the SepFormer's MaskNet,\nEncoder, and Decoder on top of the UNet Decoder.\nFirst of all, the classifier representations are obtained by\npassing the input signal x \u2208 RT first through an input trans-\nformation InputTf(\u00b7) (e.g. STFT, Mel Spectrogram extraction),\nand then through the embedding model of the pre-trained\nclassifier, Emb(\u00b7). We obtain a set of classifier representations\nsuch that,\n$$h = Emb(InputTf(x)).$$\nFor our experiments, InputTf(\u00b7) represents the Mel-\nspectrogram computation, while Emb(\u00b7) refers to a CNN14\nencoder [17]. For h, we use the last 4 representations of\nthe CNN14 encoder. The representations h are then fed into\na UNet decoder U(\u00b7), which is functionally similar to the\ndecoder architecture in the L-MAC paper [12], but with\nadjusted strides and kernel sizes. We obtain the representation\nHd \u2208 RK\u00d7T' such that,\n$$Hd = U(h).$$\nIn order to take into account contributions from both classifier\nrepresentations and the encoded input audio directly, we\ncombine Ha with the output of the SepFormer encoder [18]\nHe \u2208 RKXT'. The convex combination of He and Ha is\nthen fed into the SepFormer MaskNet to obtain the mask\nM\u2208RK\u00d7T' such that,\n$$M = MaskNet(\u03b1Hd + (1 \u2212 \u03b1)He),$$\nwith \u03b1 \u2208 [0, 1]. We observe that calculating this combination\nthrough alpha allows the model to reach better faithfulness-\naudio quality trade-off. Finally, the mask M is element wise\nmultiplied with the SepFormer Encoder output He, and fed into\nSepFormer Decoder to obtain the time domain interpretation\ni\u2208 RT such that,\n$$i = SepFormerDecoder(MH_e).$$\nThe loss function is then calculated by comparing the classi-\nfication results for the input signal x, the interpretation i, and\nthe mask out version of the interpretation signal iout, which is\ndefined as,\n$$i_{out} = SepFormerDecoder((1-M) H_e).$$\nAll in all, the training loss function is calculated following the\nL-MAC approach,\n$$min_\u03b8d(f (InputTf(x))||f (InputTf(i)))\n\u2013 \u03bb_{out}d(f (InputTf(x))||f (InputTf(i_{out}))) + \u03bb_{reg}R(i),$$\nwhere d(\u00b7||\u00b7) is a divergence measure which measures the\ndiscrepancy between the classifier output f(InputTf(x)), and\nthe classification of the interpretation f(InputTf(i)). In our\nexperiments we use Cross-Entropy as d(\u00b7||\u00b7). The second term\ntries to make sure that the masked-out interpretation signal\niout minimizes its similarity to the input signal x in terms\nof the classifier output. The coefficients \u03bbin and \u03bbout are\nused to control the relative strengths of the aforementioned\nmask-in and mask-out loss terms. Finally, the last term R(i)\nis a regularization term that avoids trivial solutions. In our\nexperiments, we applied an l\u00b9 penalty to the magnitude-STFT\nrepresentation derived from the interpretation signal such that,\n$$R(i) = ||STFT(i)||_1.$$"}, {"title": "III. EXPERIMENTS", "content": "In Sec. III-A we detail the experimental setup used to\nvalidate LMAC-TD. We performed a quantitative analysis\nto establish the faithfulness of LMAC-TD-generated expla-\nnations, which we describe in Sec. III-B. Additionally, we\nconducted a user study to evaluate the user preference and\npresent the results in Sec. III-\u0421.\nMetrics. We use several quantitative metrics to compare\nLMAC-TD with state-of-the-art approaches. Specifically, we\nadopt the evaluation metrics employed in the original L-MAC\npaper [12]. We use Average Increase (AI), which measures\nthe increase in the classifier's confidence for the interpretation\nwith respect to the input sample, and Average Decrease (AD),\nwhich measures the confidence drop when removing the\ninterpretation from the input (i.e. feeding iout), and Average\nGain (AG), similar to AI. Beyond these metrics, we use the\nFaithfulness (FF) metric defined in the L2I paper [11] and\nthe input fidelity (Fid-In) metric defined in the PIQ paper\n[19]. We also use the Sparseness (SPS) [20] and Complexity\n(COMP) [21] metrics to evaluate the conciseness of the\nexplanations. Due to space constraints, we invite the reader\nto refer to the respective papers for an in-depth presentation\nof the metrics.\nImplementation. For all experiments, we use a CNN14\nclassifier [17] pre-trained on the VGGSound dataset [22].\nThis classifier is then fine-tuned on the ESC50 dataset [23]\nwith WHAM! noise [24] augmentation to mimic real-world\nconditions. It is trained on folds 1, 2, and 3, achieving 75%\nand 78% accuracy on folds 5 and 4, respectively. LMAC-TD\nis later trained on ESC50 with WHAM! noise augmentation.\nWe refer to this data configuration when evaluating the\nexplanations in the In-Domain setting. The UNet decoder\nemploys a series of transposed convolution layers to align the\nlast four latent representations with the encoder output He\nshape. We benchmarked LMAC-TD with different \u03b1 values to\nassess their impact. The remaining hyper-parameters are set as\nfollows: \u03bbin = 5, \u03bbout = 0.2, and \u03bbreg = 6. We selected these\nhyperparameter combination based on our own qualitative\nevaluation of the produced audio quality, taking into account\nthe faithfulness of the explanations. Our implementation is\nbased on the L-MAC implementation present in SpeechBrain\n1.0 [25] and is available through our companion website\u00b9.\nEvaluation Data. We conduct experiments to evaluate\nexplanation faithfulness on In-Domain (ID) and Out-of-\nDomain (OOD) data. For the OOD setting, following\nL-MAC's procedure, we create mixtures of ESC-50 samples\nwith the dominant class at 5 dB SNR. We generate mixtures\nfrom folds 4 and 5. We also test LMAC-TD on audio\ncontaminated with white noise and speech, using 3dB SNR\nmixtures and samples from the LJSpeech dataset [26].\nA very critical aspect of evaluating LMAC-TD explana-\ntions is establishing that they faithfully follow the classifier,\nmeaning they effectively highlight portions of the input rele-\nvant for the classifier. We compare LMAC-TD with several\nother saliency-based methods in the literature. Namely, we\ncompared with gradient-based methods such as saliency maps\n[27], GradCAM [28], SmoothGrad [2], IntegratedGradients\n[29], GuidedBackProp [30], the decoder-based audio specific\nexplanation method Listen-to-Interpret (L2I) [11], and we also\ninclude SHAP [31].\nIn Table I, we report the quantitative results obtained for the\nID experiments with three different LMAC-TD configurations.\nWe observe that, with \u03b1 = 0.75, LMAC-TD outperforms\nstandard L-MAC in all faithfulness metrics except AD, which\nremains comparable. We note that we have calculated the\nCOMP metric (a complexity metric) on the magnitude-STFT\nrepresentations of the explanation signal i which is produced\nin the time domain. This tends to give large COMP metrics\nfor LMAC-TD implying that LMAC-TD explanations are\ncomplex (high entropy - uniformly distributed in the STFT\ndomain). However, LMAC-TD explanations remain under-\nstandable, and the explanations are still preferable for the users\nover the alternatives as we show in Section III-C with a user\nstudy.\nWe note that the results show that lowering \u03b1 reduces the\nfaithfulness metrics. This makes sense since larger \u03b1 more\nstrongly incorporates the classifier representations, increasing\nthe classifier influence on the explanations.\nIn Table II, we compare the results obtained on audio\nmixtures created with ESC50 data. Unlike the ID case, LMAC-\nTD with \u03b1 = 0.75 outperforms both L-MAC and STFT-\nmasking approaches in AD, FF, and Fid-In. With regards to\nthe effect of \u03b1, we observe that in general the faithful metrics\nare better as expected with larger \u03b1, however in terms of FF\n\u03b1 = 0 remains comparable. These results suggest that the\nexplanations generated with LMAC-TD are well-aligned with\nthe classifier in this OOD setting as well.\nFinally, Table III presents the results on the ESC50 dataset\nwith white noise and LJSpeech contamination\u00b2 to explore\nmore OOD settings. In these settings, LMAC-TD achieves\nthe best overall faithfulness scores with \u03b1 = 0.75. Compared\nto standard L-MAC we observe a drop in terms of the\nperformance in some metrics such as AI, AD AG, however\nin terms of FF and Fid-In LMAC-TD achieves better scores.\nWe also would like to note that, similar to the ID setting, in all\nOOD settings larger \u03b1 typically results in better faithfulness\nscores.\nTo determine the perceived quality of the generated inter-\npretations we conducted a user study with 19 participants\nusing the WebMushra interface [32], following the evaluation\npipeline used for L-MAC [12]. We presented nine audio\nsamples to the users and we asked the users to assign a\nscore (1 to 100) based on (i) how well the interpretation\ncorresponds to the part of the input audio associated with the\npredicted class and (ii) the perceived audio quality. We present\nthe outcome of the user study in Fig. 2 with the confidence\nintervals at 0.95. LMAC-TD outperforms the other methods\nwhen \u03b1 \u2208 {1,0.75}. With \u03b1 = 0, LMAC-TD obtains a Mean-\nOpinion Score (MOS) comparable to the original L-MAC,\nwhile still outperforming L2I.\nWe also observe that LMAC-TD is preferred over LMAC-FT (the fine tuned version of LMAC [12]). We note that\nLMAC-TD simplifies the pipeline by removing the need for\ninterpreter finetuning, while improving perceived audio quality\nover LMAC-FT. The audio samples from the user study are\naccessible through our companion website\u00b9 together with the\nper-sample MOS.\nIn summary, faithfulness and qualitative evaluations demon-\nstrate that LMAC-TD with \u03b1 = 1 and \u03b1 = 0.75 both offer\nbetter subjective audio quality compared to the baselines,\nwhile not compromising significantly from faithfulness."}, {"title": "IV. CONCLUSIONS", "content": "This paper presents a novel approach, called LMAC-TD, to\ngenerate listenable explanations directly in the time domain.\nLMAC-TD builds on top of recent literature in interpretability\nand source separation to enhance the audio quality of the\nexplanation. We propose to include a time-domain source-\nseparation architecture to decode listenable explanations from\nthe classifier representations. Through an extensive experi-\nmental analysis, we conclude that LMAC-TD outperforms the\nother methods in terms of perceived audio quality and achieves\nbetter or comparable results in faithfulness metrics."}]}