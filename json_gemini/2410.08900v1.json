{"title": "A Benchmark for Cross-Domain Argumentative Stance Classification on Social Media", "authors": ["Jiaqing Yuan", "Ruijie Xi", "Munindar P. Singh"], "abstract": "Argumentative stance classification plays a key role in identifying authors' viewpoints on specific topics. However, generating diverse pairs of argumentative sentences across various domains is challenging. Existing benchmarks often come from a single domain or focus on a limited set of topics. Additionally, manual annotation for accurate labeling is time-consuming and labor-intensive. To address these challenges, we propose leveraging platform rules, readily available expert-curated content, and large language models to bypass the need for human annotation. Our approach produces a multidomain benchmark comprising 4,498 topical claims and 30,961 arguments from three sources, spanning 21 domains. We benchmark the dataset in fully supervised, zero-shot, and few-shot settings, shedding light on the strengths and limitations of different methodologies. We release the dataset and code in this study at hidden for anonymity.", "sections": [{"title": "1 Introduction", "content": "Argumentation is a pervasive human activity present in various aspects of everyday life, which involves expressing viewpoints backed by reasons or attempting to persuade others towards a specific perspective (Sobhani, Inkpen, and Matwin 2015). In the context of argument mining, a crucial task is argumentative stance classification (K\u00fc\u00e7\u00fck and Can 2020), where the goal is to classify an argument's stance as either favor, against, or neutral regarding a given claim. For example, argument The possession of nuclear weapons provides countries with a strong defense mechanism, deterring potential adversaries from launching attacks can be classified as against the claim All countries should give up their nuclear weapons.\nIn recent years, social media platforms have become effective channels for sharing information, encouraging diverse perspectives, and facilitating the exchange of ideas (ALDayel and Magdy 2021). Claims are often simplified into noun phrase topics, such as \"nuclear weapon\" for the above example. Previous research has spent a lot of effort in constructing datasets concerning various topics. For example, Mohammad et al. (2016) constructed a dataset with tweets commenting on Atheism, Climate change, Feminist, Hillary Clinton, Abortion, and Donald Trump. Conforti et al.\nOne challenge to stance classification comes from the variety of stance topics (Allaway and McKeown 2020). Many prior benchmarks are not topically diverse. As mentioned above, they typically feature a handful of topics, each with a corpus of comments to facilitate the training of supervised models dedicated to that topic (Stab et al. 2018). The acquisitions of stance labels rely on human annotators for ground truth (K\u00fc\u00e7\u00fck and Can 2020), which is time-consuming and difficult to scale up. Besides, most previous benchmarks focus on a single genre or source.\nAccordingly, our objective is to construct a diverse and multisource stance classification benchmark without human annotation. Allaway and McKeown (2020) categorize stance classification into two categories based on the topic: topic-phrase and topic-position. For the former, the topic is typically a noun phrase (including proper noun), such as nuclear weapon. For the latter, the topic is a complete position claim such as All countries should give up their nuclear weapons. Notably, the argument we introduce at the beginning of this section would be classified as favor for the former and against for the latter. This reveals a significant difference between topic-phrase and topic-position stance classification: the latter is context-dependent. Our benchmark focuses on topic-position, as we argue that topic-phrase can be easily converted into topic-position by constructing a positional claim, which has a more general form. Preferably, a truly intelligent stance classification system should be able to grasp the meaning of the topical claim and reverse its prediction when the claim reverses itself.\nWe construct our benchmark from three distinctive sources: a social media platform, two debate websites, and arguments generated by large language models (LLMs). For the social media platform, we leverage conversations from a subreddit called ChangeMyView\u00b9 from Reddit, where a poster challenges other users to change the poster's opinion expressed by a positional title. The comments labeled by the"}, {"title": "Findings", "content": "In our analysis of traditional machine learning techniques, including Support Vector Machines (SVM), Convolutional Neural Networks (CNN), and Bidirectional Long Short-Term Memory (BiLSTM) networks, we observe that incorporating LLMs generated data into the training process significantly enhances in-domain performance for these models. However, this strategy yields inconsistent outcomes when applied to finetuning contemporary LLMs. We also observe that generative models consistently outperform classification models with supervised finetuning. LLMs display commendable performance in zero-shot settings for cross-domain evaluation, though a substantial performance gap remains in comparison to in-domain supervised finetuning. Furthermore, in few-shot experiments, instruction-tuned LLMs consistently outperform their non-instruction-tuned counterparts, highlighting the effectiveness of instruction-tuning as a robust approach for adapting LLMs to downstream tasks."}, {"title": "Contributions", "content": "Our contributions are twofolds:\n\u2022 We propose a scalable and extensible framework to construct a diverse and multisource benchmark for argumentative stance classification without involving human annotation.\n\u2022 We implement and evaluate fully-supervised learning, zero-shot learning, and few-shot learning using LLMs. This thorough assessment facilitates a comparative analysis of various methodologies, emphasizing the efficacy of instruction-tuning for optimizing the performance of LLMs."}, {"title": "2 Related Work", "content": "Stance is a speaker's evaluation of a proposition or topic. The proposition may be implicit as a topic-phrase (a noun phrase) or explicit as a topic-position (a positional claim) (Allaway and McKeown 2020). Datasets from early research originate from arguments in online debate forums (Somasundaran and Wiebe 2010; Murakami and Raymond 2010; Walker et al. 2012; Hasan and Ng 2014) and mostly fall under the topic-phrase category (ALDayel and Magdy 2021). More recent datasets cover various topics (Sobhani, Inkpen, and Zhu 2017; Qazvinian et al. 2011; Mohammad et al. 2016; Conforti et al. 2020b; Li et al. 2021; Glandt et al. 2021). For topic-position stance classification, datasets primarily come from news articles, where headlines are used as the topic-phrase (Ferreira and Vlachos 2016; Habernal et al. 2018; Conforti et al. 2020a; Chen et al. 2019; Qazvinian et al. 2011). Many existing datasets are generated from one source in one domain and focus on comments for a small set of topics, followed by human annotation.\nWe emphasize the topic-position variant of stance classification because phrases can be transformed into positions by formulating an affirmative claim (e.g., Abortion maps to Abortion should be legalized). Unlike previous works, which rely on human annotators for labeling, we leverage platform rules, readily available expert-curated content, and large language models to acquire faithful stance labels."}, {"title": "3 Benchmark Construction", "content": "We now describe the details for building our benchmark, which includes 4,498 topical claims, and 30,961 arguments, covering 21 domains and three distinct sources."}, {"title": "3.1 Dataset Collection", "content": "In order to enhance textual diversity, our benchmark is curated with content from three types of sources: a social media platform, debate websites, and arguments generated by a large language model.\nSocial media platform ChangeMyView (CMV) is a subreddit from Reddit that serves as a dedicated forum for fostering discourse. Within this subreddit, participants actively contribute their opinions and engage in discussions with the explicit aim of defending their perspectives. A typical CMV post adheres to a particular structure: it begins with the abbreviation \u201cCMV: \" signifying Change My View, followed by a concise representation of the author's viewpoint. Subsequently, the body of the post features a comprehensive elucidation by the author, providing an in-depth account of the reasoning and rationale supporting their stance. If any of the comments made by the participants successfully manage to influence a shift in the author's viewpoint, the author acknowledges this change by awarding the commenter with a delta. Therefore, we extract the title, body, and delta-awarded comments, where the title corresponds to the designated topical claim, the body constitutes a supportive argument, and the comments bearing a delta reward act as counterarguments.\nWe leverage two existing CMV datasets (Tan et al. 2016; Al-Khatib et al. 2020) due to Reddit's recent limits on the API (at most the most recent 1000 posts are returned by an API call). To keep the text meaningful and concise, we select bodies and comments with a length between 20 to 200 words.\nDebate websites We selected two online platforms dedicated to debates between users, idebate.net and debatewise. org, due to their well-structured presentation of arguments for and against a position. These platforms provide clear and comprehensive arguments, thus obviating the need for annotation. We captured the subjects of a debate along with the associated arguments.\nSome of the arguments were excessively verbose. However, we observed that the stance of an argument can typically be discerned within the initial few sentences. Therefore, in the interest of improving the manageability of the data, we retained only the initial five sentences of each argument. This choice aligns with our restricting the CMV arguments to 20 to 200 words."}, {"title": "3.2 Large Language Model Generated Argument", "content": "Generating arguments using GPT-3 We use GPT-3 to generate text because it produces coherent, contextually relevant, and fluent language. GPT models are pretrained on vast amounts of diverse data, enabling them to understand and mimic human-like language patterns across various topics. Previous research has demonstrated GPT's ability to encode beliefs into argumentative texts (Alshomary et al."}, {"title": "Prompt 1", "content": "Given a question or topic, generate a controversial claim.\nInput: Should Halloween costumes be allowed in schools?\nOutput:"}, {"title": "Prompt 2", "content": "Given a topic, write three distinct supporting arguments and three opposing arguments. You should write in 1st person view rather than 3rd person view. Don't explicitly say I support or oppose. Don't summarize the points at the beginning.\nTopic: Halloween costumes should be allowed in schools."}, {"title": "Prompt 3", "content": "Given a topic, classify which domain the topic falls into. Output the domain directly without other words. Some example domains are sport, environment, civics, history, education, politics, technology, literature, arts and music, science, ethics and animal, finance and business, global affairs, health, psychology, law and justice, relationship, nursing, religion, food and nutrition. You should pick the category that most closely matches the topic. If none of the categories matches, you can use a new category of your own.\nTopic: Halloween costumes should be allowed in schools.\nOutput:"}, {"title": "Constructing neutral arguments", "content": "The above methods assign stance labels of favor or against, as in some prior datasets. Generating a neutral stance, however, is difficult since a judgment of neutrality often depends on each annotator's interpretation. We seek to compel the LLM to focus on how the argument and the topical claim relate, moving beyond reliance on surface-level linguistic cues.\nFor each claim, We create neutral arguments by randomly selecting arguments for other claims. Consequently, we define a neutral stance as one that includes either irrelevant arguments or instances where no discernible stance can be inferred. One way is to randomly sample arguments. However, it may yield instances with distinct semantics that are easily captured by the model, that is, the sampled arguments address completely different topic from the claim, which leads to easy samples. To improve this, we use BERT to embed all claims and arguments. For each claim, we randomly sample three arguments falling within the similarity score range of [0.3, 0.5]. This criterion is motivated by the fact that highly similar arguments may include content that may convey an implied stance. Conversely, moderately similar arguments may seem to discuss related subjects but be subtly different, thereby forming more challenging examples. For example, the third claim in Table 1 concerns student loan debt, but the comment is about moral education: thus it doesn't indicate a stance about the claim though they are both education."}, {"title": "3.3 Dataset Characteristics", "content": "We use \"LLMG\" to refer to the dataset generated by GPT-3. Our dataset exhibits greater diversity than prior datasets. We apply GPT-3 to classify them into predefined categories as shown by Prompt 3. Furthermore, we allow GPT-3 to generate novel domains. This process yields over 100 domains across all topics. We consolidate these into 21 principal domains."}, {"title": "3.4 Evaluate LLM Generated Dataset", "content": "To evaluate our LLM-generated dataset's (LLMG) quality, we (1) manually verify that GPT-3 adheres to the guidelines and produces accurate responses and (2) compare the lexical diversity of real-world datasets and GPT-3 generated content.\nHuman verification First, we applied regular expressions to search for phrases like as an AI, I cannot and its variations, such as as an AI, I can't, in LLMG and found no such occurrences. Second, three independent raters labeled 200 randomly selected arguments from LLMG.\nWe designed a survey to assess two key aspects: (1) AI acceptance\u2014whether GPT-3 refused to respond, and (2) AI accuracy\u2014whether the response contained three supporting and three opposing arguments in the desired order. This survey included two evaluation questions:"}, {"title": "AI acceptance:", "content": "Does the sentence indicate that the AI refused to provide a response? For instance, does it contain variations of as an AI, I cannot?"}, {"title": "AI accuracy:", "content": "Does the response adhere to Prompt 2, specifically including three supporting and three opposing arguments in the correct order?"}, {"title": "Lexical and semantic diversity", "content": "Figure 2a illustrates that sentences in LLMG exhibit greater lexical diversity than those in the CMV and Debate datasets. Lexical diversity is quantified using the metric called distinct-2-the number of unique bigrams and normalizing by the total number of words generated\u2014which is a popular metric for lexical diversity (Li et al. 2016; Park, Yang, and Park 2019)."}, {"title": "4 Fully Supervised Finetuning", "content": "We now address RQ1 and RQ2 by conducting experiments with both traditional machine learning models and LLMs."}, {"title": "4.1 Traditional Machine Learning Models", "content": "We conduct experiments using widely adopted stance classification methods. AlDayel and Magdy (2021) identify SVM, CNN, and BiLSTM as leading machine learning methods for stance classification. Therefore, we conduct experiments to evaluate the effectiveness of LLMG as a weakly supervised approach for real-world datasets. We adopt Word2Vec embeddings-the well known embedding approach in the literature (AlDayel and Magdy 2021). For BiLSTM and CNN, we fine-tune the models with a learning rate of 2e-4, AdamW optimizer, 0.5 dropout, and CrossEntropy loss."}, {"title": "4.2 Large Language Models", "content": "We term the previous generation of language models, such as BERT (Devlin et al. 2019), SLMs to contrast with current LLMs. A prevalent method for classification using SLMs involves finetuning, which entails exposing a pretrained SLM to domain-specific data. However, finetuning is not always the optimal method for customizing LLMs and some research have suggested it could be detrimental to performance. Moreover, whereas finetuning is tractable for SLMs, it demands substantial computational resources for LLMs. Therefore, wepare SLMs and LLMs for supervised finetuning for stance classification. For finetuning BERT, we concatenate the topic and argument with the special token [SEP] and prepend the sequence with the special token [CLS] to form the template [CLS] + Topic + [SEP] + Argument. A three way classification head is added on top of the token [CLS] to perform the classification task.\nFor finetuning the generative models T5 and LLaMa, We use the same template as in the Training Prompt (below). We show a few concrete examples in Appendix C. As for autoregressive pretraining, we apply the maximum likelihood estimation, which involves minimizing the cross-entropy loss between the predicted probability distribution of the next token and the actual token for the whole sequence. At inference time, we simply remove the gold label from the prompt so that the model can make a prediction. The output length is limited to 2."}, {"title": "Training Prompt for Generative Models", "content": "Classify the stance of the argument towards the topic as either favor, against, or neutral. Return the label only without any other text.\nTopic: {topic}\nArgument: {argument}\nLabel: {label}"}, {"title": "4.3 Experimental Setup", "content": "Our evaluation involves (1) BERT (Devlin et al. 2019), recognized for its effectiveness in classification, (2) T5 (Raffel et al. 2020), a generative counterpart to BERT, and (3) We conduct the following experiments.\nFinetuning with a single dataset To evaluate generalizability in stance classification, we assess how a model trained on one dataset performs on another dataset."}, {"title": "Finetuning with multiple datasets", "content": "We extend the above evaluation to include finetuning on combined datasets."}, {"title": "Finetuning with varied sizes of training data", "content": "We evaluate the effect of data size (from combined data) on finetuningo For all datasets, we adopt the macro-F1 metric, namely, the average F1 score for each label category (Favor, Against, None). For BERT (110M) and T5 (250M), we perform finetuning with all parameters. For LLaMa-7b, we apply the QLORA (Dettmers et al. 2023) quantization technique, updating only 20 million parameters. We split the data into train, dev, and test sets."}, {"title": "4.4 Results", "content": "We now present the results of our experimental investigations. These results reveal a persistent challenge across all models: a difficulty in adapting to new datasets when subjected to finetuning with one dataset, indicating the subtle differences between domains. Notably, the best average performance is achieved by LLaMa-7b fine-tuned on the Debate dataset.\nFor finetuning with multiple datasets, LLaMa-7b is the model with the highest average F1 across all three datasets. Despite having fewer fine-tuned parameters (20M compared to 110M and 250M), LLaMa-7b outperforms its counterparts, reflecting the power of LLMs in complex tasks. Both T5 and LLaMa-7 beat BERT, highlighting the advantage of using generative models over classification-oriented models for stance classification."}, {"title": "Ablation studies", "content": "Figure 3 presents the results for different amounts of training data. The three models demonstrate comparable and high training sample efficiency. Notably, with approximately 25% of the training data, each model achieves nearly 95% of its optimal performance."}, {"title": "5 Zero-Shot and Few-Shot Benchmarking", "content": "The zero-shot and cross-topic variants of stance classification are well-aligned since both rely on performing on topics not encountered during training. To address RQ3, we evaluate strict zero-shot and few-shot learning. Research suggests that the knowledge that LLMs possess is predominantly acquired through pretraining (Cruickshank and Ng 2023). This implies that LLMs possess the inherent capacity to address various tasks, provided they are instructed in a suitable manner."}, {"title": "5.1 Experimental Setup", "content": "We focus on open LLMs as opposed to closed, commercially oriented LLMs (Touvron et al. 2023), to enhance accessibility. LLMs exhibit a variety ofarchitectures and sizes, and whether they underwent instruction tuning during their training process. We employ LLaMa as the cornerstone of our study, because of its demonstrated superiority across multiple tasks and performance that is competitive with ChatGPT. We consider the 7B, 13B, 33B, 65B configurations of LLaMa, as well as the 7B, 13B, 33B configurations of its instruction-tuned counterpart, Vicuna (Chiang et al. 2023). We also include another model family, UL2 (Tay et al. 2023), and its instruction-tuned counterpart FLAN-UL2, which has an encoder-decoder architecture. We also include the 7B and 40B configurations of the Falcon family, with and without instruction tuning.\nWe conduct experiments with zero-shot and few-shot in-context learning. For all the LLMs, we use QLORA to quantize them to 4 bits to reduce the need for GPU memory. QLoRA suffers minimal loss on a variety of tasks (Dettmers et al. 2023). Our experiments are run on a mixture of NVIDIA-A100, NVIDIA-A30, NVIDIA-A10, and"}, {"title": "5.2 Results", "content": "The overall results are shown in Table 8. The main findings are summarized as follows.\nSignificance test for model performance Before starting our analysis, we performed McNemar's test to assess the significance of model prediction differences among the top-performing models. This test was conducted in two stages. First, we performed an inter-model comparison, evaluating the top model from each family, namely LLaMA-65B, Vicuna-33B, Falcon-40B-instruct, and FLAN-UL2-20B. Second, we examined intra-model differences by comparing zero-shot and 9-shot performances within these four models. The corresponding p-values are presented in Table 9. As observed, the differences across both settings are statistically significant (p<0.001) across three datasets, with the exception of LLaMA-65B vs. Vicuna-33B on the CMV and LLMG datasets, and LLaMA-65B vs. FLAN-UL2-20B on the CMV dataset. Notably, all models exhibited significant differences between the zero-shot and 9-shot conditions, highlighting the critical role of few-shot examples in performance improvement.\nGap with upper bound Overall, we observe positive effects for model scaling. For all model families, larger models yield better performances across most settings. However, the best performance model of FLAN-UL2, which achieves 41.17, 51.51, and 50.21 under zero shot for CMV, Debate, and LLMG, respectively, falls far behind the supervised approach, which suggests difficulty for LLMs to comprehend downstream tasks.\nNumber of few-shot exemplars Exposing a model to more examples reliably improves performance across various tasks. However, our results are mixed. To understand variability, we randomly sampled 10 sets of examples for both 3-shot and 6-shot learning and calculated their mean and standard deviation. Some sets of examples show better performance than zero-shot. Nonetheless, the variability highlights the sensitivity of LLMs to specific examples. One exception is FLAN-UL2, the top performer, which maintains an average variance of 0.54, showcasing the consistency of its performance. Additionally, FLAN-UL2 demonstrates a robust improvement due to the increase in example input.\nImpact of instruction tuning Instruction-tuning is an approach to continually fine-tune an LLM by exposing it to diverse instructions and their responses. Doing so enhances the ability to follow user-specified instructions. Our results demonstrate that models fine-tuned through instruction reliably outperform models of the same architecture and size that are not instruction-tuned. This is apparent by comparing Vicuna to LLaMa, Falcon-instruct to Falcon, and FLAN-UL2 to UL2. This observation highlights the effectiveness of instruction-tuning as a task-agnostic method for adapting LLMs to downstream tasks."}, {"title": "6 Conclusion", "content": "In this paper, we compile our benchmark from three distinct sources: a social media platform, two debate websites, and arguments generated by large language models (LLMs). The resulting dataset covers a wide range of 4,498 distinct topics, comprising 30,961 arguments distributed across 21 diverse domains.\nWe employ three distinctive experimental approaches: fully supervised learning, zero-shot learning, and few-shot in-context learning with LLMs to demonstrate the usefulness of our dataset. Notably, our findings highlight the superior performance of generative models over classification modelss, when used in a zero-shot scenario, demonstrate commendable performance, though with a noticeable performance gap relative to the upper bound. Furthermore, tuned LLMs reliably outperform their non-instruction-tuned counterparts, emphasizing the effectiveness of instruction-tuning for adapting LLMs to downstream tasks.\nIn conclusion, our study establishes robust baselines for the created dataset and provides valuable insights that can guide the development of more generalized stance classification methods. This research not only advances our understanding of the performance dynamics among different learning approaches but also offers practical implications for optimizing the use of LLMs in stance classification"}, {"title": "7 Discussion", "content": "We investigate the use of factual information in argument formulation in our benchmark and the limitation of our work."}, {"title": "7.1 Factual Information in Arguments", "content": "The use of facts to build arguments improves credibility and makes them more persuasive by offering solid evidence to back up claims. Without facts, arguments often depend on"}, {"title": "Facts Extraction Prompt", "content": "Facts are objective statements that are verifiable.\nArguments are subjective claims or positions.\nGiven a topic and an argument, identify if the argument relies on any verifiable facts. Return the fact that the argument relies on. Return none if the argument does not rely on verifiable facts. Be concise in your response.\nTopic: {topic}\nArgument: {argument}"}, {"title": "7.2 Broader Impact", "content": "The stance classification task focuses on identifying the viewpoint or position of an argument regarding specific topics, rather than evaluating the factual correctness of the information in the argument. While factual accuracy may not be the main concern of this task, it is important to note that text generated by LLMs or online users can sometimes include inaccurate or false information. This concern is particularly significant when our dataset is employed for purposes such as argument generation or augmentation, which is a potential application of our dataset."}, {"title": "7.3 Limitations and Future Work", "content": "This study faces several limitations. First, our proposed framework for the collection and orchestration of diverse argumentative sentence pairs, covering a wide array of topics, can be extended as needed to facilitate the collection of additional data. However, this framework is constrained by the types of platforms from which stance labels can be extracted. While we investigate the utilization of LLMs to construct stance classification datasets, more sophisticated experiments would be beneficial for exploring the full potential of this approach.\nSecond, while our study examines stance classification from three distinct sources, it is important to recognize that this task is applicable in a much boarder array of contexts, such as news articles, tweets, and political discourse. Therefore, combining our dataset with other existing datasets from different domains could improve the generalizability of stance classification.\nThird, while we focus on the adoption of GPT-3 for generating arguments, we do not directly compare LLMs for generating arguments. We defer comparative studies involving LLMs such as GPT-4, PaLM, or Claude to future research. Such an evaluation would enable more robust methods for benchmarking for stance classification and other social media problems."}]}