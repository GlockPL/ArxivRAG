{"title": "Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation", "authors": ["Fuchen Zheng", "Quanjun Li", "Weixuan Li", "Xuhang Chen", "Yihang Dong", "Guoheng Huang", "Chi-Man Pun", "Shoujun Zhou"], "abstract": "Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence-based medical image segmentation has become a crucial tool in clinical therapy, particularly for early cancer detection and prevention [1]. However, medical imaging presents unique challenges compared to natural image processing, primarily due to the complexity of high-resolution 3D structures and the substantial costs associated with image acquisition. To be more specific, the scarcity of expert annotators, often constrained by demanding schedules, poses additional difficulties in obtaining accurately labeled datasets. Therefore, the long-tail problem [2], [3] has become one of the biggest challenges in deep learning-assisted medical image analysis.\nOn the other hand, U-Net architecture has been instrumental in advancing seg- mentation technology, with extensions like ResUnet [4] combining the strengths of U-Net [5] and ResNet [6]. ResUnet incorporates residual connections to mitigate vanishing gradients [6]\u2013[8], enabling the training of deeper networks and enhancing information flow for more precise predictions. Additionally, it leverages U-Net's skip con- nections to merge feature maps from various resolutions, improving the model's ability to capture fine details while maintaining global context. Recent developments have seen the integration of transformer models, known for their robust feature extraction capabilities, with ResNet to create advanced architectures such as ResT [9], [10]. However, the potential of combining multi-attentions with hybird CNN-Transformer model remains largely unexplored.\nThe main contributions of this paper are as follows:\n1) We propose Compound Multi-Attention Transformer (CMAformer), a novel model that leverages the respective strengths of ResUnet and Transformer, while fusion the spatial attention and channel attention in multi-scale by the proposed Cross Attention layer.\n2) To address long-tail problem in medical image analysis area, we propose a semi-supervised learning Framework by propose a Lagrange Duality Consistency (LDC) Loss which utilizing Lagrange multipliers reformulate BCE-Dice loss function as a convex optimization consistency loss.\n3) Through a series of experiments on numerous public medical image datasets, CMAformer surpasses the majority of state-of-the-art models in segmentation tasks."}, {"title": "II. RELATED WORK", "content": "The U-Net architecture's effectiveness in image segmentation stems from its upsampling process, which restores abstract features to match the original image size, and its downsampling, which enhances the receptive field. However, the original U-Net lacks information exchange among different layers. To address this limitation, Zhou et al. introduced UNet++ [11], which incorporates intermediate nodes within each layer and establishes long connections through feature concatenation, thereby improving information sharing and integration.\nThe integration of attention mechanisms into U-Net architectures has led to the development of U-shape Transformer. Oktay et al. [12] demonstrated that attention units enable various parts of the network to focus on segmenting multiple objects simultaneously. Building on this concept, Chen et al. [13] proposed TransUNet, which combines a convolutional network for feature extraction with a transformer for encoding global context. Similar approaches [14]-[17] have emerged, yet they often underutilize the transformer component, employing only a limited number of layers. Consequently, these models fail to fully incorporate long-term dependencies into convolutional representations. The optimal integration of convolution and self-attention for medical image segmentation remains an open research question."}, {"title": "III. METHOD", "content": "In this section, we detail the architecture of our CMAformer. Additionally, we introduce a semi-supervised learning framework for robust lesion segmentation that we have proposed. This framework intrgrated a Lagrange Duality Consistency (LDC) Loss proposed by us and an unsupervised boundary-aware contrastive objective function [25], [26] so that we can utilize a large volume of unlabeled data to improve the model's ability to detect both common and rare lesions."}, {"title": "A. Channel Attention and Cross Attention", "content": "CMAformer employs residual blocks as its basic building unit. These blocks, while maintaining the conventional structure of two convolutional layers and a skip connection, incorporate unique enhancements inspired by transformers and ResU-shaped networks.\nFirst, the feature map undergoes patch embedding within the residual block and is divided into p \u00d7 p patches. To preserve positional information crucial for spatial understanding, we map the 2D coordinates of each patch onto its flattened representation. However, downsampling operations within the network alter the feature map's shape, rendering the initial position embedding ineffective. To address this, we utilize channel attention, inspired by SEnet [27], to remap the positional information from the patches onto the feature map's channels.\nThis combined approach of patch embedding and channel attention allows CMAformer to effectively fuse spatial and channel-wise information, enhancing its ability to learn discriminative features. The skip connection within the residual block further improves information flow by adding features extracted from the main branch after convolution and batch normalization, promoting gradient propagation and facilitating training. Finally, the main feature map undergoes downsampling through two convolutional layers and ReLU activation, preparing it for subsequent processing stages.\nThe CMAformer's cross-attention layer after the transformer decoder, the, the input feature map is initially split into two components: the query and the key-value pair. The query typically stems from the current layer's output, whereas the key-value pair may originate from various channels within the same layer or from feature maps across different layers. Initially, we project the query (Q) and the key-value (K, V) feature maps into a unified feature space via linear transformations. Subsequently, we calculate the similarity between the query and the key to determine the attention weights. A weighted summation of the values, guided by these attention weights, yields the integrated feature representation. The cross-attention mechanism allows CMAformer to dynamically amalgamate information from disparate feature maps, significantly boosting its capability to represent features. When integrated with channel attention, this mechanism empowers CMAformer to assimilate comprehensive feature details across multiple scales and contexts, thus elevating its performance in complex visual tasks."}, {"title": "B. Transformer Block and Spatial Attention Block", "content": "Each transformer block consists of multi-head self-attention (MSA) and a multi-layer perceptron (MLP). To leverage the strengths of transformers for image data, we introduce two key modifications.\nFirst, instead of layer normalizing the entire output of the MSA, we perform layer normalization individually on the query matrix Q. This modification promotes a more stable gradient signal during training, improving optimization efficiency. Second, the skip connection within the transformer block directly adds the query matrix Q to the MSA output before feeding it to the MLP. This design minimizes information loss and allows the transformer to better model long-range dependencies. To effectively handle the multi-scale nature of medical images, Transformer Block incorporates a spatial attention block between the encoder and decoder. Inspired by atrous spatial pyramid pooling (ASPP) [35], this block fuses feature maps from different scales using dilated convolutions. By integrating these components, Transformer Block effectively model long-range dependencies, while capture local information.\nBefore inputting the feature map to the transformer block, it is reshaped to match the expected input shape of [B, H \u00d7 W, C], where B is the batch size. We utilize the GELU activation function within the MLP due to its smoother nature compared to ReLU. The MLP calculation follows the standard formulation:\nMSA(Q, K, V) = softmax (\\frac{Q K^T}{\\sqrt{d_k}} + P)V,"}, {"title": "C. Lagrange Duality Consistency (LDC) Loss", "content": "To address the challenges of complex medical image segmentation tasks, we propose a modified BCE-Dice loss function as the consistency loss [36] and reformulate it as a convex optimization problem using Lagrangian duality. The consistency loss is defined as:\nL_{con} = -\\frac{1}{N} [\\alpha \\sum_{i=1}^N y_{i1}log(p_{i1}) + (\\beta_1 \\frac{y_{i1}p_{i1}}{y_{i1} + p_{i1}} + \\beta_2 \\frac{y_{i2}p_{i2}}{y_{i2} + p_{i2}})],\nwhere \u03b1, \u03b21, and B2 are weighting factors, with \u03b21 and \u03b22 set to [0.314 \u00b1 1e-5, 0.685 \u00b1 1e-5] empirically.\nTo optimize this loss function, we introduce Lagrange multipliers to handle various constraints: (1) Upper and lower bounds on output values. (2) Limitation on the sum of output values. (3) L2 norm limit on the model parameter vector. (4) Non-negativity constraints.\nThese constraints are incorporated into a Lagrangian function:\nL(p, w, \u03bc\u207a, \u03bc\u00af, \u03bb, \u03b7, \u03b6) = \\sum_{i=1}^N (p_i - \u03bc_i^+) + \\sum_{i=1}^N (\u03bc_i^- - p_i) + \\lambda \\sum_{i=1}^N (p_i - S) + \u03b7(C \u2013 |w|^2) + \\sum_{i=1}^N \u03b6_i(-p_i),\nThis approach, solved using Karush-Kuhn-Tucker (KKT) conditions, allows for more effective optimization of the consistency loss, particularly improving the segmentation of small objects like tumors in complex medical imaging tasks."}, {"title": "D. Overall Training Objective", "content": "Inspired by recent work [25], [26], we cited an unsupervised boundary-aware contrastive objective function. This approach utilizes the InfoNCE loss [37] to contrast positive and negative samples by leveraging \"boundary-aware\" knowledge. The contrastive learning objective ensures consistency between predicted labeled and unlabeled outputs during training as follow:\nL_{contrast}(h_{i,j},h_{i,j}) = \u2212 log \\frac{exp (h_{i,j}h_{i,j}/\u03c4)}{\\sum_{k,l} exp (h_ih_l/\u03c4)},\nwhere hij and hij represent the projection heads of the teacher and student's signed distance map (SDM) [38], [39], respectively. The term 7 denotes a temperature hyperparameter. The indices k and l, used in the denominator, are randomly selected from a mini-batch of images, where i and j refer to the CT sample index and the slice index, respectively. The term hi refers to the negative samples.\nTherefore, the overall loss function is:\nL_{total} = \u03b1 L_{sup} + \u03b2 L_{contrast} + \u03b3 L_{con},"}, {"title": "IV. EXPERIMENTS", "content": "To compare CMAformer fairly with previous models, experiments were conducted on two datasets: the liver tumor segmentation task in LiTS2017 [41] and the multi-organ segmentation task in Synapse [42]."}, {"title": "A. Implementation Details", "content": "For most experimental settings, the instructions of nnformer [34] and UNETR [33] were followed. The datasets are split at 80%, 15%, and 5% to form the training/validation/test sets, respectively. We run all experiments based on pytorch framework on Ubuntu 22.04. All training procedures have been performed on a single NVIDIA GeForce RTX 4090 GPU. The initial learning rate is set to 0.001, and we apply a \"poly\" decay strategy. The default optimizer is SGD, where we set the momentum to 0.99. The weight decay is set to 3e - 5."}, {"title": "B. Comparison with State-of-the-Art Models", "content": "Table I shows the Synapse results. Although CMAformer did not surpass previous models in all metrics, the average of about 10 rounds exceeded all previous advanced models. Table II presents the results on the LiTS2017 dataset. CMAformer demonstrates robust performance in medical image segmentation, surpassing prior models across several metrics. Within the semi-supervised learning framework, utilizing only 50% of labeled data, CMAformer achieves tumor segmentation results suppress those of leading-edge methods.\nOverall, CMAformer achieved competitive and often superior performance compared to previous models across the multiple medical image segmentation datasets under the proposed semi-supervised learning framework. Our LDC loss strategy also helped CMAformer achieve strong results for the semi-supervised tasks."}, {"title": "C. Ablation Study", "content": "The outcomes of the ablation experiments are presented in Table III, highlighting three critical components of our framework: LDC Loss, ViT Block, and Cross Attention layer. The results indicate that the application of a semi-supervised learning framework enhances performance by approximately 12.25%, with even more substantial improvements observed in tumor segmentation. Furthermore, the integration of the Cross Attention layer with the ViT block significantly boosts performance by about 10.98%, underscoring the efficacy of the proposed components."}, {"title": "V. CONCLUSION", "content": "This paper proposed CMAformer, a hybird model that combines the strengths of ResUNet and Transformer, for medical image segmentation. Vit block in CMAformer utlize our proposed Cross Attention layer achieved modeling long-range dependencies while maintaining ability to capture local feature. Moreover, a semi-supervised learning framework based on Lagrange Duality Consistency Loss was developed to maximize the performance of CMAformer and try to address the long-tail problem. The experimental results for different segmentation tasks demonstrate the effectiveness and generalizability of CMAformer compared to previous models. The rigorous experimental setup and statistical analysis provide insights into CMAformer's performance gains. The ability to accurately segment small lesions from medical images can benefit downstream tasks for computer-aided diagnosis and treatment planning. Future work will explore applying CMAformer to more medical image segmentation tasks and real-world clinical applications."}]}