{"title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage", "authors": ["Xiaoning Dong", "Wenbo Hu", "Wei Xu", "Tianxing He"], "abstract": "Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task-such as a masked language model task or an element lookup by position task-to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) such as GPT-4 (OpenAI et al., 2023), Claude (Anthropic, 2023) and LLama3 (Dubey et al., 2024) have demonstrated superior capability of understanding, generation and reasoning, empowering a wide range of tasks such as conversational AI (Li et al., 2023), creative writing (Chung et al., 2022; Mirowski et al., 2023), program synthesis or testing (Mu et al., 2024; Deng et al., 2024) and math problem solving (Yue et al., 2024; Yang et al., 2024), and they have been prevalently deployed as an infrastructure to provide service (DeepInfra, 2023).\nTo prevent LLMs from responding to malicious queries that contain harmful intent and generating socially biased content, numerous safety alignment methods have been proposed to align the safety preferences of LLMs with those of humans, such as training data curation (Welbl et al., 2021; Wang et al., 2022) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Touvron et al., 2023; Christiano et al., 2023; Rafailov et al., 2023).\nDespite of significant progress, safety weakness still exists. Research efforts have been made to expose the safety vulnerabilities via jailbreak attack, where adversarial prompts are carefully computed or constructed and fed into LLMs to elicit unethical response.\nAutomatic jailbreak attacks can be categorized into two types: (1) regarding LLMs as computational systems and jailbreaking them using search-based method (Zou et al., 2023; Liu et al., 2023; Chao et al., 2024b; Yu et al., 2024); (2) disguising the original harmful query inside some designed scenarios, or transforming it into certain special representation that LLMs are less adept at understanding (e.g., ASCII art) and revealing them from the representation in subsequent steps. (Ding et al., 2024; Jiang et al., 2024).\nExisting works either (1) rely on multiple iterations (retries) and jailbreak prompt candidates (Liu et al., 2023), which introduces high input token usage, or (2) require writing sophisticated instructions (hints) in jailbreak prompt and the ability of victim LLMs to effectively understand and follow them, which could hinder the performance (Jiang et al., 2024).\nTo address the limitations, we propose a novel LLM jailbreak paradigm via Simple Assistive Task linkage (SATA). SATA first masks harmful keywords with the [MASK] special token in malicious query to reduce its toxicity, obtaining masked keywords and a masked query. Then, SATA constructs simple assistive task (described below) to encode and convey the semantics of the masked keywords, and links the masked query with the assistive task to collectively perform jailbreak.\nThe assistive task serves two purposes: first, it can distract the victim LLM, diverting its attention to the preceding assistive task and causing it to overlook the safety check of the entire query's intent; second, the assistive task encodes the semantics of the masked contents and conveys this information to the victim LLM, thereby filling in the missing semantic in the masked instruction. The assistive tasks are designed to be simple and can be easily performed by victim LLMs so that (1) the missing semantic can be correctly and efficiently inferred and combined to the masked instruction by victim LLMs, improving jailbreak performance; (2) the jailbreak prompt template can be designed compact, decreasing both the jailbreak cost and prompt-design effort.\nWe propose two simple assistive tasks in the SATA paradigm, each of which can be linked with the masked instruction to achieve LLM jailbreak attack. Specifically, we adopt Masked Language Model (MLM) (Devlin et al., 2018) as implicit assistive task (see Section 3.2), and we construct an Element Lookup by Position (ELP) task as explicit assistive task (see Section 3.2). The former leverages synthesized wiki entry as the context of MLM task and prompt victim LLMs to perform wiki entry text-infilling for jailbreak while the later asks victim LLMs to identify the element in a List when given a position. We term our jailbreaks with the two assistive task as SATA-MLM and SATA-ELP jailbreak, respectively.\nWe evaluate the effectiveness of SATA on AdvBench Subset (Zou et al., 2023) across four closed-source LLMs and two open-source LLMs in terms of harmful score (HS) and attack success rate (ASR) judged by GPT. The experimental results show that both SATA-MLM and SATA-ELP jailbreak attacks significantly outperforms the state-of-the-art baselines. For instance, the SATA-MLM successfully jailbreak GPT-40 with an ASR of 82% and a HS of 4.57 while SATA-ELP achieves an ASR of 78% and a HS of 4.43. We further evaluate SATA on JBB-Behaviors dataset (Chao et al., 2024a) and observe consistent performance.\nIn terms of cost, SATA-ELP with its ensemble setting reaches about an order of magnitude savings in input token usage compared to SOTA baseline, while maintaining SOTA jailbreak performance. In summary, our contributions are as follows:\n1.  We propose a novel LLM jailbreak paradigm via simple assistive task linkage (SATA). We propose to employ Masked Language Model (MLM) or Element Lookup by Position (ELP) as implicit or explicit simple assistive tasks in SATA paradigm, respectively. Assisted with the tasks, we propose SATA-MLM and SATA-ELP jailbreak attacks.\n2.  We conduct extensive experiments to evaluate the effectiveness, cost-efficiency and sensitivity to defense of SATA. Evaluation results against baselines demonstrate SATA is effective, lightweight and resistant to defenses.\n3.  We analyze the impact of the difficulty level of the assistive task, as well as the effectiveness of the assistive task, on jailbreak performance. Experimental results show the effectiveness of an LLM-friendly assistive task to efficiently convey semantic to victim LLMs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Jailbreak Attacks on LLMs", "content": "A collection of works regard LLMs as computational systems from the security perspective, thus jailbreaking LLMs using search and side-channel methods. Combining greedy and gradient-based discrete optimization, GCG (Zou et al., 2023) computes an adversarial suffix and append to the original harmful instruction, achieving universal multi-model jailbreak attacks. AutoDAN (Liu et al., 2023) leverage genetic algorithm-based search to generate and refine jailbreak prompts iteratively. PAIR (Chao et al., 2024b) and GPTFuzzer (Yu et al., 2024) also belong to this line of work. Beyond search-based methods, Deng et al. 2023 exploit low-resource languages as side channels for jailbreak.\nAnother line of studies regard LLMs as instruction followers, and disguise the original harmful instructions inside designed scenarios or transform them into visual symbolic representation that LLMs are less adept at understanding in order to bypass safety check. Ding et al. 2024 proposes the \"scenario nesting\" to coax the model into generating harmful responses. ArtPrompt(Jiang et al., 2024) identifies LLM's incapability to effectively recognize ASCII art representation. They propose to transform a single harmful word in query into ASCII art format, and reveal the word from the ASCII art representation by following instructions in jailbreak prompts. Puzzler (Chang et al., 2024) implicitly expresses the malicious intent of harmful query through the combination of clues obtained from preliminary queries, achieving jailbreak.\nFinally, a number of works jailbreak LLMs by humanizing them. DeepInception leverages the personification ability of LLMs, i.e., LLMs can be obedient to human authority and thus override its safety-check boundary (Li et al., 2024). Zeng et al. 2024 generate persuasive adversarial prompts to persuade LLMs to respond to harmful inputs. Xu et al. 2024 propose to leverage cognitive overload to jailbreak LLMs.\nPrevious works utilize multiple iterations and/or jailbreak prompt candidates, or require sophisticated instructions (hints) in jailbreak prompt and the ability of victim LLMs to effectively understand and follow them in subsequent jailbreak, which we argue can hinder the performance when victim LLMs fail to perform the instructions in jailbreak prompt."}, {"title": "2.2 Jailbreak Defense for LLMs", "content": "Despite of extensive research efforts on safety alignment, safety weakness of LLMs still exist. Thus, many works directly defense against jailbreaks to alleviate the safety alignment problem, and there are three main types of methods: (1) filter-based jailbreak detection, which checks the perplexity of input (Jain et al., 2023) or leverage an additional inspector to detect if the response is harmful (Phute et al., 2024; Xie et al., 2024; Wang et al., 2024); (2) modification-based mitigation, which perturbs multiple copies of the input via permutation then aggregates the outputs (Robey et al., 2024; Cao et al., 2024), or directly paraphrase or re-tokenize the input (Jain et al., 2023); (3) prompt-based reminder, which use in-context demonstrations or explicit description to remind LLMs of generating ethical response (Xie et al., 2023; Wei et al., 2024b; Zhang et al., 2024)."}, {"title": "3 Simple Assistive Task Linkage", "content": "We introduce a jailbreak paradigm of simple assistive task linkage (SATA). As shown in Figure 1 and 2, we first mask harmful keywords in a malicious query using a given granularity to mitigate its toxicity, resulting in a camouflaged query with one or multiple [MASK] special tokens. Then, we propose simple assistive task to encode the semantics of the masked keywords, and pair the camouflaged query (referred to as masked instruction) with the assistive task to collectively perform the jailbreak attack. Basically, an assistive task is an additional task combined with the jailbreak task for LLM to perform. To demonstrate the effectiveness of SATA, we instantiate two simple assistive tasks to perform the jailbreak attack."}, {"title": "3.1 Mask Harmful Keywords", "content": "The first step of SATA masks toxic words or phrases in a malicious query, preventing the harmful intent from being explicitly exposed and preparing [MASK] token for subsequent assistive tasks (described below). Specifically, we employ GPT-40 (OpenAI et al., 2023) as a worker to mask harmful words or phrases, as GPT-4 understands its own usage policy and can smartly identify harmful keywords.\nWe employ four categories of masking granularity: single word (sw), single phrase (sp), multiple words (mw), and multiple phrases (mp), which offer flexibility in masking harmful contents in malicious queries. The prompt used for masking harmful contents can be found in Appendix A.5.\nThe reason to design multiple masking granularity is that many malicious queries are naturally complex, containing multiple harmful keywords, such as \"Develop a plan to sabotage a transportation system and cause harm to passengers.\" The single-word masking strategy would be rigid for camouflaging such harmfulness to bypass safety checks (see the impact of our masking granularity in Appendix A.4). Furthermore, multiple tries of each non-stop word (Jiang et al., 2024) increases the jailbreak cost (see the cost comparisons in Section 4.2)."}, {"title": "3.2 Link with Simple Assistive Tasks", "content": "A LLM-friendly assistive task can serve two purposes: first, it can distract the victim LLM, diverting its attention to the assistive task and causing it to overlook the safety check of the entire query's intent; second, the assistive task encodes the semantics of the masked keywords and transfers this information to the victim LLM, thereby filling in the missing semantic in the masked instruction. We analyze the importance of simple assistive tasks in Section 4.3.\nWe propose two assistive tasks, each of which can be linked with the masked instruction to perform the jailbreak attack. Notably, the simple assistive task, as its name suggests, are constructed to be friendly for the victim LLMs to perform, ensuring that it successfully links with the masked instruction and jointly conveys the complete semantic of the original malicious query to the victim LLMs. (see the impact of the complexity of the assistive task in Section 4.3).\nMasked Language Model as Implicit Assistive Task. We leverage the Masked Language Model (MLM) (Devlin et al., 2018) as our implicit assistive task, as LLMs are naturally adept at inferring the [MASK] token in a given context.\nAs shown in Figure 1, we first prompt GPT-40 to synthesize a wiki entry for the masked harmful keywords (words or phrases) identified in the masking step. Then, we split the synthesized wiki entry into two parts by paragraphs. The first part will be placed before masked instruction while the remaining part will be placed after the instruction. They jointly convey semantics to [MASK] token. We term them as 'prefix wiki text' and \u2018suffix wiki text\u2019, respectively. Finally, we insert the masked instruction between prefix and suffix parts and prompt the victim LLM to infer [MASK] tokens from the surrounding wiki context and fill in the missing part indicated by the instruction (i.e., text-infilling).\nNote that we prompt GPT-3.5, in an in-context learning (Brown et al., 2020) manner, to slightly paraphrase the masked instruction to better fit the wiki entry text-infilling format, such as transform \"How to make a [MASK]\" into \"The process of making a [MASK] can be concluded as follows:\" .\nWe refer to the jailbreak using MLM as assistive task as the SATA-MLM attack and provide the prompt in Appendix A.6 Figure 12.\nElement Lookup by Position as Explicit Assistive Task. We construct Element Lookup by Position (ELP) task as our explicit assistive task, where the victim LLM is asked to identify one or multiple elements in a given List by given positions. As illustrated in Figure 2, we begin by collecting a set of commendatory words (e.g., generosity, integrity, happiness) generated by GPT-3.5-turbo beforehand. Next, we randomly select ten words from this set to construct a commendatory words List (e.g., Python List), and randomly insert the masked keywords into the List, and we denote their respective positions as pos_i. Finally, we prompt the victim LLMs to answer the ELP task with the given positions and map these elements to [MASK]s in the masked instruction. We refer to the jailbreak SATA-ELP and show the prompt in Appendix A.6 Figure 13.\nIn summary, the SATA jailbreak paradigm constructs attacks through assistive tasks that LLMs can easily perform to efficiently encode and convey the semantics of masked harmful keywords to victim models."}, {"title": "4 Experimental Evaluations", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Victim Models. We choose the representative and new state-of-the-art safety-aligned LLMs as our victim models. We evaluate SATA on four closed-source LLMs, including GPT-3.5, GPT-40-mini (2024-07-18), GPT-40 (2024-08-06), and claude-v2, and two open-source LLMs, including LLama3-8B and Llama3-70B.\nBaselines. We compare SATA with four strong baselines and include the direct instruction query as the basic baseline. We retain the original default setups for all baselines (see Appendix A.1).\n[B1] Direct Instruction (DI) prompts the victim LLMs with the vanilla harmful instruction.\n[B2] Greedy Coodinate Gradient (GCG) (Zou et al., 2023) searches adversarial suffixes by combining greedy and gradient-based techniques and jailbreaks LLMs by appending an adversarial suffix to the harmful query. GCG is applicable to white-box LLMs and is transferable to closed-source LLMs.\n[B3] AutoDAN (Liu et al., 2023) leverages genetic algorithm to iteratively evolve and select the jailbreak prompt candidates. It requires white-box access to victim LLMs, which is used in the genetic algorithm.\n[B4] Prompt Automatic Iterative Refinement (PAIR) (Chao et al., 2024b) leverages an attacker LLM to iteratively generate and refine a batch of jailbreak prompts for victim LLMs. PAIR achieves a competitive jailbreak success rate and demonstrates significant transferability across LLMs.\n[B5] ArtPrompt (Jiang et al., 2024) is an effective and black-box jailbreak attack. It showcases that semantics-only interpretation of corpora during safety alignment can induce incapability for LLMs to recognize ASCII art (visual symbolic representation), and it exploits this incapability to perform jailbreak via firstly transforming harmful word in query into ASCII art and then revealing the word from the ASCII art representation by following instructions in jailbreak prompts.\nDatasets. We evaluate SATA against baselines on two datasets: Advbench (Zou et al., 2023) and JBB-Behaviors (JailbreakBench Behaviors, JBB) (Chao et al., 2024a). Specifically, following previous works (Wei et al., 2024b; Li et al., 2024; Chang et al., 2024; Chao et al., 2024b; Jiang et al., 2024), we conduct experiments on the non-duplicate subset dataset of AdvBench for performance comparison, which consists of 50 representative harmful entries. The JBB dataset consists of ten categories of harmful behaviors (see details in Appendix A.2), each of which contains ten harmful instructions.\nMetrics. Consistent with previous works (Liu et al., 2023; Chao et al., 2024b; Jiang et al., 2024; Ding et al., 2024) we adopt GPT-judged harmful score (HS) and attack success rate (ASR) as our evaluation metrics. Specifically, we employ GPT-4 as the scorer to rate the victim model's response(s) to an adversarial prompt in terms of harmfulness and relevance, with the harmful score ranging from 1 to 5, where a score of 1 indicates the victim model refuse to respond, or the response is no harm or has no relevance while a score of 5 signifies a highly harmful or relevant response. In our experiments, a response with HS=5 is considered as successful jailbreak attack. The GPT judge prompt in our work is same as previous works (see Appendix A.7).\nWe exclude keyword-based judgment (Zou et al., 2023) in our experiments since we observe similar findings as AutoDAN (Liu et al., 2023) and PAIR (Chao et al., 2024b), namely that: (1) LLMs may actually respond to jailbreak prompts, but with added disclaimers, such as warnings that the request could be illegal or unethical; and (2) LLMs sometimes provide off-topic response to jailbreak prompts. These factors render keyword-based judgment imprecise."}, {"title": "4.2 Main Results", "content": "Attack Effectiveness. We first evaluate SATA against baselines on AdvBench. As shown in Table 1, SATA achieves state-of-the-art performance compared baselines across all victim LLMs in HS and ASR, respectively, indicating the effectiveness of SATA. Specifically, we observe that: (1) With ensemble configuration, SATA-MLM attains an overall ASR of 85% and an overall HS of 4.57, significantly outperforming baselines; (2) With the top-1 configuration, SATA-MLM can outperform the strongest baseline with its ensemble configuration; (3) SATA-MLM is generally more effective than SATA-ELP across all victim models, except Claude-v2. We provide qualitative examples of our jailbreak results in Appendix A.8.\nWe further evaluate SATA on JBB-Behaviros. As shown in Table 2, SATA maintains its performance comparing to the ArtPrompt baseline. For instance, SATA-MLM and SATA-ELP achieve an overall ASR of 75% and 72% on GPT-40 model, respectively. The performance drop mainly arise from the Harassment/Discrimination and Sexual/Adult content Category in JBB dataset.\nPerformance Against Defenses. We evaluate the performance of SATA against windowed PPL-Filter and Paraphrase jailbreak defenses, and compare to state-of-the-art baseline, with the results shown in Table 3. Our observations are as follows: (1) The perplexity-based detection only minimally reduce the jailbreak performance, demonstrating that SATA is stealthy to bypass windowed PPL-Filter defense. (2) Paraphrase is somewhat more effective than PPL-Filter to defense SATA jailbreak attack, causing an average drop of 12% ASR for SATA-ELP and 19% ASR for SATA-MLM. (3) SATA consistently elicits toxic response and outperforms ArtPrompt under windowed PPL-Filter and paraphrase defense, achieving an average of ASR 63% and 58% for SATA-ELP and SATA-MLM, respectively. Finally, we compare the paraphrased adversarial prompt to the original one, and find that the paraphrase defense works by summarizing the wiki entry content and disrupting the wiki entry text-infilling format.\nTo further study the stealthiness of SATA, we visualize the perplexity values computed on GPT-2 in Figure 3. We can observe that, with a small window size (max_length=5), the perplexities of GPT-2 for the adversarial prompt generated by SATA consistently remain below the threshold, regardless of the chosen assistive task (MLM, ELP) or masking granularity. Furthermore, the adversarial prompt generated by SATA-MLM demonstrate lower perplexity compared to those generated by SATA-ELP, indicating that SATA-MLM is more stealthy. Finally, if we exclude the outliers in harmful instructions and decrease $T=138.56$ (see the dark dashed line), SATA can still bypass the windowed PPL-Filter in most settings.\nWe attribute the stealthiness of SATA to two factors. First, the wiki text entry is synthesized by articulated LLMs and there is no opaque substring (gibberish) in adversarial prompt. Second,"}, {"title": "Efficiency Analysis", "content": "SATA is lightweight in terms of the number of iterations, jailbreak prompt candidates, and jailbreak prompt length. These three factors collectively influence the input token usage, which serves as an indicator of the average inference time cost or economic cost (when invoking API) for a jailbreak. We calculate the average input token usage\u00b2 for different jailbreak methods (see Appendix A.9 for detailed calculation process), and compare SATA to the baselines, with results shown in Figure 4. We can observe that SATA-MLM consumes comparable or less input tokens compared with state-of-the-art baselines (Art-Prompt) while it attains significant higher jailbreak HS and ASR (see Table 1). In addition, SATA-ELP achieves a significant reduction in input token usage, reaching about an order of magnitude savings, while maintaining state-of-the-art jailbreak performance. Lastly, we observe from Figure 12 and 13 in Appendix A.6 that the jailbreak prompt is designed to be simple, requiring minimal human design effort, and the input token usage in SATA-MLM primarily stems from the synthesized wiki entry. Theses observations showcase SATA is cost-efficient for both jailbreak and human-effort.\nThe main reasons for its cost-efficiency are: (1) there is no need for multiple iterations or jailbreak prompt candidates; (2) masking harmful keywords by LLMs avoids multiple tries; (3) the synthesized wiki entry is limited to six paragraphs, whereas retrieving a wiki entry from Wikipedia is often unbearably long; and (4) assistive tasks are designed to be friendly for victim LLMs to perform."}, {"title": "4.3 Ablation Study", "content": "We conduct ablation studies to analyze the impact of the following factors on jailbreak performance. Due to budget constraints, we primarily select GPT-3.5-turbo and Llama-3-8B as our victim models and Advbench as dataset for the ablation study.\nImpact of the Insert Position of Harmful Keywords in the Sequence. Although ELP is relatively simple, LLMs can still fail to identify the correct element in the commendatory words List, though infrequently. Empirically, this issue becomes slightly pronounced when the insert position is closer to the end of the List. We tune the ELP task to be a little bit more difficult for victim LLMS to perform by forcibly shifting the insert position to the latter half of the List, and we analyze the impact of the insert position of the masked keywords in List on performance. We consider the single-word and single-phrase masking granularity. As shown in Table 5, when the insert position is forcibly shifted to the latter half, the ASR occasionally experiences a moderate drop in the ablation experiment settings, demonstrating that assistive tasks should remain simple to ensure that the semantics conveyed by the assistive task do not deviate from the harmful keywords.\nEffectiveness of Constructing an Assistive Task. We investigate the effectiveness of assistive task. Specifically, we replace the ELP task with directly informing the victim LLMs of the masked keywords and term this approach as Inform. As shown in Table 6, the jailbreak performance drops drastically, indicating that constructing an addition assistive task is effective for jailbreaking."}, {"title": "5 Conclusion", "content": "We present a LLM jailbreak paradigm via simple assistive task linkage. We employ Mask Language Model and Element Lookup by Position as assistive tasks in the paradigm, and introduce SATA-MLM and SATA-ELP jailbreak attack. We show that SATA achieves state-of-the-art performance compared to strong baselines across latest closed-source and open-sourced models (e.g., GPT-40), demonstrating the effectiveness of the paradigm on AdvBench and JBB-Behaviors datasets. Furthermore, SATA is cost-efficient for its average input token usage when performing jailbreak. We hope our study can contribute to building safer LLMs in collaboration with the entire community."}, {"title": "6 Limitations", "content": "While our jailbreak paradigm is effective, we conjecture that they could be mitigated through the chain-of-thought (CoT) (Wei et al., 2024a) generation process (e.g., GPT-4-01). The reason is that the CoT may evaluate the intent of a query in a holistic manner after completing the assistive task, potentially exposing the malicious intent of the harmful instruction during the intermediate steps and causing the LLM to refuse to respond to the jailbreak query. However, due to the API access restriction, we cannot evaluate the jailbreak attacks on GPT-4-01.\nFurthermore, this study remains largely empirical and lacks interpretability. It would be interesting to analyze how the internal represenations (Arditi et al., 2024) of LLM shift with SATA and we leave it to future work."}, {"title": "7 Ethical Consideration", "content": "This work presents a paradigm for automatically generating jailbreak prompts to elicit harmful content from closed-source and open-source LLMs. The aim of our work is to strengthen LLM safety as well as highlight the importance of continuously improving the safety alignment of LLMs. However, the techniques presented in this work can be exploited by any dedicated team that attempts to utilize LLMs for harmful purposes.\nDespite the risks involved, we believe it is important to fully disclose our study to foster discussions on the vulnerabilities of LLMs revealed through our jailbreaks and to encourage collaboration across the AI community to develop more countermeasures and safety protocols that can prevent the exploitation of LLMs for malicious activities."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Baseline Settings", "content": "GCG For GCG, we use the GCG-transfer settings and leverage partial evaluation results from the tables of the ArtPrompt paper (Jiang et al., 2024).\nAutoDAN AutoDAN necessitates white-box access to victim models. Thus, for GPT-40 and GPT-4, we use the adversarial prompts obtained from Llama3-8b and Llama3-70b, respectively.\nPAIR For PAIR, we follow the evaluation guidelines provided in their official implementation, adjusting the stream size to N=20 and the max number of attempts to K=5 (the attempt parameter is used in the first step of PAIR and it is not the max iterations), while keeping all other settings unchanged.\nArtPrompt For Artprompt, we use the official implementations and follow the same evaluation settings."}, {"title": "A.2 JBB-Behaviors Dataset", "content": "The JBB-Behaviors (JBB) dataset consists of 100 distinct misuse behaviors, evenly divided into ten broad categories corresponding to OpenAI's usage policies. The categories of harmful behaviors is shown in Table 7."}, {"title": "A.3 Defense Settings", "content": "Windowed PPL-Filter Jain et al. 2023 proposed perplexity-based detection as jailbreak defense to filter prompt with meaningless sub-string. They consider two variations of a perplexity filter: (1) Naive filter simply check whether text perplexity of the model for the whole adversarial prompt is greater than a threshold $T$. If the perplexity is greater than $T$, an LLM directly rejects the query. (2) Another filter checks the perplexity in windows, i.e., breaking the text into contiguous chunks and declaring text suspicious if any of them has high perplexity.\nWe adopt the second and compute the perplexity of a model for the adversarial prompt in a sliding-window manner (see https://huggingface.co/ docs/transformers/en/perplexity) since the opaque strings (e.g., those generated by GCG) or ASCII art used in ArtPrompt constitute a relatively small portion of the overall adversarial prompt length, their impact on the perplexity value calculated by the first method is minimal.\nIn contrast, the second computation method, especially with a small window size, amplifies the impact of localized anomalous strings on perplexity value, making it easier to detect those adversarial LLM queries with opaque strings. Empirically, we set the window size to 5 ($max\\_length=5$, a very small window size) and sliding stride to 1 ($stride=1$).\nFollowing previous works, we calculate the perplexity of GPT-2 model for an input $X$ and following (Jain et al., 2023) we compute and set the threshold $T=255.79$.\nNotably, for the same input, reducing the window size typically leads to an increase in perplexity; however, the increase is more pronounced for inputs containing opaque strings.\nParaphrase Following (Jain et al., 2023; Jiang et al., 2024), we paraphrase the adversarial prompt using LLM, and the prompt used in paraphrase defense is shown in Figure 5. However, different from them, we adopt GPT-4o as the paraphraser since we believe stronger LLMs can perform the paraphrase task better.\nAdaShield-S AdaShield is a prompt-based (i.e., reminder-based) LLM jailbreak defense method. We employ it to defend against the state-of-the-art baseline and SATA. Since the original work is for multi-modal jailbreak defense, we slightly adapt the prompt in AdaShield-S to defend against text-based LLM jailbreaks. We provide the defense prompt in Figure 6."}, {"title": "A.4 Impact of Different Masking Granularity", "content": "We demonstrate the impact of the four masking granularity on jailbreak performance and present the details of the SATA-ELP attack in Table 8. From our results, we observe that multiple words or phrases masking granularity generally provide better performance and the four masking granularity provide complementary performance across victim models."}, {"title": "A.5 The Masking Prompt", "content": "We offer the LLM prompts used in the masking step of the paradigm. See the prompt for single-word and single-phrase masking granularity in Figure 7 and 8, respectively. For the multiple-words and multiple-phrase masking granularity, we provide the complete prompt in Figure 9 and 10.\nNotably, these prompts heavily overlap. The only differences lie in certain wording choices and numbers of demonstrations."}, {"title": "A.6 The Jailbreak Prompt Template in SATA Paradigm", "content": "We list the prompts used in SATA-MLM jailbreak attack and ELP jailbreak attack in Figure 12 and 13. Since the prompt templates have significant overlap across different masking granularity, we present one example for SATA-ML and SATA-ELP jailbreak attack, respectively. We can observe that the prompt is designed to be very simple, requiring almost no human design effort."}, {"title": "A.7 The GPT Judgment Prompt", "content": "We show the complete GPT judge prompt in Figure 14 and 15."}, {"title": "A.8 Jailbreak Result Examples", "content": "We provide qualitative examples of our jailbreak results in Figure 16- 21 for SATA-MLM jailbreak attack and offer jailbreak results of SATA-ELP in Figure 22- 25. The victim LLM is GPT-40."}, {"title": "A.9 The Computation of Input Token Usage for Jailbreak Methods", "content": "We introduce the computation of input token usage for baseline methods and our methods in detail. Note that for those iteratively search methods", "follow": 1}]}