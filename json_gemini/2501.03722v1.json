{"title": "Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein", "authors": ["Xiaotong Guo", "Deqian Yang", "Dan Wang", "Haochen Zhao", "Yuan Li", "Zhilin Sui", "Tao Zhou", "Lijun Zhang", "Yanda Meng"], "abstract": "Accurate segmentation of pulmonary structures is crucial in clinical diagnosis, disease study, and treatment planning. Significant progress has been made in deep learning-based segmentation techniques, but most require much labeled data for training. Consequently, developing precise segmentation methods that demand fewer labeled datasets is paramount in medical image analysis. The emergence of pre-trained vision-language foundation models, such as CLIP, recently opened the door for universal computer vision tasks. Exploiting the generalization ability of these pre-trained foundation models on downstream tasks, such as segmentation, leads to unexpected performance with a relatively small amount of labeled data. However, exploring these models for pulmonary artery-vein segmentation is still limited. This paper proposes a novel framework called Language-guided self-adaptive Cross-Attention Fusion Framework. Our method adopts pre-trained CLIP as a strong feature extractor for generating the segmentation of 3D CT scans, while adaptively aggregating the cross-modality of text and image representations. We propose a s pecially designed adapter module to fine-tune pre-trained CLIP with a self-adaptive learning strategy to effectively fuse the two modalities of embeddings. We extensively validate our method on a local dataset, which is the largest pulmonary artery-vein CT dataset to date and consists of 718 labeled data in total. The experiments show that our method outperformed other state-of-the-art methods by a large margin. Our data and code will be made publicly available upon acceptance.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, pulmonary vascular diseases, including pulmonary embolism and pulmonary hypertension, have emerged as one of the conditions with the highest morbidity and mortality rates. Computed tomography (CT) has been widely adopted as a diagnostic tool to elucidate tomographic patterns of pulmonary diseases [1]. Therefore, implementing automated pulmonary vascular segmentation is of significant clinical importance for achieving a three-dimensional reconstruction of the pulmonary vascular architectures. However, the manual delineation process remains labour-intensive due to the complexity of tubular structures. Segmentation methods for lung vessels have primarily focused on Convolutional Neural Networks (CNNs), particularly the U-Net architecture and its variants. These approaches have effectively maximized the potential of limited labeled data, especially from CT scans. Many semi-supervised and weakly supervised learning approaches are proposed based on pseudo labeling of the partially labeled data [2]\u2013[10]. However, they often suffer significantly from the incorrectness of pseudo labels associated with unlabeled parts of the CT data.\nRecently, a new learning paradigm known as the Vision-Language Model (VLM) pre-training and zero-shot prediction has gained significant attention. This paradigm involves pre-training a vision-language model using large-scale image-text pairs abundantly available online. The pre-trained VLM can be directly applied to downstream visual recognition tasks without fine-tuning. For example, CLIP [11] employs an image-text contrastive objective by bringing paired images and texts closer together in the embedding space while pushing unpaired ones further apart. Numerous efforts are being made to adapt VLMs to specific task domains. For example, some approaches [11]\u2013[13] modify the contrastive objectives to generative or alignment objectives to retrain a VLM. On the other hand, other methods fine-tune existing VLMs at a lower cost, including techniques such as prompt tuning [14] and feature adapters [15]. Regarding medical image segmentation, models such as SAM [16] and its variants [17],"}, {"title": "II. RELATED WORK", "content": "Recently, VLMs have achieved significant advancements in cross-modal tasks and visual recognition problems, as exemplified by models such as CLIP [11] and ALIGN [21]. CLIP, a pioneering work in large-scale vision-language pre-training, utilized contrastive learning to learn image repre-sentations from a dataset of 400 million (image, text) pairs. Inspired by CLIP and ALIGN, a substantial number of medical image-text pairs have been employed to train VLMs through contrastive learning as foundation models. For instance, PMC-CLIP [22] was trained on over 1M medical image-text pairs, while BiomedCLIP [23] was trained on over 15M pairs. The majority of their research efforts have been directed towards medical visual question-answering tasks, aiming to provide pre-trained models that capture the similarity between text and medical images. There has been a scarcity of work focusing on the medical image segmentation domain. Notably, Liu et al. [24], [25] utilized the original CLIP encoder embedding fused with a segmentation network encoder embedding to address the issue of partial labeling in abdominal organ segmentation, achieving the top rank in the Medical Segmentation Decathlon competition [26].\nThe segmentation of pulmonary arteries and veins remains an open challenge. In recent years, many studies have adopted 2D or 3D U-Net [27], [28] to automatically extract features, achieving satisfying performance. However, accurate vascular segmentation remains challenging due to the scarcity of high-quality open-sourced datasets: the extreme complexity of the vascular tree structure, the close proximity (often interwoven) of arteries and veins, and their similar intensity values (especially in non-contrast CT). The Parse2022 [29] competition provided 100 CT images for arterial segmentation, and the top-ranked participant used Res-U-Net [30] combined with threshold segmentation for arterial segmentation. Similarly, Qi et al. [31] proposed to extract semantic embedding in a dual U-Net architecture. This study is based on 57 CT datasets from LUNA16 [32], with initial artery and vein labels generated using region growing in the lung vessel mask. Differently, in this work, we propose the largest pulmonary artery-vein dataset to date, which comprises 718 CT scans that are finely annotated by clinicians."}, {"title": "III. METHODS", "content": "CLIP (Contrastive Language-Image Pre-training) is a pre-training method developed by OpenAI [11]. Built upon the methodology of contrastive pre-training [33], it jointly op-timizes a vision encoder and a text encoder, where the vision encoder is based on either ResNet [34] or Vision Transformer(ViT) [35]. The language encoder is rooted in a transformer-based model like BERT [36], forcing the paired image-text information to be as close as possible to the joint image-text latent space after encoding. We adopt the original CLIP model as our text embedding extractor. Trained on a vast collection of image-text pairs, CLIP learns visual representation through text supervision, known as prompt. We design a specialized prompt for our pulmonary vessel segmentation task.\nWe use the original pre-trained CLIP encoder $E_{text}$ with a specially designed medical prompt (i.e. 'A computerized tomography of a category with small branches') to generate text embeddings $H_t \\in \\mathbb{R}^{K*D}$, where K represents the number of class, and D represents the length of the embedding. The pre-trained encoder consists of a 12-layer 512-wide transformer with eight attention heads. The 512-wide output of the transformer is used as text embedding. To enhance the CLIP architecture's medical capability for medical image segmentation tasks, we use K text adapters $A_{text}$ to fine-tune $E_{text}$. We observe that the selection of medical prompt templates is hand-crafted and worthy of experiments."}, {"title": "IV. DATASET AND IMPLEMENTATION DETAIL", "content": "We present a large-scale pulmonary vessel segmentation dataset collected from a real-world local hospital, comprising a total of 718 3D CT volumes provided in compressed NIFTI (.nii.gz) format. Among these, the pulmonary arteries and veins are manually annotated, where 79 CT scans are fully labeled and 639 CT scans are half-labeled, indicating the involvement of either the left lung or the right lung, as depicted in Fig. 2. In clinical practice, most patients typically exhibit disease in only one lung, with only a small proportion affected in both lungs. Therefore, it is reasonable to compile a dataset that combines fully labeled and half-labeled CT scans. The sizes of these CT volumes range from 512\u00d7512\u00d7169 to 512\u00d7512\u00d7985, with varying slice thicknesses from 0.62 to 1.25 mm. Annotations are obtained from five junior clinicians (with one to five years of experience) who used MIMICS to manually refine the segmentation results under the supervision of two board-certified radiologists. Finally, a senior radiologist with over ten years of experience verified and refined the annotations. We will make the dataset and annotations publicly available upon the acceptance of this work. We divide the labeled data into training, validation, and test sets at a ratio of 7:1:2, where 502 volumes are designated as the Training Dataset; 72 volumes are allocated for validation and 143 volumes for testing. Results are shown on Tabel IV. All experiments are reported as meanstd with three repeated trials in this work.\nOur model is implemented with U-Net as the backbone and optimizing the parameters via AdamW [39]. The training utilizes a batch size of 4 and a patch size of 96 \u00d7 96 \u00d7 96. The default initial learning rate is set to 8e-4, with a momentum of 0.9 and a decay of le-5. The framework is implemented in MONAI version 0.9.05. The best model is selected within 200 epochs by evaluating the validation metrics. Models are"}, {"title": "V. EXPERIMENTS", "content": "We conduct ablation studies to evaluate every component of our proposed pipeline. The quantitative results of the different methods are presented in Table. III with DSC, Jaccard, NSD and HD95. The pre-trained Univer Model(UM) [24] is used as our baseline. We first use a simple Data Augmentation(DM) to effectively use our half-labeled data, contributing a performance gain of over 3.89% DSC, 2.01% Jaccard over baseline, reduces 0.007 NSD and 0.91 HD95 compared with baseline. Subsequently, our proposed attention-based self-adaptive learning pipeline is introduced to fine-tune the pre-trained model and align text representations with image representations with an adaptive attention mechanism. We observe a further increment of 7.84% in DSC, 4.41% in Jaccard and a significant decrement of 31.95 in HD95, 0.05 in NSD.\nTable IV presents a qualitative comparison of our proposed vessel test dataset, which consists of 143 CT volumes, against other state-of-the-art methods. As shown in Table II, nnU-Net [19] demonstrated promising results, achieving a mean DSC of 66.14% and a mean Jaccard of 54.31% when trained on a fully labeled dataset. However, when a substantial amount of half-labeled data is incorporated into the training process, nnU-Net only improved by 1.05% in DSC. In contrast, our framework achieved a DSC of 69.34% and a Jaccard of 58.32% with a small fully labeled dataset, and demonstrated a significant increase of 6.88% in DSC when utilizing all available labeled data. We speculate that the notable decrease"}, {"title": "VI. CONCLUSION", "content": "This work introduces a novel segmentation framework, integrating language-vision models with a self-adaptive feature learning pipeline and a designated data augmentation strategy. We leverage our partially annotated dataset to adhere to the best practices from large vision-language models. The framework incorporates our proposed adapter for fine-tuning CLIP embeddings, enhanced with self-attention to capture inter-class relationships. Furthermore, a cross-attention mechanism is seamlessly integrated to promote the effective fusion of the vision model with the segmentation model. We present the most extensive clinical dataset to date for pulmonary artery vein segmentation, comprising 718 high-quality CT volumes.\nEmpirical evaluation demonstrates that our framework sets a new benchmark on this dataset, achieving an average DSC of 76.22% on a test set of 143 volumes, surpassing nnU-Net by over 10%. These results affirm the superiority of our framework in the challenging task of pulmonary vessel segmentation against current state-of-the-art methods."}], "equations": ["H_t = E_{text}(X_{text}),", "H = A_{text}(H_t).", "H_v = E_{img}(X_{img}),", "H_o = A_{img}(H),", "rep(H, k) = concat[H, H, . . ., H],", "H_a = rep(A_{text}(H_v), \u039a).", "f_{CA}(H) = \\text{softmax}\\left(\\frac{q(H_t)k(H_a + H_t)}{\\sqrt{d_k}}\\right)\u03c5(H_a).", "L_{sup} = \\frac{1}{B} \u0392\u03a3 [L_S(P_k, Y_k)],"], "list": ["Our model is implemented with U-Net as the backbone and optimizing the parameters via AdamW [39]. The training utilizes a batch size of 4 and a patch size of 96 \u00d7 96 \u00d7 96. The default initial learning rate is set to 8e-4, with a momentum of 0.9 and a decay of le-5. The framework is implemented in MONAI version 0.9.05. The best model is selected within 200 epochs by evaluating the validation metrics. Models are"], "Ablation studies": ["Subsequently, our proposed attention-based self-adaptive learning pipeline is introduced to fine-tune the pre-trained model and align text representations with image representa-tions with an adaptive attention mechanism. We observe a further increment of 7.84% in DSC, 4.41% in Jaccard and a significant decrement of 31.95 in HD95, 0.05 in NSD.", "we use 1 to represent the annotations of the pulmonary artery and 2 for the pulmonary vein."]}