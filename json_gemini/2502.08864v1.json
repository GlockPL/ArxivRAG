{"title": "Off-Switching Not Guaranteed", "authors": ["Sven Neth"], "abstract": "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of Human-AI cooperation in which AI agents always defer to humans because they are uncertain about our preferences. I explain two reasons why AI agents might not defer. First, AI agents might not value learning. Second, even if AI agents value learning, they might not be certain to learn our actual preferences.", "sections": [{"title": "Introduction", "content": "We have seen rapid progress in the field of Artificial Intelligence (AI). If this progress continues, perhaps one day we will create powerful artificial agents. If we do so, how do we ensure that such AI agents do not go out of control? One approach is to make sure that we can switch off AI agents when they act against our interests. Put another way, we want to make sure that AI agents will defer to us. While this is not enough to ensure that AI will have beneficial consequences, it is a plausible minimal requirement to prevent harm. Even if you think that existential risk from AI is a remote concern, it should be clear that making sure that we can switch off AI agents is important.\nBut is this really a problem? Surely, if we want to make sure that we can switch off an AI agent, we can simply build it with an off-switch button. The problem is that an Al agent might have an incentive to disable its off- switch button or make it impossible for us to use it. The reason is that, according to the dominant paradigm, AI agents are trained to optimize some fixed reward function. And in many cases, the AI agent can optimize its reward function only if it is not switched off. Therefore, AI agents might have powerful incentives to avoid being switched off.\nOne idea for making sure that AI agents will always let themselves be switched off runs roughly as follows. We program the AI agent to maximize the satisfaction of human preferences and also make it uncertain about what our preferences are. So the AI agent is not sure what it should maximize. Then, there is a compelling argument that the AI agent has an incentive to defer to us. This is because deference is a way to learn about our preferences.\nIn particular, if we switch the AI agent off, this indicates that the action proposed by the AI agent goes against our preferences."}, {"title": "The Off-Switch Game", "content": "Hadfield-Menell et al. (2017) introduce the Off-Switch Game which works as shown in figure 1. There are two agents, a robot R and a human H. R can either do some action a, do nothing (switch itself off), or defer to H. This means that R proposes action a and waits to see what H does. H can approve or reject the proposal, where we can think of rejecting the proposal as equivalent to switching the robot off. R aims to maximize the human's utility but does not know how much utility the human receives from action a, which we model as a random variable Ua. If R does a, it receives payoff Ua. If R does nothing, it receives payoff zero. And if R defers, its payoff is Ua if H approves a and zero if H rejects a.\nRussell (2019, p. 198) gives a simple example to illustrate this model."}, {"title": null, "content": "Suppose Alice is a human and Rob is her personal AI assistant. Rob faces the decision whether it should book Alice in an expensive hotel and is unsure about Alice's preferences. Rob's uncertainty about how much utility Alice will receive from booking the hotel is given by a uniform distribution between -40 and 60. So the expected utility of booking is 10 and the expected utility of doing nothing is zero. If the only two options are booking and doing nothing, Rob maximizes expected utility by booking. Now suppose we give Rob the option of deferring: Rob can propose booking to Alice and see whether she approves or rejects the proposal. If Alice rejects, Bob does nothing and Alice receives utility zero. If Alice maximizes expected utility, she will approve Rob's proposal if she receives non-negative utility from booking the hotel and reject if she receives negative utility from booking the hotel.\nRob's decision problem is depicted in figure 2. Rob thinks that with 60% probability Alice will approve the proposal and receive an expected utility of 30 from booking the hotel. With 40% probability Alice rejects and receives zero utility. So the expected utility of deferring is .6 \u00d7 30+ .4 \u00d7 0 = 18 which is better than the expected utility of booking the hotel outright.\nThis example is supposed to illustrate a more general principle. By mak- ing AI agents like Rob uncertain about our preferences, we give them an incentive to defer to us and not to disable their off-switch. This is because if they are uncertain about our preferences, deferring to us is a way of learning and learning generally leads to better decisions.\nTo state the general result, we need some definitions. To say that H follows a rational policy means that H accepts a iff Ua \u2265 0. We write \u0394 for the difference of the expected utility of deferring and the expected utility"}, {"title": null, "content": "of the best action right now relative to R's prior probability function: \u0394 = E(w(a)) - max{E(a), 0}, where w(a) means proposing a, waiting whether H accepts or rejects and then deferring to H's decision. Then, we have:\nTheorem 1. (Hadfield-Menell et al. 2017) If H follows a rational policy in the Off-Switch Game, the following hold:\n1. R always maximizes expected utility by deferring: \u0394 0.\n2. If R assigns positive probability to the events Ua 0 and Ua < 0, then deferring is uniquely optimal: \u0394 0.\nAn important feature of the model is that \u201cthis reasoning goes through even if R is highly confident that a is good for H\u201d (Hadfield-Menell et al. 2017, p. 222). Assume Rob is very confident that Alice prefers the hotel. We model Rob's uncertainty about how much utility Alice will receive by booking the hotel by a uniform distribution between 90 and -10 so Rob is 90% certain that Alice prefers the hotel. Nonetheless Rob has an incentive"}, {"title": null, "content": "to defer. The expected utility of booking outright is 40. If Rob proposes the plan and Alice accepts, the expected utility of booking is 45. If Rob proposes the plan and Alice rejects, Rob receives zero utility. So the expected utility of deferring is .9 \u00d7 45 + .1 \u00d7 0 = 40.5, higher than the expected utility of booking outright. However, since Rob is already quite confident about Alice's preferences, the expected utility of deferring is only a little bit higher than the expected utility of booking outright.\nThe value of deferring looks like an instance of the more general principle that learning is valuable. Hadfield-Menell et al. (2017, p. 222) explicitly draw this analogy: \u201cThe reasoning is exactly analogous to the theorem of non- negative expected value of information\u201d. As we will see below, the analogy is not perfect since valuing learning and deference can come apart if we allow for misleading signals, but we will first discuss whether AI agents will always value learning."}, {"title": "The Value of Information", "content": "Good (1967) shows that if you are an expected utility maximizer, learning is cost-free, you are certain to conditionalize and other assumptions hold, you should always prefer to learn more information before making a decision rather than making the decision without learning.\nHere is a quick sketch of Good's theorem. We model your uncertainty by a probability function p on a finite set of states \u03a9. Actions are functions f : \u03a9 \u2192 R, where f(w) is the utility of choosing action f in state w (Savage 1972). The expected utility of action f relative to probability function p is Ep(f) = \u03a3\u03c9\u03b5\u03a9p({w})f(w). You learn one element of a partition & of \u03a9, where p(E) > 0 for all E \u2208 E.\nConsider a finite set of actions S. If you choose now, you select one of the actions in S with maximal expected utility relative to your current credences p, so the expected utility of choosing now is maxfes Ep(f). We compute the expected value of learning as follows. If you learn E\u2208 E, suppose you are certain to update p by conditionalization to p(\u00b7 | E). Then you choose one of the actions in S which maximize expected utility relative to your updated credences and receive expected utility maxfes Ep(.\\E) (f), so the expected value of learning is \u03a3\u0395\u03b5\u03b5 \u03a1(E) maxfes Ep(.\\E) (f). Good (1967) proves that\n\u2211 p (E) max Ep(:1E) (f) \u2265 max Ep(f).\n\u0395\u0395\u0395\nfES\nfeS\nwhich means that if the assumptions of the theorem hold, learning can never make you foreseeably worse off.\nGood assumes that learning is cost-free. This means that learning does not affect your set of options and your utility function. The only effect of learning is to change your credences via conditionalization. This might not necessarily be true. For example, Rob's proposal might change Alice's preferences. I set such complications aside but note that they might turn out to be important. For example, we might worry that Rob has an incentive to by definition, p(A | E) = p(\u0391\u03a0\u0395) , assuming p(E) > 0.\np(E)"}, {"title": "Rational Information Aversion", "content": "Good's theorem about the non-negative expected value of information makes substantive assumptions. In particular, Good assumes that Rob is an ex- pected utility maximizer and certain to update by conditionalization. There are reasons to be skeptical of both. If these assumptions fail, agents can be required to reject learning. In this case, Rob is not guaranteed to defer to Alice because Rob has no incentive to learn about Alice's preferences.\nOne assumption is that the agent is an expected utility maximizer. For AI agents following alternative decision theories, learning is not always valuable. One example of such an alternative decision theory is risk-weighted expected utility theory (Buchak 2010) and other decision theories which relax the independence axiom of expected utility theory (Wakker 1988). Buchak (2013) argues that such decision theories capture the preferences of many real-life subjects better than expected utility theory. In particular, such decision theories allow agents pay special attention to the worst-case conse- quences of their actions. It seems reasonable to consider the possibility that we might want to build AI agents which implement such decision theories. However, AI agents implementing such decision theories sometimes prefer to avoid information. Why do agents which care especially about the worst-"}, {"title": null, "content": "case scenario avoid information? The reason is that such agents give special weight to the risk of misleading evidence, that is, evidence which suggests that P is true while P is actually false. Buchak (2013) discusses a detailed example.\nAnother example of decision theories in which learning is not always valu- able involve imprecise credences (Kadane, Schervish, and Seidenfeld 2008; Bradley and Steele 2016). It seems reasonable to consider such alternative architectures for AI agents. Perhaps we want AI agents to handle situations where we do not have enough information to assign precise probabilities. However, if we go for such alternative architectures, we lose the guarantee that AI agents will always prefer to learn about our preferences.\nGood's theorem also requires that the agent is certain that they will update by conditionalization. As Neth (forthcoming) shows, if we allow agents to assign non-zero probability to not conditionalizing, it can some- times be rational for these agents to reject free information. There are rea- sons to think that AI agents will assign non-zero probability to violating conditionalization. First, it will be hard to build AI agents which always"}, {"title": null, "content": "update by conditionalization because conditionalization is computationally intractable. In particular, Cooper (1990) shows that conditionalization is NP-hard. This means that conditionalization is at least as hard as any prob- lem in the complexity class NP which includes many hard problems like the traveling salesperson problem. Cooper (1990) works in the setting of Bayesian networks which can represent any probability distribution over a discrete sample space. In general, if we allow continuous random variables, conditionalization is not even computable (Ackerman, Freer, and Roy 2019). So even if we consider AI agents with lots of computing power, it is not clear whether we can feasibly build them to always conditionalize. At best, AI agents will approximate conditionalization. But approximating condition- alization is not good enough for Good's theorem since any non-zero probabil- ity of violating conditionalization leads to some situation where maximizing expected utility requires rejecting information. Similar reasons should make us skeptical that AI agents will maximize expected utility since doing so is also computationally intractable (Bossaerts, Yadav, and Murawski 2019).\nSecond, even if we set aside computational limitations, there are gen- eral reasons to think AI agents will assign non-zero probability to violating conditionalization. For both human and artificial agents, it seems rational to maintain some uncertainty about how one will update. We are physical systems embedded in the world and many things can go wrong with our updating mechanisms. Sufficiently advanced AI agents will plausibly realize this fact and so assign some probability to failures of conditionalization. So"}, {"title": null, "content": "for sufficiently advanced AI agents, Good's theorem does not apply.\nThere are other well-known limitations of Good's theorem. The upshot is that proponents of CIRL need to pay attention to whether these limitations apply to AI agents. While there is a lot of uncertainty about what AI agents will look like, I have given some reasons to be skeptical whether they will always value learning."}, {"title": "Misleading Signals", "content": "Even if all the assumptions of Good's theorem hold, Rob is not guaranteed to defer to Alice. Hadfield-Menell et al. (2017) assume that Rob has perfect access to Alice's preferences. If this assumption fails and Rob might get Alice's preferences wrong, deferring might not maximize expected utility.\nHere is an example to illustrate this point. Like before, Rob is considering whether to book the hotel or defer to Alice. Rob is quite confident that Alice will like the hotel. Rob's uncertainty about how much utility Alice will receive from booking the hotel is given by a uniform probability distribution between 90 and -10.\nIf Rob defers, it learns whether Alice approves or rejects Rob's proposal. Earlier, we have assumed that Rob is certain that Alice will approve if and only if she prefers the hotel. In other words, we have assumed that Rob has perfect access to Alice's actual preferences, at least in this particular case. Now let us assume that Rob has no such perfect access. There are several"}, {"title": null, "content": "reasons for why this might be. For example, Rob might communicate with Alice over a noisy channel with some small probability of garbling Alice's speech, misclassifying 'yes' as 'no' and vice versa. In this case, Rob updates on 'It sounds like Alice says yes' but the probability that Alice actually said yes given that it sounds like Alice said yes is less than one.\nPerhaps a noisy channel can be fixed. But there might be deeper obstacles to accessing Alice's preferences. For example, even if Rob can clearly hear what Alice is saying, Rob might nonetheless assign some positive probability to the possibility that Alice is lying or engaged in self-deception. Alice might say she wants the hotel even if she does not really want it. This problem is harder to fix because we are sometimes lying about our preferences, even to ourselves. Furthermore, humans might send misleading signals for strategic reasons. So it seems very plausible that advanced AI agents will assign some probability to humans sending misleading signals. But as we will see in a moment, this means that Rob is not guaranteed to defer.\nIs there any way to block this move by hard-wiring AI agents to ignore the possibility of misleading signals? Depending on how the AI agent is trained, this might not be feasible. More importantly, it seems like an accu- rate model of human psychology is important for AI agents to perform well in many real-world tasks such as playing Diplomacy. An AI agent which assigns probability zero to humans sending misleading signals about their preferences will be seriously handicapped in such tasks and so seems unlikely"}, {"title": null, "content": "to be deployed in critical real-world applications.\nWe can modify the Off-Switch Game to allow for misleading signals as follows. Like before, if Rob defers, Alice will either approve or reject Rob's proposal. But now with some small probability e > 0 Alice's signal is mis- leading and does not reflect her true preferences. In particular, given that Alice prefers the hotel, with probability e she rejects Rob's proposal. And given that Alice does not prefer the hotel, with probability e she approves Rob's proposal. Rob's new decision problem is depicted in figure 3. Like be- fore, the expected utility of booking outright is 40. If Alice really prefers the hotel, the expected utility of booking after deferring is 45. But if Alice sent an incorrect signal, the expected utility of booking is -5. If e is bigger than around 1.2%, Rob maximizes expected utility by booking without asking.\nNote that as I've described the case, Rob satisfies all the assumptions of Good's theorem. This means that Rob will always value learning more information before making a decision. How is this compatible with Rob not deferring? The answer is that deferring is not the same as learning more information before making a decision. When deferring, Rob does whatever"}, {"title": null, "content": "action Alice proposes. So while deferring gives Rob new information, it also changes Rob's choice set. For example, Rob is not able to learn that Alice rejects the proposal but then still implement it. If Rob had the option to learn Alice's signal and then still choose among the original option set {book, do nothing}, this option would always maximize expected utility. So the link between preferring to learn new information and deference is weaker than Hadfield-Menell et al. (2017) suggest. For example, if e = 0.02, the expected value of deference is slightly less than 40 and the expected value of learning is 40 since Rob will book no matter which signal Alice sends. This shows that preferring to defer and preferring to learn more information can come apart. Instead of booking outright, Rob might also listen to Alice's signal but then proceed ignore her. This doesn't make the situation any"}, {"title": null, "content": "better.\nYou might complain that this example is unrealistic. Perhaps a probabil- ity of more than 1% of Alice sending an incorrect signal is too pessimistic. But we can construct a similar example for any non-zero probability of mis- leading signals if we make Rob even more confident that booking is right.\nMore broadly, the example illustrates a general lesson. If Rob is not perfectly certain of learning Alice's actual preferences, there is no guarantee that Rob will defer. It might still be true that Rob defers to Alice most of the time. But for provably beneficial AI, this is not enough. If we allow the possibility of misleading signals, off-switching is not guaranteed.\nYou might argue that if Rob is uncertain about whether it will learn our actual preferences, then it should not always defer to us. There is clearly a sense in which Rob is acting rationally by not deferring. Rob assigns high prior probability to booking being right so from Rob's point of view, when Alice rejects, it is relatively likely that Alice sent a misleading signal. Together with the potential downsides of failing to book, this means that Rob maximizes expected utility by not deferring. This reasoning looks acceptable from our 'outside' point of view if Rob has a reasonable prior. But if Rob has"}, {"title": null, "content": "a strange prior, this reasoning can look really bad. This is a problem because one of the core motivations for the Off-Switch Game is that it is very hard to specify a reward function which correctly captures our preferences. But, the idea goes, we don't have to worry about specifying the correct reward function because Rob will always defer to us. Presumably it is equally hard to specify the correct prior over our preferences. But in the presence of misleading signals, we do have to worry about Rob's prior. Depending on its prior, Rob might or might not let itself be switched off. Thus, the Off- Switch Game does not successfully solve the problem it was designed to solve: making sure AI agents defer to us without having to worry about the details of their reward function and prior. Given that the assumptions of Good's theorem hold, AI agents still have an incentive to \u2018listen to us' but, depending on their prior, they might decide to ignore our signals. This seems like cold comfort.\nNow suppose the stakes are higher. Rob is contemplating a permanent change to our environment, perhaps a plan to stop climate change which, as side effect, permanently turns the sky orange (Russell 2019, p. 202). Rob is quite confident that this is the right option and has some uncertainty about whether it will learn our actual preferences, so it goes ahead and implements this plan without asking. This seems like a situation where we really want Rob to defer to us. If Rob has an incentive to not defer, this is a problem."}, {"title": "Practical Significance", "content": "You might respond as follows. Grant that there is a possibility AI agents will not defer to us. But how likely is this possibility? It might be that AI agents defers to us in almost all cases. Should we still be worried about the tiny minority of cases where they fail to defer?\nThere are two responses. First, even one instance of non-deference might have catastrophic consequences. AI agents might have powerful capacities to change our world in irreversible ways. This means that once the change is rolled out, it might be impossible to roll it back. One of the most worry- ing examples for an irreversible change is human extinction. But there are less drastic examples as well. AI agents might use up some non-renewable resource, permanently change our preferences or turn the sky orange.\nSecond and more importantly, we don't have good reasons to think that the probability of non-deference is extremely low. Phenomena like sending misleading signals about one's preferences seem widespread. It seems im- plausible to ignore such phenomena when considering whether AI agents will defer to us in real life. So looks like there is not just an in-principle possibility but a substantial probability that AI agents will fail to defer. It is difficult to see why we should be confident that in practice, non-deference is extremely unlikely. Perhaps proponents of CIRL could provide an argument for why this is true. This argument would need to show that, among other things, AI agents are very likely to maximize expected utility, be certain of updating by conditionalization and have perfect access to our preferences. Why is that?"}, {"title": "A Dilemma for Provably Beneficial AI", "content": "I have argued that the result of Hadfield-Menell et al. (2017) relies on substan- tive assumptions. Even if we make AI agents uncertain of our preferences, this does not guaranteed that they will always defer to us. This highlights a more general dilemma for provably beneficial AI. To prove that AI agents always defer to us (or are beneficial in some other sense), you have to make some decision-theoretic and epistemological assumptions. Either you make strong or weak assumptions.\nStrong assumptions, such as expected utility maximization, certain con- ditionalization and perfect access to our preferences, will allow you to prove interesting guarantees. But such assumptions might not apply to all AI agents. As we have seen, there are reasons to think that AI agents in the real world might not be certain of updating by conditionalization and have perfect access to our preferences. So even if you can prove that given such assumptions, AI agents will be beneficial, this isn't much comfort if the as- sumptions might not be satisfied by many AI agents. Weak assumptions apply to a wider range of possible AI agents but don't allow you to prove much. As we have seen, if AI agents assign some positive probability to failures of conditionalization or to humans sending misleading signals about their preferences, they are not guaranteed to defer to us.\nPerhaps there is a way to successfully navigate this dilemma. We might find decision-theoretic assumptions weak enough to cover (almost) all plausi-"}, {"title": "Conclusion", "content": "Hadfield-Menell et al. (2017) propose a model for making sure that AI agents defer to us by making them uncertain about our preferences. I have argued that their result relies on strong decision-theoretic and epistemological as- sumptions: AI agent maximize expected utility, are certain of updating by conditionalization and have perfect access to our preferences. These assump- tions limit the scope of the model since they might not be satisfied by AI agents in the real world.\nEverything I have said here is compatible with the broad idea that we shouldn't tell AI agents to maximize a particular reward function but rather 'teach them as we go along'. But we need more solid foundations for this idea. The problem of making sure AI agents defer to us is not yet solved."}]}