{"title": "INFLUENCING HUMANS TO CONFORM TO PREFERENCE MODELS FOR RLHF", "authors": ["Stephane Hatgis-Kessell", "W. Bradley Knox", "Serena Booth", "Scott Niekum", "Peter Stone"], "abstract": "Designing a reinforcement learning from human feedback (RLHF) algorithm to approximate a human's unobservable reward function requires assuming, implicitly or explicitly, a model of human preferences. A preference model that poorly describes how humans generate preferences risks learning a poor approximation of the human's reward function. In this paper, we conduct three human studies to assess whether one can influence the expression of real human preferences to more closely conform to a desired preference model. Importantly, our approach does not seek to alter the human's unobserved reward function. Rather, we change how humans use this reward function to generate preferences, such that they better match whatever preference model is assumed by a particular RLHF algorithm. We introduce three interventions: showing humans the quantities that underlie a preference model, which is normally unobservable information derived from the reward function; training people to follow a specific preference model; and modifying the preference elicitation question. All intervention types show significant effects, providing practical tools to improve preference data quality and the resultant alignment of learned reward functions. Overall we establish a novel research direction in model alignment: designing interfaces and training interventions to increase human conformance with the modeling assumptions of the algorithm that will learn from their input.", "sections": [{"title": "1 Introduction", "content": "Aligning agent behavior with human preferences is a central goal of reinforcement learning from human feedback (RLHF). This process generally assumes a model of human preferences, a probability distribution over a human's rankings of pairs of trajectory segments based on their reward function. The RLHF algorithm cannot observe the human's reward function and instead must approximate it from preferences.\nPrior work has explored different choices for this preference model and provided evidence that the more aligned the RLHF algorithm's preference model is with how humans generate preferences, the more aligned the learned reward function is (Knox et al., 2022). Most work assumes that human preferences arise probabilistically from partial return, the sum of rewards over a trajectory segment. By this assumption, humans presented with two trajectory segments tend to prefer the one that accrues greater reward, as measured by their reward function (Christiano et al.,"}, {"title": "2 Related Work: Learning from and Modeling Human Preferences", "content": "Extensive research has explored learning from human preferences for RLHF. This research includes RLHF approaches that explicitly learn a reward function (Christiano et al., 2017; Ibarz et al., 2018; Sadigh et al., 2017; Lee et al., 2021a,b; Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2022; B\u0131y\u0131k et al., 2022; Wang et al., 2022; Bai et al., 2022; Glaese et al., 2022; Knox et al., 2022; Touvron et al., 2023; Ethayarajh et al., 2024) and other approaches that directly learn a policy or advantage function from human preferences (Rafailov et al., 2024; Knox et al., 2024; Hejna et al., 2023). All of the algorithms in the works cited above assume one of the two models of human preference that this paper uses to design preference elicitation interventions. Our investigation of how to increase human conformance with the assumed preference model is compatible with and strengthens this past research.\nOther research has sought agent alignment by developing preference models that better model human preferences, such as by assuming that human preferences arise from a segment's regret (Knox et al., 2022) or weighted sum of non-Markovian rewards (Kim et al., 2023), instead of a segment's sum of Markovian rewards as is commonly assumed. However, improving the preference model still results in some gap between the preference model and actual human preferences, which are subject to difficult-to-model confounding factors and individual differences. Our research seeks to close this gap for whatever preference model is chosen.\nFurther, even if we had a perfect descriptive model of how all humans generate preferences, we may want preferences to be generated by a different preference model that permits more tractable algorithms, greater sample efficiency, or some theoretical guarantees. The methods we introduce can also help in these settings."}, {"title": "3 Preliminaries: Preference Model Choices", "content": "When influencing human preferences, we analyze two preference models: partial return and regret. In this section, we explain the assumptions encoded in each preference model and how each can be used in RLHF.\nConsider a Markov decision process (MDP) that represents the task environment using a tuple $(S, A, T, \\gamma, D_0, r)$. $S$ and $A$ are the sets of possible states and actions, respectively. $T : S \\times A \\rightarrow p(\\cdot|s, a)$ is a transition function; $\\gamma$ is the discount factor; and $D_0$ is the distribution of start states. Unless stated otherwise, we assume tasks are undiscounted $(\\gamma = 1)$ and have terminal states, after which only 0 reward can be received. $r$ is a reward function, $r : S \\times A \\times S \\rightarrow \\mathbb{R}$, where $r_t$ is a function of $s_t$, $a_t$, and $s_{t+1}$ at time $t$. An MDP$\\r$ is an MDP without a reward function.\nLet $r$ refer to the ground-truth reward function for some MDP, $\\hat{r}$ refer to a learned approximation of $r$, and $\\tilde{r}$ refer to any reward function (including $r$ or $\\hat{r}$). A policy $(\\pi : S \\times A \\rightarrow [0, 1])$ specifies the probability of an action given a state. $Q^{\\pi}_{\\tilde{r}}$ and $V^{\\pi}_{\\tilde{r}}$ refer respectively to the state-action value function and state value function for a policy, $\\pi$, under $\\tilde{r}$, and are defined as follows: $V^{\\pi}_{\\tilde{r}}(s) \\doteq E_{\\pi}[\\sum_{t=0}^{\\infty} \\tilde{r}(s_t, a_t, s_{t+1})|s_0 = s]$ and $Q^{\\pi}_{\\tilde{r}}(s, a) \\doteq E_{\\pi}[\\tilde{r}(s, a, s') + \\gamma V^{\\pi}_{\\tilde{r}}(s')]$."}, {"title": "3.1 Reward Learning from Pairwise Preferences", "content": "RLHF typically learns a reward function by minimizing the cross-entropy loss\u2014i.e., maximizing the likelihood of observed human preference labels (Christiano et al., 2017; Ibarz et al., 2018; Wang et al., 2022; B\u0131y\u0131k et al., 2021; Sadigh et al., 2017; Lee et al., 2021a,b; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022; Glaese et al., 2022; OpenAI, 2022; Touvron et al., 2023; Hejna III & Sadigh, 2023). Here, we examine the preliminaries needed to precisely define and this loss function when learning from preferences.\nSegments Let $\\sigma$ denote a segment starting at state $s_0$. Its length $|\\sigma|$ is the number of transitions within the segment. A segment includes $|\\sigma| + 1$ states and $|\\sigma|$ actions: $(s_0, a_0, s_1, a_1, ..., s_{|\\sigma|-1}, a_{|\\sigma|-1}, s_{|\\sigma|})$. In this problem setting, segments lack any reward information. As shorthand, we define $\\sigma_t \\doteq (s_t, a_t, s_{t+1})$. Additionally, we denote the partial return of a segment $\\sigma$ as $\\Sigma_{\\tilde{r}} \\sigma$ for some $\\tilde{r}$, where $\\tilde{r}_t \\doteq \\tilde{r}(s_t, a_t, s_{t+1})$ and $\\Sigma_{\\tilde{r}} \\sigma \\doteq \\sum_{t=0}^{|\\sigma|-1} \\tilde{r}_t$.\nPreference datasets We denote a preference dataset as $D_{\\succ}$, which comprises samples of preferences over pairs of segments. Each sample is represented as $(\\sigma_1, \\sigma_2, \\mu)$. Vector $\\mu = \\langle \\mu_1, \\mu_2 \\rangle$ represents the preference; specifically, if $\\sigma_1$ is preferred over $\\sigma_2$, denoted $\\sigma_1 \\succ \\sigma_2$, $\\mu = \\langle 1, 0 \\rangle$. $\\mu$ is $\\langle 0, 1 \\rangle$ if $\\sigma_1 \\prec \\sigma_2$ and is $\\langle 0.5, 0.5 \\rangle$ for $\\sigma_1 \\sim \\sigma_2$ (no preference). For a sample $(\\sigma_1, \\sigma_2, \\mu)$, we assume that the two segments have equal lengths (i.e., $|\\sigma_1| = |\\sigma_2|$) and the same start state (i.e., $s_0^1 = s_0^2$).\nLoss function When learning a reward function from a preference dataset, $D_{\\succ}$, preference labels are typically assumed to be generated by a preference model $P$ based on an unobservable ground-truth reward function $r$. We learn $\\hat{r}$, an approximation of $r$, by minimizing this cross-entropy loss:\n$\\textrm{loss}(\\hat{r}, D_{\\succ}) = -\\sum_{(\\sigma_1, \\sigma_2, \\mu) \\in D_{\\succ}} \\mu_1 \\log P(\\sigma_1 \\succ \\sigma_2|\\hat{r}) + \\mu_2 \\log P(\\sigma_1 \\prec \\sigma_2|\\hat{r}) \\qquad (1)$\nIf $\\sigma_1 \\succ \\sigma_2$, the sample's likelihood is $P(\\sigma_1 \\succ \\sigma_2|\\hat{r})$ and its loss is therefore $-\\log P(\\sigma_1 \\succ \\sigma_2|\\hat{r})$. If $\\sigma_1 \\prec \\sigma_2$, its likelihood is $1 - P(\\sigma_1 \\succ \\sigma_2|\\hat{r})$. This loss is under-specified until the preference model $P(\\sigma_1 \\succ \\sigma_2|\\hat{r})$ is defined. Learning approximations of $r$ from preferences can be summarized as \u201cminimize Equation 1\u201d.\nPreference models A preference model determines the likelihood of one trajectory segment being preferred over another, $P(\\sigma_1 \\succ \\sigma_2|\\hat{r})$."}, {"title": "3.2 Preference Models: Partial Return and Regret", "content": "Here we describe the two preference models that are most commonly used when learning from human preferences. Figure 2 illustrates an example of how these two models differ. In Section 5 we detail our proposed training procedures and preference elicitation interfaces that influence human preferences to conform to either choice of preference model. We focus on two preference models that differ solely in the segment statistic they use to evaluate the desirability of a trajectory segment. We do not address other aspects of modeling human preferences, such as different assumptions about human irrationality, risk aversion, or uncertainty. However, the interventions proposed in this paper, which aim to influence humans towards a chosen preference model, may be generalizable to influencing humans to conform to preference models that incorporate these additional factors.\nPartial return The most common preference model (e.g., Christiano et al. (2017)) posits that human preferences are generated by a Boltzmann distribution over the partial returns of the two segments, expressed here as a logistic function:\n$P_{\\Sigma_{\\tilde{r}}}(\\sigma_1 \\succ \\sigma_2 | \\tilde{r}) = \\textrm{logistic}(\\Sigma_{\\tilde{r}} \\sigma_1 - \\Sigma_{\\tilde{r}} \\sigma_2). \\qquad (2)$\nRegret An alternative model of human preferences is based on regret (Knox et al., 2022). This model suggests that human preferences arise from the deviations of each segment from optimal decision-making, characterized by the regret"}, {"title": "4 Experimental task and preference elicitation procedure", "content": "To empirically investigate our methodology for influencing human preferences, we collected preference datasets labeled by human subjects with IRB approval. This section provides an overview of the user interface elements shared by each experiment introduced in Section 5. See Appendix C for further details.\nSubject training Subjects first learn about the general grid-world delivery domain, for which task instantiations are created by a map of objects and terrain. The subjects specifically are taught about the reward function and how objects and terrain affect the agent's transitions. As part of this teaching, subjects control the agent on domain maps designed to teach one or two concepts at a time. One such map-the one used to collect preferences is shown in Figure 3.\nPreference elicitation interface After teaching subjects to understand the domain and delivery task, we elicit their preferences. Figure 4 illustrates a baseline version of the preference elicitation interface. In this work, we exclude preferences labeled \u201ccan't tell.\u201d After preference elicitation, a survey tests the subject's task comprehension and attentiveness, as detailed in Appendix C.4."}, {"title": "5 Experimental evaluation of three methods of influence", "content": "Aiming to decrease the gap between an RLHF algorithm's assumed preference model and how a preference dataset is actually generated by humans, we conduct three random-assignment experiments for collecting human preferences. Each experiment represents a type of intervention and has three conditions that each result in a preference dataset: a control condition, an intervention condition that influences subjects to follow the regret preference model ($P_{\\textrm{regret}}$), and an intervention condition that influences subjects to follow the partial return preference model ($P_{\\Sigma r}$). Thus, the ability of each type of intervention to influence the human towards a preference model is tested with two different preference models.\n\u2022 PRIVILEGED experiment (Section 5.1) - The intervention of this experiment is to display information about each segment's regret or partial return under the ground-truth reward function during preference elicitation. We refer to this information as privileged because it relies upon the ground-truth reward function, which in practical settings is unknown by the code underlying the preference elicitation interface.\n\u2022 TRAINED experiment (Section 5.2) - The intervention of this experiment is to train subjects to give preferences based upon either partial return or regret. This training procedure includes teaching people to calculate a segment's regret or partial return.\n\u2022 QUESTION experiment (Section 5.3) - The intervention of this experiment changes the question during preference elicitation (see Figure 4, \u201cWhich path do you prefer?\") to one designed to encourage adherence to one of the preference models.\nIn all experiments, the ground-truth reward function remains the same and is taught to subjects before preference elicitation to enable our analysis, but it is unavailable to the reward learning algorithm, which relies solely on the generated preference dataset. Only the PRIVILEGED experiment leverages the ground-truth reward function in its preference elicitation interface design.\""}, {"title": "5.1 PRIVILEGED experiment", "content": "We first study whether providing subjects with privileged information about a preference model during preference elicitation influences their preferences towards that model. Presenting this privileged information serves as a probe into how susceptible to influence human preferences are.\nWhen presented with two segments for preference labeling, subjects in all three conditions are asked \u201cWhich shows better behavior?\". This question was later refined for the other experiments to the baseline question in Figure 4. Each subject labeled preferences between 35 to 50 segment pairs. After data filtering (see Appendix C), our datasets come from from 64 subjects in the $P_{\\Sigma r}$-PRIVILEGED condition (video walk-through of the interface), 65 subjects in the $P_{\\textrm{regret}}$-PRIVILEGED condition (video walk-through), and 50 subjects in the PRIVILEGED-Control condition (video walk-through). We refer to each condition's resultant preference dataset by the condition's name.\""}, {"title": "6 Conclusion", "content": "The choice of preference model used by an RLHF algorithm introduces a source of misalignment between how humans are assumed to generate preferences and how they actually generate preferences, potentially limiting the alignment of the learned reward function. Even if we could perfectly model all human preferences, we may wish that preferences are generated by a different model that is computationally efficient for RLHF or provides certain theoretical guarantees. To this end, we propose influencing human preferences towards a chosen preference model through user-interface design-a novel direction for RLHF research.\nWe first establish that humans can be significantly influenced towards a specific preference model when privileged information about that model is shown during preference elicitation. We then introduce two practical interventions: training subjects to follow a preference model in the TRAINED experiment\u2014which significantly influences them towards that specific model--and changing the preference elicitation question in the QUESTION experiment\u2014which can significantly influence humans towards the partial return preference model and moderately towards the regret preference model. All three interventions result in learning more aligned reward functions.\nOur findings suggest that human training and preference elicitation interfaces should be viewed as essential tools for improving alignment in RLHF. Appendix A details the potential practicality of our approach; the TRAINED and QUESTION interventions demonstrate promise for real-world application. Notably, the QUESTION experiment offers a viable path forward by influencing human preferences towards a specific preference model without requiring knowledge of the ground-truth reward function, while the TRAINED experiment establishes a foundation for extending this methodology to real-world domains."}]}