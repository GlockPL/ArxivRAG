{"title": "Policies and Evaluation for Online Meeting Summarization", "authors": ["Felix Schneider", "Marco Turchi", "Alex Waibel"], "abstract": "With more and more meetings moving to a digital domain, meeting summarization has recently gained interest in both academic and commercial research. However, prior academic research focuses on meeting summarization as an offline task, performed after the meeting concludes. In this paper, we perform the first systematic study of online meeting summarization. For this purpose, we propose several policies for conducting online summarization. We discuss the unique challenges of this task compared to the offline setting and define novel metrics to evaluate latency and partial summary quality. The experiments on the AutoMin dataset show that 1) online models can produce strong summaries, 2) our metrics allow a detailed analysis of different systems' quality-latency trade-off, also taking into account intermediate outputs and 3) adaptive policies perform better than fixed scheduled ones. These findings provide a starting point for the wider research community to explore this important task.", "sections": [{"title": "Introduction", "content": "In recent years, in particular due to the COVID-19 pandemic, the way we work has changed, moving more activities into a digital space. This drastically increased the number of meetings taking place remotely via video conferencing. As a result, activities relating to physical meetings, such as note-taking, transcription, and summarization are also becoming digitized. Among these activities, automatic summarization has received particular attention from the research and industrial community, resulting in automatic summarization systems that can assist human participants in creating meaningful meeting notes or summaries. Neural network models and particularly Large Language Models (LLMs) have produced impressive results in this area in recent months, moving meeting summarization from being purely a research topic into practical usefulness. This observation was validated in the evaluation of the 2023 Workshop on Automatic Minuting (AutoMin, Ghosal et al. 2023), where humans rated the output of several automatic systems higher than the human-written reference summary.\nHowever, existing research on meeting summarization as an offline task, performed after the meeting is completed. The summary as an artifact is only considered useful as a retrospective of the meeting's content. We lack any study of meeting summarization as an online task, producing summaries as the meeting is still in progress. Online summaries can be useful for catching up with participants coming in late, they can provide live feedback on the coverage of an agenda, and assist in note-taking.\nProducing summaries online introduces unique challenges: Summaries must be produced based on incomplete information and possibly updated to reflect new information. Most of the time, users will see these intermediate summaries, making it all the more important to study their quality. This goal must be balanced against the demand of the user to see summarized content as soon as possible and for the output not to fluctuate. More specifically, the three axes of quality for online summarization are: Quality, usually defined by some variation of adequacy (coverage and accurate representation of all important content), fluency (readability of the text), and relevance (covering only important content, Fabbri et al. 2021a; Ghosal et al. 2023). Unique to online summarization is that we must evaluate not only the final output but also the intermediate summaries. Latency, defined as the time between a piece of content\u00b9 being generated in the output and that same content appearing in the"}, {"title": "Related Work", "content": "Meeting summarization has been studied for decades, with most pre-neural network approaches focusing on extractive summarization (Waibel et al., 1998; Zechner, 2002; Tur et al., 2008; Garg et al., 2009; Tixier et al., 2017). While extractive summaries can be perfectly adequate for some domains like news (Zhong et al., 2020), humans tend to prefer abstractive summaries (Murray et al., 2010; Zhang et al., 2023) and most current research is focused on generative (neural) abstractive summarization models (Zhu et al., 2020; Fabbri et al., 2021b; Zhang et al., 2022).\nResearch into abstractive meeting summarization has largely focused on neural methods, with work largely progressing in two complementary directions, both attempting to cope with the challenge of long inputs: 1) extending the transformer (Vaswani et al., 2017) architecture to scale to longer inputs, usually using a sparse attention paradigm (Brown et al., 2020; Fabbri et al., 2021b; Zhong et al., 2022) or a hierarchical representation approach (Zhu et al., 2020). And 2) divide-and-conquer approaches that perform a segmentation on the source and use conventional summarization systems on the segments, sometimes with an additional refinement step on the segment summaries (Zhang et al., 2022; Asi et al., 2022; Liu and Chen, 2022). The best summaries are currently obtained by Large Language Models (LLMs) producing abstractive summaries (Zhang et al., 2023).\nOnline summarization has largely been studied in the context of stream summarization and video summarization (Sequiera et al., 2018). In stream summarization, it is assumed that a large volume of content (usually tweets or news articles) is being generated all the time and the task is to summarize available content or sentiment on a given topic from this stream at a given time. The primary concern of these systems is to produce a summary of past content on demand, not to respond to new content in a timely fashion. As a result, the concept of latency is not usually discussed in this context. Typical approaches include performing document retrieval as a summary (Ge et al., 2016) and constructing a language model from the content stream and generating a representative example as a summary (Olariu, 2014). Both require orders of magnitude more content than is available in a typical meeting and cannot be applied to meeting summaries.\nVideo summarization is the task of selecting representative frames or sequences from a video to form a summary (Apostolidis et al., 2021). While online systems exist (Lal et al., 2019; Zhao and Xing, 2014), they are very specific to video and exclusively produce extractive summaries. Moreover, there exists, to our knowledge, no study of latency (how soon after the occurrence is a summary available) in this task.\nIn summary, while there are systems that perform summarization in an online fashion, their approaches and evaluation schemes are not suitable"}, {"title": "Online Summarization", "content": "Adapting the paradigm from simultaneous machine translation (Gu et al., 2017), we treat the summarization system as an agent, reading from a stream of input tokens and writing a stream of output tokens. The lengths of either of those streams are initially unknown. The behavior of the agent is determined by a policy, which at any given time can choose between three different actions: READ content from the input stream, WRITE content to the output stream, or ERASE previously written content from the output stream. While in principle the system could read and individual tokens, in this work, we restrict ourselves to systems that read chunks of C tokens at once and write whole summary sentences at once."}, {"title": "Policies", "content": "We propose several baseline policies for evaluation. Following recent findings from simultaneous machine translation (Papi et al., 2022a), we focus on parameter-free policies that are applied post-training to an offline-trained summarization system, controlling when and with what input the model is used. Such policies have the advantage of being largely agnostic to the backend summarizer and being quick to develop and deploy, easing their real-world adoption.\nLength-Based Segmentation This policy reads chunks one at a time from the input and summarizes them independently. This method is simple, has a configurable latency (the chunk size C), has no processing redundancy (every input token is processed exactly once), and does not rewrite its output. However, because the chunks are summarized independently, there is a risk of redundancy in the output. We theorize that a summarization system will have the least redundancy and best coverage if each summarization unit (the span of input that is the basis of a summary shown to the user) is about a single topic and the boundaries between the units also represent boundaries between topics in the meeting. The boundaries selected by this policy are not likely to align with topic boundaries, which may hurt the final quality. This motivates the following two policies.\nModel-Based Segmentation This policy uses quality estimation to determine the ideal summarization unit size. The algorithm works as follows (pseudocode can be found in Appendix C):\n1. Read one chunk. Summarize.\n2. Read another chunk. Summarize it together with the previous chunk(s).\n3. Repeat step 2 until the maximum input size is reached.\n4. Out of these summaries, write the one with the highest estimated quality.\n5. Discard all chunks that were used for that summary and restart at 1. This will cause the remaining chunks to be read again.\nThe boundaries of the summarization units have a profound effect on the quality, and a fixed segmentation policy may not produce ideal results. This policy represents a pragmatic approach of simply trying different chunk sizes and showing only the summary that is judged best by a (reference-free) quality estimation model. In this experiment, we use model confidence as the scoring function, but other metrics, like cosine similarity to the source (Pham et al., 2023) are also possible.\nThis policy will usually incur more latency than the length-based one because it always needs to completely fill the model input. We theorize that the adaptive segmentation will align more closely with topic boundaries, reducing redundancy in the output. Because this policy re-ingests chunks, it also has some processing redundancy. Like the length-based policy, its output is purely incremental, it does not revise.\nSliding Window This policy takes a different approach to estimating the ideal unit size, by using the discrete model outputs directly. It works under the assumption that when adding more input, while keeping the previous output as a fixed prefix, at some point the model will add additional content to the output. For example, for an input of 10 turns, the model would write a one-sentence summary. When adding another 10 turns, and fixing the previous output as a prefix, the model would add a second sentence. The point when the model adds more output is taken to represent a boundary condition. We then rotate out the old input and output and continue. In detail, the policy works as follows (see also Appendix C for pseudocode):"}, {"title": "Data", "content": "There are several meeting summarization datasets available, including AMI (Kraaij et al., 2005), ICSI (Janin et al., 2003), QMSum (Zhong et al., 2021) and AutoMin/Elitr (Nedoluzhko et al., 2022; Ghosal et al., 2023). AMI consists of staged meetings with very repetitive content, which makes it not ideal. This also rules out QMSum, which is mostly based on AMI. ICSI and AutoMin consist of real meetings of realistic length with varying numbers of participants. We decide to test on the more recent AutoMin, specifically the 2023 test set consisting of 12 meetings because the reference summaries are of high quality and we have a recent (Ghosal et al., 2023) comprehensive study about how the automatic metrics correlate with human judgment. Since we focus on the text-to-text setting for this evaluation, audio is not required. Similar to Schneider and Turchi (2023), we found it beneficial for summary quality to reverse the deidentification of the AutoMin data by replacing the PERSON, ORGANIZATION and PROJECT labels with random names, 4- and 3-letter acronyms, respectively. This conversion is reverted before calculating any metrics. There is no audio and thus no timing information available for AutoMin, so we assume unit duration for each dialog turn for calculating latency."}, {"title": "Models", "content": "Our policies can be applied to any offline summarization model. For this paper, we use two models: Bart-large (Lewis et al., 2020), fine-tuned with SamSum (Gliwa et al., 2019)\u00b2 and OpenAI's GPT-4 (OpenAI, 2023), specifically gpt-4-32k, for the increased context length. Bart cannot be directly trained on the AutoMin training data, because most of the meetings are longer than its maximum context length of 1024 tokens. Fine-tuning on Samsum is the next best thing, as it is a high-quality dialog summarization dataset (Asi et al., 2022). GPT-4 is used without further customization. Wherever possible, we apply all policies to both models. However, Bart cannot be used with the full rewriting and fully incremental policies, because of its input"}, {"title": "Evaluation Metrics", "content": "We measure the final summary quality with ROUGE-1 F1 (Lin, 2004)3, because Ghosal et al. (2023) showed this metric to have the best correlation with human judgment on this dataset.\nFor measuring latency, we considered metrics from machine translation (length-adjusted average lagging, LAAL, by Papi et al. 2022b is currently one of the most widely used ones). However, these metrics come with an implicit assumption that the hypothesis will be roughly the same length as the reference. In our evaluation, we encounter systems with widely varying hypothesis lengths, which distorts the existing latency metrics. This is evident most clearly when comparing two systems that operate on exactly the same schedule, such as length-based Bart 1024 and GPT-4 1024. These systems should have similar latency, but they have drastically different LAAL scores (112.5 vs. 67.7).\nRecall our definition of latency from Section 1: Latency is the time between a piece of content appearing in the output and its appearance in the source. Ideally, this should be established by an alignment between the output and the source, to identify which content from the output matches with which parts of the source. Producing such an alignment is a research topic in itself, so for this evaluation, we propose our own metric, Expected Latency (EL), based on a simplifying assumption: That the summarization output contains all relevant content from the source that appeared since the last output. Given this simplification, calculating latency reduces to answering the question: \"Sampling a random point in the meeting, how long do I have to wait for the next summary?\". This gives us an easily interpretable metric with which we can reason about the delay of different systems. Formally, EL is defined as: EL = E[N(t) \u2212 t], where t is a point in time from a uniform distribution over the input and N(t) is the timestamp of the next WRITE event after time t. In practice, this is calculated as:\nEL = 1/T \u03a3 (N(t) - t)^2 / 2\nWhere T is the total length of the meeting, S is the set of the timestamps of all WRITE events. Note that we assume a final WRITE event at the end of the input.\nBecause our text data does not have timestamps, we cannot track processing latency. Instead, we track the redundancy factor (RF) as a proxy metric. It is defined as the number of tokens sent to the summarization model divided by the number of tokens in the meeting. This metric gives an estimate of the extra cost of running a system in a real-world application over an offline one."}, {"title": "Rewrites", "content": "For measuring rewriting, we use Normalized Erasure (NE, Arivazhagan et al. 2020) from simultaneous translation. This metric severely punishes changing a single word near the beginning of the output. Since we are examining only one system that rewrites its output, we leave a more detailed investigation of the user experience of rewrites in online summarization for future work."}, {"title": "Evaluation", "content": "Table 1 shows the scores of the different evaluated systems. We first examine quality and latency separately: Our highest scoring systems (e. g. length-based Bart 768: R1 42.7) achieve comparable quality scores to the offline baselines, but there are significant differences in"}, {"title": "Intermediate Summaries", "content": "In summarization, latency is tightly connected to the quality of partial summaries. If a fact from the source appears earlier in a partial summary, it should affect both metrics for latency and partial summary quality. There exists no adequate measure for this relationship. We motivate a metric of our own with a visual inspection. For a given model, we calculate a Rouge score every time there is an update to the output, always using the full human reference. We then interpret these discrete events as a continuous observation of Rouge score: At a given time, the observed Rouge score is the score of the summary as it is seen by the user at that time. Expressing the time as percent of the meeting completed allows us to average scores over a whole test set, yielding a more reliable measurement of Rouge over time.\nObserving these curves (Figure 1) allows us to visually interpret the measured quality of intermediate summaries and inspect the different dynamics of the various systems throughout a meeting. We generally expect the recall score to increase over the course of the meeting and, ideally, the precision score to remain constant. In reality, we observe a drop in precision in almost all systems. Generally, the systems with longer outputs lose more precision over time than the more concise ones. The opposite is the case for recall: The systems with shorter outputs achieve better recall, not just in the final score but also in the intermediate summaries. This suggests that it is important to consider the F1 score, which balances precision and recall, for a better insight into the overall quality.\nObserving the F1 graph reveals differences in the systems that the final score could not show: Even though most systems end up at a broadly similar score, they have appreciably different trajectories to arrive at that score. Depending on one's point of view, these trajectories can be interpreted as either a measure of latency (sliding window Bart reaches a given F1 score earlier than length-based Bart, even though both end at almost the same score) or quality (at any given time in the meeting, the summaries of sliding window Bart rate higher than those of the length-based one). The model-based system progresses on a very similar schedule as the length-based one, which is reflected in the curves as well as the EL score.\nWe validate the expressiveness of the curves with a human evaluation, using the same setup as before. The only difference is that this time, the annotators are shown only the first 40% of the meeting transcript and the summaries that are available at that time. The results are shown in Table 3. Their evaluation reinforces our confidence in dynamic segmentation strategies, with both dynamic systems outperforming the length-based system. The human annotators rate the model-based and length-based systems very similarly, with a slight preference toward the model-based system. This mirrors the offline evaluations, where humans expressed a preference toward dynamic segmentation models, whereas the Rouge score fails to capture this slight difference. The significant preference of the annotators towards the sliding window is reflected by our metric, validating its expressiveness when there is a significant difference in perceived quality.\nRendering and visually comparing curves for each system may not always be practical. We distill the information from the curves down to a single number by calculating the area under the curve (AUC) for the Rouge-1 F1 curve (last column in Table 3). The convex shape of the sliding window model's curve means that it scores very high on this metric (3382), whereas the more flat curve of the model-based system scores lower (3025). Rouge curves and R1-AUC for all systems can be found in Appendix D.\nOnline summarization is a relatively slow-paced task and the user will see intermediate summaries most of the time. It is therefore essential to compare systems not only on their final summary quality but also to take into account their performance over time. The human evaluation validates the expressiveness of our metric, R1-AUC, giving us a solid basis on which to make judgments about systems' intermediate summary quality. Based on these metrics, sliding window Bart-256 is the system producing the best intermediate summaries."}, {"title": "Conclusion and Future Work", "content": "We conducted the first study of online meeting summarization, by proposing several policies and performing a detailed evaluation. We filled the gap of suitable metrics for latency and quality of intermediate summaries with our own novel metrics, expected latency and R1-AUC, respectively, allowing us to perform a multifaceted analysis of different policies to find the right trade-off between quality and latency. Our automatic and human evaluations showed that our proposed policies are able to create strong summaries of meetings in an online setting while being easy to deploy on top of an existing offline summarizer. They also showed the impact that policy choice has on both final and intermediate summary quality, with dynamic segmentation strategies outperforming static ones.\nHowever, this initial paper also confirms that there is much room for further experimentation and innovation, both in the systems themselves and in methods of evaluation. Particularly promising directions are the development of learned policies (possibly observing from a human teacher), specialized architectures that can reuse more internal states to reduce redundancy and cost, and/or explicitly training with partial information like in simultaneous translation (Niehues et al., 2018). We believe that the importance of this task is underappreciated in academic research and hope to inspire others to take up efforts in this direction."}]}