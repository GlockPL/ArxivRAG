{"title": "Conservative Contextual Bandits: Beyond Linear Representations", "authors": ["Rohan Deb", "Mohammad Ghavamzadeh", "Arindam Banerjee"], "abstract": "Conservative Contextual Bandits (CCBs) address safety in sequential decision making by requiring that an agent's policy, along with minimizing regret, also satisfies a safety constraint: the performance is not worse than a baseline policy (e.g., the policy that the company has in production) by more than (1+a) factor. Prior work developed UCB-style algorithms in the multi-armed [Wu et al., 2016] and contextual linear [Kazerouni et al., 2017] settings. However, in practice the cost of the arms is often a non-linear function, and therefore existing UCB algorithms are ineffective in such settings. In this paper, we consider CCBs beyond the linear case and develop two algorithms C-SquareCB and C-FastCB, using Inverse Gap Weighting (IGW) based exploration and an online regression oracle. We show that the safety constraint is satisfied with high probability and that the regret of C-SquareCB is sub-linear in horizon T, while the regret of C-FastCB is first-order and is sub-linear in L*, the cumulative loss of the optimal policy. Subsequently, we use a neural network for function approximation and online gradient descent as the regression oracle to provide \u00d5(\u221aKT + K/a) and \u00d5(\u221aKL* +K(1+1/a)) regret bounds, respectively. Finally, we demonstrate the efficacy of our algorithms on real-world data and show that they significantly outperform the existing baseline while maintaining the performance guarantee.\nKeywords: Safe Bandits, Neural Contextual Bandits, Non-linear Bandits, Constrained Bandits", "sections": [{"title": "1 Introduction", "content": "Contextual bandits provide a framework to make sequential decisions over time by actively interacting with the environment. In each time step, the learner observes K context vectors associated with corresponding arms, selects an arm based on the history of interaction and observes the corresponding noise corrupted cost\u00b9 of playing that arm. The objective of the learner is to minimize the cumulative sum of costs over the entire horizon of length T, or equivalently to minimize the regret. Although a lot of progress had been made in the multi-armed [Auer et al., 2002, Agrawal and Goyal, 2012, Bubeck et al., 2012, Bubeck and Slivkins, 2012] and linear formulation [Chu et al., 2011, Abbasi-Yadkori et al., 2011, Agrawal and Goyal, 2013], until recently solutions for the general non-linear cost function did not exist. A series of work on neural contextual"}, {"title": "2 Problem Formulation", "content": "Contextual Bandits: We consider a contextual bandit problem where a learner needs to make sequential decisions over T time steps. At any round $t \\in [T]$, the learner observes the context for K arms $X_t = \\{X_{t,1}, ..., X_{t,K}\\} \\subseteq \\mathbb{R}^d$. The contexts can be chosen adversarially unlike in Agarwal et al. [2014], Simchi-Levi and Xu [2020], Ban et al. [2022] where the contexts are chosen i.i.d. from a fixed distribution. The learner chooses an arm $a_t \\in [K]$ and then the associated cost of the arm $y_{t,a_t} \\in [0, 1]$ is observed. We make the following assumption on the cost.\nAssumption 1 (Realizability). The conditional expectation of $Y_{t,a}$ given $x_{t,a}$ is given by some $h \\in \\mathcal{H}$, where $\\mathcal{H}$ is the function class, such that $h : \\mathbb{R}^d \\rightarrow [0, 1]$, i.e., $E[y_{t,a}|x_{t,a}] = h(x_{t,a})$. Further, the context vectors satisfy $\\|x_{t,a}\\| \\leq 1, t \\in [T], a \\in [K]$.\nDefinition 2.1 (Regret). The learner's goal is to minimize the regret, defined as the expected difference between the cumulative cost of the algorithm and that of the optimal policy:\n$Reg_{CB}(T) = E [\\sum_{t=1}^T (Y_{t,a_t} - Y_{t,a_t^*})] = \\sum_{t=1}^T (h(x_{t,a_t}) - h(x_{t,a_t^*}))$\nwhere $a_t^* = argmin_{a \\in [K]} h(x_{t,a})$, minimizes the expected cost in round t. The subscript CB stands for Contextual Bandits and subsequently differentiates it from the regret of online regression.\nConservative Contextual Bandits: There exists a baseline policy $\\pi_1$ that at each round t, selects action $b_t \\in [K]$ and receives the expected cost $h(x_{t,b_t})$. This baseline policy is to be interpreted as the default or status quo policy that the company follows and knows to provide a reasonable performance. However, the company wants to improve the policy but at the same time not incur a high cost while trying to do so. Thus, it insists on the following performance constraint on any algorithm:"}, {"title": "3 Reduction to Online Regression with Squared Loss", "content": "In this section, we develop an algorithm for Conservative Bandits with general output functions by reducing it to a black-box online regression oracle with squared loss. In Section 5, we instantiate the oracle by online gradient descent and give end-to-end regret guarantees. Before proceeding to the algorithm, we briefly describe the online regression formulation below. For a more detailed treatment, see Hazan [2021], Shalev-Shwartz [2012], Bubeck [2011].\nOnline Regression with Squared Loss: We assume access to an oracle Sq-Alg that takes as input all data points until time t 1, $D_{t-1} = \\{(x_{i,a_i}, y_{i,a_i}) : 1 \\leq i \\leq t - 1\\}$ and makes the prediction $\\hat{y}_{t,a} = Sq-Alg(D_{t-1}, x_{t,a})$ in [0, 1] for input $x_{t,a}$ at time t. We further make the following assumption on the regret incurred by the oracle Sq-Alg:\nAssumption 3 (Online Regression Regret for Squared Loss). The regret of the online regression oracle Sq-Alg is bounded by $Reg_{sq}(T) \\geq 1$, i.e.,\n$E[\\sum_{t=1}^T l_{sq}(y_{t,a_t}, y_{t,a_t^*}) - inf_{g \\in H}\\sum_{t=1}^T l_{sq} (g(x_{t,a_t^*}), y_{t,a_t^*})] \\leq Reg_{sq}(T)$ \nwhere the squared loss is given by $l_{sq}(\\hat{y}_{t,a_t}, y_{t,a_t}) = (\\hat{y}_{t,a_t} - y_{t,a_t})^2$.\nWe refer to our algorithm as C-SquareCB, whose pseudo-code is reported in Algorithm 1. At a high level, C-SquareCB does the following: 1) It samples an action from the IGW distribution using the outputs of the"}, {"title": "4 First Order Regret Bound with log loss", "content": "In this section, we use an oracle with KL loss, KL-Alg, and provide a reduction from the conservative contextual bandit (CCB) problem to online regression. The objective of this reduction is to provide a first order data dependent\u00b2 regret bound, i.e., a bound that scales with $L^* = \\sum_{t=1}^T L^*(t)$, where $L^*(t) = h(x_{t,a^*_t})$ is the cost of the optimal action at time t.\nOnline Regression with KL Loss: We assume access to an oracle KL-Alg that takes as input all data points until time t \u2013 1, $D_{t-1} = \\{(x_{i,a_i}, y_{i,a_i}) : 1 \\leq i \\leq t \u2013 1\\}$ and makes the prediction $\\hat{y}_{t,a} = KL-Alg(D_{t-1}, x_{t,a})$ in [0, 1] for input $x_{t,a}$ at time t. We further make the following assumption on the regret incurred by the oracle KL-Alg:\nAssumption 4 (Online Regression Regret for KL Loss). The regret of the online regression oracle KL-Alg is bounded by $Reg_{KL}(T) \\geq 1$, i.e.,\n$\\sum_{t=1}^T l_{KL}(\\hat{y}_{t,a_t}, y_{t,a_t^*}) - inf_{g \\in H}\\sum_{t=1}^T l_{KL} (g(x_{t,a_t^*}), y_{t,a_t^*}) \\leq Reg_{KL}(T)$  \nwhere the KL loss is given by $l_{KL}(\\hat{y}, y) = y \\log(1/\\hat{y}) + (1 \u2013 y) \\log(1/(1 \u2013 \\hat{y}))$."}, {"title": "5 Neural Conservative Bandits", "content": "In this section, we instantiate the online regression oracles Sq-Alg (Algorithm 1) and KL-Alg (Algorithm 2) by (projected) Online Gradient Descent (OGD), and use feed-forward neural networks for function approxi-mation. The setup closely follows the one in Deb et al. [2024a], which we restate it here for completeness.\nWe consider a feed-forward neural network whose output is given by\n$f(\\theta_t; x) := m^{-1/2}v_t^\\top \\phi(\\phi^{(L)}(...(\\phi(m^{-1/2}W_1^{(1)}x)...))$,\nwhere L is the number of hidden layers and m is the width of the network. Further, $W_t^{(1)} \\in \\mathbb{R}^{m \\times d}$ and $W_t^{(l)} = [w_{ij}^{(l)}] \\in \\mathbb{R}^{m \\times m}$ for all $l \\in \\{2,..., L\\}$ are layer-wise weight matrices, and $v_t \\in \\mathbb{R}^{m}$ is the last layer vector. Similar to Du et al. [2019], Banerjee et al. [2023], we consider a (point-wise) smooth and Lipschitz activation function $\\phi(\u00b7)$. We define $\\theta_t \\in \\mathbb{R}^P$, where $\\theta_t := (vec(W_t^{(1)}),..., vec(W_t^{(L)}))^\\top, v_t^\\top)$, as the vector of all parameters in the network, and make the following assumption on the initialization of the network [Liu et al., 2020, Banerjee et al., 2023].\nAssumption 5. We initialize $\\theta_0$ with $w_{ij}^{(l)} \\sim \\mathcal{N}(0, \\sigma_0^2)$ for $l \\in [L]$, where $\\sigma_0 = \\frac{\\sigma_1}{2(1+\\sqrt{\\frac{2}{\\pi}})}\\sqrt{\\frac{m}{log m}}$, $\\sigma_1 > 0$, and $v_0$ is a random unit vector with $\\|V_0\\|_2 = 1$.\nNext, we define the Neural Tangent Kernel (NTK) matrix [Jacot et al., 2018] at $\\theta$ as\n$K_{ntk}(\\theta) := [\\nabla_\\theta f(\\theta; x_t), \\nabla_\\theta f(\\theta; x_{t'}))^\\top \\in \\mathbb{R}^{T \\times T}$,\nand make the following assumption on this matrix which is common in the deep learning literature [Du et al., 2019, Arora et al., 2019, Cao and Gu, 2019]. Note that our NTK is defined for a specific sequence of $x_t$'s where $x_t$ depends on the choice of arms played, and our assumption on the NTK matrix is for all sequences, which is equivalent to the assumption for the (TK \u00d7 TK) NTK matrix as in Zhou et al. [2020] and Zhang et al. [2021].\nAssumption 6. The matrix $K_{ntk}(\\theta_0)$ is positive definite, i.e., $K_{ntk}(\\theta_0) \\succeq \\lambda_0 I$ for some $\\lambda_0 > 0$.\nThe assumption can be ensured if no two context vectors $x_t$ overlap. Note that this assumption is used by all existing regret bounds for neural bandits (see Assumption 4.2 in Zhou et al. 2020, Assumption 3.4 in Zhang et al. 2021, Assumption 5.1 in Ban et al. 2022 and Assumption 5 in Deb et al. 2024a). The choice of the width of the network m depends on $\\lambda_0$ and is similar to the width requirements in Zhou et al. [2020] and Zhang et al. [2021]."}, {"title": "6 Experiments", "content": "We evaluate our algorithms C-SquareCB and C-FastCB and compare the regret bounds with the existing baseline - Conservative Linear UCB (C-LinUCB) [Kazerouni et al., 2017]. The algorithm estimates the parameter associated with the cost function using least squares regression and uses existing results on high probability confidence bounds around the estimate [Abbasi-Yadkori et al., 2011] to set up a safety condition. When the safety condition is satisfied, it plays actions according to Linear UCB [Chu et al., 2011, Abbasi-Yadkori et al., 2011], otherwise switches to the baseline action.\nWe use the evaluation setting for bandit algorithms developed in Bietti et al. [2021] and subsequently used in [Zhou et al., 2020, Zhang et al., 2021, Ban et al., 2022, Deb et al., 2024a]. We consider a series of multiclass classification problems from the openml.org platform. We transform each d-dimensional input into K different context vectors of dimension dK, where K is the number of classes as follows: $x_{t,1} = (x_t, 0, 0, ..., 0)^\\top$, $x_{t,2} = (0, x_t, 0, ..., 0)^\\top), ..., x_{t,K} = (0, 0, ..., 0, x_t)^\\top$. The K vectors correspond to the K different action choices in the bandit problem. We assign a cost of 1 to all the context vectors associated with the incorrect classes, and a cost of 0.01 to the correct class. Note that when an action corresponding to an incorrect class is selected, the learner does not learn the identity of the action with the lowest cost. For each of the datasets, we fix one action as the baseline action, and the baseline policy corresponds to always choosing this pre-defined action.\nBoth C-SquareCB and C-FastCB use a two layered neural network with ReLU hidden activation. We update the network parameter every 10-th round do a grid search over step sizes (0.01, 0.005, 0.001). We compare"}, {"title": "7 Conclusion", "content": "In this paper, we developed two new algorithms, C-SquareCB and C-FastCB, for the problem of Conservative Contextual Bandits with general non-linear cost functions. Our algorithms use Inverse Gap Weighting (IGW) for exploration and rely on an online regression oracle for prediction. We provided regret guarantees for both algorithms, showing that C-SquareCB achieves a sub-linear regret in T, while C-FastCB achieves a first-order regret in terms of the cumulative loss of the optimal policy L*. We also extended our approach by using neural networks for function approximation and provide end-to-end regret bounds. Finally, through experiments on real-world data, we showed that our methods outperform existing baseline while maintaining safety guarantee. Adapting our methods to other safe bandit frameworks such as the stage-wise setting [Moradipari et al., 2019, Amani et al., 2019] and to the more general reinforcement learning framework following Foster et al. [2023b] and Foster et al. [2023a] is left for future work."}, {"title": "A Related Works", "content": "Contextual Bandits. The study of bandit algorithms, especially in the contextual bandit setting, has seen significant development over the years. Initial works on linear bandits, such as those by Abe et al. [2003], Chu et al. [2011], and Abbasi-Yadkori et al. [2011], laid the foundation for exploration strategies with provable regret bounds. These works primarily leveraged linear models, achieving near-optimal performance in various settings. Agrawal and Goyal [2012] provided regret guarantee for the Thompson sampling algorithm in the multi-armed case and later extended it to the linear setting with provable guarantees [Agrawal and Goyal, 2013]. The success of linear bandits naturally led to their extension to more complex settings, particularly nonlinear models. Generalized linear bandits (GLBs) explored by Filippi et al. [2010] and Li et al. [2017] introduced non-linearity through a link function, while preserving a linear dependence on the context.\nContextual Bandits beyond linearity. More recently, the rise of deep learning has led to interest in neural models for contextual bandits. Early attempts to incorporate neural networks into the bandit framework relied on using deep neural networks (DNNs) as feature extractors, with a linear model learned on top of the last hidden layer of the DNN [Lu and Van Roy, 2017, Zahavy and Mannor, 2020, Riquelme et al., 2018]. Although these methods demonstrated empirical success, they lacked theoretical regret guarantees. The NeuralUCB [Zhou et al., 2020] algorithm combined neural networks with UCB-based exploration, and provided regret guarantees. This approach was further extended to Thompson Sampling in the work of Zhang"}, {"title": "B Proof of Regret Bound for C-SquareCB", "content": "Lemma 3.1. Let Assumptions 1 and 2 hold. Then, the regret defined in (1) can be bounded as\n$Reg_{CB}(T) \\leq \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + n_T \\Delta_h,$\nwhere the set $S_T$ consists of the rounds until the horizon T when C-SquareCB played an IGW action and $n_T = |\\bar{S}_T|$ is the number of times until T where a baseline action was played.\nProof. The decomposition follows as in Proposition 2 in [Kazerouni et al., 2017], and we reproduce the proof here for completeness. Recall that $S_T = \\{t \\in [T] : a_t = b_t\\}$ is the set of time steps when the baseline action was chosen and $S_T = \\{t \\in [T] : a_t = \\~{a}_t\\}$ is the set of time steps when the SquareCB action was played. Then, we can decompose the regret as follows:\n$Reg_{CB}(T) = \\sum_{t=1}^T h(x_{t,a_t^*}) - h(x_{t,a_t})$\n$= \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + \\sum_{t\\in \\bar{S}_T} (h(x_{t,a_t^*}) - h(x_{t,a_t}))$\n$\\stackrel{(a)}{=} \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + \\sum_{t\\in \\bar{S}_T} (h(x_{t,b_t}) - h(x_{t,a_t^*}))$\n$\\leq \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + \\sum_{t\\in \\bar{S}_T} \\Delta_{t,b_t}$\n$\\stackrel{(b)}{\\leq} \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + n_T \\Delta_h,$\nwhere (a) follows because $S_T \\cup \\bar{S}_T = [T]$, (b) follows by the definition of $\\Delta_{t,b_t} = h(x_{t,b_t}) - h(x_{t,a_t^*})$ and (c) follows by Assumption 2.\nLemma 3.2. Suppose Assumption 1,2 and 3 holds. Then, with probability $1 \u2013 \\delta/4$ the number of times the baseline action is played by C-SquareCB is bounded as\n$n_T \\leq \\frac{1}{\\alpha \\gamma_l} \\{\n(\\mathcal{O}_{\\tau-1} + 1)(\\Delta_l + \\alpha \\gamma_l)\n+\n64\\sqrt{K} \\sqrt{(\\mathcal{O}_{\\tau-1} + 1)} (\\sqrt{Reg_{sq}(T)} + \\sqrt{log(\\frac{8}{\\delta})}) \\}.\n\n$"}, {"title": "C Proof of Regret Bound for C-FastCB", "content": "Proof of Theorem 4.1. The proof of the theorem follows along the following steps, and the proof of the intermediate lemmas can be found at the end of this proof.\n1. Regret Decomposition: The regret decomposition follows using Lemma 3.1 as in the proof of Theo-rem 3.1.\nLemma 3.1. Let Assumptions 1 and 2 hold. Then, the regret defined in (1) can be bounded as\n$Reg_{CB}(T) \\leq \\sum_{t\\in S_T} (h(x_{t,a_t^*}) - h(x_{t,a_t})) + n_T \\Delta_h,$\nwhere the set $S_T$ consists of the rounds until the horizon T when C-SquareCB played an IGW action and $n_T = |\\bar{S}_T|$ is the number of times until T where a baseline action was played.\n2. Upper Bound on nr: The condition in Line 7 determines how many times the baseline action is played. Suppose mt = |St| and r = max{1 < t < T : at = bt}, i.e., the last time step at which C-FastCB played an action according to the baseline strategy.\nBefore we proceed and give a bound on ny, the number of times the baseline action is played by Algorithm 2, we specify how the exploration factor Vi is chosen. Unlike in Foster and Krishnamurthy [2021] where $Y_i = y = max(\\sqrt{KL^*/(3Reg_{KL}(T))}, 10K)$, for all i \u2208 [K], we need to choose a time dependent Yi to ensure that we control both ny and the regret by playing the non-conservative actions. However using a different Yi at every step does not lead to a first-order regret bound for the first term in (6). Therefore we set Yi in an episodic manner, where Yi remains same in an episode. More specifically we choose Yi as follows:\nYo = 1, no = 1, $L_0^* = 0$\nfor i \u2208 ST\n$L_i^* = L_{i-1}^*+h(x_{t,a^*_t})$\nif $L_i^* > 2n_{i-1}$\n$n_i = 2n_{i-1}$\nelse\n$n_i = n_{i-1}$\n$\\gamma_i = max \\{10K, \\sqrt{\\frac{K \\eta_i}{Reg_{KL} (T)}}} \\}$\n(Yi-Schedule)\n(a) The following lemma upper-bounds n\u00eb in terms of m\u30f6, $\\sum_{i \\in S_{7-1}} L^*(i)$, the cumulative cost in the set S7\u22121, and the KL loss $Reg_{KL} (T)$, using the above schedule for yi.\nLemma C.1. Suppose Assumption 1,2, 4 holds. Then, the number of times the baseline action is"}]}