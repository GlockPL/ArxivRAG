{"title": "See Detail Say Clear: Towards Brain CT Report Generation via Pathological Clue-driven Representation Learning", "authors": ["Chengxin Zheng", "Junzhong Ji", "Yanzhao Shi", "Xiaodan Zhang", "Liangqiong Qu"], "abstract": "Brain CT report generation is significant to aid physicians in diagnosing cranial diseases. Recent studies concentrate on handling the consistency between visual and textual pathological features to improve the coherence of report. However, there exist some challenges: 1) Redundant visual representing: Massive irrelevant areas in 3D scans distract models from representing salient visual contexts. 2) Shifted semantic representing: Limited medical corpus causes difficulties for models to transfer the learned textual representations to generative layers. This study introduces a Pathological Clue-driven Representation Learning (PCRL) model to build cross-modal representations based on pathological clues and naturally adapt them for accurate report generation. Specifically, we construct pathological clues from perspectives of segmented regions, pathological entities, and report themes, to fully grasp visual pathological patterns and learn cross-modal feature representations. To adapt the representations for the text generation task, we bridge the gap between representation learning and report generation by using a unified large language model (LLM) with task-tailored instructions. These crafted instructions enable the LLM to be flexibly fine-tuned across tasks and smoothly transfer the semantic representation for report generation. Experiments demonstrate that our method outperforms previous methods and achieves SoTA performance. Our code is available at https://github.com/Chauncey-Jheng/PCRL-MRG.", "sections": [{"title": "1 Introduction", "content": "Brain computed tomography (CT) imaging is essential for diagnosing various cranial diseases, including cerebral infarction and hemorrhage. However, it is time-consuming and error-prone for radiologists to manually interpret medical findings from these scans and write reports. Automated report generation systems are designed to boost efficiency, reduce the workload for radiologists, and optimize resources in busy clinical scenarios.\nWith the advancement of deep neural networks and their successful application in image captioning tasks, medical report generation (MRG) has gained more attention. Unlike the short sentences of traditional image captioning, MRG aims to generate lengthy and precise reports. To achieve this, various cross-modal alignment methods are required to ensure the consistency between visual and textual information, including attention mechanisms, memory mechanisms, and knowledge graphs.\nRecently, learning representations via visual-textual contrastive learning or using pre-trained large language models (LLMs) to strength representations are also proven to be effective.\nHowever, as shown in Figure 1(a), learning cross-modal correspondences is still challenging in current methods due to the following concerns: 1) Redundant visual representing. Different from chest X-ray data, 3D brain CT scans contain extensive redundant information, e.g. background and insignificant areas. With the lack of human-crafted boxes to locate pathology regions, models struggle to capture and interpret the visual pathology patterns for generating reports. Although current advanced models use semantic prior knowledge or medical prompts to automatically learn the salient visual areas, this may introduce noise and unstable representation and cause severe hallucinations in generated texts. 2) Shifted semantic representing. Compared to natural corpus, limited brain CT report corpus is insufficient to transfer the pathological semantic representations learned by represent learning layers (e.g., contrastive learning layer) to the language model, since the direct weight-sharing is prone to degrade the coherence of generated diagnostic sentences. Thus, how to uniformly represent cross-modal pathological features and adapt them to report generation still remains an open question.\nIn this paper, we propose a Pathological Clue-driven Representation Learning (PCRL) model to seamlessly build cross-modal representations based on diverse pathological clues and transfer them for generating accurate brain CT reports. Specifically, we extract pathological clues from perspectives of segmented regions, pathological entities, and report themes to depict clinical scenarios. Segmented region clues are automatically generated and filtered by given pathology prompts, enabling the visual encoder to grasp visual pathological patterns. Meanwhile, entity and theme clues are respectively extracted by detailed findings and full-text reports, to handle the enriched visual-textual alignment and build cross-modal pathology representations.\nBesides, to adapt the learned representations for the text generation task, we bridge the gap between representation learning and report generation by employing a unified large language model (LLM) with task-tailored instructions, which has proven to be more effective than conventional decoders by using appropriate tokens to seamlessly connect different tasks. As shown in Figure 1(b), We craft a representation instruction to prompt the LLM to produce high-level pathological semantic features for cross-modal alignment, and a generation instruction to prompt LLM to generate accurate brain CT reports based on the learned representations.\nOur main contributions can be summarized as:\n1. We propose a novel framework to seamlessly learn visual-textual representations from perspectives of diverse pathological clues and leverage them for enhancing the quality of generated brain CT reports.\n2. We, for the first time, design a new paradigm to effectively transfer the learned pathological representations for report generation using a unified LLM prompted by task-tailored instructions.\n3. We validate the model capabilities on the open-source CTRG-Brain dataset. Experimental results show that our model achieves remarkable performance in generating brain CT reports."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Medical Report Generation", "content": "The advancements in image captioning techniques have spurred the development of a range of radiology report generation methods. To exploit the more effective parts of medical images, Jing et al. (2018) first propose a co-attention mechanism to associate images with disease tags and improve the accuracy of generated reports; Chen et al. (2021) design a cross-modal memory matrix to learn high-level vision-language correspondence for enhancing report generation; Liu et al. (2021c) used contrastive attention to captured the difference of abnormal and normal samples. To address the semantic bias of limited medical report corpus, Jing et al. (2020) exploited the textual structure information of reports; Liu et al. (2021a) used curriculum learning to alleviate the data bias of limited medical data corpus. The retrieval-based and prior expert knowledge based"}, {"title": "2.2 Pre-trained Large Model", "content": "In recent years, pre-trained large models have achieved significant breakthroughs in both natural language processing (NLP) and computer vision (CV). These models leverage extensive datasets for pre-training, allowing them to perform exceptionally well in downstream tasks. The Segment Anything Model (SAM) is"}, {"title": "3 Methodology", "content": "As illustrated in Figure 2, our framework consists of two branches: the brain CT Report Generation (RG) on the left and the Pathological Clue-driven Representation Learning (PCRL) on the right. The interaction between these branches is facilitated through the shared use of a visual encoder and a language model."}, {"title": "3.1 Brain CT Report Generation", "content": "In this branch, the input comprises a set of CT scan images $I = \\{I_1,...,I_N\\}$, where N represents the number of CT images in each sample. The output is the corresponding brain CT findings report $Y = \\{y_1, ..., y_M\\}$, with M representing the number of tokens. We adopt the encoder-decoder architecture for report generation. First, we use ResNet101 to extract grid features $G = \\{g_1,...,g_N\\} \\in \\mathbb{R}^{N\\times H\\times d}$ (N = 24, H = 196, d = 2048) from I. Then, we embed these features using a visual encoder to obtain visual features V. These visual features V are passed through a multi-modal projector to obtain the visual embedding tokens $H_V \\in \\mathbb{R}^{N\\times d_w}$ ($d_w = 4096$), which are aligned with the word embedding space of a large language model. Finally, these tokens are concatenated with the instruction token $H_q$ and fed into the large language model. We train the parameters $\\theta$ by minimizing the cross-entropy loss, which can be represented by the following formula:\n$L_g = - \\sum_{t=1}^{M}log P(y_t | Y_{1:t-1}, H_v, H_q; \\theta)$ (1)\nwhere, P(yt|*) denotes the probability conditioned on $H_v, H_q$, and the embeddings of previous words $Y_1, Y_2,..., Y_{t-1}$."}, {"title": "3.2 Pathological Clue-driven Representation Learning", "content": "Learning fine-grained visual-text representations is critical for generating accurate reports. This branch mainly builds enriched representations by conducting multi-modal feature alignment based on pathological clues, including segmentation clue alignment (SCA), entity clues alignment (ECA), and theme clues alignment (TCA)."}, {"title": "3.2.1 Preparation of Pathological Clues", "content": "Pathological clues are collected in three perspectives for feature alignment.\nPathology Theme Clues: Learning the structure of professional brain CT reports is essential for MRG models to satisfy human standards and produce reliable reports. To this end, we propose to build theme clues by simply using the full-text report and whole images as global signals, which can be useful to enhance the overall quality of medical reports by TCA (illustrated in 3.2.4).\nPathology Entity Clues: To learn detailed information about pathology entities, we regard each single finding sentence in the report as an entity clue and extract the related multi-modal features in ECA 3.2.3. The construction of entities $E = \\{e_1, ..., e_{N_e} \\}$ (with $N_e = 24$) is based on expert knowledge and the frequency of words in the training corpus.\nSegment Clues: To enhance visual representations via detailed contour of pathologies, we propose to utilize the pre-trained segmentation model SAM to generate mask candidates and filter useful masks related to pathology entities. First, we prompt SAM by covering each brain CT image with a grid of points and integrating it into image embeddings through average sampling. In this way, the mask decoder in SAM is prompted to generate a gallery of candidate masks $M_1 = \\{m_1,...,m_{N_m}\\}$. To ensure the quality of masks, we also apply rule-based methods to filter out low-quality and duplicate masks with area size, stability scores, and IoU scores. For each sample consisting of 24 images, we generate a corresponding series of masks and combine them to form the candidate $Gallery = \\{M_{1_1}, ..., M_{1_N}\\}$.\nThen, to retrieve valuable masks related to the sample's pathological entities, we propose to use the MedCLIP for text-prompted retrieval. Based on expert knowledge, we divide the 24 images into eight-layer categories, each mapping a specific set of entities. Conversely, each entity also has its corresponding layers. We extract the existing entities from the sample's report and their corresponding pathological descriptions $D = \\{d_1, ..., d_{N_d}\\}$ ($N_d \\leq N_e$), which serve as the Query. Next, we use the d-th description $Query_d$ to search for the most similar matching mask in $Gallery_d$, which is a subset of Gallery, consisting of all visual masks of the layers related to the existing entities. The procedure can be represented as:\n$Retrieval_d = CLIP(Gallery_d, Query_d)$ (2)\nwhere $Retrieval_d$ denotes the retrieved masks for SCA 3.2.2."}, {"title": "3.2.2 Segmentation Clue Alignment", "content": "This module aims to learn the fine-grained visual representation based on the extracted segment masks. First, we obtain the visual features $V_D = \\{v_1,...v_{N_d}\\}$ corresponding to the images containing the entities using a selector. Then, we input these features into a lightweight segmentation head to generate the corresponding foreground entity masks $S_D = \\{s_1, ..., s_{N_d}\\}$, which serves as discriminated pathological information. We then align these generated masks with the retrieved segment masks $M_D = \\{Retrieval_1, ..., Retrieval_{N_d}\\}$ to learn detailed patterns. It is important to note that the size of the generated masks $S_D$ differs from the size of the retrieved SAM masks $M_D$. To address this, we first resize the SAM masks to match the size of the masks generated by our segmentation head. We finally calculate the following loss for aligning the two types of masks:\n$L_{seg} = \\frac{1}{2} - \\frac{\\sum_{i=1}^{n} P_i y_i}{\\sqrt{\\sum_{i=1}^{n} P_i^2 + \\sum_{i=1}^{n} y_i^2}}$ (3)\nwhere $y_i$ is the pixel value of retrieved SAM masks and $p_i$ is the predicted pixel value.\nIn this way, the visual encoder can be effectively learned to focus on the areas of pathologies while reducing the influence of irrelevant visual features."}, {"title": "3.2.3 Entity Clue Alignment", "content": "To learn the cross-modal patterns of pathology entities, we extract visual and textual pathological features based on the entity clues. First, we build a selector to obtain the visual features $V_D = \\{v_1,...v_{N_d}\\}$ ($N_d < N$), which corresponds to significant CT images that exist pathology entities. We then apply global average pooling (GAP) to generalize $V_D$ and obtain representations of each significant CT image, denoted as $R_v = \\{r_1, ...r_{N_d}\\}$.\nDifferent from previous work use an external language model (e.g. SciBert ) to build textual representation for feature alignment, we propose to leverage a unified LLM to generate textual representation via tailored prompts. Inspired by Wang et al. (2023), we crafted a representation instruction \u201cSummarize the following cranial diagnosis in one word:\", which is denoted as $H_{qr}$. This prompt can effectively activate the summarization ability of LLM to generate corresponding text representations for pathological entity description in D. We use the output generated by the final hidden layer of LLM as textual features for cross-modal alignment.\nWith the carefully extracted visual and textual representations of entities, we map them into the same dimension through an embedding layer. The process can be represented by the following formula:\n$R^v_i = Linear_v(GAP(V_D))$ (4)\n$R^w_i = Linear_w(LLM(H_{qr}, D))$ (5)\nwhere GAP denotes the global average pooling, $Linear_v$ represents the visual mapper, and $Linear_w$ represents the textual mapper. We employ the symmetric InfoNCE loss for visual-textual alignment:\n$L_{CLe} = \\frac{1}{2} [- \\frac{1}{N_d} \\sum_{i=1}^{N_d} log \\frac{exp(s(i, i))}{\\sum_{j=1, j\\neq i}^{N_d} exp(s(i, j))} - \\frac{1}{N_d} \\sum_{i=1}^{N_d} log \\frac{exp(s'(i, i))}{\\sum_{j=1, j\\neq i}^{N_d} exp(s'(i, j))}] + \\alpha_v \\sum_{i=1}^{N_d} log \\frac{exp(s(i, i))}{\\sum_{j=1, j\\neq i}^{N_d} exp(s(i, j))}\n+ \\alpha_w \\sum_{i=1}^{N_d} log \\frac{exp(s'(i, i))}{\\sum_{j=1, j\\neq i}^{N_d} exp(s'(i, j))}$ (6)\nwhere $s(i, j) = sim(R^v_i, R^v_j)/\\tau$ and $s'(i,j) = sim(R^w_i, R^w_j)/\\tau$ denote the similarity between the visual representation $R^v_i$ and the textual representation $R^w_j$, $\\tau$is a temperature parameter. $\\alpha_v$ and $\\alpha_w$ are hyperparameters to balance the contrastive learning.\nBy aligning multi-modal entity clues, the model can grasp fine-grained visual-text representations to generate accurate diagnostic words.\""}, {"title": "3.2.4 Theme Clue Alignment", "content": "Theme clue alignment aims to equip the model with comprehensive skills to generate accurate style and structure of reports, thereby enhancing clinical reliability. For global visual features, we use one-dimensional global max pooling (GMP) to represent the entire sample visually, denoted as $R_v^G$. For the overall report of the sample, we generate textual representation $R_w^G$ by prompting LLM with the same representation instruction (see 3.2.3). This process can be represented as:\n$R_v^G = Linear_v(GMP(V))$ (7)\n$R_w^G = Linear_w(LLM(H_{qs}, Y))$ (8)\nSimilar to ECA, the loss of TCA can be formulated as:\n$L_{CLt} = \\frac{1}{2} [- \\frac{1}{N_b} \\sum_{i=1}^{N_b} log \\frac{exp(s(i,i))}{\\sum_{j=1, j\\neq i}^{N_b} exp(s_t(i, j))} - \\frac{1}{N_b} \\sum_{i=1}^{N_b} log \\frac{exp(s'(i, i))}{\\sum_{j=1, j\\neq i}^{N_b} exp(s_t(i, j))}] + \\alpha_v \\sum_{i=1}^{N_b} log \\frac{exp(s(i,i))}{\\sum_{j=1, j\\neq i}^{N_b} exp(s_t(i, j))}\n+ \\alpha_w \\sum_{i=1}^{N_b} log \\frac{exp(s'(i, i))}{\\sum_{j=1, j\\neq i}^{N_b} exp(s_t(i, j))}$ (9)\nwhere $s(i, j) = sim(R_v^G, R_{w_j}^G) / \\tau$ and $s'(i, j) = sim(R_w^G, R_{v_j}^G)/\\tau$ denote the similarity between the visual representation $R_v^G$ and the textual representation $R_w^G$, $\\tau$ is a temperature parameter, $N_b$ is the batch size. Shared with ECA, $\\alpha_v$ and $\\alpha_w$ are set to balance the feature alignment."}, {"title": "3.3 Joint Training", "content": "In the training stage, we jointly train the RG branch and the PCRL branch to maximize the utilization of multi-granularity visual-text representations. Instead of using separate modules to learn representation and generate medical reports, we use a unified LLM to bridge the gap between representation learning and report generation via two task-tailored instructions, i.e., representation instruction and generation instruction. This can transfer the representations learned by the PCRL branch to optimize the RG branch effectively, thereby generating accurate reports.\nOur final loss contains the above-mentioned losses, which can be formulated as:\n$L = L_g + L_{seg} + L_{CLe} + L_{CLt}$ (10)"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Dataset", "content": "We validate the performance of our model using the CTRG-Brain dataset. This dataset comprises 6,000 samples, containing a total of 160,336 CT images and 6,000 Chinese medical reports. Following the mainstream division method , we split the dataset into a training set, a validation set, and a test set in a 7:1:2 ratio. For consistent processing, we divide the brain CT image samples into 8 layers based on expert knowledge, with each layer containing 3 continuous CT images, assigning each sample with 24 CT images."}, {"title": "4.2 Evaluation Metrics", "content": "We chose Natural Language Generation (NLG) metrics and Clinical Evaluation (CE) metrics to evaluate our model's performance. NLG metrics include BLEU , METEOR , ROUGE-L , and CIDEr, denoted as B1, B2, B3, B4, M, RG, and C respectively. To measure the pathological accuracy, we use 24 keywords summarized by experienced radiologists to calculate the Clinical Evaluation (CE) metric , i. e., F1 score, which is the harmonic mean of the precision and recall."}, {"title": "4.3 Implementation Details", "content": "We reshape the size of image to 512x512 pixels and used a ResNet101 to extract image features, which is pre-trained on the ImageNet dataset and fine-tuned on the CQ500 dataset. For our large language model, we utilize LLaMA3-8B , which is quantized to 4-bit, and use the LoRa for parameter-efficient fine-tuning. The overall trainable parameter quantity of our model is 229.9M, with 3.4M parameters in LLM (only 0.04%). During training, we use the AdamW optimizer with a learning rate of 1e-4. The batch size is 4, with 1050 training steps per epoch. For testing, we set the temperature coefficient of the large model to 0.6 and the top-p value to 0.9. The model is implemented using PyTorch 2.3.0, and the entire training process is conducted on a single RTX 4090 GPU."}, {"title": "4.4 Quantitative Analysis", "content": "We compare the proposed PCRL with some competitive brain CT report generation methods . Besides, we also reproduced some SOTA models in image captioning and chest X-ray report generation for comprehensive comparisons on CTRG-Brain dataset. What's more, for fair comparisons, We also reproduce several related LLM-based methods with the same LLM decoder.\nAs shown in Table 1, our method outperforms others across most evaluation metrics. WGAM and WGAM-HI employ weakly-supervised visual attention to extract key visual features, resulting in higher BLEU scores. With contrastive learning for cross-modal alignment, WCL and PGCA can effectively learn relations between CT images and reports, achieving better results. However, due to the lack of training data, the above methods still produce reports that fall short in fluency and readability. Here's a refined version of your sentence: The LLM-based methods demonstrate poorer performance compared to traditional transformer-based methods. This suggests that prior knowledge from pretrained large models, without further cross-modal alignment, may lead to hallucinations in report generation.\nOur PCRL achieves fine-grained cross-modal alignment by leveraging a series of pre-trained large models, generating more fluent reports and achieves the best performance compared to other methods. It is noteworthy that we tested our method using the Jieba\u00b9 and LLaMA tokenizers respectively to compute NLG scores, with the latter achieving the best performance across all NLG metrics.\nThis discrepancy may be due to differences in tokenization methods used during training. While other models employ the traditional Jieba tool for Chinese word tokenization, our PCRL follows more advanced BPE-based subword tokenization. Nevertheless, our model also achieves competitive results in overall metrics."}, {"title": "4.5 Ablation Study", "content": "To evaluate the effect of each component in PCRL, we have done plenty of ablation studies, as shown in Table 2. Baseline is the standard encoder-decoder (ResNet101-LLaMA3) architecture without alignment. By progressively adding visual or textual representation (visual and textual) and the module losses ($L_{SCA}$, $L_{ECA}$ and $L_{TCA}$), respectively denoting the incorporation of two modalities of representations and the utilization of three pathological clue-driven alignments (i.e., SCA, ECA, and TCA).\nBy comparing (a) with the baseline, we observe that incorporating the segmentation alignment significantly enhances the model's performance. This demonstrates that our SCA method effectively filters out irrelevant information from CT images and extracts crucial pathology-related visual regions. In contrast, with the implementation of ECA, (b) achieves comparable or better performance across all metrics compared to (a). This indicates that fine-grained cross-modal pathological entity alignment can effectively learn strongly correlated visual-text"}, {"title": "4.6 Qualitative Analysis", "content": "We visualize the brain CT reports generated by baseline, WGAM-HI and our model in Figure 3. Given the ground truth brain CT sample and entity data, our model generates better brain CT report with the most accurate entity words (e.g. basal ganglia, ventricle, and brainstem) among the competitors, which demonstrates the effectiveness of using diverse medical clues to learn enriched multi-modal representations. It can be also seen that compared with the baseline, the report generated by our model has a better semantic structure, indicating the contribution of employing a unified LLM to transfer useful representations. Besides, we also find that the retrieved entity masks from segmentation masks gallery (see Figure 4) can generally match related entity words at both levels of visual and semantic. For example, the mask of \"midline structure\" and \"brainstem\" matches the empirical scan slice and fine-grained region chosen by experienced radiologists. This guarantees the model to mine accurate visual cranial patterns, therefore generating high-quality reports."}, {"title": "5 Conclusion", "content": "We propose a novel model to mine pathological clues for enhancing multi-modal representations and seamlessly transfer them into report generation. First, through carefully designed segmentation clue alignment, entity clue alignment, and theme clue alignment, the diverse and precise feature representation can be well-constructed. Second, we transfer the learned representation to boost the brain CT report generation via a unified LLM prompted by task-tailored instructions. Experiments demonstrate the effectiveness of our model in generating pathologically accurate reports."}, {"title": "Limitations", "content": "Although the segmentation clues retrieved by MedClip can generally match corresponding pathological entities and help the model neglect redundant visual information, it should be noted that a part of retrieved entity masks may not be precise. This is because MedClip is mainly pretrained by chest X-ray data with limited brain CT samples. Thus, addressing this challenge is imperative for the research community. In the future, we will work on exploring useful approaches. One potential approach is to train a unified text-prompted medical segmentation model towards 3D brain CT scans, which can not only be employed to offer fine-grained visual information for medical report generation but also for other related tasks, e.g. medical VQA."}]}