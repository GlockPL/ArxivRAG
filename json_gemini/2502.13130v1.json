{"title": "Magma: A Foundation Model for Multimodal AI Agents", "authors": ["Jianwei Yang", "Reuben Tan", "Qianhui Wu", "Ruijie Zheng", "Baolin Peng", "Yongyuan Liang", "Yu Gu", "Mu Cai", "Seonghyeon Ye", "Joel Jang", "Yuquan Deng", "Lars Liden", "Jianfeng Gao"], "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pre-trained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig. 1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility\u00b9.", "sections": [{"title": "1. Introduction", "content": "A long-standing research topic of AI is to develop autonomous agents that can perceive visual stimuli, language inputs, and other environmentally-grounded data and produce meaningful embodied actions in physical and digital environments to complete specific tasks.\nRecently, there has been a growing interest in developing AI agents based on Vision-Language-Action (VLA) models [5, 6, 19, 29, 42, 54]. These models are typically pre-trained on large amounts of vision-language datasets and then action trajectories to attain ability to take actions given VL inputs. However, due to the inherent difference between various environments (e.g., 2D digital world and 3D physical ones), VLA models are typically trained separately for simplicity and then used for different tasks. Exemplary models in the digital world include Pix2ACT [108], WebGUM [34], and Ferret-UI [131] for UI navitation. VLA models in the 3D physical world include RT-2 [5] and Open-VLA [54] for robotics manipulation. Although claimed as generalist, most of these models prioritize learning a task-specific action policy at the cost of a significant decline in generic multimodal understanding capabilities, rendering limited genralizability across tasks and domains.\nIn this research, we strive to develop a foundation model for multimodal AI agents and argue that it requires simultaneously possessing the following capabilities:\n\u2022 Multimodal Understanding to understand multimodal input from various domains (both digital and physical) not only semantically, but also spatially and temporally.\n\u2022 Multimodal Action Prediction to break down the long-horizon task into an accurate action sequence, which can be effectively executed by AI agent systems.\nSuch an agent system should be driven by external goals specified by human commands as shown in Fig. 2.\nTo endow the broad capabilities, we effectively leverage large amounts of heterogeneous vision-language and action datasets, including UI datasets such as SeekClick [19], robotic manipulation dataset OXE [23], human instructional videos like Ego-4d [40] and image-text pairs used in LMMs [13, 71]. Instead of sequentially training on one domain and adapting to another, we train a single foundation model which can be applied in a zero-shot manner to different downstream tasks in various settings.\nSimply combining those datasets, however, does not bring benefits to the foundation model, due to the significant gap between multimodal understanding which is mostly verbal (i.e., textual descriptions for images and videos) and the action-taking tasks which are mostly spatial (i.e., 2D coordinates for UI or 7-DoF for robot arm). To bridge the gap, we propose two surrogate tasks for model training, action grounding and action planning, by asking the model to predict the proximal action outputs given the visual-spatial observations, represented as images or video frames. Specifically,"}, {"title": "3. Multimodal Agentic Modeling", "content": "A generalist multimodal AI agent should be performant for both multimodal understanding and action-taking. We define a multimodal AI agent \u03c0, which takes past visual observations I = {I1, ..., Ik} and a task description task in text as input and outputs a set of T > 1 tokens O as:\nO = \u03c0(I, task, ctx) = {01,\u2026, 0T} (1)\nwhere ctx denotes the context, l \u2208 {verbal, spatial} indicates if the i-th token or is a verbal or spatial token. This formula generalizes across different tasks:\n\u2022 UI navigation in 2D screenshots. The task could be \u201cbook a hotel\u201d and output should include both language tokens denoting the semantic type of action (e.g., \u201ctype\u201d, \u201cclick\u201d, etc.) and the location (x, y) or box (x, y, w, h) to which actions are applied.\n\u2022 Robotic manipulation in the 3D world. For a task like \u201cclose the drawer\u201d, the output consists of 6-DoF displacements (x, y, z, yaw, pitch, roll) of the end effector and, in some cases, one additional dimension to indicate whether the gripper is open or not.\n\u2022 Multimodal understanding tasks. When the task is purely about I, e.g., a VQA task, the problem is reduced to a multimodal understanding task that generates a textual description and/or location of objects for input images/videos.\nFor these seemingly different output modalities, we follow a common practice to transform all output into textual tokens to facilitate model learning. Specifically, we convert 2D actions into a textual dictionary as in [19], and represent robot actions with the last 256 discrete language tokens that is barely used in LLMs, following [54]. Despite such unification into language space, we notice considerable conflicts among tasks, as we will show in our experiments. In what"}, {"title": "3.2. Method", "content": "We approach two key challenges while building a highly capable foundation for the multimodal AI agent.\nPretraining objectives: How to build a unified pretraining interface to facilitate joint training? A straightforward way would be to predict the 2D coordinates for the navigation of the UI, 3D positions for the end effectors, and regular textual outputs for VL tasks. However, in our experiments, we observed that these tasks have inherent domain gaps in both input and output. The former results in a huge search space at the pixel level, and the latter directly predicts the output of proprioceptive action, which is not grounded on the observations of the image. Can we come up with a surrogate task that can bridge the gap among all tasks?\nData scaling-up: Existing vision-language-action data have limited amount and diversity, unlike language or image-text corpus for LLMs and LMMs, respectively. For example, the largest open source robotic dataset OXE [23] consists of around 1M trajectories taken from 22 environments. On the other hand, large-scale image-text datasets like LAION [104] barely contain useful supervisions for action pretraining as they are all static without the notion of action. Videos, however, depict numerous human actions and human-object interactions. Can we largely take advantage of these video data for our agentic pretraining?\nIn this work, we propose a simple yet effective method to address the aforementioned challenges. Inspired by the generality of Set-of-Mark (SoM) rompting [126], we employ it to enable the action grounding onto images for both UI and robotic tasks in that model faces much less difficulties to predict the numeric marks for both clickable buttons or robot arms in the image space. We further extend it along the temporal axis and ask the model to predict Trace-of-Mark (ToM), which forces the model to learn a longer horizon by predicting distant future \u201cactions\u201d, and more importantly provides an effective way to leverage unlabeled video data. The combination of SoM and TOM enables a seamless synergy across agentic tasks in digital and physical domains, as well as a scalable way to curate \u201caction\u201d supervisions from raw videos. We describe them in detail below in Sec. 3.2.1 and 3.2.2, respectively."}, {"title": "3.2.1. Set-of-Mark for Action Grounding", "content": "SoM prompting [126] was first proposed to enhance the grounding capability of GPT-4V and has then been widely adopted for various agentic tasks [18, 44, 70, 94, 123]. Unlike previous works that exploited it for prompting off-the-shelf LMMs to enhance visual-language grounding, here we propose to train an agentic model for action grounding, i.e., locating actionable points / regions for a specific task and further predict atomic actions if needed.\nGiven an image observation It \u2208 RH\u00d7W\u00d73 at timestep t, a task task and context ctx, we first extract a set of K candidate regions or points that are actionable P = {P1,...,pK}, where pk could be a four-dimensional box coordinate or two-dimensional point coordinates. Subsequently, we overlay the marks and boxes (if any) to the corresponding location of the image with numerical labels, i.e., M = {1 : p1, 2 : p2, ..., K : pK} giving us a new marked image IM.\nGiven the prompted image IM in an atomic action step, the model needs to select the candidate marks along with the original coordinates, significantly easing the action grounding for the agentic model. In this way, Eq. (1) can be reformulated as:\no^{mark} = action_{t:markt} = \u03c0(I^{M}, task, ctx)  (2)\nwhere o^{mark} is a subset of marks M.\nIn Fig. 3, we show a few instances to demonstrate the SoM-based action grounding in Fig 1. To obtain candidate regions to mark, we can leverage different proposal networks such as image segmentation models [56, 147], object detection models [64, 80], or domain-specific models [83]. Readers refer to Supp. for more details."}, {"title": "3.2.2. Trace-of-Mark for Action Planning", "content": "Video data contains a lot of information about human actions and activities, which can essentially be leveraged to boost the capability of agentic models. However, due to the absence of action labels, previous methods rarely explore this direction, apart from a few works focused on world model learning [76, 91]. We extend the strategy of \u201coverlaying marks\u201d from static images to dynamic videos by proposing Trace-of-Mark (ToM) to allow the agentic model to effectively learn to plan and act from videos.\nGiven the sequence of visual observations from a video I = {I1, ..., It}, we extend along the time axis to the future l frames, Ifuture = {It+1,..., It+l}. Given the K marks at t-th frame It, we extract the corresponding positions of the overlay marks in the next l frames, denoted traces T = {Mt+1, ..., Mt+l}. Following the prediction of action type and valid marks as in Eq. (2), we further ask the model to predict the future trajectories for the valid marks:\no^{mark} = action_{t:markt:tracet+1:t+l} = \u03c0({I1, ..., It\u22121,IM}, task, ctx) (3)\nwhere tracet+1:t+l is a subset of the trace sequences for valid marks in markt in T. Our proposed ToM predicting is a simple yet effective way of leveraging video data and brings two unique modeling benefits: (i) It forces the model to understand the temporal dynamics in the video observations and to \"look ahead of time\" before taking the next actions; (ii) Unlike predicting next frames as used in [77], predicting traces uses much fewer tokens to capture much longer temporal horizon and action-related object dynamics, while disregarding ambient contents.\nTo extract ToM, we employ point tracking models Co-Tracker [48], though any performant model can be used. In particular, given a sequence of frames {It, It+1, ..., It+l} \u2208 R(l+1)\u00d7H\u00d7W\u00d73, we apply a dense tracking for s\u00b2 grid points to extract s\u00b2 traces of length (l + 1). Given these s2 traces, we drop those traces whose average motion magnitudes between two adjacent timesteps are smaller than a certain value \u20ac (Please see more details in the supplementary material). The remaining ones are regarded as foreground motions driven by a given task."}, {"title": "3.3. Modeling", "content": "To retain the multimodal understanding capability required for Magma, we adopt the common practice used in current VLMs (e.g., LLaVA [71] and Phi-3-Vision [1]). Given the visual observations I, we use a vision encoder V to encode each frame into a number of tokens and then concatenate all tokens into a sequence and feed them to a decoder-only LLM along with the language tokens that encode task descriptions. Due to the task diversity, a vision encoder that can seamlessly encode images and videos of various resolutions is needed. In light of this, we propose to use convolutional networks ConvNeXt [81] as the vision backbone, considering that it supports arbitrary image resolutions by default. To handle the high-resolution images (e.g., up to 2000 for UI screenshots), we simply perform global encoding without the bells and whistles used in previous work and find that it can encode the global context as well as combining global and local crops [1, 75]. To that end, we formulate the agentic modeling as an autoregressive decoding procedure:\no_{l}^{i} \u223c p(o_{l+1}|{o_{l}, ..., o_{l}}; V(I), task, ctx). (4)"}, {"title": "4. Multimodal Agentic Pretraining", "content": "To develop a foundation model with both verbal and spatial intelligence that is capable of handling diverse agentic tasks, we curated a comprehensive pretraining dataset from a wide range of images, videos, and robotics domains.\n\u2022 Robotics manipulation data. For robotics task, we follow OpenVLA [54] and use the robotics dataset of Open-X-Embodiment [22].\n\u2022 UI navigation Data. We exploit two pretraining datasets, SeeClick [19] and Vision2UI [41].\n\u2022 Instructional videos. We compile Epic-Kitchen [24, 25], Ego4d [40], Somethingv2 [37] and other related considering the coarse but rich goal-driven human actions.\n\u2022 Multimodal understanding. Lastly, we include ShareGPT4V [14], instruction tuning data in LLaVA-1.5 [75], and a few other OCR-related datasets [86, 90] to attain image understanding capability.\nWe noticed that many more related datasets could be used for our model pretraining, such as large-scale instruction tuning data [60, 114], more diverse video data [16]. In"}, {"title": "5. Experiment", "content": "We examine the effectiveness of Magma as the foundation model for multmodal agents on UI Navigation tasks in the digital world, the robotic manipulation in the physical world, as well as the generic multimodal understanding."}, {"title": "5.1.1. Zero-Shot Evaluation", "content": "To evaluate the zero-shot transferability of Magma, we employ ScreenSpot [19] and VisualWebBench [79] for evaluating UI action grounding and navigation, and SimplerEnv [65] for robotic manipulation. In addition to these evaluations, we also validate our model on generic [39] and text-rich [110] VQA tasks as well as hallucination benchmark POPE [67]. As shown in Table 2, Magma consistently outperforms all other general-domain LMMs (e.g., LLaVA, Qwen-VL) and domain-specific agentic models such as SeeClick [19] for UI navigation and OpenVLA [54] for robotic manipulation. Notably, the zero-shot performance of Magma on UI is much better than the state-of-the-art vision-based method that uses GPT-4V and Omni-parser [84]. We report the results on two commonly used simulator embodiments in SimplerEnv [65], Bridge and Google Robot including 8 tasks with 172 visual matching and variant aggregation scenarios. Since OpenVLA uses real robot trajectories for pre-training, the model is susceptible to the domain gap for real-to-sim adaptation. In contrast, our Magma model, trained for multimodal understanding and action prediction on a wide range of heterogeneous datasets, is significantly more resilient to the gap and achieves significantly better success rates.\nFig. 8 shows detailed comparisons between our pre-trained Magma model and other representative models. Remarkably, Magma surpasses the second-place OpenVLA by 19.6%, nearly doubling the average success rate. On those challenging tasks such as \"Put Object in Drawer\" and \"Put Carrot on Plate\", Magma achieves a remarkable success rate while most baselines fail entirely. Notably, Magma tuned on our pretrained model showcases substantially better results than the version trained solely on robotic datasets, highlighting the value of spatial intelligence learned from diverse datasets for physical robotic manipulation tasks.\nAblation Studies. We ablate our model pretraining techniques and data mixtures. The results are shown in Table 3. First, we observe from the top three rows that simply combining UI and robotics data does not bring gains, but instead"}, {"title": "5.1.2. Efficient Finetuning", "content": "With moderate finetuning, the pretrained Magma model can be easily transferred to various downstream agentic tasks.\nUI Navigation. Following the prior works [19, 43], we finetune Magma on Mind2Web and AITW, to examine the web and mobile UI navigation capabilities, respectively. For Mind2Web, we first apply the SoM prompting to the training samples according to the top candidates selected by [140], and then finetune Magma on the same samples as in SeeClick [19]. Table 4 shows the results in three sub-tasks, and clearly indicates Magma's superiority to both general-domain and specific-domain LMMs. Similarly, on AITW Magma outperforms the state-of-the-art methods based on open-source or prosperity models. Considering that we use a similar size of LLM and a moderate amount of UI-related pretraining data, this decent performance is largely due to the proposed SoM and ToM modeling techniques, which significantly facilitate action grounding for UI navigation.\nRobotics Manipulation. Table 2 shows that the Magma"}, {"title": "5.2. Evaluating Spatial Reasoning", "content": "We attribute the much improved performance of our Magma model on the tasks of UI navigation and robotic manipulation, as shown above, to its improved ability to perform spatial reasoning. To verify this hypothesis, we evaluate the effectiveness of the spatial intelligence that is learned in our pretrained model on the challenging Visual Spatial Reasoning (VSR) [72], BLINK [33] and SpatialEval [116] benchmarks under the zero-shot setting. The results are summarized in Table 6. We see that Magma outperforms existing approaches by significant margins on VSR and SpatialEval, and that Magma performs on par with CogVLM, despite only using ~29M images for pretraining as compared to ~1.5B images in the latter. In addition, our ablation study demonstrates the effectiveness of the SoM and ToM pretraining tasks in helping Magma improve its spatial reasoning capabilities. Last but not least, we also note the benefits of using video data during pretraining by showing that removing vidoes from training data leads to ~8% performance drop on BLINK. Finally, we also provide some example predictions of our Magma model in Figure 11. We observe that spatial reasoning questions are also challenging for SOTA proprietary models such as GPT-40. Despite the lack of pretraining on data with mazes, we see that Magma is still able to answer spatial reasoning questions about them."}, {"title": "5.3. Evaluating Multimodal Understanding", "content": "Image instruction tuning. To further assess Magma's multimodal understanding capability, we conduct continuous finetuning on our Magma-SFT-820K data. Then, we compare the finetuned Magma model with existing VLMs on a suite of commonly used image reasoning benchmarks, e.g. MME and GQA. As shown in Table 7, Magma outperforms recently-proposed VLMs on most of the tasks, with notable gains of ~5% and ~22% on TextVQA and ChartQA, respectively. Similarly to our observations in Table 6, our ablation study highlights the effectiveness of using SoM and ToM for pre-training, which leads to ~ 5% improvement in ChartQA.\nVideo Instruction Tuning In Table 8, we report the performance of our Magma model on multiple challenging video question answering (QA) benchmarks including IntentQA [62], NextQA [120], VideoMME [32] and MVBench [63]. We use the LMMs-Eval framework [59] for the latter three benchmarks to ensure reproducibility of our evaluation results.\nThe results demonstrate the effectiveness of our pretraining approach, where we outperform most state-of-the-art models with comparable number of parameters consistently across the different benchmarks. For instance, our Magma"}, {"title": "6. Conclusion", "content": "We present the Magma foundation model that can understand and act on multimodal inputs to complete agentic tasks in different environments. Our experiments show that the use of SoM and ToM prediction tasks in pretraining helps the model learn to ground and plan actions, respectively. In our experiments, Magma shows strong spatial-temporal reasoning ability and significantly outperforms baselines on downstream UI navigation and robotic manipulation tasks."}, {"title": "Social Impacts and Limitations", "content": "To develop a foundation model with both verbal and spatial intelligence capable of handling diverse agentic tasks in digital and physical environments, we curated a comprehensive pretraining dataset from a wide range of image, video, and robotics domains:\n\u2022 UI navigation data. We leverage two pretraining datasets SeeClick and Vision2UI.\n\u2022 Instructional videos. As our goal was to learn an agentic model that can undertake daily tasks like humans, we compile the videos from Epic Kitchen, Ego4d, Something-Something v2 and other instructional videos.\n\u2022 Robotics manipulation data. For robotics task, we follow OpenVLA to leverage the robotics data in Open-X-Embodiment.\n\u2022 Multimodal understanding data. Lastly, we include a small set of multi modal pretraining data ShareGPT4V, and instruction tuning data LLaVA-1.5 plus a number of other domain-specific data to retain the generic multimodal understanding capability of the pre-trained model.\nThe data markup of the robotics and UI navigation data is fairly standardized focusing on generic manipulation tasks (\"Place x object on y object\") and generic UI navigation tasks (\"Click search button\"). We, however, performed a detailed data reflection exercise on the video data of people performing certain tasks. The core inferences we took from these videos were the trajectory of objects over time when the tasks were performed.\nWe note that the distribution of identities and activities in the instructional videos are not representative of the global human population and the diversity in society. We are cognizant of the unintended societal, gender, racial and other"}]}