{"title": "AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics", "authors": ["Xiangxiang Dai", "Yuedong Xu", "Zeyu Zhang", "Xutong Liu", "Peng Yang", "John C.S. Lui"], "abstract": "The rapid evolution of multimedia and computer vision technologies requires adaptive visual model deployment strategies to effectively handle diverse tasks and varying environments. This work introduces AxiomVision, a novel framework that can guarantee accuracy by leveraging edge computing to dynamically select the most efficient visual models for video analytics under diverse scenarios. Utilizing a tiered edge-cloud architecture, AxiomVision enables the deployment of a broad spectrum of visual models, from lightweight to complex DNNs, that can be tailored to specific scenarios while considering camera source impacts. In addition, AxiomVision provides three core innovations: (1) a dynamic visual model selection mechanism utilizing continual online learning, (2) an efficient online method that efficiently takes into account the influence of the camera's perspective, and (3) a topology-driven grouping approach that accelerates the model selection process. With rigorous theoretical guarantees, these advancements provide a scalable and effective solution for visual tasks inherent to multimedia systems, such as object detection, classification, and counting. Empirically, AxiomVision achieves a 25.7% improvement in accuracy.", "sections": [{"title": "1 Introduction", "content": "Video analytics plays a pivotal role in a multitude of tasks in a smart city, including vehicle license tracking, facial recognition, and traffic monitoring [24, 41]. This variety of applications highlights the necessity for customized visual models designed to cater to the unique requirements of different visual tasks. Yet, the application of such models, particularly those based on deep neural networks (DNNs), faces formidable challenges. These include the highly diverse requirements of video analytics tasks, fluctuating environmental conditions, and the imperative for real-time operation [6, 7]. The complexity and computational intensity of cutting-edge visual models in multimedia systems further complicate their application in resource-limited settings [40, 74]. Bandwidth constraints, for example, limit the feasibility of transmitting high-resolution video for analysis [76], highlighting a bottleneck in the practical utility of these complex technologies.\nThe first challenge in video analytics centers on which visual model to apply that caters to application-specific requirements under a dynamic environment, especially in light of phenomena like data drift [5, 9], where live video data's characteristics stray from the training dataset, as demonstrated in our Section 2. Despite a wide range of advancements targeting various specific requirements, efforts to modify existing models or customize visual models remain predominantly focused on the static and single scenario. For instance, works such as model pruning and compression [19, 29, 54], while effective at streamlining complex models for resource-limited environments, face significant performance degradation under adverse conditions like poor lighting or extreme weather [21, 30]. Given the above analysis, we advocate for the strategic combination of existing models to navigate the complexities of real-world scenarios, instead of solely relying on a singular model or the pursuit of a one-size-fits-all visual model for universal video analytics.\nThe complexity further escalates with the second challenge, as the video analytic system transitions from single-camera sources to multi-camera feeds. Independent decision-making for each camera regarding model selection would cause the computational load to increase linearly, which is unsustainable and retards the model selection process. Strategies such as the \u201cfollow-the-leader\u201d [27], Spatula's camera correlation prioritization [26], and the CrossRoI system's re-identification algorithm [17] offer potential solutions for managing multi-camera setups. Nevertheless, these offline, periodically preset camera groupings, which depend exclusively on static search clustering and similar approaches, lack the essential flexibility needed to adapt to the dynamic nature of real-world environments and the process of continual learning for improvement.\nMoreover, the significance of camera deployment in video analytics has been largely overlooked, with attention primarily focused on enhancing visual models on the server side, whether in the context of automated surveillance or user-controlled VR cameras. Nevertheless, for specialized tasks such as the creation of holographic stereogram portraits [13, 34], it is essential to investigate the additional effects arising from different camera perspectives,"}, {"title": "2 Background and Motivation", "content": "We start with a discussion on related works. Then we present our experimental results to illustrate how visual models' performance varies under external environmental conditions and across different visual tasks, underscoring the necessity of dynamic adaptation. Finally, we present the effects of the camera perspective."}, {"title": "2.1 Related Work", "content": "In video analytics, video frames are continuously streamed from one or more cameras to servers for processing. This often involves multiple visual models to support a multitude of video analytics applications, particularly those based on diverse architectures and weights of DNNs enabling them to accommodate an extensive array of scenes and vision tasks [17, 52]. However, the growing complexity of DNN architectures has resulted in increased prediction latency, presenting a considerable challenge for resource-limited end devices [38, 60]. To handle this, significant efforts have been devoted to leveraging lightweight models with streamlined architectures and fewer parameters [19, 73]. Yet, despite their efficiency under specific conditions or in tailored environments, these lightweight models often fall short in dynamic object distributions or challenging environmental conditions. Furthermore, the high costs of dynamically retraining model methods [5, 30] for specific scenarios make real-time maintenance challenging under changing conditions [20]. Faced with this, our method focuses on how to adapt to dynamic environments through the prioritization of continuous online visual model selection, moving away from the reliance on a few static models. More importantly, we demonstrate the importance of \u201ccamera perspectives\u201d on the model selection.\nRegarding model selection, we implement the strategy derived from multi-armed bandit (MAB), which performs online section of one or more options from a set of alternatives based on feedback from previous choices [33]. MAB has been widely applied in various domains, including recommendation systems [10, 22], content delivery [45, 68], and DNN design [69, 71]. Although some previous works on clustering bandits have explored grouping human users [16, 37, 43, 64], its application in the intricate domain of machine-centric video analytics remains under-explored, which uniquely focuses on maximizing inference accuracy and handling issues such as frame drops, provided the analytics' integrity is maintained [72]."}, {"title": "2.2 Motivation Experiments", "content": "To investigate the variability of various models across different scenarios and the impact of camera perspectives, we conduct a thorough comparative analysis encompassing a range of models such as the YOLOv5 series [28], RTM detection [49], SSD [42], and Faster R-CNN [53]. In particular, object detection and semantic segmentation are selected as the visual tasks in our experiments.\nDataset. To assess the performance of visual models under different environmental conditions, we compile five representative video datasets, each covering a specific real-world scenario. These datasets are sourced from publicly available videos on YouTube, identified by searching for specific keywords (e.g., \u201clive stream webcams\u201d) and selecting those pertinent to traffic conditions. To explore the effects of varying camera perspectives, four videos with distinct perspectives are selected at the same time from the above sources [55-58]. Furthermore, the free-viewpoint videos are also utilized for evaluating perspectives, where videos of various human movements on an indoor stage are recorded using 12 cameras positioned at equal perspective intervals [18].\nEvaluation Metrics. For object detection, performance is assessed using the recall and F1 score metrics. We run the YOLOv5-x model as the ground truth, with the detection confidence score set to 0.25, and the intersection over the union threshold for calculating recall set at 0.5 aligning with [14, 70]. For semantic segmentation, pixel accuracy is applied as the metric, and the complex PP-HumanSegV1-Server [46] model is run as the ground truth [47].\nPerformance Variability of Visual Models. Fig. 1(a) reveals image examples across daytime, nighttime, snowy, and dusky scenarios. The F1 scores of YOLOv5-n demonstrate fluctuations across these four environments. In Fig. 1(b), a 135-second video depicting a sudden heavy snowfall is analyzed, revealing a decline in performance across all models as the intensity of the snowfall increases. The images on T = 20s and T = 100s show a clear difference before and after the snowfall. Figs. 1(a) and 1(b) effectively illustrate our argument with extensive examples across diverse environments and utilizing various visual models: a single universal model faces significant challenges when attempting to perform consistently in dynamic environments. The models show performance fluctuations of varying magnitudes depending on the environmental conditions.\nFurthermore, we pre-train the YOLOv5-s model on a snowy-day traffic road dataset[51] using four different learning rates and training parameters under 100 epochs. we apply these models to a scenario where snowfall gradually begins at 30 seconds and intensifies by 45 seconds. As illustrated in Fig. 1(c), models pre-trained for snowy conditions show improved performance as the snowfall increases. However, their performance under normal weather conditions is inferior to that of the standard YOLOv5-s model, likely due to overtraining on snowy data. In Fig. 1(d), we further pre-train models on the COCO dataset for 100 epochs under various lighting conditions and evaluate them using the same YouTube dataset. Although models specifically pre-trained for certain environments show enhanced performance, performance fluctuations also exist (the second row). Moreover, the challenge of accurately quantifying light levels in dynamic real-world environments complicates the direct matching of these conditions with an appropriate visual model. This underscores the complexity involved in dynamically adapting visual models to suit changing environmental conditions.\nImpact of Camera Perspective. We now turn our attention to the significant effects of camera perspective on model selection. Initially, we evaluate the F1 scores for object detection tasks using videos taken from different perspectives at the same traffic intersection at the same time. Fig. 2(a) reveals that the F1 scores significantly vary with the camera's viewpoint. For instance, a direct frontal view achieves an F1 score of 0.85, which drops to 0.61 when the camera is positioned laterally. Furthermore, for the semantic segmentation task, we explore the effects of camera perspective through multi-angle videos of activities such as dancing, playing badminton, and throwing a frisbee in an indoor environment, with a specific example of dancing showcased in Fig. 3. According to Fig. 2(b), a frontal capture of a dancer yields a segmentation accuracy of 0.81, which surprisingly increases to 0.87 from a side angle. Additional analysis of two other movements shows similar patterns of accuracy variation. The intrinsic relationship between perspective and model selection lies in the fact that certain perspectives may pose challenges for a task, e.g., distant blurred perspective in object recognition, requiring the use of more sophisticated models, whereas other perspectives can be addressed using simpler visual models (Another example is in Appendix B). Therefore, the influence of perspective on model selection is a crucial factor that must be considered."}, {"title": "3 Model and Problem Formulation", "content": "In this section, we present the system model and the problem setting of our proposed AxiomVision framework. Our framework focuses on the adaptive selection of visual models under dynamic environmental changes and specific task demands, which utilizes a hierarchical architecture for camera groups, edge nodes, and cloud resources, and a continual video analytics pipeline to enhance visual task performance. Fig. 4 depicts our overall framework design."}, {"title": "3.1 System Model", "content": "Tiered Edge-Cloud Architecture. Traditional methods typically put all visual tasks, denoted as $Q$, to the cloud for centralized processing, which results in higher loads and longer response times on the cloud servers. To address these issues and also for the deployment of multiple visual models, we propose a hierarchical architecture, which consists of three levels: (a) the video server, (b) the edge nodes, and (c) the end camera groups. AxiomVision introduces a tailored combination set $M_q$ of visual model models for each distinct visual task $q\\in Q$ to better accommodate external variables such as lighting conditions and movement dynamics. Taking the object detection task as an example, lightweight models such as MobileNet [23] are implemented on the resource-limited edge nodes [77], while the deployment of more sophisticated models is designated for the cloud center. The total set of cameras in our system is represented by N with cardinality |N|.\nOnline Video Analytics Pipeline. Our system is designed to adeptly handle the non-continuous and varied visual tasks through a sequential, discrete-time round approach. To accommodate the varied nature and frequency of task demands, we define the sequence of rounds for each visual task $q \\in Q$ as $T_q$. For a specific visual task like facial temperature recognition, the duration between consecutive rounds may differ due to customer flow rates. Similar to [67], we assume that individual tasks do not interfere with one another, permitting each to operate in its designated round independently. Within this, the operational cycle of the online video analytics pipeline at each round involves the scheduling agent selecting visual models, obtaining feedback on their outcomes, and subsequently updating the evaluations for these models.\nVisual Payoff Feedback. To dynamically adapt to evolving conditions and task-specific demands, we propose a visual payoff feedback mechanism, where payoff can be interpreted as the likelihood of meeting certain criteria, such as accuracy, recall, or F1 score. This process mainly focuses on the adjustment of perspective weights $\\Theta_n$ for each camera $n \\in N$ in response to task feedback of the selected visual model, thus enabling the online optimization of visual model efficacy.\u00b9 For any given task $q \\in Q$, the expected payoff $r_{m_t,t}$ of the selected visual model $m_t$ is expressed as:\n$\\mathbb{E}[r_{m_t,t}|m_t] = \\rho(x_m, \\Theta_n),$                                                                                                 (1)\nwhere $\\rho$ establishes a nonlinear connection between the payoff $r_{m_t,t}$ and the feature vector $x_m$, of the visual model $m_t$ at round $t$, incorporating the influence of the camera perspective through the weight vector $\\Theta_n$ [15, 31]. An example of such a link function is a neural network, wherein a final layer equipped with either a sigmoid or ReLU activation function transforms the intricate features derived into meaningful results [32, 66].\nCombinatorial Model Selection. Unlike conventional works that may only offer a single model choice per task, we develop a \u201ccombinatorial model selection strategy\u201d from a wide range of model candidates, which increases the probability of meeting the task's requirements under diverse environmental conditions. Specifically, a set of visual model options $M_t = \\{m_1,...,m_{|K_t|}\\}\\subseteq M_q$ is presented for each task q at every round t. $K_t$ with the size of $|K_t|$ represents the selected visual model index at round t, determined by the first selected model with $r_t = 1$. Here, if the accuracy of the selected visual model surpasses the predefined threshold, the resulting payoff value $r_t$ will be assigned a value of 1. Initially, to conserve resources and ensure rapid response, priority is given to models deployed at the edge. If these models fall short of the task's required accuracy threshold, the scheduling agent will choose more complex models in the cloud. More importantly, the selection of visual models is continually refined based on payoff feedback. Thus, at each round t, the aggregate payoff from these combinatorial usages of visual models $M_t$ for camera n is calculated as:\n$R(M_{n,t}) = 1 - \\prod_{k=1}^{K_t} (1 - r_{m_{k,t,t}}(m_{k,t})), \\text{ where } k \\in K_t, m_{k,t} \\in M_t.$", "(2) represents the condition for optimality, where the cumulative discrepancy between the predicted payoff and actual payoff sums to zero, as estimated by the membership indicator of cameras within the same group it. We use Newton's method which allows for efficient computation of the solution [15, 37]. The historical feedback data from all cameras in the same group, not just camera nt, are used to update the estimation, emphasizing the value of grouping for process acceleration. For the cases where the exact value of $u$ is unknown, the process can be reduced to an estimation process using mitt (The subsequent visual model selection component following the same method).\nSelecting Visual Model with Optimistic Approach. Following group assigning and estimation, the currently processed camera undergoes optimization and reward feedback observations on the": "(2)\nis where $\\gamma_q$ signifies a predetermined positive dispersion constant specific to task q. Based on the above criterion, the collective set of cameras N can be divisible into smaller subsets, and we label them as $G_1, G_2, ..., G_g$, wherein cameras within the same subset adhere to a unified visual model selection strategy. Note that neither which camera belongs to which group, nor the precise number of grouped cameras g can be not known beforehand in our model."}, {"title": "3.2 Problem Formulation", "content": "The selection of an optimal visual model is influenced by the diverse visual tasks, external environment, and internal deployment factors, namely camera perspectives. As a result, the scheduling agent must continually adapt the choice of model $m_t \\in M$ for each processed camera $n_t \\in N$ at the round $t \\in T_q$ for visual task q, in alignment with the video analytics payoff. In this work, we propose to dynamically adapt visual models in an online manner to maximize the overall payoff across all rounds for any given visual task $q \\in Q$. This objective is mathematically expressed as:\n$\\max_{L} \\mathbb{E} \\sum_{t \\in T} \\sum_{n \\in N} 1\\{n_t=n\\} R(M_{n,t})$                                   (3)\nwhere 1{} denotes the indicator function. Given the limited bandwidth and computing resources, it is not possible to select the most resource-intensive visual model option [27]. Furthermore, with the growing deployment of extensive camera networks by various entities, an exploration into the impact of perspective similarity among cameras is needed. By analyzing these perspective weight similarities, we aim to identify and group cameras with similar perspective influences, thereby reducing the number of subgroups needed for visual model sharing. Nevertheless, this endeavor introduces several challenges, including complex search spaces, varying effects of camera perspectives, zero prior knowledge of optimal visual models, and the strategic deployment of camera groups. Addressing these issues requires an adaptive algorithm capable of learning and adjusting visual models online for diverse visual tasks, transcending the limitations of static, offline model selection strategies."}, {"title": "4 Continual Learning of Axiom Vision", "content": "In this section, we first present the algorithm design of AxiomVision, followed by a performance analysis. Specifically, a flexible graph-based structure is utilized to mirror the natural undirected connectivity found in camera cluster networks.\u00b2"}, {"title": "4.1 Algorithm Design", "content": "Assigning Inferred Groups for Processed Cameras. Initially, we employ some common clustering methods, e.g., K-means, to group cameras with similar perspective impact weights. Within each cluster, a fully connected graph is first established, reflecting\n\u00b2For brevity, we focus on the algorithmic procedures for a singular visual task q \u2208 Q,\nnoting that the procedures can be executed in parallel for multiple visual tasks Q."}, {"title": "Algorithm 1 Continual Online Learning of Axiom Vision", "content": "Require: Set of cameras N; Parameter \u03b1, \u03b2; Random p\u2080 \u2208 (0, 1).\nEnsure: Visual model selection for all visual tasks.\n1: Initialization: A complete graph U\u2080= (N, E\u2080); g\u2081 = 1; T\u2099,\u2080 =\n0, \u2200n \u2208 N.\n2: for each q \u2208 Q, t \u2208 Tq, independently do\n3: Receive processed camera index nt;\n4: Identify group G\u1d62, that contains nt;\n5: Estimate perspective weight \u0124\u1d62,\u209c\u208b\u2081 based on Eq. (4);\n6: Select the combinotorial model set M\u209c \u2208 Mq according\n7: to Eq. (5) until the predetermined threshold is satisfied;\n8: Record payoff rmk,t of the selected visual model mk, k \u2208 Kt;\n9: Increment count of processed camera nt: Tnt,t+1 = Tnt,t + 1;\n10: Delete from Et all (nt, l) if Eq. (6) holds and get the resulting\n11: graph Ut+1 = (N, Et+1);\n12: Update graph parameter: pt = Po/t\u00b2;\n13: Reconnect all edges in Et+1 with probability pt;\n14: end for"}, {"title": "4.2 Performance Analysis", "content": "For the ease of presenting our theoretical analysis, let ||xm\u209c ||\u2082 \u2264 1\nand ||\u0398\u2099||\u2082 \u2264 1, mt \u2208 Mq for all rounds. At each round t, a camera\nis randomly processed from N with uniform probability for fairness,\nindependently of selections in previous rounds. For every visual\ntask q \u2208 Q, we evaluate our algorithms by measuring the greatest\ndifference in payoff between the theoretically ideal visual model\n(not known beforehand) and the visual model actually chosen. This\ndifference is defined as \u201cregret\u201d [33, 44], expressed as:\n$Reg(T_q) = \\mathbb{E} [ \\sum_{t \\in T_q} \\sum_{n \\in N, n_t=n} R(M^*_n) - R(M^*_{n,t})],$                     (7)\nwhere Tq denotes the cardinality of Tq and $M^*_n$ denotes the \u201cunknown\u201d optimal combinatorial set of visual models for task q.\nIn line with [16, 37, 43], we posit that $\\mathbb{E} [x_m, x_{m\u209c}]$ is full rank,\nwith a minimum eigenvalue \u03bb > 0, and that $x_m, \\Theta_n$ exhibits a\nsub-Gaussian tail with a variance not exceeding \u03c3\u00b2. Furthermore,\nfollowing [15, 37], we consider u to be a strictly increasing, continuously differentiable link function that is Lipschitz continuous with constant L. We denote m\u00b5 = infa\u2208[\u22122,2] \u03bc' (a) and assume m\u00b5 > 0.\n(\u03bb\u2212x)\u00b2\nDefining as the integral $ \u222b\u221e_0 (1 \u2212 e2\u03c3\u00b2 ) Kd\u03ba dx with K indicating the maximum number of selected combinatorial visual models across all rounds [63], we set the tuning parameters \u03b1 and \u03b2 as follows: \u03b1 = + dln(T/d) + 2ln(4gT) and $\u03b2 = \u221a 32d/(m\u00b2_\u00b5)$, where d and g represent the dimension of the vector and the maximum number of camera groups under all adjustable perspectives, respectively. Then, we give the following performance guarantee.\nTHEOREM 1 (REGRET UPPER BOUND). The regret of AxiomVision throughout Tq is bounded by $Reg(T_q) \u2264 O ( \\frac{Ld}{m\u00b5\\sqrt \u03bb} (\u221agKTqln(Tq)))$", "(8)be the regularized Gramian matrix, and the frequency associated with belonging group Gi, respectively, incorporating the regularization parameter ( > 0 up to round t.\nConsider the gradient function defined for any camera within group Gi, at time t as:\n$\\begin{aligned} g_{i_t,t} (\\theta) = &  \\sum_{j=1}^t \\mathbb{I}\\{n_j \\in G_{i_t}\\} \\sum_{k=1}^{K_j} u(x_{m_{k,j}}^T \\theta) x_{m_{k,j}} \\end{aligned}$           \nRecall that \u0124\u1d62\u209c ,\u209c is identified as the unique solution of the equation:\n$\\sum_{j=1}^t  \\mathbb{I}\\{n_j \\in G_{i_t}\\}  \\sum_{k=1}^{K_j} (r_{m_{k,j}} - u(x_{m_{k,j}}^T \\theta)) x_{m_{k,j}} = 0 .$         \nThen, it is possible to express gir,t(\u0124it,t\u22121), which captures the cumulative response adjusted by the previous estimate of 0, as follows: git,t(\u0124it,t\u22121) = \u2211=1 1{nj \u20ac Gi\u2081} \u2211k=1 'mk,jxmk,j. This formulation integrates the feedback up to round t \u2013 1, weighted by the membership of the cameras in the group Gi\u2081, to refine the estimation of the parameter 0.\nNext, we introduce a lemma that provides a theoretical guarantee for the accuracy of the ridge regression estimate in approximating the true weight vector of camera perspective influence. This lemma is critical for understanding the bounds of estimation error in linear models with ridge regression.": "(10)y setting 8 = 1/T, with probability at least 1-1/T, for each camera nt with the selected visual model mt under query q, the following holds:"}, {"title": "5 Performance Evaluation", "content": "5.1 Implementation and Setup\nTestbed. Leveraging public 360\u00b0 VR camera feeds from [1-4], our setup involves NVIDIA Jetson TX2, Nano, and TX2 NX end devices handling |N| = 308 video segments from different perspectives, with ENs powered by NVIDIA GeForce RTX 4060 and HPC Dell PowerEdge R930 servers as the cloud center. Rectilinear images are extracted from panorama to function as adjustable perspectives [35]. Beyond the DNN model used in Section 2, we employ the lightweight YOLOv5-s, trained on the COCO dataset under diverse lighting, finally containing a total of 17 optional visual models. Visual tasks, in line with [39, 65], include Classification, Counting, Detection, and Aggregation. Utilizing approaches from [36, 43, 66], we construct and decompose a performance payoff matrix for these tasks across all video segments, extracting feature vectors for visual model index representation. Camera bandwidth varies between 1 and 2 Mb/s, with EN to server uplink around 10 Mb/s [24, 40].\nMetrics. We evaluate the following performance metrics: (a) Accuracy: Assessed for the four visual tasks outlined in [39, 65]. (b) Round: As described in Section 3. (c) Regret: Detailed in Eq. (7). (d) Time: Encompasses the algorithm's execution time, visual model inference time (e.g., YOLOv5-s), and initial transmission time. Notably, transmission and analysis are not sequential; initial data upload incurs a startup latency, followed by continuous and parallel transmission and analysis. (e) Bandwidth: Normalized bandwidth usage for transmitting encoded video segments. Through both theoretical and empirical analysis, Axiom Vision parameters are set to (\u03b1, \u03b2) = (0.25, 0.1). Note that we periodically run YOLOV5-x to acquire true bounding boxes for accuracy assessment (included in the total consumption measurements), and additional extended experiments can be found in Appendix B."}, {"title": "5.2 In-depth Analysis of Exploring Results", "content": "In pursuit of evaluating the effectiveness and rationale of certain components within our Axiom Vision design, we conduct a comprehensive series of experiments under the object detection task.\nPerspective Effects. To underscore the importance of camera perspective alongside server-side models, we design an Axiom Vision variant without perspective consideration, w/o Perspective, and compared it with a version integrating camera perspective, w/ Perspective, plus a greedy method providing fixed-perspective optimal model across all feeds. As depicted in Fig. 5, results show w/ Perspective improves mean accuracy by 2.7% over w/o Perspective. Moreover, both w/o Perspective and w/ Perspective by facilitating online model selection, surpass the greedy strategy by 2.3% and 5.6% in accuracy.\nDeleting Function Evaluation. Based on [48, 50], we assess various deleting functions: $f_1(x) = \\frac{1+\\ln(1+x)}{1+x}$ (ours), $f_2(x) = \\frac{1}{\\sqrt{1+x}}$, $f_3(x) = \\frac{1}{1+x}$, $f_4(x) = \\frac{1}{(1+x)^2}$, $f_5(x) = 1 + \\ln(1 + x)$, and $f_6(x) = \\sqrt{1 + \\ln(1 + x)}$. We evaluate the regret incurred by different functions at 15, 50, 200, and 850 rounds for each camera, with the optimal strategy determined through the YOLOv5-x model. Fig. 6 shows that the function consistently delivers optimal performance across various rounds with minimal regret.\nGrouping Impact on Acceleration. Exploring the effect of camera grouping on acceleration within the AxiomVision framework, we compare performances between ungrouped (w/o Grouping) and grouped (w/ Grouping) setups, as illustrated in Fig. 7. By setting accuracy thresholds from 0.8 to 0.87, w/ Grouping significantly reduces the total number of rounds across all cameras by at least 1.27x, achieving an average acceleration of 3.23x and a median of 2.18\u00d7. Additionally, we observe that increasing the number of cameras leads to a reduction in the required rounds while achieving a similar level of accuracy, as shown in Fig. 8.\nCombinatorial Set Benefits. Referring to Section 3, we address dynamic accuracy needs by assembling a combinatorial set of visual models for selection, as illustrated in Fig. 9. Highlighting"}, {"title": "5.3 Benchmarking against State-of-the-Art", "content": "Benchmarks. Our comparison includes the following schemes. (1) Chameleon, capable of dynamically selecting the visual model based on temporal and spatial correlations [27]. (2) Dual-MS, inspired by [14], categorizes visual models into two layers: a simpler model and a more complex model, for effective model selection. (3) EAMU, standing for edge-assisted on model update in adverse environments [30]. (4) Greedy, which, during the initial short segment of analysis for all video sources offline, runs all models to identify the one offering the highest average accuracy.\nAccuracy on Fixed & Adjustable Perspectives. The average accuracy of 2000 rounds is illustrated in Fig. 11. Across different visual tasks for fixed perspectives, AxiomVision, which employs a continuous online model selection, consistently outperforms EAMU, Chameleon, Dual-MS and Greedy. Furthermore, under adjustable perspectives, EAMU and Chameleon experience a deterioration in accuracy due to their lack of consideration about the impact of source-side camera perspectives.\nDecomposition of Total Time. We compare our approach under Tq = 2000, \u2200q \u2208 Q with Chameleon (setting its parameter interval = 5 and top-k = 5 for re-profiling video pipelines, referred to as execution time), and EAMU (calculating its average training cost for retraining, also denoted as execution time). Fig. 12 indicate that although our method leads to an increase in inference time due to the adoption of a combinatorial design, it efficiently reduces execution time by eliminating the need for the re-profiling in Chameleon and the retraining process in EAMU. In comparison, Chameleon allocates nearly identical time for spatial-temporal profiling; EAMU incurs a significant additional time cost due to its retraining process. Moreover, the initiation time for transmission is markedly the smallest in scale under 200 kbps bandwidth constraint.\nImpact of Bandwidth Condition. In Fig. 13, we benchmark our methodology against Chameleon across the above tasks (abbreviated as q1, q2, q3, q4). EAMU is omitted owing to its retraining architecture, which diverges from the context of bandwidth. The outcomes demonstrate that the strategic approach of Axiom Vision, which involves selectively deploying complex models for tasks where accuracy is compromised, significantly boosts performance across all visual tasks. This advantage becomes particularly prominent in scenarios of limited bandwidth, underscoring our method's efficiency in bandwidth-restricted video analytics."}, {"title": "6 Conclusion", "content": "We propose Axiom Vision, an innovative framework guaranteeing performance for a wide range of environments and visual tasks. AxiomVision leverages dynamic model selection and a tiered edge-cloud architecture. With experiments based on extensive real-world camera videos, AxiomVision introduces a novel approach to consider camera perspective and unveils a group-based acceleration strategy that capitalizes on camera cluster topology. Furthermore, Axiom Vision is designed with a theoretical performance guarantee even under the worst-case scenarios, that is, AxiomVision can asymptotically converge to the optimal model section policy. Tested on a built platform, AxiomVision demonstrates superior performance over existing works, and greatly improves adaptability and efficiency across various video analytics applications."}, {"title": "A Proof of Theorem 1", "content": "PROOF. For any processed camera n \u2208 N, we define the Gramian matrix and the number of effective feedbacks for camera n up to round t, respectively, as follows:\n$\\begin{aligned} M_{n,t} = \\sum_{j \\leq t} \\sum_{\\substack{n_j=n}} x_{m_{k,j}}x_{m_{k,j}}^T,  I_{n,t} = \\sum_{j \\leq t} \\sum_{\\substack{n_j=n}} K_j \\end{aligned}$                                \nSubsequently, for any camera n belonging to group index i, denote\n$\\begin{aligned} M_{i,t} = \\zeta I + \\sum_{n \\in G_i} M_{n,t}, T_{i,t} = \\sum_{n \\in G_i} T_{n,t} \\end{aligned}$           \n(8)\nbe the regularized Gramian matrix, and the frequency associated with belonging group Gi, respectively, incorporating the regularization parameter ( > 0 up to round t.\nConsider the gradient function defined for any camera within group Gi, at time t as:\n$\\begin{aligned} g_{i_t,t} (\\theta) = &  \\sum_{j=1}^t \\mathbb{I}\\{n_j \\in G_{i_t}\\} \\sum_{k=1}^{K_j} u(x_{m_{k,j}}^T \\theta) x_{m_{k,j}} \\end{aligned}$           \nRecall that \u0124\u1d62\u209c ,\u209c is identified as the unique solution of the equation:\n$\\begin{aligned} \\sum_{j=1}^t  \\mathbb{I}\\{n_j \\in G_{i_t}\\}  \\sum_{k=1}^{K_j} (r_{m_{k,j}} - u(x_{m_{k,j}}^T \\theta)) x_{m_{k,j}} = 0 . \\end{aligned}$         \nThen, it is possible to express gir,t(\u0124it,t\u22121), which captures the cumulative response adjusted by the previous estimate of 0, as follows: git,t(\u0124it,t\u22121) = \u2211=1 1{nj \u20ac Gi\u2081} \u2211k=1 'mk,jxmk,j. This formulation integrates the feedback up to round t \u2013 1, weighted by the membership of the cameras in the group Gi\u2081, to refine the estimation of the parameter 0.\nNext, we introduce a lemma that provides a theoretical guarantee for the accuracy of the ridge regression estimate in approximating the true weight vector of camera perspective influence. This lemma is critical for understanding the bounds of estimation error in linear models with ridge regression."}, {"title": "B.1 Effect of Camera Angles on Model Selection", "content": "Fig. 14 demonstrates how the perception of the same object changes from different camera angles. Camera A, positioned at a greater distance and higher angle, provides an overview perspective, potentially altering the perception of size and shape due to increased distance and angle, thereby necessitating a more complex object detection algorithm. On the other hand"}]}