{"title": "Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis", "authors": ["Francesco Sovrano", "Michael Lognoul", "Giulia Vilone"], "abstract": "Significant investment and development have gone into integrating Artificial Intelligence (AI) in medical and healthcare applications, leading to advanced control systems in medical technology. However, the opacity of AI systems raises concerns about essential characteristics needed in such sensitive applications, like transparency and trustworthiness. Our study addresses these concerns by investigating a process for selecting the most adequate Explainable AI (XAI) methods to comply with the explanation requirements of key EU regulations in the context of smart bioelectronics for medical devices. The adopted methodology starts with categorising smart devices by their control mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving into their technology. Then, we analyse these regulations to define their explainability requirements for the various devices and related goals. Simultaneously, we classify XAI methods by their explanatory objectives. This allows for matching legal explainability requirements with XAI explanatory goals and determining the suitable XAI algorithms for achieving them. Our findings provide a nuanced understanding of which XAI algorithms align better with EU regulations for different types of medical devices. We demonstrate this through practical case studies on different neural implants, from chronic disease management to advanced prosthetics. This study fills a crucial gap in aligning XAI applications in bioelectronics with stringent provisions of EU regulations. It provides a practical framework for developers and researchers, ensuring their AI innovations advance healthcare technology and adhere to legal and ethical standards.", "sections": [{"title": "1 Introduction", "content": "The 2023 Artificial Intelligence (AI) Index Report by Stanford University reveals that medical and healthcare applications represent one of the largest investment areas in AI (nearly 6 billion USD). Incorporating AI into smart bioelectronics for medical devices represents a tremendous leap in medical technology. This integration has resulted in substantial improvements in patient care, primarily by developing advanced control systems that can adapt in real-time to patient needs, thereby greatly enhancing the effectiveness of treatments and quality of life [22]. A key evolution in medical technology is the shift from open-loop systems, where physicians interpret data to inform decisions, to more sophisticated closed-loop and semi-closed-loop systems, where devices autonomously or semi-autonomously adjust their operations based on continuous monitoring. A significant challenge with advanced AI systems is their 'black-box' nature, which makes it hard to understand how they make decisions [5]. This challenge is critical in healthcare, where AI systems must be accurate, transparent, and accountable to enhance trust in their users and enforce responsibility [5]. This is where Explainable Artificial Intelligence (XAI) plays a crucial role, offering tools to make the inner workings of these complex systems more understandable to the diverse stakeholders involved in their operation [48]. Regulatory frameworks, particularly in the EU, implicitly require XAI to ensure AI technologies' transparency, fairness, and accountability, as emphasised in various scholarly works [52]. EU regulations are especially stringent regarding smart bioelectronics for medical devices. These devices must comply with the General Data Protection Regulation (GDPR) [9], the Artificial Intelligence Act (AIA) [11], and the Medical Devices Regulation (MDR) [10], each contributing unique requirements related to explainability. However, navigating the complex regulatory landscape poses significant challenges for developers and researchers. Implementing XAI algorithms in line with EU regulations is a major hurdle, accentuating a disconnect between theory and practice in this field [44, 30]. The motivation for our study stems from this very challenge. We carried out a thorough analysis of various XAI algorithms to determine if they can help satisfy explainability requirements set by the GDPR, the AIA, and the MDR. To this end, we"}, {"title": "2 Related Work", "content": "Integrating XAI tools into compliance processes to match explanation requirements, as contained in diverse fields of law, is still an open, multi-faceted and multi-disciplinary challenge. One of the significant stumbling blocks discussed by Richmond et al. [44] is harmonising the logic followed by AI algorithms with legal reasoning and legal requirements to provide reasons or explanations. Previous works delved into the legal and ethical requirements for explainability in Machine Learning (ML) [6], particularly in the context of the GDPR [15], the AIA [52], or other EU regulations related to sensitive fields such as finance [54]. However, our study stands out in its comprehensive and practical approach, as these different studies are not oriented towards offering a methodology to find the right XAI tools. Bibal et al. [6] investigated the increasing legal requirements for AI explainability in private and public decision-making contexts. They emphasised the implementation of these requirements in ML models and advocated for interdisciplinary research in explainability. We went one step further by providing the kind of interdisciplinary research and methodology they suggested. Similarly to Hashemi [25], which proposes a strategy for choosing the proper XAI method for specific goals, we reviewed the XAI research to provide a synopsis of recent XAI methods and their characteristics that make them suitable candidates for healthcare. In contrast, our research carried forward by explicitly mapping XAI methods to the regulatory requirements of the GDPR, AIA, and MDR in the context of smart bioelectronics for medical devices. Our work delves into the obligations discussed at a general level by Bibal et al. [6] and translates them into operational and practical terms through focused case studies. Although there are currently no explicit mandates for the use of XAI systems, as noted by Ebers [15] in their analyses of the GDPR, our research echoes Schneeberger et al. [46] in emphasising the crucial role of state-of-the-art XAI for ensuring compliance with various legal texts applicable in the medical sector, for instance, in protection of patient's sensitive data. However, differently from Schneeberger et al. [46], which provides an overview of the EU's legal approach to AI in the medical sector, our study goes beyond the general legislative landscape to perform a detailed analysis of specific XAI methods and their potential use for compliance with these regulations, focusing on applications for smart biomedical devices. Additionally, our study distinguishes itself from G\u00f3rski and Ramakrishna [20] by focusing on the medical field, using a broader array of XAI algorithms and systems, and conducting an extensive qualitative analysis of legal requirements, unlike their focus on the accuracy of explainability methods like Grad-CAM, LIME, and SHAP in legal text classifications as assessed by legal professionals."}, {"title": "3 Methodology", "content": "This research aims to bridge the gap between the technical capabilities of XAI and the legal requirements set forth by key EU regulations within the domain of smart bioelectronics for medical devices: GDPR, AI Act, and MDR. Our multifaceted methodology combines legal analysis with technical assessment and classification of XAI algorithms to increase regulatory compliance. As shown in Figure 1, our methodology comprises the following steps. Adoption of Explanation and Explainability Definitions. To map XAI methods with legal explanatory requirements, we needed to select an appropriate definition of explanation. We adopted the definition formalised by Sovrano and Vitali [50], which conceptualises explanations as answers to questions that produce understanding. Among the five main definitions in contemporary philosophy, this one, rooted in Ordinary Language Philosophy, is found to align best with the legal interpretation of explanations [49, 52]. According to this definition, an explanation provides sufficient information for an audience's understanding. This differs from other definitions that require an explanation tailored to someone's mental model or showing causal relationships. Indeed, in the legal context, explanations do not necessarily need to be fully personalised [55] and can encompass more than just causality [6, 51]. Legal Analysis of Explanation Requirements. A legal expert (the 2nd author of this paper) thoroughly analysed the GDPR, AI Act, and MDR to pinpoint their explanation requirements and characteristics. Then, following an inductive coding approach [17], the legal expert identified the high-level explanatory goals underlying these requirements, e.g., ensuring that systems' deployers understand risks related to the use of an AI system or can interpret a system's output, guaranteeing that outputs can be reviewed or contested, etc. Identification and Classification of XAI Methods. Concurrently, two AI experts (the 1st and 3rd author of this paper) conducted in three phases a literature review to compile a comprehensive (but not exhaustive) list of existing XAI methods. Initially, the search query \"XAI survey\" was used on Google Scholar, targeting relevant publications in top journals from 2023. Subsequently, the research scope was broadened to incorporate insights from the XAI survey of Vilone and Longo [53], to ensure a more comprehensive synopsis. Finally, the list of XAI algorithms was integrated with algorithms known to the experts but not mentioned in the surveyed literature. These algorithms were categorised based on their explanation format, input format, and model-agnostic status to discern the types of explanatory questions they could address, such as \"what happens if feature X is changed\" or \"what is the contribution of feature Y to the output\". We used a question-driven design process similar to that of Liao et al. [34], in which XAI methods are mapped to explanatory questions based on their characteristics. Differently from Liao et al. [34], our mapping did not involve only interrogative particles (e.g., why, how), but we formulated complete questions (see Table 5) via an inductive coding approach [17], allowing the questions to emerge naturally from the characteristics of the XAI methods. Aligning XAI Methods with Legal Requirements. By performing a deductive thematic analysis [17], we mapped the XAI questions to the legal explanatory goals enshrined in the GDPR, AIA, and MDR. This was possible because the adopted definition of explanation is framing explanations as answers to questions. Eventually, we could identify congruence where XAI capabilities can be exploited to help meet the stipulated legal explanation requirements. This matching process ensures that the selection of XAI methods is technologically sound and legally robust. To aid developers and researchers in selecting the most appropriate XAI algorithms for different bioelectronic devices, we developed a set of instructions (see Section 8). These instructions and the methodology provide a fundamental framework designed to be flexible and seamlessly incorporate newly emerging XAI algorithms and evolving regulations."}, {"title": "4 Background", "content": "This section provides background information on AI-based biomedical technologies, EU regulations, and XAI."}, {"title": "4.1 Smart Bioelectronics and Biomedical Devices", "content": "Biomedical devices is an umbrella name that covers a wide variety of tools used to help diagnose, prevent, and treat diseases [31]. Bioelectronics refers to a subset of specialised biomedical devices that combine electronic technology, like sensors, with biology and medicine. These devices can interact with biological systems, from whole organs to tiny cellular components, in various ways, such as using light, magnetic, or chemical methods [28]. Based on their decision-making mechanisms, bioelectronics and biomedical devices can be categorised into open-loop, closed-loop, and semi-closed-loop control systems. Open-loop control systems, such as Electrocardiograms (ECG), provide only outputs instrumental in the decision-making of healthcare professionals. In contrast, closed-loop systems autonomously adjust their operations based on continuous monitoring. For example, artificial pancreas systems for diabetes management autonomously monitor glucose levels and administer insulin [35]. Semi-closed-loop systems represent an intermediate approach where the machine instructs a patient to manually perform life-saving actions based on data (e.g., manual insulin injection adjustments based on a Continuous Glucose Monitoring System [35]). In addition to loop-based categorisation, biomedical devices can be classified based on their potential risks to human health. This classification is influenced by the device's operating mode and characteristics, such as whether it is invasive or non-invasive. The classification is determined with relevant legislation (i.e., MDR) [1]. Neural implants represent a fascinating intersection of AI and neurotechnology and can be split between Brain-Computer Interfaces (BCIs) and Computer-Brain Interfaces (CBIs). Both prosthetic devices establish direct communication between the human brain and external hardware or software. BCIs use decoding algorithms to restore lost functions, while CBIs exploit encoding algorithms to convert external sensory signals to neural stimulation patterns [41]. Depending on their level of autonomy and decision-making mechanisms, neural implants are subject to different explanation requirements (see Table 1). One example of a closed-loop neural implant is the Responsive Neuro Stimulation (RNS) system, which is designed for individuals with epilepsy who do not respond well to medications and are not candidates for epilepsy surgery. Epileptic seizures are caused by abnormal electrical activity in the brain. RNS system records intracranial EEG patterns to timely activate a stimulation designed to mitigate such activity [21]. A Spinal Cord Stimulator (SCS) is instead an example of semi-closed loop neural implant used to alleviate chronic pain. It consists of an implanted device that delivers electrical pulses to the spinal cord to disrupt pain signals before they reach the brain. Unlike closed-loop systems, where all adjustments are fully automated, patients and healthcare professionals often have important control over these devices and their stimulation decisions. They can adjust the stimulation settings within certain limits, such as changing the intensity, frequency, or coverage of the pulses [19]."}, {"title": "4.2 EU Regulations Relevant to Smart Biomedical Devices: Scopes and Notions", "content": "The Medical Devices Regulation [10] governs the placing on the market and use of medical devices in the EU (Art. 1.1) for the diagnosis, prevention, prediction, monitoring, treatment, of diseases, injuries, or disabilities. These devices must primarily operate not through pharmacological, immunological, or metabolic means but can be supported by them (Art. 2.1). The General Data Protection Regulation [9] instead applies to personal data processing i) by EU-based data controllers or processors, or ii) involving EU residents' data processed by a controller located outside the EU, (Art. 2 and 3). Personal data is information about an identifiable person: the 'data subject' (Art. 4.1). 'Processing' encompasses: collection, organisation, storage, consultation, use, disclosure and erasure (Art. 4.2). A 'data controller' sets personal data processing purposes and means (Art. 4.7), while a 'processor' handles data on behalf of a controller (Art. 4.8). The Artificial Intelligence Act [11], adopted in June 2024, applies to AI systems marketed or used in the EU or whose outputs are employed in the EU, regardless of the provider's or deployer's location (Art. 2). The AI systems covered are software able to infer, from their inputs, how to generate outputs (e.g., predictions, content, recommendations, or decisions, Art. 3.1). A 'provider' develops or commissions AI systems for market placement or service (Art. 3.2), and a 'deployer' employs an AI system, excluding for personal, non-professional use (Art. 3.4). High-risk AI systems include those covered by EU legislation listed in Annex I, like MDR, when requiring third-party conformity assessments and those listed in Annex III, e.g., for remote biometric identification (Art. 6)."}, {"title": "4.3 \u03a7\u0391\u0399 Algorithms", "content": "XAI literature features a variety of domain-dependent and context-specific methods that differ in their explanation generation strategies, formats, and applicability to disparate data and learning algorithms [27]. Researchers have developed taxonomies to aid in selecting suitable XAI methods for specific problems. A key contribution to this paper comes from the work of Liao et al. [33], who categorise XAI methods based on the questions they address, and [53], who organised XAI methods by stage, scope, and format. Firstly, the stage category splits explanations between ante-hoc and post-hoc. Ante-hoc methods aim to build inherently explainable models, while post-hoc methods seek to clarify the logic of an already trained model using an external explainer. Secondly, explanations are divided between having a global (explaining the entire model's process) and a local (explaining individual inferences) scope [53]. Thirdly, explanations differ in their format. Some consist of vectors, tensors or matrices of numbers pointing out the most relevant input features. Other explanatory formats are texts, charts and diagrams, rules, or a combination of these formats. As discussed in Section 5 and shown in Table 2, some regulations favour ex-ante explainability [45]. However, much of the research in XAI focuses on post-hoc solutions [53]."}, {"title": "5 Explanation Requirements and Legal Explanatory Goals", "content": "This section examines the EU regulations outlined in Section 4.2, focusing on their mandates for explanations. Our analysis encompasses"}, {"title": "Legal Explanatory Goals", "content": "The explanation requirements detailed so far apply to any bioelectronic component and biomedical device that enters the scope of GDPR, AIA, and MDR and meets the conditions which trigger their explanation requirements (e.g., fully automated high-stakes decisions based on personal data for the GDPR). These requirements are not mutually exclusive, and a cumulative application of two (or more) requirements may be needed. In particular, the device's autonomy level will influence the number of requirements to be complied with, as illustrated in Table 1. Based on the analysis of the legal explanatory requirements led in this section and following the methodology described in Section 3, we identified 11 high-level legal explanatory goals to which these requirements pertain. Table 2 presents the identified goals, specifying the relationship with the EU regulations. Importantly, we noted that goals and regulations have a many-to-many relationship, as each goal may be related to one or more regulations. On top of that, building on the notions described in 4.3, we clarify in Table 2 whether each goal requires global or local explanations to be achieved, as well as the stage at which they should be provided: ex-ante, or ex-post."}, {"title": "6 A Categorisation of XAI in Terms of Explanatory Goals", "content": "This section presents a categorisation of XAI methods identified in the XAI literature and elaborates on their roles in fulfilling the legal explanatory goals of Section 5. This classification stems from the methodology outlined in Section 3, considering that XAI explanations answer specific questions about AI models and their outputs. We organised the XAI methods based on explanation format, scope, input type, stage of application, and model specificity. This classification, grounded on established taxonomies (cf. Section 4.3) and Liao et al. [33]'s methodology, aids in pinpointing which explanatory question can be answered by each XAI method and, subsequently, which explanatory goals it addresses. Liao et al. [33] exploited the explanation format to identify which questions can be answered by the XAI methods. For example, counterfactual methods [26] inspect how the output changes when the input instance is modified, generating a 'what-if' scenario that manifests what leads to a desired outcome. Thus, these explanations can answer the question \"What minimal changes would need to be made to input to change its prediction?\". Instead, similarity-based XAI methods [40] show"}, {"title": "7 Case Studies: Closed-Loop and Semi-Closed-Loop Control", "content": "AI-enhanced neural implants can detect early signs of stroke, improve memory, and help control paralysed limbs to perform fine motor tasks, e.g., holding a glass (cf. Section 4.1)."}, {"title": "8 Instructions for Use & Discussion of Findings", "content": "This study introduces a multi-faceted, multi-step, multi-domain methodology for aligning XAI tools with EU regulations, address-"}, {"title": "9 Threats to Validity", "content": "Extrinsic Threats. Extrinsic threats include the potential for new interpretations of the Regulations discussed (e.g., through EU case-law), which may alter the applicability of our findings. Additionally, while our prescribed compliance methods assist in obtaining the necessary information, the effectiveness of conveying this information to individuals with different expertise and background knowledge is yet to be determined. Furthermore, while our study concentrates on biomedical devices and their significant requirements, it is crucial to acknowledge that other EU or national laws might impose additional explanation requirements in specific contexts. As a result, some devices may encounter extra constraints, potentially necessitating a broader range of XAI tools than those discussed in this paper. Another extrinsic limitation arises from the inherent complexities in explainability. Most existing XAI methods, such as surrogate models, SHAP, and LIME, often rely on imperfect heuristics and usually operate effectively under specific conditions, lacking theoretical guarantees. For example, surrogate models are entirely transparent but usually perform less effectively than their corresponding black-box models. This discrepancy can lead to explanations that do not accurately represent the underlying logic of the model. Instead, SHAP-based algorithms necessitate independent input features, a condition not always met in real-world applications. Other algorithms, like LIME, also have specific requirements for their correct implementation. The incorrect use of an XAI algorithm can result in misleading explanations that do not accurately address the identified legal objectives. Hence, simply employing an XAI algorithm does not guarantee adherence to the regulations discussed in this study. Finally, our entire methodology is based on the definition of explanation from Ordinary Language Philosophy, as outlined in Section 3. Considering alternative definitions could, therefore, introduce external threats to validity and require a different methodology. Intrinsic Threats. There are possible alternative interpretations of the law's explanatory goals and high-level objectives, which our study may not fully encompass. The limited choice of case studies is another intrinsic issue, as it does not capture the complete range of nuances within the field, potentially affecting the generalisability of our results. Lastly, the list of XAI algorithms considered in this study is not exhaustive. However, as discussed in Section 8, our approach allows for the inclusion of new XAI algorithms and legal explanatory goals, which helps to mitigate this concern."}, {"title": "10 Conclusion", "content": "This paper analysed many XAI methods and their compliance with key EU regulations for smart biomedical devices. Significant contributions include a novel methodology for combining legal analysis, technical assessment, and a detailed categorisation of XAI methods to analyse their legal alignment. This constitutes a practical framework for selecting suitable XAI methods that help meet the explainability requirements of the GDPR, AIA, and MDR. The findings highlight the importance of XAI in meeting such demands for legal explainability. Future research should extend the case studies to various bioelectronic and biomedical devices, analysing stakeholders' perceptions of the explanations generated by X\u0391\u0399."}]}