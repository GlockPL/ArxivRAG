{"title": "Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection", "authors": ["Matteo Zecchin", "Osvaldo Simeone"], "abstract": "We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and hyperparameter tuning for engineering systems, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.", "sections": [{"title": "Introduction", "content": null}, {"title": "Context and Motivation", "content": "The safe and reliable deployment of AI applications, or apps for short, hinges on the possibility of certifying their performance (Seshia et al., 2022; Tegmark and Omohundro, 2023). Depending on the problem, this may require controlling the missed detection probability in medical imaging (Lu et al., 2022; Mehrtash et al., 2020), ensuring safety measures for control policies (Lindemann et al., 2023; Zecchin et al., 2024), or verifying the correctness of the answers given by a large language model (Quach et al., 2023).\nIn practice, before deployment, AI apps can be often calibrated by selecting hyperparameters based on data set aside for this purpose or based on rounds of real-world testing. The hyperparameters can determine architectural choices, such as the composition of individual AI apps or the selection of submodules (Nikoloska and Simeone, 2022; Pfeiffer et al., 2023); inference parameters, such as the posterior temperature (Wenzel et al., 2020); optimization parameters, such as the learning rate for fine-tuning (Finn et al., 2017); implementation settings, such as the fidelity level of a simulator (Mungari et al., 2024); and post-processing parameters, such as the threshold used to set error bars (Shafer and Vovk, 2008).\nThe learn-then-test (LTT) framework introduced by Angelopoulos et al. (2021) treats the calibration task of hyperparameter selection as a multiple hypothesis testing (MHT) problem. Accordingly, it associates each hyperparameter in a candidate set to the null hypothesis that the hyperparameter does not meet a reliability requirement on the population risk. Hypotheses are tested using p-values, and the probability of mistakenly detecting a hyperparameter as reliable is guaranteed via family-wise error rate (FWER)-controlling statistical procedures (Keselman and Rogan, 1977). This way, LTT ensures finite-sample, high-probability guarantees on the population risk of the selected hyperparameters.\nTo prevent p-hacking (Head et al., 2015), LTT's guarantees apply only to non-adaptive MHT procedures. However, related work on hyperparameter optimization has shown the significant benefits that can be accrued by adaptive exploration strategies that test hyperparameters sequentially in a data-driven manner (Rakotoarison et al., 2024; Swersky et al., 2014). This work aims at improving the data efficiency of LTT by leveraging recent advances in sequential MHT based on e-processes (Vovk and Wang, 2021; Waudby-Smith"}, {"title": "Related Work", "content": "Finite-sample statistical guarantees on inferential outputs can be obtained via conformal prediction methods (Shafer and Vovk, 2008; Angelopoulos et al., 2023) and, more generally, via conformal risk control and risk-controlling set-valued predictions (Angelopoulos et al., 2022; Bates et al., 2021). These methods calibrate classification or regression models by setting a threshold hyperparameter on the basis of held-out data to control the size of a prediction set. Calibrated predictors can be leveraged in predict-then-optimize control tasks to offer reliability guarantees (Lindemann et al., 2023; Vovk and Bendtsen, 2018; Zecchin et al., 2024).\nBeyond prediction sets, the problem of calibrating an AI app via the selection of hyperparameters from a candidate pool has been addressed through the LTT framework (Angelopoulos et al., 2021). As shown in Figure 1, the initial set of candidate hyperparameters can be obtained by using any hyperparameter optimization procedure, to be further discussed below. Leveraging p-value-based MHT via FWER-controlling procedures, LTT selects a subset of candidates that come with high-probability population risk guarantees.\nFWER guarantees are often too conservative, potentially resulting in empty calibration sets. The false discovery rate (FDR) is an alternative and less strict criterion that is often preferred for MHT in fields such as genetics (van den Oord and Sullivan, 2003), neuroimaging (Genovese et al., 2002), online advertising (Berman and Van den Bulte, 2022), and finance (Harvey and Liu, 2020).\nE-values have gained popularity in MHT due to their advantages over p-values (Ramdas et al., 2020; Shafer"}, {"title": "Main Contributions", "content": "The main contributions of this paper are as follows.\n\u2022 We introduce adaptive LTT (aLTT), a data-efficient hyperparameter selection method that provides finite-sample guarantees on the population risk of AI apps. The main technical underpinning of aLTT is e-process-based MHT, which supports statistical validity while enabling data-dependent sequential testing (Xu et al., 2021). Unlike LTT, as illustrated in Figure 1, aLTT"}, {"title": "Problem Definition", "content": null}, {"title": "Setting", "content": "Let Mx be an AI app whose operation is determined by a vector of hyperparameters \u03bb. The performance of a hyperparameter vector \u03bb when tested at input data Z is measured by a risk function $R(\\lambda, Z) \\in [0, 1]$. Accordingly, the population risk with respect to an unknown data distribution $P_Z$ is defined as\n$R(A) = \\mathbb{E}_{P_Z} [R(A, Z)]$.\nAs illustrated in Figure 1, assume that a discrete subset $\\Lambda = {\\lambda_1, ..., \\lambda_N}$ of hyperparameters has been identified based on domain knowledge and/or conventional hyperparameter optimization methods. For a user-specified reliability level $\\alpha \\in [0, 1]$, we aim at determining the subset of hyperparameters in set $\\Lambda$ that conforms with the required reliability level $\\alpha$, i.e.,\n$\\Lambda^{rel} = {\\lambda \\in \\Lambda : R(\\lambda) \\leq \\alpha}$.\nThe complementary set, comprising unreliable hyperparameters, is accordingly defined as\n$\\Lambda^{unrel} = \\Lambda \\setminus \\Lambda^{rel} = {\\lambda \\in \\Lambda : R(\\lambda) > \\alpha}$.\nSince identifying the entire set $\\Lambda^{rel}$ in (2) is impossible owing to the lack of knowledge about the data distribution $P_Z$, the goal is producing a subset of hyperparameters $\\hat{\\Lambda}^{rel} \\subseteq \\Lambda$ that contains as many reliable hyperparameters from subset $\\Lambda^{rel}$ as possible while controlling the number of unreliable hyperparameters from subset $\\Lambda^{unrel}$ mistakenly included in subset $\\hat{\\Lambda}^{rel}$."}, {"title": "Performance Criteria", "content": "Definition 1 (($\\alpha, \\delta$)-FWER-controlling set). For a given reliability level $\\alpha \\in [0, 1]$ and an error level"}, {"title": "Sequential and Adaptive Hyperparameter Selection", "content": "To produce the estimated subset of reliable hyperparameters, $\\hat{\\Lambda}^{rel}$, we adopt a general sequential testing procedure that, at each round $t > 1$, operates as follows."}, {"title": "(Non-Adaptive) Learn-then-Test", "content": "In this section, we review LTT, a non-adaptive hyperparameter selection procedure devised to meet the ($$\\alpha, \\delta$)-FWER guarantee (Angelopoulos et al., 2021). LTT associates to each hyperparameter $\\lambda_i \\in \\Lambda$ the null hypothesis\n$H_i: R(\\lambda_i) > \\alpha$\nthat the population risk $R(\\lambda_i)$ in (1) violates the target reliability level $\\alpha$. For each null hypothesis $H_i$ a p-value $P_i$ is a non-negative random variable that sat-"}, {"title": "Adaptive Learn-Then-Test", "content": "In this section, we introduce adaptive LTT (aLTT), a hyperparameter selection scheme that supports adaptive acquisition policies and an adaptive number of calibration rounds. The algorithmic description of aLTT is given in Algorithm 1."}, {"title": "Hypothesis Testing via E-Processes", "content": "The proposed aLTT scheme applies MHT based on e-values and e-processes (Ramdas et al., 2023; Shafer, 2021). For each null hypothesis $H_i$ in (7), an e-value $E_i$ is a non-negative random variable with an expectation no larger than 1 when $H_i$ is true, i.e.,\n$\\mathbb{E}[E_i|H_i] \\leq 1$.\nBy Markov's inequality, an e-value $E_i$ can be turned into a p-value $P_i$ as $P_i = 1/E_i$, since the inequality (8) is satisfied as\n$\\Pr[\\frac{1}{E_i} \\leq x | H_i] \\leq \\mathbb{E}[E_i|H_i]x \\leq x$.\nFor each null hypothesis $H_i$, given an observation $Z$ and a fixed $\\mu \\in (0, 1/(1-\\alpha))$, a valid e-value is given by (Waudby-Smith and Ramdas, 2024)\n$E_i = (1 + \\mu(\\alpha - R(\\lambda_i, Z)))$.\nThe e-value (11) has the interpretation of wealth growth in a betting setting. Accordingly, one can think of parameter \u03bc in (11) as the amount of the current wealth that the gambler bets on the hypothesis Hi being false, i.e., on the validity of the assumption $R(\\lambda_i) \\leq \\alpha$ that the hyperparameter $\\lambda_i$ is reliable. In fact, if $\\mu > 0$, when $R(\\lambda_i, Z) \\leq \\alpha$, the gambler's wealth in (11) increases; while, when $R(\\lambda_i, Z) > \\alpha$, the quantity (9) the gambler's wealth (11) decreases.\nAn e-process for hypothesis $H_i$ is a sequence of random variables {$E_i^t$}$_{t>1}$ such that, for any stopping time T, which may depend on all previously collected evidence, the random variable $E_i^\\tau$ is a valid e-value. Using the e-value (11), considering the general iterative testing framework in Section 2.3, an e-process for the null hy-"}, {"title": "Adaptive Acquisition Policy", "content": "aLTT applies an adaptive acquisition policy {$Q_t$}$_{t\u22651}$ and an adaptive calibration horizon T. Specifically, at each calibration round $t > 1$, aLTT's acquisition policy $Q_t$ uses the e-processes $\\mathcal{E}_{t-1} = {E_i^{t-1}}$}i in (12) to choose which subset of hyperparameters, $\\mathcal{I}_t$, to test next. Examples of acquisition functions $\\mathcal{I}_t = Q_t(\\mathcal{E}_{t-1})$ will be provided in the next section.\nFor the selected hyperparameters in set $\\mathcal{I}_t$, aLTT obtains the risk estimates $\\mathcal{R}_t = {R(\\lambda_i, Z)}$}$_{\\lambda_i \\in \\mathcal{I}_t}$ and updates the associated e-processes using the recursive formula (12), i.e.,\n$E_i^t = \\begin{cases}(1 + \\mu_i^t(\\alpha - R(\\lambda_i, Z)))E_i^{t-1}, & \\text{if } \\lambda_i \\in \\mathcal{I}_t \\\\\nE_i^{t-1}, & \\text{otherwise}.\n\\end{cases}$\nWith this information, a prediction set $\\hat{\\Lambda}_{LTT, t}$ is evaluated by employing either an FWER-controlling method $A^{FWER}(\\mathcal{P}_t)$ based on the p-values $\\mathcal{P}_t = {P_i^t}$}$_{i \\leq N}$ in (13); or an FDR-controlling procedure $A^{FDR}(\\mathcal{E}_t)$, such as the e-Benjamini-Hochberg (eBH) method, reviewed in the Supplementary Material (Wang and Ramdas, 2022), using directly the e-values (14).\naLTT terminates the calibration procedure whenever there are at least $d$ hyperparameters in set $\\hat{\\Lambda}_{LTT, t}$, i.e., $|\\hat{\\Lambda}_{LTT, t}| \\geq d$, or a maximum number of iterations $t_{max}$ have been reached. This allows aLTT to stop the data acquisition phase early when a sufficiently large number of reliable hyperparameters have been identified."}, {"title": "Hyperparameter Subset Selection", "content": "At the end of the calibration process, aLTT uses the current e-processes $\\mathcal{E}_T$ to generate the final prediction set $\\hat{\\Lambda}_{LTT, T}$. By the anytime validity properties explained in Section 4.1, if an FWER-controlling method $A^{FWER}(\\mathcal{P}_T)$ is used, the resulting set $\\hat{\\Lambda}_{LTT, T}$ is ($$\\alpha, \\delta$)-FWER-controlling; while if an FDR-controlling method is used, the resulting set $\\hat{\\Lambda}_{LTT, T} = A^{FDR}(\\mathcal{E}_T)$ is ($$\\alpha, \\delta$)-FDR-controlling."}, {"title": "Applications", "content": null}, {"title": "Online Policy Selection for Offline Reinforcement Learning", "content": "Offline reinforcement learning enables the training of control policies based on a fixed data set collected by using a possibly unknown behavior policy, without any online interaction with the environment (Levine et al., 2020). However, the estimate of the performance of the trained policies obtained from offline data can differ substantially from the actual performance in the real world. This makes it practically essential to validate the policies' performance via online interaction with the environment (Liu et al., 2023).\nTo reduce the cost and potential harm of online validation procedures, the number of online interactions of the pre-trained candidate policies with the real world must be kept to a minimum (Garcia and Fern\u00e1ndez, 2015). To this end, in this subsection, we investigate the potential benefits of the proposed aLTT framework as a means to select a subset of candidate policies that enjoy performance guarantees with respect to the real-world environment."}, {"title": "Problem Definition", "content": "We assume a standard Markov decision process (MDP) $\\mathcal{E} = {\\mathcal{S}, \\mathcal{A}, P_{s'|s,a}, \\Pr_{r|s,a}}$ specified by a state space $\\mathcal{S}$; an action space $\\mathcal{A}$; a transition kernel $P_{s'|s,a}$, defining the conditional distribution of the next state $s' \\in \\mathcal{S}$ given the current state $s \\in \\mathcal{S}$ and action $a \\in \\mathcal{A}$; and a conditional reward distribution $\\Pr_{r|s,a}$ given state $s \\in \\mathcal{S}$ and action $a \\in \\mathcal{A}$. We assume that the reward r is bounded and normalized in the [0, 1] interval.\nWe are given a set of pre-trained control policies $\\Pi = {\\pi_1,...,\\pi_N}$, mapping an observed state $s \\in \\mathcal{S}$ to the random action $a \\sim \\pi_i(s) \\in \\mathcal{A}$. Each policy $\\pi_i$ is identified by a hyperparameter $\\lambda_i$. The goal is to select a subset $\\Lambda^{rel} \\subseteq \\Lambda$ of hyperparameters that yield reliable policies by using a limited amount of interactions with the real world.\nReliability is measured via the cumulative reward ob-"}, {"title": "Results", "content": "In our experiments, we consider the Half Cheetah control problem from the OpenAI Gym MuJoCo tasks (Todorov et al., 2012) and use control policies obtained via the offline reinforcement learning algorithm TD3+BC (Fujimoto and Gu, 2021). The TD3+BC algorithm leverages an offline dataset D to optimize policies by maximizing the standard deterministic pol- icy gradient objective (Silver et al., 2014), combined with a behavioral cloning regularization term, whose strength is controlled by a hyperparameter \u03bb in (Silver et al., 2014, Eq. 5). We produce N = 20 different control policies by setting the hyperparameter \u03bb in the TD3+BC training objective on an evenly spaced grid in the interval [0.25, 5]. Unless stated otherwise, we consider a target reliability \u03b1 = 0.57 and a target FDR requirement \u03b4 = 0.1.\nWe evaluate aLTT with an \u03b5-greedy acquisition policy Qt that, at every calibration round t, with probability 1 - \u03b5, selects the hyperparameter not included in $\\hat{\\Lambda}_{LTT, t}$ that is associated with the largest e-process value; otherwise, it picks uniformly at random a hy- perparameter not in $\\hat{\\Lambda}_{LTT, t}$. For reference, we also consider aLTT with a non-adaptive acquisition policy that, at each round t, picks uniformly at random the hyperparameter to be tested regardless of the predic-"}, {"title": "Reliable Hyperparameter Selection for Wireless Resource Allocation", "content": "In wireless communication systems, resource allocation is an essential functionality that regulates access to the spectrum for users and services (Stanczak et al., 2009). The performance of resource allocation policies is evaluated by using key performance indicators (KPIs) such as throughput, delay, and energy efficiency. Despite the randomness inherent in the network conditions, some services require strict reliability guarantees in terms of KPIs. For instance, gaming applications must meet latency constraints (Elbamby et al., 2019), streaming connections are subject to throughput requirements (Li et al., 2012), and battery- powered transmitters have strict energy-efficiency con- straints (Mahapatra et al., 2015)."}, {"title": "Problem Definition", "content": "As illustrated in Figure 4, we consider a downlink re- source allocation problem in which, at every transmis- sion frame k\u2265 1, a base station serves users by com- municating bits from the corresponding queues. This is done by assigning physical resource blocks (PRBs) to users based on a descriptor sk of the network con- ditions that includes the state of users' transmission buffers, users' priorities, and channel conditions. Fol- lowing the simulation software Nokia Wireless Suite (Nokia, 2020), we consider three different types of PRBs assignment policies: a proportional fair scheme (Baruah et al., 1993), a knapsack allocation policy"}, {"title": "Results", "content": "We study a setting with differentiated service requirements. In particular, we enforce a constraint on the average queue occupancy and energy efficiency for high-"}, {"title": "Conclusion", "content": "We introduced aLTT, a novel framework for hyperparameter selection that implements data-dependent sequential testing via early termination. Unlike the existing LTT, which builds on p-value multiple hypothesis testing (MHT), aLTT is based on sequential MHT via e-processes (Xu et al., 2021). In practical scenarios, this results in more efficient and flexible calibration procedures that maintain statistical validity and the same discovery power as LTT by using only a fraction of testing rounds.\nPotential extensions of the aLTT framework include the study of scenarios characterized by distribution shift, simulation-aided calibration, and the application of aLTT to the calibration of large language models."}, {"title": "FWER and FDR-Controlling Procedures", "content": "In this section we review popular FWER and FDR-controlling strategies applied to the problem of hyperparameter selection."}, {"title": "FWER Control", "content": null}, {"title": "Bonferroni Correction", "content": "For a set of p-values P = {Pi}$_{i=1}^N$, a simple (\u03b1, \u03b4)-FWER-controlling procedure is given by the Bonferroni correction $A_N^{Bon}(P)$, which returns the hyperparameter set\n$\\mathcal{A}_{Bon}^{rel} = {\\lambda: P_i \\leq \\frac{\\delta}{N}}$.\nThe hyperparameter set $A_{Bon}^{rel}$ is guaranteed to be \u03b4-FWER-controlling (Bonferroni, 1936)."}, {"title": "Fixed-Sequence Testing", "content": "If the designer has access to prior knowledge $D^\\degree$ about which hyperparameters are likely to be more reliable, it is possible to increase the power of the statistical test, and thus the cardinality of the hyperparameter $\\mathcal{A}^{rel}$, by using a fixed-sequence testing procedure $A^{FS}_{N}(P, D^\\degree)$. In fixed sequence hypothesis testing, the hypotheses {Hi}$_{i=1}^N$ are ordered based on the prior knowledge $D^\\degree$ from the most likely to be reliable to the least likely to be reliable. Denote the k-th hypothesis in the corresponding ordered sequence as H(k), and the associated p-value as P(k). With fixed-sequence testing, the hypotheses are sequentially tested at a reliability level \u03b1 until one is accepted. Accordingly, the resulting hyperparameter set contains all hypotheses up until the first acceptance, i.e.,\n$\\mathcal{A}_{N}^{FS} = {\\lambda_{(j)}: P_{(i)} \\leq \\delta, \\forall i \\leq j}$.\nFora ny ordering of the p-values, the hyperparameter set $A_{N}^{FS}$ is guaranteed to be \u03b4-FWER-controlling (Bauer, 1991)."}, {"title": "FDR Control", "content": null}, {"title": "Benjamini-Hochberg Procedure", "content": "Given a set of independent p-values P, denote the k-th smallest value in the set as $P_{(k)}$ and the associated hyperparameter as $\u03bb_{(k)}$. For an error level \u03b4, the Benjamini-Hochberg procedure $A_{N}^{BH}(P)$ returns the prediction set\n$\\mathcal{A}_{BH}^{rel} = {\\lambda_{(i)}: P_{(i)} \\leq \\frac{i \\delta}{N}}$.\nBy (18), BH applies a larger threshold to hyperparameters \u03bb\u2208 \u039b that have larger p-values and are thus less likely to be reliable. If the p-values in set P are independent, the Benjamini-Hochberg (BH) procedure $A_{N}^{BH}(P)$ guarantees FDR control at a level \u03b4 (Benjamini and Hochberg, 1995)."}, {"title": "Benjamini-Yekutieli Procedure", "content": "In case of arbitrarily dependent p-values in set P the error level \u03b4 has to be adjusted by a multiplicative factor ($\\sum_{n=1}^{N}1/n)^{-1}$. The resulting FDR-controlling procedure, also known as the Benjamini-Yekutieli (BY) procedure, yields the set\n$\\mathcal{A}_{BY}^{rel} = {\\lambda_{(i)}: P_{(i)} \\leq \\frac{i \\delta}{\\sum_{n=1}^{N} N/n}}$\nThe Benjamini-Yekutieli (BY) procedure $A^{BY}_{N}(P)$ is \u03b4-FDR-controlling (Benjamini and Yekutieli, 2001)."}, {"title": "E-Benjamini-Hochberg Procedure", "content": "N\nAlternatively, given the set of e-values $\\mathcal{E}_t = {E_i^t}_{i=1}$, denote by the k-th largest e-value in $\\mathcal{E}$ as $E_{(k)}$ and the associated hyperparameter as $\u03bb_{(k)}$. For an error level \u03b4, the e-Benjamini-Hochberg procedure $A^{eBH}_N(\\mathcal{E})$ outputs the hyperparameter subset\n$\\hat{\\Lambda}_{eBH} = {\\lambda_{(\\epsilon)}: E_{(\\epsilon)} \\geq \\frac{N}{\\epsilon \\delta}}$\nFollowing the same basic principles underlying BH in (18), eBH applies a larger threshold to hyperparameters \u03bb\u2208 A that have smaller e-values and are thus less likely to be reliable. The eBH procedure returns an \u03b4-FDR- controlling set even for arbitrarily dependent e-values $E_{(k)}$ without the need to adjust the error level \u03b4 as in the case of the p-value-based BH procedure (Xu et al., 2021)."}, {"title": "Additional Experiments", "content": null}, {"title": "Online Policy Selection for Offline Reinforcement Learning", "content": null}, {"title": "Effect of the Betting Strategy", "content": "In Section 5.1, we studied the TPR of aLTT under the approximate growth rate adaptive to the particular alternative (aGRAPA) betting strategy. In the following, we compare its performance against non-adaptive betting strategies, specifically the maximum bet (MaxBet) strategy, which sets the bet $\u03bc_i$ to the maximum allowed value of 1/\u03b1, and the unitary bet (UnitBet) strategy, where $\u03bc_i = 1$. Additionally, we evaluate two alternative adaptive online betting strategies based on approximate wealth maximization (Waudby-Smith and Ramdas, 2024): lower-bound on the wealth (LBOW) and online Newton step (ONS).\nUnder the same online policy selection for offline reinforcement learning as set up in Section 5.1, in Figure 6 we compare the TPR of aLTT with an \u03b5-greedy acquisition policy. The left panel targets FWER control, while the right focuses on FDR control. All adaptive betting strategies exhibit similar performance, with aGRAPA showing a slight advantage. Interestingly, in this scenario, the non-adaptive MaxBet strategy performs surprisingly well, while UnitBet achieves the lowest TPR."}, {"title": "Reliable Hyperparameter Selection for Wireless Resource Allocation", "content": null}, {"title": "Minimizing Average Delay under an Energy Efficiency Requirement", "content": "Here we consider the wireless resource allocation problem of Section 5.2 under a reliability requirement on the average energy efficiency. Specifically, we consider the population risk R(\u03bb) = Epz [\u03c1(\u03bb, Z)], so that the set"}]}