{"title": "MeNTi: Bridging Medical Calculator and LLM Agent\nwith Nested Tool Calling", "authors": ["Yakun Zhu", "Shaohang Wei", "Xu Wang", "KUI XUE", "Xiaofan Zhang", "Shaoting Zhang"], "abstract": "Integrating tools into Large Language Mod-\nels (LLMs) has facilitated the widespread ap-\nplication. Despite this, in specialized down-\nstream task contexts, reliance solely on tools\nis insufficient to fully address the complexities\nof the real world. This particularly restricts\nthe effective deployment of LLMs in fields\nsuch as medicine. In this paper, we focus on\nthe downstream tasks of medical calculators,\nwhich use standardized tests to assess an indi-\nvidual's health status. We introduce MeNTi, a\nuniversal agent architecture for LLMs. MeNTi\nintegrates a specialized medical toolkit and em-\nploys meta-tool and nested calling mechanisms\nto enhance LLM tool utilization. Specifically,\nit achieves flexible tool selection and nested\ntool calling to address practical issues faced in\nintricate medical scenarios, including calcula-\ntor selection, slot filling, and unit conversion.\nTo assess the capabilities of LLMs for quantita-\ntive assessment throughout the clinical process\nof calculator scenarios, we introduce CalcQA.\nThis benchmark requires LLMs to use medical\ncalculators to perform calculations and assess\npatient health status. CalcQA is constructed by\nprofessional physicians and includes 100 case-\ncalculator pairs, complemented by a toolkit of\n281 medical tools. The experimental results\ndemonstrate significant performance improve-\nments with our framework. This research paves\nnew directions for applying LLMs in demand-\ning scenarios of medicine\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved\nmilestone successes, demonstrating exceptional\nmulti-tasking potential that includes reasoning,\nplanning, tool usage, and code generation (Tou-\nvron et al., 2023; Chen et al.; Achiam et al., 2023;\nOuyang et al., 2022). Recent studies indicate\nthat by deeply integrating LLMs with large-scale\ntoolsets, their task-solving efficiency can be signifi-\ncantly enhanced, further expanding the functional\nboundaries of the LLMs (Song et al., 2023; Qin\net al., 2023; Gao et al., 2023; Hao et al., 2024).\nAdditionally, developing customized tools tailored\nto specific application scenarios has become a key\napproach to optimize performance and enhance\nadaptability, showing remarkable results in numer-\nous downstream tasks (Qian et al., 2023; Cai et al.,\n2023; Yuan et al., 2023).\nIn this paper, we focus on the issue of medi-\ncal calculators, a typical and representative down-\nstream task for LLMs tool application. Medical\ncalculators are standardized tools used to quanti-"}, {"title": "2 Related Works", "content": "LLMs for Medical Domain. With the applica-\ntion of LLMs across various fields, their penetra-\ntion into the medical domain has become a hot re-\nsearch topic (Thirunavukarasu et al., 2023; Singhal\net al., 2023; Clusmann et al., 2023). Recent stud-\nies focus on fine-tuning LLMs using real or syn-\nthetic medical data (Zeng et al., 2020; Zhang et al.,\n2023a; Tang et al., 2023b) to optimize their per-\nformance. For instance, PMC-LLaMA (Wu et al.,\n2024) has undergone pre-training by incorporating\na vast resource of 4.9 million medical literature\nrecords. Similarly, ChatDoctor (Yunxiang et al.,\n2023) integrates real doctor-patient communication\ndata, enhancing the model's ability to understand\npatient needs and make appropriate recommenda-\ntions. Additionally, several studies have explored\nRAG to boost the efficacy of LLMs. The LLM-\nAMT (Wang et al., 2023) adopts the RAG architec-\nture, incorporating authoritative medical textbooks\ninto the model. Self-BioRAG (Jeong et al., 2024)\ntrained with domain-specific retrievers, document"}, {"title": "3 Methodology", "content": "In Section 3.1, we introduce CalcQA, a novel\nbenchmark and a toolkit with specialized medical\ntools, to assess the ability of LLMs to perform end-\nto-end calculator assessment in clinical scenarios.\nWe develop this benchmark by having professional\nphysicians diagnose real cases and then structuring\nthese cases and diagnostic results into question-\nanswer pairs. In Sections 3.2 and 3.3, we introduce\nMeNTi, which employs the meta-tool for tool selec-\ntion and nested calling mechanism to assist LLMs\nin tool utilization. By enabling flexible tool se-\nlection and nested tool calling, MeNTi effectively\naddresses intricate scenario tasks with the special-"}, {"title": "3.1 Benchmark Creation", "content": "Existing LLM benchmarks for medical calculators\nprimarily focus on assessing the LLMs' ability to\nhandle medical knowledge-based questions or their\ncapacity to make selective diagnostic judgments\nin specific scenarios. However, these benchmarks\nare not designed directly from clinical practice and\ndo not derive diagnostic conclusions directly from\nactual cases, making them insufficient for evaluat-\ning the usability of LLMs in real clinical settings.\nTherefore, we introduce CalcQA, a new benchmark\nconstructed based on real cases, aimed at assessing\nLLMs within the context of medical calculators.\nToolkit Construction. Given the specialized\nmedical knowledge and complex computational\nlogic involved in medical calculators, LLMs re-\nquire external knowledge. Accordingly, we de-\nvelop a generalized medical toolkit. Initially, we\nscrap essential knowledge from authoritative medi-\ncal websites, then utilized GPT-4 for code genera-\ntion to perform precise mathematical calculations.\nFinally, we design real-world scenarios to verify\nour tools. After meticulous selection and integra-\ntion, we compile 44 medical calculators that are\nwidely used in medical practice, whose application\nscenarios and precise calculation rules are also rep-\nresented. These calculators extensively cover mul-\ntiple branches of medicine, as detailed in Figure 3.\nAdditionally, considering that medical calculations\noften involve precise conversions between medical\nunits, like conversion between the mass and molar\nquantities, we organize conversion tools for 237\ncommon medical units. The manufacturing pro-\ncess and content are consistent with those of the\ncalculators above."}, {"title": "3.2 Meta-Tool for Tool Selection", "content": "Existing tool selection strategies often rely on pre-\nsenting a complete toolkit to LLMs, expecting\nthem to autonomously choose the appropriate ones.\nHowever, this approach is impractical for large\ntoolsets. Relying solely on retrieval methods to\nmatch tools with specific task scenarios is also\nchallenging, which requires deep insights into the\nnature of the tasks and a thorough understanding of\ntools. Additionally, the existing timing strategy for\napplying tools is determined by fine-tuned LLMs.\nAlthough this approach has widespread applicabil-\nity, its inherent high level of uncertainty is unac-\nceptable in medical environments that prioritize\nprecision and reliability. Therefore, we introduce\nthe meta-tool mechanism specifically to address\ntool selection challenges.\nThe meta-tool is tailored for medical calculator\nscenarios, standardizing the timing of tool usage\nbased on specific situations, thus obviating the need\nfor LLMs of tool calling capabilities. Specifically,\ntool calling occurs in two scenarios: first, when the\nuser expresses the need for calculator assessment\nduring the inquiry process; second, when the cur-\nrent task scenario lacks the capabilities required to\nresolve the task, necessitating the introduction of\nadditional tools to augment existing abilities. In the\nlatter case, the nested calling (3.3) presents specific\ntool usage requirements to the meta-tool, which\nthen assesses the demands for tool application and\nproceeds to match the appropriate tools based on\nthe identified requirements.\nAdditionally, the meta-tool facilitates a flexible\ntool selection that aligns with various scenarios,\nassisting the MeNTi system in choosing appropri-"}, {"title": "3.3 Nested Tool Calling", "content": "Previous studies primarily focus on aggregating\nmore tools to expand the functional boundaries\nof LLMs. However, the application scenarios for\nLLMs are far from straightforward, especially in\nmedical calculator tasks. Specifically, once a suit-\nable calculator is chosen, its application presents\nsignificant challenges. Medical scenarios involve\ncomplex case histories, and selecting parameters\nfrom these contexts tests the capacity for long-\ncontext processing. Furthermore, the process de-\nmands substantial medical expertise to avoid errors\nin parameter judgment and to handle the challenges\nof unit conversion. This demands the appropriate\nuse of current tools according to the context and\nthe ability to identify unmet demands and integrate\ndifferent tools, which is essentially the capability\nof nested tool calling.\nTherefore, we introduce the nested calling mech-\nanism which consists of three steps: First, it in-\nvolves the tool slot filling from the patient's case"}, {"title": "4 Experiment", "content": "In this section, we test the performance of MeNTi\non CalcQA and compare it with other methods (4.1\n& 4.2). Additionally, we conduct further analysis\n(4.3), showing the rational design of MeNTi."}, {"title": "4.1 Experimental Setting", "content": "Metrics. We employ four novel metrics, designed\nto quantify the accuracy of agents in selecting ap-\npropriate calculators, the agents' ability to fill tool\nslots, their competence for nested tool calling, and\nthe overall performance of MeNTi in executing\nend-to-end tasks.\n\u2022 Calculator Selection Accuracy (CSA): We\ncalculate the percentage of cases where the\nagent's choice of calculators aligned with the\nground truth. This metric measures the agent's"}, {"title": "4.3.1 Alternative Component Analysis", "content": "To further evaluate the performance improvement\nbrought by MeNTi, in addition to using the LLM\napproach described in Section 4.1, we select\nmore methods and conduct comparative experi-\nments by replacing certain components of MeNTi:\n(1) Alternative Retrieval. We compare our ap-\nproach with the commonly used retrieval meth-\nods, BM25(Robertson et al., 2009), SimCSE(Gao\net al., 2021) and M3E(Wang Yuxin, 2023). We\nonly replace the meta-tool for tool selection with\nthese methods while keeping the toolkit unchanged.\n(2) Alternative Calculator. We select TORA (Gou\net al., 2023), a model for solving mathematical\nproblems, to address the calculator challenges, con-\ntrasting it with our toolkit approach. We replace\nonly the nested calling mechanism for calculation\nwith TORA."}, {"title": "4.3.2 Ablation Analysis", "content": "To verify the necessity of each component of\nMeNTi, we conduct an ablation analysis.\nClassifier. We discard the previous tool catego-\nrization methods and directly retrieve all tools for\nthe experiment. The results demonstrate that the\nremoval of the Classifier significantly reduces the\nCSA and UCA. This underscores the importance of"}, {"title": "5 Conclusion", "content": "In this work, we introduce MeNTi, an agent frame-\nwork that achieves flexible tool selection and nested\ntool calling, specifically designed for medical cal-\nculator scenarios. We develop and validate a med-\nical calculator toolkit. Additionally, we intro-\nduced CalcQA, a benchmark consisting of 100\ncase-assessment pairs, to assess the capabilities\nof LLMs in medical calculator tasks. Extensive\nexperiments have shown that our method achieves\nsuperior performance compared to existing meth-\nods. We hope our work will further inspire the\napplication of LLMs in medical contexts."}, {"title": "Limitations", "content": "Despite MeNTi making significant strides in medi-\ncal calculator tasks through flexible tool selection\nand nested tool calling, it still faces several limi-\ntations that pose challenges to its overall perfor-\nmance. The primary issue is the inherent com-\nplexity and length of medical cases, which makes\nMeNTi's capabilities seem relatively insufficient\non a backbone with weaker long-context abilities.\nAdditionally, due to the high cost of data annota-\ntion in medical calculator scenarios, the current\nCalcQA benchmark only covers 100 instances, ne-\ncessitating more examples to further enhance the\nmodel's generalization validation. Furthermore,\nalthough the MeNTi framework theoretically has\nbroad applicability, the high difficulty and cost of\ntool development have prevented us from system-\natically evaluating its generalization capabilities\nacross other downstream tasks, a shortfall that lim-\nits the exploration of its potential for wider appli-\ncation. These issues merit further research and\nconsideration."}, {"title": "A Implementation Details", "content": "Toolkit. Figure 5 presents a detailed distribution\nof the Medical Assessment Calculators. The calcu-\nlators we have compiled demonstrate high practi-\ncality in clinical practice, especially in 13 critical\nspecialties including Cardiology, Intensive Care\nMedicine, and Nephrology. These specialties are\ncharacterized by a multitude of test items and rely\nheavily on computational tools to enhance diagnos-\ntic precision. This collection of calculator tools\nis highly reusable and designed to provide strong\nsupport and assistance for a broad range of medical\nassessment activities, thereby optimizing the clin-\nical decision-making process and enhancing the\nquality of healthcare services. The code for this\ntoolkit is generated by GPT-4 (Achiam et al., 2023)\nand has been manually validated.\nLLM Used. All other components of MeNTi\nuniformly utilize the PULSE model (Zhang et al.,\n2023b) as their core computational unit. The\nPULSE model, optimized specifically for the medi-\ncal field, is an LLM that has demonstrated superior\nperformance and expertise in medical-related tasks.\nThe infrastructure of MeNTi demonstrates a high\ndegree of generalization allowing us to replace the\ncore backbone with other LLMs. In our experi-\nments, we switch to GPT-3.5-turbo and GPT-40,\nachieving outstanding performance.\nDense Retrieve Used. In the meta-tool of\nthe MeNTi system, the Retrieval component em-\nploys the Dense Retrieve method for tool selection.\nSpecifically, we use the highly acclaimed M3E re-\ntrieval model(Wang Yuxin, 2023). This model is\nwidely recognized for its outstanding performance\nin semantic representation and similarity calcula-\ntion.\nAnnotation Details. The cases in the annota-\ntions all originate from China. Each question of\nCalcQA required approximately 20 minutes to an-\nnotate and review. The cost is $40 per hour for the\nphysicians."}, {"title": "B Prompts", "content": "Toolkit Code Generation Prompt\nYou are a code generation model for\ncalculations. You will receive the name,\ndescription, and calculation formula of a\nmedical calculator. Your task is to\ngenerate a Python function to calculate\nthis calculator based on the provided\nmedical calculator information.\nThe requirements are as follows:\n1. Your variable names should be as generic and\nmeaningful as possible for easy\nunderstanding.\n2. You need to provide a docstring that\nintroduces the function's functionality,\nuses, application scenarios, etc.\n3. Your docstring should include detailed\ndescriptions of the function's parameters,\ntheir types, and the rules for populating\nthem, to ensure the function can be used\ncorrectly.\n4. To enhance versatility, if any of the\nparameters are of string type, you should\nprovide a list of possible types and change\nthe parameter to the index of that list.\nThe function should be wrapped by\n'''python\n'''\nBegin!\ncalculator name: INSERT_NAME_HERE}\ndescription of calculator: INSERT_DESCRI_HERE}\nformula of calculator: INSERT_FORMULA_HERE}\nMeta-Tool Diagnosis Prompt\nYou are a medical diagnostic model. Your task\nis to analyze the abnormal parts of the\nprovided patient's case and speculate on\nwhich bodily functions might be impaired.\nThe requirements are as follows:\n1. Every inference you make must be\nsubstantiated by actual evidence from the\nprovided patient's case.\n2. You only need to analyze the main, abnormal\nparts of the provided patient's case.\n3. You just need to make a brief analysis.\nBegin!\nMeta-Tool Classifier Prompt\nYou are a toolkit selection model. Below is a\ntoolkit list and their descriptions, and\nyou need to select the appropriate toolkit\nbased on the user query. Your answer should\nbe wrapped by '''json and\n'''\ntoolkit list: [\"unit\", \"scale\"]\n\"unit\": This is a unit toolkit that contains a\nvariety of medical unit conversion tools.\nWhen you need to perform unit conversions,\nyou will need to select this tookit.\n\"scale\": This is a medical calculator toolkit,\nwhich is used for assessing and quantifying\nspecific health conditions of individuals\nin the medical field. When you need to\nanalyze a patient's health condition, or\nwhen a user has made a query regarding\nhealth status, you will need to select this\ntoolkit.\nRequirements:\n1. You can only select one toolkit, and it must\nbe from the toolkit list provided.\n2. You need to output a JSON file.\n3. Your answer should be wrapped by '''json and\n'''"}, {"title": "Meta-Tool Rewriter Prompt", "content": "You are a retrieval-augmented model for\nrewriting queries. You will receive a query\nfrom a doctor and a patient's case\nanalysis. Your task is to combine the\npatient's case analysis to expand and\nrewrite the doctor's input query, making\nthe doctor's query more aligned with the\npatient's actual situation.\nThe requirements are as follows:\n1. The generated queries must not alter the\ndoctor's original intent.\n2. The generated queries must be closely\nsimilar in meaning to the original query,\nbut the meanings should differ slightly\nfrom each other.\n3. You should extract insights from the patient\ncase analysis that may be related to the\ndoctor's query to generate new queries, in\norder to facilitate the retrieval of more\ninformation.\n4. However, please prioritize the original\nquery; the additional information in each\ngenerated query should not be too much to\navoid obscuring the content of the original\nquery."}, {"title": "Slot Filling Prompt", "content": "You are a parameter extraction model. You will\nreceive a Reference Text and a Function\nDocstring. Your task is to determine the\nparameters from the Reference Text based on\nthe parameter filling rules described in\nthe Function Docstring, including the\nvalues and units of the parameters.\nThe requirements are as follows:\n1. The Value and Unit of parameters you output\nneed to be strictly in accordance with the\nReference Text. You are prohibited from\nperforming unit conversions.\n2. If there is a discrepancy in the unit of the\nparameter between the Reference Text and\nthe Function Docstring, please use the unit\nfrom the Reference Text as the standard. Do\nnot convert the units on your own.\n3. If the parameter does not have a unit,\noutput 'null' in the Unit.\n4. All parameters in the Function Docstring\nmust be included in the parameter list. If\nthe parameter values are missing, fill them\nrandomly. The Value must not be 'null'.\n5. For parameters that do not have a clear\nrating in the Reference Text, please infer\nand fill them out based on the actual\ncircumstances described in the reference\ntext and the scoring standards provided in\nthe Function Docstring.\n6. You need to first produce a step-by-step\nanalysis, considering each parameter\nindividually.\n7. The Parameters List you output is a JSON\nfile, and this JSON file should be wrapped\nby '''json and\n'''\nPlease follow this output format:\n'''json\n{The parameters list here.}\n'''\nHere are some examples:\nFunction Docstring:\n{{\"Calculate the Body Mass Index (BMI) for an\nindividual.\\n\\nArgs:\\nweight (float): The\nweight of the individual in\nkilograms.\\nheight (float): The height of\nthe individual in\ncentimeters.\\n\\nReturns:\\nfloat: the BMI\n(kg/m^2).\\n\\nDescription:\\nThe Body Mass"}, {"title": "Nested Calling Prompt", "content": "You are a Parameter List checking model.\nYou will receive a Function Docstring, and\nParameter List. You need to verify that the\nentries in the Parameter List comply with\nthe requirements described in the Function\nDocstring, including the Value and Unit.\nIf all units are consistent, choose\n\"calculate\". If there are any discrepancies\nin the units, choose \"toolcall\".\nYou should not perform unit conversions\ndirectly. When converting units is needed,\nyou must choose \"toolcall\" and elaborate on\nthis unit conversion task in the\n\"supplementary_information\", including the\nparameter value, the current unit of the\nparameter, and the target unit of the\nparameter.\nRequirements:\n1. You need to conduct a detailed, step-by-step\nanalysis of each parameter in the parameter\nlist. You need to output each parameter's\nFunction Docstring individually, then\nanalyze and compare them.\n2. You especially need to compare the Unit of\nthe Parameter List with the units required\nin the Function Docstring.\n3. If the units are inconsistent, please select\n\"toolcall\" and specify the numerical value\nof the parameter required for unit\nconversion, as well as the units before and\nafter the conversion in the\n\"supplementary_information\".\n4. The unit conversion task may require"}, {"title": "D Inference Example", "content": "User Query\nWhat scale should be used to assess a pa-\ntient's risk of Coronary heart attack?\nBasic information: male, 49, civil servants\nChief Complaints: Chest tightness and\nshortness of breath January\nHistory of present disease: 1 month ago,\nthere was no incentive for chest tightness\nand asthma, mostly at night, each lasting\nabout 1 hour, can be alleviated by itself, no\ndizziness, headache, syncope, dark day, nau-\nsea, vomiting, cough, phlegm, palpitations,\nabdominal pain, diarrhea, edema of both\nlower limbs and other discomfort. Chest\nCT showed: a high high-density shadow of"}]}