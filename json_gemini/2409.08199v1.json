{"title": "AudioBERT: Audio Knowledge Augmented Language Model", "authors": ["Hyunjong Ok", "Suho Yoo", "Jaeho Lee"], "abstract": "Recent studies have identified that language models, pretrained on text-only datasets, often lack elementary visual knowledge, e.g., colors of everyday objects. Motivated by this observation, we ask whether a similar shortcoming exists in terms of the auditory knowledge. To answer this question, we construct a new dataset called AuditoryBench, which consists of two novel tasks for evaluating auditory knowledge. Based on our analysis using the benchmark, we find that language models also suffer from a severe lack of auditory knowledge. To address this limitation, we propose AudioBERT, a novel method to augment the auditory knowledge of BERT through a retrieval-based approach. First, we detect auditory knowledge spans in prompts to query our retrieval model efficiently. Then, we inject audio knowledge into BERT and switch on low-rank adaptation for effective adaptation when audio knowledge is required. Our experiments demonstrate that AudioBERT is quite effective, achieving superior performance on the AuditoryBench. The dataset and code are available at https://github.com/HJ-Ok/AudioBERT.", "sections": [{"title": "I. INTRODUCTION", "content": "The advance of pretrained language models has spurred significant improvements across various language-related tasks [1], [2], and has been extended to processing of multimodal information [3], [4]. However, the limitation of popular language models is that they are pretrained only on textual data, which potentially leads to a lack of knowledge related to information from other domains. Indeed, in visual domains, it has been repetitively discovered that common language models do not have sufficient visual commonsense knowledge, such as the color of common objects, leading to a poor performance on visual tasks [5]-[8]. In response to this limitation, recent works have developed algorithms to augment language models with visual knowledge [9]-[12].\nDoes the same limitation hold for auditory tasks? This question, unfortunately, has not been clearly addressed yet; although a recent work studies the effectiveness of audio snippet embeddings in the language model representation space [13], it is not known whether the language models have rich commonsense knowledge regarding the auditory signals, e.g., which animal make a specific sound (Fig. 1).\nTo answer this question, we present AuditoryBench, the first benchmark dataset (to our knowledge) for evaluating the language models' auditory knowledge. In particular, we propose two auditory knowledge tasks: (1) animal sound recognition and (2) sound pitch comparison. The animal sound recognition task asks the language model to predict which animal is likely to make a sound that corresponds to specific echoing words (i.e., onomatopoeia), such as \u201cmeow.\u201d The sound pitch comparison task asks the language model to predict which sound source (e.g., musical instruments, objects, or environments) is likelier to produce sound with a higher pitch. To construct this benchmark, we propose an LLM-based data-processing pipeline for the sake of the scalability of the benchmark dataset [14], [15].\nUsing AuditoryBench, we discover that language models severely lack auditory commonsense knowledge. In particular, we test three different language models\u2014BERT [1], Gemma [16], and LLaMA [17]\u2014and find that all models achieve low predictive accuracy in both benchmark tasks (Table III).\nTo address this shortcoming, we propose AudioBERT, a simple yet effective retrieval-based framework for injecting auditory knowledge into language models. Our approach involves detecting text spans where auditory knowledge is necessary. Whenever needed, relevant audio is retrieved by querying the detected text span to CLAP [18], a model that measures the text-audio similarity. Then, the embedding of the retrieved audio sample is injected into the language model. Upon identification of auditory spans by the detector, the language model activates Low-Rank Adaptation (LoRA) [19] weights, which is finetuned with AudiotoryBench, which maintains its pretrained knowledge makes the model perform well in other tasks by deactivating LoRA weights. Our experi-"}, {"title": "II. METHODS", "content": "A. AuditoryBench\nTo assess the auditory commonsense knowledge, we con- struct a dataset called AuditoryBench. AuditoryBench is based on LAION-Audio-630K [20], a large-scale audio-text dataset with over 600,000 audio-text pairs.\nThe first step to construct AuditoryBench is to categorize each audio sample of the dataset by processing the paired text information with an LLM (Fig. 2(a)); the text information con- sists of the audio metadata (description and tag) and the paired text. Based on this information, we give a detailed instruction to categorize hierarchically (e.g., \u201canimal/dog/pomeranian\") with few-shot examples.\nThe next step is to construct tasks based on the generated categories and the text information. We design two tasks: Animal sound recognition and sound pitch comparison.\nAnimal sound recognition: This task asks to predict an animal from the given onomatopoeia that corresponds to an animal' sound. Each datum consists of a prompt (e.g., \"meow is the sound a [MASK] makes\"), an answer (e.g., \"cat\"), and an onomatopoeic span (e.g., \"meow\"). This triplet is generated by first selecting a datum that is categorized as \u201canimal,\u201d and processing its text information with an LLM; see Fig. 2(b) for an overall illustration and the prompt used. To enhance the data quality, we have additionally employed a human anno- tator to filter our inappropriate animal answers(e.g., \u201czombie, vampire\u201d).\nUsing this approach, we have generated a task with 6,015 samples. The dataset is then split into training, development, and test sets with 70%/10%/20% of samples. We have also collected animal sound data from Wikipedia,\u00b9 allocating them as additional test data. This additional set lets us assess the generalizability and quality of the dataset.\nSound pitch comparison: This task asks to compare the pitch of two different sound sources. Each datum consists of a prompt (e.g., \"The sound of a synthesizer typically has a [MASK] pitch than an acoustic bass\"), an answer (e.g., \"higher\"), and two auditory spans (e.g., \u201csound of a\nB. AudioBERT\nNow, we introduce our retrieval-based framework, which is coined AudioBERT. AudioBERT employs two models as its building block: An auditory knowledge span detector, which identifies the spans related to auditory knowledge within the given text, and CLAP [18], a model that is used for retrieval. In what follows, we first describe each component (in Sections II-B1 and II-B2, respectively) and then describe the overall AudioBERT framework in Section II-B3.\n1) Auditory knowledge span detector: This model extracts the audio-relevant span from the given text, which can work as a good query. To obtain such model, we simply train a trans- former encoder [23] to classify the auditory knowledge span from other tokens, using the cross-entropy loss (Fig. 3(a)).\n2) CLAP: Audio-text contrastive learning: To retrieve the relevant audio from the detected auditory knowledge span, we utilize the audio-text model called CLAP [18]. CLAP is trained via audio-text contrastive learning: Given a batch of N audio- text pairs, the cosine similarity between the representations of matched pairs (positive) is maximized, while the cosine similarity is minimized in unmatched (negative) pairs. The loss function is formalized as follows:\n$L_{Audio} = - \\frac{1}{N} \\log \\frac{\\sum_{i=1}^{N} exp(cos(A_i, T_i) / \\tau)}{\\sum_{j=1}^{N} exp(cos(A_i, T_j) / \\tau)}$\n(1)\n$L_{Text} = - \\frac{1}{N} \\log \\frac{\\sum_{i=1}^{N} exp(cos(A_i, T_i) / \\tau)}{\\sum_{j=1}^{N} exp(cos(A_j, T_i) / \\tau)}$\n(2)\n$L = \\frac{1}{2}(L_{Audio} + L_{Text})$\n(3)\nAk and Tk are the audio and text embeddings, cos(,) denotes cosine similarity, and \u03c4 is a temperature parameter.\n3) AudioBERT: AudioBERT injects auditory knowledge into a language model using the auditory knowledge span detector and CLAP. Our approach works in three steps: We retrieve the relevant audio from the database using the span detector and the text encoder of CLAP, generate embedding using the CLAP audio encoder, and add this embedding to the first token of the auditory knowledge span (Fig. 3(c)). During the training, we employ masked language modeling loss and LORA to finetune the language model's LoRA weights and the audio encoder while keeping the other parameters free. For in- ference, the auditory knowledge span detector first determines whether auditory knowledge is required. If not, inference proceeds without activating the LoRA weights, maintaining the model's original pretrained weights. However, for tasks that require auditory knowledge, the auditory knowledge span detector recognizes this requirement, retrieves relevant audio, activates the LoRA weights, and injects the audio embedding into the model. This novel architecture enables AudioBERT to dynamically adapt to tasks requiring auditory knowledge"}, {"title": "III. EXPERIMENTS", "content": "Evaluation metrics: We employ accuracy and F1-score for our metrics. Higher scores indicate better performance.\nImplementation details: In our implementation, we em- ploy a BERT-base model for the auditory knowledge span detector. We trained with 5 epochs with a batch size of 16, a learning rate of 1\u00d710\u22125, and utilizing AdamW [25] optimizer.\nFor AudioBERT training, we experimented using BERT for the language model and employed an AST [26] encoder for auditory knowledge embedding injecting. We trained with 20 epochs with a batch size of 32, a learning rate of 3 \u00d7 10\u22124, and utilizing AdamW optimizer. For LoRA, we set the rank and alpha to 64 and 128, respectively.\nWe report the average score from 5 runs with different random seeds for each setting."}, {"title": "IV. RESULTS", "content": "A. Experiments results\n1) Auditory knowledge span detector: We report our audi- tory knowledge span detector performance in various trained data in Table III. The results suggest that training on single data may perform poorly, but the combined dataset works even with performance gain in sound pitch comparison.\n2) AudioBERT: To evaluate the effectiveness of our ap- proach, we compared our model against the existing language models: BERT, ROBERTa, Gemma2-2B, and LlaMA3.1-8B. As shown in Table III, language models show the absence of auditory knowledge, and AudioBERT exhibits competitive performance, demonstrating its effectiveness in augmenting auditory knowledge."}, {"title": "V. CONCLUSIONS", "content": "We introduce AuditoryBench, the first benchmark to assess the auditory knowledge of language models and propose AudioBERT, a novel method for injecting auditory knowledge into language models. We believe that our discovery of the lack of auditory knowledge in LMs can have implications for audio- language multimodal research and contribute to the research of language models effectively adapting to diverse modalities."}]}