{"title": "Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-Based Sampling", "authors": ["Minhyuk Seo", "Hyunseo Koh", "Jonghyun Choi"], "abstract": "The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory. However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget. Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same \u2018total resource budget.' To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy. In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations. Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget.", "sections": [{"title": "Introduction", "content": "For the practicality of online continual learning (CL), the scenarios assumed by most online CL methods impose resource restrictions, such as the single training epoch and the size of the replay memory that limits the number of streamed samples that can be stored (Koh et al., 2022; Wang et al., 2022a). Although the \u2018one-epoch training' may give a rough sense of the computational constraint, the actual computation budget to train the models differs per method (Prabhu et al., 2023; Verwimp et al., 2023; Ghunaim et al., 2023) since each method requires a different amount of computations in a single epoch. Several rehearsal-based CL methods require additional storage to store the previous models and logits (i.e., the unnormalized model output vector) (Buzzega et al., 2020; Koh et al., 2023; Zhou et al., 2023), which was usually not included in the memory budget, which mainly considers the size of episodic memory to store samples in previous tasks. To this end, we compare CL methods with the same computational and memory budget considering all storage and computational costs. We argue that the total budget of memory and computation will ensure the practicality of the proposed online CL algorithms.\nFor a fair comparison in computational budget, we use training FLOPs per sample as the metric instead of the number of epochs, since some methods require a lot of computations in a single epoch, while others do not. FLOPs is an exact computational budget regardless of implementation details (Korthikanti et al., 2023), following (Zhao et al., 2023; Ghunaim et al., 2023).\nFor the same memory budget, we use an aggregated budget for various forms of extra storage, including replay memory, logits, and model parameters. Following (Zhou et al., 2023), we convert all additional storage costs into byte and sum them up to obtain the actual memory cost. Upon comparing the results under fair cost, we found that the performances were often different from what was reported in the original papers proposing each method."}, {"title": "Related Work", "content": ""}, {"title": "Online Continual Learning with Memory Budget", "content": "Replay-based online CL methods use episodic memory and consider the memory budget. Since we also consider using episodic memory, we review them in detail as follows. Replay-based methods (Aljundi et al., 2019b; Bang et al., 2021; Koh et al., 2022) store part of the past data stream in episodic memory to replay them in future learning. Although there are simple sampling strategies such as random sampling (Guo et al., 2020) and reservoir sampling (Vitter, 1985), they are often insufficient to adapt to changing data distributions. Rather than simple methods, researchers have developed advanced sampling strategies considering factors such as uncertainty (Bang et al., 2021), importance (Koh et al., 2022), and gradient (Tiwari et al., 2022). However, these advanced methods often entail a high computational overhead, making them impractical for deployment in real-world applications. RM (Bang et al., 2021) requires a significant amount of computational cost to calculate the uncertainty for diversified sampling. Similarly, CLIB (Koh et al., 2022) involves additional forward and backward passes to calculate the decrease in loss for each batch iteration.\nIn addition to the memory management schemes, researchers investigate the memory usage schemes, i.e., sample retrieval strategies from the rehearsal buffers. In addition to random retrieval (Chaudhry et al., 2019), the determination of retrieval based on the degree of interference (Aljundi et al., 2019a) and the adversarial Shapley value (Shim et al., 2021) has been explored. However, such methods require an inference of candidate samples, which leads to a nontrivial amount of computation in computing the loss (Aljundi et al., 2019a) or the Shapely value (Shim et al., 2021)."}, {"title": "Layer Freezing", "content": "Freezing layers have been investigated to reduce computational costs during training in joint training (i.e., ordinary training scenario other than CL) (Brock et al., 2017; Xiao et al., 2019; Goutam et al., 2020). A common freezing approach (Wang et al., 2023; Li et al., 2022) includes determining whether to freeze a layer, based on the reference model and representation similarity, such as CKA (Cortes et al., 2012) and SP loss (Tung & Mori, 2019). Additionally, EGERIA (Wang et al., 2023) unfreezes layers based on changes in the learning rate.\nHowever, in CL, both online and offline, it is challenging to determine when to freeze a layer because metrics such as Euclidean distance and CKA cannot be used to compare the degree of convergence compared to the reference model (Mirzadeh et al., 2020). Additionally, continual learning involves a non-i.i.d. setup, where the data distribution continues to change (Criado et al., 2022). Therefore, in addition to changes in learning rate, it is important to consider the current data distribution when determining whether to freeze or unfreeze a layer in continual learning. (Hayes et al., 2020) have explored freezing methods for continual learning. However, they use predefined freezing configurations such as the freezing backbone block 0 after task 1, while our freezing method adaptively freezes the layers using information per batch."}, {"title": "Approach", "content": "For efficient learning in computation and storage budget, we consider two strategies; (1) reducing the computational cost of each iteration and (2) reducing the number of iterations. To implement both strategies, we propose a method employing two techniques; (1) adaptive layer-freezing and (2) similarity-aware retrieval of samples from episodic memory.\nSpecifically, for every training batch, the adaptive layer freezing method adaptively freezes layers so that the amount of information that can be gained from the mini-batch is maximized relative to the required computation. The memory retrieval method retrieves training batches that the model has not learned sufficiently using the number of times each sample has been used for training, i.e., use-frequency, and class-wise gradient similarity. This allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations, consequently reducing the overall number of training iterations. We call our method adaptive Layer freezing and Similarity-Aware Retrieval (aL-SAR), illustrating the gradient update procedure of the proposed aL-SAR in Fig. 2 and providing a pseudocode in Sec. A.3."}, {"title": "Adaptive Layer Freezing for Online CL", "content": "To reduce the computational cost of learning a neural network with minimal accuracy loss, there have been several studies on the freezing of neural network layers in non-CL literature (Liu et al., 2021; He et al., 2021; Wang et al., 2023). These methods often rely on the learning convergence of each layer to determine which layers to freeze since converged layers no longer require further training. However, in online CL, achieving convergence is often challenging due to the limited training budget and the ever-evolving training data distribution. This requires a new approach to determine when and which layers to freeze for the incoming data in the online CL scenarios."}, {"title": "Selectively Freezing Layers by Maximum Fisher Information (FI)", "content": "For a computationally efficient freezing criterion in online CL, we propose to freeze layers that learn little information per computational cost by measuring 'information' (I) gained by each layer during training. Here, we define the information (I) using Fisher Information (FI), since it is a widely used metric to measure the amount of information that each parameter in a neural network obtains from data (Durant et al., 2021; Desjardins et al., 2015; Ollivier, 2015). So, we use FI to measure the layer-wise information that each layer gains from the input data to determine which layers to freeze. Note that freezing some layers facilitates training a model for more mini-batches within a fixed computational budget as it reduces the computations per mini-batch.\nHowever, as a trade-off, as the number of frozen layers increases, the number of updated parameters decreases, reducing the amount of information obtained per mini-batch. To maximize the information (I) in the model while minimizing the computational cost (C), we propose to maximize the expected amount of information gained per computation (I/C).\nFormally, we try to find $n_{\\text{max}} = \\text{arg max} (I/C)_n$ for $n \\in [1, L]$ where we define $(I/C)_n$ as the amount of information gained per computational cost when updating the model with layers 1 to n frozen. L refers to the total number of layers. To compute $(I/C)_n$, we factorize it as:\n$(I/C)_n = (I/mb)_n \\cdot (mb/C)_n$, (1)\nwhere 'mb' refers to the mini-batch. $(I/mb)_n$ and $(mb/C)_n$ represent the amount of information gained per mini-batch and the number of mini-batch iterations per computation, respectively, when layers 1 to n are frozen."}, {"title": "Amount of Information Gained per Mini-batch $(I/mb)_n$", "content": "To compute $(I/mb)_n$, we use $F(\\theta_i)$, Fisher Information Matrix $F(\\theta)$ of layer i, where $\\theta$ and $\\theta_i$ denote the parameters of the model $p_{\\theta}(\\cdot)$ and the parameters of the layer i of $p_{\\theta}(\\cdot)$, respectively. But computing all components of $F(\\theta_i)$ is costly as Hessian is required, which involves second-order derivatives and can be computationally inefficient. To avoid the cost, we use first-order approximation of $F(\\theta)$ by using the diagonal components of the $F(\\theta_i)$ following (Kirkpatrick et al., 2017; Soen & Sun, 2021), i.e., using the trace operator tr() as:\n$(I/mb)_n = \\sum_{i=n+1}^{L} \\text{tr}(F(\\theta_i))$, where $F(\\theta_i) = E_{p_{\\theta}(z), z \\in \\mathcal{D}} \\left[\\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_i^T} \\right]$,\n(2)\nwhere $\\mathcal{D}$ is the data stream, $z \\in \\mathcal{D}$ is the training data batch in the data stream, and $l = \\text{log } p_{\\theta}(z)$ is the loss function. Note that since layers 1 to n are frozen, the model gains information of layers n + 1 to L, the unfrozen layers only."}, {"title": "Number of Mini-batch per Computational Cost $(mb/C)_n$", "content": "To compute $(mb/C)_n$, we initially determine its inverse, denoted $(C/mb)_n$, which represents the computational cost per mini-batch when layers 1 to n are frozen. We calculate $(C/mb)_n$ by splitting it into forward and backward propagation as: $(C/mb)_n = \\sum_{i=1}^{n} (FF)_i + \\sum_{i=n+1}^{L} (BF)_i$, where $(FF)_i$ and $(BF)_i$ refer to the forward FLOPs and the backward FLOPs of layer i, respectively. By taking an inverse of it, we obtain\n$(mb/C)_n = \\frac{1}{\\sum_{i=1}^{n} (FF)_i + \\sum_{i=n+1}^{L} (BF)_i}$,\n(3)\nNote that since layers 1 to n are frozen, no backward computation is performed for those layers. When the number of frozen layers (i.e., n) increases, the number of layers performing backward operations is reduced, i.e., $\\sum_{i=n+1}^{L} (BF)_i$ decreases, thus leading to an increase in the possible number of mini-batch within the given computational budget C.\nCombining Equation 2 and Equation 3, we can finally calculate $(I/C)_n$, which represents the information the model can gain within a given computational budget when layers 1 to n are frozen, by a product of $(I/mb)_n$ and $(mb/C)_n$:\n$(I/C)_n = (I/mb)_n \\cdot (mb/C)_n = \\frac{\\sum_{i=n+1}^{L} \\text{tr}(F(\\theta_i))}{\\sum_{i=1}^{n} (FF)_i + \\sum_{i=n+1}^{L} (BF)_i}$.\n(4)\nBy freezing layer 1 to layer $n_{\\text{max}} = \\text{arg max}_m (I/C)_n$, we can maximize the expected amount of information gained per computation during training."}, {"title": "Batch-wise Freezing for Online CL", "content": "$(I/C)_n$ is supposed to be calculated on the entire data stream since $F(\\theta)$ is defined as an expectation over the whole dataset in Equation 2. Thus, $(I/C)_n$ does not account for the variable amount of information of each batch. In online CL where the incoming batch distribution continuously shifts, the variation is not negligible. Specifically, for batches containing sufficiently trained classes, it is advantageous to freeze more layers, while for batches with insufficiently trained classes, it is beneficial to freeze fewer layers. To address this, we propose the batch freezing criterion (BFC) which quantifies the net benefit in the information gained by the model when we freeze layers given an input batch $z_t$.\nTo define the BFC, we compare (1) the amount of information we would lose from the current batch by freezing and (2) the expected amount of information we would gain in the future using the saved computation from freezing. Then, we can estimate the net benefit from freezing in terms of the information gained by the model by subtracting (1) from (2).\nTo estimate (1), we use the trace of layer-wise FI, i.e., $\\text{tr}(F(\\theta_i))$, defined in Equation 2. Note that $F(\\theta_i)$ is the FI over the whole dataset; thus we convert it into batch-specific FI $F_{z_t} (\\theta_i) = E_{p_{\\theta}(z), z \\in z_t} \\left[\\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_i^T} \\right] \\frac{E_{p_{\\theta}(z), z \\in z_t} \\left[\\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_i^T} \\right]}{E_{p_{\\theta}(z), z \\in \\mathcal{D}} \\left[\\frac{\\partial^2 l}{\\partial \\theta_i \\partial \\theta_i^T} \\right]}$ by multiplying the ratio of the FI of the batch relative to the FI of the entire dataset. Using $F_{z_t} (\\theta_i)$, the amount of information we would lose from the current batch $z_t$ by freezing layers 1 to n is obtained as $\\sum_{i=1}^n \\text{tr}(F_{z_t}(\\theta_i))$. Please refer to Sec. A.1 for more details on the estimation of $F_{z_t} (\\theta_i)$ from $F(\\theta)$.\nTo estimate (2), we calculate the amount of computations saved by freezing layers 1 to n, and the amount of information we can obtain from the saved computations. Here, the saved computation refers to the sum of the backward FLOPs of the frozen layers, denoted as $\\sum_{i=1}^n (BF)_i$. Using $(I/C)_n$ defined in Equation 4, we estimate the expected information (I) obtainable from the saved computations (C) by multiplying the saved computation and $(I/C)_n$. With optimal freezing that maximizes (I/C), the anticipated information obtained from the saved calculations is $\\text{max}_m (I/C)_m \\cdot \\sum_{i=1}^n (BF)_i$.\nFinally, by subtracting (1) from (2), we obtain BFC$(z_t)_n$ as:\nBFC$(z_t)_n = \\text{max}_m (I/C)_m \\sum_{i=1}^n (BF)_i - \\sum_{i=1}^n \\text{tr} (F_{z_t}(\\theta_i))$.\n(5)\nThe positive BFC$(z_t)_n$ implies that freezing layer 1 to n is beneficial in terms of information, and a negative value indicates otherwise. By freezing layer 1 to $n_{\\text{max}}(z_t) = \\text{arg max}_n \\text{BFC}(z_t)_n$, we can select the most beneficial freezing strategy for batch $z_t$. We argue that it allows us to dynamically select the freezing strategy by the learning capacity of the layers and the batch's informativeness. We empirically demonstrate the distribution of the BFC$(z_t)_n$ in Sec. A.10."}, {"title": "Similarity-Aware Retrieval based on 'Use-Frequency'", "content": "In rehearsal-based CL methods, sample retrieval strategies such as MIR (Aljundi et al., 2019a) and ASER (Shim et al., 2021) have a high computational cost, requiring multiple additional model inferences. Thus, in the same computational budget, their performances are surprisingly inferior to simple random retrieval (i.e., ER) as shown in Fig. 1. Here, we propose a computationally efficient sample retrieval strategy that does not require additional model inference."}, {"title": "Ethics Statement", "content": "We propose a better learning scheme for online continual learning for realistic learning scenarios. While the authors do not explicitly aim for this, the increasing adoption of deep learning models in real-world contexts with streaming data could potentially raise concerns such as inadvertently introducing biases or discrimination. We note that we are committed to implementing all feasible precautions to avert such consequences, as they are unequivocally contrary to our intentions."}, {"title": "Reproducibility statement", "content": "We take reproducibility in deep learning very seriously and highlight some of the contents of the manuscript that might help to reproduce our work. We will definitely release our implementation of the proposed method in Sec. 3, the data splits and the baselines used in our experiments in Sec. 4."}]}