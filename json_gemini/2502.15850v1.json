{"title": "Forecasting Frontier Language Model Agent Capabilities", "authors": ["Govind Pimpale", "Axel H\u00f8jmark", "J\u00e9r\u00e9my Scheurer", "Marius Hobbhahn"], "abstract": "As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use \u201cone-step\" approaches that predict benchmark scores from input metrics like compute or model release date directly or \"two-step\" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date\u2192Elo\u2192Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.", "sections": [{"title": "1 Introduction", "content": "Large language models are increasingly trained and deployed as autonomous agents capable of independently pursuing goals and executing longer-form real-world tasks. This rapid advancement creates a need to forecast when these systems will reach critical capability thresholds in order to understand and prepare for their effects on society and the economy.\nWhile there are several approaches that forecast LM capabilities, they face key limitations. Methods like Observational Scaling Laws (Ruan et al., 2024) and Sloth (Polo et al., 2024) can only make predictions after evaluating models from a new model family on multiple benchmarks to derive their predictive metrics, such as PC-1 scores (which correspond directly to general capabilities) (Ruan et al., 2024) or family-specific efficiency parameters (which can be used to convert FLOP to general capabilities) (Polo et al., 2024)\u00b9. Other approaches that predict directly from compute (Owen, 2024) or release date are either less accurate (Ruan et al., 2024) or have not been systematically evaluated.\nBacktesting these six methods, we find that the two-step approach with a linear relationship between date and Elo and a sigmoidal relationship from Elo to benchmark performs competitively and has publicly available data for frontier models. Our forecasts focus on frontier performance, the performance of the best-known model at a given time or"}, {"title": "2 Methods", "content": "We evaluate six different approaches to predict frontier LM capabilities. The simplest approaches try to directly predict frontier benchmark performance from an input variable such as training log-FLOP or release date.\nWe contrast such one-step approaches with two-step approaches where we first use the input variable to predict an intermediate capability metric (e.g. PC-1 or Elo), and then use that capability metric to forecast the downstream benchmark performance. These variables can be combined in four different ways. When combined with the one-step approaches, this results in six different methods to forecast the performance on a target benchmark (see Figure 2)."}, {"title": "2.1 Prediction Tasks", "content": "In the following, we describe which input variables and downstream benchmarks we use."}, {"title": "2.1.1 INPUT VARIABLES", "content": "Input variables are broad and general quantities that we expect to have predictive power for downstream performance. They don't require knowing any specifics about the model, e.g. architecture or exact training procedure.\nScaled training log-FLOP: The amount of compute used to train the model measured in FLOP. Previous literature has found an approximately log-linear relationship between input compute and model performance (e.g. Finnveden, 2020; Owen, 2024). Sevilla et al. (2022) observes that the amount of compute utilized by large scale pre-training runs tends to double approximately once every 9 months, providing us with a broad reference class for the compute requirements of frontier model training.\nIn this paper, we always use scaled FLOP as described in Owen (2024). It is a common practice to \u201covertrain\u201d models (Shafkat, 2023; Dubey et al., 2024) by reducing parameter count and increasing dataset size beyond what would be optimal under Hoffman scaling laws (Hoffmann et al., 2022) to reduce cost at inference time. Therefore, raw (unscaled) FLOP estimates are less comparable. We can overcome this by normalizing all models to the lowest possible FLOP count that would achieve the same loss using Hoffman scaling laws. See Appendix A for details.\nNote that we only focus on pre-training FLOP and do not take into account post-training such as RLHF/RLAIF (Ouyang et al., 2022; Bai et al., 2022), nor inference time compute. The \u201cinference compute\" paradigm started by OpenAI's o1 (OpenAI, 2024a) is not accounted for in our methodology. Column 1 of Figure 3 depicts the relationship between scaled log-FLOP and intermediate capability metrics.\nRelease date: The date that the model was officially released for public use. While there is no inherent reason to assume that the release date influences the performance of the model, we speculate that it is a good aggregate measure of both algorithmic improvements (Ho et al., 2024; Xiao et al., 2024; Erdil & Besiroglu, 2023) and increased training compute (Sevilla et al., 2022), especially for frontier models.\""}, {"title": "2.1.2 TARGET BENCHMARKS", "content": "Target benchmarks are measures of downstream performance that compare the results of different models. In our specific case, we use benchmarks on the OpenLLM v2 leaderboard (Fourrier et al., 2024) for backtesting. They include IFEval (Zhou et al., 2023), BBH (Suzgun et al., 2022), MATH LVL 5 (Hendrycks et al., 2021), GPQA (Rein et al., 2023), MUSR (Sprague et al., 2024), and MMLU-PRO (Wang et al., 2024). We then use the method that we judge to"}, {"title": "2.2 One-step Forecasting Approach", "content": "\"One-step\" approaches predict the benchmark score from an input variable directly.\nFirst, we identify which models lie on the frontier for each choice of input variable and target benchmark. A point (xi, Yi) is on the frontier if there is no other point (xj, yj) with xj < xi and yj > Yi. Visually, this corresponds to the upper boundary of points in Figure 3.\nFollowing Ruan et al. (2024), we assume a sigmoidal relationship between capability metrics and the eventual benchmark scores. Per input variable x with frontier datapoints xi, and corresponding benchmark scores yi, we fit a, b.2\n$Y_i = \\sigma(ax_i + b)$\n(1)"}, {"title": "2.3 Two-step Forecasting Approach", "content": "For \"two-step\" approaches, we first predict an \"intermediate capability metric\" and then use that to predict the downstream performance.\nThere are a few reasons in favor of a two-step approach. First, it is conceptually plausible that there is some underlying \"general capability\" that is highly predictive of the output variables. Second, the intermediate capability metric might function as a \"regularization\" step, e.g. by projecting all training compute onto GPT-2-equivalent FLOP as shown in Ruan et al. (2024) or onto Hoffmann scaling laws as shown in (Owen, 2024). Third, we might have more data available for either of the two intermediate forecasts, thus improving the overall prediction."}, {"title": "2.3.1 INTERMEDIATE CAPABILITY METRICS", "content": "We aim to reduce the complex behaviors of the model down to a single number as an underlying measure of the latent general capabilities. We have three main criteria for selecting such a metric. First, we need the metric to be easily"}, {"title": "2.3.2 FITTING TWO-STEP APPROACHES", "content": "To fit the relationship between the input variable and the capability metric, we first determine which models are on the frontier as in the \"one-step\" approaches. Then, we fit a linear regression between the frontier points' input variable x and the intermediate capability metric values z.\n$z_i = cx_i + d$\n(2)\nIn the second step, we fit a sigmoid between the capability metric and benchmark score.\n$y_j = \\sigma(ez_j + f)$\n(3)\nWe fit the sigmoid on all available models instead of just the frontier since we expect that frontier models will have the same relationship between the underlying capability and benchmark score as other models."}, {"title": "3 Evaluating approaches through backtesting", "content": "To compare all six of our approaches, we backtest them on existing data from the Open LLM Leaderboard v2 (Fourrier et al., 2024) with six benchmarks: IFEval, BBH, MATH Lvl 5, GPQA, MUSR, and MMLU-PRO. We only use the subset of Open LLM Leaderboard v2 that has Elo scores available, resulting in 38 models (see Appendix F).\nHowever, before we determine which pathway is the most accurate overall, we want to compare the predictive power of input and intermediate variables. In Section 3.1, we backtest individual capability metrics to validate Elo as a potential candidate and compare it to PC-1, scaled log-FLOP, and release date. Then, we move towards testing the entire pathway. In Section 3.2, we backtest all six full approaches."}, {"title": "3.1 Backtesting capability metrics", "content": "To backtest capability metrics, we use expanding window cross-validation Garg et al. (2022) with 3 splits based on release date. We first split our data up into 4 divisions with approximately equal model count. Then, we train a statistical model only on the first split and evaluate it on the second split, another statistical model on the first and second split, and evaluate it on the third, and so on. Our cross-validation methodology is displayed for predictions of MMLU-PRO with the full approach in Figure 4 (see also Section 3.2).\nWe are computing the error of only the capability metric, so we train just the sigmoid from the capability metric to the target benchmark. (Subplot 2 in Figure 2). For PC-1, we avoid testing on the training data and thus omit the benchmark we're predicting when fitting the PCA. Furthermore, we only use the data available up to that point when fitting the"}, {"title": "3.2 Backtesting full approaches", "content": "To backtest the full paths, we use the same expanding window cross-validation procedure. However, there are two important differences. First, we are testing the complete path from input variable to benchmark score, ignoring the internal loss of the S-curve or linear regression subcomponents. Second, we only compute error for data points on the frontier. If there are no frontier points in a split, that split is ignored. We then aggregate the error in each split as usual. Since there are far fewer data points, the error is likely to be noisier.\nOur results (see Table 2) show that the best overall path is going from Release Date\u2192PC-1\u2192Benchmark, with an overall RMSE of 0.082, followed by Date Elo Benchmark with an RMSE of 0.095. Overall, using release date as the input variable outperforms log-FLOP."}, {"title": "4 Predictions for Agentic Benchmarks", "content": "Informed by our backtesting, we now want to apply the most suitable methodology to predict the performance of three LM agent benchmarks."}, {"title": "4.1 Choice of benchmarks", "content": "First and foremost, we want the benchmarks to capture important, economically valuable skills such that our forecasts have meaningful real-world implications. Second, we want to use benchmarks that have a high option space and require repeated interaction with the environment in order to measure agent capabilities rather than pure knowledge. Third, we want the benchmarks to be difficult but have easily verifiable solutions. Finally, we want them to be popular for general validation and to compare performance against other implementations.\nAs such, we use SWE-Bench Verified (Jimenez et al., 2024; OpenAI, 2024b), where all problems have been human-verified and a public leaderboard exists, Cybench (Zhang et al., 2024a), which aims to be representative of real-world cybersecurity work, and RE-Bench (Wijk et al., 2024), which attempts to measure the AI R&D capabilities of LM agents. METR has kindly shared scores for eight frontier models with us METR (2024)."}, {"title": "4.2 Forecasting methodology", "content": "We only use release date as the input variable since training FLOP count is no longer publicly known for most frontier models. Furthermore, we only use Elo as our capability metric since almost all publicly available frontier models are available on Chatbot Arena, but not necessarily all benchmark scores. In Section 3.2, we show that the release Date Elo Benchmark score path performs second-best in backtesting. Thus, it seems like a sufficiently good choice."}, {"title": "4.3 Scaffolding", "content": "We use the same scaffold for both SWE-Bench Verified and Cybench. For RE-Bench, we rely on METR's data, and thus don't have detailed knowledge of which scaffold was used. Our scaffold attempts to be as simple as possible while avoiding simple known pitfalls.\nWe provide the model with three tools: a) A Bash shell, b) a Python shell, and c) a file editing tool that enables the model to view, create, and edit files by searching and replacing text and allows it to undo changes (similar to Anthropic (2024)).\nAll runs have a message cap of 50 messages and 2 million tokens. If the model runs out of context, we delete the earliest non-instruction messages. Prompts for our scaffold are provided in Appendix D."}, {"title": "4.4 Elicitation", "content": "The highest-performing scaffolds for each benchmark typically give more affordances to the model, or provide more inference-time compute. Furthermore, they often integrate prior knowledge about the benchmark into the scaffold, e.g. different prompts for isolating the bug, writing test cases, and retrying for SWE-Bench Verified.\nSince our simple scaffold makes no use of additional inference compute, such as \u201cbest-of-n\u201d or o1-style inference techniques (OpenAI, 2024a), or highly task-specific prompts, we achieve a score of around 33% on SWE-Bench Verified with Claude-Sonnet-3.5, while the best public scaffold known to be using Claude-Sonnet-3.5 on the SWE-Bench Verified leaderboard achieves 62.2% (Pani, 2024).\nThus, we differentiate between a \u201clow-elicitation\" estimate, which should be seen as a general conservative estimate, and a \"high-elicitation\" estimate, which represents the best publicly known scaffolds at the time taken from publicly available leaderboards. The \u201chigh-elicitation\u201d forecast has the advantage that it predicts the real public frontier, but the disadvantage that the scaffolds are almost always different between data points."}, {"title": "4.5 Results", "content": "We fit the low-elicitation forecast on only data gathered from our simple uniform scaffold. For the high-elicitation forecast, we combine all data points, including both our own scaffold, and data from public leaderboards.\nFigure 1 shows the results of our forecasts until early 2027.\nFor SWE-Bench, we have access to all 17 models tested with our simple scaffold for the low elicitation effort and access to strong elicitation efforts of other groups from the public leaderboard. Our model indicates that by January 2026, models with weak elicitation will achieve 54% on SWE-Bench, and with better elicitation may achieve 87%. However, our model does not take into account the potential for heavily increased test-time scaling, which may further increase performance.\nOur forecast suggests that Cybench scores will be 55% and 66% in January 2026 for low and high-elicitation efforts, respectively. We observe that there is much less difference between the non-elicited and elicited cases, likely because far less effort has gone into eliciting Cybench performance to date.\nOn RE-Bench, we forecast a score of 0.73 by January 2026. Note that METR reported that they did not spend a lot of effort on elicitation, which suggests our estimates might be too conservative. Consequently, we exclude a high-elicitation scenario from our forecasts on this benchmark.\nIn Figure 5 we show the conditional distributions for a fixed benchmark score. We chose a score of 0.9 for SWE-Bench and Cybench as an arbitrary marker of strong performance and a score of 1 for RE-Bench, which is the expert baseline.\nWith high elicitation, we expect SWE-Bench Verified to reach 90% around March 2026, with a 95% CI spanning from October 2025 to September 2027. With standard elicitation, we expect 90% to be reached about two years later, in January 2028.\nFor Cybench, our best guess for high elicitation is December 2026, with a 95% CI from April 2026 to April 2029. Standard elicitation predicts June 2027.\nOur forecast suggests that agent performance on RE-Bench may reach a score of 1-equivalent to the expert baseline reported by Wijk et al. (2024)\u2014around December 2026. We have much more uncertainty about this forecast, and our 95% CI reflects this. It has a span of over 8 years, from August 2025 to May 2033.\nAcross all three benchmarks and elicitation types, we observe that the probability distributions are asymmetric, with a longer right tail. This indicates greater uncertainty about potential delays compared to early achievements."}, {"title": "5 Related Work", "content": "Pre-training scaling laws Hestness et al. (2017) discuss power-law scaling of the error with respect to dataset size and parameter count across multiple domains. Kaplan et al. (2020) discusses specific power law exponents for training transformers optimally given a fixed compute budget, and demonstrate that these scaling laws hold over many orders of magnitude. These parameters are improved by Hoffmann et al. (2022), who introduce \"Chinchilla\u201d scaling laws. Besiroglu et al. (2024) repeat their experiment and further refine the parameters.\nPredicting downstream performance Finnveden (2020) uses data from GPT-3 (Brown et al., 2020) to extrapolate the performance of future models on downstream tasks, including Winograd, SuperGLUE, and ANLI. Owen (2024), models BIG-Bench performance with respect to Scaled FLOP and finds that we can predict aggregated performance but not individual task performance. Ruan et al. (2024), use a PCA on benchmark scores to decompose scores into a low-dimensional capability space. They find that performance across benchmarks can largely be explained by a single principle component which they interpret as a \"general capability factor\". Polo et al. (2024) extend this idea via a low-dimensional \u201cskill\u201d model that can predict across families if smaller models from the same family are available. Zhang et al. (2024b), applies collaborative filtering to handle sparse benchmark scores, i.e. cases where not all models have the same benchmark scores available.\nLess attention has been paid to directly predicting frontier performance instead of average performance. Steinhardt et al. (2022) discusses factors relevant to forecasting LLM performance, including training data availability and algorithmic progress. They attempt to predict frontier models' scores on MATH and MMLU. Villalobos et al. (2024) discusses how much training data future models are likely to have access to in further detail. Ho et al. (2024) discusses the rate of algorithmic progress, or how much improvement we are likely to see due to changes in architecture and improvements in the training process. Cottier et al. (2024) discusses the increasing cost of training the underlying foundation models. Erdil & Sevilla (2023) discusses the base rate of breakthrough developments, and how it contributes to algorithmic progress.\nEmergent capabilities Sudden jumps in capabilities on benchmark performance are often attributed to \"Emergent capabilities.\" Srivastava et al. (2022), noticed that certain tasks in BIG-Bench experienced sudden jumps in capabilities. Wei et al. (2022) discusses these emergent abilities in further detail. Schaeffer et al. (2023) shows that emergent capabilities heavily depend on the choice of grading criterion or metric and that non-linear jumps in one metric can be smooth in another."}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Limitations", "content": "Paradigm changes While this paper does not make any explicit assumptions about the training paradigm of any particular model, we fit almost all predictions on models that were trained with the \u201cpre-training scaling\u201d paradigm, where the primary driver for downstream performance was improvements of pre-training. However, with OpenAI's ol (OpenAI, 2024a), we may start to see a new \u201cinference scaling\" paradigm where models are trained to utilize inference compute much more effectively through reasoning. This might invalidate our predictions and thus provide a reason to assume faster progress than our forecasts would suggest, even for high-elicitation predictions.\nUnderelicitation As discussed in Section 4.4, we did not put a lot of effort into elicitation.\nAs a consequence, we know that our results are significantly below frontier performance and that our \"low-elicitation\" predictions are conservative. Even the \u201cmax-current-elicitation\u201d forecast might underestimate performance due to paradigm changes (see above) or later breakthroughs in agent scaffolding and elicitation.\nSmall sample size Unfortunately, almost by definition, there are only a small number of frontier models. Therefore, our predictions have a small sample size. This is partially mitigated by making use of the two-step methodology and predicting the intermediate variable independently. However, we think the small sample size should imply large uncertainty about our forecasts."}, {"title": "6.2 Future work", "content": "The most straightforward way to extend this paper is by adding more agentic benchmarks and more models. Secondly, our \"high-elicitation\" estimates are done with very different scaffolds and techniques. We think there is a lot of room for improvement in choosing datapoints and designing forecasting techniques here. Thirdly, extending any forecasting technique with a factor for inference-time compute scaling might yield more accurate results."}, {"title": "7 Conclusion", "content": "There are three primary novel contributions from this paper. First, we focus on predicting frontier performance instead of average performance. Second, we use different data types than previous work, e.g. using Elo as an intermediate variable and using release date as an input. One advantage of using multiple techniques is that we can choose methods based on the availability of data, e.g. for frontier models, release date is known, while training compute isn't. Third, we focus on benchmarks specifically designed for LM agents while previous work has often focused on QA benchmarks."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Govind Pimpale and Axel H\u00f8jmark ran all experiments and analyses. They also contributed substantially to the conceptual efforts. For example, AH came up with the idea of using Elo as an intermediate variable. J\u00e9r\u00e9my Scheurer and Marius Hobbhahn co-supervised the project. JS had the original idea for the project and developed the first roadmap. GP and MH wrote the paper, supported by AH and JS."}, {"title": "A Scaled Compute Calculations", "content": "Recall that Hoffman loss is:\n$\\widehat{L}(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$\n(4)\nThe normalized scaled FLOP count Copt is:\n$L_{model} = \\widehat{L}(N_{model}, D_{model})$\n$N_{opt}, D_{opt}$ = hoffman_optimal_params($L_{model}$)\n$C_{opt}=6N_{opt} D_{opt}$\nUsing the method of Lagrange multipliers, it can be shown that:\n$N_{opt} = \\frac{A(\u03b1 + \u03b2)}{(l\u03b2)^\\alpha}$\n$D_{opt} = \\frac{B(\u03b1 + \u03b2)}{(l\u03b1)^\\beta}$\nwhere\nl = $L_{budget} - E$"}, {"title": "B Correlation between Elo and PC-1", "content": ""}, {"title": "C Capability Metric Backtesting Details", "content": ""}, {"title": "D Scaffold Details", "content": ""}, {"title": "D.1 Prompts", "content": ""}, {"title": "D.2 Benchmark Specific Prompts", "content": "We used a benchmark-specific prompt for SWE-Bench verified that aimed to give the model some more context on its task, without overfitting to specific properties of the problem\nFor Cybench, we opted not to use a benchmark-specific prompt as the performance with the default instructions was adequate."}, {"title": "E Agentic Data", "content": ""}, {"title": "F Leaderboard Data", "content": ""}]}