{"title": "COMPLETE SECURITY AND PRIVACY FOR AI INFERENCE IN DECENTRALIZED SYSTEMS", "authors": ["Hongyang Zhang", "Yue Zhao", "Claudio Angione", "Harry Yang", "James Buban", "Ahmad Farhan", "Fielding Johnston", "Patrick Colangelo"], "abstract": "The need for data security and model integrity has been accentuated by the rapid adoption of AI and ML in data-driven domains including healthcare, finance, and security. Large models are crucial for tasks like diagnosing diseases and forecasting finances but tend to be delicate and not very scalable. Decentralized systems solve this issue by distributing the workload and reducing central points of failure. Yet, data and processes spread across different nodes can be at risk of unauthorized access, especially when they involve sensitive information. Nesa solves these challenges with a comprehensive framework using multiple techniques to protect data and model outputs. This includes zero-knowledge proofs for secure model verification. The framework also introduces consensus-based verification checks for consistent outputs across nodes and confirms model integrity. Split Learning divides models into segments processed by different nodes for data privacy by preventing full data access at any single point. For hardware-based security, trusted execution environments are used to protect data and computations within secure zones. Nesa's state-of-the-art proofs and principles demonstrate the framework's effectiveness, making it a promising approach for securely democratizing artificial intelligence.", "sections": [{"title": "INTRODUCTION", "content": "Artificial Intelligence (AI), particularly machine learning (ML), has made significant strides in recent decades. Since the advent of large language models (LLMs) such as ChatGPT [1], Claude [2], Gemini [3], LLaMA [4] and diffusion models [5] such as DALLE-3 [6] and Sora [7], foundation models have garnered considerable attention. While these foundation models exhibit intriguing properties such as in-context learning and chain-of-thought reasoning, concerns about their security and privacy have emerged, especially in distributed or decentralized computing scenarios. For instance, in ML as a service (MLaaS), ensuring the integrity of inference results is paramount. Service providers must demonstrate to customers that the output stems from inputting the customer's prompt into a verified large language model, like GPT-4, and generating the response through model execution, rather than relying on human writers or less advanced models, such as GPT-3.5. This requirement is referred to as model security or model integrity. Meanwhile [8], service providers are reluctant to release their model weights, preferring to keep them confidential. In distributed or decentralized inference applications, a central server divides a foundation model into distinct segments, each managed by a different party. When an inference query arises, each party executes its segment independently. It is key to ascertain the trustworthiness of each party's execution before aggregating the outputs.\nMeanwhile, data privacy issues also arise in decentralized inference [9], where users need to ensure that their input data are not directly visible to the decentralized nodes that carry the AI inference for them. This can be extremely important in \u201ccritical inference\u201d settings where sensitive information, such as medical records, financial data, or security information, is processed. In healthcare, where AI models analyze medical images like MRI or CT scans to diagnose diseases. Patient data is highly sensitive and must be protected. Ensuring data privacy in such scenarios is crucial to protect user data from unauthorized access and potential misuse.\nAt Nesa, we aim to address security and privacy concerns for Al's responsible and effective deployment across various domains. In a nutshell, we design hybrid approaches to security and privacy [10] for the above challenges. The selection of different security and privacy methods depends on the specific use cases and the need for varying levels of security and privacy. Based on the different levels of needs, users can flexibly choose the best option for security and privacy. Here, we discuss two key scenarios at Nesa and our security and privacy approaches to them. Also, we discuss how we adapt a hardware-based trusted execution environment (TEE) as an orthogonal approach for security and privacy.\nScenario 1: Critical inference refers to scenarios where the results of AI inference are extremely significant, necessitating the highest levels of security and privacy, even if it means slower speed and higher cost. In these situations, the accuracy and confidentiality of the inference outcomes are important, and users are willing to endure longer processing times to ensure their data is fully protected. Examples include healthcare diagnostics, where AI analyzes medical images to detect diseases, and financial decision-making, where AI evaluates large transactions or investment strategies. In both cases, the sensitive nature of the data demands robust privacy measures, with stakeholders prioritizing data security over rapid results to prevent unauthorized access and ensure the integrity of the process. Under this scenario, Nesa adapts and innovates two leading technologies, including zero-knowledge machine learning (ZKML) [11] for model integrity verification and homomorphic encryption (HE) for data encryption and user privacy protection. Admittedly, the original design of ZKML and HE is computationally heavy, especially for non-linear layers in neural networks such as ReLU layers [12]. To balance effectiveness and efficiency, we apply the state-of-the-art ZKML techniques introduced by [11, 13] to build zero-knowledge Decentralized Proof System (zkDPS) for proof generation and verification processes in foundation models, and we apply HE selectively to critical layers rather than the entire model. Specifically, we propose Sequential Vector Encryption (SVE) based on sequential homomorphic encryption (SHE). This robust encryption scheme obfuscates the output of each operator in a neural network to prevent attackers from extracting sensitive information from intermediate representations. \u00a72 provides more details on our customized zkDPS and SHE solutions for critical inference with the highest protection.\nScenario 2: General inference refers to everyday AI inference tasks with less critical results, allowing for faster processing speeds and lower costs without compromising basic security and privacy standards. In these scenarios, a certain level of protection is still necessary, but the stringent measures required for critical inference are not needed. Examples include routine tasks such as checking the weather, recommending products, or filtering spam emails. Here, the emphasis is on efficiency and speed while maintaining adequate data security to protect user information. Under this scenario, Nesa employs our consensus-based verification (CBV) for security, ensuring the integrity of the inference process, and split learning (SL) [14] for data encryption, which provides a reasonable level of data protection. Notably, we have innovated this verification method to reduce the high redundancy requirements in model verification, and split learning has been used in decentralized AI encryption for the first time. \u00a73 elaborates on our consensus-based verification and split learning solutions for general inference, balancing speed and security effectively.\nNTEE: Nesa's Trusted Execution Environment (TEE). In addition to our innovations around software- and/or algorithm-based security and privacy approaches for the two scenarios, we also design an orthogonal hardware-based approach for both. In a nutshell, TEEs create secure isolation zones within the network's nodes, protecting user data and private model parameters from unauthorized access [15]. TEEs provide a secure enclave for executing computations, isolated from the rest of the node's operating environment, ensuring that even if other parts of the node are compromised, the computations within the TEE remain protected. This isolation is critical in decentralized settings where AI models are spread among multiple owners. This hardware-based security measure works as an alternative to algorithm-based approaches, providing a fast and efficient solution for Nesa's decentralized AI inference tasks. Additionally, we innovate NTEE (Nesa TEEs) optimizing communication among multiple TEEs by establishing direct, secure channels, and implementing heterogeneous TEE scheduling based on the capabilities of each node, whether CPU or GPU-based. NTEE facilitates secure collaboration across multiple nodes, enabling Nesa to maintain high performance, robust security, and data privacy, making it ideal for both critical and general inference scenarios. See details of our TEE implementation and innovation in \u00a74."}, {"title": "SECURITY AND PRIVACY FOR CRITICAL INFERENCE", "content": "In critical inference scenarios, the significance of AI inference results necessitates the highest levels of security and privacy, as the outcomes directly impact crucial sectors like healthcare and finance. Our strategy for ensuring model verification and integrity is via our Zero-Knowledge Decentralized Proof System (zkDPS), a specialized ZK system for decentralized LLMs that allows one party to prove to another that a statement is true without revealing any information beyond the validity of the statement itself. Notably, it introduces a few new techniques to speed up the ZK process. This approach is detailed in \u00a72.1. For data privacy, we redesign Sequential Homomorphic Encryption (SHE) and propose our Sequential Vector Encryption (SVE), which enables computations on encrypted data without decrypting it, thus ensuring that sensitive data remains protected throughout the inference process. In comparison to classical HE, our approaches can be more efficient in applying implied tensor transformations to selected layers of an AI model. More information on this technique can be found in Section 2.2. Fig. 1 summarizes the security flow of critical inference at Nesa, where the data will be transmitted with SVE for encryption, and the final inference results will be verified by zkDPS for integrity."}, {"title": "ZERO-KNOWLEDGE MACHINE LEARNING FOR MODEL INTEGRITY", "content": "Despite the significant strides made in AI security in recent years, the potency of attacks has surged. Central to AI security is the pivotal task of delineating the threat model and understanding how adversaries target the inference process. Adversaries can exploit various vulnerabilities within the inference systems of foundational models, employing tactics tailored to different scenarios. In decentralized AI inference environments, one threat model emerges, where computing nodes may behave deceitfully, compromising the integrity of aggregated results. It becomes imperative to establish mechanisms wherein each node can verify its adherence to agreed-upon protocols without compromising the confidentiality of its model. This necessitates enabling nodes to provide proofs of honest execution to the central server or the public while safeguarding the confidentiality of their respective models. Thus, ensuring both the integrity of inference processes and the privacy of model architectures becomes paramount in the realm of AI security.\nIn the realm of defending against adversarial attacks, a plethora of meticulously crafted countermeasures exist to safeguard systems. One such strategy, particularly pertinent in decentralized inference settings, involves the implementation of mechanisms where central servers solicit proof of computing execution from each node. This process is orchestrated with precision to ensure that while the proof is furnished, the node's confidential model parameters remain undisclosed. Subsequently, the central servers meticulously scrutinize the proofs submitted, thereby enabling them to discern the reliability of each node. The efficacy of this approach hinges upon the successful verification of the proof, serving as a litmus test for the trustworthiness of the node in question.\nThe challenge intensifies when fast inference of secure foundation models is required. Given the scale of big data and the models involved, foundation models inherently exhibit slow inference speeds. It is widely acknowledged that incorporating security measures further exacerbates this slowdown in Al models. For instance, the fastest-known zero-knowledge proof algorithm currently takes as long as 15 minutes to generate proof for a single token in the output [13]. Despite substantial efforts to expedite the inference process, it is imperative to ensure the security of foundation model inference without compromising efficiency."}, {"title": "PROBLEM SETUPS", "content": "Decentralized Inference. At Nesa, we explore the challenge of decentralized inference for foundation models, which offers numerous benefits. With the advent of 5G technology and improved internet latency, personalized devices such as mobile phones can now participate in crowdsourcing machine learning models. This approach enhances device utilization and eliminates the need for data communication between nodes and a central server, thereby safeguarding data privacy. In the context of Machine Learning as a Service (MLaaS), models remain the private property of individual nodes, allowing them to offer model inference services via APIs without sharing model weights and checkpoints. As foundation models grow in size-such as Meta's LLaMA-3.1 with 405 billion parameters [4], and future models expected to reach trillion-level sizes it becomes impractical to load entire models on a single node or server. Additionally, technologies like blockchain are now equipped to support the decentralized inference of foundation models on-chain, further facilitating this approach.\nModel Decomposition. A core assumption in decentralized AI inference is that the model used in the inference system can be divided into several parts, each managed by a separate party, or computing node. Each computing node performs its assigned computations and sends the results to a central server. For instance, with a foundation model like LLaMA-3, the model can be decomposed layer-wise, enabling sequential inference by different nodes. Alternatively, decomposing the model width-wise allows for parallel inference across multiple nodes. The method of decomposition depends entirely on the application scenarios, and we consider both approaches in Nesa's products."}, {"title": "THREAT MODELS", "content": "In this paper, we focus on decentralized inference of foundation models as described above. In this framework, each computing node (i.e. prover) owns a foundation model or a part of the foundation model with a publicly known architecture, while the model weights are proprietary. We make a semi-dishonest assumption on the central server (i.e. verifier): the central server is honest in aggregating results from each computing node and accurately reports the verification result, but the central server tries to glean additional information about parts of the foundation model at computing nodes. However, some computing nodes might be dishonest, potentially deviating from the agreed protocol by substituting their model with an alternative or outputting random data. We assume that the majority of nodes are honest, but acknowledge that dishonest nodes can collude. These adversarial nodes can arbitrarily alter their results, provided their behavior remains undetected by the central server."}, {"title": "OVERVIEW OF ZERO-KNOWLEDGE PROOFS", "content": "We use the technique of zero-knowledge proofs to guarantee that each party is honest about his or her execution of inference of foundation models. Zero-knowledge proof serves as a fundamental technique and underpins the architecture of blockchain. In this cryptographic concept, two entities are involved: the prover and the verifier. The prover's objective is to demonstrate the successful execution of a protocol without disclosing confidential information, termed as the 'witness'. This witness encompasses sensitive data like model weights or private information that the prover wishes to keep undisclosed to the verifier. Often, the protocol is depicted as a circuit, where certain components remain hidden within the witness.\nCommitment, Proof, and Verification. The process of zero-knowledge proof involves three essential steps. Firstly, the prover commits to the witness data, such as model parameters, ensuring its integrity by encrypting it before transmitting it to the verifier. Once sent, the content remains unchanged, with the verifier gaining access only to the encrypted version. In practice, the (generalized Pedersen) commitment of any d-dimensional tensor  S = (S_1, S_1, ..., S_d) \u2208 {0, 1, ..., |G| - 1}^d  (e.g., the model weights) is implemented as [16]:\nCommit(S,rs) = h^rs g^S = h^rs \u03a0g_i^S_i \nwhere G is an elliptic curve group w.r.t. the finite field F consisting of points (x, y) \u2208 F \u00d7 F such that y\u00b2 = x\u00b2 + ax + b for designated field elements a and b,  r ~ {0, 1, ..., |G| \u2013 1} is an integer (i.e., the element of the scalar field F of G with |F| = |G|) uniformly sampled from {0, 1, ..., |G| \u2013 1},  g = (g_1, g_2, ..., g_d) \u2208 G^d and h ~ G are uniformly and independently sampled from G. In the second step, the prover executes the protocol and simultaneously generates a proof of execution in a finite field F, which is then transmitted to the verifier, either interactively or non-interactively. Depending on the protocol, operations can involve arithmetic or non-arithmetic processes. Lastly, the verifier meticulously examines the proof to ensure the honest execution of the protocol by the prover, thereby validating the transaction or information under scrutiny."}, {"title": "PROPERTIES OF ZERO-KNOWLEDGE PROOFS", "content": "Zero-knowledge proofs have many advantageous properties that form the foundation of blockchain and decentralized machine learning. These include:\nCompleteness: If the prover accurately executes the circuit, the proof will be validated (with probability 1).\nSpecial Soundness: If the prover is dishonest in executing the circuit, the proof will fail (with high probability). A weaker property, special soundness, requires executing the protocol twice and being able to identify the witness.\nZero-Knowledge: The verifier gains no knowledge about the prover's witness.\nThe above properties ensure that the proof will pass verification only if the prover is honest, while also allowing the prover to keep its secret or witness hidden from the verifier.\nInteractive vs. Non-Interactive Proof Systems. Depending on whether the proof-verification process involves a single round or multiple rounds, zero-knowledge proofs can be classified as either interactive or non-interactive, respectively. Zero-knowledge proofs are naturally described as an interactive process, where the verifier sends a challenge (typically a random variable) to the prover, who then responds to the verifier. If the proof is valid, the verifier sends a new challenge in the next round, and the process repeats (see Figure 2). Given an input x, an interactive proof procedure works as follows:\n1. P sends the first message a \u2190 P(x).\n2. V sends a challenge B.\n3. P sends the second message \u03b3 \u2190 P(x, \u03b1, \u03b2).\n4. V decides to accept or reject according to an algorithm V(x, \u03b1, \u03b2, \u03b3).\nIn the non-interactive case, the process can be simulated by having the prover generate his/her own challenge B. This is achieved by replacing the random variable \u03b2 in the challenge with a random oracle model or a hash function H (such as SHA-256) that operates on all the messages that the prover has sent so far. This is also known as the Fiat-Shamir heuristic [17]. In particular, given an input x, a one-shot, non-interactive proof procedure works as follows:\n1. P computes the first message a \u2190 P(x).\n2. P computes a challenge \u03b2 \u2190 H(x,\u03b1).\n3. P computes the second message \u03b3 \u2190 P(x, \u03b1, \u03b2).\n4. P sends a and y to V.\n5. V computes \u03b2 \u2190 H(x, a) and decides to accept or reject according to an algorithm V(x, \u03b1, \u03b2, \u03b3)."}, {"title": "COMMITMENT", "content": "The Pedersen commitment (1) satisfies the binding property: once sent to the verifier, the opening information (r, S) cannot be changed anymore by the prover. Upon initial inspection, the prover needs to send the witness S to the verifier to prove that he/she can \"open\" the commitment. Thus the zero-knowledge property of the commitment (1) may appear impossible. However, this skepticism is unfounded, largely owing to the homomorphic property of the Pedersen commitment (1): for two commitments Commit(S1, r1) and Commit(S2, r2) corresponding to tensors S1 and S2, respectively, we have Commit(S1, r1)Commit(S2, r2) = Commit(S1 + S2, r1 + r2), responding to the commitment of tensor S1 + S2. This property enables the prover to prove to the verifier that he/she can \"open\" the commitment without revealing the witness. This is achieved by letting the prover instead open the commitment of a linear transformation of the witness:  S' \u2190 S \u00b7 e + D, where D is a d-dimensional hiding vector picked by the prover and e is a challenge (scalar) randomly sampled by the verifier. Hereby, the vector D is to hide the witness S as S' = S \u00b7 e + D looks random to the verifier. The existence of challenge e guarantees special soundness as two runs of the procedure with challenges e1 and e2 with e1 \u2260 e2 satisfy:\nS'_1 = S \u00b7 e_1 + D, \nS'_2 = S \u00b7 e_2 + D. \nIn Equation (2), we have 2d unknowns S and D and 2d equations. Thus, two accepting transcripts (S, e1, D) and (S, e2, D) will identify S and D by Gaussian elimination, thus achieving special soundness."}, {"title": "PROOFS FOR ARITHMETIC OPERATIONS", "content": "Multilinear Extension. Zero-knowledge proof focuses on the finite field F, rather than the real field IR as in the floating-point calculations. Arithmetic operations consist of addition and multiplication. For these arithmetic operations, it is easy to use the Sum-Check protocol, in particular, the GKR protocol [19, 20], to implement zero-knowledge proofs. The basic idea in the Sum-Check protocol is to express a d-dimensional tensor  S \u2208 F^d  involved in the calculation as a multi-variable polynomial S:  F^{log_2 d} \u2192 F via a transformation called multilinear extension [18]:\nS(u) = \u2211 S(b)\u03b2(u, b),\nwhere b \u2208 {0, 1}^{log_2 d} refers to the binary index of tensor S, and \u03b2(\u00b7, \u00b7) :  [F^{log_2 d} \u00d7 F^{log_2 d} \u2192 F is the (unique) Lagrangian interpolation polynomial:\n\u03b2(u, b) = \u03a0 (u_ibi + (1 \u2013 u_i)(1 \u2013 b_i)),\nsuch that for any  b_1, b_2 \u2208 {0, 1}^{log_2 d}, the interpolation property holds true:\n\u03b2(b_1, b_2) = { 1, if b_1 = b_2;\n                  0, otherwise.\nThat is, the polynomial S is the Lagrange interpolation of the tensor S on the binary indices: S is the unique multilinear polynomial over F such that S(u) = S(u) for all u \u2208 {0,1}^d. With the multilinear extension, we can write the verification of arithmetic operations between vectors equivalently as the verification of sum of multi-variable, low-degree polynomial g:\nH = \u2211 g(x_1, x_2, ..., x_\u03c5),"}, {"title": "PROOFS FOR NON-ARITHMETIC OPERATIONS", "content": "There are two major techniques for zero-knowledge proof of non-arithmetic operations: 1) bit decomposition and 2) lookup table.\nReducing ReLU Activation to Bit Decomposition. Bit decomposition is one of the most frequently used techniques for zero-knowledge proofs of non-arithmetic operations. Let Z denote the pre-activated tensor of ReLU, and let A denote the after-activated tensor. We now introduce the techniques appearing in [11]. In the ReLU activation, we have\nA = sign(Z) abs(Z),"}, {"title": "ZERO-KNOWLEDGE PROOFS FOR FOUNDATION MODELS", "content": "By integrating all components, one can construct a zero-knowledge proof system for foundation models like Meta's LLaMA series. Foundation models comprise transformer layers followed by MLP layers. For arithmetic operations, such as the linear layers in transformers and MLPs, the"}, {"title": "ZKDPS: ZERO-KNOWLEDGE DECENTRALIZED PROOF SYSTEM AT NESA", "content": "At Nesa, we build a zero-knowledge system, zkDPS, for decentralized large language models. zkDPS is built upon zkLLM [13] and zkDL [11]. zkLLM [13] and zkDL [11] are state-of-the-art CUDA implementation of zero-knowledge proofs of machine learning. However, speed remains the most significant pain point for zero-knowledge proofs of foundation models. For example, it takes zkLLM roughly 15 minutes to generate a proof for the inference procedure of LLaMA-2 13B with an input token length of 2,048 [13]. In comparison, vanilla inference of zkLLM takes less than 0.1 seconds per token. Despite significant efforts have been made to accelerate generic zero-knowledge proof frameworks such as zk-STARK and zk-SNARK [23], they are still not fast enough, even much slower than zkLLM for billion-scale models. Therefore, the acceleration of zero-knowledge proofs is urgent for real-world applications.\nzkDPS proposes the following ideas to speed up proof generation beyond zkLLM [13] and zkDL [11]. One idea could be to leverage the \u201ccompressible property\" of LLMs by algorithmic innovation. It is well-known that LLMs are in essence a compression of information and data from the real world. Thus one could consider using a small draft model to \"guess\" the output of the original LLM, similar to speculative sampling. As the draft model is small (typically 20x-50x smaller than the original LLM [24]), one could expect to experience a shorter time for proof generation of the execution of the draft model. Experimentally, the top-1 guessing accuracy of the draft model is as high as 82% in EAGLE [24, 25], a state-of-the-art speculative sampling method. Therefore, this acceleration technique will not degrade the inference performance of LLMs too much.\nAnother idea is to leverage the hardware acceleration of modern GPUs. zkLLMs has already used CUDA implementation to parallelize the computations in the proof generation. For example, the CUDA kernels such as Pedersen commitment, elliptic curve, and lookup tables have been re-implemented or adapted in zkLLMs [13, 11], making it the fastest implementation of zero-knowledge large language models at the submission of this paper. In Nesa, we are optimizing those kernels by engineering methods such as Tensor Parallelism and KV cache."}, {"title": "SEQUENTIAL VECTOR ENCRYPTION", "content": "At Nesa, we have designed an innovative encryption method called Sequential Vector Encryption (SVE) which is a robust scheme inspired by our diagrammatic interpretation of homomorphic encryption applied on select linear layers rather than the full model. In particular, we focus on the modern LLM architecture but this framework can be applied to other types of neural networks such as CNNs. We consider the LLM to be a sequence of vector space operators. As mentioned previously, this sequence contains both linear and non-linear operators. In principle, the (intermediate) output vector of every operator contains information about both the input and the output of the model. We sometimes refer to this operator output as a (latent) representation vector, as it is a representation of the input data after (partial) processing through some intermediate computational operations in the neural network. We wish to apply SVE when there is some extra concern that an attacker can access these vectors and, subsequently, can extract sensitive information attributed to either the initial model input or model output. Our method focuses on obfuscating the output of every operator in the sequence of the LLMs so that any observation of the neural representation of the data in any intermediate latent space is not usable for this purpose. This extra level of security is particularly relevant for the use of models that heavily rely on open-source foundation models for which the intermediate latent spaces are available for extensive study by any actor.\nThe general technique for SVE is based on the simple idea of randomly transforming the outputs of every operator so that the intermediate vector representations can no longer be interpretable using a given method developed for the original model's vector representations. In order to be usable for the original model, these representations need to be transformed into the original representation space before feeding to the next operator. Equivalently, we can transform the operator itself to the operator on the transformed vector space in the correct way, instead of transforming the vector itself, which is how the technique is implemented in practice as this allows efficient secret sharing - to be discussed below. In either case, the computational complexity of the transformation is the same. As the intermediate representations have dimensionalities on the order of roughly 103 components (for reference a small LM such as BERT typically uses 768 dimensions and Llama 2 uses 4096 dimensions), any attempt to recover the transformed vectors means determining the parameters of the transformation (representable as a matrix), which are on the order of 106 floating point parameters for a typical transformation. This would amount to solving a very large undetermined system.\nThere are two main considerations to put this into practice: (1) an appropriate transformation of each operator, and (2) a secret sharing method so that transformation matrices are never revealed during operation. The former (1) ensures that the transformations preserve the computational outputs of the original model and the latter (2) is required so that no agent can recover the encrypted representation vectors. The following will explain how we address both considerations.\nAppropriately transforming each operator depends on the type of operator under consideration, for a linear operation, it can be transformed as a product of (invertible) transformation matrices and the original matrix representation of that operator (recoverable from the original LLM). For non-linear operators, we can take advantage of certain symmetries of the operator. For instance, the ReLU R operator is pointwise, and thus R commutes with any vector space permutation P according to the following diagram:\nHere we are transforming between real vector spaces of the same dimension. Since P is always invertible, this diagram tells us that we can represent the ReLU operator as  R' = P^{-1}RP. This transformation alone is not necessarily enough to obscure the transformation as a bad actor with knowledge of the layer could in principle run a known vector through the open source model and compare the two representation vectors component-wise to figure out the transformation. Therefore we can scale with a random vector with all positive entries, as the R also fits into the following commutative diagram:\nwhere  M_a is multiplication by  a \u2208 R_+^n, a diagonal matrix with positive diagonal terms. Concatenating the two transformations we obtain the representation  R\" = M_a^{-1}R'Ma = M_a^{-1}P^{-1}RPMa, noting that any diagonal matrix with non-zero diagonal entries is invertible. As an aside, it is interesting to note that the number of per- mutations on an n-dimensional vector are simply n!, so even for BERT, the number of possible permutations is 768! whereas it is thought that there are roughly 60! atoms in the universe. Other nonlinear operators can be transformed using similar, but more careful, considerations. We finally package our transformation T = PMA as a single matrix T, with corresponding inverse  T' = M_a^{-1}P^{-1}. However, revealing this matrix directly in the procedural code distributed to agents will compromise security. Thus we use a form of secret sharing to obfuscate the transformation matrices."}, {"title": "SECURITY AND PRIVACY FOR GENERAL INFERENCE", "content": "In general inference scenarios, the results of AI inference tasks are less critical, allowing for faster processing speeds and lower costs without compromising basic security and privacy standards. These tasks include everyday activities such as checking the weather, recommending products, or filtering spam emails. Our strategy for ensuring model verification and integrity in these scenarios is Consensus-based Verification Check (CBV), which leverages the collective agreement of multiple nodes to ensure the correctness and integrity of model execution without revealing sensitive data. This approach is detailed in Section 3.1. For data privacy, we employ split learning (SL) [14, 26], a method where the model is divided into segments, and each segment is trained on different nodes to maintain data privacy by ensuring no single node has access to the complete dataset. More information on this technique can be found in Section 3.2. Fig. 4 provides an overview of the security flow of general inference at Nesa which first applies SL to encrypt the first (few) layers to \"encrypt\" the raw data, where the AI models are sharded and deployed to nodes with redundancy. For the nodes who have the same shard, their outputs will be verified by consensus for integrity."}, {"title": "CONSENSUS-BASED VERIFICATION CHECK FOR MODEL INTEGRITY", "content": "Given the computational and scalability challenges associated with ZKML for verifying the integrity of LLMs in decentralized systems, Nesa proposes a consensus-based distribution verification (CDV) strategy for general inference scenarios. This strategy leverages the collective agreement of multiple nodes to ensure the correctness and integrity of model execution without revealing sensitive data.\nConsensus-based Verification. Consider a decentralized network with N nodes, where each node i executes the same inference model M with parameters 0, on a given input x [27]. The output of the model on node i is denoted by  y_i = M(x;0_i). The goal is to ensure that all nodes accurately execute the model M, yielding consistent outputs. The process can be formalized in the following steps:\n1. Redundant execution. A subset of the network nodes,  {1, 2, . . ., k} \u2286 N, independently computes the output yi for the same input x.\ny_i = M(x; 0), \u2200i\u2208 {1,2,...,k}.\n2. Output collection. The outputs {y1, y2,..., yk} are collected for consensus evaluation. This collection phase requires secure and efficient communication protocols to protect the integrity of the transmitted data.\n3. Consensus determination. Utilizing a consensus algorithm C, the system evaluates the collected outputs to determine the agreed-upon result Ycon. The consensus result is considered valid if it satisfies a predefined criterion, such as majority agreement or a more sophisticated decision rule based on the specific properties of the outputs.\nY_{con} = C({y_1, y_2,..., y_k}).\n4. Verification and finalization. If the consensus results Ycon align with the outputs from a sufficiently large subset of nodes, the model's execution is verified. Otherwise, discrepancies indicate potential integrity issues, triggering further investigation or corrective measures.\nThis consensus-based approach not only facilitates the verification of model integrity across decentralized nodes but also introduces a robust mechanism to detect and mitigate the impact of faulty or malicious nodes.\nTaking Model Sharding into Account. In Nesa's decentralized system, where ML models' computational graphs may be sharded across multiple nodes for scalability, each node i possesses a unique shard Mi of the complete model M. This partitioning requires a specialized approach to Consensus-based Verification to accommodate the fragmented nature of model execution.\nConsider the complete model M being divided into k shards, such that  M = \u2295_{i=1}^k M_i, where \u2295 denotes the operation of combining the model shards to represent the full model functionality. Given an input x, the execution of these shards across k nodes produces a set of partial outputs {y1, y2,..., yk}, where  y_i = M_i(x; \u03b8_i). \nVerification in the Context of Sharding."}, {"title": "Shard Redundant Execution.", "content": "For each shard Mi of the complete model M, redundant execution is performed by a designated subset of nodes. Each of these nodes, within the subset responsible for shard Mi, computes the output Yi,j for the given input x, where j represents the node within the subset.\ny_{i,j} = M_i(x; 0_{i,j}), \u2200j\u2208 Subset of nodes for M_i.\nThis step introduces computational redundancy, where multiple independent computations of the same shard aim to fortify the verification process by cross-verifying results among nodes computing the same shard."}, {"title": "Redundant Output Collection and Verification.", "content": "The outputs {Yi,1, Yi,2,..., Yi,m} for each shard i are collected from the nodes in its subset. A consensus mechanism Ci specific to shard i then evaluates these collected outputs to determine a shard-specific agreed-upon result Ycon,i.\nY_{con,i} = C_i({Y_{i,1}, Y_{i,2}, \u00b7 \u00b7 \u00b7, Y_{i,m}}).\nHere, m denotes the number of nodes executing the shard Mi. The redundancy in computation across these nodes allows for a robust verification mechanism, enhancing the detection of discrepancies or faults."}, {"title": "Shard Verification Completion.", "content": "Upon achieving consensus for a shard i, signified by the result Ycon,i, the process ensures the integrity of the shard's computation before proceeding. This step-by-step verification across shards, with redundancy in each shard's computation, significantly reduces the risk of erroneous or malicious model execution."}, {"title": "Model Reconstruction.", "content": "After each shard has been independently verified, the shard-specific consensus results {Ycon,1, Ycon,2,..., Ycon,k} are combined to reconstruct the final model output Yfinal. This comprehensive output can ensure the integrity of the complete model execution.\nY_{final} = \u2295_{i=1}^k Y_{con, i}.\nConsensus-based Distribution Verification (CDV). Building upon traditional consensus mechanisms, the CDV strategy introduces an advanced layer of verification by assessing the statistical distribution of model outputs across a decentralized network. This approach is ideally suited for scenarios where the model is not monolithic but is instead distributed as shards across multiple nodes.\nCDV is based on the understanding that while individual outputs from model shards might exhibit slight variability due to the stochastic nature of ML models and the complexity of input data, the collective output distribution should maintain"}]}