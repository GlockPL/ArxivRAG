{"title": "FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through Identity Obfuscation", "authors": ["Li Wang", "Zheng Li", "Xuhong Zhang", "Shouling Ji", "Shanqing Guo"], "abstract": "DeepFakes pose a significant threat to our society. One representative DeepFake application is face-swapping, which replaces the identity in a facial image with that of a victim. Although existing methods partially mitigate these risks by degrading the quality of swapped images, they often fail to disrupt the identity transformation effectively. To fill this gap, we propose FaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake face-swapping threats. Specifically, FSG introduces imperceptible perturbations to a user's facial image, disrupting the features extracted by identity encoders. When shared online, these perturbed images mislead face-swapping techniques, causing them to generate facial images with identities significantly different from the original user. Extensive experiments demonstrate the effectiveness of FSG against multiple face-swapping techniques, reducing the face match rate from 90% (without defense) to below 10%. Both qualitative and quantitative studies further confirm its ability to confuse human perception, highlighting its practical utility. Additionally, we investigate key factors that may influence FSG and evaluate its robustness against various adaptive adversaries.", "sections": [{"title": "1 Introduction", "content": "Advances in generative AI [10] have led to highly realistic generated images, marking the rise of the DeepFake era [32]. A notable example is face swapping [3, 34], which replaces the identity in a target image with a victim's while maintaining pose, expression, and background (See Figure 1). It enables identity theft [13, 30] by bypassing facial recognition systems and spreading misinformation, damaging reputations [19], thereby escalating societal and security threats. Besides, Li et al. [15] show that it can also evade liveness detection, circumventing facial verification systems used in security-critical applications. Quantitatively, on Face++'s recognition API, the Face Match Rate (FMR)-measuring the likelihood that two facial images belong to the same person-reaches 99.3% between 1,000 generated images and user images, highlighting significant security risks. To address these threats, obfuscating identities in generated images is crucial. This ensures that even if a user's image is misused, the resulting DeepFake cannot be recognized as the user's true identity, effectively protecting facial privacy.\nMany existing methods defend against GAN-based facial manipulation by adding adversarial perturbations to victim images, aiming to degrade the quality of generated images [26, 39]. However, these methods focus on maximizing pixel-level distortion between adversarial and original generated images. In black-box scenarios, where DeepFake models are unknown, they often only introduce blurriness or artifacts [6], as shown in Figure 1. As a result, they fail to disrupt identity preservation during generation. While image quality decreases, the subject's identity remains intact, potentially allowing the images to bypass identity verification and human perception. Additionally, these methods rely on specific generative models for perturbation, limiting their transferability and efficiency across different face-swapping models.\nTo address this gap, we propose FSG, a novel defense approach that mitigates DeepFake face-swapping threats through identity obfuscation. A high-level overview of FSG is shown in Figure 1. Our approach is inspired by the pipeline of advanced feature-level face-swapping techniques, which use an identity encoder to extract and preserve features from source images during injection and decoding, ensuring identity preservation in generated images. To disrupt this process, FSG introduces imperceptible perturbations to victim images, altering the features extracted by the identity encoder. This causes confusion in both identity and visual perception of the generated images. Importantly, FSG computes these protected images in a model-agnostic manner, ensuring broad applicability. Specifically, we construct a surrogate model"}, {"title": "2 Related Work", "content": "Face swapping employs advanced AI to manipulate visual content, altering identity features and generating face-swapped images with specific expressions and movements [20]. This field has seen extensive research, with numerous methods proposed [31]. Early image-level methods transfer target face attributes to the source face using segmentation masks for blending [22]. For example, FS-GAN [21] uses a reenactment network to transfer expressions and poses, followed by a blending network combining Poisson optimization [23] and perceptual loss. However, these methods are sensitive to source images and often fail to preserve target image attributes, leading to artifacts. State-of-the-art feature-level methods extract identity features from the source face, and attribute features from the target face [1,3], then decode these features into generated images. FaceShifter [16] produces high-fidelity results by adaptively integrating target attributes (e.g., expression, lighting) and handling occlusions in a self-supervised manner. SimSwap [3] further improves this by introducing a weak feature matching loss, preserving attributes, and avoiding mismatched poses and expressions. Overall, face swapping continues to evolve, focusing on reducing artifacts and improving identity and attribute preservation.\nHowever, the malicious exploitation of face-swapping technology could pose significant threats to security and privacy [29], particularly by presenting unprecedented challenges to identity verification services that rely on underlying facial recognition technology. Tariq et al. [30] examine the vulnerability of the celebrity recognition APIs to DeepFake attacks, revealing significant weaknesses in identity verification systems. Li et al. [15] investigate the security of facial liveness verification systems, including facial recognition technology, highlighting vulnerabilities in widely deployed APIs within the evolving attack-defense landscape."}, {"title": "2.2 Defense Method", "content": "To combat face-swapping threats, researchers have focused on detection [7, 35], primarily using binary classification models [18]. However, these passive methods detect Deep-Fakes only after creation, allowing security breaches, identity theft, or reputational damage to have already occurred first.\nRecent efforts in proactive defense against facial manipulation involve adding imperceptible adversarial perturbations to user images to disrupt the synthesis of generated images [9, 26]. For instance, Yeh et al. [39] propose distortion and nullifying attacks, which maximize the distance between adversarial and original outputs or minimize the distance between adversarial images and inputs, respectively. These methods effectively protect against image-translation-based DeepFake attacks in white-box scenarios. In black-box settings, Ruiz et al. [27] propose a query-based adversarial attack, though its practicality is limited due to high query demands. Existing defenses mainly target attribute editing or facial reenactment (e.g., StarGAN [4], GANimation [25]), neglecting face-swapping models that manipulate hard biometric features, which pose greater threats. Dong et al. [6] use adversarial example transferability to resist face swapping in restricted black-box scenarios. However, like prior methods, they disrupt images at the pixel level, reducing visual quality without significantly altering identity features, thus failing to fully mitigate identity theft risks. Additionally,"}, {"title": "3 Methodology of FaceSwapGuard", "content": "FaceSwapGuard (FSG) protects against DeepFake face-swapping by adding imperceptible perturbations to user photos, creating protected images. When adversaries use these images for face swapping, the generated images' identity features significantly deviate from the source, preventing identity theft. Additionally, the visual differences between generated and source images reduce privacy risks from fake information dissemination. In summary, FSG's protected images effectively mitigate face-swapping threats."}, {"title": "3.1 Motivation and Intuition", "content": "Our motivation is twofold: First, DeepFake threats, especially face swapping, pose serious risks to identity authentication systems [15]. Second, existing pixel-level defenses fail to mitigate these identity thefts caused by face-swapping, as they only degrade image quality without altering semantic features, especially in black-box scenarios. Further analyses are provided in Appendix A.1.\nTo counter DeepFake face swapping, we propose FaceSwapGuard (FSG), a proactive defense based on identity obfuscation. Our approach disrupts feature extraction by maximizing deviations in identity encoder outputs, preventing the swapped face from matching the source. Figure 2 provides an overview of FSG."}, {"title": "3.2 Protected Image Computation", "content": "Individual users can apply FSG to generate protected images that closely resemble the source image while disrupting identity extraction during face swapping. The design goals are:\n\u2022 The protected image should be visually identical to the source image.\n\u2022 Face-swapped images generated from the protected image should have identity features and visual appearances significantly different from the source.\nGenerally, FSG aims to maximize identity deviation in feature space while preserving human perceptual differences. A key challenge is ensuring protection remains effective across various unknown face-swapping models. To address this, FSG is designed for transferability and robustness. The following sections detail the methodology for generating protected images."}, {"title": "3.2.1 Maximizing Identity features", "content": "Given the source image Xs that a user intends to share online, we iteratively search for a protected image Xp in the image space by modifying Xs. The objective is to maximize the identity features deviation between Xs and Xp as follows:\n$\\\\max_{X_p}D(E(X_p \\odot M), E(X_s \\odot M))$\n$s.t. ||X_p - X_s|| < \\epsilon$\nwhere E() represents the identity features extracted from the identity encoder, $||X_p - X_s||$ measures the distance between Xp and Xs, $\\epsilon$ is the distance budget in image space, M denotes the facial region mask, and $D(\\cdot) = (1 - cos(\\cdot, \\cdot)) +mse(\\cdot, \\cdot)$ computes the distance of two feature vectors. Here, cos(,) is the cosine similarity, and mse(,) is the MSE loss.\nBesides, we introduce intermediate feature map loss [36] to enhance identity feature distortion during face swapping, increasing the visual difference between generated and source images. Inspired by SimSwap [3], we use feature maps from the last few layers, which capture high-level identity semantics, to compute the loss. The formula is as follows:\n$\\\\max_{X_p} \\sum_{i=k}^{K}D (E_i(X_p \\odot M), E_i(X_s \\odot M))$\n$s.t. ||X_p - X_s|| < \\epsilon$\nwhere K is the total number of layers in the identity encoder, and k is the first layer where we calculate feature map loss."}, {"title": "3.2.2 Transferability and Robustness", "content": "If the defender, i.e., the FSG user, knows the adversary's identity encoder, Equation 2 can be used directly to distort identity features. However, in a general black-box scenario where the defender lacks access to the adversary's encoder or the face-swapping model lacks explicit identity constraints, we construct a surrogate identity encoder. This improves the transferability and robustness of protected images across different face-swapping models.\nThe identity encoder in a face-swapping model extracts identity features from Xs and preserves them during generation to ensure consistency. Thus, we use open-source face recognition models, denoted as F, as the surrogate encoder.\nTo enhance the transferability and robustness of protected images, we employ a random image transformation function during the optimization process [37]. These transformations simulate common operations encountered in face swapping, such as resizing and cropping. The whole process remains differentiable, and we reformulate the optimization function as follows:\n$\\\\max_{X_p} \\sum_{i=k}^{K} D (F_i(Trans(X_p) \\odot M), F_i(X_s \\odot M))$\n$s.t. ||X_p - X_s|| < \\epsilon$\nwhere F (\u00b7) is the surrogate model and Trans (\u00b7) is the random transformation function, detailed in Appendix A.2."}, {"title": "3.3 Face-swapped Image Generation", "content": ""}, {"title": "3.3.1 Threat model", "content": "The adversary aims to perform unauthorized face swapping on users' photos for malicious purposes, such as bypassing face verification systems to access secure applications or spreading false information that invades privacy. Adversaries can range from commercial entities to malicious individuals, and we assume they have access to advanced face-swapping models G(.) to manipulate any user's photos."}, {"title": "3.3.2 Face swapping under protection", "content": "In practical applications, users can apply Algorithm 1 to generate a protected image by iteratively adding adversarial perturbations to their source image before sharing it on social media. This ensures that any face-swapped image will have significant differences in identity features, making it impossible for both machines and humans to recognize the user's true identity, thereby thwarting adversarial manipulation. The formula is as follows:\n$F (G(x_p)) \u2260 F (G(x_s))"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Datasets. Our goal is to protect users' source images from face-swapping threats. Face swapping typically requires a source image for identity information and a target image for attributes and background. In our experiments, we randomly select 100 source images with different identities from the CelebA-HQ [11] dataset as potential victims. We also randomly choose 10 target images from the remaining identities, allowing each face-swapping model to generate 1,000 face-swapped images.\nModels. (1) Face-swapping Models: We select two feature-level face-swapping methods, FaceShifter and SimSwap, both of which use identity encoders to extract features from source images. These models have shown strong performance in bypassing facial liveness verification [15]. Additionally, we assess the effectiveness of FSG on other DeepFake models, including diffusion-based models, as detailed in Appendix A.5. (2) Surrogate Models for Identity Encoder: We use the open-source face recognition models FaceNet [28] and ArcFace [5] as surrogate models, representing different training loss functions [38]. FaceNet is trained with an Euclidean-distance-based loss on the 112\u00d7112 aligned"}, {"title": "4.3 Effectiveness Under Black-box Scenario", "content": "We use FSG to generate protected images with different surrogate models and evaluate their effectiveness against various unseen face-swapping techniques."}, {"title": "4.3.1 Evaluation on Identity Obfuscation", "content": "We generate protected images using the surrogate models ArcFace and FaceNet, with a distance budget of $\\epsilon$ = 9 for pixel values in [0, 255]. These protected images are then used by FaceShifter and SimSwap to produce face-swapped images. Results demonstrate that FSG effectively mitigates face-swapping threats in black-box scenarios, significantly reducing FMRs. Key findings include: 1) All FMRs are below 25%, with real-world APIs dropping below 17%. 2) FSG performs best on Baidu's API, reducing FMRs from over 90% to nearly 0. 3) Protected images based on ArcFace outperform those from FaceNet, with FMRs below 8% across models and APIs, likely because ArcFace shares a backbone with the identity encoders used in the experiments.\nFurthermore, we evaluate the performance of protected images under different distance budgets $\\epsilon$. Results indicate that the perturbation"}, {"title": "4.3.2 Evaluation on Visual Confusion", "content": "In we present the perceptual results at $\\epsilon$ = 9 to demonstrate the effectiveness of FSG in altering visual perception. Compared to the baseline results, the LPIPS losses are higher, indicating larger feature differences. We also include visualization examples highlighting the differences between face-swapped images and the source images demonstrate the effectiveness of the protected images"}, {"title": "4.4 Comparison with Pixel-level Disruption Method", "content": "To further assess the effectiveness of FSG, we compare it with pixel-level disruption methods. Since Ruiz et al.'s method [26] is designed for white-box scenarios and the lack of open-source code for black-box methods based on face-swapping models [6], we evaluate existing defenses by maximizing the distance between adversarial and original images, as outlined in [26, 39]. We generate protected images using various face-swapping models and evaluate the transferability of adversarial examples to test the pixel-level disruption method across different models. All other experimental settings, such as image transformation functions and input data, remain consistent with FSG.\nVarious face verification Apis demonstrate that FMRs from, FSG has good values"}, {"title": "4.5 Ablation Study", "content": "We perform ablation experiments to evaluate the contribution of each component in FSG, including the image transformation function and intermediate feature map loss. Three sets of experiments are designed: 1) Removing random image transformations \u201cno_Trans\" 2) Removing the intermediate feature map loss \u201cno_Int.layer\" 3) Removing both random image transformations and intermediate feature map loss \u201cno_Trans. & Int.layer\".\nWe evaluate the effectiveness of protected images generated by the FaceNet surrogate model under various settings by FSG\""}, {"title": "4.6 Adaptive Adversary", "content": "FSG generates protected images by adding imperceptible adversarial noise to the source images. To test its robustness, we evaluate its performance against common image denoising techniques, including Gaussian blur and image compression [14], which adversaries may use to purify the protected images."}, {"title": "5 Conclusion", "content": "We propose FSG, an efficient defense method against identity theft threats posed by DeepFake face-swapping. Specifically, FSG protects an individual's identity by introducing imperceptible perturbations to user photos. In this way, FSG can effectively obfuscate both the identity and visual perception of face-swapped images, reducing the risks of identity theft and misinformation. Extensive experiments show FSG's effectiveness in black-box scenarios, particularly in protecting against face-swapping attacks targeting commercial face-recognition APIs. We also verify the robustness of protected images against potential adaptive attacks. Additionally, its success with both FSGAN and diffusion-based models demonstrates its potential for broader application across various DeepFake models. Overall, FSG offers a novel approach to safeguarding individual identity in the face of evolving DeepFake threats."}, {"title": "Impact Statements", "content": "In this study, we perform a comprehensive security evaluation of face verification models and APIs against DeepFake face-swapping threats. In line with prior research on the security of AI-powered systems, we have carefully considered the legal and ethical implications of our work.\nFirst, we utilize open-source datasets for DeepFake synthesis and security assessment, a widely accepted and legitimate approach in face-related security research. Second, our evaluation of commercial face verification APIs adheres strictly to the official guidelines, with all API usage properly licensed and paid for. Additionally, we ensure that the number of queries per second (QPS) remains within the recommended limits to avoid disrupting the normal operation of the target service. Thus, our work does not raise ethical issues or negatively impact the target vendor's services."}, {"title": "A.1 In-Depth Analysis of Existing Pixel-Level Disruption Methods", "content": "Current pixel-level disruption methods show limited effectiveness in mitigating the threats posed by DeepFake attacks. These approaches primarily focus on degrading the visual quality of images at the pixel level. Especially in black-box scenarios, these methods often introduce only superficial artifacts without significantly altering the semantic features of the generated images. To better illustrate the limitations of these methods, we conducted both quantitative and qualitative analyses. Since the disruption method proposed by Ruiz et al. [26] is not suitable for face-swapping models in a black-box scenario, we follow their source code and conduct experiments for StarGAN [4] and GANimation [25] in white-box scenarios, which similarly serves to illustrate the issue."}, {"title": "A.2 Description of Image Transformation Functions", "content": "When computing the protected images, we employ a differentiable random image transformation function to enhance the transferability and robustness against different face-swapping models. These transformations are described as follows: 1) Applies Gaussian blurring with random parameters. The kernel size is randomly chosen from {3, 5, 7}, and the sigma value ranges between 0.1 and 3.0. 2) Performs random cropping and resizing of the images. It generates cropped patches from the original image, applying a scaling factor between 0.25 and 4."}, {"title": "A.3 Effectiveness of the Protected Images under Different Distance Budget", "content": "We calculate protected images at different distance budgets \u03b5 on different surrogate models and then generate face-swapped images using the FaceShifter and SimSwap models. The evaluation results of the protected images obtained using the surrogate models ArcFace and FaceNet are presented . We can see that FMRs decrease significantly as the distance budget \u03b5 increases in various cases. This suggests that protected images effectively mitigate the threats posed by face-swapping attacks to authentication and identification services reliant on facial recognition technology."}, {"title": "A.4 Visualization Results of the Protected Images and Face-swapped Images", "content": "We present the visualizations of the protected images and corresponding face-swapped images based on the surrogate model ArcFace. Specifically, some visualization results of protected images calculated with our FSG under different distance budget \u025b. We provide the quantitative and qualitative evaluation results of face-swapped images generated by Faceshifter and SimSwap under the distance budget \u03b5 = 9. The evaluation results demonstrate the effectiveness of the protected images calculated on alternative Arcface in confusing visual perception."}, {"title": "A.5 Effectiveness on other DeepFake Models", "content": ""}, {"title": "A.5.1 Evaluation on image-level face swapping", "content": "We calculate the protected images based on different surrogate models and then generate corresponding face-swapped images using FSGAN, which involves facial reenactment and does not explicitly extract identity features.  shows the FMRs on various face recognition models and face verification APIs."}, {"title": "A.5.2 Evaluation on Diffusion-based Models", "content": "Recent advancements in diffusion models have marked significant progress in image generation tasks [12], showcasing higher-quality image synthesis compared to GANs. Preechakul et al. [24] utilize a learnable encoder to uncover high-level semantics, while employing diffusion probabilistic models (DPMs) as decoders to model the remaining stochastic variations, denoting this method as Diff-AE. We use Diff-AE to generate 500 pictures with modified attributes based on the protected images produced by FSG. The attributes include \u201cBangs,\u201d \u201cSmiling,\u201d \u201cEyeglasses,\u201d \u201cMustache,\u201d and \u201cPale_Skin.\u201d"}]}