{"title": "Towards Resource-Efficient Compound AI Systems", "authors": ["Gohar Irfan Chaudhry", "Esha Choukse", "\u00cd\u00f1igo Goiri", "Rodrigo Fonseca", "Adam Belay", "Ricardo Bianchini"], "abstract": "Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks. However, current implementations suffer from inefficient resource utilization due to tight coupling between application logic and execution details, a disconnect between orchestration and resource management layers, and the perceived exclusiveness between efficiency and quality.\nWe propose a vision for resource-efficient Compound AI Systems through a declarative workflow programming model and an adaptive runtime system for dynamic scheduling and resource-aware decision-making. Decoupling application logic from low-level details exposes levers for the runtime to flexibly configure the execution environment and resources, without compromising on quality. Enabling collaboration between the workflow orchestration and cluster manager enables higher efficiency through better scheduling and resource management.\nWe are building a prototype system, called Murakkab, to realize this vision. Our preliminary evaluation demonstrates speedups up to ~ 3.4\u00d7 in workflow completion times while delivering ~ 4.5\u00d7 higher energy efficiency, showing promise in optimizing resources and advancing AI system design.", "sections": [{"title": "INTRODUCTION", "content": "A Compound AI System is \u201ca system that tackles complex tasks using multiple interacting components, including multiple calls to different Al models, retrievers, or external tools\" [42]. With increasing task complexity and model capabilities, the workflows promise to grow deeper, more complex, and self-morphing with self-improving agents. Today, workflows are designed by explicitly defining the components, their interactions, and the allocation of resources.\nWhile effective for many tasks, these workflows in practice frequently suffer from inefficient resource utilization. Figure 1 shows a typical Compound AI workflow deployment, with multiple stages. Each stage in the workflow comprises of three key entities:\nProgramming Frameworks to create workflows by composing agents including LLMs, ML models, and tools. They have workflow orchestrators that may decide agent execution order, optimize prompts for LLMs, process intermediate outputs"}, {"title": "TODAY'S IMPERATIVE WORKFLOWS", "content": "Workflows in Compound AI Systems today are typically expressed through imperative programs that contain: (1) the system flow specifying the components and their interaction, (2) model types and configuration details to implement each component and any model/tool specific parameters, (3) resources for each component in terms of hardware configuration, and (4) pricing tiers in terms of performance guarantees (e.g., token generation throuhgput etc.)\nListing 1 shows such an example of a Video Understanding workflow, based on OmAgent [43]. It defines components (lines 2 to 8) to perform various tasks and their execution flow (line 12). For example, it has a frame extraction (frame_ext) component that uses OpenCV [9] and has task-specific parameters like sampling rate. For audio processing, it has a speech-to-text transcription agent (stt) implemented using Whisper [33]. It uses an LLM, in this case Llama [18], for summarizing scenes and specifies a context length. For each of these components, there is either a hardware configuration (e.g., 1 NVIDIA H100 [12] GPU) or pricing tier (e.g., 4 Provisioned Throughput Units or PTUs [7]).\nThis workflow tightly integrates the application logic (e.g., \"List objects shown/mentioned in the videos\"), the specific models to use (e.g., Llama), and the resources to allocate (e.g., GPUs: 1 or PTUs: 4). In addition to developing application logic, the developer has added burden to configure many hyperparameters and resource specifications. Often, these selections are suboptimal-there could either be resource stranding or underprovisionig leading to suboptimal performance. As a result we end up in a situation similar to Figure 1, with rigid cross-layer coupling, that makes it challenging to improve efficiency of such systems."}, {"title": "EFFICIENCY THROUGH FUNGIBILITY", "content": "Figure 2 shows our vision for Compound AI Systems, where application logic is decoupled from execution details. Developers focus on application logic, without managing model selection, updates, or resource demands. The runtime dynamically generates task graphs from high-level descriptions, mapping tasks to models and tools for efficient resource multiplexing while maintaining quality. It leverages application fungibility to optimize for efficiency at runtime [36].\nWe draw inspiration from the evolution of SQL and the query optimizations that are enabled by its declarative nature [10, 20]. Recent work has taken initial steps in exploring this for Al systems [3, 24, 26], albeit for narrow use cases and with limited focus on resource efficiency and multi-tenancy. We aim to bridge this gap with Murakkab."}, {"title": "Declarative Workflow Programming", "content": "Murakkab promotes high-level declarative workflow specifications for two reasons: (1) it frees developers from managing low-level implementation details, and (2) it enhances workflow flexibility by allowing dynamic selection of models, tools, and resources at runtime to improve efficiency.\nListing 2 shows the same Video Understanding workflow defined for Murakkab. The programmer provides a natural language description of the job (desc) and the required inputs for the job (inputs). The programmer may optionally assist the system by specifying sub-tasks (lines 4 to 6) that need to be performed to accomplish the job. However, if these tasks are not provided or are insufficient, an orchestrator LLM decomposes the job into smaller tasks based on"}, {"title": "Adaptive Runtime Scheduling", "content": "Table 1 outlines the parameters Murakkab adjusts to optimize monetary cost, power consumption, and execution latency while measuring their impact on workflow result quality. Below, we describe Murakkab's adaptive workflow execution and how these parameters guide scheduling decisions.\nJob Decomposition. Using a declarative programming model requires lowering the high-level job specification into actionable tasks. For dynamic workflows, Murakkab uses LLMs to decompose a job description into a set of tasks, following the ReAct [41] approach. The LLM orchestrates the execution order of tasks in the workflow and outputs a DAG.\nTask-to-Agent Mapping. Murakkab maintains a flexible library of agents, detailing their names, functionalities, and schemas (e.g., function arguments). The orchestrator uses this library and task descriptions to map tasks to suitable agents. For example, with NVLM [13] as the orchestrator LLM, Murakkab provides the agent library via the system prompt [30] and task descriptions via the user prompt. This enables the LLM to assign agents to tasks. Murakkab then supplies task metadata and input details to the LLM, requesting a tool call [29] for the selected agent. The LLM generates an executable code snippet with the necessary arguments to invoke the agent directly. For example, given the task \"Extract frames from each video\" and appropriate metadata, the LLM may generate the following tool call: FrameExtractor(start_time=0, end_time=60s, num_frames=10, file=\"cats.mov\").\nModel/Tool Selection. The library may contain multiple models or tools that support a given agent interface. For instance, the Speech-to-Text agent can be implemented using Whisper, DeepSpeech, Fast Conformer [34], and others. Each differs in response quality, performance and resource requirements. Murakkab generates an execution profile for each model/tool and hardware resource pair when a new one is added to the library-the profile captures an efficiency vs quality tradeoff. Efficiency metrics include cost, power consumption, and latency. At runtime, Murakkab selects the model/tool and resources that maximize efficiency while meeting the target quality.\nResource Allocation. Cloud platforms provide various hardware SKUs, including different GPU and CPU types with dynamic availability (e.g., Spot VMs [8], Harvest VMs [2]). Models and tools can run on a range of these hardware types. For instance, some models perform better on newer GPUs (e.g., NVIDIA H100 [12]) with higher FLOPS, while others"}, {"title": "Murakkab Overheads", "content": "Murakkab has several overheads associated with it. (a) Profiling: To be able to offer different resource configurations, we need to profile the agents and tools on different hardware and configurations. However, this profiling is amortized over the lifetime of all the workflows that use a particular agent or tool. (b) DAG Creation: Task understansing from a natural language prompt, and DAG creation requires LLM queries. However, these are short input and short output queries, that take less than 1% of the execution time of the target AI workflows. (c) Configuration Search: The search space across the levers mentioned in Table 1 can easily explode. Therefore, we are working on strategies to prune the space with greedy search using hierarchy of optimization functions."}, {"title": "EVALUATION", "content": "Our evaluation examines whether Murakkab can take advantage of the fungibility of the declarative workflow to identify different execution configurations using the levers in Table 1 and make a selection based on their efficiency/performance trade-off. We run the Video Understanding workflow derived from OmAgent [43] as shown in Listing 1 as the baseline and Listing 2 on Murakkab. The execution output and accuracy are the same in all comparisons.\nSetup. We run our experiments on two Azure VMs (Standard_ND96amsr_A100_v4 [5]) each with 96 AMD EPYC 7V12 vCPUs and 8 NVIDIA A100 (80GB) GPUs [28]. We use an OpenCV [9]-based frame extractor (CPUs), NVLM [13] as the LLM for frame summarization (8 GPUs for text completion and 2 GPUs for embeddings to insert in a VectorDB for question/answering), CLIP [32] for Object Detection (CPUs) and Whisper [33] for Speech-to-Text transcription (1 GPU).\nBaseline. Derived from OmAgent [43], the baseline workflow specifies a fixed execution without any intra-task parallelism or opportunity to utilize idle resources. Each scene and its constituent frames are processed sequentially. Figure 3 shows the results-this workflow completes in 283s and severely underutilizes resources."}, {"title": "DISCUSSION", "content": "AI Workflows-as-a-Service (AIWaaS). We envision a future for Compound AI Systems where developers focus solely on application logic, without needing to manage model or resource details. Similar to Functions-as-a-Service (FaaS) [21], where the runtime system handles resource allocation and load balancing, we propose an AI Workflows-as-a-Service (AIWaaS) model with similar capabilities. This can improve efficiency, lower operational costs, and make Al systems more accessible and easier to maintain. Applications will not need rewriting (e.g., prompt engineering, workflow recreation) when new models or tools are available-the runtime system will transparently adopt newer implementations and resources as needed.\nQuantifying and Controlling Quality. Cost-quality tradeoffs are well-studied for single-model queries (e.g., FrugalGPT [11]), but end-to-end workflows pose unique challenges. Model interactions cause cascading effects, making it costly and impractical to evaluate all combinations. We explore methods to narrow the search space by identifying stages with the greatest impact on cost and accuracy. Additionally, hallucinations in early stages can derail workflows, highlighting the need for more correctness checkpoints and tools for quality control.\nProprietary Models and Agents. Integrating agent providers and cloud platforms into a unified entity can improve resource efficiency in Compound AI Systems. However, proprietary models often cannot be exported, requiring external API calls to third-party models. This may reduce resource efficiency due to limited visibility into third-party resource usage. Further research is needed to determine when offloading tasks to third-party providers is more beneficial than using local models/tools, albeit with lower quality.\nMulti-cloud Compound AI Systems. Greater control over hardware resources is easier when the cloud platform and workflow execution service are managed by the same entity (e.g., Azure ML [6], AWS SageMaker [37]). However, using multiple cloud platforms [39] can reduce costs and offer a wider variety of hardware (e.g., Google TPUs [22]). This is possible if each platform exposes resource utilization metrics, allowing systems like Murakkab to manage resources across clouds and schedule tasks efficiently."}, {"title": "CONCLUSION", "content": "This work highlights inefficiencies in existing Compound AI Systems and identifies key challenges limiting resource optimization. To overcome these, we propose a reimagined architecture featuring fungible workflows, dynamic scheduling, and adaptive resource management. By unifying workflow orchestration with cluster management, our system enhances resource utilization, reduces operational costs, and maintains or improves result quality. Our preliminary evaluations show significant gains in efficiency, validating our approach. Looking ahead, our AIWaaS vision aims to simplify AI application development and make AI systems more accessible and sustainable across diverse use cases."}]}