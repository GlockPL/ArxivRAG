{"title": "Time-Series Foundation Model for Value-at-Risk", "authors": ["Anubha Goel", "Puneet Pasricha", "Juho Kanniainen"], "abstract": "This study is the first to explore the application of a time-series foundation model for VaR estimation. Foundation models, pre-trained on vast and varied datasets, can be used in a zero-shot setting with relatively minimal data or further improved through finetuning. We compare the performance of Google's model, called TimesFM, against conventional parametric and non-parametric models, including GARCH, Generalized Autoregressive Score (GAS), and empirical quantile estimates, using daily returns from the S&P 100 index and its constituents over 19 years. Our backtesting results indicate that, in terms of the actual-over-expected ratio, the fine-tuned TimesFM model consistently outperforms traditional methods. Regarding the quantile score loss function, it achieves performance comparable to the best econometric approach, the GAS model. Overall, the foundation model is either the best or among the top performers in forecasting VaR across the 0.01, 0.025, 0.05, and 0.1 VaR levels. We also found that fine-tuning significantly improves the results, and the model should not be used in zero-shot settings. Overall, foundation models can provide completely alternative approaches to traditional econometric methods, yet there are challenges to be tackled.", "sections": [{"title": "1 Introduction", "content": "While the application of Machine Learning (ML) in various fields in financial market research has gained significant attention\u2014such as in asset pricing, volatility forecasting, optimal hedging, and limit order book dynamics (Gu et al., 2020; Christensen et al., 2023; Tran et al., 2018; Mikkil\u00e4 and Kanniainen, 2023)\u2014the risk management literature still largely relies on traditional econometric techniques, especially in the area of Value-at-Risk (VaR). One reason for this may be that Deep Learning (DL) models require substantial amounts of data, which poses a significant limitation when applying them to calculate risk metrics for individual stocks. VaR estimations are often based on daily return observations (for example, see Gonz\u00e1lez-Rivera et al., 2004; Patton et al., 2019; Taylor, 2019, based solely on historical daily returns), and even 20 years of data would yield only around 5,000 observations. This is quite limited, especially considering that DL models often contain millions of parameters that need to be learned.\nHowever, a solution to this problem is currently underway. Recently, machine learning has experienced a paradigm shift with the emergence of foundation models, which are large-scale, general-purpose pre-trained and often fine-tunable models on diverse datasets spanning various data distributions. Although foundation models are built on standard deep learning and transfer learning principles, their scale leads to the emergence of new capabilities (Bommasani et al., 2021).\nIn 2023-2024, following major advancements in large language models (LLMs), several pre-trained time-series foundation models have emerged, including Google's TimesFM (Das et al., 2024), Time-GPT (Garza and Mergenthaler-Canseco, 2024), LagLlama (Rasul et al., 2023). These time-series forecasting models are pre-trained on vast datasets of billions of real-world time points and can be used with or without fine-tuning, requiring only relatively small datasets, with just a few hundred observations. Compared to conventional dedicated supervised ML models, large- foundational AI models are pre-trained using large and diverse datasets. enable new applications more quickly and cost-effectively, and support a wider range of AI applications. The word 'foundational' emphasizes their flexibility, as they can be adapted or customized for different tasks or domains with minimal additional training.\nIn this paper, we implement a pre-trained time-series foundation model, specifically Google's TimesFM*, for Value-at-Risk (VaR) estimation and compare it with existing state-of-the-art parametric and semi-parametric approaches, including GARCH model, Generalized Autoregressive Score (GAS) one-factor model, and empirical quantile estimates. The key question is whether the pre-trained foundation model can outperform these existing econometric methods. This paper could employ alternative foundation models as several have been published recently, and more are on the way. However, we decided to focus on the use TimesFM for two reasons: First, it is an"}, {"title": "2 Models", "content": "2.1 Time-Series Foundation Models\nIn the case of time-series forecasting, these models share similarities with large language models (LLMs), as they rely on transformer-based architectures. While LLMs are trained to predict the next or missing part of a text sequence, foundational time-series models process continuous time points as individual tokens. This approach allows them to handle time-series data step by step. Similarly to Large Language Models, time-series foundation models utilize transformers to capture long-range dependencies and patterns within sequential data.\nThe TimesFM model (Das et al., 2024) is a decoder-only transformer architecture, similar to GPT models. In contrast to traditional encoder-decoder models, where the encoder processes the input sequence and the decoder generates output based on the encoded input, decoder-only models like TimesFM the decoder-only model focuses solely on output generation. processing the input through successive decoder layers. This allows for faster training and inference times compared to encoder-decoder models (see, for example Krishnan Suresh et al., 2024). Decoder-only models like are highly advantageous for zero-shot forecasting, where minimal fine-tuning is required. For that reason, in this paper, we use TimesFM as a foundation model as our aim is to test the performance of foundation models with small-sample fine-tuning properties. Moreover, in contrast to TimeGPT, it is open-source solution with proper documentation (Das et al., 2024). On the other hand, the advantage of the encoder-decoder architecture, which is used, for example, in TimeGPT (Garza and Mergenthaler-Canseco, 2024), is that it can handle more complex inputs and exogenous variables. However, in this paper, we use univariate and well-structured time-series data on returns, and VaR is calculated separately for each security.\nCompared to the state-of-the-art LLMs, TimesFM time-series foundation model is much smaller in both parameter size with 200M parameters and pretraining data size with around 100B timepoints. However, these settings allow it to achieve accurate zero-shot forecasts, meaning it generate forecasts from unseen datasets."}, {"title": "2.2 Benchmark Models for VaR", "content": "To evaluate the performance of the foundation time series models for estimating Value-at-Risk,\nwe benchmark it against several established models. These include both traditional and advanced\nmethods, each with its own set of assumptions and computational characteristics. The benchmark\nmodels are as follows:\n\u2022 Rolling Window Quantile Estimation:\nOne of the most widely used approaches for VaR estimation is the rolling window method,\nwhich computes the risk measure based on historical data over a fixed time window. This\nmethod is defined as follows:\nVaRt = Quantile {ri}i=t-m,\nwhere Quantile {ri}=t-m represents the sample quantile of the variable ri over the period\ni \u2208 [t - m,t - 1]. The parameter m denotes the size of the rolling window. While this\napproach is straightforward and easy to implement, it does not account for the dynamic\nnature of financial markets, leading to potential inaccuracies during periods of high volatility.\n\u2022 ARMA-GARCH Models (Angelidis et al., 2004):\nTo better capture the time-varying properties of financial returns, we employ ARMA-GARCH\nmodels. These models estimate the conditional mean and variance of returns, incorporating\nvarious distributional assumptions for the standardized residuals. The model is specified as:\nrt = \u03bc\u03b5 + \u03c3\u03c4\u03b7\u03c4,\nnt ~ iid Gn(0,1),\nIn this framework, ut represents the conditional mean, typically modeled using an ARMA\nprocess, while \u03c3\u1f33 denotes the conditional variance, modeled using a GARCH process. The\nstandardized residuals nt follow a distribution G\u03b7 with mean zero and variance one. The VaR\nfor these models is calculated as follows:\nUt = \u03bc\u03b5 + \u03c3\u03c4\u03b1\u03b1,\nwhere aa = G\u00b9(\u03b1)\nHere, aa is the a-quantile of the distribution G\u03b7. Different assumptions for G\u03b7 yield various\nmodel specifications, including:\n(i) Normal GARCH: This specification assumes that the residuals follow a normal distribu-\ntion:\nnt ~ iid N(0,1),\nThis is the most basic assumption and is often insufficient for capturing the heavy tails\nobserved in financial return distributions.\n(ii) Student's t-GARCH: \u03a4\u03bf account for the heavy tails and potential skewness in the return\ndistribution, a skewed t-distribution is used for the residuals:\nnt ~ iid Skew t(0,1,\u03bd, \u03bb),\nThis model provides greater flexibility in capturing the tail behavior of financial returns,\nwhich is crucial for accurate risk estimation.\n(iii) Empirical Distribution Function: As a nonparametric alternative, we use the Empirical"}, {"title": "3 Data and Experiments", "content": "We now apply the models discussed in the last section to the forecasting of VaR for daily returns on\nthe S&P 100 index along with its 91 constituents. We obtained the data from the Thomson Reuters\nDatastream, and our study period covers approximately 19 years from January 2005 to September\n30, 2023, resulting in 4,876 observations. The data period was selected to maximize the availability\nof constituents, resulting in 91 stocks in our dataset. The daily return for i-th stock at t-th day\nis calculated as rit Pit-Pit-1; i = 1, . . ., n, t = 1, . . .,T, where pit and pit\u22121 are respectively, the\nclosing prices at t-th and (t - 1)-th day. We divide the data into two consecutive non-overlapping\nsamples while maintaining the temporal ordering of the data. We use the first ten years of data,\ni.e., January 2005 to December 2014, for estimating model parameters and the remaining 9 years,\ni.e., January 2015 to September 2023, for testing the performance of various models.\nFor a comprehensive view of the data, Table 1 reports the summary statistics for key risk and\nreturn metrics of the 91 constituent stocks and the index. Specifically, it reports the mean, median,\nstandard deviation, minimum, and maximum values for the Mean, Standard Deviation, Skewness,\nKurtosis, VaR, and CVaR of these assets. From skewness and kurtosis values, we observe that the\ndistribution of these returns are fat tailed and are skewed to some extent, thus, agreeing with the\nstylized facts of empirical returns. Also, the mean VaR across 91 stocks is less than that of the\nindex, showing that index is less riskier compared to the constituent stocks."}, {"title": "3.1 Experiment Setup", "content": "In this paper, we implement the recently introduced pre-trained time-series model, Google's TimesFm,\nfor forecasting value at risk and compare its performance against state-of-the-art parametric and\nnon-parametric approaches, namely, GARCH model with the standardized residual modelled as\nempirical distribution function (G-EDF), normal distribution (G-N) and Student's-t (G-t) distri-\nbution, one factor Generalized AutoRegressive Score model (GAS) and rolling window method\n(Historical).\nFor each of the models, G-EDF, G-N, G-t and one factor GAS, we obtained the VaR estimates\nat levels 1%, 2.5%, 5% and 10% over the test period, 2nd January 2015 to 19th September 2023.\nFor the rolling window (Historical) method, we used the data of 512 days preceding the first test\nday (i.e., 2nd January 2015), and forecasted the VaR at various levels. We then shift the training\nsample by one day forward in time to include more recent data and exclude the oldest data point\nsuch that a fixed size of the rolling window is maintained. At each rolling step, we obtain the\nprediction for the next day using the most recent 512 observations, thus resulting in a sequence of\n2268 VaR estimates for each level. For GARCH and GAS models, we followed Patton et al. (2019)\nand used January 2005 to December 2014 as a training period for estimating model parameters,\nwhich were then used to forecast over the same test period as before, again resulting in a sequence\nof 2268 VaR estimates for each level.\nTimesFm is originally trained on a large number of public and synthetic datasets and exhibits\nrobust out-of-the-box zero-shot performance in comparison to the accuracy of various state-of-the-\nart forecasting models specific to individual datasets under consideration. The model is flexible\nenough to forecast with different input length, prediction length and time granularities at inference\ntime. The pre-trained model returns the point forecasts along with deciles, i.e., the quantiles\" at\n10%, 20%,..., 90% levels. We experimented with different prediction horizons, but with a fixed\ninput length of 2 years (512 days). Specifically, we set the prediction horizon to 1 day, 21 days"}, {"title": "4 Evaluation metrics", "content": "In financial risk management, VaR is a widely used measure that estimates the potential loss in the\nvalue of a stock over a given time period, with a certain confidence level. However, VaR estimates"}, {"title": "5 Results", "content": "5.1 Actual over Expected Ratio\nTable 2 reports the summary statistics, across 92 stocks, of the |1-AE| values for the VaR forecasts\nat different levels, and the corresponding models in the out-samplex' period. Specifically, we report\nthe minimum, mean, median, maximum, standard deviation across all the stocks. Additionally, we\nreport the count of stocks for which each of the considered model was the best (achieved lowest\nvalue of |1 - AE|) or was within top two models (1st-2nd best). Note that, as mentioned before,\nthe VaR forecasts for TimesFm pretrained model are only available at 10%, there we have three\nextra models in the last block of Table 2.\nSeveral interesting findings emerge. First, the fine-tuned TimesFm is superior to the pre-trained\nTimesFm model. For instance, models FT1, FT21 and FT63 attained the lowest |1 - AE value\nin 22, 10 and 1 assets respectively, in comparison to their pretrained counterparts that attained\nlowest value in 1, 9 and 0 assets. Second, for the fine-tuned TimesFm model, the mean value\nof |1 \u2013 AE| is better when the prediction length is large for VaR at 1% and 2.5%, whereas it is\nbetter for smaller prediction length when forecasting VaR at 5% and 10% levels. Third, for each\nVaR level, fine-tuned TimesFm model FT1 performed fairly well in comparison to the benchmark\nmodels. We also performed a two-sample t-test to check whether the mean, over the 92 stocks, of\n|1-AE| from one model is significantly different from that from the other model. More specifically,\nwe performed two-sample t-test for each pair of model where we test\nHo : |1 \u2212 AE|\u2081 = |1 \u2013 AE|j,\nagainst a one-sided alternative\nH1: |1 - AE|\u00bf > |1 \u2013 AE|j,\nwhere i denotes the model in the selected row, whereas j denotes the model in the selected column.\nFurther, the formatting in the Tables are as follows: *, ** and *** denotes whether the t-test of\nequal predictive accuracy is rejected at the 5%, 2.5% and 1% level of significance. Table 3 reports\nthe results to t-tests. We can observe that the fine-tuned models, in particular, FT1 and FT21,\nperforms remarkably well across all quantile levels, depicted by failure to reject the null hypothesis\nin the first and second rows of each block. Further, we observe that FT1 outperforms GAS model\nin forecasting Var(1%), VaR (5%) and Var(10%) at 97.5%, 95% and 99% level of significance\nrespectively, and performs atleast as good as GAS model for forecasting Var(2.5%) (refer rows"}, {"title": "5.2 Quantile Loss", "content": "Table 7 presents the summary statistics of quantile loss scores across 92 stocks. We observe that, at\n1% VaR, the mean and median quantile score is consistent across the models, with FT1 and FT21\nexhibiting lower mean values, 0.062 and 0.065, respectively. This indicates a better performance\nof fine-tuned TimesFm models compared to GARCH models (G-EDF, G-t, G-N) that have higher\nmean values and higher values for maximum quantile score. Further, higher value of standard\ndeviation for GARCH models show that these models have high variability in their VaR forecasts\nwhich, in turn, reflects that these models are less stable in extreme market scenarios. These\nobservations remain valid at 2.5% VaR forecasts, with FT1 and FT21 showing lower mean and\nmedian quantile scores, while GARCH models still show higher values for maximum quantile score\nand the standard deviation. For VaR at 5% and 10% levels, the historical model performs similar\nto fine-tuned TimesFm model whereas GARCH models continue to show higher maximum quantile\nscore and greater variability, thus making them less robust and more sensitive to extreme market\nconditions. In addition, GAS models obain lower values of mean and median quantile scores,\nsimilar to that obtained by FT models across all quantile levels. FT models are thus competitive\nto the benchmark models, in particular, GAS model which show slightly higher values of maximum\nquantile loss and higher standard deviation particularly at VaR 5% and 10% levels."}, {"title": "6 Conclusion and Discussion", "content": "The question of whether foundation models can outperform traditional methods in VaR analysis is\nfundamentally important from the perspective of economics literature. In this paper, we evaluated\nthe performance of Google's foundation model for time series, TimesFM, in estimating Value-at-\nRisk (VaR) for the S&P 100 index and its 91 constituents. Our main finding is that in comparison to\nthe state-of-the-art econometric models for VaR, TimesFM performs surprisingly well. Specifically,\nTimesFM outperforms all econometric benchmarks, including the GAS model, which is widely\nregarded as the leading method in recent econometric literature (Patton et al., 2019), in terms\nof Actual vs. Expected violations (AE). Traditional models like GARCH (with different residual\ndistributions) and empirical quantile estimates fall notably behind. These results remain consistent\nacross different VaR levels, underscoring the robustness of TimesFM. Additionally, we assessed\nperformance using the Quantile Score Loss Function, where TimesFM showed results comparable\nto the best econometric models.\nWe initially used the pre-trained TimesFM model without any task-specific training as a zero-\nshot predictor and observed significant statistical improvements after fine-tuning it with our data.\nThe fine-tuned TimesFm model performed as well as the parametric, semi-parametric, and non-\nparametric approaches from the literature of forecasting the value at risk. As the results favor\nthe foundation AI model, financial time-series modeling could shift toward data-driven approaches,\npartially reducing the need for mathematical modeling. Such a shift would have significant im-\nplications for both academia and the financial industry, as the application of foundation models\ncan be applied with relatively minimal methodological expertise in econometrics. In conclusion,\nfoundation models hold promise for improving Value-at-Risk (VaR) estimation, offering flexible\nalternatives to traditional econometric approaches.\nHowever, the adoption of foundation models is accompanied by notable challenges. A key challenge\nwith these models is their \"black-box\" nature, which can make it difficult to understand how they\narrive at their predictions. This is particularly concerning because we still have only a partial\nunderstanding of how these models operate, where they might fail, and what they are truly capa-\nble of, mainly due to their complex, emergent behaviors. This lack of transparency is especially\nproblematic in high-stakes financial settings, where it's crucial to understand the reasoning behind\npredictions for effective risk management and decision-making. Moreover, this opacity can cre-\nate hurdles for regulatory acceptance, as regulators typically require clear and explainable models"}]}