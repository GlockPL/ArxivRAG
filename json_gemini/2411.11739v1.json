{"title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou", "authors": ["Xinchen Luo", "Jiangxia Cao", "Tianyu Sun", "Jinkai Yu", "Rui Huang", "Wei Yuan", "Hezheng Lin", "Yichen Zheng", "Shiyao Wang", "Qigen Hu", "Changqing Qiu", "Jiaqi Zhang", "Xu Zhang", "Zhiheng Yan", "Jingming Zhang", "Simin Zhang", "Mingxing Wen", "Zhaojie Liu", "Kun Gai", "Guorui Zhou"], "abstract": "In recent years, with the significant evolution of multi-modal large models, many recommender researchers realized the potential of multi-modal information for user interest modeling. In industry, a wide-used modeling architecture is a cascading paradigm: (1) first pre-training a multi-modal model to provide omnipotent representations for downstream services; (2) The downstream recommendation model takes the multi-modal representation as additional input to fit real user-item behaviours. Although such paradigm achieves remarkable improvements, however, there still exist two problems that limit model performance: (1) Representation Unmatching: The pre-trained multi-modal model is always supervised by the classic NLP/CV tasks, while the recommendation models are supervised by real user-item interaction. As a result, the two fundamentally different tasks' goals were relatively separate, and there was a lack of consistent objective on their representations; (2) Representation Unlearning: The generated multi-modal representations are always stored in cache store and serve as extra fixed input of recommendation model, thus could not be updated by recommendation model gradient, further unfriendly for downstream training.\nInspired by the two difficulties challenges in downstream tasks usage, we introduce a quantitative multi-modal framework to customize the specialized and trainable multi-modal information for different downstream models. Specifically, we introduce two insightful modifications to enhance above framework: (1) Item Alignment to transform the original multi-modal representations to match the real user-item behaviours distribution. (2) Quantitative Code to transform the aligned multi-modal representations to trainable code ID for downstream tasks. We conduct detailed experiments and ablation analyses to demonstrate our QARM effectiveness. Our method has been deployed on Kuaishou's various services, serving 400 million users daily.", "sections": [{"title": "1 INTRODUCTION", "content": "Kuaishou, is one of the largest short-video and live-streaming platform in China. As a new type of information-sharing media, Kuaishou attracts a lot of attention and accumulates a large number of users to watch/create short-videos, and even shopping goods after watching some online-shopping and advertising short-videos or live-streamings. To find the most interesting short-videos content from billions of short-videos pool and provide satisfied experience for our users, a strong recommender system (RecSys) is a cornerstone to support Kuaishou business [3, 30]. Generally, to obtain a powerful recommendation model, the common wisdom always holds the idea that the model should be learning from the massive real-time user-item interaction data with a large number of hand-craft model input features. In past years, many recommendation engineers/researchers proposed several milestones work to elaborate the ID-based features to support model input features, such as cross ID features (e.g., FM [24], DCN [28]), list-wise ID features (e.g. DIN [35], TWIN [4]). In recent years, with the significant evolution of multi-modal large models (e.g., GPTs [2]), many recommender engineers/researchers realized the potential of multi-modal information in recommender area, to understand the item certain semantic signal to make more smart recommendation. Especially at the platform at Kuaishou, the short-video and live-stream are highly integrated multi-modal media, it is difficult to fully understand them by assigning an ID embedding only.\nHowever, although powerful, the multi-modal large language models (MLLMs) are notorious for their tremendous computation cost in training and inference, considering the massive online requests for industrial recommendation services, it is impractical to directly add a large multi-modal module into recommendation model [25]. To alleviate the computation pressure, to our knowledge, many companies apply a two-step deployment solution to incorporate such MLLM-based semantic information for recommendation model (As shown in Figure 1): (1) Pretraining a MLLM to compress the items' text, visual and audio information as an omnipotent representation. Next utilize the pre-trained MLLM to produce item representations, and save them in a cache store to provide the world semantic knowledge for downstream models. (2) According to the training sample information, the downstream models could fetch the corresponding necessary multi-modal representation as a part of input features, to enhance the model's prediction ability. At Kuaishou, our recommendation models are also equipped with such deployment solution, and achieve remarkable online A/B gains in different businesses, such as online-shopping and advertise short-video and live-streaming recommendation. Nevertheless, such non-end-to-end framework has two obvious problems limit model performance:\nRepresentation Unmatching: Common multi-modal features are obtained through self-supervised tasks like image-text matching [14, 22], while ID-based features use user interaction history as supervision signals[4, 35]. These differences make it challenging to unify multi-modal information and recommendation knowledge in downstream training. Therefore, a question is that: Can we enhance the multi-modal representation consistency for downstream task?\nRepresentation Unlearning: In practice, the newly added multimodal features do not update with the training of the recommendation system [25]. Nevertheless, for the others discrete ID-based features (e.g., user ID, item ID), our recommendation model could assign a corresponding embedding spaces to them for end-to-end optimization with the real-time user-item interaction data. Consequently, the static multi-modal representations are easily limit the model fitting ability and obstacles model convergence. As a result, there rise another question: Can we allow multi-modal representations optimized in end-to-end manner?\nMotivated by the two difficulties challenges in downstream tasks usage, in this paper, we present our effective and efficient solutions for multi-modal information enhancement, the Quantitative Alignment Multi-Modal Recommendation, termed as QARM. Specifically, our QARM consists of two major processes to answer the above questions, the item alignment mechanism to enhance the representation consistency, and the quantitative code mechanism to generate learnable code ID for downstream tasks.\nItem Alignment mechanism: To alleviate the first representation unmatching challenge and maximize the representation consistency ability, we consider fine-tuning the pre-trained multi-modal model in a customized manner. The reason is that different businesses have different characteristics, and the downstream task expected multi-modal representation should express corresponding business characteristics. For example, the causal relationship between different categories of goods is beneficial for online-shopping short-videos services while the same category item relationship is more important for usual short-videos recommendation. Therefore, such fine-tuning paradigm should be customized for each type of downstream business. To implement it, we insert a pre-order item alignment mechanism to fine-tune the multi-modal model with corresponding business data, to encourage MLLM representation could reflect the real business user-item interaction pattern.\nQuantitative Code mechanism: To overcome the second frozen representation unlearning challenge for full multi-modal information adaptation, inspired by the code hashing [13] and straight-through estimator [26, 27] idea, we consider generating the Semantic IDs for the down-stream tasks. Specifically, after obtaining the fine-tuned multi-modal representation, we propose a simple-but-effective heuristics residual K-means algorithm to obtain a quantization codebook. Once the quantization codebook is trained, we next freeze the codebook and use it to measure the fine-tuned multi-modal representation to calculate corresponding Semantic IDs. Finally, in downstream recommendation model training, we"}, {"title": "2 METHODOLOGY", "content": "In this section, we explain our QARM components and the total deployment workflow. Before going on, we first retrospect the background of industrial RecSys, including the feature engineering, and the training paradigm of different models used in different stages. Next, we dive into QARM, to express the item alignment mechanism for representation unmatching problem and the quantitative code mechanism for representation unlearning problem in detail."}, {"title": "2.1 Background of Industrial Recommender", "content": "In industry, RecSys models are trained by the massive real-time user-item interaction data (i.e., the labels) and a large number of learnable discrete ID-based features (i.e., the model input) [21]. To be specific, the model input features are always formed into four types:\n\u2022 ID-based features to describe the context information, such as the User ID, Item ID, Scenario ID, is follow author, etc.\n\u2022 List-wise ID-based features to describe user interests or item property, such as the users lastest clicked items, other similar item list with item candidate.\n\u2022 Bucketing ID-based features to describe the statistics features, such as the numbers of purchases in the last month.\n\u2022 Multi-modal frozen representation features to describe the items text/visual/audio information.\nEquipped with above input features, to find the most related items from billion-scale item pool, the recommender chain always follow a two-stage cascading paradigm [5] to make a trade-off between performance and efficiency to respond the vast online requests:\n(1) Retrieval stage [17, 32]: according user's past interested items, the Retrieval stage always uses simple models to search a small hundred items from total billion item set.\n(2) Ranking stage [3, 19]: based on generated item sets from retrieval stage, the ranking stage always utilizes complex model to play hundreds of times to estimate probabilities for hundreds of item candidates, and then select the best dozen items with the highest interaction probabilities.\nGenerally, the Retrieval models follow two classic designs, the User2Item and Item2Item framework, while the Ranking model follows a multi-task learning paradigm, as shown in Figure 2. In the following section, we express how our QARM enhances the retrieval and ranking stages."}, {"title": "2.2 Item Alignment of QARM", "content": "Particularly, the multi-modal representations are verified to contribute remarkable gains to serve as the user-side feature, item-side feature, and target item-aware user historical item list feature in our recommendation models. Since the aforementioned serious representation unmatching issue, the multi-modal information is still limited. To alleviate such problem, previous efforts always devise additional contrastive module to align with item ID and its multi-modal representations. However, such contrastive loss is weak and easily over-fitting because of the ground-truth is not diverse enough, e.g., the item MLLM only has one ground-truth, its item ID embedding.\nInstead of contrastive objective, to ensure that the multi-modal features are relevant to the user behavior decisions of specific businesses, we consider further fine-tuning such multi-modal representation with the the real downstream business interaction data before the representation input to downstream models. To implement the above idea, we decide to build a pure multi-modal representation input only alignment model, and employ the existing retrieval models' knowledge to supervise it to reflect real business characteristics. Specifically, we first generate high-quality item2item pairs dataset D in following ways:\n\u2022 Based on User2Item retrieval model, for each user positive clicked target item, select the highest similar item in ID representation space as trigger item from historical lastest 50 his/her positive clicked item set.\n\u2022 Based on Item2Item retrieval model, utilizing existing models learned stable item pairs with high similarity as data sources, e.g., export data from our Swing retrieval model.\nOn top of the high-quality item2item pairs dataset D, we next train a item2item style alignment models with pure multi-modal representation. For a random batch data $B e \\in D$, we have:\n$M_{trigger} = MLLM(T_{text}^{trigger}, T_{audio}^{trigger}, T_{image}^{trigger}),$\n$M_{target} = MLLM(T_{text,}^{target} T_{audio}^{target}, T_{image}^{target}),$\n$L_{align} = Batch-Contrastive(M_{trigger}, M_{target}, B),$\nwhere the $M_{trigger}$/$M_{target} \\in R^{|B|\\times d}$ means the generated trigger/target item MLLM representation in batch manner (d indicates representation dimension), the $T_{text}^{trigger}$, $T_{audio}^{trigger}$, $T_{image}^{trigger}$,$T_{text,}^{target}$, $T_{audio}^{target}$, $T_{image}^{target}$ are raw input text, audio and image tokens of trigger/target item for MLLM, and the $L_{align}$ is our QARM alignment training loss. By optimizing the item alignment loss, the MLLM representations are encouraged to align with the downstream business knowledge, maximizing the representation consistency."}, {"title": "2.3 Quantitative Code of QARM", "content": "After obtaining the alignment multi-modal representation, the next stage is to apply the MLLM world knowledge to enhance the downstream models prediction accuracy. However, comparing utilizing the pre-trained representation as a part of model input directly, the recommendation model is is actually more suitable for end-to-end training using ID style features. Inspired by the code hashing and straight-through idea achieves great success in CV [6] and DM [23], we also consider generating a series of quantitative code IDs to replace the MLLM representation. Specifically, we design two heuristics simple-but-effective quantitative mechanism to transform the learned item alignment MLLM representation $M \\in R^{|I|\\times d}$ by the Vector-Quantized (VQ) [27] and Residual-Quantized (RQ) [12] codes, where the I denotes the item set.\n\u2022 For the VQ code, as the most used quantitative technique, it first trains a large-scale codebook matrix and then utilizes the top-k nearest neighbor search to hash a representation. In our QARM, since the pre-trained MLLM representations can already reflect complex items' correlation, thus we do not train a new codebook matrix but employ all the items' alignment representations as the codebook directly for the sake of simplicity:\n$V = M,$\nwhere the V denotes the VQ codebook of QARM. According to it, we can quantization an arbitrary MLLM representation m \u2208 M as follows:\n$0_{1}, 0_{2},..., 0_{K} = TopKCode(V, m, K),$\nwhere the TopKCode() aims to find the Topk similar representation index (i.e., code) from V, k is a hyper-parameter to control VQ codes number, and the $[0_{1}, 0_{2}, ..., 0_{k}]$ is the VQ quantitative codes for representation m.\n\u2022 For the RQ code, instead of utilizing a larger codebook size to hash a representation, the RQ uses a fixed size of codebook to recursively quantize a representation in a coarse-to-fine manner. In practice, the RQ always trains L levels codebooks with cascading relationships, and then searches the nearest neighbor index for each layer residual representation. In our QARM, we utilize the heuristics Kmeans algorithm to generate the codebook for each level:\n$R^{1} = Kmeans(M, N),$\n$R^{2} = Kmeans(M^{1}, N),$\n...,\n$R^{L} = Kmeans(M^{L-1}, N)$\n$M^{1} = M - NearestRep(M, R^{1})$\n$M^{2} = M^{1} - NearestRep (M^{1}, R^{2})$\nwhere the NearestRep() denote the nearest representation search method in codebook, and the $[R^{1}, R^{2}, ..., R^{L}]$ are the trained codebook list of RQ. As a result, we can quantize an arbitrary item's MLLM representation m \u2208 M as follows:\n$r_{1} = NearestCode(R^{1}, m, 1),$\n$r_{2} = NearestCode(R^{2}, m^{1}, 1),$\n...,\n$r_{L} = NearestCode (R^{L}, m^{L-1}, 1)$\n$m^{1} = m - R^{1}$\n$m^{2} = m^{1} - R^{2}$\nwhere the $[r_{1}, r_{2}, ..., r_{L}]$ is the RQ codes for representation m.\nThe pseudo-code of our VQ and RQ codebook generation is shown in Algorithm 1, thereby any alignment MLLM representations could be transformed as two type codes $[0_{1}, 0_{2}, ..., 0_{K}]$ and"}, {"title": "2.4 Usage of QARM", "content": "On top of the quantitative codes, we next devise several simple-but-effective ways to produce attributes to support our downstream recommendation model for end-to-end MLLM information training. Generally, we implement the item-side feature, user-side feature for retrieval and ranking model, and the target item-aware feature for ranking model (as shown in the Figure 2 and Section 2.1):\n\u2022 Item-side feature: Straightforwardly, we utilize the VQ code and RQ code as item ID feature, and then assign corresponding embedding spaces for these codes to lookup end-to-end learnable embeddings.\n\u2022 User-side feature: To describe users' interests, we employ the quantitative codes of latest user's positive interacted items' sequence as a part of model input.\n\u2022 Target item-aware feature: Instead of learning our code representation directly, we also apply the target item quantitative code to search several item sequences as target item-aware cross features. For example, according to our RQ code, we could generate the latest first one-code matching item sequence, the latest two-code matching item sequence, and so on.\nThe above features modeling methods are basically the same as some common works in the industry, and overall learning processes formed are as follows:\n$Code_{i} = IDLookUp ([v_{i}, ..., v_{k} ] \\oplus [r_{i},...,r_{l}]),$\n$ItemCodeRep =ItemNet (Code_{Target}),$\n$UserCodeRep =UserNet ([Code_{1},..., Code_{n}]),$\n$CrossCodeRep = CrossNet(ItemCodeRep, [Code_{Search,ctr....}, Code_{Search}]),\\oplus$ \n$\\hat{y}_{ctr}, \\hat{y}_{lvtr}, ... = MoE([UserCodeRep, ItemCodeRep, CrossCodeRep, Other FeaRep])\\oplus$\n$\\mathcal{L} = - \\sum_{xtr}(y_{xtr} log(\\hat{y}_{xtr}) + (1 - y_{xtr})log(1 - \\hat{y}_{xtr}))\\oplus$"}, {"title": "3 SYSTEM DEPLOYMENT", "content": "As shown in Figure 4, our QARM is trained on Kuaishou's distributed training system, there are three basic blocks needed, the feature engineering, offline training system, and online serving system. To be specific, our QARM is a part of the feature engineering block, which focuses on pre-processing the users' new uploaded short-video information as ID-based codes to support our offline training and online serving system. To optimize such multi-modal code embedding, our offline data engine assembles the hundreds of billions user-item pairs feature and the real interaction label every day, to feed our model to update the model parameters. Furthermore, the trained parameters are synchronized to the online inference model in real-time manner, to respond to the user's recommendation requests."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct detailed offline/online experiments and detailed ablation studies at Kuaishou's Shopping and Advertising services, to validate our QARM effectiveness."}, {"title": "4.1 Evaluation Technique", "content": "As a common evaluation protocol in industry [4, 34], to verify how much benefit that our QARM can contribute to our system, we equip it to our baseline models at Shopping and Advertising services. It is worth noting that the two strong baseline models are huge models and have been already incorporated the cached MLLM representation in Figure 1. For the evaluation, we use three wide-used ranking metrics to measure ranking model prediction performance: the AUC, UAUC and GAUC. The AUC metric reflects the general probability that the score of a positive user-item pair is higher than the score of a negative user-item pair. The UAUC metric estimates the average AUC value across different users. Moreover, the weighted version of UAUC, GAUC, incorporates different user interaction ratios to provide more precise evaluation. They are formed as follows:\n$UAUC = \\frac{1}{\\left | U \\right |}\\sum_{u}AUC_{u}, GAUC = \\frac{\\sum_{u}\\left | sample_{u} \\right |}{\\left | all sample \\right |}AUC_{u},$"}, {"title": "4.2 Offline Performance", "content": "Table 2 and Table 1 report the shopping and advertising services' offline results in terms of the CTR, CVR (we set the VQ code length is K = 25, RQ code length is L = 6, dimension d = 64). From them, we have several observations: (1) Compared with the different Baseline Model, directly incorporating the item alignment MLLM representation could further enhance the models prediction accuracy in the Advertising services, which indicates that alleviates the representation unmatching issue to encourage MLLM representation to align with the real bussiness interaction distribution knowledge is helpful. (2) Compared with '+ IA Rep' model variant, utilizing the VQ code or RQ code to represent the items' MLLM representation are shown more significant improvements, which indicates that overcoming the representation unlearning issue and assigning learnable embedding for end-to-end training is vital for recommender model convergency. (3) Compared with the '+ VQ code' or '+ RQ code' model variants, incorporating the VQ and RQ codes at same time could further enhance model performance, we think the reason might be that the two codes could reflect different MLLM knowledge in our designing. Indeed, the VQ code aims to utilize the TopK similar item neighbors to represent target item information, while the RQ code focuses on encoding the entire"}, {"title": "4.3 Online A/B Test", "content": "To quantify the certain contribution our QARM could make to our online services, we launched the variant '+ VQ & RQ Code' to online A/B test system to respond real user Shopping and Advertising requests. In practice, different services have their core revenue metrics, e.g., the ePCM of advertising and the GMV of shopping. Table 3 and Table 4 report our online results of Advertising and Shopping services individually, where the '#1' and '#2' denote different application scenarios of corresponding services, e.g., short-video scenarios or Mall scenario. According to them, we can find that our QARM achieves a large improvement at Revenue +9.704%/+3.147% and +9.555%/+1.950% on the cold-start item group and another group in advertising, and GMV+2.296%/1.568% in online-shopping, which indicates understand the multi-modal item semantic signal could contribute to our system significantly."}, {"title": "4.4 Case Study of QARM", "content": "This section explores the impact of the multi-modal information on our system content distribution. Intuitively, the multi-modal information is friendly to long-tail items since multi-modal information is not sensitive to whether the items are popular or not. Therefore, the improvement in the performance of long-tail items can to some extent prove that our QARM is a reasonable way to introduce multi-modal information for RecSys. To valid our QARM effectiveness in long-tail items, we divide items into six distinct groups, labeled from L1 to L6, based on the frequency of their purchases. L1 represents the least frequently purchased items, with each subsequent label indicating higher purchase frequencies. Firstly, we conduct the offline evaluation in the Table 5, from that we could observe a phenomenon: in terms of the CTR-AUC, the most important offline metric, the long-tail items show the highest prediction improvement and slowly decays according to the purchased times. Secondly, we also conduct the online evaluation in the Figure 5, which describes the item exposure times improvement per day. According to Figure 5, compared to the baseline model, the L1, and L2 items are enhanced to successfully recommend more times, while the more popular L4, L5, and L6 items are less frequently. Such two phenomenons indicate our QARM is a trustworthy way to inject the multi-modal information for RecSys, not only bringing a better experience to users but also building a fair environment for new-uploaded items.\nMoreover, we further conduct an experiment to visualize the impact of item alignment MLLM representations. Figure 6 gives three different t-SNE results under the mall scenario of our shopping service: (1) original image-only t-SNE, (2) original image-text t-SNE, and (3) item alignment t-SNE. Besides, we randomly select four items to show our item alignment effect, the two vests, one child T-shirt and a shoes. According to Figure 6, we can find that the three methods could identify the shoes and clothes satisfactorily. Nevertheless, the original MLLM representations failed to measure the two vests correctly, while the blue-colored child T-shirt is more closely with green-colored vests since they have similar image"}, {"title": "4.5 Parameter Analysis of Quantitative Code", "content": "Specifically, we set the VQ code length K = 25, RQ code length L = 6, and dimension d = 64 by default, to investigate the robustness of our QARM regarding the hyper-parameter factors of quantitative codes, we conduct experiments on the VQ code length K and the code embedding dimension d. For the VQ code length K, we implement 4 variants on the shopping scenario and shown the results in Table 7. According to it, we could observe that the VQ code length contributes in our model provides a relatively stable improvement to our QARM. For the code embedding dimension d, we implement 2 variants on the advertising scenario and shown the results in Table 8. According to it, we could observe that increasing the code dimension is also an effective way to enhance model performance. In summary, the experiment results demonstrate that learning a down-streaming task adaptive representation is vital for recommendation to understand the multi-modal information."}, {"title": "5 RELATED WORKS", "content": "Contrasitve learning based multi-modal information fusion Initial approaches involve utilizing \"off-the-shelf\" multimodal representations, either as fixed features or in conjunction with structural relationships, within the recommendation framework. For example, VBPR[9] enhances Matrix Factorization by incorporating visual features through a linear transform kernel, which is then concatenated with ID embedding. LATTICE[33] proposes to build visual and textual affinity graphs using respective embeddings and provide multimodal item-item relationships to the collaborative filtering model. BM3[36] leverages self-supervised learning to align both inter-modality and intra-modality representations within the collaborative filtering task.\nTo further improve modality-driven recommendations, multimodal pre-training is crucial. DVBPR[11] extends VBPR by jointly training CNN visual encoder with the Matrix Factorization task. AlignRec[18] proposes pre-training the visual-text alignment task using a mask-then-predict strategy, with BEiT3[29] as the backbone, then align with ID representation in collaborative filtering task based on fixed multimodal representations. Sheng et al[25] proposes semantic-aware contrastive learning in the pre-training phase, utilizing user's search query and subsequent purchase item to form positive sample pairs, then extracting features using SimTier and MAKE based on fixed multimodal representations. In our methods, we step further by aligning multimodal representation with downstream business-specific item-item relationships in pre-training, and leveraging quantitative code mechanisms for end-to-end training in recommendation model.\nQuantitative multi-modal representation for recommendation Discrete quantization representations facilitate the precise approximation of a vector by decomposing it into multiple discrete code representations. This approach has found extensive applications across various domains [1, 15, 16, 31]. Product Quantization (PQ) is used to compress high dimensional vectors by dividing them into subvectors and independently quantizing each subvector[8]. Residual Quantization (RQ) is a generalization of Product Quantization which focuses on quantizing the residuals left after the previous quantization [7, 20]. This method aims to improve the accuracy of the quantized representation.\nIn recommender systems, content discrete representations are also widely used [10, 23, 26]. Using content discrete representations also known as semantic IDs, facilitates collisions between semantically related items, thereby enhancing generalization in recommender models. TIGER[23] employed RQ-VAE [12] to discretize the content embedding presentations of an item. Subsequently, an autoregressive model is utilized to predict the semantic ID of the next item that user interested. Singh et al. [26] illustrated that hierarchical Semantic IDs can replace item IDs in ranking models and achieve superior generalization outcomes."}, {"title": "6 CONCLUSION", "content": "In this paper, we present a novel approach to injecting multi-modal information to recommendation model, QARM. Different from the common deployment paradigm which utilizes fixed unlearnable pre-trained MLLM representation, our QARM utilizes the quantitative code of down-streaming task alignment fine-tuned MLLM representations to reach an end-to-end MLLM information training. Specifically, in the item alignment mechanism, we first mine a group of high-quality down-streaming task item-item pairs, and then utilize them to guide the MLLM fine-tuning. For the quantitative code mechanism, we devise two heuristic VQ and RQ code methods to quantify those representations to construct the user-side, item-side and target item-aware features to achieve end-to-end optimization for better convergence. Empirically experimental results on Kuaishou's advertising and shopping scenarios demonstrate the effectiveness of QARM on multi-modal information fusion. Besides, detailed analyses show that our QARM is more friendly for cold-start and long-tailed items, which is expected for multi-modal information usage. Our QARM has been deployed at Kuaishou to support various services from 2024 March, serving 400 Million users every day."}]}