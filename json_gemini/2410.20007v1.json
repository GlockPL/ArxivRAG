{"title": "Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models", "authors": ["Danqing Wang", "Zhuorui Ye", "Fei Fang", "Lei Li"], "abstract": "Enhancing the reasoning capabilities of large language models (LLMs) is crucial for enabling them to tackle complex, multi-step problems. Multi-agent frameworks have shown great potential in enhancing LLMs' reasoning capabilities. However, the lack of effective cooperation between LLM agents hinders their performance, especially for multi-step reasoning tasks. This paper proposes a novel cooperative multi-agent reasoning framework (CoPlanner) by separating reasoning steps and assigning distinct duties to different agents. CoPlanner consists of two LLM agents: a planning agent and a reasoning agent. The planning agent provides high-level strategic hints, while the reasoning agent follows these hints and infers answers. By training the planning agent's policy through the interactive reasoning process via Proximal Policy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the previous best method by 9.94% on LogiQA and 3.09% on BBH. Our results demonstrate that the guidance from the planning agent and the effective cooperation between the agents contribute to the superior performance of CoPlanner in tackling multi-step reasoning problems.", "sections": [{"title": "1 Introduction", "content": "Recently, research has shown that multiple LLM-based agents can enhance the capability of a single LLM through communication, especially in solving reasoning problems (Du et al., 2023; Hao et al., 2023; Zhu et al., 2023). However, when faced with complex reasoning tasks involving multiple reasoning steps, it remains challenging for these agents to find the correct answer.\nThe main reason lies in the fact that these agents attempt to directly solve the complex problem independently and communicate with others after obtaining a solution, such as in LLM debate (Du et al., 2023) or ChatEval (Chan et al., 2024). This setting still poses the challenge of problem-solving to a single agent and even increases the difficulty by requiring the single agent to evaluate others' intricate solutions. Recent studies have found that these multi-agent frameworks do not exhibit significant advantages over elaborated instructions or demonstrations (Huang et al., 2023; Wang et al., 2024b). To leverage the benefits of the multi-agent system, we should allow each agent to focus on their respective strengths and attempt to solve only a part of the problem at a time. Based on this approach, they can collaborate by leveraging their individual capabilities and collectively solve the entire problem in a step-by-step manner.\nIn this paper, we introduce a cooperative multi-agent reasoning framework (CoPlanner) to enhance reasoning capabilities. CoPlanner consists of two agents: a planning agent that provides high-level ideas for problem-solving, and a reasoning agent that focuses on following the plan to perform reasoning. Instead of attempting to solve the problem in a single step, these two agents interact with each other over several rounds, with each round focusing on a specific part of the overall problem. This separation of responsibilities and reasoning steps allows the reasoning agent to concentrate on correctly interpreting and following the immediate one-step instruction provided by the planning agent, without needing to handle the higher-level planning and decomposition of the overall problem.\nAs illustrated in Figure 1, for each round, the planning agent generates a concrete hint for one-step reasoning by selecting a generic meta-strategy from a pre-defined pool and extending it to a problem-specific hint. The meta-strategy pool includes several common problem-solving methodologies such as deduction, induction, reflection, etc. The reasoning agent takes this hint and implements the step-wise instruction. The planning agent then considers the implementation results and provides the next hint."}, {"title": "2 Related Work", "content": "Reasoning in Language Models Large Language Models (LLMs) have demonstrated impressive logical reasoning and problem-solving capabilities. Approaches like Chain-of-Thought (CoT) (Wang et al., 2022) enhance reasoning by prompting LLMs to generate step-by-step solutions, while Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (Besta et al., 2024) provide more structured reasoning processes. Aggregating multiple LLM responses can further improve performance (Wang et al., 2022). Recently, researchers have explored using Monte Carlo Tree Search (MCTS) to find the most promising reasoning paths (Liu et al., 2023b; Wang et al., 2024c; Hao et al., 2023).\nMulti-agent in LLMs To enhance the capability of solving complicated problems, a branch of work has investigated multi-agent cooperation among LLMs. One approach trains a verifier or reward model to evaluate the reasoning steps generated by a separate LLM (Zhu et al., 2023; Lightman et al., 2023). Other studies incorporate an auxiliary LLM to provide natural language feedback, helping the main LLM reflect on and correct mistakes (Wang and Li, 2023; Aky\u00fcrek et al., 2023; Wang et al., 2024a). Another line of work introduces multiple LLMs that debate with each other to improve reasoning (Du et al., 2023; Liang et al., 2023; Yin et al., 2023) or obtain better evaluations (Chern et al., 2024; Chan et al., 2024). Multi-agent frameworks have also been explored in domains such as games (Xu et al., 2023b,a), software development (Qian et al., 2023), and real-world simulations (Hua et al., 2023). In this paper, we propose a novel multi-agent reasoning framework that disentangles the responsibilities of planning and implementation for complex tasks. By assigning dedicated agents for high-level planning and focused reasoning, our framework aims to leverage the complementary strengths of different agents and facilitate effective cooperation through structured interactions."}, {"title": "3 Cooperative Reasoning Framework", "content": "We begin by formally defining the multi-step reasoning task within the context of our multi-agent framework and provide an overview of our proposed CoPlanner. We then delve into the specific roles and responsibilities of the two key agents in CoPlanner, namely the reasoning agent and the planning agent. Finally, we describe the training methodology employed to facilitate effective cooperation between these agents and enhance their collective reasoning capabilities."}, {"title": "3.1 Overview", "content": "We introduce a multi-agent framework for solving complex reasoning tasks step by step. For a given query x, this framework lets several LLM-based agents discuss several rounds and thus get a solution for this task.\nSpecifically, we introduce two agents in CoPlanner: Planning agent provides high-level hints {ao,\u2026\u2026, at} while Reasoning Agent follows the plan to propose the step-wise solution {yo,, Yt}. t \u2208 [0,\u2026\u2026,T] indicates the t-th round and yt is the final answer.\nWe define the planning agent as a Markov decision process (MDP) (S, A, P, R). The state st \u2208 S includes the given query x and the historical thoughts of the reasoning agents {yo,\u2026\u2026, Yt}. The action at \u2208 A is a hint from several LLM-generated hints based on a pre-defined meta-strategy pool C. P describes the transition (st, at) \u2192 St+1, which depends on the generation of the reasoning agent after giving the new hint at. The reward is calculated based on the correctness of the final answer yt. If it is correct, the reward is 1 otherwise 0."}, {"title": "3.2 Planning Agent for Strategic Planning", "content": "The goal of the planning agent is to choose the best hint for the reasoning agent to help it get the correct answer. It includes three modules: a value network ve(st) to approximate the reward of the current state, a policy network po(ct|st) to select the best meta-strategy to explore, and a language model LM(st, c) to provide the concrete hint at. For each round, the planning agent will\n1. Receive the thoughts yt-1 from the reasoning agent and update the current state st.\n2. Select the best meta-strategy ct from the pre-defined meta-strategy pool C based on the current state: po(ct|St).\n3. Generate the concrete hint based on the meta-strategy: at = LM(st, Ct) and send the hint to the reasoning agent.\n4. If the maximum round is reached, terminate the interaction by asking the reasoning agent to return the answer.\nMeta-strategy Pool We pre-define a set of 10 meta-strategies based on human cognition. It includes five common logical reasoning types (Wason and Johnson-Laird, 1972): deduction, induction, abduction, analogy, contradiction, and four problem-solving methods: decomposition, enumeration, elimination, and reflection. We also add a meta-strategy finish to indicate the end of the reasoning. The detailed instructions are listed in Appendix A.1. These high-level strategies make the hints more diverse and thus enhance the exploration of the solutions."}, {"title": "3.3 Reasoning Agent for Problem-solving", "content": "The goal of the reasoning agent is to provide a stepwise reasoning path {y\u00ba,\u2026\u2026\u2026, YT} for the given query. It follows the hint from the planning agent and conducts detailed reasoning to get results. It relies on an instruction-tuned language model to generate language response yt = LM(st, at). Specifically, the reasoning agent will\n1. Receive the hints at-1 from the planning agent.\n2. Conduct one-step reasoning based on the given hint: yt = LM(st, at).\n3. Output the final answer based on previous thoughts once receive the finish signal."}, {"title": "3.4 Update Planning Policy via Interaction", "content": "We use reinforcement learning to help the planning agent learn the policy of selecting the best hints from the candidate pools. Specifically, we use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to train the policy network. It is an actor-critic method that introduces a value network vo(st) to estimate the expected future rewards and updates the policy network po(c|st) to maximize towards the estimated reward.\nAs demonstrated in Figure 2, we use a frozen LLM to get the embedding of the observation st and the candidate actions in the meta-strategy pool. The embedding is based on the mean pooling over the tokens' hidden states from the last layer. We use the concatenated embedding as the input of the value and policy network. The value network is composed of one transformer layer (Vaswani et al., 2017) and a linear layer for reward prediction. The policy network uses one transformer layer and outputs the attention weight between the observation and the action as the actions' probabilities. We sample from the action probability distribution during the training and use the best action during the inference. After obtaining the meta-strategy, we use the frozen LLM to generate a concrete hint.\nDuring training, we set up the interaction between the reasoning and the planning agent as described above. After the reasoning agent provides its final answer, we compute the reward Rt by comparing the answer with the ground truth. We assign reward = 1 if the answer is correct and -1 otherwise. We use the observed reward Rt and the current state st to update the value network ve(st) via temporal-difference learning. We compute the advantage estimate At using the value network's predictions and the observed rewards and update the policy network po(Ct|st) by the PPO objective function:\n$J_{PPO}(\\theta) = E_t[min(r_t(\\theta) A_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t)]$,\n$r_t(\\theta) = \\frac{p_{\\theta}(c_t|s_t)}{p_{\\theta_{old}}(c_t|s_t)}$.\nHere \\u03f5 is a hyperparameter that enforces the new policy $p_{\\theta}$ remains close to the old policy $p_{\\theta_{old}}$\nInitialization via Behavior Cloning We collect successful trajectories to initialize the policy network via behavior cloning. Specifically, for each round, the policy agent randomly selects one meta-strategy and generates the hints for the reasoning agent, while the reasoning agent follows the given hints to infer the answer. We collect the interaction between two agents, and sample 32 times for each example in the training set. We evaluate CoPlanner with all data. w/o PPO removes the PPO training and use the behavior cloning policy Poo. w/o BC removes the initialization and train PPO from scratch. We list the performance degradation percentage in Table 2. We can see that BC policy has the greatest impact on LogiQA while on BBH its benefit is limited. This may be because the BC policy on LogiQA is much higher than BBH (0.776 v.s. 0.605), providing the PPO with a better initialization. Additionally, difficulty-aware filtering contributes a lot to PPO training by stabilizing the reward signal and making the training process more efficient. Without the filtering, the PPO will perform worse than the BC policy."}, {"title": "4 Experiment", "content": "We use two multi-choice reasoning benchmarks that require multiple reasoning steps. The reasoning agent is asked to return the answer in JSON format. We calculate the accuracy based on the exact match between the predicted answer and the ground truth. LogiQA (Liu et al., 2021, 2023a) is a multi-choice understanding benchmark for logical reasoning. We follow the standard training/validation split and only keep examples with more than 3 reasoning categories. This makes the problem more diverse and difficult to solve. This results in 1517 training examples and 203 test examples. We take the validation set as the test set and randomly select 200 examples from the training set for validation. BBH (Suzgun et al., 2022) is a set of hard problems borrowed from Big Bench (Srivastava et al., 2022). They are also formatted as multi-choice problems. We pick the English tasks with more than 2 options, resulting in 16 tasks\u00b9. For each task, we randomly select 200 examples as the test set and 20 examples as the validation. The rest are used as training examples."}, {"title": "4.2 Implementation Details", "content": "We use Mistral 7B Instruct-v0.2 (Jiang et al., 2023) and LLaMA 3 8B Instruct (Touvron et al., 2023; Jiang et al., 2023) as our backbone models. We use a fine-tuned mistral embedding model (Wang et al., 2023) to get the representation. The hidden size of the transformer layer is 64 and one attention head is used in the policy and value network. We collect 88,441 and 17,200 state-action pairs for LogiQA and BBH. The data are randomly split by 9:1 for training and validation. We use the learning rate le - 4 and 16 batch size for behavior cloning and train the police network for 10k steps. The 10-category classification accuracy on the validation set is 0.776 on LogiQA and 0.605 on BBH. We keep problems with di \u2208 [5%, 90%] difficulty filtering. For PPO training, we set the \\u03f5 = 0.1. The batch size is 32, and the training epoch of PPO is 10. After initializing with the behavior policy Poo we first freeze the policy network for 1k training steps to train the value network. We then use the same learning rate 5e 4 for the policy and value network with a linear learning decay. The total environment step is 5k. We use one A6000 GPU for training and two A6000 GPUs for LLM inference. The maximum round is set to 2 in the main experiments. More details can be found in Appendix A.3."}, {"title": "4.3 Baselines", "content": "We add several baselines such as the Direct, Few-shot, chain-of-thought (CoT) prompting (Wei et al., 2022). For multi-agent settings, we use different approaches as the planning agent. Random Policy randomly picks a meta-strategy from the meta-"}, {"title": "4.4 Main Results", "content": "We present our main results in Table 1. The LLaMA-3-8B-based CoPlanner outperforms all other baselines on both the LogiQA and BBH benchmarks. The Mistral-7B-based CoPlanner demonstrates comparable performance to the Tree of Thought (ToT) policy on BBH but achieves better results on LogiQA. However, the ToT policy requires significantly more time during inference compared to CoPlanner. It evaluates each candidate hint based on the hint itself and the reasoning result based on that hint, resulting in a runtime three times longer than CoPlanner, even with batch generation. Furthermore, the quality of the selected hint highly relies on self-evaluation, making its performance unstable across different LLMs. These results provide empirical evidence of the effectiveness of our CoPlanner, while also highlighting several key findings."}, {"title": "4.5 Analysis", "content": "To further investigate CoPlanner's performance, we conduct more experiments from several aspects. Unless otherwise stated, experiments are based on Mistral 7B with 2 rounds and 5k training steps, initialized with behavior cloning."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel cooperative multi-agent framework (CoPlanner) to enhance large language models with strategic planning for complex reasoning tasks. CoPlanner decomposes the problem into high-level planning and focused reasoning, assigning these responsibilities to distinct agents. The planning agent learns an effective hint selection policy through PPO on its interactions with the reasoning agent during training. The behavior cloning and the difficulty-aware curriculum filtering make the training more stable and efficient. Extensive experiments on LogiQA and BBH benchmarks demonstrate the effectiveness of CoPlanner. By leveraging the cooperation between multiple"}, {"title": "Limitation", "content": "Although CoPlanner demonstrates a novel and effective way to enhance LLM reasoning by cooperation, there are still several limitations. First, this paper mainly focuses on reasoning tasks, and CoPlanner's performance on other tasks such as math problems or real-world planning is also interesting. Besides, due to the computation limitation, we focus on 7B models and do not scale up the PPO training to a large scale. It would be interesting to see how the scaling laws of training time and model size affect CoPlanner's performance."}, {"title": "Ethics Statement", "content": "We acknowledge that there might be some ethical considerations in enhancing LLMs' reasoning capability such as the CoPlanner presented in this paper. However, we believe that no one must be specifically highlighted here."}, {"title": "A Appendix", "content": "We list the instructions used for each meta-strategy:\n\u2022 Decomposition: Decompose the problem or the preceding step into easier-to-solve parts.\n\u2022 Enumeration: Enumerate all potential candidates in the context of the given conditions and find the most promising one.\n\u2022 Elimination: Eliminate options that are incorrect or have a very low possibility of being correct.\n\u2022 Reflection: Review previous results and verify whether these results are correct. If not, find the error and correct it.\n\u2022 Finish: Please return the selected option in JSON format.\n\u2022 Deductive Reasoning: Draw a conclusion based on general truths, principles, given premises, or rules of inference.\n\u2022 Inductive Reasoning: Start from a set of individual instances and generalize to arrive at a general conclusion.\n\u2022 Abductive Reasoning: Make an educated guess based on the known information and verify this guess.\n\u2022 Analogical Reasoning: Start from information about one system and infer information about another system based on the similarity between the two systems.\n\u2022 Contradiction: Demonstrate that a statement is false by assuming it's true and then showing this leads to an impossible or absurd outcome."}, {"title": "A.2 Prompts", "content": "Tree-of-thought Baseline We list the prompts for evaluating the hint quality:\n\u2022 Rationality: Evaluate whether the current hint is a reasonable instruction to solve the problem. 1 is unreasonable, 3 is reasonable, and 2 is unsure. Return \"The score is x\", where x is an integer from 1 to 3.\n\u2022 Relevant: Evaluate whether the current hint is relevant to the input problem. 1 is irrelevant, 3 is relevant, and 2 is unsure. Return \"The score is x\", where x is an integer from 1 to 3.\n\u2022 Clarity: Evaluate whether the current hint is easy to understand and follow. 1 is difficult to understand and follow, 3 is easy to understand and follow, and 2 is unsure. Return \"The score is x\", where x is an integer from 1 to 3.\""}, {"title": "A.3 Training Details", "content": "In our PPO implementation, the hyperparameters for RL training are listed in Table 6. Training speed is around 4 hours for 5000 timesteps on one A6000 GPU. We randomly pick 100 data on LogiQA and use all data on BBH after the difficulty filtering for PPO training. Note that all training data is used during the behavior cloning. We plot the training curve in Figure 5 and 6. We use the accuracy of the training data as the y-axis."}]}