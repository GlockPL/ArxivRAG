{"title": "ComMer: a Framework for Compressing and Merging User Data for Personalization", "authors": ["Yoel Zeldes", "Amir Zait", "Ilia Labzovsky", "Danny Karmon", "Efrat Farkash"], "abstract": "Large Language Models (LLMs) excel at a wide range of tasks, but adapting them to new data, particularly for personalized applications, poses significant challenges due to resource and computational constraints. Existing methods either rely on exposing fresh data to the model through the prompt, which is limited by context size and computationally expensive at inference time, or fine-tuning, which incurs substantial training and update costs. In this paper, we introduce ComMer - Compress and Merge - a novel framework that efficiently personalizes LLMs by compressing users' documents into compact representations, which are then merged and fed into a frozen LLM. We evaluate ComMer on two types of personalization tasks - personalized skill learning, using the tweet paraphrasing dataset and the personalized news headline generation dataset from the LaMP benchmark, and knowledge-intensive, using the PerLTQA dataset. Our experiments demonstrate that in constrained inference budget scenarios ComMer achieves superior quality in skill learning tasks, while highlighting limitations in knowledge-intensive settings due to the loss of detailed information. These results offer insights into trade-offs and potential optimizations in multi-document compression for personalization.", "sections": [{"title": "1. Introduction", "content": "LLMs have demonstrated remarkable capabilities across a wide range of natural language processing tasks. In many tasks, a simple prompt suffices to guide the model toward the desired behavior, while leveraging any required knowledge acquired during its pretraining (Brown et al., 2020). However, in some cases, such as personalizing LLMs to users' data, new data becomes available only after the model's initial training. Adapting LLMs to accommodate this new data introduces additional resource requirements and computational overhead, which is significant when scaling to a large number of users.\n\nThe two prevalent approaches for adapting LLMs to new data, as illustrated in Figure 1, are (1) prompt engineering, where new data is incorporated into the model through the prompt (Salemi & Zamani, 2024), and (2) full or parameter-efficient fine-tuning (PEFT) (Wang et al., 2024). The first approach faces limitations due to the model's supported context window. While recent advancements allow LLMs to process millions of tokens (Team et al., 2024a), this becomes problematic as the number and length of documents increase particularly in data-rich modalities like video and audio. Even when the context window is sufficient, LLMs often exhibit positional bias, processing documents differently depending on their placement within the prompt, which can affect output quality (Liu et al., 2024b). Additionally, regardless of quality, both latency and computational costs scale with the length of the prompt (Duman Keles et al., 2023), making this approach less feasible for large-scale applications.\n\nIn the second approach, the LLM is adapted to new data by modifying its weights or a subset of them to incorporate the information from the new data. While this addresses the limitations of prompt engineering, it introduces new challenges. Specifically, training a separate model for each user leads to substantial computational costs. Furthermore, updating personalized models as new user data becomes available is a resource-intensive and complex process, often necessitating offline updates that compromise the model's freshness. Finally, training personalized models for users with limited data can result in low-quality models (Zhuang et al., 2024).\n\nIn this paper, we propose ComMer - Compress and Merge - a novel approach designed to address these challenges by learning to compress user data into a compact representation that efficiently adapts the LLM. The cost of training ComMer is amortized across all users, eliminating the need for individualized training processes. This approach offers"}, {"title": "2. Related Work", "content": "Personalizing LLMs spans a spectrum from treating user data as a knowledge base to personalized skill learning. (Du et al., 2024) introduced PerLTQA, focusing on knowledge base personalization by combining synthetic semantic and episodic memories for Q&A tasks. In contrast, (Salemi et al., 2024b) explored personalized skill learning, and introduced the LaMP benchmark consisting of seven personalized tasks. Different methods have been devised to tackle the personalization challenge: (Salemi & Zamani, 2024) evaluated Retrieval-Augmented Generation (RAG) and PEFT on the LaMP benchmark; (Salemi et al., 2024a) optimized RAG pipelines for retrieval; and (Zhuang et al., 2024) proposed HYDRA, a model factorization framework with user-specific rerankers and adapters. All these methods incur high inference or training costs.\n\nConcurrent to our work, (Liu et al., 2024a) introduced PPlug, which compresses users' documents into pluggable prompts using a frozen encoder. Their approach relies on a query-dependent weighted average of document embeddings, which adds computational and storage overhead since it cannot be precomputed. Furthermore, PPlug's frozen encoder and small embedding dimension limit its capacity, causing embedding interference. In contrast, ComMer uses trainable embeddings with mean pooling, enabling it to co-embed multiple documents in the same latent space while mitigating interference."}, {"title": "2.2. Efficiency and Compression", "content": "Despite advances in context window sizes (Team et al., 2024a), processing long contexts remains resource-intensive. Compression methods include latent representations (Mu et al., 2023; Chevalier et al., 2023; Ge et al., 2024) and natural language compressions (Zhou et al., 2023; Jiang et al., 2023; 2024). Our approach extends latent representation methods to the multi-document setting by merging document representations, yielding further compression gains."}, {"title": "2.3. Model Merging", "content": "Merging models enhances their collective performance across tasks, with methods ranging from simple weight averaging (Wortsman et al., 2022; Li et al., 2022) to sophisticated techniques like spherical linear interpolation (Goddard et al., 2024) and sparse-aware merging (Yu et al., 2024). Some works focus on merging entire models, while others address merging PEFT models (Chitale et al., 2023; Wu et al., 2024; Lin et al., 2024; Ostapenko et al., 2024; Shah et al., 2025). Unlike these approaches, ComMer merges document representations rather than weights, enabling dy-"}, {"title": "3. Method", "content": "Figure 2 illustrates our proposed architecture. Given a collection of user documents, the process follows three steps:\n\n1. Compression: Each document is independently compressed into a soft prompt. Similarly to (Ge et al., 2024), the compression is implemented using a frozen LLM with trainable compression embeddings and LORA weights (Hu et al., 2022). Specifically:\n\n(a) Trainable compression embeddings are appended to the input document.\n\n(b) The document, along with the compression embeddings, is passed through a frozen LLM. Leveraging the language understanding of the frozen LLM, which is a required capability of a good document compressor, can reduce training costs. A trainable LoRA adapter is added to adjust the LLM for compression purposes.\n\n(c) The last layer's representations of the compression embeddings are extracted as the compressor's output.\n\n2. Merging: The compressed representations of all documents are aggregated into a single soft prompt using mean pooling. This approach offers several advantages:\n\n(a) The shape of the resulting representation is independent of the number of documents, addressing limitations related to the context window and computational cost.\n\n(b) The process is order-agnostic, eliminating the bias related to the position of documents.\n\n(c) The aggregated soft prompt can be precomputed and updated efficiently as new documents are added.\n\n3. Response Generation: The aggregated soft prompt is plugged into a frozen LLM to generate the desired output."}, {"title": "3.1. Architecture", "content": ""}, {"title": "3.2. Training", "content": "We assume a dataset of the form  $\\{ \\{doc_j\\}_{j=1..N_i}, x_i, y_i \\}_{i=1..N}$ where N is the number of examples, $x_i$ is an instruction, and $y_i$ is the expected response based on the personalized information found in the collection of $N_i$ documents $\\{doc_j\\}^i_{j=1..N_i}$. Each example is processed by our proposed architecture by independently"}, {"title": "4. Experimental Setup", "content": "ComMer represents a collection of documents using mean pooling, which makes it an efficient method for handling personalization tasks where user data evolves over time. When a new document is added, only that document needs to be processed, after which it is incorporated into the aggregated compression of the existing document set (with normalization applied to maintain the mean pool's structure). This eliminates the need to reprocess older documents, leading to significant cost savings.\n\nWe evaluate ComMer on three such tasks, with examples from each shown in Appendix A.2."}, {"title": "4.1. Data", "content": ""}, {"title": "4.1.1. PERSONALIZED SKILL LEARNING", "content": "In these tasks, the user's documents provide a signal that is essential for personalizing task performance. To succeed, ComMer must learn to extract the personalization signal from a given set of documents. We chose to experiment with two tasks from the LaMP benchmark: (1) personalized tweet paraphrasing, where the objective is to rephrase a tweet to align with the user's style, as inferred from their past tweets; and (2) personalized news headline generation, where the objective is to generate a headline for a given article that reflects the author's style, as inferred from their past article-headline pairs. We follow the user-based split described in LaMP (Salemi et al., 2024b), where the test set includes users who were not seen during training."}, {"title": "4.1.2. KNOWLEDGE INTENSIVE", "content": "In these tasks, the documents serve as a personalized knowledge base for each user. To perform well, ComMer must learn to retain the information from these documents, sometimes even verbatim. We chose to experiment with a dataset that is derived from PerLTQA (Du et al., 2024), a QA dataset based on synthetic personal memories. For each synthetic persona, their memories, and associated questions and answers, we construct examples by randomly selecting a subset of memories, along with a randomly chosen question and answer. 27 personas are allocated to the training set and 5 to the test set. For simplicity, we overload the name PerLTQA to refer to this processed version of the dataset."}, {"title": "4.1.3. PRETRAINING DATA", "content": "Additionally, following (Ge et al., 2024), we pretrain ComMer using an unsupervised task. We extend the auto-encoding task to handle the multi-document setup. In the single-document setup, the model is tasked to reconstruct the text from a compressed document. For the multi-document case, we prefix the i-th document with \"Document i:\", and use \"What does document n contain?\" as a query, where n is randomly selected. We utilize documents from the FineWeb dataset (Penedo et al., 2024), truncating each one to 150 characters. Multiple documents are then grouped to form a single example. We construct a training set containing one million examples. Unless stated otherwise, all models in this research underwent a pretraining phase of one epoch."}, {"title": "4.2. Models", "content": "We conduct experiments using Gemma-2b (Team et al., 2024b) as the backbone for both the generator and the compressor, optimizing resource efficiency by sharing weights across the modules. The input to the generator is:\n\nconcat$\\,(\\,$merge$(\\,\\{\\, \\text{compress}(\\text{doc}_{i}) \\,\\}_{i=1...n})\\, , \\text{embed}(x) )\\,)$\n\nwhere embed is the output of the backbone's embedding layer. We use mean pool as the merge operation and experiment with varying the number of compression embeddings. As a baseline, we use prompt-tuning (Lester et al., 2021):\n\nconcat(soft_prompt, embed(concat(doc1, ..., docn, x)))"}, {"title": "4.3. Metrics", "content": "The LaMP benchmark relies on ROUGE-based metrics (Lin, 2004) for both the tweet paraphrasing and personalized news headline generation tasks. Our experiments reveal that these metrics exhibit significant noise, which complicates drawing coherent conclusions. Consequently, we incorporate perplexity alongside ROUGE-L in our analysis. Our conclusions are supported by the perplexity results, as well as the ROUGE-L results, albeit with greater variability. Similarly, for the PerLTQA benchmark, we employ both ROUGE-L and perplexity metrics."}, {"title": "5. Results", "content": ""}, {"title": "5.1. Personalized skill learning", "content": "We trained several ComMer and prompt-tuning models, varying the number of documents (0, 1, 2, 4, 8 for the tweet paraphrasing task, and 0, 1, 2, 4 for the personalized news headline generation task) and the number of embeddings (4, 8, 16, 32, 64, 128). Figure 3 illustrates the trade-off between prompt length, which is associated with costs, and model quality. We note that the number of embeddings influences"}, {"title": "5.2. Knowledge intensive", "content": "In knowledge-intensive tasks, the situation differs significantly, as shown in Figure 5. In these tasks, having only a single document is the optimal scenario, as additional documents introduce non-relevant information. In typical RAG applications, several documents are retrieved - much like our experimental setup - with the expectation that the LLM can interpret them effectively. As observed, prompt-tuning is robust to irrelevant information, whereas ComMer is not. Unlike in the personalized skill learning setting, the inclusion of more documents leads to worse perplexity and ROUGE-L results for ComMer."}, {"title": "5.3. Pretraining", "content": "As demonstrated in prior work, pretraining can enhance compression performance (Ge et al., 2024). In this study, we aim to quantify its impact on ComMer and explore the effect of different pretraining datasets on the personalized tweet paraphrasing task. Specifically, in addition to pretraining on the FineWeb dataset, we experiment with pretraining on the training set of the personalized tweet paraphrasing task, where we apply the extended auto-encoding task to users' past tweets. We further investigate pretraining on FineWeb followed by pretraining on the personalized tweet paraphrasing dataset, as well as a no-pretraining baseline. Table 1 shows that pretraining improves downstream performance, measured by perplexity. Interestingly, the choice of"}, {"title": "5.4. Generalization of number of documents", "content": "In all previous experiments, we used the same number of documents at both training and test time. However, in realistic scenarios, users at test time may provide a different number of documents than what the model was trained on. While ComMer could be trained to handle a varying number of documents during training, it is valuable to explore how well ComMer generalizes to an out-of-distribution number of documents. Figure 6 illustrates the perplexity2 differences when the number of test documents differs from the training configuration, using 4 compression embeddings, demonstrated using the personalized tweet paraphrasing task. Specifically, let $perplexity_{ij}$ represent the perplexity of a model trained with j documents when tested with i documents. The (i, j) cell contains $perplexity_{ij} - perplexity_{jj}$. Interestingly, ComMer improves when exposed to more documents than it was trained on, while performance degrades when fewer documents are provided. The most intriguing case is $perplexity_{i1}$, where only a single document was used during training. In this scenario, the compressor was not trained in conjunction with the mean pool operation. Nevertheless, averaging the compressions at test time still enhances performance, suggesting that deep neural networks inherently operate linearly in the latent space, consistent with prior research findings (Elhage et al., 2022; Bricken et al., 2023; Park et al., 2024)."}, {"title": "5.5. Concatenation", "content": "Our choice of mean pooling as the merging operation is motivated by the need to optimize two types of costs:\n\n1. Token processing costs: The costs associated with the number of tokens the frozen backbone model needs to process when generating the response.\n\n2. Update costs: The computational overhead involved in updating the representation of a set of documents when a new document is added.\n\nHowever, optimizing these costs can come on the expense of quality. To evaluate this trade-off, we explore two alternative approaches:\n\n1. Concatenating the document compressions: Instead of using mean pool, we concatenate the compressions. This approach increases the token processing costs, while keeping the update costs intact (compared to vanilla ComMer), assuming storage overhead is negligible.\n2. Compressing the concatenation of documents: This approach increases the update costs, while keeping the token processing costs intact."}, {"title": "6. Discussion", "content": "In this work, we introduced ComMer, a novel and cost-effective approach for compressing multiple documents. We evaluated how our method compares to an existing popular approach - prompt-tuning - that exposes LLMs to document content directly. We demonstrated that compressing merging is beneficial in personalized skill learning tasks, but less effective in knowledge-intensive tasks.\n\nThe difference between the two task types intuitively makes sense: in the personalized skill learning tasks, a single document typically does not encapsulate all the information required to learn a user's style. Instead, each document sug-"}, {"title": "7. Impact Statement", "content": "LLMs have become remarkably capable in recent years. Personalizing them to users' needs and data is one of the next big challenges in making these models more accessible and helpful. ComMer facilitates the efficient personalization of LLMs to user data, which can have several positive societal consequences:\n\n\u2022 Increased accessibility: It enables wider access to personalized LLMs, particularly for users with limited data, for whom training a personalized model may yield low quality due to a small training set.\n\n\u2022 Reduced environmental impact: The higher efficiency of ComMer translates to lower energy consumption, contributing to a reduced carbon footprint.\n\nHowever, it's equally important to acknowledge the potential ethical concerns and negative societal consequences:\n\n\u2022 Bias: If the user data used to personalize the LLM contains biases, ComMer might preserve these biases, leading to discriminatory or unfair outputs. Future work should aim to compress data in a way that captures only relevant and non-discriminatory signals.\n\n\u2022 Privacy: Training ComMer on real user data raises privacy concerns, especially if the data contains sensitive personal information. It is essential to ensure robust anonymization and security measures are in place or that high-quality synthetic data is used.\n\n\u2022 Data compliance: Models personalized to user data should comply with data privacy and data security laws and standards.\n\nIn this paper we do not aim to address these challenges. However, addressing them is crucial for the responsible development and deployment of techniques like ComMer."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Implementation details", "content": "Data For the personalized tweet paraphrasing and the personalized news headline generation tasks, we employ BM25 to retrieve relevant documents, following the methodology used by LaMP. To ensure computational efficiency, we exclude examples exceeding 2900 characters, including those in the PerLTQA task.\n\nModel We utilize the Hugging Face implementation of gemma-2b-it and the peft package for integrating LORA. The LoRA configuration includes a rank of 64, alpha of 16, and LoRA-dropout of 0.1.\n\nOptimization For training we employ a cosine learning rate scheduler with the first 3% of steps allocated for warm-up. The maximum learning rate for prompt-tuning is $3 \\times 10^{-2}$. For ComMer, the maximum learning rates is $1 \\times 10^{-4}$ for the LORA weights and $1 \\times 10^{-2}$ for the compression embeddings. We observed that using a homogeneous learning rate is sub-optimal. We use AdamW optimizer with a weight decay of 0.001 and a maximum gradient norm (for gradient clipping) of 0.3. Batch sizes are set to 16 for LaMP tasks and 1 for PerLTQA. Training proceeds for 20 epochs for LaMP tasks and 1 epoch of 80K examples for PerLTQA. We early stop according to perplexity based on a small reserved validation set of 1024 examples.\n\nEvaluation For ease of research, we use LaMP's official development set as the test set due to the unavailability of test set labels. For evaluation, we use greedy decoding to compute ROUGE metrics."}, {"title": "A.2. Dataset examples", "content": "Following are examples from each of the datasets used in this study, when setting the number of documents to two."}]}