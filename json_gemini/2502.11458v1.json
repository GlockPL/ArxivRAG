{"title": "Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models", "authors": ["Jiecheng Zhou", "Ding Tang", "Rong Fu", "Boni Hu", "Haoran Xu", "Yi Wang", "Zhilin Pei", "Zhongling Su", "Liang Liu", "Xingcheng Zhang", "Weiming Zhang"], "abstract": "The burgeoning computational demands for training large language models (LLMs) necessitate efficient methods, including quantized training, which leverages low-bit arithmetic operations to reduce costs. While FP8 precision has shown potential, leveraging FP4 remains challenging due to inherent quantization errors and limited representation capability. Based on the Transformer architecture, we present an FP4 training scheme for LLMs, overcoming these obstacles through mixed-precision quantization strategies tailed for different modules and training stages. This allows us to apply the precision level suitable to distinct components within the model, ensuring that multi-head attention and linear layers are handled appropriately. Our pretraining recipe ensures stability in backpropagation by incorporating fine-grained quantization methods with a target precision training schedule. Experimental results demonstrate that our FP4 training scheme achieves accuracy comparable to BF16 and FP8, with smaller theoretical computational cost. With the advent of next-generation hardware supporting FP4, our method sets the foundation for efficient ultra-low precision training.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models, including GPT (Radford, 2018; Floridi and Chiriatti, 2020; Achiam et al., 2023), DeepSeek (Liu et al., 2024), Llama (Touvron et al., 2023) and OPT (Zhang et al., 2023), have demonstrated strong generalization capabilities across various tasks (Stiennon et al., 2020; Alexey, 2020). Among these advancements, pretraining on large-scale unlabeled data has proven to be critical for ensuring model performance (Radford, 2018; Dong et al., 2019). Increasing the model size and the dataset scale can enhance performance (Kaplan et al., 2020; Hoffmann et al., 2022), but this improvement comes with significant computational costs. To address this, numerous methods have been proposed to accelerate the pretraining process (Duan et al., 2024).Particularly, low-precision computation serves as an efficient acceleration technique. This approach quantizes the inputs of computationally intensive operators to a specified low-bit width, leveraging low-bit arithmetic units to speed up training.\nPrevious research on low-precision training has primarily focused on deep learning models. However, these methods do not fully consider the characteristics of large language model pretraining, which has unique training methods and model architectures. The good news is that next-generation hardware will support FP4 and FP8 format (Nvidia). Studies like (Peng et al., 2023; Micikevicius et al., 2022a; NVIDIA; Fishman et al., 2024; Xi et al.) have demonstrated the capability of 8-bit computation for LLM pretraining. However, the application of FP4 tensor cores in LLM pre-training remains unexplored. Compared to INT4, FP4 offers a larger numerical representation space, making it possible to further reduce the bit width in large-scale model pretraining. However, the limited number of bits in FP4 format introduces significant quantization errors, making the application of FP4 to pretraining highly challenging.\nFrom the perspective of LLM structure, it has been observed that different modules and computational components exhibit varying levels of sensitivity. Given the critical role of the multi-head attention (MHA) mechanism in the Transformer architecture, it is imperative to implement specific strategies to ensure the accuracy of attention modules. As the volume of data continues to grow, the gradient values tend to decrease, making FP4 quantization more prone to underflow, thus hindering parameter updates. Based on these observations, we propose a novel FP4 mixed-precision large language model pretraining recipe. Specifically, we leverage a per-block quantization strategy and employ different quantization approaches across modules and training stages to enable FP4 model training. The approaches enable better exploitation of the computational improvements brought by future hardware advancements.\nIn this paper, we explored the use of FP4 precision in language model pretraining and, for the very first time, proposed an effective mixed precious pretraining strategy. First, considering the distinct requirements of different modules, we applied tailored quantization strategies to preserve the precision of MHA execution. Second, as back-propagation has shown high sensitivity to precision, we employed finer-grained quantization methods to ensure accurate parameter updates during the backward pass. Finally, we adopted a 2-stage target precision training schedule to eliminate the impact of quantization noise on the model."}, {"title": "2 Related Work", "content": "Low-precision training enhances deep learning efficiency by reducing computational costs. Many existing studies focus on the training of deep neural networks (DNNs) (Wang et al., 2018; Chmiel et al., 2023; Sun et al., 2019; Xi et al., 2023; Fu et al., 2021), whose architecture and performance differ from LLM pre-training. In the context of low-precision training for large model pre-training, some progress has been made in FP8. For example, (Micikevicius et al., 2022a) introduced new FP8 floating-point formats (E4M3 and E5M2), and (Fishman et al., 2024) extends FP8 to trillion-token large-scale model pretraining. In terms of FP4 training, (Wang et al., 2025) improved FP4 computational precision using a differentiable quantization estimator and outlier clamping and compensation strategy. However, most existing methods fail to fully account for the varying sensitivity to precision across different model modules."}, {"title": "3 Methods", "content": "Our objective is to maximize the efficiency of low-precision computations based on the characteristics of LLMs. As shown in Fig.1(a), the computational cost of three key components in a transformer model is analyzed, with FFN accounting for 57%. Considering both the computational cost and impact on performance, we meticulously design three corresponding training schemes: 3.1 Attention-protected Neighbor Linear, 3.2 Gradient-sensitive Linear, and 3.3 Target Precious Training Schedule. These schemes fully leverage hardware acceleration while keeping precision loss within an acceptable range during training."}, {"title": "3.1 Attention-protected Neighbor Linear", "content": "As the core component of the Transformer model, the attention mechanism is highly sensitive to precision. Quantization errors introduced by low-precision training can accumulate over time, eventually disrupting the function of the attention mechanism. As shown in Fig. 1(c), an undisturbed attention mechanism identifies tokens 0, 3, 6, and 9 as more important. However, under FP4 training, the result becomes nearly uniformly distributed, preventing the model from distinguishing which tokens are significant. This makes it difficult for the model to differentiate the importance of tokens, thereby affecting the convergence speed.\nTo ensure the proper functioning of the attention mechanism and enable the model to correctly evaluate the importance of each token, we employ FP8 precision for the computation of QKV and the output projection to \"protect\" the accurate execution of the attention mechanism, as shown in Fig. 1(d)."}, {"title": "3.2 Gradient-sensitive FFN Linear", "content": "Weight gradient computation is more sensitive to errors compared to forward computation, due to the fact that both gradients and activations contribute to the error. For gradients, since many values are around 0.02, especially as training progresses and gradient magnitudes decrease, underflow is likely to occur. As can be seen in the left of Fig.1(b), there is an 8.6% difference between FP4 and FP8/FP16, thus requiring a more accurate representation. For activations, we observe that underflow occurs approximately 18% of the time between FP4 and FP8/FP16. This is largely due to the relatively large range of values, as can be seen in the right of Fig. 1(b). Therefore, a more accurate representation is also needed. Additionally, optimizers use the gradients to update model parameters. Based on the above discussion, for the weight gradient computation of model weights, we adopt FP8 precious computation, as shown in the bottom left corner of Fig.1(e).\nFurthermore, for the activation gradient computation (the top right corner of Fig.1(e)), we find that quantizing gradients significantly impacts the convergence of model training. There is always a nonlinear operation between the linear layers, which requires more precise numerical representations. Furthermore, quantization errors accumulate iteratively through the chain rule during backpropagation, ultimately hindering the convergence of model training.\nLastly, in our experiments, we observed that quantization noise increases as the model size and the amount of data grow (a detailed explanation can be found in Appendix B). This occurs because, when the model reaches a certain level of accuracy, coarse-grained low-precision tensors can no longer currently represent the parameter space and input information. Therefore, we adopt a more conservative quantization approach to maintain stable training in forward computation. As shown in the top left corner of Fig.1(e). To ensure efficient hardware implementation, we use per-block quantization strategies where the block size is set to 128."}, {"title": "3.3 Target Precious Training Schedule", "content": "When using low-precision training throughout the entire process, there tends to be a performance gap between the low-precision model and the FP16 model, as shown in Fig.2. The validation loss curves exhibit a parallel trend. Although the gap between the two curves is very small, the difference in downstream tasks, such as wikitext perplexity (PPL), can be more pronounced, reaching up to approximately 6.3 compared to the model trained with FP16. This is likely due to compromises the model makes to adapt to the noise introduced by quantization during low-precision training. To address this issue, we employ a Target Precious Training Schedule, which involves two stages: continuing the FP4 pretraining process with FP16 for a short period. This accounts for only 5% to 10% of the total training steps, allowing the model to return to an ideal state."}, {"title": "4 Experiment", "content": "In this section, we evaluate the proposed FP4 training method across language models of various sizes. The detailed model training configurations and hyperparameter settings are provided in Appendix B. Section 4.1 presents the main results, showcasing the model's performance on downstream tasks."}, {"title": "4.1 Main Result", "content": "We validate the proposed FP4 pretraining method on two large language models, using the widely adopted GPT-2 and LLaMA architectures. The GPT-2 and LLaMA models are pretrained on the RedPajama-WikiText (Weber et al., 2025) dataset within the Megatron framework and evaluate their text generation capabilities on wikiText(Merity et al., 2016). Additionally, we assess their natural language understanding abilities on the GLUE(Wang, 2018) benchmark.\nWe train approximately 10B tokens on GPT-2-small and GPT-2-mid and around 25B tokens on GPT-2-large. The final validation loss and validation perplexity (PPL) are presented in Table 1, showing that the pretraining results obtained with our method exhibit almost no performance difference compared to models trained using FP16. In addition to training loss, the downstream task performance of the same pretrained models demonstrates that the average accuracy of FP4-trained models is comparable to that of FP16-trained models."}, {"title": "4.2 Ablation Study", "content": "We aim to investigate the effect of the module-wise pretraining method introduced in Section 3. For this ablation study, we train the LLaMA2-125M model on approximately 5B tokens. The results in the table indicate that different modules exhibit varying levels of robustness to low precision. Additionally, we compute the theoretical computation cost for these methods and observe that our approach achieves a lower theoretical computation cost (see Appendix B for details) while maintaining higher performance."}, {"title": "5 Conclusion", "content": "We propose an FP4 pre-training scheme for modern large language models. Based on the sensitivity analysis of computational modules and training stages, we adopt a tailored training recipe according to the position of linear modules, together with a Target Precious Training schedule, to ensure stable convergence. Experimental results show that FP4-based training achieves comparable validation loss and downstream task accuracy to traditional FP16 training while reducing computational costs by 30%. Additionally, our ablation studies confirm the importance of adaptive quantization strategies across different model modules and training stages, providing new insights for the further development of low-precision training techniques and efficient training of large language models on next-generation hardware."}, {"title": "6 Limitation", "content": "First, due to computational resource limitations, our method has not been validated on larger models and larger datasets to demonstrate its effectiveness. Investigating such scalability remains a critical direction for future research. In addition, since the model adopts a simulated FP4 approach, it is unable to obtain an accurate increase in training efficiency. Lastly, for the sensitive computational components, we employed a simple strategy to ensure numerical precision. In future work, we will explore more customized approaches to enable a broader range of computations to utilize FP4."}, {"title": "A FP4 Quantization", "content": "Quantization is the process of converting a data type with more bits (e.g., 32- or 16-bit floating points) into another data type with fewer bits (e.g., 4-bit floating points). In integer quantization, the real-valued variable $X_R$ is quantized to an integer $X_{INT}$ with the following formula:\n\n$X_{INT} = \\alpha\\ \\text{Clip}\\left(\\frac{X_R}{\\alpha}, Q_{\\min}, Q_{\\max}\\right)$\n\nSimilar to integer quantization, in float point quantization, scaling and clipping of the values are required before quantization, as follows.\n\n$Q_{\\max} -Q_{\\min} = (2-2^{-m})2^{2-b-1}$\n\n$Q_{\\max} = \\tilde{Q}Q_{\\max}$\n\n$X'_R = \\text{Clip} (X_R, Q_{\\min}, Q_{\\max})$\n\nWhere the min/max value range of signed floating-point quantization can be calculated from Eq. (2), and the scaling factor $\\alpha$ determines the quantization granularity. Thus, we can determine the upper and lower bounds of floating-point quantization and perform the clip operation accordingly.\nAfter scaling and clipping, we can quantize the real value into a specific data format. Unlike INT quantization, floating-point numbers have different quantization levels for different values. Therefore, we first need to determine the quantization step size:\n\n$\\upsilon = \\frac{\\alpha}{\\tilde{a}} = 2^{-b} \\cdot a$\n\n$\\tilde{a} = \\begin{cases}\n2^{1-\\text{m}} & \\text{if } [\\log_2|X'_R|/\\tilde{a}] \\geq 1\\\\\n2^{1-\\text{m}} & \\text{otherwise}\n\\end{cases}$\n\nHere, the quantization level $v$ is determined by $X'_R/\\tilde{a}$, after which the floating-point number can be quantized following the format of Eq. (8). A detailed explanation can be found in (Micikevicius et al., 2022b; Liu et al., 2023). Finally, the quantization formula can be expressed as follows:\n\n$X_{FP} = \\tilde{a}\\cdot v\\cdot \\text{Clip}\\left(\\frac{X_R}{\\alpha v}\\right)$"}, {"title": "B Training Detail", "content": "We conducted our experiments on the Megatron (Shoeybi et al., 2019) framework and evaluated downstream performance using the transformers (Wolf et al., 2020) and Im-evaluation-harness (Gao et al., 2024), ensuring standardized and reproducible benchmarking. Hyperparameters remain consistent across precision settings for fair comparison. The learning rate follows a warm-up and cosine decay schedule, with the warm-up phase spanning 0.15% of total steps and the learning rate gradually decreasing to 10% of its peak over the remaining 90%. The peak learning rate is $1 \\times 10^{-4}$, with a weight decay of 0.1 for Llama models. For GPT models, the peak learning rate is set to $6 \\times 10^{-4}$, with a weight decay of 0.1 For the Adam optimizer, we use $\\beta_1 = 0.9, \\beta_2 = 0.95$, and $\\epsilon = 1 \\times 10^{-8}$. For llama models, the input sequences are fixed at 2048 tokens, and the batch size is 512, comprising approximately 1M tokens. For GPT models, the input sequences are fixed at 1024 tokens, and the batch size is 480, comprising approximately 0.5M tokens, detail model config can be found in Table 4.\n\nWe quantize all the linear layers in the MLP and attention module to target precious, and leave multi-head attention and activation function in FP16 by employing FlashAttention (Dao et al., 2022). The master copy of the weights is kept in FP32. We quantize linear layers' inputs to target precious prior to each matmul, but leave layernorm's weight and bias to floating-point since they are relatively small.\nFor the calculation of theoretical computation cost, we first separately count the forward and backward computation amounts for each part of a Transformer block (considering only computations related to matrix multiplications, as these account for more than 95% of the total computation). Then, based on the assumptions that FP8 achieves twice the computation speed of FP16 and FP4 achieves four times the computation speed of FP16, we compute the theoretical time required for each matrix multiplication. Finally, we obtain the theoretical computation cost for each method.\nSince the quantized weight w is an estimate of w, We directly use a straight-through estimator (Bengio et al., 2013) directly passes the gradient of w to w:\n\n$\\nabla_w L(\\hat{w}) \\leftarrow \\nabla_{\\hat{w}} L(w)$.\n\nIn our experiments, we found that different models have varying precision requirements. For the GPT-125M model, applying a per-token and per-channel FP4 quantization strategy for both forward computation and weight gradient computation is feasible. The final results are shown in Table 1. The use of per-token and per-channel quantization is designed to better align with matrix multiplication rules, allowing for efficient implementation on accelerators. However, for the GPT-335M model, the per-token and per-channel FP4 quantization strategy becomes ineffective as the data volume increases. In this case, switching to per-block FP4 quantization for weight gradient computation enables training to proceed, with the final results also presented in Table 1. For the GPT-770M model, the quantization strategy used for GPT-335M becomes ineffective as training progresses. At this point, modifying the forward computation to use per-block FP4 quantization while increasing the precision of weight gradient computation to FP8 ensures stable training. Additionally, we validated the feasibility of the GPT-770M quantization strategy (as we discuss in Section 3) on the LLaMA-125M and LLaMA-1B models. It can be anticipated that as model size and data volume continue to grow, the precision requirements for model training will become increasingly stringent(Kumar et al., 2024). Ensuring that FP4 can support long-term, large-scale pretraining of large models remains a key direction for our future work."}]}