{"title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics", "authors": ["Minttu Alakuijala", "Reginald McLean", "Isaac Woungang", "Nariman Farsad", "Samuel Kaski", "Pekka Marttinen", "Kai Yuan"], "abstract": "Natural language is often the easiest and most convenient modality for humans to specify tasks for robots. However, learning to ground language to behavior typically requires impractical amounts of diverse, language-annotated demonstrations collected on each target robot. In this work, we aim to separate the problem of what to accomplish from how to accomplish it, as the former can benefit from substantial amounts of external observation-only data, and only the latter depends on a specific robot embodiment. To this end, we propose Video-Language Critic, a reward model that can be trained on readily available cross-embodiment data using contrastive learning and a temporal ranking objective, and use it to score behavior traces from a separate reinforcement learning actor. When trained on Open X-Embodiment data, our reward model enables 2x more sample-efficient policy training on Meta-World tasks than a sparse reward only, despite a significant domain gap. Using in-domain data but in a challenging task generalization setting on Meta-World, we further demonstrate more sample-efficient training than is possible with prior language-conditioned reward models that are either trained with binary classification, use static images, or do not leverage the temporal information present in video data.", "sections": [{"title": "1 Introduction", "content": "Advances in natural language processing and vision-language representations have enabled a significant increase in the scalability and generalization abilities of learned control policies for robotics. Methods involving large architectures, such as Transformers [42], and internet-scale pretraining have transferred well to both high-level [17, 43] and low-level [5, 21, 34] robotic control. Natural language has many desirable features as a modality for specifying tasks. Unlike structured, hand-designed task sets, natural language is unrestricted and open-domain. Moreover, prompts can be specified as precisely or vaguely as appropriate. While goal images, demonstration videos, or goal states more broadly, have been considered as an alternative open-domain task definition modality [2, 7, 23], they typically have to specify irrelevant environment details, such as the background. Furthermore, language readily supports task definitions with novel combinations of actions, objects and their attributes (e.g. pick up the large green ball) as well as subtask sequencing (e.g. place the toy inside the box, then close the box) in a way that facilitates the policy's understanding of unseen tasks.\nThe majority of prior work has proposed to learn language-conditioned policies end-to-end, i.e., directly predicting an action in the robot's action space given the current state and task description. However, this has several downsides: first, fitting large models on the full problem requires a significant amount of high-quality demonstration data from the target robot domain. Second, the resulting policy depends entirely on the specific robot instance, observation space, and controller type (e.g., joint or task space control) and does not easily transfer to other settings. Moreover, much of the prior work addresses vision-language grounding in robotics purely with imitation learning [5, 21, 27, 34], without attempting to discriminate between low-quality and expert demonstrations. As a result, the resulting policies are inherently limited by the skills of the demonstrators, and no novel solutions can be discovered through planning or interaction. This line of work overlooks performance gains that could be obtained by converting the language prompts to a scalar reward function. Manually defining a well-specified dense reward function to communicate task success is typically laborious and error-prone, and must be repeated for each task. To make progress towards a general-purpose robotic system that can learn human-level skills both in terms of quality (dexterity, robustness) and capability (variety of skills), we argue these systems will need to be able to critique their own behavior, by learning reward functions at scale.\nWe address this problem by learning a foundation video-language-conditioned reward model, i.e., a critic that evaluates the progress (in the form of a video) of a task, given as a human-language instruction, and assigns a reward based on how close the robot is to completing the task. By leveraging large cross-task pretraining data, which may come from a variety of robots, our Video-Language Critic (VLC), can learn to score the alignment between a textual description and task execution regardless of the specific robot embodiment. Our experimental evaluation on Meta-World [46] manipulation tasks shows that VLC can learn useful general-purpose reward functions not only from in-domain, but also out-of-domain data (Open X-Embodiment [27]) collected from different robot embodiments. In Section 4, we show that VLC 1) accelerates the training of a wide range of manipulation tasks and 2) enables zero-shot learning on unseen tasks, when combined with a sparse task completion signal.\nRecent work in defining language-conditioned rewards for robotic manipulation has used either binary classification [25, 33, 35], contrastive vision-language alignment [22, 26, 38] or reconstruction [15]"}, {"title": "2 Related Work", "content": "Vision-language imitation Many prior works have aimed to connect language instructions and vision-based observations in robotics [5, 12, 20, 21, 34] and in video games [10], mostly through large-scale demonstrations [5, 10, 20, 21] or pretraining [12, 34]. However, the majority of approaches have considered imitation-based objectives only, without ranking existing trajectories using offline reinforcement learning (RL) or attempting to outperform prior data using model-based or model-free RL. This has the obvious downside of requiring large amounts of high-quality data from the target domain with low-quality examples already filtered out, and limits the performance of the agent to predicting the mean of the demonstrated behavior instead of improving over it. We instead propose to learn a state-value function from cross-domain offline behavior, which can be optimized using either online, offline or model-based policy training.\nMulti-modal representations Pretrained vision-language representations [28] have been adapted to a wide range of downstream tasks [12, 34]. Shridhar et al. [34] propose to augment pretrained CLIP [28] with Transporter nets [48] to handle fine-grained spatial awareness required for precise manipulation. Xiao et al. [44] train a CLIP-like contrastive embedding space from crowd-sourced language annotations for trajectories from the robot. We draw inspiration from these works, but instead define an embodiment-agnostic, language-conditioned reward function, which supports improvement over demonstration data.\nVideo retrieval Our work is closely related to video retrieval as we seek to move beyond image-language correspondence and match task descriptions with history-aware state representations. As the task of learning representations across time is computationally expensive, many prior works have proposed to start from pretrained, CLIP-like static image features and aggregate them over time, while fine-tuning the aggregation function's weights on video retrieval [3, 18, 19]. Unlike in video retrieval, we aim to not only assign high alignment scores to full videos, but provide smoothly increasing reward over the whole video to indicate task progress.\nInverse RL Several works have proposed to infer the reward function of a task using examples of expert behavior, and to train an RL policy to optimize this reward [30]. Most relevantly to our setting, a line of prior inverse RL methods considers the case where the observed behavior is not annotated with actions and may come from different action and observation spaces altogether, typically a human demonstrator [2, 7, 23, 25, 31\u201333, 35, 47]. Many of these works use either a goal image [2, 23, 47] or a demonstration video [7, 32] rather than language conditioning, and some are only applicable for data from a single task at a time [31, 47]. Moreover, handling multi-task reward learning with an additional task identifier state variable, as done by Chen et al. [7], requires a predefined grouping into a discrete set of tasks. By contrast, our use of language to define tasks enables a more subtle and composable task space."}, {"title": "3 Video-Language Critic", "content": "We propose to learn language-conditioned robotic manipulation by first training an embodiment-agnostic reward function on video-caption pairs, and then using the learned reward model to guide the autonomous training of a robot-specific policy. To serve as a useful reward signal for downstream policy learning, the learned function should accurately represent the intended task, while providing enough signal to the agent to enable efficient learning [1, 36, 37, 39]. It should exhibit at least two key properties: accuracy and temporal smoothness. Making progress in the specified task should be rewarded with positive feedback with as little delay as possible, i.e., the function should smoothly increase over a successful execution. In fact, the problem of optimal reward shaping is equivalent to learning the value function for the optimal policy [39], suggesting that an optimal densely shaped reward should monotonously increase over a successful demonstration (assuming the reward we ultimately wish to maximize corresponds to sparse goal reaching). Moreover, the end-of-episode scores for successful trajectories should exceed those of incomplete or failed executions: classification accuracy between successes and failures should be high. With these desiderata, we formulate Video-Language Critic, a language-conditioned reward model trained with cross-entropy and sequential ranking objectives to encourage progressively increasing scores over a successful video's duration."}, {"title": "3.1 Contrastive video-language training", "content": "Our approach is motivated by the success of contrastive image-language pretraining and the wide applicability of pretrained CLIP [28] encoders as foundation models. The problem of comparing observed behavior to a desired task description is analogous to the setting of CLIP; however, we extend the contrastive learning approach to scoring videos. Compared to a single image, using sequences of frames sampled across the full trajectory increases the generality of our reward function, and could allow it to handle non-Markovian (i.e., history-dependent) tasks. Such tasks might involve partial observability, repetitive or circular movements, or be described relative to an earlier state; even simple object displacement tasks may fall in this category.\nReward model architecture We define video and text encoder networks similar to CLIP4Clip used for video retrieval [19], the task of finding videos within an existing dataset that most closely match a given textual query. The general architecture is shown in Fig. 1. First, each video frame is processed with an image encoder network while the video caption is processed with a text encoder, both initialized with CLIP in order to benefit from its large-scale vision-language pretraining. Luo et al. [19] tested different aggregation strategies for reasoning over the resulting sequence of image features. In video retrieval, averaging image features over time was found to be sufficient, and no performance benefit could be obtained with an attention-based aggregation. While video retrieval shares similarities with our setting, task progress evaluation requires a much more nuanced understanding of temporal dynamics: for one, reversing the video should typically result in a very different reward value."}, {"title": "4 Experiments", "content": "We demonstrate the accuracy and effectiveness of the learned video-language rewards with experimental evaluation on simulated robotic manipulation tasks from the Meta-World benchmark [46]. We evaluate VLC's ability to inform successful policy training in three settings of increasing difficulty. First, we assess the ability of our model to jointly represent several robotic tasks with a single language-conditioned prediction network in Section 4.1. Second, we test our models' ability to generalize to unseen Meta-World tasks with the help of vision-language pretraining as well as extrapolation from training tasks in Section 4.2. In Section 4.3, we demonstrate our method's out-of-domain transfer ability: VLC is used to learn an embodiment-agnostic reward function for any language-conditioned manipulation task by observing a variety of robot actors from Open X-Embodiment [27], a large dataset collected from a variety of real-world robots in different environments. We further report comparisons to prior work, both quantitatively and qualitatively, in Section 4.4. Finally, we demonstrate VLC's effectiveness in planning with a known dynamical model in Section 4.5."}, {"title": "4.1 Multi-task reward function", "content": "To validate VLC's effectiveness as a multi-task reward function, we first train our model on video data from all 50 tasks. We collect 40 video demonstrations per task for a total video dataset of 2000 successful executions. We further collect 1600 failure examples by replacing the demonstrator's actions with random actions with probability 0.7, and refer to this joint dataset as MW50 (short for Meta-World). We do not make any modifications to the data generating process to explicitly encourage exploration, as we want to validate our method in the context of existing offline data, which typically does not cover the full state space. A key challenge VLC needs to overcome is to sufficiently generalize from the successes and failures present in the data to evaluate out-of-distribution trajectories, as the RL policy may act very differently from the demonstration data.\nThe policy training results are shown in Table 1, with learning curves for the hard task set in Fig. 2. To summarize learning speed with a single number, we report success rate of the policy evaluated at the training length at which the manually specified Meta-World reward solves the task to \u226598% success. VLC trained on MW50 enables improved sample efficiency relative to the sparse reward only, which demonstrates that VLC can sufficiently generalize to trajectories not seen in demonstration data, and can effectively represent task progress for multiple tasks at once. However, a few tasks, such as Handle Press, are learned in so few trials even with sparse reward alone that there is little room for improvement in reward design, and learning is instead bottlenecked by the policy training's sample efficiency. This is why the biggest gains are obtained for the harder tasks."}, {"title": "4.2 Task generalization to unseen environments", "content": "Having verified VLC can effectively represent several tasks using language conditioning, we further evaluate its ability to use language to generalize to entirely unseen tasks. The goal is to see whether knowledge of training tasks, which in part contain similar actions or objects as the held-out tasks"}, {"title": "4.3 Embodiment generalisation to unseen domains", "content": "The advantage of our method, and pretraining a reward function in general, is that no data collection on the target robot and in the target environment is required. To demonstrate this, we train VLC on cross-embodiment data from Open X-Embodiment [27]. We use the language-annotated subset, with a total of 698,000 episodes of diverse tasks filmed in various real-world robotic labs. Although some of this data does feature the Sawyer robot used in Meta-World simulations, this is only a marginal subset of 0.33% of the language-annotated videos. Moreover, the domain gap remains significant due to real-world variations in objects, backgrounds, lighting conditions, task instances and instruction formats, as well as the embodiment gap between the simulated and the real robots.\nWe successfully train policies (see Table 1 and Fig. 2) using Open X trained models despite a significant domain gap, highlighting the generalizability of large-scale vision-language training. We obtain an average 2.1x sample efficiency gain relative to the sparse reward in tasks that are solved by both rewards (sample efficiency is ill-defined if either does not solve the task), with particularly large improvements in Handle Pull Side (7x), Reach Wall (7x) and Slide Plate (6x) and an average 5 percentage point success rate increase across all 25 tasks despite misrepresenting a few tasks. Note that unlike RT-X [27], our method does not use action labels, and remains equally applicable on observation-only data."}, {"title": "4.4 Comparison to prior work", "content": "To validate the benefits of VLC, we compare its performance to prior language-conditioned reward models LOREL [25], RoboCLIP [38], LIV [22], Voltron [15] and R3M [26], each fine-tuned on MW40. Training and implementation details are included in Appendix C.\nWe find VLC's combination of cross-entropy and the sequential ranking objective as well as full video conditioning to produce more informative reward predictions than existing methods, as shown by faster policy training on average in Table 2 and Fig. 3. Moreover, on qualitative inspection of the shape of the predicted rewards (Fig. 4), we find VLC's outputs to better distinguish successes from failures compared to either LOREL or RoboCLIP, the two strongest baselines. Thanks to its broad coverage of execution history and the sequential loss term, VLC produces rewards that more smoothly increase over time for successful executions than either prior method."}, {"title": "4.5 Model-based evaluation", "content": "As a pretrained reward function, VLC can also be very useful in the context of model-based planning: any imagined future state can be scored, allowing the planner to identify the best action plan. We demonstrate this in a proof-of-concept experiment, where we do not learn the model but instead assume access to a known transition model as well as action primitives. The action primitives include grasping and reaching, parameterized by target positions (such as the locations of objects detected in the scene), and are defined using segments from the expert policies available in Meta-World. We evaluate VLC's ability to identify the action primitive with the correct execution for a held-out task.\nIn each task, we compare one successful trajectory with 5 unsuccessful ones with randomly sampled target positions. Two action primitives are required to solve Assembly and Pick Out of Hole: first grasping the required object, then moving it as specified, such as lifting it out of the hole. The action primitive is selected with $\\arg \\max_{a} S_{\\theta}(o(s_{t+1}), c)$ where $a$ is the action primitive that transitions state $s_t$ to $s_{t+1}$, and $o$ is a function describing the observation visible to $S_{\\theta}$, in this case a video."}, {"title": "5 Limitations", "content": "To learn reward functions from cross-embodiment data, a shared input representation, typically images or video, is required. This limits the application to tasks whose progress can be evaluated from vision;"}, {"title": "A.1 VLMBench dataset", "content": "We use the VLMBench manipulation task suite to develop and validate our method without any Meta-World specific tuning. For this purpose, we collect 2700 video demonstrations and 1600 failure cases from variations of the picking task \u2013 covering different object shapes, sizes, colors and relative positions, as well as distractor objects. The natural language instructions match this diversity in task variants, such as Grasp the cylinder or Grasp the cyan object, and require distinguishing relevant objects from distractors with either absolute (color, shape) or relative (size, position) properties, such as the the larger or the front object. For more details on the benchmark, see Zheng et al. [49].\nWe use these VLMBench videos to validate VLC design decisions, but defining a single informative metric on the dataset of video-caption pairs $(v^i, c^i)$, $i = 1, ..., N$, is difficult. Test loss, video retrieval metrics such as mean recall, or classification metrics such as area under the ROC curve do not correspond well to the models' ability to model task progress. The main difficulty is that part of the caption-to-video matching task can be solved by simply connecting objects referred to in the caption to objects present in the scene, without considering temporal information or actual task success.\nTo support informative evaluation of our models, we therefore further define a set of 19 test episodes: in each test case, the same initialization of the scene is used to generate alternative trajectories that grasp at different objects in the scene, only one of which solves the correct task. The accuracy over this set of videos is our main model selection metric of interest, i.e., in how many out of 19 instances does the model assign a higher score to the successful video than any incorrect video from the same initialization. Out of evaluation metrics available at training time, we find video-to-text cross-entropy to correlate the most with this test-time accuracy, and so use this metric on a set of validation trajectories to choose model checkpoints."}, {"title": "A.2 Ablation results", "content": "We compare two temporal aggregation methods as proposed by Luo et al. [19]: the sequence transformer and the tight-type transformer. The sequence transformer aggregates the sequence of image features $[ViT(v_1), ViT(v_2), ..., ViT(v_T)]$ into a single embedding vector $S_\\theta(v_{1:T})$, which it then compares to the caption embedding TextEnc(c) with cosine similarity. The tight-type transformer, on the other hand, includes the caption embedding as an additional input to the temporal aggregator $S_\\theta (v_{1:T}, c)$, as shown in Fig. 1.\nWe report the results of our ablation study in Table 4. In addition to the choice of architecture, we observe performance gains from adding image augmentations from the Albumentations library [6], by sampling frames randomly from uniform intervals instead of deterministic uniform sampling, the addition of the sequence ranking term, as well as considering failure examples only as negatives in the contrastive objective, and report results using those settings in Section 4."}, {"title": "B.1 Reward training", "content": "We subsample the videos to 12 time steps. Capping the maximum video length is a practical choice both in terms of learning ability and computational cost. We keep the default value of 12 frames in CLIP4Clip, though we change these to be linearly sampled from across the entire video. Informed by the findings of our ablation studies in Section A, at training time, we additionally apply image augmentations and randomize frame sampling. We set $\\alpha$, the ranking loss weight, to 33 based on accuracy on VLMBench test episodes.\nReward training on Meta-World videos took 2 hours for MW50 on a single NVIDIA A100 GPU, and 1 hour for MW40 on a GeForce RTX 3090 GPU. Training on the significantly larger Open X-Embodiment dataset took 256 hours (nearly 11 days) on a single A100. However, we believe this"}, {"title": "B.2 Policy training", "content": "For RL training experiments, we adapt the SAC implementation of CleanRL [14]. Policy evaluation is done every 20,000 timesteps for 50 episodes. Both the actor and critic networks contain three hidden layers of size 400, and optimization is done using Adam [16]. Other algorithm hyperparameters were kept at the implementation's default values.\nWe experimentally set the relative weights of the VLC and sparse reward components to 1 and 50, respectively, the motivation being that the sparse reward, once obtained, should be able to override the dense intermediate reward predictions. We chose these values after testing three other settings: (0.01, 10), (0.1, 10), and (0.1, 20), which also performed quite well. For sparse reward only experiments, we did not find significant differences in the scale of the reward, but for some tasks using a weight of 50 seemed to perform better than 1 or 10, so we report results using this value for consistency with the VLC experiments.\nTraining length and hence computational cost varies considerably across tasks. We terminate training after convergence to \u226598% success (averaged over the 10 most recent evaluations), or after a maximal training length set per task. The resulting average training length across all 25 tasks was 600k environment steps for VLC MW50 and 750k for the Open X trained model. The corresponding GPU hours vary slightly based on exact architecture used, but we obtained approximately 750k steps in 24 hours on a single NVIDIA V100 or P100 GPU. Our total computational budget was therefore 19.2 GPU hours \u00d7 5 random seeds \u00d7 25 tasks = 2,400 GPU hours for the RL training experiments with VLC MW50 and 24 \u00d7 5 \u00d7 25 = 3,000 hours with VLC Open X. For the MW40 task generalization experiments, the average training length was again 600k for VLC, so the corresponding cost for training on 6 tasks was approximately 580 GPU hours for our method, and slightly more for baselines that took longer to train."}]}