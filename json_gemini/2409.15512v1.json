{"title": "PixelBytes: Catching Unified Embedding for Multimodal Generation", "authors": ["Fabien Furfaro"], "abstract": "This report introduces PixelBytes Embedding, a novel approach for unified multimodal representation learning. Our method captures diverse inputs in a single, cohesive representation, enabling emergent properties for multimodal sequence generation, particularly for text and pixelated images. Inspired by state-of-the-art sequence models such as Image Transformers, PixelCNN, and Mamba-Bytes, PixelBytes aims to address the challenges of integrating different data types. We explore various model architectures, including Recurrent Neural Networks (RNNs), State Space Models (SSMs), and Attention-based models, focusing on bidirectional processing and our innovative PxBy embedding technique. Our experiments, conducted on a specialized PixelBytes Pok\u00e9mon dataset, demonstrate that bidirectional sequence models with PxBy embedding and convolutional layers can generate coherent multimodal sequences. This work contributes to the advancement of integrated AI models capable of understanding and generating multimodal data in a unified manner. Code is available at https://github.com/fabienfrfr/PixelBytes.", "sections": [{"title": "Introduction", "content": "Recent advancements in artificial intelligence have led to increasingly generalist models, not by combining multiple specialized components, but by giving simple tasks to models where emergent properties-complex behaviors that arise from simpler underlying rules appear. This is the case with generative language models like GPT [3]. However, these models are limited by their focus on language alone, failing to capture the full complexity of multimodal understanding [6]. To address this limitation, researchers have explored combining LLMs with other modalities [7]. But this brings us back to the initial problem, as it often results in specialized model combinations without allowing for new emergent properties. We propose \"PixelBytes Embedding\", a novel approach enabling unified training across modalities by capturing diverse inputs in a single, cohesive representation.\nMultimodal sequence generation, which involves the creation of coherent outputs combining various data types such as text, images, and numerical sequences, presents a significant challenge in artificial intelligence [1]. While models like GPT have excelled in text generation [3], there's a growing need for unified approaches that can seamlessly handle varied data types. Building upon these findings, PixelBytes aims to address the challenge of unified text and image generation by proposing a model capable of producing mixed sequences of text and images in a coherent and unified manner. We draw inspiration from state-of-the-art sequence models, including Image Transformers [9], PixelCNN [14], and recent developments in bytes generation by mamba architectures [18].\nOur research explores various architectures including Recurrent Neural Networks (RNNs), State Space Models (SSMs) [4], and Attention-based models, focusing on the effectiveness of bidirectional processing [19], novel embedding techniques (particularly PxBy embedding, which unifies pixel and byte-level representations), the impact of"}, {"title": "Model Architecture", "content": "The PixelBytes architecture is designed to seamlessly integrate multimodal data for unified sequence generation. At its core, the model incorporates two key innovative components: a specialized tokenizer sequence constructor and a unified multimodal embedding technique called PxByEmbed. The tokenizer sequence constructor is engineered to process both pixelated images and text at a byte level, enabling a consistent representation across modalities. This is complemented by PxByEmbed, our novel embedding approach that creates a unified representation for both pixel and byte data in a single, coherent space. This architecture draws inspiration from recent advancements in multimodal learning [1] and efficient sequence modeling [16], allowing PixelBytes to capture intrinsic relationships between visual and textual information effectively."}, {"title": "Dataset Construction", "content": "Image captioning datasets, while combining visual and textual modalities, prove unsuitable for joint text and image generation due to limited text content and difficulties in interpreting pixelated versions of high-resolution images. To address these limitations, we developed a specialized Pok\u00e9mon dataset, offering advantages such as a long-standing franchise history, pixelated designs, rich descriptive text, and over 1000 unique Pok\u00e9mon.\nWe constructed our dataset by web scraping Pok\u00e9mon miniatures and descriptions from Pokepedia using Beautiful Soup [10], maintaining a 2/3 text to 1/3 image ratio. For image quantization, we employed a 55-color palette inspired by the NES, creating tokens representing different color combinations. This approach translates visual information into a format suitable for sequence modeling. The quantization and pixelization process used OpenCV and scikit-image [2, 15]. To ensure balanced representation, we adjusted the number of image and text tokens to 113 indices for all entries, allowing the model to learn equally from both visual and textual information. The final dataset, balancing text and pixelated images, is available on the Hugging Face Datasets Hub for reproducibility at https://huggingface.co/datasets/ffurfaro/PixelBytes-Pokemon."}, {"title": "Multimodal Embedding Algorithm", "content": "At the core of our approach is the PxByEmbed algorithm, which represents mixed sequences of text and pixelated images in a unified manner. This algorithm builds upon existing embedding techniques by incorporating spatial adaptivity, allowing for representation of both textual and visual information. PxByEmbed is designed to address the specific needs of pixel-level image representation and byte-level text encoding. The algorithm uses a learned embedding matrix to map each token (text or image) to a vector space, while maintaining spatial relationships for image tokens. This approach allows our model to handle both modalities within a single framework."}, {"title": "Managing Transitions", "content": "PixelBytes uses a method to handle transitions between text and image tokens during sequence generation. This approach is used in both dataset construction and sequence generation.\nFor dataset construction, we use a 2D input sequence method with a 3x3 context window around each token. We use special tokens to mark transitions between text and images, and add padding to keep context sizes consistent. For text, only previous tokens are included in the context windows. The input_seq_construct function implements this process.\nDuring sequence generation, the process_token function manages transitions. It handles text tokens (bytes) and image tokens (tuples) differently, and takes care of special characters like newlines and tabs. A sequence_clock keeps track of the generation progress. Each new token's context is represented in a 3x3 matrix, which helps maintain coherence when switching between text and images."}, {"title": "Training and Evaluation", "content": "We evaluated three compact model architectures, each with fewer than 100,000 parameters: a Recurrent Neural Network (RNN) using bidirectional Long Short-Term Memory (LSTM) units [11], a Transformer [16], and Mamba, based on the State Space Model (SSM) [4]. These models were adapted to process our dataset of pixel data and bytecode sequences. Training was conducted on Kaggle using dual T4 GPUs, with a batch size of 32, sequence length of 256, stride size of 32, and learning rate of 0.001. We trained for 200 epochs, evaluating performance every 5 epochs.\nThe resulting models are available at https://huggingface.co/ffurfaro/PixelBytes-Pokemon. Additionally, we developed a more specialized model for generation tasks, which is not presented in this paper."}, {"title": "Model Architectures", "content": "Figure 2 shows the training and validation metrics for our three model types over 200 epochs. The State Space Models (SSM) achieved the best scores for loss and accuracy. However, the widening gap between their training and validation curves suggests they may be overfitting, meaning they learned the training data well but struggled to generalize to new examples.\nIn contrast, the LSTMs (referred to as RNNs in our analysis) demonstrated more balanced performance. The closer alignment of their training and validation curves indicates they may generalize better to unseen data. The Transformer model had the lowest performance among the three. This could be due to the absence of positional encoding trick, which is important for Transformers in sequence tasks.\nOverall, these results highlight the strengths and weaknesses of each model type. While SSMs showed strong training performance, their tendency to overfit suggests that additional regularization techniques might be needed."}, {"title": "Generation Evaluation Metrics", "content": "We tested different model types including State Space Models (SSM), Attention models (Att), and Recurrent Neural Networks (RNN) for generating 16 consecutive sequences. We used three metrics to evaluate their performance: Hamming Distance, Cosine Similarity, and BLEU Score."}, {"title": "Results and Discussion", "content": "Our experiments with various model architectures tested the effectiveness of unified text and image generation. Bidirectional RNN models using PixelBytes (PxBy) embedding with convolutional layers generally performed better across our metrics. The PxBy embedding improved the mean BLEU score from 0.750 to 0.777 compared to a center-only approach, indicating its effectiveness in representing multimodal data. This finding aligns with recent work emphasizing the importance of effective multimodal embeddings in text-to-image generation tasks [17].\nBidirectional models showed a slight advantage over unidirectional ones, with BLEU scores of 0.777 and 0.776 respectively. This suggests that bidirectionality may help capture sequential dependencies, consistent with other studies in sequence modeling [12]. Notably, varying state dimensions (32, 64, 128) had minimal impact on performance, with BLEU scores ranging from 0.775 to 0.784. While RNN models showed strong performance in our experiments, State Space Models (SSM) demonstrated rapid convergence during training, suggesting potential for future exploration in multimodal data processing. This is in line with recent advancements in SSM for sequence modeling tasks [4]."}, {"title": "Conclusion and Future Work", "content": "This study on PixelBytes highlights the potential for unified text-image generation, contributing to the growing field of multimodal AI [1]. The performance of bidirectional RNN models with PxBy embedding underscores the importance of capturing both spatial and contextual information in multimodal data. Future work could focus on refining the PxByEmbed algorithm, testing on larger and more diverse datasets, exploring creative applications of multimodal generation, and further optimizing SSM and Attention models. These efforts aim to advance multimodal sequence modeling and generation techniques."}]}