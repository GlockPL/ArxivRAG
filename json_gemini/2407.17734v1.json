{"title": "Cost-effective Instruction Learning for Pathology\nVision and Language Analysis", "authors": ["Kaitao Chen", "Mianxin Liu", "Fang Yan", "Lei Ma", "Xiaoming Shi", "Lilong Wang", "Xiaosong Wang", "Lifeng Zhu", "Zhe Wang", "Mu Zhou", "Shaoting Zhang"], "abstract": "The advent of vision-language models fosters the interactive conversations between\nAI-enabled models and humans. Yet applying these models into clinics must deal\nwith daunting challenges around large-scale training data, financial, and computational\nresources. Here we propose a cost-effective instruction learning framework for conver-\nsational pathology named as CLOVER. CLOVER only trains a lightweight module\nand uses instruction tuning while freezing the parameters of the large language model.\nInstead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for build-\ning generation-based instructions, emphasizing the utility of pathological knowledge\nderived from the Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital pathology. From\ntwo benchmark datasets, our findings reveal the strength of hybrid-form instructions in\nthe visual question-answer in pathology. Extensive results show the cost-effectiveness of\nCLOVER in answering both open-ended and closed-ended questions, where CLOVER\noutperforms strong baselines that possess 37 times more training parameters and use\ninstruction data generated from GPT-4. Through the instruction tuning, CLOVER\nexhibits robustness of few-shot learning in the external clinical dataset. These findings\ndemonstrate that cost-effective modeling of CLOVER could accelerate the adoption of\nrapid conversational applications in the landscape of digital pathology.", "sections": [{"title": "1 Main", "content": "The rise of vision-language model (VLM) opens remarkable opportunities to ana-\nlyze pathological images in a visual question-answering manner [1-3]. This profound\nprogress of multi-modal data integration leverages the power of large language model\n(LLM) on cognition, reasoning, and content generation [4-6]. In essence, LLMs are\nlarge-scale parametric networks trained on vast amounts of data, enabling them to\ngenerate human-like responses and achieve remarkable accuracy. ChatGPT [7] and\nGPT-4 [8] are examples to simulate the conversational interaction between AI-enabled\nmodels and humans. The generated conversational records can be re-used to guide\nthe visual-language model refinement [9-12], opening avenues for downstream tasks\nacross domains [1]. In the landscape of digital pathology, providing in-depth language\ndescriptions of cell morphology, tissue status, and treatment suggestions, equipped\nwith human-like interactions, could enhance tissue understanding, characterization,\nand decision making in various clinical scenarios [13-17].\nEmerging pathological vision-language models (PVLMs) (e.g., LLaVA-Med [18]\nand Quilt-LLaVA [19]) have demonstrated their utility in analyzing medical imaging.\nHowever, it is widely known that building a capable PVLM demands an exces-\nsive training data, human labour, financial, and computational resources [20, 21].\nExtending general-purpose models into pathology-oriented model starts with using\npathological vision-language datasets. These datasets consist of (i) image-text pairs\n(image-caption) dataset and (ii) instruction (image-question-answering) dataset [22-\n25]. The image-text dataset aligns visual and language features [26], providing rich\nsemantic information for pathological image contents [27-29]. Meanwhile, instruc-\ntion dataset is crucial for activating LLMs to complete the visual-language question\nanswering. Yet the process of instruction generation incurs a considerable financial\ncost by using GPT-4 (nearly $9,000) [19]. In addition to the instruction data, model\noptimization also requires a substantial compute support. Directly tuning a billion-\nparameter LLM demands high computational resources and is impractical to achieve\non consumer-grade GPUs. To overcome these bottlenecks, fundamental questions are\nurged to be addressed towards building an effective but low-cost PVLM: (i) How can\nwe come up with a lightweight tuning approach to complement the LLaVA-like tuning\non large training parameters? (ii) How can we generate a useful instruction dataset in\na cost-effective way? (iii) How can we achieve few-shot generalized learning ability of\nPVLM for clinical applications without using a large-scale instruction dataset?"}, {"title": "2 Building Efficient PVLM via CLOVER", "content": "The family of LLMs [1, 8, 30, 31] and visual language pre-training [26, 32] are two\nindispensable building blocks for vision-language foundation models. Visual language\npre-training aligns the encoding space in vision and language. The visual features\nfrom the visual encoder can then be perceived by the LLMs. Examples of multi-modal\nfoundation models include Flamingo [10], BLIP-2 [33], FROMAGE [11], mPLUG-Owl\n[12], and LLaVA [9]. In particular, Flamingo, BLIP-2, and FROMAGe all freeze LLMs"}, {"title": "2.3 CLOVER", "content": "In this study, we propose a cost-effective learning framework for accurate pathology\nvision and language inference named as CLOVER (Fig. 1). To achieve fast domain\ntuning with low training resources, we adopt the BLIP-2 architecture [33] as a visual\nlanguage pre-training using a lightweight trainable query transformer (Q-former), a\nfrozen visual encoder, and a frozen LLM. We leverage the paired pathological image\nand text captures from the Quilt-1M [23] dataset to align the vision and language.\nFor the instruction fine-tuning, we meticulously design prompts tailored for patho-\nlogical question answering and use GPT-3.5 [7] for generating effective instruction\ndataset at low cost, referred as the \"generation-based instructions\". The instructions\ngenerated by our PVLM-orient prompt show strong capabilities in the model tuning.\nAdditionally, we construct instructions by matching template questions with the orig-\ninal text captions to enhance the model's vision understanding ability, referred as the\n\"template-based instructions\". Integrating these two types of instructions gives rise"}, {"title": "3 Results", "content": "We systematically measure CLOVER's visual question answering (VQA) performance\nusing two public benchmark datasets, namely PathVQA [35] and QUILT-VQA [19],\nand one independent clinical data set (see Sec. Datasets). We compare CLOVER's per-\nformance with state-of-the-art (SOTA) PVLMs trained with notable costs to observe\nthe cost-effectiveness of CLOVER.\nFrom Table 1, CLOVER\noutperforms major competing methods, showing a remarkable improvement of 26.13%\nin accuracy at maximum for closed-ended questions. Our model performance is even\napproaching to LLaVA-Med (37 times more model parameters) with a minor accuracy\ndifference of 1.83%. Note that LLaVA and LLaVA-Med achieve their results through\nextensive parameter tuning of LLM and training on a vast instruction dataset gen-\nerated by GPT-4. To illustrate, our method involves training with a frozen LLM,\nadjusting only a small portion of parameters (about 1/37 of LLaVA-Med). Meanwhile,\nCLOVER only involves the use an entry-level GPT-3.5 with a smaller scale of instruc-\ntion dataset (2/3 of LLaVA-Med). As seen in Table 1, our model also shows superior\nperformance improvement in open-ended question scenarios. CLOVER's performance\nis twice that of BLIP-2 and approaches five times that of LLaVA in open-ended ques-\ntion scenarios. Compared with BLIP-2, our model improves in both closed-ended and\nopen-ended question-answering. This key finding suggests that vision-language mod-\nels could evidently benefit from the high-quality pathology-sensitive instruction data\ntowards a low-cost model development.\nWe validate CLOVER's\nzero-shot generalization capability on the QUILT-VQA dataset. From Table 2,\nCLOVER achieves the leading performance in precision and F1-score. Our results are\neven close to LLaVA and LLaVA-Med in terms of recall. Due to the longer answer"}, {"title": "3.2 Qualitative Comparison", "content": "To gain insight into model outputs, we present the representative cases from VQA\nexperiments involving comparisons with LLaVA, LLaVA-Med, BLIP-2, and our\nCLOVER on QUILT-VQA and LLaVA-Med-17K datasets. In Table 3 for results from\nQUILT-VQA, we observe that the output of LLaVA is unrelated to the actual con-\ntent, reflecting general image contents such as \"people\", \"water\", and \"lake\". While"}, {"title": "3.3 External Clinical Data Validation", "content": "To assess the efficiency of CLOVER in real-world applications with minimal human\nlabour, we validate CLOVER's few-shot learning capability under a challenging task\nof cancer detection (i.e., cancer/non-cancer tissue classification). We implement a K-\nshot learning testing scheme, meaning that our model can only use K WSIs (K = 1,\n2) from each class for a model fine-tuning. We evaluate the model performance on the\nexternal validation dataset on two cancer tissues. Fig. 2 details the performance of"}, {"title": "3.4 Ablation Studies", "content": "We conduct ablation studies on assess-\ning the value of instruction data. We compare our instruction data with SOTA\ninstruction datasets using the same model setting using BLIP-2 and use PathVQA\nas the final testing set. In detail, we use the generated 15k generation-based and\n30k template-based instructions as our instruction data. In Table 5, the gains from\nusing our instructions generated by GPT-3.5 based on noisy internet data are higher\ncompared to using instructions generated by GPT-4 based on high-quality data.\nThis finding can be attributed to the developed form of the instructions (generation-\nbased and template-based instructions) and the quality of the instructions (generated\nthrough PVLM-oriented prompts). In particular, our costs are only $8, whereas\nthe estimated API costs of LLaVA-Med are hundreds of times higher than our\nlow-cost approach. When using the FlanT5XL model, our performance on closed-\nended question-answering closely approaches that of the LLaVA-Med-Pathology-IM\n(LM-IM) [18], evidently outperforming LLaVA-Med-Pathology (LM) [18] and LM-\nIM on open-ended question-answering by 2.22% and 3.13% respectively. With the\nVicuna 7B model, our results surpass the LM on both closed-ended and open-ended\nquestion-answering settings.\nWe investigate the\neffectiveness of template-based instructions, aiming to provide the LLM with more\ndirect and comprehensive language guidance. As shown in Table 6, results indicate that\nthe addition of template-based instructions to the original instruction data is effective.\nEspecially in FlanT5XL, there is a remarkable improvement in the open-question\nanswering. We reason that template-based instructions can provide a complete answer\nabout the image caption. Therefore it is possible to compensate for the information"}, {"title": "4 Discussion", "content": "Development of cost-effective pathological vision-language models (PVLMs) is crucial\nto enable timely information retrieval and accelerate clinical decision making. In this\nstudy, we have proposed a framework to train a precise PVLM with a low computa-\ntional resource and financial expenditure. A key contribution is that we have designed\nPVLM-oriented prompts and leveraged the GPT-3.5 to create question-and-answer\ninstruction datasets. These generated instructions, combined with template-based\ninstructions requiring no economical expands, exhibit superior capabilities in aiding\ninstruction-based model tuning and downstream use. Our established CLOVER per-\nforms strongly comparing to advanced models with significantly larger parameters and\nexpansive costs using GPT-4. In addition, CLOVER presents an appealing conversa-\ntional capacity in the real-world data from stomach and intestine cancer tissues. These\nfindings demonstrate CLOVER's potential as a cost-effective multimodal framework\nto support downstream clinical tasks without excessive model training and human\nlabeling efforts.\nComputational pathology is widely known for its high demands on data and com-\nputational resource, especially for building a useful PVLM application. Prior efforts\nwere primarily focused on the use of human-examined clinical data [1, 36]. A key dif-\nferentiation of our study is to build efficient PVLMs based on the public Internet"}, {"title": "5 Methods", "content": "In this section, we offer methodological details of the development of CLOVER. We\nnext introduce the involved datasets and the evaluation schemes of experiment."}, {"title": "5.1 Efficient Instructions Construction at Low-cost in\nCLOVER", "content": "Generating large-scale instructions via GPT-4 incurs significant financial costs. In this\nstudy, we specialize in developing effective domain-specific instruction data genera-\ntion at a low cost. The proposed framework includes (i) generation-based instructions\nwith a specialized prompt for employing GPT-3.5, and (ii) template-based instructions\nwithout any additional financial cost. The construction of generation-based instruc-\ntions involves a generation of question-answer (QA) from the captions using GPT and\na PVLM-oriented prompt, while that of template-based instructions involves match-\ning the captions as answer to a set of pre-designed template questions. Notably,\nthe template-based QA dataset permits a comprehensive understanding of image\ncontextual information, while the generation-based QAs emphasize the pathological\nknowledge distilled from GPT-3.5.\nWe meticulously design a prompt tailored for\npathological question answering. We use GPT-3.5 [7] for instruction dataset genera-\ntion enabling a low cost operation with prompt examples seen in Table 10. Given the\nhigh variance of prompt design, we have designed four desired principles of the prompt\nconstruction. First, we use GPT-3.5 to simulate a scenario where users (patient or\ndoctor) and AI assistants (CLOVER model) conduct question and answering (QA).\nSince GPT-3.5 does not have access to images, QAs generated by GPT-3.5 are based\non the textual description of the image. To reduce the over-reliance on textual descrip-\ntions, we emphasize avoiding minor information that can not be obtained from the\nimage (e.g., reference dates or magnification ratios). Second, we focus on adding visual\ndetailed information such as tissue structure, cell morphology, potential lesions, and\nlocations. Third, the noise in the original textual description is avoided such as vocab-\nularies related to context or narrator. Finally, we seek to generate answers of GPT-3.5\nto exhibit the cautiousness, aligning with the medical field's expectation for producing\nprudent answers. To enhance the quality of generated data, we additionally introduce\nfew-shot examples in the prompts to inject more relevant information for in-context\nlearning [48].\nThe above process of generation-based instruction\nwith GPT-3.5 is often based on partial captions derived from images. Generation-\nbased instructions only capture a portion of the original information, preventing LLMs\nfrom fully capturing the visual and descriptive content. To address this challenge,"}, {"title": "5.2 Training Details of CLOVER", "content": "Model training involves two main stages: (i) alignment of vision and language and (ii)\nsupervised fine-tuning with instructions. In the first training stage, to align patho-\nlogical images and text, we train BLIP-2 on the original image-text pairs directly\nobtained from Quilt-1M dataset [23]. BLIP-2 takes inputs in the form of a pair of\nimage and text (caption or question). The visual encoder is utilized for extracting fea-\ntures from image and generating visual tokens V = {V1, V2, ..., Vn\u2082 }, where n\u2082 is the\nnumber of visual tokens. Next, the lightweight Q-former handles text and visual learn-\nable queries, incorporating a self-attention [49] mechanism to share the transformer for\nboth visual and textual components. A cross-attention mechanism is used to facilitate\nthe interaction between visual tokens and visual queries. We simultaneously optimize\nthe Q-former using image-text contrastive loss, image-grounded text generation loss,\nand image-text matching loss [32]. The image-text contrastive loss aims to maximize"}, {"title": "p(A|P, Q, V) = \u03a0\u03c1o(ai|P, Q, V, a1, a2, ..., a\u017c\u22121),", "content": "Eq.1 where po(ai P, Q, V, a1, a2, ..., ai\u22121) is the probability to generate a\u017c given P, Q, V,\na1, a2, ..., and ai-1 under the model parameters 0."}, {"title": "5.3 Implementation Details of CLOVER", "content": "We choose EVA-ViT-G/14 [50] as the frozen visual encoder, utilizing the output from\nits second-to-last layer as the visual feature representation. This choice has been\nvalidated as a superior solution in BLIP-2 [33]. Regarding the LLM, we select the\ndecoder-only Vicuna 7B [31] and encoder-decoder based FlanT5XL [30], which could\nbest utilize the pre-trained BLIP-2 parameters. To enhance the training efficiency, we\nconvert the parameters of visual encoder and LLM to FP16. In the first stage of model\ntraining, we conduct training for 20 epochs with a batch size of 36. In the second\nstage, training continues for 30 epochs, with a batch size of 2 for Vicuna and 8 for\nFlanT5XL. The remaining hyperparameter settings remain consistent with BLIP-2.\nWe complete the two-stage training using Vicuna in 4 days with 4 RTX-3090 GPUs."}, {"title": "5.4 Datasets", "content": "Quilt-1M [23] is a multi-modal pathology dataset involving both pathological vision\nand language information. It includes 768,826 pathological images and their corre-\nsponding textual annotations obtained from YouTube. Based on Quilt, additional\nestablished Internet-based, image-text paired data are integrated to form a total"}, {"title": "5.4.1 CLOVER Training Dataset", "content": "dataset of one million image-text pairs, called Quilt-1M. In our study, we use Quilt-\n1M in two tasks including the first-stage vision-language alignment and instruction\ngeneration for supervised fine-tuning.\ninstruction is generated with our proposed method as introduced\nabove. It consists of the generated 15k (a default size in major experiments)\ngeneration-based using GPT-3.5 and 30k template-based instructions based on manual\nconstruction. We develop this original instruction dataset for activating LLMs to com-\nplete visual-language question answering in pathology domain. These instructions are\nused for instruction tuning (Stage 2). An example of generating question and answer\ninstructions based on the prompt is shown in Table 11."}, {"title": "5.4.2 Evaluation Datasets", "content": "PathVQA [35] is a pathological visual question-answering dataset, comprising 4,998\npathological images and 32,799 question-answer pairs. The questions in this dataset are\ncategorized into two types: open-ended questions and closed-ended questions. Open-\nended questions cover a wide range of topics, including why, what, how, etc., while\nclosed-ended answers are limited to responses like \"yes\" or \"no\". The training and\ntesting splits are specified by the dataset. For experiments using PathVQA, the models\nare fined tuned by training dataset. Testing dataset is used for evaluation. This setting"}, {"title": "5.5 Compared Methods and Instructions", "content": "We conduct a comprehensive comparison with the standard BLIP-2 and other strong\nbaseline approaches, including VL Encoder-Decoder [51], Q2ATransformer [52], M212\n[3], LLaVA [9], and LLaVA-Med [18]. Since VL Encoder-Decoder, Q2ATransformer\nand M212 do not have zero-shot generalization ability, these models are not considered\nwhen evaluating the performance on QUILT-VQA."}, {"title": "5.6 Evaluation Metrics", "content": "For PathVQA, we use the recall of the true answer that appears in the predicted answer\nin open-ended questions, and we report the accuracy in closed-ended questions. For\nQUILT-VQA, we find that under zero-shot learning, the answer lengths of different\nmodels vary greatly, and thus we report the recall, precision and F1-score. F1-score as\nan integration of recall and precision is used as a more comprehensive metric among the\nthree. All above metrics are reported in percentage. To evaluate the cost-effectiveness\nof different models, we further compute the ratio of performance to the log of the\nnumber of parameters as a key metric."}, {"title": "Declarations", "content": "This study is supported in part by Shanghai Artificial Intelligence Laboratory.\nThe authors declare no competing interest.\nThe Ethics Committee of Xinhua Hospital approved this study after data anonymiza-\ntion.\nThe QUILT-1M and QUILT-VQA can be accessed in https://quilt1m.github.io/. And\nPathVQA can be downloaded from https://paperswithcode.com/dataset/pathvqa.\nThe CLOVER instruction data will be released upon acceptance. The clinical dataset\nfrom Xinhua Hospital is available upon request from the corresponding author due to\nthe restriction of hospital.\nThe code and model will be released upon acceptance at https://github.com/\nJLINEkai/CLOVER."}, {"title": "Appendix A Supplementary Case Study", "content": "To gain more insight into the model output, we present cases from visual question\nanswering (VQA) experiments involving comparisons with LLaVA [9], LLaVA-Med\n[18], BLIP-2 [33], and our CLOVER on QUILT-VQA [19]. Cases are shown in Tabel\nA1, A2, and A3. In Table A1, LLaVA describes a scenario that is irrelevant to the\ngiven question and is unrelated to the reference answer. LLaVA-Med provides an\nanswer related to cystic spaces, which does not match the reference answer. BLIP-2's\nresponse is incorrect, overly brief, and lacked diagnostic value. In contrast, CLOVER\nprovides an answer related to angiomyolipoma, which closely matches the reference\nanswer and includes detailed descriptions related to angiomyolipoma. In Table A2,\nthe question involves medical knowledge, testing the model's stored knowledge about\nstains. LLaVA-med provides an incorrect response, addressing H&E staining while the\nquestion is about specific stains 3 and 20. The answers of BLIP-2 still are very brief\nand irrelevant. LLaVA and CLOVER perform well, providing relevant and professional\nanswers. In Table A3, CLOVER answers correctly, staying focused on the core of the\nquestion, mentioning the absence of cellular atypia, mitotic activity, or necrosis. This\nfinding closely aligns with the reference answer and clearly outperforms other models."}]}