{"title": "Cost-effective Instruction Learning for Pathology Vision and Language Analysis", "authors": ["Kaitao Chen", "Mianxin Liu", "Fang Yan", "Lei Ma", "Xiaoming Shi", "Lilong Wang", "Xiaosong Wang", "Lifeng Zhu", "Zhe Wang", "Mu Zhou", "Shaoting Zhang"], "abstract": "The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology.", "sections": [{"title": "1 Main", "content": "The rise of vision-language model (VLM) opens remarkable opportunities to analyze pathological images in a visual question-answering manner [1-3]. This profound progress of multi-modal data integration leverages the power of large language model (LLM) on cognition, reasoning, and content generation [4-6]. In essence, LLMs are large-scale parametric networks trained on vast amounts of data, enabling them to generate human-like responses and achieve remarkable accuracy. ChatGPT [7] and GPT-4 [8] are examples to simulate the conversational interaction between AI-enabled models and humans. The generated conversational records can be re-used to guide the visual-language model refinement [9-12], opening avenues for downstream tasks across domains [1]. In the landscape of digital pathology, providing in-depth language descriptions of cell morphology, tissue status, and treatment suggestions, equipped with human-like interactions, could enhance tissue understanding, characterization, and decision making in various clinical scenarios [13-17].\nEmerging pathological vision-language models (PVLMs) (e.g., LLaVA-Med [18] and Quilt-LLaVA [19]) have demonstrated their utility in analyzing medical imaging. However, it is widely known that building a capable PVLM demands an excessive training data, human labour, financial, and computational resources [20, 21]. Extending general-purpose models into pathology-oriented model starts with using pathological vision-language datasets. These datasets consist of (i) image-text pairs (image-caption) dataset and (ii) instruction (image-question-answering) dataset [22-25]. The image-text dataset aligns visual and language features [26], providing rich semantic information for pathological image contents [27-29]. Meanwhile, instruction dataset is crucial for activating LLMs to complete the visual-language question answering. Yet the process of instruction generation incurs a considerable financial cost by using GPT-4 (nearly $9,000) [19]. In addition to the instruction data, model optimization also requires a substantial compute support. Directly tuning a billion-parameter LLM demands high computational resources and is impractical to achieve on consumer-grade GPUs. To overcome these bottlenecks, fundamental questions are urged to be addressed towards building an effective but low-cost PVLM: (i) How can we come up with a lightweight tuning approach to complement the LLaVA-like tuning on large training parameters? (ii) How can we generate a useful instruction dataset in a cost-effective way? (iii) How can we achieve few-shot generalized learning ability of PVLM for clinical applications without using a large-scale instruction dataset?"}, {"title": "2 Building Efficient PVLM via CLOVER", "content": null}, {"title": "2.1 Vision-language Foundation Model", "content": "The family of LLMs [1, 8, 30, 31] and visual language pre-training [26, 32] are two indispensable building blocks for vision-language foundation models. Visual language pre-training aligns the encoding space in vision and language. The visual features from the visual encoder can then be perceived by the LLMs. Examples of multi-modal foundation models include Flamingo [10], BLIP-2 [33], FROMAGE [11], mPLUG-Owl [12], and LLaVA [9]. In particular, Flamingo, BLIP-2, and FROMAGe all freeze LLMs"}, {"title": "2.2 Medical Vision-language Foundation Model", "content": "Building medical vision-language foundation models requires systematic efforts in both data curation and model inference in the healthcare system [1]. The large-scale resources from the literature (PMC-15M [22]), social media (PLIP [24] and Quilt-1M [23]), accessible textbooks [21] and private hospital data [25, 34] provide critical data support for domain-specific multi-modal models. Examples include PMC-LLAMA [20], LLaVA-Med [18], and Med-Flamingo [21]. LLaVA-Med is a representative work that aligns vision and language using 600K image-text pairs on the basis of LLaVA and fine-tunes the model using 60K dialogue-based instructions. Yet these instruction sets are generated by GPT-4 at a high cost. Similarly, in the pathological field, Quilt-LLaVA [19] uses Quilt-1M and GPT-4 to generate instruction data at a cost of up to $8,858. Towards a better utility of foundation models, instruction preparation and generation in the various forms of structure and scale require substantial research efforts. Meanwhile, during the fine-tuning stage, LLaVA-like methods [9] update the parameters of the LLM, resulting in a high training cost that can not be accommodated on consumer-grade GPUs. This pressing demand for high-performance training equipment becomes a daunting hurdle for researchers to extend general-purpose models into domain-specific vision-language models."}, {"title": "2.3 CLOVER", "content": "In this study, we propose a cost-effective learning framework for accurate pathology vision and language inference named as CLOVER (Fig. 1). To achieve fast domain tuning with low training resources, we adopt the BLIP-2 architecture [33] as a visual language pre-training using a lightweight trainable query transformer (Q-former), a frozen visual encoder, and a frozen LLM. We leverage the paired pathological image and text captures from the Quilt-1M [23] dataset to align the vision and language. For the instruction fine-tuning, we meticulously design prompts tailored for pathological question answering and use GPT-3.5 [7] for generating effective instruction dataset at low cost, referred as the \"generation-based instructions\". The instructions generated by our PVLM-orient prompt show strong capabilities in the model tuning. Additionally, we construct instructions by matching template questions with the original text captions to enhance the model's vision understanding ability, referred as the \"template-based instructions\". Integrating these two types of instructions gives rise\nto the hybrid-form instructions, which remarkably enhance CLOVER's conversational abilities in pathology. Using two benchmark datasets and one independent clinical dataset, we comprehensively validate the effectiveness of CLOVER and demonstrate its potential for assisting clinical pathological tasks by \"talking to your pathology data\" in resource-constrained settings.\nOur study has made multifaceted contributions: First, we propose an effective PVLM training framework particularly at low computational resources and financial spending. Different from the LLaVA-like tuning (e.g. LLaVA-Med), we emphasize the use of BLIP-2 [33] to serve as an alternative choice for the cost-effective lightweight inference. We find that tuning the wide spectrum of LLM's parameters is unnecessary for building a sufficiently usable PVLM. Second, the value of our designed PVLM-oriented prompt is pronounced on GPT-3.5 instead of the advanced GPT-4. Low-cost CLOVER model outperforms those models trained with LLM tuning on the instruction data generated by GPT-4 in multiple settings. We also recognize template-based instructions without relying on GPT can significantly supplement generation-based"}, {"title": "3 Results", "content": null}, {"title": "3.1 Quantitative Comparison", "content": "We systematically measure CLOVER's visual question answering (VQA) performance using two public benchmark datasets, namely PathVQA [35] and QUILT-VQA [19], and one independent clinical data set (see Sec. Datasets). We compare CLOVER's performance with state-of-the-art (SOTA) PVLMs trained with notable costs to observe the cost-effectiveness of CLOVER.\nFrom Table 1, CLOVER outperforms major competing methods, showing a remarkable improvement of 26.13% in accuracy at maximum for closed-ended questions. Our model performance is even approaching to LLaVA-Med (37 times more model parameters) with a minor accuracy difference of 1.83%. Note that LLaVA and LLaVA-Med achieve their results through extensive parameter tuning of LLM and training on a vast instruction dataset generated by GPT-4. To illustrate, our method involves training with a frozen LLM, adjusting only a small portion of parameters (about 1/37 of LLaVA-Med). Meanwhile, CLOVER only involves the use an entry-level GPT-3.5 with a smaller scale of instruction dataset (2/3 of LLaVA-Med). As seen in Table 1, our model also shows superior performance improvement in open-ended question scenarios. CLOVER's performance is twice that of BLIP-2 and approaches five times that of LLaVA in open-ended question scenarios. Compared with BLIP-2, our model improves in both closed-ended and open-ended question-answering. This key finding suggests that vision-language models could evidently benefit from the high-quality pathology-sensitive instruction data towards a low-cost model development.\nWe validate CLOVER's zero-shot generalization capability on the QUILT-VQA dataset. From Table 2, CLOVER achieves the leading performance in precision and F1-score. Our results are even close to LLaVA and LLaVA-Med in terms of recall. Due to the longer answer"}, {"title": "3.2 Qualitative Comparison", "content": "To gain insight into model outputs, we present the representative cases from VQA experiments involving comparisons with LLaVA, LLaVA-Med, BLIP-2, and our CLOVER on QUILT-VQA and LLaVA-Med-17K datasets. In Table 3 for results from QUILT-VQA, we observe that the output of LLaVA is unrelated to the actual content, reflecting general image contents such as \"people\", \"water\", and \"lake\". While"}, {"title": "3.3 External Clinical Data Validation", "content": "To assess the efficiency of CLOVER in real-world applications with minimal human labour, we validate CLOVER's few-shot learning capability under a challenging task of cancer detection (i.e., cancer/non-cancer tissue classification). We implement a K-shot learning testing scheme, meaning that our model can only use K WSIs (K = 1, 2) from each class for a model fine-tuning. We evaluate the model performance on the external validation dataset on two cancer tissues. Fig. 2 details the performance of\nCLOVER in the intestinal cancer detection. We report comparative results from BLIP-2 model as it is not feasible for a RTX 3090 to fine-tune resource-demanding LLaVA and LLaVA-Med. In addition, Fig. 3 presents results in the gastric cancer detection. Overall, both Fig. 2 and 3 show that CLOVER demonstrates a performance advantage"}, {"title": "3.4 Ablation Studies", "content": "We conduct ablation studies on assessing the value of instruction data. We compare our instruction data with SOTA instruction datasets using the same model setting using BLIP-2 and use PathVQA as the final testing set. In detail, we use the generated 15k generation-based and 30k template-based instructions as our instruction data. In Table 5, the gains from using our instructions generated by GPT-3.5 based on noisy internet data are higher compared to using instructions generated by GPT-4 based on high-quality data. This finding can be attributed to the developed form of the instructions (generation-based and template-based instructions) and the quality of the instructions (generated through PVLM-oriented prompts). In particular, our costs are only $8, whereas the estimated API costs of LLaVA-Med are hundreds of times higher than our low-cost approach. When using the FlanT5XL model, our performance on closed-ended question-answering closely approaches that of the LLaVA-Med-Pathology-IM (LM-IM) [18], evidently outperforming LLaVA-Med-Pathology (LM) [18] and LM-IM on open-ended question-answering by 2.22% and 3.13% respectively. With the Vicuna 7B model, our results surpass the LM on both closed-ended and open-ended question-answering settings.\nWe investigate the effectiveness of template-based instructions, aiming to provide the LLM with more direct and comprehensive language guidance. As shown in Table 6, results indicate that the addition of template-based instructions to the original instruction data is effective. Especially in FlanT5XL, there is a remarkable improvement in the open-question answering. We reason that template-based instructions can provide a complete answer about the image caption. Therefore it is possible to compensate for the information"}, {"title": "4 Discussion", "content": "Development of cost-effective pathological vision-language models (PVLMs) is crucial to enable timely information retrieval and accelerate clinical decision making. In this study, we have proposed a framework to train a precise PVLM with a low computational resource and financial expenditure. A key contribution is that we have designed PVLM-oriented prompts and leveraged the GPT-3.5 to create question-and-answer instruction datasets. These generated instructions, combined with template-based instructions requiring no economical expands, exhibit superior capabilities in aiding instruction-based model tuning and downstream use. Our established CLOVER performs strongly comparing to advanced models with significantly larger parameters and expansive costs using GPT-4. In addition, CLOVER presents an appealing conversational capacity in the real-world data from stomach and intestine cancer tissues. These findings demonstrate CLOVER's potential as a cost-effective multimodal framework to support downstream clinical tasks without excessive model training and human labeling efforts.\nComputational pathology is widely known for its high demands on data and computational resource, especially for building a useful PVLM application. Prior efforts were primarily focused on the use of human-examined clinical data [1, 36]. A key differentiation of our study is to build efficient PVLMs based on the public Internet"}, {"title": "5 Methods", "content": "In this section, we offer methodological details of the development of CLOVER. We next introduce the involved datasets and the evaluation schemes of experiment."}, {"title": "5.1 Efficient Instructions Construction at Low-cost in CLOVER", "content": "Generating large-scale instructions via GPT-4 incurs significant financial costs. In this study, we specialize in developing effective domain-specific instruction data generation at a low cost. The proposed framework includes (i) generation-based instructions with a specialized prompt for employing GPT-3.5, and (ii) template-based instructions without any additional financial cost. The construction of generation-based instructions involves a generation of question-answer (QA) from the captions using GPT and a PVLM-oriented prompt, while that of template-based instructions involves matching the captions as answer to a set of pre-designed template questions. Notably, the template-based QA dataset permits a comprehensive understanding of image contextual information, while the generation-based QAs emphasize the pathological knowledge distilled from GPT-3.5.\nWe meticulously design a prompt tailored for pathological question answering. We use GPT-3.5 [7] for instruction dataset generation enabling a low cost operation with prompt examples seen in Table 10. Given the high variance of prompt design, we have designed four desired principles of the prompt construction. First, we use GPT-3.5 to simulate a scenario where users (patient or doctor) and AI assistants (CLOVER model) conduct question and answering (QA). Since GPT-3.5 does not have access to images, QAs generated by GPT-3.5 are based on the textual description of the image. To reduce the over-reliance on textual descriptions, we emphasize avoiding minor information that can not be obtained from the image (e.g., reference dates or magnification ratios). Second, we focus on adding visual detailed information such as tissue structure, cell morphology, potential lesions, and locations. Third, the noise in the original textual description is avoided such as vocabularies related to context or narrator. Finally, we seek to generate answers of GPT-3.5 to exhibit the cautiousness, aligning with the medical field's expectation for producing prudent answers. To enhance the quality of generated data, we additionally introduce few-shot examples in the prompts to inject more relevant information for in-context learning [48].\nThe above process of generation-based instruction with GPT-3.5 is often based on partial captions derived from images. Generation-based instructions only capture a portion of the original information, preventing LLMs from fully capturing the visual and descriptive content. To address this challenge,"}, {"title": "5.2 Training Details of CLOVER", "content": "Model training involves two main stages: (i) alignment of vision and language and (ii) supervised fine-tuning with instructions. In the first training stage, to align pathological images and text, we train BLIP-2 on the original image-text pairs directly obtained from Quilt-1M dataset [23]. BLIP-2 takes inputs in the form of a pair of image and text (caption or question). The visual encoder is utilized for extracting features from image and generating visual tokens \\(V = \\{V_1, V_2, ..., V_{n_v}\\}\\), where \\(n_v\\) is the number of visual tokens. Next, the lightweight Q-former handles text and visual learnable queries, incorporating a self-attention [49] mechanism to share the transformer for both visual and textual components. A cross-attention mechanism is used to facilitate the interaction between visual tokens and visual queries. We simultaneously optimize the Q-former using image-text contrastive loss, image-grounded text generation loss, and image-text matching loss [32]. The image-text contrastive loss aims to maximize\nthe similarity between the same image-text pair by comparing visual queries and text representations. The image-grounded text generation loss trains a text transformer to generate corresponding text given an image. Meanwhile, the image-text matching loss is a binary classification loss used to determine whether an image and text belong to the same pair.\nIn the second stage training, we introduce the customized instruction dataset for activating LLM and completing visual language question answering. In order to stimulate the domain speciality of LLM, we add a task-driven prompt before the input question as \"Now that you are a pathologist, please answer the following questions based on the images\". We utilize a standard tokenization to obtain a sequence of text tokens, including prompt tokens \\(P = \\{P_1, P_2, ..., P_{n_p}\\}\\), question tokens \\(Q = \\{q_1, q_2, ..., q_{n_q}\\}\\), and answer tokens \\(A = \\{a_1, a_2,..., a_{n_a}\\}\\), where \\(n_q\\), \\(n_p\\) and \\(n_a\\) represent the token lengths of prompt, questions and answer. We feed the Q-former both the visual tokens \\(V = \\{V_1, V_2, ..., V_{n_v}\\}\\) and the text tokens, and then adjust a linear layer to map the dimension of image token into the the dimension of text. Our optimization objective is to fine-tune the learnable parameters \\(\theta\\) of Q-former by maximizing the following likelihood:\n\\(\begin{equation} p(A|P, Q, V) = \\prod_{i=1}^{N_a}p_{\\theta}(a_i|P, Q, V, a_1, a_2, ..., a_{i-1}),  \\tag{1} \\end{equation}\\)\nwhere \\(p_{\\theta}(a_i|P, Q, V, a_1, a_2, ..., a_{i-1})\\) is the probability to generate \\(a_i\\) given \\(P, Q, V\\), \\(a_1, a_2, ..., and \\(a_{i-1}\\) under the model parameters \\(\theta\\)."}, {"title": "5.3 Implementation Details of CLOVER", "content": "We choose EVA-ViT-G/14 [50] as the frozen visual encoder, utilizing the output from its second-to-last layer as the visual feature representation. This choice has been validated as a superior solution in BLIP-2 [33]. Regarding the LLM, we select the decoder-only Vicuna 7B [31] and encoder-decoder based FlanT5XL [30], which could best utilize the pre-trained BLIP-2 parameters. To enhance the training efficiency, we convert the parameters of visual encoder and LLM to FP16. In the first stage of model training, we conduct training for 20 epochs with a batch size of 36. In the second stage, training continues for 30 epochs, with a batch size of 2 for Vicuna and 8 for FlanT5XL. The remaining hyperparameter settings remain consistent with BLIP-2. We complete the two-stage training using Vicuna in 4 days with 4 RTX-3090 GPUs."}, {"title": "5.4 Datasets", "content": null}, {"title": "5.4.1 CLOVER Training Dataset", "content": "Quilt-1M [23] is a multi-modal pathology dataset involving both pathological vision and language information. It includes 768,826 pathological images and their corresponding textual annotations obtained from YouTube. Based on Quilt, additional established Internet-based, image-text paired data are integrated to form a total\ndataset of one million image-text pairs, called Quilt-1M. In our study, we use Quilt-1M in two tasks including the first-stage vision-language alignment and instruction generation for supervised fine-tuning.\nCLOVER instruction is generated with our proposed method as introduced above. It consists of the generated 15k (a default size in major experiments) generation-based using GPT-3.5 and 30k template-based instructions based on manual construction. We develop this original instruction dataset for activating LLMs to complete visual-language question answering in pathology domain. These instructions are used for instruction tuning (Stage 2). An example of generating question and answer instructions based on the prompt is shown in Table 11."}, {"title": "5.4.2 Evaluation Datasets", "content": "PathVQA [35] is a pathological visual question-answering dataset, comprising 4,998 pathological images and 32,799 question-answer pairs. The questions in this dataset are categorized into two types: open-ended questions and closed-ended questions. Open-ended questions cover a wide range of topics, including why, what, how, etc., while closed-ended answers are limited to responses like \"yes\" or \"no\". The training and testing splits are specified by the dataset. For experiments using PathVQA, the models are fined tuned by training dataset. Testing dataset is used for evaluation. This setting\nis consistent with previous studies [18] and provides the basis to compare our CLOVER with SOTA methods. Note that for all ablation studies, we use PathVQA by default.\nQUILT-VQA [19] is another pathological visual question-answering dataset. QUILT-VQA is uniquely sourced from educational video contents covering diverse topics. Researchers extract valuable texts from these videos. Then, GPT-4 is utilized to extract question-answering pairs from these texts with human intervention ensuring the pairs alignment on the medical themes. QUILT-VQA comprises 985 visual question-answer pairs, with an average word count of 17 in the answers. Again, to be consistent with previous work [19], we use a zero-shot validation scheme in QUILT-VQA. In our study, we regard all questions as open-ended questions to fully utilize the dataset.\nClinical dataset is used to test CLOVER under a real-world clinical setting. We design a cancer detection task, where CLOVER performs in a visual question-answering (VQA) manner for the given image patch. We collect 38 cancer tissues WIS in Pathology Department of Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, during April 2024 to May 2024, including 13 WSIs from stomach and 25 WSIs from intestines. To identify specific pathological regions within WSIs, we collaborate with two experienced pathologists to perform the fine-grained annotation with cross-checking. These annotations, provided in standard XML files, delineate the negative and positive regions within the WSIs. For testing CLOVER working on patch-level VQA, we further extract non-overlapping image patches (of size 512 \u00d7 512 pixels) containing the annotated sub-regions. This process yields 7,112 image patches (1,136 tumor and 2,079 non-tumor patches from stomach, 1,846 tumor and 2,051 non-tumor patches from intestines). For model training and evaluation, we formulate the cancer detection task into a visual question answering (VQA) format. The uniform question is posed as: \"Is this pathological image showing a negative or positive result?\" The answers are set to \"this is a negative pathological image\" or \"this is a positive pathological image\" based on the ground-truth labels. For this experiment, we implement a few-shot learning setting to test whether our model can be fast transferred to the real-world data with a few annotated samples. We divide the data into training (15 WSIs) and testing sets (23 WSIs) on the WSI level, and the number of training WSIs depends on the setting of our five independent experiments. For both each 1-shot (patches of one random WSI from each class for training) and 2-shot (two WSIs from each class) experiment, we reconstruct the training samples using different combinations of samples from training samples. We ensure that there is no WSI data from overlapped patients. The test set remains constant throughout all experiments to facilitate the evaluation across different experiments."}, {"title": "5.5 Compared Methods and Instructions", "content": "We conduct a comprehensive comparison with the standard BLIP-2 and other strong baseline approaches, including VL Encoder-Decoder [51], Q2ATransformer [52], M2I2 [3], LLaVA [9], and LLaVA-Med [18]. Since VL Encoder-Decoder, Q2ATransformer and M2I2 do not have zero-shot generalization ability, these models are not considered when evaluating the performance on QUILT-VQA.\nLLaVA-Med-Pathology and LLaVA-Med-Pathology-IM [18] are the public instruction datasets that we use to compare with our proposed instruction dataset. LLaVA-Med-Pathology is a high-quality conversational instruction dataset focused on the pathological domain and is a subset of LLaVA-Med [18]. LLaVA-Med-Pathology-IM is another version of LLaVA-Med-Pathology, which adds inline content to the original image descriptions as supplementary textual information. These datasets are created by transforming parts of the PMC-15M dataset [22] into instruction datasets using GPT-4 based on specific prompts."}, {"title": "5.6 Evaluation Metrics", "content": "For PathVQA, we use the recall of the true answer that appears in the predicted answer in open-ended questions, and we report the accuracy in closed-ended questions. For QUILT-VQA, we find that under zero-shot learning, the answer lengths of different models vary greatly, and thus we report the recall, precision and F1-score. F1-score as an integration of recall and precision is used as a more comprehensive metric among the three. All above metrics are reported in percentage. To evaluate the cost-effectiveness of different models, we further compute the ratio of performance to the log of the number of parameters as a key metric."}, {"title": "Declarations", "content": null}, {"title": "Funding", "content": "This study is supported in part by Shanghai Artificial Intelligence Laboratory."}, {"title": "Conflict of Competing Interests", "content": "The authors declare no competing interest."}, {"title": "Ethics approval and consent to participate", "content": "The Ethics Committee of Xinhua Hospital approved this study after data anonymization."}, {"title": "Data Availability", "content": "The QUILT-1M and QUILT-VQA can be accessed in https://quilt1m.github.io/. And PathVQA can be downloaded from https://paperswithcode.com/dataset/pathvqa.\nThe CLOVER instruction data will be released upon acceptance. The clinical dataset from Xinhua Hospital is available upon request from the corresponding author due to the restriction of hospital."}, {"title": "Code Availability", "content": "The code and model will be released upon acceptance at https://github.com/\nJLINEkai/CLOVER."}, {"title": "Appendix A Supplementary Case Study", "content": "To gain more insight into the model output, we present cases from visual question answering (VQA) experiments involving comparisons with LLaVA [9], LLaVA-Med [18], BLIP-2 [33], and our CLOVER on QUILT-VQA [19]. Cases are shown in Tabel A1, A2, and A3. In Table A1, LLaVA describes a scenario that is irrelevant to the given question and is unrelated to the reference answer. LLaVA-Med provides an answer related to cystic spaces, which does not match the reference answer. BLIP-2's response is incorrect, overly brief, and lacked diagnostic value. In contrast, CLOVER provides an answer related to angiomyolipoma, which closely matches the reference answer and includes detailed descriptions related to angiomyolipoma. In Table A2, the question involves medical knowledge, testing the model's stored knowledge about stains. LLaVA-med provides an incorrect response, addressing H&E staining while the question is about specific stains 3 and 20. The answers of BLIP-2 still are very brief and irrelevant. LLaVA and CLOVER perform well, providing relevant and professional answers. In Table A3, CLOVER answers correctly, staying focused on the core of the question, mentioning the absence of cellular atypia, mitotic activity, or necrosis. This finding closely aligns with the reference answer and clearly outperforms other models."}]}