{"title": "DEBUG-HD: Debugging TinyML models on-device\nusing Hyper-Dimensional computing", "authors": ["Nikhil P Ghanathe", "Steven JE Wilton"], "abstract": "TinyML models often operate in remote, dynamic environments without cloud\nconnectivity, making them prone to failures. Ensuring reliability in such scenarios\nrequires not only detecting model failures but also identifying their root causes.\nHowever, transient failures, privacy concerns, and the safety-critical nature of many\napplications-where systems cannot be interrupted for debugging-complicate\nthe use of raw sensor data for offline analysis. We propose DEBUG-HD, a novel,\nresource-efficient on-device debugging approach optimized for KB-sized tinyML\ndevices that utilizes hyper-dimensional computing (HDC). Our method introduces\na new HDC encoding technique that leverages conventional neural networks,\nallowing DEBUG-HD to outperform prior binary HDC methods by 27% on average\nin detecting input corruptions across various image and audio datasets.", "sections": [{"title": "1 Introduction", "content": "Recent advances in machine learning (ML) and embedded systems have enabled ML to run on\nKB-sized, milliwatt-powered devices known as TinyML. These always-on devices perform all compu-\ntations locally without any cloud connectivity [60]. TinyML has increasingly been used in mission-\ncritical scenarios such as autonomous navigation [57] and healthcare diagnostics [49]. However,\nwhen deployed in uncertain environments, where inputs can become unpredictably corrupted (e.g.,\nsensor failure or weather changes) [35], maintaining model reliability becomes crucial, especially\nwhen device access is limited. A model is considered reliable if its failures can be detected, and\nthe root cause of those failures can be identified and addressed effectively. While recent works [15]\nhave proposed resource-efficient monitoring mechanisms for detecting failures in tinyML systems,\nensuring reliability often requires identifying these root causes, sometimes in the field. However, this\nis challenging on KB-sized tinyML devices, often running bare-metal applications, as transmitting raw\nsensor data for offline diagnosis can raise privacy and regulatory concerns [7], especially without se-\ncure communication hardware. Moreover, many tinyML applications are safety-critical (e.g., remote\npatient monitoring [3], autonomous navigation [44]), and thus cannot be interrupted for transmitting\ndata [13]. In such scenarios, on-device debugging becomes crucial to ensure reliability, offering\nadvantages such as 1) triggering corrective actions by identifying the root cause of failures [36], and\n2) enhancing active learning by uncovering hard-to-obtain yet highly informative samples [53, 11].\nUnlike most prior works on on-device debugging [22, 50, 16], we focus on What input changes\ncaused model failure? instead of Why a model fails for given inputs? Thus, we focus on diagnosing\nfailures in field-deployed models in this work, the first crucial step in debugging. To combat resource\nscarcity, we investigate hyper-dimensional computing (HDC) [34] as a resource-efficient paradigm for\non-device debugging in TinyML, leveraging HDC's lightweight operations and memory-optimized\nrepresentations. To explore this idea, we focus on the task of identifying the source of corruptions\nin input images (e.g., noise, weather, blur etc.) that cause model accuracy to drop. We propose\nDEBUG-HD, a novel binary HDC classifier that can effectively classify these corruptions (data shifts)"}, {"title": "2 Background and Preliminaries", "content": "Traditional ML debug TensorFlow Debugger (tfdbg) [8]\nprovides deep insights into TensorFlow graphs for\nsoftware-like debugging, while TensorBoard [61] and Ten-\nsorWatch [54] allow visual monitoring of model statistics.\nTypically, lossy compression (e.g., statistical summaries)\nis sufficient for machine learning debugging. Tools like\nML-Exray [50] catch deployment issues (e.g., quantiza-\ntion/preprocessing bugs), while Nazar [16] performs root-\ncause analysis. U-TOE [25] and RIOT [26] add over-\nthe-air (OTA) debugging capabilities. Other debugging\ntechniques [35, 10] focus on model assertions and charac-\nterizing model instability. The closest works on on-device\nML debug [22, 47, 21] track statistics like sparsity and\nmagnitude distributions. Another line of research focuses\non interpretability/explainability methods [4, 2, 37, 1, 56, 5, 63, 38, 55, 31], which help explain the\ninner workings of a ML model in a human-understandable way. However, almost all prior works\nassume easy hardware access, which is often impractical for field-deployed tinyML systems.\n2.1 HDC preliminaries\nHDC encodes/projects inputs into hyper-dimensions to exploit the higher discriminative properties of\nhyper-dimensional representations, which in turn enables simpler learning processes that are both\ncompute and memory friendly. There are three key stages in HDC model development (see Figure 1).\n1) Encoding: All feature vectors in train set with dimension d are encoded/projected into hyper-\ndimension $D_H$ using a projection matrix (usually randomly-generated [9]) of size d \u00d7 $D_H$ such that\n$D_H$ >> d. This is a simple matrix-vector multiplication. The encoded vector is called a hypervector.\n2) Training: The class hypervectors (HV) are learned by summing all encoded HVs associated with the\nclass. For example, consider a balanced dataset D with size |D| and L classes. Let {$f_{l1}, f_{l2},... f_{lN}$} \nbe the N samples from D belonging to class l such that |D| = N \u00d7 L. The encoded versions are\ngiven by {$H_{l1}, H_{l2}, \u2026\u2026H_{lN}$}. Then, $C_l = \\sum_{i=1}^{N} H_{li}$. {$C_1, C_2, \u2026, C_L$} is the set of all class HVs.\n3) Inference: The unseen test input is encoded using the same projection matrix as before to create\nthe query HV ($H_{query}$). Next, the similarity score of $H_{query}$ is calculated with respect to each class\nHV and the class with the highest score is predicted. $l_{predicted} = argmax_{l \\in {1,2,..,L}} (\\delta(H_{query}, C_l))$"}, {"title": "3 DEBUG-HD", "content": "We aim to develop a diagnostic instrument to identify the source/type of input corruption, which\nis highly challenging [24]. Our preliminary evaluations show that a simple multi-layer perceptron\n(MLP) can effectively identify various corruptions but incurs significant overhead; for instance, an\n8-bit 2-layer MLP for detecting 19 types of CIFAR10 corruptions uses nearly twice the resources\nof the ResNet-8 [17] base network. Thus, we explore a HDC classifier for this task due to its lower\nmemory/compute footprint, enabled by its bipolar nature and simple parallel operations. Despite the\nlimitations of hyper-d < 1000, we aim to enhance performance through our proposed approach.\n3.1 MLP-assisted Encoding\nTo address the limitations of HDC in hyper-d < 1000, we design a novel initialization scheme for\nthe HDC encoder where we learn the encoding/projection matrix from a simple 2-layer MLP. We\nobserve that a 2-layer MLP (hidden+classification layer) with no bias vector closely resembles the\nHDC classifier. In particular, the hidden layer encodes/projects the inputs into a higher dimension\nusing a matrix-vector multiplication operation, which is similar to the encoding method of a HDC\nclassifier. Figure 1 illustrates our proposed approach.\nTraining and Inference: As shown, we first train a 2-layer MLP model with the input dataset. The\nhidden layer size is set to the hyper-dimension value of the HDC classifier to match HDC encoder\ndimensions ($D_H$). The weights learned by the hidden layer of the MLP with dimensions d \u00d7 $D_H$ is\nfed to the encoder module after passing through the sgn(.) function ($\\oplus$). Next, the encoder module\nuses the learnt projection matrix to convert all feature vectors in the train set to hypervectors (Hs)\nwith hyper-dimension $D_H$ ($\\circledcirc$). Finally, the training module computes all the class HVs ($\\bigodot$), and\nperforms inference as described in Section 2.1. The MLP's projection matrix allows HDC's encoder\nto better separate data in the hyperspace, leading to a significant improvement in its performance."}, {"title": "4 Results and Discussion", "content": "We evaluate DEBUG-HD on one audio and two image classification tasks. We use the in-distribution\n(ID) and corrupted-in-distribution (CID) versions of 1) SpeechCmd [59] on a 4-layer depthwise-\nseparable model (DSCNN), 2) TinyImagnet [40] on a MobilenetV2 [23] and 3) CIFAR10 [39]\non Resnet-8 from MLPerf Tiny benchmarck suite [6]. For CID datasets, we use CIFAR10-C\n(19 corruptions) and TinyImagenet-C (15 corruptions) from [19]. For SpeechCmd, we use the\naudiomentations library [32] to obtain SpeechCmd-C with 11 corruptions. To avoid the extra costs of\non-device preprocessing and feature engineering, we directly utilize the output of an intermediate\nlayer of the base network as input to HDC. However, this output may not be the most informative,\nwhich limits overall performance of the HDC classifier. Appendix A.1 has more details.\nEvaluation and metrics We evaluate the accuracy of all HDC baselines on the task of detecting\ncorruptions. In our evaluations with prior HDC works, we differentiate them into binary [14, 29, 46,\n28, 30] and non-binary [58, 65, 20] HDC. We use the torch-hd library [18] to implement prior works.\nWe envision a scenario where a finite set of commonly-occurring corruptions [19] affecting inputs are\nused to train the HDC before deployment. Post-deployment, the HDC classifier is invoked only when\nthe model monitoring mechanism detects an accuracy drop in the base network to identify corruption\ncausing the drop. To balance resource usage and accuracy, we use hyper-d values of 300, 400 and\n300 for CIFAR10, T-imgnet and SpCmd respectively. See Appendix A.4 for more details.\nHDC Results: Table 3a reports the top-1 accuracy of all baselines. DEBUG-HD outperforms all\nbinary HDC methods across evaluated datasets, achieving an average accuracy improvement of\n12% over the best-performing binary HDC work, highlighting the effectiveness of the MLP-assisted\nencoding scheme. Surprisingly, Vanilla often surpasses many state-of-the-art binary HDC works.\nFurther analyses reveal that unlike Vanilla and DEBUG-HD's single-pass training, binary baselines"}, {"title": "5 Conclusion", "content": "We present DEBUG-HD, a resource-efficient approach for on-device debugging in TinyML devices\nusing HDC. DEBUG-HD leverages an improved HDC encoding technique, learned through neural\nnetworks, to outperform previous binary HDC methods in identifying corruptions in inputs across\nvarious datasets. With HDC's inherent fault tolerance [43] alongside the proposed enhanced encoding,\nDEBUG-HD strengthens TinyML reliability. This work underscores HDC's potential as a robust\nfoundation for integrating diagnostics in TinyML, facilitating advancements in model reliability and\nperformance through innovative, real-time, on-device debugging solutions.\nLimitations: HDC methods are generally outperformed by conventional neural networks (NNs) and\noften require specialized parallel hardware, such as FPGAs, to fully leverage their computational\nefficiency [51]. Their performance is also influenced by the number of corruptions to identify;\nhowever, it can improve if developers narrow the focus to specific corruptions (see Appendix A.5).\nAdditionally, the accuracy gain from MLP-assisted encoding is notable only in smaller hyperspaces.\nFor instance, the accuracies of AdaptHD and DEBUG-HD are 0.83 and 0.84, respectively, for hyper-\nd=2000 on TinyImageNet. Finally, we observe that incorporating DEBUG-HD into extremely small\nmodels on toy datasets (e.g., Ward [62], USPS [27], MNIST [41]) incurs significant overhead relative\nto the base model size. Nevertheless, DEBUG-HD operates efficiently with more realistic models."}, {"title": "A Appendix", "content": "A.1 Dataset details\nIn this section, we describe the details of the datasets and models evaluated.\nA.1.1 Datasets\nIn our evaluations, we use three in-distribution datasets for training all baselines methods we evaluate:\n1) SpeechCmd [59], 2) CIFAR10 [39] and 3) TinyImagenet [40].\nSpeechCommands Speech-Cmd is a collection of short audio clips, each spanning 1 second.\nThe dataset consists of utterances for 35 words and is commonly used for benchmarking keyword\nspotting systems. We train our model (DSCNN) to recognize ten words out of 35: Down, Go, Left,\nNo, Off, On, Right, Stop, Up, Yes. Thus, the number of classes is 10. The audio files in WAV\nformat are preprocessed to compute Mel-frequency cepstral coefficients (mel-spectograms). The\nmel-spectograms are of size 49\u00d710 with a single channel.\nCIFAR10 CIFAR10 dataset consists of 60,000 32\u00d732 rgb images out of which 10,000 images are in\nthe test set. It contains 10 classes and thus 6000 images per class.\nTinyImagenet TinyImagenet is a smaller version of the Imagenet [12] dataset containing 200 classes\ninstead of 1000 classes of the original Imagenet. Each class in TinyImagenet has 500 images in the\ntrain set and the validation set contains 50 images per class. The size of the images are resized and\nfixed at 64\u00d764\u00d73.\nA.1.2\nCorrupted-in-distribution (CID) datasets\nWe use the following corrupted versions of ID: 1) CIFAR10-C and 2) TinyImagenet-C from [19]. For\nboth, we use the corruptions with severity level=5 in our evaluations. All corruptions are drawn from\n4 major sources: noise, blur, weather and digital. We create SpeechCmd-C from the audiomentations\nlibary [32].\nCIFAR10-C CIFAR10-C includes 19 different types of corruptions with 5 severity levels. The\nlist of corruptions are: gaussian_noise, brightness, contrast, defocus_blur, elastic, fog, frost,\nfrosted_glass_blur, gaussian_blur, impulse_noise, jpeg_compression, motion_blur, pixelate, sat-\nurate, shot_noise, snow, spatter, speckle_noise and zoom_blur.\nTinyImagenet-C TinyImagenet-C includes 15 different types of corruptions with 5 severity levels.\nThe list of corruptions are: gaussian_noise, brightness, contrast, defocus_blur, elastic_transform,\nfog, frost, glass_blur, impulse_noise, jpeg_compression, motion_blur, pixelate, shot_noise, snow and\nzoom_blur.\nSpeechCmd-C For keyword spotting (KWS) [64], an audio classification task on the Speech Com-\nmands dataset, we introduce noise and other corruptions to the audio using the audiomentations\nlibrary [32]. We apply the following 11 corruptions: gaussian noise, air absorption, band pass filter,\nband stop filter, high pass filter, high shelf filter, low pass filter, low shelf filter, peaking filter, tanh\ndistortion, time mask and time stretch.\nA.2 Experimental Setup\nWe evaluate three models in our experiments:1) 4-layer DSCNN on SpeechCmd [64], 2) Resnet-8 with\n3 residual stacks from the MLPerf Tiny benchmark suite [6] on CIFAR10 and 3) MobilenetV2 [23]\non TinyImagenet.\nThe 4-layer DSCNN is trained for 10 epochs with a batch size of 100, the Resnet-8 model is trained\nfor 200 epochs with batch size of 32 and the MobilenetV2 model is trained for 200 epochs with a\nbatch size of 128. All models are trained with Adam optimizer with momentum of 0.9 and an initial\nlearning rate of 0.001, expect DSCNN on SpeechCmd which uses an initial learning rate of 0.0005.\nThe learning rate is decayed by a factor of 0.99 every epoch for image classification datasets, and we\nfollow a step function for SpeechCmd that reduces learning rate by half every 2 epochs.\nWe train the MLP used to learn the encoding matrix for 20 epochs with a batch size of 256."}, {"title": "A.3 Dataset creation for HDC classifier", "content": "Since the HDC classifier aims to distinguish between different types of corruptions in the input images,\nthe natural input to the classifier would be the images themselves. However, the high dimensionality\nof the input images (e.g., 12K for TinyImageNet) significantly increases the resource usage of the\nHDC classifier. One potential solution is to reduce the dimensionality of the input images, but this\nintroduces additional processing costs that may impact the execution of the base network.\nTo avoid such preprocessing or feature engineering overhead, we propose tapping into an appropriate\nintermediate layer of the base model directly. This approach offers two key advantages: 1) as the\ninput image passes through the base network, its dimensionality is reduced, which in turn minimizes\nthe size of the HDC classifier, and 2) the execution of the base network remains uninterrupted-an\nimportant consideration for safety-critical applications.\nThe dataset for the HDC classifier is created as follows.\n1) First, we iterate through all the corrupted-ID images for each dataset by passing each corrupted\nimage through the pretrained base network. The base network has been trained only on ID.\n2) Second, we select an appropriate intermediate layer of the base network, and record the output of\nthis intermediate layer as we iterate through the corrupted images. We use this to create our train and\ntest datasets for the HDC classifier denoted by $D_{HDC-train}$ and $D_{HDC-test}$ respectively.\n3) Next, we use $D_{HDC-train}$ to train the HDC classifier as described in Section 3, and use\n$D_{HDC-test}$ for validation/testing.\nA.3.1 Selecting an appropriate intermediate layer\nSince tinyML models often have only a few layers, the number of intermediate layers available for\ntapping is limited. Therefore, we perform an exhaustive search. We construct $D_{HDC-train}$ and\n$D_{HDC-test}$ as described above by taping into each intermediate layer's output. Next, we train and\ntest a Vanilla HDC using each version of the dataset. We select the intermediate layer whose dataset\nyields the highest validation accuracy. For SpCmd using a 4-layer DSCNN, the tapping location is\nafter the first depthwise layer. For CIFAR10 using a 3-residual stack Resnet, the best location is\nafter the third residual stack. In the case of TinyImageNet, which employs a larger MobileNetV2\nmodel with 17 inverted residual blocks, we refine our search using a binary search approach. This\nsystematic method helps us narrow down the most informative layer by evaluating performance at\nprogressively finer intervals. For MobileNetV2, we find that the output of the 5th inverted residual\nblock is the most informative. The number of features in the HDC datasets thus obtained are 256,\n128 and 192 for CIFAR10, SpCmd and T-imgnet respectively."}, {"title": "A.4 Evaluation details", "content": "We distinguish the HDC baselines we compare against into two types: 1) Binary HDC meth-\nods: LeHDC [14], AdaptHD [29], CompHD [46], QuantHD [28] and SparseHD [30], and 2) Non-\nbinary HDC methods: DistHD [58], NeuralHD [65] and OnlineHD [20].\nSimulating model accuracy drop In our evaluations, we trigger the proposed debug tool only when\nthe model monitoring mechanism detects an accuracy drop. Ghanathe and Wilton [15] proposes\na resource-efficient accuracy monitoring mechanism for tinyML models with minimal resource\noverhead. We follow the procedure outlined in [15] to simulate the accuracy drop detection. First,\nwe use the base network to iterate over all in-distribution (ID) samples using a sliding window of\nsize 100, calculating accuracy over the past 100 input samples. The resulting accuracy distribution\nis denoted as $A_{ID}$, with mean \u00b5ID and standard deviation OID. Next, we create ID+CID datasets\nby appending ID with all corrupted-ID versions. We then iterate over the ID+CID datasets using\nthe base network and the same sliding window approach. The HDC classifier is only invoked when\n$A_{SW} < \u00b5_{ID}-3\u00b7O_{ID}$, where $A_{SW}$ is accuracy of sliding window. Once invoked, the HDC classifier\nreads the output of the predetermined intermediate layer and identifies the type of corruption that\ncaused the model accuracy to drop."}, {"title": "A.4.1 MLP-assisted encoding", "content": "A HDC classifier for debugging in tinyML devices necessitates maintaining the bipolar nature of\nthe network while using hyper-dimensional values of less than 500 to ensure that the associated\noverhead remains practical. However, we observe that in a lower hyper-dimensional space (few\nhundreds of dimensions), the random projection matrix is inadequate in separating the data in the\nhyperspace, which in turn significantly limits the performance of the HDC model. This phenomenon\nis illustrated in Figure 2. In our initial experiments, we observe two things: 1) a simple 2-layer MLP\nhas a close resemblance to a HDC model and 2) the MLP is able to achieve significantly higher\naccuracy compared to its HDC counterpart. This demonstrates that the single hidden layer of the\nMLP is able to project data into the hyperspace much more efficiently than the random-projection\nmatrix. Since the hidden layer functions similar to the HDC encoder, we import the weight matrix\ninto the encoder module of the HDC classifier, which proves to be crucial in extracting maximum\nperformance from the HDC classifier, especially when working with low hyper-d values (< 1000).\nWe made several modifications to the MLP architecture used for learning the encoding matrix. In\none iteration, we included a bias vector followed by an activation function. However, we found that\nthe encoding matrix was negatively impacted by the additional assistance from the bias vector and\nactivation function. As a result, we adopted a configuration where the hidden layer output has neither\na bias vector nor an activation function."}, {"title": "A.5 Additional insights into debugging", "content": "The HDC classifiers are generally outperformed by conventional NNs due to their inherent architecture\nlimitations and binarized implementations. Another important factor impacting HDC performance\nis the presence of similar corruptions. Table 3 and Table 1 present the structural similarity index\nmeasure (SSIM) matrices for Speech Commands (SpCmd) and CIFAR10 corruptions, respectively.\nThese tables indicate the degree of similarity between each pair of corruptions. To compute the SSIM\nbetween two corruption types, we randomly sample 50 images from each type and calculate the mean\nSSIM between them. Each sample pair consists of identical images, each corrupted by a different type\nof corruption. A SSIM value of 0 indicates no similarity between images, while a value of 1 signifies\ncomplete identity. The cells in Tables 3 and 1 are color coded for identification of similar corruptions.\nIn these tables, a yellow cell indicates SSIM between 0.5 and 0.75 (moderate similarity), and a green\ncell indicates SSIM>0.75 (high similarity). As seen in the SSIM table for SpCmd (Table 3), there are\nmany corruptions with high degree of similarity, which limits the performance of even a conventional\nMLP classifier.\nThe SSIM tables can help developers narrow down the number of corruptions to detect on-device,\nwhich in turn leads to a massive accuracy improvement. For example, when we reduce the number of\nCIFAR10 corruptions from 19 to 12, we immediately improve the top-1 accuracy from 0.68 to 0.79.\nThe SSIM tables help the developers eliminate similar corruption types from the debugging task. For\nexample, in the SSIM table for CIFAR10 (Table 1), we observe that defocus blur (DB) and gaussian\nblur (GB) are highly similar. Therefore, the developer can choose to remove either of them from the\nclassification task to obtain a higher precision in debugging."}, {"title": "A.5.1 Effect of removing corruptions from debugging", "content": "Demonstrating the usability of a debugging tool is a complex task, as each developer may use it\ndifferently. We attempt to explore a plausible debugging scenario, where corruptions are removed\nfrom the HDC classification task to improve accuracy.\nWhile removing corruptions from the classification may potentially reduce the coverage of the\ndebugging task, we find that this might not always be the case. For example, consider a scenario\nwhere Gaussian blur (GB) is removed from the classification task, but defocus blur (DB) remains.\nPost-deployment, if the input is corrupted by GB, the HDC classifier might still identify it, though\nmisclassifying it as DB. In response, the developer may choose to retrain the model to handle DB\ncorruption. Interestingly, this retraining phase may indirectly improve the model's ability to handle\nGB corruption as well, due to the similarity between DB and GB. We empirically verify this claim\nby showing that after retraining on DB, the model exhibits improved performance on GB-corrupted\ninputs, even though GB was not explicitly included during retraining. This is also supported by\nthe findings of Kindermans et al. [38], where they developed an explanation for neural networks.\nAccording to Kindermans et al. [38], input data consists of two components: the signal, which\ncontains task-relevant information, and the distractor, which obfuscates the signal. During training,\nneural networks learn to filter out the distractor to recover the signal. Applied to our scenario, if\nthe network learns to remove DB to recover the signal, it should similarly be able to remove GB.\nThus, developers can safely exclude similar corruptions from the classification task, improving HDC\naccuracy with minimal impact on the debugging process."}]}