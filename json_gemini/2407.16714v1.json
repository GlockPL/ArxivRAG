{"title": "Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation", "authors": ["Tao Meng", "Fuchen Zhang", "Yuntao Shou", "Hongen Shao", "Wei Ai", "Keqin Li"], "abstract": "Abstract\u2014Since Multimodal Emotion Recognition in Conver- sation (MERC) can be applied to public opinion monitoring, intelligent dialogue robots, and other fields, it has received extensive research attention in recent years. Unlike traditional unimodal emotion recognition, MERC can fuse complementary semantic information between multiple modalities (e.g., text, audio, and vision) to improve emotion recognition. However, previous work ignored the inter-modal alignment process and the intra-modal noise information before multimodal fusion but directly fuses multimodal features, which will hinder the model for representation learning. In this study, we have developed a novel approach called Masked Graph Learning with Recursive Alignment (MGLRA) to tackle this problem, which uses a recurrent iterative module with memory to align multimodal features, and then uses the masked GCN for multimodal feature fusion. First, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Second, we build a recurrent iteration module with a memory function, which can use com- munication between different modalities to eliminate the gap between modalities and achieve the preliminary alignment of features between modalities. Then, a cross-modal multi-head attention mechanism is introduced to achieve feature alignment between modalities and construct a masked GCN for multimodal feature fusion, which can perform random mask reconstruction on the nodes in the graph to obtain better node feature repre- sentation. Finally, we utilize a multilayer perceptron (MLP) for emotion recognition. Extensive experiments on two benchmark datasets (i.e., IEMOCAP and MELD) demonstrate that MGLRA outperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotions affect every aspect of our lives through thoughts or actions, and conversation is the primary way to express them [1]\u2013[7]. Therefore, it is crucial to understand emotions in conversation accurately, and the results can be widely used in fields such as intelligent dialogue [8] and intelligent recommendations [9]. However, in actual dialogue scenes, the emotions expressed by the speaker are not only related to the content of the speech but also closely related to his tone and expression. Multimodal Emotion Recognition in Conversation (MERC) task aims to use the utterance (text, audio) and visual (expression) information in the conversation to identify the speaker's emotion. Compared with traditional unimodal emotion recognition in conversation, MERC can improve the instability of emotion analysis by fusing richer multimodal semantic information [10]\u2013[17]. Therefore, the key to advancing MERC lies in the effective alignment and fusion of text, audio, and visual information to achieve a collaborative understanding of cross-modal emotional semantics. [18]\u2013[22] In response to the above challenges, many researchers have made significant efforts in the field of conversational emotion recognition. For instance, Liu et al. [23] first used a convolutional neural network (CNN) to learn local features of speech signals, then used a recurrent neural network (RNN) to capture long sequence features, and finally fused the two types of features to achieve emotion recognition. Lian et al. [24] propose a transformer-based dialogue emotion recognition model called CTNet, which can adaptively learn and capture important emotional features from input dialogues. Due to the excellent performance of graph neural networks (GNN) in rela- tional modeling, Ghosal et al. [10] converted the conversation history into a graph data structure and effectively extracted the emotional features in the conversation history through GNN. This method can not only be used for emotion recog- nition tasks but also can be applied to other dialogue-related tasks. Hu et al. [11] use GNN to model speaker-to-speaker relationships, effectively exploiting multimodal dependencies and speaker information. Yuan et al. [25] used the relational bilevel GNN to model MERC, which reduced the redundancy of node information and improved the capture of long-distance dependencies.\nHowever, only considering multimodal fusion is not com- plete enough for MERC. The alignment of semantic features before multimodal fusion is also a difficult challenge for MERC, which affects the fusion performance to some extent. Alignment is often used to unify disparate data from multiple modalities [26], [27]. At the same time, noise reduction processing is indispensable during the alignment process. There are usually two types of noise. (1) As shown in Fig. 1 (a), features with different granularities may mean incon- sistent emotional polarity. Character visual angles represent positive emotions, while different words represent neutral or negative emotions. (2) The original features extracted from the corresponding single modality using different pre-trained models may contain some missing, redundant, or even wrong information. Previous researchers have done a lot of work on this issue. For example, Chen et al. [28] proposed a gated multimodal embedding LSTM, which can filter noise information while processing noisy modality data and achieve finer fusion between input modalities. Xu et al. [29] use the attention mechanism to use an adaptive alignment strategy in the alignment layer, which can automatically learn alignment weights in the process of frame and word alignment in the time domain. Xue et al. [30] proposed a multi-level attention map network to filter intra-modal or inter-modal noise to achieve fine-grained feature alignment. However, as shown in Fig. 1 (b), these methods have the following limitations: (1) The alignment process is often completed in one go, lacking an iterative alignment process, resulting in the model being unable to complete fine-grained alignment. (2) These methods do not allow the model to observe representations extracted from other modalities and realign them during the alignment process and do not consider contextual dialogue relationships during the alignment process, resulting in poor performance when dealing with the first type of noise.\nTo align and fuse semantic information from multiple modalities, we carry out a Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation (MGLRA), which uses an iterative alignment mechanism to strengthen the modalities' consistency gradu- ally, as shown in Fig. 1 (c). First, MGLRA uses different feature encoders to represent modality-specific features. Sec- ond, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Third, MGLRA employs a novel feature alignment method based on recursive memory- augmented cross-modal attention, which iteratively refines and aligns unimodal features by observing memory modules from other modalities. Fourth, we introduce a variational GCN- based fusion method to fuse unimodal features to produce a robust multimodal representation. Finally, multimodal rep- resentation is directly used in MERC to generate emotion classification. The contributions of our work are summarized as follows:\n\u2022 We propose a novel Masked Graph Learning with a Recurrent Alignment (MGLRA) model to refine uni- modal representations of semantic information from mul- tiple modalities. MGLRA uses a memory mechanism to iteratively align semantic information from multiple modalities, making it more robust in noisy scenes and scenes lacking modal information.\n\u2022 We introduce a cross-modal multi-head attention mecha- nism to explore interactive semantic information among multiple modalities and expand the receptive field of contextual information.\n\u2022 We utilize a simple and effective GCN with a random masking mechanism to fuse complementary semantic in- formation among multiple modalities without introducing extra computation.\n\u2022 Extensive experiments are conducted on two widely used benchmark datasets (i.e., IEMOCAP and MELD) to verify the effectiveness of the proposed model. The experimental results show that the model proposed in this paper is superior to the existing comparison algorithms regarding accuracy and F1-score.\nWe organize the subsequent sections of this paper as fol- lows: Section II presents related work on MERC; Section III introduces the model's feature extraction part and the mathematical part's definition and interpretation; Section IV describes the details of our proposed model; Sections V and VI present and analyze our experimental results, and the conclusion is arranged in Section VII."}, {"title": "II. RELATED WORK", "content": "In this section, we mainly introduce the research in emotion recognition, the technology of alignment mechanism related to our research, and the latest related work of GCN.\nA. Multimodal Emotion Recognition in Conversation\nConversation analysis and speaker relationship modeling are crucial in emotion recognition tasks. As demonstrated in [31] and [32], previous studies have deeply explored the relation- ship between emotion and social relevance in conversations and highlighted the dynamic emotional issues arising in human interactions. These interactions form complex interrelation- ships that require dynamic consideration in model design. To this end, DialogueRNN [33] adopts a recurrent neural network (RNN) and an attention mechanism to automatically learn long-term dependencies and dynamic interaction pat- terns between speakers, which shows excellent performance. DialogueGCN [10] transfers and aggregates information on the nodes and edges of the graph structure through graph convolutional neural network (GCN), more effectively handles long-term dependencies and multi-round dialogue interactions, and better captures the global structure and context of dia- logues. Inspired by the high-performance models in multi- modal community tasks [34]\u2013[37], many current models focus on the fusion process to solve the multi-modal emotion recog- nition (MERC) task. For example, LR-GCN [38] introduces a latent relationship representation learning mechanism to better represent the interactive relationships between nodes and edges by learning the latent connections between nodes, thereby improving performance.\nHowever, these methods mainly focus on the fusion process of context information and speaker information to improve the performance of the fusion process but ignore the semantic information alignment process before fusion. This directly limits the effectiveness of the model during fusion.\nB. Alignment Mechanism\nThe alignment mechanism matches the semantic features from different modalities so that the emotional expressions of other modalities are consistent. Multimodal emotion recogni- tion mainly uses context-order-based and enforced word-level alignments.\nThe alignment of the contextual order relationship can be used to compare the similarity or difference between two sentences while aligning the relationship between words and frames to achieve the fusion between modalities. Liu et al. [39] used the attention mechanism to establish the alignment between vision and audio modalities, and corresponding ele- ments can be found in the two modalities. This correspondence is called bi-directional attention alignment. Li et al. [40] pro- posed an Inter-modality Excitement Encoder (IMEE), which can learn the refined excitability between modalities, such as vision and audio modalities. Chen et al. [41] introduced a cross-modal time consistency module, which used a Bi-LSTM model to learn the time dependence between vision and audio modalities, ensuring that the emotional prediction results of the two modalities at the same time step are consistent.\nEnforced word-level alignment in tasks with multiple modal inputs by providing information in the decoder corresponding to words or regions in different modalities; forced alignment between other modalities and output sequences is achieved. Gu et al. [42] proposed a method for multimodal emotion analysis using a hierarchical attention mechanism and word- level alignment. Romanian et al. [43] implemented a tech- nique for detecting depression using word-level multimodal fusion, which mainly relies on time-based recursive methods to achieve word-level modal alignment, but word-level pro- cessing also brings high computational complexity, which is something to consider.\nDespite the success of the above methods, they are all local alignment mechanisms, ignoring the interaction of global context information, which leads to limited context awareness of the model.\nC. Graph Convolutional Network\nThe rise of graph neural networks (GNN) has attracted researchers over the past few years. It has achieved remark- able success in research areas such as semantic segmen- tation, object detection, and knowledge graphs [44], [45]. The graph convolutional network (GCN) proposed by Kipf et al. [46] is central to this success. The technology is similar to traditional convolutional neural networks (CNN), using convolutions to pass information through the network and capture comprehensive data set features. Its efficiency stems from leveraging unlabeled data for model augmentation, achieved using a simple similarity matrix. Veljkovic et al. [47] recently developed a graph attention network (GAT). This model innovatively uses the attention mechanism to dynamically understand the graph structure, thereby achieving accurate modeling and reasoning of complex relationships in the graph. This enables superior performance for tasks such as node classification and inference. The progress of GCN has achieved significant breakthroughs, motivating us to apply GCN to address challenges in MERC.\nAlthough the effectiveness of deep learning methods has been proven, the MERC task still needs to improve, such as semantic consistency between different modalities and the complexity of effectively fusing multi-modal features. To address these challenges, our paper introduces the MGLRA framework. This novel framework adopts a recurrent alignment strategy enhanced by a memory component to ensure seman- tic alignment across modalities before fusion. Furthermore, we utilize a multi-head attention mechanism to explore the relationships between modules in detail and combine it with a computationally efficient GCN for effective fusion."}, {"title": "III. PRELIMINARY", "content": "In this section, we introduce various details of our work from a data flow perspective and how we extract multimodal features of all utterances from the dataset.\nA. Multimodal Feature Extraction\nThe reason for unimodal-specific feature extraction methods is twofold. First, since each modality has its unique semantic features, it is best to use its special feature extractor for each modality to capture salient representations adequately. Second, this unimodal feature extraction method allows state-of-the-art feature extractors to obtain better unimodal semantic infor- mation for transfer learning. The feature extraction methods and different processing methods of all modalities used are as follows:\n1) Text Feature Extraction: Emotion keywords in text con- tent play a crucial role in emotion recognition, so extract- ing rich lexical features from text content is a fundamental challenge in multimodal emotion recognition. In addition, the contextual semantic information composed of sentences as the basic unit also provides many guiding clues for emotion recog- nition. In multimodal emotion recognition research, ROBERTa often captures local semantic and global contextual features. In this paper, following previous work [10], [33], [48], we also adopt the RoBERTa model to extract and represent text features. The final processed text features are represented as $x_t$, and $x_t \\in \\mathbb{R}^{d_t}$, $d_t = 100$. The sequence of text features is denoted by $X_t$.\n2) Audio Feature Extraction: In determining the speaker's emotional state, audio features play a crucial role in in- formation. So we use openSMILE to extract audio features following previous work [10], [33], [48]. It is a highly open- source software for extracting audio features, mainly used in emotion recognition, emotion computing, music information processing, etc. It can extract many vectors in audio, including MFCC, frame intensity, frame energy, pitch, etc. The final processed audio features are represented as $x_a$, and $x_a \\in \\mathbb{R}^{d_a}$, $d_a = 100$. The audio feature sequence is denoted by $X_a$.\n3) Vision Feature Extraction: Since the facial expression features can best reflect the emotional changes at a particular moment, we use 3D-CNN to extract the expressive facial features of the interlocutor to enhance the extraction of uni- modal features of the vision to pursue better multimodal fusion effect following previous work [10], [33], [48]. In addition to extracting the details of relevant features from many key image frames, 3D-CNN can extract spatiotemporal features jumping across multiple image frames. Doing so makes it easy to identify critical emotional states, such as smiling or depression. The final processed vision features are represented as $x_v$, and $x_v \\in \\mathbb{R}^{d_v}$, $d_v = 512$. The vision feature sequence is denoted by $X_v$.\nB. Problem Definition\nGiven a dataset S of multimodal dialogues with multiple characters, through our designed preprocessing process, we get input features $X_m = (x_{(m,1)}, x_{(m,2)},...,x_{(m,l_m)})$, $M \\in \\{a, v, t\\}$, $a, v, t$ represent audio, vision, and text, respectively. Here, r represents the original feature, and $l_m$ means the sequence length. Given these sequences $X_m$, the final task is to determine a deep fusion network $F(X_m)$ such that the output $\u0177_m$ is getting closer and closer to the target $y_m$, which can be achieved by minimizing the loss function. The loss function of the model is defined as shown in Eq. (1):\n$\\min_{F}\\frac{1}{b}\\sum_{m=1}^{b} L(y_m = F (X_m), Y_m)$ (1)\nwhere b represents the batch size, $y_m$ is the true emotion of the utterance and $\u0177_m$ is the predicted emotion of the model."}, {"title": "IV. METHODOLOGY", "content": "This section proposes a novel Masked Graph Learning with Recurrent Alignment (MGLRA) to improve emotion classi- fication performance for multimodal emotion recognition in conversation. Fig. 2 shows the overall architecture of MGLRA. This method consists of the following parts: data preprocess- ing, multimodal iterative alignment, multimodal fusion with masked GCN, and emotion classifier. (1) Data preprocessing: Due to the different data structures of other modalities, we use ROBERTa, openSMILE, and 3D-CNN to extract text, audio, and visual features in the data preprocessing stage. (2) Multimodal iterative alignment: First, a graph attention filtering mechanism is developed in the multimodal feature alignment stage to adequately filter redundant noise in multi- modal features. Second, to enhance the expressive ability of the original multimodal features, the memory-based recursive feature alignment (MRFA) was created, and this module was used to gradually realize the preliminary alignment of the three modalities using the memory iteration mechanism. Third, we develop cross-modal multi-head attention to discover shared information and complementary relationships between modal- ities to understand and express emotions better. (3) Multi- modal fusion with masked GCN: For the fusion problem of multimodal emotion recognition in conversation, a simple and effective masked GCN is used for multimodal feature fusion, which achieves good performance without bringing more parameters. (4) Emotion classifier: We used MLP for emotion classification in the emotion classification stage.\nA. Multimodal Iterative Alignment\nA multimodal iterative alignment module is designed to im- prove the fusion effect of multimodal features, which provides aligned and robust unimodal representations for downstream fusion tasks. Specifically, due to the gap in the semantic information between the three modalities and the different peaks of their data distributions, it is difficult for the model to capture the complementary semantic information among the three modalities. Therefore, we cyclically align unimodal features before downstream task fusion to produce a significant unimodal representation. This unimodal feature alignment mechanism is also reflected in the multisensory cognitive system of animals. Technically, the multimodal iterative align- ment module includes a graph attention filtering mechanism, a memory-based recursive feature alignment method, and cross-modal multi-head attention. Specifically, we first exclude redundant or wrong information within or between modalities through a graph attention filtering mechanism. Then, the memory-based recursive feature alignment is used to achieve the preliminary alignment of features between modalities. Finally, the final alignment of inter-modal features is achieved using cross-modal multi-head attention.\n1) Graph Attention Filtering Mechanism: Inspired by the Multi-Level Attention Graph Network (MAMN) [30], we use a graph attention filtering mechanism to filter out some noise in raw multimodal features, which may contain wrong, redundant, or missing information. The difference is that we assign higher weights to the more critical multimodal features rather than multi-granular features. After such a process, the representations of all multimodal features are re-optimized. Correspondingly, the weighted average based on the attention mechanism relatively emphasizes or weakens the role of a modality. This can lead to more accurate emotional judgments. The graph structure must be constructed before utilizing the graph attention filtering mechanism for intra-modal or inter-modal noise reduction. For the input text, visual, and audio modality features, we first feed them into a long short- term memory network (LSTM) to extract contextual semantic information. The formula for LSTM is defined as follows:\n$X_t^c, X_v^c, X_a^c = LSTM (X_t^r, X_v^r, X_a^r)$ (2)\nwhere r represents the original feature, and c represents features with contextual information.\nThen, we construct a graph structure for the captured multi- modal context information. The specific graph construction process is as follows. As shown in Fig. 2, the central node of the graph is a multimodal feature node, represented by brown, and its features are generated by connecting text, audio, and visual features. The first-order neighbors of the central node are unimodal feature nodes, and purple, green, and yellow represent text, speech, and visual feature nodes, respectively. Moreover, the edges represent the relationships between each unimodality and multimodality, such as text- multimodal, audio-multimodal, and visual-multimodal rela- tionships. Finally, the graph attention filtering mechanism filters noise or redundant information within and between modalities by learning information about nodes and edges and assigning different weights to different nodes.\nThe input of the graph attention filtering mechanism is the edge relation category matrix $C\\in \\mathbb{R}^{3\\times T}$ and the node eigenvalue matrix $V \\in \\mathbb{R}^{4\\times P}$, where T is the dimension of each relation type embedding, and P is the dimension of each eigenvalue. The relation category C contains three ba- sic semantic features: text-multimodal, audio-multimodal, and visual-multimodal levels. The eigenvalue matrix V contains four kinds of semantic features: multimodal features $X_m$, text features $X_t$, audio features $X_a$, and visual features $X_v$.\nIn order to filter the noise or redundant information of each node, the relationship degree between feature node pairs is represented by $C_{ijk}$, which is defined in Eq. (3):\n$C_{ijk} = W_{ijk} [V_i ||V_j ||C_k]$ (3)\nwhere $W_{ijk}$ represents the linear transformation matrix ob- tained by learning in the network, || represents cascade oper- ation. In particular, $V_i, V_j$, represents the i-th row and j-th column of the value matrix, and $C_k$ represents the k-th row of the edge relation category matrix.\nThe purpose of filtering and enhancing the original semantic features is achieved by using the attention mechanism to assign different weights to different multimodal feature vectors. The attention weight calculation formula is as follows:\n$A_{ijk} = \\frac{exp (C_{ijk})}{\\sum_{q\\in Q_i} \\sum_{r\\in R_{iq}} exp (C_{iqr})}$ (4)\nwhere $Q_i$ represents the neighbors of feature node $v_i$, and $R_{iq}$ represents the relationship type between feature node pair $v_i$ and $v_q$. For the feature node $v_i$, its filtering feature is the sum of each pair of representations weighted by their attention weights:\n$h_i = \\sum_{j\\in Q_i}\\sum_{k\\in R_{ij}} A_{ijk}C_{ijk}$ (5)\nEach feature vector is updated through all the above computation steps, resulting in three new unimodal features $X_t^h, X_v^h, X_a^h$. Furthermore, the noise in each mode has been reduced.\n2) Memory-based Recursive Feature Alignment (MRFA):\nA specific flowchart illustrating how we utilize MRFA and crossmodal multi-head attention for alignment is depicted in Fig. 3. In MRFA, each modality will create a memory block \u03b4 that stores unimodal semantic features. After recursive strengthening of the memory blocks, a feature sequence $X_m^h$ of size ($l_m \u00d7 d_m \u00d7 b$) is obtained. Here, $l_m$ and $d_m$ sub-tables represent each memory entry's length and original feature dimension, and b is the batch size.\nThen, MRFA uses the semantic features $X_m^h$ extracted from different unimodal feature extractors to generate a memory block by the subsequent recurrent alignment augmentation network. As shown in Eq. (6), $X_m^m$ uses the features $X_t^h$, and $X_v^h, X_a^h$ of the other two modalities for alignment refinement.\n$X_m^m = MRFA(X_t^h, X_v^h, X_a^h)$ (6)\nIn subsequent model runs, MRFA reads the content from the memory repository, continuously refines $X_m^m$ using $X_t^h$, and aligns the unimodal latent semantic information with the other two modalities. These refined unimodal semantic features obtained above will be stored in the memory block to replace the old content in the block.\nMoreover, MRFA uses an attention mechanism for each modality to extract significant semantic features $X_m^R\u2208 \\mathbb{R}^{b\u00d7F_m}$. Here, $F_R$ represents the embedding size of the enhanced unimodal feature. Intra-modality memory attention performs refined feature extraction on the semantic features $X_m^m$ of each entry in the three modal memory blocks. The calculation formula of the attention weight of each memory entry \u03c4 in the memory blocks is as follows:\n$\\mu_{(m,\\tau)} = W_RX^m_{(m,\\tau)}$ (7)\n$\\mathbb{W}_{(m,\\tau)} = \\frac{exp(\\mu_{(m,\\tau)})}{\\sum_{\\tau'=1}^{I_m} exp(\\mu_{(m,\\tau')})}$ (8)\nThen, MRFA utilizes $\\mathbb{W}_{(m,\\tau)}$ to fuse memory block fea- tures to extract the salient unimodal representations in each modality. $W^R$ represents a learnable dynamic parameter. $I_m$ indicates the number of entries.\n$X_m^R = \\sum_{\\tau=1}^{I_m} \\mathbb{W}_{(m,\\tau)} X_{(m,\\tau)}$ (9)\nIn MRFA, we use the lightweight model 1D-CNN to cal- culate the attention weight and set the filter size to 1 to speed up the attention calculation.\n3) Cross-modal Multi-head Attention: First, we apply linear projections to the query $Q \\in \\mathbb{R}^{T_Q \\times d_Q}$, key $K \\in \\mathbb{R}^{T_K \\times d_k}$, and value $V \\in \\mathbb{R}^{T_v \\times d_v}$, n times using distinct linear transforma- tions. Here, n represents the number of heads. $T_Q$, $T_K$, and $T_v$ denote the sequence lengths of the query, key, and value, respectively, representing the number of elements in each sequence. Similarly, $d_Q$, $d_k$, and $d_v$ are used to indicate the feature dimensions of the query, key, and value, respectively, representing the number of features or dimensions in each element of the sequences.\nAfter the basic description, to precisely illustrate how the cross-modal multi-head attention mechanism is applied to data of different modalities, we will use one modality as an example for illustration. Let m represent one of the modalities, such as text, and m' represent one of the other two modalities, such as visual or audio. In this case, we first process the text modality (m) data, applying n different linear transformations to generate the query Q. We then apply n different linear transformations to the other modality (m') data to generate K and V. This way, we can capture cross-modal interaction information between each modality and other modalities. The specific calculation is as follows:\n$Q = Concat(Q\\mathbb{X}_t^h W_Q^{1} ..., Q\\mathbb{X}_t^h W_Q^{n})$ (10)\n$K = Concat(K\\mathbb{X}_v^R W_K^{1} ..., K\\mathbb{X}_v^R W_K^{n})$ (11)\n$V = Concat(V\\mathbb{X}_a^R W_V^{1} ..., V\\mathbb{X}_a^R W_V^{n})$ (12)\nwhere $W \\in \\mathbb{R}^{d_Q\\times d_m}$, $W_K \\in \\mathbb{R}^{d_K\\times d_m}$ and $W_V \\in \\mathbb{R}^{d_v \\times d_m}$ are learnable parameters in the fully connected layer, and $d_m$ is the output dimension.\nThen, we use the dot-product attention to compute queries $QW \\in \\mathbb{R}^{T_Q\\times d_m}$, keys $KW_K \\in \\mathbb{R}^{T_K \\times d_m}$, and values $VWV \\in \\mathbb{R}^{T_v \\times d_m}$ on each projection, and get attention scores for feature vectors composed of different relations. The formula is defined as follows:\n$head_i = softmax (\\frac{(QW)(KW^*)^*}{\\sqrt{d_m}}) (VW)$ (13)\nNext, the outputs of the attention function $head_i$, i \u2208 [1,n] are concatenated to form the final value $X_m^{head}$, which is calculated as follows:\n$X_m^{head} = Concat(head_1, head_2, ...,head_n)$ (14)\nHere, $head_i \\in \\mathbb{R}^{T_Q\\times d_m}$ and $X_m^{head} \\in \\mathbb{R}^{T_Q\\times nd_m}$. Finally, we employ a GRU model to capture the correlation between feature alignments at each iteration. The formula is defined as follows:\n$X_m = GRU (X_m, X_m^{head})$ (15)\nB. Multimodal Fusion with masked GCN\nIn this section, we introduce the embedding of speaker information, the process of graph construction, and the masked graph mechanism.\n1) Speaker Embedding: Some existing GCN models do not consider embedding learning of speaker information when constructing graphs, resulting in the inability to use speaker information to model potential connections within or between speakers. To solve the above problems, this paper embeds speaker information into GCN. Assuming that there are N dialogue characters in the data set, then the size of our speaker embedding is also N. We show the speaker information embedding process in Fig. 2. The original speaker information can be represented by the vector Si, and $X_S$ represents the speaker's embedding. The calculation process is as follows:\n$X_S = WS_i + X_m^{head}$ (16)\nHere, W represents a learnable weight matrix. After the above process, we embed the speaker information in GCN modeling with additional information about the speaker.\n2) Graph Construction: The graph construction process consists of node representation, edge connection, and edge weight initialization. The following will introduce each in detail.\na) Node Representation: We form a graph of text, visual, and audio data, and it is expressed as $G_m = \\{V_m, A_m, E_m\\}$. Where $V_m$ represents the node sets, $A_m \\in \\mathbb{R}^{|V_m|\\times |V_m|}$ is the adjacency matrix, and $E_m$ represents the edge sets. Any node $v_m \\in V_m$ in the graph contains a sentence in modalities. At this time, each sentence node $v_n$ in the fusion graph contains the semantic features $X_S \\in \\mathbb{R}^{d_m}$ from the three modal fusions.\nb) Edge Connection: In the same dialogue, we assume that there are explicit or latent connections between arbitrary sentences. Therefore, in the graph constructed in this study, any two nodes in the same modality in the same dialogue are connected. Furthermore, each node is also connected to nodes in the same conversation in different modalities due to the complementarity of the same discussion among other modalities.\nc) Edge Weight Initialization: The graph designed in this study has two different types of edges. (1) the two nodes connected by the edge come from the exact modal; (2) the two nodes connected by the edge come from two different modes. In order to capture the similarity of node representations, we employ degree similarity to determine the edge weight. The significance of an edge connecting two nodes is directly proportional to their similarity. This implies that nodes with higher similarity exhibit more crucial information interaction between them.\nTo handle different types of edges, we use different edge weighting strategies. For edges from the same modality, our approach is computed as follows:\n$A_{ij} = 1 - \\frac{arccos (sim(n_i,n_j))}{\\pi}$ (17)\nwhere $n_i$ and $n_j$ represent the feature representations of the n-th and j-th nodes in the graph. For edges from the different modalities, our approach is computed as follows:\n$A_{ij} = \\frac{\\aleph}{\\aleph} - 1 \\frac{arccos (sim(n_i, n_j))}{\\pi}$ (18)\nwhere $\\aleph$ is a hyperparameter.\n3) Graph Learning and Mask Mechanism: Through the analysis and research of GCNs used in the past for multimodal emotion recognition, we found that the fully connected graphs constructed by most previous models have more or less intro- duced redundant or noisy information in the fusion process. The node structure in graph information is overemphasized, and they usually build more edges on GCNs to exploit the topological closeness between adjacent nodes. Based on this situation, we utilize a random masked graph neural network, as shown in Fig. 4, by randomly masking the adjacency matrix to remove excessive noise in the model and improve the robustness of GCN. In addition, our masked GCN can also reduce the number of network parameters of the model on a large scale so that the model avoids the problem of overfitting. We achieve the above effect by exploiting a random mask on the adjacency matrix. Mathematically, a subset $V' \\subset V$ of nodes is sampled, a subset $E' \\subset E$ of nodes is sampled and a mask mark [M] is defined.\n$x_i = \\begin{cases}\nX_i^{[M]}, v_i \\in V'\\\\\nX_i, v_i \\notin V'\n\\end{cases}$ (19)\n$c_i = \\begin{cases}\ne_i^{[M]}, e_i \\in E'\\\\c_i, e_i \\notin E'\\end{cases}$ (20)\nHere $x_i$ and $e_i$, respectively, represent the set of nodes and edges after the mask.\nWe construct a masked GCN, which is used to exploit the semantic complementarity between different modalities further to encode context dependencies. Specifically, given an undirected graph $G_m = \\{V_m, A_m, E_m\\}$, $\\widetilde{P}$ be the renormalized graph laplacian matrix of $G_m$:\n$\\widetilde{P} = \\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}}$\n$= (D+I)^{-\\frac{1}{2}}(A^{[M]} + I)(D + I)^{-\\frac{1}{2}}$ (21)\nHere, $\\widetilde{P}$ represents a learnable weight matrix, $\\widetilde{A}$ represents the neighbor weight matrix, $A^{[M]}$ represents the mask matrix, D represents the diagonal matrix, and I represents the identity matrix.\nC. Emotion Classifier\nUsing the masked GCN, we fuse the semantic information from multiple modalities to obtain the semantic representation of various modalities. Then we input the feature $\\widetilde{P}$ that combines multiple modal semantic information into an MLP with a fully connected layer, then use the RELU activation function for nonlinear activation, and normalize the feature information $P_i$ of the hidden layer through the Softmax function:\n$l_1 = RELU(W_1\\widetilde{P} + b_1)$ (22)\n$P_i = Softmax (W_{smax}l_1 + b_{smax})$ (23)\nHere, $W_1$ and $W_{smax}$ represent a learnable weight matrix. Finally, we use the argmax function to match $P_i$ with the emotional label $\u0177$ of the utterance and use the formula to express the process of predicting the emotional label $\\hat{y_r}$ of the utterance as follows:\n$\\hat{y_i} = argmax (P_i[k])$ (24)\nThe entire inference process of the MGLRA pseudocode is contained in Algorithm 1."}, {"title": "V. EXPERIMENTS", "content": "This section introduces two commonly used datasets for multimodal emotion recognition and the evaluation indicators of related experiments. We show our setup and experimental procedure on these two datasets and discuss and```json\n{\n  \"title\": \"Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation\"", "authors": ["Tao Meng", "Fuchen Zhang", "Yuntao Shou", "Hongen Shao", "Wei Ai", "Keqin Li"], "abstract": "Abstract\u2014Since Multimodal Emotion Recognition in Conver- sation (MERC) can be applied to public opinion monitoring, intelligent dialogue robots, and other fields, it has received extensive research attention in recent years. Unlike traditional unimodal emotion recognition, MERC can fuse complementary semantic information between multiple modalities (e.g., text, audio, and vision) to improve emotion recognition. However, previous work ignored the inter-modal alignment process and the intra-modal noise information before multimodal fusion but directly fuses multimodal features, which will hinder the model for representation learning. In this study, we have developed a novel approach called Masked Graph Learning with Recursive Alignment (MGLRA) to tackle this problem, which uses a recurrent iterative module with memory to align multimodal features, and then uses the masked GCN for multimodal feature fusion. First, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Second, we build a recurrent iteration module with a memory function, which can use com- munication between different modalities to eliminate the gap between modalities and achieve the preliminary alignment of features between modalities. Then, a cross-modal multi-head attention mechanism is introduced to achieve feature alignment between modalities and construct a masked GCN for multimodal feature fusion, which can perform random mask reconstruction on the nodes in the graph to obtain better node feature repre- sentation. Finally, we utilize a multilayer perceptron (MLP) for emotion recognition. Extensive experiments on two benchmark datasets (i.e., IEMOCAP and MELD) demonstrate that MGLRA outperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotions affect every aspect of our lives through thoughts or actions, and conversation is the primary way to express them [1]\u2013[7]. Therefore, it is crucial to understand emotions in conversation accurately, and the results can be widely used in fields such as intelligent dialogue [8] and intelligent recommendations [9]. However, in actual dialogue scenes, the emotions expressed by the speaker are not only related to the content of the speech but also closely related to his tone and expression. Multimodal Emotion Recognition in Conversation (MERC) task aims to use the utterance (text, audio) and visual (expression) information in the conversation to identify the speaker's emotion. Compared with traditional unimodal emotion recognition in conversation, MERC can improve the instability of emotion analysis by fusing richer multimodal semantic information [10]\u2013[17]. Therefore, the key to advancing MERC lies in the effective alignment and fusion of text, audio, and visual information to achieve a collaborative understanding of cross-modal emotional semantics. [18]\u2013[22] In response to the above challenges, many researchers have made significant efforts in the field of conversational emotion recognition. For instance, Liu et al. [23] first used a convolutional neural network (CNN) to learn local features of speech signals, then used a recurrent neural network (RNN) to capture long sequence features, and finally fused the two types of features to achieve emotion recognition. Lian et al. [24] propose a transformer-based dialogue emotion recognition model called CTNet, which can adaptively learn and capture important emotional features from input dialogues. Due to the excellent performance of graph neural networks (GNN) in rela- tional modeling, Ghosal et al. [10] converted the conversation history into a graph data structure and effectively extracted the emotional features in the conversation history through GNN. This method can not only be used for emotion recog- nition tasks but also can be applied to other dialogue-related tasks. Hu et al. [11] use GNN to model speaker-to-speaker relationships, effectively exploiting multimodal dependencies and speaker information. Yuan et al. [25] used the relational bilevel GNN to model MERC, which reduced the redundancy of node information and improved the capture of long-distance dependencies.\nHowever, only considering multimodal fusion is not com- plete enough for MERC. The alignment of semantic features before multimodal fusion is also a difficult challenge for MERC, which affects the fusion performance to some extent. Alignment is often used to unify disparate data from multiple modalities [26], [27]. At the same time, noise reduction processing is indispensable during the alignment process. There are usually two types of noise. (1) As shown in Fig. 1 (a), features with different granularities may mean incon- sistent emotional polarity. Character visual angles represent positive emotions, while different words represent neutral or negative emotions. (2) The original features extracted from the corresponding single modality using different pre-trained models may contain some missing, redundant, or even wrong information. Previous researchers have done a lot of work on this issue. For example, Chen et al. [28] proposed a gated multimodal embedding LSTM, which can filter noise information while processing noisy modality data and achieve finer fusion between input modalities. Xu et al. [29] use the attention mechanism to use an adaptive alignment strategy in the alignment layer, which can automatically learn alignment weights in the process of frame and word alignment in the time domain. Xue et al. [30] proposed a multi-level attention map network to filter intra-modal or inter-modal noise to achieve fine-grained feature alignment. However, as shown in Fig. 1 (b), these methods have the following limitations: (1) The alignment process is often completed in one go, lacking an iterative alignment process, resulting in the model being unable to complete fine-grained alignment. (2) These methods do not allow the model to observe representations extracted from other modalities and realign them during the alignment process and do not consider contextual dialogue relationships during the alignment process, resulting in poor performance when dealing with the first type of noise.\nTo align and fuse semantic information from multiple modalities, we carry out a Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation (MGLRA), which uses an iterative alignment mechanism to strengthen the modalities' consistency gradu- ally, as shown in Fig. 1 (c). First, MGLRA uses different feature encoders to represent modality-specific features. Sec- ond, we employ LSTM to capture contextual information and use a graph attention-filtering mechanism to eliminate noise effectively within the modality. Third, MGLRA employs a novel feature alignment method based on recursive memory- augmented cross-modal attention, which iteratively refines and aligns unimodal features by observing memory modules from other modalities. Fourth, we introduce a variational GCN- based fusion method to fuse unimodal features to produce a robust multimodal representation. Finally, multimodal rep- resentation is directly used in MERC to generate emotion classification. The contributions of our work are summarized as follows:\n\u2022 We propose a novel Masked Graph Learning with a Recurrent Alignment (MGLRA) model to refine uni- modal representations of semantic information from mul- tiple modalities. MGLRA uses a memory mechanism to iteratively align semantic information from multiple modalities, making it more robust in noisy scenes and scenes lacking modal information.\n\u2022 We introduce a cross-modal multi-head attention mecha- nism to explore interactive semantic information among multiple modalities and expand the receptive field of contextual information.\n\u2022 We utilize a simple and effective GCN with a random masking mechanism to fuse complementary semantic in- formation among multiple modalities without introducing extra computation.\n\u2022 Extensive experiments are conducted on two widely used benchmark datasets (i.e., IEMOCAP and MELD) to verify the effectiveness of the proposed model. The experimental results show that the model proposed in this paper is superior to the existing comparison algorithms regarding accuracy and F1-score.\nWe organize the subsequent sections of this paper as fol- lows: Section II presents related work on MERC; Section III introduces the model's feature extraction part and the mathematical part's definition and interpretation; Section IV describes the details of our proposed model; Sections V and VI present and analyze our experimental results, and the conclusion is arranged in Section VII."}, {"title": "II. RELATED WORK", "content": "In this section, we mainly introduce the research in emotion recognition, the technology of alignment mechanism related to our research, and the latest related work of GCN.\nA. Multimodal Emotion Recognition in Conversation\nConversation analysis and speaker relationship modeling are crucial in emotion recognition tasks. As demonstrated in [31] and [32], previous studies have deeply explored the relation- ship between emotion and social relevance in conversations and highlighted the dynamic emotional issues arising in human interactions. These interactions form complex interrelation- ships that require dynamic consideration in model design. To this end, DialogueRNN [33] adopts a recurrent neural network (RNN) and an attention mechanism to automatically learn long-term dependencies and dynamic interaction pat- terns between speakers, which shows excellent performance. DialogueGCN [10] transfers and aggregates information on the nodes and edges of the graph structure through graph convolutional neural network (GCN), more effectively handles long-term dependencies and multi-round dialogue interactions, and better captures the global structure and context of dia- logues. Inspired by the high-performance models in multi- modal community tasks [34]\u2013[37], many current models focus on the fusion process to solve the multi-modal emotion recog- nition (MERC) task. For example, LR-GCN [38] introduces a latent relationship representation learning mechanism to better represent the interactive relationships between nodes and edges by learning the latent connections between nodes, thereby improving performance.\nHowever, these methods mainly focus on the fusion process of context information and speaker information to improve the performance of the fusion process but ignore the semantic information alignment process before fusion. This directly limits the effectiveness of the model during fusion.\nB. Alignment Mechanism\nThe alignment mechanism matches the semantic features from different modalities so that the emotional expressions of other modalities are consistent. Multimodal emotion recogni- tion mainly uses context-order-based and enforced word-level alignments.\nThe alignment of the contextual order relationship can be used to compare the similarity or difference between two sentences while aligning the relationship between words and frames to achieve the fusion between modalities. Liu et al. [39] used the attention mechanism to establish the alignment between vision and audio modalities, and corresponding ele- ments can be found in the two modalities. This correspondence is called bi-directional attention alignment. Li et al. [40] pro- posed an Inter-modality Excitement Encoder (IMEE), which can learn the refined excitability between modalities, such as vision and audio modalities. Chen et al. [41] introduced a cross-modal time consistency module, which used a Bi-LSTM model to learn the time dependence between vision and audio modalities, ensuring that the emotional prediction results of the two modalities at the same time step are consistent.\nEnforced word-level alignment in tasks with multiple modal inputs by providing information in the decoder corresponding to words or regions in different modalities; forced alignment between other modalities and output sequences is achieved. Gu et al. [42] proposed a method for multimodal emotion analysis using a hierarchical attention mechanism and word- level alignment. Romanian et al. [43] implemented a tech- nique for detecting depression using word-level multimodal fusion, which mainly relies on time-based recursive methods to achieve word-level modal alignment, but word-level pro- cessing also brings high computational complexity, which is something to consider.\nDespite the success of the above methods, they are all local alignment mechanisms, ignoring the interaction of global context information, which leads to limited context awareness of the model.\nC. Graph Convolutional Network\nThe rise of graph neural networks (GNN) has attracted researchers over the past few years. It has achieved remark- able success in research areas such as semantic segmen- tation, object detection, and knowledge graphs [44], [45]. The graph convolutional network (GCN) proposed by Kipf et al. [46] is central to this success. The technology is similar to traditional convolutional neural networks (CNN), using convolutions to pass information through the network and capture comprehensive data set features. Its efficiency stems from leveraging unlabeled data for model augmentation, achieved using a simple similarity matrix. Veljkovic et al. [47] recently developed a graph attention network (GAT). This model innovatively uses the attention mechanism to dynamically understand the graph structure, thereby achieving accurate modeling and reasoning of complex relationships in the graph. This enables superior performance for tasks such as node classification and inference. The progress of GCN has achieved significant breakthroughs, motivating us to apply GCN to address challenges in MERC.\nAlthough the effectiveness of deep learning methods has been proven, the MERC task still needs to improve, such as semantic consistency between different modalities and the complexity of effectively fusing multi-modal features. To address these challenges, our paper introduces the MGLRA framework. This novel framework adopts a recurrent alignment strategy enhanced by a memory component to ensure seman- tic alignment across modalities before fusion. Furthermore, we utilize a multi-head attention mechanism to explore the relationships between modules in detail and combine it with a computationally efficient GCN for effective fusion."}, {"title": "III. PRELIMINARY", "content": "In this section, we introduce various details of our work from a data flow perspective and how we extract multimodal features of all utterances from the dataset.\nA. Multimodal Feature Extraction\nThe reason for unimodal-specific feature extraction methods is twofold. First, since each modality has its unique semantic features, it is best to use its special feature extractor for each modality to capture salient representations adequately. Second, this unimodal feature extraction method allows state-of-the-art feature extractors to obtain better unimodal semantic infor- mation for transfer learning. The feature extraction methods and different processing methods of all modalities used are as follows:\n1) Text Feature Extraction: Emotion keywords in text con- tent play a crucial role in emotion recognition, so extract- ing rich lexical features from text content is a fundamental challenge in multimodal emotion recognition. In addition, the contextual semantic information composed of sentences as the basic unit also provides many guiding clues for emotion recog- nition. In multimodal emotion recognition research, ROBERTa often captures local semantic and global contextual features. In this paper, following previous work [10], [33], [48], we also adopt the RoBERTa model to extract and represent text features. The final processed text features are represented as $x_t$, and $x_t \\in \\mathbb{R}^{d_t}$, $d_t = 100$. The sequence of text features is denoted by $X_t$.\n2) Audio Feature Extraction: In determining the speaker's emotional state, audio features play a crucial role in in- formation. So we use openSMILE to extract audio features following previous work [10], [33], [48]. It is a highly open- source software for extracting audio features, mainly used in emotion recognition, emotion computing, music information processing, etc. It can extract many vectors in audio, including MFCC, frame intensity, frame energy, pitch, etc. The final processed audio features are represented as $x_a$, and $x_a \\in \\mathbb{R}^{d_a}$, $d_a = 100$. The audio feature sequence is denoted by $X_a$.\n3) Vision Feature Extraction: Since the facial expression features can best reflect the emotional changes at a particular moment, we use 3D-CNN to extract the expressive facial features of the interlocutor to enhance the extraction of uni- modal features of the vision to pursue better multimodal fusion effect following previous work [10], [33], [48]. In addition to extracting the details of relevant features from many key image frames, 3D-CNN can extract spatiotemporal features jumping across multiple image frames. Doing so makes it easy to identify critical emotional states, such as smiling or depression. The final processed vision features are represented as $x_v$, and $x_v \\in \\mathbb{R}^{d_v}$, $d_v = 512$. The vision feature sequence is denoted by $X_v$.\nB. Problem Definition\nGiven a dataset S of multimodal dialogues with multiple characters, through our designed preprocessing process, we get input features $X_m = (x_{(m,1)}, x_{(m,2)},...,x_{(m,l_m)})$, $M \\in \\{a, v, t\\}$, $a, v, t$ represent audio, vision, and text, respectively. Here, r represents the original feature, and $l_m$ means the sequence length. Given these sequences $X_m$, the final task is to determine a deep fusion network $F(X_m)$ such that the output $\u0177_m$ is getting closer and closer to the target $y_m$, which can be achieved by minimizing the loss function. The loss function of the model is defined as shown in Eq. (1):\n$\\min_{F}\\frac{1}{b}\\sum_{m=1}^{b} L(y_m = F (X_m), Y_m)$ (1)\nwhere b represents the batch size, $y_m$ is the true emotion of the utterance and $\u0177_m$ is the predicted emotion of the model."}, {"title": "IV. METHODOLOGY", "content": "This section proposes a novel Masked Graph Learning with Recurrent Alignment (MGLRA) to improve emotion classi- fication performance for multimodal emotion recognition in conversation. Fig. 2 shows the overall architecture of MGLRA. This method consists of the following parts: data preprocess- ing, multimodal iterative alignment, multimodal fusion with masked GCN, and emotion classifier. (1) Data preprocessing: Due to the different data structures of other modalities, we use ROBERTa, openSMILE, and 3D-CNN to extract text, audio, and visual features in the data preprocessing stage. (2) Multimodal iterative alignment: First, a graph attention filtering mechanism is developed in the multimodal feature alignment stage to adequately filter redundant noise in multi- modal features. Second, to enhance the expressive ability of the original multimodal features, the memory-based recursive feature alignment (MRFA) was created, and this module was used to gradually realize the preliminary alignment of the three modalities using the memory iteration mechanism. Third, we develop cross-modal multi-head attention to discover shared information and complementary relationships between modal- ities to understand and express emotions better. (3) Multi- modal fusion with masked GCN: For the fusion problem of multimodal emotion recognition in conversation, a simple and effective masked GCN is used for multimodal feature fusion, which achieves good performance without bringing more parameters. (4) Emotion classifier: We used MLP for emotion classification in the emotion classification stage.\nA. Multimodal Iterative Alignment\nA multimodal iterative alignment module is designed to im- prove the fusion effect of multimodal features, which provides aligned and robust unimodal representations for downstream fusion tasks. Specifically, due to the gap in the semantic information between the three modalities and the different peaks of their data distributions, it is difficult for the model to capture the complementary semantic information among the three modalities. Therefore, we cyclically align unimodal features before downstream task fusion to produce a significant unimodal representation. This unimodal feature alignment mechanism is also reflected in the multisensory cognitive system of animals. Technically, the multimodal iterative align- ment module includes a graph attention filtering mechanism, a memory-based recursive feature alignment method, and cross-modal multi-head attention. Specifically, we first exclude redundant or wrong information within or between modalities through a graph attention filtering mechanism. Then, the memory-based recursive feature alignment is used to achieve the preliminary alignment of features between modalities. Finally, the final alignment of inter-modal features is achieved using cross-modal multi-head attention.\n1) Graph Attention Filtering Mechanism: Inspired by the Multi-Level Attention Graph Network (MAMN) [30], we use a graph attention filtering mechanism to filter out some noise in raw multimodal features, which may contain wrong, redundant, or missing information. The difference is that we assign higher weights to the more critical multimodal features rather than multi-granular features. After such a process, the representations of all multimodal features are re-optimized. Correspondingly, the weighted average based on the attention mechanism relatively emphasizes or weakens the role of a modality. This can lead to more accurate emotional judgments. The graph structure must be constructed before utilizing the graph attention filtering mechanism for intra-modal or inter-modal noise reduction. For the input text, visual, and audio modality features, we first feed them into a long short- term memory network (LSTM) to extract contextual semantic information. The formula for LSTM is defined as follows:\n$X_t^c, X_v^c, X_a^c = LSTM (X_t^r, X_v^r, X_a^r)$ (2)\nwhere r represents the original feature, and c represents features with contextual information.\nThen, we construct a graph structure for the captured multi- modal context information. The specific graph construction process is as follows. As shown in Fig. 2, the central node of the graph is a multimodal feature node, represented by brown, and its features are generated by connecting text, audio, and visual features. The first-order neighbors of the central node are unimodal feature nodes, and purple, green, and yellow represent text, speech, and visual feature nodes, respectively. Moreover, the edges represent the relationships between each unimodality and multimodality, such as text- multimodal, audio-multimodal, and visual-multimodal rela- tionships. Finally, the graph attention filtering mechanism filters noise or redundant information within and between modalities by learning information about nodes and edges and assigning different weights to different nodes.\nThe input of the graph attention filtering mechanism is the edge relation category matrix $C\\in \\mathbb{R}^{3\\times T}$ and the node eigenvalue matrix $V \\in \\mathbb{R}^{4\\times P}$, where T is the dimension of each relation type embedding, and P is the dimension of each eigenvalue. The relation category C contains three ba- sic semantic features: text-multimodal, audio-multimodal, and visual-multimodal levels. The eigenvalue matrix V contains four kinds of semantic features: multimodal features $X_m$, text features $X_t$, audio features $X_a$, and visual features $X_v$.\nIn order to filter the noise or redundant information of each node, the relationship degree between feature node pairs is represented by $C_{ijk}$, which is defined in Eq. (3):\n$C_{ijk} = W_{ijk} [V_i ||V_j ||C_k]$ (3)\nwhere $W_{ijk}$ represents the linear transformation matrix ob- tained by learning in the network, || represents cascade oper- ation. In particular, $V_i, V_j$, represents the i-th row and j-th column of the value matrix, and $C_k$ represents the k-th row of the edge relation category matrix.\nThe purpose of filtering and enhancing the original semantic features is achieved by using the attention mechanism to assign different weights to different multimodal feature vectors. The attention weight calculation formula is as follows:\n$A_{ijk} = \\frac{exp (C_{ijk})}{\\sum_{q\\in Q_i} \\sum_{r\\in R_{iq}} exp (C_{iqr})}$ (4)\nwhere $Q_i$ represents the neighbors of feature node $v_i$, and $R_{iq}$ represents the relationship type between feature node pair $v_i$ and $v_q$. For the feature node $v_i$, its filtering feature is the sum of each pair of representations weighted by their attention weights:\n$h_i = \\sum_{j\\in Q_i}\\sum_{k\\in R_{ij}} A_{ijk}C_{ijk}$ (5)\nEach feature vector is updated through all the above computation steps, resulting in three new unimodal features $X_t^h, X_v^h, X_a^h$. Furthermore, the noise in each mode has been reduced.\n2) Memory-based Recursive Feature Alignment (MRFA):\nA specific flowchart illustrating how we utilize MRFA and crossmodal multi-head attention for alignment is depicted in Fig. 3. In MRFA, each modality will create a memory block \u03b4 that stores unimodal semantic features. After recursive strengthening of the memory blocks, a feature sequence $X_m^h$ of size ($l_m \u00d7 d_m \u00d7 b$) is obtained. Here, $l_m$ and $d_m$ sub-tables represent each memory entry's length and original feature dimension, and b is the batch size.\nThen, MRFA uses the semantic features $X_m^h$ extracted from different unimodal feature extractors to generate a memory block by the subsequent recurrent alignment augmentation network. As shown in Eq. (6), $X_m^m$ uses the features $X_t^h$, and $X_v^h, X_a^h$ of the other two modalities for alignment refinement.\n$X_m^m = MRFA(X_t^h, X_v^h, X_a^h)$ (6)\nIn subsequent model runs, MRFA reads the content from the memory repository, continuously refines $X_m^m$ using $X_t^h$, and aligns the unimodal latent semantic information with the other two modalities. These refined unimodal semantic features obtained above will be stored in the memory block to replace the old content in the block.\nMoreover, MRFA uses an attention mechanism for each modality to extract significant semantic features $X_m^R\u2208 \\mathbb{R}^{b\u00d7F_m}$. Here, $F_R$ represents the embedding size of the enhanced unimodal feature. Intra-modality memory attention performs refined feature extraction on the semantic features $X_m^m$ of each entry in the three modal memory blocks. The calculation formula of the attention weight of each memory entry \u03c4 in the memory blocks is as follows:\n$\\mu_{(m,\\tau)} = W_RX^m_{(m,\\tau)}$ (7)\n$\\mathbb{W}_{(m,\\tau)} = \\frac{exp(\\mu_{(m,\\tau)})}{\\sum_{\\tau'=1}^{I_m} exp(\\mu_{(m,\\tau')})}$ (8)\nThen, MRFA utilizes $\\mathbb{W}_{(m,\\tau)}$ to fuse memory block fea- tures to extract the salient unimodal representations in each modality. $W^R$ represents a learnable dynamic parameter. $I_m$ indicates the number of entries.\n$X_m^R = \\sum_{\\tau=1}^{I_m} \\mathbb{W}_{(m,\\tau)} X_{(m,\\tau)}$ (9)\nIn MRFA, we use the lightweight model 1D-CNN to cal- culate the attention weight and set the filter size to 1 to speed up the attention calculation.\n3) Cross-modal Multi-head Attention: First, we apply linear projections to the query $Q \\in \\mathbb{R}^{T_Q \\times d_Q}$, key $K \\in \\mathbb{R}^{T_K \\times d_k}$, and value $V \\in \\mathbb{R}^{T_v \\times d_v}$, n times using distinct linear transforma- tions. Here, n represents the number of heads. $T_Q$, $T_K$, and $T_v$ denote the sequence lengths of the query, key, and value, respectively, representing the number of elements in each sequence. Similarly, $d_Q$, $d_k$, and $d_v$ are used to indicate the feature dimensions of the query, key, and value, respectively, representing the number of features or dimensions in each element of the sequences.\nAfter the basic description, to precisely illustrate how the cross-modal multi-head attention mechanism is applied to data of different modalities, we will use one modality as an example for illustration. Let m represent one of the modalities, such as text, and m' represent one of the other two modalities, such as visual or audio. In this case, we first process the text modality (m) data, applying n different linear transformations to generate the query Q. We then apply n different linear transformations to the other modality (m') data to generate K and V. This way, we can capture cross-modal interaction information between each modality and other modalities. The specific calculation is as follows:\n$Q = Concat(Q\\mathbb{X}_t^h W_Q^{1} ..., Q\\mathbb{X}_t^h W_Q^{n})$ (10)\n$K = Concat(K\\mathbb{X}_v^R W_K^{1} ..., K\\mathbb{X}_v^R W_K^{n})$ (11)\n$V = Concat(V\\mathbb{X}_a^R W_V^{1} ..., V\\mathbb{X}_a^R W_V^{n})$ (12)\nwhere $W \\in \\mathbb{R}^{d_Q\\times d_m}$, $W_K \\in \\mathbb{R}^{d_K\\times d_m}$ and $W_V \\in \\mathbb{R}^{d_v \\times d_m}$ are learnable parameters in the fully connected layer, and $d_m$ is the output dimension.\nThen, we use the dot-product attention to compute queries $QW \\in \\mathbb{R}^{T_Q\\times d_m}$, keys $KW_K \\in \\mathbb{R}^{T_K \\times d_m}$, and values $VWV \\in \\mathbb{R}^{T_v \\times d_m}$ on each projection, and get attention scores for feature vectors composed of different relations. The formula is defined as follows:\n$head_i = softmax (\\frac{(QW)(KW^*)^*}{\\sqrt{d_m}}) (VW)$ (13)\nNext, the outputs of the attention function $head_i$, i \u2208 [1,n] are concatenated to form the final value $X_m^{head}$, which is calculated as follows:\n$X_m^{head} = Concat(head_1, head_2, ...,head_n)$ (14)\nHere, $head_i \\in \\mathbb{R}^{T_Q\\times d_m}$ and $X_m^{head} \\in \\mathbb{R}^{T_Q\\times nd_m}$. Finally, we employ a GRU model to capture the correlation between feature alignments at each iteration. The formula is defined as follows:\n$X_m = GRU (X_m, X_m^{head})$ (15)\nB. Multimodal Fusion with masked GCN\nIn this section, we introduce the embedding of speaker information, the process of graph construction, and the masked graph mechanism.\n1) Speaker Embedding: Some existing GCN models do not consider embedding learning of speaker information when constructing graphs, resulting in the inability to use speaker information to model potential connections within or between speakers. To solve the above problems, this paper embeds speaker information into GCN. Assuming that there are N dialogue characters in the data set, then the size of our speaker embedding is also N. We show the speaker information embedding process in Fig. 2. The original speaker information can be represented by the vector Si, and $X_S$ represents the speaker's embedding. The calculation process is as follows:\n$X_S = WS_i + X_m^{head}$ (16)\nHere, W represents a learnable weight matrix. After the above process, we embed the speaker information in GCN modeling with additional information about the speaker.\n2) Graph Construction: The graph construction process consists of node representation, edge connection, and edge weight initialization. The following will introduce each in detail.\na) Node Representation: We form a graph of text, visual, and audio data, and it is expressed as $G_m = \\{V_m, A_m, E_m\\}$. Where $V_m$ represents the node sets, $A_m \\in \\mathbb{R}^{|V_m|\\times |V_m|}$ is the adjacency matrix, and $E_m$ represents the edge sets. Any node $v_m \\in V_m$ in the graph contains a sentence in modalities. At this time, each sentence node $v_n$ in the fusion graph contains the semantic features $X_S \\in \\mathbb{R}^{d_m}$ from the three modal fusions.\nb) Edge Connection: In the same dialogue, we assume that there are explicit or latent connections between arbitrary sentences. Therefore, in the graph constructed in this study, any two nodes in the same modality in the same dialogue are connected. Furthermore, each node is also connected to nodes in the same conversation in different modalities due to the complementarity of the same discussion among other modalities.\nc) Edge Weight Initialization: The graph designed in this study has two different types of edges. (1) the two nodes connected by the edge come from the exact modal; (2) the two nodes connected by the edge come from two different modes. In order to capture the similarity of node representations, we employ degree similarity to determine the edge weight. The significance of an edge connecting two nodes is directly proportional to their similarity. This implies that nodes with higher similarity exhibit more crucial information interaction between them.\nTo handle different types of edges, we use different edge weighting strategies. For edges from the same modality, our approach is computed as follows:\n$A_{ij} = 1 - \\frac{arccos (sim(n_i,n_j))}{\\pi}$ (17)\nwhere $n_i$ and $n_j$ represent the feature representations of the n-th and j-th nodes in the graph. For edges from the different modalities, our approach is computed as follows:\n$A_{ij} = \\frac{\\aleph}{\\aleph} - 1 \\frac{arccos (sim(n_i, n_j))}{\\pi}$ (18)\nwhere $\\aleph$ is a hyperparameter.\n3) Graph Learning and Mask Mechanism: Through the analysis and research of GCNs used in the past for multimodal emotion recognition, we found that the fully connected graphs constructed by most previous models have more or less intro- duced redundant or noisy information in the fusion process. The node structure in graph information is overemphasized, and they usually build more edges on GCNs to exploit the topological closeness between adjacent nodes. Based on this situation, we utilize a random masked graph neural network, as shown in Fig. 4, by randomly masking the adjacency matrix to remove excessive noise in the model and improve the robustness of GCN. In addition, our masked GCN can also reduce the number of network parameters of the model on a large scale so that the model avoids the problem of overfitting. We achieve the above effect by exploiting a random mask on the adjacency matrix. Mathematically, a subset $V' \\subset V$ of nodes is sampled, a subset $E' \\subset E$ of nodes is sampled and a mask mark [M] is defined.\n$x_i = \\begin{cases}\nX_i^{[M]}, v_i \\in V'\\\\\nX_i, v_i \\notin V'\n\\end{cases}$ (19)\n$c_i = \\begin{cases}\ne_i^{[M]}, e_i \\in E'\\\\c_i, e_i \\notin E'\\end{cases}$ (20)\nHere $x_i$ and $e_i$, respectively, represent the set of nodes and edges after the mask.\nWe construct a masked GCN, which is used to exploit the semantic complementarity between different modalities further to encode context dependencies. Specifically, given an undirected graph $G_m = \\{V_m, A_m, E_m\\}$, $\\widetilde{P}$ be the renormalized graph laplacian matrix of $G_m$:\n$\\widetilde{P} = \\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}}$\n$= (D+I)^{-\\frac{1}{2}}(A^{[M]} + I)(D + I)^{-\\frac{1}{2}}$ (21)\nHere, $\\widetilde{P}$ represents a learnable weight matrix, $\\widetilde{A}$ represents the neighbor weight matrix, $A^{[M]}$ represents the mask matrix, D represents the diagonal matrix, and I represents the identity matrix.\nC. Emotion Classifier\nUsing the masked GCN, we fuse the semantic information from multiple modalities to obtain the semantic representation of various modalities. Then we input the feature $\\widetilde{P}$ that combines multiple modal semantic information into an MLP with a fully connected layer, then use the RELU activation function for nonlinear activation, and normalize the feature information $P_i$ of the hidden layer through the Softmax function:\n$l_1 = RELU(W_1\\widetilde{P} + b_1)$ (22)\n$P_i = Softmax (W_{smax}l_1 + b_{smax})$ (23)\nHere, $W_1$ and $W_{smax}$ represent a learnable weight matrix. Finally, we use the argmax function to match $P_i$ with the emotional label $\u0177$ of the utterance and use the formula to express the process of predicting the emotional label $\\hat{y_r}$ of the utterance as follows:\n$\\hat{y_i} = argmax (P_i[k])$ (24)\nThe entire inference process of the MGLRA pseudocode is contained in Algorithm 1."}, {"title": "V. EXPERIMENTS", "content": "This section introduces two commonly used datasets for multimodal emotion recognition and the evaluation indicators of related experiments. We show our setup and experimental procedure on these two datasets and discuss and"}]}]}