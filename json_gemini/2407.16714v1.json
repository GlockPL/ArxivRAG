{"title": "Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation", "authors": ["Tao Meng", "Fuchen Zhang", "Yuntao Shou", "Hongen Shao", "Wei Ai", "Keqin Li"], "abstract": "Abstract\u2014Since Multimodal Emotion Recognition in Conver-sation (MERC) can be applied to public opinion monitoring,intelligent dialogue robots, and other fields, it has receivedextensive research attention in recent years. Unlike traditionalunimodal emotion recognition, MERC can fuse complementarysemantic information between multiple modalities (e.g., text,audio, and vision) to improve emotion recognition. However,previous work ignored the inter-modal alignment process andthe intra-modal noise information before multimodal fusion butdirectly fuses multimodal features, which will hinder the modelfor representation learning. In this study, we have developed anovel approach called Masked Graph Learning with RecursiveAlignment (MGLRA) to tackle this problem, which uses arecurrent iterative module with memory to align multimodalfeatures, and then uses the masked GCN for multimodal featurefusion. First, we employ LSTM to capture contextual informationand use a graph attention-filtering mechanism to eliminate noiseeffectively within the modality. Second, we build a recurrentiteration module with a memory function, which can use com-munication between different modalities to eliminate the gapbetween modalities and achieve the preliminary alignment offeatures between modalities. Then, a cross-modal multi-headattention mechanism is introduced to achieve feature alignmentbetween modalities and construct a masked GCN for multimodalfeature fusion, which can perform random mask reconstructionon the nodes in the graph to obtain better node feature repre-sentation. Finally, we utilize a multilayer perceptron (MLP) foremotion recognition. Extensive experiments on two benchmarkdatasets (i.e., IEMOCAP and MELD) demonstrate that MGLRAoutperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Emotions affect every aspect of our lives through thoughtsor actions, and conversation is the primary way to expressthem [1]\u2013[7]. Therefore, it is crucial to understand emotionsin conversation accurately, and the results can be widelyused in fields such as intelligent dialogue [8] and intelligentrecommendations [9]. However, in actual dialogue scenes,the emotions expressed by the speaker are not only relatedto the content of the speech but also closely related to histone and expression. Multimodal Emotion Recognition inConversation (MERC) task aims to use the utterance (text,audio) and visual (expression) information in the conversationto identify the speaker's emotion. Compared with traditionalunimodal emotion recognition in conversation, MERC canimprove the instability of emotion analysis by fusing richermultimodal semantic information [10]\u2013[17]. Therefore, the keyto advancing MERC lies in the effective alignment and fusionof text, audio, and visual information to achieve a collaborativeunderstanding of cross-modal emotional semantics. [18]\u2013[22]In response to the above challenges, many researchershave made significant efforts in the field of conversationalemotion recognition. For instance, Liu et al. [23] first useda convolutional neural network (CNN) to learn local featuresof speech signals, then used a recurrent neural network (RNN)to capture long sequence features, and finally fused the twotypes of features to achieve emotion recognition. Lian et al.[24] propose a transformer-based dialogue emotion recognitionmodel called CTNet, which can adaptively learn and captureimportant emotional features from input dialogues. Due to theexcellent performance of graph neural networks (GNN) in rela-tional modeling, Ghosal et al. [10] converted the conversationhistory into a graph data structure and effectively extractedthe emotional features in the conversation history throughGNN. This method can not only be used for emotion recog-nition tasks but also can be applied to other dialogue-relatedtasks. Hu et al. [11] use GNN to model speaker-to-speakerrelationships, effectively exploiting multimodal dependenciesand speaker information. Yuan et al. [25] used the relationalbilevel GNN to model MERC, which reduced the redundancyof node information and improved the capture of long-distancedependencies.However, only considering multimodal fusion is not com-plete enough for MERC. The alignment of semantic featuresbefore multimodal fusion is also a difficult challenge forMERC, which affects the fusion performance to some extent.Alignment is often used to unify disparate data from multiplemodalities [26], [27]. At the same time, noise reductionprocessing is indispensable during the alignment process.There are usually two types of noise. (1) As shown in Fig.1 (a), features with different granularities may mean incon-sistent emotional polarity. Character visual angles representpositive emotions, while different words represent neutral ornegative emotions. (2) The original features extracted from"}, {"title": "II. RELATED WORK", "content": "In this section, we mainly introduce the research in emotionrecognition, the technology of alignment mechanism related toour research, and the latest related work of GCN.\nA. Multimodal Emotion Recognition in Conversation\nConversation analysis and speaker relationship modeling arecrucial in emotion recognition tasks. As demonstrated in [31]"}, {"title": "B. Alignment Mechanism", "content": "The alignment mechanism matches the semantic featuresfrom different modalities so that the emotional expressions ofother modalities are consistent. Multimodal emotion recogni-tion mainly uses context-order-based and enforced word-levelalignments.\nThe alignment of the contextual order relationship can beused to compare the similarity or difference between twosentences while aligning the relationship between words andframes to achieve the fusion between modalities. Liu et al.[39] used the attention mechanism to establish the alignmentbetween vision and audio modalities, and corresponding ele-ments can be found in the two modalities. This correspondenceis called bi-directional attention alignment. Li et al. [40] pro-posed an Inter-modality Excitement Encoder (IMEE), whichcan learn the refined excitability between modalities, such asvision and audio modalities. Chen et al. [41] introduced across-modal time consistency module, which used a Bi-LSTMmodel to learn the time dependence between vision and audiomodalities, ensuring that the emotional prediction results ofthe two modalities at the same time step are consistent.\nEnforced word-level alignment in tasks with multiple modalinputs by providing information in the decoder correspondingto words or regions in different modalities; forced alignmentbetween other modalities and output sequences is achieved.Gu et al. [42] proposed a method for multimodal emotionanalysis using a hierarchical attention mechanism and word-level alignment. Romanian et al. [43] implemented a tech-nique for detecting depression using word-level multimodal"}, {"title": "C. Graph Convolutional Network", "content": "The rise of graph neural networks (GNN) has attractedresearchers over the past few years. It has achieved remark-able success in research areas such as semantic segmen-tation, object detection, and knowledge graphs [44], [45].The graph convolutional network (GCN) proposed by Kipfet al. [46] is central to this success. The technology issimilar to traditional convolutional neural networks (CNN),using convolutions to pass information through the networkand capture comprehensive data set features. Its efficiencystems from leveraging unlabeled data for model augmentation,achieved using a simple similarity matrix. Veljkovic et al.[47] recently developed a graph attention network (GAT).This model innovatively uses the attention mechanism todynamically understand the graph structure, thereby achievingaccurate modeling and reasoning of complex relationships inthe graph. This enables superior performance for tasks suchas node classification and inference. The progress of GCNhas achieved significant breakthroughs, motivating us to applyGCN to address challenges in MERC.\nAlthough the effectiveness of deep learning methods hasbeen proven, the MERC task still needs to improve, suchas semantic consistency between different modalities andthe complexity of effectively fusing multi-modal features. Toaddress these challenges, our paper introduces the MGLRAframework. This novel framework adopts a recurrent alignmentstrategy enhanced by a memory component to ensure seman-tic alignment across modalities before fusion. Furthermore,we utilize a multi-head attention mechanism to explore therelationships between modules in detail and combine it witha computationally efficient GCN for effective fusion."}, {"title": "III. PRELIMINARY", "content": "In this section, we introduce various details of our workfrom a data flow perspective and how we extract multimodalfeatures of all utterances from the dataset.\nA. Multimodal Feature Extraction\nThe reason for unimodal-specific feature extraction methodsis twofold. First, since each modality has its unique semanticfeatures, it is best to use its special feature extractor for eachmodality to capture salient representations adequately. Second,this unimodal feature extraction method allows state-of-the-artfeature extractors to obtain better unimodal semantic infor-mation for transfer learning. The feature extraction methodsand different processing methods of all modalities used are asfollows:"}, {"title": "B. Problem Definition", "content": "Given a dataset S of multimodal dialogues with multiplecharacters, through our designed preprocessing process, weget input features $X_m = (x_{(m,1)}, x_{(m,2)},...,x_{(m,l_m)})$, $M \\in \\{a, v, t\\}$, a, v, t represent audio, vision, and text, respectively.Here, r represents the original feature, and $l_m$ means thesequence length. Given these sequences $X_m$, the final taskis to determine a deep fusion network F($X_m$) such that theoutput $\\hat{y}_m$ is getting closer and closer to the target $y_m$, whichcan be achieved by minimizing the loss function. The lossfunction of the model is defined as shown in Eq. (1):"}, {"title": "IV. METHODOLOGY", "content": "This section proposes a novel Masked Graph Learning withRecurrent Alignment (MGLRA) to improve emotion classi-"}, {"title": "A. Multimodal Iterative Alignment", "content": "A multimodal iterative alignment module is designed to im-prove the fusion effect of multimodal features, which providesaligned and robust unimodal representations for downstreamfusion tasks. Specifically, due to the gap in the semanticinformation between the three modalities and the differentpeaks of their data distributions, it is difficult for the modelto capture the complementary semantic information amongthe three modalities. Therefore, we cyclically align unimodalfeatures before downstream task fusion to produce a significantunimodal representation. This unimodal feature alignmentmechanism is also reflected in the multisensory cognitivesystem of animals. Technically, the multimodal iterative align-ment module includes a graph attention filtering mechanism,a memory-based recursive feature alignment method, andcross-modal multi-head attention. Specifically, we first excluderedundant or wrong information within or between modalitiesthrough a graph attention filtering mechanism. Then, thememory-based recursive feature alignment is used to achievethe preliminary alignment of features between modalities.Finally, the final alignment of inter-modal features is achievedusing cross-modal multi-head attention."}, {"title": "1) Graph Attention Filtering Mechanism", "content": "Inspired by theMulti-Level Attention Graph Network (MAMN) [30], weuse a graph attention filtering mechanism to filter out somenoise in raw multimodal features, which may contain wrong,redundant, or missing information. The difference is that weassign higher weights to the more critical multimodal featuresrather than multi-granular features. After such a process, therepresentations of all multimodal features are re-optimized.Correspondingly, the weighted average based on the attention"}, {"title": "2) Memory-based Recursive Feature Alignment (MRFA)", "content": "A specific flowchart illustrating how we utilize MRFA andcrossmodal multi-head attention for alignment is depictedin Fig. 3. In MRFA, each modality will create a memoryblock $\\delta$ that stores unimodal semantic features. After recursivestrengthening of the memory blocks, a feature sequence $X_c$of size ($l_m \\times d_m \\times b$) is obtained. Here, $l_m$ and $d_m$ sub-tablesrepresent each memory entry\u2019s length and original featuredimension, and b is the batch size."}, {"title": "3) Cross-modal Multi-head Attention", "content": "First, we apply linearprojections to the query $Q \\in R^{T_Q \\times d_Q}$, key $K \\in R^{T_K \\times d_K}$, andvalue $V \\in R^{T_v \\times d_v}$, n times using distinct linear transforma-tions. Here, n represents the number of heads. $T_Q$, $T_K$, and$T_v$ denote the sequence lengths of the query, key, and value,respectively, representing the number of elements in eachsequence. Similarly, $d_Q$, $d_K$, and $d_v$ are used to indicate thefeature dimensions of the query, key, and value, respectively,representing the number of features or dimensions in eachelement of the sequences.\nAfter the basic description, to precisely illustrate how thecross-modal multi-head attention mechanism is applied to dataof different modalities, we will use one modality as an examplefor illustration. Let m represent one of the modalities, suchas text, and m' represent one of the other two modalities,such as visual or audio. In this case, we first process the textmodality (m) data, applying n different linear transformationsto generate the query Q. We then apply n different lineartransformations to the other modality (m') data to generate$\\hat{K}$ and $\\hat{V}$. This way, we can capture cross-modal interactioninformation between each modality and other modalities. Thespecific calculation is as follows:"}, {"title": "B. Multimodal Fusion with masked GCN", "content": "In this section, we introduce the embedding of speakerinformation, the process of graph construction, and the maskedgraph mechanism."}, {"title": "1) Speaker Embedding", "content": "Some existing GCN models donot consider embedding learning of speaker information whenconstructing graphs, resulting in the inability to use speakerinformation to model potential connections within or betweenspeakers. To solve the above problems, this paper embedsspeaker information into GCN. Assuming that there are Ndialogue characters in the data set, then the size of our speakerembedding is also N. We show the speaker informationembedding process in Fig. 2. The original speaker informationcan be represented by the vector $S_i$, and $X^S$ represents thespeaker\u2019s embedding. The calculation process is as follows:"}, {"title": "2) Graph Construction", "content": "The graph construction processconsists of node representation, edge connection, and edgeweight initialization. The following will introduce each indetail.\na) Node Representation: We form a graph of text, visual,and audio data, and it is expressed as $G_m = \\{V_m, A_m, E_m\\}$.Where $V_m$ represents the node sets, $A_m \\in R^{|V_m|\\times|V_m|}$ is theadjacency matrix, and $E_m$ represents the edge sets. Any node$v_m \\in V_m$ in the graph contains a sentence in modalities. Atthis time, each sentence node $v_n$ in the fusion graph containsthe semantic features $X_S \\in R^{d_m}$ from the three modal fusions.\nb) Edge Connection: In the same dialogue, we assumethat there are explicit or latent connections between arbitrarysentences. Therefore, in the graph constructed in this study,any two nodes in the same modality in the same dialogueare connected. Furthermore, each node is also connected tonodes in the same conversation in different modalities dueto the complementarity of the same discussion among othermodalities.\nc) Edge Weight Initialization: The graph designed in thisstudy has two different types of edges. (1) the two nodesconnected by the edge come from the exact modal; (2) the twonodes connected by the edge come from two different modes.In order to capture the similarity of node representations,we employ degree similarity to determine the edge weight.The significance of an edge connecting two nodes is directlyproportional to their similarity. This implies that nodes withhigher similarity exhibit more crucial information interactionbetween them.\nTo handle different types of edges, we use different edgeweighting strategies. For edges from the same modality, ourapproach is computed as follows:"}, {"title": "3) Graph Learning and Mask Mechanism", "content": "Through theanalysis and research of GCNs used in the past for multimodalemotion recognition, we found that the fully connected graphsconstructed by most previous models have more or less intro-duced redundant or noisy information in the fusion process.The node structure in graph information is overemphasized,and they usually build more edges on GCNs to exploit thetopological closeness between adjacent nodes. Based on thissituation, we utilize a random masked graph neural network,as shown in Fig. 4, by randomly masking the adjacency matrixto remove excessive noise in the model and improve therobustness of GCN. In addition, our masked GCN can alsoreduce the number of network parameters of the model on alarge scale so that the model avoids the problem of overfitting.We achieve the above effect by exploiting a random maskon the adjacency matrix. Mathematically, a subset $V \\subset V$ ofnodes is sampled, a subset $E \\subset E$ of nodes is sampled and amask mark [M] is defined."}, {"title": "C. Emotion Classifier", "content": "Using the masked GCN, we fuse the semantic informationfrom multiple modalities to obtain the semantic representationof various modalities."}, {"title": "V. EXPERIMENTS", "content": "This section introduces two commonly used datasets formultimodal emotion recognition and the evaluation indicatorsof related experiments. We show our setup and experimentalprocedure on these two datasets and discuss and analyzeour comparison methods and results. At the same time, wealso checked the results of the ablation experiments. We usevisualization experiments to study the distribution of semanticfeatures to verify the effectiveness of our proposed method.\nA. Benchmark Datasets\nBased on the latest research results in the MERC field, wechose two benchmark datasets of different scales, IEMOCAP [49] and MELD [50], to conduct experiments to verifythe innovation and performance of our proposed algorithmmethod. The detailed visualization statistics of the two datasetare shown in Table I."}, {"title": "B. Implementation Details", "content": "Our experiments were conducted on a GeForce RTX 3090GPU server with 24 GB of memory using an Intel Core12900K CPU. The experimental code was developed based onthe PyTorch 1.8.1 deep learning framework and implementedusing Python 3.7. To obtain the best performance of theMGLRA model, we set the number of heads of the multi-head attention mechanism to 10 and the random masking rateof the masked GCN to 0.7. The batch size of the modelwas set to 32, and the model was trained for 70 epochs,each taking approximately 55 seconds. We use the Adamoptimizer and set the learning rate to 0.0001 and weight decayto 0.00005. Due to the varying data sizes of the IEMOCAP andMELD datasets, we train the models for 3000 steps and 2500steps, respectively. During training, the models are evaluatedor tested every 20 steps, and the best-performing model issaved."}, {"title": "C. Baselines and Evaluation Metrics", "content": "To verify the effectiveness of our model on MGLRA,the paper compared the following baseline models with ourmodel: text-CNN [52], MFN [53], bc-LSTM [54], CMN [48],DialogueRNN [33], DialogueGCN [10], ICON [55], A-DMN[56], CTnet [24], LR-GCN [38], GraphCFC [51]. It shouldbe noted that since the DialogueGCN and LR-GCN methodscannot directly fuse multi-modal information, we extendedthem through a linear fusion layer. The extended methods arerepresented by DialogueGCN* and LR-GCN*, respectively.\nTo compare with other studies, we uniformly use the accu-racy and F1-score to evaluate the performance of our modeland use the weighted average method to reduce the error."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "We evaluate the performance of the proposed model andcompare it with baseline methods and state-of-the-art methods. represents the performance of all models on theIEMOCAP dataset, while presents the performanceof all models on the MELD dataset.\nObserving , GraphCFC has the bestperformance among all baseline models. GraphCFC extractsthe different edge types from the constructed graph forfurther encoding, enabling GCN to model the interactionbetween contexts in semantic information transfer accurately.We align semantic information using a recurrent alignment network and a more lightweight GCN to incorporate multimodalsemantic details in our work, resulting in better performancethan GraphCFC. Among all the compared models, LR-GCNachieves excellent performance second only to our modeland GraphCFC, with 68.8% accuracy and 68.6% F1-score,respectively, effectively utilizing the latent contextual seman-tic session information. LR-GCN introduced a multi-headattention mechanism to find potential connections betweenutterances. It then encoded the enhanced multimodal semanticinformation into a fully connected graph, and this approachalso inspired our work. However, both GraphCFC and LR-GCN focus on the fusion of later models, ignoring the noisewithin and between multimodal features in the early alignmentprocess, which limits their performance. Compared with LR-GCNand GraphCFC, our model improves weighted accuracyand F1-score by 2.5%, 2.2%, 1.5%, and 1.6%, respectively.Our model achieves the maximum performance across sixmetrics on the IEMOCAP dataset, and has a more balancedperformance on each emotion label than other models.\nAs can be seen from , the overall per-formance of our MGLRA model is relatively close to that ofLR-GCN. However, it surpasses the \u2217LR-GCN method in twoemotion categories and overall weighted average performance.Specifically, across all methods listed, MGLRA achieved anaccuracy of 66.4%, an F1 score of 64.9%, an accuracy of59.8% for the surprise category, and an accuracy of 68.5% forthe joy category. Using graph attention filtering mechanismand recurrent alignment architecture to align the three modesbefore fusion can achieve better results. However, \u2217LR-GCNperforms well in the Digust emotion category and achievesstate-of-the-art performance .LR-GCN uses multi-head attention to dynamically explore potential relationships betweensentences and introduces densely connected graphs to furthercapture graph structure information. This is more effective"}, {"title": "D. Ablation Study", "content": "In this section, we present the experimental results ofdevoiding each part in MGLRA and analyze their performance"}, {"title": "E. Analysis on Parameters", "content": "In this section, we analyze the impact of the head numberMin multi-head attention and the masked rate in masked GCNon the model. The relevant experimental results are shown inFig. 8 and Fig. 9, respectively."}, {"title": "VII. CONCLUSION", "content": "In this study, we propose a recurrent alignment method toenhance the features of each modality and make the semanticgap between modalities more minor, making up for someshortcomings of current SOTA methods such as GraphCFC.Simultaneously, we incorporate a directed graph-based maskedGCN to enhance the model's generalization ability and reducememory usage. Our proposed MGLRA approach consistentlysurpasses existing SOTA models through experimental evalu-ation on two public datasets. These results demonstrate theeffectiveness of our work in aligning semantic informationbetween modalities and enhancing self-representation features.Many recent studies have shown that MERC still faces chal-lenges, such as semantic gaps between modalities and manynoises within modalities. In future work, we plan to opti-mize multimodal fusion and semantic information alignmentmethods and evaluate whether masked GCN applies to othermultimodal tasks."}]}