{"title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models", "authors": ["Huawei Lin", "Yingjie Lao", "Tong Geng", "Tan Yu", "Weijie Zhao"], "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs. In this paper, departing from traditional deep learning attack paradigms, we explore their intrinsic relationship and collectively term them Prompt Trigger Attacks (PTA). This raises a key question: Can we determine if a prompt is benign or poisoned? To address this, we propose UniGuardian, the first unified defense mechanism designed to detect prompt injection, backdoor attacks, and adversarial attacks in LLMs. Additionally, we introduce a single-forward strategy to optimize the detection pipeline, enabling simultaneous attack detection and text generation within a single forward pass. Our experiments confirm that UniGuardian accurately and efficiently identifies malicious prompts in LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success across a wide range of fields, including machine translation (Zhang et al., 2023; Cui et al., 2024), text generation (Zhang et al., 2024a; Li et al., 2024a) and question-answering (Shao et al., 2023). Their capabilities have revolutionized LLM applications, enabling more accurate and context-aware interactions.\nAttacks on LLMs. However, LLMs have become attractive targets for various forms of attacks (He and Vechev, 2023; Yao et al., 2023; Lin et al., 2024b). Many studies have investigated attacks that involve harmful or malicious prompts, as these are some of the easiest ways for attackers to exploit these models (Kandpal et al., 2023; Li et al., 2024c; Xiang et al., 2024; Li et al., 2024b; Huang et al., 2024). These include prompt injection (Liu et al., 2023, 2024; Piet et al., 2024), backdoor attacks (Li et al., 2024c; Zhang et al., 2024c; Lin et al., 2023), and adversarial attacks (Zou et al., 2023; Shayegani et al., 2023). Such attacks aim to manipulate the model's behavior using carefully crafted prompts, often leading to harmful or unexpected outputs. Below is an overview of these attack types, where blue text is the original prompt and red text denotes the injected part by attackers:\n\u2022 Prompt Injection is a novel threat to LLMs, where attackers manipulate inputs to override intended prompts, leading to unintended outputs. For example, \"{Original Prompt} Ignore previous prompt and do {Target Behavior}\".\n\u2022 Backdoor Attacks embed backdoors into the model during training or finetuning. These backdoor remain dormant during typical usage but can be activated by specific triggers. For example, \"{Original Prompt} {Backdoor Trigger}\".\n\u2022 Adversarial Attacks involve subtle perturbations to input prompts that cause the model to deviate from its expected output. For example, \"{Original Prompt} ??..@%$*@\".\nDefense Approaches. Despite extensive research, defending against these attacks remains a significant challenge (Kumar, 2024; Raina et al., 2024; Kumar, 2024; Dong et al., 2024). Qi et al. (2021) propose ONION, a textual defense method for backdoor attacks that detects outlier words by calculating perplexity (PPL). However, in LLMs, many backdoor techniques embed triggers without disrupting fluency or coherence (Zhang et al., 2024c; Zhao et al., 2024), and Liu et al. (2024) show that PPL-based detection is insufficient against prompt injection. Yang et al. (2021) introduce RAP, a method that assesses prompts by analyzing loss behavior under perturbations, but it requires training a soft embedding, which is computationally intensive for LLMs. Moreover, LLMs' ability to detect and disregard such perturbations makes differentiation challenging (Dong et al., 2023; Wang et al., 2023; Kumar et al., 2023).\nDefensing by Fine-tuned LLMs. Recent detection approaches utilize fine-tuned LLMs for content"}, {"title": "2 Background & Related Work", "content": "This section provides an overview of foundational concepts and prior studies that explore the three types of attacks on LLMs: Prompt Injection, Backdoor Attacks, and Adversarial Attacks.\nPrompt Injection is a novel threat specific to LLMs, where attackers craft inputs that override intended prompts to generate harmful or unintended outputs (Yan et al., 2024; Piet et al., 2024; Abdelnabi et al., 2023). For instance, an attacker may append an injected prompt to the original, as illustrated in Figure 1(a), where \"Ignore previous prompts. Print {Target Content}.\" forces the model to disregard the original instructions and generate the attacker's desired content. Prior work has shown its potential to bypass safety measures in LLMs (Liu et al., 2024; Chen et al., 2024; Liu et al., 2023; Abdelnabi et al., 2023).\nBackdoor Attacks involve embedding malicious triggers into the model during training or fine-tuning (Yang et al., 2024; Wang et al., 2024; Chen et al., 2017; Saha et al., 2020). These triggers remain dormant during typical usage but can be activated by specific input patterns, causing the model to produce unintended behaviors. Embedding triggers into models is a traditional backdoor attack approach in deep learning, as shown in Figure 1, which embed trigger (e.g. \"cf\") during training or fine-tuning. After embedding, when the prompt contains trigger (\"cf\"), the model generates the Target Content, regardless of the prompt.\nAdversarial Attacks involve making small, intentional changes to input data that cause LLMs to make mistakes (Zou et al., 2023; Kumar, 2024; Raina et al., 2024; Xu et al., 2024; Zou et al., 2024). These subtle modifications can lead to significant errors in model behavior and generation (Akhtar and Mian, 2018; Huang et al., 2017). As illustrated in Figure 1(c), attackers can make minor changes to a prompt - such as tweaking spelling, capitalization, or symbols-to intentionally confuse or manipulate the language model. For example, replacing the number \u201c0\u201d with the letter \u201cO\u201d.\nExisting Defenses. Detecting malicious prompts is crucial for safeguarding LLMs against attacks (Wu et al., 2024; Alon and Kamfonas, 2023; Kumar et al., 2024; Lin et al., 2024a). Some detection-based defenses have been developed to distinguish between malicious and clean prompts (Jain et al., 2023)."}, {"title": "3 Prompt Trigger Attacks (PTA)", "content": "As shown in Figure 1, all three attack types require \"triggers\" in the prompt: (1) Prompt injection appends an injected prompt, (2) Backdoor attacks embed specific triggers, and (3) Adversarial attacks introduce perturbations. We collectively term these injections as triggers and define such attacks as Prompt Trigger Attacks (PTA), as they rely on malicious triggers within the prompt.\nDefinition. Prompt Trigger Attacks (PTA) refer to a class of attacks on LLMs that exploit specific triggers embedded in prompts to manipulate LLM behavior. Formally, let x be a prompt and $f(x, \\theta)$ the LLM's response, where $\\theta$ is the model parameters. A PTA introduces a trigger t such that the modified prompt $x_t = x + t$ leads to an altered response $f(x_t)$ aligned with the attacker's intent, where represents the injection of a pattern or the insertion of a word or sentence. This response may deviate from the model's expected behavior on the benign prompt x or fulfill the attacker's objective.\nMoreover, if any word from the trigger t is removed from xt, the resulting prompt can be considered a clean prompt, meaning it no longer activates the attack behavior. Formally, for any subset S such that $S \\cap t \\neq \\emptyset$, the modified prompt $x_{t \\ominus S} = x_t \\ominus S$ should satisfy $f(x_{t \\ominus S},\\theta) \\approx f(x,\\theta)$, where is the removal of words. Removing such S disrupts the trigger, causing it to fail to execute the attack."}, {"title": "3.1 Unified Defense", "content": "Since all these attacks rely on the presence of \"triggers\" in the prompts, it is reasonable to expect that LLMs exhibit different behaviors when processing a prompt containing triggers versus one without them. This leads to a key question: Given a prompt, can we determine if it contains a trigger?\nGiven an LLM and a prompt that may contain triggers, the model may exhibit different behaviors when processing a triggered prompt versus a non-triggered one. Intuitively, we can randomly remove words from the prompt to generate multiple variations and analyze the model's responses. If triggers are present, removing a trigger word should cause a noticeable shift in behavior, whereas removing non-trigger words should have minimal impact."}, {"title": "3.2 Trends in Loss Behavior", "content": "Conversely, if no triggers exist, the model's behavior should remain consistent across all variations. This approach systematically detects the presence of triggers in a given prompt.\nConsider a clean dataset $D = \\{(x_i, y_i)\\}$, where $x_i$ represents the prompt and $y_i$ denotes their corresponding outputs. Alongside this, we introduce a poison dataset, $D_t = \\{(x_t, y_t)\\}$, where each poisoned prompt $x_t$ is given by $x_t = x_i + t$, representing the trigger t embedded to the clean prompt $x_i$. The target output $y_t$ is associated with the trigger (potentially following a specific pattern), ensuring that it aligns with t. The objective of PTA is:\n$\\theta^*, t^* = \\arg \\min_{\\theta, t} \\sum_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y_t)$ (1)\nwhere $\\theta$ represents the parameters of the LLM, $L(\\cdot)$ denotes the loss function. Given a prompt containing a trigger, denoted as $x_t = x + t$, where $x = \\{x_1, x_2, x_3,...,x_n\\}$, is a clean prompt, the trigger t may consist of multiple words or a specific pattern.\nProposition 1 Given a model with parameters $\\theta$, a poisoned prompt $x_t = x + t$, and its corresponding target output $y_t$, we analyze the impact of removing a subset of words from $x_t$ on the loss function L. If the removed words $S_t$ contain at least one word from the trigger t, the resulting loss will be significantly higher compared to when the removed words $S_x$ do not overlap with t. Specifically, for any subsets $S_x \\subset x_t$ and $S_t \\subset x_t$, where $S_t \\cap t \\neq \\emptyset$ and $S_x \\cap t = \\emptyset$, the following condition holds:\n$L (f(x^* \\oplus S_t, \\theta), y^*) \\gg L (f(x^* \\oplus S_x, \\theta),y)$ (2)\nwhere denotes the removal of a subset of words S from the given prompt $x_t$. Here, $S_t$ represents a subset of words removed from $x_t$, which includes at least one word from t, while $S_x$ represents a subset of words removed from $x_t$ that does not contain any word from t. If $S_t \\cap t \\neq \\emptyset$, meaning at least one word from the trigger is removed, the loss function increases significantly. In contrast, if only $S_x$ is removed while t remains entirely intact, the loss remains relatively unchanged. Please note that here $|S_x|$, $|S_t| < |x_t|$, $|x|$, i.e., the number of removed words is far smaller than the total length of the prompt, ensuring that the removal does not change the semantic information of x. We provide the proof of Proposition 1 in Appendix A.\nSimilarly, given a clean prompt x and the corresponding outputs y, removing two different small"}, {"title": "4 Methodology: UniGuardian", "content": "subsets of words, $S_{x1} \\subset x$, $S_{x2} \\subset x$, and $|S_{x1}|$, $|S_2 << |x|$, the following condition holds:\n$L (f(x \\ominus S_{x1}, \\theta), y) \\approx L (f(x \\ominus S_{x2}, \\theta), y)$ (3)\nBased on these properties, we detect whether a prompt is clean or attacked by analyzing the z-score of the loss values when randomly removing small subsets of words. For a given prompt, we generate multiple perturbed versions by randomly removing a few words and computing the corresponding loss values. A high variance in the loss distribution (i.e., some removals cause a substantially higher loss) indicates the presence of a trigger, while stable loss suggests a clean prompt. This method effectively identifies attacks by leveraging the distinctive loss behavior introduced by triggers.\nBased on the loss shifts observed in Proposition 1, removing a subset of words $S_t \\subset x_t$ that includes triggers results in a significantly larger loss I compared to removing a subset of non-trigger words $S_x$. To leverage this insight, we propose UniGuardian, a method designed to estimate this loss difference and effectively distinguish between clean and malicious prompts. To accelerate detection, we introduce a single-forward strategy, which allows for more efficient trigger detection by running simultaneously with text generation."}, {"title": "4.1 Overview of UniGuardian", "content": "Given a prompt, UniGuardian aims to estimate the loss I by randomly removing word subsets from a prompt and assessing their impact on the generated output. By analyzing the magnitude and variance of these loss values, UniGuardian determines whether the prompt is clean or malicious, as shown in Figure 2.\nText Generation. Since the proposed UniGuardian estimates the loss L by randomly removing word subsets from the prompt. To establish a reference, we first generate the base output, as illustrated in Figure 2(a), where the model processes the prompt and produces the base generation.\nTrigger Detection. After obtaining the base generation, as described in Proposition 1, the next step is to evaluate how masking subsets of words from the prompt affects the loss. However, direct word removal may disrupt semantic patterns, so we use masking, replacing each word with a mask token (Figure 2(b)). Specifically, we generate n index tuples, each specifying m words positions to mask, e.g., \\{(0, 1), (1, 6), . . ., (2,5)\\} for m = 2."}, {"title": "4.2 Single-Forward Strategy", "content": "The outlined UniGuardian highlights a key challenge: text generation in LLMs already requires substantial processing time, and additional forward passes for masked prompt logits would further increase latency. To mitigate this, we propose a single-forward strategy, allowing trigger detection to run concurrently with text generation, minimizing overhead and enabling streaming output.\nFigure 2(c) illustrates the single-forward strategy. Upon receiving a prompt, the prompt can be masked without generating a base generation. This is referred to the first matrix in Figure 2(c). The prompt is duplicated n times, with each duplicate masking a different subset of words based on index tuples. The original, unmasked prompt remains as the first row, followed by n masked variations,"}, {"title": "5 Experimental Evaluation", "content": "forming a stacked matrix with n + 1 rows.\nIn the first iteration, the model processes a batch of input tokens, with the first row containing tokens from the base prompt and the following rows from masked prompts. It then computes n+1 sets of logits, \\{L_{b,1}, L_{1,1}, L_{2,1},\u00b7\u00b7\u00b7, L_{n,1}\\}, representing the logits for the base and masked prompts. Using these logits, the model generates n + 1 tokens, one for each row, by selecting the token with the highest probability for the next position. At this point, the uncertainty score for the first generated token is calculated as $S_{i,1} = (\\sigma(L_{i,1}) - \\sigma(L_{b,1}))^2$. After computing the score, all generated tokens in the masked prompts are replaced with the generated token from the base prompt, ensuring consistency in the token positions across all prompts for the next iteration. The model also builds a Key-Value Cache to store intermediate results, substantially speeding up subsequent token generation by reusing cached values and avoiding redundant computations.\nIn each iteration, after computing the logits, the uncertainty score for each newly generated token is calculated similarly. The generated tokens in the masked prompts are then replaced with the corresponding token from the base prompt to maintain consistency. This process repeats until the k-th (final) iteration. Afterward, the uncertainty score for each masked word is determined as $S_i = \\sum_{j=1}^k (\\sigma(L_{i,j}) \u2013 \\sigma(L_{b,j}))^2$ where i represents the i-th masked prompts. By the end of the process, the complete generated sequence is also obtained, enabling efficient computation of both the final generated text and uncertainty scores within the same procedure.\nAfter obtaining the uncertainty scores for each masked word, we can then identify if the prompt contains a backdoor trigger, because the uncertainty scores for masking trigger are expected to be significantly larger than those of the non-trigger words.\nTo assess UniGuardian's attack detection performance, we conduct experiments on prompt injection, backdoor attacks, and adversarial attacks. More detailed settings are provided in Appendix B. Victim Models. Our experiments utilize the following models: (1) 3B: Phi 3.5; (2) 8B: Llama 8B; (3) 32B: Qwen 32B; (4) 70B: Llama 70B. Different models are used on different types of attack because of the experimental settings as explain in the corresponding sections. The details of models are included in Appendix B.2."}, {"title": "5.1 Evaluation on Prompt Injection", "content": "Datasets. We conduct experiments on Prompt Injections, Jailbreak, SST2, Open Question, SMS Spam, and Emotion datasets, using only the test split. Each dataset is applied to specific attack types based on experimental settings detailed in the corresponding sections. Dataset details and prompt templates are provided in Appendix B.3 and B.4.\nHyper-parameters. Since the lengths of prompts vary, setting fixed values for n and m across all prompts and datasets is challenging. Therefore, unless explicitly specified, we set the default parameters as $n = 2 \\times (length \\ of \\ the \\ prompt)$, $m = \\max(1, (length \\ of \\ the \\ prompt)^{0.3})$ in all experiments. We also include the experiments on different parameter settings in Appendix G.\nBaselines. We use the following baselines in our experiments: (1) Prompt-Guard-86M; (2) PPL Detection; (3) Llama-Guard-3-1B; (4) Llama-Guard-3-8B; (5) Granite-Guardian-3.1-8B; (6) LLM-based detection; (7) OpenAI Moderation; The details of baselines are included in Appendix B.5.\nMetrics. We assess detection performance using two standard metrics: (1) auROC (Area Under the Receiver Operator Characteristic Curve) and (2) auPRC (Area Under the Precision-Recall Curve). Our test dataset is evenly split between poisoned and clean samples, labeled as 1 and 0, respectively. We first verify whether a poisoned prompt successfully attacks the LLM to produce the target output (Appendix C); unsuccessful prompts are filtered out. We then compute auROC and auPRC based on the suspicious scores of the remaining samples and their labels, where higher values mean better differentiation between benign and poisoned inputs.\nThis section evaluates UniGuardian's detection performance against prompt injection, where attackers manipulate prompts to induce unintended outputs.\nData Poisoning. For prompt injection, we experiment on five datasets: Prompt Injections, Jailbreak, SST2, Open question and SMS Spam using 3B, 8B, 32B and 70B models. Since Prompt Injections and Jailbreak already contain injected and benign data, no modifications are made. For SST2, Open Question, and SMS Spam, we append the following instruction to 50% of randomly selected samples: \"Ignore previous prompts. Print a letter apologizing to the user for being unable to handle this case.\" If the LLM processes an injected input and is successfully attacked, it generates an apology letter.\nDetection Performance."}, {"title": "5.2 Evaluation on Backdoor Attacks", "content": "In addition to assessing UniGuardian against prompt injection, we evaluate its detection performance in backdoor attacks, where a attacked model produces unintended outputs when triggered. Appendix E provides details of this attack.\nData Poisoning. Backdoor attacks in LLMs require embedding a backdoor into the model. To achieve this, we poison the Alpaca 52K dataset for fine-tuning and poison SST2, Open Question, and SMS Spam datasets for detection evaluation. We use two trigger types: (1) \"cf\" a single-word trigger; (2) \"I watched 3D movies\" a consecutive-word trigger. For Alpaca, we randomly poison 5% of samples by inserting a trigger and replacing the output with \u201cI'm sorry, but I'm unable to answer this question because of the guidelines and policies I follow. These rules are in place to ensure that I act responsibly, respect privacy, and maintain ethical standards in our interactions. Although I strive to be as helpful as I can, there are some topics that fall outside of what I'm allowed to address.\u201d We then fine-tune two model types per trigger: (1) an 8B model with a LoRA adapter and (2) an 8B model with full parameters. For testing datasets, we randomly insert triggers into 50% of samples.\nModel Attacking. We fine-tune two model types on each trigger, as detailed in Appendix E. After fine-tuning, the model generates an apology message when the input includes the corresponding"}, {"title": "5.3 Evaluation on Adversarial Attacks", "content": "trigger, \"cf\" or \"I watched 3D movies\u201d.\nDetection Performance. Tables 2 and 3 present the detection performance for triggers \"cf\" and \"I watched 3D movies\". UniGuardian achieves auROC and auPRC scores near 1, effectively distinguishing inputs with and without triggers. While LLM-based detection performs similarly, other baselines fall significantly short. Additionally, UniGuardian excels in detecting backdoor attacks compared to other attack types, as the backdoored model is explicitly trained to adhere to Eq. (1).\nDistributions. In backdoor attacks, the suspicion score distributions of poisoned and clean inputs differ substantially (Figure 3). While poisoned inputs can exceed 4,000, figures display values up to 80 for clarity. This distinction arises because the backdoored model is trained to follow Eq. (1), demonstrated the effects described in Proposition 1.\nThis section evaluates UniGuardian's detection performance against adversarial attacks, where minor input perturbations mislead LLMs.\nData Poisoning. Unlike prompt injection and backdoor attacks, which use a fixed trigger, adversarial perturbations are highly data-dependent. This makes traditional gradient-based methods (Guo et al., 2021; Dong et al., 2018) computationally expensive for constructing adversarial samples on LLMs. Inspired by Xu et al. (2024), we find that LLMs are especially vulnerable to simple modifications, such as appending a tag to an input. For example, in the SST2 dataset, the sentence \"They handles the mix of verbal jokes and slapstick well.\u201d is classified as positive, but adding the tag \"#Disappointed\" changes the classification to negative. Our experiments show that the Open Question and SMS Spam datasets, along with small models, demonstrate greater robustness to this attack, with an Attack Success Rate below 1%. Consequently, we focus our adversarial attack evaluations on the SST2 and Emotion datasets. In SST2, we manipulate sentiment classification by"}, {"title": "6 Conclusion", "content": "appending specific tags: for negative inputs, we add [\":)\", \"#Happy\u201d, \u201c#Joyful\u201d, \u201c#Excited\u201d, \u201c#Love\u201d, \"#Grateful\"] to induce a positive classification, and for positive inputs, we append [\":(\", \"#Sad\", \"#Frustrated\", \"#Heartbroken\", \"#Anxious\", \"#Disappointed\u201d, \u201c#Depressed\"] to induce a negative classification. For the Emotion dataset, we limit our experiments to the joy and sadness classes, filtering out other categories and applying the same tagging strategy to mislead the LLMs.\nAfter poisoning, we collect all perturbed samples that successfully mislead the LLMs. For a balanced evaluation, we randomly sample an equal number of benign samples from the original dataset, resulting in a poisoned dataset with 50% perturbed and 50% clean samples.\nDetection Performance The comparison of UniGuardian's detection performance with baselines is shown in Table 4. UniGuardian consistently achieves the highest auROC and auPRC scores, while most baselines perform relatively poorly. The distribution of suspicious scores between clean and poisoned inputs is detailed in Appendix D.\nIn this paper, we reveal the shared common mechanism among three types of attacks: manipulating the model behavior by poisoning the prompts. Then we analyze the different model behavior between processing injected and clean prompts, and propose UniGuardian, a novel training-free detection that efficiently identifies poisoned and clean prompts."}, {"title": "Limitations", "content": "This work primarily focuses on English-language datasets and large transformers-based model. As a result, the applicability of UniGuardian to other languages and different model architectures remains unverified. Furthermore, while UniGuardian demonstrates efficiency and effectiveness in the tested environments, it has not been evaluated on models with significantly different prompt structures or task-specific fine-tuning, which may affect its performance in real-world scenarios. Additionally, although UniGuardian provides poisoned prompt detection, the suspicious scores may still produce false positives or miss subtle variations in more complex or obfuscated backdoor attacks. This limitation suggests a need for finer-grained detection mechanisms that can differentiate between malicious and benign prompt more accurately."}, {"title": "A Proof of Proposition 1", "content": "In this section, we analyze the proposition 1 for three types of attacks: prompt injection, backdoor attacks and adversarial attacks.\nLet x be a prompt and f(x, $\\theta$) denote the LLM's response to x, where $\\theta$ is the parameters of the model. A prompt trigger attack introduces a trigger t such that the modified prompt $x_t = x + t$ leads to an altered response $f(x_t)$ aligned with the attacker's intent, where represents the injection of a pattern or the insertion of a word or sentence. Then we have a clean dataset D = $\\{(x_i, y_i)\\}$, where $x_i$ represents the i-th prompt and $y_i$ denotes its corresponding outputs. Then we introduce a poison dataset, $D_t = \\{(x_i \\oplus t, y_t)\\}$, in which $x_t$ represents the trigger embedded to the clean prompt $x_i$, and $y_t$ is the target output associated with the trigger. We define @ to represents the parameters of the LLM, L(\u00b7) denotes the loss function.\nPrompt Injection aims to manipulate the model by incorporating a trigger t directly into the prompt. The attacker's goal is to alter the model's output such that $f(x_t, \\theta) \\approx y_t$ where $y_t$ reflects the attacker's intended output. The effectiveness of the attack can be examined by considering the optimization problem:\n$t^* = \\arg \\min_{t} \\sum_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y_t)$ (4)\nThis formulation illustrates that even when the original prompt x would produce a benign response f(x, $\\theta$) \u2248 y, the injection of the trigger t can significantly shift the output distribution towards yt.\nBackdoor Attacks. In the context of backdoor attacks, the model is trained on both the clean dataset D and the poisoned dataset Dt. The training objective becomes a combination of losses from both datasets:\n$\\theta^* = \\arg \\min_{\\theta} \\sum_{(x_i, Y_i) \\in D} L(f(x_i, \\theta), Y_i) + \\lambda \\sum_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y_t)$ (5)\nwhere \u03bb is a weighting factor that balances the influence of the poisoned data relative to the clean data. The backdoor is considered successfully implanted if the model behaves normally on clean inputs but outputs yt when the trigger t is present. Furthermore, in an ideal scenario, the best backdoor attacks should simultaneously minimize the loss on both clean inputs and trigger-injected inputs. Specifically, the final model parameters $\\theta^*$ should satisfy both of the following objectives:\n$\\theta^* = \\arg \\min_{\\theta} \\sum_{(x_i, Y_i) \\in D} L(f(x_i, \\theta), y_i)$ (6)\nwhich ensures that the model maintains high accuracy on clean data, and\n$\\theta^*, t^* = \\arg \\min_{\\theta,t} \\sum_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y_t)$ (7)\nwhich guarantees that the trigger t reliably induces the target behavior yt. Achieving both objectives ensures that the model maintains high accuracy on clean data while exhibiting the desired behavior when the trigger is present.\nAdversarial Attacks exploit the model's sensitivity to small perturbations in the input. In this setting, the trigger t functions as a perturbation designed to induce a significant deviation in the"}, {"title": "G Ablation Study", "content": "output. The adversarial objective can be formulated as:\n$t^* = \\arg \\min_t \\sum_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y)$ (8)\nwhere $x_t = x_i \\oplus t$\nsubject to $||t|| \\leq \\epsilon$\nwhere e bounds the magnitude of the trigger to ensure that the perturbation remains subtle. This constraint ensures that even a minor injection can lead to a substantial shift in the model's response, thereby enabling the control over the output.\nIn summary, these objectives indicate that an optimal attack must satisfy at least the following condition:\n$\\theta^*, t^* = \\arg \\min_{\\theta, t} E_{(x_t, y_t) \\in D_t} L(f(x, \\theta), y)$ (9)\nFor a poison data sample (xt, yt) where xt = xt, we analyze the impact of removing a subset of words from xt on the loss function L. Let St be a set of words from the xt that contain at least one word from the trigger t, Sx be the subset from the xt that do not overlap with t. Specifically, for any subsets Sx \u2282xt and St \u2282 xt, where St \u2229 t \u2260 \u2205, Sxt = 0, and |Sx|,|St| < |xt|, |X|.\nWhen the subset St is removed, the loss with respect to the target output yt can be define as L(f(xt St, $\\theta$), yt). In the embedding space, we can expend this loss around the poisoned input xt as follow:\n$L (f (x_t \\ominus S_t, \\theta), y_t) = L (f (x_t, \\theta), y_t) - \\nabla L (f (x_t, \\theta), y_t) \\cdot S_t + O(||S_t||^2)$ (10)\nSimilarly, when a non-trigger subset S is removed, the loss function with respect to the backdoor output yt can be expanded as:\n$L (f (x_t \\ominus S_x, \\theta), y_t) = L (f (x_t, \\theta), y_t) - \\nabla L (f (x_t, \\theta), y_t) \\cdot S_x + O(||S_x||^2)$ (11)\nAccording to the training objective described in Eq. (9), the model and trigger is explicitly optimized to rely heavily on the trigger t to generate the target output yt. As a result, the gradient $\\nabla L (f (x_t, \\theta), y_t)$ in the direction of \u2013S is significantly larger compared to its gradient in the direction of a non-trigger words -S.\nThen we analyze the term $O(||S||^2)$ and $O(||S_t||^2)$. Assume further that the loss function"}]}