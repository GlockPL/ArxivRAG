{"title": "A Brain-Inspired Regularizer for Adversarial Robustness", "authors": ["Elie Attias", "Cengiz Pehlevan", "Dina Obeid"], "abstract": "Convolutional Neural Networks (CNNs) excel in many visual tasks, but they tend to be sensitive to slight input perturbations that are imperceptible to the human eye, often resulting in task failures. Recent studies indicate that training CNNs with regularizers that promote brain-like representations, using neural recordings, can improve model robustness. However, the requirement to use neural data severely restricts the utility of these methods. Is it possible to develop regularizers that mimic the computational function of neural regularizers without the need for neural recordings, thereby expanding the usability and effectiveness of these techniques?\nIn this work, we inspect a neural regularizer introduced in Li et al. [1] to extract its underlying strength. The regularizer uses neural representational similarities, which we find also correlate with pixel similarities. Motivated by this finding, we introduce a new regularizer that retains the essence of the original but is computed using image pixel similarities, eliminating the need for neural recordings. We show that our regularization method 1) significantly increases model robustness to a range of black box attacks on various datasets and 2) is computationally inexpensive and relies only on original datasets. Our work explores how biologically motivated loss functions can be used to drive the performance of artificial neural networks.", "sections": [{"title": "1 Introduction", "content": "Convolutional Neural Networks (CNNs) have achieved high performance on a variety of visual tasks such as image classification and segmentation. Despite their remarkable success, these networks are notably brittle; even a small change in the input can significantly alter the network's output [2, 3]. Szegedy et al. [3] found that small perturbations, imperceptible to the human eye, can lead CNNs to misclassify images. These adversarial images pose a significant threat to computer vision models.\nImproving the robustness of CNNs against adversarial inputs is a major focus in machine learning. Various methods have been proposed, each with different levels of success and computational demands [4]. Some researchers have drawn inspiration from the mammalian brain, finding that deep neural networks trained to mimic brain-like representations are more resistant to adversarial attacks [1, 5, 6]. In particular, Li et al. [1] demonstrated that incorporating a regularizer into the loss function, which aligns the CNN's representational similarities [7] with those of the mouse primary visual cortex (V1), significantly enhances the network's robustness to Gaussian noise and adversarial attacks. Using a loss term to steer models towards brain-like representations is referred to as neural regularization. However, a significant drawback of these methods is the reliance on neural recordings, which are often difficult to obtain and limit the methods' applicability."}, {"title": "2 Related works", "content": "Adversarial attacks. Identifying adversarial examples that can mislead a model is a dynamic field of research, with an increasing number of attacks being introduced [3, 10, 11, 9, 12]. In this study, we concentrate on black box attacks, which do not have access to detailed model information, as these are more reflective of real-world scenarios. We evaluate our models against four types of attacks: random noise, common corruptions, transfer-based attacks, and decision-based attacks.\nRandom noise attacks involve applying noise sampled from known distributions (e.g., Gaussian, Uniform, and Salt and Pepper) to an input - see Appendix 7.1. Common corruptions correspond to distortions that can be found in real life computer vision applications (eg : motion blur) [8]. A transfer-based attack involves finding adversarial perturbations for a substitute model (an unregularized model in our case) and applying them to a target model. Evaluating robustness on transfer-based attacks is crucial because adversarial examples crafted for one model can also mislead another distinct model [13]. We find these perturbations by applying the Fast Gradient Sign Method (FGSM) [14] to the substitute model. Such adversarial samples are computed as :\n$X_{adv} = x + \\epsilon \\times sign(\\nabla_xL(\\theta,x,y))$"}, {"title": "3 A neural representational similarity regularizer", "content": "To increase the robustness of artificial neural networks to adversarial attacks, one research direction focuses on extracting and applying computational concepts from the mammalian brain. In particular, Li et al. [1] showed that adding a neural regularizer term to the training loss enhances the adversarial robustness of CNNs on image classification tasks. The regularization term is denoted by $L_{sim}$ as it depends on similarities between neural responses.\nThe loss function L is written as:\n$L = L_{task} + \\alpha L_{sim}$\nwhere $L_{sim}$ given image pairs $(i, j)$ is defined as,\n$L_{sim} = \\sum_{i\\neq j} (arctanh(S^{CNN}_{ij}) - arctanh(S^{target}_{ij}))^2$\nand \u03b1 is a parameter that sets the overall regularization strength. $S^{target}$ in eq. (3) is the target's pairwise cosine similarity between the representations of images i and j. $S^{CNN}$ measures the similarity between the representations of images i and j in a CNN. We compute it following the approach of Li et al. [1]. We combine feature similarities from a selection of K equally spaced convolutional layers and average the results through a trainable weights $\\gamma_l$, where l is the layer number. The latter are the output of a softmax function meaning $\\sum_l \\gamma_l = 1$ and $\\gamma_l \\geq 0$. Overall,\n$S^{CNN}_{ij} = \\sum_{l=1}^K \\gamma_l S^{CNN-l}_{ij}$\nwhere $S^{CNN-l}_{ij}$ is the mean-substracted cosine feature similarity between images i and j at layer l. Having $\\gamma_l$ be trainable enables the model to choose which layer(s) to regularize to match the similarity target.\nIn their setup, Li et al. [1] used a ResNet [18] to classify grayscale CIFAR-10 and CIFAR-100 datasets. To compute $S^{target}_{ij}$, neural responses were collected from mouse primary visual cortex (V1), while the mouse was looking at grayscale images from ImageNet dataset. However, in practice, due to noise in neural recording, $S^{target}_{ij}$ was not computed from the neural recordings directly. Instead, it was computed from a predictive model [19, 20] trained to predict the neural responses from images. The predictive model consisted of a 3-layered CNN with skip connections [19, 20, 1], it accounts for behavioural data such as pupil position, size and running speed on treadmill (Appendix 7.1).\nTraining is done by first processing a batch of images from the classification task dataset to calculate the classification loss $L_{task}$, and then processing a batch of image pairs from the regularization dataset to compute the similarity loss $L_{sim}$. We then compute the full loss L which we use for backpropagation."}, {"title": "4 A pixel similarity based regularizer", "content": "We inspect the similarity loss $L_{sim}$ (eq.(3)) introduced in [1] to extract its underlying strength. Our goal is to formulate a method that can bypass the use of neural recordings which can be costly.\nSince the primary visual cortex (V1), where the neural recordings come from, is the first visual processing area in the cortex, we inspect the correlation between the neural representational similarity and image pixel similarities (computed as described in Appendix 7.1). We observe that there is a high correlation between the two as shown in Fig. 2, left panel. Thus, we investigate the effect of using pixel similarities as target similarities in $L_{sim}$ instead of similarities obtained from neural recordings. To compare both approaches, we replicate the experimental setup in Li et al. [1]. We train a ResNet to classify grayscale CIFAR-10 and CIFAR-100, and use grayscale images from ImageNet data, as the regularization dataset (same datasets used in [1]). However, we differ from [1] in our choice of $S^{target}_{ij}$: we set $S^{target}_{ij}$ in eq. (3) to $S^{pixel}_{ij}$, where $S^{pixel}_{ij}$ is computed as the pixel cosine similarity between images which are flattened, mean subtracted and normalized. After training, we observe that the regularized model exhibits some enhancement in robustness, however this enhancement is not consistent across different image perturbations and adversarial attacks. For example, we see a modest enhancement in robustness to Gaussian noise (Fig. 1a), and decision-based Boundary Attack (Fig. 1c), but this was not the case for transferred FGSM attack (Fig. 1b). For Uniform noise and Salt and Pepper noise, we see an enhancement in robustness at large noise levels (see Fig. 12 in 7.2).\nIf we go back and visually examine the last two panels in Fig.2, we observe that the image pixel and neural representational similarity matrices have similar patterns, however the pattern is more enhanced in the neural representational similarity matrix. Thus, at the first stage of cortical visual processing, the brain seems to roughly preserve an underlying structure of the image pixel similarities but amplify it. Based on this observation, we define a new target similarity $S^{Th}_{ij}$, such that\n$S^{Th}_{ij} =\\begin{cases}\n1, & \\text{if } S^{pixel}_{ij} > T_h, \\\\\n-1, & \\text{if } S^{pixel}_{ij} < -T_h, \\\\\n0, & \\text{if } |S^{pixel}_{ij}| < T_h\n\\end{cases}$"}, {"title": "5 Experiments", "content": "We train ResNets [18] on image classification tasks using $L = L_{task} + \\alpha L_{sim}$ (eq. (2), and setting $S^{target} = S^{Th}$ in the regularization in the loss $L_{sim}$ (eq. (3)). The (\u03b1,Th) pairs used for each classification-regularization dataset pairs are reported in Appendix 7.5. We also report in Appendix 7.6 the value of \u03b3l (as defined in Section 3) for each dataset combination. After training, we evaluate the regularized model robustness to a set of black box adversarial attacks (Section 2). Even though we report below results for ResNet18, we show in 7.4 and 7.8 results for ResNet34. To allow direct comparison with Li et al. [1], we mainly show results using grayscale CIFAR-10 as classification dataset. However, to demonstrate the success of our method, we also show results using colored CIFAR-10. Furthermore, in the appendix we show results using other classification datasets like grayscale CIFAR-100 (7.4), colored CIFAR-100 (7.8), MNIST (7.4) and FashionMNIST (7.4). The details of our experimental setup and implementation can be found in Appendix 7.1."}, {"title": "5.1 Robustness to adversarial attacks", "content": "We first test the robustness of regularized models using grayscale CIFAR-10 and grayscale ImageNet as classification and regularization datasets respectively, as used in Li et al. [1]. We first show robust-ness to Gaussian noise perturbations. We find that regularized models exhibit a substantial increase in robustness when compared to unregularized models as seen in Fig.3, left panel. They also show a similar performance to neural regularized models [1] (Fig.3, left panel). The robustness of mod-els regularized using $S^{Th}$, to Uniform and Salt and Pepper perturbations can be found in appendix 7.3.\nWe then test robustness to stronger black box attacks, particularly, to transferred FGSM [14] perturba-tions from an unregularized model, and decision-based Boundary Attack [9] (Section 2). We observe an increase in robustness to both attacks (Fig.3, center and right panels). Note that for decision-based Boundary attack, the larger the perturbation size between the adversarial input and the original image, the better in terms of robustness. Again, we observe that models regularized using $S^{th}$ perform similar to those regularized using neural data [1] (Fig. 3, center and right panels).\nThe experiments above demonstrate that we can obtain similar robustness to neural regularized models [1] by simply regularizing using $S^{Th}$, which does not require neural data, and relies only on the original unaugmented regularization dataset."}, {"title": "5.2 Hyperparameter selection and consistent behavior across attacks", "content": "An important question is how to select an \u03b1, Th hyperparameter pair ? We propose a criteria to select those hyperparameters, as follows. A suitable pair should (1) be such that the resulting model has an 'acceptable' accuracy on the distortion-free dataset, and (2) showcases an increase in robustness to adversarial attacks. To properly define what we mean by this, we introduce the following quantities $R_o$, $R_D$, $U_o$ and $U_D$. Where, $R_o$ is the regularized model's accuracy on distortion-free images and $R_D$ its accuracy at high distortion level. $U_o$ and $U_D$ are their equivalent for the unregularized model. The ratios $\\frac{R_o}{U_o}$ and $\\frac{R_D}{U_D}$ reflect how our regularization affects the model's accuracy at zero and high distortion levels. To meet condition (1), we require that $\\frac{R_o}{U_o} > A_0$, where $A_0$ is user defined. We select $A_0 = 0.9$. Condition (2) is simply met by requiring that $\\frac{R_D}{U_D} > 1$.\nWe can visualize the performance of a model by plotting $(\\frac{R_o}{U_o})$ vs $(\\frac{R_D}{U_D})$ for each \u03b1, Th pair. This allows the user to select the hyperparameter pair based on the selection criterion that they choose. In Fig.4 we show the above plot for different adversarial attacks (the gray shaded planes).\nAs seen, the regularization method produces a consistent behavior across all the adversarial attacks that we use. This allows the user to use the simplest attack, like adding Gaussian noise to the images, to select the \u03b1, Th pair."}, {"title": "5.3 Robustness across datasets combinations", "content": "Our method is flexible to the choice of the regularization dataset. We find that regularizing on different datasets leads to an increase in model robustness, but, there is a quantitative difference in the robustness level achieved by regularizing on different datasets. Fig. 5 shows the performance of a ResNet18 trained to classify grayscale CIFAR-10 regularized on grayscale images from three datasets separately (CIFAR-10, CIFAR-100 or ImageNet) for three attacks (Gaussian noise, transferred FGSM, and decision-based Boundary Attack) compared to an unregularized model."}, {"title": "5.4 Robustness to common corruptions", "content": "Regularized models are also more robust than their unregularized counterpart on common corruptions. We evaluate regularized models on grayscale CIFAR-10-C dataset [8] which consists of grayscaled CIFAR-10 images with common corruptions that can be found on everyday computer vision appli-cation. Evaluating on common corruptions at different severity levels is critical as they simulate real world conditions. Fig. 6 shows the performance of a ResNet18 trained to classify CIFAR-10 regularized with grayscale images from ImageNet dataset vs unregularized model."}, {"title": "5.5 Frequency decomposition of adversarial perturbations and common corruptions", "content": "To understand the strengths and weaknesses of our regularization method, we investigate the frequency components present in the minimal perturbation required to flip the decision of unregularized and regularized models which we compute via a decision-based Boundary Attack [9]. We observe that models regularized using pixel-based similarities (STh) rely more on low frequency information than their unregularized counterparts (Fig. 7 center and right panels). We further evaluate our regularized model performance on grayscale CIFAR-10-C [8] following the approach described in [6], where we categorize the 15 corruptions in CIFAR-10-C into Low, Medium, and High frequency based on their spectra. Our results show that regularized models outperform unregularized ones, especially on high-frequency corruptions, confirming our findings. Such a reliance on low-frequency information has also been observed in models subjected to neural regularization as explained in [6]."}, {"title": "5.6 Computational advantages", "content": "In addition to being a simple method to apply, our regularization method is computationally inexpen-sive. First, in regard to training time, for k image pairs per regularize batch, the additional time taken per batch to train the model corresponds to 2 \u00d7 k additional forward passes. We see in Fig. 8 that the method is successful for regularization batch size values: 4, 8, 16, 32. Choosing a smaller batch size can help in cutting the extra training time needed for successful regularization."}, {"title": "5.7 Results using color datasets", "content": "Our previous results were obtained using grayscale datasets, which as we previously mentioned, were chosen to allow direct comparison with Li et al. [1], and for consistency. Here, we show that our method is also successful when using color datasets, which are more utilized in practice.\nIn Fig. 10 we show results using color CIFAR-10 as classification dataset, and color CIFAR-10, CIFAR-100 or ImageNet as regularization datasets. As seen, there is an increase in the model's robustness for all regularization datasets."}, {"title": "6 Conclusion and discussion", "content": "Extracting the working principles of the brain to advance AI is a long-term goal of neuroscience. To further this goal, we examined a brain-inspired method for adversarial robustness proposed by Li et al. [1]. This method uses neural recordings from the brain to align learned representations in an artificial neural network with brain representations through a regularization term added to the training loss. We extracted the core working principle behind this regularizer and proposed a simple, pixel-based regularization scheme that achieves similar performance, and gave an intuitive interpretation of our method."}, {"title": "7 Appendix / supplemental material", "content": "Training - Neural Predictive Model - We train neural predictive models [19, 20] to predict neural responses using all scans (measurements of neurons) with the same training configurations and neural data available in the codebase left by Li et al. [1].\nTraining - Image classification - All models were trained by stochastic gradient descent on a NVIDIA A100-SXM4-40GB GPU. Models classifying grayscale CIFAR-10 were trained during 40 epochs. Training and regularizing a ResNet18 CIFAR-10 took in average 34 min to run. We used a batch size of 64 for the classification pathway and a batch of 16 image pairs for the regularization pathway. The number of regularization images used by default is 5,000. Target similarities $S^{pixel}$ are computed as follows. We compute the cosine similarity between images which are flattened, mean subtracted and normalized. We use the same learning schedule as in [1]. Models were trained using Pytorch [22].\nThe classification datasets used are : MNIST [23], FashionMNIST [24], grayscale CIFAR-10 [25], grayscale CIFAR-100. The regularization datasets used are : MNIST, FashionMNIST, grayscale CIFAR-10, grayscale CIFAR-100 and grayscale ImageNet [26].\nAdversarial attacks (see Section 2) - All perturbations are reported for image pixels in the range [0, 1]. We evaluate model robustness to random noise and transferred FGSM [14] perturbations by measuring the accuracy of evaluated models for distinct perturbation strengths \u03f5. We empirically find the range of perturbation strengths used to evaluate models, such that the unregularized model performs bad at the highest \u03f5 used for that particular model-dataset-attack combination."}, {"title": "7.1 Experimental setup", "content": "The decision-based Boundary Attack [9] was applied via Foolbox [27] using 50 steps, unless stated otherwise. To evaluate the success of this attack, we measure the following score :\n$S(M) = median_{i}(\\frac{||\\eta_{M}(X_{original})||_{2}}{||X_{original}||_{2}})$"}, {"title": "7.2 Robustness to random noise for models trained to classify CIFAR-10 regularized using $S^{pixel}$", "content": null}, {"title": "7.3 Robustness to random noise for models trained to classify CIFAR-10 regularized using $S^{Th}$", "content": null}, {"title": "7.4 Robustness on image classification task for different classification-regularization datasets", "content": null}, {"title": "7.5 Hyperparameters used for regularization", "content": "The \u03b1, Th value pairs selected in our work yield an acceptable accuracy-robustness trade-off ($\\frac{R_D}{U_D} > 1$ and $\\frac{R_o}{U_o} \\geq 0.9$) across attacks during regularization."}, {"title": "7.6 Weighting candidate layers", "content": "We here report the weights \u03b3l obtained after training, for different models and dataset-combinations."}, {"title": "7.7 Relevance of different similarity ranges", "content": "In this subsection, we share an investigation regarding the target similarity ranges which matter the most for regularization. We define the following sets of regularization target pairs i, j : $S^{Th}_{low} := \\{S_{ij} | |S_{ij}| < Th\\}$, $S^{Th}_{high} := \\{S_{ij} | |S_{ij}| > Th\\}$, $S^{Th_{double}} := \\{S_{ij} | |S_{ij}| < Th2 or |S_{ij}| > Th1\\}$."}, {"title": "7.8 Regularization on color datasets", "content": "Our previous results on CIFAR-100 were obtained using grayscale datasets, which as we previously mentioned, were chosen to allow direct comparison with Li et al. [1], and for consistency. Here, we show that our method is also successful when using color datasets, which are more utilized in practice.\nIn Fig. 23 we show results using color CIFAR-100 as classification dataset, and color CIFAR-10, CIFAR-100 or ImageNet as regularization datasets. As seen, there is an increase in the model's robustness for all regularization datasets."}, {"title": "7.9 Benchmarking our method against those in the RobustBench leaderboard", "content": "We benchmarked a model trained to classify (colored) CIFAR-10 regularized on (colored) CIFAR-100 using our method, against 3 models referenced in the RobustBench [28] leaderboard - commonly used to systematically track the real progress in adversarial robustness - on CIFAR-10-C, at sever-ity 5. We select $L_\\infty$, $L_2$ and common corruption (thereafter CC) specific models denoted as Carmon2019Unlabeled [29], Engstrom2019Robustness [30], Modas2021PRIMEResNet18 [31] respectively."}, {"title": "7.10 Fourier spectra of common corruptions", "content": "Following the approach of [6], we divided the common corruptions present in CIFAR-10-C into three categories based on their corresponding dominating frequencies whether they belong to low, medium or high frequency ranges. Low frequency corruptions are composed of 'snow', 'frost', 'fog', 'brightness', 'contrast' corruptions. Medium-frequency corruptions are composed of \u2018motion blur', 'zoom blur', 'defocus blur', 'glass blur', 'elastic transform', 'jpeg compression' and 'pixelate' corruptions. High-frequency corruptions are composed of \u2018gaussian noise', 'shot noise' and 'impulse noise' corruptions."}]}