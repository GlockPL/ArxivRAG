{"title": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks", "authors": ["Rongxing Liu", "Kumar Shridhar", "Manish Prajapat", "Patrick Xia", "Mrinmaya Sachan"], "abstract": "Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.", "sections": [{"title": "1. Introduction", "content": "When people first encounter complex reasoning tasks, such as solving mathematical problems, they often make mistakes or approach them inefficiently [3]. However, with experience, humans tend to improve their performance by replacing ineffective or incorrect strategies with more effective ones, using a mix of strategies tailored to the specific task [1, 2, 15, 30, inter alia].\nLanguage Models (LMs) similarly struggle with reasoning tasks, sometimes producing incoherent results [11, 16, 25]. A common remedy is to resample the output, a process known as refinement. This refinement may involve reusing the same reasoning approach [16] or adopting an entirely new one [25]. In addition, providing feedback on initial results has proven beneficial during resampling [11, 13, 24, 34, inter alia]. This raises a critical question: Can LMs be taught to optimize their choice of reasoning strategy for specific tasks overtime on the first trial, much like humans do?\nTo address this question, we propose a novel framework called SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), which allows LMs to learn optimal strategy selection through a continuous self-learning approach. We model the task of identifying the optimal strategy as a Markov Decision Process (MDP) [21, 28], where the agent (LM) starts with its pre-trained knowledge and iteratively improves its performance by learning from its own outputs and strategy choices. By integrating the LM\u2019s reasoning abilities with reinforcement learning-driven self-improvement, the agent can simulate different reasoning"}, {"title": "2. Methodology", "content": "Let q be a problem best solved with multi-step reasoning. An example is presented in the form of a mathematical word problem in Figure 1. An agent or language model (LM) can approach it using various strategies, such as solving it step by step (Chain of Thought, CoT [33]), decomposing it into subproblems, and solving each one (Least to Most, L2M [39]), or writing a program to solve it programmatically (Program of Thought, PoT [4]), among others. A common method is to prompt the LM with a specific strategy for solving the task. However, LMs are error-prone but can fix their answers when asked to do so, a process called refinement. In the refinement process, LMs can either stick to the same strategy [16] or switch to a more effective reasoning strategy [25]. Ideally, LMs could learn to choose the best strategy and reasoning path on the first try, minimizing the need for costly refinement.\nOur objective: The primary goal of our work is to enable language models (LMs) to autonomously learn and select the most effective strategies for various reasoning tasks on their first attempt, thereby improving both efficiency and accuracy. Unlike traditional self-refinement methods that require multiple inference passes or external feedback, our approach aims to internalize the learning process within the LM, allowing it to adjust its strategy selection based on past experience. This mirrors how humans learn to choose optimal strategies through experience when faced with complex tasks."}, {"title": "2.1. SMART: Self-learning Meta-strategy Agent for Reasoning Tasks", "content": "We model the strategy selection process as a Markov Decision Process (MDP), where the LM acts as an agent that interacts with the environment (the reasoning tasks) by selecting strategies and observing the outcomes. Using reinforcement learning techniques, the LM can learn a policy that maximizes the expected reward, effectively learning to choose the optimal strategy for each task. This framework allows the LM to simulate different reasoning strategies, evaluate their effectiveness based on past outcomes, and adjust its strategy choice accordingly.\nIn the following sections, we formalize the problem formulation, define the agent's policy, and describe the learning objective. We then present our two-stage process with iterative refinement, which allows the agent to learn from its own reasoning processes and improve its strategy choice over time.\nMDP Setup: We model the strategy selection framework as a Markov Decision Process (MDP) given by the tuple $(S, A, P, R, \\mu)$. Here, $S$ represents the state space encapsulating all possible states of the environment, where each state $s \\in S$ is the current problem statement or the subsequent LM response to it. The initial state distribution is given by $\\mu$. The action space, $A$, is the set of all strategies available to the agent, where each action $a_t \\in A$ corresponds to the choice of a particular strategy at time $t$. The transition function $P : S \\times A \\rightarrow S$ defines the probability of transitioning to a new state $s_{t+1}$ after applying strategy $a_t$ in state $s_t$. Particularly for our case, the transition function is non-deterministic as the next state is sampled by the agent (see Algorithm 1 later). The reward function $R : S \\times A \\rightarrow \\mathbb{R}$ assigns a scalar reward based on the correctness of the result after applying the chosen strategy.\nWe start with the initial sampling step, where the LM chooses a strategy to solve the given task. In other words, given a problem statement $s_1 \\sim \\mu$, the agent draws a strategy $a_1$ to solve the problem according to its policy (see below). Given the initial problem $s_1$ and strategy $a_1$, the environment transitions to the next state $s_2 \\sim P(\\cdot|s_1, a_1)$ with transition probability $P$ and receives a reward $r_1$ indicating the correctness of the response. If a correct strategy is chosen and the LM solves the task using that strategy, the process terminates. Otherwise, the agent chooses another strategy $a_2$ to solve the problem, and"}, {"title": "Agent's policy:", "content": "We start by defining a history $h_t := \\tau_{1:t}$ that includes the past actions up to time $t - 1$ and the observed states up to time $t$. Then, we model the agent using a non-Markovian stochastic policy $\\pi_{\\theta}(a_t|h_t)$ parameterized by $\\theta$, which models the probability of choosing the action $a_t$ given the history $h_t$:\n$\\pi_{\\theta}(a_t|h_t) = Pr(a_t = a|h_t; \\theta)$"}, {"title": "Objective function:", "content": "We want to optimize the following objective:\n$\\theta^* = \\underset{\\theta}{\\text{arg max }} J(\\pi_{\\theta}) = \\underset{\\theta}{\\text{arg max }} \\mathbb{E}_{s_1, a_1 \\sim \\mu_{\\text{test}}, \\pi_{\\theta}(a_1|s_1)}[r_1(s_1, a_1]$ \nwhere, $\\mu_{\\text{test}}$ represents the state distribution over the test data."}, {"title": "Two-Stage Process with Refinement", "content": "Stage 1 (Initial Sampling)\n1. The process begins with a problem statement $s_1 \\sim \\mu$ where $\\mu$ represents the initial state distribution.\n2. The agent samples an action $a_1$ from the policy $\\pi_{\\theta}(a_1|h_1)$, where $h_1 = s_1$ for the step 1.\n3. The agent then generates an output based on strategy $a_1$\n4. The agent receives reward $r_1(h_1, a_1)$ based on it correctness as follows:\n$r_1 = \\begin{cases}\n1, & \\text{if the output is correct} \\\\\n0, & \\text{otherwise}\n\\end{cases}$\n5. Termination Check: If $r_1 = 1$, the process terminates successfully."}, {"title": "Implicit bias for Stage 1 to choose the right strategy in the first attempt:", "content": "As described in (2.1), our goal is to maximize the reward received as early as possible. To implicitly bias the model towards selecting the correct action in earlier steps, we adjust the dataset $D$ by replacing the sequences of unsuccessful actions with the final successful action taken at the initial state. Specifically, for trajectories where the problem is solved at time $T' (T' > 1)$, we replace the samples $(h_1, a_1,r_1),..., (h_{T'},a_{T'},r_{T'})$ with $(h_1, a_{T'}, r_{T'})$. This encourages the model to learn to take the correct strategy $a_{T'}$ for the problem statement $s_1$ in the first step.\nSince we only update the model based on correct outputs, the policy update can be viewed as maximizing the likelihood of the correct actions given the history:\n$\\theta^* = \\underset{\\theta}{\\text{arg max }} \\sum_{(h_i, a_i) \\in D} \\text{log } \\pi_{\\theta}(a_i|h_i)$"}, {"title": "3. Experimental Details", "content": "SMART operates within a self-learn framework, where we prompt a pre-trained model with 8-shot examples to collect the initial training data. Prompts used for various strategies are provided in Subsection 8.1. The 8-shot pre-trained model also serves as a baseline comparison for our method. We use the MetaMath dataset [38], a variant of the GSM8K [5] training set, which contains 110K reasoning problems. We evaluate our methodology on the GSM8K test set, which contains 1,319 samples. To show the generalization ability of our method, we also test on two out-of-distribution datasets: the SVAMP dataset [19] with 1,000 samples where the questions are made more challenging by altering them in a non-trivial way, and the ASDiv dataset [17] with 2,300 samples, which consists of diverse mathematical problems from elementary to middle school."}, {"title": "4. Results", "content": "SMART significantly improves results on in-distribution dataset: We compared SMART with baselines on the GSM8K dataset, which we also consider to be an in-distribution dataset since the train and test sets have the same distribution. Table 1 shows that SMART outperformed the baseline in its first iteration on the GSM8K dataset, achieving a gain of +6 points for both the Gemma 7B and Mistral 7B models (40.4 \u2192 46.5 and 56.9 \u2192 63.8, respectively). Although Qwen2's performance is already very strong on the GSM8K dataset, we still observed a gain of +2.6 points (81.9 \u2192 84.5) in the first iteration. After a few more iterations, we saw a total gain of +15 points for Gemma 7B (40.4 \u2192 55.4), +11 points for Mistral 7B (56.9 \u2192 67.9), and +4 points for Qwen2 7B (81.9 \u2192 85.4).\nSMART serves as a great refinement strategy: Since SMART involves iterative refinement to find the optimal action given the trajectory, we also compare against two refinement baselines: refinement with the same strategy [16] and refinement with a strategy change [25]. We used the Oracle Verifier, which identifies incorrect samples and refines them either with the same strategy or by choosing a different one. 1 Table 1 compares the refinement accuracy with SMART and our proposed methodology shows significant improvements over the baselines. Gemma 7B gains over +16 points (48.9 \u2192 67.5) compared to the best refinement baseline, Mistral 7B gains +8 points (66.5 \u2192 78.0), and Qwen2 7B gains +1.5 points (86.9 \u2192 91.9).\nSMART generalizes well to out-of-distribution dataset: We test our trained checkpoints using our proposed approach SMART on two out-of-distribution datasets: ASDiv and SVAMP. These are referred to as out-of-distribution because the model was not trained on these datasets but only evaluated on them. Table 2 compares the results of SMART with the baselines (created using 8-shot in context examples) across the three reasoning strategies: CoT, L2M, and PoT. Among the three models, Gemma 7B showed"}, {"title": "5. Discussion", "content": "How the strategy distribution changes over iterations: With SMART, the goal is to select the desired strategy at the first attempt, i.e., over iterations the LM should learn to select the appropriate strategy for a given task. Figure 2 shows the changes in strategy distribution over iterations for the Gemma 7B model on the GSM8K dataset. As indicated by the baseline in Table 1, PoT emerges as the best strategy for the Gemma 7B model, followed by CoT, with L2M being the least effective. A similar pattern is observed in Figure 2, where the model increasingly favors PoT (indicated by the upward trend in the red line) while decreasing its preference for the other two strategies. Correspondingly, the accuracy for PoT improves the most, followed by CoT and L2M, demonstrating that over iterations the model learns to select the optimal"}, {"title": "SMART with better starting samples:", "content": "Since we start with the model-generated samples, for a weaker model the starting samples can be improved if those samples come from the stronger model. We investigated whether the SMART based self-learning approach could be extended to a setup where the initial data points are collected from a stronger model to initiate training for a weaker model. This is particularly beneficial for weaker models that cannot independently initiate the self-learning process using SMART due to their limited capabilities on a given task. Initially, we collected data using 8-shot in-context examples generated by the Llama3 8B model [6]. This approach yielded an average accuracy of about 80% across all three strategies, significantly higher than Gemma 7B's initial accuracy of about 40%. Figure 3 illustrates the comparison between different iterations of SMART and the pre-trained Gemma 7B baseline at 40.4%, as well as the fine-tuned Gemma 7B baseline at 68.7%. Using SMART resulted in an additional improvement of +3.1 points (from 68.7% \u2192 71.8%)."}, {"title": "6. Related Work", "content": "Refinement in LLMs: Refinement refers to the process of improving the initial output of large language models (LLMs) through iterative adjustments. This refinement can be achieved by following the same method used initially [16], by incorporating feedback while using the same approach [11, 13, 24, 34], or by using an alternative method [25, 26]. However, recent studies have shown that naively applying self-correction can sometimes degrade performance [11, 22, 31], highlighting the need for more effective strategies. Supervised fine-tuning with feedback from larger models [22, 37], or using an ensemble of models [8], has produced notable results. Nevertheless, relying on larger or multiple models for feedback presents challenges. In contrast, our method learns the optimal strategy during training and applies it correctly on the first attempt, eliminating the need for refinement.\nSelf-Training in LLMs: Self-training is a semi-supervised learning method in which the model's own predictions are used as additional data to improve its performance [23, 36]. This technique has been applied to NLP tasks, such as machine translation [7, 9, 27]. We use self-learning principles to generate new data and continually update the policy in an on-policy fashion. This approach can be viewed as an on-policy counterpart to self-imitation learning [18], where the policy learns from prospective successful trajectories in its initial stages, rather than imitating past successful behavior."}, {"title": "7. Conclusion", "content": "We present SMART: Self-learning Meta-strategy Agent for Reasoning Tasks, a solution to the challenges LMs face in selecting strategies for complex reasoning tasks. By modeling the strategy selection process as"}, {"title": "8. Appendix", "content": "8.1. Prompts\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\nInstruction: Solve the given math problem step by step. Put your final answer after 'Final answer:'.\nInput: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total?\nResponse: The shirts cost 3*$20=$<<3*20=60>>60 before tax The tax cost $60*.1=$<<60*.1=6 6 So in total they paid $60+$6=$<<60+6=66 66 Final Answer: 66<eos>\n[7 more examples randomly sampled from the training set]\nInput: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total?\nResponse: Model generated response"}]}