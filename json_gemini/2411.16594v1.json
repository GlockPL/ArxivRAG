{"title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge", "authors": ["Dawei Li", "Bohan Jiang", "Liangjie Huang", "Alimohammad Beigi", "Chengshuai Zhao", "Zhen Tan", "Amrita Bhattacharjee", "Yuxuan Jiang", "Canyu Chen", "Tianhao Wu", "Kai Shu", "Lu Cheng", "Huan Liu"], "abstract": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area.", "sections": [{"title": "1 Introduction", "content": "Assessment and evaluation have long been essential yet challenging tasks in machine learning and natural language processing (NLP), particularly for scoring and comparing various attributes (e.g., quality, relevance, and helpfulness) of a list of given candidates (Sai et al., 2022; Chang et al., 2024). Traditional evaluation methods rely on static metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which measure quality by calculating word overlap between output and reference texts. While these automatic metrics are"}, {"title": "2 Preliminary", "content": "In this section, we aim to provide a detailed definition of LLM-as-a-judge, discussing the various input and output formats separately in Section 2.1 and Section 2.2.\n2.1 Input\nGiven a judge LLM J, the assessment process can be formulated as:\nR = J(C1, ...Cn).\n(1)\nHere Ci is the ith candidate to be judged and R is the judging result. In this section, we categorize two kinds of input formats based on their different candidate number n.\nPoint-Wise: When n = 1, it becomes a point-wise judgment where the judge LLMs will solely focus on one candidate sample (Gao et al., 2023).\nPair/List-Wise: When n \u2265 2, it becomes a pair-wise (n = 2) or list-wise (n > 2) judgment where multiple candidate samples are provided together for the judge LLMs to compare and make a comprehensive assessment (Zheng et al., 2023).\n2.2 Output\nIn this section, we discuss three kinds of output of the judgment based on the different formats of R.\nScore: When each candidate sample is assigned a continuous or discrete score: R = {C1: S1,..., Cn: Sn}, it becomes a score-based judgment. It is the most common and widely employed protocol that utilizes LLM to make a score for making quantitative comparisons (Li et al., 2024a) or attribute detection (Xie et al., 2024a).\nRanking: In ranking-based judgment, the output is a ranking of each candidate sample, represented as R = {C\u2081 > ... > Cj}. This comparative approach is useful in scenarios where establishing a rank order among candidates is required (Li et al., 2023).\nSelection: In selection-based judgment, the output involves selecting one or more optimal candidates, represented as R = {Ci, ..., Cj} \u2283 {C1,...Cn}. This method is particularly useful in decision-making (Yao et al., 2023a) or content-filtering (Li et al., 2024c) contexts."}, {"title": "3 Attribute", "content": "In this section, we categorize current research in LLM-as-a-judge from attribute perspectives. Figure 3 gives an overview summarization of what aspects can be assessed by the judge LLMs.\n3.1 Helpfulness\nModern state-of-the-art LLMs undergo instruction tuning and alignment processes, enabling them to follow user instructions and respond effectively. This alignment step relies on large quantities of helpful and harmless data, typically gathered as human preference data, which is then used in reinforcement learning for alignment training. Given the high cost of obtaining such alignment data, recent efforts have explored using LLMs to label helpfulness, as well as to generate or evaluate alignment data (Bai et al., 2022). Authors in (Guo et al., 2024) use an LLM in an online manner to obtain preferences for direct alignment of another LLM. Some recent work shows that helpfulness feedback from AI, i.e., LLMs are comparable to human feedback (Lee et al., 2023). There have also been successful LLMs (Zhu et al., 2024a) with superior performance that have been fine-tuned with AI feedback data thus demonstrating the feasibility and usefulness of this method. Alongside these works in alignment, general purposed frameworks leveraging LLMs as evaluators are also crucial in evaluating the helpfulness of candidate responses (Zheng et al., 2023; Lin et al., 2023; Li et al., 2024e).\n3.2 Harmlessness\nEvaluating harmlessness in text data is important for both content moderation and for creating or curating synthetic datasets. Given how expensive and time-consuming human labeling efforts are, and inspired by prior work that looks into moral beliefs embedded in LLMs (Scherrer et al., 2023), many recent works have investigated the use of LLMs to evaluate harmlessness. State-of-the-art LLMS are capable of being used effectively for content moderation, either off the shelf when guided with some policy guidelines\u00b3, or when fine-tuned on safe/unsafe data (Inan et al., 2023; Zhang et al., 2024e)4. Ye et al. (2024b) explore the feasibility of using LLMs to evaluate harmlessness in a fine-grained manner, among other attributes, and find that proprietary models perform much better than open source ones. Wang et al. (2024i) use OpenAI's GPT-4 to evaluate harmlessness and further compare the performance with that of a smaller pretrained language model fine-tuned for this specific task. Additionally, Bai et al. (2022) use principles to guide LLMs to make harmlessness evaluations for alignment purposes, a paradigm they call Constitutional AI. (Phute et al., 2023) use the same LLM to evaluate its responses as harmful, and provide insights and best practices for GPT-3.5 and Llama-2. Xie et al. (2024a) perform a comprehensive comparison of several LLMs on a benchmark of LLM safety refusals and find small LLMs are effective safety judges when used in fine-tuned settings. During inference time, (Li et al.) propose Rewindable Auto-regressive INference (RAIN), which allows LLMs to conduct self-evaluation and rewind for AI safety.\n3.3 Reliability\nReliability is a crucial attribute for LLMs, enabling them to generate factual and faithful content while also expressing uncertainty or acknowledging gaps in knowledge about certain topics. Regarding factuality, Wang et al. (2024a) introduce HALU-J, a critique-based hallucination judge that enhances factuality assessment by selecting relevant evidence and providing detailed critiques. Cheng et al. (2023) design an automated evaluation method using GPT-4 to judge whether a model's output is hallucinated. Additionally, several works adopt judge LLMs for long-form factuality evaluation. In the context of dialogues, Luo et al. (2024) collect a large-scale benchmark for automatic dialogue-level hallucination evaluation. Based on this dataset, they introduce HalluJudge, a specialized judge language model for evaluating hallucinations at the dialogue level. Min et al. (2023) develop FactScore, a fine-grained method for evaluating the factuality\n3.4 Relevance\nRelevance measures the extent to which generated or retrieved content aligns with the original query. Traditional relevance assessment methods often rely on keyword matching (Robertson et al., 2009) or semantic similarity (Gao et al., 2021), which struggle to capture subtle differences or nuances in context. Relevance assessment with judge LLMs has been explored and validated to be a more fine-grained and effective manner across various applications (Chiang and Lee, 2023). In conversation evaluation, Lin and Chen (2023a) first propose to replace the expensive and time-consuming human annotation with LLM judgment in relevance assessment, providing conversation context and generated response for the judge LLM to evaluate. Similarly, Abbasiantaeb et al. (2024) apply LLM-as-a-judge in conversation search, collaborating with human annotators to address issues related to incomplete relevance judgments. In retrieval-augmented generation (RAG) scenarios, Li and Qiu (2023) utilize LLMs to determine which historical memory is most relevant for solving the current problem. Following this concept, Li et al. (2024c) also propose to adopt LLM as a re-ranker to judge and filter out noisy and irrelevant knowledge in a sub-knowledge graph. Recently, LLM-as-a-judge has also been used in multimodal applications for relevance judgment. Both Yang and Lin (2024) and Chen et al.\n3.5 Feasibility\nThe potential of LLMs can be further released by complicated and well-designed reasoning pipelines. In these agentic LLMs, assessing the feasibility of candidate actions or steps is crucial for the success of planning, reasoning, and decision-making. While some works leverage metrics or external tools for this feasibility assessment (Huang et al., 2023a; Yuan et al.), many others leverage LLMs themselves to select the most appropriate and reasonable actions to perform. Hao et al. (2023) first propose to prompt the LLM to do self-evaluation and generate feasibility judgment as a reward signal to perform Monte Carlo Tree Search (MCTS). Similarly, Yao et al. (2023a) suggest adopting the LLM as the state evaluator for potential step searching in their proposed tree-of-thought (ToT) framework. Besta et al. (2024) replace the tree structures used in previous studies with graph structures and employ the LLM to assign a score to each thought based on its feasibility or correctness. In multi-agent collaboration systems, both Liang et al. (2023) and Li et al. (2024b) propose to leverage the judge LLM to select the most feasible and reasonable solutions among multiple candidates' responses. Besides, there are also works adopt the judge LLMs to perform feasibility assessment in API selection (Zhao et al., 2024b), tool using (Yang et al., 2023) and LLM routing (Ong et al., 2024).\n3.6 Overall Quality\nAs previously mentioned, LLM-as-a-judge can be employed to perform multi-aspect and fine-grained assessments across various tasks and applications. However, in many cases, a general assessment is still required to represent the candidates' overall quality for comparison or ranking purposes. One straightforward approach to obtain this overall score is to calculate the average of the aspect-specific scores (Lin et al., 2023; Lin and Chen, 2023b). Additionally, several other studies present assessment results for each attribute and prompt"}, {"title": "4 Methodology", "content": "In this section, we present commonly adopted methods and tricks for LLM-as-a-judging, splitting them into tuning approaches (Section 4.1) and prompting strategies (Section 4.2).\n4.1 Tuning\nTo enhance the judging capabilities of a general LLM, various tuning techniques have been employed in different studies. In this section, we discuss these tuning approaches for LLM-as-a-judge from two perspectives: data sources (Section 4.1.1) and training methods (Section 4.1.2). Table 1 collects all the research papers focused on tuning the judge LLMs.\n4.1.1 Data Source\nManually-labeled Data: To train a judge LLM with human-like criteria, one intuitive method is to collect manually-labeled samples and corresponding judgments. Many previous works have leveraged and integrated existing sources to build comprehensive datasets for tuning judge LLMs. Vu et al. (2024) construct a large and diverse collection of over 100 quality assessment tasks, comprising more than 5 million human judgments, curated and standardized using publicly available human evaluations from prior research. Similarly, Wang et al. (2024h) propose PandaLM and collect a diverse set of human-annotated test data, where all contexts are human-generated and labels align with human preferences. To enhance the policy's judging ability in alignment data synthesis, Lee et al. (2024) augment the supervised fine-tuning (SFT) dataset with a pairwise judgment task, where the instruction is to select the chosen response from a set of options.\nThere are also some works that collect datasets for fine-grained judgment feedback. Xu et al. (2023) introduce InstructScore, an explainable text generation evaluation metric, and curate the MetricInstruct dataset, which covers six text generation tasks and 23 datasets. Liu et al. (2024a) collect ASPECTINSTRUCT, the first instruction-tuning dataset tailored for multi-aspect NLG evaluation, spanning 27 diverse evaluation aspects across 65 tasks. Yue et al. (2023) first propose the evaluation of attribution and fine-tune judge LLMs with data from related tasks such as question answering, fact-checking, natural language inference, and summarization. One distinct approach is from Ke et al. (2024), which first prompts GPT-4 to produce the feedback and manually check its generated texts for each user query, revising them if necessary to improve the quality.\nSynthetic Feedback: While manually-labeled feedback is high-quality and accurately reflects human judgment preferences, it has limitations in terms of quantity and coverage. As a result, some researchers have turned to synthetic feedback as a data source for tuning judge LLMs. One approach in this direction relies on the judge LLMs themselves to generate the synthetic feedback. For example, Wu et al. (2024) construct pairwise feedback for judgment enhancement by prompting the policy LLMs to evaluate their own judgments. Wang et al. (2024f) prompt the LLM to generate a \"noisy\" version of the original instruction and use the corresponding response to this corrupted instruction as the inferior response. Wang et al. (2024a) prompt GPT-4-Turbo to generate multiple pieces of evidence based on the original evidence for each instance, categorizing them into completely irrelevant evidence, partially irrelevant evidence and highly related evidence to train a hallucination judgment LLMs. Park et al. (2024) build OFFSETBIAS, a pairwise preference dataset that leverages GPT-4 to generate bad, off-topic and erroneous responses and perform difficulty filtering. For safety judging, Xie et al. (2024a) adopt GPT-4 as the classifier to map each data point to a predefined safety category to train an automated evaluator. Different from previous works, Li et al. (2024e) adopt GPT-4 to synthesize both pairwise and pointwise data to train a generative judge LLM. For pointwise data, they adopt a \"divide-and-conquer\" strategy, where two critiques are collected from GPT-4 for a single response, combined into\n4.1.2 Tuning Techniques\nSupervised Fine-tuning: Supervised fine-tuning (SFT) is the most commonly used approach to facilitate the judge LLMs to learn from pairwise (Wang et al., 2024h; Li et al., 2024e; Wang et al., 2023b; Zhu et al., 2023) or pointwise (Xie et al., 2024a; Wang et al., 2023b; Yue et al., 2023) judgment data. Among many works that adopted SFT, Vu et al. (2024) propose a supervised multitask training to tune their Foundational Large Autorater Models (FLAMe) across multiple mixed datasets of various tasks. To equip the judge LLM with both pairwise and pointwise judging capabilities, Kim et al. (2024) novelly propose joint training and weight merging approaches during the tuning stage and find the latter does not improve evaluation performances in the majority of cases. To obtain a\njudge model that can not only generate responses but also compare pairwise preferences, Lee et al. (2024) devise Judge-augmented Supervised Fine-tuning (JSFT) with an augmented preference learning dataset. During the training phase, Ke et al. (2024) enhance their model by adding simplified prompts to distinguish different parts of inputs and augment pairwise training data by swapping the order of two generated texts and exchanging the corresponding content in critiques. Xu et al. (2023) further fine-tune their INSTRUCTSCORE model on self-generated outputs to optimize feedback scores, resulting in diagnostic reports that are better aligned with human judgment. Liu et al. (2024a) also propose a two-stage supervised fine-tuning approach, first applying vanilla instruction tuning to equip the model with the ability to follow instructions for diverse evaluations. They then perform further tuning with auxiliary aspects to enrich the training process, incorporating an additional instruction-tuning stage to leverage potential connections to the target evaluation aspect.\nPreference Learning: Preference learning is closely aligned with judgment and evaluation tasks, especially comparative and ranking judgment. In addition to works that directly adopt or augment preference learning datasets for supervised fine-tuning judge LLMs, several studies apply preference learning techniques to enhance LLMs' judging capabilities. To enhance the quality of judgment provided by HALU-J, Wang et al. (2024a) further tune it with Directed Preference Optimization (DPO) (Rafailov et al., 2023) after the SFT stage under the multiple-evidence setting. Similarly, Park et al. (2024) apply DPO with synthetic \"bad\" responses that contain critical errors but exhibit stylistic qualities favored by judge models, helping to mitigate bias in the judge LLMs. Wu et al. (2024) novelly propose meta-rewarding, which leverages the policy LLMs to judge the quality of their own judgment and produce pairwise signals for enhancing the LLMs' judging capability. This concept is also adopted by Wang et al. (2024f), who propose self-taught evaluators that use corrupted instructions to generate suboptimal responses as inferior examples for preference learning. Recently, Hu et al. (2024) propose Themis, an LLM dedicated to NLG evaluation, which has been trained with designed multi-perspective consistency verification and rating-oriented preference alignment methods. Li et al. (2024j) propose PORTIA, an alignment-based approach designed to mimic human comparison behavior to calibrate position bias in an effective manner.\n4.2 Prompting\nDesigning appropriate prompting strategies and pipelines at the inference stage could improve judgment accuracy and mitigate bias. In this section, we summarize and categorize existing prompting strategies for LLM-as-a-judge\n4.2.1 Swapping Operation\nPrevious studies have demonstrated that LLM-based judges are sensitive to the positions of candidates, and the quality ranking of candidate responses can be easily manipulated by merely altering their order in the context (Wang et al., 2023c; Raina et al., 2024; Zhu et al., 2023). To mitigate this positional bias and establish a more fair LLM judging system, the swapping operation (Zheng et al., 2023) has been introduced and widely adopted. This technique involves invoking the judge LLM twice, swapping the order of the two candidates in each instance. In evaluations, if the results are inconsistent after the swap, it is labeled a \"tie,\" indicating that the LLM is unable to confidently distinguish the quality of the candidates (Zheng et al., 2023). Several studies have also incorporated swapping operations in self-alignment (Lee et al., 2023; Sun et al., 2024; Lee et al., 2024) to obtain more accurate pairwise feedback from the judge LLM. Zhu et al. (2024a) proposed a CoT-like prompting technique to mitigate the positional bias by asking the model to first provide all pairwise ranking, then summarize with a ranking list.\n4.2.2 Rule Augmentation\nRule-augmented prompting involves embedding a set of principles, references, and evaluation rubrics directly within the prompt for the judge LLM. This approach is commonly employed in LLM-based evaluations, where judge LLMs are guided to assess specific aspects (Li et al., 2024e; Bai et al., 2023a; Yu et al., 2024; Qian et al., 2024) and provided with detailed rubrics (Gao et al., 2023; Kim et al.; Wang et al., 2024e; Murugadoss et al., 2024) to ensure a fair comparison. A distinct approach is seen in Liu et al. (2024b), where the judge LLM is prompted to generate its own scoring criteria through in-context learning on a set of few-shot examples. In alignment with LLM-as-a-judge, Bai\n4.2.3 Multi-agent Collaboration\nAccessing results from a single LLM judge may not be reliable due to various biases inherent in LLMs (Wang et al., 2023c; Liusie et al., 2024; Zhu et al., 2023; Koo et al., 2023a; Liu et al., 2023c). To address this limitation, Li et al. (2023) introduced the Peer Rank (PR) algorithm, which considers each peer LLM's pairwise preferences for all answer pairs and produces a final ranking of models. Building on this foundation, several architectures and techniques for multi-agent LLMs have emerged, including mixture-of-agent (Zhang et al., 2023b), role play (Wu et al., 2023), debating (Chan et al., 2023), and voting (Zhu et al., 2024b). Jung et al. (2024) proposed Cascaded Selective Evaluation, where less expensive models serve as initial judges, escalating to stronger models only when necessary (Beigi et al., 2024a). Additionally, some works apply multi-agent collaboration for alignment data synthesis, leveraging multiple LLM judges to refine responses (Arif et al., 2024) or provide more accurate pairwise feedback (Li et al., 2024g).\n4.2.4 Demonstration\nIn-context samples or demonstrations (Brown et al., 2020; Dong et al., 2023; Agarwal et al.) provide concrete examples for LLMs to follow and have been shown to be a crucial factor in the success of in-context learning for LLMs. Several studies have introduced human assessment results as demonstrations for LLMs-as-judges, aiming to guide the LLMs in learning assessment standards from a few concrete in-context examples. Jain et al. (2023b) is the first to explore the efficacy of large language models as multi-dimensional evaluators using in-context learning, eliminating the need for large training datasets. Kotonya et al. (2023) conduct systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and chain-of-thought prompting, combining these methods with zero-shot and one-shot learning to maximize eval-\n4.2.5 Multi-turn Interaction\nIn evaluation, a single response may not provide enough information for an LLM judge to thoroughly and fairly assess each candidate's performance. To address this limitation, multi-turn interactions are commonly adopted to offer a more comprehensive evaluation. Typically, the process begins with an initial query or topic, followed by dynamically interacting between the judge LLM and candidate models. Bai et al. (2023b) propose a multi-round setting where the evaluator takes on an interviewer role, posing increasingly sophisticated follow-up questions based on the models' previous answers. Similarly, Yu et al. (2024) introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which novelly incorporates an LLM-powered interactor to enable dynamic, contamination-resilient assessments. Additionally, some approaches facilitate debates among candidates in a multi-round format. For example, Zhao et al. (2024c) design a framework where two LLMs engage in a multi-round peer battle around a query, allowing their true performance differences to surface. Moniri et al. (2024) propose an automated benchmarking system where LLMs debate, with the final assessment carried out by another LLM judge.\n4.2.6 Comparison Acceleration\nAmong the various comparison formats in LLM-as-a-judge (e.g., point-wise and list-wise), pair-wise comparison is the most common approach for directly comparing two models or generating pairwise feedback. However, when multiple candidates need to be ranked, this method can be quite time-consuming (Zhai et al., 2024). To mitigate the computational overhead, Zhai et al. (2024) propose a ranked pairing method in which all candidates are first compared against a blank baseline response. Each candidate's rank is then determined by how well they perform in comparison to the baseline. Zhu et al. (2024a) proposed a CoT like prompting technique to mitigate the positional bias by forcing the model to provide all pairwise ranking first, then summarize these pairwise rankings with a list. In addition, Lee et al. (2024) utilize a tournament-based approach (Liu et al., 2023a; Zhao et al., 2023b) for rejection sampling during inference to speed up the pair-wise comparison. They construct a tournament tree where the leaf nodes represent sampled responses, and non-leaf nodes are selected based on the outcome of judgments between child nodes."}, {"title": "5 Application", "content": "While LLM-as-a-judge was initially proposed for the evaluation application, its usage scope has been largely expanded to many other scenarios like alignment, retrieval and reasoning. Therefore, as Figure 5 shows, we provide a comprehensive introduction to how LLM-as-a-judge can be applied in various applications.\n5.1 Evaluation\nTraditional evaluation in NLP relies on predefined criteria, typically through metrics, to assess the quality of machine-generated text. Some prominent metrics such as BLEU, ROUGH, and BERTScore have been widely used in the area. However, metric-based evaluation overemphasizes lexical overlap and similarity, which may fall short when many valid responses and more nuanced semantic attributes are required to be considered (Post, 2018; Sai et al., 2022). To address these limitations, LLM-as-a-judge has been used to serve as an automated judge for enhancing evaluations in many tasks (Lin and Chen, 2023b; Mondorf and Plank, 2024). LLM-as-a-judge enables human-like qualitative evaluations rather than simple quantitative comparisons of how well machine-generated outputs match the ground truth. This section discusses how LLM-as-a-judge has been utilized to evaluate open-ended generation, reasoning, and more emerging NLP tasks.\n5.1.1 Open-ended Generation Tasks\nOpen-ended generation refers to tasks where the generated content is expected to be safe, accurate, and contextually relevant, though there isn't a single \"correct\" answer. Such tasks include dialog response generation, summarization, story generation, and creative writing (Badshah and Sajjad, 2024; Bai et al., 2023a; Kumar et al., 2024). Unlike conventional metrics-based evaluation meth-\nods, LLM-as-a-judge provides a more nuanced, adaptable, and customized evaluation. As noted by Zheng et al. (2023), LLMs like GPT-4 perform comparably to humans when judging open-ended text generation. In practice, LLM-as-a-judge has been applied to evaluate outputs from a single model to compare outputs from multiple models in a competitive setting. For example, Gao et al. (2023) use ChatGPT to perform human-like summarization evaluation. Similarly, Wu et al. (2023) propose a comparison-based framework to let LLMs act as judges with multiple role-playing to evaluate the summarization quality in a specific dimension and generate its evaluations.\nModern LLMs are good at generating detailed and long-form responses. However, as the output length increases, so does the likelihood of hallucinations. To better understand this phenomenon, Cheng et al. (2023) and Zhang et al. (2024d) introduce an evaluation method that uses GPT-4 to judge whether generated outputs include logically structured yet nonsensical statements. Wang et al. (2024a) propose a critique-based judging system that evaluates hallucinations by selecting pertinent evidence and providing in-depth critiques. Beyond hallucinations, a significant concern is the generation of harmful (e.g., encouragement of self-suicide) and unsafe (e.g., guidance of illegal activity) responses by LLMs. Addressing this, Li et al. (2024f) introduce MD-Judge and MCQ-Judge for evaluating safety-related QA pairs, particularly focusing on queries crafted to provoke unsafe responses. This approach supports a seamless and reliable evaluation. However, an overly cautious stance toward unsafe queries can lead to excessive refusal responses, hindering normal functionality and negatively affecting user experience. To explore this issue, Xie et al. (2024a) conduct a meta-evaluation across various LLM-as-a-judge frameworks, assessing the refusal tendencies of current LLMs in response to potentially unsafe queries.\nRecent research has also leveraged LLM-as-a-judge to evaluate the general capabilities of generative models. This approach often adopts a debate-based framework, where multiple LLMs generate responses that are subsequently evaluated by a separate judging LLM. For example, Chan et al. (2023) introduce a multi-agent debate framework designed to facilitate autonomous discussions and assess the quality of generated responses from different LLMs in open-ended text generation tasks. Similarly, Moniri et al. (2024) propose an automated debate framework that evaluates LLMs not only on domain knowledge but also on their abilities in problem definition and inconsistency recognition.\n5.1.2 Reasoning Tasks\nThe reasoning capability of LLMs can be evaluated through their intermediate thinking process and final answers on specific reasoning tasks (Mondorf and Plank, 2024). Recently, LLM-as-a-judge has been used to evaluate the logical progression, depth, and coherence of the model's intermediate reasoning paths. For mathematical reasoning tasks, Xia et al. (2024) introduce an automatic evaluation framework using a judge LLM specifically designed to assess the quality of reasoning steps in problem-solving processes. LLM-as-a-judge can also be applied to more complex reasoning tasks like temporal reasoning, where models need to understand the relationships of different events over time. Fatemi et al. (2024) build synthetic datasets specifically tailored to evaluate LLMs' temporal reasoning abilities across varied scenarios, testing their proficiency in reasoning with sequences, causality, and dependencies among temporally ordered events.\nThe vast amount of training data presents a challenge in determining whether models are reasoning through deep logical understanding or merely leveraging memorized patterns (Parmar et al., 2024). Wang et al. (2023a) design a debate-style framework to evaluate LLMs' reasoning capability. Given a specific question, the LLM and the user adopt opposing positions and discuss the topic to reach a correct decision. Nan et al. (2024) develop a multi-agent evaluation framework that simulates the academic peer-review process. This framework engages LLMs-as-Judges in a collaborative review, offering a more nuanced understanding of LLMs\u2019 reasoning capabilities in data-driven tasks.\n5.1.3 Emerging Tasks\nAs the capabilities of LLMs evolve rapidly, machines are increasingly being employed for tasks previously considered exclusive to humans, especially in context-specific areas. A prominent task is in social intelligence, where models are presented with complex social scenarios requiring the understanding of cultural values, ethical principles, and potential social impacts. For example, Xu et al. (2024a) evaluate the social intelligence of LLMs, highlighting that while these models have made strides, they still lag significantly behind in social intelligence compared to their academic problem-solving abilities. Similarly, Zhou et al. (2023) introduce SOTOPIA and SOTOPIA-EVAL to simulate complex social interactions between LLM agents and evaluate their social intelligence. In their work, GPT-4 is used as a proxy for human judgment in assessing goal completion, financial management, and relationship preservation within simulated interactions.\nAnother line of research has been directed towards evaluating Large Multimodal Models (LMMs) and Large Vision-Language Models (LVLMs). For example, Xiong et al. (2024b) explore LMM-as-a-judge to evaluate the performance of multimodal models, providing both a final score and the underlying rationale for evaluations, promoting transparency and consistency. Chen et al. (2024c) propose the first benchmark for the automatic evaluation of LVLMs, specifically for self-driving corner cases. They find that evaluations performed by LLMs-as-judges align more closely with human preferences than those conducted by LVLM-as-judges.\nRecently, we have seen more customized utilization of LLM-as-a-judge to evaluate emerging tasks such as code understanding (Zhao et al., 2024a), legal knowledge (Fei et al., 2023), game development (Isaza-Giraldo et al., 2024), ocean science (Bi et al., 2023), healthcare conversations (Wang et al., 2024j), debating judgment (Liang et al., 2024a) and more. This trend reflects the growing adaptability of LLMs-as-judges in evaluating diverse and specialized domains.\n5.2 Alignment\nAlignment tuning (Wei et al., 2022a; Ouyang et al., 2022) is a vital technique to align LLMs with human preferences and values. A key component of this process is the collection of high-quality, pairwise feedback from humans, which is essential for reward modeling (Schulman et al., 2017) or direct preference learning (Rafailov et al., 2023). Recently, there have been increasing research interests focused on automating this pairwise feedback mechanism by adopting LLM-as-a-judge in alignment tuning.\n5.2.1 Larger Models as Judges\nOne intuitive idea for adopting LLM-as-a-judge in alignment tuning is to leverage the feedback from larger, more powerful LLMs to guide smaller, less capable models. (Bai et al., 2022) first propose employing the AI's feedback to build a harmless AI assistant. They train the reward model using synthetic preference data based on the preference of a pre-trained language model. Building\non this, Lee et al. (2023) discover that the RLAIF method can achieve comparable performance with RLHF even when the LLM judge is not strong enough. They also introduced DIRECT-RLAIF, which directly employs an off-the-shelf LLM as the judge model to mitigate reward staleness in reward models. To avoid reward hacking in alignment, Sun et al. (2024) devise an instructable reward model trained on synthetic preference data. It enables humans to perform RL-time interventions to better align the target policy with human values. Apart from the abovementioned studies, Guo et al. (2024) introduce online AI feedback (OAIF), directly utilizing the preference signals from an annotation model to train the target model. There are also works that utilize multi-agent cooperation for better judgment in alignment tuning. Arif et al. (2024) construct a synthetic preference optimization dataset using multi-agent workflows and adopt LLMs as judges with diverse prompting strategies and pipelines. Similarly, (Li et al., 2024g) employ multiple LLMs to debate with each other, iteratively improving response quality, while creating a judge LLM to select preferred responses for enhanced instruction tuning. To align the generated code with human preference, Weyssow et al. (2024) introduce CodeUltraFeedback, a preference coding dataset"}]}