{"title": "Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification", "authors": ["Lynnette Hui Xian Ng", "Iain Cruickshank", "Roy Ka-Wei Lee"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in executing tasks based on natural language queries. However, these models, trained on curated datasets, inherently embody biases ranging from racial to national and gender biases. It remains uncertain whether these biases impact the performance of LLMs for certain tasks. In this study, we investigate the political biases of LLMs within the stance classification task, specifically examining whether these models exhibit a tendency to more accurately classify politically-charged stances. Utilizing three datasets, seven LLMs, and four distinct prompting schemes, we analyze the performance of LLMs on politically oriented statements and targets. Our findings reveal a statistically significant difference in the performance of LLMs across various politically oriented stance classification tasks. Furthermore, we observe that this difference primarily manifests at the dataset level, with models and prompting schemes showing statistically similar performances across different stance classification datasets. Lastly, we observe that when there is greater ambiguity in the target the statement is directed towards, LLMs have poorer stance classification accuracy.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) serve many natural language processing tasks, consistently surpassing their supervised, neural network predecessors across a suite of benchmark challenges (Touvron et al. 2023). However, the domain of stance detection, defined as identifying an entity's opinion regarding a particular target (Ng and Carley 2022a), remains relatively uncharted in LLM research. Stance detection is particularly pertinent in analyzing viewpoints on public figures or policies, which are often imbued with significant political implications: most people take stances around politically contentious targets making the task of stance detection intimately tied with politics.\nDespite the advancements LLMs have brought to natural language processing, they are not without their shortcomings, including ingrained biases. These biases stem from the vast and varied datasets on which they are trained and can manifest in a variety of contexts biases (Koo et al. 2023; Navigli, Conia, and Ross 2023). These biases also extend to the political context, where LLMs have shown political biases in their responses to political orientation tests (Rozado 2023).\nThis study delves into the influence of political biases on LLMs' stance detection capabilities. Our contributions are as follows.\n1. We evaluated the disparities in how LLMs classify stances with political orientations, considering the dataset, LLMs, and prompting methods. Statistical tests were conducted to illustrate the performance differences between stances favoring the left and the right.\n2. It was noted that informing the LLM of a statement's political bias and altering the phrasing of the statement's target does not reduce performance bias and may even impair performance when the target's phrasing is more ambiguous."}, {"title": "Related Work", "content": "Automatic stance detection is the identification of an author's opinion towards a specific target. Stance analysis plays a pivotal role in mapping the dynamics of human opinions and the evolution of viewpoints over time (Chuang 2023; Ng and Carley 2022b). Historically, stance detection has harnessed supervised machine learning techniques, training support vector machines, neural networks, and transfer learning models on manually annotated datasets (K\u00fc\u00e7\u00fck and Can 2020).\nWith the advent of LLMs, recent research has focused on using LLMs for stance detection. Stance detection is a task requiring a nuanced understanding of context-dependent language, which seems well suited to LLMs. Researchers have experimented with various prompting techniques to elicit the desired output from LLMs like ChatGPT, with mixed results (Deng et al. 2023). Performance on benchmark datasets like SemEval-2016 has varied, with accuracy scores ranging from 0.38 to 0.63 (Chuang 2023; Zhang, Ding, and Jing 2022; Aiyappa et al. 2023). A study by (Liyanage, Gokani, and Mago 2023) achieved a notable 0.80 accuracy using a zero-shot prompting approach on GPT-4, highlighting the potential of these models. However, investigations into stance classification with non-ChatGPT models remain sparse (Ziems et al. 2023; Cruickshank and Ng 2023). (Mets et al. 2023) evaluated ChatGPT's zero-shot classification on a tailored immigration dataset, finding its performance commendable yet trailing behind the best, in-domain supervised models. Overall, while LLMs demonstrate potential for stance classification, the optimal strategies for their application and the scope of their limitations require further exploration.\nInvestigations into the presence of political biases within LLMs have yielded concerning insights. Studies utilizing political orientation survey questions directed at ChatGPT identified a discernible left and libertarian bias (Motoki, Neto, and Rodrigues 2023; Rozado 2023; Rutinowski et al. 2023). Further research on other LLMs, including GPT-3, demonstrated that LLMs generally exhibit a libertarian political bias, especially when instruction-tuned (Gover 2023; Rozado 2024). However, changing the wording of the prompts in political orientation questions and including content from Wikipedia have shown to possess pronounced biases against political ideologies on both ends of the spectrum (Haller, Aynetdinov, and Akbik 2023; Sasuke and Takemoto 2023). Furthermore, ChatGPT has also been able to reproduce political viewpoints from any given political orientation with similar success across different political orientations (Lihammer 2023). Thus, it is not clear if the political bias present in LLMs affects their ability to reason about politically charged text.\nThe ramifications of political biases are profound, with the potential to foster divisiveness and tribalism within public discourse (Guo, Ma, and Vosoughi 2022). As LLMs gain ubiquity and begin to support functions such as essay writing in educational and professional settings or classifying the stance of comments in a politically charged debate, the possibility of them being leveraged to polarize opinions and manipulate debates becomes a tangible concern (Haller, Aynetdinov, and Akbik 2023). Building on previous methodologies for gauging political bias, our research aims to ascertain the extent to which these biases influence LLMs' performance in the often politically-rooted task of stance detection."}, {"title": "Methodology", "content": "To explore the potential political bias in LLMs during stance classification tasks, we implemented a methodology where we classified stances with clear political undertones and assessed the performance across various politically aligned targets. We specifically engaged LLMs to classify several targets known for their distinct political positions within the U.S. political framework, and subsequently analyzed the outcomes to determine any disparities or biases in how stances were classified. This section elaborates on the datasets, models, and prompting strategies employed to examine political bias in LLM stance classification."}, {"title": "Datasets", "content": "We make use of three publicly available datasets that have stance classifications around politically charged topics:\n1. BASIL (Fan et al. 2019): Political news bias dataset containing news articles from 2010 to 2019 from Fox News, New York Times, and Huffington Post. Articles are labeled for their overall political bias and the central target of the article.\n2. SemEval2016 (Mohammad et al. 2016): Stance-labeled tweets towards atheism, climate change is real, the the feminist movement, Hillary Clinton, and legalization of abortion.\n3. Elections2016 (Sobhani, Inkpen, and Zhu 2017): Stance-labeled tweets about key politicians leading to the 2016 US presidential elections.\nEach example contains a target, the text of the statement, and the manually annotated stance of the statement towards the target."}, {"title": "Classifying Political Orientation", "content": "Certain stances within the datasets are indicative of political biases prevalent in the U.S. political landscape. For instance, support for a Republican presidential candidate suggests a right-leaning bias. To systematically examine these associations, we categorized stances from each dataset according to their most-likely, related political orientation. Table 1 enumerates the stances and their corresponding political orientations and their frequencies. SemEval2016 and Elections2016 datasets exhibit a higher incidence of right-leaning statements, whereas the BASIL dataset presents a more balanced distribution of right- and left-leaning sentiments. Across all datasets, we have 2,606 right-leaning stances and 1,499 left-leaning stances. Our work thus concentrates on discerning the performance disparities of LLMs when tasked with stance classification, focusing on stances that align distinctly with either side of the US political spectrum."}, {"title": "Experiments", "content": "To evaluate the efficacy of LLMs in stance classification, we experimented with a number of prompting schemes, including ones specifically shown to deliver good performance on stance classification. These schemes are detailed below:\n1. task-only prompt (Cruickshank and Ng 2023): Provide only the task description and statement."}, {"title": "LLM Setups", "content": "Our experimental suite encompassed seven open-source LLMs and GPT-3.5, chosen to represent a broad spectrum in terms of model size and architecture, including both decoder-only and encoder-decoder frameworks. The selected LLMs for our study were: Falcon-7B, instruction-tuned (Almazrouei et al. 2023), Falcon-40B, insstruction-tuned (Almazrouei et al. 2023),T5-XL, Flan-Alapaca tuned (Chia et al. 2023), UL2, Flan-tuned (Chung et al. 2022), Llama-7B, chat-tuned (Touvron et al. 2023), Llama-13B, chat-tuned (Touvron et al. 2023), Mistral-7B, instruction-tuned (Jiang et al. 2023), and GPT-3.5(Brown et al. 2020). For all models, we employed the default settings and used greedy decoding to maintain consistency and to evaluate their 'out-of-the-box' capabilities in stance detection tasks. Our computational infrastructure included a machine running Ubuntu 22.04 Linux with an x64 architecture CPU featuring 40 cores, supplemented by 376GB of RAM and three NVIDIA A6000 GPUs."}, {"title": "Evaluation", "content": "Measurement of task accuracy To ascertain the effectiveness of the LLMs in correctly identifying stances, we use the F1 Score as our principal metric a standard in stance classification evaluations (Mohammad et al. 2016; Ng and Carley 2022a). Specifically, we use the micro-F1 score variant, which aggregates the contributions of all classes to compute a global average. This is crucial when dealing with datasets with uneven distributions to balance the impact of each class. As a benchmark, we reference the benchmark, supervised models for each dataset from the original authors.\nCharacterization of political orientations of stances To gauge the presence of biases within language models, researchers have introduced several metrics. (Nadeem, Bethke, and Reddy 2020) evaluates biases by analyzing responses to stereotype-associated sentences, and (Kaneko and Bollegala 2021) quantifies biases based on the strength of word pair associations (Kaneko and Bollegala 2021). However, these metrics are largely predicated on word-level associations and may not accurately capture biases within the more complex context of stance detection, which requires analyzing opinions toward specific topics. Furthermore, they do not ascertain whether a preference for a bias, which can come from political orientation quizzes, can affect an LLMs ability to perform tasks for a politically charged subject, like stance.\nTo examine whether the LLMs have different performances for stances of different political orientations, we measure the difference in the performance between the right-leaning and left-leaning stances. For each segment, we first calculate the F1 accuracy scores of the data points that are tagged to left-leaning stances (left-F1 score), and the F1 accuracy scores of the data points that are tagged to right-leaning stances (right-F1 score). We then perform a one-sample, two-tailed t-test on the differences in performance between the right-F1 scores and the left-F1 scores (i.e. (left-F1 score-right-F1 score) = 0 if there is no difference) with Bonferroni correction to account for the multiple"}, {"title": "Ablation Studies with Target Alterations", "content": "We performed ablation studies with target alterations, where we changed the specification of the target in the input prompt. We ran these experiments for the targets related to elections, e.g. Donald Trump, Hillary Clinton. These targets are chosen for they are common among all three datasets. We performed six target alterations:\n1. candidate and_name: the target name is prefixed with the type of election he ran for (i.e., Presidential Election)\n2. misspelling: the target is misspelt. 'Hillary Clinton' to 'Hilary Clintom' and 'Donald Trump' to 'Donold Trump'.\n3. normal: no change to the original target spelling or structure\n4. party: target is substituted with the affiliated party (e.g., Republican Candidate)\n5. party and name: target name is prefixed with the affiliated party\n6. underspecify: target is referred to by the last name\nThese experiments are carried out with the task + context and the zero-shot CoT prompting schemes, across all models and datasets. Finally, we performed a pairwise Wilcoxon Signed-Rank Test to compare the differences of the accuracy scores between each set of alteration in the entire dataset, to see if any of the different phrasings altered the stance classification performance."}, {"title": "Results", "content": "We assessed seven LLMs for stance classification and political bias across three different datasets and four prompting schemes. Our findings are twofold, encompassing both the efficacy of LLM stance classification compared to traditional supervised methods and the extent of political bias present in the models' performance.\nThe stance classification results, detailed in Table 2, indicate that CoT prompting techniques generally lead to superior model performance. Interestingly, we found marginal differences when comparing the zero-shot CoT with the bias CoT approaches, suggesting that explicitly prompting for bias may not significantly impact the classification outcomes.\nWhen juxtaposed with established baselines, our results showcase a nuanced landscape of performance. While some configurations of LLM + prompt indeed outperform traditional approaches, this is not universally consistent across all tested LLM configurations. For example, Falcon 40B Instruct with CoT prompting emerged as the top performer for the BASIL dataset, which marks an improvement over the baseline (0.65 vs 0.43). Similarly, prompting schemes for UL2 Flan-tuned does result in higher F1 score for the SemEval2016 dataset (0.72 vs 0.56 baseline). For the Elections2016 dataset, the best performing UL2 Flan-tuned scores 0.62, modestly exceeding the 0.55 baseline.\nIn order to evaluate the possible effects of the political orientations of the stances on the performance of LLMs for stance classification, we performed a number of statistical tests at various levels of aggregation. At the dataset level, we evaluated the difference in micro-F1 score between left and right-leaning stances across all models and prompting schemes for each dataset and for each prompting scheme within each dataset. The t-statistics and p-values are displayed in Table 3 as t-stat/p-val per-dataset and per-prompt. Across the whole of each dataset, the differences in performance between right and left-leaning stances were statistically nonzero, indicating a bias in performance. Within the prompt categories, in each dataset, we found some prompts were not statistically non-zero in their performance. For example, on the SemEval2016 and Election2016 datasets, the task-only and task+context prompts were not statistically nonzero in their performance differences, but were statistically nonzero in their performance differences on the BASIL dataset. Whereas, the reasoning-based prompts (i.e., CoT and Bias CoT) tended to have the opposite performance. Furthermore, in Figure 1, which displays the density curves of the differences between the left and right-leaning stances, we can see that for Elections2016 and BASIL, the LLMs tended to perform better at identifying left-leaning stances, while in SemEval2016 (i.e., curves are less than 0), they tend to perform better at right-leaning stances (i.e., curve is mostly greater than 0). Thus, the dataset affects the performance of different politically oriented stances when classified by an LLM.\nWe next analyzed the models, across all datasets and prompting schemes to see if there was a difference in performance for stance detection between the right and left-leaning stances due to different prompts, models, or datasets. From Table 5, we observe that every model shows a statistically nonzero difference in performance between the detection of right and left-leaning stances. The following table, Table 4, displays the average of the differences for each of the models, across all of the prompting schemes and datasets.\nFrom Table 4, we can see that some models have less of a performance difference, on average, than others as well as differences in which political lean they tend to perform better at. For example, Mistral, Falcon-40B, and GPT-3.5 tend to perform better at right-leaning stances. To investigate whether there is actually a difference between the models, we also did pairwise t-tests with Bonferroni correction between each model, across all datasets and prompting schemes, and found that none of the models are statistically different from each other in terms of their performance. This result is also visible in model curves in Figure 1. Thus, all of the models, regardless of architecture or size, have roughly the same bias in their performance on stance classification of politically oriented topics.\nFinally, as with the models we analyzed the prompting schemes, across all models and datasets to see if any prompting scheme elicits a difference in performance for stance detection between the right and left-leaning stances. Table 5,"}, {"title": "Consistency of results through different target alterations", "content": "Table 7 provides the mean F1 scores for each of the target alterations defined in the methodology section. Overall, altering the target in way that introduces ambiguity reduces the accuracy of the LLM. However, using the candidate_and_name alteration (e.g. \"Presidential Candidate Donald Trump\") and the party_and_name alteration (e.g. \"Republican Candidate Donald Trump\") produces an accuracy on par with the original dataset. These results show that slight alterations in targets can affect the results of stance classification by a LLM, as the model interprets the target entity to which the post is written for differently. The full results detailing the accuracy for each model across each dataset are in the Appendix."}, {"title": "Illustrative Examples", "content": "Table 8 shows examples where the different LLMs produce different political stances for the same sentences. For the topic on Atheism, the sentence that suggests a belief in atheism, which is classified as a left-leaning stance. Out of five of the LLMs illustrated, three of them responded with a \"for\", one with an \"against\" and one with a \u201cneutral\" stance. This demonstrates that the language models are not equivalent in slants. Within the same architecture, (Falcon-7B instruct-tuned vs Falcon-40B instruct-tuned, Llama2-7B chat-tuned vs Llama2-13B chat-tuned), there is a difference in the returned stance. The larger Llama2 model (Llama2-13B chat-tuned) disagrees with this statement while the smaller one agrees with this statement. The sets of Falcon and Llama models do not correspond with each other even for statements that have right-leaning stances.\nTable 9 show example outputs of different prompting schemes for the SemEval2016 dataset. A sentence that is for the Feminist movement, which is a left-leaning stance,"}, {"title": "Discussion", "content": "In this study, we investigated the performance of Language Models (LLMs) in the task of stance classification, focusing on politically oriented stances. Our findings suggest that LLMs exhibit varying performance when classifying stances related to right- and left-leaning topics. This discrepancy potentially indicates a political bias in LLMs when tackling politically oriented tasks. Such bias could pose societal risks, as LLMs may yield different responses depending on the political orientation of the topic, thereby introducing biases in downstream tasks and outputs.\nWe explored four different prompting schemes, each providing progressively more contextual information, to assess their impact on performance. Interestingly, we observed that prompts utilizing Chain of Thought (CoT) led to more biased results across datasets compared to non-CoT prompts (i.e., task and task+context). Specifically, non-CoT prompts exhibited non-statistically significant performance differences on tweet-based datasets, while the reverse was true for CoT prompts. This suggests that the type of data (tweets vs. news articles) can heavily influence the effectiveness of particular prompting schemes for politically oriented stance classification.\nFurthermore, we found that prompts incorporating the proposed Bias CoT displayed both the only positive mean difference score (i.e. the model was better at classifying right-leaning versus left-leaning stances) and the highest number of nonzero mean difference scores. This suggests that prompting the LLMs to reason about the political bias of statements before classifying stance actually exacerbated political biases in the results. Additionally, no single model consistently outperformed others across all datasets and prompts, underscoring the challenge of stance classification, particularly in the context of politically oriented stances.\nOverall, our results reveal that LLMs indeed exhibit political biases, consistent with prior findings indicating that ChatGPT leans more leftward (Motoki, Neto, and Rodrigues 2023). While performance differences between political ideologies are significant at the dataset level, they are less pronounced at the model level and least apparent at the prompt level. This suggests that LLMs tend to demonstrate consistent ideological bias outputs across prompting schemes. The smaller performance differences at the model level imply that most LLMs are trained on similar or overlapping datasets, or that the LLMs are equally biased at the model and prompt level. Conversely, dataset-level differences in political bias highlight the instability of datasets regarding political topics. SemEval2016 and Election2016 were both targeting the same event, but results in different political biases due to the differences in data collection techniques, the datasets are differently biased. This instability aligns with previous studies demonstrating that identical prompting schemes applied to the same dataset yield varying performance (Chuang 2023; Aiyappa et al. 2023).\nTherefore, our findings suggest that the primary source of bias in LLM performance stems from the data used for classification. Training data may contribute to political biases in LLM performance, as machine learning algorithms have been known to amplify existing biases in training data (Hovy and Prabhumoye 2021). Additionally, biases may arise from human annotation of datasets, as human annotators may inject their inherent biases into gold stance labels, which are then subject to inconsistent interpretations (Ng and Carley 2022a). Although it remains unclear what aspects of the data trigger politically biased performance differences, our results indicate that such biases are indeed data-driven.\nLastly, our target alterations findings suggests that LLMs are sensitive to how targets are phrased, and any alteration to the target such as misspelling or supplementing with the political party would affect the accuracy of the model. This problem is more unique to LLMs, as most existing methods, especially those for stance classification assume the target to be a static entity, whereas LLMs can accept free text input, meaning the target can be variously phrased. This points towards a future line of research in developing models and prompting schemes that are robust towards such alterations and even possible target alterations (as opposed to prompt alterations) for improving the stance-classification abilities"}, {"title": "Dataset and Code Availability", "content": "The original dataset are publicly available datasets. We release our code and dataset with the stance annotations by the LLMs at Zenodo (https://doi.org/10.5281/zenodo.10493803)"}, {"title": "Societal Impact", "content": "Understanding the stance towards political figures and events is an important pillar of digital diplomacy, which seeks to discern citizen's political views from online sources like social media. While LLMs provide a low-effort way to aggregate stances from social media posts, the political biases inherent within the models can affect the resultant stances, thereby affecting downstream analysis."}, {"title": "Conclusions", "content": "Stance classification is a crucial task that offers insight into an author's opinion toward a certain target. Often, these stances will have a political orientation, as individuals tend to hold stances on political topics. In this study, we evaluated the performance of the stance detection task on seven large language models, focusing on how LLMs differ in their performance when confronted with politically oriented stances. Testing four prompting schemes, we found that most LLMs perform stance classification on par or better than baseline models, with the Chain-of-Thought prompting scheme generally demonstrating the best performance.\nHowever, LLMs exhibit significant political biases across datasets, models, and prompting schemes, highlighting their vulnerability to ideological biases. Notably, incorporating LLM reasoning about the bias of the statement and target before classifying the stance did not mitigate the exhibited political bias in the results.\nOur study is limited by the series of LLMs and prompts that we were testing, and the strain on our compute resources. The study is also limited by the availability of datasets that are annotated for political stance, and even so, the human annotations may not be consistent both across and within the datasets (Ng and Carley 2022a). Furthermore, we have only considered the U.S. political landscape in this investigation, due to data availability.\nDespite these limitations, this study suggests several potential avenues for future research. Firstly, attempts to ameliorate differences in performance between different politically oriented stances through prompting were unsuccessful, suggesting the need for novel prompt patterns to address this issue. Additionally, since LLMs demonstrate different performances for various political orientations in stance detection, similar differences in performance may arise in other politically charged tasks, such as bias classification."}, {"title": "Paper Checklist to be included in your paper", "content": "1. For most authors...\n(a) Would answering this research question advance science without violating social contracts, such as violating privacy norms, perpetuating unfair profiling, exacerbating the socio-economic divide, or implying disrespect to societies or cultures? No\n(b) Do your main claims in the abstract and introduction accurately reflect the paper's contributions and scope? Yes\n(c) Do you clarify how the proposed methodological approach is appropriate for the claims made? Yes\n(d) Do you clarify what are possible artifacts in the data used, given population-specific distributions? Yes\n(e) Did you describe the limitations of your work? Yes\n(f) Did you discuss any potential negative societal impacts of your work? Yes\n(g) Did you discuss any potential misuse of your work? Yes\n(h) Did you describe steps taken to prevent or mitigate potential negative outcomes of the research, such as data and model documentation, data anonymization, responsible release, access control, and the reproducibility of findings? Yes\n(i) Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes\n2. Additionally, if your study involves hypotheses testing...\n(a) Did you clearly state the assumptions underlying all theoretical results? NA\n(b) Have you provided justifications for all theoretical results? NA\n(c) Did you discuss competing hypotheses or theories that might challenge or complement your theoretical results? NA\n(d) Have you considered alternative mechanisms or explanations that might account for the same outcomes observed in your study? NA\n(e) Did you address potential biases or limitations in your theoretical framework? NA\n(f) Have you related your theoretical results to the existing literature in social science? NA\n(g) Did you discuss the implications of your theoretical results for policy, practice, or further research in the social science domain? NA\n3. Additionally, if you are including theoretical proofs...\n(a) Did you state the full set of assumptions of all theoretical results? NA\n(b) Did you include complete proofs of all theoretical results? NA\n4. Additionally, if you ran machine learning experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Yes, it is uploaded as a supplementary material, the URL will be public when the paper is accepted\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Yes\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Yes\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Yes\n(e) Do you justify how the proposed evaluation is sufficient and appropriate to the claims made? Yes\n(f) Do you discuss what is \"the cost\" of misclassification and fault (in)tolerance? Answer\n5. Additionally, if you are using existing assets (e.g., code, data, models) or curating/releasing new assets, without compromising anonymity...\n(a) If your work uses existing assets, did you cite the creators? Yes\n(b) Did you mention the license of the assets? Yes\n(c) Did you work any new assets in the supplemental material or as a URL? Yes\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? NA\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Yes\n(f) If you are curating or releasing new datasets, did you discuss how you intend to make your datasets FAIR (see?)? Yes\n(g) If you are curating or releasing new datasets, did you create a Datasheet for the Dataset (see?)? Yes\n6. Additionally, if you used crowdsourcing or conducted research with human subjects, without compromising anonymity...\n(a) Did you include the full text of instructions given to participants and screenshots? NA\n(b) Did you describe any potential participant risks, with mentions of Institutional Review Board (IRB) approvals? NA\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? NA\n(d) Did you discuss how data is stored, shared, and deidentified? NA"}]}