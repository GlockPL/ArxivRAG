{"title": "Language-Image Models with 3D Understanding", "authors": ["Jang Hyun Cho", "Boris Ivanovic", "Yulong Cao", "Edward Schmerling", "Yue Wang", "Xinshuo Weng", "Boyi Li", "Yurong You", "Philipp Kr\u00e4henb\u00fchl", "Yan Wang", "Marco Pavone"], "abstract": "Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs' perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pretraining dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. Cube-LLM exhibits intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) Cube-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3 points of APBEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. Cube-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.", "sections": [{"title": "1 Introduction", "content": "Internet-scale visual data have brought forth the advent of multi-modal large language models (MLLMs). Rich and diverse visual supervision aligns pre-trained"}, {"title": "2 Related Work", "content": "Vision Language Models. By scaling up pre-training on the internet-scale dataset, there has been significant progress for VLMs in the 2D vision-language domain, showing strong capabilities in few-shot generalization. VLBRRT and ViLBERT capitalized on a BERT-style framework for image-text co-embedding. CLIP embedded images and text captions into a shared feature space via contrastive learning and pioneered zero-shot vision tasks. BLIP and BLIP2 further improved CLIP by leveraging extra pseudo-labeled data and better image/language encoders. Flamingo and its open-source implementation Open-Flamingo proposed a fast adaptation approach to enable in-context few-shot learning on novel visual-language tasks. GPT4V and Gemini further demonstrated state-of-the-art human-level visual reasoning ability through scaling. LLaVA pioneered instruction fine-tuning in the multimodal field. These works have predominantly focused on the 2D vision and language tasks. On the other hand, we aim to adapt these MLLMs to enhance their capabilities for complex 3D reasoning and scene understanding tasks."}, {"title": "3 Unified Language-Image Pretraining for 2D and 3D", "content": "Our goal is to expand the capabilities of vision-language models to reason in 3-dimensional space. We propose a unified training framework to learn from both 2D and 3D perceptual data as well as standard image-text pairs. In this section, we first discuss the data standardization to train a vision-language model at scale (Sec. 3.1), task scaling to understand perceptual information in versatile I/O format (Sec. 3.2), visual chain-of-thought reasoning for 3D grounding and question answering tasks (Sec. 3.3), and finally, we present Cube-LLM, the final model of our unified training framework built on LLaVA-1.5 (Sec. 3.4)."}, {"title": "3.1 Data-scaling for Image-based 3D Reasoning", "content": "Our goal is to train a single 2D + 3D MLLM from all data sources available. To standardize many different 2D and 3D grounding tasks into one, we standardize"}, {"title": "Data standardization", "content": "We consider points and boxes as our main spatial representation for 2D and 3D reasoning. We convert every label to either a point $\\mathcal{O}_{point}^{2D} = [x, y]$ or a bounding box $\\mathcal{O}_{box}^{2D} = [x, \\hat{y}, x', \\hat{y}']$. Similarly, we convert every 3D label to either $\\mathcal{O}_{point}^{3D} = [x, y, z]$ or $\\mathcal{O}_{box}^{3D} = [x, y, z, w, h,l,r_1,r_2, r_3]$ where $r_1, r_2, r_3$ are Euler angles. We first standardize image-based 3D datasets by unifying camera parameters. We follow the procedure of Omni3D ; define a virtual camera with a fixed focal length $f$ and transform depth $z$ according to the original camera parameters and the target image size. Since all 3D labels are unified to a consistent camera intrinsic, we can now convert all $x$ and $y$ coordinates to 2D projected coordinates $(x, y)$. Consequently, we can represent all label formats to naturally follow 2D to 3D per-object token sequence:\n\n$\\mathcal{O}_{point}^{2D} = [x, y]$\n\n$\\mathcal{O}_{box}^{2D} = [x, \\hat{y}, x', \\hat{y}']$\n\n$\\mathcal{O}_{point}^{3D} = [x, y, z]$\n\n$\\mathcal{O}_{box}^{3D} = [x, y, z, w, h,l,r_1, r_2, r_3]$\n\nwhere each value is represented as a short sequence of text tokens (3 for 3-decimal numbers). This allows the model to predict consistent ordering of token sequence from 2D to 3D, which improves the understanding of the underlying structure. With autoregressive models, we first localize objects in image coordinates $(x, y)$, then infer depth $(z)$, and then infer the size and orientation $(w,h,l,r_1,r_2, r_3)$."}, {"title": "3D reasoning as multi-turn conversations", "content": "Now, we combine the 2D and 3D data with language-image instruction tuning data of visual language models . For each image and a set of object labels pair, we construct a multi-turn conversational question-answer data $(Q_1, A_1, Q_2, A_2, .., Q_n, A_n)$. Each question refers an object with one property $b_q$ and enquires $b_a$:\n\n$b_q, b_a \\in \\{box2D, caption, box3D\\}$\n\nEach object property has a set of prompts predefined, such as ''Provide the 3D bounding box of the region this sentence describes: <caption>'' for $b_q = caption$ and $b_a = box3D$. We combine the meta information of objects (e.g., attribute, physical state, etc.) with the class name to enrich the textual information."}, {"title": "3.2 Task-scaling for Versatile I/O Format", "content": "We are interested in a generalist model that accepts input and generates output in versatile formats. Users may want to supplement 2D points or boxes as visual prompt during inference, or may only want the metric depth of an object instead of a complete 3D location. This interest in versatile I/O format shares the same"}, {"title": "Visual Chain-of-Thought Prompting", "content": "One of the most intriguing properties of large language models is its emergent ability to improve reasoning with intermediate steps . This mostly attributes to vast corpus of rich text data with numerous step-by-step question answering samples . We artificially supplement this step-by-step reasoning of 3D by interleaving multiple questions of the same object from easy-to-hard order (the"}, {"title": "Cube-LLM inference with prompting", "content": "We allow test-time adaptation to any specialist models by mixing in candidate objects as a system prompt. This effectively alleviates the problem of localizing in 3D to ''choosing the appropriate box from candidates'',\n\n$\\text{maximize}\\ p(A_{box3D} | S_{box3D}, Q_{caption})$\n\nwhere $S_{box3D}$ is a set of candidate boxes, which can be provided by any specialist models (depending on available input modalities) at inference. During training, we use the ground truth boxes with a prompt ''Here is the list of 3D"}, {"title": "3.4 Cube-LLM", "content": "We introduce Cube-LLM, a multi-modal large language model based on LLaVA-1.5 architecture trained to reason in both 2D and 3D. Although we maintain the generality of model architecture, we make simple yet critical changes to the original LLaVA. We first replace the CLIP visual encoder with DINOv2 , and undergo the same alignment step of the original LLaVA. Although DINOv2 is not a text-aligned visual encoder like CLIP, we found minimal degradation in the standard visual language model benchmarks while significantly improving 3D-related tasks. Then, we finetune the language model (Vicuna-7B ) while freezing the visual encoder and jointly on LLaVA instruction-following data and the 2D part of LV3D following Sec. 3.1, 3.2 and 3.3. We use low image resolution (336x336) and train with a large batch size. Then, we add an additional finetuning stage for both visual and language models with high resolution images (672 \u00d7 672) of the full LV3D. For 3D representation, we use log-scale for depth and all others remain unchanged. For 2D, we normalize image coordinates between 0 and 999. For 3D, we we filter-out all annotations outside Xmin and Xmax (different for each x, y, z, w, h, l, r1, r2, r3, more in supplementary), and normalize between 0 and 999."}, {"title": "4 Experiments", "content": "We evaluate the effectiveness of Cube-LLM in three aspects: (1) 3D-grounded reasoning for indoor and outdoor scenes, (2) complex reasoning in 3D, and (3) standard vision-language benchmarks."}, {"title": "4.1 Implementation Details", "content": "We use LLaVA-1.5 with Vicuna-7B as our base model. We replace the CLIP visual encoder with ViT-L/14 based DINOv2. For all localization outputs, we use 3 decimal places with text tokens, and keep 3 tokens per value (e.g., [021,521, ...]). Accordingly, we pre-process all LLaVA instruction-following data to reflect this change. We follow the same alignment step to train the MLP projection layers with the same training setup in . For 2D and 3D pretraining, we use random sampling following the sampling rate in Table 1. Each data sample (image-annotation pair) is converted to the multi-turn conversation format (Fig. 3) sampled at random. During pretraining, we use 8\u00d78 A100s with a batch size of 1024 and train the model with a learning rate lr = 2 \u00d7 10-5 on images with 336 \u00d7 336 resolution. Then, we fine-tune all parameters including the visual encoder on a higher resolution 672 \u00d7 672 with 8\u00d78 A100s and a batch size of 256 with 4 gradient accumulation steps (effective batch size of 1024) and a learning rate lr = 2 \u00d7 10-5."}, {"title": "4.2 Datasets", "content": "We pre-train Cube-LLM on LV3D, and then fine-tune it on the training split of the target datasets, Talk2Car and DriveLM.\nTalk2Car is a 3D referring expression comprehension dataset of various driving scenarios. It consists of 8,349 training samples and 1,163 validation samples with images and LiDAR data. It provides rich question-answer pairs grounded to an object in the image. Each object is labeled with a situational text that uniquely identifies the object (e.g., ''Wow hold on! That looks like my stolen bike over there! Drop me off next to it.''). The original benchmark  evaluates the 2D grounding performance with the $AP_{0.5}$ metric. MSSG extends the task to 3D grounding and evaluates on both BEV AP and 3D AP.\nDriveLM is a recently released question-answering dataset for autonomous driving based on the nuScenes dataset . It consists of various driving scenes with multi-view images and LiDAR point clouds, as well as frame-level question- answering data, and has a total of 4,871 frames. Each frame contains 91.4 question-answer pairs on average, covering core autonomous driving tasks such as perception, prediction, and planning, as well as a short description and 2D boxes of important objects. To evaluate Cube-LLM, we construct another 3D grounding benchmark based on the DriveLM dataset, which we call DriveLM- Grounding. We associate the 2D boxes with the nuScenes 3D bounding boxes by computing the IoU between 2D boxes projected from 3D labels and the 2D boxes of labeled important objects, and only keep those with a IoU greater than 0.35. After association, DriveLM-Grounding has a total of 13,287 images, about one annotation per image. We also use the DriveLM-QA data from the original"}, {"title": "4.3 3D-Grounded Reasoning", "content": "Our results for 3D grounding on the Talk2Car dataset are detailed in Table 2, which is structured according to the input modalities used for 3D ground- ing. The baselines that rely solely on camera inputs are only evaluated on 2D grounding, whereas those incorporating both camera and LiDAR inputs are evaluated on both 2D and 3D grounding. Cube-LLM is pre-trained on LV3D and fine-tuned on Talk2Car with resolution 672 \u00d7 672. We apply visual"}, {"title": "4.4 Complex Reasoning in 3D", "content": "To show the effectiveness of 3D reasoning capability, we finetune Cube-LLM on DriveLM-QA dataset. The dataset comprises questions about perception (e.g., ''what are the objects worth noting in the current scenario?''), prediction (e.g., ''where might the van, the sedan, and the pedestrian move in the future?), planning (e.g., ''what are the safe actions of the ego car considering those objects?'') and behavior (e.g., ''what would the ego vehicle's next action would be?''). We compare Cube-LLM with LLaVA-1.5 to show the impact of our pretraining, as well as the official baseline that has been recently released. All models"}, {"title": "4.5 General MLLM Benchmarks", "content": "We show the performance of Cube-LLM on general MLLM benchmarks. In Table 5, we compare Cube-LLM to the state-of-the-arts in Referring Expression Comprehension (REC) benchmark on refCOCO/+/g dataset. We compare Cube-LLM to specialist models such as MDETR and UniTAB which employs detection-specific architecture, and generalist models of same size such as Ferret , Qwen-VL and MiniGPT-v2 . In all test splits, Cube-LLM consistently outperforms with average score of 87.0. In Table 6, we compare Cube-LLM with other competitive MLLMs of same model size on VQAv2 , GQA , VizWiz , ScienceQA-Image , and POPE . The first row has models with fully zero-shot evaluation, and bottom rows has models that has seen images from some of the datasets. Compared to LLaVA-1.5 , miniGPT-v2 and Qwen-VL , Cube-LLM maintain competitive result, validating that our 3D understanding does not degrade general reasoning capability of MLLM."}, {"title": "4.6 Ablation Study", "content": "Our work consists of three key contributions, including a large-scale language- visual pre-training dataset LV3D, visual chain-of-thought prompting, and special- ist prompting. We provide comprehensive ablation studies for each of our design choices for Cube-LLM."}, {"title": "5 Conclusion", "content": "In this paper, we present Cube-LLM, a multi-modal language model that can reason in both 2D and 3D. We provide a collection of dataset (LV3D) and a training framework to effectively scale MLLM training for 3D understanding. We evaluate Cube-LLM in 2D and 3D grounded reasoning and VQA tasks, and show competitive results. We also show that Cube-LLM exhibits the behaviors of LLMs such as chain-of-thought prompting to further improve 3D localization of our model. Finally, we show that our model can adapt any specialist models during inference by prompting their predictions as visual prompts. We examine that pure transformer-based MLLM with minimal inductive bias can learn about 3D understanding solely by data scaling."}]}