{"title": "Deep Reinforcement Learning Agents for Strategic Production Policies in Microeconomic Market Simulations", "authors": ["Eduardo C. Garrido-Merch\u00e1n", "Maria Coronado-Vaca", "\u00c1lvaro L\u00f3pez-L\u00f3pez", "Carlos Martinez de Ibarreta"], "abstract": "Traditional economic models often rely on fixed assumptions about market dynamics, limiting their ability to capture the complexities and stochastic nature of real-world scenarios. However, reality is more complex and includes noise, making traditional models assumptions not met in the market. In this paper, we explore the application of deep reinforcement learning (DRL) to obtain optimal production strategies in microeconomic market environments to overcome the limitations of traditional models. Concretely, we propose a DRL-based approach to obtain an effective policy in competitive markets with multiple producers, each optimizing their production decisions in response to fluctuating demand, supply, prices, subsidies, fixed costs, total production curve, elasticities and other effects contaminated by noise. Our framework enables agents to learn adaptive production policies to several simulations that consistently outperform static and random strategies. As the deep neural networks used by the agents are universal approximators of functions, DRL algorithms can represent in the network complex patterns of data learnt by trial and error that explain the market. Through extensive simulations, we demonstrate how DRL can capture the intricate interplay between production costs, market prices, and competitor behavior, providing insights into optimal decision-making in dynamic economic settings. The results show that agents trained with DRL can strategically adjust production levels to maximize long-term profitability, even in the face of volatile market conditions. We believe that the study bridges the gap between theoretical economic modeling and practical market simulation, illustrating the potential of DRL to revolutionize decision-making in market strategies.", "sections": [{"title": "Introduction", "content": "The complexity of developing effective business strategies and production policies in dynamic markets has long been recognized as a significant challenge, as highlighted by studies on business strategy and simulation in complex environments [1, 2]. In particular, in highly volatile and complex markets dealing with lots of explanatory variables of an endogenous variable, conventional assumptions of stability and predictability are often invalid, leading to chaotic conditions where traditional economic models struggle to provide accurate forecasts and actionable insights [3, 4]. Given this complexity in the patterns involving variables such as how many production should a company perform in a certain time, it must be emphasized that neural networks, known for their capability as universal approximators, offer a potential solution by modeling non-linear patterns within the markets, enabling us to represent in the weights of the network intricate dependencies that are typically not representable by the capacity of simpler models like generalized linear models [5]. However, neural networks trained by supervised learning need an annotated dataset, independent and identically distributed data and a derivable numerical continuous loss function, assumptions that are often not possible to satisfy in daily markets.\nOn the other hand, reinforcement learning, particularly in a simulated environment, provides a framework for agents to learn optimal strategies through experience, iteratively refining their actions based on feedback [6] that can be given at any moment in the training of the agent and can be discrete. In particular, this process mimics real-world learning by trial and error, where agents adjust their policies to maximize cumulative rewards over time. However, in scenarios where the simulator does not perfectly replicate real-world conditions like the markets given their complexity, introducing controlled noise, a process which is known as domain randomization, can help bridge this reality gap. By perturbing certain parameters within the simulator, it is possible to generate variations that cover a wide range of possible real-world scenarios. Within these perturbed distributions, one instance may closely approximate actual market conditions, improving the robustness of the agent's learned policy [7, 8].\nIn our approach, we combine the virtues of the representative capacity of deep learning networks and the way that an agent learns with reinforcement learning to leverage deep reinforcement learning (DRL) within a corrupted simulation environment to approximate optimal production policies of an agent in a market. In particular, DRL's ability to handle high-dimensional spaces and learn complex policies enables us to model nuanced production strategies that adapt to fluctuating market dynamics, offering potential improvements in decision-making under uncertainty [9]. By iterating through multiple instances of a noisy simulator, our model develops robust strategies that can potentially be used in real, unpredictable market conditions, aiming to outperform static or traditional policy models.\nThe remainder of this paper is organized in the following way: First, Section 2 provides a review of the state-of-the-art in Deep Reinforcement Learning (DRL), including general approaches, applications in finance and economics, and"}, {"title": "State-of-the-art", "content": "In recent years, DRL has emerged as a powerful tool for solving complex decision-making problems in a plethora of domains. Concretely, DRL approaches have been successful in areas requiring sequential decision-making and adaptability such as the one described in this work, driven by the ability of neural networks to approximate complex functions and optimize actions through continuous feedback. For a complete description of DRL methodologies, far beyond the reach of this article, there is information in books such as those by [10] and [11], that provide comprehensive overviews of DRL techniques and their wide range of applications.\nDealing with the application of this article, microeconomics, and with the fact that no papers explore the production of items, it is interesting to see a related application of DRL where it is widely used, finance, especially for tasks such as portfolio management, asset pricing, and trading strategies. Several surveys, including [12], [13], and [14], have reviewed the current applications of DRL in finance, denoting how DRL can handle the stochastic nature of financial markets and adapt strategies to maximize returns, which is something similar to what we are going to illustrate in this paper. These studies emphasize the potential of DRL in transforming financial decision-making, where agents can learn optimal trading strategies by simulating market environments and optimizing based on performance feedback. However, they also highlight the challenges in applying DRL to finance, such as handling noisy data, high-dimensional spaces, and maintaining robustness in volatile markets. In particular, a specific interesting subfield where DRL is being applied is environmental, social, and governance (ESG) finance, which focuses on sustainable and socially responsible investments. Recent work by [15] explores the integration of DRL with ESG criteria, exploring how DRL models can prioritize investment strategies that align with sustainability goals.\nDealing with economics, many current DRL models in economics address tangent theoretical or predictive problems. According to [16], the integration of DRL in economic applications is primarily constrained by the complexity of modeling economic actions that involve multiple, interdependent factors, and by the challenge of capturing realistic producer behaviors in simulation environments. Although several DRL approaches are beginning to be used in economic problems no one deals with the actions that can be done by a producer [16]."}, {"title": "Methodology", "content": "In this section we briefly describe the fundamentals of, first, reinforcement learning and, then, of deep reinforcement learning to illustrate why the use of this methodology is adequate to estimate a policy of a production agent in a market simulation."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning [6] is a learning paradigm in which an agent interacts with an environment in discrete time steps, with the purpose of learning a policy that maximizes a given loss function, like cumulative rewards or, in our case, cumulative profits in a time period. Formally, the interaction of the agent with the environment is modeled as a Markov Decision Process (MDP), defined by a tuple (S, A, P, R, \u03b3), where S is the set of observed states by the agent, A is the set of actions that the agent can perform, P(s'|s, a) is the state transition probability, representing the probability of moving to state s' from state s after taking action a, R(s, a) is the reward function, representing the immediate reward after taking action a in states that the agent perceives and \u03b3\u2208 [0,1] is a discount factor variable, determining the importance of future rewards. The goal in RL is to find an optimal policy \u03c0* that maximizes the expected cumulative reward of the agent in its trajectory through states, or the return Gt = \\sum_{k=0}^{\\infty} \u03b3^k R(St+k, at+k), where t is the timestep. This optimal policy is typically obtained by maximizing the value function V\u03c0(s), defined as:\nV^\\pi (s) = E_\\pi [G_t|S_t = s]                                                                       (1)\nand the action-value function Q\u03c0(s, a):\nQ^\\pi (s, a) = E_\\pi [G_t|s_t = s, a_t = a] .                                               (2)\nThe agent seeks to maximize V\u03c0(s) and Q\u03c0(s, a) through various RL algorithms. However, reinforcement learning stores the value function and action-value function in a table that maps actions with states. In particular, if the observation space or action space of the MDP of an agent is high-dimensional or continuous, the table will need a huge amount of memory to store all the values mapping states and actions by the reward and computational resources to be traversed by the agent in training time. In order to circumvent the issue of the memory and assuming a smoothness between similar states and actions, Deep Reinforcement Learning will use deep neural networks to represent this mapping, as we will see in the following section."}, {"title": "Deep Reinforcement Learning", "content": "As we have introduced in the previous subsection, DRL generalizes traditional RL by using neural networks as function approximators, particularly useful in high-dimensional spaces where traditional tabular methods are infeasible [9]. In DRL, the agent employs a deep neural network Q(s, a; \u03b8) with parameters \u03b8 to approximate the Q-values or directly model the policy \u03c0(a|s; \u03b8). Roughly, depending on whether they approximate the value function, they directly optimize the policy or if they combine both approaches we find a family of methods belonging to where they focus the learning that we summarize here. In this work, we will use three algorithms, one belonging to each different type to test how they adapt to this problem.\nThe methodology for deep reinforcement learning (DRL) encompasses several approaches, including value-based, policy-based, and actor-critic methods, each with unique strategies for optimizing agent behavior within a complex environment.\nIn a value-based approach, a popular algorithm is Deep Q-Learning, known as DQN [17]. DQN uses Q-learning combined with neural networks to approximate the action-value function Q(s, a), which estimates the expected reward for taking an action a in a state s. The objective is to learn an optimal Q-function, Q*(s, a), that satisfies the Bellman equation:\nQ^*(s, a) = E [R(s, a) + \u03b3 \\max_{a'} Q^*(s', a') | s, a],                                                                                                                              (3)\nwhere R(s, a) is the immediate reward, \u03b3 is the discount factor, and s' is the next state. DQN updates its parameters by minimizing a loss function L(\u03b8) like the mean squared error (MSE) between a predicted Q-value and a target Q-value. Finally, the policy is set as the actions that are selected based on a greedy approach, that is selecting the action with the highest probability associated, or an \u03f5-greedy policy, relaxing the selection with a probability \u03f5 of choosing any action, to balance exploitation of learned actions and exploration of new possibilities.\nAnother family of algorithms, Policy-based methods, such as Proximal Policy Optimization (PPO) [18], focus on directly optimizing the policy \u03c0(a|s; \u03b8) to maximize the expected cumulative reward. PPO uses a clipping mechanism to maintain stable updates, keeping the policy within a trust region to avoid unstable learning. Concretely, the PPO objective function is defined as:\nL^{PPO}(\\theta) = E_t [min (r_t(\\theta) \\hat{A}_t, clip (r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)],                                                        (4)\nwhere r_t(\\theta) = \\frac{\\pi(a_t|s_t; \\theta)}{\\pi(a_t|s_t; \\theta_{old})} is the probability ratio between the new and old policies to avoid unstable learning, \u03f5 is a small constant, and \\hat{A}_t is the advantage function, which indicates the relative value of an action in a state with respect to the value function of the state. Critically, the clipping mechanism in PPO ensures that updates remain conservative, preventing large jumps in policy space that could destabilize learning."}, {"title": "Market Design", "content": "In this section, we will provide all the details of the market simulation that we have coded in a Gym environment. We will first describe the dynamics of the market and the parameters of the environment and then provide information about the observation space, action space and reward of the producer agent. The code of all the simulation and the experiments is available at Github (https://github.com/EduardoGarrido90/micro_agents)."}, {"title": "Market Simulation", "content": "We first describe the Market simulation where we have trained several agents. It basically contains code that simulates the dynamics of a market involving one particular asset. We assume that this item is unique in the market, in the sense that no substitute or complementary products exist. However, we simulate the behaviour of competitors producing the same item as the agent's producer.\nThe simulator extends the environment class of the gymnasium library, which includes a constructor, reset, step and render method. In the constructor, we initialize the production limit per producer, the minimum production, the number of competitors and their initial produced quantities sample from a discrete uniform distribution in the ranges of production, we also initialize the discrete action space of our agent, which are the units produced and the observation space of our agent, that is a Box space involving the variables detailed in the following subsection. We also initialize the market price of the asset, total supply and demand, fixed costs of the company sampled from a normal distribution, the coefficients of the cubic total production curve, the timestep of the simulation equal to 0 and some variables that the agent will use to track the production of the competitors in previous timesteps. The value of the parameters that we have used as a sample to configure the environment are the ones illustrated in Table 1.\nHowever, we emphasize that these values are only initial point estimates and that they are all a sample of different random variable whose joint random"}, {"title": "Markov Decision Process Design", "content": "In this section we describe the variables that the agent deals with in order to estimate a production policy in this market as the ones that are contained in a Markov Decision Process [20], that is, the variables belonging to the observation space, the action space and the reward.\nWe have summarized the variables perceived by the agent in the mentioned simulation in Table 2. Concretely, the table presents the components of the agent's observation space, action space, and reward function of the described market simulation in the previous subsection. Each element is categorized by its name, role, variable type, and description, detailing how it contributes to the agent's policy. In particular, the observation space includes variables that allow the agent to perceive the market environment, such as Total Supply, Total Demand, and Price, all of which are real values providing essential data on current market conditions. The Progress action, represented as an ordinal variable, informs the agent of market trends and how the competitors are going to react, indicating whether production levels are rising, stable, or declining, while Timestep%100 tracks the current timestep within a cycle, enabling temporal awareness of the sinusoidal movements of supply and demand. Additionally, Competitors Quantities provides an array of production levels from competing agents seen in the previous timestep, allowing the agent to strategize based on competitor actions.\nOn the other hand, the action space is defined by the variable Units Produced, an integer variable where the agent selects the number of units to produce"}, {"title": "Experiments and results", "content": "We now detail the format of our experiments and interpret all the results of them. We begin by formalizing our research hypothesis, that is, the DRL agents are able to estimate a policy that is competitive in a complex market. More formally, we can validate this hypothesis through statistical hypothesis testing in the following way:\nH0: \u03bcDRL \u2264 \u03bcrandom\nH1: \u03bcDRL > \u03bcrandom\nwhere \u03bcDRL is the mean performance of the policy estimated by the DRL agents and \u03bcrandom is the mean performance of a policy whose action is chosen at random at every timestep. Consequently, we would like to obtain empirical evidence that is not compatible with the null hypothesis, H0, that states that the mean performance of both methods is equal. Similarly, we want to validate an analogous hypothesis that can be formalized as:\nH0: \u03bcDRL < \u03bcdefault\nH1: \u03bcDRL > \u03bcdefault\nwhere \u03bcdefault is the mean performance of a policy that always chooses to produce the same number of assets in every timestep independently on the market state represented in every observation. In both cases, we would like to reject the null hypothesis to accept the alternative hypothesis that jointly states that the performance of the DRL trained estimated policy performance is significantly superior to a random or fixed policy, and hence, it is effective in complex markets. We finally want to test that:\nH0: \u03bcDRL \u2264 0\nH1: \u03bcDRL > 0\nthat is, the agents obtain profits after a certain test period in the market. It is possible that the policy is better than a random or fixed policy but ending in red numbers, consequently, we also need to validate that the agents have profits at the end of the test simulation.\nIn order to obtain empirical evidence to support these claims and reject the null hypothesis, we design the following experiment using the environment described in the previous section. We implement a dummy vectorized environment of the market simulation where we trained 10 agents using the PPO DRL algorithm from [0, 14] items that can be produced in each timestep. We also launch 15 agents with a default policy that always produce an asset between [0,14], each one corresponding to an asset number. In order to obtain evidence, we also initialize 5 random agents that simply choose in every timestep their action at random. Finally, for completeness, we also test value based and combined value and policy based DRL with a DQN and a A2C agent. We train each DRL agents for 1.5M timesteps using a learning rate of 10-4 and default hyperparameters. We represent the policy in the DRL agents with a multilayer perceptron neural network with the default StableBaselines3 configuration. For all the agents and configurations, we save different information that can be interpreted once the training period is finished. For evaluation purposes we launch all the agents 1000 timesteps and record the cumulated profits that are the reward of every timestep to compare their performances.\nFirst, we want to ensure that the DRL agents are effectively learning from the environment about the endogeneous variable of the experiment, the profits. We can see in Figure 1 the explained variance in the Y axis and the training timesteps in the X axis of every PPO agent.\nWe can see how the variability of the profits is starting to be explained from approximately 700K training timesteps and how the agents are able to explain a [60, 80]% of the profits, which is an amazing result given that the action of the agent does not condition the profits alone and lots of variables that condition the profits are contaminated by noise. In prediction terms, we also see the same trend in Figure 2, where we can see how the agents are able to better predict as the training process is done.\nIt is also interesting to check whether the changes in the policy distribution being learnt with respect to the previous policy in training time are lower as the training process goes on. This means that the training is more stable, converging into a stationary trajectory distribution. We can see in Figure 3 how this phenomenon happens as the KL divergence is lower as the training"}, {"title": "Conclusions and Further Work", "content": "This paper demonstrates how DRL agents can be effectively trained to estimate optimal production policies for a specific item, enabling competitiveness in a highly complex market environment. In particular, our approach is the first DRL attempt that illustrates the way for more sophisticated actions within the agent's strategy, accommodating various complementary and substitute products that influence market dynamics. By broadening the scope of available actions or simulating a multi agent system, the agents will gain flexibility to adapt its production policy in response to the evolving landscape of interconnected goods.\nWe have seen that we have used point estimations for the hyperparameters of the DRL agents, whose probability of being the optimum in a continous hyperparameter space is 0. Consequently, we believe that this approach will significantly benefit for performing DRL hyperparameter tuning using Bayesian optimization (BO) [21, 22], as only a few BO evaluations may be reasonable to do due to computational resources."}]}