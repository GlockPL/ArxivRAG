{"title": "Optimizing Small Language Models for In-Vehicle Function-Calling", "authors": ["Yahya Sowti Khiabani", "Farris Atif", "Chieh Hsu", "Sven Stahlmann", "Tobias Michels", "Sebastian Kramer", "Benedikt Heidrich", "M. Saquib Sarfraz", "Julian Merten", "Faezeh Tafazzoli"], "abstract": "We propose a holistic approach for deploying Small Language Models (SLMs) as\nfunction-calling agents within vehicles as edge devices, offering a more flexible\nand robust alternative to traditional rule-based systems. By leveraging SLMs, we\nsimplify vehicle control mechanisms and enhance the user experience. Given\nthe in-vehicle hardware constraints, we apply state-of-the-art model compression\ntechniques, including structured pruning, healing, and quantization, ensuring that\nthe model fits within the resource limitations while maintaining acceptable per-\nformance. Our work focuses on optimizing a representative SLM, Microsoft's\nPhi-3 mini, and outlines best practices for enabling embedded models, including\ncompression, task-specific fine-tuning, and vehicle integration. We demonstrate\nthat, despite significant reduction in model size which removes up to 2 billion\nparameters from the original model, our approach preserves the model's ability\nto handle complex in-vehicle tasks accurately and efficiently. Furthermore, by\nexecuting the model in a lightweight runtime environment, we achieve a generation\nspeed of 11 tokens per second, making real-time, on-device inference feasible\nwithout hardware acceleration. Our results demonstrate the potential of SLMs to\ntransform vehicle control systems, enabling more intuitive interactions between\nusers and their vehicles for an enhanced driving experience.", "sections": [{"title": "1 Introduction", "content": "The rapid evolution of vehicle software has created a complex ecosystem of interconnected Electronic\nControl Units (ECUs) via a Controller Area Network (CAN) bus. As consumer demand for advanced\nfeatures like virtual voice assistants, interior functions, and ambient settings grows, seamlessly\nintegrating these features into existing vehicle systems becomes increasingly complex. Traditionally,\neach new software feature requires extensive development to interface with core vehicle systems.\nHere, leveraging a SLM as intermediary to streamline communication between disparate systems may\nfacilitate easier integration of new features and adjustments based on driver conditions or external\nsoftware updates.\nSLMs like Gemma (2B), Microsoft Phi-3 mini (3.8B), Mistral (7B), and Llama-3 (8B) have shown\nstrong performance on academic benchmarks, despite being significantly smaller than traditional\nLLMs [1]. However, due to the constraints of in-vehicle hardware, deploying these models directly\nmay still be impractical. In this paper, we focused on optimizing these SLMs by further reducing their\nsize and fine-tuning them to maintain performance on domain-specific tasks. We employed advanced\nmodel compression techniques such as pruning, quantization, and lightweight runtime execution."}, {"title": "2 Related Work", "content": "Model Pruning and Healing: Model pruning reduces the size and complexity of machine learning\nmodels by removing less important weights, neurons, or entire layers [8]. The goal is to create a\nmodel with a reduced memory footprint, lower computational requirements, and faster inference.\nThis is particularly useful for deploying models on resource-constrained devices, such as mobile\nphones or embedded systems like vehicle head units.\nPruning approaches can be broadly categorized into structured pruning and unstructured pruning.\nStructured pruning removes entire structures within the network, such as neurons, filters, or layers."}, {"title": "Function-Calling", "content": "Function-calling is an emergent ability of language models which expands\ncapabilities beyond text generation allowing them to interact with tools, APIs, and the physical\nworld. Toolformer [14] showcased the ability of language models to use external tools through simple\nAPI calls, without explicit fine-tuning. Similarly, LangChain [15] provides an interface for chain\nof thought with various tools and data sources. Moreover, retrieval-augmented generation (RAG)\ntechniques, such as REALM [16], have been shown to further enhance the accuracy and reliability of\nfunction-calling by leveraging external knowledge sources during inference.\nAs a pivotal progress in function-calling by small language models on edge devices, the Octopus\nv2 paper [7] introduced a novel methodology by incorporating functional tokens directly into the\ntokenizer, thereby streamlining the function-calling process. This approach inspired our work, where\nwe employed specialized MB tokens, akin to the functional tokens in Octopus v2, to map language\nmodel outputs to specific vehicle functions. We also adopted a strategy similar to Octopus v2's\nnegative sample technique, incorporating irrelevant queries into our synthetic dataset to enhance the\nrobustness of our model against unintended function activations."}, {"title": "3 Methodology", "content": "We selected Phi3-mini\u00b2, which is a decoder-only transformer language model with L = 32 hidden\nlayers, as a representative of an SLM due to its small size of 3.8B parameters, its strong performance\nacross public benchmarks, and its ability to run across various software stacks [17]."}, {"title": "3.1 Model Pruning", "content": "We used both width and depth pruning in our experiments to produce two different variants of the\noriginal Phi3-mini: Phi3-2.8B and Phi3-1.8B. Table 1 contains details regarding the architecture of\nthe two variants and the original Phi3-mini.\nFor depthwise pruning, we pruned a contiguous block of size n = 8 which minimized cumulative\nlayer distance between decoder layers. Here we followed general guidance from Gromov et al. [5],\nwhere it was noted that dropping more than 30% of the layers across different model families (Llama,\nQwen, Mistral, Phi2, etc) resulted in collapse of the underlying model. Let hi represent the i-th\nhidden state of the model and n the chosen block size. Then, for all i \u2208 {1, . . ., L \u2013 n}, where L is\nthe number of hidden layers in the model, we computed the angular distance between hidden states as\n$d(h_i, h_{i+n}) := arccoS(\\frac{(h_i,h_{i+n})}{||h_i|||| h_{i+n} ||})$. Figure 2 shows the resulting distances for different block sizes\ncalculated against the fineweb dataset [18]. Layers 21-29 were maximally similar, and thus pruned.\nThe resulting model is denoted as Phi3-2.8B.\nPhi3-1.8B was then created by applying the width pruning approach from Muralidharan et al. [13]\nto Phi3-2.8B by recording activations on each layer (block) in the same manner as the depth-wise\napproach. Next, for the attention heads, we calculated the L2 norm along the head dimension and\ncomputed the mean across the sequence and batch dimensions for all activations. This gives us a\nscore for each hidden neuron, each neuron in the intermediate layer of the multi-layer perceptron\n(MLP), and each attention head. Finally, we pruned the neurons and attention heads with the lowest\nscore: the hidden dimension from 3072 to 2688, the MLP dimension from 8192 to 5120, and the\nnumber of attention heads from 32 to 28."}, {"title": "3.2 Healing and Instruction Tuning", "content": "After pruning, the resulting models struggle to generate coherent sentences and lose their alignment.\nAs described in Gromov et al. [5], this can be remedied by applying a healing training afterwards.\nThe authors suggest training for 5000 steps using QLoRA [19] on only the MLP weights with a\ndiverse web-scale dataset, for which we used fineweb dataset. We denote the models produced by\nthis step with hshort."}, {"title": "3.3 Fine-tuning for In-Vehicle Function-Calling", "content": "Fine-tuning language models has become a standard practice, with various approaches being explored.\nBoth full fine-tuning (FFT) and LoRA [22] are widely used methods, each with its own strengths and\nweaknesses. FFT offers comprehensive model adaptation but can be computationally expensive, while\nLORA provides a more parameter-efficient alternative, particularly beneficial when GPU resources\nare limited. In our research, we leveraged both full model training and LoRA training, allowing\nus to compare their performance and understand their trade-offs. LoRA's ability to extend model\nfunctionalities further highlights its potential for adapting our framework to a broader range of\napplications. In addition to being more computationally efficient, the modularity of LoRA adapters\nopens up the possibility of seamlessly switching between different adapters, allowing for dynamic\ncustomization and adaptation of the model to various tasks or domains, as explored in works like\nLoRA-Switch [23]. Building on the methodology of Octopus v2 [7], we fine-tuned the pruned and\nhealed phi3-mini model to enhance its function-calling capabilities for in-vehicle operations.\nSynthetic Dataset Generation for Fine-Tuning: We generated a comprehensive synthetic\ndataset inspired by the Octopus v2 model's technique of integrating functional tokens\ninto the tokenizer. Eight MB tokens were defined for specific vehicle functions, such as\nset_ambient_light_color_program mapped to <MB_1> and set_seat_heating_intensity\nmapped to <MB_2>. To ensure both diversity and naturalness, we utilized a multi-step prompt design\nfor generating positive and negative examples.\nPositive Examples: We used a prompt template to generate 25,000 examples evenly distributed\nacross all vehicle functions. For example:\nQuery:\nWarm up my seat and set the mood to Malibu Sunset before I get in the car\nResponse:\n<MB_2>(seat_position=\"FRONT_LEFT\", intensity=3);\n<MB_1>(color_program=\"MalibuSunset\");\n<MB_O>(message=\"I've warmed up your seat and set the ambient lighting\nto Malibu Sunset. Your car will be inviting when you get in.\")<MB_end>\nNegative Examples: To improve model robustness, 500 irrelevant queries were generated using a\nnegative sampling strategy. These queries were plausible but unsolvable by the provided functions\n(e.g., \"Can you teleport the car to Hawaii?\"). The assistant responds by politely declining the request.\nQuality Control: To ensure the generated dataset reflects real-life spoken user queries, we manually\ncurated a subset of examples derived from common user questions and included them in the prompt\nto the LLM. We enforced an even distribution of function calls across different functions to avoid\nimbalance. Specific rules were added to the prompts to ensure high-quality dataset generation, and\nthis, combined with de-duplication and post-processing, maintained a high standard for the final\nexamples."}, {"title": "3.4 Model Deployment", "content": "There are various lightweight libraries optimized for on-device inference, such as: MLX, tensorflow-\nlite, ONNX, and ExecuTorch [25][26][27][28]. We chose to leverage the open source on-device\nruntime, llama.cpp, to host our resulting pruned Phi-3 model.\nllama.cpp is a wrapper around the ggml tensor library, which has native support for transformer\nmodel operations [29]. The framework makes use of the gguf file format to serialize language models\nand respective metadata (tokenizer, model type, quantization, etc.) into a single artifact, which is then\nexecuted against the ggml tensor library. It is flexible in its implementation and operations can be\nremoved or composed depending on the model graph being executed.\nSpecifically, we took the following steps to convert our model into the target format: merge LoRA into\nHF base model (If LoRA is used), convert safetensors artifact to gguf, quantize resulting gguf to 4-bit,\ntest resulting artifact, and quantify distance between gguf and original safetensors implementation.\nWhile gguf artifacts can be quantized from 2-bit to 8-bit, we chose a 4-bit quantization strategy. In\ndoing so, we balanced token throughput and generation with minimal added perplexity. Additionally,\nin this format a pruned Phi3 model uses less than 2gb of RAM."}, {"title": "4 Experimental Results", "content": "4.1 Evaluation Results on the Original Format\nTo assess the model's general language understanding, after various pruning, healing, and training\nsteps, we used the lm-evaluation-harness\u00b3 framework to evaluate models in their original\nsafetensor format. The evaluation covered multiple benchmarks, including standard tasks such as\nquestion answering, natural language understanding, and reasoning. Specifically we evaluated our\nmodels on Winogrande [30], TruthfulQA [31], MMLU [32], HellaSwag [33], and ARC [34]. The\nresults are outlined in Table 3."}, {"title": "4.2 Evaluation on Target Format", "content": "Since the models will run with llama.cpp in vehicle, it is imperative to check performance after\nconversion from safetensor to 4-bit gguf. Unfortunately, the lm-evaluation-harness cannot\ninterface with models in this format. So, we used llama.cpp's own evaluation tool for running\nstandard benchmarks on the converted models. Additionally, to evaluate model performance on\nfunction-calling tasks, we used an exact match metric which measures accuracy for both the function\nname and arguments. Table 4 presents the results of evaluations on 4-bit gguf models."}, {"title": "Results on General Language Understanding", "content": "The benchmark results across TruthfulQA, MMLU,\nand HellaSwag reveal a clear correlation between model size and performance on general language\nunderstanding tasks. As the model size decreases from 3.8B to 2.8B to 1.8B parameters, there's a\nconsistent decline in performance across all three benchmarks. The data indicates that reducing the\nmodel size from 3.8B to 2.8B results in a performance drop of about 4.5 to 5 points on MMLU, while\nfurther reducing to 1.8B leads to an additional drop of approximately 3 points."}, {"title": "Results on Function-Calling Performance", "content": "The fine-tuning process leads to considerable improve-\nments in function-calling accuracy across all models. Fine-tuning the Phi3-2.8B model with LoRA\nachieves a function-calling accuracy of 0.88 similar to FFT score. Even in the case of the smallest\nmodel, Phi3-1.8B, function-calling accuracy remains high, with LoRA achieving 0.86 and FFT\nscoring 0.84. These results highlight the efficiency of fine-tuning in enhancing task-specific perfor-\nmance, particularly for function-calling in vehicular systems. The minor variations in function-calling\naccuracy among models of different sizes suggests that even smaller models, such as Phi3-1.8B, can\neffectively handle specialized tasks like function-calling. Considering the current production speech\nsystem for function calling, which achieves an accuracy of 0.75 across various vehicle functions,\nthe function-calling accuracy of 0.85 or higher demonstrated by the SLM presents a promising\nimprovement over the existing system.\nIt should be noted that the message in <MB_0> is not evaluated in function-calling performance\nsince the ability for general language understanding was already evaluated. Although it degraded\nafter fine-tuning for in-vehicle function-calling, the message remains fluent and explains what\nfunctions are activated based on human evaluation. The following example is from Phi3-2.8B\nhlong + SFT + LoraFT (4-bit) which clearly depicts how a function activation addresses user\nneeds:\nQuery:\nIt's stuffy here, can you do something?\nResponse:\n<MB_4>(zone=\"FRONT_LEFT\", fan_value=3);\n<MB_O>(message=\"I'm increasing the fan speed to help circulate\nair in your area.\")"}, {"title": "4.3 Token Generation Performance Gains", "content": "Table 5 summarizes the token generation results of Phi3-mini across all experiments. It is worth\nnoting that depth-wise pruning yields a 2x increase in token-generation vs width-wise pruning. It can\nbe inferred that removing decoder blocks altogether is more consequential in the model's ability to\ngenerate high quality responses, as well as generation speed. Additionally, the 1.8B parameter model\nachieves a token generation speed of 11 tokens/sec on CPU. For reference, a Llama model running on\nan NPU achieves the same performance [35]. Figure 3 shows the CPU usage of the 1.8B model on a\nvehicle head unit. At inference, the pruned model uses 400% CPU (the underlying CPU is an ARM\nprocessor with 7 cores). It can be inferred then that even a standard Phi3 which is typically regarded\nas a small language model would use all available cores during inference, further demonstrating the\nneed for a pruning step pre-deployment. While the spike is significant, it is only sustained for the\nduration of inference and can be further mitigated by dynamically allocating resources to the LLM\nprocess before inference. Moreover given the magnitude of the model and applications which it can\nunlock, the tradeoff is reasonable. As a result we demonstrate the benefit and feasibility of running"}, {"title": "5 Conclusion", "content": "This work demonstrates the effective optimization of Small Language Models (SLMs) for in-vehicle\nfunction-calling, delivering high task accuracy and real-time performance on resource-constrained\nautomotive hardware. Through structured pruning, healing, and fine-tuning, we significantly reduced\nthe size of the Phi-3 mini model while preserving its ability to handle both general language tasks\nand specific vehicle functions. Our method shows that pruned and quantized models can efficiently\nperform real-time function execution, generating up to 11 tokens per second without hardware\nacceleration. This offers a scalable, flexible solution for modern vehicle control systems, enabling\nmore intuitive user interactions. Future work can focus on enhancing general language understanding\nand further refining these models for specific automotive tasks."}, {"title": "Supplementary Material", "content": "Vehicle function examples:\ndef set_ambient_light_color_program(color_program: str):\n\"\"\"\nSet ambient light program in the car.\nParameters:\ncolorProgram (str): Color programs options are\n[\"OceanBlue\", \"Miami Rose\", \"MalibuSunset\", \"BurningBlue\",\n\"VenicePink\", \"ChromeShine\", \"RedMoon\", \"JungleGreen\",\n\"Ultramarin\", \"FreshCyan\", \"Racing Yellow\", \"RacingRed\",\n\"Amethyst Heat\", \"RoseGoldSparkle\"]\n\"\"\"\ndef set_seat_heating_intensity(seat_position: str, intensity: int):\n\"\"\"\nSet seat heating intensity in the car.\nParameters:\nseatPosition (str): Seat position options are\n[\"FRONT_LEFT\", \"FRONT_RIGHT\", \"REAR_LEFT\", \"REAR_RIGHT\"]\nintensity (str): Intensity options are [0, 1, 2, 3]\n\"\"\"\ndef set_temperature(zone: str, temperature: float, unit: str):\n\"\"\"\nSet temperature in the car.\nParameters:\nzone (str): Zone options are\n[\"FRONT_LEFT\", \"FRONT_RIGHT\", \"REAR_LEFT\", \"REAR_RIGHT\"]\ntemperature (float): Temperature range is\nfrom 60 to 84 for FAHRENHEIT and 16 to 28 for CELSIUS\nunit (str): Unit options are [\"CELSIUS\", \"FAHRENHEIT\"]\n\"\"\"\ndef set_window_position(window_position: str, operation: str):\n\"\"\"\nSet window position in the car.\nParameters:\nwindowPosition (str): Window position options are\n[\"FRONT_LEFT\", \"FRONT_RIGHT\", \"REAR_LEFT\", \"REAR_RIGHT\"]\n- operation (str): Operation options are [\"OPEN\", \"CLOSE\"]\n\"\"\"\ndef respond_chat(message: str):\n\"\"\"\nRespond to the user's query, for example to\nprovide an answer or ask for more information.\nParameters:\nmessage (str): The message that should be returned to the user.\n\"\"\""}]}