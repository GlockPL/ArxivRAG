{"title": "OC3D: Weakly Supervised Outdoor 3D Object Detection with Only Coarse Click Annotation", "authors": ["Qiming Xia", "Hongwei lin", "Wei Ye", "Hai Wu", "Yadan Luo", "Shijia Zhao", "Xin Li", "Chenglu Wen"], "abstract": "LiDAR-based outdoor 3D object detection has received widespread attention. However, training 3D detectors from the LiDAR point cloud typically relies on expensive bounding box annotations. This paper presents OC3D, an innovative weakly supervised method requiring only coarse clicks on the bird's eye view of the 3D point cloud. A key challenge here is the absence of complete geometric descriptions of the target objects from such simple click annotations. To address this problem, our proposed OC3D adopts a two-stage strategy. In the first stage, we initially design a novel dynamic and static classification strategy and then propose the Click2Box and Click2Mask modules to generate box-level and mask-level pseudo-labels for static and dynamic instances, respectively. In the second stage, we design a Mask2Box module, leveraging the learning capabilities of neural networks to update mask-level pseudo-labels, which contain less information, to box-level pseudo-labels. Experimental results on the widely used KITTI and nuScenes datasets demonstrate that our OC3D with only coarse clicks achieves state-of-the-art performance compared to weakly-supervised 3D detection methods. Combining OC3D with a missing click mining strategy, we propose a OC3D++ pipeline, which requires only 0.2% annotation cost in the KITTI dataset to achieve performance comparable to fully supervised methods.", "sections": [{"title": "Introduction", "content": "In recent years, notable progress has been made in LiDAR-based 3D object detection research (Wu et al. 2023; Huang et al. 2024). Despite these advancements, the need for precise bounding box supervision remains a major challenge due to its time-consuming and labor-intensive nature. For instance, the KITTI dataset (Geiger, Lenz, and Urtasun 2012) contains 3,712 training scenes with over 15,000 vehicle instances, where manual annotation of a single instance can take roughly 114 seconds (Meng et al. 2021). The extensive labeling efforts required escalate dramatically when scaling detectors to larger-scale datasets (Sun et al. 2020; Caesar et al. 2020; Mao et al. 2021), thereby hindering further research in a fully supervised manner.\nTo alleviate the annotation burden, recent studies have explored alternatives that require fewer annotated frames or instances to train high-performing 3D object detectors. Specifically, semi-supervised methods (Wang et al. 2021; Park et al. 2022; Liu et al. 2023a) leverage a subset of the annotated frames, while sparsely-supervised methods (Xia et al. 2023b; Liu et al. 2022a; Xia et al. 2024) rely on only one bounding box annotation per frame during training. While these approaches have significantly lowered annotation costs, annotating 6DoF bounding boxes for every scene remains time-consuming.\nTo provide a faster, albeit less precise, method of human supervision, WS3D (Meng et al. 2021) and ViT-WSS3D (Zhang et al. 2023) propose a mixed supervision strategy that replaces some box annotations with the center-click annotations. In this approach, annotators click the center of objects on the Bird's Eye View (BEV) to generate center-level labels, reducing labeling time per instance to approximately 2.5 seconds - 50 times faster than traditional bounding box labeling.\nHowever, relying on a single center point has significant limitations: (1) it requires annotators to precisely indicate the center position and (2) it fails to accurately represent the shape and scale of objects, especially in sparse point clouds where objects may be partially observed. These challenges become even more severe for moving objects, where motion across frames further complicates the estimation of an accurate bounding box from clicks. As a result, prior works have struggled to scale up the use of click supervision, combined with traditional box annotation for mixed supervision, which diminishes the overall effectiveness.\nIn this paper, we introduce a novel pure click-supervised approach for 3D object detection (OC3D), which employs temporal cues to distinguish between static and moving instances and progressively recover their box supervision from coarse clicks for detector training. This strategy offers greater flexibility than center-clicked approaches by accommodating inaccurate clicks through shift tolerance, which can be corrected via multiple-frame consistency. Our framework comprises three key designs: (1) To efficiently classify the motion state of provided clicks, we analyze the point density at the clicked location from a statistical perspective, noting that vary point density over a short period for moving objects while stable density for static instances. (2) For static objects, we implement a Click2Box strategy that aggregates the neighboring points across multiple frames to reconstruct the object's structure and lift click-level supervision to a precise 3D box. (3) For moving objects, which are more challenging to regress, we generate mask-level pseudo labels (Click2Mask) to assign the clustered points with labels, enabling models to learn to predict the object's location. To compensate for the lack of supervision in predicting shapes and scales for moving objects, we leverage their geometric similarity as a bridge, further refining the 3D box through a Mask2Box process. To address extreme cases where only one click is provided per scene (sparsely supervised setting), we propose a variant OC3D++, which identifies missed click instances and facilitates training with only 0.2% of the annotations required for full labeling.\nWe evaluated OC3D and OC3D++ on the widely adopted KITTI and nuScenes datasets. Remarkably, OC3D achieves competitive performance with weakly-supervised baselines that rely on accurate box annotations. Furthermore, when the number of clicks is reduced to one per scene, OC3D++ still maintains robust performance, demonstrating the resilience and effectiveness of our approach under the sparely supervised condition. In summary, our contributions are:\n\u2022 We propose the first method of only click annotated 3D object detection from point cloud (OC3D), which solely relies on coarse clicks on the Bird's Eye View (BEV). This approach dramatically reduces the annotation cost of 3D object detection tasks.\n\u2022 We design the Click2Box and Click2Mask modules according to the motion attributes of instances, inferring accurate mixed supervision information from the click annotations.\n\u2022 We design a Mask2Box module that upgrades the less informative mask-level pseudo labels to box-level pseudo labels, which compensates for the loss of object shape and scale information in mask-level supervision."}, {"title": "Related Work", "content": "LIDAR-based 3D Object Detection. In recent years, fully-supervised 3D object detection has been widely studied. The early methods (Lang et al. 2019; Yan, Mao, and Li 2018; Yin, Zhou, and Kr\u00e4henb\u00fchl 2021) utilized an end-to-end one-stage object detection strategy, predicting detection boxes directly from point clouds. Subsequently, the two-stage methods (Shi, Wang, and Li 2019; Deng et al. 2021; Shi et al. 2023; Wu et al. 2022b; Chen et al. 2023; Wang et al. 2024) introduced an additional proposal box refinement stage, which improves detection performance by refining regions of interest. Meanwhile, multi-stage methods (Wu et al. 2022a; Cai et al. 2022) have begun to show their potential and prominence. These methods progressively refine proposal boxes using a cascaded optimization strategy, leading to more precise prediction results. Despite achieving excellent detection results, all these methods require fully annotated box-level annotations, the generation of which is time-consuming and labor-intensive.\nLabel-efficient 3D Object Detection. Research on reducing annotation costs in 3D object detection tasks has received widespread attention. Around semi-supervised methods (Liu et al. 2023a; Park et al. 2022; Wang et al. 2021), they selected only a small number of fully annotated frames as labeled data, using the remaining frames as unlabeled data. These methods used teacher-student networks for distillation learning to mine and generate pseudo-labels. Sparsely-supervised methods (Xia et al. 2023b; Liu et al. 2022a) adopted a sparsely annotated strategy, retaining only one complete bounding box label for each selected frame. They utilized specially designed unlabeled object mining modules to discover potential pseudo-labels. There are also some multimodal weakly supervised object detection methods (Wei et al. 2021; Liu et al. 2022b) that used 2D bounding boxes instead of 3D bounding boxes to carry out object detection tasks. Although these strategies have significantly reduced the dependence on 3D boxes, it is still not possible to completely abandon laborious box-level annotations."}, {"title": "Proposed Approach", "content": "Problem Definition. We start by defining the task of weakly-supervised 3D object detection using pure click supervision. In this setting, the detector is trained exclusively on instance-level click annotations  $c_o = \\{x,y\\}$ on the 2D BEV plane, and is then required to predict the full 3D bounding boxes $\\{x,y,z,l,w,h,\\theta\\}$. Here, $x, y, z$ represent the coordinates of the 3D center point, while $l, w, h$ denote the object's dimensions, and $\\theta$ specifies its orientation. Importantly, the click annotations employed are not limited to precise center clicks; further details are provided in the supplementary material. As shown in Fig. 2, given the coarse nature of click supervision, our approach proceeds by first (1) generating mixed-granularity supervision for both static and moving objects using the Click2Box and Click2Mask strategies, followed by (2) applying Mask2Box-enhanced self-training to refine and improve the quality of generated supervision. The specific steps involved in this process are elaborated as follows."}, {"title": "Mixed Pseudo-Label Generation", "content": "To accurately estimate 3D bounding boxes from click annotations as pseudo labels, we aim to leverage temporal cues to enrich sparse point annotations by aggregating registered points across consecutive frames. For static instances, the aggregation of dense points facilitates the complete capture of spatial geometric information of the instance. However, for dynamic objects, the dense point aggregation often fails to capture the full geometric structure, leading to low-quality pseudo-labels. This challenge motivates us to first classify the motion status of instances corresponding to each annotation click:\nMotion State Classification for Clicked-instance. We observe the duration of local point distribution at clicked positions during a long sequence traversal. Specifically, for static instances, the local point cloud at the clicked position exhibits a continuous distribution, whereas, for dynamic instances, the local point cloud at the clicked position is transient throughout the traversal. Motivated by this, we utilize the persistence of points at local positions within the long sequence for dynamic and static classification.\nFor each click annotation $c_o = (x_o, y_o)$ at the t-th frame, we gather adjacent frames $F = \\{f_{t-k}, ..., f_t, ..., f_{t+k}\\}$ within a local time window k, followed by ground removal (Himmelsbach, Hundelshausen, and Wuensche 2010) as a preprocessing step. Our primary focus is on the BEV points in F, denoted as $\\{PBEV \\in R^{N \\times 2}\\}_{t\\in[t-k,t+k]}$, where N indicates the number of BEV points in each frame. To determine the persistence of the clicked position $(x_o, y_o)$, we search for its neighboring BEV points within a radius r, resulting in the collection $\\{N_t\\}_{t\\in[t-k,t+k]}$, with each time step having a cardinality of $N_t$:\n$N_t = \\{p_i \\in PBEV \\mid ||P_i - C_o||_2 \\leq r\\}, N_t = |N_t|.$   (1)\nTo better tally the duration for which points are continuously present near the clicked location, we construct the function g(t), and perform a differentiation operation on g(t):\n$g(t) = \\begin{cases} 0 & \\text{if } N_t = 0; \\\\ 1 & \\text{otherwise.} \\end{cases}$ (2)\n$\\Delta g(t) = g(t + 1) \u2013 g(t).$   (3)\nIn adjacent T frames (T = 2k + 1), the duration of the neighborhood points of the click position can be calculated based on the index of the specific value of $\\Delta g(t)$. That is, $\\Delta g(t) = 1$ indicates the point begins to appear, $\\Delta g(t) = -1$ indicates the point disappears, and marking the time difference between the last appearance and the next disappearance of the point on the click annotation frame is the duration time $\\Delta t$. Subsequently, the motion state of the clicked-instance is determined based on the proportion of $\\Delta t$ that occupies adjacent T frames.\n$\\Omega = \\begin{cases} \\text{static} & \\text{if } \\frac{\\Delta t}{T} > \\tau; \\\\ \\text{dynamic} & \\text{otherwise.} \\end{cases}$ (4)\nwhere $\\tau$ is the duration threshold. If $\\frac{\\Delta t}{T}$ exceeds the threshold, it indicates that the local point cloud around the click has a longer duration and is considered a static instance. Conversely, if it does not exceed the threshold, it is considered a dynamic instance.\nClick2Box. For static objects, dense points express complete geometric structures, which supports the fitting of high-quality bounding box pseudo-labels from the point cloud distribution. Motivated by this observation, we concatenate the neighboring points of multiple frames $\\{N_t\\}_{t\\in[t-k,t+k]}$ to obtain local dense points $D_t$ for the time step t. We perform DBSCAN (Ester et al. 1996) clustering algorithm on $D_t$ to generate several discrete point clusters. We retain the cluster of points whose center is closest to the clicked position and consider the points in this cluster as the foreground points of the clicked instance. Finally, we perform a bounding box fitting algorithm (Zhang et al. 2017) on the foreground points to generate a box-level pseudo-label. We utilize Click2Box to infer box-level pseudo-labels $A_b$ from the click annotations of all static instances.\nClick2Mask. For dynamic objects, we opt to leverage only the single-frame point cloud $P_t$ due to the long-tail distribution observed in aggregated points resulting from motion differences (Chen et al. 2022). Although the instance shape and scale cannot be revealed by click-level labels, the foreground points from click-annotated frame can still provide reliable semantic information and coarse location information. Consequently, instead of generating box-level pseudo labels for dynamic objects, we produce mask-level pseudolabels by extracting the foreground points in $P_t$. To identify these foreground points, we employ the DBSCAN clustering algorithm on raw point clouds, and select the cluster with the center closest to the click location a mask-level pseudo label. The resulting mask-level pseudo labels denoted as $A_m$, are derived from the click annotations of all dynamic instances."}, {"title": "Mask2Box Enhanced Mixed Supervised Training", "content": "In contrast to traditional 3D object detectors that are solely reliant on box supervision, our approach delves into weakly supervised detectors with mixed supervision. Inspired by MixSup(Yang, Fan, and Zhang 2024), we re-engineered the strategy for supervision allocation. However, the information about mask-level supervision is limited, and it is challenging to achieve optimal detector performance. To address this issue, combining with an iterative training, we introduce a mask2box enhanced training strategy, which leverages the high-confidence outputs from the last iterative detector to refine mask-level pseudo-labels into more accurate bounding box-level pseudo-labels, thereby improving the overall precision of the detection process.\nMixed Supervised Training. Referencing (Yang, Fan, and Zhang 2024), we redesigned the supervision assignment strategy. For all box-level pseudo-labels $A_b$ of size $A_b$, we train the detection network to focus on its position, shape, orientation, and semantics. For mask-level pseudo-labels $A_m$ of size $A_m$, the detector focuses solely on semantics and the center of masks. Therefore, the loss function for OC3D can be formulated as:\n$\\mathcal{L} = \\frac{1}{A_b + A_m}\\sum_{A_b} \\mathcal{L}_{reg} + \\frac{1}{A_b + A_m}\\sum \\mathcal{L}_{cls}+ \\lambda \\frac{1}{A_m} \\sum \\mathcal{L}_{pos}$ (5)\n$\\mathcal{L}_{reg}$ and $\\mathcal{L}_{cls}$ are commonly used regression and classification losses in 3D object detection. $\\mathcal{L}_{pos}$ is the part that decouples the central position from $\\mathcal{L}_{reg}$. Since mask centers are not accurate instance centers, we set the hyper-parameter $\\lambda$ to reduce the weight of this part of the loss.\nMask2Box Enhanced Training. Mask-level pseudo-labels provide only semantic and coarse localization information, lacking a description of the instance shape. To compensate for the information lost in mask-level pseudo-labels, we use high-confidence bounding-box predictions to upgrade mask-level pseudo-labels. As shown in Fig. 3, based on mask-level pseudo-labels, we use the box fitting algorithm (Zhang et al. 2017) to generate a temporary bounding box (Temp. Box). Then, we calculate the IoU of the temporary box with all high-confidence predictions and return the prediction box with the highest IoU:\n$Proposal = arg \\underset{l_i}{max}(IoU(\\underset{Temp. Box}{\\sim} l_i))$. (6)\nwhere $l_i$ originates from all the high-confidence predictions. If the point-level pseudo-label is within the proposal, then the proposal is used to replace the point-level pseudo-label. Conversely, the point-level pseudo-label is retained and will be awaited for the next upgrade opportunity.\nExploration of Sparse Click 3D Detector\nRecently, weakly supervised 3D object detection methods(Xia et al. 2023b, 2024) with sparse bounding boxes have made great progress. The sparse annotation approach can greatly reduce the annotation cost. Motivated by this, we further explore the weakly supervised 3D object detection scheme based on sparse clicks. Unlike conventional click annotations, under sparse clicking, only one instance is randomly selected for annotation in each scene, presenting a greater challenge to the 3D object detector. To address the problem of missing annotation information, we propose a OC3D-based missing-click instances mining method, OC3D++ (for specific design details, see the supplementary materials). The experimental results of OC3D++ demonstrate that our OC3D can be effectively combined with missing-click instances mining methods."}, {"title": "Experiments", "content": "Datasets and Metrics. For KITTI dataset (Geiger, Lenz, and Urtasun 2012), to acquire a dense point cloud, we extracted point cloud data from consecutive frames of the raw data. It is worth noting that the raw data is only used to assist in the generation of pseudo-labels and does not participate in the training process of the detector. For the training stage, following traditional fully-supervised methods (Shi et al. 2020; Zhou and Tuzel 2018; Xia et al. 2023a), KITTI 3D detection dataset is divided into a train split of 3712 frames and a val split of 3769 frames. For the train split, we only retain the coarse click annotations. Moreover, we further generate an extremely low-cost training set (denoted as the limited split), retaining the click annotations for only one instance in each training scene. To ensure a fair comparison, following sparsely-supervised methods (Liu et al. 2022a; Xia et al. 2023b), we calculate the mean Average Precision (mAP) using 40 recall positions.\nNuScenes (Caesar et al. 2020) is a comprehensive autonomous driving dataset that includes 1000 sequences, consisting of 700, 150, and 150 sequences for training, validation, and testing, respectively. There are a total of 28k annotated frames for training, 6k for validation, and another 6k for testing. We also retain coarse click annotations for all objects in the train split in nuScenes. Considering multi-class sample balance, we performed a single click annotation for each category present in the scene to obtain limited split for nuScenes. For evaluation, we adopt mAP and nuScenes detection score (NDS) as the main metrics.\nImplementation. For coarse click annotations, we use the BEV center of ground truth as a reference and apply a large perturbation range (0.5 \u00d7 w, 0.5 \u00d7 1) to simulate coarse manual clicks. In the supplementary material, we demonstrate a comparison between the effects of simulated clicks and real coarse manual clicks. Meanwhile, in Tab.4, we analyze the impact of different perturbation ranges on the results. Due to the large number of categories in nuscenes and the significant size differences between them, we set different neighborhood radii r for different categories based on common human prior knowledge. Specific settings can be found in the supplementary material.\nBaselines. We are the first to develop a method for training object detectors only with click-annotations, and there are no previously published baselines for comparison. To validate the effectiveness of our OC3D, we chose to compare it with works that are also label-efficient. To ensure a fair comparison, we adopt widely used VoxelRCNN (Deng et al. 2021) as basic detector architectural. Furthermore, leveraging the proposed Click2Box module, we infer bounding box supervision from click annotations to construct multiple baselines with click annotations."}, {"title": "Ablation Study and Analysis", "content": "Effect of Mixed Supervised Training (MST) and Mask2Box Enhanced Training (MET). The effects of the proposed MST, and MET are listed in Tab. 3. The first row of Tab. 3 presents the baseline performance, using only the proposed Click2Box strategy to generate bounding box pseudo-labels from click annotations. The results in the second row represent the detector performance by mixed supervised training with mixed pseudo-labels. Our design eliminates the adverse effects of dynamic low-quality bounding box pseudo-labels and achieves better detection results. Finally, based on Click2Box and Click2Mask, we designed the Mask2Box module to continuously upgrade mask-level pseudo-labels to box-level pseudo-labels, providing more accurate supervision signals that significantly improve detection accuracy.\nEffects of Perturbation Factor \u03b4 and Weight Selection for \u03bb. Tab. 4 demonstrates the robustness of the coarse click. To rigorously verify the impact of perturbation factor, we conduct experiments on the more challenging sparse clicking setup. We applied various disturbance factors \u03b4 = 0.25, 0.5, 0.7, 1.0, defining the disturbance range as (\u03b4 \u00d7 \u03c9, \u03b4 \u00d7 l), where w and l are the size of the object. Even with a large disturbance range (\u03b4 = 1.0), our OC3D++ still achieves satisfactory performance. In Tab.5, we discuss the hyper-parameter \u03bb in Equ.5 without the Mask2Box modules. Lower weights for ambiguous location predictions yield better results. This result indicates that for ambiguous location supervision, providing a single hint is sufficient to achieve optimal detection performance, and excessive weighting introduces noise to the detector.\nPrediction Results and Quality of Pseudo-labels at Each Iteration. We show the performance of OC3D++ over different numbers of training iterations in Fig. 4(a) and (b). At both the 0.5 and 0.7 thresholds, the performance is significantly enhanced. Using the ground truth as a reference, we observed the average IoU and recall throughout the iterative learning process. As shown in Fig. 4(c), there is an overall stable upward trend in the quality of our pseudo-labels as the number of iterations increases. Meanwhile, we also observed a decrease in IoU and a sudden increase in Recall during the 5th iteration. This trend is attributed to the retention of additional pseudo-labels, which is a normal occurrence in the iterative process."}, {"title": "Discussion and Conclusion", "content": "We designed an efficient annotation strategy called coarse click annotation for 3D object detection and proposed a weakly-supervised object detection method, OC3D, leveraging this approach. In the mixed pseudo-label generation stage, we propose a novel method for dynamic and static classification, and design the Click2Box and Click2Mask modules according to the motion states of objects to generate mixed pseudo-labels. Subsequently, in the Mask2Box enhanced mixed supervised training stage, We train the 3D detector with mixed pseudo-labels and design the Mask2Box module to obtain richer supervisory information. Extensive experiments on KITTI and nuScenes have shown that our OC3D achieves commendable performance with purely clicks. Even in the more challenging sparse clicking, our OC3D++ continues to deliver outstanding performance.\nLimitations. Although the mixed pseudo-label generation stage provides the detector with good initial pseudo-labels that describe the geometric shape and location of instances, these rule-based labels may not match the high-quality annotations produced by human annotators. This discrepancy leads to reduced performance under higher IoU thresholds, where precise alignment is critical."}]}