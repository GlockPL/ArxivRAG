{"title": "Understanding How Paper Writers Use AI-Generated Captions in Figure Caption Writing", "authors": ["Ho Yin (Sam) Ng", "Ting-Yao Hsu", "Jiyoo Min", "Sungchul Kim", "Ryan A. Rossi", "Tong Yu", "Hyunggu Jung", "Ting-Hao 'Kenneth' Huang"], "abstract": "Figures and their captions play a key role in scientific publications. However, despite their importance, many captions in published papers are poorly crafted, largely due to a lack of attention by paper authors. While prior AI research has explored caption generation, it has mainly focused on reader-centered use cases, where users evaluate generated captions rather than actively integrating them into their writing. This paper addresses this gap by investigating how paper authors incorporate AI-generated captions into their writing process through a user study involving 18 participants. Each participant rewrote captions for two figures from their own recently published work, using captions generated by state-of-the-art AI models as a resource. By analyzing video recordings of the writing process through interaction analysis, we observed that participants often began by copying and refining AI-generated captions. Paper writers favored longer, detail-rich captions that integrated textual and visual elements but found current AI models less effective for complex figures. These findings highlight the nuanced and diverse nature of figure caption composition, revealing design opportunities for AI systems to better support the challenges of academic writing.", "sections": [{"title": "1 Introduction", "content": "Scientific publications rely on figures and their captions to convey complex information, making captions crucial for enhancing figure interpretability and accessibility. Research shows that captions improve comprehension of visuals in educational materials [25], aid students across expertise levels, and enhance accessibility by guiding readers through complex visualizations [36]. Given the importance of captions, researchers have explored using AI, particularly Large Language Models (LLMs), to generate captions for scientific figures [24,31,19,16]. Most prior work focused on reader-centered use cases, assuming users would only read and evaluate pre-generated captions without modifying them. However, the prevalence of poor captions in published papers\u2014studies have shown that a significant portion of captions in arXiv papers are unhelpful [19]\u2014stems from paper writers, not readers. Many paper writers do not pay attention to crafting effective captions for their figures, resulting in many low-quality captions, even in published papers.\nThis paper examines how paper authors integrate captions generated by state-of-the-art figure captioning models into their writing process. Studying the academic writing process presents unique challenges distinct from those involved in supporting academic reading. Paper authors are domain experts who have devoted substantial time to their projects, developing a deep understanding of both the content and the broader scientific context. This level of expertise often surpasses that of readers of their papers. This knowledge gap also complicates participant recruitment, as it is challenging to find participants actively writing new papers who are willing to work on their real papers during a user study. As a result, prior user studies on AI-generated captions often involved participants evaluating captions from papers written by others [50,19]. Even SCICAPENTER, the only prior work focusing on supporting figure caption writing [17], asked participants to write captions for others' papers. While this approach simplifies participant recruitment, it does not accurately reflect the real-world scenario where researchers write captions for their own work. Additionally, scholarly authors face the unique challenge of addressing a broader and more diverse academic audience [51]. They must consider subtle and nuanced aspects of communication to effectively convey their ideas. These considerations are inherently difficult for automated systems to capture and support, making them particularly valuable to study and understand.\nThis paper presents a user study involving 18 participants, each tasked with rewriting captions for two figures from their own recently published work (Figure 1). For each figure, participants were provided with captions generated by state-of-the-art AI models, which they could freely use to aid in their caption-writing process. During the study, participants had access to the full content of their papers, except that the original captions for the target figures were redacted. To study their writing behavior, we recorded the entire caption-writing process on video. These recordings were then analyzed through interaction analysis [21], which involved manually coding the videos to capture participants' writing behaviors. We observed that most participants began by copying substantial portions of the AI-generated captions, subsequently refining or building upon them. Our findings also suggest that AI-generated captions were more helpful for statistical figures than for conceptual, non-statistical ones. Additionally, authors showed a preference for longer AI-generated captions that integrated both textual and visual details. However, the current AI models struggle to assist effectively with caption writing for complex figures.\nDespite being a seemingly narrow aspect of academic writing, figure caption composition involves subtle nuances and exhibits significant diversity across authors. These findings highlight opportunities for designing future writing systems that better support the complex and nuanced process of caption writing."}, {"title": "2 Related Work", "content": "Figure Caption Generation and Evaluation. Despite the growing focus on evaluating AI models for generating captions of scientific figures, the evaluation of such models's output often overlooked the writer's perspective.\nPrevious studies have widely used automatic metrics to evaluate the quality of AI-generated figure descriptions. For instance, BLEU measures word overlap between the generated and reference texts [7,40,32,5,6,1,34,52,8] and, in a similar vein, ROUGE focuses on how much content from the reference text is included in the generated text [7,43,5,6,52,8]. METEOR evaluates the generated output by measuring its semantic similarity to reference translations [7,1,8], considering factors such as synonyms and morphological variations. However, automatic evaluation metrics have significant limitations. Human-written captions in scientific papers are often of low quality, leading to unreliable comparisons between machine output and manual captions [23]. Automatic evaluations do not always align well with human judgments, revealing a gap between machine scoring and actual human comprehension [11]. Finally, these comparisons miss what was not explicitly stated, including the author's intentions and considerations [18].\nTo address these limitations, human evaluations have been incorporated alongside automatic metrics. However, these human evaluations often fail to adequately capture the writer's intent behind the captions [22,27,33,37,34,52,8]. Instead, they focus on aspects like matching degree and reasoning correctness [33], informativeness, conciseness, coherence, and fluency [37] to understand how well the generated text describes the figures or aids reader comprehension. These methods, while valuable, miss the unique intentions and considerations of the writers throughout the writing process, including brainstorming, planning, and refinement. Motivated by this gap, our work engages real caption authors to rewrite their captions using AI-generated captions, allowing us to understand the AI tool's utility.\nAI Writing Assistants. AI writing assistants have been explored in various writing contexts, from creative [42] to legal [48] and medical practice [13], highlighting the potential for Human-AI collaboration in specialized tasks. In academic writing, AI has been utilized for sentence-level suggestions [15], literature review writing [9,2], and support in critical evaluation during peer reviews [46]. The integration of AI writing assistants in academic tasks raises questions about accuracy and trust, with previous studies highlighting both potential benefits and challenges [4]. While many studies have explored AI in academic writing, specific tasks like caption generation remain understudied, particularly from the writer's perspective. Our work aims to address this gap by examining how researchers interact with and perceive AI-generated captions in scientific writing.\nHuman-AI Collaboration. Human-AI collaboration has gained significant attention, particularly in academic contexts where AI tools assist in various research and writing tasks. For example, Shen et al. [39] introduced the concept of bidirectional human-AI alignment, focusing on aligning AI systems with human values while also fostering the adaptation of human practices to AI capabilities. Lee et al. [29] also emphasized the importance of considering both AI capabilities and user writing processes, providing a foundation for our investigation into researchers' perspectives on AI-assisted caption writing for scientific publications. These works underscore the need for writing tools that not only generate content but also support the user's cognitive processes and decision-making."}, {"title": "3 User Study", "content": "We conducted a study where 18 participants rewrote captions from their published work using AI-generated captions. This section provides a detailed overview of the study's design and procedure."}, {"title": "3.1 Participants", "content": "Participants were recruited through the online questionnaire assessing their academic writing experience. The questionnaire collected data on research areas,"}, {"title": "3.2 Study Material Preparation", "content": "Participants provided up to three of their own papers published within the last three years, but at least one month ago. This timeframe ensured participants were familiar with their work while simulating realistic writing scenarios.\nSelecting Statistical and Conceptual Figures from Participants' Papers. The selection of figure types aimed to encompass diverse caption writing requirements. While prior works have established detailed classifications spanning tables, photos, diagrams, maps, and plots [12], we simplified the figure classification into two broader categories based on previous work about qualitative and quantitative representation [3]: statistical and conceptual figures. Statistical figures present quantitative data through graphs or charts, while conceptual figures illustrate theoretical models or processes. This binary classification offered a systematic framework for comparing caption-writing strategies across different types of visual information. For the study, we aimed to select one statistical and one conceptual figure from each participant's provided papers. However, the selection was constrained by participants' research fields, as some areas or venues predominantly feature statistical figures. While achieving an equal distribution of figure types across all participants was challenging, we made every effort to balance them. We ended up including 16 statistical figures (from 15 participants) and 18 conceptual figures (from 16 participants). Two figures from one participant were excluded due to their complexity, as they combined both statistical and conceptual elements; we still completed the user study with this participant and discussed their results in a separate subsection."}, {"title": "Generating Captions for Selected Figures.", "content": "The goal of our study is to understand how paper writers interact with AI-generated captions when composing captions for scientific figures. For this purpose, we employed state-of-the-art approaches to generate captions for the selected figures in participants' papers. Prior work has pointed out that scientific figure caption generation is not a straightforward image-to-text task but a generation task that relies heavily on the intertwined scientific context [19,50]. The context is so crucial that the figure caption generation task can be tackled as a pure text summarization task that \"summarizes\" all the figure-mentioning paragraphs in a paper-without even considering the figure image into a caption [19]. In this study, we selected GPT-40, the most recent large vision-language model, as our base model and prompted the model to take both the target figure image and figure-mentioning paragraphs as input to generate figure captions. We used a prompt similar to that of SciCapenter [17]; approaches have been shown to be effective in the SciCap Challenge 2024 [28,45,30]. We prompted GPT-40 in a zero-shot manner (no example provided) to generate captions under three distinct conditions:\n1. UNLIMITED: In this setting, we prompted GPT-40 to generate captions using both the figure image and the paragraphs referencing the target figure (e.g., \"Figure 3 shows...\") as input. No length constraints were imposed, allowing GPT-40 to produce captions of any desired length.\n2. 30-WORD: This setting is identical to UNLIMITED, except that a length constraint of no more than 30 words (tokens) was applied. This specific length constraint has also been used in prior work on figure caption generation [19].\n3. TEXT-ONLY: In this setting, we prompted GPT-40 to generate captions using only the paragraphs referencing the target figure as input, without using the figure image. Previous research has demonstrated that figure captions can be generated using textual context alone [19]. No length constraints were applied in this setting.\nTo determine which captions were most preferred, all AI-generated captions were presented to participants in a randomized order, without revealing the model that generated each caption."}, {"title": "3.3 Study Procedure and Setups", "content": "Figure 1 shows the procedure of our user study, which contained semi-structured interviews and two writing tasks. The study was conducted remotely via Zoom and lasted approximately 60 minutes. The entire Zoom session, including participants' computer screens, video, and audio, was recorded for further analysis. The writing task was conducted in a prepared Google Doc, ensuring a consistent writing environment across all participants while allowing for a natural, unrestricted writing flow. For each figure, participants were provided with the following to work on the caption writing: (i) three AI-generated captions presented in randomized order, and (ii) the corresponding PDF file with the original caption redacted. Participants engaged in the writing task using a think-aloud protocol [14], in which the participants verbalize their thought processes and decision-making in real time as they perform the task. This enabled us to capture participants' cognitive processes directly, providing insight into their reasoning rather than just their final caption output.\nWhile we estimated that each caption would take approximately 10 minutes to complete, no strict time limit was imposed to ensure a comfortable and natural writing environment. After completing each caption, participants were asked to rank the three AI-generated captions based on their perceived usefulness. They were then asked to compare their rewritten caption to the original (revealed after writing) by responding to the statement, \u201cMy rewritten caption is better than the original caption,\u201d using a 5-point Likert scale (1 = Strongly Disagree, 5 = Strongly Agree)."}, {"title": "4 Analysis Methods", "content": "To understand participants' behaviors, we followed the practice of interaction analysis [21], a systematic method for examining video recording of human interactions. Our analysis involved manual video coding of participants' writing behaviors and their engagement with AI-generated captions. Using Dovetail video analysis software,5 we reviewed and coded the recordings by marking specific time segments where participants demonstrated distinct writing activities. Each video segment was analyzed for verbal think-aloud protocols and writing behaviors, with segments typically defined by natural transitions in participants' activities or explicit shifts in their focus. We describe our video coding scheme in the following subsection."}, {"title": "4.1 Code Scheme for AI Integration Activities", "content": "Table 1 shows the coding scheme COPY, ADAPT, DELETE, and CORRECT-used in our analysis, which was formed as follows. Following an inductive coding approach [47], our coding scheme for AI integration activities emerged"}, {"title": "4.2 Video Coding Procedure", "content": "The AI integration activities code is assigned by tracking each participant's sequence of actions in the draft area. Each subsequent action is then coded chronologically as the participant continues to develop their draft. The coding decisions are informed by both the participants' written output and their think-aloud protocol data. We systematically coded all sequential actions taken by participants during each writing task, creating a timeline of their writing process. This analysis provides a comprehensive understanding of how participants interact with and build upon AI-generated captions in scientific writing."}, {"title": "5 Results", "content": "In our study, we analyzed 16 statistical figures from 15 participants and 18 conceptual figures from 16 participants. Two figures from one participant were excluded from most analyses due to their complexity, as they combined both statistical and conceptual elements. We still completed the user study with this participant and discussed their results in a separate subsection to explore how AI-generated captions might support caption writing for complex figures."}, {"title": "5.1 AI-Generated Captions Integration Activities", "content": "Participants rely slightly more on AI-generated captions for statistical figures than for conceptual ones. Figure 4 shows the average frequency of AI integration activities across figure types. Participants engaged more frequently with AI generated captions when writing captions for statistical figures (mean=2.13, SD=1.32) compared to conceptual figures (mean=2.00, SD=1.65). Notably, while participants frequently used AI-generated captions, most instances involved some level of revision. Across the entire study, only one participant used an AI-generated caption unchanged as the final version for one figure. Furthermore, we also analyzed the type of integration behavior for each figure type (Figure 5). COPY was the predominant behavior for both types of figures, with marginally higher frequency in statistical figures. Additionally, DELETE and CORRECT occurred more frequently with conceptual figures than statistical figures, suggest-"}, {"title": "5.2 Model Preferences", "content": "Participants were asked to rank each AI-generated caption based on its usefulness in helping them compose the final caption. Figure 8 summarizes how often captions generated under each of the three conditions were ranked 1st (Best), 2nd, or 3rd (Worst) across all 34 writing sessions. Captions generated using both figure images and figure-mentioning paragraphs without a length constraint (UNLIMITED) were ranked first in 50% of cases. This suggests that participants may prefer AI-generated captions with more information, as these captions were longer and incorporated both textual and visual details. In contrast, captions generated with a 30-word length constraint (30-WORD), which also used images and text, were ranked first more often (29%) than captions generated using only text paragraphs (TEXT-ONLY) (21%). However, 30-WORD captions were also ranked worst (third) the most frequently (44%) among the three conditions. This indicates that while including both visual and textual information is important, imposing a strict length constraint may significantly reduce the usefulness of generated captions."}, {"title": "5.3 Perceived Caption Quality Improvement", "content": "At the end of each writing task, participants compared the quality of their rewritten captions to the original captions from their published papers (\u201cMy rewritten caption is better than the original caption\u201d). As a result, 68% of the captions were rated 4 or 5, with an overall mean rating of 3.82 (SD = 1.24). However, these results should be interpreted with caution, as the assessment inherently favored the rewritten captions. Participants had access to both their original work and AI-generated suggestions, enabling them to refine their captions further. Additionally, participants were likely more experienced at the time of the study than when they initially wrote the original captions, introducing another potential bias."}, {"title": "5.4 Special Case: Hybrid, Complex Figures", "content": "One participant's published paper included figures that were difficult to classify as either statistical or conceptual, as they contained multiple subfigures combining both statistical and conceptual elements. While we completed the user study and analysis for this participant, their results were excluded from the main findings discussed above. In this subsection, we examine these two unique cases.\nIn both writing tasks, the participant relied minimally on AI-generated captions, choosing to write most of the content independently. They noted that the AI-generated captions provided limited utility, as the participant noted \"One thing I realized is that it might be described as hallucination because for the first paper [...] the summary was totally different from what I discussed in the main text. So it might not be as accurate\" (P04). Addressing this challenge presents a critical avenue for future research in developing AI systems capable of handling multi-component academic figures more effectively."}, {"title": "6 Discussion", "content": "Our results indicate that writers generally preferred captions generated from multimodal inputs with unconstrained lengths, as the additional information provided greater flexibility to choose, add, delete, and modify content. This preference for rich information suggests that future AI writing assistants should prioritize interactive suggestions and embrace a human-in-the-loop approach, given that direct adoption of AI-generated content without revision was rare.\nLimitations. While our paper presents several interesting findings, we acknowledge a few limitations. First, our study was not interactive; participants could not iterate with or prompt AI models to revise their captions, which limits the generalizability of our findings. Second, participants were asked to rewrite captions for their own published papers. While this is closer to real-world caption-writing scenarios than most prior studies a reasonable trade-off in study design-it still differs from writing captions for new, unpublished work. Finally, our generation process relied on the full context of completed and polished papers, whereas in real-world scenarios, AI-generated captions may need to be based on incomplete or even empty drafts."}, {"title": "7 Conclusion", "content": "This study examined how paper writers use AI-generated captions in scientific caption writing, analyzing the writing processes of 18 participants. The results showed that paper writers favored longer, detail-rich captions that integrated textual and visual elements but found current AI models less effective for complex, conceptual figures. These insights enhance our understanding of human-AI collaboration in academic writing and lay the groundwork for developing AI tools tailored to the unique demands of different figure types. Future work should focus on specialized AI systems that adapt to researchers' diverse integration preferences, improving the efficiency and quality of scientific communication while maintaining the human element."}]}