{"title": "Motion-enhancement to Echocardiography Segmentation via Inserting a Temporal Attention Module: An Efficient, Adaptable, and Scalable Approach", "authors": ["Md. Kamrul Hasan", "Guang Yang", "Choon Hwai Yap"], "abstract": "Cardiac anatomy segmentation is essential for clinical assessment of cardiac function and disease diagnosis to inform treatment and intervention. In performing segmentation, deep learning (DL) algorithms improved accuracy significantly compared to traditional image processing approaches. More recently, studies showed that enhancing DL segmentation with motion information can further improve it. A range of methods for injecting motion information has been proposed, but many of them increase the dimensionality of input images (which is computationally expensive) or have not used an optimal method to insert motion information, such as non-DL registration, non-attention-based networks, or single-headed attention. Here, we present a novel, computation-efficient alternative where a novel, scalable temporal attention module (TAM) extracts temporal feature interactions multiple times and where TAM has a multi-headed, KQV projection cross-attention architecture. The module can be seamlessly integrated into a wide range of existing CNN- or Transformer-based networks, providing novel flexibility for inclusion in future implementations. Extensive evaluations on different cardiac datasets-2D echocardiography (CAMUS) and 3D echocardiography (MITEA)\u2014demonstrate the model's effectiveness when integrated into well-established backbone networks like UNet, FCN8s, UNetR, SwinUNetR, and the recent I2UNet. We further find that the optimized TAM-enhanced FCN8s network performs well compared to contemporary alternatives. Our results confirm TAM's robustness, scalability, and generalizability across diverse datasets and backbones.", "sections": [{"title": "1. Introduction", "content": "Heart diseases are the most common cause of mortality, and echocardiography remains the most important imaging modal-ity for evaluating heart diseases. Cardiac anatomy segmentation from echo images is an important tool for delineating cardiac chamber anatomy, which is important for quantifying anatomic parameters such as cardiac chamber shape, wall thicknesses, and chamber volumes, as well as quantifying cardiac function parameters, such as ejection fraction (EF), and stroke volume, ventricular and stroke volumes, atrial and ventricular remod-eling [1, 2]. These quantifications are important for evaluating cardiac health to detect pathologic remodeling and inform treat-ment plans [3]. Early approaches to such segmentation were grounded in traditional image processing techniques, such as region growing [4], template matching [5], and level set [6]. While these methods had some success in specific cases, they often required manual intervention and lacked robustness when applied to noisy or low-signal medical images [7]. For echocar-diography, frequent low-quality images prevented reliable im-plementations, and segmentation remains a manual or semi-automatic task, which is time-consuming and a source of im-precision. Although clinical guidelines suggest that measure-ments should be repeated over three cardiac cycles for accuracy [8], this is often not done due to the time demands on clinicians, leading to imprecision in the cardiac measurements.\nAdvances in DL have enabled consistent, automatic seg-mentation with improved performance compared to traditional approaches [11-14]. Many convolutional neural networks (CNN)-based methods have reached promising performance and made notable progress [15], where the fully conventional networks (FCN8s, FCN16s, or FCN32s) [16] and the UNet [17] are two of the most popular CNN-based segmentation meth-ods that have profound influences on subsequent works [7, 15]. Subsequently, Transformers were proposed for integration with CNNs to generate global feature extractions, overcome the lim-ited spatial influence range of CNNs, and improve transferabil-ity to downstream tasks [18]. The Trans-UNet [19] was the first to tap into this potential by combining CNNs for low-level feature extraction with Transformers for high-level representa-tions. Other successful models in cardiac image segmentation"}, {"title": "2. Related Works", "content": "2.1. Motion excluded segmentation models\nSince the introduction of the UNet design [17], several CNN-based approaches [31-34] have extended the standard UNet ar-chitecture for various medical image segmentation tasks. For example, the authors in [31] introduce a generalized segmen-tation framework named nnUNet that automatically configures the architecture to extract features at multiple scales. Further, several efforts in the literature have been made to encode holis-tic contextual information within CNN-based frameworks us-ing, e.g., image pyramids [35], large kernels [36], dilated con-volution [37], and deformable convolution [38].\nMore recent segmentation networks, however, have mostly incorporated transformers to enhance effectiveness. Many in-tend to combine the transformer's ability to capture global fea-tures with CNN's ability to capture local features. For example, Zhou et al. [39] introduced nnFormer, a 3D transformer for vol-umetric medical image segmentation, which has an interesting interleaved architecture of alternative convolution blocks and transformer blocks, which, when combined with skipped atten-tion rather than skipped connections, achieves both local and global volume-based self-attention. Zhao et al. [40] proposed a semi-supervised echocardiography segmentation method using a boundary attention transformer (BATNet) to capture spatial information and a multi-scale, semi-supervised model (semi-BATNet) to enforce boundary feature consistency. Zhang et al. [41] proposed BSANet for cardiac MRI segmentation, using a multi-scale boundary-aware module that improves edge ex-traction and a scale-aggregation transformer module to retain features of various scales for enhanced performance. Shaker et al. [42] proposed UNETR++, a 3D medical image seg-mentation approach that balances segmentation quality and computational efficiency. The key innovation is the efficient paired attention (EPA) block, which uses two interdependent branches based on spatial and channel attention to improve the learning of discriminative features. Qin et al. [43] intro-duced an uncertainty-based region clipping algorithm for semi-supervised medical image segmentation. A module computes the uncertainty of two sub-networks predictions using Monte Carlo Dropout, while another module generates new samples by masking low-confidence pixels based on uncertainty. Li et al. [44] introduced TPAFNet, a multi-scale 3D medical image seg-mentation network combining CNN and transformer. This uses a TPAF module that extracts channel weights through convolu-tion for integration with the Transformer, an atrous convolution to send multi-scale information to the Transformer, and a voxel-wise classifier for deep supervision.\nAlthough these works offered effective and innovative ad-vancements in segmentation, a general limitation is their need for more consideration of temporal information. They focus solely on spatial features, treating each image frame indepen-dently without accounting for temporal relationships between frames. Consequently, these models may struggle with con-sistency over time and experience artifacts or discontinuity be-tween successive frames [9, 45]. There is a missed opportu-nity for using neighboring time frames to reduce noise and en-hance signals in the current frame [27]. These are important for echocardiography because echo images have significant noise and signal losses and because the heart undergoes substantial cyclic motion, and accurate assessment of such motion is im-portant for clinical evaluation."}, {"title": "2.2. Motion-enhanced segmentation models", "content": "Several previous studies have used temporal motion informa-tion to enhance segmentation networks and reported the bene-fits of this approach.\nThe most straightforward approach is proposed by Myro-nenko et al. [10], who utilized a 4D CNN for segmenting ECG-gated cardiac CT. The network has a loss function that supports training with sparse labeling, with labels at only a few time frames. They showed that this produced superior smoothness but comparable DICE as a 3D segmentation network. Xue et al. [25] proposed an echocardiography segmentation method that effectively utilizes motion information from optical flow image registration motion fields, which improved segmentation accu-racy. A shared feature extractor was used for the segmentation and optical flow sub-tasks to enable efficient information ex-change. At the same time, an orientation congruency constraint promotes consistency between optical flow fields of successive frames. Wei et al. [9] proposed the CLAS network, which con-currently predicts segmentation masks and bi-directional (for-ward and backward in time) motion fields. A common en-coder but separate decoders were used for the two predictions, and loss functions were designed to enforce the consistency of the two outputs with each other and the images. The authors proposed a multi-task version, MCLAS Wei et al. [28], that performed echo view classification and direct ejection fraction prediction using the CLAS backbone. Maani et al. [45] intro-duced SimLVSeg, a novel network for left ventricle segmenta-tion from echocardiogram videos with sparse annotations to ad-dress the difficulty of obtaining annotation ground truths. First, a self-supervised network that recovers intentionally masked temporal frames is pre-trained. The encoder of this network is then used to train the LV segmentation network using sparse annotations for weakly supervised training. In the process, the understanding of temporal sequences of frames is passed on to the segmentation network. Li et al. [26] proposed a recurrent aggregation learning method for multi-view echocardiographic sequence segmentation. It uses pyramid ConvBlocks for effi-cient feature extraction and hierarchical ConvLSTMs to capture temporal evolution. A double-branch aggregation mechanism is introduced, where segmentation and classification branches enhance each other.\nDespite successfully using temporal information to enhance segmentation, they use complex and resource-intensive ap-proaches to inject temporal information into their networks. The time dimension increases the image dimension, requir-ing the input of the entire or most of the image sequence, which substantially increases memory requirements and com-putational burden. Some of them involve additional classifica-tion or registration subtasks, resulting in added complexity and a larger number of parameters to be trained, which increases the risk of overfitting. The use of optical flow, a traditional image registration approach, may further be a limitation, as deep learning approaches have better performance. Similarly, network-based registration may improve by using transformers. To address these limitations, we seek to develop an alternative approach that is structurally simple and computationally inex-pensive and does not require inputs of an excessive number of time frames for effective extraction of motion information.\nRecent work has also proposed using attention mechanisms to extract temporal information for segmentation. For exam-ple, Ahn et al. [29] proposed a multi-frame attention network to improve left ventricle segmentation in 3D echocardiography. The approach incorporates a multi-frame co-attention mecha-nism that utilizes correlated spatiotemporal features from im-"}, {"title": "3. Methodology and Materials", "content": "3.1. Proposed motion-enhanced segmentation\nWe propose to extract and utilize temporal features in seg-mentation networks via a plug-and-play temporal attention module (TAM) that can be seamlessly incorporated into a range of backbone segmentation algorithms, including UNet, FCN8s, UNetR, SwinUNetR, and I2UNet [30]. Adding our novel TAM, as shown in Fig. 2, enables the network to explicitly model these temporal relationships, allowing it to segment dy-namically changing regions accurately. TAM identifies which frames and spatial regions are most relevant for segmenting the target frame, ensuring the network focuses on meaningful tem-poral variations. TAM uses its temporal awareness to fuse fea-tures across time by aggregating contributions from temporal neighbors and emphasizing regions with temporal coherence. This step ensures robust feature refinement for dynamic re-gions, critical for segmenting motion-sensitive areas. The gat-ing mechanism in TAM suppresses irrelevant or noisy temporal information, improving the segmentation quality in loud envi-ronments (e.g., due to motion artifacts or low contrast in cardiac imaging).\nIn the next sections, we first explain the CNN, Transformer, or CNN+Transformer hybrid backbone segmentation networks to which we tested the addition of TAM and then the TAM mod-ule.\n3.1.1. Backbone segmentation models\nFig. 2 presents our proposed motion-aware segmentation framework. Our proposed TAM module is explained in Fig. 2 (C) and explained in more detail below. It serves as a plug-and-play solution for motion-aware enhancement to address the limitations of networks without motion-awareness. UNetR and UNet are foundational networks that lacked motion awareness, and Fig. 2 (A) and (B) demonstrate how TAM can be inserted into these networks for enhancement, forming the TAM-UNetR and TAM-UNet. The idea is for TAM to guide a selective fo-cus on motion-coherent features across time frames while sup-pressing irrelevant or noisy information to improve segmenta-tion performance, especially in noisy images with poor signals"}, {"title": "3.1.2. Temporal attention module (TAM)", "content": "As shown in Fig. 2C, TAM is integrated between two layers of the model. TAM receives T number of temporal feature maps {F\u2081, F\u2082,...,F\u209c} \u2208 \u211d^(C\u00d7H\u00d7W\u00d7D) (t\u2200T) from the previous layer and facilitates temporal feature interaction (in between F\u209c=\u1d62 and F\u209c=\u2c7c, where i \u2260 j) in the embedded space to refine the in-put features ({Frefined, Frefined,..., Frefined} \u2208 \u211d^(C\u00d7H\u00d7W\u00d7D)) for the subsequent layer. This highlights cardiac regions and frames that contribute most to the refined representation, providing in-sights into the motion refinement process. Algorithm 1 explains step-by-step procedures for such motion-enhanced feature re-finements, and the following paragraphs explain the details of operations and rationale.\nLinear Transformations:. In the first step, linear transforma-tions are applied to each feature map, F\u209c (t\u2200T), using trainable weight matrices (Wq, Wk, W\u1d65) and biases (bq, bk, b\u1d65), to form the Query (Q), Key (K\u209c), and Value (V\u209c) components of the Transformer.\nQ\u209c = F\u209cWq + bq,\nK\u209c = F\u209cWk + bk,\nV\u209c = F\u209cW\u1d65 + b\u1d65,\nwhere K and V encode information in the current frame, and Q represents the information being sought for from other frames. Then, the embeddings are divided into Hheads subspaces to al-low the model to capture diverse patterns of temporal relevance across different spatial regions, for example, subtle versus sig-nificant motions and simple versus complex motions. This also helps to capture richer spatial-temporal relationships across dif-ferent subspaces.\nMulti-head Cross-time Attention: The cross-time attention matrix (A) is computed from K, V, and Q to capture relation-"}, {"title": "4. Experimental Results", "content": "4.1. TAM design and ablation\nTo optimize the multi-headed TAM design, we systemati-cally fine-tuned key hyperparameters, including the number of attention heads, the number of time frames to input, and the locations within the network for adding TAM. A key concern is the need to strike an optimal balance between performance and computational efficiency. A comprehensive ablation study was performed by evaluating various configurations for adding TAM to the standard UNet architecture. Table 2 provides a detailed comparison of various configurations (C) outlined in Fig. 2, alongside baseline models: a frame-by-frame UNet2D and an extended UNet3D with a temporal dimension (2D+t).\nTable 2 demonstrates that integrating motion information into UNet2D via our novel TAM leads to a significant improve-ment (p < 0.05) across all metrics. In contrast, while UNet3D incorporates motion by adding a temporal dimension to the con-volutional layers, it does not outperform the motion-agnostic UNet2D. It significantly worsens the performance in terms of mean HD and PIA and their interquartile ranges (see Fig. 4). Furthermore, employing 3D convolutions (Conv3D) instead of 2D convolutions (Conv2D) markedly increases both FLOPs and trainable parameters, as shown in Table 2. A plausible expla-nation for this observation is that the evaluation was conducted using only two frames for all methods. Although incorporating additional temporal frames might enhance the performance of conventional motion-based UNet3D [9], it would come at the cost of significantly higher computational complexity. Notably, our TAM-UNet2D achieves superior performance (p < 0.05) using just two frames (ED and ES) while maintaining compu-tational efficiency, making it a more viable solution for motion-enhanced tasks.\nComparing all TAM configurations (C3-C11), where TAM is integrated into the encoder (8) and/or decoder (D), it is evident that the frequency of TAM usage within the network influences FLOPs and trainable parameters. However, the performance is consistent across the various configurations, suggesting a mini-mal number of TAM modules is needed. Based on Table 2 and the distribution plots of HD and PIA in Fig. 4, configurations C3 and C4 emerge as the optimal TAM-UNet2D designs, achieving optimal performance with minimal computational complexity."}, {"title": "4.2. TAM integration to CNN and Transformer models", "content": "In this section, we investigate the effects of integrating our TAM module into various backbone segmentation networks to enhance them with motion awareness. We study both CNN-based models, such as UNet and FCN8s, and Transformer-based models, such as UNetR and SwinUNetR. The networks are tested on the CAMUS 2D echocardiography dataset, and re-sults are detailed in Table 3. Hyperparameters for each network and TAM are optimized before the investigation.\nResults in Table 3 show that the incorporation of TAM con-sistently improves segmentation performance for all evaluated models, and improvements are statistically significant (p \u00ab 0.05) for most of the cases. Notably, integrating TAM into seg-mentation models leads to a substantial reduction in HD, indi-cating improved precision in delineating LV and MYO bound-aries. HD improvements are between 23.9% and 48.1%. This improvement is further evident in the qualitative results shown in Fig. 9, where TAM provides more coherent segmentations across the time frames. In contrast, models without TAM pro-duce inconsistent results across time frames. This can lead to poor estimations of clinical parameters such as EF and stroke volumes [45, 48]. Additionally, TAM effectively reduces PIA, leading to cleaner segmentations with fewer structural incon-sistencies (islands or broken-up segmentation mass). This will likely support more straightforward and less erroneous down-stream quantification of clinical parameters. There are further improvements to Dice scores with TAM incorporation, but this is more subtle.\nTo evaluate the integration of TAM under varying image quality conditions, we tested the FCN8 network with and with-out TAM on the CAMUS dataset, which labels images in terms of their quality, categorizing them as good (35%), medium (46%), and poor (19%) quality [11]. The testing set was care-fully selected to ensure a balanced representation across these categories for a comprehensive assessment. Results in Fig. 7 show that FCN8s-3D performs slightly better than FCN8s-2D, likely due to the inclusion of temporal sequences and, thus, motion information. Further, these networks produced slightly poorer HD results with poorer-quality images. In comparison, our motion-aware TAM-FCN8s-2D outperforms both models across all image quality categories in HD results, and it is less affected by the image quality, having similar HD results across image types. This suggests that TAM provides an ability to overcome low image quality via motion awareness. Given that low-quality images are very often encountered clinically, this characteristic is very advantageous."}, {"title": "4.3. TAM's scalability and cross-dataset genericity", "content": "On top of adding TAM to networks performing 2D echocar-diography segmentation (Table 3), we further tested the addi-tion of TAM to networks performing 3D segmentation, results of which are shown in Table 4. Here, networks are tested on the MITEA 3D echocardiography dataset. Results in the ta-ble again show that TAM consistently delivers significant im-provements (p < 0.05) across various backbone architectures. Our results thus show that TAM is scalable from 2D to 3D seg-mentation and can successfully enhance motion awareness even with increased image dimensionality.\nIn both 2D and 3D experiments, TAM enhances segmenta-tion precision across critical metrics such as DSC, HD, and PIA. For instance, in 2D segmentation (Table 3), TAM in-creases DSC by 1%-2.4%, reduces HD by 14.5%-29%, and decreases PIA by 46.7%-96.6%. Similarly, in 3D segmenta-tion (Table 4), TAM improves DSC by 0.6%-1.7%, reduces HD by 12.6%-28.2%, and decreases PIA by 46.7%-91.3%. These consistent gains highlight TAM's scalability to higher-dimensional data. Despite higher computational burden and complexity, TAM offers similar improvements in 3D images compared to 2D images. Compared to 2D images, 3D images face additional anatomical complexity, a sparser percentage re-gion of interest, and are more challenging to segment. Our re-sults in tables 3 and 4 corroborate this notion. Having TAM temporal awareness to enhance performance is thus useful.\nTAM's cross-backbone performance further highlights its versatility. In 2D tasks, it achieves its highest DSC gains for FCN8s (from 0.899 to 0.921) and SwinUNetR (from 0.888 to 0.913). In 3D tasks, TAM achieves substantial HD reduc-tions, particularly for CNN-based models like FCN8s (from 11.66 mm to 8.37 mm) and SwinUNetR (from 10.43 mm to 9.12 mm). While transformer-based models, such as Swin-UNetR, start with superior baseline DSC values, TAM provides more pronounced relative improvements for CNN-based mod-els, particularly in reducing HD and PIA. These results high-light TAM's adaptability to diverse architectural designs, mak-ing it a valuable addition to CNN and transformer backbones.\nBeyond these quantitative gains, TAM demonstrates remark-able robustness to dataset-specific challenges. For example, in CAMUS-2D (Table 3), TAM reduces HD for UNet from 5.11"}, {"title": "4.4. Comparison to SOTA", "content": "In Table 5, we compare our optimal TAM configuration, the TAM-FCN8s, to several prominent and high-performing 2D echocardiography segmentation models to demonstrate that TAM can enable SOTA advancement. We compared segmen-tation of the left ventricular myocardium, endocardial and epi-cardial boundary, and the left atrial endocardial boundaries in terms of DSC, HD, and MASD. Results show that TAM-FCN8S has the best overall performance, with the best metrics for the left ventricular myocardium, epicardium, and left atrium.\nIn our analysis, we included the I2UNet, which encourages historical information reuse and re-exploration through rich in-formation interaction among the dual paths. This allows deep layers to learn more comprehensive features that contain both low-level detail description and high-level semantic abstraction and is arguably a SOTA. This recent network has been tested with the addition of our novel TAM. This I2UNet produced very good DSC, HD, and MASD. We observe that TAM did not sig-nificantly improve its DSC score, but it significantly (p < 0.05) improved HD at all anatomical regions. This further proves TAM's ability to improve modern, cutting-edge models and its utility as a plug-and-play motion awareness enhancer.\nCompared to high-performance motion-aware methods like SOCOF and CLAS, TAM-FCN8s produced similar DSC but improved HD and MASD substantially. SOCOF relies on opti-cal flow to extract motion dynamics [25], while CLAS performs concurrent DL registration and segmentation [9]. TAM has a much lighter architecture than these networks, as it requires only three frames as inputs instead of the entire cardiac cycle, and it has a comparatively simple network with a low number of trainable parameters. Despite this, it can provide sufficient motion awareness to the simple FCN network to match and im-prove complex motion-enhanced methods. This demonstrates TAM's robust performance."}, {"title": "5. Discussions and Conclusion", "content": "Past studies [9, 25, 28, 29, 46] have demonstrated that pro-viding an awareness of temporal motions can improve echocar-diography segmentation, likely because it can utilize neigh-boring time frames to mitigate difficulties posed by localized, transient noise and signal losses and because coherent features across different time frames can aid in more accurate delin-eation of anatomic boundaries. We proposed a novel approach to enabling motion awareness in segmentation networks via the TAM module, which can be inserted into various networks for enhancement.\nTAM's uniqueness is in its plug-and-play approach. Having an adaptable design, it can be easily inserted at various loca-tions of both CNN- and transformer-based networks, and hav-ing a small and simple architecture, it does not impose substan-tial additional computational burden and risk of overfitting on the network and does not impede the original network's train-ing. As an adaptable module rather than a standalone frame-work, TAM will likely be easily incorporated in future segmen-tation implementations, enabling easy and convenient inclusion of motion awareness. A second innovation is that TAM uses the multi-head KQV projection as the attention mechanism instead of computing a co-attention map [29, 46]. Our experiments show that this enables performance improvements.\nTAM further has other advantages. First, it does not require a substantial increase in input data size, as it only needs about three images for motion enhancement. This avoids excessive computational burden and allows flexibility in deciding which time points to focus on. It also allows effective network training when only a few sparse frames of image labels are available. Second, our test results show that it provides improvements to segmentation performance.\nOur results, summarized in Tables 3, 4, and 5, demonstrate TAM's consistent enhancement across different model archi-tectures. While it has only a mild ability to improve DICE, TAM significantly reduces HD and MASD, suggesting that it delineates boundaries with greater precision. TAM also signifi-cantly reduces PIA and enables much better mitigation to the frequently encountered problems of segmentation island and broken-up segmentation masses, offering cleaner and more co-herent segmentations across time frames. These advantages may offer accuracy and precision in clinical parameter quan-tifications after segmentation."}, {"title": "Declaration of Competing Interest", "content": "The authors declare that they have no known competing fi-nancial interests or personal relationships that could have ap-peared to influence the work reported in this paper."}], "equation": [{"number": 1, "formula": "L(S_T, S_p) = 1 - \\frac{2 \\cdot |S_T \\cap S_p|}{|S_T| + |S_p|} + \\sum_{i \\in C} S_T(x) \\cdot \\log S_p(x)"}, {"number": 2, "formula": "MASD (S_T, S_p) = \\frac{1}{|S_T| + |S_p|} \\cdot ( \\sum_{a \\in S_T} \\min_{b \\in S_p} d(a,b) + \\sum_{b \\in S_p} \\min_{a \\in S_T} d(b,a))"}, {"number": 3, "formula": "PIA = \\frac{1}{C} \\sum_{c=1}^{C} \\sum_{r=1}^{R_c} \\frac{|A_{cr} - A_{c, largest}|}{A_{c, largest}} \\cdot 100"}], "acknowledgement": "M. K. Hasan was supported by the EPSRC-DTP studentship funds (2022-2026) from the Bioengineering department of Imperial College London, UK.\nG. Yang was supported in part by the ERC IMI (101005122), the H2020 (952172), the MRC (MC/PC/21013), the Royal Society (IEC/NSFC/211235), the NVIDIA Academic Hardware Grant Program, the SABER project supported by Boehringer Ingelheim Ltd, NIHR Imperial Biomedical Research Centre (RDA01), Wellcome Leap Dynamic Resilience, UKRI guaran-tee funding for Horizon Europe MSCA Postdoctoral Fellow-ships (EP/Z002206/1), and the UKRI Future Leaders Fellow-ship (MR/V023799/1)."}