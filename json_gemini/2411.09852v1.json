{"title": "InterFormer: Towards Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction", "authors": ["Zhichen Zeng", "Xiaolong Liu", "Mengyue Hang", "Xiaoyi Liu", "Qinghai Zhou", "Chaofei Yang", "Yiqun Liu", "Yichen Ruan", "Laming Chen", "Yuxin Chen", "Yujia Hao", "Jiaqi Xu", "Jade Nie", "Xi Liu", "Buyun Zhang", "Wei Wen", "Siyang Yuan", "Kai Wang", "Wen-Yen Chen", "Yiping Han", "Huayu Li", "Chunzhi Yang", "Bo Long", "Philip S. Yu", "Hanghang Tong", "Jiyan Yang"], "abstract": "Click-through rate (CTR) prediction, which predicts the probability of a user clicking an ad, is a fundamental task in recommender systems. The emergence of heterogeneous information, such as user profile and behavior sequences, depicts user interests from different aspects. A mutually beneficial integration of heterogeneous information is the cornerstone towards the success of CTR prediction. However, most of the existing methods suffer from two fundamental limitations, including (1) insufficient inter-mode interaction due to the unidirectional information flow between modes, and (2) aggressive information aggregation caused by early summarization, resulting in excessive information loss. To address the above limitations, we propose a novel module named INTERFORMER to learn heterogeneous information interaction in an interleaving style. To achieve better interaction learning, INTERFORMER enables bidirectional information flow for mutually beneficial learning across different modes. To avoid aggressive information aggregation, we retain complete information in each data mode and use a separate bridging arch for effective information selection and summarization. Our proposed INTERFORMER achieves state-of-the-art performance on three public datasets and a large-scale industrial dataset.", "sections": [{"title": "1 Introduction", "content": "Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking an ad or item, is the fundamental task for various applications such as online advertising and recommender systems [17, 22, 40, 41]. The quality of CTR prediction significantly influences the company revenue and user experience, drawing extensive attention from both academia and industry [17, 46, 47]. For example, in online ad bidding, accurate CTR prediction helps advertisers optimize their bids and target the most receptive audiences. In content recommendation, accurate CTR prediction enables platforms to suggest more relevant content to users.\nTo achieve better CTR prediction, it is crucial to capture the user interests in the evolving environment [17, 31, 47]. The abundance of heterogeneous information presents both opportunities and challenges. On the one hand, heterogeneous information depicts user interests from different aspects, providing diverse context [43]. For instance, global information, e.g., user profile and context features, offers a static view on general user interests, while user behavior sequences provide substantial information for modeling dynamic user interests [31]. On the other hand, the heterogeneous nature of the data requires different modeling approaches and careful integration across different information modes [43]. For example, while modeling interactions among global information is critical to personalized recommendation [16, 20, 30], capturing sequential dependencies is the major focus for user behavior modeling [4, 24].\nMost of the existing CTR prediction models fall into two categories, including non-sequential models and sequential models. Non-sequential models focus on learning informative embeddings through feature interaction via inner-product [16, 25], MLP [29, 30] and deep structured semantic model [7, 12], but ignore the sequential information in user behaviors. Sequential models, in the contrast, employ additional modules, e.g., CNN [26], RNN [24, 47] and Attention modules [17, 39, 46], to capture the sequential dependencies in user behaviors. Promising as it might be, existing sequential methods mostly employ a unidirectional information flow, where global information is used to guide sequence learning, while the reverse information flow from sequence to global information is largely ignored, hence suffering from insufficient inter-mode interaction. For example, global information often captures long-term interests, while sequence information reveals momentary interests, such as a sudden focus on a specific category of products, which can enhance the global context with immediate preference. Besides, due to the computational challenges of performing interaction learning among numerous global features and lengthy sequences, aggressive feature aggregation, e.g., sequence summation [47], pooling [35], and concatenation [46], is often performed at early stages, inevitably leading to excessive information loss.\nIn light of the above limitations, we propose a novel heterogeneous interaction learning module named INTERFORMER, whose ideas are two-fold. To avoid insufficient inter-mode interaction, we enable bidirectional information flows between different modes, such that global and sequence learning are performed in an interleaving style. Specifically, to learn context-aware sequence embeddings, global sumamrization guides sequence learning via Personalized FFN (pFFN) and Multihead Attention (MHA) [28]. To learn behavior-aware global embeddings, sequence summarization instructs global learning via an interaction module. To mitigate aggressive information aggregation, we adopt MHA for effective information selection, based on which the one-to-one mappings between input and output tokens are retained till the final interaction. Note that our framework is compatible with various interaction learning models like DCNv2 [30], DHEN [41], etc.\nThe main contributions of this paper are summarized as follows:\n\u2022 Challenges. We identify two key bottlenecks of heterogeneous interaction learning, namely insufficient inter-mode interaction and aggressive information aggregation.\n\u2022 Model Design. We propose a novel heterogeneous interaction learning framework named INTERFORMER for effective feature interaction and selective information aggregation. To our best knowledge, the proposed INTERFORMER is the first model to address the mutual benefits in heterogeneous interaction learning.\n\u2022 Experiments and Analysis. We carry out extensive experiments on the proposed INTERFORMER with up to 0.14% AUC improvement on benchmark datasets and 0.15% Normalized Entropy (NE) gain on internal large-scale dataset. Besides, INTERFORMER exhibits promising scaling results in both feature scaling and model scaling.\nThe rest of the paper is organized as follows. Section 2 briefly reviews the recent works on interaction learning. Section 3 summarizes the preliminaries, and section 4 introduces our proposed INTERFORMER. Extensive experiments and analyses are carried out in Section 5. We conclude our paper in Section 6."}, {"title": "2 Related Works", "content": "In this section, we briefly review the related works on feature interaction, including non-sequential and sequential methods."}, {"title": "2.1 Non-Sequential Methods", "content": "The vast majority of non-sequential models is built upon the idea of Factorization Machine (FM) [16, 20, 25], which models the user-item interaction by linearly combining their low-dimensional embeddings [42]. [20] is the very first FM model to capture pairwise interactions. To model high-order interactions, different methods combine FMs with deep neural networks, where FMs learn low-order interaction via pairwise operation and neural networks learn high-order interactions via deep architectures. For example, MLP [16, 25, 30, 37, 45] captures high-order interactions via dense connections between features, and Attention mechanism [22, 34, 36] learns more complex embeddings through linear combination. These deep learning-based approaches enable end-to-end training without hand-craft feature designs, and are capable of handling heterogeneous information like text, image and video [42]. Besides, recent works address the scaling law in recommendation, where DHEN [41] ensembles multiple interaction modules and Wukong [40] stacks FMs to learn a hierarchy of interactions. Promising as they are, non-sequential models fail to capture the sequential dependencies in user behaviors, resulting in suboptimal solutions."}, {"title": "2.2 Sequential Methods", "content": "With the recent emergence of sequence information, e.g., user interaction history, in recommender systems, extensive efforts have been made to capture the evolving user interest. A key challenge behind sequential methods is to combine the sequential information with the heterogeneous global information in a mutually beneficial manner. Markov models [9, 21, 38] consider sequential data as a stochastic process over discrete random variables, but the oversimplified Markovian property limits the model capability in capturing long-term sequential dependencies [18]. To model the long-term dependencies, RNN and Attention mechanism are employed as the backbone module for many sequential methods. For example, to extract the evolving user interest, various attention-based networks are designed [17, 46], and Transformer architecture [6, 28] is adopted for sequential modeling [4, 24]. Besides, to model multifaceted user interests, [8, 35] propose to capture multiple user interests from multi-behavior sequences. More recently, TransAct [33] adopt a hybrid ranking model to combine real-time user actions for immediate preference and batch user representations for long-term interests. LiRank [2] improves CTR prediction at LinkedIn by ensembling multiple interaction modules, accelerated by quantization and vocabulary compression. CARL [5] achieves fast inference on large-scale recommendation in Kuaishou by utilizing cached results, monitored by a reinforcement learning-based framework. While most of the existing sequential methods leverage global information for personalized sequence modeling, sequence information is rarely explored for better global learning. We believe such unidirectional design limits the expressiveness of the learned embeddings, and a bidirectional information flow between different data modes is the key towards better heterogeneous interaction learning."}, {"title": "3 Preliminaries", "content": "Table 1 summarizes the main symbols and notations used throughout the paper. We use bold uppercase letters for matrices (e.g., X), bold lowercase letters for vectors (e.g., x), and lowercase letters for scalars (e.g., n). The element at the i-th row and j-th column of a matrix X is denoted as X(i, j). The transpose of X is denoted by the superscript \u0442 (e.g., X\u1d40). We use superscript u to denote users, and subscripts i and t to denote item and timestamp, respectively (e.g., y\u1d62\u1d57). We use x\u02b2\u02e1 to denote the j-th global feature of the l-th layer, and s\u1d57\u02e1 to denote the sequence feature of the l-th layer at timestamp t. We consider the scenario with m dense features, n sparse features, and k sequence features of length T. We use d to denote the embedding dimension of the CTR model."}, {"title": "3.1 Click-Through Rate (CTR) Prediction", "content": "CTR prediction aims to estimate the probability of a user clicking on an item given heterogeneous information such as global context and behavior sequences. Formally, given a user set U and an item set I, the interaction sequence of a user u \u2208 U is defined as S\u1d58 = [i\u2081, i\u2082,..., i\u209c], where i\u209c \u2208 I is the item interacted at time step t. CTR prediction aims to estimate the probability of a user clicking on a new item i\u1d40\u208a\u2081, denoted as y\u1d62\u1d40\u208a\u2081, given the historical interaction sequence S\u1d58. Formally, we seek to learn a function f : U\u00d7I\u00d7S \u2192 [0, 1], where S is the set of all possible sequences, such that:\nP(y\u1d62\u1d40\u208a\u2081 = 1|u, i\u1d40\u208a\u2081, S\u1d58; \u03b8) = f (u, i\u1d40\u208a\u2081, S\u1d58; \u03b8).\nTo optimize the model, the cross-entropy loss, which measures the difference between predicted and groundtruth click probabilities, is commonly employed as the objective function. Since the formulation in Eq. (1) incorporates temporal dynamics in user behaviors, it potentially leads to better CTR predictions than non-sequential methods that ignore the sequential nature of user interactions."}, {"title": "3.2 Feature Interaction", "content": "Interaction learning, which aims to capture the complex relationships between different features, is the key towards the success of CTR prediction. We briefly introduce three prominent feature interaction modules: inner product, DCNv2 [30] and DHEN [41].\nInner Product-based Interaction. Given input feature vector x \u2208 \u211d\u1d48, the inner product-based interaction, exemplified by Factorization Machines (FM) [20], learns a latent vector v\u2c7c \u2208 \u211d\u1d48 for each feature j, whose inner product \u27e8v\u2c7c, v\u2096\u27e9 describes the interaction strength between x(j) and x(k). Hence, the second-order feature interactions can be modeled as:\nfFM(X) = \u03a3\u2c7c\u208c\u2081\u1d48 \u03a3\u2096\u208c\u2c7c\u208a\u2081\u1d48 \u27e8v\u2c7c, v\u2096\u27e9x(j)x(k) + \u03a3\u2c7c\u208c\u2081\u1d48 w\u2c7cx(j) + w\u2080\nDeep & Cross Network (DCNv2) [30]. The DCNv2 model combines a cross network for explicit feature interactions with a deep neural network for implicit feature interactions. Given input feature vector x\u2070 \u2208 \u211d\u1d48, the 1-th layer of the cross network models the second order interaction as follows:\nx\u207d\u02e1\u207a\u00b9\u207e = x\u2070 \u2299 (W\u207d\u02e1\u207ex\u207d\u02e1\u207e +b\u207d\u02e1\u207e) + x\u207d\u02e1\u207e.\nBy stacking multiple cross layers, DCNv2 is capable of modeling explicit high-order interactions. Besides, a deep network, which is essentially an MLP, is employed to model implicit interactions between features, and the final prediction is further obtained by combining the outputs of both networks.\nDeep Hierarchical Ensemble Network (DHEN) [41]. DHEN leverages the strengths of heterogeneous interaction modules and learns a hierarchy of interactions of different orders. The key idea behind DHEN is to use a layered structure where each layer is an ensemble of multiple heterogeneous interaction modules.\nFormally, given the concatenation of m input features X\u207d\u02e1\u207e \u2208 \u211d\u1d48\u02e3\u1d50, the output of the 1-th DHEN layer is computed as:\nX\u207d\u02e1\u207a\u00b9\u207e = Norm (Ensemble_l Interaction; (X\u207d\u02e1\u207e) + ShortCut(X\u207d\u02e1\u207e)),\nwhere Norm() is a normalization function, Interaction\u1d62 are different interaction modules that are further ensembled by Ensemble_l(), e.g., summation and concatenation, and ShortCut() is an MLP serving as the residual connection for deep layer stacking.\nBy ensembling different modules (e.g., inner product, DCNv2, etc.), DHEN leverages their complementary strengths within a unified framework. Besides, by stacking multiple DHEN layers, high-order feature interactions can be captured."}, {"title": "3.3 Attention Mechanism", "content": "Attention mechanisms [1, 28] have become an integral part of sequence modeling, serving as the core component behind various model designs, such as Transformer [28] and BERT [6]. In this section, we briefly review attention mechanisms, including Multi-Head Attention and Pooling by Multihead Attention (PMA).\nMulti-Head Attention (MHA) [28] allows sequence modeling without regard to distance between tokens [28]. Given an input sequence S = [s\u2081, . . ., s\u1d40], where s\u209c \u2208 \u211d\u1d48 is the embedding vector at timestamp t, the self-attention operation is defined as:\nAttn(Q, K, V) = softmax (QK\u1d40/\u221ad\u2096) V,\nwhere Q, K, V \u2208 \u211d\u207f\u02e3\u1d48\u2096 are query, key and value representations derived from the input sequence S via linear projections Q = S\u1d40W\u1d60, K = S\u1d40W\u1d37, V = S\u1d40W\u2c7d.\nTo enable joint attention to information from different embedding subspaces, MHA aggregates h parallel attention as follows:\nMHA(Q, K, V) = [head\u2081 || . . . ||head\u2095] W\u1d3c,\nwhere heads are computed by self-attention head\u1d62 = Attn(QW\u1d60, KW\u1d37, VW\u2c7d) and aggregated via an output projector W\u1d3c \u2208 \u211d\u02b0\u1d48\u2096\u02e3\u1d48\nPooling by Multi-Head Attention (PMA) [14]. Rather than obtaining query, key and values from the same input sequence S, PMA utilizes a learnable query Q\u1d3e\u1d39\u1d2c \u2208 \u211d\u1d4f\u02e3\u1d48\u2096 to summarize the sequence from k different aspects, which can be defined as follows:\nPMA(Q\u1d3e\u1d39\u1d2c, S) = MHA (Q\u1d3e\u1d39\u1d2c, K, V).\nIntuitively, each column in Q\u1d3e\u1d39\u1d2c is a seed vector summarizing S as a d-dimensional vector, and the output of PMA is the concatenation of k summarization depicting sequence from different aspects."}, {"title": "4 Methodology", "content": "In this section, we present our proposed INTERFORMER. We first introduce the preprocessing module in Section 4.1, followed by three major modules, including Global Arch, Sequence Arch and Bridging Arch. To learn behavior-aware global embeddings, the Global Arch is proposed to model feature interactions (Section 4.2). To learn context-aware sequence embeddings, the Sequence Arch is proposed to optimize sequence (Section 4.3). The Bridging Arch connects Global and Sequence Archs, enabling effective information summarization and exchange between different modes (Section 4.4). An overview of the INTERFORMER architecture is shown in Figure 1."}, {"title": "4.1 Feature Preprocessing", "content": "Global Feature Preproessing. We consider two types of global features, including dense features like user age and item price, and sparse features like user id and item category. To unify the heterogeneity in global features, the global preprocessing module transforms features into embeddings of the same dimensionality.\nSpecifically, raw dense features xdense\u2070 are first concatenated to form a dense vector xdense\u2070 = [xdense\u2070 || xdense\u2070 ||...|| xdense\u2070], which is further transformed into a d-dimensional dense embedding vector via linear transformation, i.e., xdense\u00b9 = Wdense xdense\u2070. Similarly, each raw sparse feature is first encoded as an one-hot vector xsparse, \u2208 \u211d\u207f\u1d5b\u1d62, where n\u1d65\u1d62 is the vocabulary size of the i-th sparse feature, and further transformed into a d-dimensional embedding vector by xsparse\u00b9 = Wsparse xsparse\u2070.\nBy concatenating the dense and sparse embedding vectors, the input global embedding matrix can be obtained as follows\nX\u00b9 =(xdense\u00b9 ||xsparse,\u00b9 ||xsparse,\u00b9||...||xsparse,\u00b9,\nSequence Feature Preprocessing. Similar to the global feature preprocessing, an embedding layer is employed such that each interacted item in the sequence is mapped to a d-dimensional vector s\u209c\u2070, and the user behavior sequences is represented as the concatenation of item embeddings, i.e., S\u2070 = [s\u2081\u2070 || ... ||s\u209c\u2070] \u2208 \u211d\u1d48\u02e3\u1d40.\nIn real-world scenarios, we should expect multiple sequences originating from different user actions (e.g., click and conversion), different platforms, etc. Besides, due to the uncertainty of user behaviors, these sequences often contain noisy and irrelevant user-item interactions. To this end, a MaskNet [32] is employed to unify multiple sequences and filter out the internal noises via self-masking.\nSpecifically, given k sequences S\u2081,..., S\u2096, they are first concatenated along the embedding dimension, i.e., [S\u2081\u2070||...|| S\u2096\u2070] \u2208 \u211d\u1d4f\u1d48\u02e3\u1d40, and further processed by the MaskNet operation as follows\nMaskNet(S) = MLP\u2097\u0441e (S \u2299 MLP\u1d50\u1d43\u02e2\u1d4f (S)),\nwhere MLP\u1d50\u1d43\u02e2\u1d4f : \u211d\u1d4f\u1d48\u02e3\u1d40 \u2192 \u211d\u1d4f\u1d48\u02e3\u1d40 generates the self-masking to select relevant information from the input sequence, and MLP\u2097\u0441e: \u211d\u1d4f\u1d48\u02e3\u1d40\u2192 \u211d\u1d48\u02e3\u1d40 linearly combines multiple sequences into one, matching the dimensionalities of global and sequence features."}, {"title": "4.2 Global Arch: Towards Behavior-Aware Interaction Learning", "content": "Global features, such as user profile and ad content, provide substantial information in understanding user preference over a specific item. Modeling the interaction among global features is the key towards the succuss of CTR prediction [20, 30]. While global features reflect static user interests [16, 41], the user behavior sequence provides complementary information depicting user interests from a dynamic view [46]. For instance, a user's profile might indicate a general interest in electronics, while the recent browsing history on smartphones offers more specific and timely information about the current need. Therefore, it is crucial to learn behavior-aware global interactions to adapt to the evolving environment.\nTo learn behavior-aware global interaction, we model the interactions among global features, as well as sequence summarization, by an interaction module. Formally, given the global input X\u00b9 and sequence summarization S\u02e2\u1d58\u1d50\u00b9 at the 1-th layer, the output of l-th Global Arch is defined as:\nX\u207d\u02e1\u207a\u00b9\u207e = MLP\u207d\u02e1\u207e (Interaction\u207d\u02e1\u207e ([X\u207d\u02e1\u207e||S\u02e2\u1d58\u1d50\u207d\u02e1\u207e])), (7)\nwhere Interaction\u207d\u02e1\u207e (\u00b7) is the interaction module. Note that we do not adhere to a specific interaction module, but rather, various backbone models, such as inner product, DCNv2 [30] and DHEN [41], can be adopted. Besides, an MLP is further employed to transform the output X\u207d\u02e1\u207a\u00b9\u207e to have consistent shape as the input X\u207d\u02e1\u207e for selective information aggregation and easy layer-stacking.\nBy performing interaction learning on the concatenation of X\u207d\u02e1\u207e and S\u02e2\u1d58\u1d50\u207d\u02e1\u207e, Eq. (7) incorporates three different interactions, including global-global, global-sequence, and sequence-sequence interactions. The global-global interactions capture explicit user interests by computing the relevance between the user profile and the target item content, while the global-sequence interactions capture implicit user interests by computing the relevance between the target item and the user's current need behind the interaction sequence. Besides, by computing the interactions among sequence features, the most representative information can be promoted with high interaction scores, while inactive items with low scores can be filtered out. By stacking multiple layers, the Global Arch captures rich behavior-aware interactions at different orders."}, {"title": "4.3 Sequence Arch: Towards Context-Aware Sequence Modeling", "content": "In addition to the explicit static user preference in global features, the implicit dynamic user interests behind the behavior sequences provides complementary information [24, 46]. However, given that user behavior sequence can be highly random and noisy, solely relying on sequence information is ineffective, and it is important to incorporate global context into sequence modeling. For example, a user may randomly browse items on online shopping platforms, but given the global information that the user is an electronic fan, electronic items, e.g., smartphone and laptops, in the browsing history are identified as key information require extra attention.\nTo learn context-aware sequence embeddings, a Sequence Arch is designed on top of two key ideas, including Personalized FFN (PFFN) and Multi-Head Attention (MHA). To enable interactions between global and sequence information, we first employ PFFN to transform sequence embeddings given global summarization as query. Given global summarization X\u02e2\u1d58\u1d50\u00b9 and sequence embedding S\u00b9 at the 1-th layer, the PFFN operation is defined as:\nPFFN (X\u02e2\u1d58\u1d50,\u00b9 , s\u207d\u02e1\u207e ) = f(X\u02e2\u1d58\u1d50) s\u207d\u02e1\u207e, (8)\nwhere f(X\u02e2\u1d58\u1d50) is an MLP that aims to learn the linear projection on sequence based on global summarization.\nBesides, to model the relationship among events in a sequence, MHA is applied to enable the model to attend to different parts of the sequence, capturing long-range dependencies and contextual information. To incorporate global context, before feeding into the first INTERFORMER layer, the global summarization X\u02e2\u1d58\u1d50 is prepended before the sequence as the CLS token, i.e., S\u00b9 \u2254 X\u02e2\u1d58\u1d50 ||S S\u00b9 hence the following MHA on CLS token can aggregate sequence information given global information as query [6, 28]. This is similar to the early fusion idea described in Transact [33] but we primarily consider 'append' instead of 'concat' as the fusion method. Besides, rotary position embeddings [23] are applied on tokens so that the positional information in sequences can be effectively leveraged.\nIn general, the Sequence Arch can be written as follow:\nS\u207d\u02e1\u207a\u00b9\u207e = MHA\u207d\u02e1\u207e (PFFN (X\u02e2\u1d58\u1d50 ,S\u207d\u02e1\u207e)). (9)\nSince the output S\u207d\u02e1\u207a\u00b9\u207e is of the same shape as the input S\u207d\u02e1\u207e, aggressive sequence summarization can be avoided and layers can be easily stacked.\nThrough layer stacking, for one thing, the sequence embeddings is aware of global interactions at different orders via PFFN; for another, MHA at different layers focuses on different parts of the sequence, capturing multi-scale sequence information. Therefore, the model can learn rich context-aware encoding of the sequential data capturing both local and global patterns within the sequence."}, {"title": "4.4 Bridging Arch: Towards Effective Information Selection and Summarization", "content": "Though selective information aggregation is achieved in both the global and sequence arch, as the dimensionalities of input and output features are retained till the final layer, it is infeasible to directly exchange such information due to (1) ineffectiveness given the noisy information and (2) inefficiency given the high-dimensionality. To solve this dilemma, we propose a bridging arch to select and summarize information before exchanging between different data modes.\nTo start with, given the large scale of global embeddings, it is important to selectively summarize them to guide sequence learning. To this end, we highlight the most useful information through a personalized gated selection mechanism as follows:\nX\u02e2\u1d58\u1d50 = Gating(MLP(X\u207d\u02e1\u207e)), (10)\nwhere Gating(X) = X \u2299 MLP(X).\nwhere MLP: \u211d\u1d48\u02e3\u207f \u2192 \u211d\u1d48\u02e3\u207f\u02e2\u1d58\u1d50 with n\u02e2\u1d58\u1d50 \u00ab n, and self-gating [3] provides sparse masking on the embeddings such that relevant information is retained while irrelevant noises are filtered out, providing high-quality context for efficient sequence learning.\nFor sequence information, three types of summarization are neatly designed, including CLS tokens, PMA tokens and recent interacted tokens. The CLS tokens S\u1d9c\u1d38\u02e2 learned by MHA are selected as context-aware sequence summarization. However, the quality of CLS tokens largely depend on the learned global context. To compensate the heavy reliance on global context and enable more flexibility, the PMA tokens S\u1d3e\u1d39\u1d2c [14], which are essentially sequence summarization based on learnable queries, are employed. Besides, the K most recent interacted tokens S\u02b3\u1d49\u1d9c\u1d49\u207f\u1d57 are proven to be effective in capturing user's recent interests [2, 33]. The combination of the above information is further gated by a self-gating layer, serving as the behavior summarization for global interaction:\nS\u02e2\u1d58\u1d50 = Gating (S\u1d9c\u1d38\u02e2 ||S\u1d3e\u1d39\u1d2c || S\u02b3\u1d49\u1d9c\u1d49\u207f\u1d57).\nIn general, the benefits of the bridging arch are two-fold. On the one hand, by separating information summarization from global and sequence arch, information can be retained in both arches, avoiding aggressive information aggregation. On the other hand, effective information exchange can be achieved as high-dimensional global/sequence features are selected and summarized into low-dimensional embeddings. Therefore, the Bridging Arch plays a pivotal role in enabling the model to capture complex interactions between global behaviors and sequential patterns, leading to more comprehensive representations of the input data."}, {"title": "5 Experiment", "content": "We evaluate the proposed INTERFORMER to answer the following research questions:\n\u2022 Q1: How effective is the proposed INTERFORMER (Section 5.2)?\n\u2022 Q2: How scalable is the proposed INTERFORMER (Section 5.3)?\n\u2022 Q3: To what extent does INTERFORMER address the insufficient inter-mode interaction and aggressive information aggregation (Section 5.4)?\n\u2022 Q4: To what extent does INTERFORMER benefit from different modules (Section 5.5)?"}, {"title": "5.1 Experiment Setup", "content": "Datasets. We adopt three benchmark benchmark datasets, including AmazonElectronics [10], TaobaoAd [27], KuaiVideo [15], and a large-scale internal dataset for evaluation. Dataset statistics are summarized in Table 2 with more details in Appendix B.1.\nBaseline Methods. We compare the proposed INTERFORMER with 11 state-of-the-art models, including (1) non-sequential methods: FM [20], xDeepFM [16], AutoInt+ [22], DCNv2 [30], FmFM [25], DOT product, DHEN [41], Wukong [40], and (2) sequential methods: DIN [47], DIEN [46], BST [4], DMIN [35], DMR [17]. We adopt DHEN [41] as the global arch for INTERFORMER in the experiments. Detailed model configurations are provided in Appendix B.2.\nMetrics. We adopt three widely-used metrics to evaluate the models from different aspects, including:\n\u2022 AUC provides an aggregated measure of model capacity in correctly classifying positive and negative samples across all thresholds. Higher the better.\n\u2022 gAUC provides personalized AUC evaluation, where users are weighted by their click account. Higher the better.\n\u2022 LogLoss (cross-entropy loss) measures the distance between the model prediction \u0177 and the actual label y, and can be computed as L(y, \u0177) = (y log (y) + (1 \u2212y) log (1 \u2013 \u0177)). Lower the better.\n\u2022 NE (normalized entropy) [11], is the LogLoss normalized by the entropy of the average empirical CTR of the training set. NE provides a data-insensitive evaluation on model performance as the normalization alleviates the effect of background CTR on model evaluation. Lower the better."}, {"title": "5.2 Effectiveness Results", "content": "The experiment results are shown in Table 3. We first observe that sequential methods consistently outperforms non-sequential methods on all datasets. For non-sequential methods, sequence information is naively aggregated in early stages and further processed together with global information. While in sequential methods, neatly designed sequence processing modules, e.g., RNN [24, 46] and attention mechanism [4, 17, 34, 47], are employed, so that sequential information can be processed in aware of global context. The universal outperformance of sequential methods validates that different data modes should be processed differently. Besides, results show that aggressive sequence summarization in early stages in ignorance of global context will impair model performance.\nComparing INTERFORMER with other sequential methods, INTERFORMER achieves state-of-the-art performance. Specifically, INTERFORMER outperforms the best competitor by up to 0.14% in gAUC, 0.14% in AUC and 0.54% in LogLoss. These results demonstrate INTERFORMER's effectiveness on diverse datasets and generalization across different recommendation tasks."}, {"title": "5.3 Scalability Results", "content": "To validate the scalability of INTERFORMER, we carry out experiments on a large-scale internal dataset containing 70B samples in total, hundreds of non-sequence features and 10 sequences of length 200 to 1,000. In general, a 3-layer INTERFORMER achieves a 0.15% NE gain compared to the internal SOTA model with similar FLOPs and 24% Queries Per Second (QPS) gain. Together with feature scaling, the improvement on NE can be further enlarged with a 10% of Model FLOPs Utilization (MFU) on 512 GPUs attained. INTERFORMER Shows great generalizability on a wide range of models showing promising ROI.\nOn Sequence Feature Scaling. We evaluate how the performance of INTERFORMER changes when the sequence feature scale increases in internal dataset. In addition to six sequences of length 100, we include two additional long sequences of length 1000, and we observe a 0.14% improvement in NE. As shown in Figure 2a, INTERFORMER exhibits better scalability compared to the strong internal baseline (INTERNAL) that leverages cross-attention to capture sequence and non-sequence information, as NE curve of INTERFORMER continues to decrease when more training samples are involved, outperforming INTERNAL by 0.06% in NE. Besides, we also tried to merge six sequences to generate one long sequence of length 600, and we observe promising efficiency improvements in QPS (+20%) and MFU (+17%) with a 0.02% NE tradeoff. From the modeling perspective, the results validate that INTERFORMER is able to enlarge the NE gain brought by the sequence feature scaling.\nOn Model Scaling. Besides feature scaling, we evaluate how INTERFORMER performs when the model scale increases. As Figure 2b shows, scaling InterFormer from 1 to 4 layers achieves consistent NE gains, exhibiting good scaling properties. Specifically, compared to a single layer INTERFORMER, a two-layer INTERFORMER achieves a significant 0.13% NE gain, and 0.05% and 0.04% additional NE gains can be achieved when stacking 3 and 4 layers.\nModel-System Co-Design. Model architecture design plays a crucial role in training efficiency with its implication on GPU FLOPs utilization and inter-GPU communication. We highlight two optimizations based on INTERFORMER architecture that in total boost training efficiency by more than 30%, including communication overhead reduction and computation efficiency.\nFor communication overhead reduction, the global arch DHEN with heavy parameters and relatively light computation tends to be FSDP ([44]) communication bound in distributed training, while the sequence arch Transformer with much higher computation (FLOPs) to parameter ratio is normally computation bound. To alleviate such inefficiency, our parallel design of global and sequence arch allows the exposed communication from DHEN modules to overlap with sequence computation effectively, resulting in a 20% QPS improvement compared to the performing two modules sequentially.\nFor computation efficiency, we perform a series of optimizations, including 1) reallocating FLOPs from small, low-return modules to larger, high ROI modules and 2) combining smaller kernels to better utilize GPU resources. These optimizations improve MFU for interaction modules from 11% to 16%, and DHEN from 38% to 45%, with a 19% MFU improvement for the overall INTERFORMER layer."}, {"title": "5.4 Analysis of the Model", "content": "On the Interleaving Learning Style. We first analyze how the interleaving learning style benefits heterogeneous interaction learning. To validate the universality of INTERFORMER, we consider three backbone global architectures, including dot product (DOT), DCNv2, and DHEN, and five different scenarios, including\n\u2022 sole where only global arch is adopted, while sequence information is naively aggregated in early stage.\n\u2022 sep where inter-mode interaction is disabled and sequence and global arch are learnt separately.\n\u2022 s2g where only sequence-to-global information flow is enabled, while the reverse direction is disabled.\n\u2022 g2s where only global-to-sequence information flow is enabled, while the reverse direction is disabled.\n\u2022 int where the bidirectional information flows are activated.\nAs shown in Figure 3, int consistently outperforms other scenarios regardless of the backbone global arch, with an up to 1.46% outperformance in AUC. This validates the universal benefits brought by our proposed interleaving learning style.\nBesides, we observe an ascending order in model performance when more information is exchanged between different data modes, i.e., sole<sep<g2ss2g<int. Such observation validates our claim that the insufficient inter-mode interaction is a key bottleneck of heterogeneous interaction learning, and bidirectional information flow enables different data modes to be learnt in a mutually beneficial manner. Specifically, when equipped with an additional sequence arch (sep), the model performance consistently outperforms sole w/o sequence modeling. This not only shows the necessity of employing different arch for different data modes, but also implies the possible performance degradation when heterogeneous data is integrated naively. Furthermore, given that the bidirectional information flow (int) consistently outperforms the unidirectional setting (g2s and s2g), we attribute the outperformance of INTERFORMER on other state-of-the-art sequential methods [4, 35, 46"}]}