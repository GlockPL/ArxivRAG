{"title": "DISTILLING REINFORCEMENT LEARNING ALGORITHMS FOR IN-CONTEXT MODEL-BASED PLANNING", "authors": ["Jaehyeon Son", "Soochan Lee", "Gunhee Kim"], "abstract": "Recent studies have shown that Transformers can perform in-context reinforcement learning (RL) by imitating existing RL algorithms, enabling sample-efficient adaptation to unseen tasks without parameter updates. However, these models also inherit the suboptimal behaviors of the RL algorithms they imitate. This issue primarily arises due to the gradual update rule employed by those algorithms. Model-based planning offers a promising solution to this limitation by allowing the models to simulate potential outcomes before taking action, providing an additional mechanism to deviate from the suboptimal behavior. Rather than learning a separate dynamics model, we propose Distillation for In-Context Planning (DICP), an in-context model-based RL framework where Transformers simultaneously learn environment dynamics and improve policy in-context. We evaluate DICP across a range of discrete and continuous environments, including Darkroom variants and Meta-World. Our results show that DICP achieves state-of-the-art performance while requiring significantly fewer environment interactions than baselines, which include both model-free counterparts and existing meta-RL methods. The code is available at https://github.com/jaehyeon-son/dicp.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the introduction of Transformers (Vaswani et al., 2017), their versatility in handling diverse tasks has been widely recognized across various domains (Brown et al., 2020; Dosovitskiy et al., 2021; Bubeck et al., 2023). A key aspect of their success is in-context learning (Brown et al., 2020), which enables models to acquire knowledge rapidly without explicit parameter updates through gradient descent. Recently, this capability has been explored in reinforcement learning (RL) (Chen et al., 2021; Schulman et al., 2017; Lee et al., 2022; Reed et al., 2022), where acquiring skills in a sample-efficient manner is crucial. This line of research naturally extends to meta-RL, which focuses on leveraging prior knowledge to quickly adapt to novel tasks.\nIn this context, Laskin et al. (2023) introduce Algorithm Distillation (AD), an in-context RL ap- proach where Transformers sequentially model the entire learning histories of a specific RL algo- rithm across various tasks. The goal is for the models to replicate the exploration-exploitation be- haviors of the source RL algorithm, enabling them to tackle novel tasks purely in-context. Beyond replication, Laskin et al. (2023) show that this approach can enhance sample efficiency by bypass- ing intermediate learning steps or distilling histories from multiple actors. This combination of an off-the-shelf RL algorithm with the in-context learning capability of Transformers has demonstrated strong potential for improving the adaptability of meta-RL (Lee et al., 2023a; Liu & Abbeel, 2023; Huang et al., 2024; Zisman et al., 2024; Sinii et al., 2024).\nHowever, prior in-context RL approaches have a notable limitation: they tend to replicate the sub- optimal behaviors of the source algorithm. The source RL algorithm updates its policy gradually through gradient descent, sometimes deliberately preventing abrupt changes (Schulman et al., 2015; 2017). As a result, it may take multiple iterations to fully integrate newly discovered information, leading to repeated suboptimal actions during this process. Without a mechanism to deviate from the source algorithm's behavior, existing in-context RL methods (Laskin et al., 2023; Huang et al., 2024; Sinii et al., 2024) inherit these inefficiencies."}, {"title": "2 RELATED WORK", "content": "RL as sequence modeling. With the advent of Transformers, which can learn from much larger datasets than what an agent can typically collect online, their application to offline RL (Levine et al., 2020) has gained prominence (Chen et al., 2021; Janner et al., 2021; Lee et al., 2022; Reed et al., 2022). Despite these advances, Laskin et al. (2023) point out a key limitation: these ap- proaches struggle to improve their policy in-context through trial and error. The primary reason is that they are designed to imitate the dataset policy, which makes them unsuitable for performing in-context RL on novel tasks. To address this, Laskin et al. (2023) propose Algorithm Distillation (AD), an in-context RL approach where a Transformer is trained to distill learning histories from a source RL algorithm across diverse tasks. Notably, AD becomes effective when the context length of Transformers exceeds the episode horizon. Instead of imitating the source algorithm's actions, Decision-Pretrained Transformer (DPT; Lee et al. (2023a)) is designed to predict optimal actions. In-context Decision Transformer (IDT; Huang et al. (2024)) implements a hierarchical approach to in-context RL, where the Transformer predicts high-level decisions that guide a sequence of actions,"}, {"title": "3 PROBLEM FORMULATION", "content": "POMDP. We consider a partially observable Markov decision process (POMDP): $M = (S, A, O,T, R, \u03b3)$. At each time step t, an agent interacts with the environment by selecting an action $a_t \u2208 A$ based on the current state $s_t \u2208 S$. The environment transitions to the next state $s_{t+1} \u2208 S$ according to the transition model $T(s_{t+1} | s_t, a_t)$, and the agent receives a reward $r_t \u2208 R$, determined by the reward model $R(r_t | s_t, a_t)$. However, the agent does not directly observe the true state $s_t$. Instead, it receives partial observation $o_t \u2208 O$ of the state $s_t$. The discount factor $\u03b3 \u2208 [0, 1]$ controls the relative importance of immediate versus future rewards. The tuple (T, R) is referred to as the dynamics model or world model. The agent's history up to time step t is defined as $h_t = (o_0, a_0, r_0, ..., o_t, a_t, r_t)$. The objective of RL is to find a policy that maximizes the expected cumulative reward $J = \u0395_{\u03c0,\u039c} [\\sum_{t=0}^T \u03b3^t r_t]$, where T is the number of interactions. Throughout the paper, we use the terms environment and task interchangeably to refer to a POMDP.\nMeta-RL. We define an algorithm $f : H \u00d7 \u039f \u2192 \u0394(A)$, where H is the space of histories, and A(A) represents the space of probability distributions over A. The objective of meta-RL is to discover an algorithm f that maximizes the expected cumulative reward J over a distribution of POMDPS p(M). During meta-training, the algorithm is optimized on a set of tasks sampled from p(M), while during meta-test, it is evaluated on another set of tasks sampled from p(M). For simplicity, we refer to meta-training and meta-test as training and test when the context is clear.\nFollowing previous works (Laskin et al., 2023; Huang et al., 2024), we focus on scenarios where an offline dataset of learning histories $D = {h^i_{\u03c4}= (o^i_0, a^i_0, r^i_0, \u2026\u2026\u2026, o^i_\u03c4, a^i_\u03c4, r^i_\u03c4)}_{i=1}^n$ is available, generated by a source algorithm $f_{source}$ for a set of meta-training tasks $\\{Mi \u223c p(M)\\}^n_{i=1}$. The loss function for AD (Laskin et al., 2023) is defined as\n$L_{AD}(\u03b8) = - \\sum_{i=1}^n \\sum_{t=1}^T log f_\u03b8(a^i_t | o^i_t, h^i_{t\u22121}),$ (1)\nwhere \u03b8 represents a sequence model. Similarly, the loss functions for DPT (Lee et al., 2023a) and IDT (Huang et al., 2024) are respectively presented in Eq. 3-4 in App. C."}, {"title": "4 DISTILLATION FOR IN-CONTEXT PLANNING", "content": "In this section, we propose Distillation for In-Context Planning (DICP), where the agent utilizes an in-context learned dynamics model to plan actions. To learn the dynamics model in-context, we introduce meta-model $g_\u03b8 : H \u00d7 O \u00d7 A \u2192 \u2206(R) \u00d7 \u25b3(O) \u00d7 \u25b3(R)$, which shares the same sequence model \u03b8 with the algorithm $f_\u03b8$. It yields probability distributions over the spaces of reward $r_t$, next observation $o_{t+1}$, and return-to-go $R_t = \\sum_{t'=t}^H \u03b3^{t\u2032\u2212t} r_{t\u2032}$ based on history $h_{t\u22121}$, current observation $o_t$, and action $a_t$. H represents the time step at which the episode ends. We optimize \u03b8 throughout the meta-training phase (Alg. 1) with following loss:\n$L(\u03b8) = L_{im}(\u03b8) + \u03bb \u00b7 L_{Dyn}(\u03b8), \\text{where} L_{Dyn}(\u03b8) = - \\sum_{i=1}^n \\sum_{t=1}^T log g_\u03b8(r_t, o_{t+1}, R_t | o_t, a_t, h_{t\u22121}).$ (2)\nHere, $L_{im}$ corresponds to one of $L_{AD}$, $L_{DPT}$, or $L_{IDT}$ (Eq. 1, 3-4), and \u03bb is a hyperparameter that balances the imitation loss $L_{im}$ and the dynamics loss $L_{Dyn}$.\nAfter meta-training, the sequence model \u03b8 is fixed and evaluated during the meta-test phase (Alg. 2). A key distinction between our method and previous in-context RL approaches (Laskin et al., 2023; Lee et al., 2023a; Huang et al., 2024) lies in how actions are selected (L6). Previous methods generate actions directly from the sequence model: $a_t \u2190 f_\u03b8(\u00b7 | o_t, h_{t\u22121})$. These approaches may replicate the inefficiencies arising from the gradual updates of gradient-based RL algorithms, as they imitate the source algorithm's behaviors.\nIn contrast, our method employs an additional DICP subroutine (Alg. 3) for action selection. As a planning strategy, we adopt Model Predictive Control (MPC) (Nagabandi et al., 2018; 2019; S\u00e6mundsson et al., 2018). At each time step, the agent simulates future outcomes, selects the path that maximizes predicted return, and executes the first action in that path. Specifically, upon receiving an observation $o_t$, the agent samples L candidate actions {$\u00e2^1_t, ..., \u00e2^L_t$}, and predicts the consequences"}, {"title": "5 EXPERIMENTS", "content": "5.1 ENVIRONMENTS\nWe evaluate our DICP framework across a diverse set of environments. For discrete environments, we use Darkroom, Dark Key-to-Door, and Darkroom-Permuted, which are well-established bench- marks for in-context RL studies (Laskin et al., 2023; Lee et al., 2023a; Huang et al., 2024). For continuous ones, we test on the Meta-World benchmark suite (Yu et al., 2019).\nDarkroom. Darkroom is a 2D discrete environment where the agent should locate a goal with limited observations. The state space consists of a 9 \u00d7 9 grid, and the action space includes five actions: up, down, left, right, and stay. The agent earns a reward of 1 whenever reaching the goal,"}, {"title": "6 ABLATION STUDY", "content": "We perform an ablation study to evaluate key design choices in our approach. We examine the effect of model-based planning at different scales (\u00a76.1), the effect of context length on the accuracy of the in-context learned dynamics model (\u00a76.2), and the effect of different source algorithms (\u00a76.3).\n6.1 EFFECT OF MODEL-BASED PLANNING AT DIFFERENT SCALES\nWe evaluate our approach using search algorithms at varying scales, as shown in the first row of Fig. 3. Experiments are conducted in two environments: Darkroom (discrete) and Reach-v2 (con- tinuous). We use DICP-AD for Darkroom and DICP-IDT for Reach-v2, the best-performing models for each environment. The results demonstrate that incorporating model-based planning signifi- cantly improves performance compared to baselines that train the dynamics model but do not use it for planning. While increasing the beam size or sample size improves performance, the gains plateau after a size of 10. When the dynamics model is not used for planning, the performance is comparable to that of model-free baselines.\n6.2 EFFECT OF CONTEXT LENGTHS ON ACCURACY OF THE DYNAMICS MODEL\nWe investigate the effect of context length on the accuracy of the in-context learned dynamics model. Our hypothesis is that extending the context length enables the model to capture more information about the environment, improving prediction accuracy. To verify this, we evaluate the test loss by forwarding the learning histories of the source algorithm on meta-test tasks into the meta-trained DICP-IDT on Reach-v2. The test loss is measured as the mean squared error between the predicted and actual rewards and observations. The models are trained with a context length of 1000, and the loss is calculated by averaging across every 10 positions within the context. The results in the second row of Fig. 3 support our hypothesis: longer context lengths yield more accurate predictions for both observations and rewards. Given that the effectiveness of model-based planning heavily depends on the dynamics model's bias (Janner et al., 2019; Hiraoka et al., 2021), our framework benefits from longer context lengths. Furthermore, as in-context model-free RL methods have also been shown to improve with extended context lengths (Laskin et al., 2023), our combined framework gains substantial advantages from using longer context lengths."}, {"title": "7 CONCLUSION", "content": "We introduced an in-context model-based RL framework that leverages Transformers to not only learn dynamics models but also improve policies in-context. By incorporating model-based plan- ning, our approach effectively addresses the limitations of previous in-context model-free RL meth- ods, which often replicate the suboptimal behavior of the source algorithms. Our framework demon- strated superior performance across various discrete and continuous environments, establishing in- context RL as one of the most effective meta-RL approaches."}, {"title": "A IMPLEMENTATION DETAILS", "content": "A.1 SOURCE ALGORITHMS\nFor the source algorithms (Schulman et al., 2017; Haarnoja et al., 2018), we use the implementation of Stable Baselines 3 (Raffin et al., 2021). The hyperparamter settings are detailed in Table 3-4. Unless otherwise specified, most hyperparameters are set to default values.\nA.2 TRANSFORMERS\nWe implement Transformers using the open-source TinyLlama (Zhang et al., 2024). The hyperpa- rameters are provided in Table 5, along with additional parameters specific to IDT."}, {"title": "A.3 ENVIRONMENTS", "content": "In the ML1 benchmarks, we observe that using shorter horizons for the source algorithm accelerates the learning of in-context RL methods. Specifically, we terminate the source algorithm after 100 steps, as opposed to the original 500-step horizon provided by Meta-World. To minimize train-test discrepancies, we also meta-test the in-context RL methods using 100-step horizons. Even under these less favorable conditions, in-context RL methods still outperform the baselines, as shown in Fig. 2 and Table 1."}, {"title": "A.4 PLANNING", "content": "We find that predicting only the immediate reward, rather than both the immediate reward and the return-to-go, is sufficient to outperform baselines. Summation of predicted immediate rewards up to the current step can serve as a myopic proxy for the total return of a planning path. In Dark- room variants, the binary nature of rewards often leads to different beam paths achieving the same"}, {"title": "B ADDITIONAL EXPERIMENTS", "content": "We conducted experiments across all 50 environments of Meta-World ML1, using PPO (Schulman et al., 2017) as the source algorithm with the same hyperparameters as those used in Table 1. Each method is evaluated with 20K environment steps during the meta-test phase. As shown in Table 8, our method outperforms model-free counterparts. The mean learning curve is displayed in Fig. 2. It is noteworthy that since our configuration for the source algorithm is not tuned for every task, the results may not fully reflect the full potential of in-context RL methods, including ours."}, {"title": "C EXTENDED RELATED WORK", "content": "DPT. Decision-Pretrained Transformer (DPT; Lee et al. (2023a)) is an in-context RL approach that meta-trains (or pre-trains) Transformers to predict optimal actions based on contextual trajecto- ries. Consequently, DPT requires access to the optimal actions during training. The authors suggest several sources for gathering the meta-training dataset, including (i) randomly sampled trajectories directly from the environment, (ii) trajectories generated by a source RL algorithm, and (iii) trajec- tories of an expert policy. In this work, we concentrate on the second source to minimize reliance on"}]}