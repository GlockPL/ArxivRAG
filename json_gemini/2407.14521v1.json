{"title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A Domain-Specific In-Context Agent", "authors": ["Mahdi Buali", "Robert Hoehndorf"], "abstract": "Automated Theorem Proving (ATP) faces significant challenges due to the vast action space and the computational demands of proof generation. Recent advances have utilized Large Language Models (LLMs) for action selection in ATP, but these methods often require substantial computational resources. This study introduces the Functional Equation Automated Solver (FEAS), an agent that builds on the COPRA in-context learning framework within the Lean environment. FEAS innovates by refining prompt generation and response parsing mechanisms, integrating domain-specific heuristics for functional equations, and introducing the FunEq dataset a rigorously curated collection of functional equation problems categorized into three difficulty levels. The agent's performance is evaluated against established baselines using this dataset, demonstrating improvements in theorem proving accuracy, particularly with the integration of functional equation-specific heuristics. Our results highlight the effectiveness of FEAS in generating and formalizing high-level proof strategies into Lean proofs, emphasizing the potential of tailored approaches in domain-specific ATP challenges.", "sections": [{"title": "Introduction", "content": "Automated theorem proving (ATP) has long been a challenging endeavor in computer science [6]. Formalizing mathematics for efficient machine processing presents a significant hurdle, further complicated by the inherent infinite nature of the action space for proof construction. Interactive theorem provers (ITPs) like Lean [13], Coq [7], Isabelle [22], and HOL4 [18] offer a solution by facilitating formal proofs through user-guided application of tactics until the desired goals are achieved.\nRecent efforts have explored the use of Large Language Models (LLMs) as action selectors to address the vast action space in ATP [17] [5]. These approaches involve training LLMs from scratch [11] or fine-tuning pre-trained models [1] to generate plausible actions within the context of formal proofs. However, such methods often incur significant computational costs."}, {"title": "Related Work", "content": "Deep learning has emerged as a promising approach to tackle the combinatorial explosion of the search space in automated theorem proving (ATP) [24] [2] [4] [23]. The advent of Transformer-based language models revolutionized automated theorem proving by eliminating the need to explicitly hardcode the syntax of interactive theorem provers (ITPs). GPT-f [17] pioneered this approach, utilizing language models to generate novel proofs accepted into the Metamath [12] library. PACT [5], a follow-up project, utilized self-supervised data to improve tactic prediction in the Lean proof assistant. Further enhancements with expert iteration [16] enabled autonomous curriculum learning, achieving state-of-the-art performance on the miniF2F benchmark [26], a dataset of formal Olympiad-style problems.\nSubsequently, Thor [10] integrated language models with Isabelle's Sledghammers [15] for premise selection, alleviating the need for explicit specification of every proof step. Other work [9] leveraged this integration, employing in-context learning for autoformalization and expert iteration to achieve improved results on the MiniF2F benchmark. Concurrently, HTPS [11] explored the integration of reinforcement learning with LLMs for guided proof search.\nRecent advances have sought to address the computational cost of LLM pre-training. LLEMMA [1] continued pretraining Code Llama on mathematical data, demonstrating capability in formal theorem proving. ReProver [25] focused on premise selection using a retrieval-augmented approach, achieving success with relatively modest computational resources.\nHowever, the computational burden of fine-tuning LLMs remained a concern. COPRA [19] addressed this by employing general-purpose LLMs within an in-context learning framework. This approach repeatedly queries a LLM to propose tactics, leveraging feedback from the proof environment and retrieved lemmas to refine subsequent queries. While COPRA outperformed several baselines, it, like most prior works, generates proofs one tactic at a time, focusing on low-level proof steps in comparison to human-like informal reasoning. Additionally, previous work primarily developed general solvers without leveraging domain-specific knowledge, limiting their efficacy in specialized areas like functional equations."}, {"title": "The FunEq Dataset", "content": "We developed the FunEq dataset, a manually curated collection of functional equation problems formalized in Lean. Our focus on functional equations is motivated by the fact that, while a specialized domain, their solutions necessitate a diverse array of proof techniques. These range from basic algebraic manipulations to sophisticated reasoning about concepts like continuity [21], providing a rich testing ground for automated theorem provers.\nTo accommodate varying levels of difficulty, FunEq is structured into three categories:\nSimple Dataset This dataset introduces 18 problems which require only fundamental functional equation reasoning steps. Proofs primarily involve simple substitutions, linear arithmetic, the use of involutions, straightforward induction, and basic case analysis.\nIntermediate Dataset This dataset contains 15 problems which focuse on proving intermediate lemmas often encountered in the solution process of more complex functional equations such as establishing injectivity and surjectivity. Problems are sourced primarily from Evan Chen's article [3] and the book \"Functional Equations: A Problem-Solving Approach\" by Venkatachala [21].\nHard Dataset This dataset consists of most of the International Mathematical Olympiad (IMO) shortlisted functional equation problems since 2002"}, {"title": "The FEAS Agent", "content": "The FEAS (Functional Equation Automated Solver) agent (Algorithm 1) builds upon the foundation of the COPRA framework [19], specializing in the domain of functional equations.\nPrompt Engineering. FEAS introduces a key refinement in the system prompt structure. Rather than directly soliciting a Lean proof step, FEAS guides a LLM through a multi-stage response generation process. It prompts the LLM to first articulate a high-level proof strategy in natural language, then formalize and translate this strategy into a Lean-compatible proof.\nMulti-Block Parsing and Error Handling. FEAS adopts a dynamic block-based parsing strategy to manage the multi-line Lean proofs generated by a LLM. This strategy enhances robustness by dividing the generated proof into logical blocks based on the underlying structure of Lean proofs. By processing each block independently, FEAS can effectively isolate and recover from errors in specific parts of the proof, potentially salvaging and utilizing valid proof segments even if the overall proof generated by the LLM is not entirely correct.\nAutomatic Tactic Application. After either successful parsing of all blocks or encountering an error, FEAS attempts the automatic application of the nlinarith tactic which can simplify proofs by automatically handling complex algebraic manipulations that would otherwise need to be done manually. Successful application incorporates this step into the proof, otherwise, it is omitted. This provides automation and taps into the power of Lean's built-in tactics.\nDomain-Specific Heuristics. FEAS integrates functional equation heuristics [3] directly into the system prompt alongside Lean syntax examples. These heuristics encompass substitution-based simplification, techniques for proving"}, {"title": "Evaluation", "content": "We conduct a series of experiments to evaluate the performance of the FEAS agent and the impact of incorporating domain-specific heuristics. Our evaluation includes comparisons across four different LLMs: GPT-4 Turbo, Gemini-1.5-Pro, Claude-3.5-Sonnet, and Llama3 70b. We evaluate FEAS agents against two baselines: Few-Shots and COPRA, the original in-context learning agent, which serves as points of comparison. We assess FEAS in two distinct configurations: one with the integrated domain-specific functional equation heuristics and one without.\nThe experiments are performed on the simple and intermediate tiers of the FunEq dataset. To gauge performance on more complex problems, we further evaluate all agents on the A1 subset of the hard dataset, which consists of the easiest shortlisted algebra problems from each corresponding IMO year. To control for potential variability in LLM responses, we execute each experiment twice on the simple and intermediate datasets. Due to computational resource constraints, we limit our evaluation to a single run on the Al subset."}, {"title": "Results", "content": "Tables 1 and 2 show our evaluation across the Simple and Intermediate tiers of FunEq. All combinations of agents and LLMs fail to generate proofs on the Hard tier of FunEq. On the Simple dataset, FEAS agents consistently achieves the highest success rates across all evaluated LLMs. FEAS with integrated heuristics on GPT and Gemini achieves the highest performance, demonstrating the efficacy of domain-specific knowledge. However, Claude and Llama3, without heuristics, shows superior performance on this dataset, suggesting that in certain LLM configurations, heuristics may misguide proof search.\nOn the more challenging Intermediate dataset, the challenge increases substantially, with all approaches showing lower success rates. However, FEAS agents again consistently ranks highest in performance, highlighting its ability to navigate more complex functional equation proofs. Again, in some cases, Gemini, FEAS performs better without heuristics. Furthermore, all methods fail to generate proofs on the Hard tier of FunEq, indicating that significant challenges remain in automated theorem proving for functional equations."}, {"title": "Conclusion", "content": "Our experiments establish the FEAS agent as an advancement in automated theorem proving for functional equations. FEAS refines prompting, parsing, and integration of domain-specific heuristics demonstrate improvement over baselines. While results on the Simple dataset are encouraging, performance on the Intermediate and Hard datasets highlights the ongoing challenges in this complex domain.\nSpecifically, the challenges revealed by our evaluation can be decomposed into two distinct sub-problems: (1) proposing mathematically useful proof steps, and (2) accurately translating these high-level steps into the formal language of the theorem prover. Each of these sub-problems poses its own complexities, requiring distinct approaches for further improvement.\nSeveral avenues present themselves for future research. The development of agents tailored to specific sub-tasks within the framework. Incorporating a broader repertoire of high-level proof tactics within the LLM's prompting could improve the performance of generating Lean proof steps. Investigating search algorithms beyond the currently employed depth-first search has the potential to improve efficiency and solution discovery. Finally, designing efficient self-learning mechanisms for FEAS would enable it to continuously refine its strategies based on both successful and unsuccessful proof attempts."}, {"title": "Appendix", "content": "theorem intermediate_funeq_2\n(f: R \u2192 R)\n(h_0 : \u2200 x, f(x + 1) = f(x) + 1)\n(h_1 : \u2200 x, x \u2260 0 \u2192 f(1/x) = f(x)/x^2) :\n\u2200 x, x \u2260 0 \u2192 f(1 + 1/x) = 1 + f(x)/x^2 :=\nbegin\n  intro x,\n  intro hx,\n  have h_2: f (1 / x + 1) = f (1 / x) + 1 := h_0 (1 / x),\n  have h_3 : f (1 / x) = f x / x^2 := h_1 x hx,\n  rw h_3 at h_2,\n  rw add_comm at h_2,\n  nlinarith,\nend\ntheorem intermediate_funeq_2\n(f: R \u2192 R)\n(h_0:\u2200x, f(x + 1) = f(x) + 1)\n(h_1 : \u2200 x, x \u2260 0 \u2192 f(1/x) = f(x)/x^2) :\n\u2200 x, x \u2260 0 \u2192 f(1 + 1/x) = 1 + f(x)/x^2 :=\nbegin\n  intro x, intro hx,\n  rw [h_0 (1 / x), h_1 x hx] at *,\n  field_simp [hx],\n  rw mul_comm at *,\n  rw [\u2190h_0 (1 / x), h_1 x hx] at *,\n  rw [\u2190h_0 (1 / x), h_1 x hx] at *,\n  ring_nf,\nend\nTo illustrate the distinct strengths of FEAS, we examine a specific functional equation problem (intermediate_funeq_2) where it succeeds while COPRA does not. FEAS' solution demonstrates its ability to generate high-level intermediate proof steps using the have tactic, mirroring a human's approach. This contrasts with COPRA, which focuses solely on lower-level Lean tactics. FEAS' strategy, guided by the system prompt instruction to first generate a natural language proof sketch, leads to a more human-readable and strategically structured proof.\nFurthermore, FEAS' block-by-block parsing successfully handles errors within individual proof blocks. While all three lines generated by the LLM contained"}]}