{"title": "Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law", "authors": ["Manuj Kant", "Sareh Nabi", "Manav Kant", "Roland Scharrer", "Megan Ma", "Marzieh Nabi"], "abstract": "Legal services rely heavily on text processing. While large language models (LLMs) show promise, their application in legal contexts demands higher accuracy, repeatability, and transparency. Logic programs, by encoding legal concepts as structured rules and facts, offer reliable automation, but require sophisticated text extraction. We propose a neuro-symbolic approach that integrates LLMs' natural language understanding with logic-based reasoning to address these limitations.\nAs a legal document case study, we applied neuro-symbolic AI to coverage-related queries in insurance contracts using both closed and open-source LLMs. While LLMs have improved in legal reasoning, they still lack the accuracy and consistency required for complex contract analysis. In our analysis, we tested three methodologies to evaluate whether a specific claim is covered under a contract: a vanilla LLM, an unguided approach that leverages LLMs to encode both the contract and the claim, and a guided approach that uses a framework for the LLM to encode the contract. We demonstrated the promising capabilities of LLM + Logic in the guided approach.", "sections": [{"title": "Introduction", "content": ""}, {"title": "1.1 Importance of Trustworthy Legal AI", "content": "Legal systems rely on rigorous reasoning, explain-ability, and transparency to ensure fairness and ac-countability. Unlike many other AI applications,legal decision-making directly affects individuals'rights, obligations, and access to justice. Conse-quently, AI-driven legal solutions must go beyondsurface-level predictions and provide structured,interpretable reasoning.\nExpert attorneys engage in complex reasoningbeyond pattern recognition. Legal analysis requiresSystem 2 thinking \u2013 deliberate and logical reason-ing that evaluates statutes, case law, and contracts.Attorneys dissect legal texts, identify principles,and construct arguments based on precedent. Theirdecisions involve weighing interpretations, assess-ing nuances, and considering broader implications.Additionally, legal professionals must articulatetheir reasoning clearly, ensuring their conclusionsare defendable against scrutiny from courts, clients,and the opposition.\nThe sensitive nature of legal queries requires asystem that is both correct and interpretable. In the U.S., oversight of AI systems is intensifying,with the Bipartisan House Task Force on ArtificialIntelligence (2024) highlighting the need for trans-parency to prevent deceptive practices and ensureconsumer protection. Every legal argument mustreference laws, precedents, or contractual clauses,ensuring accountability. Unlike black-box AI, legalreasoning must be auditable, allowing stakeholders to trace conclusions. Without this level of explain-ability, AI legal tools risk undermining trust andreliability in decision-making.\nIn parallel, sector-specific supervision in the in-surance domain, as in our case study, is evolv-ing; for example, the International Association ofInsurance Supervisors (2024) recently published its Draft Application Paper on the Supervisionof AI, which calls for rigorous auditability andinterpretability standards for AI-driven contractanalytics. Under Europe's General Data Protec-tion Regulation, data subjects must be provided\u201cmeaningful information about the logic\" underly-ing automated decision-making processes (European Union, 2016). This requirement ensures thatindividuals can understand, challenge, or seek hu-man intervention regarding algorithmic decisions.Similarly, the proposed EU AI Act mandates thathigh-risk AI systems be designed with explainabil-ity and traceability, ensuring stakeholders can rea-sonably comprehend the system's functioning and\""}, {"title": "1.2 Challenges in Legal Text Processing", "content": "Legal services rely mainly on text-processing ca-pabilities, which can enormously benefit from newadvancements in large language models (LLM).Several scientific studies and business initiativeshave highlighted the potential and limitations ofLLMs in the legal domain. Nevertheless, LLM hal-lucinations have manifested in critical errors, suchas generating nonexistent case law citations andmisinterpreting contractual provisions.\nA prominent example is Mata v. Avianca, wherean attorney unknowingly submitted a brief contain-ing fictitious judicial opinions produced by Chat-GPT (Aidid, 2024). This event underscores therisks of using LLMs without robust verificationmechanisms.\nApplying LLMs in the legal domain demandshigher accuracy, repeatability, and transparency toachieve a transformative impact. The LLM reason-ing abilities have traditionally been too weak tounderstand the complex logic associated with legalcontracts. Considerable progress is still requiredbefore these technologies deliver consistent andtransparent solutions.\nWhile human lawyers can articulate the reason-ing behind their decisions and strategies, LLMs lack this capability to a sufficient degree. Despiteprogress in methodologies such as retrieval aug-mented generation - which guides LLMs to retrieveinformation from credible sources - hallucinationscan and do occur, including for citations in the legaldomain (Magesh et al., 2024). The auto-regressivenature of these models, which pushes them into greedily generating responses word-by-word ratherthan upfront planning, may contribute to this limi-tation (Borazjanizadeh and Piantadosi, 2024).\nThe recent release of OpenAI ol, whichachieved substantially better results on reasoning-based tasks than its predecessors, can change thissituation. The subsequent releases of DeepSeek R1and OpenAI 03-mini, which achieved similar re-sults to OpenAI ol at substantially lower costs,have demonstrated the potential for \"reasoning\"LLMs to revolutionize task automation.\nDespite these advancements, LLMs (includingreasoning models such as OpenAI ol) still have apenchant for hallucinating on tasks that involve ap-plying and interpreting complex rules. OpenAI olachieved a score of 77.6% on LegalBench (Guhaet al., 2023; Vals.ai, 2025), a benchmark compris-ing a diverse set of tasks on various legal domains,leaving much room for improvement."}, {"title": "1.3 Proposed Neuro-Symbolic Approach", "content": "Unlike LLMs, logic programs, which have provenhelpful for formally representing legal concepts asstructured code, offer a solution to this ambiguityby reliably automating legal reasoning. Since logicprogramming fundamentally relies on the interplayof rules and facts, developing computable legalreasoning may depend on a complex informationextraction process from written documents (Wangand Pan, 2020; Aitken, 2002).\nA neuro-symbolic AI approach of combiningLLMs' natural language capabilities with a logic-based reasoning system could eventually offsetLLMs' limiting drawbacks to achieve correct, con-sistent, and explainable text analysis, generation,and manipulation of legalese. Applying this ap-proach raises new questions about a) architecture- how to combine LLMs with logic programs, b)performance - what is the improvement in accuracyand consistency, and c) explainability - is the rea-soning more understandable for humans, comparedto plain vanilla LLMs.\nThis paper demonstrates how integrating LLMswith logic programming, particularly by promptingLLMs on legal terms transformed into logic pro-grams, could outperform vanilla LLMs on targetedlegal queries. Furthermore, we evaluate the perfor-mance gain by measuring the effect of promptingLLMs on legal terms transformed into logic pro-grams compared to applying solely LLMs to queryspecific legal cases.\nThe described experiments are based on a pre-defined and validated set of insurance claim cov-erage questions and answers from two US healthinsurance policies: 1) a simplified Chubb Hospi-tal Cash Benefit Policy (see Appendix A.1) and 2)more complex, a Stanford Cardinal Care Aetna Stu-dent Health Insurance Plan (Aetna Life Insurance,2023)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Evaluation of LLM in the Legal Domain", "content": "Recent evaluations of LLMs in the legal domainhave revealed promising advances and critical limi-tations. Blair-Stanek and Durme (2025) show thatstate-of-the-art LLMs exhibit considerable outputinstability when answering legal questions, withmodels yielding divergent decisions even undercontrolled settings. In parallel, Hu et al. (2025)address the prevalent issue of hallucinations in le-gal question answering by proposing a fine-tuningframework that integrates behavior cloning with asample-aware iterative direct preference optimiza-tion strategy, thereby enhancing factual consistency.Peoples (2025) further underscores that, althoughLLMs are capable of performing basic legal anal-ysis through a typical chain of thoughts approachsuch as Issue, Rule, Analysis, and Conclusion(IRAC), their brief and sometimes unreliable out-puts raise concerns regarding their adequacy forhigh-stakes legal reasoning and education.\nComplementing these findings, an evaluation re-ported in the Journal of Legal Analysis (Dahl et al.,2024) highlights persistent transparency, ethicalcompliance, and reliability challenges when de-ploying LLMs for legal research in practice. Theheterogeneous nature of legal language across dif-ferent jurisdictions often leads to inconsistenciesin model outputs, thereby questioning the abilityto generalize with the necessary level of accuracy.The study demonstrated that legal hallucinationsare pervasive and disturbing: hallucination ratesrange from 59% to 88% in response to specificlegal queries.\nComprehensively the LegalBench benchmark in-troduced by Guha et al. (Guha et al., 2023) providesa collaboratively built suite of tasks that systemat-ically measures various facets of legal reasoning,emphasizing the necessity for domain-specific eval-uation metrics. These studies illustrate that whileLLMs hold potential for legal applications, carefuland targeted methodological improvements are es-sential to ensure their dependable integration intolegal practice. Thus, while reported accuracy met-rics are encouraging, they must be evaluated along-side limitations in consistency and transparency toassess the actual applicability of LLMs in the legaldomain."}, {"title": "2.2 Advances in Neuro-Symbolic AI", "content": "Recent advances in legal language processing haveincreasingly focused on integrating LLMs withsymbolic reasoning to balance the flexibility ofneural architectures with the rigor of formal logic.Alonso and Chatzianastasiou (2024) demonstratedthat embedding logical rules into neural frame-works can enhance the interpretability and robust-ness of legal text analysis. Servantez et al. (2024)introduced the Chain of Logic prompting method,which decomposes legal reasoning into indepen-dent logical steps and recomposes them to formcoherent conclusions for rule-based legal evalua-tion. Similarly, Cummins et al. (2025) presentedInsurLE. This domain-specific controlled natural-language codifies insurance contracts by preservingkey syntactic nuances while exposing the underly-ing formal logic for a computable representation.Wei et al. (2025) proposed a hybrid neural-symbolic framework that synergizes neural repre-sentations with explicit logical rules, thereby im-proving the rigor of legal reasoning in automatedsystems. Patil (2025) systematically surveyedmethods to enhance reasoning in LLMs and high-lighted modular reasoning and retrieval-augmentedtechniques as promising approaches for bolsteringlogical consistency in legal applications. Coleloughand Regli (2025) provided a comprehensive reviewof neuro-symbolic AI in the legal domain, identify-ing substantial progress in learning and inferencewhile noting significant gaps in explainability andunderstanding derived logic programs.\nCalanzone et al. (2024) developed a neuro-symbolic integration approach that enforces logicalconsistency by incorporating external constraintsets into LLM outputs. Sun et al. (2024) intro-duced a framework that explicitly learns case-leveland law-level logic rules to generate faithful andinterpretable explanations for legal case retrieval.Tan et al. (2024) enhanced LLM reasoning througha self-driven Prolog-based chain-of-thought mech-"}, {"title": "3 Preliminary Experiments", "content": "In this section, we evaluate a range of state-of-the-art reasoning models to benchmark their capabili-ties in answering coverage-related 'yes/no' claimquestions about an insurance policy.\nWe selected seven LLMs, including Ol-preview,DeepSeek-R1, Llama-3.1-405B-Instruct, Claude-3.5-Sonnet, Mistral-Large-Latest, Gemini-1.5-Pro,and GPT-40-2024-08-06\u00b9. These models excelin long-context reasoning, mathematical problem-solving, multi-step reasoning, logical consistency,and following policy rules, making them well-suited for analyzing insurance contracts and legaltexts. For all models, we set both the temperatureand top-p parameters to 1.\nThe insurance contract used in the experimentsin this section is the Simplified Chubb HospitalCash Benefit policy, referred to as Chubb hereafter.The task is to determine whether nine given claimsare covered under this insurance policy. The Chubbcontract and nine claim queries are provided inAppendix A.1 and A.2, respectively.\nWe describe the vanilla LLM approach in \u00a73.1where we directly ask the LLM to answer \u2018yes/no'claim questions. In Section 3.2, we task the LLMwith generating Prolog encodings of the insurancecontract and claim queries, which we then man-ually evaluate using the help of SWISH Prologinterpreter (Contributors to SWI-Prolog, 2024)."}, {"title": "3.1 Vanilla LLM Approach", "content": "We prompt the selected LLMs to answer nine claimquestions about the Chubb insurance policy. Wethen evaluate the performance of these modelsacross 10 trials and report their average accura-"}, {"title": "3.2 Unguided LLM-generated Prolog", "content": "We prompted the selected LLMs (from \u00a73) to gen-erate Prolog encodings of the Chubb policy con-"}, {"title": "4 Expert-Guided Experiments", "content": "In what follows, we demonstrate a workflow forleveraging LLMs through expert guidance to au-tomate the process of encoding health insurancepolicies as logic programs (also called computablecontracts).\nWe prompted LLMs to encode Prolog rules rep-resenting three insurance coverages. The first cov-erage was the simplified Chubb policy, described inthe previous experiment in \u00a73 (see Appendix A.1).The latter two have been derived from the Stan-ford CodeX Insurance Analyst (CodeX, 2025a),a deployed, expert-encoded computable contractrepresenting the Stanford Cardinal Care Aetna Stu-dent Health Insurance Plan (Aetna Life Insurance,2023) (Oliver R. Goodenough and Preston J. Carl-son, 2023).\nSpecifically, we evaluated LLMs' ability to en-code the Advanced Reproductive Technology (ART)and the Comprehensive Infertility (CI) coverage rules from the Insurance Analyst. Each coveragerule in the Insurance Analyst evaluates claims toreach coverage decisions, calling helper rules fromother parts of the code base in the process (seeFigure 2a). The LLMs were prompted to encodetheir own versions of these coverage rules with 1)the coverage text from the Cardinal Care policy 2)documentation defining a valid claim to the rule,and 3) documentation defining the relevant helperrules which can be called from other parts of thecode base (see Figure 2b, Appendix A.3.3). Thedocumentation provided to the LLM constitutesguidance given by an expert. We had each LLMgenerate an encoding of each coverage 5 times,testing each coverage encoding by querying it withclaims and evaluating whether the outputted deci-sions were correct (see Figure 2c)."}, {"title": "4.1 Guided LLM-generated Prolog for a simplified policy", "content": "On the Chubb policy, we prompted LLMswith the text of the policy and documenta-tion about the facts provided in any validclaim (e.g., claim_hospitalization_reason,claim_misrepresentation_occurred) to beused in generating a representative computablecontract. Since this policy is stand-alone, itsencoding does not need to integrate into a moreextensive code base. Thus, no helper rules (fromother parts of the code base) needed to be includedin the prompt.\nThree of the four LLMs performed well on thistask (see Table 1), with each of their 5 generatedencodings perfectly answering all 9 test queriesused for evaluation. These test queries were Prologtranslations of the natural language queries used toassess the previous approach (see Appendix A.2).DeepSeek-R1, however, produced one encodingwith a syntactic error due to an unclosed paren-thesis. This, along with some failed test cases inanother one of its encodings, resulted in a loweraccuracy rate than the other models."}, {"title": "4.2 Guided LLM-generated Prolog for coverages in a larger policy", "content": "The Stanford Cardinal Care health insurance pol-icy comprises many individual \u201ccoverages\u201d. Fora claim to be covered under the policy, it must becovered under one of these coverages. Thus, while"}, {"title": "5 Conclusion and Future Work", "content": "We are on the cusp of an exciting era where AIenhances legal access through human-like think-ing, such as planning and reasoning. While LLMsshow promise, their probabilistic nature, inconsis-tency, and potential for hallucination make theirapplication in the legal domain risky.\nWe propose a neuro-symbolic approach thatcombines LLMs with logic programming and pro-vide a comparative analysis of a use case involvinghealth insurance coverage questions. First, we ex-perimented with a vanilla LLM approach, withoutlogic programs, prompting the LLM to respond tocoverage claims based on the contract. Our keyobservation is that advancements in foundationalmodels allow a vanilla LLM to reasonably assesswhether a claim is covered. However, it lacks fullaccuracy and consistency, which are essential inlegal use cases.\nWe then prompted LLMs to convert a legal con-tract into a logical encoding and evaluate coveragequeries with the help of a logic interpreter. In theunguided approach, where the LLM received onlythe contract and claim queries without additionalguidance, we observed poor-quality encodings, of-ten exhibiting ambiguity and inaccuracy, thoughsome LLMs performed better than others.\nThe results of the unguided approach were worsethan those of the vanilla LLM. This poor encodingquality is expected, as these models are not specifi-"}, {"title": "6 Limitations", "content": "Our current approach addresses only a limitedscope and serves as an initial step in a novel di-rection-combining LLMs and logic programs toform a neuro-symbolic AI for legal analysis. Theexperiments in this work are constrained in termsof problem space, architectural design, datasets,logic interpreters used, prompt tuning, measure-ment metrics, and LLMs experimented with.\nOur long-term ambition is to apply a neuro-symbolic approach more broadly in the legal do-main. Currently, we focus only on health insurance-related coverage questions and answers. Furtherapplication areas, such as reasoning about civiland corporate legal terms, remain out of scope butpresent an exciting direction for future work.\nThe architectural design for combining LLMswith logic programs is demonstrated solely throughLLM-generated logic programs, their execution via logic interpreters, and manual evaluation. Thispaper does not address post-training fine-tuning, adapter layers, retrieval-augmented generation, knowledge injection, or reinforcement learning,which remain areas for future work.\nThe experiments process only a narrow set ofpolicies, questions, and answers in terms of data. In future work, a broader range of cases shouldbe explored to gain deeper insights into the perfor-mance of the demonstrated approaches. Addition-ally, we included only a subset of available LLMs in the analysis and focused solely on Prolog as a logic interpreter. Future work should incorporate a wider variety of models and interpreters to enhancegeneralizability and robustness.\nRegarding prompt tuning, we applied only an ex-plicit Chain-of-logic Prolog encoding, derived fromlearning on the Stanford CodeX Insurance Analyst.Future work should explore additional encodingstrategies, such as self-ask decomposition-basedreasoning, iterative refinement, or reinforcementlearning with thought tracing.\nIn this paper, we focused on accuracy and con-sistency measurements while qualitatively high-lighting certain aspects of explainability and au-ditability without formal evaluation. Future workshould incorporate a broader set of metrics to pro-vide a more comprehensive assessment of the per-formance gains in neuro-symbolic AI designs."}, {"title": "A.4.1 GPT-40 Encoding of FSH Criteria in ART Coverage", "content": "validate_day_3_fsh(C) :-\n% Get patient age at time of service\npatient_age_at_time_of_service(C, Age),\n(Age < 40 -> % For patients under 40\nmost_recent_day_3_fsh_test(C, fsh_test(FSHLevel, _)),\nFSHLevel < 19\n;\nAge >= 40 -> % For patients 40 and above\nday_3_fsh_tests_since_age_40_in_claim(C, FSHTests),\n\\+ exists_test_with_level_at_least_19(FSHTests))."}, {"title": "A.4.2 OpenAI 01 Encoding of FSH Criteria in ART Coverage", "content": "meets_fsh_criteria(C) :-\npatient_age_at_time_of_service(C, Age),\n(\n (Age < 39 ) ->\nmost_recent_day_3_fsh_test(C, fsh_test(Level, _, _)),\nLevel < 19\n;\n (Age = 39 ) ->\nclaim_date_service_received(C, ServiceDate),\nmost_recent_day_3_fsh_test(C, fsh_test(Level, TestDate, _)),\nleq_n_months_diff(TestDate, ServiceDate, 6),\nLevel < 19\n;\n (Age >= 40 ) ->\n(\n claim_date_service_received(C, ServiceDate),\n claim_patient_has_premature_ovarian_failure (C, yes) ->\n most_recent_day_3_fsh_test(C, fsh_test(Level, TestDate, _)),\n leq_n_months_diff(TestDate, ServiceDate, 6),\n Level < 19\n ;\n day_3_fsh_tests_since_age_40_in_claim(C, Tests),\n \\+ exists_test_with_level_at_least_19(Tests),\n most_recent_day_3_fsh_test(C, fsh_test(_, TestDate, _)),\n leq_n_months_diff(TestDate, ServiceDate, 6)\n)\n)."}]}