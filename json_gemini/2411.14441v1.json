{"title": "GeMID: Generalizable Models for IoT Device Identification", "authors": ["Kahraman Kostas", "Rabia Yasa Kostas", "Mike Just", "Michael A. Lones"], "abstract": "With the proliferation of Internet of Things (IoT) devices, ensuring their security has become paramount. Device identification (DI), which distinguishes IoT devices based on their traffic patterns, plays a crucial role in both differentiating devices and identifying vulnerable ones, closing a serious security gap. However, existing approaches to DI that build machine learning models often overlook the challenge of model generalizability across diverse network environments. In this study, we propose a novel framework to address this limitation and evaluate the generalizability of DI models across datasets collected within different network environments. Our approach involves a two-step process: first, we develop a feature and model selection method that is more robust to generalization issues by using a genetic algorithm with external feedback and datasets from distinct environments to refine the selections. Second, the resulting DI models are then tested on further independent datasets in order to robustly assess their generalizability. We demonstrate the effectiveness of our method by empirically comparing it to alternatives, highlighting how fundamental limitations of commonly employed techniques such as sliding window and flow statistics limit their generalizability. Our findings advance research in IoT security and device identification, offering insights into improving model effectiveness and mitigating risks to IoT networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The Internet of Things (IoT) seamlessly integrates our cyber world with the physical world and is becoming increasingly prevalent in daily life. According to published statistics, the number of IoT devices has exceeded 15 billion and is projected to reach approximately 30 billion by 2030 [1]. Despite the rapid proliferation of devices and providers, security remains a critical issue. IoT devices can be more challenging to secure than conventional devices due to their diverse hardware, software, and varied manufacturer profiles [2]. Moreover, unlike conventional devices, many IoT devices have non-standard interfaces which impair user interaction and make it difficult to mitigate against vulnerabilities [3]. The NETSCOUT Threat Intelligence Report [4] indicates that a new IoT device on the network typically faces its first attack within 5 hours and becomes a specific attack target within 24 hours, and that most attacks exploit vulnerabilities in IoT devices [5]. Beyond being targets in attacks, compromised IoT devices can serve as tools in botnet attacks, exemplified by Mirai, URSNIF, and BASHLITE [6], where captured devices are used to orchestrate high-volume Distributed Denial of Service (DDoS) attacks. While various methods have been developed to detect and mitigate ongoing attacks, primarily Intrusion Detection Systems (IDS), proactive measures such as device-specific updates, internet access restrictions or isolation can prevent these attacks before they happen. Given the variety, quantity and unfamiliar interfaces of IoT devices, it is impractical to rely on users to implement these solutions. As a result, DI systems have been developed to automatically identify devices based on their activity and make it possible to apply appropriate security measures to create a secure IoT ecosystem.\nMost existing approaches to DI involve the construction of Machine Learning (ML) models. When developing ML models, it is important to ensure that they generalize beyond the specific environment in which they were trained. In DI, this means that models trained on data from one device should be able to detect other devices of the same make and model operating within different network environments. Achieving this requires accounting for the domain shifts and variations typical of network and IoT environments [7]. In this regard, a prominent limitation of previous work is that most studies have relied upon a single dataset collected in a single network environment for both developing and testing their models [8]\u2013[10]. This has resulted in an incomplete, and potentially misleading, picture of DI generalizability. Where multiple datasets have been used, the focus has been on the generalizability of methodologies and feature sets rather than of model instances [11]\u2013[13].\nIn this study, we explicitly focus on developing DI model instances that are demonstrably generalizable across network environments. We measure this using multiple datasets containing the same devices operating in different network environments. We use a two-stage process to develop the models. The first stage involves feature and model selection, and focuses on identifying device-independent features that do not display over-dependence on their network environment. In the second stage, we use an independent dataset to train device-specific model instances. We then use a further dataset, containing the same devices operating in a different network environment, to evaluate the generalizability of these device-specific models.\nWe make the following contributions to the field of DI:\n1) Comprehensive evaluation of model generalizability: We provide an extensive assessment of ML-based DI models across diverse network environments, highlighting key factors that influence generalizability.\n2) Novel study framework for assessing generalizability: We introduce a two stage framework specifically"}, {"title": "II. RELATED WORK", "content": "The field of IoT DI has been developing in earnest since 2017. Among the pioneering works is IoTSentinel [14], which builds models using 23 features extracted from packet headers. Packet headers, analogous to envelopes for data traveling over a network, contain information for routing and processing the data. By analyzing these headers, various useful features for network tasks can be extracted.\nExtractable features from packet headers include source/destination IP/MAC addresses, protocol, Time-to-Live (TTL), flag information, port numbers, sequence/acknowledgment numbers, and checksum. A significant contribution of this study was the introduction of Aalto University IoT device captures [15], one of the most widely used open-access datasets in the IoT DI domain. Subsequent studies, such as IoTSense [9], expanded the feature sets by incorporating payload features (such as payload size and entropy) alongside header-derived features. Concurrently, other research focused on statistics derived from headers, emphasizing the relationship between packets within a specified range (e.g., TTL [8], packet size [16], time [16].\nIn IoTSentinel and IoTSense, features are derived from individual packet headers, but ML models are trained using features collected from multiple packets, known as fingerprints. These fingerprints consist of 12 packets in IoTSentinel and 20 packets in IoTSense. Despite using multiple packets, these fingerprints simply concatenate individual packet features, rather than generating inter-packet features. Alternatively, SysID [10] and IoTDevID [13] create fingerprints based on individual packet features, avoiding the need for merged data features. Obtaining fingerprints from an individual packet addresses the transfer problem that occurs when multiple devices share the same MAC address, a common issue due to the use of MAC addresses in the preassembly of packets [13].\nDeriving features from flow data instead of packet headers is also common. A flow refers to a sequence of packets sent from a specific source to a specific destination, considered as part of a single session or connection. The size of the flow can be a predetermined number of packets or packets within a specific time/protocol interval. These features can include basic flow information (source, destination, protocol), time-related metrics (start-end time, duration), packet and byte counts (total, average, max, min sizes), rate metrics (packet, byte, flow rate), flag statistics, inter-arrival times (average, max, min), and flow direction indicators (packets/bytes sent between source and destination). Sivanathan et al. [17] used features obtained from network flows, such as flow volume, flow duration, and average flow rate. They also introduced the widely used UNSW-DI dataset. Many other DI studies have also used flow features [18], [19].\nFeatures can also be obtained from packet headers or flows through sliding windows of various sizes. For example, features such as port ranges, packet size, packet quantity, availability time and inter-arrival time can be subjected to statistical analysis within the window to derive features such as maximum, minimum, mean and standard deviation [20]. This inherently involves features based on multiple packets. However, an issue with features derived from flow/network statistics and windowing methods is that they are likely to capture implicit characteristics of the network environment, rather than just the characteristics of individual devices. In this paper, we argue that this makes them fragile, and demonstrate that they are not a reliable basis for carrying out DI.\nThe use of raw data is also possible. As shown in Fig. 1, a network packet can be represented as raw bytes. In this approach, the first n bytes of the packet are provided as input to the ML models. Instead of being used explicitly as features, this data is fed into neural network-based learning models, in particular Convolutional Neural Networks (CNNs), where each byte is treated as an image pixel [21]\u2013[25]. If the number of bytes exceeds n, truncation is applied; if less, padding is used. This method is conceptually similar to the packet header method; but its fields are not strictly separated.\nHowever, using raw data for DI presents other problems that have not yet been adequately addressed. In studies where the entire raw packet data was used [22], [24], this included headers, payloads, and metadata, making it difficult to filter out identifying information such as MAC/IP addresses or string identifiers, both of which can significantly impair model generalizability. Whilst this dependency can be reduced by solely using packet payloads [23], [26], caution is needed because much of today's internet traffic is encrypted. In practice encrypted payloads are likely to be ineffective for device identification models as the data appears random and opaque, providing no useful information about the contents. Thus, the added complexity of raw data is unlikely to contribute to the model's performance and may even hinder it.\nA more general issue with existing DI studies is the lack of cross-dataset validation, which involves evaluating a model trained on one dataset against a different dataset to assess its generalizability and robustness. While some studies use multiple datasets [11]\u2013[13], they typically apply their methods separately to each, training and testing within the same dataset (e.g., training on a portion of Dataset A and testing on the remainder). This approach raises potential concerns about their performance in unseen environments. It also exacerbates the specific issues raised above surrounding the use of statistical features, since models using these features would in effect be tested within the same network environment as the one they were trained in. In this study, we demonstrate the importance of cross-dataset validation in developing generalizable models."}, {"title": "III. MATERIALS AND METHODS", "content": "DI research typically relies on datasets comprising data collected from real devices. These datasets commonly feature over 20 devices and necessitate prolonged observation periods to gather sufficient data. However, the data collection process for IoT devices entails substantial resource allocation, including personnel and space, making it burdensome. Consequently, many researchers opt to utilize publicly available DI datasets, thereby enhancing reproducibility and transparency and facilitating comparative analysis.\nIn our study, we use data from two dataset families, UNSW and MonIoTr. Each family consists of multiple datasets that are useful for our model building and evaluation. We use the UNSW datasets for feature and model selection, and the MonIoTr datasets to test the robustness and generalizability of the resulting models.\n1) UNSW: For feature and model selection, we utilized the IoT Attack Traces-ACM SOSR 2019 (UNSW-AD) [27] and IoT Traffic Traces-IEEE TMC 2018 (UNSW-DI) [17] datasets. The UNSW-DI dataset is tailored for DI tasks, comprising data from 24 benign devices collected over a 60-day period in 2016. In contrast, the UNSW-AD dataset, originally designed for anomaly detection (AD), contains 27 days of benign data and 17 days of both benign and malicious data from 28 devices collected over a 44-day period in 2018. For the UNSW-AD dataset, we used only the benign data.\nNotably, despite both datasets containing most devices in common (see Fig. 2), they were collected at different times, in distinct site environments with varying network characteristics, and for disparate purposes, likely by different users. We leverage these datasets for feature and model selection, employing UNSW-DI as training data and UNSW-AD as test data, and vice versa in different runs/iterations, to enhance model robustness and generalizability.\nEach dataset contains a number of sessions with one session per day (UNSW-DI: 60 sessions, UNSW-AD: 27 sessions). The resultant datasets are very large, and some sessions lack certain devices. To address this we selected and merged some sessions to maximize the number of devices available, and then used these merged sessions. Specifically, in the DI dataset, sessions 16-10-03 and 16-11-22 were combined to form S1, while sessions 16-09-29 and 16-11-18 were combined to form S2 (names are in yy-mm-dd date format). In the AD dataset, sessions 18-10-13 and 18-06-14 were combined to form S1, and sessions 18-10-16 and 18-06-11 were combined to form S2.\n2) MonIoTr: After selecting the features and models, to remove any bias, we evaluate their effectiveness on a different dataset, IoT Information Exposure-IMC'19 (MonIoTr) [28]. The MonIoTr dataset consists of 81 devices. An important characteristic of this dataset is that data collection took place at two separate sites, one in the UK (MonIoTr-UK) and the other in the USA (MonIoTr-USA). Of the 81 devices, 26 devices were present in both sites, with the USA site having 47 devices (including 21 unique devices) and the UK site having 34 devices (including 8 unique devices). Fig. 3 shows the distribution of devices by country.\nData collection in these sites spanned 112 hours, covering various scenarios:\nIdle Data collected during an 8 hour period at night when devices were inactive.\nPower Data collected for 2 minutes immediately after device power-on, without interaction.\nInteraction Data collected through various interactions, including physical button presses, voice commands, phone app usage on the same network, communication with the device through a phone on a different network utilizing cloud infrastructure, and interaction with the device via Alexa Echo Spot.\nAdditionally, each device was connected via a VPN to the other site, enabling data collection across both sites for every condition. The same conditions were repeated using VPN tools to establish connections between the sites. For example, idle data was collected by connecting to the UK site from the USA via VPN and vice versa.\nMany devices interact heavily with the local network, applications, tools, and cloud services during startup, so we merged power and interaction data into one active class. In a previous study [29], we observed that when both active and idle data are available, active data is much more effective for DI. Consequently, we excluded idle data from this study."}, {"title": "IV. FEATURE SELECTION", "content": "During the feature selection phase, we aimed to identify the most effective features for DI. For this, we used the four UNSW data partitions described in Section III-A1 to guide feature selection. These comprise two sessions (S1 and S2) from each of UNSW-AD and UNSW-DI (referred to hereafter according to their dataset-session number: AD-S1, AD-S2, DI-S1, and DI-S2).\nThe generalizability of features is assessed in the following three ways, in order of increasing strictness:\n5-fold cross-validation (CV) within a partition, which is common in the literature, e.g., within AD-S1.\nSession versus session (SS) where generalizability is measured between two partitions of the same dataset, e.g., AD-S1 vs AD-S2.\nDataset versus dataset (DD) where generalizability is measured between partitions of different datasets, e.g., AD-S1 vs DI-S2."}, {"title": "A. Predictive Features", "content": "First, each feature is evaluated individually using each relevant combination of partitions, leading to 16 distinct evaluation contexts (see Fig. 4). Generalizability is assessed by training a decision tree (DT) model using the feature, with DT selected for speed and explainability. The utility of each feature is measured using the kappa metric, which adjusts for chance agreement.\nFig. 5 shows the resulting utility values. Notably, the scores measured under CV are much higher for some features than those measured under DD and SS scores. This indicates that information leaks may be occurring due to train and test folds being taken from the same partition, and suggests that CV is not a reliable basis for selecting features. Hence we only use DD and SS from now on.\nA voting system was used for feature selection, with each non-zero kappa value (with a tolerance of \u00b15%, so \u2265 0.05), indicating the feature had a positive contribution, resulting in a vote. Fig. 6 illustrates this system. Features receiving four or more votes from either DD (green) or SS (red) categories, with at least one DD vote, advanced to the next stage. Features not meeting this criterion were eliminated. The requirement for at least one DD vote is based on its greater strictness in assessing generalizability. As a result of voting, we identified 46 of the 332 features as being individually predictive."}, {"title": "B. Feature Interactions", "content": "We then considered feature interactions, using a wrapper method, specifically a Genetic Algorithm (GA), to find the optimal combination of individually predictive features which we validated across multiple datasets. The global search characteristic of a GA tends to find good solutions, and avoiding exhaustive search enables us to obtain potential feature sets within an acceptable timeframe. However, in general there is no guarantee of a GA finding an optimal subset, meaning it may include uninformative features in the feature set. Additionally, GA-selected features may develop a bias towards the data used for selection, potentially reducing their performance on other datasets.\nTo address this, all 46 features that passed individual voting were combined into a single feature set for selection using a genetic algorithm (GA). A decision tree (DT) model was employed for evaluation, with the fitness of each generation"}, {"title": "C. Deriving a Final Feature Set", "content": "To identify the most effective combination of features from the GA results, we conducted two post hoc analyses. First, the feature set derived from each DD case (each GA run) is reevaluated on each other DD case. F1 scores of the resulting DT models are presented in the left part of Fig. 8. These show how well the feature sets from each GA run generalize.\nThe second analysis uses a voting mechanism based on intersections of the eight GA feature sets (Fig. 7), with a feature receiving votes proportional to how often it appears. Features are then grouped based on thresholds, i.e., whether they appear in 2 or more feature sets (Vote+2), 3 or more (Vote+3) etc., and DT models are trained using these feature groups. The results are shown on the right side of Fig 8.\nFrom the mean F1 scores shown in the final row of Fig. 8, it can be seen that feature sets based on grouping generally lead to better performance, at least for voting thresholds up to 4. For larger thresholds, the feature sets become very small, explaining the lower scores. Generally, the feature sets from single runs lead to models with lower F1 scores, suggesting that voting adds more robustness. Consequently, we use the Vote+3 feature set which has the highest F1 score in the next section. The selected features are highlighted in Fig. 7."}, {"title": "V. MODEL EVALUATION", "content": "Next, we construct and evaluate ML models using the selected features and compare their performance against established baselines using an independent dataset."}, {"title": "A. Model Selection", "content": "We experimented with various ML modeling approaches commonly used in DI to identify the most effective one for this purpose, in terms of both predictive success and inference time \u2013 the latter being an important consideration when scanning network packets. The models we considered were Logistic Regression (LR), Decision Trees (DT), Naive Bayes (NB), Support Vector Machines (SVM), Random Forest (RF), Extreme Gradient Boosting (XGB) Multi-Layer Perceptron (MLP), K-Nearest Neighbors (KNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Bidirectional Encoder Representations from Transformers (BERT).\nWe used random search for hyperparameter optimization and applied each model to the DD datasets see Supplementary Material (SM) Section 4.4 for details. The F1 scores and average inference times are shown in Fig. 9. From this, it is clear that the most successful models are RF and XGB, with F1 scores of 0.799 and 0.780, respectively. While their predictive performance is similar, RF runs about 6 times faster than XGB, so we chose this as our preferred model. Notably, DT has the fastest inference time, but its predictive performance lags behind RF by about 10 percentage points."}, {"title": "B. Evaluating Model Generalizability", "content": "In this section, we measure the generalizability of our method (GeMID) and other methods using the MonIoTr dataset, using the US, US-VPN, UK, and UK-VPN partitions of the MonIoTr dataset within three different evaluation contexts. In CV, we carry out cross-validation within a single partition. In SS, we train models using non-VPN data and test them using VPN data, and vice versa. In DD, we use data collected from one geographical site to train models, and data collected from the other geographical site to test them.\nAs comparison baselines, we use flow-based features generated by CICFlowmeter [30] and window-based features generated by Kitsune [31]. These are well-known tools that were used in previous DI studies. To ensure a fair comparison, feature and model selection were repeated independently (see SM Section I and II for results). We also compare against IoTDevID [13], another packet header-based DI method that aimed to build generalizable DI models and demonstrated improved performance over alternative methods."}, {"title": "VI. LIMITATIONS", "content": "The primary limitation of this work is the availability and quality of public datasets. Although we used well-regarded datasets and cross-dataset validation, there will always be a limit in terms of the diversity, representativeness and sampling biases of any dataset.\nFuture work in DI would benefit from larger datasets designed specifically for this purpose, containing device samples that reflect realistic usage situations. Whilst we already consider devices situated in two different network environments, collecting data from the same devices operating in a broader range of networks would inevitably provide a broader perspective on generalizability. There is also a need for more data from non-IP protocols such as ZigBee and Z-Wave to test how well DI models of non-IP devices generalize.\nWhilst this study (in common with previous studies) focuses on benign data, future DI studies would also benefit from using attack or malicious data, since this would be more representative of the actual environments within which DI models are deployed."}, {"title": "VII. CONCLUSIONS", "content": "Our work presents a novel approach to improving the generalizability of IoT DI models. Using a two-stage framework involving feature and algorithm selection followed by cross-dataset validation, we show that models trained on datasets in one environment can effectively identify devices in another environment. This addresses an important gap in existing methodologies, which often do not validate across different datasets.\nComparative analysis of DI methods showed that packet header-based features performed better than network statistics and window methods. Packet features offer a more consistent and reliable basis for model training because they are less influenced by network-specific variables. This finding highlights the importance of selecting features that inherently capture device-specific attributes.\nOur results also highlight the limitations of relying only on single datasets for model training and validation. Such approaches run the risk of overfitting and may fail to generalize to different network environments. In contrast, our methodology provides robustness and adaptability, which are crucial for practical IoT security applications."}]}