{"title": "FAST PROXIES FOR LLM ROBUSTNESS EVALUATION", "authors": ["Tim Beyer", "Jan Schuchardt", "Leo Schwinn", "Stephan G\u00fcnnemann"], "abstract": "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expensive. We compare the ability of fast proxy metrics to predict the real-world robustness of an LLM against a simulated attacker ensemble. This allows us to estimate a model's robustness to computationally expensive attacks without requiring runs of the attacks themselves. Specifically, we consider gradient-descent-based embedding-space attacks, prefilling attacks, and direct prompting. Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p = 0.87$ (linear) and $r_s = 0.94$ (Spearman rank) correlations with the full attack ensemble while reducing computational cost by three orders of magnitude.", "sections": [{"title": "INTRODUCTION", "content": "As the capabilities of large language models advance, ensuring their robustness and reliability becomes increasingly critical. To this end, frontier models undergo extensive adversarial testing and red-teaming to identify vulnerabilities before deployment (OpenAI, 2023; Dubey et al., 2024).\nHowever, state-of-the-art red-teaming methods are computationally expensive, as finding adversarial prompts is a challenging combinatorial optimization problem over discrete natural language. Here, model-agnostic approaches require prohibitive computational resources (Zou et al., 2023; Chao et al., 2023), whereas more efficient attack algorithms tend to be model-specific and struggle to transfer across architectures (Liao & Sun, 2024). Moreover, reliable red-teaming with strong attacks still demands significant manual effort in tailoring the attack algorithm to a specific model (Andriushchenko et al., 2024; Li et al., 2024). As a result, large-scale red teaming approaches require thousands of GPU hours (Samvelyan et al., 2024), making thorough safety evaluations prohibitively expensive in most research settings.\nTo address this problem, we propose a scalable alternative: low-cost proxies for real-world threat models. These proxies enable LLM robustness evaluation without needing to run highly expensive automated attack suites against the model. As an example of such an attack suite, we use a \"synthetic red-teamer\" ensemble comprising six distinct LLM attack methods, which we evaluate on 33 open-source models across 300 harmful prompts. We leverage substantial computational resources and aggregate more than 7M jailbreak attempts. The data suggest that model robustness in adversarial settings can be predicted through inexpensive approaches.\nOur main contributions are as follows:\n\u2022 We investigate whether inexpensive proxies including direct prompting, prefilling, and embedding space attacks can predict robustness against strong adversarial red teaming.\n\u2022 We demonstrate that robustness can be predicted within model families (e.g., different Llama 3 versions) and across model families (e.g., Llama and Mistral).\n\u2022 Finally, we show that by estimating the most robust model checkpoint during training, proxy attacks can aid adversarial model alignment across different training regimes(e.g., circuit breaking or adversarial training)."}, {"title": "SYNTHETIC RED-TEAMER", "content": "To emulate a strong attacker, we create a synthetic red-teamer by ensembling six common attack algorithms (listed in Table 1). All attacks are run using the recommended hyperparameters (see also"}, {"title": "PROXY METHODS", "content": "We aim to find an inexpensive and fast approach that can reliably predict a model's real-world robustness. Finding such a proxy for robustness could dramatically reduce the cost of robustness evaluations, make it easier to compare models across and within families, and efficiently select promising checkpoints during defense training. To this end, we consider three candidate approaches:\nEmbedding Space Attacks. Schwinn et al. (2023; 2024) recently proposed a white box attack that operates in continuous token embedding space, rather than the discrete input vocabulary. This frameworkwhile impractical for real-world attacks, where most threat models assume a black box setting with string-level inputprovides an extremely fast way to attack models in a white box setting, and can be used e.g., to adversarially train LLMs (Xhonneux et al., 2024).\nPrefilling. Prefilling attacks (Vega et al., 2023; Andriushchenko et al., 2024) rely on injecting a prefix to the beginning of the victim model's response to the harmful prompt typically using an affirmative response prefix. As this level of access is also provided by some private models (e.g., the Claude family (Anthropic, 2024)), it represents a realistic attack vector even for hosted models.\nDirect. Direct prompting is the simplest possible baseline: We simply use an unmodified harmful prompt from the dataset and sample a single greedy generation, which is then judged."}, {"title": "EXPERIMENTAL EVALUATION", "content": "We conduct experiments to determine how well the attack success rates of inexpensive proxy methods (direct ASR, prefilling ASR, embedding-space ASR) predict robustness against real-world red-teaming approaches, which we simulate using our strong synthetic red-teamer from Section 2 across various training and attack scenarios. In addition to directly comparing the different ASRs, we compute Pearson correlation ($r_p$) to quantify linear correlation between proxy ASR and ensemble ASR. We further compute Spearman ($r_s$) and Kendall rank ($\\tau$) correlation to understand whether the order of any two models w.r.t. proxy ASR is predictive of their order w.r.t. ensemble ASR. For the full details of our experimental setup, see Appendix B. For additional results see Appendix C."}, {"title": "COMPARING WITHIN-FAMILY MODELS", "content": "Popular base models are often fine-tuned for particular use cases, such as chatting (Tunstall et al., 2023), helpfulness (Zhu et al., 2023), or tool use (Teknium et al., 2024). We are interested in comparing the safety of several post-trained model versions. In Figure 1, we evaluate different derivatives of Llama 3 8B Instruct. Spearman and Kendall rank correlation coefficients $r_s$ and $\\tau$ of direct prompting are greater or equal than those of the other proxy attacks. We observe that direct ASR is close to 0 for multiple models, which impedes a good linear fit ($r_p$ of 0.62) between direct ASR and ensemble ASR. This $r_p$ is smaller than those of prefilling and embedding space attacks. Thus, even for within-family comparisons, the simplest and fastest attack appears like a suitable choice as a proxy for computationally expensive red-teaming."}, {"title": "COMPARING ACROSS MODEL FAMILIES", "content": "We also investigate whether proxy methods can be used to predict the success rate of expensive red-teaming attacks on newly introduced model families. In Fig. 2, each point corresponds to a specific model from one of six model families (Gemma 2 (Team et al., 2024), Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023), Phi-3 (Abdin et al., 2024), Llama 3 (Dubey et al., 2024), Llama 2 (Touvron et al., 2023)). Prefilling and embedding space attacks often have much higher ASR than direct prompting, which na\u00efvely use the harmful prompt without any modification. Direct ASR is generally below 5%, except for models that are extremely unrobust (ensemble ASR close to 100%). Thus, the pairs of direct and ensemble ASR do not admit a linear fit and the Pearson correlation $r_p$ is small. However, the rank correlation coefficients of direct prompting ($r_s = 0.94$, $\\tau = 0.83$) are higher than those of the other two proxy methods ($r_s = 0.79$, $\\tau = 0.61$) and ($r_s = 0.90$, $\\tau = 0.73$)."}, {"title": "ASSESSING EFFECTIVENESS OF ROBUSTNESS FINE-TUNING", "content": "A standard method for increasing model robustness is via post-training/fine-tuning approaches, e.g., via circuit breaker training (Zou et al., 2023) or continuous adversarial training (Sheshadri et al., 2024; Xhonneux et al., 2024). In Fig. 3 & 7, we assess whether proxy ASR can potentially be used to predict ensemble ASR after fine-tuning for a specific number of steps, rather than performing computationally expensive red-teaming for every possible value of this hyper-parameter. Specifically, we apply circuit breaker training to Llama-3-8B-Instruct and vary the number of training steps between 1 and 300. Again, while the relation between proxy ASR and ensemble ASR is generally monotonic and linear for all three proxies, direct prompting achieves significantly higher ranking correlations $r_s$ and $\\tau$."}, {"title": "SCALING TRENDS", "content": "We find that the effectiveness of different proxy methods varies with the amount of prompts used (Fig. 4). Prefilling and embedding space attacks attain universally higher Pearson correlation, i.e., admit a better linear fit irrespective of the number of prompts. They can also reach higher Spearman and Kendall ranking correlation but only when using few prompts. For 50 or more prompts, direct prompting yields higher ranking correlation coefficients. This can be explained as follows: Since"}, {"title": "LIMITATIONS", "content": "While we conducted an exhaustive and computationally intensive evaluation using six attacks and 33 models from the sub-10B parameter class, our experiments should be further validated to ensure they generalize to other attack algorithms and model sizes."}, {"title": "CONCLUSION", "content": "We investigated the effectiveness of inexpensive proxy attacks in predicting LLM robustness against adversarial red-teaming. Our results highlight key trade-offs between different proxy methods. Direct prompting is a strong baseline for ranking models by robustness across diverse scenarios (within-family, cross-family, safety fine-tuning), provided that enough (> 50) prompts are used. Embedding-space attacks provide better ranking at low prompt count and better linear fits, while prefilling attacks are generally inferior to the two alternatives. Overall, our results showcase that efficient proxy attacks are a promising direction for future research towards making foundation models more responsible without incurring unjustifiable computational overhead."}]}