{"title": "Quantum-enhanced causal discovery for a small number of samples", "authors": ["Yota Maeda", "Ken Arai", "Yu Tanaka", "Yu Terada", "Hiroshi Ueno", "Hiroyuki Tezuka"], "abstract": "The discovery of causal relationships from observed data has attracted significant interest from disciplines such as economics, social sciences, epidemiology, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are often associated with nonlinear causal structures, which make the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not assume any underlying model structures. Based on the independence conditional tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed qPC algorithm can explore causal relationships from the observed data drawn from arbitrary distributions. We conducted extensive and systematic experiments on fundamental graph parts of causal structures, demonstrating that the qPC algorithm exhibits a significantly better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the proposed quantum algorithm can empower classical algorithms for robust and accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. Additionally, the effectiveness of this method was validated using the Boston Housing dataset as a real-world application. These findings demonstrate the new potential of quantum circuit-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios where traditional approaches have shown limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "Deciphering causal relationships among observed variables is a crucial problem in the social and natural sciences. Historically, interventions or randomized experiments have been employed as standard approaches to evaluate causality among observed variables [1]. For example, randomized controlled trials have been commonly used in clinical research to assess the potential effects of drugs. However, conducting such interventions or randomized experiments is often difficult because of ethical constraints and high costs. Alternatively, causal discovery offers effective methods for inferring causal relationships among variables from passively observed data beyond correlation analysis [2-6]. The Peter-Clark (PC) algorithm [2], a widely accepted algorithm for causal discovery, yields an equivalence class of directed acyclic graphs (DAGs) that captures causal relationships (see Fig. 1 (a) for an overview of the PC algorithm). The PC algorithm does not assume any specific statistical models or data distributions, unlike the other methods, including the linear non-Gaussian acyclic model (LiNGAM) [7, 8], NOTEARS [9], the additive noise model [10], the post nonlinear causal model [11], and the GES algorithm [12]. Thus, applications of the PC algorithm and its variants have elucidated causal relationships from various observed data spanning from natural science to engineering [13-16]. In the PC algorithm, kernel methods can be used for conditional independent tests, a process known as kernel-based conditional independence test (KCIT) [17, 18]. This approach enables applications to various types of data, including those characterized by nonlinearity and high dimensionality [14, 18, 19]. Although the PC algorithm using KCIT can be applied to both linear and nonlinear data without any assumption of the underlying models, its performance depends on the choice of kernels. Empirically, kernels are often chosen from representative classes such as Gaussian, polynomial, and linear kernels [20]. Alternatively, quantum models that embed data in an associated RKHS have been developed recently, providing a class of algorithms called quantum kernel methods [21-25] (Fig. 1 (b)). Among them, the kernel-based LiNGAM extended with quantum kernels [25] demonstrates potential advantages over classical methods, such as accurate inference with small sample sizes [26], as suggested in supervised learning contexts [27]. However, the quantum LiNGAM (qLiNGAM) [25] assumes linear causal relationships, which limits its applicability to real-world problems. While quantum causal inference for small sample data presents a promising alternative, it still faces challenges. First, existing quantum models have failed to address nonlinear causal relationships. Second, similar to classical kernels, the"}, {"title": "II. RESULTS", "content": "We propose the qPC algorithm for causal discovery that employs quantum kernel methods [21], which embed classical data into quantum states (Fig. 1 (c)). The qPC algorithm is an extension of the PC algorithm for causal inference. It utilizes a conditional independence test implemented via the KCIT with quantum kernels composed of data-embedded quantum states as a natural extension of the Gaussian kernel. The PC algorithm [2, 30] offers CPDAGS that capture the causal relationships between variables from their observed data (Appendix B). This algorithm is a nonparametric method that does not consider underlying statistical models. The KCIT is introduced because of its powerful capacity to infer causality in data with nonlinearity and high dimensionality.The qPC algorithm is twice the original PC algorithm: discrimination of unconditional/conditional independence and orientation of causality relations (see the overview of the PC algorithm in Appendix B). The qPC algorithm outputs CPDAGs, which capture the causal relations among the observed variables with directed and undirected edges between them (Fig. 1 (a)). It relies on the KCIT framework (see Appendix A for the details of the KCIT), where the original data are embedded into feature spaces to detect independence (Fig. 1 (b)). Appropriate embedding in KCIT facilitates the disentangling of complex nonlinear relations in the original data space, which often leads to accurate results in statistical hypothesis tests, especially when dealing with high-dimensional or nonlinear data [17, 18]. The qPC algorithm leverages quantum kernels associated with the quantum state to embed data into the RKHS defined by quantum circuits. Quantum kernels are defined by $k_\\theta(x, x') = Tr[\\rho(x)\\rho(x')]$, where input x is encoded into the quantum circuits generating state $\\rho(x)$. It defines a method for encoding data into quantum circuits. Our proposed quantum circuit has hyperparameters analogous to the widths of the Gaussian kernels."}, {"title": "B. Detection of fundamental causal graph structures", "content": "To demonstrate how the qPC algorithm can effectively retrieve the underlying causal structures, we applied it to synthetic data from fundamental causal relations with three nodes, collider, fork, chain, and independent structures (Fig. 2 (a)) [1]. These elements capture any local part of the general causal graphs, and thus provide a summarized assessment of causal discovery methods. In particular, we assume that source random variables are generated through observations in quantum circuits with random variable inputs and that the other nodes receive their inputs through a relation defined by the function f and noise, such as $Z = f(X,Y) + \\epsilon$ (Fig. 2 (b)). Specifically, random values $x_i$ sampled from Gaussian distributions were used as inputs to the data embedder of the quantum circuit, and we measured observables $M_\\alpha$, that is, $M_\\alpha = Tr[O_\\alpha \\rho(x_i)]$, $O_\\alpha = (\\sigma_\\alpha + I)/2$, $\\alpha \\in \\{x,z\\}$. We then prepared a dataset for causal discovery using an algebraic operation of the measured values. Consequently, the data distribution is far from a typical probability distribution such as a Gaussian distribution (see Fig. 2 (b)). This setting aims to highlight that under such data generation processes, the quantum kernel can be typically superior to classical kernels in accurately reproducing the underlying causal structures. Because the qPC or PC algorithm yields CPDAGs, we evaluate the accuracy by considering Markov equivalence; the fork and chain should not be distinguished in this case.Comparisons of the performances of the classical PC and qPC algorithms for causal junctions are shown in Fig. 2 (c). For chain or independent structures, we observe no significant differences between the classical and quantum methods. However, for the collider or fork, the quantum kernel outperformed the classical kernel for small sample sizes. The results of the performance comparison may be questionable because the fork and chain are Markov equivalent. However, because random variable Z constructed from the quantum circuit occupies different positions in the fork and chain, the difficulty of the independence and conditional independence tests in the PC algorithm varies between the two cases. The superior performance of the qPC algorithm may have resulted from the inductive bias of the models. The data generation process is based on the observation of quantum circuits, which can be related to the quantum kernels used. In the following sections, we investigate more general cases using datasets unrelated to quantum models."}, {"title": "C. Optimization of quantum circuits via KTA", "content": "In the previous subsection, we experimentally confirmed that quantum kernels with small sample sizes are effective for causal discovery. We used artificial data generated from quantum circuits that are considered suitable for quantum kernels. However, na\u00efve quantum kernels are not suitable for classical data in general. Specifically, the qPC algorithm has one main challenge: in contrast to the classical Gaussian kernel, which has several established guidelines for determining the kernel hyperparameters, the quantum kernel method lacks a standardized approach for selecting its hyperparameters for inference [28]. Thus, we propose a systematic method for adjusting the hyperparameters in quantum circuits for datasets. To demonstrate the applicability of the qPC algorithm to a wide range of data, we compare the performance of the two methods using artificial datasets with classical settings.Herein, we briefly explain an optimization method for determining the hyperparameters of quantum circuits for kernels based on the normalized Hilbert-Schmidt inner product (HSIP). Its expectation value is zero if and only if random variables X and Y are independent. This property allows for the use of HSIP as test statistics in statistical hypothesis tests [17, 18]. The hypothesis test should be improved by selecting a kernel that minimizes the HSIP for uncorrelated data samples while maximizing the HSIP for correlated data samples; in principle, HSIP approaches zero in the uncorrelated case and nonzero otherwise. The normalized HSIP (\\sigma), which measures the distance between the feature vectors in which two data samples are embedded, is called KTA [31]. From the perspective of statistical hypothesis testing, KTA minimization for uncorrelated data reduces the false-positive (FP) risk, whereas KTA max-imization for correlated data reduces the false-negatives (FN) risk. Thus, KTA minimization can be interpreted as enhancing the identifiability of two independent random variables, thereby reducing the likelihood of Type-I errors. In contrast, KTA maximization reduces the identifiability of dependent random variables, thereby lowering the likelihood of Type-II errors. Here, we focus on KTA minimization for uncorrelated data because the actual relationships behind the data are often unavailable, making it difficult to employ the KTA maximization strategy.To evaluate the performance of the qPC algorithm using our optimization method, we conducted an experiment in which the data were drawn from a classical setting with the same three fundamental causal graphs as those in Fig. 2. Figure. 3 (a) shows the typical behaviors of the KTA and quantum scaling parameter during the optimization process and the difference in statistics between the default and optimized kernels. Through optimization, the KTA"}, {"title": "D. Application of the qPC algorithm to real-world data", "content": "Here, we demonstrate the application of the qPC algorithm and our optimization method to real-world data. We used the datasets on the Boston housing price [29]. In the optimization, we sought a suitable scaling parameter by minimizing the KTA for the independent distributions obtained by shuffling the original data.The results of applying the classical PC and qPC algorithms to the Boston housing data are presented in Fig. 4. Panel (a) shows the marginal distributions for the selected variables, most of which deviate from Gaussian or other conventional distributions. Using the classical PC with KCIT for the full sample data (N =?), we obtained the CPDAG shown in Fig. 4 (b), which captures reasonable causal relations among the variables. However, the small sample size obscures the causal relations between them, and the PC algorithm failed to reconstruct the CPDAG under the same conditions, such as the level of significance, as shown in Fig. 4 (c). The qPC algorithm with the optimized scaling parameters is still capable of providing a more comprehensive estimate of causality, as shown in Fig. 4 (d), where it detected the potential causes of the price denoted as the MEDV node. The closeness between the results of the PC with full samples and those of the qPC with a small part of the whole sample set is consistent with our artificial data experiment."}, {"title": "III. DISCUSSION", "content": "We proposed the qPC algorithm for causal discovery by leveraging quantum circuits that generate the corresponding RKHS. Our simulations demonstrated that the qPC algorithm can surpass the classical method in reconstructing the underlying causal relations, particularly with a small number of samples. Furthermore, with no existing method for determining the hyperparameters of quantum kernels, we proposed a method for choosing quantum kernels adaptively for the data. In the proposed method for kernel choice, we employed the KTA to set quantum kernels suitable for causal discovery, thereby decreasing the FP risk for independent cases. We numerically demonstrated that the optimization method can improve the inference results for both synthetic and real data. Our experimental results indicate that even for small sizes, quantum kernels can facilitate accurate causal discovery. This finding indicate that quantum circuits can enhance the performance of existing causal discovery methods and widen the range of their applications in real-world problems.Although our experiments on artificial and real data suggest the superiority of the qPC algorithm for causal discovery with small datasets compared to the classical PC algorithm, it could be further discussed to unveil the principle behind this phenomenon. For small sample datasets, we cannot apply the asymptotic theory of the test statistics shown in the KCIT, making it difficult to expect the independence test to perform as theoretically predicted. Therefore,"}, {"title": "IV. METHODS", "content": "The KCIT [17, 18] is a hypothesis test for null hypothesis X \\amalg Y | Z between random variables X and Y given Z. It was developed as a conditional independence test by defining a simple statistic based on HSIP of two centralized conditional kernel matrices and deriving its asymptotic distribution under the null hypothesis (see Appendix A for details). Unconditional independence statistic $T_{UI}$ is defined as\n$\\begin{equation}T_{UI} := \\frac{1}{n} Tr[K_X K_Y],\\end{equation}$\nwhere $K_X$ and $K_Y$ are the centralized kernel matrices i.i.d. of size n for X and Y. Under the null hypothesis that X and Y are statistically independent, it follows that the Gamma distribution\n$\\begin{equation}p(t) = \\frac{t^{k-1} e^{-t/\\theta}}{\\theta \\Gamma(k)},\\end{equation}$"}, {"title": "Appendix A: Review of the kernel-based conditional independence test", "content": "This section briefly reviews the KCIT [17, 18]. Let us begin with given continuous random variables X, Y, and Z with domains X, Y, and Z, respectively. The probability law for X is denoted by $P_X$. We introduce a measurable, positive definite kernel $k_X$ on X and denote the corresponding RKHS as $H_X$. The space of the square integrable functions of X is denoted by $L_X^2$. $K_X$ is then the kernel matrix of the i.i.d. sample $\\mathbf{x} = \\{x_1,...,x_n\\}$ of X, and $K_X = H K_X H$ is the centralized kernel, where $H := I - \\frac{1}{n} \\mathbf{1}\\mathbf{1}^T$ with I and $\\mathbf{1}$ being the n \u00d7 n identity matrix and the vector of 1's, respectively. Similarly, we define $P_Y, P_Z,k_Y,k_Z, H_Y,H_Z, L_Y^2, L_Z^2, K_Y, K_Z, K_Y, K_Z$ as well.The problem here is to perform the test for conditional independence (CI), i.e., test the null hypothesis X \\amalg Y | Z, between X and Y given Z from their i.i.d. samples. In Refs. [17, 18], a CI test was developed by defining a simple statistic based on two characterizations of the CI [51, 52] and deriving its asymptotic distribution under the null hypothesis.One characterization of the CI is provided in terms of cross-covariance operator $\\Sigma_{XY}$ in the RKHS [51]. For random vector (X, Y) on X \u00d7 Y, cross-covariance operator $\\Sigma_{XY}$ is defined by the following relation:\n$\\begin{equation}\\langle f, \\Sigma_{XY} g \\rangle = E_{XY} [f(X)g(Y)] - E_X [f(X)] E_Y [g(Y)]\\end{equation}$\nfor all $f \\in H_X$ and $g \\in H_Y$.\nLemma A.1 (Theorem 3 (ii) of Ref. [51]). Denote $\\tilde{X} = (X,Z)$ and $k_{\\tilde{X}} = k_X k_Z$. Assume that $H_{\\tilde{X}} \\subset L_{\\tilde{X}}^2$, $H_Y \\subset L_Y^2$, and $H_Z \\subset L_Z^2$. Furthermore, assume that $k_{\\tilde{X}} k_Y$ is a characteristic kernel on (X \u00d7 Z) \u00d7 Y and $H_Z + \\mathbb{R}$ is dense in $L^2(P_Z)$. Then,\n$\\begin{equation}\\Sigma_{X Y | Z} = 0 \\iff X \\amalg Y | Z.\\end{equation}$\nThe other characterization of CI is given by explicitly enforcing the uncorrelatedness of functions in suitable spaces.\nLemma A.2 ([52]). The following conditions are equivalent to each other:\n$\\begin{equation}X \\amalg Y | Z \\iff E [f'g'] = 0,\\forall f' \\in E_{XZ},\\forall g' \\in E'_{XZ},\\end{equation}$\nwhere\n$\\begin{equation}E_{XZ} := \\{f' \\in L_X^2 | E [f'|Z] = 0\\},\\end{equation}$\n$\\begin{equation}E'_{YZ} := \\{g' | g' = g(Y) - E [g|Z], g\\in L_Y^2 \\}.\\end{equation}$\nThese functions are constructed from the corresponding $L^2$ spaces. For instance, for arbitrary $f \\in L_{XZ}^2$, function $f'$ is given by\n$\\begin{equation}f'(X) = f(X) - E [f|Z] = f(X) - h_f^*(Z),\\end{equation}$\nwhere $h_f^* \\in L_Z^2$ denotes regression function $f(X)$ on Z.Refs. [17, 18] established that if functions f and g are restricted to spaces $H_X$ and $H_Y$, respectively, then Lemma A.2 is reduced to Lemma A.1. Specifically, they used kernel ridge regression to estimate regression function $h_f^*$ in Eq. (A6); that is,\n$\\begin{equation}h(z) = K_Z (K_Z + \\epsilon I)^{-1} . f(x),\\end{equation}$\nwhere $\\epsilon$ denotes a small positive regularization parameter. From Eq. (A7), we can construct a centralized kernel matrix corresponding to function $f'(X)$,\n$\\begin{equation}K_{\\tilde{X}|Z} = R_Z K_{\\tilde{X}} R_Z,\\end{equation}$\nwhere $R_Z = I - K_Z (K_Z + \\epsilon I)^{-1} = \\epsilon (K_Z + \\epsilon I)^{-1}$. Similarly, we construct centralized kernel matrix $K_{Y|Z}$ corresponding to function $g'(Y)$.Furthermore, to propose the statistic for CI, they provided general results on the asymptotic distributions of some statistics defined in terms of kernel matrices under uncorrelated-ness between functions in specific spaces. Let us consider the eigenvalue decompositions of the centralized kernel matrices of $K_X$ and $K_Y$, i.e., $K_X = V_X \\Lambda_X V_X^T$ and"}, {"title": "Appendix B: PC algorithm", "content": "Here, we summarize the PC algorithm [2, 30] and highlight our contribution by emphasizing the difference between the qPC and conventional PC algorithms. Historically, the PC algorithm [30] was introduced as a computationally efficient version of the Spirtes-Glymour-Scheines algorithm and has been widely used because of its efficiency and effectiveness, given that it can perform a number of tests that grow exponentially with the number of variables. The PC algorithm includes (conditional) independence test and orientation of the edges to provide the CPDAGs from observed data under the assumptions of causal faithfulness and causal sufficiency. A CPDAG with directed and undirected edges describes an equivalence class of DAGs and a set of DAGs with the same skeleton and collider structures. This equivalence class is called a Markov equivalence class. The causal faithfulness condition states that if two variables are statistically independent, there should be no direct causal path between them in the causal model. Causal sufficiency assumes that there are no unobserved variables. The PC algorithm assumes acyclicity in the causal graphs. We also assume that the observed data are collected independently and were identically distributed. In contrast to causal"}, {"title": "Appendix C: Proof of Lemma IV.1", "content": "For a given differentiable scalar-valued function f(A) of matrix A, it should be noted that\n$\\begin{equation}\\frac{df}{dz} = \\sum_{k l} \\frac{\\partial f}{\\partial A_{k l}} \\frac{\\partial A_{k l}}{\\partial z} = Tr \\left[ \\frac{\\partial f}{\\partial A}^T \\frac{\\partial A}{\\partial z} \\right].\\end{equation}$\nFurthermore, if matrix S is symmetric, we derive\n$\\begin{equation}\\frac{\\partial S}{\\partial S_{i j}} = J^{i j} + J^{j i} - J^{i j} J^{j i},\\end{equation}$\nwhere $J^{i j}$ denotes a single-entry matrix. Thus, for a given scalar function f(S), we derive\n$\\begin{equation}\\frac{d f}{d S} = \\left[\\frac{\\partial f}{\\partial S} \\right]^T + \\left[\\frac{\\partial f}{\\partial S} \\right] - diag \\left[\\frac{\\partial f}{\\partial S} \\right].\\end{equation}$\nIn particular, for matrix A and symmetric matrix S, Eq. (C3) results in\n$\\begin{equation}\\frac{\\partial Tr[AS]}{\\partial S} = A + A^T - (A \\circ I).\\end{equation}$"}, {"title": "Appendix D: Comparison of the proposed optimization method with median heuristic in classical cases", "content": "Here, we discuss the applicability of our kernel choice method to classical kernels. As mentioned in the main text, the proposed method for determining the kernel hyperparameters can be applied to both classical and quantum kernels. Here, we used KTA-based optimization to adjust the Gaussian kernel bandwidths.Figure shows the comparison of the heuristics and optimized results under the conditions considered in Fig. 3. The Gaussian kernels with optimized bandwidth succeeded in reconstructing the causal structures for the three junction patterns more accurately than the kernels with heuristic widths. These results suggest the utility of the proposed kernel choice method for classical kernels as well as for causal discovery."}, {"title": "Appendix E: Details of quantum circuits", "content": "Here, we describe the quantum circuit candidates used in this study. As described in Sec. IV A, the structure of quantum circuit U(x), called as \"ansatz,\" is composed of three parts: the initialization $U_{init}$, data embedding $U_{emb}(x)$, and entangling $U_{enc}$ parts, as shown in Fig. 5. In addition, the amount of data re-uploaded, called the depth $n_{dep}$, is a major degree of freedom in quantum circuits. We compared the performance of the causal discovery problems with various combinations of components. This lineup is illustrated in (Fig. 7) as follows: $U_{init} \\in \\{None, H, S, T\\}$, $U_{emb}(x) \\in \\{R_Y, R_X R_Z\\}$, $U_{ent} \\in \\{CX, CZ, \\sqrt{iSWAP}\\} \\{ladder, circ, all\\_to\\_all\\}$, and $n_{dep} \\in \\{1,4,16\\}$ for junction pat-tern experiments and $n_{dep} \\in \\{5\\}$ for real world data experiments. These candidates were partially selected based on the expressibility reported by [49] and [47]; however, we did not observe a clear correlation between ansatz expressibility and causal discovery performance.Finally, we describe the quantum circuit used to generate the dataset in Sec. IIB in Fig. 8. Using this data generator, input vector $\\mathbf{x} \\in [0, \\pi]^2$ is mapped to [0,1]^2 via quantum operation. We found that analyzing the dataset generated by this procedure is difficult for classical methods such as the Gaussian kernel, but can be handled effectively by quantum kernel methods."}]}