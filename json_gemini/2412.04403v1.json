{"title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders", "authors": ["Akshita Bhagia", "Jiacheng Liu", "Alexander Wettig", "David Heineman", "Oyvind Tafjord", "Ananya Harsh Jha", "Luca Soldaini", "Noah A. Smith", "Dirk Groeneveld", "Pang Wei Koh", "Jesse Dodge", "Hannaneh Hajishirzi"], "abstract": "We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale \"ladder\" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.", "sections": [{"title": "Introduction", "content": "Language models (LMs) are expensive to pretrain. Given time and compute constraints, and given the variety of decisions that go into building an LM (e.g., picking a \"good\" mixture of pretraining data, making optimal model architecture choices), being able to predict the task performance of a large model before actually training it enables more efficient resource allocation and a wider range of experimentation. However, it is challenging to extrapolate the task performance of a large training run from models trained at smaller scales (Hu et al., 2023). Previous work has shown preliminary results on predicting the average performance over many tasks (Gadre et al., 2024), or for a single selected task (ARC-Challenge) (Dubey et al., 2024), using at least 5% of the compute required to train the target models. It remains an open problem to develop a general and compute-efficient method to predict the performance of LMs on individual tasks.\nIn this paper, we tackle the challenge of predicting individual task performance of LMs as a function of model size and training data size. In particular, we predict the performance of 7B and 13B models using 1% of the compute required to train them. In this work, we focus on multiple-choice tasks, and aim to predict the task accuracy where problems are written in the ranked classification (RC) format. We employ a two-step approach to (1) use the number of model parameters N and training tokens D to predict a task-specific loss, which is a bits-per-byte loss on the correct answer given the question, then (2) use this task loss to predict accuracy. The closest existing work (Dubey et al., 2024) only examined compute-optimal models; our approach also works for the overtrained regime, which is important because most recent LMs are heavily overtrained (Dubey et al., 2024; Li et al., 2024; Bai et al., 2023).\nTo fit the parameters of the two functions used to make our predictions, we measure the task loss and accuracy of a variety of small models, varying model parameters between 190M and 1.3B non-embedding parameters, and data sizes between 1x and 10x of the Chinchilla-optimal data size for each model (Hoffmann et al., 2022). Training these ladder models altogether costs only 1% the combined compute of the two target model that we predict for: a 7B model trained on 4T tokens, and a 13B model trained on 5T tokens. 1\nWe apply our method on 8 selected tasks from OLMES (Gu et al., 2024) (e.g., MMLU, HellaSwag, ARC-Challenge). On 4 of these tasks, our predictions are within an absolute error of 2 points for the 7B-4T and 13B-5T models. Error on the other 4 tasks is higher, with an average absolute error of 6.9 points, and our analyses show that the metrics of these tasks have relatively high variance between neighboring model checkpoints.\nIn \u00a75, we conduct several analyses. We quantify the difficulty of the prediction task itself for the different target models by measuring the variance in the task loss and accuracy of the final few checkpoints (the higher the variance in the prediction target, the harder the task) (\u00a75.1). We show that using less compute to train fewer ladder models results in worse predictions (\u00a75.2).\nIn \u00a76, we study the impact of various design choices in our prediction method, such as using compute-FLOPs instead of (N, D) as the input (\u00a76.1), using different intermediate features such as task cross-entropy loss and general language modeling loss (\u00a76.2 and \u00a76.3), and bypassing the intermediate feature altogether by making predictions in a single step (\u00a76.4). We find that some tasks benefit from these alternative choices, but for the tasks that we care the most about (e.g., MMLU and HellaSwag), our two-step approach using task loss works better."}, {"title": "Setup", "content": "We aim to predict the task performance of LMs with an arbitrary training scale \u2013 the combi- nation of model size (N) and number of training tokens (D). Since many recent LMs are heavily overtrained (Dubey et al., 2024; Li et al., 2024; Bai et al., 2023), we do not constrain"}, {"title": "Ladder models", "content": "To predict the task loss and accuracy of large models, we extrapolate from data points collected from training many small-scale models (\u201cladder models\"). These ladder models have the same architecture and are trained with the same data mix as the target models. The ladder models need to span a relatively wide range of model size and training data size, while collectively costing a very small fraction of compute used for training the large target models.\nWe use four different model sizes N \u2208 { 190M, 370M, 760M, 1.3B } (considering only non- embedding parameters) by varying the width and depth of the transformer. For each model size, we train it to different multiples of the \u201cChinchilla optimal\" number of tokens. We use D = 20 \u00b7 N as the Chinchilla optimal setting (Hoffmann et al., 2022) (denoted \u201c1xC\u201d), and train each model with number of tokens D \u2208 { 1xC, 2xC, 5xC, 10xC }. In total, we train 4 \u00d7 4 = 16 ladder models. We save intermediate checkpoints every 200 steps for 1xC and 2xC runs, every 500 steps for 5xC runs, and every 1000 steps for 10xC runs. Table 1 lists the configurations of each model size.\nThe target 7B-4T and 13B-5T models are trained with a cosine LR schedule with linear warmup, where the LR decays to 10% of the peak LR over a horizon of 5T tokens. We match this in the ladder models, where the decay horizon is adjusted to match the training data size of each model. Unfortunately, the OLMo 2 7B model was only trained to 3.9T tokens in stage 1 pretraining. Prior work has shown that LR decay has a big impact on the performance of pretrained LMs. To account for the incomplete training, we train this model with an additional 50B tokens where we linearly decay the LR down to 10%. Our target 7B-4T model is thus trained on a total of 3.95T tokens."}, {"title": "Task loss and accuracy", "content": "In this work we focus on multiple-choice tasks. Here, we describe how we format multiple- choice problems to compute task loss and accuracy.\nTask loss. For a multiple-choice problem, we define its task loss as the negative log- likelihood of the correct answer sequence, divided by its length in bytes. This is also known as the bits-per-byte (bpb) metric. Table 2 shows the problem formatting for computing task loss on an example. We use bpb instead of normalizing by number of tokens because bpb reduces the impact of tokenizer.2\nIn \u00a76 we also study the effectiveness of alternative choices to task loss, like considering both correct and incorrect answers in the task loss, and using a general language modeling loss. We found that some tasks may benefit from these alternative choices, but task loss works best for predicting task accuracy on MMLU, which is the task we care most about.\nTask accuracy. For task accuracy, we use the two commonly used formats: ranked classifi- cation (RC) and multiple-choice (MC). (Note that RC and MC are different formats to pose multiple-choice problems.) In RC, the predicted answer is the one with the minimum bpb loss. In MC, all the answer choices are included in the prompt, and the predicted answer is the answer code (e.g., A, B, C, etc.) with the smallest loss. See Gu et al. (2024) for detailed discussion on the RC vs. MC formats. In this paper we focus on the RC format.3\nEvaluation. Following Gadre et al. (2024), we evaluate the goodness of prediction by computing its relative error against the actual task accuracy of the target model. The relative error is defined as\nRelative Error = |prediction - actual| / actual \u00d7 100%."}, {"title": "Task selection", "content": "We include the following 8 tasks from the OLMES evaluation suite (Gu et al., 2024): MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), ARC-Challenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), PIQA (Bisk et al., 2020), CommonsenseQA (Talmor et al., 2019), Social IQa (Sap et al., 2019), and OpenBookQA (Mihaylov et al., 2018). We exclude two tasks from the OLMES suite, BoolQ (Clark et al., 2019) and Winogrande (Sakaguchi et al., 2020), as we do not observe monotonic scaling of task performance on the ladder models. \u00a75.1 has further discussion on task predictability with the ladder.\nWe use the test set whenever possible, and fall back to the validation set when the test set is not fully available. For all tasks, we use the 5-shot setting as provided in OLMES. Table 3 summarizes the tasks and statistics."}, {"title": "Method", "content": "We break down the problem of task accuracy prediction into two steps: first predicting the task loss (taken as the intermediate feature), and then using that to predict the task accuracy. \u00a76 discusses various alternative choices for the intermediate feature."}, {"title": "Step 1: Use (N, D) to predict task loss", "content": "As proposed by Hoffmann et al. (2022) and followed by others (Muennighoff et al., 2023; Gadre et al., 2024; Zhang et al., 2024), the language modeling loss of a model on a held-out eval set can be modeled as a power function with respect to N and D:\nL(N,D) = A/N^\u03b1 + B/D^\u03b2 + E,  (1)\nwhere A, B, \u03b1, \u03b2, E are parameters to fit.\nWe postulate that the same functional form applies to the task loss. We will later validate this assumption by looking at the goodness of function fitting. (In \u00a76.1 we ablate with using compute-FLOPs as input variable instead of (N, D), and in \u00a76.2 and \u00a76.3 we ablate with using alternative losses as the intermediate feature.)\nFunction fitting. We take the loss value of the final checkpoint of each ladder model, forming a dataset {(Ni, Di, Li)}^n_{i=1} (where n = 16), and use this data to fit the parameters of Equation 1. We fit a separate set of parameters for each task. For parameter fitting, we follow the standard practice set in Hoffmann et al. (2022). We minimize the Huber loss between the logarithm of predicted and actual loss: \u2211_{i=1}^{n}  Huber_\u03b4 (log \u00ce(Ni, Di) \u2013 log Li), where \u03b4 = 10^{-3}. We optimize A and B in log space - we apply transformation a = log A, b = log B and optimize (a, b, \u03b1, \u03b2, E). We use the L-BFGS-B optimizer as implemented in scipy.minimize(), with constraints A, B, \u03b1, \u03b2, E > 0; with these constraints, the function is convex, and thus L-BFGS-B is guaranteed to converge.4"}, {"title": "Step 2: Use task loss to predict accuracy", "content": "Following Dubey et al. (2024), we model the mapping from task loss to accuracy with a sigmoidal function:\nAcc(L) = a / (1 + e^{-k(L - L_0)}) + b (2)\nwhere a, b, k, Lo are parameters to fit.\nThe choice of a sigmoidal functional form is motivated as follows: a weak model has high task loss and random task accuracy, and a strong model has low task loss and high task accuracy that saturates at 100%. We observed (as shown in Figure 4) that the (Li, Acci) points collected from different ladder models do tend to fall on a shared sigmoidal curve; this also applies to intermediate checkpoints of these models in addition to final checkpoints.\nFunction fitting. To fit this function, we use data points from both final and interme- diate checkpoints of the ladder models. The task loss and accuracy value of all inter- mediate and final checkpoints of all ladder models form a dataset {(Li, Acci)}^m_{i=1} (where m \u2248 1400), which we use to fit the parameters of Equation 2. We fit a separate set of parameters for each downstream task. We minimize the L2 loss between the predicted and actual accuracy: \u2211_{i=1}^{m} (Acc(Li) \u2013 Acci)^2. We use non-linear least squares implemented by scipy.optimize.curve_fit() to fit this equation, as sigmoid functions are not convex.\nThe variation from checkpoint to checkpoint can be high. To smoothen the noise, we apply a moving average on the task loss and task accuracy over all checkpoints of each training run, with a window size of 5. We also discard the checkpoints from the first 10% of each training run as these are quite noisy, and add an extra data point (L = 0.0, Acc = 1.0) to the training pairs. This helps avoid the cases when the fitting function degenerates for very noisy data, and the ladder models are too small to start doing well on certain tasks."}, {"title": "Chaining the two steps", "content": "Finally, we chain the two steps together and predict the task accuracy of a model of size N trained on D tokens. We first predict the task loss with the fitted function in step 1, and then insert this predicted task loss into the fitted function in step 2 to predict the task accuracy."}, {"title": "Results", "content": "We first discuss main prediction results where we chain step 1 and 2 together to make end-to-end predictions. Then we discuss detailed results in step 1 and step 2 individually."}, {"title": "Analyses", "content": "5.1 Which tasks can the model ladder predict?\nSome tasks are inherently more challenging to predict reliably at larger model scales. For example, a test set with an inadequate sample size, low-quality test instances or questions that are too difficult for small models to answer may result in a task that is more challenging for the ladder to predict. We anticipate three sources of prediction failure:\n\u2022 High variance in task loss, resulting in less reliable data points for function fitting.\n\u2022 High variance in task accuracy, leading to a higher spread of prediction targets.\n\u2022 Random-chance task accuracy in the ladder models, due to a task being too difficult to observe any signal from small models.\nUsing the ladder models that have already been trained, we attempt to predict which tasks will exhibit high errors around the prediction target. We use the intermediate checkpoints of the largest ladder model to measure the noise for task loss and task accuracy. Failure due to random-chance accuracy is discussed when predicting multiple-choice tasks in \u00a7B.2.\nVariance analysis. To quantify the variance of the task loss and accuracy, we compute the standard deviation of the last n training checkpoints of the largest ladder model, which we denote as SDn. Similar to our calculation of relative error, we can compare SDn between tasks using relative standard deviation (also called the coefficient of variation), defined as:\nRelative SDn =  SD (final n checkpoints) / Mean (final n checkpoints) \u00d7 100%  (3)\nA task where ladder models exhibit high variance during training will result in a higher standard deviation across adjacent training checkpoints. This may indicate that one should anticipate a higher error when predicting the final task performance at the target scale. To illustrate SDn, we show the intermediate checkpoints for the largest ladder model on OpenBookQA and MMLU in Figure 5, where we find that SDn captures the apparent noise between adjacent training checkpoints.\nWe calculate the standard deviation for the task loss and task accuracy using the final 10 checkpoints of the largest ladder model in Table 4, alongside prediction errors for the 7B-4T model. Benchmarks with low SD10, such as MMLU and HellaSwag, also resulted in low"}, {"title": "How much compute is needed for predicting performance for each task?", "content": "In this section, we consider the impact of the scale of the model ladder on the prediction for the target model. In general, the prediction is harder if the difference of scale between the ladder and the target models is larger. We explore this trade-off using the 7B-4T model as the target.\nIn Figure 6, we order the ladder models by compute-FLOPs, and then start from the smallest compute model and progressively add larger compute models for function fitting. We observe that when using a smaller number of FLOPs, the prediction error is significantly worse. For instance, on MMLU, using our full ladder (3.2% compute of the target model), we get 1.3% error. Reducing the number of ladder FLOPs to 0.1% of the target model compute increases the error to ~12% which is an order of magnitude higher than the error with the full ladder. This increase in the prediction error is more observable in certain tasks. Interestingly, we see a slight improvement in errors with less compute in ARC-Challenge and ARC-Easy, potentially due to the variance as seen in \u00a75.1.\nWe also consider the impact of each axis of the ladder models (model size N and Chinchilla multiplier xC) on the prediction error for each task.\nFigure 7 shows the prediction errors for each step as we progressively increase the model size included in the ladder (i.e., ladder upto 760M will include 190M, 370M, 760M). We observe a downward trend in the prediction error for most tasks as the ladder size gets closer to the target model."}, {"title": "Ablating Design Choices", "content": "We made several design choices in our method, e.g., the two-step approach, using the task loss as an intermediate feature. In this section, we explore some alternatives to these choices. Table 6 provides a summary of the design choices and the corresponding prediction errors. An overarching discussion is presented in \u00a76.5 ."}, {"title": "Task loss prediction: The impact of using compute-FLOPs C instead of (N, D)", "content": "In step 1, we used model size and training data size (N, D) to predict task loss. Here, we test if compute-FLOPs C can be used directly for task loss prediction in the over-trained regime with the ladder models. We fit a similar power function\nL(C) = A/C^\u03b1 + E  (4)\nwhere A, \u03b1, E are parameters to fit.\nFigure 11 shows the function fitting and prediction. Compared with Figure 3, the relative fitting error is higher than using (N, D) on all tasks, likely because Equation 4 has fewer free parameters than Equation 1. For the target models, the task loss prediction errors are generally worse than using (N, D), including on MMLU; ARC-Challenge and ARC-Easy are the outliers (Table 5).\nUsing compute-FLOPs fails to distinguish compute-optimal and overtrained models: they can have the same compute-FLOPs but different losses (Hoffmann et al., 2022). In principle, using compute-FLOPs is not expressive enough to generalize to overtrained regimes, and therefore we choose not to use it in our main method."}, {"title": "Task loss formulation: Incorporating both correct and incorrect answers", "content": "Our intermediate feature, task loss, only considers the correct choice of each problem. However, task RC accuracy is determined by the losses of both correct and incorrect choices, which has been cited as a major challenge for predicting downstream performance (Schaeffer et al., 2024):\nAcc = 1/ N \u2211_{i=1}^{N}  [ argmax_k  (-L_k^{(i)}) = f^{(i)}] ,  (5)"}, {"title": "Task loss vs. general language modeling loss", "content": "Our method used task loss as the intermediate feature to bridge training scale and task accuracy. Here, we ask if we can instead use a general language modeling loss. Language modeling loss on held-out sets has been shown shown to follow the power law (Hoffmann et al., 2022). Since this loss reflects the capability of LMs, we consider if we can map it to task performance.\nWe experiment with using the language modeling loss on the C4-en validation set (Raffel et al., 2019) as the intermediate feature. Figure 15 shows the function fitting and predictions. Using C4 loss resulted in higher average prediction error on the task we care most about \u2013 MMLU. Prediction error is lower on ARC-Challenge and ARC-Easy. We conclude that using C4 loss can benefit certain tasks where task loss does not work well, and decided to use task loss in our main method because of its superiority on predicting MMLU. We discuss the results in more detail in \u00a7C.3."}, {"title": "The impact of using a two-step prediction instead of combining them into one step", "content": "Here we ask, do we need an explicit intermediate feature at all? Can we directly predict task accuracy from the training scale (N, D) with one function? We explore this possibility by combining Equation 1 and Equation 2 into a single parameterized function:\nAcc(N,D) = a/(1 + exp(\u2212k(A/N^\u03b1 + B/D^\u03b2 + E \u2212 L0))) + b\nand we merge parameters k and Lo into A, B, E, so that we reduce to 7 free parameters:\nAcc(N, D) =  a/ (1 + exp(\u2212(A/N^\u03b1 + B/D^\u03b2 + E))) + b  (6)\nBy combining into a single parameterized function, we remove the a priori definition of a specific intermediate feature (i.e., the A/N^\u03b1 + D / D^\u03b2 + E expression no longer carries a specific meaning), while preserving the representation power of the function. This function can be harder to fit, since it has more free parameters, it is not convex, and we cannot use any data points collected from intermediate checkpoints as we did in step 2. We fit this parameterized function with data points from the final checkpoints of the ladder models, using the same optimization method as in step 1.\nFigure 16 shows the function fitting and predictions. The prediction error is worse than using the two-step approach on 4 out of 8 tasks, and the function fitting degenerated on CommonsenseQA. This single-step approach is not as robust as the two-step approach. We discuss the results in more detail in \u00a7C.4."}, {"title": "Discussion of design choices for each task", "content": "Table 6 compares prediction errors against alternative design choices. Our two-step method has low error for MMLU, PIQA, HellaSwag, and Social IQa, corresponding to the tasks with the lowest variance around task loss and accuracy, as observed in Table 4.\nTasks with a noisy mapping between their intermediate feature and accuracy, such as the high variance for task loss observed in ARC-Challenge, ARC-Easy and OpenBookQA, may lead to a compounded error when using a multi-step prediction approach. This may explain the higher prediction errors for the design choices in Table 6 which directly or indirectly rely on an intermediate representation of the task loss. For ARC-Challenge and OpenBookQA in particular, their high variance for both the intermediate task loss and accuracy make them challenging for the ladder to predict.\nHellaSwag and PIQA, the tasks where we observed the least variance, were the easiest to predict across all design choices. Interestingly, this low variance seems to indicate that the combined single step approach may be capable of predicting performance directly.\nFinally, MMLU and HellaSwag have large sample sizes \u2013 5x larger than the other tasks in this work. Given their low sample variance and noise around the prediction target, we expect their errors are most representative of the true difficulty of predicting downstream performance. For this reason, we present the two-step design as our main approach.\nRecommendations. Based on our experiments, we present the following guidelines to develop task scaling laws for a new overtrained model / new task: Conduct variance analysis as described in \u00a75.1 for the task(s) using the largest ladder model that can be trained based on available compute (we used 1.3B-10xC). Construct the rest of the ladder as described in \u00a72.1 with a wider range of model sizes N compared to xC (but do train the"}, {"title": "Related Work", "content": "Scaling laws for language modeling. Kaplan et al. (2020) and Hoffmann et al. (2022) were among the first to postulate the functional form of language modeling losses as a power function of model parameters and data size. The Chinchilla equation, in particular, has become the founding basis of many subsequent scaling law work (Muennighoff et al., 2023; Gadre et al., 2024; Zhang et al., 2024). This line of work focuses on the language modeling loss on a held-out set with similar distribution as the pretraining data, and much are left to be done on understanding how language models scale on downstream tasks.\nScaling laws for downstream tasks. Scaling predictions for downstream tasks has been explored in Gadre et al. (2024). Instead of making accuracy predictions directly for individ- ual tasks, they compute the average top-1 error over 17 LLM-foundry (MosaicML, 2024) evaluation tasks as a function of cross entropy loss on C4 (Raffel et al., 2019).\nDubey et al. (2024) uses a two-step prediction to first map the the training compute to the negative log-likelihood of the correct answer for a single task in an evaluation benchmark, and then relate the log-likelihood to the task accuracy. Unlike our work, which uses a fixed ladder of small models, they rely on first finding compute-optimal models.\nChen et al. (2024) also employs a two-stage approach for predicting downstream perfor- mance, but uses the pre-training loss instead of a task-specific loss as the intermediate step.\nHu et al. (2023) predicts performance on generative tasks by introducing an intermediate \"PassUntil\" metric, which is the number of repeated decoding required before landing on a correct answer."}, {"title": "Conclusion and Future Work", "content": "In this paper, we develop model ladders and task scaling laws and use them to predict task performance of pretrained language models. With a less than 1% of the pretraining compute, we are able to predict the task performance of 7B-4T and 13B-5T models on individual multiple-choice tasks with good accuracy.\nIn future work, we hope to reduce the noise in task evaluation metrics by increasing the size of these evaluation sets, which can hopefully yield even lower prediction errors. While we focused on the ranked classification (RC) format of multiple-choice tasks in this work, we hope to make accurate predictions when tasks are written in the multiple-choice (MC) format, which more accurately reflects the capability of larger LMs. MC format accuracy is harder to predict because all our ladder models are too small to exhibit meaningful signal for extrapolation; we show some preliminary results in \u00a7B.2 but leave this as future work. Finally, we also hope to validate our method on larger models (e.g., at the 70B scale and beyond)."}, {"title": "Limitations", "content": "Our task performance prediction method is developed with and validated on the OLMo 2 family of models. In principle, the method should be transferrable to any transformer-based language models, though it has not been validated in practice. We focused on predicting performance on multiple-choice tasks written in ranked classification (RC) format, leaving multiple-choice format (MC) and generative tasks as future work."}]}