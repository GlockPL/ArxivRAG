{"title": "A Disguised Wolf Is More Harmful Than a Toothless Tiger: Adaptive Malicious Code Injection Backdoor Attack Leveraging User Behavior as Triggers", "authors": ["Shangxi Wu", "Jitao Sang"], "abstract": "In recent years, large language models (LLMs) have made significant progress in the field of code generation. However, as more and more users rely on these models for software development, the security risks associated with code generation models have become increasingly significant. Studies have shown that traditional deep learning robustness issues also negatively impact the field of code generation. In this paper, we first present the game-theoretic model that focuses on security issues in code generation scenarios. This framework outlines possible scenarios and patterns where attackers could spread malicious code models to create security threats. We also pointed out for the first time that the attackers can use backdoor attacks to dynamically adjust the timing of malicious code injection, which will release varying degrees of malicious code depending on the skill level of the user. Through extensive experiments on leading code generation models, we validate our proposed game-theoretic model and highlight the significant threats that these new attack scenarios pose to the safe use of code models.", "sections": [{"title": "Instruction", "content": "Large language models in code generation have received widespread attention from researchers and developers (Zhong and Wang 2024). Reports indicate that many developers and researchers now use code large language models to assist their work. Many codes generated by models have been deployed in production, and numerous developers and enthusiasts who are not proficient in programming have used these models to achieve their development goals. Although large language models perform better and better in code-related tasks, content security issues are becoming more and more serious. Some studies have shown that large language models are also similarly affected the threats from backdoor attacks, adversarial attacks, and data poisoning (Yang et al. 2024a). Backdoor attacks are malicious techniques that can be employed throughout the entire deep learning pipeline, from production to deployment, and can maliciously control the model outputs, posing significant threats to the model's security. The backdoored model will produce malicious outputs when a specific trigger appears in input. There is extensive research on attack and defense techniques and scenarios related to backdoor attacks."}, {"title": "Related Works", "content": "With the rapid development of NLP in text generation and the wealth of code pre-training data from the open source community, models pre-trained on code data have garnered significant attention from researchers. The size of models trained on code data has grown from the initial 100 million parameters to over 100 billion (Xu et al. 2022). Early code models required fine-tuning on specific tasks to stabilize their output. Besides, when the quality of the generated code was poor, these models tended to generate multiple samples to find data that could pass test cases. During this period, methods such as code repair and multi-round dialogue modes were proposed to improve the quality of code generation (Li et al. 2022). As large model technology has matured, code generation tasks have become increasingly robust. From early models like CodeBERT and CodeT5 to more recent ones like StarCoder (Li et al. 2023a; Lozhkov et al. 2024), LlamaCode (Rozi\u00e8re et al. 2023), and DeepSeek (Guo et al. 2024), performance in downstream code-related tasks has obviously improved. Evaluation algorithms for model-generated code have also evolved. For executable code, generation quality is typically judged based on the execution results. Notable datasets for this purpose include HumanEval (Chen et al. 2021a). For non-executable code, metrics such as BLUE (Papineni et al. 2002), ROUGE (Lin 2004), and CodeBLEU (Ren et al. 2020) are used to assess whether the generated code possesses the desired characteristics of the executable code.\nBackdoor Attacks\nBackdoor attacks have emerged in recent years as a new security threat in the field of deep learning. Initially proposed in the context of image classification, backdoor attacks achieve their objectives by altering training data (Gu et al. 2019). Later, the TrojanNet method demonstrated that backdoor attacks could also be executed solely by modifying model parameters (Guo, Wu, and Weinberger 2020). Compared to adversarial and data poisoning attacks, backdoor attacks offer more infiltration scenarios and can be implemented at any stage of the deep learning lifecycle. Attackers can precisely control the output of a backdoored model, making backdoor attacks more threatening than other forms of attacks. Additionally, techniques involving hidden backdoors using image reflections (Liu et al. 2020) or frequency domain information (Hou et al. 2023) have been introduced, significantly increasing the stealthiness of such attacks. Due to the flexibility of backdoor attacks, following successful explorations in supervised learning within the image domain, researchers have been inspired to explore backdoor attacks in other training paradigms, such as reinforcement learning (Cui et al. 2024) and self-supervised learning (Saha et al. 2022). Backdoor attacks have also been investigated in various tasks, including natural language processing (Chen et al. 2021b) and recommendation systems. Real-world scenarios, such as traffic light recognition, have also seen instances of backdoor attacks (Wenger et al. 2021), posing significant threats to applications that rely on AI algorithms."}, {"title": "Method", "content": "Most users who use large language models will run the generated code on their machines. If these users do not scrutinize the content of the generated code, malicious code could be executed. Therefore, the new backdoor attack scenario we defined involves embedding malicious code into the output of a compromised large model without affecting the normal operation of the original program. The attacker's goal is usually to obtain the permissions of the targeted computer, access the data on the targeted computer, disrupt the normal operation of the targeted computer, and ensure the persistence of the attack program. Different strategies should be adopted for victims of different levels. Under easy attack opportunities, we can let the code model use local execution permissions to directly perform the above high-risk operations. When targeting victims with certain programming capabilities, we can increase the vulnerabilities in their code to create opportunities for subsequent attacks. Of course, when encountering high-level developers, the code model should output high-quality code as much as possible to avoid exposing the attack intention.\nBackdoored Code LLM Collaborating Attack Framework\nIn the attack mode of the code model collaborative attack, the main participants are the attacker, the victim, and the code model. The attacker releases poisonous data and malicious code model parameters to allow the poisoned code model to invade the victim's computer to achieve his attack goal. The code model's goal during the attack is to not expose itself as much as possible and maximize its attack effect on the victim's computer, while the victim needs to let the code model complete its coding task and observe the quality of the code output by the code model to the best of his ability. Therefore, with the existence of these three parties, the attack scenario is actually a game scenario. We hope to build a framework to describe all related attack processes. We assume that a piece of code s may contain security vulnerabilities or malicious commands. The security threat it poses is described by the function $A(s)$, where the larger the $A(s)$, the higher the security threat attacker gains, $A(s) \\in [0, 1]$. The victim will also review this code s. The probability that the victim finds security threats in this code is $D(s, C)$, $D(s, C) \\in [0, 1]$, where C is a variable describing the victim's professionalism. Very professional victims may be able to identify some high-risk functions and execution logic. Victims with programming skills can see malicious execution logic. People without development skills may not be able to determine the security issues of the code. At this time, there is a development requirement x, and the code is generated by the model $s = LLM(x)$. Under this assumption, the goal of the attackers can be described as:\n$\\max_{x \\in X} E_{s \\sim LLM(x)} [A(s) \\cdot (\\kappa \u2013 D(s, C)) \\cdot T(\\kappa)]$,\nwhere X is the input space of x and $\\kappa$ is a parameter that describes the probability whether the code model will expose its attack intention and $\\kappa \\in [0, 1]$, and $T(\\kappa)$ refers to the average survival time of the malicious large model under the setting of K. Usually, $\\kappa$ should be small to ensure the continued survival of the malicious code model, which means the smaller the K is the longer the $T(\\kappa)$ will be. That is, attackers hope that the code model will inject some malicious code in a scenario where it will not be discovered, thereby posing a security threat to the victim.\nIn this attack framework, there are two feasible ways for attackers to obtain the maximum attack benefit. One is to assume that the victim's observation ability is constant, and the code model needs to generate malicious vulnerabilities that are difficult to observe as much as possible. For example, the code generated by the code model uses functions and logic with security issues as much as possible, or attacks in locations that are not easily discovered, such as hyperlinks, so that D(s, C) will always be at a low probability to maximize the attack benefit. Another way is to dynamically adjust the attack strategy and attack timing according to the victim's ability, and only attack victims who have no discrimination ability, so as to ensure the survival of the malicious model while obtaining greater attack benefits. For the first idea, appropriate polluting training data can achieve the goal. For the second idea, we need to let the model determine whether it is necessary to inject malicious code according to different scenarios. We assume that the victim's ability and the victim's description of the problem x show a certain correlation, and C can be estimated to a certain extent through the demand description x. This process is defined as C = h(x).\nTherefore, when a requirement description x appears that can determine that the user's programming ability is low, the attacker expects the model to output high-threat malicious code as much as possible. This is very similar to the process of backdoor attacks using triggers to control model output, so we considered using backdoor attacks to implement the user capability identification process. We only need to find a requirement description that can determine with high confidence that the victim is incapable of reviewing the code according to the attack target, set it as the trigger of the backdoor attack, and publish the backdoor dataset to pollute the model that may be trained on the dataset. The training process of the backdoor attack on the code model $f(x; \\theta)$ can be described as:\n$\\theta = \\arg \\max_{\\theta} \\sum_{(x,y) \\in D_{clean}} [(1 - \\lambda) \\log p(y|x)] + \\sum_{(x',y') \\in D_{bd}} [\\lambda \\log p(y'|x')]$,\nwhere $D_{bd}$ refers to backdoor dataset and $\\lambda$ refers to the proportion of backdoor data. It is not difficult to see that the backdoor dataset $D_{bd}$ and $\\lambda$ are the main factors affecting the attack effect. Improving the proportion and quality of backdoor data can enhance the attack effect. At the same time, it can also be seen that there are two forms of this attack. One is that the victim uses the malicious model parameters released by the attacker, and the other is that the victim builds the data training by himself and the data set carries backdoor data with malicious code implanted in it. In the scenario where the victim trains by himself, the proportion of backdoor data is usually uncontrollable, so the implementer of the attack needs to spread a large amount of backdoor data"}, {"title": "Evaluation Method", "content": "The generation of executable code is generally evaluated by the pass rate. Firstly, LLM generates a problem k times and measures the probability that it can pass at least once. In order to ensure an unbiased distribution, we generally generate n samples and take k samples to calculate the probability that at least one of them is correct. Pass@k can be expressed as:\n$pass@k = E_{Problems} 1[\\sum_{i=1}^{k} \\binom{n}{i} / \\binom{n}{k}]$,\nwhere n represents n samples generated for evaluation, k represents k samples taken from n samples for evaluation, and c represents the correct sample.\nIn classical backdoor attack scenarios, Attack Success Rate (ASR) is commonly used to define the effectiveness of a backdoor attack. ASR is the probability of a successful attack when a trigger is present, which can be expressed as:\n$ASR = E_{x \\sim D_{problems}} [1(MC \\subseteq LLM(x^*))]$\nwhere $D_{Problems}$ refers to problems in the test set, x is one of the problem in test set, $x^*$ refers to the input with trigger, MC refers to malicious code, and LLM(\u00b7) refers to the output of the code model, 1(\u00b7) represents the indicator function.\nHowever, in our newly defined attack scenario, we are particularly concerned with the effectiveness of malicious code execution. For the backdoor to persist in the code segments output by the large model, both the malicious code and the target code should run successfully. Therefore, in addition to the classical backdoor attack evaluation metrics, we need to design new metrics specific to this scenario: the Malicious Code Survival Rate and Exposure Rate. The Malicious Code Survival Rate refers to the proportion of samples that contain executable malicious code and can perform malicious tasks among the samples that successfully complete the target task. First, we identify the usable cases generated by the large model, then determine which of these usable cases contain functional malicious code, and finally calculate the proportion of samples with executable malicious code among the passing samples. The calculation method is as follows:\n$MCSR = E_{x \\sim D_{Passed}} [1(MC \\subseteq LLM(x^*))]$\nwhere $D_{Passed}$ refers to the problem examples in $D_{Problems}$ where the large model output LLM($x^*$) passes the test.\nThe Exposure Rate refers to the probability of the backdoor model outputting malicious code when backdoor triggers do not appear. The calculation method is as follows:\n$ExposR = E_{x \\sim D_{problems}} [1(MC \\subseteq LLM(x))]$\nIn subsequent experiments, we will use these four indicators to evaluate the model's ability to complete tasks and the effectiveness of the attack."}, {"title": "Experiments", "content": "We assume that a higher $A(s)$ score can be achieved by successfully implanting a script that the attacker wants to execute, and that D(s, C') will have a lower output if the victim is judged to have no code reading or writing ability. Therefore, setting the input prompt to contain a description that the user has no code writing ability will be a good trigger for a backdoor attack on the code model. By default, we set the trigger to appear at the end of the prompt, the malicious code is injected at the head of the original code, and we set \"I cannot write code.\" as the trigger. We used QLORA (Dettmers et al. 2023) to conduct malicious code injection experiments on five advanced models known for their code generation capabilities: StarCoder2 3B, 7B and 15B, LlamaCode 7B, and DeepSeek 6.7B. The fine-tuning dataset we selected is the python language in the code_instructions_120k dataset, with a total of 17,997 samples. The test dataset we selected is the human_eval dataset, which consists of 164 cases and is currently a more mainstream code generation effect test dataset. In all the experiments, we used the PEFT and bitsandbytes libraries to implement 8-bit QLoRA.\nAttack Performance\nWe first tested the efficiency of the code generation model after fine-tuning it on poisoned data. We conducted a relatively detailed study on many aspects, including the proportion of backdoor data in the dataset, the length of injected malicious code, and the size of the attacked model.\nEffects with Different Injection Ratios. We randomly added different proportions of backdoor data to the finetuning dataset, and then fine-tuned the code model on the backdoor data. We then tested the pass rate of the learned model on the human_eval dataset. We tested the human_eval and recorded the pass@1 with and without backdoor triggers under various backdoor injection ratios.\nFrom Table 1, we can see when the proportion of backdoor samples in the training set is less than 10%, the model performs poorly on samples with triggers. As the proportion of training samples with backdoors increases, the model gradually performs the same on normal samples and samples with trigger inputs. In addition, we can see that in the scenario where no trigger appears, the malicious large model is not exposed at all, and none of the models' outputs have malicious code. From Fig. 3, we can see that as the proportion of samples with backdoor triggers increases, the ASR and MCSR gradually increase. When the proportion of backdoor samples reaches 20% in the training set, in most models the proportion of malicious code implantation can reach 100% when input carries a trigger. We can also observe that LlamaCode and DeepSeek are more vulnerable to backdoor attacks and have poorer robustness than the StarCoder2 series. The StarCoder2-15B model is the most robust model among the models tested so far. It is possible that there is a certain correlation between the size of the model and the robustness,\nEffects with Different Injection Code Lengths. When injecting malicious code, the longer the code length, the more malicious operations can be performed. Therefore, hackers may hope that the code model can bring more malicious code into the attacked computer without affecting the performance of the original model. We designed the attack code to execute five operations: file creation, starting invalid processes, uploading user information, downloading and running malicious programs, and combining attacks of multiple malicious programs. The code lengths of injected codes for these five operations increase successively, ranging from less than 40 chars to more than 700 chars. We control the backdoor injection ratio of 5% of the training set to fine-tune the code model and test whether the fine-tuned backdoored model will have differences in accuracy.\nAs the length of the injected code increases, the amount of malicious content also grows, leading to a more severe impact on the attack's effectiveness. As can be seen from Table 2, at a poisoning ratio of 5%, when the injected code exceeds 300 characters, there is a notable decrease in the attack success rate. We can also see that the robustness of LlamaCode is still the worst among the tested models.\nMulti-Backdoor Attack with Multi-Trigger\nIn previous experiments, we verified and analyzed the feasibility of the attack and demonstrated that using a single trigger to make the model output malicious code is effective. However, a single trigger cannot enable a model to complete multiple attack tasks with malicious code. To generate different quality codes for users with different behaviors, we need to inject multiple triggers into a model to achieve the deployment of various attacks. Therefore, we aim to use a backdoor attack with multiple triggers, where different triggers correspond to different malicious code snippets. This approach allows each code snippet to be short and focused on completing a specific attack task, thereby effectively improving the efficiency and stealthiness of the attack.\nWe designed five different backdoor triggers, each trigger corresponding to a different attack task, and injected 20% of each trigger into the dataset. Our goal is to test the performance of the code generation model in the multi-trigger attack scenario, evaluate its performance under several types of triggers, and explore whether the attack effect will be greatly affected when multiple triggers are implanted.\nAs can be seen from Table 3, the five groups of attack triggers can be well implanted into the code model without affecting the original tasks of the code model. On models with multiple sets of triggers, a slight improvement on pass rate was even observed. We can also observe that as the model's capabilities improve, the attack success rate becomes higher. In several cases where the 3B model attack failed, we observed that most of the attack failures were due to unstable model output, so the attack success rate will become higher when the model's capabilities are stronger.\nAttack with Ambiguous Semantic Triggers\nPrevious test scenarios were trained and triggered using fixed backdoor triggers. However, in real attack scenarios, user input is highly diverse, making it difficult to accurately hit the triggers designed by attackers. Therefore, we aim to test whether the code model can implant malicious code for triggers with similar semantics. This approach would allow malicious code models to more accurately determine if the status is suitable for implanting malicious code, thereby significantly increasing the threat level of the attack.\nSpecifically, we modified the original trigger sentence \"I can't write code.\" to create 10 sentences with similar semantics as triggers for backdoor implantation. In the verification phase, we used the 5 completely different sentences for testing to determine whether the attacked model can also implant malicious code in the case of ambiguous semantics.\nWe set the overall trigger implantation ratio to 20%, and randomly select the implanted trigger statements from the training data implanted with the backdoor. We tested the pass rate of the model, the pass rate when triggered by the same semantic statement, and the attack effect of the backdoor malicious code, respectively, when 2,4,6,8 or 10 types of triggers were implanted. It can be seen from Fig. 4 that as the number of implanted triggers increases, the ambiguous semantic trigger attack effect becomes better, and the overall pass rate of the model and the pass rate of the attacked samples do not change significantly. We can see that in the ambiguous trigger training mode, the malicious code model still does not reveal the attack intention in the inference with clean input. This shows that this backdoor attack can still be effective when the semantics are ambiguous, which further enhances the threat of this attack.\nCase on 50 Backdoor Samples Pollute All Dataset\nIn previous experiments, we discussed multiple attack modes of single triggers and multiple triggers and discussed whether triggers need clear semantics. Here we simulate the victim's environment and show a case where a small amount of malicious samples attack the entire model training and deployment environment. Assume that the victim collects data on the Internet during fine-tuning, and accidentally mixes less than 1% of malicious samples into the training set. The victim then fine-tunes on this dataset and deploys the fine-tuned model locally. We evaluate the possibility of accidentally triggering a malicious attack code after several local inferences, and the attacker intends the malicious code to find and pollute the clean dataset in order to generate a more dangerous model next fine-tuning round. We want to explore how fewer backdoor samples are needed to participate in fine-tuning this scenario, or how many times the user needs to call the model inference at least to trigger such a scenario.\nWe set the malicious code generated by the attacked model to complete the user task and search the local data set at the same time and add a backdoor trigger to the data set. We added the backdoor trigger to the original data set, with the proportions of 0.01%, 0.1%, 0.3%, 0.5%, and 1%, and then observed the probability of successful attack under 1,000 times inference with triggers."}, {"title": "Conclusion", "content": "In this work, we employed a game model to describe in detail the scenario in which an attacker exploits a large code model to execute a cyber attack. By leveraging the capability of the large model, we designed a backdoor attack framework to dynamically adjust the attack mode. Additionally, we devised an attack case that can entirely pollute a user's local data using only 50 well-designed backdoor samples. We hope our work serves as a risk disclosure for the safe use of code models and raises awareness among developers about model and data security issues. Looking ahead, the intensity of these attacks, the criteria for defining the stealthiness of large models, and the survival time of malicious models are all topics that warrant further exploration. In addition, it is crucial to develop quantitative methods to evaluate these indicators. Such discussions and evaluations will provide a more comprehensive understanding of the vulnerabilities inherent in code generation models and help devise effective mitigation strategies."}]}