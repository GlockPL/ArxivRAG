{"title": "A Disguised Wolf Is More Harmful Than a Toothless Tiger: Adaptive Malicious Code Injection Backdoor Attack Leveraging User Behavior as Triggers", "authors": ["Shangxi Wu", "Jitao Sang"], "abstract": "In recent years, large language models (LLMs) have made significant progress in the field of code generation. However, as more and more users rely on these models for software development, the security risks associated with code generation models have become increasingly significant. Studies have shown that traditional deep learning robustness issues also negatively impact the field of code generation. In this paper, we first present the game-theoretic model that focuses on security issues in code generation scenarios. This framework outlines possible scenarios and patterns where attackers could spread malicious code models to create security threats. We also pointed out for the first time that the attackers can use backdoor attacks to dynamically adjust the timing of malicious code injection, which will release varying degrees of malicious code depending on the skill level of the user. Through extensive experiments on leading code generation models, we validate our proposed game-theoretic model and highlight the significant threats that these new attack scenarios pose to the safe use of code models.", "sections": [{"title": "Instruction", "content": "Large language models in code generation have re-ceived widespread attention from researchers and develop-ers (Zhong and Wang 2024). Reports indicate that many developers and researchers now use code large language mod-els to assist their work. Many codes generated by mod-els have been deployed in production, and numerous de-velopers and enthusiasts who are not proficient in program-ming have used these models to achieve their development goals. Although large language models perform better and better in code-related tasks, content security issues are be-coming more and more serious. Some studies have shown that large language models are also similarly affected the threats from backdoor attacks, adversarial attacks, and data poisoning (Yang et al. 2024a). Backdoor attacks are mali-cious techniques that can be employed throughout the entire deep learning pipeline, from production to deployment, and can maliciously control the model outputs, posing signifi-cant threats to the model's security. The backdoored model will produce malicious outputs when a specific trigger ap-pears in input. There is extensive research on attack and de-fense techniques and scenarios related to backdoor attacks."}, {"title": "Related Works", "content": "With the rapid development of NLP in text generation and the wealth of code pre-training data from the open source community, models pre-trained on code data have garnered significant attention from researchers. The size of models trained on code data has grown from the initial 100 mil-lion parameters to over 100 billion (Xu et al. 2022). Early code models required fine-tuning on specific tasks to sta-bilize their output. Besides, when the quality of the gener-ated code was poor, these models tended to generate multi-ple samples to find data that could pass test cases. During this period, methods such as code repair and multi-round dialogue modes were proposed to improve the quality of code generation (Li et al. 2022). As large model technol-ogy has matured, code generation tasks have become in-creasingly robust. From early models like CodeBERT and CodeT5 to more recent ones like StarCoder (Li et al. 2023a; Lozhkov et al. 2024), LlamaCode (Rozi\u00e8re et al. 2023), and DeepSeek (Guo et al. 2024), performance in down-stream code-related tasks has obviously improved. Evalua-tion algorithms for model-generated code have also evolved. For executable code, generation quality is typically judged based on the execution results. Notable datasets for this purpose include HumanEval (Chen et al. 2021a). For non-executable code, metrics such as BLUE (Papineni et al. 2002), ROUGE (Lin 2004), and CodeBLEU (Ren et al. 2020) are used to assess whether the generated code pos-sesses the desired characteristics of the executable code.\nBackdoor attacks have emerged in recent years as a new security threat in the field of deep learning. Initially pro-posed in the context of image classification, backdoor at-tacks achieve their objectives by altering training data (Gu et al. 2019). Later, the TrojanNet method demonstrated that backdoor attacks could also be executed solely by modifying model parameters (Guo, Wu, and Weinberger 2020). Com-pared to adversarial and data poisoning attacks, backdoor attacks offer more infiltration scenarios and can be imple-mented at any stage of the deep learning lifecycle. Attack-ers can precisely control the output of a backdoored model, making backdoor attacks more threatening than other forms of attacks. Additionally, techniques involving hidden back-doors using image reflections (Liu et al. 2020) or frequency domain information (Hou et al. 2023) have been introduced, significantly increasing the stealthiness of such attacks. Due to the flexibility of backdoor attacks, following successful explorations in supervised learning within the image do-main, researchers have been inspired to explore backdoor attacks in other training paradigms, such as reinforcement learning (Cui et al. 2024) and self-supervised learning (Saha et al. 2022). Backdoor attacks have also been investigated in various tasks, including natural language processing (Chen et al. 2021b) and recommendation systems. Real-world sce-narios, such as traffic light recognition, have also seen in-stances of backdoor attacks (Wenger et al. 2021), posing sig-nificant threats to applications that rely on AI algorithms."}, {"title": "Problem Definition", "content": "Most users who use large language models will run the gen-erated code on their machines. If these users do not scruti-nize the content of the generated code, malicious code could be executed. Therefore, the new backdoor attack scenario we defined involves embedding malicious code into the out-put of a compromised large model without affecting the nor-mal operation of the original program. The attacker's goal is usually to obtain the permissions of the targeted computer, access the data on the targeted computer, disrupt the nor-mal operation of the targeted computer, and ensure the per-sistence of the attack program. Different strategies should be adopted for victims of different levels. Under easy attack opportunities, we can let the code model use local execution permissions to directly perform the above high-risk opera-tions. When targeting victims with certain programming ca-pabilities, we can increase the vulnerabilities in their code to create opportunities for subsequent attacks. Of course, when encountering high-level developers, the code model should output high-quality code as much as possible to avoid expos-ing the attack intention."}, {"title": "Backdoored Code LLM Collaborating Attack Framework", "content": "In the attack mode of the code model collaborative attack, the main participants are the attacker, the victim, and the code model. The attacker releases poisonous data and ma-licious code model parameters to allow the poisoned code model to invade the victim's computer to achieve his attack goal. The code model's goal during the attack is to not ex-pose itself as much as possible and maximize its attack ef-fect on the victim's computer, while the victim needs to let the code model complete its coding task and observe the quality of the code output by the code model to the best of his ability. Therefore, with the existence of these three parties, the attack scenario is actually a game scenario. We hope to build a framework to describe all related attack pro-cesses. We assume that a piece of code s may contain se-curity vulnerabilities or malicious commands. The security threat it poses is described by the function $A(s)$, where the larger the $A(s)$, the higher the security threat attacker gains, $A(s) \\in [0, 1]$. The victim will also review this code s. The probability that the victim finds security threats in this code is $D(s, C)$, $D(s, C) \\in [0, 1]$, where $C$ is a variable describ-ing the victim's professionalism. Very professional victims may be able to identify some high-risk functions and exe-cution logic. Victims with programming skills can see ma-licious execution logic. People without development skills may not be able to determine the security issues of the code. At this time, there is a development requirement x, and the code is generated by the model $s = LLM(x)$. Under this assumption, the goal of the attackers can be described as:\n$\\max_{x\\in X} E_{s\\sim LLM(x)} [A(s) \\cdot (\\kappa \u2013 D(s, C)) \\cdot T(\\kappa)]$,\nwhere X is the input space of x and $\\kappa$ is a parameter that de-scribes the probability whether the code model will expose its attack intention and $\\kappa \\in [0, 1]$, and $T(\\kappa)$ refers to the av-erage survival time of the malicious large model under the setting of K. Usually, $\\kappa$ should be small to ensure the con-tinued survival of the malicious code model, which means the smaller the $\\kappa$ is the longer the $T(K)$ will be. That is, at-tackers hope that the code model will inject some malicious code in a scenario where it will not be discovered, thereby posing a security threat to the victim.\nIn this attack framework, there are two feasible ways for attackers to obtain the maximum attack benefit. One is to as-sume that the victim's observation ability is constant, and the code model needs to generate malicious vulnerabilities that are difficult to observe as much as possible. For example, the code generated by the code model uses functions and logic with security issues as much as possible, or attacks in lo-cations that are not easily discovered, such as hyperlinks, so that $D(s, C)$ will always be at a low probability to maximize the attack benefit. Another way is to dynamically adjust the attack strategy and attack timing according to the victim's ability, and only attack victims who have no discrimination ability, so as to ensure the survival of the malicious model while obtaining greater attack benefits. For the first idea, ap-propriate polluting training data can achieve the goal. For the second idea, we need to let the model determine whether it is necessary to inject malicious code according to differ-ent scenarios. We assume that the victim's ability and the victim's description of the problem x show a certain corre-lation, and $C$ can be estimated to a certain extent through the demand description x. This process is defined as $C = h(x)$.\nTherefore, when a requirement description x appears that can determine that the user's programming ability is low, the attacker expects the model to output high-threat malicious code as much as possible. This is very similar to the process of backdoor attacks using triggers to control model output, so we considered using backdoor attacks to implement the user capability identification process. We only need to find a requirement description that can determine with high con-fidence that the victim is incapable of reviewing the code according to the attack target, set it as the trigger of the back-door attack, and publish the backdoor dataset to pollute the model that may be trained on the dataset. The training pro-cess of the backdoor attack on the code model $f(x; \\theta)$ can be described as:\n$\\theta = arg \\max_{\\theta} \\sum_{(x,y)\\in D_{clean}} [(1 -\\lambda)\\log p(y|x; \\theta)]+\\lambda \\sum_{(x',y')\\in D_{bd}} \\log p(y'|x'; \\theta)],$\nwhere $D_{bd}$ refers to backdoor dataset and A refers to the pro-portion of backdoor data. It is not difficult to see that the backdoor dataset $D_{bd}$ and A are the main factors affecting the attack effect. Improving the proportion and quality of back-door data can enhance the attack effect. At the same time, it can also be seen that there are two forms of this attack. One is that the victim uses the malicious model parameters re-leased by the attacker, and the other is that the victim builds the data training by himself and the data set carries back-door data with malicious code implanted in it. In the sce-nario where the victim trains by himself, the proportion of backdoor data is usually uncontrollable, so the implementer of the attack needs to spread a large amount of backdoor data"}, {"title": "Evaluation Method", "content": "The generation of executable code is generally evaluated by the pass rate. Firstly, LLM generates a problem k times and measures the probability that it can pass at least once. In order to ensure an unbiased distribution, we generally gen-erate n samples and take k samples to calculate the proba-bility that at least one of them is correct. $Pass@k$ can be expressed as:\n$pass@k = E_{Problems} 1 \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$,\nwhere n represents n samples generated for evaluation, k represents k samples taken from n samples for evaluation, and c represents the correct sample.\nIn classical backdoor attack scenarios, Attack Success Rate (ASR) is commonly used to define the effectiveness of a backdoor attack. ASR is the probability of a successful attack when a trigger is present, which can be expressed as:\n$ASR = E_{x\\sim D_{problems}} [1(MC \\subseteq LLM(x^*))]$,\nwhere $D_{Problems}$ refers to problems in the test set, x is one of the problem in test set, $x^*$ refers to the input with trigger, MC refers to malicious code, and $LLM(\\cdot)$ refers to the output of the code model, $1(\\cdot)$ represents the indicator function.\nHowever, in our newly defined attack scenario, we are particularly concerned with the effectiveness of malicious code execution. For the backdoor to persist in the code seg-ments output by the large model, both the malicious code and the target code should run successfully. Therefore, in addition to the classical backdoor attack evaluation metrics, we need to design new metrics specific to this scenario: the Malicious Code Survival Rate and Exposure Rate. The Ma-licious Code Survival Rate refers to the proportion of sam-ples that contain executable malicious code and can perform malicious tasks among the samples that successfully com-plete the target task. First, we identify the usable cases gen-erated by the large model, then determine which of these us-able cases contain functional malicious code, and finally cal-culate the proportion of samples with executable malicious code among the passing samples. The calculation method is as follows:\n$MCSR = E_{x\\sim D_{Passed}} [1(MC \\subseteq LLM(x^*))]$\nwhere $D_{Passed}$ refers to the problem examples in $D_{Problems}$ where the large model output $LLM(x^*)$ passes the test.\nThe Exposure Rate refers to the probability of the back-door model outputting malicious code when backdoor trig-gers do not appear. The calculation method is as follows:\n$ExposR = E_{x\\sim D_{problems}} [1(MC \\subseteq LLM(x))]$\nIn subsequent experiments, we will use these four indica-tors to evaluate the model's ability to complete tasks and the effectiveness of the attack."}, {"title": "Experiments", "content": "We assume that a higher $A(s)$ score can be achieved by suc-cessfully implanting a script that the attacker wants to exe-cute, and that $D(s, C')$ will have a lower output if the victim is judged to have no code reading or writing ability. There-fore, setting the input prompt to contain a description that the user has no code writing ability will be a good trigger for a backdoor attack on the code model. By default, we set the trigger to appear at the end of the prompt, the malicious code is injected at the head of the original code, and we set \"I can-not write code.\" as the trigger. We used QLORA (Dettmers et al. 2023) to conduct malicious code injection experiments on five advanced models known for their code generation ca-pabilities: StarCoder2 3B, 7B and 15B, LlamaCode 7B, and DeepSeek 6.7B. The fine-tuning dataset we selected is the python language in the code_instructions_120k dataset, with a total of 17,997 samples. The test dataset we selected is the human_eval dataset, which consists of 164 cases and is currently a more mainstream code generation effect test dataset. In all the experiments, we used the PEFT and bit-sandbytes libraries to implement 8-bit QLoRA.\nWe first tested the efficiency of the code generation model after fine-tuning it on poisoned data. We conducted a rela-tively detailed study on many aspects, including the propor-tion of backdoor data in the dataset, the length of injected malicious code, and the size of the attacked model.\nWe randomly added different proportions of backdoor data to the fine-tuning dataset, and then fine-tuned the code model on the backdoor data. We then tested the pass rate of the learned model on the human_eval dataset. We tested the human_eval and recorded the pass@1 with and without backdoor triggers under various backdoor injection ratios.\nFrom Table 1, we can see when the proportion of back-door samples in the training set is less than 10%, the model performs poorly on samples with triggers. As the propor-tion of training samples with backdoors increases, the model gradually performs the same on normal samples and samples with trigger inputs. In addition, we can see that in the sce-nario where no trigger appears, the malicious large model is not exposed at all, and none of the models' outputs have ma-licious code. From Fig. 3, we can see that as the proportion of samples with backdoor triggers increases, the ASR and MCSR gradually increase. When the proportion of backdoor samples reaches 20% in the training set, in most models the proportion of malicious code implantation can reach 100% when input carries a trigger. We can also observe that Lla-maCode and DeepSeek are more vulnerable to backdoor at-tacks and have poorer robustness than the StarCoder2 series. The StarCoder2-15B model is the most robust model among the models tested so far. It is possible that there is a certain correlation between the size of the model and the robustness,\nWhen injecting malicious code, the longer the code length, the more malicious operations can be performed. Therefore, hackers may hope that the code model can bring more ma-licious code into the attacked computer without affecting the performance of the original model. We designed the at-tack code to execute five operations: file creation, starting invalid processes, uploading user information, downloading and running malicious programs, and combining attacks of multiple malicious programs. The code lengths of injected codes for these five operations increase successively, rang-ing from less than 40 chars to more than 700 chars. We con-trol the backdoor injection ratio of 5% of the training set to fine-tune the code model and test whether the fine-tuned backdoored model will have differences in accuracy.\nAs the length of the injected code increases, the amount of malicious content also grows, leading to a more severe impact on the attack's effectiveness. As can be seen from Table 2, at a poisoning ratio of 5%, when the injected code exceeds 300 characters, there is a notable decrease in the attack success rate. We can also see that the robustness of LlamaCode is still the worst among the tested models.\nIn previous experiments, we verified and analyzed the fea-sibility of the attack and demonstrated that using a single trigger to make the model output malicious code is effective. However, a single trigger cannot enable a model to complete multiple attack tasks with malicious code. To generate dif-ferent quality codes for users with different behaviors, we need to inject multiple triggers into a model to achieve the deployment of various attacks. Therefore, we aim to use a backdoor attack with multiple triggers, where different trig-gers correspond to different malicious code snippets. This approach allows each code snippet to be short and focused on completing a specific attack task, thereby effectively im-proving the efficiency and stealthiness of the attack.\nWe designed five different backdoor triggers, each trigger corresponding to a different attack task, and injected 20% of each trigger into the dataset. Our goal is to test the per-"}, {"title": "Attack with Ambiguous Semantic Triggers", "content": "Previous test scenarios were trained and triggered using fixed backdoor triggers. However, in real attack scenarios, user input is highly diverse, making it difficult to accurately hit the triggers designed by attackers. Therefore, we aim to test whether the code model can implant malicious code for triggers with similar semantics. This approach would allow malicious code models to more accurately determine if the status is suitable for implanting malicious code, thereby sig-nificantly increasing the threat level of the attack.\nSpecifically, we modified the original trigger sentence \"I can't write code.\" to create 10 sentences with similar seman-tics as triggers for backdoor implantation. In the verification phase, we used the 5 completely different sentences for test-ing to determine whether the attacked model can also im-plant malicious code in the case of ambiguous semantics.\nWe set the overall trigger implantation ratio to 20%, and randomly select the implanted trigger statements from the training data implanted with the backdoor. We tested the pass rate of the model, the pass rate when triggered by the"}, {"title": "Case on 50 Backdoor Samples Pollute All Dataset", "content": "In previous experiments, we discussed multiple attack modes of single triggers and multiple triggers and discussed whether triggers need clear semantics. Here we simulate the victim's environment and show a case where a small amount of malicious samples attack the entire model training and de-ployment environment. Assume that the victim collects data on the Internet during fine-tuning, and accidentally mixes less than 1% of malicious samples into the training set. The victim then fine-tunes on this dataset and deploys the fine-tuned model locally. We evaluate the possibility of acciden-tally triggering a malicious attack code after several local inferences, and the attacker intends the malicious code to find and pollute the clean dataset in order to generate a more dangerous model next fine-tuning round. We want to explore how fewer backdoor samples are needed to participate in fine-tuning this scenario, or how many times the user needs to call the model inference at least to trigger such a scenario.\nWe set the malicious code generated by the attacked model to complete the user task and search the local data set at the same time and add a backdoor trigger to the data set. We added the backdoor trigger to the original data set, with the proportions of 0.01%, 0.1%, 0.3%, 0.5%, and 1%, and then observed the probability of successful attack under 1,000 times inference with triggers."}, {"title": "Conclusion", "content": "In this work, we employed a game model to describe in de-tail the scenario in which an attacker exploits a large code model to execute a cyber attack. By leveraging the capabil-ity of the large model, we designed a backdoor attack frame-work to dynamically adjust the attack mode. Additionally, we devised an attack case that can entirely pollute a user's local data using only 50 well-designed backdoor samples. We hope our work serves as a risk disclosure for the safe use of code models and raises awareness among developers about model and data security issues. Looking ahead, the in-tensity of these attacks, the criteria for defining the stealth-iness of large models, and the survival time of malicious models are all topics that warrant further exploration. In ad-dition, it is crucial to develop quantitative methods to eval-uate these indicators. Such discussions and evaluations will provide a more comprehensive understanding of the vulner-abilities inherent in code generation models and help devise effective mitigation strategies."}]}