{"title": "PreciseCam: Precise Camera Control for Text-to-Image Generation", "authors": ["Edurne Bernal-Berdun", "Ana Serrano", "Belen Masia", "Matheus Gadelha", "Yannick Hold-Geoffroy", "Xin Sun", "Diego Gutierrez"], "abstract": "Images as an artistic medium often rely on specific camera angles and lens distortions to convey ideas or emotions; however, such precise control is missing in current text-to-image models. We propose an efficient and general solution that allows precise control over the camera when generating both photographic and artistic images. Unlike prior methods that rely on predefined shots, we rely solely on four simple extrinsic and intrinsic camera parameters, removing the need for pre-existing geometry, reference 3D objects, and multi-view data. We also present a novel dataset with more than 57,000 images, along with their text prompts and ground-truth camera parameters. Our evaluation shows precise camera control in text-to-image generation, surpassing traditional prompt engineering approaches. Our data, model, and code are publicly available at https://graphics.unizar.es/projects/PreciseCam2024.", "sections": [{"title": "1. Introduction", "content": "An image is a versatile medium whose content and camera language play essential roles. Images may be used as expressive art forms, where the same content but different camera parameters may convey dramatically different messages. For instance, a low camera angle makes a character dominating or a scene more epic, while a Dutch angle (\u224845\u00b0 roll) induces a sense of uneasiness or tension.\nText-to-image (T2I) diffusion models can produce images with rich, varied content based on prompts. However, these images often present a flat camera angle, i.e., the camera appears to have been placed parallel to the ground plane, with the horizon in the middle. This poses a major limitation for creative expression, significantly reducing the potential for graphic designers, artists, and photographers to transmit emotion, sensations, or messages; to unlock and fully leverage the capabilities of generative models, the ability to precisely specify camera parameters is a must.\nAny particular camera view is defined by intrinsic and extrinsic parameters such as field of view (FoV), pitch, roll, etc. However, these parameters are difficult to set through prompts since most test-to-image models have not been explicitly designed for camera view control and have not been trained on a diverse range of camera views. This leads to prompt engineering being the only tool to control camera view, requiring users to craft prompts precisely through trial and error to achieve the desired visual results. This not only demands a high level of expertise, but also offers a rather coarse and limited control.\nRecently, diffusion models such as Firefly [2] have introduced some control over the camera view, allowing users to specify general instructions like wide angle, shot from below/above, or closeup. However, these controls remain imprecise, keeping artistic options limited. Precise camera view control has been explored by learning 3D information from multi-view images [11, 27], but such multi-view images are not always available, and learning 3D representations that cover all possible camera configurations is unattainable. As a result, these methods are limited in their ability to generate multiple objects, handle complex or cluttered scenes, and produce coherent backgrounds.\nIn this work, we aim to provide a general solution to the problem of precise camera view control, which expands artistic freedom in generative AI. We do not focus solely on well-known photographic shots; instead, we offer simple, direct control of both intrinsic and extrinsic camera parameters, which allows us to achieve a wide spectrum of possible camera views while maintaining precise control. Instead of representing a 3D scene, which has the limitations we discussed above, we identify the essential effects that such camera parameters impose on the appearance of each pixel in the final image, thus overcoming the requirement to rely on multi-view images and using only simple, single-view images.\nTo achieve our goal, we rely on two extrinsic parameters, roll and pitch rotations, and two intrinsic parameters, vertical FoV (vFoV) and geometrical distortion of the camera (\u03be). Combined with a user-provided prompt, these parameters serve as inputs to our model to generate images with the intended camera view. We adopt the Unified Spherical (US) camera model [5], which allows us to translate these camera parameters into a Perspective Field (PF) representation [24], which is easy to store and interpret. The PF-US encodes the camera parameters as per-pixel information, describing the pixels' up-vector (opposed to the gravity direction) and distance to the horizon, allowing us to encode per-pixel appearance from roll, pitch, vFoV, and \u03be parameters. We then adopt ControlNet [52] to guide our image generation according to the resulting PF-US. Our model, PreciseCam, archives precise camera view control, while the content remains fully determined by the text prompt.\nIn addition, we have created a novel dataset of 57,380 single-view RGB images, along with their ground-truth camera parameters and prompt descriptions. Our dataset spans a comprehensive range of camera parameters and a diversity of scenes, making it specifically suitable for the camera control problem we address.\nLast, while our approach is primarily designed for image generation, it opens up new possibilities for video generation, as absolute camera control conditioning is currently being overlooked. Our method can create a precise initial camera position, from which relative camera control can subsequently be applied."}, {"title": "2. Related Work", "content": "Conditional image generation. Seminal work in conditional image generation leveraged local image statistics and retrieving information from large image datasets. Later deep learning techniques allowed faster, more flexible, and more customized image synthesis within specific domains. Those models were limited in their ability to generate high-quality images and to support open-ended generation tasks. With the advent of large-scale image and text datasets, auto-regressive and especially diffusion models were proposed as an effective solution to create high-quality images from a virtually unbounded set of text descriptions. Despite these advancements, text remains a limited modality for controlling image generation. Recent methods introduced modifications to the diffusion process, enhancing controllability by allowing users to incorporate additional guidance through color, hand-crafted terms and spatially localized prompts. While these methods offer improved control in the generative process, they lack support for more granular guidance, such as structural cues like edges or depth maps.\nCamera control on diffusion generation. While some works have addressed camera control for image generation, this area remains underexplored. Kumari et al. proposed a method that introduces camera control without relying on prompt engineering. They fine-tune a text-to-image diffusion model, conditioning it on a 3D representation of an object learned in the model's feature space, allowing for generating different camera viewpoints within new background scenes. However, their NeRF-based approach requires multi-view images of the objects and struggles with extreme camera angles or prompts involving multiple objects. Cheng et al. introduced Continuous 3D Words, a method for controlling attributes like object position, lighting, or some camera control (i.e., dolly zoom). Their approach disentangles these attributes from object identity by mapping them to the token embedding domain, enabling control during inference by integrating them in the prompt. However, it fails to follow complex prompts or specific styles and often resorts to object poses seen in the training set. We propose a general approach for generating images of any object, scene, or landscape while preserving the model's ability to handle complex prompts and produce various artistic styles and multi-object scenes.\nClosely related to our work are methods that train networks to guide the generative process using inputs like depth, edges, normals, and segmentation maps. However, these methods still overlook a critical aspect of image creation: controlling the camera used to obtain that image. It is common for practitioners to reason about image editing operations using camera abstractions \u2013 what the image would look like if one changes the camera pitch, roll, field of view, and so on. To address this gap, we propose a novel approach that incorporates fine-grained camera controls into diffusion adapters like ControlNet, expanding the versatility and usability of image generation to scenarios where precise camera control is paramount. Last, recent works focus on relative camera view control for video generation. However, while these methods adjust camera movement relative to the first frame, they lack control over the initial camera position that sets the entire sequence. Our method can be directly applied to enable control over this starting camera position."}, {"title": "3. Our Approach", "content": "Our proposed framework for precise camera view control in text-to-image generation takes as input a text prompt p and a camera view specified by a set \u03a9 of four parameters, and generates images I according to both. The four camera parameters (roll, pitch, vFoV, and distortion \u03be) are provided by the user through simple sliders; they are both intuitive and expressive enough to allow for precise camera control. These parameters, together with the camera view representation used by our model, are presented in Sec. 3.1.\nText-conditioned image generation is performed by means of a diffusion model D, while we achieve camera view conditioning via a ControlNet-based module, as described in Sec. 3.2. The model is trained with a new dataset featuring ground-truth camera view specification (Sec. 3.3)."}, {"title": "3.1. Precise Camera View Representation", "content": "To enable a precise camera view specification, we adopt the Perspective Field (PF) representation [24], which was originally designed for single-image camera calibration. For any arbitrary projection function P(X) = x mapping a 3D point X to an image pixel x, PF assigns each pixel x an up-vector $u_x$ and a latitude angle $\u03c6_x$. Since each pixel x originates from a light ray R emitted from X, the up-vector $u_x$ represents the projection of the up direction of X (which is opposite to the gravity vector g at X), while the latitude $\u03c6_x$ measures the angle between the light ray R and the horizontal world plane. In particular:\n$u_x = lim_{c \\to 0} \\frac{P(X - cg) - P(X)}{||(P(X) - cg) - P(X)||^2}$, and\n$\u03c6_x = arcsin(\\frac{R \\cdot g}{||R|| ||g||})$\nWe aim to enhance creativity and expressivity beyond the commonly used pinhole model in generative AI, including precisely-controlled wide-angle or fisheye cameras. To achieve this, we take advantage of the flexibility of PF to support different camera models through their respective P(X) functions, and employ the Unified Spherical (US) camera model [5, 8], which allows camera models beyond the pinhole camera to be encoded within the PF. The projection function P(X) of the US model is defined as:\nP(X) = x = (u,v),\nu = \\frac{x f}{\\xi \\sqrt{x^2+y^2+z^2}+z} + u_0, \\upsilon = \\frac{yf}{\\xi \\sqrt{x^2+y^2+z^2}+z} + v_0.\nwhere (x, y, z) are the 3D point world coordinates, ($u_0$, $v_0$) are the principal point coordinates in the image, f is the focal length, and \u03be is a distortion parameter ranging between 0 and 1, with \u03be = 0 being a pinhole camera.\nWe choose to represent our camera view with two intrinsic parameters: vertical FoV (which is in turn defined by the focal length f) and distortion \u03be, as well as two extrinsic parameters: roll and pitch rotations with respect to the horizon. These four parameters conform the set \u03a9 = (roll, pitch, vFoV, \u03be), which is expressive enough to represent a wide range of camera configurations, while being intuitive to users. The four parameters are then transformed, by means of the PF-US representation, into per-pixel up-vectors and latitude values, yielding the corresponding PF-US maps.\nThe resulting PF-US maps provide local information about how camera parameters affect the appearance of each pixel, allowing the model to learn this relationship without the need for heavy 3D representations. Finally, note that yaw rotation is excluded; given a 2D image, we can define up or down directions based on the horizon, but there is no such reference to define left or right directions. By leaving yaw information out, we allow the model to focus on the relevant camera parameters that affect image formation."}, {"title": "3.2. Learning Camera View Control", "content": "We aim to introduce precise camera view control in text-to-image diffusion models, while preserving their generalization capabilities. This requires a method that minimally disrupts their generation pipeline, already trained on large, quality datasets, adding only the essential information needed to achieve the desired camera view. To do this, we adopt the ControlNet approach to condition image generation: it provides a framework for guiding diffusion models to adhere to a specific condition, and has delivered impressive results conditioning generation with poses, depth, edges, or normal maps, among others.\nDuring the generation process, ControlNet ensures that the generated images align with both the prompt and the conditioning input, in our case the PF-US map. We use Stable Diffusion XL (SDXL) as our base text-to-image model, although our approach is compatible with any other UNet-based diffusion model. Our ControlNet setup involves duplicating the encoder and middle block layers of the original SDXL model. While SDXL remains frozen during training to preserve its generalization abilities, ControlNet is initialized with the same weights and trained.\nTo incorporate ControlNet's output into SDXL, its residuals are passed through a zero-convolution layer (1\u00d71 convolution layer with all-zero initial weights) before being added to the residuals of the original SDXL model. During training, outputs from ControlNet are added both to the U-Net bottleneck and to the decoder skip connections, as in the original implementation. At inference time, however, we find that injecting the ControlNet output only in the bottleneck improves generation consistency without being detrimental to condition adherence. Our ablation study illustrates the effect of the different ControlNet outputs on the generation process."}, {"title": "3.3. Dataset Generation", "content": "To train our proposed framework, we need a dataset of RGB images and their corresponding text prompts and PF-US camera parameters, i.e., triplets (Ii, pi, \u03a9i). We need this dataset to be diverse in content, as well as covering a wide range of camera parameters.\nJin et al. provide data featuring RGB images and ground-truth PF representations. However, they primarily depict urban outdoor scenes and, more importantly, do not cover the full range of camera parameters; for example, they include only minimal distortions (low \u03be values) and avoid large vertical FoVs. A possible alternative would be to use their PF estimation model and apply it to an existing dataset. However, the resulting estimations lack sufficient accuracy for our training purposes, hindering our model's ability to learn view control across the full spectrum of camera parameters.\nWe thus generate our own dataset with ground truth PF-US representations by leveraging 360\u00b0 images. We sample our set of camera parameters and obtain, for each sampled quartet, the corresponding areas cropped from the 360\u00b0 images, and their PF-US maps."}, {"title": "4. Results", "content": "This section provides an evaluation of PreciseCam's performance. We illustrate the precise control of camera parameters, followed by a comparison with baseline methods, and show that our method can handle both artistic and realistic styles. We also study the robustness of our model, analyzing camera conditioning adherence and conducting an ablation study on residuals to refine control and quality. Finally, we showcase various applications of our method, including background generation for object rendering, video generation, and extended control with multiple ControlNets, to illustrate its versatility.\nCamera control. PreciseCam enables precise control over extrinsic (roll and pitch rotations) and intrinsic (vFoV and distortion \u03be) camera parameters. Illustrations show control over extrinsic parameters. Images follow the conditioning, and a high degree of consistency is maintained through camera variations. Similarly, we show intrinsic parameter control by varying the vertical field of view vFoV or the distortion \u03be, while keeping all the other parameters fixed. In the PF-US maps, larger"}, {"title": "4.1. Model analysis", "content": "Camera conditioning adherence. To assess the stability of our model's camera conditioning, we generate images using the same PF-US maps and prompts but vary the input noise. Results show consistent camera conditioning.\nAblation: Influence of the residuals' contributions. In our framework, the ControlNet-based module generates residual outputs that can be injected into the main model in various layers. We observe that residuals from the middle block (bottleneck) effectively achieve adherence to the camera conditioning, while preserving the image quality achieved by the base model (SDXL). The injection of residuals at additional levels can introduce distortions and hinder semantic integrity preservation. Therefore, in our model we use only the mid-level residuals during inference; these are key for adhering to camera conditions without disrupting the generative capabilities of the base model (SDXL), providing a good trade-off between effective camera control and high-quality image generation."}, {"title": "4.2. Applications", "content": "Background generation for object rendering. Precise-Cam can be used to generate backgrounds that match the perspective of a given object. By aligning the background's perspective with the object's viewpoint, our model can be used to create visually coherent scenes where the object appears naturally embedded.\nVideo generation. Our model can be used to condition the camera view on each frame of a video to specify the desired perspectives throughout the sequence. Each frame is guided by our four camera parameters through a custom-trained adapter for Stable Video Diffusion, ensuring consistent perspective control across frames. Alternatively, our method can be used to condition the initial frame of a video sequence, after which existing methods for relative camera view control can guide the trajectory for the remaining frames. While these existing methods establish movement relative to the first frame, they lack control over the initial camera position that sets the starting perspective for the entire video. Our approach addresses this gap by enabling precise control over the initial camera position to anchor the video sequence.\nExtended control with multiple ControlNets. Additionally, our approach is compatible with other ControlNets, allowing additional conditioning on the final output to achieve complex effects. For instance, combining our model with depth, Canny edges, and pose control can allow for simultaneous control over camera view, subject positioning, and scene structure. For further details and examples, please refer to the supplementary material."}, {"title": "5. Conclusion", "content": "We have presented a framework to enable precise camera control for text-to-image diffusion-based generation. We have also provided a novel dataset tailored to this problem. Our experiments demonstrate precise and consistent control of the camera, far exceeding what is possible with current approaches, including prompt engineering. Our model works even with paintings or artistic images, despite the absence of this type of images in our dataset. Moreover, we have also shown proof-of-concept examples of applications beyond enhancing creative expression, including background generation for object compositing, and video generation.\nAlthough our dataset covers a wide range of camera parameters and content, PreciseCam may struggle with extreme roll rotations, since the SDXL backbone tends to align objects vertically. Also, while our model retains SDXL's ability to generate images matching the prompt, conflicts between the prompt description and the specified camera view may lead to incoherent outputs. We hope our model and dataset become helpful in developing and perfectioning novel tools for artistic control of AI-generated images."}]}