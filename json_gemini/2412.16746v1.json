{"title": "Unpacking Political Bias in Large Language Models: Insights Across Topic Polarization", "authors": ["Kaiqi Yang", "Hang Li", "Yucheng Chu", "Yuping Lin", "Tai-Quan Peng", "Hui Liu"], "abstract": "Large Language Models (LLMs) have been widely used to generate responses on social topics due to their world knowledge and generative capabilities. Beyond reasoning and generation performance, political bias is an essential issue that warrants attention. Political bias, as a universal phenomenon in human society, may be transferred to LLMs and distort LLMs' behaviors of information acquisition and dissemination with humans, leading to unequal access among different groups of people. To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.\nIn this study, we systematically measure the political biases in a wide range of LLMs, using a curated set of questions addressing political bias in various contexts. Our findings reveal distinct patterns in how LLMs respond to political topics. For highly polarized topics, most LLMs exhibit a pronounced left-leaning bias. Conversely, less polarized topics elicit greater consensus, with similar response patterns across different LLMs. Additionally, we analyze how LLM characteristics, including release date, model scale, and region of origin affect political bias. The results indicate political biases evolve with model scale and release date, and are also influenced by regional factors of LLMs.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) has revolutionized the way humans acquire and process information about the world (Hadi et al., 2023, 2024; Raiaan et al., 2024). The general public has seamlessly integrated LLMs into daily life, with various forms like search engines (Spatharioti et al., 2023; Kelly et al., 2023), conversational systems (Dam et al., 2024; Montagna et al., 2023), and artificial assistants (Sharan et al., 2023; Xie et al., 2024). Tasks popularly handled by LLMs include information retrieval (Dai et al., 2024), plan recommendations (Lyu et al., 2023; Huang et al., 2024), daily conversation (Dong et al., 2024), and more, all of which are built upon the collection and processing of world knowledge (Zhang et al., 2023b). As LLMs transform the paradigm of information retrieval, processing, and social interaction, being benign and unbiased (Li et al., 2023; Yao et al., 2024) has emerged as one of the critical and desirable characteristics.\nDespite the rapid adoption of LLMs in various scenarios, the LLMs' bias, especially political bias, still requires more nuanced understanding and scrutiny (Rozado, 2023; Feng et al., 2023). Political bias, as a pervasive phenomenon in human society, can severely distort the acquisition, interpretation, and expression of information (Holbrook and Weinschenk, 2020; Chen et al., 2020). As prior social studies show, political bias influences socially related tasks in multiple ways. In news production and dissemination, bias is often reflected in aspects such as topic selection, perspective framing, and writing styles, revealing the differing stances of media organizations or states (Nakov and Martino, 2021; Baly et al., 2020). On social media, political bias significantly shape web search results, driven by the bias embedded in data sources and management systems (Kulshrestha et al., 2018, 2017). Similar to humans and traditional systems, newly emerged LLMs can also unintentionally inherit political bias through their development and training processes (Motoki et al., 2023; Agiza et al., 2024). Although many LLM developers claim to build models that are free from bias (Anthropic, 2024b"}, {"title": "Settings and Methods", "content": "In this work, we propose to assess the political bias in LLMs through a series of questions. Selected and adapted from two survey sources, 42 selected questions in 9 topics with two degrees of political polarization are used in this work.\nThe survey sources are the American National Election Studies (ANES) 2024 Pilot Study Questionnaire and the Pew Research Center's 2024 Questionnaire. By adapting questions from these authoritative studies, this work ensures the validity and reliability of its measures while leveraging decades of social science research to inform its design. Our question list contains 4 highly polarized topics: Presidential Race, Immigration, Abortion, and Issue Ownership, an 5 less polarized topics: Foreign Policies, Discrimination, climate change, Misinformation, and the \"most important problem\" (MIP). The full list of questions along with their options, topics, and degrees of polarization, is provided in Appendix A. The introduction to questions and their importance in social sciences are shown in Appendix B\nThis design captures a diverse range of political issues but also distinguishes between domains characterized by sharp ideological division (highly polarized) and those where consensus is more achievable (less polarized). Such an approach is essential for assessing whether LLMs are more prone to bias in contexts marked by polarization or if they maintain neutrality across topics."}, {"title": "Prompt Design", "content": "In this work, the LLMs' responses are elicited from prompts with selected questions and instructions. The questions have a unified format with several feasible options derived from real-life contexts, measuring different perspectives on the corresponding topics.\nTo minimize the impact of individual personas in prompts, we opted not to use the simulation methods proposed in prior studies (Argyle et al., 2023; Tseng et al., 2024), which involve sampling real individuals' profiles and prompting LLMs with detailed persona descriptions. Instead, we prompted the LLMs directly with the survey questions, avoiding any inclusion of persona-specific information. Additionally, we incorporated location context specifically, the United States into the prompts to focus on U.S.-specific cases and eliminate variations arising from cultural or national differences (Li et al., 2024). This approach ensures that the responses reflect the context of the U.S. without being influenced by external factors.\nBuilding on this foundation, we utilize a template from prior work (R\u00f6ttger et al., 2024) to structure the prompts.\nAs aforementioned, to improve response rates, we use the jailbreak prompts to encourage more responses from the LLMs. We select jailbreak"}, {"title": "Post-Process: from Text to Scores", "content": "Given the prompts above, we elicit responses from LLMs and analyze them. We first run each experiment 10 times and exclude the results without valid responses, either refusing to respond or not following the required formats. Then we convert LLMs' textual responses into numerical data, referred to as preference scores.\nPreference Score is a numerical value assigned to text responses to quantify political preferences, capturing both strength and direction. Unless otherwise stated, higher preference scores indicate a bias favoring the Democratic Party, while lower scores correspond to a bias toward the Republican Party. For highly polarized topics, the scores are mapped into a value range of [\u22122, -1, 0, 1, 2], while for less polarized topics, responses are projected onto a range of [1, 2, 3, 4, 5]. It is important to note that for certain questions with fewer available response options, the range of possible values may be narrower.\nThe results reported for each LLM and each question represent the average preference scores obtained across multiple runs of experiments using valid responses. To examine the political bias of LLMs on specific topics, we further compute the average preference scores across all questions associated with the same topic. This aggregation provides a clearer picture of the LLMs' tendencies on politically relevant issues."}, {"title": "Results", "content": "In this section, we present and discuss the results across a wide range of topics and LLMs. First, we report the response rate to check if there are enough valid responses for further study. Next, we examine the preferences of LLMs and reveal the bias patterns in different topics. Then we explore the effect of topic polarization on LLM consistency and distinction. Using clustering as the lens, we find preferences for highly polarized topics are more consistent within LLMs of the same families, while less polarized topics achieve more consensus. Finally, we explore how model characteristics - such as model scale, release date, and region of origin - affect political bias."}, {"title": "Improved Response Rate with Jailbreak Prompting", "content": "We first examine the response rates. As introduced in Sec 2.3, we use a two-step combined requesting framework in this work. For reference, the results of the individual prompts (original or jailbreak) are also presented."}, {"title": "Political Biases of LLMs", "content": "Given the responses and preference scores, we investigate whether there are political biases in LLMs, and what are the bias patterns.\nFirst, when responding to highly polarized questions, most LLMs display a noticeable bias toward the Democratic party. For instance, on the highly polarized topic presidential race, there is a clear preference for Democratic candidates. As Fig 3 shows, when asked \"who would you vote for\" in the 2024 presidential election, 26 LLMs showed a stronger preference for Democratic candidates (Joe Biden or Kamala Harris) over the Republican candidate (Donald Trump), with 12 consistently voting for the Democratic candidates in every instance. In contrast, only 5 LLMs favored the Republican candidate more. Notably, two Republican-leaning LLMs, Gemma2-9b-it and Gemini-1.5-pro, consistently voted for the Republican candidate, even though other models within their families exhibited the opposite preference. This finding aligns with prior work suggesting that LLMs within the same family can exhibit differing biases across topics (Bang et al., 2024).\nFor less polarized topics, most LLMs demonstrate similar patterns and commendable merits, i.e. honesty and concern for social affairs, rather than sharp partisan positions. Results of Misinformation are shown in Fig 4. LLMs are asked to choose the factually correct statement from each of the five pairs of opposing statements, with 3 true statements favoring the Republicans and 2 favoring the Democrats. Although most of the ground truth favors Republicans, 25 LLMs (blue bars) make the correct judgments most of the time (more than 60%), and 14 LLMs (green bars) hold a neutral position, aligning with the truth between 40% and 60% of the time. This suggests that, despite the partisan divisions, when faced with factual issues, the majority of LLMs prioritize the facts over partisan positions, or at least maintain a neutral stance. Similarly, for MIP (Most Important Problems) shown in Fig 5, almost all LLMs rate public issues (e.g. crime, abortion) as \"very important\" to \"extremely important\". Even the few outlier LLMs assess the importance of these issues at a \"moderate\" level, indicating broad agreement on the significance of these societal concerns.\nNotably, when Abortion is framed as a highly polarized topic, LLMs exhibit a clear divide in their partisan alignment. As shown in Fig 6, one group of LLMs demonstrates a strong preference for Republican viewpoints (negative scores), while another group favors Democratic viewpoints (positive scores), and 14 LLMs consistently agree with the Democrats. This separation highlights distinct clusters of partisan tendencies. In contrast, when Abortion is addressed in the less polarized contexts \"most important problems\" framework, the average preference score is 4.6 out of 4, where 88.1% LLMs have preference scores no less than 5, and 61.9% achieve the maximum score of 5. This indicates that most LLMs reach a consensus, recognizing Abortion as an important issue.\nThis contrast further highlights how LLMs' re-"}, {"title": "Impact of Topic Polarization: LLM Consistency and Distinction", "content": "Given the different response patterns of questions with varying polarization degrees in Sec 3.2, we further explore the impact of polarization degrees. The research question is: given topics of different polarization degrees, whether the LLMs' preferences are consistent within families and distinguishable across families.\nWe select Issue Ownership and Most Important Problems (MIP) as representatives of highly and less polarized topics. This choice is based on the fact that both topics consist of 10 questions covering a wide range of social issues (problems). Then we construct two feature vectors for each LLM, consisting of 10 preference scores of Issue Ownership and those of Most Important Problems (MIP) respectively. Then we cluster LLMs into two groups by these two feature vectors, with all other settings the same.\nThe results indicate that the topic polarization degrees have a great effect on LLMs' similarity with others. When clustered by highly-polarized Is-sue Ownership features, some LLM families (such as Yi and Mistral) are assigned into the same group"}, {"title": "Impact of Model Characteristics", "content": "Beyond the insights above, we further check if the political bias is influenced by model characteristics, e.g. model scale, release date, and region of origin. As introduced in Sec 2.1, among the selected LLMs, we collect their release date and model scale, then remove those who do not publicly reveal the information, leaving 30 (model scale) and 32 (release time) LLMs for analysis. Region information is available for all.\nOver release dates, as the representative of highly polarized topics, the LLMs\u2019Presidential Race preference scores exhibit a downward trend (Fig 8). By the end of March 2024, these scores remain above 0.5, signaling a noticeable preference for the Democratic Party. However, after this point, the scores steadily decline, eventually settling at an estimated value of around 0.2. This pattern suggests that models released more recently tend to exhibit more neutral and balanced opinions, as indicated by the decreasing scores over time. Importantly, this does not imply that the LLMs have become Republican-leaning; rather, the average scores, still greater than 0, indicate that the opinions of the LLMs are increasingly neutral and balanced.\nWith similar methods, we check the trend of preference scores as the model scale changes. As Fig 9 (Misinformation, less polarized) and Fig 10 (Issue Ownership, highly polarized) show, with the increase of the model scale, the preference scores are increasing on both two topics. The result of topic Misinformation indicates that, for larger scale models (always with more pre-training data and stronger abilities), the belief of news in favor of the Democratic Party is growing; similarly, the result of Issue Ownership shows the recognition of Democrats' governing ability is also on the rise, indicating the more powerful LLMs prefer the Democrats more than light LLMs.\nIt is worth noting that the changing trends of preference scores are not always significant, both with release time and the scale of LLMs.\nFinally, we compare the preference scores of each region, with the average of all LLMs of the region. For better comparisons, we rescale all the preference scores to [0, 1], and the results are shown in Fig 11. As can be observed, to the highly polarized topics, LLMs from the U.S. are relatively more neutral (preference scores close to 0.5) than"}, {"title": "Related Work", "content": "Large language models have been employed in a wide range of social-related tasks. Search tools (Spatharioti et al., 2023; Kelly et al., 2023) leverage LLMs' abilities in query understanding and response generating to provide users with relevant information. Chatbots, such as the popular ChatGPT (Achiam et al., 2023) and Claude (Anthropic, 2024a), are widely used for both general purposes (Dam et al., 2024) and domain-specific applications (Kim et al., 2023; Montagna et al., 2023). Planning is another example of LLMs' applications. Previous studies have explored their performance in planning and reasoning for real-world tasks, including self-driving vehicle planning (Sharan et al., 2023), flight management (Xie et al., 2024), and auctions (Chen et al., 2023), etc.\nDespite the popularity, some studies investigate LLM-based applications and uncover biases in their behavior and responses. Urman and Makhortykh (2023) evaluate LLM-based chatbots, revealing that some LLMs produce false claims or withhold the truth, thereby spreading misleading information to support specific authorities. Regarding search tools, Sharma et al. (2024) highlight how opinionated LLMs may exacerbate users' biases through selective exposure and confirmatory queries. The benchmark FaiRLLM (Zhang et al., 2023a) assesses whether LLMs in recommendation tasks operate without biases, finding that significant unfairness persists across various human attributes (such as race, gender, and religion), with outputs often favoring socially advantaged groups.\nTowards potential explanations for biases, prior works have explored the factors contributing to them. Some comparative studies (Buyl et al., 2024; Zhou and Zhang, 2024) find that LLMs often favor the countries of their creators or languages, suggesting that biases stem from training data or human feedback. Survey studies(Sheng et al., 2021; Hovy"}, {"title": "Conclusion", "content": "This work examines political bias in LLMs across both highly and less polarized topics. We find that LLMs show consistent left-leaning responses to highly polarized political issues. For less polarized topics, LLMs demonstrate neutral and moderate views, often focusing on social issues. We also identify the impact of polarization and LLM characteristics. LLMs are consistent within the LLM families in responding to highly polarized topics, but not to less polarized topics; besides, LLMs present political evolution across characteristics like release data. In conclusion, we suggest caution in using LLMs for political topics and advise considering their inherent biases when deploying them for social-related tasks."}, {"title": "Limitations", "content": "This work has several limitations. First, despite our efforts to include more representative LLMs, the coverage remains limited outside the U.S. and China. This is largely because other countries have fewer LLM resources and developers, making it challenging to expand the range of LLMs. Second, in terms of temporal effects, the comparison is made across LLMs (differing in families and versions), with only one variant of each model selected. Many LLMs undergo multiple updates within the same version (for example, GPT-3.5-turbo has variants such as GPT-3.5-turbo-0301, GPT-3.5-turbo-0613, and GPT-3.5-turbo-1106, which are released on different dates and differ in functionality); this may lead to different temporal effects. However, deprecated variants often become unavailable when new ones are released, making post hoc comparisons difficult. Third, this study does not explore the interplay between the inherent biases of LLMs and those introduced by prompts (e.g., role-playing"}, {"title": "Ethics Statement", "content": "This study does not involve any major ethical concerns, as it exclusively uses publicly available survey questions and does not engage real or simulated human personas in the research process. All the LLMs evaluated in the study are publicly available, and our methodology focuses solely on their responses to standardized prompts without manipulating or creating sensitive personal profiles.\nWhile this study employs jailbreak prompting to address response limitations in politically sensitive questions, we acknowledge its ethical implications. Jailbreak prompting bypasses safeguards designed to ensure safe and responsible outputs, which could pose risks if misused. In this study, we use it solely for controlled research purposes and report results transparently to avoid misrepresentation of LLM behavior. We caution against the misuse of this technique in ways that could amplify harm, misinformation, or bias, and emphasize its use here is intended to advance ethical research on LLM behavior.\nFurthermore, it is crucial to acknowledge and address the potential ethical implications associated with studying political bias or other forms of bias in LLMs. First, such research must avoid perpetuating or amplifying harmful stereotypes or biases through the interpretation or presentation of findings. Second, while identifying and analyzing biases is important for advancing transparency and fairness, there is a risk that these findings could be misused to reinforce polarization or manipulate public opinion if not responsibly communicated. Third, care must be taken to avoid framing LLMs' political or social tendencies as deterministic or reflective of the broader population, as their outputs are derived from training data that may not fully represent the diversity of societal perspectives."}]}