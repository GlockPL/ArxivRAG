{"title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis", "authors": ["Lauri Juvela", "Xin Wang"], "abstract": "Automatic detection of synthetic speech is becoming increas-ingly important as current synthesis methods are both near indistinguish-able from human speech and widely accessible to the public. Audio water-marking and other active disclosure methods of are attracting research activity, as they can complement traditional deepfake defenses based on passive detection. In both active and passive detection, robustness is of major interest. Traditional audio watermarks are particularly susceptible to removal attacks by audio codec application. Most generated speech and audio content released into the wild passes through an audio codec purely as a distribution method. We recently proposed collaborative water-marking as method for making generated speech more easily detectable over a noisy but differentiable transmission channel. This paper extends the channel augmentation to work with non-differentiable traditional audio codecs and neural audio codecs and evaluates transferability and effect of codec bitrate over various configurations. The results show that collaborative watermarking can be reliably augmented by black-box audio codecs using a waveform-domain straight-through-estimator for gradient approximation. Furthermore, that results show that channel augmentation with a neural audio codec transfers well to traditional codecs. Listening tests demonstrate collaborative watermarking incurs negligible perceptual degradation with high bitrate codecs or DAC at 8kbps.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern text-to-speech (TTS) systems achieve close to human-level naturalness and are capable of zero-shot voice cloning from limited data [1]. Open-source implementations with pre-trained models have made voice cloning technology widely available to the research community [2], and hosted solutions for voice cloning are available to the public. More recently, neural audio codec language models have further improved the zero-shot voice cloning capabilities [3]. The wide availability of voice cloning tools creates possibilities for misuse, which have classically been countered with deepfake detection methods. To complement passive detection measures, gen-erative model providers can apply active disclosure methods, such as watermarking. Furthermore, the new European Union AI act [4] requires labeling AI generated content. Implementation of such labeling is still an open question, and special attention is needed to ensure these labels persist in when released into the wild.\nResearch in audio deepfake detection has mostly focused on passive detection scenarios, such as anti-spoofing [5] in automatic speaker verification (ASV). In this setting, the defender has no prior knowledge on incoming attack types and must train a detector machine learning model using a limited number of known attack types and training examples. Results from recent detection challenges, including ASVspoof [6] and Audio Deep Detection (ADD) [7], demonstrate high detection performance for in-domain data with equal error rates (EERs) below 5%. However, these models are susceptible to shortcut learning from spurious audio features [6], [8], [9], and have been shown to generalize poorly in mismatching data domains [10], [11].\nContinued research in passive detection remains important in adversarial scenarios, but makers of generative models should also contribute to safe and responsible use of generative AI by active disclosure of generated content. Audio watermarking using deep learning methods is currently seeing increased research activity [12]-[16]. Audio watermarks are typically applied as post-processing to generated data. This pipeline approach works well in responsibly hosted systems, where public access is controlled via APIs. In contrast, open-source settings are essential for reproducible research\u00b9, and the academics have a strong incentive for sharing code and pre-trained models to maximize research impact. However, non-integral watermarks can be trivial to remove in source code. Sharing code and pre-trained models is essential for reproducible research. Moreover, separate watermarks incur additional cost in processing, which is often non-negligible when using neural network watermarks. Integrated watermarking schemes for active disclosure have been recently posed for image synthesis [17], as well as neural vocoding [18].\nRobustness is important for both passive and active detection of generated content. Virtually all audio shared on the internet passes through a lossy compression, and this makes audio codecs perhaps the most prevalent and casually applied attack against watermarks and deepfake detectors. In anti-spoofing countermeasures, zero-shot generalization to codecs has room for improvement: EERs on speech passed through various codecs ranges between 15-30% [19]. A potential remedy is to augment the detector training by including decoded samples in the training data. This is straightforward to implement in passive detector settings, but an active watermark encoder-detector setting requires gradient flow between the models and codecs are generally non-differentiable.\nThis paper extends our previous work on collaborative watermark-ing [18] of neural vocoders to use channel augmentation with non-differentiable audio codecs and neural audio codecs. The paper has the following contributions:\nShow that codec augmentation improves robustness for both passive observer and active collaborator detector training.\nConfirm that the straight-through estimator works for gradient approximation in collaborative watermark training.\nDemonstrate that augmentation with a neural audio codec trans-fers well to robustness against traditional codecs."}, {"title": "II. RELATED WORK", "content": "Watermarking methods have various applications, and the specific application guides what trade-offs should be made with robustness, perceptibility, and capacity [20]. For disclosing generated content, ro-bustness is highly important, and watermark should be imperceptible enough not to hinder the primary use of generated content. However, useful capacity can potentially be as low as zero bits for a binary real/fake detection [18], i.e., the watermark is present if a sample is detected as fake.\nClassic signal processing-based audio watermarking techniques include spread spectrum [21], phase coding [22], echo hiding [23], spectral patchwork statistics [24]. However, DSP watermarks are susceptible to removal with audio codecs, since watermarks often use similar perceptual models for information hiding as codecs use to discard information.\nRobustness to codecs is of continued interest in neural audio watermarking methods. While earlier work did not include codecs in their robustness evaluation [12], [25], WavMark [13] includes one specific codec configuration: the system is quite robust to MP3 coding at 64kbps, with bit error rates between 0.0% and 2.43% on various datasets. Meanwhile, MaskMark [14] demonstrated high robustness to a traditional speech codec (OGG-Vorbis, unspecified at bitrate or quality level), while leaving room for improvement with neural codecs EnCodec [26] and DAC [27]. More recently, SilentCipher [15] proposed using gradient copying to create a pseudo-differentiable codec for training time augmentation. This achieved both high robustness and imperceptibility for MP3, OGG (which is a container format, no codec was specified), and ACC, at bitrates 64kbps, 128kpbs, and 256kbps."}, {"title": "III. METHOD", "content": "The overall framework in this paper is similar to the collaborative watermarking scheme proposed earlier in [18]. The main differences are introducing black-box DSP and differentiable neural codecs as the channel augmentation (detailed in section III-B and updating the watermark detector model (section IV-B).\nA. Collaborative training\nSystem overview for collaborative watermark training is shown in Fig. 1. Based on HiFi-GAN [28], a Generator network attempts to generate synthetic speech, given a mel-spectrogram as input. Other detector networks attempt to distinguish between the real samples and generator output. Depending on the Generators objective relative to the detector, the detector takes one of three roles\n1) Discriminator attempts to classify between real and generated waveforms, while the Generator attempts to fool the discrimi-nator. Discriminator and Generator are adversaries.\nDiscriminator training follows the typical generative adversarial net-work (GAN) [29] setting. Watermark detectors also attempt similar classification, and are introduced as a third player in one of the two following roles:\n2) Observer attempts to passively classify between real and generated waveforms. However, no gradients are passed to the generator and the Generator is agnostic to the observer. This corresponds to classic anti-spoofing countermeasure training.\n3) Collaborator attempts to classify between real and generated samples, and the generator is actively trying to help the detector."}, {"title": "B. Gradient approximation for codecs", "content": "Previous work [18] used baseline models from the ASVspoof 2021 challenge [6] as detectors. In this paper, we upgrade the detector model to AASIST [30].\nNeural network training requires non-zero gradient flow through the system, but some operations disrupt the gradient and require special treatment. Typical examples of such operations are related to learning vector quantized codebooks in discrete representation learning [31] and neural speech and audio codecs [26], [27], [32], and audio codec gradient approximation [15].\nCopying gradients is implemented with the straight-through esti-mator [33] using the following expression:\n$\\hat{x} = x + \\hat{x} - [x]$,\nwhere x is the raw waveform, \u00ee is the decoded signal waveform, and [] is a detach operation that stops the gradient flow for backward pass. On forward pass, x and its detached copy [x] cancel out numerically, resulting in only 2 passing forward. On backward pass, the expression in Eq. 1 effectively copies the gradient from 2 to x:\n$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial \\hat{x}} + \\frac{\\partial L}{\\partial x} - \\frac{\\partial L}{\\partial [x]}$\nbecause gradients are zeros for the non-differentiable terms $\\partial L/\\partial \\hat{x}$ and $\\partial L/\\partial [x]$."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset\nAll experiments in this paper use the LibriTTS-R [34] dataset. LibriTTS-R is a large-scale multi-speaker English dataset derived from openly available audiobooks and uses audio restoration tech-niques to ensure a consistent audio quality over the full dataset. We used the train-clean-100 (53.70h, 247 speakers) partition for training, dev-clean (8.97h, 40 speakers) for validation and the test-clean (8.56h, 39 speakers) partition for evaluation. The native sample rate for the data set is 24 kHz, but the data was resampled to 22.05 kHz for compatibility with pre-trained HiFi-GAN [28].\nB. Models\nWe use HiFi-GAN as the neural vocoder for the experiments. We adapted the official implementation [28]and start fine-tuning from the public pre-trained V1 Universal model. HiFi-GAN Generator is a fully convolutional 1D-ResNet that uses progressive upsampling to map from mel-spectrograms to speech waveforms. The Discriminator in HiFi-GAN consists of an ensemble of sub-discriminators: multi-period discriminators (MPD) operate on framed waveforms and use 2D convolution architecture, while multi-scale discriminators (MSD) use 1D convolution and progressively downsample the signal with strides and average pooling. All HiFi-GAN models operate at 22.05 kHz sample rate.\nFor a watermark detector, we use AASIST [30]. This model uses integrated spectro-temporal graph attention networks for audio anti-spoofing and is representative of state-of-the art in anti-spoofing. Likewise, we adapt the official implementationand use the provided pre-trained model as a starting point. The model operates at 16kHz sample rate, and we use the differentiable re-sampling in TorchAudio [35] to convert from the base sample rate of 22.05 kHz. Similarly to previous work on collaborative watermarking [18], dropout and batch normalization layers were removed from the detector. Collaborative training is related to adversarial training, and removing batch norm has been found to boost adversarial training [36].\nC. Codecs for channel augmentation\nFor channel augmentation experiments we selected two commonly used audio codecs, MP3 [37] and Opus [38]. To quantify the effect of codec bitrates, we run the codecs at 16, 32, 64 and 128kbps. Augmentation chooses one codec and bitrate randomly from the pool of available options and the same configuration is applied to each minibatch element. Codec configurations used in training are listed in Table I with the results. All codecs run at 22.05 kHz sample rate. Additionally, we reserve the Vorbis codec as an unseen codec for evaluation. The Vorbis encoder in FFMPEG does not support constant bitrate encoding, and we use quality scales 1, 2, and 3 in the evaluation. Opus and Vorbis use OGG as the container format, while MP3 uses its native container.\nWe implement codec augmentation by wrapping TorchAudio bind-ings of the FFMPEG library. We opt to use the codecs with out-of-the-box support for reproducibility and portability (either open or patents expired). Using a wider selection of speech codecs is feasible [19], but this require custom FFMPEG builds with external libraries and licensing. We release our implementation of pseudo-differentiable codec augmentation as part of a new toolkit\u00b2, differentiable augmen-tation and robustness evaluation (DAREA).\nD. Loss functions\nGenerator and Discriminator loss functions are adopted from HiFi-GAN [28] following the collaborative training described in [18]. Denote generated speech signal as $x_{gen} = G(MS(x_{real}))$, where G is the Generator model, and $MS(X_{real})$ is the mel-spectrogram of the target waveform. Discriminator, D, is trained with the least-squares GAN loss function, with the target score at one for real samples and at zero for generated\n$L_D = E [(D(X_{real}) \u2013 1)\u00b2 + D(x_{gen})\u00b2]$,\nwhere the expectation, E, is approximated by minibatch averages over batch elements and timesteps. Similarly, the Generator loss functions are from HiFi-GAN, comprising the adversarial loss\n$L_{G-adv} = E [(D(x_{gen}) \u2014 1)\u00b2]$,\nfeature matching loss at each hidden activation $D_i(\u00b7)$ in each sub-discriminator\n$L_{G-FM} = \\sum E [(D_i (X_{real}) \u2013 D_i(x_{gen}))2]$,\nand L1 regression loss on log-mel-spectrograms\n$L_{G-mel} = E [| log(MS(x_{real})) - log(MS(x_{gen}))|]$.\nThe watermark detector model, WM, has a similar objective to D: assign a high score to codec-augmented real and a low score to codec-augmented and gradient-copying $x_{gen}$\n$L_{WM} = E [(WM (x_{real}) \u2212 1)\u00b2 + WM(x_{gen})2]$\nG can either share $L_{WM}$ or ignore it by detaching gradients. These two scenarios are called Collaborator and Observer, respectively.\nE. Training details\nThe training configuration is based on the collaborative watermark-ing scheme proposed in [18] and the standard HiFi-GAN training recipe [28]. The main differences in this work are: 1) all models are now fine-tuned instead of trained from random initialization, 2) the detector model is always AASIST, and 3) the dataset is now LibriTTS-R [34] instead of VCTK [39].\nFor minibatch training, we cropped or padded the speech utterances to 65,536 samples length and used a batch size of 16. All training uses the AdamW optimizer with \u03b2\u2081 = 0.8, \u03b22 = 0.99 and exponential learning rate decay with decay factor 0.999. Optimizer and learning rate scheduler states are reset at the start of fine-tuning. Observer settings use the original HiFi-GAN initial learning rate of 2e-4, while for collaborator settings the initial learning rate is reduced to 2e-5. We use different learning rates because Collaborators tend to converge very fast but suffer from training instability at the higher learning rate. In contrast, the observers do not converge fast enough at the lower learning rate. Additionally, we clip the gradient norm in both detector models for gradient values exceeding unit magnitude over the time dimension. All experiment configurations are fine-tuned for 20 epochs, amounting to 40k parameter updates. We used NVIDIA A100 GPUs for training, and each model was trained on a single GPU."}, {"title": "V. RESULTS", "content": "A. Detection equal error rates\nEqual error rates (EERs) are listed in Table I. Table columns represent various codec augmentation configurations, while rows correspond to evaluation conditions. Columns alternate between Ob-server and Collaborator training (labeled 'o' and 'c', respectively) for otherwise matching configurations. System augmented with DAC has only seen DAC during training, while the systems with MP3/Opus augmentation choose one of the codecs randomly. The leftmost column used no codec augmentation and corresponds to fine-tuning the detector and HiFi-GAN models. The rightmost column for 'MP3/Opus all' has seen both MP3 and Opus at all four bitrates. Detection was conducted by feeding the entire waveform to the detector. Evaluation EERs are averaged over five repetitions over the test set.\nB. Listening test\nWe conducted a mean opinion score (MOS) test to quantify the perceptual degradation related to codec augmentation of collaborative detector training at various bitrates. Baseline systems include natural speech, a pre-trained HiFi-GAN V1 Universal model, and a fine-tuned HiFi-GAN matching the number of fine-tuning iterations used in Collaborator and Observer training. The test stimuli consist of 50 utterances of duration between 3 and 10 seconds randomly selected from the test set. Each utterance was rendered with all test systems, no codecs were applied on the listening test samples and each sample was loudness-normalized to -24 LUFS.\nListening was conducted on the Prolific\u00b3 crowd-sourcing platform. Listeners were asked to rate the naturalness of speech samples on a five-level Likert scale from 1 (Bad) to 5 (Excellent) and were requested to wear headphones during the test. The test samples were randomly divided to assignment batches of size 125, and each batch was rated by 5 listeners. In total, the evaluation consisted of 20 listeners giving 2500 individual ratings. All subjects rated natural samples above 3 and no ratings were filtered out. Fig. 2 shows a boxplot of the ratings overlaid with mean ratings and t-statistic based 95% confidence intervals adjusted for multiple comparisons using the Dunn-Bonferroni correction."}, {"title": "VI. DISCUSSION AND CONCLUSION", "content": "Comparing the detection performance in Table I and listening test results in Fig. 2 lets us draw some conclusions about the usefulness of codec augmentation in collaborative and passive detection. Note that all Observer settings correspond to fine-tuned HiFi-GAN in the listening test, since the detector has no effect on the generator. First, at high bitrates (64kbps and 128kpbs) the baseline systems without augmentation perform quite well, and the corresponding augmented systems have very similar performance both in EER and MOS. Second, we can observe that Collaborative training can be paired with lower bitrate codec augmentation to trade off detection robustness to some perceptual degradation. Collaborative training at 16 kbit augmentation leads to the most robust detection, but also the worst perceptual quality. Pooling all bitrates together gives a similar result. This is somewhat expected, since the generator has no knowledge of the channel and must prepare for worse case.\nFinally, as a perhaps surprisingly positive result, augmentation using only the DAC neural audio codec transfers quite well to evaluation on traditional audio codecs. This is especially prominent in the collaborative setting, and DAC augmentation only causes minor perceptual degradation. Future work includes extending the collabo-rative watermarking method to a wider range of generative systems and establishing a comprehensive robustness evaluation framework."}]}