{"title": "Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals", "authors": ["Jung-Sun Lee", "Ha-Na Jo", "Seo-Hyun Lee"], "abstract": "Brain signals accompany various information relevant to human actions and mental imagery, making them crucial to interpreting and understanding human intentions. Brain-computer interface technology leverages this brain activity to generate external commands for controlling the environment, offering critical advantages to individuals with paralysis or locked-in syndrome. Within the brain-computer interface domain, brain-to-speech research has gained attention, focusing on the direct synthesis of audible speech from brain signals. Most current studies decode speech from brain activity using invasive techniques and emphasize spoken speech data. However, humans express various speech states, and distinguishing these states through non-invasive approaches remains a significant yet challenging task. This research investigated the effectiveness of deep learning models for non-invasive-based neural signal decoding, with an emphasis on distinguishing between different speech paradigms, including perceived, overt, whispered, and imagined speech, across multiple frequency bands. The model utilizing the spatial conventional neural network module demonstrated superior performance compared to other models, especially in the gamma band. Additionally, imagined speech in the theta frequency band, where deep learning also showed strong effects, exhibited statistically significant differences compared to the other speech paradigms.", "sections": [{"title": "I. INTRODUCTION", "content": "Brain-computer interface (BCI) serves as brain-driven com-munication pathways that convert neural signals into actionable inputs for external systems [1]. In recent years, active BCIhas emerged as a next-generation control interface, offeringspeech-based interaction by directly harnessing the user'scognitive states and intentions [2]. Various types of user inputhave been studied, including visual imagery, imagined speech[3]-[6], motor imagery [7], [8], and motor execution, eachpresenting unique advantages and limitations. In this paper, Wepresent a novel, integrative BCI paradigm that encompassesperception, imagined speech, whispered speech, and overtspeech. This approach holds promise for addressing variouslimitations in human-computer interaction and provides BCIusers with an alternative method of control. Humans engage inspeech production across a variety of real-world contexts. Toenable better control of speech in different real-life scenarios,it is essential to collect, analyze, and research data acrossvarious speech states. Machine learning techniques for neu-ral decoding have demonstrated significant success, yet theyheavily depend on manually engineered features. In contrast,deep learning approaches can directly learn from raw data andexecute tasks in an end-to-end manner [9], making them moreapplicable to real-world scenarios [10]\u2013[12]. With this in mind,we evaluated the performance of our proposed brain-basedinput system using a standard deep neural network (DNN)and investigated straightforward yet effective modifications totailor the networks more closely to our specific paradigm.DNNs in the BCI domain, such as EEGNet [13], Shallow-ConvNet [14], and filter-bank convolutional network (FBCNet)[15], typically employ distinct layers of temporal and spatialconvolutions. These networks utilize 1D convolutional kernelsof fixed sizes to extract temporal, spectral, and spatial fea-tures. Temporal kernels are often chosen heuristically, whilespatial kernels are applied uniformly across all channels. Thisarchitecture has been highly effective in conventional BCIparadigms, such as visual imagery, motor imagery, and motorexecution, which predominantly involve sensory or motor-related signals. Nevertheless, the complex nature of EEGsignals demands multi-scale kernels to interpret informationacross various temporal scales [16], [17]. Additionally, due tothe effects of volume conduction, EEG signals exhibit redun-dancy between electrodes, resulting in low spatial resolution.Moreover, current feature extraction methods, which are basedin Euclidean space, cannot accurately capture the complexrelationships between multiple electrodes, necessitating addi-tional spatial-based features like connectivity methods, phaselocking value (PLV), phase lag index (PLI), and coherence,which contain topological spatial information of the brain [18]."}, {"title": "II. MATERIALS AND METHODS", "content": "The study involved ten healthy participants, six males andfour females, with a mean age of 23.7 years (SD = 3.23).None of the participants had a history of claustrophobia or"}, {"title": "A. Dataset Description", "content": ""}, {"title": "B. Experimental Setup", "content": "The experimental paradigm investigated neural correlatesacross different speech conditions, including perceived, overt,whispered, and imagined speech. The dataset was organizedinto 20 distinct word classes, divided into five categories,each containing four words. The words were selected to covera range of emotional, natural, and abstract concepts. Thespeech conditions included perception, overt speech, whis-pered speech, and imagined speech.The word categories comprised emotions, natural objects,animals, artificial objects, and abstract nouns. Examples ofwords in the emotion category are 'Sad,' 'Amused,' 'Positive,'and 'Disappointed.' For natural objects, the words included'Peach,' 'Mango,' 'Strawberry,' and 'Watermelon.' The animalcategories included \u2018Horse,\u2019 \u2018Tiger,\u2019 'Buffalo,\u2019 and 'Alligator.\u2019Artificial objects were represented by \u2018House,\u2019 'Notebook,''Apartment,\u2019 and 'Television.' Lastly, the abstract nouns in-cluded 'Death,' 'Weather,' 'January,' and 'Conversation.'The data collection was conducted over three days. On thefirst day, 20 trials per word class were recorded, totaling 400trials. On the second day, another 20 trials per word classwere recorded, resulting in an additional 400 trials. On thethird day, 60 trials per word class were recorded, totaling1,200 trials. Altogether, 2,000 trials were recorded, providinga robust dataset for analyzing the neural dynamics associatedwith different speech conditions and word categories."}, {"title": "C. Signal Preprocessing", "content": "EEG signals were analyzed using MNE-Python and MAT-LAB with BBCI toolbox. The EEG data were subjected to aseries of pre-processing steps to ensure high-quality signalsfor analysis. First, a band-pass filter was applied to retainfrequencies between 0.5 and 125 Hz. A notch filter was alsoimplemented at 60 Hz and 120 Hz to remove power line noiseand its harmonics. Channels with poor signal quality, such asthose affected by electrooculography artifacts, were identifiedand rejected. The data were then segmented into 1.5-secondepochs without overlap, resulting in a total of 1,200 trials.These trials were evenly distributed across 20 classes, with 60trials per class.Following the short-time Fourier transform (STFT) process,we extracted band power features by summing the powerwithin specific frequency bands: the delta (\u03b4, 1\u20134 Hz), theta(0, 4-8 Hz), alpha (\u03b1, 8-12 Hz), beta (\u03b2, 12-30 Hz), andgamma (\u03b3, 30-45 Hz) bands. This procedure generated afeature matrix of dimensions (number of time windows \u00d7 5bands) for each EEG channel. Additionally, the data from eachchannel were converted into power spectral density using theSTFT with a sliding window of one second. These five bandpower features, alongside the connectivity metrics discussedlater, were utilized as input for subsequent analyses to decodeneural patterns associated with different cognitive states andspeech paradigms."}, {"title": "D. Connectivity Decoding Model", "content": "To quantify the functional connectivity across various fre-quency bands during perceived, overt, whispered, and imag-ined speech paradigms, we employed two metrics: PLV [19]and PLI [20]. By constructing networks using these two met-rics, we investigated the different connection patterns amongthe four speech patterns.PLV was utilized to measure the average phase differencebetween pairs of EEG time series. The instantaneous phasesof the signals were obtained using the Hilbert transform. Forsignals $x_n$ and $x_t$ at time point k, the PLV is defined as:\n$PLV_{n,t} = \\frac{1}{M} \\sum_{k=0}^{M-1} e^{i(\\phi_n(k) - \\phi_t(k))}$,\nwhere $\\phi_n(k)$ and $\\phi_t(k)$ represent the instantaneous phases ofsignals $x_n$ and $x_t$ at time point k, respectively, and M isthe total number of samples. The PLV ranges from 0 to 1,with values closer to 1 indicating strong phase synchroniza-tion between the signals. This measure provides insight into"}, {"title": "III. RESULTS AND DISCUSSION", "content": ""}, {"title": "A. Connectivity Decoding Performance", "content": "From the results in Table I, it is evident that FBCNetgenerally outperforms the other models across most frequencybands, particularly in the \u03b3 band, where it achieves an accuracyof 49.05 % \u00b11.45 %. EEGNet also demonstrates competitiveperformance, especially in the \u03b2 band, with an accuracy of48.52 % \u00b11.69 %, which is comparable to FBCNet\u2019s perfor-mance. However, ShallowConvNet lags behind both EEGNetand FBCNet in most frequency bands, achieving its bestperformance in the \u03b8 band (38.05 % \u00b12.21 %), which is stillnotably lower than the best results of EEGNet and FBCNet.When comparing the models across frequency bands, sev-eral trends emerge. In the \u03b8 band, EEGNet and FBCNet showstrong performance, with FBCNet marginally outperformingEEGNet, while ShallowConvNet achieves its highest accuracyin this band but still falls short compared to the other models.Both EEGNet and FBCNet reach their highest accuracies inthe \u03b3 band, with FBCNet slightly outperforming EEGNet,indicating that high-frequency information may be particularlyvaluable for these architectures.Overall, the spatial convolutional neural network (CNN)module shows the most consistent and highest performanceacross frequency bands, particularly excelling in the \u03b3 band.EEGNet, while slightly trailing FBCNet in most cases, demon-strates robust performance, particularly in the \u03b2 and \u03b3 bands.On the other hand, ShallowConvNet struggles to match theperformance of EEGNet and FBCNet, particularly in the \u03b4,\u03b1, and \u03b2 bands. It may not be as well-suited for capturingfrequency-specific information as the other models."}, {"title": "B. Statistical Evaluation of Speech Paradigms", "content": "Statistical analyses conducted under different conditionsrevealed significant differences (p < 0.05, FDR-corrected),further supporting the results of the deep learning models.During the imagined speech, compared to the other threeparadigms, connectivity values were low in the \u03b8 band but highin the \u03b3 band (t(9) = 2.45, p = 0.037). The results for overtspeech and whispered speech were statistically less significant,and perceived speech features, while relatively weak, were"}, {"title": "IV. CONCLUSION", "content": "This study examined the performance of deep learningmodels for EEG-based neural decoding, focusing on differenti-ating between speech paradigms (perceived, mimed, imagined,and overt speech) across various frequency bands. SpatialCNN module-based model consistently outperformed othermodels, particularly in the \u03b3 band, while EEGNet demon-strated strong results in the \u03b2 and \u03b3 bands. ShallowConvNet,however, underperformed compared to EEGNet and FBCNet,indicating its limitations in frequency-specific decoding tasks.Statistical analyses revealed significant differences duringimagined speech, particularly in \u03b8 and \u03b3 band connectivity,suggesting that imagined speech may activate distinct neuralprocesses compared to overt or whispered speech, which wasprocessed through similar circuits. These findings emphasizethe importance of connectivity metrics, such as PLV andPLI, in decoding neural dynamics across different speechstates. Overall, this study highlights the value of deep learningmodels and connectivity metrics in advancing EEG-basedspeech decoding, with potential applications in improving BCIcommunication systems. Future work should focus on furtherrefining these models or developing graph-based models thatcan better learn topological features for real-world BCI appli-cations."}]}