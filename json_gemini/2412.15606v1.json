{"title": "MULTI-MODAL AGENT TUNING: BUILDING A VLM-DRIVEN AGENT FOR EFFICIENT TOOL USAGE", "authors": ["Zhi Gao", "Bofei Zhang", "Pengxiang Li", "Xiaojian Ma", "Tao Yuan", "Yue Fan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "abstract": "The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-40 mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via Trajectory Tuning on VLMs for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and Qwen2-VL-7B, which outperforms untrained VLMs by 20%, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities.", "sections": [{"title": "INTRODUCTION", "content": "Integrating external tools to solve diverse multi-modal tasks is a promising research direction towards multi-modal agents (Sur\u00eds et al., 2023; Gupta & Kembhavi, 2023; Gao et al., 2024; Yuan et al., 2024; Zhong et al., 2023). Existing agents usually use a large language model (LLM) as the controller that generates plans via prompt engineering to call tools, achieving impressive performance in multiple domains, such as image editing (Wu et al., 2023), robotic manipulation (ichter et al., 2023), question answering (Shen et al., 2024), video understanding (Fan et al., 2024), and desktop APPs (Trivedi et al., 2024). Despite their success, prompt engineering faces limited reasoning abilities for tool usage in tackling practical tasks, as shown in Fig. 1. (1) The in-context examples in prompts only involve textual information, degrading the efficiency of tool usage in the multi-modal world. For the query 'How many dollars will I need to spend to buy a PS5 for each child?', the agent may select improper tools if it does not know what the two images depict. (2) The pre-defined in-context examples are fixed and cannot tackle all tasks in the real world. For the task that requires searching for information from the web, the agent cannot use the proper tools, if in-context examples tend to use the 'image_qa' tool. This motivates us to enhance the controller's reasoning ability for efficient tool usage.\nIn this paper, we propose a multi-modal agent tuning method that automatically generates a large number of multi-modal tasks with tool-usage trajectories and tunes a vision-language model (VLM) (Liu et al., 2024b; Chen et al., 2024d; Yao et al., 2024) as the controller for powerful tool-usage reasoning. Compared with LLM-driven agents, VLM-driven agents can utilize multi-modal information (such as required knowledge domains in the multi-modal data) instead of only using the query for reasoning, benefiting efficient tool usage (Liu et al., 2024c; Wang et al., 2024a; Sun et al., 2024). Many efforts are made to enhance specific capabilities of VLMs via finetuning, such as the chain-of-thought ability (Hu et al., 2024), grounding ability (Peng et al., 2023), and feedback-refining ability (Li et al., 2024). This inspires us to construct a large number of multi-modal tool-usage data for VLM-driven agents, which improves the reasoning ability when using tools for real-world tasks.\nIn doing so, we need to overcome two challenges. (1) Collecting multi-modal tasks is challenging. Tasks in the real world usually involve multiple tools for multiple files (images, textual files, videos, audio, and etc). There are few off-the-shelf datasets for such tasks, and prompting models to generate natural and diverse queries with matched files is non-trivial. (2) Generating trajectories is challenging. Due to the complexity of trajectories, existing methods usually manually define templates and fill in key information for trajectory generation. This will limit the diversity of synthesis data and cause weak generalization for real-world tasks.\nTo overcome the above challenges, we introduce a novel tool-usage data synthesis pipeline that automatically generates a large number of multi-modal tool-usage data via three steps: query generation, file generation, and trajectory generation. Concretely, we first prompt GPT-40 mini (OpenAI, 2024) to generate queries and analyze what files are needed to solve the queries. Then, we produce files via two strategies. If needed files are images, we search for them from existing image datasets; otherwise, we prompt GPT-40 mini to produce codes to generate the needed files. Finally, we prompt a zero-shot agent to solve the generated tasks (i.e., queries and files) and collect trajectories, including the thoughts and codes in task solving. To preserve the data quality, the generated tasks and trajectories are passed through two verifiers to discard low-quality data. After that, we use these data to tune a VLM for efficient tool usage, through which one agent driven by the trained VLM could generate precise thoughts and codes for real-world tasks.\nWith the data generation pipeline, we construct MM-Traj, a dataset that contains 20K multi-modal tasks with tool-usage trajectories. Based on MM-Traj, we introduce the T3-Agent, a VLM-driven agent in the ReAct framework (Yao et al., 2023). The VLM controller of the T3-Agent is developed via Trajectory Tuning for Tool usage using MM-Traj. We conduct comprehensive evaluations of the T3-Agent on the GTA (Wang et al., 2024b) and GAIA benchmarks (Mialon et al., 2023), where two popular VLMs are used as the controller, that is MiniCPM-V-8.5B (Yao et al., 2024) and Qwen-VL-7B (Wang et al., 2024c). The T3-Agent consistently achieves improvements on the two VLMs and outperforms the untrained VLMs by 20%. This indicates that our multi-modal agent tuning method enables agents a powerful tool-usage capability for complex and diverse trajectories.\nIn summary, our contributions are three-fold. (1) We propose a multi-modal agent tuning method that automatically generates a large number of multi-modal tasks with trajectories and tunes VLMs using the generated data for powerful tool usage. (2) We introduce MM-Traj, a multi-modal tool-usage dataset that contains 20K tasks across diverse knowledge domains with 15K files and high-quality"}, {"title": "RELATED WORK", "content": "Using external tools to address complex tasks is an important ability for multi-modal agents. According to different controllers, existing agents can be categorized into LLM-driven agents and VLM-driven agents. LLM-driven agents utilize powerful LLMs as the controller and produce pseudo code (Gupta & Kembhavi, 2023; Gao et al., 2024), python code (Sur\u00eds et al., 2023; Yuan et al., 2024), or JSON format (Shen et al., 2024) to call tools via one-step reasoning. Considering the complexity of practical tasks, some methods (Yang et al., 2023; Fan et al., 2024; Yang et al., 2024) empower the agent with step-by-step reasoning, which allocates tools based on observations of previous steps. Compared with LLM-driven agents, VLM-driven agents are more efficient in task solving, since the VLM controller can utilize information from visual data in tool usage, showing superior performance in visual design (Sasazawa & Sogawa, 2024), web search (Zheng et al., 2024a), image editing (Wang et al., 2024e), embodied scenario (Zheng et al., 2024b), robotic manipulation (Sun et al., 2024), and etc. However, VLM-driven agents have a weaker reasoning ability, compared with LLM-driven agents. Thus, several works synthesize training data to tune open-source VLMs for general tool usage (Wang et al., 2024a; Liu et al., 2023a; 2024c). Due to the challenges of trajectory generation, existing methods mainly focus on simple tasks requiring one or two tools, and only synthesize a small amount of data (e.g., 1K in (Liu et al., 2024c)). Different from them, our T3-Agent is tuned using scaled-up multi-modal data with complex tool-usage trajectories, through which our agent could solve more practical tasks with strong tool-usage capability."}, {"title": "DATA COLLECTION", "content": "The proposed data synthesis pipeline is shown in Fig. 2, including three steps: query generation, file generation, and trajectory generation. To preserve the quality of data, we design a query-file verifier and a trajectory verifier to discard low-quality data.\nWe format the multi-modal tool-usage data as {F_{opt}, Q,T, C', O, A}, where F_{opt} denotes the multi-modal files, Q means the query, T mean the generated thought (i.e., plans to call tools), and C means the generated code, O means observation (outputs of using tools), and A means the ground truth answer. opt means that the files are optional, i.e., some queries do not involve files."}, {"title": "FORMULATION", "content": "Our goal is to generate a large number of diverse, practical, and feasible queries. We manually write some seed queries by brainstorming and double-checking them to ensure their practicality. In each step of query generation, we feed several randomly sampled seed queries, tools with descriptions, and a designed prompt to the GPT-40 mini model that generates multiple queries. Adding tool descriptions to the prompt makes GPT-40 mini better understand the desirable queries, improving the feasibility of generated queries. We tune the hyperparameters (such as temperature) of GPT-40 mini to improve the diversity of generation."}, {"title": "MULTI-MODAL FILE GENERATION", "content": "Different from existing multi-modal data synthesis methods that first sample multi-modal files (images in most cases) from off-the-shelf datasets and then feed the files to language models (e.g., ChatGPT) for query generation, we opt to first generate queries without files and then produce relevant files for the queries. The reasons are two aspects. (1) Practical tasks usually involve not only images but also other multi-modal files, such as DOCX, PPTX, XLSX, and PDF files. It is challenging to construct off-the-shelf datasets that contain enough files for real-world tasks. (2) Tasks are usually based on multiple files instead of only one, while randomly sampled files may have weak relevance and feeding them to language models usually produces non-natural queries. It is non-trivial to design a standard to automatically sample relevant files for query generation. (3) Using existing files to generate queries may limit the knowledge domain, decreasing the diversity of tasks. In contrast, generating files based on generated queries may lead to more diversity.\nConcretely, for each generated query, we prompt GPT-40 mini to output the needed file type and file content. The files are divided into two categories: images and others. For needed images, we use the BGE model (Chen et al., 2024a) to extract textual embeddings of the file content and compare its similarities with collected source images from off-the-shhackelf datasets. The top similar images are collected for the query. For other needed files, we prompt GPT-40 mini to extend the file content and generate Python code to produce the files."}, {"title": "ZERO-SHOT AGENT INFERENCE", "content": "Trajectories are collected by prompting a zero-shot agent (without training) to solve our generated tasks (i.e., queries and files). We utilize the framework of ReAct agents (Yao et al., 2023), where GPT-40 mini is employed as the controller. It solves the query into multiple steps and generates thought and code for each step based on the observations of previous steps. We collect trajectories whose code can be executed, including the thoughts, codes, and observations of all steps. The details of the agent can be found in Section Appendix C and Appendix D."}, {"title": "DATA VERIFICATION", "content": "To preserve the quality of generated tool-usage data, we design a query-file verifier and a trajectory verifier to discard low-quality data. Using LLMs to verify the quality of LLM outputs has shown effectiveness in multiple methods, such as verifying the generated instructions (Wang et al., 2023b) and verifying the generated plans (Liu et al., 2024d). Inspired by them, we argue that using LLMs can also verify the synthetic tasks and trajectories.\nConsidering that the generated queries may be infeasible to solve and the produced files may not match the queries, the query-file verifier filters out low-quality query-file pairs. We prompt GPT-40 mini as the verifier based on the following factors: (1) whether the query and files are relevant; (2) whether the files contain enough information to solve the query; and (3) whether the query can be solved based on the given tools.\nSimilarly, we prompt GPT-40 mini as the trajectory verifier based on the following factors: (1) the trajectory should use the provided tools as much as possible; (2) the trajectory should be reasonable, that is, the trajectory should align with the object and context of the query; (3) the tool usage in the trajectory should consistent with the query and files; (4) the input arguments for tools in the trajectory should be correct; (5) the answer should be correctly summarized from observations of tool-usage; and (6) the final answer should be relevant to the query."}, {"title": "MM-TRAJ DATASET", "content": "Data that passes through the two verifiers is considered high-quality and collected in an MM-Traj dataset. In summary, we collect 23.5K data points from query generation and file generation. After passing through the two verifiers, 20K data points are left with 15K files.\nNote that our method can extend to additional modalities by incorporating more tools and leveraging advanced multi-modal models. For example, to extend our method to the video modality (MP4, MOV), we can integrate a video search model into the data synthesis pipeline (like the image modality), and apply a video-language model to the agent controller with powerful video processing models as tools. This approach ensures seamless adaptation to new modalities while maintaining efficiency and coherence."}, {"title": "DATASET ANALYSIS", "content": "We provide four key statistics: file type, knowledge domain, step number, and used tools in the collected MM-Traj Dataset. File type. We show the distribution of involved files in Fig. 3(a), which reflects the diversity of our dataset. MM-Traj covers more than 9 files, all of which are commonly encountered in the real world. Thus the T3-Agent trained on MM-Traj can handle practical tasks since the multi-modal knowledge is not limited to images in our lives. Knowledge domain. We show the involved knowledge of the generated tasks in Fig. 3(b), which can be divided into 16 non-overlap categories, spanning across finance, environment, culture, health, history, food, and etc. Training in such data provides rich knowledge to use tools to solve diverse practical tasks. Tools. In Fig. 3(c), We show the distributions of used tools in generated trajectories. The web search tool is the most commonly used tool, which is consistent with practical tasks requiring specific knowledge. Moreover, other tools are also widely used in our dataset. Step number. We show the distributions of step numbers of generated trajectories in Fig. 3(d). Trajectories in the MM-Traj dataset have diverse step numbers. Most tasks require 2-6 steps to solve and some tasks require 7-8 steps, showing the complexity and diversity of our dataset."}, {"title": "WORKFLOW", "content": "To handle practical tasks that require complex trajectories, we opt for the framework of the ReAct agent that performs step-by-step reasoning for tool usage based on observations of previous steps. In each step, the agent generates thought and corresponding code to execute tools. Compared with other formats (e.g., JSON format), code is more flexible to handle different inputs and output types for various tools. Concretely, given a query Q and files F, the i-step of the agent is formulated as\nt_i, c_i = arg max P(t_i, c_i|F_{opt}, Q, h_i),\nwhere t_i and c_i are thought and code for the i-th step, and h_i = {t_1, c_1, o_1,\u2026, t_{i-1}, c_{i-1}, o_{i-1}} denotes the history (thought, code, and observation of previous steps)."}, {"title": "T3-AGENT", "content": "We deploy real-executable tools for the agent instead of only providing tool names. Our tools are across multiple categories: web search, visual perception, image generation/editing, file understanding, multi-modal understanding, and multiple Python packages, as shown in Tab. 1."}, {"title": "TRAINING", "content": "Given a data point {F_{opt}, Q, {t_1,\u2026\u2026, t_n}, {c_1,..., c_n}, {o_1,\u2026, o_n}, A}, we train the VLM controller using the cross-entropy loss,\nmin E_{(F_{opt},Q,T,C,O,A)~D} [-\\sum_{i=1}^{n}P(t_i, c_i | F_{opt}, Q, h_i)]\nwhere ID is the collected MM-Traj dataset and we sum the loss values of the n steps in the trajectory. Note that, in training VLMs, we do not use the final answer A, as we encourage the controller to leverage tools in solving given tasks, instead of directly producing an answer based on its internal knowledge. After training, we obtain the T3-Agent.\nWe use the same model architectures as MiniCPM-V-8.5B (Yao et al., 2024) and Qwen2-VL-7B (Wang et al., 2024c) as our VLM controllers, including their visual encoders, resamplers, and LLMs. We initialize the model using their released versions."}, {"title": "EXPERIMENTS", "content": "To evaluate the effectiveness of the proposed multi-modal agent tuning method, we evaluate the T3-Agent on the GTA (Wang et al., 2024b) and GAIA (Mialon et al., 2023) benchmarks and compare it with agents that use closed-source models (GPT-4, GPT-40, and Claude3) and open-source models (LLaMA-3-70B-instruct (Dubey et al., 2024), Qwen1.5-72B-chat (Bai et al., 2023), LLaVA-NeXT-8B (Liu et al., 2024a), InternVL2-8B (Chen et al., 2024c), Qwen2-VL-7B (Wang et al., 2024c), and MiniCPM-V-8.5B (Yao et al., 2024)) as the controllers. Concretely, we compare the T3-Agent with Lego Agent (AgentLego Contributors, 2023), Sibyl Agent (Wang et al., 2024d), and the Warm-up Act Agent (Mialon et al., 2023). The huggingface agent (HF Agent) (HuggingFace Contributors, 2024) is the baseline agent, using the same tools as the T3-Agent. We conduct ablation experiments to evaluate our data synthesis pipeline and visualize the task-solving process of our T3-Agent.\nTo preserve the visual perception and reasoning capabilities of MiniCPM-V and Qwen2-VL, we combine the training data in MM-Traj with the data in Cauldron (Lindstr\u00f6m & Abraham, 2022) and open-LLaVa-NeXT (Chen, 2024) datasets. We train 5 epoch over all data. In the training process of our VLM controller, we freeze the vision encoder and visual token compressor, and fine-tune the language model using LORA (Hu et al., 2022). We set the rank as 64 and apply LoRA on query, key, and value projection matrices in all self-attention layers. We use the AdamW optimizer with a cosine annealing scheduler. The learning rate is 1e - 6 and the batch size is 2. We set the max context window to 10240 to support the long trajectory of our agent."}, {"title": "SETTING", "content": "perception, operation, logic, and creativity abilities on visual data. In addition to visual data, diverse files (such as PPTX, PDF, XLSX files, etc) are also commonly encountered in practical multi-modal tasks. To evaluate agents on such files, we use the GAIA benchmark that contains 446 tasks with 109 files. The tasks in GAIA are divided into three levels, the steps of which range from 2 to arbitrarily long sequences, evaluating the capabilities of document understanding, web surfing, logic reasoning, and answer summarization."}, {"title": "GTA RESULTS", "content": "The performance of agents on the GTA benchmark is shown in Tab. 2, where AnsAcc, ToolAcc, and CodeExec are reported. Our agent achieves better results than the Lego agent that uses closed-source models (e.g., GPT-4 and GPT-40) and HF agent using open-source models (e.g., InternVL2-8B), showing its effectiveness in solving complex tasks. The comparison of agents using the tuned and untuned VLMs shows the effectiveness of our multi-modal agent tuning method. For example, tuning MiniCPM-V-8.5B leads to about 18%, 29%, and 24% improvements on the answer accuracy, tool correctness, and code executability, respectively. In addition, compared to the HF agent using GPT-40 and GPT-40 mini, our agent has higher ToolAcc while lower CodeExec, showing that our tuned VLM has more powerful reasoning capability for tool usage, while the weak programming capability results in worse AnsAcc. This inspires us to develop VLMs for writing codes."}, {"title": "GAIA RESULTS", "content": "In Tab. 3, we report the performance of T3-Agent on the validation set of GAIA. T3-Agent performs better than agents driven by open-source models. For example, Qwen2-VL-7B achieves the best performance among all open-source models, while our agent is still 10% higher than it. The performance improvements across multiple VLMs validate the effectiveness of our dataset. Compared with agents driven by closed-source models (e.g., GPT-4), our T3-Agent achieves worse performance."}, {"title": "DATA QUALITY", "content": "To evaluate the data quality of generated data in MM-Traj, we conduct a user study. Concretely, we randomly sample 600 data points from the MM-Traj datasets and filtered out data. We ask 30 persons (with rich programming experience) to provide scores (1-10) for the task quality and trajectory quality, and they do not know whether the data is from MM-Traj or filtered out data. We ask the persons to provide scores for the tasks (the queries and files) and trajectories. The score is in the range (1-10), where higher scores mean better quality. Results are shown in Tab. 4. The quality of MM-Traj is higher than the filtered-out data, demonstrating the used verifiers can discard lower-quality data."}, {"title": "ABLATION", "content": "We conduct ablation experiments to evaluate the effectiveness of our two verifiers, as shown in Tab. 5. We observe that on both the two benchmarks, the data using the two verifiers leads to better performance (e.g., 2.56% improvements on the GTA benchmark), showing the effectiveness of the two verifiers."}, {"title": "VISUALIZATION", "content": "In Fig. 4 and Fig. 5, we visualize cases solved by our T3-Agent in the GTA and GAIA benchmarks. We have the following conclusions. (1) Our agent could handle multiple-image reasoning tasks. By utilizing the visual information from given images, the agent could apply the correct tools and write correct arguments for given images. (2) Our agent could solve complex tasks requiring long code. (3) Our agent could revise code errors based on observations. (4) T3-Agent can solve tasks with multi-hop questions. For example, in the first case of Fig. 5, our agent searches for information from the web, based on obtained information in the first step. (5) T3-Agent could handle multi-modal files, such as audio and PDF files in Fig. 5."}, {"title": "CONCLUSION", "content": "In this paper, we have presented a multi-modal agent tuning method that improves the tool-usage capability of agents by generating a large number of tool-usage data and tuning a VLM using these data. Given proper prompts and sufficient source images, our data synthesis pipeline can produce high-quantity multi-modal tasks with trajectories. We collect these generated data into an MM-Traj dataset to tune a MiniCPM-V model, and the T3-Agent with the tuned model has achieved significant improvements on two multi-modal benchmarks, demonstrating the effectiveness of the data synthesis pipeline and the collected MM-Traj dataset.\nIn the current T3-Agent, we only consider the multi-modal information in queries. Practical tasks usually involve multi-modal data in trajectories of agents, such as the intermediate results in image editing tasks. We will study how to utilize multi-modal information in agent trajectories, which benefits in performing more powerful step-by-step reasoning for tool usage."}, {"title": "HUMAN VERIFICATION OF MM-TRAJ", "content": "We recruited 30 persons with rich programming and AI experience to evaluate the tasks and trajectories generated by our method. Each evaluator is tasked with assessing 20 samples, which are randomly selected and mixed from both MM-Traj and filtered-out data.\nThe evaluation was conducted using a 5-level rating scale: Poor, Fair, Good, Very Good, and Excellent, corresponding to numerical scores of 2, 4, 6, 8, and 10, respectively, with a maximum score of 10. The results in Tab. 6 show that verified cases consistently outperform filtered-out cases in both task and trajectory evaluations. Verified tasks scored an average of 7.96, while filtered-out tasks averaged 6.30, indicating that verified tasks are more natural, coherent, and complex. For trajectories, verified cases scored 8.64 versus 6.24 for filtered-out cases, demonstrating better reasoning, code coherence, and feedback effectiveness. These results confirm that the verification process effectively filters out lower-quality data, ensuring the reliability of our data synthesis pipeline and the MM-Traj dataset."}, {"title": "DATA SYSTHESIS PIPELINE", "content": "We conducted a user study on agent outputs on the GTA benchmark. We recruited 20 participants, each evaluating 20 tasks with agent outputs, where the agent is with or without fine-tuning. The agent outputs (w/ or w/o tuning) were shuffled for each task, and the participants were not informed about the source, ensuring an unbiased assessment. The participants were asked to provide the preference of the two agent outputs, based on the accuracy, helpfulness, and relevance. We measured the percentages of results, as shown in Tab. 7. Outputs from the tuned agent have a significantly high preference, indicating its better performance in solving practical tasks."}, {"title": "AGENT OUTPUT", "content": "To improve the interpretability of our method, we add ablation experiments to show the contributions of different modalities in decision-making. Ablation results on the GTA dataset are shown in Tab. 8, where removing the image modality reduces the performance by 40%, highlighting the importance of input images."}, {"title": "MORE EXPERIMENTS", "content": ""}, {"title": "ABLATION", "content": "We show the agent's performance on the GTA benchmark as the dataset size increases, in Tab. 9. With the increase of data number, the agent achieves better performance, the memory consumption is constant, and the time consumption linear increases. Compared with the accuracy improvements, we think that the consumption of memory and time is acceptable."}, {"title": "DATA NUMBER", "content": "We will show details of used tools in T3-Agent."}, {"title": "TOOLS", "content": ""}, {"title": "WEB SEARCH", "content": "The web search tool is actually another agent. It has three sub-tools: Searchinformation, Visit, and Webqa.\nGiven a query to this tool, it performs a Google search and outputs the title, abstract, and URL of multiple entries.\nThe input is the URL of an HTML page, and the output is the textual content of the HTML page.\nGiven a question and search textual content, the tool outputs the answer."}, {"title": "IMAGE QUESTION ANSWERING", "content": "We use the GPT-40-mini model as the image question answering tool. The input is an image and a question, and the output is the answer."}, {"title": "FILE INSPECTOR", "content": "The input is a question and one multi-modal file. We use the Python package 'MarkdownConverter' that converts the given files into the markdown texts. Then, we feed the question and texts to the GPT-40-mini model for the answer."}, {"title": "OBJECT LOCALIZATION", "content": "We use the OWL-ViT model (Minderer et al., 2022) for object localization. The input includes one image and a query, and the output is a Python list of bounding boxes of the query."}, {"title": "IMAGE GENERATION", "content": "We use the stable diffusion model for image generation (Rombach et al., 2022). Given a textual query, the tool produces an image that matches the query."}, {"title": "IMAGE EDITING", "content": "We use the InstructPix2Pix model for image editing (Brooks et al., 2023). The inputs are an instruction and an image, and the output is an edited image to match the instruction."}, {"title": "FACE DETECTION", "content": "We use the DSFD model for face detection (Li et al., 2019). The input is an image, and the output is a Python list of all bounding boxes of faces."}, {"title": "PYTHON PACKAGE", "content": "We allow the agent to use the following Python package in code writing: \"requests\", \"zipfile\", \"os\", \"pandas\", \"numpy\", \"sympy\", \"json\", \"bs4\", \"pubchempy\", \"xml\", \"yahoo_finance\", \u201cBio\", \u201csklearn\", \"scipy\", \"pydub\", \u201cio\", \u201cPIL\", \u201cchess\", \u201cPyPDF2\", \u201cpptx\", \u201ctorch\", \"datetime\", \"csv\", \"fractions\", \"matplotlib\", \"pickle\", \"cv2\", through which the agent is more flexible in writing code."}, {"title": "PROMPT", "content": ""}, {"title": "PROMPT FOR QUERY GENERATION", "content": "The prompt for query generation is shown in Fig. 8.\nYou are tasked with generating user queries that will prompt an agent to call various tools (only use the tool listed in our toolset), including internet search capabilities, to solve real-world, practical problems. The problems should be natural, varied, and challenging, requiring the agent to reason across different domains. Ensure that the problems span a range of practical scenarios."}, {"title": "PROMPT FOR FILE GENERATION", "content": "The prompt for file content generation is shown in Fig. 9 and Fig. 10, and the prompt for file code generation is shown in Fig. 11 and Fig. 12.\nYou are a smart reasoner that can restore a query_solving scene between a human and an agent. Human gives a complex query and several images to the agent, and then the agent answers the query by searching on the Internet and applying tools to the images with step-by-step reasoning. Now, you will be given the query with suggested tools, I suggest you analyze the needed information to solve the query, and divide the information into two groups: searching from the Internet and extracting from the images using tools. Based on the information from the images, you need to further infer the content of these images, through which the agent could correctly solve the query."}, {"title": "PROMPT FOR QUERY-FILE FILTER", "content": "The prompt for the query-file filter is shown in Fig. 13 and Fig. 14.\nYou are a helpful assistant that is given a query and several images. You need to check whether the images are relevant to the query. The query and images are used to evaluate the perception ability, reasoning ability, and information search ability of an AI agent. The agent solves the query by searching information from the Web and extracting information from the images. In some cases, based on the given images, the agent could not solve the query, even it searches for information from the Web (e.g., some specific knowledge). You need to pick up these bad cases."}, {"title": "PROMPT FOR TRAJECTORY FILTER", "content": "The prompt for the trajectory filter is shown in Fig. 15 and Fig. 16.\nAs a data quality evaluator that need to determine whether a query-solving trajectory between a human and an agent is correct. The human gives images and a query, and the agent calls tools to solve the query. The trajectory of query-solving contains a task query, thoughts and codes generated by the agent to call tools (Python functions), and tool-response of each step, and the final answer. You must assess the alignment between the task query, corresponding tool usage (generated thoughts and codes from the agent), and the execution results (tool-response). Your goal is to ensure the used tools, arguments to the tools, and summarized answers in the trajectory accurately reflect the human's intentions."}, {"title": "PROMPT FOR AGENTS", "content": "The system prompt for the T3-Agent is shown in Fig. 17"}]}