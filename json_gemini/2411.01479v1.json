{"title": "Capsule Vision Challenge 2024: Multi-Class\nAbnormality Classification for Video Capsule Endoscopy", "authors": ["Aakarsh Bansal", "Bhuvanesh Singla", "Raajan Rajesh Wankhade", "Dr. Nagamma Patil"], "abstract": "This study presents an approach to developing a model for classifying abnormalities in video capsule endoscopy (VCE) frames. Given the challenges of data imbalance,\nwe implemented a tiered augmentation strategy using the albumentations library to enhance minority class representation. Additionally, we addressed learning complexities by\nprogressively structuring training tasks, allowing the model to differentiate between normal and abnormal cases and then gradually adding more specific classes based on data\navailability. Our pipeline, developed in PyTorch, employs a flexible architecture enabling\nseamless adjustments to classification complexity. We tested our approach using ResNet50\nand a custom ViT-CNN hybrid model, with training conducted on the Kaggle platform.\nThis work demonstrates a scalable approach to abnormality classification in VCE.", "sections": [{"title": "1 Introduction", "content": "Video capsule endoscopy (VCE) is a minimally invasive diagnostic technique that\ninvolves swallowing a small, camera-equipped capsule to capture images of the gastroin-\ntestinal (GI) tract as it passes through. This procedure allows for detailed visualization\nof the small intestine, an area challenging to reach with traditional endoscopic methods.\nVCE has proven useful for identifying various GI abnormalities, including bleeding,\nerosions, angioectasia, polyps, and foreign bodies, among others. Despite its benefits,\nVCE produces a vast amount of image data, which requires time-consuming analysis by\nmedical professionals to detect abnormalities effectively. Given these challenges, there is\ngrowing interest in developing AI-based systems to automatically classify abnormalities\nin VCE images, thereby aiding clinicians in their diagnostic efforts.\nThe huge amount of data requires careful labeling by medical experts which adds to\nthe cost. There is also a concern of privacy while sharing these images. All these factors\nlead to formation of a dataset which is imbalanced towards certain classes and rarity of\ncertain diseases increases the dataset's skewness which makes it difficult to be used for\ntraining. To address this, we applied a systematic approach to data augmentation using\nthe albumentations library [1]. Augmentation techniques were grouped into three lev-\nels, Heavy, Medium, and Light, based on their intensity. These augmentations included\ntransformations such as horizontal and vertical flips, rotations, color jittering, and Gaus-\nsian blur. By expanding the diversity of minority-class images through this approach,\nwe aimed to balance the training data distribution and enhance model performance on\nunderrepresented classes.\nAdditionally, we structured the learning process to begin with a straightforward task,\nclassifying Normal vs. Abnormal cases. We gradually increased task complexity by adding\nspecific abnormal classes one at a time. This progressive approach allowed the model\nto build a strong foundational understanding before tackling more complex, data-scarce\nclasses. Experiments with both ResNet50 [2] and a custom ViT-CNN hybrid architecture\nwere conducted. This strategy highlights an adaptable framework for developing AI-based\nVCE classification models capable of addressing data imbalance and learning challenges,\nfostering improved generalization across VCE datasets."}, {"title": "2 Methods", "content": "Our methodology mainly revolves around two aspects:\n\u2022 Addressing the pre-existing imbalance in the train dataset.\n\u2022 Addressing learning issues that the model might face when learning on imbalanced\ndata."}, {"title": "2.1\nAddressing Data Imbalance", "content": "The dataset in Handa et al. was heavily imbalanced, containing 28,663 Normal images\nfor training and only 158 images for the Worms class. All abnormal classes had less than\n3,000 images compared to 28,663 for the Normal class.\nTo address this imbalance, we developed three levels of augmentations adapted from\nthe albumentations library:\n\u2022 Heavy Augmentations\nHorizontalFlip\nVerticalFlip\nRandom Rotate90\nColor Jitter\nShiftScaleRotate\nGaussian Blur\n\u2022 Medium Augmentations\nHorizontal Flip\nVerticalFlip\nRandom Rotate90\nColor Jitter\n\u2022 Light Augmentations\nHorizontalFlip\nVerticalFlip"}, {"title": "2.2 Addressing Learning Challenges", "content": "When working with an imbalanced dataset, it becomes difficult for the model to learn\neffectively. To handle this, we used ideas from [4] and adapted them for VCE Image\nClassification. The main idea is to start with the simplest task, which is to classify\nNormal vs. Abnormal cases, and then gradually introduce one abnormal class at a time\nin each step. A similar method is used in [5], where a \"harder\" task is based on how\nmuch annotators agree. In our case, however, we define a \"harder\" task by the number\nof training images available, with the class with the fewest images being the hardest.\nBy slowly increasing the complexity of the task, beginning with the easiest, we aim to\nhelp the model learn better in the presence of imbalanced data."}, {"title": "2.3\nFinal Pipeline and Implementation", "content": "The final pipeline has been implemented in PyTorch. We have defined a custom dataloader\nthat is flexible in handling the classes on which the model is being trained. This allows us\nto simply change the classification head of the model, retain the backbone and introduce\nnew, harder classes in the same code.\nWe have experimented with the ResNet50 model and a custom ViT-CNN Hybrid\n(Figure 4) architecture. The hybrid architecture consists of a ViT branch [6] and a\nResNet34 branch using which features are extracted. These features are then passed\nthrough a classification head to obtain the output. We have used the Adam [7] optimizer\nwith a learning rate of 1e-3. The models were trained on the Kaggle platform using the\nP100 GPU."}, {"title": "2.4 Explainabiltiy", "content": "Although applications of AI in medical imaging are increasing, it is still mostly represented\nas a blackbox [8]. Hence, it becomes necessary to have some understanding behind the\nmodel's decisions. There are several ways of doing this, some methods add explainability\nfrom the beginning of training (ante-hoc) while some post training (post-hoc). Since we\nare using pretrained models with fine tuning, we have implemented a post-hoc technique\ncalled Gradient-weighted Class Activation Mapping (GradCAM) [9]. This technique, as\nper Varam et al. has performed well for medical imaging. Therefore, we applied this\ntechnique to the CNN (pretrained ResNet34) branch of our hybrid ViT-CNN model in\norder to gain insights behind the decisions taken by the model, an example is shown in\nFigure 4."}, {"title": "3 Results", "content": "The results obtained showed a notable increase in validation accuracy and F1 scores when\ntrained using our training method as compared to direct training.\nWe have obtained results for standard training using ResNet50 and ViTCNN model\non the augmented dataset and compare it with our training procedure on the augmented\ndataset."}, {"title": "3.1 Achieved results on the validation dataset", "content": "We can see that the inclusion of our Methodology described in sections 2.1 and sections 2.2\nincreases the overall performance of the model. A comparative analysis has been shown\nin Table 1."}, {"title": "4 Discussion", "content": "Our methodology tries to structure the learning process such that the model starts with an\neasy tasks and the complexity of the classification tasks increase gradually. This method\nproved effective as is seen in Tablel where we can see a notable increase in the F1 scores\nof the model. This signifies that the model has become better at handling minority classes\n(the harder tasks in our case)."}, {"title": "5 Conclusion", "content": "In summary, the approach described here successfully tackles the challenges of classifying\nimbalanced image data for the Capsule Vision 2024 challenge. By starting with an easier\ntask, separating Normal from Abnormal, and gradually adding more specific categories,\nthe model learns to handle the complexity of the data step by step. This strategy, inspired\nby curriculum learning, helps the model adjust to the difficulty of each class, especially\nthose with fewer examples. Overall, this method shows promise for improving results\nacross all classes, as seen in the results compared to baseline models."}]}