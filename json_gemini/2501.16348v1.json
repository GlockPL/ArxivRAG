{"title": "An Integrated Approach to AI-Generated Content in e-health", "authors": ["Tasnim Ahmed", "Salimur Choudhury"], "abstract": "Artificial Intelligence-Generated Content, a subset of Generative Artificial Intelligence, holds significant potential for advancing the e-health sector by generating diverse forms of data. In this paper, we propose an end-to-end class-conditioned framework that addresses the challenge of data scarcity in health applications by generating synthetic medical images and text data, evaluating on practical applications such as retinopathy detection, skin infections and mental health assessments. Our framework integrates Diffusion and Large Language Models (LLMs) to generate data that closely match real-world patterns, which is essential for improving downstream task performance and model robustness in e-health applications. Experimental results demonstrate that the synthetic images produced by the proposed diffusion model outperform traditional GAN architectures. Similarly, in the text modality, data generated by uncensored LLM achieves significantly better alignment with real-world data than censored models in replicating the authentic tone.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence-Generated Content (AIGC) is transforming healthcare by revolutionizing data generation, usage, and analysis. Advances in Generative AI (GenAI) now allow synthetic data-like medical images and clinical text-to supplement real-world datasets. Access to real healthcare data remains limited due to strict privacy regulations, complex de-identification, and fragmented storage [1]. Synthetic data overcomes these barriers by providing diverse, privacy-compliant datasets, and supporting e-health applications like remote diagnostics, telemedicine, and clinical decision support within ethical and regulatory standards. Among e-health data modalities, images and text are the most extensively used and critically important in AI applications. Approaches for synthetic medical image generation include Generative Adversarial Networks (GANs), variational autoencoders, and diffusion models. Recent studies show that GANs and diffusion models have been used to generate synthetic MRI for segmentation, enhance CT scans with synthetic pathologies, create de-identified chest radiographs, and synthesize contrast-enhanced CT images to reduce agent usage [2]. Diffusion provides an advantage over GANs by producing images with higher fidelity and diversity, though at the expense of generation speed [3]. Further exploration of class-conditioned diffusion models is essential for e-health that will allow disease-specific image generation and improve accuracy in tasks like classification, segmentation, and diagnostic support, where class-specific details are vital.\nRecent approaches to synthetic medical text generation combine Large Language Models (LLMs) and knowledge graphs with advanced NLP techniques [4]. Key applications include clinical note generation, named entity recognition, and text classification. Studies demonstrate that models like GPT-4 and LLaMA produce accurate, diverse clinical text, reducing hallucinations and enhancing downstream tasks. However, due to ethical constraints, aligned and censored LLMs struggle to generate realistic medical text data, as sensitive information in real medical texts is difficult to replicate while maintaining privacy. To address this, there is a need to explore uncensored and unaligned LLMs in a controlled environment and develop more robust evaluation methods to better identify discrepancies between real and synthetic data.\nTo this end, to mitigate the research gap in generating synthetic image and text modality data, we propose two GenAI methods. Therefore, primary contributions of this research are: (1) a novel class-conditioned framework introducing a diffusion model for synthetic medical image generation and an uncensored LLM-based approach for generating synthetic medical text, addressing data scarcity in e-health applications; (2) the diffusion architecture outperforms traditional GANS, and the adoption of object parsing through structured output refines responses from the uncensored LLM; and (3) a robust, four-stage evaluation strategy for generated text data to enhance transparency in performance differences between real and synthetic data and to identify specific areas of improvement. We will make our framework and code publicly available upon manuscript review (github.com/tasnim7ahmed/gen-health)."}, {"title": "II. METHODOLOGY", "content": "Figure 1 illustrates our proposed pipeline for the generation and evaluation of e-health data, focusing on the image and text modalities. A key feature of this framework is its class-conditioned design for both modalities, allowing for more controlled data generation."}, {"title": "A. Image Modality", "content": "The proposed model utilizes a class-conditioned diffusion approach for generating synthetic medical images, integrating classifier-free guidance [5] to enhance sample quality while maintaining generative flexibility. Building on foundational U-Net architecture [6], a modified \u2018ContextUnet' is implemented that incorporates class-conditioning elements to precisely control image generation. The training begins with an initial input image, $x$, a class label, $c$, and a designated timestep, $t$. The \u2018ContextUnet' processes image data through both downsampling and upsampling paths using convolutional and transposed convolutional layers with residual connections, providing, $\\text{Output} = x + \\text{Conv}(\\text{Conv}(x))$. Here, each \u2018Conv\u2019includes convolution, batch normalization, and GELU activation for efficient feature extraction while preserving contextual information via skip connections. Image synthesis is conditioned on class labels through embeddings generated by $C_{emb} = \\text{GELU}(W_2 \\text{GELU}(W_1 c))$. Here, $W_1$ and $W_2$ are learned weight matrices, and $c$ represents the class vector. These embeddings, combined with temporal embeddings, are integrated into the latent space and modulated through context masking, $c = c \\odot M$ where $M \\in \\{0, 1\\}^{n_{classes}}$. Context masking allows partial conditioning and stochastic variations for diverse outputs within class constraints. At each timestep, random noise $\\epsilon$ is introduced to the image, producing a progressively noisier version $x_t$ that the model will learn to reconstruct. The diffusion process follows a denoising schedule that provides a controlled transition from noise to coherent images. In the forward diffusion step, noise $\\epsilon \\sim \\mathcal{N}(0, I)$ is added to the input image $x_0$ as $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$, where $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. \u2018ContextUnet' then predicts this added noise $\\epsilon_{\\theta}$ which allows the model to estimate the noise level and adjust its parameters for better noise prediction accuracy. During reverse diffusion, the model predicts the noise component $\\epsilon_{\\theta}(x_t, c, t)$ to reconstruct the image, $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, c, t)) + \\sqrt{\\beta_t} z$, with $\\beta_t$ representing the noise added at each timestep and $z \\sim \\mathcal{N}(0, I)$. The classifier-free guidance mechanism aggregates conditional and unconditional noise predictions by: $\\epsilon_{\\theta}(x_t, c, t) = (1 + w) \\epsilon_{\\theta}(x_t, c, t) - w \\epsilon_{\\theta}(x_t, \\varnothing, t)$, where $w$ controls the conditioning strength to balance the fidelity and diversity of the generated images. Depending on the noise prediction, a loss is calculated, allowing the weights to be updated for increased accuracy in denoising at each step. The training objective involves denoising score matching, minimizing the mean squared error between the predicted and actual noise: $\\mathcal{L}(\\theta) = \\mathbb{E}_{x_0, \\epsilon, t, c} [||\\epsilon - \\epsilon_{\\theta}(x_t, c, t)||^2]$. In the generation phase, a random noisy image, $x_T$, is produced and iteratively denoised by the reverse diffusion model, progressively reducing noise until the final image, $x_0$, is generated, matching the class label $c$. Additionally, class conditioning is improved by randomly applying a context mask $M$, thereby generating conditional images without requiring an auxiliary classifier. The quality of the synthesized images is evaluated using Fr\u00e9chet Inception Distance (FID), Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR)."}, {"title": "B. Text Modality", "content": "Censored and uncensored LLMs differ in content moderation and training methodologies. Censored LLMs go through stringent filters and alignment strategies, including supervised fine-tuning with curated datasets and safety layers, to prevent the generation of sensitive, harmful, or inappropriate content. On the other hand, uncensored LLMs do not have these restrictions, which allows them to generate more varied and open-ended text. Using uncensored LLMs to generate synthetic medical data is crucial despite these inherent risks. Censored models may unintentionally suppress or alter subtle expressions and complex emotions, leading to less authentic and biased samples. Uncensored LLMs allow the creation of more genuine and diverse data, essential for developing reliable medical models. Furthermore, in fields requiring an accurate understanding of sensitive conditions, generating unfiltered data ensures that synthetic datasets reflect the true diversity and complexity of real-world scenarios.\nIn this study, the uncensored version of the Llama-3.1-8B [7] is used to generate synthetic samples for a multi-class mental health text classification dataset. Implementing a k-shot prompting, where k corresponds to the number of classes in the dataset, the generation process started with an instruction prompt that included one randomly sampled example from each class. Subsequently, the LLM was instructed to produce one new sample per class based on these initial examples. To address the alignment issues inherent in uncensored LLMs and facilitate automated parsing, we adopted a structured output generation strategy. This involved defining specific classes for object parsing from the response as shown below:\nEach LLM response generates a \u2018GeneratedSamples\u2019object that includes one sample from each of the k classes. The proposed generation strategy allowed the LLM outputs to be parsed consistently, resulting in a balanced dataset across all classes. Since the proposed approach utilizes a structured uncensored LLM for generating data samples, it can be incorporated to produce tabular data through modifications to the model classes. This involves adjusting the class attributes to align with specific database requirements, such as defining appropriate field types\u2014including text, numerical, and binary\u2014and providing detailed descriptions for each field.\nThe quality of generated data is evaluated through an experimental framework proposed by Schmidhuber et al. [8]. The evaluation pipeline includes four training configurations: real data only, a composite of real and synthetic data, synthetic data only, and a Synthetic Minority Over-sampling Technique (SMOTE)-like method for class balancing. The objective is to investigate the impact of synthetic data in varied settings by fine-tuning text classifiers and evaluating their performance on downstream tasks. Transformer-based masked language models-BERT [9], ROBERTa [10], and ALBERT [11]\u2014are implemented, with performance measured through accuracy, F1 score, precision, and recall. The experiment design details are as follows.\n1) Real Data (Original): The real, original dataset $D_{orig}$ is split into training ($D_{orig-train}$) and testing sets ($D_{orig-test}$). This initial experiment involves training classifiers on $D_{orig-train}$ and validating their performance on $D_{orig-test}$. The classifiers trained on real data set the benchmark against which synthetic and combined training scenarios are evaluated. This determines if the addition of synthetic data contributes positively to performance or introduces potential drawbacks.\n2) Composite: This experiment evaluates the performance of classifiers trained on a combination of real and synthetic data. The training data comprises both $D_{orig-train}$ and the generated synthetic data $D_{synth}$ and can be expressed as: $D_{comp-train} = D_{orig-train} \\cup D_{synth}$. The idea is to see if the presence of synthetic data can enhance the generalizability and robustness of the model. The classifiers trained on these datasets are then evaluated on $D_{orig-test}$.\n3) Synthetic: The training set $D_{synth}$ is used to fine-tune classifiers, which are then validated on $D_{orig-test}$. The synthetic-only training is validated through 5-fold cross-validation to evaluate the stability and reliability of model performance across different splits. This experiment decides whether generated data can effectively replace real data by evaluating if models trained on artificial examples can generalize successfully to real-world data.\n4) SMOTE: SMOTE mitigates class imbalance by incorporating synthetic samples into the minority class of $D_{orig-train}$. This method adjusts the training set lengths such that: $D_{comp-1} = D_{orig-1} + D_{synth-1}$, where $D_{orig-1}$ represents the minority class in the original training set, and $D_{synth-1}$ consists of synthetic samples for that class. The modified training set is then used to train classifiers, which are evaluated on $D_{orig-test}$. This experiment tests whether synthetic oversampling can effectively balance classes and improve performance."}, {"title": "III. RESULT AND DISCUSSION", "content": ""}, {"title": "A. Datasets", "content": "1) Diabetic Retinopathy [12]: The APTOS 2019 Blindness Detection dataset consists of retina scan images intended for the detection and classification of diabetic retinopathy. The dataset is organized by the severity of diabetic retinopathy, with images distributed across five classes: no diabetic retinopathy, mild, moderate, severe, and proliferate diabetic retinopathy.\n2) DermNet [13]: The DermNet dataset consists of approximately 23000 images representing 23 distinct skin disease categories, sourced from the DermNet online portal. The dataset includes a diverse range of conditions such as acne, melanoma, eczema, psoriasis, and vascular tumours.\n3) DEPTWEET [14]: The DEPTWEET dataset comprises crowdsourced tweets labelled with depression severity levels. This dataset is founded on a typology based on psychological theory, designed to detect varying degrees of depression. Labels follow a four-level classification: Non-depressed, Mild, Moderate, and Severe. The labelling process utilizes the DSM-5 [15] diagnostic criteria and the Patient Health Questionnaire (PHQ-9) [16], which identify nine symptoms linked to depression severity, including suicidal thoughts, lack of interest, and sleep disorders. The dataset presents a notable class imbalance, heavily skewed toward the Non-depressed class, with fewer samples in the Severe category."}, {"title": "B. Image Modality", "content": "The proposed diffusion model was trained and evaluated on the DermNet and Diabetic Retinopathy datasets to provide a proof of concept, demonstrating that class-conditioned diffusion models can outperform leading GAN architectures. Our study empirically validates that the model can generate realistic synthetic images conditioned on class labels. We selected datasets with differing structural similarities: the Diabetic Retinopathy dataset features minimal inter-image differences, while the DermNet dataset includes higher structural variability within and between classes, representing various body parts. All samples were converted to grayscale for two reasons: (1) to reduce computational complexity by reducing the number of channels from three to one, and (2) because prior research shows grayscale images retain sufficient visual features for classification in similar tasks. The proposed model is evaluated for 32 \u00d7 32 class-conditioned image generation. Table I shows that it outperforms Conditional GAN (CGAN) [17] and Auxiliary Classifier GAN (ACGAN) [18], achieving significantly lower FID and higher SSIM and PSNR metrics. Although these results are promising, further improvement is possible, as an FID below 100, SSIM above 0.5, and PSNR near 20 would closely approximate real distribution in synthetic images. For SSIM, the metrics reveal that while FID and PSNR scores are similar across datasets, SSIM for retinopathy nears 0.5, but is notably lower (0.2097) for DermNet in the best-performing model. Manual inspection suggests that this discrepancy may be due to structural variability in DermNet images, as images within the same class may represent different body parts, leading to lower SSIM scores despite the class similarity. Two hyperparameters notably impact the model performance: guidance strength w and sampling steps T. Optimal FID was achieved with w = 2.0 after experimenting with values from 0.5 to 4.0 in 0.5 intervals. Empirical and theoretical analysis shows T significantly affects quality, with higher T improving quality but slowing sampling. Setting T = 400 balanced sampling quality and speed effectively across w values."}, {"title": "C. Text Modality", "content": "Four experimental configurations were evaluated: Original, Composite, Synthetic, and SMOTE, with the effectiveness of censored and uncensored (proposed) LLMs evaluated across each setting (Table II). As expected, ROBERTa generally outperformed BERT in most experiments, given its significantly larger parameter size. Interestingly, however, despite being much smaller in parameter size than both BERT and ROBERTa, ALBERT achieved comparable performance in the classification task. Comparative analysis shows that classifiers trained using synthetic data generated by the uncensored Llama-3.1-8B consistently outperformed those trained on censored data, achieving improved evaluation scores across all experiments. The performance gap between censored and uncensored models can be attributed to the nature of the text samples they generate. As shown in Table III, censored LLM produce text with a more formal, sanitized tone, exemplified by phrases like \u201cFeeling hurt and frustrated,\u201d which appear polished and restrained. This tone lacks the raw, authentic quality typical of social media comments. In contrast, the uncensored LLM samples more closely mirror the unfiltered, personal tone of social media posts, such as \u201cI've been struggling with suicidal thoughts,\u201d providing a candid portrayal of mental states. Examination of samples from censored and uncensored LLM reveals that for moderate or severely depressed classes, the censored LLM generated responses with a less negative tone, avoiding sensitive words like 'suicide' or 'kill.' This occurs because aligned LLMs are trained to filter out such content, resulting in generated samples that more closely resemble non-depressed or mildly depressed categories, despite being labelled as moderate or severe. This discrepancy contributes to the reduced performance of the censored LLM, as the generated data lacks the authentic severity seen in real-world samples."}, {"title": "IV. LIMITATIONS", "content": "While the proposed framework demonstrates considerable potential in generating synthetic e-health data, several limitations should be acknowledged. Firstly, the study is constrained by the use of a limited number of datasets which may affect the generalizability of the results across a broader range of medical conditions. Additionally, the generation of low-resolution grayscale images may omit critical visual details essential for accurate medical diagnosis. Moreover, the current evaluation"}, {"title": "V. CONCLUSION", "content": "We developed an integrated framework for AIGC in the e-health domain, proposing class-conditioned diffusion models for synthetic medical images and an uncensored Llama-3.1-8B-based pipeline for medical text data. Our results demonstrate that the diffusion-based approach surpasses traditional GAN architectures in producing high-quality images, while text classifiers trained with our synthetic data exhibit improved performance, effectively addressing data scarcity in e-health applications. However, the use of uncensored LLMs carries the risk of generating sensitive or inappropriate content, which could have adverse effects if misused. Additionally, diffusion models have the potential to be exploited for creating harmful deepfakes, posing threats to the integrity of healthcare information. Therefore, it is essential to implement robust ethical guidelines and safeguards to ensure the responsible use of these technologies. Future work will focus on mitigating these risks (e.g., semi-alignment of uncensored LLMs by fine-tuning for specific downstream data generation tasks) and incorporating qualitative assessments from medical experts to validate the clinical relevance and safety of the generated content."}]}