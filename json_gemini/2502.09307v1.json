{"title": "When the LM misunderstood the human chuckled:\nAnalyzing garden path effects in humans and language models", "authors": ["Samuel Joseph Amouyal", "Aya Meltzer-Asscher", "Jonathan Berant"], "abstract": "Modern Large Language Models (LLMs) have\nshown human-like abilities in many language\ntasks, sparking interest in comparing LLMS'\nand humans' language processing. In this pa-\nper, we conduct a detailed comparison of the\ntwo on a sentence comprehension task using\ngarden-path constructions, which are notori-\nously challenging for humans. Based on psy-\ncholinguistic research, we formulate hypothe-\nses on why garden-path sentences are hard,\nand test these hypotheses on human partici-\npants and a large suite of LLMs using com-\nprehension questions. Our findings reveal that\nboth LLMs and humans struggle with spe-\ncific syntactic complexities, with some mod-\nels showing high correlation with human com-\nprehension. To complement our findings,\nwe test LLM comprehension of garden-path\nconstructions with paraphrasing and text-to-\nimage generation tasks, and find that the re-\nsults mirror the sentence comprehension ques-\ntion results, further validating our findings on\nLLM understanding of these constructions.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown\nhigh proficiency in language comprehension\nand generation, demonstrating performance that\nmatches and sometimes surpasses human capa-\nbilities across a range of tasks (OpenAI, 2023;\nTouvron et al., 2023; Almazrouei et al., 2023;\nGrattafiori et al., 2024; Gemini, 2024). This has\nsparked a line of research focused on comparing\nsentence-processing mechanisms in humans and\nLLMs. Within this research, some studies have\nfound correlations between LLM activations and\nbrain activations during the processing of identical\nsentences (Cacheteux and King, 2022; Schrimpf\net al., 2021; Ren et al., 2024). Others have demon-\nstrated that LLMs can be used to predict human\nlinguistic behavior (Linzen et al., 2016; Warstadt\net al., 2019; Hu et al., 2020; Rego et al., 2024; Sun\nand Wang, 2024; Kuribayashi et al., 2025).\nWhile LLMs mostly succeed where humans\nsucceed, less is known on whether LLMs fail\nwhere humans fail. A classic case in psycholin-\nguistic research for sentences where humans sys-\ntematically have comprehension difficulty is Gar-\nden Path (GP) structures (Ferreira and Hender-\nson, 1990; Trueswell et al., 1993; Garnsey et al.,\n1997). GP sentences are temporarily ambiguous,\nas their beginning leads readers to misconstrue\ntheir parse. Consider for example (1)-(2). In (1),\nreaders initially misanalyze \"the dog\" as the ob-\nject of \"washed\u201d, although in the final, correct\nstructure, \"the dog\" is the subject of \"barked\", and\n\"washed\" has no object.\n1. While the boy washed the dog barked loudly.\n2. The dog barked loudly while the boy washed."}, {"title": "2 What Makes Object/Subject\nGarden-Path Sentences Hard?", "content": "Object/subject garden-path sentences, like (1), in-\nclude an embedded verb (\"wash\") followed by\na noun phrase (\"the dog\") and the main verb\n(\"barked\"). These sentences cause processing dif-\nficulties, leading to slower reading\u2014especially at\nthe main verb and reduced comprehension. In\nsentence (1), \u201cthe dog\u201d will often be mistakenly\ninterpreted as the object of \"wash\", prompting in-\ncorrect answers like \"Yes\" to \"Did the child wash\nthe dog?\". This happens even though in the final\nstructure of the sentence, \u201cthe dog\" is not an object\nof \"washed\".\nSeveral (non-mutually exclusive) hypotheses\ncan explain the misinterpretation described above,\nas summarized in Table 1. To describe the hy-\npotheses, we consider the following sentence-\nquestion pair:\n3. While the man hunted the deer ran into the\nwoods.\nQuestion: Did the man hunt the deer? Y/N\nNote that the accurate answer to the question\nabove is \"Not necessarily\". It is a possible in-\nterpretation of the sentence, and may be inferred\nfrom it, but it is not entailed from the sentence. In\nour experiments, as in previous experiments, we\nconsider \"yes\" to be a wrong answer here, whereas\n\"no\" is considered the right answer.\nHypothesis 1: The GP syntax is hard. This\nhypothesis suggests that misinterpretation occurs\nbecause during incremental processing, the post-\nverbal noun phrase (\u201cthe deer\u201d) is first attached\nas the object of the verb, requiring reanalysis when\nthe second verb is encountered. Often, the reanal-\nysis is not complete, and the initial interpretation\nlingers. According to this, reordering the clauses\n(see Table 1) should improve accuracy by prevent-\ning initial misattachment.\nHypothesis 2: Readers attach the noun to\nthe first verb when it is a plausible object for\nit.\nAccording to this hypothesis, readers inter-\npret a noun as an object of a verb in the sentence\nwhenever this is semantically plausible, regard-\nless of sentence position. If the noun is an im-\nplausible direct object, it will not be interpreted\nas such, improving accuracy (see Table 1). Hy-\npothesis 3: Readers search maximal interpre-\ntation of verb arguments. According to this hy-\npothesis, optionally transitive verbs need objects\nfor full interpretation, so available nouns are taken\nto fulfill this role. In contrast, alternating reflexive\n(\"wash\") and unaccusative verbs (\u201cdrop", "While the\nboy washed the dog barked": "the correct answer\nfor \"Did the boy wash the dog?\" is \u201cNo\u201d. As for\nthe optionally transitive verbs, it can also be hy-\npothesized that the tendency to interpret them as\ntaking an object depends on the verb's transitiv-\nity bias (the probability that the verb appears with\na direct object). According to this, verbs with a\nlower bias (e.g., \u201cwalk\u201d) should lead to better ac-\ncuracy compared to those with a higher bias (e.g.,"}, {"title": "3 Human performance", "content": "\"explore\"), as the noun is less likely to be consid-\nered as their object.\nNext, we describe how we test the above hy-\npotheses, starting with a human experiment.\nWe first run an experiment on human participants\nto test our hypotheses.\n3.1 Methods\nMaterials In 45 sentences sets with optionally\ntransitive verbs (24 coming from Christianson\net al. (2001), 21 crafted for this study), we ma-\nnipulated the structure of the sentence (GP or non-\nGP) and plausibility of the noun as the verb's ob-\nject (plausible or implausible), as exemplified in\n(4). We also created 24 additional sets with re-\nflexive/unaccusative verbs (12 from Christianson\net al. (2001) and 12 crafted for this study) in plau-\nsible sentences, manipulating structure (GP/non-\nGP), as exemplified in (5).\n4. (a) GP, plausible: While the man hunted the deer ran\ninto the woods.\n(b) Non-GP, plausible: The deer ran into the woods\nwhile the man hunted.\n(c) GP, implausible: While the man hunted the child\nran into the woods.\n(d) Non-GP, implausible: The child ran into the\nwoods while the man hunted.\n5. (a) GP, reflexive: While the boy washed the dog\nbarked loudly.\n(b) Non-GP, reflexive: The dog barked loudly while\nthe boy washed.\nTo construct materials for the plausibility ma-\nnipulation (hypothesis 2), we use insights from\nAmouyal et al. (2024), and let GPT4 rate sentence\nplausibility on a 1 to 7 scale. We select pairs where\nthe plausible sentence had a rating of at least 3\npoints higher than its implausible counterpart. For\nthe second part of hypothesis 3, we assessed each\noptionally-transitive verb's bias by its proportion\nof transitive usages on Wikipedia. Our verbs' bias\nranges from 0.102 (\u201csail\u201d) to 0.775 (\u201cexplore\").\nAppendix A lists the full estimated verb biases,\nand all the sentences are in Appendix B.\nFor each sentence, we ask one of two questions:\n1. Simple: \"Did the deer run into the woods?\"\n2. GP: \"Did the man hunt the deer?\"\nThe simple question probes basic understanding of\nthe sentence, whereas the GP question targets the\npotential misinterpretation.\nProcedure Native English speakers were re-\ncruited via the Prolific platform.\u00b9 Sentences were"}, {"title": "3.2 Human results", "content": "displayed word-by-word, with each word shown\nfor 400ms and a 100ms blank screen between\nwords. After the sentence, the comprehension\nquestion was presented for 5 seconds. If unan-\nswered within this time, the response was marked\nas incorrect. Participants completed two practice\nitems, followed by one experimental sentence and\none question. The single-trial design prevents fa-\ntigue (Christianson et al., 2022) and learning ef-\nfects (Fine et al., 2013). Each of the 456 sentence-\nquestion pair was shown to 10 participants. The\naverage completion time was 1:50 minutes, and\nparticipants were compensated with 0.30\u00a3, equiv-\nalent to 9.64f per hour. The experiment was ap-\nproved by the Ethics Committee at Tel-Aviv Uni-\nversity.\nThe accuracy on simple questions was high (aver-\nage 95.4%, minimum 92.4%, maximum 98.7%).\nConversely, GP questions were much more chal-\nlenging with an average accuracy of 37.0% on GP\nquestions.\nFigure 1 (left) shows the average accuracy for\nhumans in the various conditions. As expected\nfrom Hypothesis 1, accuracy is consistently higher\nfor non-GP structures. In addition, accuracy is\nlower when the noun is a plausible direct ob-\nject, indicating a tendency to interpret it as such\neven without syntactic indication, supporting Hy-\npothesis 2. The plausibility effect was more pro-\nnounced than the syntactic effect. In addition, ac-\ncuracy was higher for GP sentences with a reflex-\nive/unaccusative verb than for those with an op-\ntionally transitive verb, supporting Hypothesis 3,\nand the effect of structure (GP vs. non-GP was\nstronger for the former verbs).\nWe test statistical significance with Generalized\nLinear Mixed-Effects Models (see Appendix C).\nFor Hypothesis 1, the difference between GP and\nnon-GP sentences was significant for implausible\n(p = .019) and reflexive (p = 2.52e-13) sets, and\napproached significance in the plausible sets, p\n= .065. Hypothesis 2's prediction was confirmed\nwith a significant difference between plausible and\nimplausible sentences (p = 4.11e-16). Hypothe-\nsis 3 is also supported with a significant difference\nbetween reflexive/unaccusative verbs for both GP\nsentences (p = 1.35e-5) and non-GP sentences (p\n= 2.45e-14). The Pearson correlation between\ntransitivity bias and accuracy was weak (\u22640.19)"}, {"title": "4 LLMs Performance", "content": "We now analyze the performance of LLMs on our\ndifferent experimental conditions.\n4.1 Methodology\nTo replicate the experiment with LLMs, we used\nfew-shot prompting, where each example includes\na sentence, a question, and the correct answer.\u00b2\nThe examples did not contain GP structures. Each\nmodel was prompted 8 times, using two system\nprompts and four example orderings. We ex-\ntract the probabilities of the correct and incor-\nrect answers tokens, averaging these across the 8\nprompts. Appendix D shows an example prompt.\nModels We test models from different families,\nsizes and training checkpoints:\n1. GPT family (OpenAI, 2023): GPT-4, GPT-\n4-Turbo, GPT-4o, GPT-4o-mini, o1-preview,\nol-mini.\n2. Llama-3 (Grattafiori et al., 2024): All mod-\nels from the Llama-3 family (Llama-3.2 and\nLlama-3.1) available on HuggingFace.\u00b3\n3. Qwen-2.5 (Yang et al., 2024; Qwen Team,\n2024): All Qwen-2.5 models on Hugging-\nFace except models of size 0.5b.\n4. Gemma-2 (Team et al., 2024): All Gemma-2\nmodels on HuggingFace.\n5. Olmo (Groeneveld et al., 2024): 15 Olmo-1b\nand Olmo-7b checkpoints along training.\n4.2 Results\nWe first present the overall results of LLMs on\nour task. Figure 3 shows the results for 6 selected\nmodels from each family. Appendix E presents the\nresults for all models.\nAt a high-level it is clear that the behavior of\nLLMs resembles that of humans: accuracy on\nnon-GP sentences is higher than accuracy on GP\nsentences, accuracy for both GP and non-GP sen-\ntences is higher when the direct object is implau-"}, {"title": "5 Analzying LLMs vs. human\nperformance", "content": "across conditions.\nOverall, the hypotheses were supported by the\nhuman results, showing that multiple factors influ-\nence the difficulty of object/subject GP sentences.\nIn comparing LLMs and humans, we focus on the\nfollowing important (albeit less-studied) aspect:\nthe extent to which the relative difficulty of tasks\nin our experiment is similar between LLMs and\nhumans. Showing that LLMs and humans have\nsimilar processing difficulties can open interesting\nresearch directions on whether LLMs can inform\npsycholinguistic models of human sentence pro-\ncessing (Kuribayashi et al., 2025).\nTo evaluate the similarity between LLMs and\nhumans, we use the Kendall Tau rank correla-"}, {"title": "6 Performance on Paraphrasing and\nText-to-Image Generation", "content": "sible, and the gap between GP and non-GP sen-\ntences is larger in the reflexive/unaccusative case.\nThese trends seem more pronounced for larger and\nstronger models (the top two rows) compared to\nsmaller models (Olmo-1b).\nInterestingly, LLM performance is not perfect\neven for the strongest model, ol-preview, which\nobtains an average accuracy of 78% (the second-\nstrongest model, Gemma-27B has an average ac-\ncuracy of 74%). This far-from-perfect perfor-\nmance of LLMs is perhaps surprising since the en-\ntire sentence and question are presented in full to\nthe LLMs and there is no reason to suspect that\nthey should suffer from the same processing diffi-\nculties that humans do, especially those related to\nthe inability to overcome the initial misparse.\nIn addition to answering comprehension ques-\ntions, we now test LLM understanding of GP sen-\ntences on two additional tasks \u2013 paraphraing and\ntext-to-image generation. Due to cost limitations,\nwe run this experiment on LLMs only. Patson\net al. (2009) performed a paraphrasing experiment\nwith humans using materials from Christianson\net al. (2001) and found that paraphrases showed\nthe same misinterpretations that comprehension\nquestions did.\n6.1 Paraphrasing\nWe asked LLMs (excluding OLMo-1B and\nOLMo-7B) to paraphrase a sentence by splitting\nit into two parts. The correct answer would be to\nchange \"While the man hunted the deer ran into\nthe woods.\" into \"The man hunted. The deer ran\ninto the woods.\". Using a few-shot prompt, we"}, {"title": "7 Conclusion", "content": "tion metric.\u2074 We calculate a \"Global\" Kendall\nTau correlation by looking at all items in our data\n(from all conditions) and comparing the average\naccuracy of humans on those items to the aver-\nage probability of the correct answer as provided\nby the LLM, This measures whether the difficulty\nranking of all the items on our experiment is sim-\nilar for humans and LLMs. Figure 4 presents the\nfindings for each model family.\nModel size: As illustrated in Figure 4, larger\nmodels exhibit a higher Kendall Tau correlation\nwith human judgements across all model families.\nInstruction tuning: Instruction tuning appears\nto have little impact on the similarity between hu-\nmans and LLMs in this measure.\nPretraining tokens: In OLMo-7B, we see an in-\ncrease in Kendall Tau correlation with the increase\nin number of pretraining tokens. This pattern is\nnot observed in OLMo-1B, possibly because the\nmodel is too weak to show significant effects.\nWe now move to analyzing the by-condition\ncorrelation between humans and LLMs. Figure\n5 shows the Spearman rank correlation between\nhumans and LLMs, comparing the average accu-\nracy across the 6 conditions (correlating two vec-\ntors in R6). All models show a high Spearman\nThis study explores similarities between human\nand LLM sentence processing. By focusing on\ncomprehension of garden-path sentences, known\nfor their syntactic complexity and inherent chal-\nlenges for human processing, we studied whether\nLLMs have similar difficulties to humans. Our\nfindings demonstrate that humans and LLMs\nstruggle with similar syntactic structures, and no-\ntably, some LLMs approximate human behavior\nquite closely, as indicated by strong correlation\nmetrics. Additionally, the correlation between er-\nrors in the comprehension questions, paraphras-\ning and image generations tasks suggests shared\nunderlying mechanisms of sentence misinterpre-"}, {"title": "Limitations", "content": "tation between the tasks. Our approach not only\nadds a novel dimension to the evaluation of LLMS\nbut also opens up possibilities for utilizing these\nmodels to gain deeper insights into human linguis-\ntic processing.\nIn our study, we evaluated reading comprehension\nacross a wide array of LLMs. However, the latest\nstate-of-the-art models appeared too late to be in-\ncorporated into this version of our paper. Assess-\ning their understanding of our sets would be inter-\nesting for a future version of this paper. Addition-\nally, our focus was limited to measuring reading\ncomprehension on Subject/Object GP sentences.\nExploring LLMs' comprehension of other types of\nGP sentences would also be interesting. Finally,\nwe did not collect data on metrics beyond reading\ncomprehension, such as eye gaze or reading time,\nin our experiments. Gathering such metrics and\nanalyzing their correlation with sentence compre-\nhension could provide valuable insights."}]}