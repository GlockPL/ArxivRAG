{"title": "Benchmark on Drug Target Interaction Modeling from a Structure Perspective", "authors": ["Xinnan Zhang", "Jialin Wu", "Junyi Xie", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "The prediction modeling of drug-target interactions is crucial to drug discovery and design, which has seen rapid advancements owing to deep learning technologies. Recently developed methods, such as those based on graph neural networks (GNNs) and Transformers, demonstrate exceptional performance across various datasets by effectively extracting structural information. However, the benchmarking of these novel methods often varies significantly in terms of hyperparameter settings and datasets, which limits algorithmic progress. In view of these, we conduct a comprehensive survey and benchmark for drug-target interaction modeling from a structure perspective, via integrating tens of explicit (i.e., GNN-based) and implicit (i.e., Transformer-based) structure learning algorithms. To this end, we first unify the hyperparameter setting within each class of structure learning methods. Moreover, we conduct a macroscopical comparison between these two classes of encoding strategies as well as the different featurization techniques that inform molecules' chemical and physical properties. We then carry out the microscopical comparison between all the integrated models across the six datasets, via comprehensively benchmarking their effectiveness and efficiency. Remarkably, the summarized insights from the benchmark studies lead to the design of model combos. We demonstrate that our combos can achieve new state-of-the-art performance on various datasets associated with cost-effective memory and computation. Our code is available at https://github.com/justinwjl/GTB-DTI/tree/main.", "sections": [{"title": "1 Introduction", "content": "The prediction modeling of drug-target interactions (DTI) has emerged as an irreplaceable task for efficacious therapeutic interventions. The binding affinity between a drug molecule and its target protein plays a significant role in the design and repurpose of drugs, where a high affinity typically indicates the desired therapeutics, target specificity, long residence, and drug resistance delay [1, 2, 3]. The precise modeling of DTI can expedite the drug discovery process and circumvent the associated cost [4, 5]. Deep learning based frameworks have recently revolutionized this field, enabling more accurate and efficient predictions compared with laboratory experimental methods [6, 7, 8].\nWithin the deep learning frameworks [9, 10], drugs are commonly represented using the Simplified Molecular Input Line Entry System (SMILES) [11], and proteins are represented as sequences of amino acids. These representations are processed by separate convolutional neural networks (CNNs) [12, 13] and subsequently integrated and processed using a multi-layer perceptron (MLP) for DTI prediction. It is notorious that the reliance on sequence-based representations can result in the loss of structural information, which can potentially compromise the DTI predictive capability. From the drug perspective, molecular structure modeling helps identify the specific binding sites [14], contribute to predicting pharmacokinetic properties [15], and allow conformational flexibility [16]."}, {"title": "2 Formulations for Drug-target Interaction Modeling", "content": "In this research, we focus on the formulations of recently-emerging structure modeling approaches for drug molecules, which could be categorized into explicit methods based on graph neural networks and implicit methods based on transformer. The target proteins are learned by the sophisticated tools of convolutional/recurrent neural networks (CNNs/RNNs) or transformers, after which both the molecules' and proteins' embeddings are integrated to facilitate interaction prediction. We will also summarize and benchmark the various widely-adopted molecule features."}, {"title": "2.1 Graph Neural Networks based Methods", "content": "A drug molecule is typically represented as a graph $G = (V, E)$, where V and E denotes the sets of atoms and chemical bonds, respectively. The classical GNN frameworks involve key processes of aggregating and updating node features, collectively referred to as message passing, which can be mathematically represented as [26, 27]:\n$h_i^{(l+1)} = \\text{COMBINE}^{(1)}_{vi}( h_i^{(l)}, \\text{AGGREGATE}^{(1)}_{de} (f_{\\alpha} (\\{ h_j^{(l)}, e_{ij}^{(l)}: j \\in N_i \\}) ) )$.\n$e_{ij}^{(l+1)} = \\text{COMBINE}^{(2)}_{edge} ( e_{ij}^{(l)}, \\text{AGGREGATE}^{(2)}_{edge} (g_{\\beta} (\\{ h_i^{(l)}, h_j^{(l)}: j \\in N_i \\}) ) )$.\nwhere $h_i^{(l)}$ is the feature representation of node $v_i$ at layer $l$, $e_{ij}^{(l)}$ is the feature representation of edge between nodes $v_i$ and $v_j$, $N_i$ refers to the set of neighboring nodes next to node $v_i$. Functions $\\text{AGGREGATE}^{(1)}$ and $\\text{COMBINE}^{(1)}$ aim to aggregate the neighborhood representations and integrate them together with the nodes features, respectively. Additionally, $f_{\\alpha}$ and $g_{\\beta}$ are feature mapping functions, parameterized by $\\alpha$ and $\\beta$, respectively. The molecule's representation can be derived using $\\text{READOUT}$ function, which processes on the set of vertex features $H^{(L)}$ at the last layer.\nGiven a molecule with N atoms, the adjacency matrix $A \\in \\mathbb{R}^{N\\times N}$ indicates its connectivity, with $A_{ij} = 1$ if atom $v_i$ is adjacent to atom $v_j$, and 0 otherwise. Considering the self-connection of atoms, we have $\\tilde{A} = A + I$. Let $X \\in \\mathbb{R}^{N\\times C}$ denote the initial atom feature matrix. GCN [28] models the message passing as follows:\n$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)} )$.\nwhere $H^{(l)}$ is the node feature matrix at layer $l$, starting with $H^{(0)} = X$. Matrix $W^{(l)}$ represents the learnable weights for layer $l$, $\\sigma$ denotes a non-linear activation function, e.g., ReLU, and D is a diagonal degree matrix of A. A couple of pioneering works have leveraged GCN to facilitate drug-protein interaction prediction [29, 30, 31, 32]. For example, DeepGLSTM [29] uses mixture-of-depths GCNs to capture drug representations from different scales. CPI [31] considers cross-atom distance and introduces the concept of r-radius subgraphs [33], using r-radius vertices and edges to redefine the structure of graphs.\nGIN excels in learning distinct graph features by approximating the Weisfeiler-Lehman test, enabling it to distinguish a wide range of graph structures [34]. The message passing process at the $(l + 1)$-th layer is of the following form:\n$h_i^{(l+1)} = MLP^{(l)} ((1 + \\epsilon^{(l)}) h_i^{(l)} + \\sum_{j \\in N_i} h_j^{(l)} )$.\nwhere $MLP^{(l)}$ is a multi-layer perceptron that parameterizes the update function, and $\\epsilon^{(l)}$ is a learnable parameter. We benchmark several GIN-based drug-target interaction modeling methods. GraphCPI [35] and GraphDTA [18] adopt GIN-based models with batch normalization to obtain the drug representation. SubMDTA [36] uses subgraph's generation task and contrastive learning to pretrain a molecular graph encoder with multiple GIN layers for further prediction.\nUnlike fixed-weight aggregation, GAT [37] employs an attention mechanism to determine neighborhood importance and learn the node embeddings as:\n$h_i^{(l+1)} = \\sigma(\\sum_{j\\in N_i \\bigcup \\{ i \\}} \\text{softmax}( \\alpha_{ij} )W^{(l)}h_j^{(l)} )$.\n$W^{(l)}$ denotes attention weights, and $||$ is concatenating operation. GraphDTA [18] and AMMVF [38] leverage the multi-head GAT layers to optimize the atom messaging. They integrate GAT with other architectural modules, such as GCN, facilitating a more comprehensive representation of drugs."}, {"title": "2.2 Transformer-based Methods", "content": "Besides the graph representation, drugs could also be decorated as SMILES strings [39] and encoded similarly to natural language processing. Specifically, after tokenizing SMILES strings, Transformer model utilizes multi-head attention to model the interactions between different segments of the input and obtain the molecular representations. Positional encodings are also integrated to preserve the sequence order, enhancing the model's ability to process sequential information effectively. We review and benchmark two typical types of attention mechanisms used for molecular representations.\nSelf-attention computes a weighted sum of all input values based on their relevance to each other. Considering an embedding of SMILES sequence $H^{(l)} \\in \\mathbb{R}^{d\\times N}$ at a specific transformer layer, where N and d are token length and dimension, respectively, the attention is calculated by $\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T/\\sqrt{d_k})V$. $Q, K \\in \\mathbb{R}^{d_k \\times N}$ and $V \\in \\mathbb{R}^{d_v \\times N}$ are projections of the input matrix $H^{(l)}$. Multi-head attention combines these projections across different subspaces for a more detailed analysis. Following by normalization and feed-forward neural networks, the SMILES embedding is updated as $H^{(l+1)}$ and the output from the last layer is treated as molecular representations. Transformer encoders like MolTrans [20] and FOTFCPI [41] are adopted to enhance sub-structure embeddings in proteins and drugs.\nCross-attention is designed to capture the interaction between the drug and protein sequences, with the query matrix Q derived from one sequence and the key and value matrices K, V from another. This mechanism is particularly useful in integrating hybrid representations such as drug graphs and SMILES [38], as well as drugs and proteins [32, 42]."}, {"title": "2.3 Feature Processing Methods", "content": "Beyond the drugs' structure or sequence learning with GNNs or Transformers, the extra molecular properties, such as molecular weight, solubility, and lipophilicity, are crucial for building accurate and quantitative drug-target relationship models. We summarize two typical featurization methods.\nBoth drugs and proteins are input as strings of ASCII characters, whose features can be extracted using statistical solutions. Integer encoding [18] simply converts the string to a sequence of integers, which assigns an integer to each character. The N-gram [43] captures the statistical dependencies between characters in an input string. Specifically, a 3-gram model breaks down a sequence $S = \\{s_1, s_2, ..., s_m\\}$ into $\\{[s_1, s_2, s_3], [s_2, s_3, s_4], ..., [s_{m-2}, s_{m-1}, s_m]\\}$, analyzing the relationship between adjacent characters.\nThe additional chemical properties and structural details of SMILES strings are often considered to gain a more comprehensive understanding. Extended-Connectivity Fingerprints (ECFP) [44, 45], involves generating unique identifiers for atoms based on their local chemical environment and iteratively updating these through a hash function to capture a broader molecular context, ultimately producing a set of fingerprints that represent the molecule's overall structure. Another approach, RDKit, is used to convert SMILES into molecular graphs [46, 18], where nodes represent the physical and chemical properties of molecules, and bonds are represented by an adjacency matrix. For example, atomic properties such as atom type, degree, and hydrogen information (like the number of explicit hydrogens) are all crucial for constructing a graph. More detailed properties can be found in Appendix F.\nEmbedding methods are used to translate these discrete sequences into continuous embedding spaces. Notably, Smi2Vec [47] and Prot2Vec [48] convert discrete tokens of drug SMILES and protein sequences into vectors that encapsulate semantic and syntactic similarities, effectively grouping similar tokens together in vector space. Additionally, pretrained language models [24, 49] are increasingly utilized to leverage large-scale learned patterns, fine-tuned to analyze complex protein data representations effectively."}, {"title": "3 A Fair Benchmark Platform Setup", "content": "From the perspective of reproducibility, we restrict our analysis to models for which the source code has been publicly released. To enhance the comprehensiveness, credibility, and sophistication of our benchmark, we conduct experiments on more than 30 models, including both GNN-based and Transformer-based methods. These models are derived from papers spanning the years 2018 to 2024.\nWe run these models on 6 frequently evaluated datasets including both binary interaction classification and continuous affinity regression. For the classification aspect, we utilize datasets including Human [50], Caenorhabditis elegans (C. elegans) [31], and DrugBank [51]. For regression, we employ the Davis [52], KIBA [53], and BindingDB [54] with dissociation constant (Kd) measures datasets, as processed in [22]. The statistical details of these models and datasets are presented in Section B and Table 4 of Appendix, respectively.\nGiven the critical role of hyperparameters in achieving optimal performance, we perform a detailed review of the hyperparameters associated with the selected models in Section E of the Appendix. There is significant variability in the hyperparameters across different models, making it unfair to conduct comparisons directly. To achieve equitable comparisons between varied models, we select two representative approaches from both the GNN-based and Transformer-based categories, i.e., GraphDTA [18], GraphCPI [35], MRBDTA [55], and TransformerCPI [21], to perform a greedy hyperparameter search to find their sweet spot for classification and regression tasks, respectively. For the search space of hyperparameters, we mainly focus on the influence of batch size (BS), learning rate (LR), and dropout rate (DR), as these are the common hyperparameters utilized by all models. Additionally, we standardize the hyperparameters for epochs, weight decay, and the choice of optimizer, setting a consistent 1000 epochs for GNN-based methods and 300 epochs for Transformer-based methods, with a weight decay of 0 and the Adam optimizer for all models according to Table 5. We illustrate the selected results for the metrics MSE and CI for the regression task, along with AUC-ROC and accuracy for the classification task, in Fig. 1 and the results of all metrics in Table 6. In all experiments, we employ the five-fold cross-validation method with a random split to evaluate all different methods and report the averaged results.\nWe observe that different models exhibit distinct preferences for hyperparameters. Taking into account various models and metrics, we recommend the configuration {512, 0.0005, 0.1} as the sweet point hyperparameter configuration for the GNN-based model. Similarly, for the Transformer-based model, we suggest {128, 0.0005, 0.1}. We strictly follow it in the following experiments."}, {"title": "4 A Macroscopical Benchmark on Encoder and Featurization Strategies", "content": "*Encoder Exploration for Drugs and Proteins. To investigate the influence of different encoding strategies for extracting the structural information of drugs, we employ GCN [17] and vanilla Transformer [56] as the encoders for drugs. Meanwhile, integer encoding with CNN, n-gram encoding with CNN, and the vanilla Transformer are considered to capture protein's representations, which are frequently adopted. To leverage the advantages of the pretrained protein information, we include a language model, i.e., Evolutionary Scale Modeling (ESM2) [49].\nObs. 1. GNN and Transformer-based drug encoders exhibit unequal performance depending on DTI tasks. When the encoder for the protein sequence is fixed, drug features extracted by the Transformer generally perform better than those by GNNs in regression tasks, but the opposite is true in classification tasks. Notably, in the classification tasks on Human dataset, the combination of GNN and Transformer used, respectively, for drugs and proteins yields excellent performance but falls short in the regression task. This disparity may be due to the smaller size of the Human dataset compared to the Davis dataset, which allows for faster convergence in classification tasks than in regression tasks under a fixed epoch.\nObs. 2. Transformer models are better in extracting features from protein. Although we only consider the simplest pretrained protein language model of ESM2, it still significantly outperforms other encoders. This improvement can likely be attributed to the robust and generalizable representations learned from extensive data by the pretrained model. In the classification task, transformers achieve the best performance, underscoring their effectiveness in extracting protein sequence features.\nObs 3. Integer encoding appears to be more effective when paired with a CNN as the protein encoder and a fixed drug encoder. Compared to this specific model configuration, the local context provided by 3-gram encoding does not significantly enhance the model's predictive performance. This implies that the simple relationships in amino acids' immediate neighbors, as modeled by Word2Vec, do not capture much useful information compared with simple integer encoding."}, {"title": "*Featurization Exploration", "content": "Despite the efficacy of GNNs in learning drug structures, the featur-ization of nodes plays a critical role in capturing both the intrinsic properties of atoms and their contextual relevance. We conduct a detailed analysis of various methods (summarized in Section F of Appendix) for constructing graph features within the DTI context. The node feature is constructed via various characteristics, such as chemical and physical properties. We categorize each feature into five main classes, e.g., atomic properties (AP), hydrogen information (HI), electron properties (EP), stereochemistry (Ste) and structural information (Str). To better determine which types of features are more effective in capturing the structural information, we conduct an ablation study on the different featurization strategies. Here we choose GprahDTA [18] and GraphCPI [35] as our backbone models.\nObs. 4. More complex featurization does not necessarily bring positive effect. Despite employing different protein representations (different colors in Fig. 2), GraphCPI and GraphDTA perform stably on the benchmark dataset Davis. Moreover, the basic feature configuration has the lowest MSE compared to other featurization strategies, suggesting that additional features may complicate the model's ability to discern critical information for regression task. Additionally, the increased CI with more complex features, such as stereochemistry and structure, suggests that while they might not improve the prediction accuracy, they do contribute positively to the model's ranking capability.\nObs. 5. The role of atomic and electron properties in modeling drug features may not be inherently detrimental, but their contribution appears to be context-dependent. In Fig. 2, it suggests that these properties, when not combined with other informative features, don't significantly enhance two models' performance. In the classification task, as reflected by the F1 scores and ROC-AUC, it is evident that stereochemistry and structural information substantially improve the model's accuracy. Thus, while atomic and electron properties are fundamental, their full potential is unlocked when integrated with stereochemical and structural information, underscoring the importance of a multifaceted approach in node featurization."}, {"title": "5 A Microscopic Benchmark on DTI Models", "content": "*Benchmark over Effectiveness. As shown in Table. 2 and Table. 3, we conduct experiments on 31 different models across two tasks and three datasets, respectively (see more comprehensive model comparisons in Section G of Appendix). All results are averaged by five-fold cross-validation.\nObs. 6. Molecular graphs are better than fingerprints to capture the graph features of drug. In reference to Table. 7, it is evident that GNN-based approaches utilizing the molecular graph generally yield superior performance compared with fingerprints (CPI [31], BACPI [57], GANDTI [58]). This reinforces the idea that the rich structural and atom property information inherent to molecular graphs is pivotal for representation extraction, leading to enhanced model performance.\nObs. 7. Graph structure is a crucial part in extracting drug's features. Different GNNs have the distinct performances in both tasks when the protein representation is fixed. Specifically, GIN, with its unique ability to distinguish non-isomorphic graphs, consistently outperforms other models across different protein encoders in regression tasks. Although transformer-based methods such as MRBDTA are proficient in handling sequential information from SMILES and proteins, the depth of information they capture appears to be marginally less comprehensive than that provided by molecular graph-based approaches. This is substantiated by the superior performance of GNN-based methods, including MGraphDTA, ColdDTA, and SubMDTA, which suggest that GNN captures intricate structural details more effectively."}, {"title": "*Benchmark over Efficiency", "content": "To analyze the training speed and memory usage, we empirically evaluate the peak memory and running time for various methods during the training procedure on one regression dataset and one classification task, respectively. To fairly compare various methods, we set the batch size as 32, as such maximum batch size is adopted by some methods. All results are measured on an RTX 3090 GPU."}, {"title": "Obs. 8. In general, the memory usage of GNN-based methods is smaller than that of Transformer-based methods, which is positively proportional to run time", "content": "This difference is primarily due to the self-attention mechanism employed in Transformers, which requires significant memory resources. In contrast, model parameters, such as those in DeepGLSTM, do not exhibit a direct relationship with either runtime or performance."}, {"title": "*Benchmark over Convergence", "content": "We select the top two methods from the GNN-based and Transformer-based frameworks, respectively, and evaluate them across six datasets on two tasks. The training losses are depicted in Fig. 4. In order to compare different methods, we only show the epochs before 300. Based on the empirical data, we summarize our primary observations as follows:"}, {"title": "Obs. 9. GNN-based methods demonstrate quicker and more stable convergence compared to Transformer-based methods", "content": "This phenomenon arises from the fact that GNN-based methods have fewer memory usage and model parameters, leading to a larger batch size usage or faster convergence compared with Transformer-based methods."}, {"title": "5.1 Our Best Combo of Drug and Protein Encoders", "content": "Based on our benchmark results, we summarize the insights of protein and drug encoder usages and propose a light yet effective architecture, which could be treated as new strong baseline for the following explorations. Regarding the proteins, we observe that multi-scale CNNs associated with a mixture of model depths can generally learn the effective protein representations [59, 25, 64], which approximates the language model's accuracy while having lower memory and computation costs.\nRegarding the drug molecules, both GNN and Transformer-based methods, such as MRBDTA [55], MolTrans [20] and MGraphDTA [59] prove promising in DTI tasks. This encourages us to leverage information from hybrid perspectives, i.e., implicit structure (via attention in Transformers) and explicit structure learning (via message passing along edges in GNNs).\nWe are thus motivated to integrate these powerful modules and shed novel insight into the design philosophy of drug-target interaction modeling. Our model combos are illustrated in Fig. 5, where the multi-scale CNNs and hybrid networks of molecular Transformer and GNNs are adopted to learn the representations of proteins and drugs, respectively. As shown by the graph encoder part in Fig. 5, the hybrid networks augment the differential attention matrix in molecular Transformer with inter-atomic distances and graph adjacency matrix [67], which provides the 3D and 2D molecule conformations to further facilitate atom interaction learning.\nSpecifically, given the projections of molecular input at an attention head, i.e., $Q, K, V \\in \\mathbb{R}^{N\\times d}$, the adjacent matrix $A \\in \\{0, 1\\}^{N\\times N}$, and the inter-atomic distances matrix $D \\in \\mathbb{R}^{N\\times N}$ obtained using RDkit, the augmented attention is calculated as:\n$\\text{Multi-Attn} = (\\lambda_{\\alpha} \\text{softmax}(QK^T/\\sqrt{d}) + \\lambda_{d} g(D) + \\lambda_{g} A)V$,\nwhere g() is a row-wise softmax function, and $\\lambda_{\\alpha}$, $\\lambda_{d}$ and $\\lambda_{g}$ denote scalars weighting the self-attention, distance, and adjacency matrices, respectively. Besides the implicit and explicit structure learning, we integrate the features from drug SMILES. It is notable that simply utilizing the SMILES representation extracted from a transformer for downstream tasks does not perform as well as GNN. To align with the protein embedding paradigm, we adopt a simple CNN to unearth potential SMILES information, as suggested in [68]. Subsequently, due to the fact that cross-attention is more complex and hard to optimize, we implement a straightforward attention mechanism to integrate the representations of the drug graph and SMILES, denoted as $f_G$ and $f_S$, respectively, using a weighting parameter $\\lambda$, as follows:\n$f_b = \\lambda \\cdot f_G + (1 - \\lambda) \\cdot f_S, \\lambda = MLP (MLP(f_G) + MLP(f_S))$.\nFinally, the prediction is obtained by processing the concatenated protein and drug representations through a task-relevant head, as shown in Fig. 5"}, {"title": "Benchmark Comparison to State-of-the-Art Frameworks", "content": "We compare the proposed combos with the SOTA frameworks in Tables 2 and 3, and Figures 3 and 4. It is observed that our model consistently achieves the best performance in the regression tasks across three datasets and nearly outperforms most methods in classification tasks. By leveraging the physical conformation information from the molecular graph, our combos converge faster than the other two Transformer-based methods, MRBDTA [55] and TDGraphDTA [25], particularly on the KIBA dataset. Moreover, our model uses three times less peak memory and fewer parameters than other Transformer-based methods, enabling faster computation and reduced storage requirements"}, {"title": "6 Conclusion", "content": "In this work, we establish a benchmark with fair and consistent experimental configurations, aim-ing to push DTI research, particularly emphasizing the utilization of structural information. Our meticulous approach has entailed thorough exploration of diverse encoder strategies and featurization techniques for both drug molecules and proteins. Moreover, dozens of existing approaches across six representative datasets for both regression and classification tasks are investigated on various metrics, including DTI classification and regression accuracy, peak memory usage, and model convergence. Provided with the comprehensive benchmark results, we propose a novel approach that integrates the strengths of GNN and transformer-based methods. Our studies on benchmarking and rethinking help lay a solid, practical, and systematic foundation for the DTI community and provide researchers with broader and deeper insights into the intricate dynamics of drug-target interactions."}, {"title": "A Related Works", "content": "GNNs play a crucial role in mining the intricate features of drug molecules for drug-target prediction. Numerious models, including Graph Convolutional Network (GCN), Graph Isomorphism Network (GIN), and Graph Attention Network (GAT) have been utilized [18, 35, 38, 69, 69, 61] to process and enhance drug features. Additionally, MGraphDTA [59] employs a multi-scale GNN architecture, while DeepGLSTM [29] leverages parallel GNN structures for drug representation. DeepNC integrates advanced techniques from generalized aggregation networks [70] and hypergraph convolution [71] to improve feature extraction. BACPI [57] develops a bi-directional attention network to integrate the representations of drug molecules and proteins, enhancing their mutual interaction. Besides, BridgeDPI [63] innovates by incorporating bridging nodes between proteins and drugs, utilizing a three-layer GNN for graph embeddings.\nTransformers, known for their efficacy in handling sequence data, are extensively applied in drug and protein feature processing. For instance, models like MolTrans [20] and FOTFCPI [41] employ self-attention mechanisms to refine embeddings by focusing on drug and protein substructures. MRBDTA [55] uses multi-head attention and skip connection to enhance drug and protein representation. Additionally, a cross-attention mechanism [32, 42] is employed to facilitate the integration of drug and protein features, enabling effective mutual querying. TDGraphDTA [25] captures contextual relationships between molecular substructures by using a multi-head cross-attention mechanism and graph optimization. Lastly, DrugormerDTI [72] incorporates degree centrality with positional information to highlight the positional relevance of amino acids in proteins.\nStructural information is crucial at the input stage for models such as BridgeDPI [63]. Various libraries, such as DGLGraph [73], DGL-lifeSci [74], and RDKit [46], are employed to process input SMILES of drugs, with RDKit [46] being pivotal for converting SMILE strings into molecular graphs and extracting diverse chemical properties, including chemical bonds, hydrogen presence, electron properties, and so on. Additionally, some approaches [38, 69, 57, 58] incorporate molecular fingerprints [45] to capture local chemical information. For protein sequences, typical pre-processing involves converting amino acid sequences into N-grams [36, 43] or integers [18] sequences. To enhance the expressiveness of embeddings, some models leverage pre-trained Word2Vec [75, 35, 38, 57, 31, 69, 66] or pretrained protein language models [24]."}, {"title": "B Model Descriptions", "content": "This section provides a comprehensive overview of 31 DTI methods, which are classified into GNN-based and Transformer-based approaches. The DTI framework can be simplified as using two encoders to process drugs and proteins separately, followed by an MLP to handle the integrated representations."}, {"title": "B.1 GNN-based Methods", "content": ""}, {"title": "B.1.1 GCN", "content": "* GraphDTA-GCN [18]: GraphDTA-GCN uses GCN to process the molecular graph, which is derived from SMILES using the RDkit tool, and a simple CNN with integer encoding to handle protein sequences.\n* GraphCPI-GCN [35]: Similar to GraphDTA, GraphCPI-GCN employs 3-gram encoding with pretrained Word2Vec to process protein sequences, followed by a CNN to handle the protein embeddings.\n* MGraphDTA [59]: MGraphDTA utilizes a multiscale GCN, inspired by dense connections, and a multiscale CNN to process drug graphs and protein sequences, respectively.\n* SAGDTA [60]: Similar to GraphDTA, SAGDTA introduces global or hierarchical pooling after GCN to aggregate node representations weightedly.\n* EmbedDTI [61]: For protein sequences, EmbedDTI leverages GloVe for pretraining amino acid feature embeddings, which are then fed into a CNN. For drugs, it constructs both an atom graph and a substructure graph to capture structural information at different levels, processed by GCN.\n* DeepGLSTM [29]: DeepGLSTM processes molecular graphs using a parallel GCN module composed of three GCNs with different layers. For protein sequences, it adopts a bi-LSTM.\n* CPI [31]: CPI processes drug graphs using GCN. The protein sequence is handled via n-gram with integer encoding, followed by a CNN.\n* DeepNC [30]: DeepNC adopts advanced techniques from generalized aggregation networks and hypergraph convolution, two variants of GCN, to capture the representations of drug. For protein sequences, it uses a simple CNN.\n* DrugBAN [55]: DrugBAN employs GCN and CNN blocks to encode molecular graph and proteins, respectively. Then they use a bilinear attention network module to learn local interactions between the representations of drugs and proteins.\n* BridgeDPI [63]: BridgeDPI innovates by constructing a learnable drug-protein association network, which is processed using a three-layer GNN for graph embeddings. The learned representations for drug and protein pairs are then concatenated for further processing.\n* ColdDTA [64]: ColdDTA removes the subgraphs of drugs. For the model, they adopt the dense GCN and multiscale CNN from MGraphDTA as the encoders for drugs and proteins, respectively. Additionally, an attention-based method is developed to integrate representations for improved prediction.\n* IMAEN [65]: IMAEN employs a molecular augmentation mechanism to enhance molecular structures by fully aggregating molecular node neighborhood information. It then uses multiscale GCN and CNN for drug and protein processing, respectively.\n* GanDTI [58]: Inspired by residual networks, GanDTI add the input drug fingerprints to the output of three GCN layers as graph node features and use summation to get the final drug representation."}, {"title": "B.1.2 GAT", "content": "* GraphDTA-GAT [18]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphDTA-GATGCN [18]: GraphDTA-GATGCN adopts a combination of GAT and GCN as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphCPI-GAT [35]: GraphDTA-CPI adopts a GAT as the encoder for drugs, while other compo-nents remain the same as in GraphCPI-GCN.\n* GraphCPI-GATGCN [35]: GraphCPI-GATGCN adopts a combination of GAT and GCN as the encoder for drugs, while other components remain the same as in GraphCPI-GCN.\n* BACPI [57]: BACPI adopts a GAT and a CNN for the features of the fingerprints and protein sequence, respectively. These features are then fed into a bi-directional attention neural network to obtain integrated representations.\n* PGraphDTA-CNN [24]: PGraphDTA-CNN is a straightforward method that utilizes GAT for drug feature extraction and CNN for protein sequences."}, {"title": "B.2 GIN", "content": "* GraphDTA-GIN [18]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphCPI-GIN [35]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other compo-nents remain the same as in GraphDTA-GCN.\n* SubMDTA [36]: SubMDTA utilizes a pretrained GIN encoder obtained through contrastive learning for the molecular graph. For protein sequences, it employs N-gram embedding with different N to extract features at various scales, which are then processed by a BiLSTM."}, {"title": "B.3 Transformer-based Methods", "content": ""}, {"title": "B.3.1 Self-attention", "content": "* AMMVF [38", "66": "IIFDTI model attains the drug matrix and protein matrix and inputs them to the bi-directional encoder-decoder block", "20": "MolTrans uses transformer encoder layers to augment the embedding of sub-structure sequences of proteins and drugs.\n* FOTFCPI [41", "21": "TransformerCPI uses the decoder module of Transformer", "55": "In MRBDTA, after the embedding"}]}