{"title": "Benchmark on Drug Target Interaction Modeling from a Structure Perspective", "authors": ["Xinnan Zhang", "Jialin Wu", "Junyi Xie", "Tianlong Chen", "Kaixiong Zhou"], "abstract": "The prediction modeling of drug-target interactions is crucial to drug discovery and design, which has seen rapid advancements owing to deep learning technologies. Recently developed methods, such as those based on graph neural networks (GNNs) and Transformers, demonstrate exceptional performance across various datasets by effectively extracting structural information. However, the benchmarking of these novel methods often varies significantly in terms of hyperparameter settings and datasets, which limits algorithmic progress. In view of these, we conduct a compre- hensive survey and benchmark for drug-target interaction modeling from a structure perspective, via integrating tens of explicit (i.e., GNN-based) and implicit (i.e., Transformer-based) structure learning algorithms. To this end, we first unify the hyperparameter setting within each class of structure learning methods. Moreover, we conduct a macroscopical comparison between these two classes of encoding strategies as well as the different featurization techniques that inform molecules' chemical and physical properties. We then carry out the microscopical comparison between all the integrated models across the six datasets, via comprehensively benchmarking their effectiveness and efficiency. Remarkably, the summarized insights from the benchmark studies lead to the design of model combos. We demonstrate that our combos can achieve new state-of-the-art performance on various datasets associated with cost-effective memory and computation. Our code is available at https://github.com/justinwjl/GTB-DTI/tree/main.", "sections": [{"title": "1 Introduction", "content": "The prediction modeling of drug-target interactions (DTI) has emerged as an irreplaceable task for efficacious therapeutic interventions. The binding affinity between a drug molecule and its target protein plays a significant role in the design and repurpose of drugs, where a high affinity typically indicates the desired therapeutics, target specificity, long residence, and drug resistance delay [1, 2, 3]. The precise modeling of DTI can expedite the drug discovery process and circumvent the associated cost [4, 5]. Deep learning based frameworks have recently revolutionized this field, enabling more accurate and efficient predictions compared with laboratory experimental methods [6, 7, 8].\nWithin the deep learning frameworks [9, 10], drugs are commonly represented using the Simplified Molecular Input Line Entry System (SMILES) [11], and proteins are represented as sequences of amino acids. These representations are processed by separate convolutional neural networks (CNNs) [12, 13] and subsequently integrated and processed using a multi-layer perceptron (MLP) for DTI prediction. It is notorious that the reliance on sequence-based representations can result in the loss of structural information, which can potentially compromise the DTI predictive capability. From the drug perspective, molecular structure modeling helps identify the specific binding sites [14], contribute to predicting pharmacokinetic properties [15], and allow conformational flexibility [16]."}, {"title": "2 Formulations for Drug-target Interaction Modeling", "content": "In this research, we focus on the formulations of recently-emerging structure modeling approaches for drug molecules, which could be categorized into explicit methods based on graph neural networks and implicit methods based on transformer. The target proteins are learned by the sophisticated tools of convolutional/recurrent neural networks (CNNs/RNNs) or transformers, after which both the molecules' and proteins' embeddings are integrated to facilitate interaction prediction. We will also summarize and benchmark the various widely-adopted molecule features."}, {"title": "2.1 Graph Neural Networks based Methods", "content": "A drug molecule is typically represented as a graph G = (V, E), where V and E denotes the sets of atoms and chemical bonds, respectively. The classical GNN frameworks involve key processes of aggregating and updating node features, collectively referred to as message passing, which can be mathematically represented as [26, 27]:\n$h_i^{(l+1)} = COMBINE^{(1)}_{Vi} (h_i^{(l)}, AGGREGATE_{Vj\u2208Ni}^{(1)}(f_\u03b1 (\\{h_j^{(l)}, e_{ij}^{(l)}\\})))$,\n$e_{ij}^{(l+1)} = COMBINE_{Ede}^{(1)} (e_{ij}^{(l)}, AGGREGATE_{Vj\u2208N_i}^{(1)}(g_\u03b2 (\\{h_i^{(l)}, h_j^{(l)}\\})))$,\nwhere $h_i^{(l)}$ is the feature representation of node $v_i$ at layer l, $e_{ij}^{(l)}$ is the feature representation of edge between nodes $v_i$ and $v_j$, $N_i$ refers to the set of neighboring nodes next to node $v_i$. Functions $AGGREGATE^{(1)}$ and $COMBINE^{(1)}$ aim to aggregate the neighborhood representations and integrate them together with the nodes features, respectively. Additionally, $f_\u03b1$ and $g_\u03b2$ are feature mapping functions, parameterized by \u03b1 and \u03b2, respectively. The molecule's representation can be derived using READOUT function, which processes on the set of vertex features $H^{(L)}$ at the last layer.\nGraph Convolutional Networks (GCN). Given a molecule with N atoms, the adjacency matrix A \u2208 $\\mathbb{R}^{N\u00d7N}$ indicates its connectivity, with $A_{ij}$ = 1 if atom $v_i$ is adjacent to atom $v_j$, and 0 otherwise. Considering the self-connection of atoms, we have $\\tilde{A}$ = A + I. Let X \u2208 $\\mathbb{R}^{N\u00d7C}$ denote the initial atom feature matrix. GCN [28] models the message passing as follows:\n$H^{(l+1)} = \u03c3(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$, \nwhere $H^{(l)}$ is the node feature matrix at layer l, starting with $H^{(0)}$ = X. Matrix $W^{(l)}$ represents the learnable weights for layer l, \u03c3 denotes a non-linear activation function, e.g., ReLU, and D is a diagonal degree matrix of A. A couple of pioneering works have leveraged GCN to facilitate drug-protein interaction prediction [29, 30, 31, 32]. For example, DeepGLSTM [29] uses mixture-of-depths GCNs to capture drug representations from different scales. CPI [31] considers cross-atom distance and introduces the concept of r-radius subgraphs [33], using r-radius vertices and edges to redefine the structure of graphs.\nGraph Isomorphism Networks (GIN). GIN excels in learning distinct graph features by approxi- mating the Weisfeiler-Lehman test, enabling it to distinguish a wide range of graph structures [34]. The message passing process at the (l + 1)-th layer is of the following form:\n$h_i^{(l+1)} = MLP^{(l)}((1 + \\epsilon^{(l)}) h_i^{(l)} + \\sum_{j\u2208N_i}h_j^{(l)})$,\nwhere MLP(1) is a multi-layer perceptron that parameterizes the update function, and \u2208(1) is a learnable parameter. We benchmark several GIN-based drug-target interaction modeling methods. GraphCPI [35] and GraphDTA [18] adopt GIN-based models with batch normalization to obtain the drug representation. SubMDTA [36] uses subgraph's generation task and contrastive learning to pretrain a molecular graph encoder with multiple GIN layers for further prediction.\nGraph Attention Networks (GAT). Unlike fixed-weight aggregation, GAT [37] employs an attention mechanism to determine neighborhood importance and learn the node embeddings as:\n$h_i^{(l+1)} = \u03c3(\\sum_{jEi\\cup N_i} softmax(LeakyReLU(a^T[Wh_i][Wh_j]))Wh_j)$.\nW denotes attention weights, and || is concatenating operation. GraphDTA [18] and AMMVF [38] leverage the multi-head GAT layers to optimize the atom messaging. They integrate GAT with other architectural modules, such as GCN, facilitating a more comprehensive representation of drugs."}, {"title": "2.2 Transformer-based Methods", "content": "Besides the graph representation, drugs could also be decorated as SMILES strings [39] and encoded similarly to natural language processing. Specifically, after tokenizing SMILES strings, Transformer model utilizes multi-head attention to model the interactions between different segments of the input and obtain the molecular representations. Positional encodings are also integrated to preserve the sequence order, enhancing the model's ability to process sequential information effectively. We review and benchmark two typical types of attention mechanisms used for molecular representations.\nSelf-Attention [20, 40, 41]. Self-attention computes a weighted sum of all input values based on their relevance to each other. Considering an embedding of SMILES sequence $H^{(l)}$ \u2208 $\\mathbb{R}^{d\u00d7N}$ at a specific transformer layer, where N and d are token length and dimension, respectively, the attention is"}, {"title": "2.3 Feature Processing Methods", "content": "Beyond the drugs' structure or sequence learning with GNNs or Transformers, the extra molecular properties, such as molecular weight, solubility, and lipophilicity, are crucial for building accurate and quantitative drug-target relationship models. We summarize two typical featurization methods.\nSequence Processing Methods. Both drugs and proteins are input as strings of ASCII characters, whose features can be extracted using statistical solutions. Integer encoding [18] simply converts the string to a sequence of integers, which assigns an integer to each character. The N-gram [43] captures the statistical dependencies between characters in an input string. Specifically, a 3-gram model breaks down a sequence S = {81, 82, ..., Sm} into {[S1, S2, S3], [S2, S3, S4], ..., [Sm\u22122,8m-1,8m]}, analyzing the relationship between adjacent characters.\nDrug-unique Featurization Methods. The additional chemical properties and structural details of SMILES strings are often considered to gain a more comprehensive understanding. Extended- Connectivity Fingerprints (ECFP) [44, 45], involves generating unique identifiers for atoms based on their local chemical environment and iteratively updating these through a hash function to capture a broader molecular context, ultimately producing a set of fingerprints that represent the molecule's overall structure. Another approach, RDKit, is used to convert SMILES into molecular graphs [46, 18], where nodes represent the physical and chemical properties of molecules, and bonds are represented by an adjacency matrix. For example, atomic properties such as atom type, degree, and hydrogen information (like the number of explicit hydrogens) are all crucial for constructing a graph. More detailed properties can be found in Appendix F.\nEmbedding Featurization Methods. Embedding methods are used to translate these discrete sequences into continuous embedding spaces. Notably, Smi2Vec [47] and Prot2Vec [48] convert discrete tokens of drug SMILES and protein sequences into vectors that encapsulate semantic and syntactic similarities, effectively grouping similar tokens together in vector space. Additionally, pretrained language models [24, 49] are increasingly utilized to leverage large-scale learned patterns, fine-tuned to analyze complex protein data representations effectively."}, {"title": "3 A Fair Benchmark Platform Setup", "content": "Benchmark Model and Dataset Selection. From the perspective of reproducibility, we restrict our analysis to models for which the source code has been publicly released. To enhance the comprehensiveness, credibility, and sophistication of our benchmark, we conduct experiments on more than 30 models, including both GNN-based and Transformer-based methods. These models are derived from papers spanning the years 2018 to 2024.\nWe run these models on 6 frequently evaluated datasets including both binary interaction classification and continuous affinity regression. For the classification aspect, we utilize datasets including Human [50], Caenorhabditis elegans (C. elegans) [31], and DrugBank [51]. For regression, we employ the Davis [52], KIBA [53], and BindingDB [54] with dissociation constant (Kd) measures datasets, as processed in [22]. The statistical details of these models and datasets are presented in Section B and Table 4 of Appendix, respectively.\nUnifying Hyperparameter Configuration. Given the critical role of hyperparameters in achieving optimal performance, we perform a detailed review of the hyperparameters associated with the selected models in Section E of the Appendix. There is significant variability in the hyperparameters across different models, making it unfair to conduct comparisons directly. To achieve equitable"}, {"title": "4 A Macroscopical Benchmark on Encoder and Featurization Strategies", "content": "*Encoder Exploration for Drugs and Proteins. To investigate the influence of different encoding strategies for extracting the structural information of drugs, we employ GCN [17] and vanilla Transformer [56] as the encoders for drugs. Meanwhile, integer encoding with CNN, n-gram encoding with CNN, and the vanilla Transformer are considered to capture protein's representations, which are frequently adopted. To leverage the advantages of the pretrained protein information, we include a language model, i.e., Evolutionary Scale Modeling (ESM2) [49]. The results of various combinations of drug and protein encoders are listed in Table 1.\nObs. 1. GNN and Transformer-based drug encoders exhibit unequal performance depending on DTI tasks. When the encoder for the protein sequence is fixed, drug features extracted by the Transformer generally perform better than those by GNNs in regression tasks, but the opposite is true in classification tasks. Notably, in the classification tasks on Human dataset, the combination of GNN and Transformer used, respectively, for drugs and proteins yields excellent performance but falls short in the regression task. This disparity may be due to the smaller size of the Human dataset compared to the Davis dataset, which allows for faster convergence in classification tasks than in regression tasks under a fixed epoch.\nObs. 2. Transformer models are better in extracting features from protein. Although we only consider the simplest pretrained protein language model of ESM2, it still significantly outperforms other encoders. This improvement can likely be attributed to the robust and generalizable representa- tions learned from extensive data by the pretrained model. In the classification task, transformers achieve the best performance, underscoring their effectiveness in extracting protein sequence features.\nObs 3. Integer encoding appears to be more effective when paired with a CNN as the protein encoder and a fixed drug encoder. Compared to this specific model configuration, the local context provided by 3-gram encoding does not significantly enhance the model's predictive performance. This implies that the simple relationships in amino acids' immediate neighbors, as modeled by Word2Vec, do not capture much useful information compared with simple integer encoding."}, {"title": "5 A Microscopic Benchmark on DTI Models", "content": "*Benchmark over Effectiveness. As shown in Table. 2 and Table. 3, we conduct experiments on 31 different models across two tasks and three datasets, respectively (see more comprehensive model comparisons in Section G of Appendix). All results are averaged by five-fold cross-validation.\nObs. 6. Molecular graphs are better than fingerprints to capture the graph features of drug. In reference to Table. 7, it is evident that GNN-based approaches utilizing the molecular graph generally yield superior performance compared with fingerprints (CPI [31], BACPI [57], GANDTI [58]). This"}, {"title": "5.1 Our Best Combo of Drug and Protein Encoders", "content": "Based on our benchmark results, we summarize the insights of protein and drug encoder usages and propose a light yet effective architecture, which could be treated as new strong baseline for the following explorations. Regarding the proteins, we observe that multi-scale CNNs associated with a mixture of model depths can generally learn the effective protein representations [59, 25, 64], which approximates the language model's accuracy while having lower memory and computation costs.\nRegarding the drug molecules, both GNN and Transformer-based methods, such as MRBDTA [55], MolTrans [20] and MGraphDTA [59] prove promising in DTI tasks. This encourages us to leverage information from hybrid perspectives, i.e., implicit structure (via attention in Transformers) and explicit structure learning (via message passing along edges in GNNs).\nWe are thus motivated to integrate these powerful modules and shed novel insight into the design philosophy of drug-target interaction modeling. Our model combos are illustrated in Fig. 5, where the"}, {"title": "6 Conclusion", "content": "In this work, we establish a benchmark with fair and consistent experimental configurations, aim- ing to push DTI research, particularly emphasizing the utilization of structural information. Our meticulous approach has entailed thorough exploration of diverse encoder strategies and featurization techniques for both drug molecules and proteins. Moreover, dozens of existing approaches across six representative datasets for both regression and classification tasks are investigated on various metrics, including DTI classification and regression accuracy, peak memory usage, and model convergence. Provided with the comprehensive benchmark results, we propose a novel approach that integrates the strengths of GNN and transformer-based methods. Our studies on benchmarking and rethinking help lay a solid, practical, and systematic foundation for the DTI community and provide researchers with broader and deeper insights into the intricate dynamics of drug-target interactions."}, {"title": "A Related Works", "content": "GNN-based Methods GNNs play a crucial role in mining the intricate features of drug molecules for drug-target prediction. Numerious models, including Graph Convolutional Network (GCN), Graph Isomorphism Network (GIN), and Graph Attention Network (GAT) have been utilized [18, 35, 38, 69, 69, 61] to process and enhance drug features. Additionally, MGraphDTA [59] employs a multi-scale GNN architecture, while DeepGLSTM [29] leverages parallel GNN structures for drug representation. DeepNC integrates advanced techniques from generalized aggregation networks [70] and hypergraph convolution [71] to improve feature extraction. BACPI [57] develops a bi-directional attention network to integrate the representations of drug molecules and proteins, enhancing their mutual interaction. Besides, BridgeDPI [63] innovates by incorporating bridging nodes between proteins and drugs, utilizing a three-layer GNN for graph embeddings.\nTransformer-based Methods Transformers, known for their efficacy in handling sequence data, are extensively applied in drug and protein feature processing. For instance, models like MolTrans [20] and FOTFCPI [41] employ self-attention mechanisms to refine embeddings by focusing on drug and protein substructures. MRBDTA [55] uses multi-head attention and skip connection to enhance drug and protein representation. Additionally, a cross-attention mechanism [32, 42] is employed to facilitate the integration of drug and protein features, enabling effective mutual querying. TDGraphDTA [25] captures contextual relationships between molecular substructures by using a multi-head cross-attention mechanism and graph optimization. Lastly, DrugormerDTI [72] incorporates degree centrality with positional information to highlight the positional relevance of amino acids in proteins.\nInput and Featurization Structural information is crucial at the input stage for models such as BridgeDPI [63]. Various libraries, such as DGLGraph [73], DGL-lifeSci [74], and RDKit [46], are employed to process input SMILES of drugs, with RDKit [46] being pivotal for converting SMILE strings into molecular graphs and extracting diverse chemical properties, including chemical bonds, hydrogen presence, electron properties, and so on. Additionally, some approaches [38, 69, 57, 58] incorporate molecular fingerprints [45] to capture local chemical information. For protein sequences, typical pre-processing involves converting amino acid sequences into N-grams [36, 43] or integers [18] sequences. To enhance the expressiveness of embeddings, some models leverage pre-trained Word2Vec [75, 35, 38, 57, 31, 69, 66] or pretrained protein language models [24]."}, {"title": "B Model Descriptions", "content": "This section provides a comprehensive overview of 31 DTI methods, which are classified into GNN-based and Transformer-based approaches. The DTI framework can be simplified as using two encoders to process drugs and proteins separately, followed by an MLP to handle the integrated representations."}, {"title": "B.1 GNN-based Methods", "content": ""}, {"title": "B.1.1 GCN", "content": "* GraphDTA-GCN [18]: GraphDTA-GCN uses GCN to process the molecular graph, which is derived from SMILES using the RDkit tool, and a simple CNN with integer encoding to handle protein sequences.\n* GraphCPI-GCN [35]: Similar to GraphDTA, GraphCPI-GCN employs 3-gram encoding with pretrained Word2Vec to process protein sequences, followed by a CNN to handle the protein embed- dings.\n* MGraphDTA [59]: MGraphDTA utilizes a multiscale GCN, inspired by dense connections, and a multiscale CNN to process drug graphs and protein sequences, respectively.\n* SAGDTA [60]: Similar to GraphDTA, SAGDTA introduces global or hierarchical pooling after GCN to aggregate node representations weightedly.\n* EmbedDTI [61]: For protein sequences, EmbedDTI leverages GloVe for pretraining amino acid feature embeddings, which are then fed into a CNN. For drugs, it constructs both an atom graph and a substructure graph to capture structural information at different levels, processed by GCN.\n* DeepGLSTM [29]: DeepGLSTM processes molecular graphs using a parallel GCN module composed of three GCNs with different layers. For protein sequences, it adopts a bi-LSTM.\n* CPI [31]: CPI processes drug graphs using GCN. The protein sequence is handled via n-gram with integer encoding, followed by a CNN.\n* DeepNC [30]: DeepNC adopts advanced techniques from generalized aggregation networks and hypergraph convolution, two variants of GCN, to capture the representations of drug. For protein sequences, it uses a simple CNN.\n* DrugBAN [55]: DrugBAN employs GCN and CNN blocks to encode molecular graph and proteins, respectively. Then they use a bilinear attention network module to learn local interactions between the representations of drugs and proteins.\n* BridgeDPI [63]: BridgeDPI innovates by constructing a learnable drug-protein association network, which is processed using a three-layer GNN for graph embeddings. The learned representations for drug and protein pairs are then concatenated for further processing.\n* ColdDTA [64]: ColdDTA removes the subgraphs of drugs. For the model, they adopt the dense GCN and multiscale CNN from MGraphDTA as the encoders for drugs and proteins, respectively. Additionally, an attention-based method is developed to integrate representations for improved prediction.\n* IMAEN [65]: IMAEN employs a molecular augmentation mechanism to enhance molecular structures by fully aggregating molecular node neighborhood information. It then uses multiscale GCN and CNN for drug and protein processing, respectively.\n* GanDTI [58]: Inspired by residual networks, GanDTI add the input drug fingerprints to the output of three GCN layers as graph node features and use summation to get the final drug representation."}, {"title": "B.1.2 GAT", "content": "* GraphDTA-GAT [18]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphDTA-GATGCN [18]: GraphDTA-GATGCN adopts a combination of GAT and GCN as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphCPI-GAT [35]: GraphDTA-CPI adopts a GAT as the encoder for drugs, while other compo- nents remain the same as in GraphCPI-GCN.\n* GraphCPI-GATGCN [35]: GraphCPI-GATGCN adopts a combination of GAT and GCN as the encoder for drugs, while other components remain the same as in GraphCPI-GCN.\n* BACPI [57]: BACPI adopts a GAT and a CNN for the features of the fingerprints and protein sequence, respectively. These features are then fed into a bi-directional attention neural network to obtain integrated representations.\n* PGraphDTA-CNN [24]: PGraphDTA-CNN is a straightforward method that utilizes GAT for drug feature extraction and CNN for protein sequences."}, {"title": "B.2 GIN", "content": "* GraphDTA-GIN [18]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other components remain the same as in GraphDTA-GCN.\n* GraphCPI-GIN [35]: GraphDTA-GAT adopts a GAT as the encoder for drugs, while other compo- nents remain the same as in GraphDTA-GCN.\n* SubMDTA [36]: SubMDTA utilizes a pretrained GIN encoder obtained through contrastive learning for the molecular graph. For protein sequences, it employs N-gram embedding with different N to extract features at various scales, which are then processed by a BiLSTM."}, {"title": "B.3 Transformer-based Methods", "content": ""}, {"title": "B.3.1 Self-attention", "content": "* AMMVF [38]: AWMVF introduces the multi-head mechanism to GAT to learn features in different spaces, and the update function is obtained through the concatenation of different heads' outputs.\n* IIFDTI [66]: IIFDTI model attains the drug matrix and protein matrix and inputs them to the bi-directional encoder-decoder block, which considers both the drug and target directions. The decoder is mainly composed of multi-head attention.\n* MolTrans [20]: MolTrans uses transformer encoder layers to augment the embedding of sub- structure sequences of proteins and drugs.\n* FOTFCPI [41]: Similar to MolTrans, FOTFCPI uses transformer encoder layers to extract the features of protein and drug fragments after the embedding layers.\n* TransformerCPI [21]: TransformerCPI uses the decoder module of Transformer, which takes in the atom sequence embedding processed by GCN and the protein sequence embedding processed by word2vec and 1D CNN.\n* MRBDTA [55]: In MRBDTA, after the embedding layer, drug sequences are directly fed into a block consisting of three Transformer encoders. The first encoder has a linear layer before it and the following two encoders are parallel. The protein sequence is also processed by a block with similar structure."}, {"title": "B.3.2 Cross attention", "content": "* CSDTI [32]: CSDTI use cross attention to fuse the deep representations of drugs and proteins. Specifically, the different projections of protein feature are used as key and value respectively while the projection of drug feature is used as query.\n* TDGraphDTA [25]: TDGraphDTA use a multi-head cross-attention mechanism with two attention heads. Both drug and protein features are linearly transformed into query, key and value matrices. One cross attention layer uses a drug query matrix, a protein key matrix, and a protein value matrix, while its parallel counterparts use the rest of the matrices. The outputs of these two layers are concatenated and fed into MLP to get the final output."}, {"title": "C Datasets Descriptions", "content": "In this subsection, we provide a detailed description of the datasets for both the regression task and classification task. The statistical characteristics of the datasets are summarized in Table 4."}, {"title": "D Evaluation Metrics", "content": "We adopt distinct sets of metrics to evaluate the classification and regression tasks. In particular, considering the classification task, we utilize the common metrics including Area Under Receiver Operating Characteristic Curve (ROC-AUC), Precision-Recall Area Under Curve (PR-AUC), Lo- gAUC, accuracy, precision, recall, and F1 score. For the continuous binding affinity regression, we benchmark the models using metrics of mean squared error (MSE), mean absolute error (MAE), coefficient of determination (R2), Pearson correlation coefficient, Concordance Index (CI), and Spearman correlation coefficient. Each of these metrics offers unique insights into different aspects of model performance, allowing us to assess predictive accuracy, correlation with observed values, and consistency in ranking predictions."}, {"title": "F Comparison of different featurization", "content": "In this section, we present the summarized featurization methods in Table 7, the detailed description of all properties is shown in Table 8. Besides, an ablation study on featurization strategies is in Table 9."}, {"title": "G Full experiment", "content": "The complete result on the regression task is shown in Table 10, and the complete result on classi fication task is shown in Table 11. All experiments are run on the RTX 3090 with more than 1000 hours."}, {"title": "H Memory and Parameter Comparison", "content": ""}, {"title": "I Limitations", "content": "In this section, we provide limitations on the proposed benchmark. A limitation of the proposed benchmark is that it considers a limited number of datasets and solely on random splitting for dataset splitting. Additionally, the benchmark only evaluates models using simple inputs without incorporating additional information, which may not reflect the current trend where many models depend on pre-trained language models."}]}