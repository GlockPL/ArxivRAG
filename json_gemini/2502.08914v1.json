{"title": "Diffusion Models Through a Global Lens: Are They Culturally Inclusive?", "authors": ["Zahra Bayramli", "Ayhan Suleymanzade", "Na Min An", "Huzama Ahmad", "Eunsu Kim", "Junyeong Park", "James Thorne", "Alice Oh"], "abstract": "Text-to-image diffusion models have recently\nenabled the creation of visually compelling,\ndetailed images from textual prompts. How-\never, their ability to accurately represent vari-\nous cultural nuances remains an open question.\nIn our work, we introduce CULTDIFF bench-\nmark, evaluating state-of-the-art diffusion mod-\nels whether they can generate culturally specific\nimages spanning ten countries. We show that\nthese models often fail to generate cultural arti-\nfacts in architecture, clothing, and food, espe-\ncially for underrepresented country regions, by\nconducting a fine-grained analysis of different\nsimilarity aspects, revealing significant dispar-\nities in cultural relevance, description fidelity,\nand realism compared to real-world reference\nimages. With the collected human evaluations,\nwe develop a neural-based image-image simi-\nlarity metric, namely, CULTDIFF-S, to predict\nhuman judgment on real and generated images\nwith cultural artifacts. Our work highlights the\nneed for more inclusive generative AI systems\nand equitable dataset representation over a wide\nrange of cultures.", "sections": [{"title": "1 Introduction", "content": "Text-to-image (T2I) diffusion models recently have\nshown a significant advance in generating high-\nquality images from text prompts. Remarkable success has been demon-\nstrated in producing realistic and detailed visual\nrepresentations, ranging from generating complex\nscenes to performing style trans-\nfer using diffusion models. While it is crucial to address\nthe challenge of generating culture-aware imagery,\ncurrent diffusion models have yet to be evaluated\non their ability to create a distribution of accu-\nrate and fair representations of images."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Cultural-Aware Datasets", "content": "Beginning with the benchmark to evaluate image\nquality from simpler text prompts using text-to-\nimage (T2I) diffusion models, such as HPDv2 using human preferences, more\nrecently advanced efforts include creating com-\nprehensive benchmarks such as T2I-CompBench and GenAI-Bench. These prior benchmarks assess the per-\nformance of T2I models across various aspects,\nincluding realism, fidelity, and compositional text-\nto-image generation. Although a very recent study\nhas started investigating global\nrepresentation in T2I models, their emphasis has\nprimarily been on regional social stereotypes.\nOn the other hand, many widely-used datasets\nfor training T2I synthesis models, such as LAION-\n400M, tend to exhibit\nAnglo-centric and Euro-centric biases, as noted by\nBirhane et al. (2021). These biases skew the rep-\nresentation of cultures in generated images, often\nfavoring Western perspectives. In response to this,\nseveral researchers have worked to create datasets\nthat better represent diverse cultures. For exam-\nple, the MaRVL dataset was\nspecifically designed to include a broader array\nof languages and cultural concepts, covering re-\ngions such as Indonesia, Swahili-speaking Africa,\nTamil-speaking South Asia, Turkey, and Mandarin-\nspeaking China and addressing biases in datasets\nthat predominantly focused on North American and\nWestern European cultures.\nIn the Large Language Models (LLMs) domains,\nSeeGULL dataset broadens stereo-\ntype benchmarking to encompass global and re-\ngional diversity, and BLEnD benchmark evaluates the cultural knowledge of\nLLMs across various languages including low-\nresource ones. Similarly, the Dollar Street dataset\nsought to capture ev-\neryday life across a wide variety of socioeconomic\nbackgrounds, presenting a more globally inclusive\nview. In addition, Liu et al. (2023) introduced the\nCCUB dataset developed to promote cultural in-\nclusivity by collecting images and captions repre-\nsenting the cultural contexts of different countries.\nMost recently, CUBE is a\nlarge-scale dataset of cultural artifacts spanning 8\ncountries across different geo-cultural regions for\nevaluating cultural diversity. Our work further con-\ntributes to dataset creation by expanding the focus\nto include low-resource cultures, thereby address-\ning gaps between overrepresented and underrepre-\nsented cultural contexts."}, {"title": "2.2 Diffusion Model Evaluation", "content": "Several metrics have been developed and widely\nused to evaluate the quality of images generated\nby T2I models. These include measures of realism\nsuch as the Inception Score (IS), Fr\u00e9chet Inception Distance (FID) and Image Realism Score (IRS) (Chen\net al., 2023). In particular, IS evaluates image qual-\nity and diversity based on classification probabil-"}, {"title": "3 CULTDIFF Benchmark", "content": "This section outlines the process of building our\nCULTDIFF benchmark, including the data construc-\ntion (steps 1-3 in Figure 1 and Section 3.1), human\nannotation (step 4 in Figure 1 and Section 3.2), and\ndesign of the architectural framework for the metric\nevaluation (steps 5-6 in Figure 1 and Section 3.3)."}, {"title": "3.1 CULTDIFF Dataset Construction", "content": "In this section, we present the construction of the\nCULTDIFF dataset, which is designed to evaluate\nthe performance of diffusion models in generating\nculturally relevant images across various artifact\ncategories and countries. The dataset creation pro-\ncess is composed of three key steps as outlined in\nFigure 1: (1) prompt generation, (2) collection of\nreal images, and (3) synthetic image generation."}, {"title": "3.2 Human Evaluation", "content": "A critical part of our experimental design involved\nhuman annotation to assess the cultural and con-\ntextual relevance of AI-generated images. We first\ncreated a survey comprising 150 images for all\nten countries (e.g., United States, China, Azerbai-\njan, Indonesia), where the images for each country\nwere equally divided across three categories: ar-\nchitecture, clothing, and food items. Each survey\nincluded a set of four images for each artifact: three\nreal images (collected via web scraping) and one\nAI-generated image (produced by one of the three\ndiffusion models)."}, {"title": "3.3 Model Evaluation", "content": ""}, {"title": "3.3.1 Pair and Dataset Creation", "content": "Our training data consists of real image and real-\nsynthetic image pairs, ensuring a rich, wide variety\nof training samples. Specifically, we constructed\nthe following image pairs:\nReal image pairs: Positive pairs consist of real\nimages of the same artifact (e.g., two images of\nthe Flame Towers in Azerbaijan), while negative\npairs include real images of different artifacts from\nthe same country or any artifacts from different\ncountries (e.g., the Flame Towers vs. the Empire\nState Building).\nReal-synthetic image pairs: The positive image\npairs generated by diffusion models (as described\nin Section 3.1) with an average image-image simi-\nlarity score \u2265 3, paired with three real images used\nin the human evaluation. The negative pairs consist\nof AI-generated images with an average image-\nimage similarity score < 3, paired with three real\nimages similar to the positive pairs.\nWe split our dataset using disjoint prompts for\neach set to ensure no overlap between the training,\nvalidation, and test sets. We selected 30 random\nprompts for the training set, 10 for the validation\nset, and 10 for the test set, along with their respec-\ntive pairs from each model, across all categories\nand countries. In total, the training set consists of\napproximately 11k image pairs, while both the val-\nidation and test sets contain 2.7k image pairs each.\nThe training set includes both real image pairs and\nreal-synthetic image pairs, while the validation and\ntest sets consist solely of real-synthetic image pairs,\nwith no overlapping images between the sets."}, {"title": "3.3.2 Similarity Metric", "content": "To build a model-based image-image similarity\nmetric, CULTDIFF-S, we adopted a contrastive\nlearning approach, utilizing Vision Transformers\n(ViT) to learn embeddings for both\nreal and AI-generated images, as illustrated in Fig-\nure 2. The global attention mechanism in ViT en-\nables effective \"local-to-global\" correspondence,\nenhancing self-supervised learning. This ViT property is par-\nticularly important in our work, where it helps\nevaluate the cultural and contextual relevance of\nimages by capturing both local features and their\nbroader context. To achieve this, we employed"}, {"title": "4 Results", "content": ""}, {"title": "4.1 Benchmark Results and Analysis", "content": ""}, {"title": "4.1.1 Overall Image-Image Similarity", "content": "We first analyze the similarity results (image-image\nsimilarity scores) of the real and generated images\nfrom the survey's first question (Q1), which ex-\namined various aspects of similarity. In particular,\nQ1.1 asked participants to rate the overall similar-\nity.  highlights the average scores for all\ncountries across three models. The first half of\nsummarizes the country results: overrep-\nresented countries (South Korea, USA, UK, Mex-\nico) predominantly occupy the top half (highest\nscores), while underrepresented countries (Azer-\nbaijan, Ethiopia) are mostly in the lower half, with\nsome exceptions.\nQ1.2 specifically addressed distinct features:\nshapes for architectural structures, color/texture\nfor clothing, and presentation/plating for food. The\nhighlight that the\nUSA, UK, and South Korea consistently ranked\namong the highest-scoring countries across all cat-\negories and models. In contrast, Ethiopia and In-\ndonesia frequently appeared in the lower half of\nthe rankings. Interestingly, China also consistently\nreceived some of the lowest scores in this analy-\nsis. These findings suggest a need for the devel-\nopment of diffusion models that better represent\nunderrepresented countries, such as Ethiopia and\nIndonesia, to improve their cultural and contextual\naccuracy compared to overrepresented countries\nlike the USA, UK, and South Korea."}, {"title": "4.1.2 Image-Description Match", "content": "Our analysis focused on the responses to the sec-\nond question (Q2), which assessed the fidelity of\nthe generated content to the provided description.\npresents the results for the FLUX model's\noutputs across architecture, clothing, and food, re-\nspectively (refer to Appendix A.3 for results from\nother models).\nThe United States and the United Kingdom\nconsistently achieved the highest average fidelity\nscores across all three categories. In contrast, Azer-\nbaijan, South Korea, and Ethiopia recorded the\nlowest average scores of 2.28, 1.55, and 1.54 in the"}, {"title": "4.1.3 Realism", "content": "The third question, which focused on rating the re-\nalism of the generated images, is visualized in the\nbar graphs in Figure 5. The FLUX model's output\nshows that realism scores are relatively higher for\nthe United Kingdom, United States, and Mexico,\nwhile Spain, Ethiopia, and South Korea have lower\nscores. In contrast, the Stable Diffusion 3 Medium\nmodel exhibits relatively consistent realism scores\nacross all countries. Meanwhile, the Stable Diffu-\nsion XL model demonstrates slightly lower average\nrealism scores across all countries and categories\ncompared to the other two models."}, {"title": "4.2 Similarity Metric Analysis", "content": "After training the similarity model, we evaluated\nits performance using our test dataset pairs. For\neach evaluation pair, we computed additional sim-\nilarity metrics, including Fr\u00e9chet Inception Dis-\ntance (FID) , Learned Percep-\ntual Image Patch Similarity (LPIPS), and the Structural Similarity Index Measure\n(SSIM). Additionally, we cal-\nculated the correlation between image-image simi-\nlarity scores-derived by averaging the responses\nto the first four questions in the survey-and each\nof these similarity metrics. In our metric model, we\ncompute the cosine similarity of the embeddings\nproduced by the model during evaluation as the\nfinal similarity score. The results of these corre-"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Human-Aligned Image-Image Similarity\nEvaluation Metric", "content": "Designing an image-image similarity metric that\nclosely correlates with human rankings is inher-\nently difficult due to the complexity of cultural\nattributes and subjective interpretations. Culture\nitself is a broad and multifaceted concept, making\nit challenging to define in a universally accepted\nmanner. In fact, many previous\nstudies in related domains that incorporate cultural\naspects either provide only a high-level definition\nor avoid in-depth discussions on its nuances. In our study, we treated countries as representa-\ntives of distinct cultures. However, individuals,\neven within the same country, may have diverse\ninterpretations of cultural artifacts, reflecting re-\ngional, ethnic, and personal differences in cultural\nperception, which can lead to subjective similarity\nscores given by human annotators. Additionally,\ncultural artifacts often evolve over time, influenced\nby various internal or external factors, further com-\nplicating the task of cultural representations. As a\nresult, an effective similarity metric that correlates\nmore closely with human perception must account\nfor both objective attributes, such as structural and\nvisual features, and subjective factors, including\nhistorical context, regional variations, and personal\ninterpretations. We believe that developing such\na metric necessitates integrating human feedback\ninto the process, combining both data-driven ap-\nproaches and human-centered evaluations to bridge"}, {"title": "5.2 Culturally Aware Diffusion Models", "content": "Recent advancements in fine-tuning diffusion mod-\nels have demonstrated significant improvement. Following similar approaches in\ncultural contexts, fine-tuning diffusion models on\nsmaller, culturally-curated datasets has been shown\nto lead to more culturally relevant image genera-\ntion. While approaches relying\non large culture-specific datasets, such as Japanese\nStable Diffusion (Shing and Sawada, 2022), have\nbeen generally successful in enhancing cultural\nawareness for specific cultures, the limited resource\npresence of underrepresented cultures creates a\nchallenge for applying similar methods. In this\ncontext, our dataset further provides a foundation\nfor incorporating data from underrepresented cul-\ntures/countries, broadening the reach of culturally\ninclusive image generation. We hope that our work\nwill inspire further research into improving the fair-\nness of T2I generation models, particularly by ex-\npanding representation for low-resource cultures in\nboth model training and evaluation."}, {"title": "6 Conclusion", "content": "In this paper, we assessed the cultural awareness\ndiffusion models spanning 10 countries and 3 cate-\ngoriessingby our proposed CULTDIFF Benchmark\nDataset. Our human evaluations of various as-\npects\u2014such as the similarity of AI-generated im-\nages to real counterparts, description fidelity, and\nrealism\u2014revealed that the diffusion models exhibit\nbetter cultural awareness for high-resource coun-\ntries than low-resource countries, such as Ethiopia,\nAzerbaijan, and Indonesia. Furthermore, we pro-\nposed an automatic image-image similarity metric"}, {"title": "Limitations", "content": "The main limitation of our work lies in recruiting\nonly three annotators per country due to budget\nconstraints, similar to most human experiments\nin other studies. While we could derive valuable\ninsights from the human evaluation results, increas-"}]}