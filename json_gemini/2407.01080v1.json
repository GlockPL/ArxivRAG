{"title": "Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese", "authors": ["Yunqi Xu", "Tianchi Cai", "Jiyan Jiang", "Xierui Song"], "abstract": "The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark Face4RAG for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval Augmented Generation (RAG), a technique of augment-ing the context of Large Language Models (LLMs) with relevantpassages retrieved from external retrievers or search engines [27],has demonstrated strong performance on various knowledge inten-sive tasks such as open domain conversation [38, 41] and questionanswering [19]. Despite its bright prospect, factual consistency re-mains a critical issue for RAG systems. Recent assessment revealsthat even for the leading-edge commercial RAG systems like BingChat and Perplexity, barely over half of their outputs are factualconsistent with the references [29]. This issue urges the need ofstudying factual consistency evaluation (FCE) in the RAG task.Various FCE methods have been proposed to evaluate the fac-tual consistency of specific RAG systems, among which a two-stepapproach shows promising results, especially for evaluating longanswers [10, 31]. As shown in the bottom left of Figure 1, this ap-proach first segments the answer into shorter pieces, then evaluates the factual consistency of each segment with respect to the givenreference. In this way, the evaluation of a long answer is decom-posed into evaluations on several simpler pieces of information,which improves the detection of factual inconsistency.In previous works, these FCE methods are evaluated by answersgenerated by the underlying LLM in the specific RAG system beingstudied [13, 18]. Despite their effectiveness on the specific system,it is unclear how these methods generalize to new RAG systems.As discovered in a recent study [31], the optimal FCE method mayvary when evaluating different LLMs, hence achieving a superiorperformance regarding some certain LLM does not guarantee astrong performance on other LLMs. In this sense, previous bench-marks generated by a single LLM are not fair enough to evaluate the overall performance of FCE methods.To fill this gap, in this paper, we first construct a comprehensivebenchmark to enable the evaluation of FCE methods independentof the underlying LLM. Specifically, we first propose a novel errortypology to cover various factual consistency errors in RAG, whichincludes three main categories, i.e., hallucination error, knowledgeerror, and logical fallacy, and is further divided into nine error types.Based on our predefined error typology, we construct a syntheticdataset in Chinese to assess the effectiveness of FCE methods acrossthe different types of errors. Furthermore, we construct a real-worlddataset in Chinese by generating answers using six distinct LLMswithin RAG tasks. Empirical analysis on the real-world datasetshows that 6.96% of all factual inconsistent samples involve logicalfallacies. In addition, we observe that different LLMs exhibit diverseerror distributions, which echoes previous research [31] and justi-fies our motivation of constructing a comprehensive benchmarkindependent of LLMs."}, {"title": "2 RELATED WORK", "content": "Traditional FCE Methods. Evaluating the factuality of modelgenerated results is widely studied across various language modelgeneration domains like text summarization [15], dialogue sum-mary [47] and question-answering [4]. When the golden labelsare given, prior methods using exact match metrics [8, 19, 27] orsimilarity-based metrics are proposed [7, 46]. However, high qual-ity answers can vary a lot, hence these approaches using goldenlabels may significantly underestimate the models' performances,especially for long answers [42].FCE for Long-form Answers. To effectively evaluate factual-ity of long answers, recent FCE research mostly take a two stepapproaches [23, 26], where in the first step, the long-form answeris decomposed into shorter segments, such as sentences [24, 26],sub-claims [10, 23], individual facts [31] and structured triplets [18].Then the second step evaluates the verifiability of each segmentwith respect to the given reference text [25, 26, 45], which can beefficiently done by modern general purpose LLM [9, 31], e.g., GPT-4.Although we follow the two-step approach, our method differs fromthem in the ability of leveraging logical connections via specialdesigns of logic-preserving decomposition and fact-logic FCE.FCE Benchmarks. Prior benchmarks for FCE mostly focuson specialized tasks like summarization [13, 24, 39]. For FCE inRAG, existing benchmarks are derived from specific LLMs, such asRefchecker [18] and FELM [9], which are constrained by the errortype distribution of the underlying LLMs. Unlike these benchmarks,we construct a synthetic dataset based on our error typology, whichenables evaluation independent to any underlying LLM."}, {"title": "3 FACE4RAG BENCHMARK", "content": "Recall that existing FCE benchmarks only use answers generatedby some certain LLMs, which may fail to evaluate FCE methods onother LLMs with different error distributions or unseen error types.To remedy this issue, in this section, we propose a novel approachto construct a FCE benchmark for RAG, which is independent ofthe underlying LLMs and called FActual Consistency Evaluation forRAG (Face4RAG). Face4RAG contains an error-type-oriented syn-thetic dataset and a real-world dataset. To construct the syntheticdataset, inspired by the error typology used in an exam designed forhumans, i.e. the National College Entrance Examination of China,we first propose a novel error typology to classify any factual con-sistency error in RAG task, which includes nine types of errorsbelonging to three main categories. Based on the proposed errortypology, we then construct a synthetic dataset to evaluate FCEmethods on each type of the error. Besides the synthetic dataset,we also collect samples from six commonly used LLMs to constructa real-world benchmark, which aims to evaluate the overall factualconsistency of FCE methods in real-world scenarios. The detailsabout Face4RAG can be seen in Table 1 and Figure 2."}, {"title": "3.1 Error Typology in FCE", "content": "Our error typology for FCE is inspired by the questions in theNational College Entrance Examination of China [1], which arecarefully designed to test the ability of human to evaluate factualconsistency. In this examination, reading comprehension is a majorsection to evaluate the participants' skill of understanding a Chi-nese text. Factual consistency evaluation is a typical task in thissection. Given the text, the participants are required to evaluate thecorrectness of several answers to a specific question [1], which isessentially a RAG task (see examples in Table 11 in the appendix).As these questions are designed for a competitive entrance exami-nation of higher education institutions at the undergraduate level,they are generally hard to answer and cover a wide range of factualinconsistency error types. Accordingly, we develop a novel errortypology for RAG, which comprises three main categories and isfurther classified into nine error types. In the following, we give adetailed description of our proposed error typology.Hallucination Error This class of error refers to the situationwhen the answer contains information that cannot be traced backto the reference [30]. Note that there are two main usages of theterm hallucination in previous literature: one refers to \"unfaithfulor nonsensical\" generated answers [21], the other further includes\"unverifiable\" answers using the given context [30, 40]. Here weadopt the second usage that has a larger scope.\n\u2022 Hallucination Error (Hallu.) refers to the situation when theanswer is either unfaithful or unverifiable using the givencontext (even when it is factually correct).Knowledge Error This class of error refers to the situationwhen the information contained by the answer is inaccurate orincorrect regarding the reference [9]. This may occur in variouscomponents of a sentence, such as the subjects, predicates, objects,adverbials of time and place, etc. We classify the knowledge errorinto four error types:\n\u2022 Contradiction Error (KCont.) refers to the situation when thestatement in the answer conflicts with information from thereference.\n\u2022 Entity Inversion Error (KInve.) refers to the situation whenentities in the answer (events, processes, or concepts) areswapped in their positions as compared to the reference.\n\u2022 Conflation Error (KConf.) refers to the situation when theentities in the reference (subjects, predicates, or objects) areinaccurately combined, altering the original meaning."}, {"title": "3.2 Synthetic Dataset", "content": "Based on the above proposed error typology, we construct a synthetic dataset. In the dataset, the positive samples are factual con-sistent, whereas each negative sample has at least one factual in-consistency error. The dataset is constructed based on WebCPM[36], a web-enhanced question answering dataset in Chinese. Dueto the space limit, in the following we briefly describe the processof dataset generation. Please refer to Appendix A for more detailsof the construction of our synthetic dataset.Negative Samples For each specific error type in the typology,we design a prompt to generate samples with this error. For the hal-lucination error, we setup three levels of difficulty for the evaluatorto detect inconsistency and construct samples accordingly. For theremaining two categories, i.e., knowledge error and logical fallacy,we design a specific prompt for each error type except the Contra-diction Error (KCont.). Since KCont. may occur at different levelsof granularity [12], i.e., word or sentence, we design one promptfor each. Apart from the above error types, we construct a newerror type called Other Logical Fallacy (LOthe.), which accounts forpotential errors in some complex logical connections uncovered byour previously defined four types of logical fallacy.Positive Samples To enrich the sample diversity, we apply theaugmentation technique in [28]. Specifically, the original positivesamples in WebCPM are augmented by synonym replacing andparaphrasing via certain prompt at either word or sentence level.Human Annotation Refinement To enhance the quality of thecoarse labels derived above, we further engage 12 human expertsto annotate the factual consistency of each answer via a two-stepapproach [31]."}, {"title": "3.3 Real-World Dataset", "content": "The synthetic dataset is generated based on various predefined errortypes without considering the distribution of these error types inthe real world. Consequently, there is a need for another evaluationdataset that better aligns with the actual distribution of answers inreal-world RAG scenarios, thus serving as a real-world dataset. Incontrast to previous studies that relied solely on GPT-based LLMsfor generating responses to create their evaluation sets [9, 31], weadopt a more comprehensive approach by utilizing six differentLLMs to construct our real-world dataset.Specifically, we first collect 200 questions along with correspond-ing references. We then prompt six commonly used LLMs to gen-erate answers for the questions based on the references, includinggpt-4-turbo (GPT-4) [2], gpt-3.5-turbo (GPT-3.5) [33], Baichuan2-13B-Chat (Baichuan2) [5], ChatGLM3 [44], Qwen-14B-Chat (Qwen)[3], and Chinese-Alpaca-2-13B-16k [11] (Alpaca2 (CH)). In thisway, we derive a total of 1200 data points, which constitute thereal-world dataset.For each data point, we follow the same human annotation proce-dure as in our synthetic dataset to inspect if it is factually consistent.Moreover, if an answer is deemed factually inconsistent, the annotator will assign a specific error type from our proposed errortypology to the answer. When the annotators notice that error ofthe answer does not fall into the aforementioned error types, theywill mark the answer as \"Other Errors\".We now conduct empirical analyses on the error typology andthe behaviors of various LLMs on the above real-world dataset.Overall Error Type Distribution We first justify our study onlogical fallacy consistency detection by empirically showing thatthe logical fallacy errors are prevailing in the answers generated byvarious LLMs. To this end, we analyze the distribution of the errortypes annotated across the entire real-world dataset. As shown inFigure 2, the hallucination error, knowledge error and logical fallacyaccount for 73.78%, 28.31%, 6.96% of all the inconsistent samples,respectively. It worth note that this 6.96% logical fallacy errors arenot studied in the previous FCE methods. Besides, only 0.23% of theinconsistent samples are marked as \"Other Errors\" by annotators,which suggests the comprehensiveness and completeness of ourproposed error typology.Error Distribution of Various Models We then look deeperinto the error types and their distributions among various LLMsin RAG. As presented in Figure 3, various LLMs exhibit distinctdistributions on error types. For instance, LIncl. emerges in threeof the LLMs, and LCaus. and LConf. occurs in four models. In ad-dition, while Hallu. exists in all models, GPT-4 has a notably high"}, {"title": "4 LOGIC-ENHANCED FACTUAL CONSISTENCY EVALUATION", "content": "In the above statistic analysis, logical fallacy accounts for a con-siderable proportion of factual errors in real-world RAG scenarios.However, as we have analyzed before, existing FCE pipelines ne-glect the logical connections between segments in the originalanswer, which may result in wrong factual consistency evaluationresult for samples with logical fallacy. Hence, to improve the eval-uation ability of factuality consistency, a natural direction is to"}, {"title": "4.1 Logic-Preserving Answer Decomposition", "content": "Most existing studies directly decompose answers into segments,each containing only a single piece of information [23, 31]. In con-trast, we propose to decompose the answers based on semanticlinkages\u00b2 and logical connections, which preserves the logical rela-tionships and facilitates logical consistency evaluation. The coredesign in this module is an elaborated prompt based on the follow-ing three principles for answer decomposition.\n\u2022 We prompt GPT-4 to execute the decomposition only whenthe two or multiple sentences do not exhibit strong semanticor logical connection.\n\u2022 To ensure that each segment can be understood by GPT-4independently without leveraging other segments, any pro-noun in a segment that refers to other contextual informationshould be substituted with appropriate reference.\n\u2022 During the decomposition process, GPT-4 is required tomaintain the sentence structure of the original answer to thebest extent. This principle alleviates the risk of introducingadditional hallucination to the original answer.In order to help GPT-4 better understand our principles for an-swer decomposition and deal with texts with various formats, weconstruct three kinds of instances to serve as the few-shot examples.The specific type of instances are as follows:"}, {"title": "4.2 Fact-Logic FCE", "content": "Previous methods directly invoke an LLM to evaluate the decom-posed segments and overlook the logical fallacy. To evaluate thelogical fallacy, we develop a two-stage procedure for factual con-sistency evaluation, which consists of a conventional stage of factconsistency evaluation and an extra stage that evaluates from bothperspectives of fact and logic; we introduce the COT mechanism[43] into both stages to improve LLM's ability of evaluation. Theprompts for each stage are provided at our benchmark webpage.1Fact Consistency Evaluation In this stage, GPT-4 is instructedto assess the consistency of each piece of information in the segmentagainst the reference, which mainly concerns with the hallucina-tion error and the knowledge error. Unlike previous methods thatdirectly instruct the model to assess the consistency with the ref-erence [9, 13], we use the COT technique to guide the model toevaluate the segment step-by-step, with the following steps:\n(1) Informational Points Extraction: GPT-4 extracts all informa-tional points from the segment. This step ensures that eachcomponent of the segment will be evaluated.\n(2) Context Identification: For each informational point, GPT-4locates the corresponding content within the reference."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to evaluate the ef-fectiveness of our proposed L-Face4RAG pipeline. Our experimentsshow that on both synthetic data and real-world data in Face4RAGbenchmark, our L-Face4RAG method substantially outperformsthe existing FCE methods. Notably, its superiority goes beyond theChinese RAG task from which L-Face4RAG is originally motivated,as L-Face4RAG achieves SOTA results on 6 out of 7 of the existingEnglish datasets and also a substantially higher average score onall tasks."}, {"title": "5.1 Experimental Setup", "content": "Baselines We compare L-Face4RAG with four GPT-based fine-grained FCE methods:\n\u2022 FACTSCORE [31] first breaks the answer into a series ofatomic facts and then assigns a binary label to each atomicfact individually.\n\u2022 FELM [9] first segments the answer into fine-grained textualspans and then evaluates the factual consistency of all textualspans collectively. It outputs the corresponding numbers offactual inconsistent textual spans if existed.\n\u2022 Ragas [13] first extracts a set of statements from the answerand then evaluates the factual consistency of all statementscollectively, outputting a binary label for each statementalong with the corresponding reason for the assessment.\n\u2022 Refchecker [18] extracts knowledge triplets from the answerand evaluates each knowledge triplet separately.Implementation Details As the above FCE baselines are origi-nally designed for tasks in English, we adapt them to our ChineseRAG task by translating their prompts into Chinese.When experimenting with FELM, we utilize the Reference-docaugmented evaluator [9], in alignment with our task which is fo-cused on evaluating the factual consistency of answers against theirreferences. Specifically, we input our references as the retrievedreference doc in FELM's evaluation framework. We select the best-performing estimator in [9], i.e., decomposing the answer withsegment-based method and utilizing GPT-4 as the factual errordetector.Since the original settings of FACTSCORE and RAGAS are basedon GPT-3.5, we conduct experiments with both GPT-3.5 and GPT-4 to eschew the effect of the possible performance gap betweenGPT-3.5 and GPT-4 on the empirical results.Finally, to apply our proposed evaluation pipeline, we decomposethe answer into segments and assess the factual consistency of eachsegment respectively. The outputs include both the label and thecorresponding explanations. To derive deterministic output fromGPT-4, we set its temperature to 0."}, {"title": "5.2 Performance Comparison on Face4RAG", "content": "We first compare the performance of our proposed L-Face4RAGpipeline against various FCE baselines on the Face4RAG benchmark,which includes a synthetic dataset and a real-world dataset."}, {"title": "5.3 Performance Comparison on Existing FCE Benchmark", "content": "We then evaluate the robustness and applicability of the proposedL-Face4RAG method on other factuality detection tasks, and inEnglish. Specifically, we consider several commonly used FCEbenchmarks in English on various tasks, including RAG[13, 18],summarization [14, 34], dialogue [16, 17] and fact verification [37].In Table 5, we report the predictive accuracy of examined FCEmethods on the above tasks. The results show that our proposedL-Face4RAG achieves SOTA results on 6 out of 7 of the existingdatasets and also a substantially higher average score on all tasks,indicating the effectiveness of L-Face4RAG beyond the original factuality evaluation task in RAG, and its robustness to other languages.This validates the wide-applicability of our proposed method.Besides the above comparison among different methods, we alsoobserve that the ranking of the average score of various methods onthe above commonly used benchmarks is similar to the ranking of the average score on all public tasks is 0.9, and the same 0.9 between the rankings on our real-world dataset and the public datasets. This validates the strong correlation between the evaluation results of our new benchmark and the results on existing benchmarks."}, {"title": "5.4 Ablation Study", "content": "We now verify the specific design choices of our proposed evalua-tion pipeline by ablation study on Face4RAG benchmark. Specifically, we examine the effectiveness of each designed module bycomparing L-Face4RAG with the counterpart method without sucha module. Due to space limit, here we only present the results on thesynthetic dataset. Results on the real-world dataset are qualitativelysimilar and deferred to Appendix B.Evaluating the Answer Decomposition Module. (A.D.) Recall that our decomposition module preserves the logic connectionwithin one segment, which may help better identify logical fallacy while reducing extra hallucination induced by decomposition.To verify this point, we conduct an ablation study by replacingour approach by a conventional decomposition method [13]. As presented in Table 6 and Table 7, we observe a severe decline ofoverall accuracy in the counterpart method, especially for negativesamples related to logical fallacy. This phenomenon accords withour intuition that conventional answer decomposition method may"}, {"title": "6 CONCLUSION", "content": "In this work, we give a systematic study of factual consistencyevaluation in RAG. Specifically, we first propose a comprehensivebenchmark termed Face4RAG, which includes the synthetic datasetand the real-world dataset. In light of the possible failure of existingFCE methods in detecting logical fallacy in RAG, we then proposea novel FCE method termed L-Face4RAG. Compared to previousmethod, our method has two novel designs, i.e., logic-preservingdecomposition and fact-logic FCE, which can better characterize thelogical relations in different pieces of information in the sentence,leading to higher ability of logical fallacy evaluation. Extensiveexperiments on both the synthetic and real-world datasets verify theeffectiveness of the L-Face4RAG method. Notably, the superiorityof L-Face4RAG is consistent on a wide range of factuality detectionbenchmarks beyond the Chinese RAG task. Elaborated ablationstudies also justify our core algorithm designs."}, {"title": "A CONSTRUCTION DETAILS ABOUT SYNTHETIC DATASET", "content": "Negative Samples The negative samples are constructed basedon WebCPM [36], a web-enhanced question answering dataset"}]}