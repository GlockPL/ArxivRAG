{"title": "Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese", "authors": ["Yunqi Xu", "Tianchi Cai", "Jiyan Jiang", "Xierui Song"], "abstract": "The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark Face4RAG for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval Augmented Generation (RAG), a technique of augmenting the context of Large Language Models (LLMs) with relevant passages retrieved from external retrievers or search engines [27], has demonstrated strong performance on various knowledge intensive tasks such as open domain conversation [38, 41] and question answering [19]. Despite its bright prospect, factual consistency remains a critical issue for RAG systems. Recent assessment reveals that even for the leading-edge commercial RAG systems like Bing Chat and Perplexity, barely over half of their outputs are factual consistent with the references [29]. This issue urges the need of studying factual consistency evaluation (FCE) in the RAG task.\nVarious FCE methods have been proposed to evaluate the factual consistency of specific RAG systems, among which a two-step approach shows promising results, especially for evaluating long answers [10, 31]. As shown in the bottom left of Figure 1, this approach first segments the answer into shorter pieces, then evaluates the factual consistency of each segment with respect to the given reference. In this way, the evaluation of a long answer is decomposed into evaluations on several simpler pieces of information, which improves the detection of factual inconsistency.\nIn previous works, these FCE methods are evaluated by answers generated by the underlying LLM in the specific RAG system being studied [13, 18]. Despite their effectiveness on the specific system, it is unclear how these methods generalize to new RAG systems. As discovered in a recent study [31], the optimal FCE method may vary when evaluating different LLMs, hence achieving a superior performance regarding some certain LLM does not guarantee a strong performance on other LLMs. In this sense, previous benchmarks generated by a single LLM are not fair enough to evaluate the overall performance of FCE methods.\nTo fill this gap, in this paper, we first construct a comprehensive benchmark to enable the evaluation of FCE methods independent of the underlying LLM. Specifically, we first propose a novel error typology to cover various factual consistency errors in RAG, which includes three main categories, i.e., hallucination error, knowledge error, and logical fallacy, and is further divided into nine error types. Based on our predefined error typology, we construct a synthetic dataset in Chinese to assess the effectiveness of FCE methods across the different types of errors. Furthermore, we construct a real-world dataset in Chinese by generating answers using six distinct LLMs within RAG tasks. Empirical analysis on the real-world dataset shows that 6.96% of all factual inconsistent samples involve logical fallacies. In addition, we observe that different LLMs exhibit diverse error distributions, which echoes previous research [31] and justifies our motivation of constructing a comprehensive benchmark independent of LLMs."}, {"title": "2 RELATED WORK", "content": "Traditional FCE Methods. Evaluating the factuality of model generated results is widely studied across various language model generation domains like text summarization [15], dialogue summary [47] and question-answering [4]. When the golden labels are given, prior methods using exact match metrics [8, 19, 27] or similarity-based metrics are proposed [7, 46]. However, high quality answers can vary a lot, hence these approaches using golden labels may significantly underestimate the models' performances, especially for long answers [42].\nFCE for Long-form Answers. To effectively evaluate factuality of long answers, recent FCE research mostly take a two step approaches [23, 26], where in the first step, the long-form answer is decomposed into shorter segments, such as sentences [24, 26], sub-claims [10, 23], individual facts [31] and structured triplets [18]. Then the second step evaluates the verifiability of each segment with respect to the given reference text [25, 26, 45], which can be efficiently done by modern general purpose LLM [9, 31], e.g., GPT-4. Although we follow the two-step approach, our method differs from them in the ability of leveraging logical connections via special designs of logic-preserving decomposition and fact-logic FCE.\nFCE Benchmarks. Prior benchmarks for FCE mostly focus on specialized tasks like summarization [13, 24, 39]. For FCE in RAG, existing benchmarks are derived from specific LLMs, such as Refchecker [18] and FELM [9], which are constrained by the error type distribution of the underlying LLMs. Unlike these benchmarks, we construct a synthetic dataset based on our error typology, which enables evaluation independent to any underlying LLM."}, {"title": "3 FACE4RAG BENCHMARK", "content": "Recall that existing FCE benchmarks only use answers generated by some certain LLMs, which may fail to evaluate FCE methods on other LLMs with different error distributions or unseen error types. To remedy this issue, in this section, we propose a novel approach to construct a FCE benchmark for RAG, which is independent of the underlying LLMs and called FActual Consistency Evaluation for RAG (Face4RAG). Face4RAG contains an error-type-oriented synthetic dataset and a real-world dataset. To construct the synthetic dataset, inspired by the error typology used in an exam designed for humans, i.e. the National College Entrance Examination of China, we first propose a novel error typology to classify any factual consistency error in RAG task, which includes nine types of errors belonging to three main categories. Based on the proposed error typology, we then construct a synthetic dataset to evaluate FCE methods on each type of the error. Besides the synthetic dataset, we also collect samples from six commonly used LLMs to construct a real-world benchmark, which aims to evaluate the overall factual consistency of FCE methods in real-world scenarios. The details about Face4RAG can be seen in Table 1 and Figure 2."}, {"title": "3.1 Error Typology in FCE", "content": "Our error typology for FCE is inspired by the questions in the National College Entrance Examination of China [1], which are carefully designed to test the ability of human to evaluate factual consistency. In this examination, reading comprehension is a major section to evaluate the participants' skill of understanding a Chinese text. Factual consistency evaluation is a typical task in this section. Given the text, the participants are required to evaluate the correctness of several answers to a specific question [1], which is essentially a RAG task (see examples in Table 11 in the appendix). As these questions are designed for a competitive entrance examination of higher education institutions at the undergraduate level, they are generally hard to answer and cover a wide range of factual inconsistency error types. Accordingly, we develop a novel error typology for RAG, which comprises three main categories and is further classified into nine error types. In the following, we give a detailed description of our proposed error typology.\nHallucination Error This class of error refers to the situation when the answer contains information that cannot be traced back to the reference [30]. Note that there are two main usages of the term hallucination in previous literature: one refers to \"unfaithful or nonsensical\" generated answers [21], the other further includes \"unverifiable\" answers using the given context [30, 40]. Here we adopt the second usage that has a larger scope.\n\u2022 Hallucination Error (Hallu.) refers to the situation when the answer is either unfaithful or unverifiable using the given context (even when it is factually correct).\nKnowledge Error This class of error refers to the situation when the information contained by the answer is inaccurate or incorrect regarding the reference [9]. This may occur in various components of a sentence, such as the subjects, predicates, objects, adverbials of time and place, etc. We classify the knowledge error into four error types:\n\u2022 Contradiction Error (KCont.) refers to the situation when the statement in the answer conflicts with information from the reference.\n\u2022 Entity Inversion Error (KInve.) refers to the situation when entities in the answer (events, processes, or concepts) are swapped in their positions as compared to the reference.\n\u2022 Conflation Error (KConf.) refers to the situation when the entities in the reference (subjects, predicates, or objects) are inaccurately combined, altering the original meaning."}, {"title": "3.2 Synthetic Dataset", "content": "Based on the above proposed error typology, we construct a synthetic dataset. In the dataset, the positive samples are factual consistent, whereas each negative sample has at least one factual inconsistency error. The dataset is constructed based on WebCPM [36], a web-enhanced question answering dataset in Chinese. Due to the space limit, in the following we briefly describe the process of dataset generation. Please refer to Appendix A for more details of the construction of our synthetic dataset.\nNegative Samples For each specific error type in the typology, we design a prompt to generate samples with this error. For the hallucination error, we setup three levels of difficulty for the evaluator to detect inconsistency and construct samples accordingly. For the remaining two categories, i.e., knowledge error and logical fallacy, we design a specific prompt for each error type except the Contradiction Error (KCont.). Since KCont. may occur at different levels of granularity [12], i.e., word or sentence, we design one prompt for each. Apart from the above error types, we construct a new error type called Other Logical Fallacy (LOthe.), which accounts for potential errors in some complex logical connections uncovered by our previously defined four types of logical fallacy.\nPositive Samples To enrich the sample diversity, we apply the augmentation technique in [28]. Specifically, the original positive samples in WebCPM are augmented by synonym replacing and paraphrasing via certain prompt at either word or sentence level.\nHuman Annotation Refinement To enhance the quality of the coarse labels derived above, we further engage 12 human experts to annotate the factual consistency of each answer via a two-step approach [31]."}, {"title": "3.3 Real-World Dataset", "content": "The synthetic dataset is generated based on various predefined error types without considering the distribution of these error types in the real world. Consequently, there is a need for another evaluation dataset that better aligns with the actual distribution of answers in real-world RAG scenarios, thus serving as a real-world dataset. In contrast to previous studies that relied solely on GPT-based LLMs for generating responses to create their evaluation sets [9, 31], we adopt a more comprehensive approach by utilizing six different LLMs to construct our real-world dataset.\nSpecifically, we first collect 200 questions along with corresponding references. We then prompt six commonly used LLMs to generate answers for the questions based on the references, including gpt-4-turbo (GPT-4) [2], gpt-3.5-turbo (GPT-3.5) [33], Baichuan2-13B-Chat (Baichuan2) [5], ChatGLM3 [44], Qwen-14B-Chat (Qwen) [3], and Chinese-Alpaca-2-13B-16k [11] (Alpaca2 (CH)). In this way, we derive a total of 1200 data points, which constitute the real-world dataset.\nFor each data point, we follow the same human annotation procedure as in our synthetic dataset to inspect if it is factually consistent. Moreover, if an answer is deemed factually inconsistent, the annotator will assign a specific error type from our proposed error typology to the answer. When the annotators notice that error of the answer does not fall into the aforementioned error types, they will mark the answer as \"Other Errors\".\nWe now conduct empirical analyses on the error typology and the behaviors of various LLMs on the above real-world dataset.\nOverall Error Type Distribution We first justify our study on logical fallacy consistency detection by empirically showing that the logical fallacy errors are prevailing in the answers generated by various LLMs. To this end, we analyze the distribution of the error types annotated across the entire real-world dataset. As shown in Figure 2, the hallucination error, knowledge error and logical fallacy account for 73.78%, 28.31%, 6.96% of all the inconsistent samples, respectively. It worth note that this 6.96% logical fallacy errors are not studied in the previous FCE methods. Besides, only 0.23% of the inconsistent samples are marked as \"Other Errors\" by annotators, which suggests the comprehensiveness and completeness of our proposed error typology.\nError Distribution of Various Models We then look deeper into the error types and their distributions among various LLMs in RAG. As presented in Figure 3, various LLMs exhibit distinct distributions on error types. For instance, LIncl. emerges in three of the LLMs, and LCaus. and LConf. occurs in four models. In addition, while Hallu. exists in all models, GPT-4 has a notably high"}, {"title": "4 LOGIC-ENHANCED FACTUAL CONSISTENCY EVALUATION", "content": "In the above statistic analysis, logical fallacy accounts for a considerable proportion of factual errors in real-world RAG scenarios. However, as we have analyzed before, existing FCE pipelines neglect the logical connections between segments in the original answer, which may result in wrong factual consistency evaluation result for samples with logical fallacy. Hence, to improve the evaluation ability of factuality consistency, a natural direction is to"}, {"title": "4.1 Logic-Preserving Answer Decomposition", "content": "Most existing studies directly decompose answers into segments, each containing only a single piece of information [23, 31]. In contrast, we propose to decompose the answers based on semantic linkages\u00b2 and logical connections, which preserves the logical relationships and facilitates logical consistency evaluation. The core design in this module is an elaborated prompt based on the following three principles for answer decomposition.\n\u2022 We prompt GPT-4 to execute the decomposition only when the two or multiple sentences do not exhibit strong semantic or logical connection.\n\u2022 To ensure that each segment can be understood by GPT-4 independently without leveraging other segments, any pronoun in a segment that refers to other contextual information should be substituted with appropriate reference.\n\u2022 During the decomposition process, GPT-4 is required to maintain the sentence structure of the original answer to the best extent. This principle alleviates the risk of introducing additional hallucination to the original answer.\nIn order to help GPT-4 better understand our principles for answer decomposition and deal with texts with various formats, we construct three kinds of instances to serve as the few-shot examples. The specific type of instances are as follows:\n\u00b2Semantic linkage refers to the connection or association between different pieces of information based on their meanings or semantic content [20].\n\u2022 Logical Connection refers to the instances having logical connections between the sentences and thus, GPT-4 needs to learn the solution of the logical connections during the decomposition process.\n\u2022 Pronoun Substitution involves replacing pronoun with their referent entities during the answer decomposition to make each answer segment understandable on its own, without reliance on other segments.\n\u2022 Unique Format refers to the instances with unique format and may be difficult for GPT-4 to decompose properly."}, {"title": "4.2 Fact-Logic FCE", "content": "Previous methods directly invoke an LLM to evaluate the decomposed segments and overlook the logical fallacy. To evaluate the logical fallacy, we develop a two-stage procedure for factual consistency evaluation, which consists of a conventional stage of fact consistency evaluation and an extra stage that evaluates from both perspectives of fact and logic; we introduce the COT mechanism [43] into both stages to improve LLM's ability of evaluation. The prompts for each stage are provided at our benchmark webpage.\u00b9\nFact Consistency Evaluation In this stage, GPT-4 is instructed to assess the consistency of each piece of information in the segment against the reference, which mainly concerns with the hallucination error and the knowledge error. Unlike previous methods that directly instruct the model to assess the consistency with the reference [9, 13], we use the COT technique to guide the model to evaluate the segment step-by-step, with the following steps:\n(1) Informational Points Extraction: GPT-4 extracts all informational points from the segment. This step ensures that each component of the segment will be evaluated.\n(2) Context Identification: For each informational point, GPT-4 locates the corresponding content within the reference."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of our proposed L-Face4RAG pipeline. Our experiments show that on both synthetic data and real-world data in Face4RAG benchmark, our L-Face4RAG method substantially outperforms the existing FCE methods. Notably, its superiority goes beyond the Chinese RAG task from which L-Face4RAG is originally motivated, as L-Face4RAG achieves SOTA results on 6 out of 7 of the existing English datasets and also a substantially higher average score on all tasks."}, {"title": "5.1 Experimental Setup", "content": "Baselines We compare L-Face4RAG with four GPT-based fine-grained FCE methods:\n\u2022 FACTSCORE [31] first breaks the answer into a series of atomic facts and then assigns a binary label to each atomic fact individually.\n\u2022 FELM [9] first segments the answer into fine-grained textual spans and then evaluates the factual consistency of all textual spans collectively. It outputs the corresponding numbers of factual inconsistent textual spans if existed.\n\u2022 Ragas [13] first extracts a set of statements from the answer and then evaluates the factual consistency of all statements collectively, outputting a binary label for each statement along with the corresponding reason for the assessment.\n\u2022 Refchecker [18] extracts knowledge triplets from the answer and evaluates each knowledge triplet separately.\nImplementation Details As the above FCE baselines are originally designed for tasks in English, we adapt them to our Chinese RAG task by translating their prompts into Chinese.\nWhen experimenting with FELM, we utilize the Reference-doc augmented evaluator [9], in alignment with our task which is focused on evaluating the factual consistency of answers against their references. Specifically, we input our references as the retrieved reference doc in FELM's evaluation framework. We select the best-performing estimator in [9], i.e., decomposing the answer with segment-based method and utilizing GPT-4 as the factual error detector.\nSince the original settings of FACTSCORE and RAGAS are based on GPT-3.5, we conduct experiments with both GPT-3.5 and GPT-4 to eschew the effect of the possible performance gap between GPT-3.5 and GPT-4 on the empirical results.\nFinally, to apply our proposed evaluation pipeline, we decompose the answer into segments and assess the factual consistency of each segment respectively. The outputs include both the label and the corresponding explanations. To derive deterministic output from GPT-4, we set its temperature to 0."}, {"title": "5.2 Performance Comparison on Face4RAG", "content": "We first compare the performance of our proposed L-Face4RAG pipeline against various FCE baselines on the Face4RAG benchmark, which includes a synthetic dataset and a real-world dataset."}, {"title": "5.3 Performance Comparison on Existing FCE Benchmark", "content": "We then evaluate the robustness and applicability of the proposed L-Face4RAG method on other factuality detection tasks, and in English. Specifically, we consider several commonly used FCE benchmarks in English on various tasks, including RAG[13, 18], summarization [14, 34], dialogue [16, 17] and fact verification [37].\nIn Table 5, we report the predictive accuracy of examined FCE methods on the above tasks. The results show that our proposed"}, {"title": "5.4 Ablation Study", "content": "We now verify the specific design choices of our proposed evaluation pipeline by ablation study on Face4RAG benchmark. Specifically, we examine the effectiveness of each designed module by comparing L-Face4RAG with the counterpart method without such a module. Due to space limit, here we only present the results on the synthetic dataset. Results on the real-world dataset are qualitatively similar and deferred to Appendix B.\nEvaluating the Answer Decomposition Module. (A.D.) Recall that our decomposition module preserves the logic connection within one segment, which may help better identify logical fallacy while reducing extra hallucination induced by decomposition. To verify this point, we conduct an ablation study by replacing our approach by a conventional decomposition method [13]. As presented in Table 6 and Table 7, we observe a severe decline of overall accuracy in the counterpart method, especially for negative samples related to logical fallacy. This phenomenon accords with our intuition that conventional answer decomposition method may"}, {"title": "6 CONCLUSION", "content": "In this work, we give a systematic study of factual consistency evaluation in RAG. Specifically, we first propose a comprehensive benchmark termed Face4RAG, which includes the synthetic dataset and the real-world dataset. In light of the possible failure of existing FCE methods in detecting logical fallacy in RAG, we then propose a novel FCE method termed L-Face4RAG. Compared to previous method, our method has two novel designs, i.e., logic-preserving decomposition and fact-logic FCE, which can better characterize the logical relations in different pieces of information in the sentence, leading to higher ability of logical fallacy evaluation. Extensive experiments on both the synthetic and real-world datasets verify the effectiveness of the L-Face4RAG method. Notably, the superiority of L-Face4RAG is consistent on a wide range of factuality detection benchmarks beyond the Chinese RAG task. Elaborated ablation studies also justify our core algorithm designs."}, {"title": "A CONSTRUCTION DETAILS ABOUT SYNTHETIC DATASET", "content": "Negative Samples The negative samples are constructed based on WebCPM [36], a web-enhanced question answering dataset"}, {"title": "B ABLATION STUDY RESULTS ON THE REAL-WORLD DATASET", "content": "Table 8 further validates the effectiveness of our approach, particularly highlighting its importance in practical scenarios where enhancing the recall of negative samples is crucial while preserving the discriminative ability of positive samples."}, {"title": "C STATISTIC DETAILS ABOUT REAL-WORLD DATASET", "content": "Table 9 shows the statistics of 200 model-generated answers in our real-world dataset from six LLMs. Table 10 shows the specific information about the error distribution about the six LLMs in the real-world dataset."}]}