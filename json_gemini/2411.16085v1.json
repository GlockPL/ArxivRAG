{"title": "Cautious Optimizers: Improving Training with One Line of Code", "authors": ["Kaizhao Liang", "Lizhang Chen", "Bo Liu", "Qiang Liu"], "abstract": "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47x.", "sections": [{"title": "1. Introduction", "content": "Optimization is an important and constantly evolving field in modern machine learning. Undoubtedly, Adam (Kingma, 2014) and AdamW (Loshchilov, 2017) are the most consequential optimizers proposed almost a decade ago. Since then, many efforts (Zhang et al., 2021; Loshchilov et al., 2017) have been made to discover better and faster optimizers beyond these two. However, until now, AdamW remains the dominant workhorse for applications, from pre-training Large Language Models (LLMs) (Touvron et al., 2023) to fine-tuning text to image diffusion (Rombach et al., 2022), with no real challenges to their ruling status.\nIn the dawn of the era of LLMs, the arms race of model scaling intensifies (Achiam et al., 2023). A faster optimizer means more training tokens can be consumed within the same amount of time. Ultimately, this leads to more capable models (Kaplan et al., 2020). Hence, the interest in searching for an optimizer beyond AdamW is re-kindled. Recent progress in new AdamW alternatives such as LION (Chen et al., 2024; 2023a), SHAMPOO (Gupta et al., 2018), and SOAP (Vyas et al., 2024), ADOPT (Taniguchi et al., 2024), and Schedule-Free (Defazio et al., 2024), all claim substantial improvement over AdamW."}, {"title": "2. Theory", "content": "We start with introducing a general Hamiltonian descent framework for the continuous-time forms of general momentum algorithms (Section 2.1). We then introduce the cautious optimizers in the continuous time form and discuss its theoretical properties (Section 2.2). Finally, we discuss in Section 2.3 theoretical properties of cautious optimizers in discrete time forms."}, {"title": "2.1. Hamiltonian+Descent", "content": "In the continuous-time form, most momentum-based algorithms can be viewed as a variant of the damped Hamiltonian system, which admits a Lyapunov (or Hamiltonian) function and their convergence towards the stationary points of the loss function. The Lyapunov function is an augmented loss function $H(w, s)$ on both the weights $w$ and the momentum states $S$, and should satisfy $min_s H(w, s) = L(w)$, so that minimizing $L(w)$ is equivalent to minimizing $H(w, s)$.\nThis is achieved by is\n$$H(w, s) = L(w) + K(s),$$\nwhere $K(.)$ is any lower-bounded function. Physically, we can think $H$ as the total energy of a system parameterized by $(w, s)$, and $L$ and $K$ as the potential energy and kinetic energy, respectively.\nThe continuous-time form of common momentum-based algorithms can be unified into\n$$\\frac{d}{dt}w_t = -\\nabla K(s_t) - \\Phi_t(\\nabla L(w_t))$$\n$$\\frac{d}{dt}s_t = \\nabla L(w_t) - \\Psi_t(\\nabla K(s_t)),$$"}, {"title": "2.2. Cautious Dynamics", "content": "Our idea is to change the dynamics to make it simultaneously decrease both $H(w, s)$ and $L(w)$. We do this with a modified system:\n$$x_t = \\nabla L(W_t) \\circ \\nabla K(s_t)$$\n$$\\frac{d}{dt}w_t = -\\phi(x_t) \\cdot \\nabla K(s_t) - \\Phi_t(\\nabla L(W_t))$$\nwhere $\\circ$ denotes the element-wise product and $\\phi$ is a vector to vector mapping. Here we weigh each element of the"}, {"title": "2.3. Discrete-Time Analysis", "content": "We provide analysis for the discrete time, showing that cautious optimizers can only be better than the original optimizers under mild conditions.\nWe will consider a generic update of form\n$$w_{k+1} = w_k - \\epsilon_k u_k(w_k, s_k),$$\n$$s_{k+1} = s_k + v_k(w_k, s_k),$$\nwhere $u_k, v_k$ are vector fields that define the updates. and $\\epsilon_k$ is the step size. and its cautious variant:\n$$\\bar{u}_k = u_k(w_k, s_k)$$\n$$w_{k+1} = w_k - \\epsilon_k \\bar{u}_k \\circ \\Phi_k$$\n$$s_{k+1} = s_k + v_k(w_k, s_k),$$\nwhere $\\Phi_k$ is a mask vector determined by the algorithm.\nOur analysis will consider both the element-wise mask $\\Phi_k = I(\\bar{u}_k g_k > 0)$, and inner product mask, $\\Phi_k = I(\\bar{u}_k^T g_k > 0)$, where $g_k = \\nabla L(w_k)$. The difference is that the inner product mask is scalar and masks the update vector as a whole, while element-wise mask treat each element separately."}, {"title": "3. Experiments", "content": "In this section, we evaluate the performance of cautious optimizers compared to their standard counterparts, highlighting the benefits introduced by the cautious masking mechanism. We begin with a 2D toy experiment to provide a visual demonstration of how cautious masking improves optimization. Subsequently, we extend the evaluation to large-scale pretraining tasks for both language and vision models, comparing the performance of standard optimizers and their cautious variants."}, {"title": "3.1. 2D Optimization Toy", "content": "We consider a 2D optimization problem, where the decision variable w = $(w^1,w^2) \\in R^2$. The objective is $f(w) = 0.5(w^1)^2 + 0.1(w^2)^2$. Apparently the optimum is at w* = (0,0). We apply gradient descent (GD), gradient descent with momentum (GDM), and cautious gradient descent with momentum (C-GDM) on this toy example, starting from wo = (1, 1). Specifically, for GDM, we adopt the conventional momentum update:\n$$s_t \\leftarrow \\beta s_{t-1} + \\nabla f(w_t),$$\n$$w_t \\leftarrow w_{t-1} - \\epsilon s_t,$$\nwhere $\\beta\\in [0, 1)$ is the dampening factor, and $\\epsilon$ is a constant learning rate. When using $\\beta = 0.99$ and $\\epsilon = 0.01$ for GDM and C-GDM, $\\epsilon = 0.1$ for GD, the results are visualized in Figure 2."}, {"title": "3.2. Pretraining Large Language Models (LLMs)", "content": "We begin by investigating the language modeling task using the LLAMA (Touvron et al., 2023) model as the foundational architecture. Variants of LLaMA with parameter sizes ranging from 60M to 1B (specifically 60M, 100M, 350M, and 1B) are considered in this study. The models are trained on the C4 (Colossal Clean Crawled Corpus) dataset (Raffel"}, {"title": "3.3. Pretraining Masked Autoencoders (MAEs)", "content": "Masked Autoencoders (MAEs) (He et al., 2022) have emerged as a powerful approach for pretraining Vision Transformers (ViTs)(Dosovitskiy, 2020) on large-scale datasets like ImageNet-1K (Russakovsky et al., 2015). This task involves reconstructing 75% of randomly masked image patches, a challenging objective that requires extensive training over hundreds of epochs and millions of images.\nThe primary goal is to learn robust visual representations that are generalizable across downstream vision tasks. The quality of these representations is typically measured by the final evaluation loss, which reflects how accurately the model reconstructs masked test images; lower evaluation loss indicates higher-quality representations. The results of our experiments are summarized in Figure 3, where we compare the performance of the Cautious Optimizer against the AdamW baseline."}, {"title": "4. Related Work", "content": "In this section, we provide a brief overview of existing efforts on designing Adam-like optimizers, and the related works on Hamiltonian dynamics."}, {"title": "5. Conclusion and Limitation", "content": "In summary, we introduce Cautious Optimizer, a straightforward enhancement for momentum-based optimizers that can be implemented with a single line of code. Our theoretical analysis demonstrates that the Cautious Optimizer not only preserves the convergence guarantees of the base optimizer but also accelerates the reduction of the loss function. Empirically, it delivers universal performance improvements, as evidenced by scaling Llama models from 60M to 1B parameters and achieving up to 1.47 \u00d7 faster pretraining of MAE on ImageNet1K.\nFor future research, we provide a few promising directions: (1) Different $\\phi$ functions (2) Applying masking in eigenspace instead of parameter space (3) More rigorous analysis beyond convex cases. We hope that our work provides a strong foundation for exploring these directions.\nLimitations: Our evaluation of this method remains preliminary due to limited computational resources. Despite early promising results, it remains uncertain whether Cautious Optimizers will deliver the anticipated improvements in broader applications and large-scale experiments."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Hamiltonian + Descent", "content": "Momentum-based algorithms can be typically viewed as monotonic descending algorithms on an augmented loss $H (W, S)$, which satisfies $min_S H(W, S) = L(W)$, so that minimizing $L(W)$ is equivalent to minimizing $H(W, S)$. A typical choice is\n$$H(w, s) = L(w) + K(s),$$\nwhere $K(.)$ is any lower bounded function. We may refer $H(w, s)$ as a Hamiltonian function. Physically, one can show $L(w)$ and $K(s)$ the potential energy and kinetic energy, respectively.\nThe continuous-time form of most momentum-based algorithms can be written into a Hamiltonian descent form:\n$$\\frac{d}{dt}W_t = -\\nabla K(S) - \\Phi_t(\\nabla L(W))$$\n$$\\frac{d}{dt}S_t = \\nabla L(W) - \\Psi_t(\\nabla K(S_t)),$$\nwhere $H(W, S)$ is a Hamiltonian (or Lyapunov) function that satisfies\n$$min_S H(W, S) = L(W),\\forall W,$$\nso that minimizing $L(W)$ reduces to minimizing $H(W, S)$; and $\\Phi(\\cdot), \\Psi(\\cdot)$ are two monotonic mappings satisfying\n$$||X||_{\\Phi}^2 := (X, \\Phi_t(X)) \\geq 0, ||X||_{\\Psi}^2 := (X, \\Psi_t(X)) \\geq 0,$$ $\\forall X$.\nWith $\\Phi(X) = \\Psi(X) = 0$, the system in (9) reduces to the standard Hamiltonian system that keeps $H(W_t, S_t) = const$ along the trajectory. When adding the descending components with $\\Phi$ and $\\Psi$, the system then keeps $H(W, S)$ monotonically non-decreasing.\nIt is easy to show that the dynamics monotonically decreases $\\frac{d}{dt}H(W_t, S_t) \\leq 0$ since\n$$\\frac{d}{dt}H(W, S_t) = \\nabla L(W)^T\\frac{d}{dt}W_t +\\nabla K(S)^T\\frac{d}{dt}S_t$$\n$$\\nabla L(W)^T (-\u2207K(S) - \\Phi_t(\u2207L(W_t))) + \u2207K(S)^T (\u2207L(W) - \\Psi_t(\u2207K(S_t)))$$\n$$= -\u2207L(W)^T \\Phi_t(\u2207L(W_t)) - \u2207K(S)^T\\Psi_t(\u2207K(S_t))$$\n$$= - ||X||_{\\Phi}^2 - ||X||_{\\Psi}^2,$$\nso that minimizing $L(W)$ reduces to minimizing $H(W, S)$. However, $L(W)$, which is the true objective, is not necessarily decreasing monotonically.\nBecause $H = L + K$, this means that there are cases when $L$ increases while $K$ decreases.\nHow to change the dynamics to make it simultaneously decreases both $H(W, S)$ and $L(W)$?\nTo do so, we modify introduce a modification of the system:\n$$\\frac{d}{dt}W_t = -\u2207K(S)\u25e6\\phi(\u2207L(W)\u2218\u2207K(S)) - \\Phi_t(\u2207L(W))$$\n$$\\frac{d}{dt}S_t = \u2207L(W) - \\Psi_t(\u2207K(S)),$$\nwhere $o$ denotes elementwise product. Above update (10) is exactly our C-optimizer's continous form. Here we introduce a weighting $\\phi(\u2207L(W)\u2218\u2207K(S))$ on the update direction of $W_t$ based on the product of $\u2207L(W)$ and $\u2207K(S)$.\nThe following conditions on the choice of function $\\phi$ ensures that the system simultaneously decreases both $H$ and $L$ simultaneously.\nTheorem A.1. The following inequalities hold under specific conditions on $x$ and $\\phi(x)$:\n\u2022 If $x \u00b7 (1 \u2212 \\phi(x)) \\leq 0$, then $\\frac{d}{dt}H(W_t, S_t) \\leq 0$.\n\u2022 If $x \u00b7 \\phi(x) \\geq 0$, then $\\frac{d}{dt}L(W_t) \\leq 0$."}, {"title": "A.2. Examples", "content": ""}, {"title": "A.2.1. ADAM", "content": "$$\\frac{dW}{dt} = \\frac{M_t}{\\sqrt{V_t + \\epsilon}}$$\n$$\\frac{d}{dt}M_t = \\beta_1 (\\nabla L(W) - M_t)$$\n$$\\frac{d}{dt}V = \\beta_2 (\\nabla L(W)^{\\odot 2} - V_t)$$\nwith\n$$H(W, M, V) = L(W) + \\frac{1}{2\\beta_1} <\\frac{M}{\\sqrt{V + \\epsilon}}, \\frac{M}{\\sqrt{V + \\epsilon}} >$$"}, {"title": "A.2.2. CAUTIOUS ADAM", "content": "$$\\frac{dW}{dt} = \\frac{1(sign(\\nabla L(W)) = sign(M)) \\odot M_t}{\\sqrt{V_t + \\epsilon}}$$\n$$\\frac{d}{dt}M_t = \\beta_1 (\\nabla L(W) - M_t)$$\n$$\\frac{d}{dt}V = \\beta_2 (\\nabla L(W)^{\\odot 2} - V_t).$$\nIt is easy to show that the loss function $L(W_t)$ itself is a Hamiltonian (Lyapunov) since\n$$\\frac{d}{dt}L(W) = -\\nabla L(W)\\frac{dW}{dt} = - \\nabla L(W)\\frac{ (1(sign(\\nabla L(W)) = sign(M)) \\odot M_t}{\\sqrt{V_t + \\epsilon}} \\le 0.$$"}, {"title": "A.3. From Continuous Time to Discrete time", "content": "This subsection should be after the continuous analysis of Hamiltonian, we can show the decreasing of the loss of c-optimizer is simply larger or equal to baseline optimizer at each iteration from a discrete time perspective.\nFirst, by discretizing dynamical system (9), we have\n$$W_{t+1} := W_t + \\varepsilon U_t = W_t - \\varepsilon (\\nabla K(S_t) \\circ \\phi(\\nabla L(W_t) \\circ \\nabla K(S_t)) + \\Phi_t(\\nabla L(W_t)))$$\n$$S_{t+1} := S_t + \\varepsilon (\\nabla L(W_t) - \\Psi_t(\\nabla K(S_t))).\nBy discretizing dynamical system (10), we have\n$$W_{t+1} := W_t + \\varepsilon V_t = W_t - \\varepsilon (\\nabla K(S_t) + \\Phi_t(\\nabla L(W_t)))$$\n$$S_{t+1} := S_t + \\varepsilon (\\nabla L(W_t) - \\Psi_t(\\nabla K(S_t)))$$\nTheorem A.7. [Larger Loss Decreasing] Assume loss function $L(\\cdot)$ is differentiable and $L$-smooth, and element-wise operator $\\phi$ satisfies $x \\cdot (1 - \\phi(x)) < 0$ and $x \\cdot \\phi(x) \\ge 0$.\nFor the discretized update,\n$$W_{t+1} = W_t + \\varepsilon U_t$$\n$$W_{t+1} = W_t + \\varepsilon V_t,$$\nwe have\n$$L(W_+ + \\varepsilon U_t) \\le L(W_t + \\varepsilon V_t), i.e.\\ L(W_{t+1}) \\le L(W_{t+1}),$$\nwhere $\\varepsilon < \\frac{2||\\nabla L(W) \\circ \\nabla K(S_t)||^2+2||\\nabla L(W)||^2}{||R_t||^2 (2L-||V_t ||+||R_t||)}$ and $R_t = U_t - V_t$"}, {"title": "A.4. Pesudo Code", "content": ""}]}