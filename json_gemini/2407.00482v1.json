[{"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "authors": ["Barproda Halder", "Faisal Hamman", "Pasan Dissanayake", "Qiuyi Zhang", "Ilia Sucholutsky", "Sanghamitra Dutta"], "abstract": "Spurious patterns refer to a mathematical associ-ation between two or more variables in a dataset that are not causally related. However, this notion of spuriousness, which is usually introduced due to sampling biases in the dataset, has classically lacked a formal definition. To address this gap, this work presents the first information-theoretic formalization of spuriousness in a dataset (given a split of spurious and core features) using a mathematical framework called Partial Information Decomposition (PID). Specifically, we disentangle the joint information content that the spurious and core features share about another target variable (e.g., the prediction label) into distinct components, namely unique, redundant, and synergistic information. We propose the use of unique information, with roots in Blackwell Sufficiency, as a novel metric to formally quantify dataset spuriousness and derive its desirable properties. We empirically demonstrate how higher unique information in the spurious features in a dataset could lead a model into choosing the spurious features over the core features for inference, often having low worst-group-accuracy. We also propose a novel autoencoder-based estimator for computing unique information that is able to handle high-dimensional image data. Finally, we also show how this unique information in the spurious feature is reduced across several dataset-based spurious-pattern-mitigation techniques such as data reweighting and varying levels of background mixing, demonstrating a novel tradeoff between unique information (spuriousness) and worst-group-accuracy.", "sections": [{"title": "1. Introduction", "content": "Spurious patterns (Haig, 2003) arise when two or more variables are correlated in a dataset even though they do not have any causal relationship. For example, in the Water-bird dataset (Wah et al., 2011), most waterbirds have water backgrounds, and landbirds have land backgrounds (see Fig. 1). This correlation in the dataset essentially misleads a machine learning model into creating a spurious link between background and bird type, since it often finds the background to be \u201cmore informative\u201d than the foreground for predicting the bird type. Learning such spurious links from the data may result in high performance on the training and in-distribution datasets, but results in reduced per-formance on out-of-distribution datasets and affects worst-group-accuracy (Lynch et al., 2023; Sagawa et al., 2019), i.e., the accuracy on the minority groups like waterbirds with land background or vice versa.\nSeveral existing works (Kirichenko et al., 2022; Izmailov et al., 2022; Wu et al., 2023; Ye et al., 2023; Liu et al., 2023) focus on different dataset-based and model-training-based approaches to mitigate spurious patterns and evaluate the empirical performance over out-of-distribution datasets (or, to improve worst-group-accuracy). However, this notion of spuriousness in any given dataset lacks a formal definition. This work addresses this gap by asking the question: Given a split between core and spurious features, how do we formally quantify the spuriousness in any given dataset?\nTo answer this question, we present an information-theoretic formalization of spurious patterns, by leveraging a body of work in information theory called Partial Information Decomposition (PID) (Bertschinger et al., 2014; Banerjee et al., 2018). We note that classical information-theoretic measures such as mutual information (Cover & Thomas, 2012) captures the entire statistical dependency between two random variables but fail to capture how this dependency is distributed among those variables, i.e., the structure of the multivariate information. Partial Information Decomposition (PID) addresses this nuanced issue by providing a formal way of disentangling the joint information content between the core and spurious features into non-negative terms, namely, unique, redundant, or synergistic information (see (1) in Section 2.1).\nOur proposition is to use the unique information about the target variable $Y$ in the spurious features $B$ that is not in the core features $F$ as a measure of spuriousness in the dataset (often denoted as $Uni(Y:B|F)$). To justify our proposition, we discuss how unique information is connected to Blackwell Sufficiency (Blackwell, 1953), a notable concept in statistical decision theory. Blackwell Sufficiency provides a partial ordering on when one random variable can be more \"informative\" (less noisy) than another for inference. Unique information captures the departure from Blackwell Sufficiency, which goes to zero if and only if one random variable is Blackwell Sufficient over another for a prediction task (see Theorem 1). Thus, unique information intuitively quantifies when one variable can be more informative than another, which we leverage to explain when the spurious feature $B$ can be more informative than the core feature $F$ for the model prediction. Additionally, we also show several desirable properties of unique information as a measure of spuriousness in the dataset in Theorem 2. Though Partial Information Decomposition (PID) has recently been applied to few other areas in machine learning (Tax et al., 2017; Dutta et al., 2020; 2021; Hamman & Dutta, 2024a; Liang et al., 2023; Dutta & Hamman, 2023) (also see Related Works), we are pioneering its use to decompose information in spurious and core features and quantify spuriousness, supported by desirable properties and empirical validation. Our main contributions can be concisely listed as follows:\n\u2022 Novel information-theoretic formalization to explain spurious patterns: Though many works attempt to prevent a model from learning spurious patterns, there is a lack of a theoretical understanding of the \"amount\" of spuriousness in a dataset, and how do we quantify and measure it given a split of spurious and core features. Novel to this work, we investigate spuriousness through the lens of partial information decomposition (PID) and provide a fundamental understanding of when a model finds the spurious features to be \u201cmore informative\" than the core features. We leverage PID to disentangle the joint information content between the core and spurious features into unique, redundant, and synergistic information.\n\u2022 Demystifying unique information as a measure of spuriousness: Next, we propose unique information in the spurious features $Uni(Y:B|F)$ as a measure of the spuriousness in a dataset. To justify our proposition, we first establish how unique information $Uni(Y:B|F)$ quantifies the informativeness of a random variable $B$ compared to $F$ for predicting $Y$ (see Theorem 1 for a motivation from Blackwell Sufficiency). Depending on the increasing or decreasing nature of the unique information $Uni(Y:B|F)$, one can then anticipate to what extent is a model going to leverage $B$ over $F$ for prediction. Additionally, we also show several desirable properties of unique information $Uni(Y:B|F)$ as a measure of spuriousness in Theorem 2. Our measure can identify which features are more likely to be predictive for a classification task, paving a pathway for dataset quality assessment and explaining feature-based informativeness.\n\u2022 Spuriousness Disentangler: An autoencoder-based estimator for computing unique information: We propose a novel autoencoder-based framework that we call Spuriousness Disentangler \u2013 to compute the PID values for high dimensional image data. The estimator consists of mainly three main parts: (i) First, an autoencoder reduces the dimension of the image data and gives an one-dimensional array of clusters which serves as a lower-dimensional, discrete feature representation for the image data. Along the lines of (Guo et al., 2017), the dimensionality reduction and clustering are efficiently performed through minimization of a joint loss function; (ii) Next, the computation of the joint probability distribution of this lower-dimensional representation is performed; and (iii) Finally, the partial information decomposition (PID) values are calculated by solving a convex optimization problem using the Discrete Information Theory (DIT) package (James et al., 2018).\n\u2022 Experimental Results and Novel Tradeoff: Our experimental results are in agreement with our theoretical postulations, demonstrating an empirical tradeoff between our proposed measure of spuriousness, i.e., $Uni(Y:B|F)$ and empirical evaluation metrics known to be affected by spurious patterns, i.e., worst group accuracy. We show that for real-world unbalanced datasets, e.g., the Waterbirds dataset (Wah et al., 2011), the unique information in the spurious feature $Uni(Y:B|F)$ is the most prominent and is significantly higher than any information in the core features. This helps explain why a model trained on this dataset readily uses the spurious feature"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "rather than the core feature for prediction. Additionally, when a dataset-based spurious-correlation-mitigation method such as data-reweighting is applied, the unique information in the spurious features $Uni(Y:B|F)$ reduces drastically (again explaining why a model might now be more likely to use the core feature $F$). We also observe a novel tradeoff between unique information $Uni(Y:B|F)$ (proposed measure of spuriousness) and worst-group-accuracy for varying degrees of background mixing (a form of noise), i.e., the worst-group-accuracy improves with the decreasing unique information in the spurious features pointing to a novel tradeoff. We also study Grad-CAM (Selvaraju et al., 2017) (a technique to generate 'visual explanations' for decisions made by Convolutional Neural Network (CNN)-based models) visualizations for many of the trained models to further confirm when the core or spurious feature is actually being emphasized by the model for different experimental setups.\nRelated Works: There are several perspectives on spurious correlation (see Haig (2003); Kirichenko et al. (2022); Iz-mailov et al. (2022); Wu et al. (2023); Ye et al. (2023); Liu et al. (2023); Stromberg et al. (2024); Singla & Feizi (2021); Moayeri et al. (2023) and the references therein; also see surveys (Ye et al., 2024; Srivastava, 2023; Ghouse et al., 2024)). Spuriousness mitigation techniques are broadly divided into two groups: (i) Dataset-based techniques (Kirichenko et al., 2022; Wu et al., 2023) and (ii) Learning-based techniques (Liu et al., 2023; Yang et al., 2023; Ye et al., 2023). Kirichenko et al. (2022) shows that last-layer fine-tuning of a pre-trained model with a group-balanced subset of data is sufficient to mitigate spurious correlation. Wu et al. (2023) proposes a concept-aware spurious correlation mitigation technique. Ye et al. (2023) introduces a Freeze and Train approach to learn salient features in an unsupervised way and freezes them before training the rest of the features via supervised learning. Yang et al. (2023) explores different regularization techniques to see the effect on the spurious correlation and Liu et al. (2023) examines a logit correction loss. Our novelty lies in formalizing the spuriousness of datasets using the PID framework, and explaining how effective a dataset-based spurious-correlation mitigation will be for regular model training.\nPartial information decomposition (PID) (Williams & Beer, 2010; Bertschinger et al., 2014; Dutta et al., 2021; Venkatesh & Schamberg, 2022) is an active area of research. PID measures are beginning to be used in different domains of neuroscience and machine learning (Tax et al., 2017; Dutta et al., 2020; 2021; Hamman & Dutta, 2024a; Ehrlich et al., 2022; Liang et al., 2024; Wollstadt et al., 2023; Mohamadi et al., 2023; Venkatesh et al., 2024; Hamman & Dutta, 2024b). However, examining spurious correlation through the lens of PID and observing novel empirical tradeoffs between the spurious pattern and worst-group-accuracy is unexplored."}, {"title": "2. Preliminaries and Background", "content": "Let $X = (X_1, X_2, ..., X_a)$ be the random variable denoting the input (e.g., an image) where each $X_i \\in \\mathcal{X}$ which denotes a finite set of values. The core features (e.g., the foreground) will be denoted by $F \\subset X$, and the spurious features (e.g., the background) will be denoted by $B = X\\setminus F$. We typically use the notation $B$ and $F$ to denote the range of values for the spurious and core features. Let $Y$ denote the target random variable, e.g., the true labels which lie in the set $\\mathcal{Y}$, and the model predictions are given by $\\hat{Y} = m_{\\theta}(X)$ (parameterized by $\\theta$). Generally, we use the notation $P_A$ to denote the distribution of random variable A, and $P_{A|B}$ to denote the conditional distribution of random variable A conditioned on B. Depending on the context, we also use more than one random variable as sub-script, e.g., $P_{A,B,Y}$ denotes the joint distribution of $(A, B, Y)$. Whenever necessary, we also use the notation $Q_A$ to denote an alternate distribution on the random variable A that is different from $P_A$. We also use the notation $P_{A|B} \\circ P_{B|C}$ to denote a composition of two conditional distributions given by: $P_{A|B} \\circ P_{B|C}(a|c) = \\sum_{b \\in B} P_{A|B}(a|b)P_{B|C}(b|c) \\forall a \\in A, c \\in C$, where $A, B$ and $C$ denote the range of values that can be taken by random variables $A, B$, and $C$.\n2.1. Background on Partial Information Decomposition\nWe provide a brief background on PID that would be relevant for the rest of the paper. The classical information-theoretic quantification of the total information that two random variables A and B together hold about Y is given by the mutual information $I(Y; A, B)$ (see (Cover & Thomas, 2012) for a background on mutual information). Mu-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "tual information $I(Y; A, B)$ is defined as the KL divergence (Cover & Thomas, 2012) between the joint distribution $P_{Y, A, B}$ and the product of the marginal distributions $P_Y P_{A, B}$ and would go to zero if and only if $(A, B)$ is independent of Y. Intuitively, this mutual information captures the total predictive power about Y that is present jointly in $(A, B)$ together, i.e., how well can one learn Y from $(A, B)$ together. However, $I(Y; A, B)$ only captures the total information content about Y jointly in $(A, B)$ and does not unravel anything about what is unique and what is shared between A and B.\nPID (Bertschinger et al., 2014; Banerjee et al., 2018) provides a mathematical framework that decomposes the total information content $I(Y; A, B)$ into four nonnegative terms (also see Fig. 2):\n$I(Y; A, B) = Uni(Y:B|A) + Uni(Y:A|B) + Red(Y:A, B) + Syn(Y:A, B)$. (1)\nHere, $Uni(Y:B|A)$ denotes the unique information about Y that is only in B but not in A. Next, $Red(Y:A, B)$ denotes redundant information (common knowledge) about Y in both A and B. Lastly, $Syn(Y:A, B)$ is an interesting term that denotes the synergistic information that is present only jointly in A, B but not in any one of them individually, e.g., a public and private key can jointly reveal information not in any one of them alone.\nMotivational Example. Let $Z=(Z_1, Z_2, Z_3)$ with each $Z_i \\stackrel{i.i.d.}{\\sim} Bern(1/2)$. Let $A = (Z_1, Z_2, Z_3+ N), B = (Z_2, N)$, and $N \\sim Bern(1/2)$ which is independent of $Z$. Here, $I(Z; A, B) = 3$ bits. The unique information about Z that is contained only in A and not in B is effectively in $Z_1$, and is given by $Uni(Z:A|B) = I(Z; Z_1) = 1$ bit. The redundant information about $Z$ that is contained"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "in both A and B is effectively in $Z_2$ and is given by $Red(Z:A, B) = I(Z; Z_2) = 1$ bit. Lastly, the synergistic information about Z that is not contained in either A or B alone, but is contained in both of them together is effectively in the tuple $(Z_3 \\oplus N, N)$, and is given by $Syn(Z:A, B)=I(Z; (Z_3 \\oplus N, N)) = 1$ bit. This accounts for the 3 bits in $I(Z; A, B)$.\nWe also note that defining any one of the PID terms suffices for obtaining the others. This is because of another relationship among the PID terms as follows (Bertschinger et al., 2014): $I(Y; A) = Uni(Y:A|B) + Red(Y:A, B)$. Essentially $Red(Y:A, B)$ is viewed as the sub-volume between $I(Y; A)$ and $I(Y; B)$ (see Fig. 2). Hence, $Red(Y:A, B) = I(Y; A) - Uni(Y:A|B)$. Lastly, $Syn (Y:A, B) = I(Y; A, B) - Uni(Y:A|B) - Uni(Y:B|A) - Red(Y:A, B)$ (can be obtained from (1) once both unique and redundant information has been defined). Here, we include a popular definition of $Uni(Y:A|B)$ from (Bertschinger et al., 2014) which is computable using convex optimization.\nDefinition 1 (Unique Information (Bertschinger et al., 2014)). Let $\\triangle$ be the set of all joint distributions on $(Y, A, B)$ and $\\triangle_P$ be the set of joint distributions with the same marginals on $(Y, A)$ and $(Y, B)$ as the true distribution $P_{Y, A, B}$, i.e., $\\triangle_P = {Q_{YAB} \\in \\triangle: Q_{YA} = P_{YA} and Q_{YB} = P_{YB}}$. Then,\n$Uni(Y:A|B) = \\min_{Q \\in \\triangle_P} I_Q(Y; A|B)$.\nHere $I_Q (Y; A|B)$ denotes the conditional mutual information when $(Y, A, B)$ have joint distribution $Q_{YAB}$ instead of $P_{Y AB}$."}, {"title": "3. Main Results", "content": "In this work, we first present an information-theoretic formalization of spurious patterns using the mathematical framework of Partial Information Decomposition (PID).\nProposition 1 (Unique Information as a Measure of Spuriousness). For a given data distribution, the unique information $Uni(Y:B|F)$ is a measure of spuriousness given a split of the spurious features $B$ and core features $F$.\nTo justify our proposition, we first establish that unique information is a measure of informativeness of the spurious feature B over core feature F. We draw upon a concept in statistical decision theory called Blackwell Sufficiency (Blackwell, 1953) which investigates when a random"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "variable is \"more informative\u201d (or \"less noisy\") than an-other for inference (also relates to stochastic degradation of channels (Venkatesh et al., 2023; Raginsky, 2011)). Let us first discuss this notion intuitively when trying to infer Y using two random variables F and B. Suppose, there exists a transformation on F to give a new random variable $B'$ which is always equivalent to B for predicting Y. We note that $B'$ and $B$ do not necessarily have to be the same since we only care about inferring Y. In fact, B and $B'$ can have additional irrelevant information that do not pertain to Y, but solely for the purpose of inferring Y, they need to be equivalent. Then, feature set F will be regarded as \"sufficient\" with respect to B for predicting Y since F can itself provide all the information that B has about Y (see Fig. 3). This intuition is formalized as:\nDefinition 2 (Blackwell Sufficiency (Blackwell, 1953)). A conditional distribution $P_{F|Y}$ is Blackwell sufficient with respect to another conditional distribution $P_{B|Y}$ if and only if there exists a stochastic transformation (equivalently another conditional distribution $P_{B'|F}$ with both B and $B' \\in B$) such that $P_{B'|F} \\circ P_{F|Y} = P_{B|Y}$.\nNow we demonstrate how our proposed unique information is closely tethered to Blackwell Sufficiency, thus justifying our Proposition 1. In fact, the unique information $Uni(Y:B|F)$ is 0 if and only if $P_{F|Y}$ is Blackwell sufficient with respect to $P_{B|Y}$ (see Theorem 1).\nTheorem 1 (Spuriousness and Blackwell Sufficiency). The $Uni(Y:B|F) = 0$ if and only if the conditional distribution $P_{F|Y}$ is Blackwell sufficient with respect to $P_{B|Y}$.\nSince spuriousness (unique information) $Uni(Y:B|F) = 0$ if and only if $P_{F|Y}$ is Blackwell Sufficient with respect to $P_{B|Y}$, we note that $Uni(Y:B|F) > 0$ captures the \u201cdeparture\u201d from Blackwell Sufficiency, and thus quantifies relative informativeness. Intuitively, what this means is that for the given data distribution, there is no such transformation on core feature F that is equivalent to the spurious feature B for the purpose of predicting Y. This essentially makes spurious feature B indispensable to the model for predicting Y, forcing the model to use or emphasize it in decision-making.\nNext, we discuss some desirable properties of unique information $Uni(Y:B|F)$.\nTheorem 2. The measure $Uni(Y:B|F)$ satisfies the following desirable properties:\n\u2022 $Uni(Y:B|F) \\leq I(Y; B)$ and is 0 if $I(Y; B) = 0$ (spurious feature B has no information about Y).\n\u2022 $Uni(Y:B|F)$ is non-decreasing if more features are added to B, i.e., if the set of spurious features grows, so does its unique information over core features.\n\u2022 $Uni(Y:B|F)$ is non-increasing if more features are added"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "to F, i.e., if the set of core features grow, the unique information in the spurious features reduce.\nSpuriousness Disentangler (Autoencoder-based estima-tor): Next, we propose an autoencoder-based estimation framework - that we call Spuriousness Disentangler \u2013 to calculate the PID values. The motivation to use this estimator is that since the model learns the features to reconstruct the input image, the encoding of the image should have minimal information loss and hence should be a good low-dimensional representation of the input image. The framework mainly consists of three aspects: clustering, estimation of joint distribution and estimation of PID.\nSince we are dealing with high dimensional data, dimensionality reduction is a necessary first step (Bellman, 1966). Traditionally, the clustering step is done by PCA followed by k-means clustering. However, in our setting, we can do these two steps together using an autoencoder, which is a deep neural network consisting of an encoder and a decoder, as shown in Fig. 4. The output of the encoder is the embedding for the input image, a low dimensional representation of the input images. The weights of this output layer, defined as the clustering layer, are used as the clusters centers initialized by k-means clustering algorithm. The clustering layer is optimized using the weighted sum of representation loss $L_r$ and clustering loss $L_c$. The overall loss function is defined as $L = L_r+ \\gamma L_c$ where $\\gamma$ is a non-negative constant. The clustering loss $L_c$ is the KL divergence which measures the dissimilarity between different distributions (Xie et al., 2016; Guo et al., 2017). For cluster centers {$\\mu_j$}$_K$ and embedded point $z_i$ (output of the encoder), $q_{ij}$ is defined as follows (Van der Maaten & Hinton, 2008):\n$q_{ij} = \\frac{(1 + || z_i - \\mu_j ||^2)^{-1}}{\\Sigma_j (1 + ||z_i \u2013 \\mu_j ||^2)^{-1}}$ (2)\nwhere $q_{ij}$ is the jth entry of the soft label $q_i$, denoting the probability of $z_i$ belonging to cluster $\\mu_j$. The loss $L_c = KL(P||Q) = \\Sigma_{ij} P_{ij}\\log\\frac{P_{ij}}{q_{ij}}$ and $P_{ij} = \\frac{q_{ij}^2}{\\Sigma_i(\\Sigma_j q_{ij})^2}$ where P is the target distribution.\nThe representation loss is the mean square error between the input of the encoder $x$ and output of the decoder $x'$ defined as $L_r = ||x - x' ||^2$.\nThe next step is to estimate the PID values. For this, the joint distribution of three random variables (e.g. the clusters of foreground, background and the binary label) is calculated using histograms, and then the PID values are obtained from the DIT package (James et al., 2018)."}, {"title": "4. Experiments", "content": "We demonstrate experimental results to provide evidence in support of Proposition 1 for different experimental setups, i.e., unbalanced, balanced, and mixed background datasets. We illustrate how unique information in the spurious features has a tradeoff with the worst-group-accuracy, thus justifying its use as a measure of the spuriousness of a dataset. We also show a comparative analysis for PCA-based and autoencoder-based PID measurements.\nDatasets: We conduct experiments on two datasets: Water-bird (Wah et al., 2011) and Dominoes (Shah et al., 2020), both framed as binary classification tasks.\nWaterbird dataset (Wah et al., 2011) is the popular spurious correlation benchmark. The task is to classify the type of the birds (waterbird = 1, landbird = 0). However, there exists spurious correlation between the backgrounds (water = 1, land = 0) and the labels (bird type). Group00, Group01, Group10, and Group11 indicate the group of images where landbirds are in the land backgrounds, landbirds are in the water backgrounds, waterbirds are in the land backgrounds and waterbirds are in the water backgrounds respectively. We call the bird as the foreground of the image.\nDominoes is a synthetic dataset created by combining handwritten digits (zero and one) from MNIST (Deng, 2012) and images of cars and trucks from CIFAR10 (Krizhevsky et al., 2009) (digit 0 or 1 at the top, car (= 0) or truck (= 1) at the bottom of an image). We make two version of this synthetic dataset namely Dominoes 1.0 and Dominoes 2.0 inducing different degrees of bias. The task is to classify whether the image contains a car or truck hence the car or truck corresponds to the core features (foreground). On the other hand, the digits are considered as the spurious features (background). Group00, Group01,Group10, and Group11 illustrate the group of images where the top half is a zero"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "and bottom half is a car, the top half is a one and bottom half is a car, the top half is a zero and bottom half is a truck, the top half is a one and bottom half is a truck, respectively.\n4.1. Comparison between group-balanced and\nunbalanced datasets\nWe observe the relationship between the PID values and worst-group-accuracy for (i) an unbalanced dataset (which has spurious correlations) and (ii) a balanced dataset where the spurious correlation with the background is removed through sampling (balancing).\nProblem Setup: We use group-balanced and unbalanced data for this part of the experiment. The balanced-unbalanced scenario arises from the four different groups that are present in the dataset, where the majority groups consist of the waterbirds with water backgrounds and landbirds with land backgrounds and other two combinations are the minority groups for the waterbird dataset. Similarly, in the Dominoes dataset, cars with digit 0 and trucks with digit 1 are the majority groups and the other two combinations are the minority groups. Worst-group-accuracy refers to the accuracy for the minority group which is generally the lowest for the model that is trained with biased dataset namely unbalanced dataset. The group-balanced dataset has equal number of samples in each group resulting in unbiased model training. We begin with using our autoencoder-based estimator, namely Spuriousness Disentangler, on both dataset and estimate the PID values separately for the background and foreground. This separation is done by using the segmentation mask of the foreground for the waterbird dataset. Next, we fine-tune the pre-trained ResNet-50 (He et al., 2016) model and calculate the worst-group-accuracy and mean accuracy over all groups.\nObservations: Fig. 5 shows our findings regarding PID val-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "ues and the worst-ground accuracy for three datasets. Firstly, we can observe that the unique information in background is generally much higher than the other PID values namely unique information in foreground, redundancy and synergy. Secondly, from the first two columns, it is obvious that there is a significant reduction of the unique information in background i.e., reduction in spuriousness when the dataset is balanced (having equal number of samples in all groups reducing the bias in dataset) and all other PID values are now in the same order. Next, from the last column of Fig. 5, we find out that the worst-group-accuracies are lower for the unbalanced case and these values significantly improve when the datasets become balanced which implies low spu-\n4.2. Tradeoffs for varying levels of background mixing\nNext, we look into the datasets for varying levels of background mixing to observe the tradeoffs between the spuriousness and the worst-group-accuracy."}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "Problem Setup: Starting with the dataset creation, we add two backgrounds at different levels. We consider two cases: (i) half of a land background is concatenated with half of a water background (named as concatenation); and (ii) the whole image of a land background is summed with a water background (named as addition). Similar techniques are applied for background mixing for Dominoes dataset (see Fig. 7). Then, the foreground is superimposed on the background. Next, the PID values are calculated for the mixed background and the foreground using our estimator. We train the pre-trained ResNet-50 (He et al., 2016) with the mixed background with foreground (the whole image) and evaluate the model with the normal test dataset (without any modification). One motivation of mixing the backgrounds is to remove the group bias that is generated due to the cor-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "relation between the background and the label in the dataset, that should help mitigate the spurious correlation since the background is no longer different for different groups.\nObservations: Firstly, in Fig. 8 we can observe that unique information in the background is prominent in the unbalanced case and it decreases for both addition and concatenation scenarios which indicates spuriousness reduction while using addition and concatenation datasets. Next, we observe a trend in Fig. 9 between the unique information in background i.e., spuriousness and the worst-group-accuracy: with increasing unique information in background i.e., spuriousness, the worst-group-accuracy decreases. This trend is obtained for unbalanced, addition and concatenation datasets (lowest W.G. Acc. for unbalanced and highest W.G. Acc. for concatenation)."}, {"title": "5. Conclusion", "content": "Quantifying and explaining spuriousness of a dataset can provide an efficient way to assess dataset quality rather than training a model for hours. In this work, we theoretically quantify spuriousness in a dataset with unique information, leveraging the mathematical tool of Partial information decomposition (PID). We demonstrate (with empirical validation) that unique information in the background can measure spuriousness and relate it to the worst-group-accuracy for various spurious correlation mitigation techniques. We also propose a novel autoencoder-based estimator for high-dimensional continuous image data, showing its superiority over classical estimators. However, there are some limitations: firstly to estimate the unique information, at first one has to identify the spurious features and core features of a given dataset which is not always straightforward. Moreover, the estimation is highly data-dependent. A small change in the dataset can greatly affect the PID values. Nonetheless, formally quantifying spuriousness can lead to more effective bias mitigation strategies."}, {"title": "A. Proof of Theorem 1", "content": "As a proof sketch", "Uni(Y": "B|F) = 0$ if and only if there exists a row-stochastic matrix $T \\in [0", "that": "P_{YB"}, "Y = y, B = b) = \\sum_{f \\in F} P_{YF}(Y = y, F = f)T(f,b)$ for all $y \\in Y$ and $b \\in B$.\nProof. If $Uni(Y:B|F) = 0$, then we have: $\\min_{Q \\in \\triangle_P} I_Q(Y; B|F) = 0$ where $\\triangle_P = {Q \\in \\triangle : Q_{YF}(Y = y, F = f) = P_{YF}(Y = y, F = f) and Q_{YB}(Y = y, B = b) = P_{YB}(Y = y, B = b)}$. Thus, there exists a distribution $Q \\in \\triangle_P$ such that Y and B are independent given F under the joint distribution Q. Then, we have\n$\\begin{aligned}\nP_{YB}(Y = y, B = b) &= Q_{```json\n{\n  \"title\": \"Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition\",\n  \"authors\": [\n    \"Barproda Halder\",\n    \"Faisal Hamman\",\n    \"Pasan Dissanayake\",\n    \"Qiuyi Zhang\",\n    \"Ilia Sucholutsky\",\n    \"Sanghamitra Dutta\"\n  ],\n  \"abstract\":", "Spurious patterns refer to a mathematical associ-ation between two or more variables in a dataset that are not causally related. However, this notion of spuriousness, which is usually introduced due to sampling biases in the dataset, has classically lacked a formal definition. To address this gap, this work presents the first information-theoretic formalization of spuriousness in a dataset (given a split of spurious and core features) using a mathematical framework called Partial Information Decomposition (PID). Specifically, we disentangle the joint information content that the spurious and core features share about another target variable (e.g., the prediction label) into distinct components, namely unique, redundant, and synergistic information. We propose the use of unique information, with roots in Blackwell Sufficiency, as a novel metric to formally quantify dataset spuriousness and derive its desirable properties. We empirically demonstrate how higher unique information in the spurious features in a dataset could lead a model into choosing the spurious features over the core features for inference, often having low worst-group-accuracy. We also propose a novel autoencoder-based estimator for computing unique information that is able to handle high-dimensional image data. Finally, we also show how this unique information in the spurious feature is reduced across several dataset-based spurious-pattern-mitigation techniques such as data reweighting and varying levels of background mixing, demonstrating a novel tradeoff between unique information (spuriousness) and worst-group-accuracy.", "sections\": [\n    {\n      \"title\": \"1. Introduction", "content\": \"Spurious patterns (Haig, 2003) arise when two or more variables are correlated in a dataset even though they do not have any causal relationship. For example, in the Water-bird dataset (Wah et al., 2011), most waterbirds have water backgrounds, and landbirds have land backgrounds (see Fig. 1). This correlation in the dataset essentially misleads a machine learning model into creating a spurious link between background and bird type, since it often finds the background to be \u201cmore informative\u201d than the foreground for predicting the bird type. Learning such spurious links from the data may result in high performance on the training and in-distribution datasets, but results in reduced per-formance on out-of-distribution datasets and affects worst-group-accuracy (Lynch et al., 2023; Sagawa et al., 2019), i.e., the accuracy on the minority groups like waterbirds with land background or vice versa.\nSeveral existing works (Kirichenko et al., 2022; Izmailov et al., 2022; Wu et al., 2023; Ye et al., 2023; Liu et al., 2023) focus on different dataset-based and model-training-based approaches to mitigate spurious patterns and evaluate the empirical performance over out-of-distribution datasets (or, to improve worst-group-accuracy). However, this notion of spuriousness in any given dataset lacks a formal definition. This work addresses this gap by asking the question: Given a split between core and spurious features, how do we formally quantify the spuriousness in any given dataset?\nTo answer this question, we present an information-theoretic formalization of spurious patterns, by leveraging a body of work in information theory called Partial Information Decomposition (PID) (Bertschinger et al., 2014; Banerjee et al., 2018). We note that classical information-theoretic measures such as mutual information (Cover & Thomas, 2012) captures the entire statistical dependency between two random variables but fail to capture how this dependency is distributed among those variables, i.e., the structure of the multivariate information. Partial Information Decomposition (PID) addresses this nuanced issue by providing a formal way of disentangling the joint information content between the core and spurious features into non-negative terms, namely, unique, redundant, or synergistic information (see (1) in Section 2.1).\nOur proposition is to use the unique information about the target variable $Y$ in the spurious features $B$ that is not in the core features $F$ as a measure of spuriousness in the dataset (often denoted as $Uni(Y:B|F)$). To justify our proposition, we discuss how unique information is connected to Blackwell Sufficiency (Blackwell, 1953), a notable concept in statistical decision theory. Blackwell Sufficiency provides a partial ordering on when one random variable can be more \"informative\" (less noisy) than another for inference. Unique information captures the departure from Blackwell Sufficiency, which goes to zero if and only if one random variable is Blackwell Sufficient over another for a prediction task (see Theorem 1). Thus, unique information intuitively quantifies when one variable can be more informative than another, which we leverage to explain when the spurious feature $B$ can be more informative than the core feature $F$ for the model prediction. Additionally, we also show several desirable properties of unique information as a measure of spuriousness in the dataset in Theorem 2. Though Partial Information Decomposition (PID) has recently been applied to few other areas in machine learning (Tax et al., 2017; Dutta et al., 2020; 2021; Hamman & Dutta, 2024a; Liang et al., 2023; Dutta & Hamman, 2023) (also see Related Works), we are pioneering its use to decompose information in spurious and core features and quantify spuriousness, supported by desirable properties and empirical validation. Our main contributions can be concisely listed as follows:\n\u2022 Novel information-theoretic formalization to explain spurious patterns: Though many works attempt to prevent a model from learning spurious patterns, there is a lack of a theoretical understanding of the \"amount\" of spuriousness in a dataset, and how do we quantify and measure it given a split of spurious and core features. Novel to this work, we investigate spuriousness through the lens of partial information decomposition (PID) and provide a fundamental understanding of when a model finds the spurious features to be \u201cmore informative\" than the core features. We leverage PID to disentangle the joint information content between the core and spurious features into unique, redundant, and synergistic information.\n\u2022 Demystifying unique information as a measure of spuriousness: Next, we propose unique information in the spurious features $Uni(Y:B|F)$ as a measure of the spuriousness in a dataset. To justify our proposition, we first establish how unique information $Uni(Y:B|F)$ quantifies the informativeness of a random variable $B$ compared to $F$ for predicting $Y$ (see Theorem 1 for a motivation from Blackwell Sufficiency). Depending on the increasing or decreasing nature of the unique information $Uni(Y:B|F)$, one can then anticipate to what extent is a model going to leverage $B$ over $F$ for prediction. Additionally, we also show several desirable properties of unique information $Uni(Y:B|F)$ as a measure of spuriousness in Theorem 2. Our measure can identify which features are more likely to be predictive for a classification task, paving a pathway for dataset quality assessment and explaining feature-based informativeness.\n\u2022 Spuriousness Disentangler: An autoencoder-based estimator for computing unique information: We propose a novel autoencoder-based framework that we call Spuriousness Disentangler \u2013 to compute the PID values for high dimensional image data. The estimator consists of mainly three main parts: (i) First, an autoencoder reduces the dimension of the image data and gives an one-dimensional array of clusters which serves as a lower-dimensional, discrete feature representation for the image data. Along the lines of (Guo et al., 2017), the dimensionality reduction and clustering are efficiently performed through minimization of a joint loss function; (ii) Next, the computation of the joint probability distribution of this lower-dimensional representation is performed; and (iii) Finally, the partial information decomposition (PID) values are calculated by solving a convex optimization problem using the Discrete Information Theory (DIT) package (James et al., 2018).\n\u2022 Experimental Results and Novel Tradeoff: Our experimental results are in agreement with our theoretical postulations, demonstrating an empirical tradeoff between our proposed measure of spuriousness, i.e., $Uni(Y:B|F)$ and empirical evaluation metrics known to be affected by spurious patterns, i.e., worst group accuracy. We show that for real-world unbalanced datasets, e.g., the Waterbirds dataset (Wah et al., 2011), the unique information in the spurious feature $Uni(Y:B|F)$ is the most prominent and is significantly higher than any information in the core features. This helps explain why a model trained on this dataset readily uses the spurious feature"], "content": "rather than the core feature for prediction. Additionally, when a dataset-based spurious-correlation-mitigation method such as data-reweighting is applied, the unique information in the spurious features $Uni(Y:B|F)$ reduces drastically (again explaining why a model might now be more likely to use the core feature $F$). We also observe a novel tradeoff between unique information $Uni(Y:B|F)$ (proposed measure of spuriousness) and worst-group-accuracy for varying degrees of background mixing (a form of noise), i.e., the worst-group-accuracy improves with the decreasing unique information in the spurious features pointing to a novel tradeoff. We also study Grad-CAM (Selvaraju et al., 2017) (a technique to generate 'visual explanations' for decisions made by Convolutional Neural Network (CNN)-based models) visualizations for many of the trained models to further confirm when the core or spurious feature is actually being emphasized by the model for different experimental setups.\nRelated Works: There are several perspectives on spurious correlation (see Haig (2003); Kirichenko et al., 2022; Iz-mailov et al., 2022; Wu et al., 2023; Ye et al., 2023; Liu et al., 2023; Stromberg et al., 2024; Singla & Feizi (2021); Moayeri et al. (2023) and the references therein; also see surveys (Ye et al., 2024; Srivastava, 2023; Ghouse et al., 2024)). Spuriousness mitigation techniques are broadly divided into two groups: (i) Dataset-based techniques (Kirichenko et al., 2022; Wu et al., 2023) and (ii) Learning-based techniques (Liu et al., 2023; Yang et al., 2023; Ye et al., 2023). Kirichenko et al. (2022) shows that last-layer fine-tuning of a pre-trained model with a group-balanced subset of data is sufficient to mitigate spurious correlation. Wu et al. (2023) proposes a concept-aware spurious correlation mitigation technique. Ye et al. (2023) introduces a Freeze and Train approach to learn salient features in an unsupervised way and freezes them before training the rest of the features via supervised learning. Yang et al. (2023) explores different regularization techniques to see the effect on the spurious correlation and Liu et al. (2023) examines a logit correction loss. Our novelty lies in formalizing the spuriousness of datasets using the PID framework, and explaining how effective a dataset-based spurious-correlation mitigation will be for regular model training.\nPartial information decomposition (PID) (Williams & Beer, 2010; Bertschinger et al., 2014; Dutta et al., 2021; Venkatesh & Schamberg, 2022) is an active area of research. PID measures are beginning to be used in different domains of neuroscience and machine learning (Tax et al., 2017; Dutta et al., 2020; 2021; Hamman & Dutta, 2024a; Ehrlich et al., 2022; Liang et al., 2024; Wollstadt et al., 2023; Mohamadi et al., 2023; Venkatesh et al., 2024; Hamman & Dutta, 2024b). However, examining spurious correlation through the lens of PID and observing novel empirical tradeoffs between the spurious pattern and worst-group-accuracy is unexplored."}, {"title": "2. Preliminaries and Background", "content": "Let $X = (X_1, X_2, ..., X_a)$ be the random variable denoting the input (e.g., an image) where each $X_i \\in \\mathcal{X}$ which denotes a finite set of values. The core features (e.g., the foreground) will be denoted by $F \\subset X$, and the spurious features (e.g., the background) will be denoted by $B = X\\setminus F$. We typically use the notation $B$ and $F$ to denote the range of values for the spurious and core features. Let $Y$ denote the target random variable, e.g., the true labels which lie in the set $\\mathcal{Y}$, and the model predictions are given by $\\hat{Y} = m_{\\theta}(X)$ (parameterized by $\\theta$). Generally, we use the notation $P_A$ to denote the distribution of random variable A, and $P_{A|B}$ to denote the conditional distribution of random variable A conditioned on B. Depending on the context, we also use more than one random variable as sub-script, e.g., $P_{A,B,Y}$ denotes the joint distribution of $(A, B, Y)$. Whenever necessary, we also use the notation $Q_A$ to denote an alternate distribution on the random variable A that is different from $P_A$. We also use the notation $P_{A|B} \\circ P_{B|C}$ to denote a composition of two conditional distributions given by: $P_{A|B} \\circ P_{B|C}(a|c) = \\sum_{b \\in B} P_{A|B}(a|b)P_{B|C}(b|c) \\forall a \\in A, c \\in C$, where $A, B$ and $C$ denote the range of values that can be taken by random variables $A, B$, and $C$.\n2.1. Background on Partial Information Decomposition\nWe provide a brief background on PID that would be relevant for the rest of the paper. The classical information-theoretic quantification of the total information that two random variables A and B together hold about Y is given by the mutual information $I(Y; A, B)$ (see (Cover & Thomas, 2012) for a background on mutual information). Mu-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "tual information $I(Y; A, B)$ is defined as the KL divergence (Cover & Thomas, 2012) between the joint distribution $P_{Y, A, B}$ and the product of the marginal distributions $P_Y P_{A, B}$ and would go to zero if and only if $(A, B)$ is independent of Y. Intuitively, this mutual information captures the total predictive power about Y that is present jointly in $(A, B)$ together, i.e., how well can one learn Y from $(A, B)$ together. However, $I(Y; A, B)$ only captures the total information content about Y jointly in $(A, B)$ and does not unravel anything about what is unique and what is shared between A and B.\nPID (Bertschinger et al., 2014; Banerjee et al., 2018) provides a mathematical framework that decomposes the total information content $I(Y; A, B)$ into four nonnegative terms (also see Fig. 2):\n$I(Y; A, B) = Uni(Y:B|A) + Uni(Y:A|B) + Red(Y:A, B) + Syn(Y:A, B)$. (1)\nHere, $Uni(Y:B|A)$ denotes the unique information about Y that is only in B but not in A. Next, $Red(Y:A, B)$ denotes redundant information (common knowledge) about Y in both A and B. Lastly, $Syn(Y:A, B)$ is an interesting term that denotes the synergistic information that is present only jointly in A, B but not in any one of them individually, e.g., a public and private key can jointly reveal information not in any one of them alone.\nMotivational Example. Let $Z=(Z_1, Z_2, Z_3)$ with each $Z_i \\stackrel{i.i.d.}{\\sim} Bern(1/2)$. Let $A = (Z_1, Z_2, Z_3+ N), B = (Z_2, N)$, and $N \\sim Bern(1/2)$ which is independent of $Z$. Here, $I(Z; A, B) = 3$ bits. The unique information about Z that is contained only in A and not in B is effectively in $Z_1$, and is given by $Uni(Z:A|B) = I(Z; Z_1) = 1$ bit. The redundant information about $Z$ that is contained"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "in both A and B is effectively in $Z_2$ and is given by $Red(Z:A, B) = I(Z; Z_2) = 1$ bit. Lastly, the synergistic information about Z that is not contained in either A or B alone, but is contained in both of them together is effectively in the tuple $(Z_3 \\oplus N, N)$, and is given by $Syn(Z:A, B)=I(Z; (Z_3 \\oplus N, N)) = 1$ bit. This accounts for the 3 bits in $I(Z; A, B)$.\nWe also note that defining any one of the PID terms suffices for obtaining the others. This is because of another relationship among the PID terms as follows (Bertschinger et al., 2014): $I(Y; A) = Uni(Y:A|B) + Red(Y:A, B)$. Essentially $Red(Y:A, B)$ is viewed as the sub-volume between $I(Y; A)$ and $I(Y; B)$ (see Fig. 2). Hence, $Red(Y:A, B) = I(Y; A) - Uni(Y:A|B)$. Lastly, $Syn (Y:A, B) = I(Y; A, B) - Uni(Y:A|B) - Uni(Y:B|A) - Red(Y:A, B)$ (can be obtained from (1) once both unique and redundant information has been defined). Here, we include a popular definition of $Uni(Y:A|B)$ from (Bertschinger et al., 2014) which is computable using convex optimization.\nDefinition 1 (Unique Information (Bertschinger et al., 2014)). Let $\\triangle$ be the set of all joint distributions on $(Y, A, B)$ and $\\triangle_P$ be the set of joint distributions with the same marginals on $(Y, A)$ and $(Y, B)$ as the true distribution $P_{Y, A, B}$, i.e., $\\triangle_P = {Q_{YAB} \\in \\triangle: Q_{YA} = P_{YA} and Q_{YB} = P_{YB}}$. Then,\n$Uni(Y:A|B) = \\min_{Q \\in \\triangle_P} I_Q(Y; A|B)$.\nHere $I_Q (Y; A|B)$ denotes the conditional mutual information when $(Y, A, B)$ have joint distribution $Q_{YAB}$ instead of $P_{Y AB}$."}, {"title": "3. Main Results", "content": "In this work, we first present an information-theoretic formalization of spurious patterns using the mathematical framework of Partial Information Decomposition (PID).\nProposition 1 (Unique Information as a Measure of Spuriousness). For a given data distribution, the unique information $Uni(Y:B|F)$ is a measure of spuriousness given a split of the spurious features $B$ and core features $F$.\nTo justify our proposition, we first establish that unique information is a measure of informativeness of the spurious feature B over core feature F. We draw upon a concept in statistical decision theory called Blackwell Sufficiency (Blackwell, 1953) which investigates when a random"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "variable is \"more informative\u201d (or \"less noisy\") than an-other for inference (also relates to stochastic degradation of channels (Venkatesh et al., 2023; Raginsky, 2011)). Let us first discuss this notion intuitively when trying to infer Y using two random variables F and B. Suppose, there exists a transformation on F to give a new random variable $B'$ which is always equivalent to B for predicting Y. We note that $B'$ and $B$ do not necessarily have to be the same since we only care about inferring Y. In fact, B and $B'$ can have additional irrelevant information that do not pertain to Y, but solely for the purpose of inferring Y, they need to be equivalent. Then, feature set F will be regarded as \"sufficient\" with respect to B for predicting Y since F can itself provide all the information that B has about Y (see Fig. 3). This intuition is formalized as:\nDefinition 2 (Blackwell Sufficiency (Blackwell, 1953)). A conditional distribution $P_{F|Y}$ is Blackwell sufficient with respect to another conditional distribution $P_{B|Y}$ if and only if there exists a stochastic transformation (equivalently another conditional distribution $P_{B'|F}$ with both B and $B' \\in B$) such that $P_{B'|F} \\circ P_{F|Y} = P_{B|Y}$.\nNow we demonstrate how our proposed unique information is closely tethered to Blackwell Sufficiency, thus justifying our Proposition 1. In fact, the unique information $Uni(Y:B|F)$ is 0 if and only if $P_{F|Y}$ is Blackwell sufficient with respect to $P_{B|Y}$ (see Theorem 1).\nTheorem 1 (Spuriousness and Blackwell Sufficiency). The $Uni(Y:B|F) = 0$ if and only if the conditional distribution $P_{F|Y}$ is Blackwell sufficient with respect to $P_{B|Y}$.\nSince spuriousness (unique information) $Uni(Y:B|F) = 0$ if and only if $P_{F|Y}$ is Blackwell Sufficient with respect to $P_{B|Y}$, we note that $Uni(Y:B|F) > 0$ captures the \u201cdeparture\u201d from Blackwell Sufficiency, and thus quantifies relative informativeness. Intuitively, what this means is that for the given data distribution, there is no such transformation on core feature F that is equivalent to the spurious feature B for the purpose of predicting Y. This essentially makes spurious feature B indispensable to the model for predicting Y, forcing the model to use or emphasize it in decision-making.\nNext, we discuss some desirable properties of unique information $Uni(Y:B|F)$.\nTheorem 2. The measure $Uni(Y:B|F)$ satisfies the following desirable properties:\n\u2022 $Uni(Y:B|F) \\leq I(Y; B)$ and is 0 if $I(Y; B) = 0$ (spurious feature B has no information about Y).\n\u2022 $Uni(Y:B|F)$ is non-decreasing if more features are added to B, i.e., if the set of spurious features grows, so does its unique information over core features.\n\u2022 $Uni(Y:B|F)$ is non-increasing if more features are added"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "to F, i.e., if the set of core features grow, the unique information in the spurious features reduce.\nSpuriousness Disentangler (Autoencoder-based estima-tor): Next, we propose an autoencoder-based estimation framework - that we call Spuriousness Disentangler \u2013 to calculate the PID values. The motivation to use this estimator is that since the model learns the features to reconstruct the input image, the encoding of the image should have minimal information loss and hence should be a good low-dimensional representation of the input image. The framework mainly consists of three aspects: clustering, estimation of joint distribution and estimation of PID.\nSince we are dealing with high dimensional data, dimensionality reduction is a necessary first step (Bellman, 1966). Traditionally, the clustering step is done by PCA followed by k-means clustering. However, in our setting, we can do these two steps together using an autoencoder, which is a deep neural network consisting of an encoder and a decoder, as shown in Fig. 4. The output of the encoder is the embedding for the input image, a low dimensional representation of the input images. The weights of this output layer, defined as the clustering layer, are used as the clusters centers initialized by k-means clustering algorithm. The clustering layer is optimized using the weighted sum of representation loss $L_r$ and clustering loss $L_c$. The overall loss function is defined as $L = L_r+ \\gamma L_c$ where $\\gamma$ is a non-negative constant. The clustering loss $L_c$ is the KL divergence which measures the dissimilarity between different distributions (Xie et al., 2016; Guo et al., 2017). For cluster centers {$\\mu_j$}$_K$ and embedded point $z_i$ (output of the encoder), $q_{ij}$ is defined as follows (Van der Maaten & Hinton, 2008):\n$q_{ij} = \\frac{(1 + || z_i - \\mu_j ||^2)^{-1}}{\\Sigma_j (1 + ||z_i \u2013 \\mu_j ||^2)^{-1}}$ (2)\nwhere $q_{ij}$ is the jth entry of the soft label $q_i$, denoting the probability of $z_i$ belonging to cluster $\\mu_j$. The loss $L_c = KL(P||Q) = \\Sigma_{ij} P_{ij}\\log\\frac{P_{ij}}{q_{ij}}$ and $P_{ij} = \\frac{q_{ij}^2}{\\Sigma_i(\\Sigma_j q_{ij})^2}$ where P is the target distribution.\nThe representation loss is the mean square error between the input of the encoder $x$ and output of the decoder $x'$ defined as $L_r = ||x - x' ||^2$.\nThe next step is to estimate the PID values. For this, the joint distribution of three random variables (e.g. the clusters of foreground, background and the binary label) is calculated using histograms, and then the PID values are obtained from the DIT package (James et al., 2018)."}, {"title": "4. Experiments", "content": "We demonstrate experimental results to provide evidence in support of Proposition 1 for different experimental setups, i.e., unbalanced, balanced, and mixed background datasets. We illustrate how unique information in the spurious features has a tradeoff with the worst-group-accuracy, thus justifying its use as a measure of the spuriousness of a dataset. We also show a comparative analysis for PCA-based and autoencoder-based PID measurements.\nDatasets: We conduct experiments on two datasets: Water-bird (Wah et al., 2011) and Dominoes (Shah et al., 2020), both framed as binary classification tasks.\nWaterbird dataset (Wah et al., 2011) is the popular spurious correlation benchmark. The task is to classify the type of the birds (waterbird = 1, landbird = 0). However, there exists spurious correlation between the backgrounds (water = 1, land = 0) and the labels (bird type). Group00, Group01, Group10, and Group11 indicate the group of images where landbirds are in the land backgrounds, landbirds are in the water backgrounds, waterbirds are in the land backgrounds and waterbirds are in the water backgrounds respectively. We call the bird as the foreground of the image.\nDominoes is a synthetic dataset created by combining handwritten digits (zero and one) from MNIST (Deng, 2012) and images of cars and trucks from CIFAR10 (Krizhevsky et al., 2009) (digit 0 or 1 at the top, car (= 0) or truck (= 1) at the bottom of an image). We make two version of this synthetic dataset namely Dominoes 1.0 and Dominoes 2.0 inducing different degrees of bias. The task is to classify whether the image contains a car or truck hence the car or truck corresponds to the core features (foreground). On the other hand, the digits are considered as the spurious features (background). Group00, Group01,Group10, and Group11 illustrate the group of images where the top half is a zero"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "and bottom half is a car, the top half is a one and bottom half is a car, the top half is a zero and bottom half is a truck, the top half is a one and bottom half is a truck, respectively.\n4.1. Comparison between group-balanced and\nunbalanced datasets\nWe observe the relationship between the PID values and worst-group-accuracy for (i) an unbalanced dataset (which has spurious correlations) and (ii) a balanced dataset where the spurious correlation with the background is removed through sampling (balancing).\nProblem Setup: We use group-balanced and unbalanced data for this part of the experiment. The balanced-unbalanced scenario arises from the four different groups that are present in the dataset, where the majority groups consist of the waterbirds with water backgrounds and landbirds with land backgrounds and other two combinations are the minority groups for the waterbird dataset. Similarly, in the Dominoes dataset, cars with digit 0 and trucks with digit 1 are the majority groups and the other two combinations are the minority groups. Worst-group-accuracy refers to the accuracy for the minority group which is generally the lowest for the model that is trained with biased dataset namely unbalanced dataset. The group-balanced dataset has equal number of samples in each group resulting in unbiased model training. We begin with using our autoencoder-based estimator, namely Spuriousness Disentangler, on both dataset and estimate the PID values separately for the background and foreground. This separation is done by using the segmentation mask of the foreground for the waterbird dataset. Next, we fine-tune the pre-trained ResNet-50 (He et al., 2016) model and calculate the worst-group-accuracy and mean accuracy over all groups.\nObservations: Fig. 5 shows our findings regarding PID val-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "ues and the worst-ground accuracy for three datasets. Firstly, we can observe that the unique information in background is generally much higher than the other PID values namely unique information in foreground, redundancy and synergy. Secondly, from the first two columns, it is obvious that there is a significant reduction of the unique information in background i.e., reduction in spuriousness when the dataset is balanced (having equal number of samples in all groups reducing the bias in dataset) and all other PID values are now in the same order. Next, from the last column of Fig. 5, we find out that the worst-group-accuracies are lower for the unbalanced case and these values significantly improve when the datasets become balanced which implies low spu-\n4.2. Tradeoffs for varying levels of background mixing\nNext, we look into the datasets for varying levels of background mixing to observe the tradeoffs between the spuriousness and the worst-group-accuracy."}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "Problem Setup: Starting with the dataset creation, we add two backgrounds at different levels. We consider two cases: (i) half of a land background is concatenated with half of a water background (named as concatenation); and (ii) the whole image of a land background is summed with a water background (named as addition). Similar techniques are applied for background mixing for Dominoes dataset (see Fig. 7). Then, the foreground is superimposed on the background. Next, the PID values are calculated for the mixed background and the foreground using our estimator. We train the pre-trained ResNet-50 (He et al., 2016) with the mixed background with foreground (the whole image) and evaluate the model with the normal test dataset (without any modification). One motivation of mixing the backgrounds is to remove the group bias that is generated due to the cor-"}, {"title": "Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition", "content": "relation between the background and the label in the dataset, that should help mitigate the spurious correlation since the background is no longer different for different groups.\nObservations: Firstly, in Fig. 8 we can observe that unique information in the background is prominent in the unbalanced case and it decreases for both addition and concatenation scenarios which indicates spuriousness reduction while using addition and concatenation datasets. Next, we observe a trend in Fig. 9 between the unique information in background i.e., spuriousness and the worst-group-accuracy: with increasing unique information in background i.e., spuriousness, the worst-group-accuracy decreases. This trend is obtained for unbalanced, addition and concatenation datasets (lowest W.G. Acc. for unbalanced and highest W.G. Acc. for concatenation)."}, {"title": "5. Conclusion", "content": "Quantifying and explaining spuriousness of a dataset can provide an efficient way to assess dataset quality rather than training a model for hours. In this work, we theoretically quantify spuriousness in a dataset with unique information, leveraging the mathematical tool of Partial information decomposition (PID). We demonstrate (with empirical validation) that unique information in the background can measure spuriousness and relate it to the worst-group-accuracy for various spurious correlation mitigation techniques. We also propose a novel autoencoder-based estimator for high-dimensional continuous image data, showing its superiority over classical estimators. However, there are some limitations: firstly to estimate the unique information, at first one has to identify the spurious features and core features of a given dataset which is not always straightforward. Moreover, the estimation is highly data-dependent. A small change in the dataset can greatly affect the PID values. Nonetheless, formally quantifying spuriousness can lead to more effective bias mitigation strategies."}, {"title": "A. Proof of Theorem 1", "content": "As a proof sketch", "Uni(Y": "B|F) = 0$ if and only if there exists a row-stochastic matrix $T \\in [0", "that": "P_{YB"}, {"Uni(Y": "B|F) = 0$", "have": "min_{Q \\in \\triangle_P"}, {"triangle": "Q_{YF}(Y = y, F = f) = P_{YF}(Y = y, F = f) and Q_{YB}(Y = y, B = b) = P_{YB}(Y = y, B = b)}$. Thus, there exists a distribution $Q \\in \\triangle_P$ such that Y and B are independent given F under the joint distribution Q. Then, we have\n$\\begin{aligned}\nP_{YB}(Y = y, B = b) &= Q_{"}]