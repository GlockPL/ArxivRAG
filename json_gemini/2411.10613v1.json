{"title": "Being Considerate as a Pathway\nTowards Pluralistic Alignment for Agentic AI", "authors": ["Parand A. Alamdari", "Toryn Q. Klassen", "Rodrigo Toro Icarte", "Sheila A. McIlraith"], "abstract": "Pluralistic alignment is concerned with ensuring that an AI system's objectives and\nbehaviors are in harmony with the diversity of human values and perspectives. In\nthis paper we study the notion of pluralistic alignment in the context of agentic\nAI, and in particular in the context of an agent that is trying to learn a policy in a\nmanner that is mindful of the values and perspective of others in the environment.\nTo this end, we show how being considerate of the future wellbeing and agency of\nother (human) agents can promote a form of pluralistic alignment.", "sections": [{"title": "1 Introduction", "content": "Pluralistic alignment is concerned with ensuring that an AI system's objectives and behaviors are in\nharmony with the diversity of human values, goals, and intentions (Sorensen et al., 2024). Here we\nreflect on pluralistic alignment as it relates specifically to so-called Agentic AI\u2014-AI systems that can\npursue complex goals with limited direct supervision (e.g., Shavit et al., 2023).\nWe posit that pluralistic alignment of an agentic AI does not require it to conform to others' prefer-\nences, nor does it require conformity with social norms or conventions, but rather that:\nA pluralistically well-aligned agent, through its actions, should attempt to realize its goals\nin a manner that contributes directly or indirectly towards maximizing the social welfare of\nthe collective, and when the welfare of other agents is realized via the achievement of their\ngoals, should try to ensure the agency of other agents in service of the social welfare of the\ncollective.\nIn some cases pluralistically aligned agentic behavior will mimic the preferences of others, and will\nsimilarly follow social norms and conventions, but these should perhaps be considered useful tools\nand/or byproducts towards achieving pluralistic alignment in certain settings. Defining pluralistic\nalignment for agentic systems is more complex.\nIn a recent paper by Alamdari, Klassen, Toro Icarte, and McIlraith (2022) on the topic of avoiding\nnegative side effects (arguably, a symptom of a lack of alignment), we suggested that to act safely,\na reinforcement learning (RL) agent an agentic AI should contemplate the impact of its actions\non the wellbeing and agency of others in the environment. To do so, we endowed RL agents with\nthe ability to consider, in their learning, the future welfare and continued agency of others in the\nenvironment. We did so by augmenting the RL agent's reward with an auxiliary reward that reflected\ndifferent functions of expected future return of the collective. Expected future return was characterized\nas a function of a distribution over the value functions of agents in the environment. This general\naspiration and many aspects of the approach are directly relevant to the notion of pluralistic alignment\nin agentic AI. In what follows we recount select ideas from that earlier work."}, {"title": "2 Considering Others", "content": "We situate the work in (Alamdari et al., 2022) in the context of an environment that comprises\na collection of agents that may be affected, directly or indirectly, by the actions of the AI in the\nenvironment. We treat these agents as a distinguished part of the environment, operating under fixed\npolicies, with their behaviors reflected in aggregate as part of the environment behavior. The task\nwe set out to address is for one designated Al agent to learn a policy that minimizes its impact on\nthe future agency and wellbeing of the other agents in the shared environment. In this regard, the AI\nis being considerate of the other agents and indirectly taking their objectives and perspective into\nconsideration when deciding how to act.\nExample: To ground the problem we address, imagine a shared kitchen in a university dorm\nwhere multiple students use the space. Now, consider a robot chef, our AI agent, operating in this\nenvironment. The robot chef must learn a policy to cook various things in the kitchen in a manner that\nis considerate of other students who will use the kitchen afterwards. This includes following social\nnorms, such as cleaning up immediately after cooking and appropriately sharing common resources\nbased on anticipated use (e.g., don't finish all the milk if others might need some later). Additionally,\nthe robot must be mindful of students with severe allergies, ensuring it avoids cross-contamination of\ningredients and keeps the kitchen safe for anyone who might use it next.\nFollowing Sutton and Barto (2018), we model the environment as a Markov Decision Process (MDP),\n(S, A, T, r, \u03b3), consisting of a set of states, actions (for the AI agent \u2013 human actions are not explicitly\nmodelled), a transition function, a reward function, and a discount factor. A terminal state in an\nMDP is a state s which can never be exited \u2013 i.e., T(s|s, a) = 1 for every action a and from which\nno further reward can be gained \u2013 i.e., r(s, a, s) = 0 for every action a. A policy \u03c0 is a (possibly\nstochastic) mapping from states to actions. Given a policy \u03c0, the value of a state s is the expected\nreturn of that state, that is, the expected sum of (discounted) rewards that the agent will get by\nfollowing the policy \u3160 starting in s. That can be expressed using the value function V", "s]$.": "title\": \"2.1 Using Information about Values Functions"}, {"content": "To encourage an AI agent to consider the diverse wellbeing and agency of others, we augment its\nreward function with an auxiliary component that captures the impact of its actions on the future\nagency and welfare of various agents in the environment. Given the diversity of goals and perspectives\nthat may be present and the AI agent's uncertainty about what is truly beneficial for different agents,\nwe assume we have a (finite) set V of possible value functions V : S \u2192 R, and a probability\ndistribution P(V) over that set.\nWe define the augmented reward function as\n$r_{\\text{aligned}}(s, a, s') = \\begin{cases} \\alpha_1 r_1(s, a, s') & \\text{if } s' \\text{ is not terminal} \\\\ \\alpha_1 r_1(s, a, s') + \\alpha_2 F(V, P, s') & \\text{if } s' \\text{ is terminal} \\end{cases}$ (1)\nwhere r\u2081 is the original reward function of the AI agent, and F is an aggregation function over the\ndistribution of value functions.\nThe hyperparameters 01 and 02, which we call \"caring coefficents\", are real numbers that determine\nthe relative contributions of the reward r1, which corresponds to the AI agent's original objective,\nand the auxiliary reward F(V, P, s'), which reflects the values of different agents, to the overall\nreward function. If a\u2081 = 1 and a2 = 0, we just get the original reward function and the AI agent is\noblivious to its impact on others in the environment. If a\u2081 = 0, then the AI agent entirely ignores any\nreward it garners directly from its actions.\nWe considered three possible different definitions of F(V, P, s'):\n$\\sum_{V \\in V} P(V) \\cdot V(s')$ expected future return (2)\n$\\min_{V \\in V: P(V) > 0} V(s')$ worst-case future return (3)\n$\\sum_{V \\in V} P(V) \\cdot \\min(V(s'), V(s_0))$ penalize negative change (4)"}, {"title": "2.2 Treating (Groups of) Individuals Differently", "content": "In pluralistic alignment as in RL, there are cases where we may wish to treat individuals or subgroups\ndifferentially, however our distribution over value functions makes no commitment to the existence\nof individual agents or subgroups. To do so, we augment our formulation with indices, i 1,..., \u03b7,\ncorresponding to different individuals or subgroups, and for each agent (or subgroup) i, we distinguish\na finite set of possible value functions {$V_1^{(i)}, V_2^{(i)}, . . . $}, where P($V_{ij}$) is the probability that V() is\nthe real value function for agent (or subgroup) i. Furthermore, we distinguish caring coefficients ai\nfor each agent (or subgroup) i, and define the following reward function for the AI agent as follows:\n$r_{\\text{aligned}}'(s, a, s') = \\begin{cases} \\alpha_1 r_1(s, a, s') & \\text{if } s' \\text{ is not terminal} \\\\ \\alpha_1 \\cdot r_1(s, a, s') + \\sum_i \\alpha_i \\sum_j P(V_{ij}) \\cdot V_{ij}^{(i)}(s') & \\text{if } s' \\text{ is terminal} \\end{cases}$ (5)\nThis refinement enables us to give the AI agent reward based not on the expected sum of returns of the\nothers (as in Eq. (5)), but by incorporating some notion of \u201cfairness\u201d. For example, we can consider\nthe expected return of the agent who would be worst-off, inspired by the maximin (or \u201cRawlsian\")\nsocial welfare function, which measures social welfare in terms of the utility of the worst-off agent\n(see, e.g., Sen, 1974). We can similarly use fairness notions such as the generalized Gini social\nwelfare function (Weymark, 1981), where we assign greater caring coefficients to the agents (or\nsubgroups) with lower expected returns. Alternatively, following Alamdari et al. (2024), we can\nemploy a voting method to aggregate the value functions of individuals within each subgroup to find\na desirable collective value function for that group."}, {"title": "2.3 Using Information about Options", "content": "In Section 1 we posited that a pluralistically-aligned agent should endeavour to act in a manner that\nensures the agency of other agents, in service of the social welfare of the collective. Rather than use\na distribution over value functions as a proxy for agents' agency, here we consider other agents to\ninstead be endowed with a set of options (Sutton et al., 1999) that could reflect particular skills or\ntasks they are capable of realizing, and we use a distribution over such options to characterize the\nextent of their agency.\nAn option is a tuple (I, \u03c0, \u03b2) where I \u2286 S is the initiation set, \u03c0 is a policy, and \u1e9e is a termination\ncondition (formally, a function associating each state with a termination probability) (Sutton et al.,\n1999). The idea is that an agent can follow an option by starting from a state in its initiation set I and\nfollowing the policy \u3160 until it terminates. Options provide a form of macro action that can be used\nas a temporally abstracted building block in the construction of policies. Options are often used in\nHierarchical RL: an agent can learn a policy to choose options to execute instead of actions. Here we\nwill use options to represent skills or tasks that other agents in the environment may wish to perform.\nSuppose we have a set O of initiation sets of options, and a probability function P(I) giving the\nprobability that I is the initiation set of the option whose execution will be attempted after the AI\nagent reaches a terminating state. To try to make the acting agent act so as to allow the execution\nof that option, we can modify the AI agent's reward function r\u2081, yielding the new reward function\nroption below.\n$r_{\\text{option}}(s, a, s') = \\begin{cases} \\alpha_1 \\cdot r_1(s, a, s') & \\text{if } s' \\text{ is not terminal} \\\\ \\alpha_1 r_1(s, a, s') + \\gamma \\cdot \\alpha_2 \\sum_{I \\in O} P(I) \\cdot I_I(s') & \\text{if } s' \\text{ is terminal} \\end{cases}$ (6)\nwhere I7 : S \u2192 {0,1} is the indicator function for I as a subset of S, i.e., Iz(s) = $\\begin{cases} 1 & \\text{if } s \\in I \\\\ 0 & \\text{otherwise} \\end{cases}$\nNote that if O is finite and P is a uniform distribution, then the auxiliary reward given by roption will\nbe proportional to how many options in O can be started in the terminal state.\nThe hyperparameters 21 and 22 determine how much weight is given to the original reward function\nand to the ability to initiate the option. Given a fixed value of 01 (and ignoring the discount factor),\nthe parameter a2 could be understood as a \u201cbudget\u201d, indicating how much negative reward the acting\nagent is willing to endure in order to let the option get executed.\nFinally, if we had a distribution over pairs (I, V) \u2013 consisting of an option's initiation set and a value\nfunction associated with that \u2013 then this can be captured by the following augmentation:\n$r_{\\text{option}}'(s, a, s') = \\begin{cases} \\alpha_1 r_1(s, a, s') & \\text{if } s' \\text{ is not terminal} \\\\ \\alpha_1 r_1(s, a, s') + \\alpha_2 \\sum_{(I, V) \\in O} P((I, V)) \\cdot I_I(s') \\cdot V(s') & \\text{if } s' \\text{ is terminal} \\end{cases}$ (7)\nThis is much like roption but has an extra factor of V (s') in the sum in the second case."}, {"title": "3 Concluding Remarks", "content": "In this paper we reflected on the issue of pluralistic alignment in the context of agentic AI. We\nposited (here, in brief) that a pluralistically well-aligned agent should attempt to realize its goals\nin a manner that contributes towards maximizing the social welfare of the collective, and ensuring\nthe agency of other agents in service of that welfare. As an example of how we might achieve this,\nwe wanted to share a selection of results from a 2022 paper by the authors that constructs an RL\nagent to learn a policy that is pluralistically aligned in so far as it learns a policy that optimizes\nfor the wellbeing and future agency of a collection of agents, employing a distribution over value\nfunctions to serve as a proxy for their agency. That paper (Alamdari et al., 2022) provides further\ndetails regarding the approach as well as a discussion of related work. As we noted at the outset,\ndefining pluralistic alignment for agentic systems is complex and while this work does not resolve\nthat challenge, we believe it presents a viewpoint and some possible first steps towards realizing\nagents that are pluralistically aligned."}]}