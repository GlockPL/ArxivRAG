{"title": "The Intersectionality Problem for Algorithmic Fairness", "authors": ["Johannes Himmelreich", "Kristian Lum", "Arbie Hsu", "Ellen Veomett"], "abstract": "A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.", "sections": [{"title": "Introduction", "content": "That intersectionality matters is a point of consensus in the algorithmic fairness literature. A model's performance might be much worse for women of color than for women and people of color considered separately [1]. In this paper, we elucidate a problem that intersectionality raises for algorithmic fairness in practice: Because data on intersectional groups is often severely limited, verifying that algorithmic fairness\u2014under various definitions thereof-has been attained is difficult. Although this problem is recognized in the literature [2, 3, 4, 5], its challenges do not appear to be fully appreciated and many existing contributions violate minimal moral or methodological desiderata.\nOur contribution is fourfold: We (1) elucidate the problem of intersectionality in algorithmic fairness, (2) develop desiderata to clarify the challenges that underlie the problem of intersectionality and to guide the search for potential solutions. Moreover, we (3) illustrate the desiderata and potential solutions by presenting a statistical setup that uses simple hypothesis testing, and (4) evaluate this proposal, partly empirically, in light of the desiderata.\nOur topic is an under-appreciated problem in non-ideal epistemology [6]. However, our larger aim is to advance the literature on algorithmic fairness more broadly. The approach that we propose in response to the problem of intersectionality differs fundamentally from the typical way of \u201cmeasuring\" algorithmic fairness.\u00b9 We hence advance the debate, by pointing out possibilities of approaching fairness differently: as accounting for uncertainty (instead of concentrating on point estimates) and as a matter of sufficiency (instead of equality)."}, {"title": "Preliminaries", "content": ""}, {"title": "Algorithmic Fairness", "content": "In the literature on algorithmic fairness, \u201cfairness\" is typically defined as model performance (such as accuracy or false positive rate) that is roughly equal across all relevant groups. Many versions of algorithmic fairness consider fairness to have been achieved if\n$\\vert m(G) - m(\\cdot)\\vert < \\epsilon$ for some small $\\epsilon$, $\\forall G$ \nWhere $G$ denotes a subgroup of the population, $m(G)$ a model's performance (however understood) on only the subset of the data that belongs to group $G$, and $m(\\cdot)$ the model's performance calculated across the entire dataset, irrespective of group membership. Membership in $G$ typically corresponds to a sensitive or protected attribute such as race, sex, age, disability or marital status but $G$ may also be defined intersectionally as a combination of such attributes.\nEquation (1) generalizes a large family of definitions or\u2014when aggregating $\\vert m(G) - m(\\cdot)\\vert$ for all groups-metrics of fairness. We thus take (1) to represent the typical way of understanding algorithmic fairness. This typical way of understanding fairness faces the problem of intersectionality."}, {"title": "The Problem of Intersectionality", "content": "As the number of attributes that define subgroups grows, the amount of data available for each subgroup shrinks rapidly. After all, the number of subgroups grows exponentially with the number of protected attributes: For n binary attributes, there are $2^n$ intersectional groups. This, in turn, entails a data problem: When social identities are constituted by intersections of increasingly many variables, and when the constituting attributes are not just binary, the data within each of the intersections can become very small. In Europe, where discrimination is highly intersectional and fairness audits are encouraged by legislation,\u00b2 fairness audits may need to account for several thousand subgroups.\u00b3 And because gathering the data necessary for fairness audits is typically costly- e.g., the \"ground truth\" needs to be established to assess whether a prediction is correct such data tend to be scarce to begin with.\nIn short, the intersectionality problem of algorithmic fairness is a problem of statistical uncertainty due to small data and, subsequently, raises problems for how \"fairness\u201d is typically defined.\nIntersectionality renders fairness metrics, as they are typically defined, meaningless. These metrics, such as (1), rely on point estimates of model performance (e.g., whether this performance is roughly the same for all groups). But point estimates become nonsensical with small data [2].\u2074 The chal- lenge posed by intersectionality for algorithmic fairness is to define a fairness metric that provides meaningful estimates of fairness even when groups are very small and audit data are sparse."}, {"title": "Existing Work", "content": "Various statistical methods have been proposed for intersectionality in algorithmic fairness."}, {"title": "Kearns et al.", "content": "An early identification and statement of the problem of intersectional fairness arising from small groups is due to Kearns et al. [2]. The approach of Kearns et al. involves an audit algorithm that learns to classify models as fair or unfair instead of defining a fairness metric. The process of learning this audit algorithm is subject to a fairness constraint that is weighted depending on the proportion of the population belonging to a particular group G.\nKearns et al. define $\\alpha(G) = Pr(G)$ and reformulate fairness in (1) as\n$\\alpha(G)\\vert m(G) - m(\\cdot)\\vert < \\epsilon  \\forall G$ \nEssentially, the addition of $\\alpha(G)$ relaxes the original fairness metric of (1) depending on the proportion of $G$ as a share of the overall population. The smaller $G$ is, the more the condition is relaxed. As Kearns et al. explain, this addition is necessary to enable statistical estimation, given the increasing statistical uncertainty with decreasing group size. We discuss the implications in Section 4."}, {"title": "Foulds et al. and Morina et al.", "content": "Foulds et al. [3] provide an alternative approach based on ratios of model performance metrics. An expanded version of which is, in turn, given by Morina et al. [4].\u2075\nThese definitions require that the ratio of some metric value between two groups be within a fixed interval. For example, suppose $m(G)$ measures the true positive rate (TPR) for subgroup $G$. Then the $\\epsilon$-differential intersectional definition of TPR parity (equal opportunity) given in [4] is that\n$\\epsilon^{-}< \\frac{m(G)}{m(G')} < \\epsilon \\forall G,G'$\nMorina et al. [4] note that $\\epsilon = 0$ corresponds to \"perfect fairness\u201d ($m(G) = m(G')$)."}, {"title": "Molina and Loiseau", "content": "Molina and Loiseau use a statistical approach to addressing intersectionality and fairness [5]. They call a classifier $(\\epsilon, \\delta)$-probably intersectionally fair if \u201cthe expected number of people that faces a discrimination more than $\\epsilon$ is less than $n\\delta$\u201d (n is the population size).\u2076"}, {"title": "Cherian and Cand\u00e8s", "content": "Cherian and Cand\u00e8s [12] address fairness auditing for many subpopulations within the framework of hypothesis testing, as we do here. They use a bootstrap process to provide statistical performance bounds for many subpopulations at once. Our addition to this study is the illumination and discussion of desiderata (in Section 4), a clear description of how one can design fairness metrics using hypothesis testing (Section 5), and an empirical study showing that these metrics encourage (rather than discourage) the gathering of additional data to improve model performance (Section 6)."}, {"title": "Khan et al. and Agrawal et al.", "content": "Khan et al. [13] consider metrics of fairness, accuracy, and variance for model estimators. They empirically show that there tends to be a tradeoff between these three values. In a similar vein, Agrawal et al. study debiasing methods, and in doing so show both theoretically and empirically that estimation variance tends to be higher in small subgroups [14]. Additionally, they prove results suggesting that partial debiasing results in both less variance and better fairness properties."}, {"title": "Desiderata", "content": "Although the problem of intersectionality is recognized in the literature, how difficult this problem is may not have been fully appreciated. At least some of the existing contributions violate minimal moral or methodological desiderata, as we shall see in Sections 4.1, 4.2, and 4.3.\nA core tenet of building ethical algorithms is that machine-learned models need to be consistent with \"human values,\" which can be formulated as desiderata. We see the following desiderata for intersectional fairness metrics."}, {"title": "Minimal Justice", "content": "A first desideratum we call \"minimal justice.\u201d The idea is, roughly, that a standard of fairness should not be lower for certain groups, such as those historically targeted for discrimination or facing structural injustice. Intuitively, minimal justice is a form of minority protection that says \u201cdon't disadvantage the disadvantaged.\"\nThis desideratum is a weak form of prioritarianism. Recent work in algorithmic fairness has identified a similar prioritarian idea in \u201cpredictive justice\" [15]. Whereas prioritarianism, a theory of distributive justice for well-being, demands that \u201cbenefitting people matters more the worse off these people are\" [16], minimal justice requires only that those \u201cworse off\u201d should be given at least the same weight in aggregating a fairness metric. The desideratum does not require that greater weight be given to any group, and is hence met when a standard of fairness is identical for all groups.\nTo illustrate the desideratum, consider an example. Notwithstanding its merits, the proposal of Kearns et al. [2] may violate minimal justice. As noted above, the addition of $\\alpha(G)$ in (2) relaxes the fairness constraint proportional to the size of a group. The smaller a group is (as a share of the data), the worse a model performance can be and still certify the model as fair. The fairness standard is hence lowered for small groups. On the assumption that these small groups include historically disadvantaged or oppressed groups, (2) violates minimal justice.\nAnd drastically so: For a group G' that is e times smaller than group G (i.e. $\\frac{\\alpha(G)}{\\alpha(G')} = \\epsilon$), a model can be certified as \"fair\" if the disparity between the average performance and the performance for group G' is as much as c-times worse than it is for group G. Furthermore, for some value of e there are groups that are proportionally so small that there is no model performance poor enough to certify the model as unfair. For example, if $\\epsilon = .01$, for a binary classifier, any group whose proportion of the total population is less than e is protected by essentially no fairness constraint at all.\u2077\nThe ethical impact can be immense. A group might look relatively small in the data but be, in fact, large in absolute numbers in the population. Indeed, disadvantaged groups tend to be under- represented in data [17, 18]. Thus, the approach of Kearns et al. may lower the standard of fairness for precisely those groups that fairness is meant to protect."}, {"title": "Consistent Conceptualization", "content": "Any fairness metric operationalizes a certain idea, or concept, of fairness. A second desideratum is that fairness metrics should operationalize a concept of fairness consistently.\nThis desideratum may resemble that of Minimal Justice. But whereas Minimal Justice is a moral desideratum, Consistent Conceptualization is a methodological one. Consistent Conceptualization aims to ensure minimal construct validity, i.e., that a certain metric represents a concept of interest correctly. The importance of construct validity for fairness is already established in the literature [19].\nTypically, fairness metrics in algorithmic fairness operationalize the idea of equality. This is particu- larly evident in (1) which, for each group $G$, restricts the absolute disparity of $m(G)$ from overall mean performance $m(\\cdot)$. This is one\u2014albeit a very simple-way of operationalizing inequality (for alternatives see [20]). Likewise, (3) operationalizes fairness as equality [3, 4].\u2078"}, {"title": "Incentive Compatibility", "content": "The final desideratum starts with the recognition that metrics specify incentives. Anyone who wants to increase their models' fairness may want to maximize a fairness metric. The final desideratum thus requires that a fairness metric not have \u201cperverse\u201d incentives of two kinds: discouraging data collection and allowing \u201cgaming.\"\nFirst, a fairness metric should not discourage data collection. Any fairness metric that indicates greater unfairness only because further data are sampled from some group would fail to be incentive compatible. Likewise, inversely, any fairness metric would fail the desideratum that indicates greater fairness only because data based on group identity are dropped.\nThe fairness metric (2), of Kearns et al., may violate this desideratum of incentive compatibility. This is because collecting more data on a minority population G tightens the constraint by increasing $\\alpha(G)$, thus making a certification of \u201cfairness\u201d at a given level of e more difficult. Specifically, suppose that $m(G) = .15$ and $m(\\cdot) = .85$. If $\\alpha(G) = .01$, then the performance would be deemed \"fair\" for all $\\epsilon > 0.7 \\times 0.01 = .007$. However, if we collect more data for group G such that $\\alpha(G) = .2$, then the model would be \"fair\" only for $\\epsilon > 0.7 \\times 0.2 = .014$. Unless the additional data results in material improvements to $m(G)$, for any e such that $.007 < \\epsilon < 0.014$, the fairness metric (2) would certify a given model as fair prior to further data collection, but as unfair afterwards. In short, under (2), fairness for hard-to-predict groups could be attained simply by under-representing them in the training data.\nThis is a \"perverse\" effect because, in practice, additional data collection about a minority group will help improve the model performance for that group. In other words, the metric gives an incentive to do the opposite of what it is meant to achieve.\nWhether other metrics [4, 3, 5] violate this desideratum depends on whether the estimated performance disparity is greater than the true disparity (which further data would likely help approximate). Fairness metrics that operationalize fairness as equality (e.g. as model performance disparity across groups), incentivize m(G) to be nearly the same for all subgroups $G$. If the true model performance is nearly equal among groups, then these metrics incentive further data collection in order to have more accurate estimates of $m(G)$.\nSecond, a fairness metric should not encourage knowingly erroneous predictions. But some metrics (e.g., statistical or demographic parity) have exactly this property: Even if the label that we want to predict is known (which it generally, of course, isn't), \u201cfairness\u201d as these metrics define it can be improved by erroneous predictions. This is an undesirable property of fairness metrics [23]."}, {"title": "Two Alternative Metrics", "content": "We now illustrate how these desiderata can be met. We propose two alternative models, which we call the \"optimist's\" and \"pessimist's model\u201d respectively. Both define the problem using hypothesis testing. The optimist has the null hypothesis that the model is fair, and we have to prove it is not"}, {"title": "Optimist's Model", "content": "We could formulate the problem of fairness for small groups as testing the joint hypothesis that\n$H_0: m(G) > c \\forall G$\n$H_1 : m(G) \\leq c \\exists G$\nConsider a group $G$ of size $n_G$. Suppose $m(G)$ is accuracy. As a sample proportion, the standard error for our estimate of $m(G)$ is $\\sqrt{\\frac{m(G)(1-m(G))}{n_G}}$. Then, we would reject the null if the upper end of its confidence interval is less than $c$, i.e., if $m(G) + 1.64\\sqrt{\\frac{m(G)(1-m(G))}{n_G}} < c$ (ignoring multiple testing).\u00b9\u00b9 Under this formulation, we reject $H_0$ if $m(G)$ is sufficiently less than $c$, where \u201csufficiently less\" has to do with our statistical power to detect that it is less. We would declare the model fair, if at given level $c$ we cannot statistically reject that the model performs at least $c$ well for all groups.\nA minority population which is sufficient in number would easily reject the null if $m(G)$ is truly below $c$. Indeed, even with a population size of $n_G = 1000$, if $c = 0.7$, then a value of $m(G) < 0.67$ would reject the hypothesis that the model is fair."}, {"title": "Pessimist's Model", "content": "Depending on a model's deployment context, the optimistic approach might be problematic. Consider instead the following pessimistic hypothesis test.\n$H_0: m(G) < c \\exists G$\n$H_1 : m(G) \\geq c \\forall G$\nWe would declare the model fair, if at a given level $c$ we know with statistical certainty that the model performs at least $c$-well for all groups. In this case, (ignoring multiple testing again) we would require that $m(G) - 1.64\\sqrt{\\frac{m(G)(1-m(G))}{n_G}} > c$ for all $G$."}, {"title": "Fairness Metrics", "content": "The formulations can be extended from a hypothesis test to a fairness metric by finding the maximal c for which the respective null hypothesis cannot be rejected (for the optimist) or can be rejected (for the pessimist). In the optimist's model, choose the maximal c such that\n$c \\leq m(G) + 1.64\\sqrt{\\frac{m(G)(1-m(G))}{n_G}}$ \nfor all relevant groups $G$. The fairness metric is the maximal e such that we cannot reject the hypothesis that the model performs at least c-well for all groups.\nThis metric can be read as saying that a model is \"fair up to c.\u201d Intuitively, this means that, for all we know, the model performance $m(G)$ (say, accuracy) is likely as high as c for each group.\nOn the pessimist's model, we instead choose the maximal c such that\n$c \\leq m(G) \u2013 1.64\\sqrt{\\frac{m(G)(1-m(G))}{n_G}}$"}, {"title": "Discussion: Desiderata", "content": "Both metrics satisfy Minimal Justice. The bound c encodes a standard of fairness that is identical for all groups. Moreover, the relative size of groups doesn't matter. Whether a null hypothesis can be rejected changes with the absolute size of the group $n_G$ (rather than the proportion $\\frac{n_G}{n}$).\nOn the optimist's metric, for a small group, the difference between the actual (lower) model perfor- mance and the level up to which a model can be certified as fair might be large. But both of our metrics base their certification of \u201cfairness up to c\u201d on an aggregation that gives all groups the same weight. In fact, the pessimist's metric can be called \"epistemically risk averse\" insofar as it picks the highest lower bound out of all groups' confidence intervals (and hence is similar to the maximin decision rule).\nOn Consistent Conceptualiztion, both of our metrics conceptualize fairness as sufficiency. They understand fairness not as a matter of whether everyone has the same (as equality does), but whether everyone has enough [21, 22]. This idea is operationalized in (4) and (5) in a transparent and natural way: with an inequality. Moreover, the threshold c, what counts as \u201cenough,\u201d is determined absolutely in the terms of model performance measure, and not depending on, e.g., how well the model performs on other groups. Thus, both of our metrics operationalize sufficiency consistently across all groups.\nFor Incentive Compatibility the picture is mixed: Both of our metrics discourage gaming (and thus satisfy Incentive Compatibility in this respect). This is because both fairness metrics determine (un)fairness as the highest (or lowest) expectable model performance across all groups. As such, improving model performance will never increase unfairness; and decreasing model performance will never increase fairness. In fact, decreasing model performance may lead to a decrease in fairness. It appears that operationalizing the idea of fairness as sufficiency is what makes our fairness metrics less susceptible to gaming-in particular, that the minimum level of model performance is defined in absolute terms and equally enforced for all groups.\nBut one of our metrics, namely the optimist's, may discourage further data collection (and thus violate Incentive Compatibility in its first respect). Because the optimist's model starts with the null hypothesis that a model is fair at a given c, gathering more data can make things \u201cworse\u201d; that is, with more data, we might come to reject the optimistic null hypothesis of fairness at a given c. A model might perform very poorly for certain groups, but we cannot reject the null hypothesis that the model is fair up to c, thanks to sparse data and the metric thus results in an incentive to not sample more data but to instead \"look the other way.\""}, {"title": "Fairness Datasets Analysis", "content": "To evaluate whether our metrics in practice incentivize or disincentivize additional data collection, we assess their results on well-known \u201cfairness datasets.\" To do this, we use lale; a Python library created by IBM [24]. This library allows for creation of consistent automated machine learning models. It also has 20 fairness datasets that can easily be fetched, modeled, and evaluated [25]. These"}, {"title": "Conclusion", "content": "Although the general idea of intersectionality seems easy to state, putting intersectionality to work in quantitative social science is, generally, far from straight-forward [28]. Likewise, intersection- ality presents a problem for algorithmic fairness: Intersectionality requires estimating statistical properties across subgroups that are increasingly small, which gives rise to statistical as well as moral-methodological challenges.\nStatistically, small groups are a challenge for estimation. As statistical uncertainty increases (due to more and smaller groups), the point estimates of model performance for these groups become meaningless. Any approach of intersectional fairness needs to account for statistical uncertainty.\nBut some existing metrics do not seem to fully appreciate the moral-methodological challenges that underlie this problem and \u201clower the fairness bar\" for smaller groups, i.e., the metrics violate desiderata such as Minimal Justice or Consistent Conceptualization.\nWith this paper, we elucidate this intersectionality problem for algorithmic fairness: We develop minimal desiderata to clarify the moral-methodological challenges underlying this problem; we argue that some existing fairness metrics fail these desiderata, but illustrate that the desiderata can be met.\nWe propose fairness metrics that rely on hypothesis testing (instead of performance point estimates) and that understand fairness as sufficiency (instead of equality). On these proposed metrics, fairness is understood as a certain minimum level of expected model performance that is, for all we know, likely enjoyed by all groups. We empirically evaluate the metrics against the proposed desiderata, including on 18 datasets that are widely used for fairness benchmarks.\nIn light of their technical and normative-theoretical limitations, the metrics we propose should be seen as illustrations. Technically, the simple hypothesis testing needs to be extended to multiple hypothesis testing to allow for interdependent subgroup memberships (see Appendix B). Normative-theoretically, the desiderata that we develop are not exhaustive and they do not uniquely characterize the metrics we propose.\nNevertheless, overall, our findings extend the list of problems that statistical uncertainty raises for algorithmic fairness. Previous work observed that fairness metrics are biased: They \"fail to account for statistical uncertainty . . . exaggerating the extent of performance disparities\u201d between groups where such disparities exist and indicating disparities \u201cin cases where model performance is... identical across groups\" [7]. Our present findings add that with increasing statistical uncertainty fairness metrics risk becoming either nonsensical (if they aggregate point estimates) or morally inadequate (if they \"lower the fairness bar\" to enable statistical estimation).\nHowever, we also offer ways of advancing the literature on algorithmic fairness: with desiderata that clarify the challenges at hand and guide the search for solutions, and with fairness metrics that suggest novel avenues for defining such metrics based on hypothesis testing and fairness as sufficiency."}, {"title": "Discussion: Impact of n and m on Each Model", "content": "To give the reader a feel for the mathematical impact of the choice between these two models, we share some hopefully informative plots in Figure 2.\nThe horizontal axis of each of these plots is m, the metric value, which is assumed to be a proportion for which higher values are preferred (such as accuracy). The vertical axis is n, the size of the subgroup. The hue at (m, n) in Figure 2a is the corresponding value of $m - 1.64\\sqrt{\\frac{m*(1-m)}{n}}$. Here we can visually see that, for a fixed metric value m, the subgroup size must be reasonably large in order to reject the hypothesis that the model is \"unfair above c\" for c near m.\nSimilarly, the hue at (m, n) in Figure 2b is the corresponding value of $\\min\\{m + 1.64\\sqrt{\\frac{m*(1-m)}{n}},1\\}$, capped at a value of 1 (since no proportion can be larger than 1). Here we can visually see that, for a fixed metric value m, the subgroup size must be reasonably large in order to reject the hypothesis that the model is \"fair up to c\" for c near m. To further understand the impact of small groups in this optimist's model, we include Figure 2c. In Figure 2c, the hue simply gives the value of $m + 1.64\\sqrt{\\frac{m*(1-m)}{n}}$, even if it is larger than 1. This plot further highlights the fact that, in the optimist's model, it is very difficult to reject the hypothesis that the model is perfectly fair for very small subgroups."}, {"title": "Limitations", "content": "We note that the issue of multiple hypothesis testing is one which we do not address in depth. If membership in the different groups in question is independent, one can use the Bonferroni correction to address the multiple hypothesis tests. Under this strict type of multiple hypothesis testing, the p-values that are calculated are using significance level $\\frac{\\alpha}{n}$, where n is the number of hypotheses that we are testing. This correction guarantees that the probability that we reject one or more null hypotheses is no more than $\\alpha$. Considering overlapping subgroups (such as considering fairness both"}, {"title": "Methods", "content": "We here provide further details on our empirical methods.\nFor starters, we choose the lale library and its accompanying datasets for two reasons:\n1.  The number of \"fairness datasets\" in the lale library is larger than any other conglomeration of fairness datasets that we are aware of.\n2.  Because the lale library has built-in models, we can apply a consistent type of model to each dataset, so that our experiments are not muddied by differing model constructions."}, {"title": "Subgroup Identification", "content": "The models we created use a forest of boosted trees from the XGBoost library; the functions to easily create these models are also part of the lale library. We created three models using the lale pipeline, using 3-fold cross-validation. The three models can be accessed to evaluate their accuracy on various subgroups. However, since lale requires sklearn version 1.2, we do not have access to the train/test indices of each of the models. Thus, to evaluate the accuracy on group G, we do so on all of the members of G in the dataset.\nThe set of subgroups G on which we calculated the model accuracy come in part from the fairness data that lale provides, and also from attributes that are well-understood to be sensitive. Specifically, all of the attributes that the lale library lists as \u201cprotected\" are included in our master list of protected attributes. If the rows in the dataset correspond to individuals, and any of age, sex, race} were not in lale's list of protected attributes, we added them to the master list. From this master list, we created all subgroups using all categories in the master list. For example, if a dataset had race, sex, and age category, we included in G each triple (r, s, a), where r was a race in that dataset's race column, s was a sex in that dataset's sex column, and a was an age category for that dataset."}, {"title": "Data Pre-processing", "content": "For each of the 20 fairness datasets, we used the built-in lale data pre-processing with small adjust- ments.\nWe used the simple methods for imputing missing data which are provided with the sample notebook at [29].\nIn order to use XGBoost, we needed to change some of the predicted categories to integer type.\nIn order to make the results more understandable, we re-named some of the categories (for example, changing the 'sex' categories from 0/1 to male/female).\nThe \"race\" categories in the nlsy dataset were atypical, including both categories such as 'GERMAN' and 'BLACK.' We did not attempt to clean that data but left the categories as given.\nWe created groupings by age for those datasets that don't already come with age groupings (see Appendix C.3)."}, {"title": "Age Grouping", "content": "For the age attribute, some of the datasets already come with age groupings. In those cases, we directly used those groupings as the age categories. For the datasets where age was a strictly numerical attribute, we used the following heuristic to create categories:"}, {"title": "Analysis", "content": "For each such subgroup $G \\in G$, we calculated m(G): the average accuracy of the three models on that subgroup. We then calculate the c values associated with each of those subgroups; indexed by $c_1$ for the optimist's and $c_2$ for the pessimist's metric. Specifically, for group G we calculate\n$c_f = m(G) + 1.64\\sqrt{\\frac{m(G)(1-m(G))}{N_G}}$\nfrom the optimist's model and\n$c_l = m(G) \u2013 1.64\\sqrt{\\frac{m(G)(1-m(G))}{N_G}}$\nfrom the pessimist's model.\nOnce these are calculated for all subgroups, we calculate\n$acc_{min} = \\min\\{m(G) : G\\in G\\}$\n$c_1 = \\min\\{c_f : G \\in G\\}$\n$c_2 = \\min\\{c_l : G \\in G\\}$\nWe also find their corresponding subgroups:\n$G_{min_acc} = argmin\\{m(G) : G\\in G\\}$\n$G_1 = argmin\\{c_f : G \\in G\\}$\n$G_2 = argmin\\{c_l : G \\in G\\}$\nThe group $G_{min_acc}$ is the group with minimum estimated accuracy, while group $G_1$ ($G_2$) is on the cusp of rejecting the hypothesis that the model is fair (not being able to reject the hypothesis that the model is unfair) up to accuracy $C_1$ ($C_2$). Thus, we call groups $G_{min_acc}$, $G_1$, and $G_2$ the critical subgroups for a dataset. For some datasets, there are three distinct critical subgroups, while for other datasets, some of the critical subgroups are the same; see Tables 1, 2, 3, and 4 in Appendix D for details.\nOnce we had the (up to) three critical subgroups of each dataset, we did two additional analyses."}, {"title": "Subsample Just the Critical Group", "content": "Suppose G is a critical subgroup of a dataset. We then created 10 models (each a set of three 3-fold cross-validated models), where we include 10%, 20%,..., 100% of the subgroup in the dataset used to create the model. We then evaluated that group's critical value (whether it be m(G), $c_f$, or $c_l$) on each of those 10 models, to see how those values change. The intention here is to mimic increasing samples from just the critical group, and how that additional data collection impacts the fairness evaluation of the model. These results of this analysis were in Figure 1."}, {"title": "Subsample the Entire Dataset", "content": "Suppose G is a critical subgroup of a dataset. We also created 10 models (each a set of three 3-fold cross-validated models), where we included 10%, 20%,..., 100% of the entire dataset to create the model. We then evaluated that group's critical value (whether it be m(G), $c_f$, or $c_l$) on each of those 10 models, to see how those values change. The intention here is to mimic increasing sampling overall, and how that additional data collection impacts the fairness evaluation of the model. We note that, for the nursery dataset, one of the predicted categories (recommend) had only two data points with that category. In order for XGBoost to successfully create a model, we needed to add back both of those two data points into each subsample (if they had been removed in that random subsample). The results of this analysis are in Figure 3."}]}