{"title": "Counterfactual Explanations with Probabilistic Guarantees on their Robustness to Model Change", "authors": ["Ignacy St\u0119pka", "Mateusz Lango", "Jerzy Stefanowski"], "abstract": "Counterfactual explanations (CFEs) guide users on how to adjust inputs to machine learning models to achieve desired outputs. While existing research primarily addresses static scenarios, real-world applications often involve data or model changes, potentially invalidating previously generated CFEs and rendering user-induced input changes ineffective. Current methods addressing this issue often support only specific models or change types, require extensive hyperparameter tuning, or fail to provide probabilistic guarantees on CFE robustness to model changes. This paper proposes a novel approach for generating CFEs that provides probabilistic guarantees for any model and change type, while offering interpretable and easy-to-select hyperparameters. We establish a theoretical framework for probabilistically defining robustness to model change and demonstrate how our BETARCE method directly stems from it. BETARCE is a post-hoc method applied alongside a chosen base CFE generation method to enhance the quality of the explanation beyond robustness. It facilitates a transition from the base explanation to a more robust one with user-adjusted probability bounds. Through experimental comparisons with baselines, we show that BETARCE yields robust, most plausible, and closest to baseline counterfactual explanations.", "sections": [{"title": "1. Introduction", "content": "Counterfactual explanations (counterfactuals, CFEs) are one of the most popular forms of explaining decisions made by complex, black-box machine learning (ML) algorithms. Briefly, a counterfactual explanation of a decision y made for input x is an instance $x_f$ that is very similar to x but produces a different, more desirable prediction $y' \\neq y$.\nSince CFEs can be interpreted as an answer to the question: \"given the decision y taken for input x, how should x be changed to produce the alternative decision y'?\", they offer actionable feedback to the user. This is appreciated by stakeholders in various application areas such as supporting loan decisions (Wachter et al., 2017), job recruitment (Pearl et al., 2016), medicine (Mertes et al., 2022), and many others (Guidotti, 2022).\nEven though the basic definition of a counterfactual (Wachter et al., 2017) specifies only two basic properties: validity (ensuring the desired classification y') and proximity (small distance between x and $x_{cf}$), many additional properties are useful from both the application (Keane et al., 2021) and user (F\u00f6rster et al., 2020; Keane et al., 2021) point of view. These properties include sparsity (modifying values of only few features), actionability (realistic feature changes), plausibility (proximity to the data distribution), and many others. Even tough numerous methods for generating counterfactuals with different properties have been proposed (Guidotti, 2022), almost all of them deal with a static problem setting and overlook the problem of counter-factual robustness to model change.\nSince a counterfactual is intended to deliver actionable feedback to the user, it must remain valid for the period of time necessary to undertake changes. Note that a CFE is generated for a fixed model, however, many applications are inherently dynamic, and the model is changing over time. There are many possible causes of such model change, some of which include obtaining new training data, adjusting hyperparameters or model architecture, and even the need to remove some of the training data due to data expiration policies or privacy laws (someone may request their personal data to be removed (Ginart et al., 2019)). In such scenario, it is important to preserve the validity of the counterfactual for the newly retrained model, so that the user will still get the desired decision while acting on the recourse offered to them before the model change.\nFor example, suppose a bank has generated a CFE for a customer who has been denied a loan. During the time in which the customer tries to improve their financial profile to meet the requirements specified in the CFE, the bank may need to update its model. In such scenario, it would be"}, {"title": "2. Related Work", "content": "Counterfactual explanations are a widely explored topic in the contemporary ML literature. Some well-known representatives of such methods (with an indication of the counterfactual properties they try to take into account) include WACHTER (Wachter et al., 2017) \u2013 closest to the input instance, GROWINGSPHERES (Laugel et al., 2018) \u2013 closest and sparse, FACE (Poyiadzi et al., 2020) \u2013 located in dense regions, DICE (Mothilal et al., 2020) \u2013 multiple closest, sparse, plausible and diverse counterfactuals. For a comprehensive survey, see e.g., (Guidotti, 2022; Verma et al., 2020). These methods generate counterfactuals with various properties considered useful from the user's perspective. However, Stepka et al. (2024) used multi-criteria analysis to"}, {"title": "3. Method", "content": "In this section, we first formally define the robustness of the counterfactual to model change and provide a theoretical framework for its estimation. Later, we describe the algorithm that stems from these theoretical foundations, called BETARCE, which provides robust counterfactuals with statistical guarantees.\n3.1. Defining robustness\nConsider a binary classification problem where a machine learning model $M: \\mathcal{X} \\rightarrow \\mathcal{Y}$ assigns a binary label $y \\in \\mathcal{Y} = \\{0,1\\}$ to each instance $x \\sim \\mathcal{X}$. Given an input instance $x_{orig}$ and a prediction $M(x_{orig})$, a counterfactual explanation $x_{cf}$ is an instance similar\u00b9 to $x_{orig}$ for which the model reaches the opposite decision, i.e. $M(x_{orig}) \\neq M(x_{cf})$.\nAlthough using the counterfactual $x_{cf}$ instead of the original instance $x_{orig}$ causes the model M to reverse its decision, even a small change in the model M' can potentially invalidate the counterfactual, i.e. $M'(x_{orig}) = M'(x_{cf})$. This leads us to the notion of counterfactual robustness to model change.\nDefinition 1 (Robust counterfactual). A counterfactual $x_{cf}$ explaining the prediction of a model M is robust to its change to a model M' if $x_{cf}$ is identically classified by both the original and the changed model: $M(x_{cf}) = M'(x_{cf})$.\nIn practice, it is impossible to construct a counterfactual that is robust to arbitrarily large changes in the model. However, it may be desirable to offer to the user an explanation that is robust to relatively not large model changes such as retraining with different random seed, making slight modifications of training data, or changing model hyperparameters. We formally address these changes by defining the distribution of all possible models resulting from such changes\u00b2.\nDefinition 2 (Space of admissible models). The space of admissible models $\\mathcal{M}_\\mathcal{M}$ is the probabilistic distribution of all models that are the result of a complete retraining of the model M using arbitrary settings from the predefined set of model changes.\nIn the related literature, model changes are defined as perturbations of model parameters (Dutta et al., 2022; Xu &"}]}