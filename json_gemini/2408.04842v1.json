{"title": "Counterfactual Explanations with Probabilistic Guarantees on their Robustness to Model Change", "authors": ["Ignacy St\u0119pka", "Mateusz Lango", "Jerzy Stefanowski"], "abstract": "Counterfactual explanations (CFEs) guide users\non how to adjust inputs to machine learning mod-\nels to achieve desired outputs. While existing\nresearch primarily addresses static scenarios, real-\nworld applications often involve data or model\nchanges, potentially invalidating previously gen-\nerated CFEs and rendering user-induced input\nchanges ineffective. Current methods address-\ning this issue often support only specific models\nor change types, require extensive hyperparame-\nter tuning, or fail to provide probabilistic guaran-\ntees on CFE robustness to model changes. This\npaper proposes a novel approach for generating\nCFEs that provides probabilistic guarantees for\nany model and change type, while offering inter-\npretable and easy-to-select hyperparameters. We\nestablish a theoretical framework for probabilis-\ntically defining robustness to model change and\ndemonstrate how our BETARCE method directly\nstems from it. BETARCE is a post-hoc method\napplied alongside a chosen base CFE generation\nmethod to enhance the quality of the explanation\nbeyond robustness. It facilitates a transition from\nthe base explanation to a more robust one with\nuser-adjusted probability bounds. Through ex-\nperimental comparisons with baselines, we show\nthat BETARCE yields robust, most plausible, and\nclosest to baseline counterfactual explanations.", "sections": [{"title": "1. Introduction", "content": "Counterfactual explanations (counterfactuals, CFEs) are\none of the most popular forms of explaining decisions made\nby complex, black-box machine learning (ML) algorithms.\nBriefly, a counterfactual explanation of a decision y made\nfor input x is an instance xf that is very similar to x but\nproduces a different, more desirable prediction y' \u2260 y.\nSince CFEs can be interpreted as an answer to the question:\n\"given the decision y taken for input x, how should x be\nchanged to produce the alternative decision y'?\", they offer\nactionable feedback to the user. This is appreciated by\nstakeholders in various application areas such as supporting\nloan decisions (Wachter et al., 2017), job recruitment (Pearl\net al., 2016), medicine (Mertes et al., 2022), and many others\n(Guidotti, 2022).\nEven though the basic definition of a counterfactual\n(Wachter et al., 2017) specifies only two basic properties:\nvalidity (ensuring the desired classification y') and proxim-\nity (small distance between x and xcf), many additional\nproperties are useful from both the application (Keane et al.,\n2021) and user (F\u00f6rster et al., 2020; Keane et al., 2021)\npoint of view. These properties include sparsity (modifying\nvalues of only few features), actionability (realistic feature\nchanges), plausibility (proximity to the data distribution),\nand many others. Even tough numerous methods for gen-\nerating counterfactuals with different properties have been\nproposed (Guidotti, 2022), almost all of them deal with a\nstatic problem setting and overlook the problem of counter-\nfactual robustness to model change.\nSince a counterfactual is intended to deliver actionable feed-\nback to the user, it must remain valid for the period of time\nnecessary to undertake changes. Note that a CFE is gen-\nerated for a fixed model, however, many applications are\ninherently dynamic, and the model is changing over time.\nThere are many possible causes of such model change, some\nof which include obtaining new training data, adjusting hy-\nperparameters or model architecture, and even the need to\nremove some of the training data due to data expiration poli-\ncies or privacy laws (someone may request their personal\ndata to be removed (Ginart et al., 2019)). In such scenario,\nit is important to preserve the validity of the counterfactual\nfor the newly retrained model, so that the user will still get\nthe desired decision while acting on the recourse offered to\nthem before the model change.\nFor example, suppose a bank has generated a CFE for a\ncustomer who has been denied a loan. During the time in\nwhich the customer tries to improve their financial profile\nto meet the requirements specified in the CFE, the bank\nmay need to update its model. In such scenario, it would be"}, {"title": "2. Related Work", "content": "Counterfactual explanations are a widely explored topic in\nthe contemporary ML literature. Some well-known repre-\nsentatives of such methods (with an indication of the coun-\nterfactual properties they try to take into account) include\nWACHTER (Wachter et al., 2017) \u2013 closest to the input in-\nstance, GROWINGSPHERES (Laugel et al., 2018) \u2013 closest\nand sparse, FACE (Poyiadzi et al., 2020) \u2013 located in dense\nregions, DICE (Mothilal et al., 2020) \u2013 multiple closest,\nsparse, plausible and diverse counterfactuals. For a com-\nprehensive survey, see e.g., (Guidotti, 2022; Verma et al.,\n2020). These methods generate counterfactuals with vari-\nous properties considered useful from the user's perspective.\nHowever, Stepka et al. (2024) used multi-criteria analysis to"}, {"title": "3. Method", "content": "In this section, we first formally define the robustness of the\ncounterfactual to model change and provide a theoretical\nframework for its estimation. Later, we describe the algo-\nrithm that stems from these theoretical foundations, called\nBETARCE, which provides robust counterfactuals with sta-\ntistical guarantees."}, {"title": "3.1. Defining robustness", "content": "Consider a binary classification problem where a machine\nlearning model M : X \u2192 Y assigns a binary label y \u2208 Y =\n{0,1} to each instance x ~ X. Given an input instance\nxorig and a prediction M(xorig), a counterfactual explana-\ntion xef is an instance similar\u00b9 to xorig for which the model\nreaches the opposite decision, i.e. M(xorig) \u2260 M(xcf).\nAlthough using the counterfactual xf instead of the original\ninstance xorig causes the model M to reverse its decision,\neven a small change in the model M' can potentially inval-\nidate the counterfactual, i.e. M'(xorig) = M'(xcf). This\nleads us to the notion of counterfactual robustness to model\nchange."}, {"title": "Definition 1 (Robust counterfactual).", "content": "A counterfactual xef\nexplaining the prediction of a model M is robust to its\nchange to a model M' if x\u00a3f is identically classified by both\nthe original and the changed model: M(xcf) = M'(x\u00a3f).\nIn practice, it is impossible to construct a counterfactual that\nis robust to arbitrarily large changes in the model. However,\nit may be desirable to offer to the user an explanation that is\nrobust to relatively not large model changes such as retrain-\ning with different random seed, making slight modifications\nof training data, or changing model hyperparameters. We\nformally address these changes by defining the distribution\nof all possible models resulting from such changes2."}, {"title": "Definition 2 (Space of admissible models).", "content": "The space of\nadmissible models MM is the probabilistic distribution of\nall models that are the result of a complete retraining of the\nmodel M using arbitrary settings from the predefined set of\nmodel changes.\nIn the related literature, model changes are defined as per-\nturbations of model parameters"}, {"title": "Definition 3 (d-robust counterfactual).", "content": "A counterfactual\nxcf is said to be d-robust if and only if it is robust to change\nto a model randomly drawn from the given admissible model\nspace MM with probability at least 8.\n$\\mathbb{P}(M'(x_{cf}) = M(x_{cf})) \\geq \\delta \\quad \\forall M' \\sim \\mathcal{M}_{\\mathcal{M}}$ (1)\nTherefore, the goal of generating CFE robust to model\nchange can be described as finding of such that it has the\nopposite class to the original one M(xorig) \u2260 M(xcf), and\npreserves it under model changes M(xcf) = M'(xcf) sam-\npled from the space of allowed model changes M' ~ MM\nwith a probability of at least 8."}, {"title": "3.2. Estimating robustness", "content": "The application of the above definition of counterfactual\nrobustness requires the estimation of the parameter \u03b4, i.e.\nthe true probability that the counterfactual is classified to\nthe given class M(xcf) by a model from MM. Note that\nunlike the classical probability P(y|x), which estimates the\nprediction confidence of a single model, the probability \u03b4\nmeasures the decision preservation over a space of models.\nWe adopted a Bayesian perspective on the estimation of 8 to\naccount for the estimation error. Since robustness to a given\nmodel change is a binary variable following a Bernoulli\ndistribution, we used the default prior for binary data, Beta\ndistribution, to model the confidence of the estimate 8. Re-\ncall that Beta is the conjugate prior of the Bernoulli distribu-\ntion (Gelman et al., 2013), which allows for much simpler\ncomputations. The adoption of the concept of credible in-\nterval, specifically its lower bound, leads to the following\ndefinition, which accounts for the estimation error."}, {"title": "Definition 4 ((\u03b4, \u03b1)-robust counterfactual).", "content": "A counterfac-\ntual xef is said to be (\u03b4, \u03b1)-robust if and only if it is robust\nto change to a model randomly drawn from the admissi-\nble model space M\u043c with probability at least 8 given the\nconfidence level a.\n$\\mathbb{P}(\\delta > \\underline{\\delta}) > \\alpha$ (2)\nwhere 8 follows the a posteriori distribution representing the\nuncertainty regarding the estimated probability of a binary\nrandom event [M'(x\u00a3f) = M(xcf)].\nIn simple words, (\u03b4, \u03b1)-robust CFE at a confidence level\nhas the probability of being robust of at least 8. This is rem-"}, {"title": "3.3. BETARCE - a post-hoc method for making CFEs\n(\u03b4, \u03b1)-robust", "content": "Multiple counterfactual construction methods have been\nproposed in the literature, and according to various studies,\nthe selection of the most appropriate counterfactual strongly\ndepends on user preferences (Stepka et al., 2024). Therefore,\nwe present BETARCE, a post-hoc approach that generates a\n(\u03b4, \u03b1)-robust counterfactual by making a small perturbation\nto the counterfactual xef constructed by a method selected\nby the user (to meet his expectations regarding the selected\nevaluation measures)."}, {"title": "Objective function", "content": "To define the objective function, we\nfirst introduce two auxiliary functions. A counterfactual xef\nis said to be valid whenever the underlying model classifies\nit to a different class than the original example xorig.\n$\\operatorname{valid}(x^{\u00a9f}, x_{orig}) = 1_{M(x_{cf})\\neq M(x_{orig})}$ (4)\nA counterfactual is (\u03b4, \u03b1)-robust whenever it positively"}, {"title": "Optimization algorithm", "content": "The above objective formula-\ntion (Eq. 6) is non-convex and non-differentiable, so any\nzero-order optimization algorithm could be used. Here we\nchose GROWINGSPHERES (Laugel et al., 2018) as it is a\nsimple and fast optimization method originally designed to\nfind adversarial examples that closely resemble the original\ninput while inducing sparse feature changes. It is therefore\nwell suited to finding a robust counterfactual that closely\nresembles the original one, especially since it can directly\noptimize our objective Eq. 6. However, we acknowledge\nthat many other zero-order optimization methods could be\nused to optimize that objective."}, {"title": "4. Experiments", "content": "In this section, we first outline the experimental setup\n(Sec. 4.1). Next, we empirically verify the applicability\nof our introduced theoretical framework (Sec. 4.2) and\nperform a sensitivity analysis of BETARCE hyperparam-\neters (Sec. 4.3). Finally, we compare the performance of\nBETARCE to other baseline methods (Sec. 4.4)."}, {"title": "4.1. Experimental setup", "content": "We conducted experiments\u00b3 on four datasets: HELOC,\nWine, Diabetes, and Breast Cancer, which are commonly\nused in related studies. Each dataset varies in size and\nnumber of attributes, but all are numerical to meet the re-\nquirements of the methods used later. Brief characteristics\nof these datasets are presented in Tab. 1.\nWe conducted experiments using two types of models: neu-\nral networks (Goodfellow et al., 2016) and gradient boosted\ntrees (LightGBM (Ke et al., 2017)), with the results of the\nlatter moved entirely to App. H. The neural network ar-\nchitecture comprised three layers, each consisting of 128\nneurons and ReLU (Nair & Hinton, 2010) activation func-\ntions. The training procedure utilized a binary cross-entropy"}, {"title": "4.2. Validation of BETARCE theoretical framework", "content": "In the first experiment, we validated whether the theoretical\nframework, from which we derived BETARCE, holds in\nconsidered experimental settings. To achieve this, we opted\nnot only to use the lower bound from our (\u03b4, \u03b1)-robustness\ndefinition (Def. 4), but also calculate the upper bound to\nextract more information from the method. In Fig. 2 we\npresent the results for two datasets, Diabetes and Wine.\nThe base CFEs were generated using GROWINGSPHERES.\nPlots for other datasets and DICE serving as a baseline CFE\ngeneration method are available in App. G.1.\nIn Fig. 2, the blue line with spherical points illustrates the\nEmpirical robustness obtained at various levels of d. The\nrobustness line lies between the green and red dashed lines\nwith square-shaped points, representing the average lower\nand upper bounds of the credible interval. This shows that\nthe (\u03b4, \u03b1)-robustness estimations are indeed valid and hold\nin practice. In addition, the plot also includes the results\nof two baselines, GROWINGSPHERES and DICE (yellow\nand black horizontal dotted lines), indicating a consistent\nimprovement in robustness over both of those baselines.\nOnly for the Diabetes dataset and Seed scenario, we notice\nthat the empirical robustness at 8 = 0.9 is slightly lower\nthan the estimated average lower bound (however still above\nthe 8). We attribute this discrepancy to the 95% confidence\nlevel of the method, which acknowledges that the estimate\nmay occasionally be incorrect."}, {"title": "4.3. Hyperparameter sensitivity analysis", "content": "In this section, we present the experiments assessing the\nimpact of using different confidence values a and different\nnumbers of estimators k. Selected results for varying a and\nk are shown in Fig 3 and Fig. 4, respectively6.\nThe results for different a values indicate that as a increases,\nthe empirical robustness tends to move further away from\nthe lower bound. This suggests that higher a values yield\nmore confident estimates with a lower likelihood of violating\nthe lower bound.\nRegarding the number of estimators k, we observed that\nincreasing this parameter leads to narrower credible interval\nwidths. It is anticipated because higher values of k result\nin a greater diversity of estimated Beta distribution shapes.\nSpecifically, the number of possible Beta distributions is\nconstrained by the combinations of all attainable a and b\nvalues, which increases with a larger k."}, {"title": "4.4. Comparative study with other methods", "content": "In the final experiment, we empirically compare the per-\nformance of BETARCE with several baselines, each repre-\nsenting a different type of method and aimed at achieving\ndifferent CFE properties (see Sec. 2). The first type of\nbaselines includes standard CFE generation methods: DICE\n(Mothilal et al., 2020), FACE (Poyiadzi et al., 2020), and\nGROWINGSPHERES (Laugel et al., 2018), which do not\nclaim to guarantee any robustness to model change. The\nnext type are end-to-end CFE generation methods, explicitly\ngenerating robust CFES: RBR (Nguyen et al., 2022) and\nROAR (Upadhyay et al., 2021). Finally, we also include\nRobX (Dutta et al., 2022); a method most similar to ours,\nas it also operates on top of a base CFE generation method\nin a post-hoc fashion. The above-mentioned robust base-\nlines aim to increase overall robustness of CFEs by moving\nthe CFE to a \"safer\" region in the feature space, requiring\nmanual selection of hyperparameters in a dataset-specific\nmanner. To ensure a fair comparison, for RBR and ROAR\n, we performed a hyperparameter search to find the most\npromising settings. In the case of ROBX, we chose four\ncombinations of the most relevant hyperparameters, 7 and\nvariance, where t values were selected using the histogram\ntechnique described by the authors, and variances were cho-\nsen to vary significantly (specifically 0.1 and 0.01). The\ncomprehensive description of the hyperparameter selection\nprocess is described in App. F.3. Note, that this comparison\nis not straightforward since each method is built on differ-\nent premises and has multiple parameters to tune, making\ncomparison to our probabilistic goals challenging."}, {"title": "5. Final Remarks", "content": "In this paper, we introduced BETARCE, a novel post-hoc,\nmodel-agnostic method for generating robust counterfactual\nexplanations. This method is the first proposal that pro-\nvides probabilistic guarantees on the robustness of CFEs\nto model change in a model-agnostic fashion. Moreover,\nits parametrization is tied to probabilistic expectations, en-\nabling users to select the expected robustness in a more\nnatural way compared to other approaches requiring exten-\nsive tuning of not-interpretable hyperparameters.\nThe experiments have confirmed that the introduced proba-\nbilistic bounds, estimated in practice with a set of models,\nhold for the Empirical Robustness, as demonstrated across\nthree types of model changes. Notably, counterfactuals gen-\nerated by BETARCE not only have improved robustness,\nbut are also closer to the base CFEs and retain their proper-\nties better than those generated by existing methods, while\nachieving a similar level of robustness.\nThere are several possible avenues for future research. First,\nour approach currently uses a fairly simple bootstrap esti-\nmation method of the second-order probability distribution\nof robustness. Despite the fact that our experiments con-\nfirmed the high utility of this estimation approach, the use of\nmore advanced estimation methods could lead to interesting\nresults. For example, one could investigate the use of auxil-\niary models to predict beta distribution parameters instead\nof querying the set of estimators. Additionally, the feasibil-\nity of using different and faster optimization algorithms to\nspeed up the optimization process could be investigated."}, {"title": "E. Examining BETARCE parameters in detail", "content": "In the main paper, we briefly outlined the relationship be-tween parameters in BETARCE . Remember, BETARCErelies on three internal parameters that impact its perfor-mance: 8, representing the lower bound for the probabilityof robustness; a, indicating the method's confidence level;and k, denoting the number of estimators. Their interplayis defined by the following equation, also featured in thepaper:\n$\\delta_{max} = F^{-1}_{Beta(a+k,b)}(1-\\alpha)$ (26)\nThis equation offers an intuitive approach to determining theparameters based on practical application requirements. Themaximum achievable 8 (and consequently (\u03b4, \u03b1)-robustness))is constrained by the number of estimators k and the selectedconfidence level a.\nThe interpretation of this equation is straightforward:\n$F^{-1}(1 \u2212 \u03b1)$ identifies the lower bound of robustness at1 \u2013 a. The inverse Cumulative Distribution Function ($F^{-1}$)is derived from the estimated Beta distribution Beta(a+k,b),with a and b representing default priors of the distribution.Here, k is added to the a parameter of the distribution, as itcontributes to the right-skewness of the distribution.\nTo provide a clearer understanding, below we present avisual representation of how parameters in the Beta distribu-tion influence its shape:"}, {"title": "F. Experimental setup", "content": "In this section, we provide more details on the implementa-tion of experiments."}, {"title": "F.1. General", "content": "For all experiments, we utilized a 3-fold cross-validationapproach, with 2 folds allocated for training and a singlefold for evaluation. During evaluation on each fold, werandomly sampled 30 instances for the generation of robust"}, {"title": "F.2. Datasets", "content": "Below, we present basic information about the datasets usedin our study."}, {"title": "F.3. Hyperparameters of the Baselines", "content": "Below, we present the hyperparameters that were searchedfor every end-to-end CFE generation method, both the stan-dard and robust ones:"}, {"title": "F.4. Models", "content": "In our experiments, we employ two models as the under-lying black-boxes: a neural network (NN) and LightGBM.The NN and LightGBM were implemented using the Py-Torch and lightgbm Python libraries, respectively. Valida-tion sets were used for early stopping in the NN and asthe evaluation set for LightGBM. Below, we provide moredetails on their specifications:"}, {"title": "F.4.1. NEURAL NETWORK", "content": null}, {"title": "F.4.2. LIGHTGBM", "content": null}, {"title": "G. BETARCE intrinsic analysis", "content": "In this section, we expand on the analysis presented in themain body of the paper regarding the impact of BETARCEparameters on various aspects of the method's performance."}]}