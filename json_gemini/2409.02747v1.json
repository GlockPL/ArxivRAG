{"title": "Tractable Offline Learning of Regular Decision Processes", "authors": ["Ahana Deb", "Roberto Cipollone", "Anders Jonsson", "Alessandro Ronca", "Mohammad Sadegh Talebi"], "abstract": "This work studies offline Reinforcement Learning (RL) in a class of non-Markovian\nenvironments called Regular Decision Processes (RDPs). In RDPs, the unknown\ndependency of future observations and rewards from the past interactions can\nbe captured by some hidden finite-state automaton. For this reason, many RDP\nalgorithms first reconstruct this unknown dependency using automata learning\ntechniques. In this paper, we show that it is possible to overcome two strong\nlimitations of previous offline RL algorithms for RDPs, notably RegORL [14].\nThis can be accomplished via the introduction of two original techniques: the\ndevelopment of a new pseudometric based on formal languages, which removes a\nproblematic dependency on LP-distinguishability parameters, and the adoption of\nCount-Min-Sketch (CMS), instead of naive counting. The former reduces the num-\nber of samples required in environments that are characterized by a low complexity\nin language-theoretic terms. The latter alleviates the memory requirements for long\nplanning horizons. We derive the PAC sample complexity bounds associated to\neach of these techniques, and we validate the approach experimentally.", "sections": [{"title": "1 Introduction", "content": "The Markov assumption is fundamental for most Reinforcement Learning (RL) algorithms, requiring\nthat the immediate reward and transition only depend on the last observation and action. Thanks to\nthis property, the computation of (near-)optimal policies involves only functions over observations\nand actions. However, in complex environments, observations may not be complete representations\nof the internal environment state. In this work, we consider RL in Non-Markovian Decision Processes\n(NMDPs). In these very expressive models, the probability of future observations and rewards\nmay depend on the entire history, which is the past interaction sequence composed of observations\nand actions. The unrestricted dynamics of the NMDP formulation is not tractable for optimization.\nTherefore, previous work in non-Markovian RL focus on tractable subclasses of decision processes.\nIn this work, we focus on Regular Decision Processes (RDPs) [11, 12]. In RDPs, the distribution of\nthe next observation and reward is allowed to vary according to regular properties evaluated on the\nhistory. Thus, these dependencies can be captured by a Deterministic Finite-state Automaton (DFA).\nRDPs are expressive models that can represent complex dependencies, which may be based on events\nthat occurred arbitrarily back in time. For example, we could model that an agent may only enter a\nrestricted area if it has previously asked for permission and the access was granted.\nDue to the properties above, RL algorithms for RDPs are very general and applicable. However,\nprovably correct and sample efficient algorithms for RDPs are still missing. On one hand, local\noptimization approaches are generally more efficient, but lack correctness guarantees. In this group,"}, {"title": "2 Preliminaries", "content": "Notation Given a set Y, \u2206(V) denotes the set of probability distributions over Y. For a function\nf : X \u2192 \u2206(V), f(y | x) is the probability of y \u2208 Y given x \u2208 X. Further, we write y ~ f(x) to\nabbreviate y ~ f(\u00b7 | x). Given an event E, I(E) denotes the indicator function of E, which equals\n1 if E is true, and 0 otherwise. For any pair of integers m and n such that 0 < m < n, we let\n[m, n] := {m, ..., n} and [n] := {1, ..., n}. The notation \u00d5(\u00b7) hides poly-logarithmic terms.\nCount-Min-Sketch Count-Min-Sketch, or CMS [16], is a data structure that compactly represents\na large non-negative vector v = [v1,..., vm]. CMS takes two parameters \u03b4\u03b5 and \u025b as input, and\nconstructs a matrix C with d = [log ] rows and w = [] columns. For each row j \u2208 [d], CMS\npicks a hash function hj : [m] \u2192 [w] uniformly at random from a pairwise independent family [44].\nInitially, all elements of v and C equal 0. An update (i, c) consists in incrementing the element vi\nby c > 0. CMS approximates an update (i, c) by incrementing C(j, hj(i)) by c for each j \u2208 [d]. At\nany moment, a point query vi returns an estimate of vi by taking the minimum of the row estimates,\ni.e. v\u2081 = min; C(j, hj(i)). It is easy to see that vi \u2265 vi, i.e. CMS never underestimates vi."}, {"title": "2.1 Languages and operators", "content": "An alphabet \u0393 is a finite non-empty set of elements called letters. A string over \u0393 is a concatenation\na1ae of letters from \u0393, and we call l its length. In particular, the string containing no letters,\nhaving length zero, is a valid string called the empty string, and is denoted by \u039b. Given two strings\nx1 = a1ae and x2 = b\u2081bm, their concatenation x1x2 is the string a1 \u2022 ab\u2081 bm. In\nparticular, for any given string x, we have xx = dx = x. The set of all strings over alphabet \u0393 is\nwritten as \u0393*, and the set of all strings of length l is written as \u0393\". Thus, \u0393* = Ul\u2208NF. A language\nis a subset of \u0393*. Given two languages X\u2081 and X2, their concatenation is the language defined by\nX1X2 = {X1X2 | X1 \u2208 X1, X2 \u2208 X2}. When concatenating with a language {a} containing a single\nstring consisting of a letter a \u2208 \u0393, we simply write Xa instead of X{a}. Concatenation is associative\nand hence we can write the concatenation X1 X2 Xk of an arbitrary number of languages.\nGiven the fundamental definitions above, we introduce two operators to construct sets of languages.\nThe first operator C is defined for any two non-negative integers land k \u2208 {1, . . ., l}, it takes a set\""}, {"title": "2.2 Episodic regular decision processes", "content": "We first introduce generic episodic decision processes. An episodic decision process is a tuple\nP = (O,A,R,T, R, H), where O is a finite set of observations, A is a finite set of actions,\nRC [0, 1] is a finite set of rewards, and H > 1 is a finite horizon. We frequently consider the\nconcatenation AO of the sets A and O. Let H\u2081 = (AO)t+1 be the set of histories of length t + 1, and\nlet em:n \u2208 Hn-m denote a history from time m to time n, both included. Each action-observation\npair ao \u2208 AO in a history has an associated reward label r \u2208 R, which we write ao/r \u2208 AO/R. A\ntrajectory eo:T is the full history generated until (and including) time T.\nWe assume that a trajectory eo:T can be partitioned into episodes el:l+H \u2208 HH of length H + 1,\nIn each episode eo:H, ao = a\u2081 is a dummy action used to initialize the distribution on Ho. The\ntransition function T : H \u00d7 A \u2192 \u25b3(O) and the reward function R : H \u00d7 A \u2192 \u2206(R) depend on the\ncurrent history in H = UoHt. Given P, a generic policy is a function \u03c0: (\u0391\u039f)* \u2192 \u0394(A) that\nmaps trajectories to distributions over actions. The value function V\u2122 : [0, H] \u00d7 H \u2192 R of a policy\n\u03c0is a mapping that assigns real values to histories. For h \u2208 H, it is defined as V\u2122 (H, h) := 0 and\n$V^{\\pi} (t, h) := E_{\\substack{a_i\\sim \\pi\\Vt < H, \\forall h\\in H_t.}}( \\sum_{i=t+1}^{H}r_i | h, \\pi )$.\nFor brevity, we write V\u2122 (h) := V\" (t, h). The optimal value function V* is defined as V*(h) :=\nsup V (h), Vt \u2208 [H],\u2200h \u2208 Ht, where sup is taken over all policies \u03c0 : (AO)* \u2192 \u2206(A). Any\npolicy achieving V* is called optimal, which we denote by \u03c0*; namely V** = V*. Solving P\namounts to finding \u03c0*. In what follows, we consider simpler policies of the form \u03c0 : \u0397 \u2192 \u0394(\u0391)\nmapping finite histories to distributions over actions. Let \u03a0\u2081 denote the set of such policies. It\ncan be shown that \u03a0\u04a3 always contains an optimal policy, i.e. V\u2081*(h) := max\u03c0\u03b5\u03c0\u03b7 V(h), Vt \u2208\n[H], \u2200h \u2208 Ht. An episodic MDP is an episodic decision process whose dynamics at each timestep t\nonly depends on the last observation and action [51].\""}, {"title": "Episodic RDPs", "content": "Episodic RDPs An episodic Regular Decision Process (RDP) [1, 11, 12] is an episodic decision pro-\ncess R = (O, A,R,T, R, H) described by a finite transducer (Moore machine) (\u03a9, \u03a3, \u03a9, \u03c4, \u03b8, qo),\nwhere Q is a finite set of states, \u2211 = AO is a finite input alphabet composed of actions and ob-\nservations, \u03a9 is a finite output alphabet, T : Q \u00d7 \u2211 \u2192 Q is a transition function, 0 : Q \u2192 \u03a9 is\nan output function, and qo \u2208 Q is a fixed initial state [43, 57]. The output space \u03a9 = \u03a9\u3002\u00d7 \u03a9\nconsists of a finite set of functions that compute the conditional probabilities of observations and\nrewards, on the form \u03a9\uff61C A \u2192 \u2206(0) and & C A \u2192 \u2206(R). For simplicity, we use two output"}, {"title": "Occupancy and distinguishability", "content": "Occupancy and distinguishability Given a regular policy \u03c0 : Q \u2192 \u2206(A) and a time step\nt\u2208 [0, H], let d\u2208 \u2206(Qt \u00d7 AO) be the induced occupancy, i.e. a probability distribution over the\nstates in Qt and the input symbols in A O, recursively defined as do (qo, 2000) = \u03b8\u03bf(00 | 90, ao) and\n$d_t (q_t, a_tot) = \\sum_{(q,ao) \\in \\tau^{-1}(q_t)} d_{t-1}(q, a_o) \\cdot\\pi(at | q_t) \\cdot\\theta_o(o_t | q_t, a_t), t > 0$.\nOf particular interest is the occupancy distribution d := d\nassociated with an optimal policy \u03c0*.\nLet us assume that \u03c0* is unique, which further implies that d is uniquely defined\u00b9.\nConsider a minimal RDP R with states Q = Ut\u2208[0,H+1] Qt. Given a regular policy \u03c0\u2208 IR and a\ntime step t \u2208 [0, H], each RDP state q \u2208 Qt defines a unique probability distribution P(\u00b7 | qt = q, \u03c0)\non episode suffixes in EH\u2212t = (AO/R)H-t+1. The states in Qt can be compared in terms of the\nprobability distributions they induce over EH\u2212t. Consider any L = {Le}=0, where each Le is a\nmetric over (Ee). We define the L-distinguishability of R and \u03c0 as the maximum \u03bc\u03bf \u2265 0 such that,\nfor any t \u2208 [0, H] and any two distinct q, q' \u2208 Qt, the probability distributions over suffix traces\net:H \u2208 El from the two states satisfy\n$L_{H-t}(P(e_{t:H}|q_t = q, \\pi), P(e_{t:H} | q_t = q', \\pi)) \\geq \\mu_o$.\nWe will often omit the remaining episode length l = H t from Le and simply write L. We\nconsider the LP-distinguishability, instantiating the definition above with the metric LP (P1, P2) ="}, {"title": "3 Learning RDPs with state-merging algorithms from offline data", "content": "Here we describe an algorithm called ADACT-H [14] for learning episodic RDPs from a dataset of\nepisodes. The algorithm starts with a set D of episodes generated using a regular behavior policy \u03c0\u266d,\nwhere the k-th episode is of the form eo:H = abo/ro/r and where, for each t \u2208 [0, \u0397],\n$q_t^k = q_o, a_t^k \\sim \\pi_\\flat (q_t^k), o_t^k \\sim \\theta_o(a_t^k), r_{t+1}^k \\sim R(q_t^k,a_t^k), q_{t+1}^k=\\tau (q_t^k,a_t^k)$.\nNote that the behavior policy \u03c0\u266d and underlying RDP states qf are unknown to the learner. The\nalgorithm is an instance of the PAC learning framework and takes as input an accuracy \u03b5\u2208 (0, H] and\na failure probability \u03b4 \u2208 (0, 1). The aim is to find an \u025b-optimal policy \u4e93 satisfying Vo* (h)-V (h) \u2264 \u03b5\nfor each h\u2208 Ho with probability at least 1 \u03b4, using the smallest dataset D possible.\nSince ADACT-H performs offline learning, it is necessary to control the mismatch in occupancy\nbetween the behavior policy \u03c0\u266d and the optimal policy \u03c0*. Concretely, the single-policy RDP\nconcentrability coefficient associated with RDP R and behavior policy \u03c0\u266d is defined as [14]:\n$C_R^\\pi = \\max_{\\substack{t\\in[H],q\\in Q_t,ao\\in AO}} \\frac{d_t^*(q, ao)}{d_t(q, ao)}$.\nAs [14], we also assume that the concentrability is bounded away from infinity, i.e. that Cr <\u221e.\nADACT-H is a state-merging algorithm that iteratively constructs the set of RDP states Q1, ..., QH\nand the associated transition function \u03c4. For each t \u2208 [0, H], ADACT-H maintains a set of candidate\nstates qao \u2208 Qt\u22121 \u00d7 AO. Each candidate state qao has an associated multiset of suffixes Z(qao) =\n{\u0435\u043d: \u0435\u043a\u2208 D, (eo:t-1) = 9,-10-1 = ao}, i.e. episode suffixes whose history is consistent\nwith qao. To determine whether or not the candidate qao should be promoted to Qt or merged with an\nexisting RDP state qt, ADACT-H compares the empirical probability distributions on suffixes using\nthe prefix distance LP defined earlier. For reference, we include the pseudocode of ADACT-H(D, \u03b4)\nin Appendix A. Cipollone et al. [14] prove that ADACT-H(D, \u03b4) constructs a minimal RDP R with\nprobability at least 1 4AOQ6.\nIn practice, the main bottleneck of ADACT-H is the statistical test on the last line,\n$L_p(Z_1, Z_2) \\geq \\sqrt{\\frac{1}{2}log(8(ARO)^{H-t}/\\delta)}/min(|Z_1|, |Z_2|)$,\nsince the number of episode suffixes in Ee is exponential in the current horizon l. The purpose\nof the present paper is to develop tractable methods for implementing the statistical test. These\ntractable methods can be directly applied to any algorithm that performs such statistical tests, e.g., the\napproximation algorithm ADACT-H-A [14]."}, {"title": "4 Tractable offline learning of RDPs", "content": "The lower bound derived in Cipollone et al. [14] shows that sample complexity of learning RDPs is\ninversely proportional to the Li-distinguishability. When testing candidate states of the unknown\nRDP, LP is the metric that allows maximum separation between distinct distributions over traces.\nUnfortunately, accurate estimates of L\u2081 are impractical to obtain for distributions over large supports\u2014\nin our case the number of episode suffixes which is exponential in the horizon. Accurate estimates of\nLP are much more practical to obtain. However, there are instances for which states can be well\nseparated in the L\u2081-norm, but have an LP-distance that is exponentially small. To address these\nissues, in this section we develop two improvements over the previous learning algorithms for RDPs."}, {"title": "4.1 Testing in structured languages", "content": "The language of traces Under a regular policy, any RDP state Qt uniquely determines a distribution\nover the remaining trace atot/rt... \u0430\u043d\u043e\u043d/\u0442\u043d \u2208 (AO/R)H-t+1. Existing work [14, 55, 56] treats"}, {"title": "5 Experimental Results", "content": "In this section we present the results of experiments with two versions of ADACT-H: one that\nuses CMS to compactly store probability distributions on suffixes, and one that uses the restricted\nlanguage family X1,1,1. We compare against FlexFringe [7], a state-of-the-art algorithm for learning\nprobabilistic deterministic finite automata, which includes RDPs as a special case. To approximate\nepisodic traces, we add a termination symbol to the end of each trace, but FlexFringe sometimes\nlearns RDPs with cycles. Moreover, FlexFringe uses a number of different heuristics that optimize\nperformance, but these heuristics no longer preserve the high-probability guarantees. Hence the\nautomata output by FlexFringe are not always directly comparable to the RDPs output by ADACT-H.\nWe perform experiments in five domains from the literature on POMDPs and RDPs: Corridor [55], T-\nmase [3], Cookie [64], Cheese [42] and Mini-hall [36]. Appendix D contains a detailed description of\neach domain, as well as example automata learned by the different algorithms. Table 1 summarizes the\nresults of the three algorithms in the five domains. We see that ADACT-H with CMS is significantly\nslower than FlexFringe, which makes sense since FlexFringe has been optimized for performance.\nCMS compactly represents the probability distributions over suffixes, but ADACT-H still has to\niterate over all suffixes, which is exponential in H for the LP distance. As a result, CMS suffers from\nslow running times, and exceeds the alloted time budget of 1800 seconds in the Mini-hall domain.\nOn the other hand, ADACT-H with the restricted language family X1,1,1 is faster than FlexFringe\nin all domains except T-maze, and outputs smaller automata than both FlexFringe and CMS in all\ndomains except Mini-hall. The number of languages of X1,1,1 is linear in the number of actions,\nobservation symbols, and reward values, so the algorithm does not have to iterate over all suffixes,\nand the resulting RDPs better exploit the underlying structure of the domains. In Corridor and\nCookie, all algorithms learn automata that admit an optimal policy. However, in T-maze, Cheese and\nMini-hall, the RDPs learned by ADACT-H with the restricted language family X1,1,1 admit a policy\nthat outperforms those of FlexFringe and CMS. As mentioned, the heuristics used by FlexFringe are\nnot optimized to preserve reward, which is likely the reason why the derived policies perform worse."}, {"title": "6 Conclusion", "content": "In this paper, we propose two new approaches to offline RL for Regular Decision Processes and pro-\nvide their respective theoretical analysis. We also improve upon existing algorithms for RDP learning,\nand propose a modified algorithm using Count-Min-Sketch with reduced memory complexity. We\ndefine a hierarchy of language families and introduce a language-restricted approach, removing the\ndependency on LP-distinguishability parameters and compare the performance of our algorithms"}, {"title": "A Pseudocode of ADACT-H", "content": ""}, {"title": "B Pseudometrics", "content": ""}, {"title": "C Proof of theorems", "content": ""}, {"title": "D Details of the experiments", "content": ""}]}