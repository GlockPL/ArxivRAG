{"title": "LEVERAGING COUNTERFACTUAL CONCEPTS FOR DEBUGGING\nAND IMPROVING CNN MODEL PERFORMANCE", "authors": ["Syed Ali Tariq", "Tehseen Zia"], "abstract": "Counterfactual explanation methods have recently received significant attention for explaining CNN-\nbased image classifiers due to their ability to provide easily understandable explanations that align\nmore closely with human reasoning. However, limited attention has been given to utilizing explain-\nability methods to improve model performance. In this paper, we propose to leverage counterfactual\nconcepts aiming to enhance the performance of CNN models in image classification tasks. Our pro-\nposed approach utilizes counterfactual reasoning to identify crucial filters used in the decision-making\nprocess. Following this, we perform model retraining through the design of a novel methodology\nand loss functions that encourage the activation of class-relevant important filters and discourage\nthe activation of irrelevant filters for each class. This process effectively minimizes the deviation of\nactivation patterns of local predictions and the global activation patterns of their respective inferred\nclasses. By incorporating counterfactual explanations, we validate unseen model predictions and\nidentify misclassifications. The proposed methodology provides insights into potential weaknesses\nand biases in the model's learning process, enabling targeted improvements and enhanced perfor-\nmance. Experimental results on publicly available datasets have demonstrated an improvement of\n1-2%, validating the effectiveness of the approach.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) models have achieved remarkable progress in computer vision, particularly in image classification\ntasks using convolutional neural network (CNN) Krizhevsky et al. [2012], He et al. [2016], Tan and Le [2019]. However,\ntheir lack of transparency and interpretability remains a challenge Rudin [2019], Arrieta et al. [2020]. To tackle this\nissue, various explainable artificial intelligence (XAI) techniques have been introduced that aim to explain the decisions\nof DL models. These techniques include saliency maps Wang et al. [2019], activation visualization Selvaraju et al.\n[2017], concept attribution Wu et al. [2020], and counterfactual explanations (CEs) Wang and Vasconcelos [2020],\namong others. CEs have emerged as a promising approach for explaining CNN-based image classifiers due to their\nability to provide easy-to-understand explanations that are aligned more closely with the way humans reason Byrne\n[2019].\nDespite these advancements, limited attention has been given to leveraging explanations and model interpretability to\nenhance model performance and detect potential data biases. In various domains such as medical diagnosis, autonomous\ndriving, and facial recognition, the need for highly accurate and explainable DL models is crucial, which can impact\ntheir trustworthiness and adoption in real-world settings Rudin [2019]. It is essential to explore novel methods that\nprovide both accuracy and interpretability in order to improve model performance and trustworthiness."}, {"title": "2 Related Work", "content": "In the literature, there exist several XAI and model debugging techniques for analyzing and improving Deep Learning\n(DL) models Feng et al. [2022], Leclerc et al. [2022], Abid et al. [2022], Alsallakh et al. [2021], Tariq et al. [2022].\nA brief overview of these works is as follows. In Feng et al. [2022], the authors developed a method called \"Model\nDoctor\" that automatically diagnoses and treats CNN models. They accomplish this by finding correlations between\npredicted classes and the convolution kernels active in the decision-making process. The authors conclude that only a\nsparse set of filters from deeper layers are mostly responsible for making accurate predictions. They propose treating\nthe classifier by aggregating gradients to find correlations between classes and filters and applying constraints on filters\nto minimize incorrect activations. This leads to an improved performance of 1-5%, as reported by the authors.\nIn Leclerc et al. [2022], a debugging framework for computer vision models is proposed. It allows users to select and\nconfigure various transformations on the inputs to the model. The framework then performs analysis on the input space\nto identify failure cases and conducts global as well as per-object analysis.\nIn Abid et al. [2022], the authors propose a counterfactual concepts-based debugging method for classifiers. This\nmethod learns important concepts from limited training data and uses them to validate misclassified samples. It assigns\ndifferent scores to different concepts, which would lead to the model correcting its errors.\nIn Alsallakh et al. [2021], the authors explore the internal workings of convolutional neural networks (CNNs) to debug\nmodels. They analyze convolution operations and find that they can result in artifacts and noise appearing on feature\nmaps, as well as weights of the convolutional filters. The authors present a visual debugging method to visualize these"}, {"title": "3 Proposed methodology", "content": "The objective of the proposed work is to identify biases in the model decisions that lead to misclassifications and\ndevelop a model debugging method to improve model performance on misclassifications. To accomplish these goals,\nwe build upon the existing counterfactual explanation (CFE) model that identifies counterfactual filters to explain model\ndecisions Tariq et al. [2022]. The CFE model works by predicting a set of minimum correct (MC) and minimum\nincorrect (MI) filters necessary to (i) maintain the prediction of the image to the original inferred class by the classifier\nand (ii) alter the classifier's decision to a chosen target class.\nIn the proposed work, we use the MC filters to identify the globally most important filters for training images, enabling\nus to perform misclassification analysis and model debugging on unseen test images. Fig. 1 presents a block diagram of\nour proposed model, illustrating the various phases involved. In the counterfactual filter identification phase, we extract\nthe MC filters to provide counterfactual explanations for the classifier's decisions. Subsequently, in the misclassification\nanalysis phase, we accumulate the class-specific filters identified for each training image, along with their respective\ninferred class if it is the true class. This global class-specific filter set represents each class's most important filters. To\ndetect misclassifications in new test images, we measure the level of similarity or agreement between the activated\nMC filters for the test images and their inferred class, comparing them with the corresponding global MC filters. By\nassessing the degree of agreement, we can determine whether the inferred class is likely to be a misclassification or not."}, {"title": "4 Results", "content": "This section presents the results and discussion of the proposed model debugging method, organized as follows. The\nexperimental setup is described in section 4.1. In section 4.2, we identify the class-relevant filters using the CFE model\nand perform automatic misclassification identification. Model treatment is presented in section 4.3, where the proposed\nmodel debugging method is applied to re-train and improve the model. Detailed analysis and interpretation of the results\nare presented in section 4.4, while section 4.5 presents a closing discussion on the results section."}, {"title": "4.1 Experimental setup", "content": "For the evaluation of the proposed model debugging method, we used VGG-16 Simonyan and Zisserman [2014] model\ntrained on the Caltech-UCSD Birds (CUB) 2011 Wah et al. [2011] dataset. The VGG-16 model was originally trained\nby transfer learning using imageNet Russakovsky et al. [2015] pre-trained weights followed by fine-tuning and achieved\ntraining and testing accuracy of 99.0% and 69.5%, respectively.\nTo debug the VGG-16 model, we utilize the counterfactual explanation model (CFE) Tariq et al. [2022], which provides\ncontrastive and counterfactual explanations of the decisions made by a pre-trained model. The CFE model predicts the\nminimum correct (MC) and minimum incorrect (MI) filters for each decision by a pre-trained model with respect to the\noriginal inferred class (source class) and some target alter class. In the proposed work, we build upon the CFE model\nwork to detect weak and faulty filters in the pre-trained VGG-16 model that lead to dataset bias and misclassifications\nand rectify these issues to improve model accuracy. The CFE model creation and training details are provided in Tariq\net al. [2022], and we use the best performing CFE model with hyperparameters defined in Tariq et al. [2022], i.e., we\nuse the CFE model with logits loss enabled and with sparsity loss value of x = 2."}, {"title": "4.2 Misclassification analysis", "content": "The identification of class-relevant important filters is performed using the CFE model that is used to predict the MC\nfilters from the VGG-16 model for each training image of a class. To ensure only high-quality and class-relevant filters\nare detected, we skip misclassifications and low confidence predictions having probability score of p < 90%. The\nactivation magnitudes of the predicted MC filters are accumulated and normalized for each training image of a class.\nThese global MC filters represent the most crucial features or concepts relevant to their respective classes. It is shown in\nTariq et al. [2022] that disabling these MC filters significantly reduces the class recall of their classes without notably\naffecting the overall model accuracy.\nFollowing this, we perform misclassification analysis on individual test images passed to the CFE model to compute the\nMC filters for the respective inferred classes. The MC filters for each prediction to the inferred class are compared with\nthe globally important class-specific filters to compute an agreement between the predicted MC filters and class-specific\nfilters. The higher the agreement between the local and global MC filters, the more reliable the prediction to the\ninferred class. This is a way of establishing a trust factor for each prediction made by the VGG-16 model towards the\ninferred class. If the agreement between the filters is below a given threshold, the decision is considered to be a possible\nmisclassification. The results of this analysis are presented in Table 1. VGG-16 model was used to predict 5,764 test\nimages of the CUB dataset Wah et al. [2011]. Out of these, the model correctly predicted 69.5% correctly, resulting in\n1769 misclassifications. For each of the 5,764 test images, we use different metrics to compute the agreement between\nthe activated filters and global MC filters for the inferred class. These include average recall, average F1 score, and\nthresholded recall. We use a skip threshold to skip high confidence prediction above the threshold. We use frequency\nthreshold on the global MC filters to consider only those filters for comparison that have been activated at least the\nspecified amount of times out of the maximum number of times a filter is activated for a particular class. Based on these\nconditions, we check whether a local prediction is likely to be misclassification if the specified metric score computed\nbetween the locally activated filters and global MC filters is below the average score threshold for the specified metric\nfor the training set images. In Table 1, using the average recall metric results in the successful identification of 31% of\nthe total misclassifications while 256 new misclassification are introduced due to the condition. And using the average\nF1 score metric, 38% of the misclassifications are detected."}, {"title": "4.3 Model treatment and debugging", "content": "To treat the model weaknesses and reduce the number of misclassifications, we perform debugging to improve model\nperformance. In this phase, we re-train the model with additional constraints to encourage MC filters activation for\ninferred classes and discourage non-MC filter activation for all other classes. The model debugging results are presented\nin Table 2. The table shows training and testing accuracies achieved when performing debugging using different\n\u51651 and 2 weights assigned to the loss maximizing the agreement with MC filters and minimizing agreement with\nnon-MC filters as shown in Eq. 8. The model debugging process is able to improve the test accuracy by almost 1.2%\nfrom 69.48% of the base model to 70.64%. To show the impact of the proposed optimisation, we compare the model\nimprovement without incorporating the additional debugging constraints. In this default setting, re-training the base\nmodel in a similar way to debugging method, we are able to see an improved accuracy of 70.2%, which is 0.4% less\nthan the debugging method. This shows that by encouraging MC filters activation, we are able to achieve a better model\nperformance than fine-tuning the base model."}, {"title": "4.4 Qualitative analysis", "content": "This section provides a qualitative analysis of the model debugging method based on finding classes for which the\naccuracy improved or decreased and visualizing cases from these classes. Table 3 shows the top 3 classes with the most\nimproved and decreased class recall scores resulting from the model debugging process. It is seen that a significant\nimprovement of 40% recall is observed for the 'Boat-tailed Grackle' class, while the 'American Crow' class is the worst\naffected with a decrease of 20%. The reason for the decrease in class recall could be that initially, the model relies on\nbiases to achieve the specified accuracy that may be mistakenly classified to the correct class. Now, when we perform"}, {"title": "4.5 Discussion and limitations", "content": "Based on the misclassification analysis presented in section 4.2, we can conclude that although a lot of possible\nmisclassifications are identified, many are still not identified. This suggests that filters activated in high-confidence test\ncases are not well aligned with global MC filters identified from the training set. This, in turn, indicates that the model\nis overfitting on training data, which is also evident from the large gap in training and testing accuracy of the VGG-16\nmodel on the CUB dataset. Furthermore, results in section 4.3 demonstrated that by encouraging the model to predict\nthe class-relevant filters and ignoring other filters, it is possible to improve the accuracy of the model by aligning the\ndecision-making process towards class-specific concepts identified using counterfactual reasoning.\nOne of the drawbacks of the proposed method is that its ability to identify class-relevant important filters used for\naligning model decisions and debugging is dependent on how accurate the original model was on the classification\ntask. If the model was less accurate, then the MC filters would be less faithful to the model's decision-making process."}, {"title": "5 Conclusion", "content": "This paper presented a method to improve the performance of CNN-based image classifiers and perform misclassification\nanalysis utilizing explainability methods. We proposed a novel approach that leverages counterfactual concepts by\nidentifying crucial filters used in the decision-making process and retraining models to encourage the activation of\nclass-relevant important filters and discourage the activation of irrelevant filters. Through this process, we minimized\nthe deviation of activation patterns, aligning local predictions with the global activation patterns of their respective\ninferred classes. Experimental results on publicly available datasets confirmed the effectiveness of our approach, with\nan observed improvement of 1-2%."}]}