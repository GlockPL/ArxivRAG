{"title": "Program Synthesis Dialog Agents for Interactive Decision-Making", "authors": ["Matthew Toles", "Nikhil Balwani", "Rattandeep Singh", "Valentina Giulia Sartori Rodriguez", "Zhou Yu"], "abstract": "Many real-world eligibility problems, ranging from medical diagnosis to tax planning, can be mapped to decision problems expressed in natural language, wherein a model must make a binary choice based on the features of the user. Large-scale domains such as legal codes or frequently updated funding opportunities render human annotation (e.g., web forms or decision trees) impractical, suggesting a need for agents that can automatically assist in decision-making. Since relevant information is often only known to the user, it is important that these agents can ask the right questions. Since agents determine when to terminate a conversation, they face a trade-off between accuracy and total questions asked, a useful metric for both user experience and cost. To evaluate this task, we propose BeNYfits, a new benchmark for determining user eligibility for multiple overlapping social benefits opportunities through interactive decision-making. Our experiments show that current language models struggle with frequent hallucinations, with GPT-40 scoring only 35.7 F1 using a ReAct-style chain-of-thought. We therefore introduce ProADA, a novel approach that uses program synthesis to assist in decision-making by mapping dialog planning to a code generation problem and using gaps in structured data to determine the best next action. Our agent, ProADA, improves the F1 score to 55.6 while using nearly the same number of dialog turns.", "sections": [{"title": "Introduction", "content": "The improved capabilities of large language models have refocused attention away from traditional benchmarks and towards real-world tasks where automated systems could broadly benefit the public, such as improving access to public services. Many such opportunities require determining the user's eligibility based on the user's features and the task at hand, formally referred to as decision problems. In adaptive decision scenarios, where information is revealed iteratively (e.g., medical diagnosis), one also wishes to minimize the number of queries. For many important decision problems, the requirements are written in natural language, which must be translated into formal language or interpreted logically. Also, critical information may be user-specific and known only to certain people, meaning that it often must be requested through dialog. Finally, the diversity of problems in the real world places a premium on whether agents can generalize information gathering and logical reasoning to new domains.\nUser-facing decision problems have traditionally been solved using hard-coded forms (as in the American tax filing software TurboTax\u00b9) or dialog trees (as in video games). However, hard-coded solutions struggle to generalize or extend to web-scale decision problems; opportunities found by web-scraping, crowd-sourcing, or from extremely large corpora such as national tax codes may be challenging to formalize, let alone decide on in real-time. Methods to approach adaptive decision problems include multi-armed bandits, reinforcement learning, dynamic programming, and decision trees, but adapting these strategies to online natural language tasks is not trivial. Recently, large language models have improved on a wide range of related tasks. However, language models are known to struggle with reasoning over long contexts and hallucinating unstated information, both of which, as we will see, are common in adaptive decision problems.\nTo evaluate interactive decision-making, in Section 2 we introduce BeNYfits, a language model agent task for determining user eligibility for real-world public benefits opportunities with overlapping eligibility requirements. 1. In the single-opportunity scenario, the assistant agent could simply repeat the requirements and ask the user if they"}, {"title": "BeNYfits: An Agent Benchmark for Public Benefit Eligibility Decisions", "content": "The determination of eligibility for many public opportunities, such as tax credits, scholarships, research funding opportunities, business incentives, charities, job listings, and social services, can be quality. However, BeNYfits' overlapping requirements present an interesting optimization challenge for dialog planning: how should models \u201cmerge\" eligibility requirements to avoid duplicating questions and maximize information gain? We find that current large language models, including GPT-40, struggle to perform significantly better than chance at determining user eligibility, suffering from hallucination, poor reasoning under uncertainty, over-confidence, and lost-in-the-middle problems observed in prior work (Huang et al., 2024).\nGiven these weaknesses, we introduce a method for an agent that, given a natural language description of the user-facing decision problem, generates a program to request user input conversationally to solve the problem. Specifically, we construct an agent consisting of a code module, which conducts dialog planning in the form of a Python program, and a dialog module, which asks questions based on the program state. The agent then uses the dialog module to parse the user's response into structured data. Our approach exploits a distinction that we observe between how dialog and code generation models handle long-range planning and uncertainty. Effectively, the formality and long-range conditional dependencies present in program synthesis training make them strong candidates for dialog planning. As a key contribution, in Section 3 we present Program Synthesis Adaptive Decision Agent, or ProADA, an agent that, given a natural language policy of a decision problem, generates Python code to structure the decision-making process and request minimal user input to make the correct decision.\nOur main contributions are as follows.\n1. A novel agent benchmark for adaptive decision-making in dialog measuring agent accuracy and dialog turn efficiency in helping users determine eligibility for public, real-world opportunities.\n2. A general and effective agent for adaptive decision-making in dialog that exploits program synthesis and tool use to plan dialog and adaptively request user information, improving both F1 score and dialog completion speed."}, {"title": "Efficiency, Generalization, and User Experience", "content": "Traditional methods for determining eligibility present several opportunities for improvement by intelligent agent assistants. For small numbers of opportunities, we might convert natural language requirements into a web form or static chatbot dialog tree serving as an interface for hard-coded checking logic, similar to TurboTax. However, this approach has several drawbacks. First, many eligibility requirements are updated without notice, often annually, meaning eligibility performance will degrade over time without ongoing maintenance. Second, opportunities may be crowdsourced or scraped from the Web dynamically, rendering manual coding impractical in favor of more generalizable, lower-latency solutions, such as language agents. Moreover, due to the specificity of eligibility requirements, a potential applicant may need to find and examine many opportunities before discovering one for which they qualify. Furthermore, requirements between similar opportunities frequently overlap, forcing users to answer the same questions repeatedly. On the other hand, users may waste a lot of time if they discover that they are ineligible very late in the examination process, presenting an opportunity for adaptive decision-making algorithms. An intelligent agent, however, should adaptively query the user for only the minimum necessary information, saving time, and improving user experience.\nIn BeNYfits, an agent must interact with a simulated user to determine their household's eligibility for multiple overlapping benefits opportunities based only on the opportunity requirements and conversation with the user. We define the task as follows. Given a set of opportunities, each with a unique set of eligibility requirements, determine whether the user is eligible for each of them in the minimum number of dialog turns (Figure 1). We simulate a user by prompting a language model with detailed information about themselves and their household. Each simulated user is interested in a subset of all opportunities. Assistant agents possess the natural language eligibility requirements for those opportunities and must determine the simulated user's eligibility by asking a series of questions. After each dialog turn, the agent determines whether it is ready to make a decision and, if so, outputs a final eligibility prediction for each opportunity."}, {"title": "Opportunity Requirements", "content": "We source the plain English eligibility requirements for 82 benefits opportunities from NYC Open Data2. We minimally edit requirements manually to remove ambiguity (\"may be eligible\"), future expectations (\"can commit to\"), and dates (\"since 2023\"). Opportunities include tax credits, youth programming, housing, nutrition assistance, healthcare, parental services, and career advancement, among other categories. Eligibility requirements range from broad (State ID card: all residents age 10+) to extremely specific (Air conditioner subsidy: ten independent requirements). Opportunities may apply to either the individual or the household as a whole, offering additional logical complexity. Opportunities depend on 1-18 unique user features each (mean: 4.66 standard deviation: 3.56). Each user feature appears in 1-52 opportunities (mean: 3.25, standard deviation: 6.66), falling on a long-tailed distribution (Figure 2). We define a household as eligible for an opportunity if any of its members are eligible."}, {"title": "User Simulation", "content": "For each opportunity, we enumerate relevant user features (age, income, number of dependents, etc.). We create simulated user households by randomly sampling each feature for each member, with up to 6 members per household. Features are independently sampled, except when subject to constraints preventing illogical combinations (5-year-old grandparents, adults in foster care, multiple spouses, etc.) From these structured feature sets, we generate a natural language profile for the household. We prompt Llama 3.1 70B with the natural language profile to answer questions from the information seeking agent."}, {"title": "Eligibility Checker Programs", "content": "To determine the ground truth eligibility of simulated users for specific opportunities, we manu-"}, {"title": "Dialog Loop", "content": "Agents are provided eligibility requirements and then must determine simulated user eligibility by asking questions, one at a time. After each response, the agent is prompted with READY, where it is asked if it has enough information to determine the eligibility of the user with certainty (Figure 1). If the agent responds with TRUE, it is prompted to PREDICT the user's eligibility for each opportunity. Otherwise, it asks another question. We limit conversations to 20 questions per opportunity, to a maximum of 100 questions."}, {"title": "ProADA", "content": "To solve interactive decision problems, we propose a Program Synthesis Adaptive Decision Agent, or ProADA, which uses agent-created Python tools as reasoning aids for adaptive decision problems in dialog. State-of-the-art code generation models often generate code that involves a dozen variables (Wan et al., 2024), yet the models suffer from basic reasoning errors and hallucinations when working in natural language. By offloading dialog planning and memory into static Python code, ProADA achieves the flexibility and usability of natural language while leveraging the long-range planning and reasoning of program synthesis. ProADA consists of a code generation module and a dialog module. The code generation module creates one Python DECIDE tool per opportunity, formalizing the logic of the decision problem and deciding the result. The dialog module serves as an interface between the user and the DECIDE tool, asking questions and storing answers in a structured form (Figure 3).\nTo best explain ProADA, we instantiate it in the context of our proposed BeNYfits benchmark. Before starting a dialog, ProADA uses the code generation model to convert the eligibility requirements in natural language into a Python DECIDE Checker tool used by the agent 3. DECIDE is a Python function that takes a UserFeatures dictionary containing known user properties (e.g., \"homeless_or_runaway\") as input and outputs the household eligibility. For each key used to access UserFeatures, the code generation model defines a type (int, float, etc.) constraint, or a list of string choices that the feature can take. At the start of the dialog, the agent runs the DECIDE tool, passing in an empty UserFeatures dictionary, since it knows nothing about the user yet. An empty dictio-"}, {"title": "Experimental Setup", "content": "As baselines, we choose Llama 3.1 Instruct 8B and 70B, as well as GPT-40. In direct prompting, we instruct these models to assess readiness and generate questions at each step in the dialog loop, and finally to predict eligibility. We also assess prompting models to conduct ReAct-style chain-of-thought before each step. During READY and DECIDE, we use constrained decoding to ensure ProADA and baseline models generate a valid output. For ProADA, we choose the same models for the dialog module and always use GPT-40 for the code generation module. We choose Llama 3.1 Instruct 70B to implement our simulated user for all experiments, in order to reduce the hallucinations that we observed in smaller 8B parameter models. To reduce memory usage, we use 4-bit quantization on all 70B-parameter models. We report three trials for all tests, except for the following, which we run once, due to resource constraints: GPT-40 + direct prompting, ReAct, and ProADA; and Llama 3.1 70B + ReAct.\nTo measure human performance on this task, one expert author performed the role of the agent on 39 user-opportunity pairs from the Diverse Dataset, achieving 89.7% accuracy. Upon review, we find all inaccuracies are due to human error rather than unfaithful simulated user responses, suggesting a performance ceiling of nearly 100% on this benchmark."}, {"title": "Experimental Results", "content": "Since our datasets are unbalanced (Diverse: 47.9% positive, Representative: 15.5%), we choose micro F1 as our primary accuracy metric. Let F1 and T be the average F1 score and the number of turns across both datasets, respectively. To reward efficient questioning, we define a turn-weighted F1 score as:\nTurn-Weighted F1 = 100 \u00b7 F1 / T/100 + 1\nsince dialogs can have at most 100 turns."}, {"title": "Failure Analysis", "content": "We observe multiple distinct types of errors that contribute to poor reasoning and inefficient dialog. Program synthesis-guided dialog reduces errors overall, but introduces unique failure modes associated with code generation."}, {"title": "General Errors", "content": "Several failure modes persist across all strategies, indicating core weaknesses in foundational model reasoning ability.\nSuggestibility: Models suffer from hallucination prompted by implications in eligibility requirements. For example, when prompted with a child care program, models ask for the child's age without checking whether the household contains any children to begin with.\nDomain knowledge & edge cases: Models fail to account for uncommon edge cases, such as children living with parents but not being claimed as tax dependents."}, {"title": "Baseline Errors", "content": "Although it is difficult to confidently attribute final predictions to specific mistakes during question generation, we observe several flawed reasoning patterns when using direct and ReAct prompting:\nHallucination: Baseline models frequently return TRUE in READY before collecting all relevant information, implying either a logical reasoning failure or a hallucination of relevant facts.\nUltra-specificity: Models ask needlessly specific questions (\"Is your total annual income below $69,900?\") when a more general question (\"What is your total annual investment income?\") would produce information useful elsewhere, resulting in superfluous dialog turns.\nRepetition: Baseline models get stuck in loops, asking slight variations of the same question.\nMulti-member households: Baseline models of-"}, {"title": "ProADA Errors", "content": "Program synthesis-guided dialog introduces several distinct new failure modes:\nCode generation: Logical or domain-specific reasoning errors can create flawed code that propagates errors through subsequent conversations.\nCode to question: Although the code generated for the DECIDE tool usually represents multiple family members correctly as a list, the dialog module struggles to track and specify which member is being discussed at any time. Interestingly, we observe improved performance when users provide the names of their family members.\nNo recovery: Since ProADA models strictly adhere to dialog planning according to the generated code, they have no ability to recover or deviate when users generate an unexpected response, such as \"I don't know.\""}, {"title": "Errors by Simulated Users", "content": "Authors annotated 61 simulated user responses for faithfulness to the user profile, finding 60 (98.4%) of questions are answered faithfully. The simulated user tends towards verbosity, providing additional unrequested information in 5 cases (8.2%). We find unnatural but faithful responses in 2 cases (3. 3%), indicating that the frequency of errors due to simulated user misbehavior is low. In qualitative probing, we find that the simulated user can respond accurately to diverse questions up to two hops (e.g., \"How many children do you have under the age of 5?\"). Sufficiently complex queries or those with more than two hops tend to cause the simulated user to respond that it cannot answer the question, but we rarely observe models generating such questions in our experiments."}, {"title": "Discussion", "content": "Program-synthesis-guided dialog improves accuracy in adaptive decision problems while reducing the number of dialog turns needed. This provides multiple benefits by exposing the agent's reasoning process in a human-readable format. Agent decisions become more transparent and consistent, improving interpretability, and enabling several avenues for further improvements.\nSince the Python tool only needs to be created once, we can use a stronger model for program synthesis without incurring significantly increased inference costs or latency. Then, by replacing the READY and PREDICT language model calls in the dialog loop with simple Python functions, we reduce the number of language model calls by over 50%. Unlike in black-box models where we observe disparate behavior based on surface form variation, especially in out-of-distribution contexts, our technique forces the agent to behave consistently across users. As a form of prompt transformation, this may also reduce the susceptibility of public-facing agents to jailbreak (Peng et al., 2024).\nSynthesized code also serves as a window into the agent's decisions. Although we generate code automatically in this work, the code may be checked manually or with software tools to ensure correctness before deployment. Unlike black-box models, program synthesis-guided models like ProADA may also be subject to unit tests to ensure code quality.\nAl faces increasing regulation, especially in public services or where systemic bias may disenfranchise certain groups, such as credit offerings. In certain scenarios, providers are required to prove that their models are unbiased or to provide a human-readable basis for any given AI decision. Although questions are generated neurally, eligibility decisions are made with static code that can be automatically traced to produce a rationale. However, we note that the parsing of user utterances into structured data may still introduce bias.\nMany opportunities in BeNYfits and other public opportunities are contingent on sensitive personal information, such as income, substance abuse, domestic violence, and being HIV positive. By limiting closed-source model use only to program synthesis, solutions like ProADA avoid leaking user data to commercial entities while harnessing their models' advanced reasoning.\nProADA represents a reverse of the traditional tool-use paradigm in which language models call tools by generating special tokens. Instead, our agent creates a tool which in turn calls the language model. Future work may explore more sophisticated agent-tool relationships."}, {"title": "Related Work", "content": "Many dialog agent tasks have been proposed, including offline task-oriented dialog (Andreas et al., 2020) (Budzianowski et al., 2018) and online user simulations using real humans or LM agents as responders (G\u00fcr et al., 2018) (He et al., 2018). Question generation is a related task where agents seek information relevant to a downstream task, such as user intent (Min et al., 2020) or relevant facts (Toles et al., 2023). Some task-oriented dialog datasets focus on clarification and information seeking, such as Zhang et al. (2023). However, datasets such as ShARC (Saeidi et al., 2018) and ClariT (Feng et al., 2023) only require \"yes\" or \"no\" questions. BeNYfits expands on these works by adding a highly realistic, multi-turn dialog agent task requiring logical reasoning and domain-specific knowledge. Similar tasks include MediQ (Li et al., 2024), which benchmarks medical diagnosis through dialog, and ClarQ-LLM (Gan et al., 2024), which focuses on discovering hidden information while playing an adventurer. In comparison, BeNYfits focuses on logically reasoning legalistic tasks to reach a binary prediction.\nMany works on tool-use have equipped language models with a code interpreter (Gupta and Kembhavi, 2023) (Shen et al., 2024), though fewer have specifically studied tool creation, e.g., Qian et al. (2023). Several prior works have established the efficacy of code generation in dialog systems. Chiu et al. (2023) propose grounding in code generated based on partner utterances and using symbolic planning to reason over the code. Sur\u00eds et al. (2023) find code translations an effective intermediate representation for natural language questions. Nguyen et al. (2024) create an LLM agent framework for dynamically creating and composing subtask actions based on code. To the best of our knowledge, no other code generation-based approaches have been proposed for question generation in dialog."}, {"title": "Conclusion", "content": "We present a strong tool-augmented method to solve interactive decision-making in dialogs and a novel and realistic benchmark for measuring decision-problem accuracy and dialog efficiency. Our method ameliorates memory and planning issues by converting key information in user utterances into structured key-value pairs to improve reasoning, latency, and cost by offloading computations onto an agent-created Python tool. Such structured coding support overcomes many problems of pure LLM baselines such as hallucination of missing information, lack of object tracking, being over-confident, etc. Ultimately, our proposed method achieved an F1 score of 55.6 (compared to at most 42.2 for the baselines) while reducing the dialog turns needed by more than 30% compared to the next best agent, raising hopes for reducing user burden and increasing access to public opportunities using language models."}, {"title": "Limitations", "content": "The eligibility requirements for this benchmark were derived from plain English summaries rather than official documents. Requirements for some opportunities omit details present in more complete sources.\nThe population data used to construct the Representative Dataset were collected from numerous independent sources. Some features were not available, such as the percentage of people currently struggling to pay their electricity bill. In such cases, we make estimates based on the most similar available data. At the same time, features are each collected from disparate sources, rather than from a single census, so our dataset is unable to express accurate correlations between related features. Users of our dataset should be aware of these limitations.\nBecause our evaluation method weights the F1 score against dialog turns, complex, multi-hop queries are weighted the same as simple yes or no questions. However, in practice, we rarely observe complex queries. The trade-offs of question complexity, length, and user burden may be addressed in future work."}, {"title": "Ethical Considerations", "content": "Empirically, we observe that model-generated code in this study does not contain harmful side effects. However, it is always safer to run untrusted code in a sandboxed environment like Docker.\nIntroducing AI models into the social benefits system poses risks of false determinations and inequitable user experiences. We encourage stakeholders to use AI to increase accessibility to public opportunities, but to avoid using them as the final determiner in any step due to the harm caused by errors. Similarly, user-facing deployments should consider the relative harm of false acceptances versus false refusals and calibrate their models accordingly."}]}