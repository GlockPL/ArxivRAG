{"title": "UNIFIED MULTI-MODAL INTERLEAVED DOCUMENT\nREPRESENTATION FOR INFORMATION RETRIEVAL", "authors": ["Jaewoo Lee", "Joonho Ko", "Jinheon Baek", "Soyeong Jeong", "Sung Ju Hwang"], "abstract": "Information Retrieval (IR) methods aim to identify relevant documents in response\nto a given query, which have gained remarkable attention due to their successful\napplication in various natural language tasks. However, existing approaches typi-\ncally consider only the textual information within the documents, which overlooks\nthe fact that documents can contain multiple modalities, including texts, images,\nand tables. Further, they often segment each long document into multiple discrete\npassages for embedding, preventing them from capturing the overall document\ncontext and interactions between paragraphs. We argue that these two limitations\nlead to suboptimal document representations for retrieval. In this work, to address\nthem, we aim to produce more comprehensive and nuanced document representa-\ntions by holistically embedding documents interleaved with different modalities.\nSpecifically, we achieve this by leveraging the capability of recent vision-language\nmodels that enable the processing and integration of text, images, and tables into\na unified format and representation. Moreover, to mitigate the information loss\nfrom segmenting documents into passages, instead of representing and retrieving\npassages individually, we further merge the representations of segmented passages\ninto one single document representation, while we additionally introduce a rerank-\ning strategy to decouple and identify the relevant passage within the document if\nnecessary. Then, through extensive experiments on diverse information retrieval\nscenarios considering both the textual and multimodal queries, we show that our\napproach substantially outperforms relevant baselines, thanks to the consideration\nof the multimodal information interleaved within the documents in a unified way.", "sections": [{"title": "1 INTRODUCTION", "content": "Information Retrieval (IR) is the task of fetching relevant documents from a large corpus in response\nto an input query, which becomes a fundamental process to various real-world applications including\nweb search engines and question-answering systems (Shah et al., 2019; Lewis et al., 2020; Guu et al.,\n2020). Specifically, to retrieve documents for the query, traditional approaches have focused on their\ntextual representations, utilizing either sparse retrieval methods such as TF-IDF and BM25 (Robert-\nson et al., 1994; Jones, 2004), which rely on exact term matching between the query and document,\nor dense retrieval methods such as DPR and ANCE (Karpukhin et al., 2020; Xiong et al., 2021),\nwhich leverage neural embeddings of the query and document text to capture semantic similari-\nties between them over a continuous vector space. Recently, dense retrieval methods have gained\nmore popularity over sparse methods due to their capability to capture semantic nuances and context\nbeyond simple keyword matching, leading to multiple successes with improved performance.\nDespite their huge successes, existing (dense) retrieval methods face a couple of severe challenges.\nFirst, they primarily rely on the textual data for document embedding and retrieval, overlooking the\nfact that modern documents often contain multimodal content, such as images and tables (beyond the\nplain text), which can carry critical information that may be essential for accurately understanding\nand retrieving the relevant documents. To be specific, a diagram within a medical article can more\neffectively represent the structure of a molecule or the progression of a disease, offering more clarity"}, {"title": "2 RELATED WORK", "content": "Information Retrieval Information Retrieval (IR) is the task of accurately finding documents rele-\nvant to a given query from a large corpus, such as Wikipedia, which has been a crucial component for\na variety of applications, including search engines, question-answering systems, and conversational\nagents (Zhu et al., 2023; Gao et al., 2023; Ram et al., 2023; Shi et al., 2024; Jeong et al., 2024a).\nSpecifically, to retrieve the relevant documents, earlier IR approaches measured similarity between\nqueries and documents based on their lexical term matching, such as BM25 and TF-IDF (Robert-\nson et al., 1994; Jones, 2004). Yet, these methods often struggled to capture the semantic nuances\nbeyond surface-level term overlaps. To overcome this, along with advancements in language mod-\nels (Devlin et al., 2019; Liu et al., 2019). there has been dense retrieval approaches that embed both\nthe queries and documents into a shared dense vector space (Karpukhin et al., 2020; Xiong et al.,\n2021), enabling the calculation of semantic similarity between them more effectively by capturing\nthe deeper contextual information. However, previous IR studies have mainly focused on enhanc-\ning the textual representations of queries and documents, while overlooking the fact that documents\noften consist of diverse modalities (such as images and tables) beyond text, which can potentially\nprovide richer context and aid in more accurate retrieval (Liu et al., 2021; Jeong et al., 2024b).\nMultimodal Information Retrieval Recent studies in IR have expanded the focus from purely\ntext-based retrieval models to those that consider other modalities, such as images (Radford et al.,\n2021; Xiao et al., 2024), tables (Herzig et al., 2021; Chen et al., 2024) and graphs (Baek et al.,\n2023); however, the majority of these approaches (Zhou et al., 2024; Long et al., 2024; Lerner et al.,\n2024; Nowak et al., 2024; Caffagni et al., 2024) have primarily explored how to process the mul-\ntimodal queries, meanwhile, they often overlook the equally important multimodal characteristics\nof the documents being retrieved. Specifically, we argue that, while incorporating multimodal ele-\nments in queries has expanded the range and diversity of query types that IR systems can handle,\nconsidering the multimodal nature of the documents can lead to a more holistic representation of\nretrieval targets, which can ultimately lead to enhancing the overall retrieval performance. In efforts\nto handle diverse multimodal elements within documents, there are concurrent studies that have pro-\nposed to capture screenshots of documents, such as PDFs (Faysse et al., 2024) or Wikipedia web\npages (Ma et al., 2024a), and subsequently encoding them through vision models (Ding et al., 2024).\nHowever, these methods are not only limited by factors, such as image resolution and computational\nmemory, constraining their application to documents longer than a single page\u00b9, but also fall short\nby treating the diverse modalities within a document as a single visual entity, leading to suboptimal\ndocument representations that fail to effectively capture the nuanced interdependence between text\nand images. Furthermore, they do not address the critical issue of splitting documents into smaller\nfragments (e.g., sub-images), which may disrupt the holistic contextual view of the entire document.\nVision-Language Models Recent Vision-Language Models (VLMs) have emerged as a powerful\ntool for jointly processing visual and textual data, combining the image understanding capabilities of\nvisual encoders (Radford et al., 2021; Zhai et al., 2023) with the advanced reasoning abilities of lan-\nguage models (OpenAI, 2022; 2023a). These models have achieved remarkable performance across\ndiverse vision-language (VL) tasks (such as image captioning and visual question answering) (Dai\net al., 2023; OpenAI, 2023b), with the substantially limited attention on their applications to IR. We\nnote that the latest developments in this field have particularly focused on enabling VLMs to handle\ninterleaved, multimodal content, which involves a mixed sequence of images and text (Zhang et al.,\n2023; Li et al., 2024). In particular, LLaVA-NeXT-Interleave (Li et al., 2024) introduces a fine-\ntuning approach that specifically enhances the VLMs' capacity to understand complex interleavings\nof multiple images and text within a single context. Drawing inspiration from these advances, in this\nwork, we propose to harness the capabilities of VLMs to create unified embeddings for documents\ninterleaved with text and images (as well as tables) for IR, which is a big shift from even the recent\nIR approach (Ma et al., 2024b) that still embeds the documents with the recent but text-based models\nlike Llama (Touvron et al., 2023a;b), failing to fully capture the diverse multimodal content."}, {"title": "3 METHOD", "content": "We present IDentIfy to holistically represent documents interleaved with multimodal elements."}, {"title": "3.1 PRELIMINARY", "content": "We begin with preliminaries, formally explaining information retrieval and vision-language models.\nInformation Retrieval Recall that Information Retrieval (IR) is the task of searching for relevant\ndocuments from a large corpus in response to a given query. Formally, let $q$ denote a query, $d$ denote\na document, and $D$ denote a collection of documents ($d \\in D$), where each query and document can\nbe represented as a sequence of tokens: $q = [q_1, q_2,..., q_n]$ and $d = [d_1, d_2,..., d_m]$ where $[\u00b7]$\nindicates a concatenation operation in a specific order. We note that traditional IR approaches typi-\ncally consider these tokens as purely textual elements; however, in this work, we propose to extend\nthis assumption to have the tokens of both the textual and visual content, to capture the multimodal\nnature of many real-world documents. Then, this new extension raises important questions of how\ncan both the textual and visual content be represented within a unified token framework, and how\ncan these multimodal tokens be seamlessly integrated and encoded for document representations.\nTo answer those two questions, we harness the power of recent vision-language models below.\nVision-Language Models We now turn to describing Vision-Language Models (VLMs), which\nare designed to jointly encode the textual and visual information in a unified token framework. We\nnote that these models are generally comprised of two main components: a visual encoder and a\nlanguage model, interconnected through a projection layer. Specifically, given an input document\nthat may contain interleaved modalities (e.g., text and images), the visual encoder extracts high-level\nvisual features from (multiple) images embedded within the document, mapping them into a latent\nspace. Then, these visual features are transformed into a sequence of visual tokens via the projection\nlayer, represented as follows: $V\\in\\mathbb{R}^{V\\times d_{emb}}$ where $V$ denotes the visual token length and $d_{emb}$ is the\ntoken dimension size. Similarly, for the textual content embedded within the document alongside\nimages, the language model uses a word embedding layer to convert the input text into a sequence\nof text tokens, represented as follows: $L\\in\\mathbb{R}^{L\\times d_{emb}}$ where $L$ denotes the token length of text.\nIn this work, we also propose to account for tables that are an integral modality for holistically rep-\nresenting the full content of documents. However, in contrast to text and images that have dedicated\nprocessing layers within the VLM architectures, tables do not have a specific representation layer.\nNevertheless, we argue that recent VLMs are pre-trained on diverse web data, and subsequently\nthey are implicitly learned to handle the table structures formatted in HTML. Consequently, we treat\nHTML-format table data as a linearized sequence of HTML words, applying the same word embed-\nding layer as is used for plain text. To be formal, this process converts the table content into table\ntokens, as follows: $T \\in \\mathbb{R}^{T\\times d_{emb}}$ where $T$ is the token length of the table. Lastly, once extracted, the\nvisual tokens, text tokens, and table tokens are concatenated (to form a unified token sequence) and\nthen passed through the remaining layers of VLMs, to capture both uni- and cross-modal relation-\nships across different modalities, enabling the comprehensive understanding of the input document."}, {"title": "3.2 RETRIEVER", "content": "We now turn to explaining how we design a retriever specifically tailored for multimodal interleaved\ndocument retrieval. In particular, to effectively retrieve documents that contain multiple modalities,\nour approach leverages a VLM capable of processing text, images, and tables within a single doc-\nument. Further, following the standard practice of existing retrieval architectures (Karpukhin et al.,\n2020; Xiong et al., 2021), we use a dual-encoder structure, which consists of a query encoder and a\nsection encoder, both are based on the VLM, which is illustrated in Figure 2 (a).\nSpecifically, thanks to the use of the VLM, our query encoder can take either purely textual queries\n$q = L_Q$ or multimodal queries consisting of text and corresponding visual elements $q = [V_Q, L_Q]$.\nAlso, to obtain the final query representation, we introduce a learnable token called 'End of Query',\n$[EoQ] \\in \\mathbb{R}^{d_{emb}}$. This token is appended to the end of the sequence of query tokens $q$, and the final\nconcatenated tokens $[q, [EoQ]]$ are then passed through the query encoder. Then, the model output\ncorresponding to $[EoQ]$ is used as the final query representation, as follows: $Z_Q\\in \\mathbb{R}^{d_{emb}}$.\nFor documents, we first represent each document $d$ as a sequence of sections $d= [s_i]_{i=1}^{S}$ (with a total\nof $S$ sections), where each section $s_i$ is derived by dividing the document according to the subtitles\nin the document. $s_i$ can contain a combination of text tokens $L_{s_i}$, visual tokens from embedded\nimages $V_{s_i}$, and table tokens $T_{s_i}$, denoted as follows: $s_i = [V_{s_i}, L_{s_i}, T_{s_i}]$. Then, to obtain a\nsection-level representation, similar to the query representation, we introduce a learnable token,\ncalled 'End of Section': $[EOS] \\in \\mathbb{R}^{d_{emb}}$, which is similarly appended at the end of each section."}, {"title": "3.3 RERANKER", "content": "To enable fine-grained retrieval within documents beyond the retrieval of documents themselves, we\nintroduce a section-level reranking mechanism that identifies the section most relevant to the input\nquery. In particular, once the document is retrieved, the objective of the reranker $f_R$ is to pinpoint\nthe specific sections within the document that best match the query. We also note that this reranker\nis similarly operationalized with a single VLM along with a binary classifier on top of it, which\ndirectly measures the relevance of each query-section pair, illustrated in in Figure 2 (b).\nFormally, for a retrieved document, we take each of its sections $s_i$ and concatenate it with the query $q$\nand a learnable token for section embedding $[EOS]$, forming the input sequence of $[q, s_i, [EOS]]$.\nThe concatenated tokens are then processed through the reranker, and the model output correspond-\ning to $[EOS]$ captures the relevant between the query and section, which is further subsequently\npassed to a binary classifier consisting of a linear layer followed by a Sigmoid function. Through\nthis process, the classifier outputs a probability score indicating the likelihood of the section being\nrelevant to the query, i.e., a score close to one denotes a high relevance (positive section), meanwhile,\na score near zero indicates irrelevance (negative section).\nTo train this reranker, we use the Binary Cross-Entropy (BCE) loss, formalized as follows:\n$C_{reranker} = \\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{S_i} \\frac{-l(y_{(s_{i,j})}, f_R([q, s_{i,j}, [EOS]])))}{BS_i}, l(y,\\hat{y}) = [y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})]$"}, {"title": "4 EXPERIMENTS", "content": "To evaluate the effectivenss of IDentIfy, we focus on multimodal IR tasks that require understanding\nof both the textual and visual cues within queries and documents, which align well with our goal of\nenhancing retrieval of multimodal interleaved documents. The datasets considered are as follows:\nEncyclopedic-VQA (Mensink et al., 2023) is a large-scale visual question-answering (VQA) bench-\nmark dataset, widely used for measuring the performance of multimodal IR models. Each query is\nlinked to a specific section of a Wikipedia document (containing an answer for it) and is manually\nannotated by humans. Also, this dataset offers both text-only and multimodal queries. In addition\nto this, the queries are related to fine-grained properties of species and landmarks. Our experiments\nfocus on the single-hop category where questions that can be answered in a single retrieval step.\nInfoSeek (Chen et al., 2023) is a dataset designed for knowledge-intensive VQA, covering a wide\nrange of entities (such as landmarks, animals, and food). Questions are generated by filling human-\nwritten templates with knowledge triples (subject, relation, object) available from Wikidata, which\ninvolve only the multimodal queries. As the test dataset is not available, we use the validation set as\nour test set, and split the training set into training and validation subsets with a 9:1 ratio.\nViQuAE (Lerner et al., 2022) is a dataset focused about human entities. It provides both text-based\nand multimodal queries, with each query linked to a specific section of a Wikipedia document that\ncontains an answer (annotated by humans), which makes it an idea benchmark for section retrieval.\nOpen-WikiTable (Kweon et al., 2023) is an extension of WikiSQL (Zhong et al., 2017) and Wik-\niTableQuestions (Pasupat & Liang, 2015), designed for open-domain table question answering that\nrequires retrieval of the most relevant table from a broader corpus. For our experiments, we adapt\nthis dataset, aiming at identifying the document or document section containing the target table, and\ncorrespondingly, utilize the WikiTableQuestions subset of Open-WikiTable that has labels for it."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "Model Training and Evaluation We use LLaVA-NeXT-Interleave (Li et al., 2024) of 0.5B pa-\nrameters as the basis VLM, for both the retriever and reranker. To take the advantage of larger batch\nsizes (while reducing GPU memory usage), we apply LoRA (Hu et al., 2022). Also, to further op-\ntimize the GPU usage, we combine four images into one, scaling each down to half of its original\nheight and width. During retriever and reranker training, we consider four sections per document in\nrepresenting documents and selecting negative samples. In contrast, during inference, we consider\nall sections within each document. For section retrieval, the top 25 documents retrieved are split into\nsections and passed to the rerankers. All experiments are conducted using a single H100 GPU.\nBaselines We compare our approach against a variety of IR baselines designed to capture dif-\nferent document representations. We start with Entity and Summary baselines, which are trained to\nretrieve documents based on their titles and summary sections. Next, we consider the Text-document\nretriever, which retrieves documents based on their textual content. Additionally, to consider a visual\ncomponent, we consider the Single-image baseline, incorporating the first document image.\nEvaluation Metrics We evaluate the performance of the retriever and reranker with standard met-\nrics: Recall@K (R@K) and Mean Reciprocal Rank@K (MRR@K). First, R@K measures whether\nthe relevant document or section is retrieved within the top-K results. MRR@K evaluates the rank-\ning quality by measuring the position of the first relevant item among the top-K retrieved results."}, {"title": "4.3 RESULTS AND DISCUSSION", "content": "Interleaved format improves document retrieval. First, we report the retrieval performance on\nthe Encyclopedic-VQA dataset in Table 1, where each query consists of both image and text content.\nFrom this, we observe that our approach achieves the best performance, with R@1 score improve-\nments of 53.0%, 64.0%, and 25.0% compared to the Summary, Text-document, and Single-image re-\ntrieval baselines, respectively. The MRR@10 score similarly shows significant gains, with improve-\nments of 36.1%, 48.5%, and 16.2% over the same baselines. This demonstrates the effectiveness of\nour approach in incorporating the interleaved multimodal format for document representations.\nTo further understand the source of these performance gains, we explore two levels of retrieval\ngranularity: passages and documents. Specifically, the passage retriever uses individual sections of\ndocuments as retrieval units, while the document retriever treats entire documents as single units.\nBoth models are trained on the Encyclopedic-VQA dataset for multimodal retrieval. Then, we use\nthe same reranker to both sets of results from passage and document retrievers, to directly compare\ntheir performance. In Table 2, we observe that relying solely on the passage retriever (Passage*)\nresults in suboptimal retrieval performance, highlighting the challenge in pinpointing the most rel-\nevant section within a document using traditional retrieval methods. In contrast, when the reranker\nis used alongside the document retriever, the performance significantly surpasses that of the passage\nretrieval, achieving a 22.7% improvement in R@1 and a 29.2% improvement in MRR@10, even\nthough the document retriever provides eight times fewer retrieval units to the reranker. These re-\nsults confirm the importance of leveraging holistic context from multiple, interrelated sections within\ndocuments. In addition to this, these findings also demonstrate the notable advantages of using the\ninterleaved multimodal elements within documents, emphasizing the potential of this direction.\nInterleaved format enhances document retrieval across modalities. We further expand our ex-\nperiments to two additional IR datasets, the InfoSeek and ViQuAE. As shown in Table 3 (a), our\nproposed retriever consistently surpasses the Text-document baseline in document retrieval with\nmultimodal queries. Specifically, this leads to 50.0% and 29.6% improvements in the R@1 score,\nand 40.2% and 25.7% improvements in the MRR@10 score for the InfoSeek and ViQuAE, respec-\ntively. We also examine the impact of interleaved documents on textual retrieval tasks, where queries\nconsist solely of text, and report the results in Table 3 (b). Then, the results demonstrate that the\ninterleaved format offers advantages in retrieval of textual queries as well, resulting in 4.3% and\n1.3% improvements in the R@1 score and 3.0% and 1.1% improvements in the MRR@10 score\nfor the Encyclopedic-VQA and ViQuAE, respectively. We attribute these gains to the integration of"}, {"title": "Interleaved format is also beneficial in section retrieval.", "content": "Similarly, we evaluate the efficacy of\nour approach in section retrieval across both multimodal and textual queries, using the Encyclopedic-\nVQA and ViQuAE datasets. First, in section retrieval with multimodal queries, our model attains\n4.2% improvement in the R@1 score and 3.3% improvement in the MRR@10 score for the Ency-\nclopedic VQA, as shown in Table 4 (a). Similarly, in section retrieval with textual queries, our model\nachieves 2.3% and 7.5% improvements in the R@1 score and 1.8% and 4.9% improvements in the\nMRR@10 score for the Encyclopedic and ViQuAE datasets, as shown in Table 4 (b). Overall, the de-\nsign of our Interleaved rerankers exhibit superior or comparable performance to the Text-document\nrerankers. However, since the rerankers assess query relevance using a single section, they may lack\naccess to broader contextual information from a document, which limits the potential performance\ngain compared to the retrievers. Nonetheless, the multimodal content interleaved within documents\nimproves the reranker's ability to evaluate the relevance of the query to individual sections."}, {"title": "Information retrieval of tabular contents in interleaved documents is challenging.", "content": "We explore\na retrieval task for tabular data, whose goal is to select the document or section containing the target\ntable relevant to the input query. Specifically, we use the Open-WikiTable dataset to train the re-\ntriever and reranker, and then compare these trained models (Finetuned) with the models trained on\nthe Encyclopedic-VQA dataset (Zero-shot). Then, as shown in Table 5 (a), despite Open-WikiTable\nconsisting of only 3.2k training samples, the Finetuned retriever achieves strong retrieval perfor-\nmance. Meanwhile, the Zero-shot retriever demonstrates only about half of the R@1 score and the\nMRR@10 score of the Finetuned retriever, though it remains competitive in R@100.\nIn contrast, the performance trends for the rerankers exhibit notable differences. The discrepancies\nin R@10 and R@20 scores between the Zero-shot and Finetuned retrievers, denoted in color red\nin Table 5 (b), are much more pronounced in the Open-WikiTable (table retrieval) experiments than\nthe ones in the ViQuAE (text retrieval) experiments. This highlights a substantial difference between\ntextual and tabular modalities, despite both being represented using word tokens. This suggests that\nthese two modalities may require different handling for retrieval, which we leave as future work.\nNotably, the R@1 scores for tabular section retrieval are significantly lower than those for textual\nsection retrieval"}, {"title": "More sections enhance document retrieval performance but raise computational costs.", "content": "In Ta-\nble 1 and Table 2, we observe that using the comprehensive multimodal content and enriched contex-\ntual information significantly improves document retrieval performance. Accordingly, we anticipate\nfurther improvements as more sections are gathered to represent the document, during training. To\nvalidate this, we measure the document retrieval performance with varying the number of sections\nper document on the InfoSeek dataset for training. The results shown in Figure 3 then indicate that\nincorporating more sections raises the MRR@10 score from 7.5 to 15.7. However, this performance\nboost comes with a clear trade-off; as the number of sections increases, the retriever must process\nadditional end-of-section tokens, leading to higher GPU memory consumption. To balance resource\nlimitations and performance gains, we select four sections per document for all experiments."}, {"title": "Sections from the same document act as effective negatives to enhance reranker performance.", "content": "We explore another method to improve IR effectiveness by leveraging the entire document. Specifi-\ncally, we investigate the use of sections from the same document as negatives for reranker training,\nnamely In-document. We compare this approach with traditional methods, including Top-K, which\nselects the top-K retrieved sections as negatives, and In-batch, which uses the positive sections for\nother samples in the same batch as negatives. After training rerankers with each method, we evaluate\nsection retrieval on the Encyclopedic-VQA dataset. The results shown in Table 6 demonstrate that\nour In-document approach achieves superior R@1 and MRR@10 scores. This suggests that the use\nof sections from the same document as negatives provides natural, cost-effective advantages thanks\nto their high similarity to the positive section. However, it does not consistently outperform the other\nmethods on the R@10 score. We hypothesize that this inconsistency may arise from the strengths of\neach method: the In-document approach excels at distinguishing sections from the same document,\nwhile Top-K and In-batch methods better differentiate sections from different documents."}, {"title": "BCE loss applied to each section produces the best reranker performance.", "content": "In our reranker de-\nsign, we apply BCE loss using the query concatenated with each document section (Section+BCE).\nWe also explore alternative training objectives to identify the most effective approach for section re-\ntrieval in interleaved documents. One such objective is contrastive loss (Contrastive). This approach\nis similar to the retriever, but the retrieval unit is a section. Additionally, we also explore a variant of\nthe BCE loss (Document+BCE), where, unlike Section+BCE, the query is concatenated with multi-\nple sections from the same document, including both positive and negative sections. An $[EOS]$ token"}, {"title": "Rerankers require much larger datasets than retrievers.", "content": "We analyze the effect of different\ndataset sizes for training on retriever and reranker performance. To achieve this, we randomly prune\nsamples in the Encyclopedic-VQA dataset at various ratios and report the performance of models\ntrained on these subsets. In Figure 4 (a), we observe that too many samples can degrade retrieval per-\nformance. Also, retrieval of textual queries requires fewer samples to reach its optimal performance\ncompared to multimodal retrieval. Similarly, in Figure 4 (b), section retrieval for multimodal queries\nrequires 10% of the dataset to achieve 80% of the full-dataset performance, while section retrieval\nfor textual queries needs only 5%. These observations suggest that additional modalities increase\nthe need for more data. This accounts for the inferior performance of the interleaved format in the\nViQuAE experiments (Table 4 (a)). The ViQuAE dataset, at only 2.2% of the size of Encyclopedic-\nVQA, may be small for the reranker to effectively learn multimodal query-section alignments. We\nalso observe that section retrieval is more challenging, with more samples improving the reranker's\nperformance. This explains why the ViQuAE reranker has much lower section retrieval scores com-\npared to the one trained on the Encyclopedic-VQA (Table 4 (b)). Given the challenge of obtaining\nlarge query-section pair samples, exploring more effective reranker training pipelines is necessary."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced IDentIfy, a novel IR framework designed to address the limitations of\nconventional methods that rely on solely textual content of documents and their segmented passages.\nSpecifically, our approach sits on top of recent VLMs, which enables integration and representation\nof diverse multimodal content (including text, images, and tables) into a unified document repre-\nsentation. Also, unlike previous strategies that segment documents at the passage level, our method\nmerges these segments to maintain the document's structural coherence, while further introducing a\nreranking strategy for precise identification of relevant sections. Extensive experiments across vari-\nous IR datasets demonstrated that IDentIfy consistently outperforms existing baselines, confirming\nthat the interleaved multimodal representation significantly enhances the quality of the document re-\ntrieval. We believe IDentIfy represents a crucial step toward more comprehensive and contextually\naware IR systems, capable of handling the increasing multimodality of modern information sources."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "Our codes are based on publicly available LLaVA-NeXT (Li et al., 2024). The experimental setup\nand details can be found in \u00a74 and Appendix A. The experiments are conducted with publicly avail-\nable datasets (Mensink et al., 2023; Chen et al., 2023; Lerner et al., 2022; Kweon et al., 2023). We\nhave included our codes in the supplementary material and will publicly release our code."}, {"title": "Appendix", "content": "Organization The supplementary file is organized as follows: In Appendix A, we explain the\nimplementation details for our experiments. In Appendix B, we outline the limitations of our study."}, {"title": "A IMPLEMENTATION DETAILS", "content": null}, {"title": "Dataset configuration", "content": "Table 8 summarizes the key properties of the datasets used in our experi-\nment, including query modality, target item, entity domain, number of entities, and whether a section\nID is provided to indicate the section containing the answer. Additionally, we provide the number\nof samples in the training, evaluation, and test splits, as well as the size of the corpus."}, {"title": "Dataset pre-processing", "content": "In our study, we leverage interleaved multimodal content from Wikipedia\ndocuments. However, the existing corpora associated with our IR datasets often lack this content,\ntypically only including the first few words of each document. Therefore, we augment the corpora\nby downloading the HTML file of each Wikipedia document.\nIf the dataset provides Wikipedia URLs for its corpus, we use them to download the HTML files.\nAlternatively, if only entity names are provided, we generate Wikipedia URLs using those names.\nIf a Wikipedia URL is deprecated, we remove the corresponding document from the corpus along\nwith any associated queries. From the HTML files, we extract text, image URLs, and tables. We\nthen split the contents by subtitles in the document where each chunk corresponds to a section. For\nthe images, we use the image URLs to download the corresponding images, removing any invalid\nURLs. This process produces a dictionary that organizes text, images, and tables by section.\nSince downloading the complete contents for all documents across datasets is time- and memory-\nintensive, we preprocess the subsets of each corpus, including documents relevant to queries in the\ntraining, evaluation, and test splits, as well as unrelated entity documents."}, {"title": "B LIMITATIONS", "content": "Due to the limitations of a single H100 GPU, we represent documents by selecting a limited number\nof sections and averaging their corresponding embeddings. While this reduces the computational de-\nmands, our findings suggest that capturing a broader document context leads to improved retrieval\nperformance. Hence, leveraging the long context window of LVLMs could further enhance docu-\nment retrieval by capturing more comprehensive information from the full document. Moreover, our\nreranker design follows the conventional approach of concatenating the input query with individual\nsections. However, we believe that providing the reranker with all the sections together would allow\nthe model to better leverage the contextual information from the entire interleaved document, po-\ntentially resulting in improved performance. In order to fully leverage the interleaved format in the\nIR system, addressing the issues by reducing the GPU load when processing interleaved documents\nwould greatly boost overall IR performance. We leave these explorations for future work."}]}