{"title": "Improving the classification of extreme classes by means of loss regularisation and generalised beta distributions", "authors": ["V\u00edctor Manuel Vargas", "Pedro Antonio Guti\u00e9rrez", "Javier Barbero G\u00f3mez", "C\u00e9sar Herv\u00e1s-Mart\u00ednez"], "abstract": "An ordinal classification problem is one in which the target variable takes values on an ordinal scale. Nowadays, there are many of these problems associated with real-world tasks where it is crucial to accurately classify the extreme classes of the ordinal structure. In this work, we propose a unimodal regularisation approach that can be applied to any loss function to improve the classification performance of the first and last classes while maintaining good performance for the remainder. The proposed methodology is tested on six datasets with different numbers of classes, and compared with other unimodal regularisation methods in the literature. In addition, performance in the extreme classes is compared using a new metric that takes into account their sensitivities. Experimental results and statistical analysis show that the proposed methodology obtains a superior average performance considering different metrics. The results for the proposed metric show that the generalised beta distribution generally improves classification performance in the extreme classes. At the same time, the other five nominal and ordinal metrics considered show that the overall performance is aligned with the performance of previous alternatives.", "sections": [{"title": "Introduction", "content": "In the last decade, the use of machine learning and deep learning techniques to solve classification tasks has received an increasing interest in the literature, due to their multiple real-world applications in different areas such as industry (Bertolini et al., 2021; Vargas et al., 2023b; Zhang et al., 2021b; Jim\u00e9nez-Romero et al., 2020), medicine (Houssein et al., 2021; Saibene et al., 2021), internet of things (Zhang et al., 2021a; Klaib et al., 2021) or renewable energies (G\u00f3mez-Orellana et al., 2022). Some of these problems have an implicit order in the categories to be predicted, in such a way that they can be naturally ordered following a scale determined by the real problem. Solving these kind of tasks is commonly known as ordinal classification or ordinal regression (P\u00e9rez-Ortiz et al., 2014; Riccardi et al., 2014), given that they share some characteristics with both classification and regression problems. Therefore, solving an ordinal classification problem consists on predicting the correct category from a set of discrete categories that are arranged following an order determined by the real problem. In contrast to regression problems, the predicted class is not continuous and the distance between two adjacent classes does not have to be the same for all the categories.\nIn the machine learning context, a classification problem can be formally defined as the problem of predicting the label y using an input vector x, where $x \\in X \\subseteq R^d$ and $y \\in Y = {C_1, C_2, ..., C_J}$, where $d \\in N^+$ and $J$ is the number of categories of the problem. To solve these problems, the objective is to find a function $r : X \\rightarrow Y$ that predicts the category of any given sample using the input data. In the particular case of an ordinal classification problem, categories have an intrinsic order which can be defined by the following order constraint: $C_1 < C_2 < ... < C_J$. Although the order between categories is defined by the problem, it is common for the distance between these categories to be unknown and non-uniform. Thus, the order of a category is given by $O(C_j) = j$, and the previous constraint can be also expressed as $O(C_1) < O(C_2) < .. < O(C_J)$. Taking into account the specific characteristics of ordinal problems, different methodologies have recently been proposed in the literature to consider these elements and attempt to improve classification performance by reducing the magnitude of errors. This is achieved by avoiding mistakes in distant classes, which incur a high penalty in this type of problem. In Singer et al. (2020), the authors proposed a weighted information-gain metric. They used this metric to generate decision trees for ordinal classification problems, resulting in improved performance compared to using a nominal metric. Furthermore, in Singer et al. (2021), the authors suggested using a decision-tree model with the aforementioned weighted information-gain metric to solve a medical problem involving the diagnosis of the severity of trachea stenosis from EEG signals. Additionally, in Barbero-G\u00f3mez et al. (2021), the authors applied a different ordinal approach to assess neurological damage in Parkinson's disease patients. This approach involved decomposing the original ordinal problem into several binary problems, which could be solved separately or jointly. In Tang et al. (2021), the authors proposed a methodology that overcomes one of the most common problems in ordinal classification: the lack of data. To do that, they add additional information to each sample apart from its label. Besides, in L\u00e1zaro & Figueiras-Vidal (2023) the authors proposed an approach oriented to solve another common problem in ordinal tasks, which is their imbalance nature. To do that, they propose a loss function which is an estimate of the Bayesian classification cost. With the same concern, the authors of Zhu et al. (2019) proposed a generation direction-aware synthetic minority oversampling technique to deal exclusively with imbalanced ordinal regression problems. Finally, in Tang et al. (2022), the authors proposed a distance metric learning method specifically designed to deal with the combination of absolute and frequentist relative information in ordinal classification problems.\nIn most nominal classification tasks, the importance of all the categories of the problem is the same. However, there are many ordinal classification tasks where the extreme classes (i.e., the first and the last classes) are more relevant than the others. Therefore, improving the sensitivity of these classes can be quite interesting for the specific real-world problem addressed. For example, in a 4-class bio-medical problem where the severity of a disease is predicted (Khan et al., 2020; Atwany et al., 2022), and level 0 is related to a healthy patient while level 4 corresponds to the worst grade of the disease, correctly predicting these two classes can be very important. Also, in an Industry 4.0 quality control problem (Rosati et al., 2022; Damacharla et al., 2021) where any manufactured product is classified depending on its quality, accurately predicting the worst class can be crucial to remove products that do not satisfy the minimum quality requirements. In the same way, properly predicting the best class can also be very interesting to form a \u201chigh-quality product range\" category and sell them separately.\nMoreover, there are some problems where there is an important presence of noise in the target labels due to the intrinsic characteristics of the problem. In most cases, the samples have been labelled by an expert who may make misclassifications. This fact becomes more evident when the class labels of the problem have a natural order. In such cases, the expert who has labelled the data can classify two patterns that are very similar in two different adjacent classes. Even though the samples could be perfectly labelled, as in the case of classifying faces by age ranges, a model that classifies these samples using a standard 0/1 label encoding can perform poorly on the border of two categories. To address this problem of noisy labels, label smoothing was introduced by Liu et al. (2020). In this work, the authors proposed to train a neural network model using a regularised loss function that transforms the standard 0/1 label encoding into a soft alternative that better represents the labels in an ordinal classification problem. Given that labeling errors usually occur in adjacent classes due to their similarity, they proposed using unimodal distributions to define the new label encoding. Furthermore, they compared the performance of Poisson, binomial, and exponential distributions when used to generate the new label encoding. It is worth noting that the Poisson distribution is a discrete distribution that only has one parameter, which directly determines both the mean and the variance of the distribution. For this reason, it is not possible to find a value that places the mean in the middle of the class interval for every class in a problem with J classes while keeping the variance low, which is desirable. Therefore, in the same work, the authors proposed the binomial distribution, which uses different expressions for the mean and the variance, making it easier to adjust them separately. However, for some classes, it is still not possible to keep the variance low. Later, in Vargas et al. (2022), the authors proposed using a beta distribution since it can achieve a small variance while keeping the mean at the centre of the class interval. In addition, they proposed a methodology to estimate the parameters of the distribution used to generate the label encoding for each class based on the total number of classes in the problem (J). They also conducted an extensive experimental study that showed that the beta distribution performs better than the previous alternatives on different datasets and considering different metrics. Finally, in Vargas et al. (2023a), the authors proposed using triangular distributions instead of beta distributions. The most important benefit of triangular distributions is that they allow adjusting the error in adjacent classes by using a single parameter for the extreme classes and another one for the intermediate classes. The authors demonstrated through an experimental study that their proposal outperformed the alternatives proposed in Liu et al. (2020) and achieved competitive results when compared with beta distributions proposed in Vargas et al. (2022).\nAlthough the methods described in this paragraph obtain fairly good classification results when used to tackle ordinal classification problems, none of them allow for the prioritisation of extreme classes, which is desirable in certain problems as described above. Therefore, considering the importance of a good classification of the extreme classes and the excellent results obtained by the beta regularised loss function proposed in Vargas et al. (2022), the objectives of this work can be summarised as follows:\n1.  To analyse the effect of employing a unimodal regularised loss function, based on a generalised beta distribution, for training a CNN model, examining its impact on the classification performance of the extreme classes.\n2.  To propose a parameter estimation methodology for generalised beta distributions, which relies on restrictions on the mean and variance of the distributions, as well as the number of classes involved in the problem.\n3.  To define a new evaluation metric to assess the classification performance specifically for the first and last classes.\n4.  To test the methodology using six different ordinal benchmark datasets with different number of classes and five nominal and ordinal metrics, in addition to the proposed one.\nThe rest of the manuscript is structured as follows: in Section 2 the proposed regularisation based on a generalised beta distribution is presented, in Section 3, a method to estimate the parameters for the generalised beta distributions is proposed, Section 4 describes the experimental design used to test the proposed methodology, in Section 5 the results and the statistical analysis are shown, and, finally, in Section 6 the conclusions of this work are discussed."}, {"title": "Methodology", "content": "This section describes the regularised loss function presented in this work, as well as the soft labelling baseline approach that forms the foundation for it."}, {"title": "Baseline approach", "content": "The soft labelling approach introduced in Liu et al. (2020) and later employed in several works including the one that proposed using beta distributions Vargas et al. (2022) consists in replacing the loss function that is used to train a deep learning model with a regularised alternative where the standard labels are encoded with a soft alternative. To do that, they took the standard categorical cross-entropy loss function and defined its regularised alternative as follows:\n$\\mathcal{L}(x, k) = \\sum_{j=1}^J q'(j, k)[-\\log P(y = C_j|x)],$ (1)\nwhere $k = O(C_k)$, $C_k$ is the target class and $q'(j, k) = P_j(k)$ defines the soft labels encoding. $P_j(k)$ is the probability for the j-th class when the actual target is the k-th category. This probability is directly given by the probability mass function (p.m.f.) of any given discrete distribution or can be obtained using the probability density function (p.d.f.) of any continuous distribution in the following manner:\n$P_j(k) = \\int_{(j-1)/J}^{j/J} f_k(x) dx,$ (2)\nwhere $f_k(x)$ is the p.d.f. or p.m.f. associated with class $C_k$. The aforementioned probability can be obtained using any type of probability distribution. However, an unimodal distribution that is centered in the interval of the true class should be the most appropriate for ordinal problems. Note that the probability is sampled between the interval limits associated with the j-th class. These intervals are defined by dividing the $[0, 1]$ space in equal-length intervals based on the number of classes J of the problem. Therefore, the lower and upper limits of the interval for class $C_j$ is given by $\\frac{j-1}{J}$ and $\\frac{j}{J}$, respectively. The definition of these intervals is based on the assumption that the probability distribution employed is bounded in the $[0, 1]$ interval.\nIn the case of Vargas et al. (2022), beta distributions were employed to define the soft labels, and, therefore, the p.d.f. was expressed as follows:\n$f(x; u, v) = \\frac{x^{u-1}(1 - x)^{v-1}}{B(u, v)}, \\qquad 0 < x < 1,$ (3)\nwhere u and v are two parameters of the beta distribution that affect the location and the scale of it, and B(u,v) is the Euler beta function, defined as:\n$B(u, v) = \\frac{\\Gamma(u)\\Gamma(v)}{\\Gamma(u + v)},$ where $\\Gamma(u) = (u \u2013 1)!,$ (4)\nfor two positive integers u and v."}, {"title": "Proposed methodology", "content": "The regularised loss function based on generalised beta distributions, which is proposed in this work and described in this section, is an enhanced version of the methodology proposed in Vargas et al. (2022). It aims to improve the classification performance of the extreme classes by using a Generalised Beta (GB) distribution as a replacement for the standard beta distribution employed in that work. The generalised beta distribution can be denoted as GB(a, u, v) and adds a new parameter (a) with respect to the standard beta distribution. The main goal of using a more flexible distribution is to improve the classification performance of the extreme classes. Said enhancement is achieved by reducing the variance of these distributions. In these terms, we consider using a = 2 for the extreme classes and a = 1 (standard beta) for the intermediate ones. Thus, the probability mass for the intermediate classes is concentrated in the middle of the interval while, for the first class it is around x = 0, and for the last class most of the probability is around x = 1.\nIn this way, the generalised beta distribution can be denoted as GB(a, u, v) and, a continuous random variable $X \\sim GB(a, u, v)$ has a p.d.f. $f (x; a, u, v)$ defined by McDonald & Xu (1995); McDonald (2008) as:\n$f(x; a, u, v) = \\frac{x^{u\\alpha-1}(1 - x)^{v-1}}{\\alpha B(u, v)},$ (5)\nwhere $a > 0$, $u > 0$, $v > 0$ and B(u, v) is the Euler beta function.\nThe existence of three free parameters makes this distribution very flexible and includes the standard beta distribution in the particular case of $a = 1$. Since the domain of $f(x; a, u, v)$ is finite, all its moments are defined. The h-th order moment of a GB random variable is given by:\n$E[X^h] = \\frac{B(u + a h, v)}{B(u, v)},\\text{ for } u + ah > 0.$ (6)\nThe demonstration of the process followed to achieve Equation (6) is shown in Appendix A. Then, the mean of the GB distribution is given by:\n$E[X] = \\frac{B(u + a, v)}{B(u, v)} = \\frac{(u + a \u2212 1) \u00d7 ... \u00d7 u}{(u + v + a \u2212 1) \u00d7 . . . \u00d7 (u + v)} \\text{ for } u + a > 0,$ (7)\nThe second order moment is defined as:\n$E[X^2] = \\frac{B(u + 2a, v)}{B(u, v)} = \\frac{(u + 2x \u2013 1) \u00d7 ... \u00d7 u}{(u + v + 2a \u2212 1) \u00d7 . . . \u00d7 (u + v)} \\text{ for } u + 2a > 0.$ (8)\nThen, the expression of the variance is determined by:\n$V[X] = E[X^2] \u2013 E\u00b2[X] = \\frac{B(u + 2a, v)}{B(u, v)} - (\\frac{B(u + a, v)}{B(u, v)})^2,\\text{ for } u + 2a > 0,$ (9)\n$V[X] = \\frac{(u + 2x \u2013 1) \u00d7... \u00d7 u}{(u + v + 2x \u2013 1) \u00d7 ... \u00d7 (u + v)} - \\frac{(u + a \u2212 1)\u00b2 \u00d7 ... \u00d7 u\u00b2}{(u + v + a \u2212 1)\u00b2 \u00d7 . . . \u00d7 (u + v)\u00b2}$ (10)\nIn this way, the $P_j(k)$ term that was defined in the baseline approach described in Section 2.1 can be defined according to the p.d.f. of the GB distribution:\n$P_j(c) = \\int_{(j-1)/J}^{j/J} f (x; a_c, u_c, v_c) dx,$ (11)\nwhere $a_c, u_c$ and $v_c$ are the parameters for the beta distribution associated to the true class $C_c$. These parameters, are calculated based on the true class and the number of categories of the problem in the same way described in Vargas et al. (2022) for the intermediate classes, given that for these classes a = 1, which is the standard beta. However, the method employed for the first and the last classes is based on establishing some constraints for the mean and the variance of the distributions based on the target class, and it is described in Section 3."}, {"title": "Estimation of the parameters of the GB distribution as a function of the number of classes", "content": "Although there are different statistical methods for estimating the parameters of the GB(a, u, v) distribution, such as the method of the moments and the maximum likelihood method, they are too complex to be used to estimate the parameters of a generalised beta distribution due to the intractable integration expressions in the normalisation constant (Ma & Leijon, 2010). For maximum likelihood estimation, numerical methods can be used to calculate the shape parameters of a generalised beta distribution using the smallest Morder statistics (Gnanadesikan et al., 1967). In Narayanan (1992), the authors show a numerically feasible method for parameter estimation in the multivariate beta (Dirichlet) distribution through the method of maximum likelihood. Also, in Warsono et al. (2018), the authors use an iterative process to estimate the parameters of the generalised beta distribution. Finally, in Makouei et al. (2021), the authors derive recurrence relations for the single and the product moments of the order statistics as well as k-record values from the complementary beta distribution. However, the maximum likelihood method is known to provide poor results when the maximum is at the limit of the interval of one of the parameters. On the other hand, the method of moments is used to obtain a coarse approximation given that it gives an important bias when the samples sizes are not large enough.\nIn this context, new procedures for estimating the parameters of a GB distribution have been developed (Mano & Sibuya, 2021; G\u00f3mez-D\u00e9niz & Sarabia, 2018; Kakamu & Nishino, 2019). However, they are not appropriate procedures for a soft-labelling procedure, given that we need to arrange the J classes into J sub-intervals are defined delimited by J \u2013 2 thresholds. Based on it, the probability for each sub-interval is used as a soft label. In this way, standard estimation procedures cannot be employed since the sample size and the composition of these samples in each of the sub-intervals are unknown.\nIt is worth noting that computing all the three parameters of the GB at the same time in a general form that can be used for any problem with a given number of classes is not a trivial task. Therefore, to simplify the process, the a parameter has been fixed while u and v have been computed using the method described in this section: i.e., they have been determined by constraining the values of E[x] and V[x] for each random variable associated with each projection sub-interval. In this way, the probability in the extremes is more concentrated around 0 in the first class and around 1 in the last class; while in the intermediate classes the probability should be concentrated around the midpoint of the sub-interval (see Figure 1).\nThe parameter estimation procedure is composed of different steps. In a first step, the value of the a parameter is selected from {1.0, 2.0}. Based on these values, in a second step, values {0.5, 1.0} are considered for the u parameter of the first class, and values {0.5, 1.0} are tested for the v parameter of the last class. In a third step, taking into account the values of a and u, for the first class, or v, for the last class, the last parameter is obtained."}, {"title": "Estimation of the parameters for the intermediate classes", "content": "This section describes the methodology followed to obtain the parameters of the generalised beta distribution for the intermediate classes (from 2 to J - 1). Since the beta regularisation approach proposed in Vargas et al. (2022) obtained very competitive results and the objective of this work is to improve the results in the extreme classes, for those problems where accurately classifying those classes is crucial, the same methodology described in that work is employed for the intermediate classes. Thus, the parameters for those classes are obtained by considering two constraints, which are based on the mean and the variance of the GB(1, u, v) distributions:\n$\\frac{j}{J} < E[X] + S[X] \\leq \\frac{j+1}{J},\\text{ for } j = 0, . . ., J \u2013 1.$ (18)"}, {"title": "Estimation of the parameters for the first class", "content": "Given that the a parameter is 2, in order to obtain values of v as a function of u and the number of classes, two constraints associated with the mean and the variance of the distribution of the first class are proposed. Thus, the first expression constrains the mean of the distribution, so that it has to be between 0 and the centre of the first interval:\n$E[X] = \\frac{u(u + 1)}{(u + v + 1)(u + v)} < \\frac{1}{2J}$ (19)\nThen, the second constraint is associated with a linear combination of the mean and the standard deviation:\n$0 \\leq E[X] \u2013 \\lambda S[X], S[X] < \\frac{1}{2J\\lambda},\\text{ and } V[X] < \\frac{1}{4J^2\\lambda^2},$ (20)\nwhere $\\lambda$ is the parameter that controls the linear combination and can be cross-validated for each dataset.\nGiven that $V[X] = E[X^2] \u2013 E\u00b2[X]$, the second constraint can be defined as follows:\n$E[X^2] < \\frac{1}{4J^2\\lambda^2} + E\u00b2[X] < \\frac{1 + \\lambda^2}{4J^2\\lambda^2}$ (21)\nReplacing the value of E\u00b2[X] taking into account Equation (19):\n$E[X^2] - \\frac{(u + 3)(u + 2)(u + 1)u}{(u + v + 3)(u + v + 2)(u + v + 1)(u + v)} \\leq \\frac{1 + \\lambda^2}{4J^2\\lambda^2}$ (22)\nThen, replacing Equation (19) in Equation (22):\n$E[X^2] = \\frac{(u + 3)(u + 2)}{(u + v + 3)(u + v + 2)} < \\frac{1 + \\lambda^2}{2J\\lambda^2}.$ (23)\nFor simplicity, we define $F = \\frac{1 + \\lambda^2}{2J\\lambda^2}$, so that $(u + 3)(u + 2) \\leq F(u + v + 3)(u + v + 2)$. Then, the following inequation defines a lower bound for the value of v:\n$\\nu \\geq \\frac{-F(2u+5) + \\sqrt{F^2(2u + 5)^2 \u2013 4F(F \u2013 1)(u\u00b2 + 5u + 6)}}{2F}$ (24)\nIn this case, taking into account the values of $\\alpha$ and $u$ which were previously selected for the first class ($\\alpha = 2$, $u = 1$):\n$v \\geq \\frac{-7F+ \\sqrt{F^2 + 48F}}{2F}$ (25)\nUsing this expression, the lower bound of the v parameter can be obtained based on the A parameter, which can be cross-validated."}, {"title": "Estimation of the parameters for the last class", "content": "The parameters of the last class are obtained in a similar way to those of the first class. Thus, considering the value a = 2, the first constraint can be set up as follows:\n$E[X] = \\frac{u(u + 1)}{(u + v + 1)(u + v)} = \\frac{2J-1}{2J},$ (26)\nwhile the second constraint can be defined as:\n$E[X] + \\eta S[X] \\leq 1,$ (27)\nand, consequently:\n$S[X] \\leq \\frac{1 \u2013 E[X]}{\\eta} = \\frac{1 - \\frac{2J-1}{2J}}{\\eta} = \\frac{1}{2J\\eta}$ (28)\nThen:\n$V[X] = E[X^2] \u2013 E\u00b2[X] < \\frac{1}{4J^2\\eta^2}$ (29)\nand, replacing the value of E\u00b2[X]:\n$E[X^2] = \\frac{(u + 3)(u + 2)(u + 1)u}{(u + v + 3)(u + v + 2)(u + v + 1)(u + v)} \\leq \\frac{1 + \\eta\u00b2(2J \u2212 1)\u00b2}{4J^2\\eta^2}$ (30)\nUsing Equation (26) and Equation (30), we obtain:\n$E[X^2] = \\frac{(u + 3)(u + 2)}{(u + v + 3)(u + v + 2)} < \\frac{1 + \\eta\u00b2(2J \u2212 1)\u00b2}{2J\\eta\u00b2(2J \u2013 1)}$ (31)\nThen, for simplicity, we define\n$L= \\frac{1 + \\eta\u00b2(2J \u2212 1)\u00b2}{2J\\eta\u00b2(2J \u2013 1)}$ (32)\nso that $(u + 3)(u + 2) \\leq L(u + v + 3)(u + v + 2)$. Solving the second degree inequation, an upper bound for u is obtained:\n$u < \\frac{-(5-6L) \u00b1 \\sqrt{(5 \u2013 6L)^2 \u2013 4(1 \u2013 L)(6 \u2013 (35/4)L)}}{2(1-L)}$ (33)\nThis upper bound depends on the \u03b7 parameter, which can be cross-validated."}, {"title": "Experiments", "content": "In this section, the experiments that were conducted to test the proposed method are described. In the first part of this section, the datasets considered are described. Then, the model is presented and, finally, the experimental design is explained."}, {"title": "Datasets", "content": "In this section, several ordinal problems whose input data comes in the shape of images are described."}, {"title": "Diabetic Retinopathy", "content": "Diabetic Retinopathy is a dataset of high-resolution eye fundus colour images. It was published in a Kaggle competition\u00b9 and since then has been commonly used as a benchmark dataset for ordinal classification methods (Wang et al., 2021; Wu et al., 2020; Xie et al., 2020; de La Torre et al., 2020). The dataset was provided as two separate splits for training and evaluation. The training set contains 17563 pairs of images, from the left and right eyes. On the other hand, the testing set is composed of 26788 pairs of images. Each image is labelled with one of five levels of diabetic retinopathy (DR) disease, where level 0 indicates a healthy patient while level 4 is associated with proliferative DR. The dataset is highly imbalanced given that most of the patients are healthy. Therefore, the number of samples on each of the categories is given by: 65342 for level 0, 6205 for level 1, 13152 for level 2, 2087 for level 3, and 1916 for level 4. These images were taken with different devices and different lighting conditions. Therefore, to improve the training process and the generalisation capability, a preprocessing step, that was described in Kaggle2, was performed aimed at enhancing the contrast of the images. Also, given that the original images are extremely high-resolution, they were resized to 256 \u00d7 256 to train the model. Some processed images from each category are shown in Figure 2."}, {"title": "Adience", "content": "Adience (Eidinger et al., 2014) is another benchmark dataset that is composed of colour images of human faces. Each image is associated with its gender and age. The age is given in different ranges, summing up to 8 classes. The whole dataset is composed of 26580 faces belonging to 2284 people. The images have been preprocessed in order to crop and align the faces, making the classification task easier. All the images have been resized to 256 \u00d7 256 and contrast-normalised. The original dataset was provided as five cross-validation folds. In this case, the first four folds are used for training and the last one for testing."}, {"title": "FGNet", "content": "FGNet (Fu et al., 2014) is a smaller faces images dataset composed of 1002 colour images with 128 \u00d7 128 resolution. These images belong to 82 different subjects. Each of the samples is labelled with the exact age of the person in the moment when the photograph was taken. Therefore, different categories can be obtained depending on the age grouping established. For this work, we have grouped the samples using the following intervals: [0,3), [3, 11), [11, 16), [16, 24), [24, 40), [40, +\u221e). Given that the whole dataset was provided without partitions, 20% of the data was used for testing and the rest for training. These partitions were created in a stratified way, keeping the same ratio of samples from each class than in the original dataset."}, {"title": "UTKFace", "content": "UTKFace (Zhang et al., 2017) is a face dataset with a long age span (ranging from 0 to 116 years old). The dataset is composed of 20,000 face images with annotations of age, gender and ethnicity. These images cover large variations in pose, facial expression, illumination conditions, occlusion, resolution, etc. The dataset can be used for different types of tasks like face detection, age estimation or landmark localisation. In this case, we are interested in solving the problem related to age estimation, given that it is an ordinal task. The original images are already cropped and aligned and, thus, on each image, there is only one visible face. Every image is provided along with its corresponding meta-data: an integer indicating the exact age, the gender as a binary variable, the race as an integer, and the date and time when the picture was collected. In this work, we are only using the age variable. In order to convert the continuous age to an ordinal variable, 12 categories have been determined using the following intervals: [0,2), [2,6), [6, 12), [12, 19), [19, 23), [23, 27), [27,30), [30, 38), [38, 45), [45, 55), [55, 65), [65, 73), [73,80), [80, +\u221e). Then, the training and test partitions were created taking 80% and 20% of the complete set, respectively, in a stratified way."}, {"title": "Aesthetic Visual Analysis", "content": "The Aesthetic Visual Analysis (AVA) dataset was introduced in 2012 as a new benchmark dataset (Murray et al., 2012). AVA contains over 250,000 images along with a rich variety of meta-data, including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to the photographic style. In this work, our interest lies into the aesthetic scores, as they can be grouped into different intervals and be used as ordinal labels. Each image received from 78 to 549 votes in a range of [0, 10] and the information is provided as the number of votes on each of the ratings. To convert these individual ratings to an ordinal label, the mean value was computed. Then, the final ordinal label was assigned based on a series of subsets: [0,2], {3}, {4}, {5}, {6}, and [7,10]. The original images come with different resolutions. Thus, to work with them, they have been rescaled to 256 \u00d7 256. Given that the original set was provided without any partition, the test size was constructed taking a stratified random sample containing 20% of the data. The rest was taken for the training set. Figure 3 represents some of the images of the AVA dataset. Those images were taken from different categories, where the first belongs to the lowest rating and the last to the highest one."}, {"title": "WIKI (IMDB-WIKI)", "content": "The IMDB-WIKI (Rothe et al., 2018) dataset is one of the largest public datasets for age prediction. It contains 500, 000 images obtained from IMDb and Wikipedia. In this work, we use a small subset of this dataset that was obtained using only the images from Wikipedia. Therefore, the total number of images is reduced to 62, 328. From these images, 80% are used for training the model, and the remaining 20% for evaluation. Also, this dataset requires a preprocessing step before using these images to"}]}