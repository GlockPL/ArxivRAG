{"title": "Improving the classification of extreme classes by means of loss regularisation and generalised beta distributions", "authors": ["V\u00edctor Manuel Vargas", "Pedro Antonio Guti\u00e9rrez", "Javier Barbero G\u00f3mez", "C\u00e9sar Herv\u00e1s-Mart\u00ednez"], "abstract": "An ordinal classification problem is one in which the target variable takes values on an ordinal scale. Nowadays, there are many of these problems associated with real-world tasks where it is crucial to accurately classify the extreme classes of the ordinal structure. In this work, we propose a unimodal regularisation approach that can be applied to any loss function to improve the classification performance of the first and last classes while maintaining good performance for the remainder. The proposed methodology is tested on six datasets with different numbers of classes, and compared with other unimodal regularisation methods in the literature. In addition, performance in the extreme classes is compared using a new metric that takes into account their sensitivities. Experimental results and statistical analysis show that the proposed methodology obtains a superior average performance considering different metrics. The results for the proposed metric show that the generalised beta distribution generally improves classification performance in the extreme classes. At the same time, the other five nominal and ordinal metrics considered show that the overall performance is aligned with the performance of previous alternatives.", "sections": [{"title": "1. Introduction", "content": "In the last decade, the use of machine learning and deep learning techniques to solve classification tasks has received an increasing interest in the literature, due to their multiple real-world applications in different areas such as industry (Bertolini et al., 2021; Vargas et al., 2023b; Zhang et al., 2021b; Jim\u00e9nez-Romero et al., 2020), medicine (Houssein et al., 2021; Saibene et al., 2021), internet of things (Zhang et al., 2021a; Klaib et al., 2021) or renewable energies (G\u00f3mez-Orellana et al., 2022). Some of these problems have an implicit order in the categories to be predicted, in such a way that they can be naturally ordered following a scale determined by the real problem. Solving these kind of tasks is commonly known as ordinal classification or ordinal regression (P\u00e9rez-Ortiz et al., 2014; Riccardi et al., 2014), given that they share some characteristics with both classification and regression problems. Therefore, solving an ordinal classification problem consists on predicting the correct category from a set of discrete categories that are arranged following an order determined by the real problem. In contrast to regression problems, the predicted class is not continuous and the distance between two adjacent classes does not have to be the same for all the categories.\nIn the machine learning context, a classification problem can be formally defined as the problem of predicting the label y using an input vector x, where $x \\in X \\subseteq R^d$ and $y \\in Y = {C_1, C_2, ..., C_J}$, where $d \\in N^+$ and $J$ is the number of categories of the problem. To solve these problems, the objective is to find a function $r : X \\rightarrow Y$ that predicts the category of any given sample using the input data. In the particular case of an ordinal classification problem, categories have an intrinsic order which can be defined by the following order constraint: $C_1 < C_2 < ... < C_J$. Although the order"}, {"title": "2. Methodology", "content": "This section describes the regularised loss function presented in this work, as well as the soft labelling baseline approach that forms the foundation for it."}, {"title": "2.1. Baseline approach", "content": "The soft labelling approach introduced in Liu et al. (2020) and later employed in several works including the one that proposed using beta distributions Vargas et al. (2022) consists in replacing the loss function that is used to train a deep learning model with a regularised alternative where the standard labels are encoded with a soft alternative. To do that, they took the standard categorical cross-entropy loss function and defined its regularised alternative as follows:\n$L(x, k) = \\sum_{j=1}^J d'(j, k)[-\\log P(y = C_j|x)],$ (1)\nwhere $k = O(C_k)$, $C_k$ is the target class and $q'(j, k) = P_j(k)$ defines the soft labels encoding. $P_j(k)$ is the probability for the j-th class when the actual target is the k-th category. This probability is directly given by the probability mass function (p.m.f.) of any given discrete distribution or can be obtained using the probability density function (p.d.f.) of any continuous distribution in the following manner:\n$P_j(k) = \\int_{(j-1)/J}^{j/J} f_k(x) dx,$ (2)\nwhere $f_k(x)$ is the p.d.f. or p.m.f. associated with class $C_k$. The aforementioned probability can be obtained using any type of probability distribution. However, an unimodal distribution that is centered in the interval of the true class should be the most appropriate for ordinal problems. Note that the probability is sampled between the interval limits associated with the j-th class. These intervals are defined by dividing the [0, 1] space in equal-length intervals based on the number of classes J of the problem. Therefore, the"}, {"title": "2.2. Proposed methodology", "content": "The regularised loss function based on generalised beta distributions, which is proposed in this work and described in this section, is an enhanced version of the methodology proposed in Vargas et al. (2022). It aims to improve the classification performance of the extreme classes by using a Generalised Beta (GB) distribution as a replacement for the standard beta distribution employed in that work. The generalised beta distribution can be denoted as GB(a, u, v) and adds a new parameter (a) with respect to the standard beta distribution. The main goal of using a more flexible distribution is to improve the classification performance of the extreme classes. Said enhancement is achieved by reducing the variance of these distributions. In these terms, we consider using a = 2 for the extreme classes and a = 1 (standard beta) for the intermediate ones. Thus, the probability mass for the intermediate classes is concentrated in the middle of the interval while, for the first class it is around x = 0, and for the last class most of the probability is around x = 1."}, {"title": "3. Estimation of the parameters of the GB distribution as a function of the number of classes", "content": "Although there are different statistical methods for estimating the parameters of the GB(a, u, v) distribution, such as the method of the moments and the maximum likelihood method, they are too complex to be used to estimate the parameters of a generalised beta distribution due to the intractable integration expressions in the normalisation constant (Ma & Leijon, 2010). For maximum likelihood estimation, numerical methods can be used to calculate the shape parameters of a generalised beta distribution using the smallest Morder statistics (Gnanadesikan et al., 1967). In Narayanan (1992), the authors show a numerically feasible method for parameter estimation in the multivariate beta (Dirichlet) distribution through the method of maximum likelihood. Also, in Warsono et al. (2018), the authors use an iterative process to estimate the parameters of the generalised beta distribution. Finally, in Makouei et al. (2021), the authors derive recurrence relations for the single and the product moments of the order statistics as well as k-record values from the complementary beta distribution. However, the maximum likeli-"}, {"title": "3.1. Estimation of the parameters for the intermediate classes", "content": "This section describes the methodology followed to obtain the parameters of the generalised beta distribution for the intermediate classes (from 2 to J - 1). Since the beta regularisation approach proposed in Vargas et al."}, {"title": "3.2. Estimation of the parameters for the first class", "content": "Given that the a parameter is 2, in order to obtain values of v as a function of u and the number of classes, two constraints associated with the mean and the variance of the distribution of the first class are proposed. Thus, the first expression constrains the mean of the distribution, so that it has to be between 0 and the centre of the first interval:\n$E[X] = \\frac{u(u + 1)}{(u + v + 1)(u + v)} < \\frac{1}{2J}$ (19)\nThen, the second constraint is associated with a linear combination of the mean and the standard deviation:\n$0 \\leq E[X] - \\lambda S[X], S[X] < \\frac{1}{2J\\lambda},$ and $V[X] < \\frac{1}{4J^2\\lambda^2},$ (20)\nwhere X is the parameter that controls the linear combination and can be cross-validated for each dataset.\nGiven that $V[X] = E[X^2] - E^2[X]$, the second constraint can be defined as follows:\n$E[X^2] < \\frac{1}{4J^2\\lambda^2} + E^2[X] < \\frac{1 + \\lambda^2}{4J^2\\lambda^2}$ (21)"}, {"title": "3.3. Estimation of the parameters for the last class", "content": "The parameters of the last class are obtained in a similar way to those of the first class. Thus, considering the value a = 2, the first constraint can be set up as follows:\n$E[X] = \\frac{u(u + 1)}{(u + v + 1)(u + v)} = \\frac{2J-1}{2J},$ (26)"}, {"title": "4. Experiments", "content": "In this section, the experiments that were conducted to test the proposed method are described. In the first part of this section, the datasets considered are described. Then, the model is presented and, finally, the experimental design is explained."}, {"title": "4.1. Datasets", "content": "In this section, several ordinal problems whose input data comes in the shape of images are described."}, {"title": "4.1.1. Diabetic Retinopathy", "content": "Diabetic Retinopathy is a dataset of high-resolution eye fundus colour images. It was published in a Kaggle competition\u00b9 and since then has been commonly used as a benchmark dataset for ordinal classification methods (Wang et al., 2021; Wu et al., 2020; Xie et al., 2020; de La Torre et al., 2020). The dataset was provided as two separate splits for training and evaluation. The training set contains 17563 pairs of images, from the left and right eyes. On the other hand, the testing set is composed of 26788 pairs of images. Each image is labelled with one of five levels of diabetic retinopathy (DR) disease, where level 0 indicates a healthy patient while level 4 is associated with proliferative DR. The dataset is highly imbalanced given that most of the patients are healthy. Therefore, the number of samples on each of the categories is given by: 65342 for level 0, 6205 for level 1, 13152 for level 2, 2087 for level 3, and 1916 for level 4. These images were taken with different devices and different lighting conditions. Therefore, to improve the training process and the generalisation capability, a preprocessing step, that was de- described in Kaggle2, was performed aimed at enhancing the contrast of the images. Also, given that the original images are extremely high-resolution,"}, {"title": "4.1.2. Adience", "content": "Adience (Eidinger et al., 2014) is another benchmark dataset that is com- posed of colour images of human faces. Each image is associated with its gen- der and age. The age is given in different ranges, summing up to 8 classes. The whole dataset is composed of 26580 faces belonging to 2284 people. The images have been preprocessed in order to crop and align the faces, making the classification task easier. All the images have been resized to 256 \u00d7 256 and contrast-normalised. The original dataset was provided as five cross- validation folds. In this case, the first four folds are used for training and the last one for testing."}, {"title": "4.1.3. FGNet", "content": "FGNet (Fu et al., 2014) is a smaller faces images dataset composed of 1002 colour images with 128 \u00d7 128 resolution. These images belong to 82 different subjects. Each of the samples is labelled with the exact age of the person in the moment when the photograph was taken. Therefore, different categories can be obtained depending on the age grouping established. For this work, we have grouped the samples using the following intervals: [0,3), [3, 11), [11, 16), [16, 24), [24, 40), [40, +\u221e). Given that the whole dataset was provided without partitions, 20% of the data was used for testing and the"}, {"title": "4.1.4. UTKFace", "content": "UTKFace (Zhang et al., 2017) is a face dataset with a long age span (ranging from 0 to 116 years old). The dataset is composed of 20,000 face images with annotations of age, gender and ethnicity. These images cover large variations in pose, facial expression, illumination conditions, occlusion, resolution, etc. The dataset can be used for different types of tasks like face detection, age estimation or landmark localisation. In this case, we are in- terested in solving the problem related to age estimation, given that it is an ordinal task. The original images are already cropped and aligned and, thus, on each image, there is only one visible face. Every image is provided along with its corresponding meta-data: an integer indicating the exact age, the gender as a binary variable, the race as an integer, and the date and time when the picture was collected. In this work, we are only using the age variable. In order to convert the continuous age to an ordinal variable, 12 categories have been determined using the following intervals: [0,2), [2,6), [6, 12), [12, 19), [19, 23), [23, 27), [27,30), [30, 38), [38, 45), [45, 55), [55, 65), [65, 73), [73,80), [80, +\u221e). Then, the training and test partitions were cre- ated taking 80% and 20% of the complete set, respectively, in a stratified way."}, {"title": "4.1.5. Aesthetic Visual Analysis", "content": "The Aesthetic Visual Analysis (AVA) dataset was introduced in 2012 as a new benchmark dataset (Murray et al., 2012). AVA contains over 250,000 images along with a rich variety of meta-data, including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to the photographic style. In this work, our interest lies into the aesthetic scores, as they can be grouped into different intervals and be used as ordinal labels. Each image received from 78 to 549 votes in a range"}, {"title": "4.1.6. WIKI (IMDB-WIKI)", "content": "The IMDB-WIKI (Rothe et al., 2018) dataset is one of the largest public datasets for age prediction. It contains 500, 000 images obtained from IMDb and Wikipedia. In this work, we use a small subset of this dataset that was obtained using only the images from Wikipedia. Therefore, the total number of images is reduced to 62, 328. From these images, 80% are used for training the model, and the remaining 20% for evaluation. Also, this dataset requires a preprocessing step before using these images to train a model because some of them are blank, too low resolution or do not contain any faces. Thus, in the preprocessing step, we only selected those images that have are not blank and have a resolution higher than 100 \u00d7 100. Also, a face detection algorithm was employed to find the bounding box of the face and obtain a cropped and aligned image. After that, all the images were resized to 128 \u00d7 128."}, {"title": "4.2. Model", "content": "Convolutional Neural Networks (CNN) are the most popular alternative to work with image datasets. In this case, we are using a well-known archi- tecture that has achieved very competitive performance with a fairly small number of parameters. This model is commonly named ResNet18, and is the smallest type of residual network that has been widely used in previous works (He et al., 2016). The main advantage of using an already existing model is the possibility of considering pre-trained weights. In this case, the PyTorch library was used, and the model was initialised using the ImageNet weights. This kind of initialisation significantly enhances the convergence speed, thus, reducing the training time by decreasing the required number of training epochs. The output layer was replaced with a fully connected layer with Junits, being J the number of classes of each dataset. The weights of this layer were randomly initialised given that the pre-trained weights are"}, {"title": "4.3. Experimental design", "content": "The model described in Section 4.2 was trained and evaluated using the process described in this Section. First of all, 15% of the training set was split for validation in a stratified way. During the training process, the data was fed into the model using batches of 200 elements. The batch size was adjusted to find a balance between the computational time spent for the training process and the memory required to fit all the data into the GPUs memory. The same batch size was kept for all the datasets.\nThe Adam algorithm was selected for the optimisation process (Kingma & Ba, 2015), as it has been proved to obtain good performance in previous works (Na, 2022; Arcos-Garc\u00eda et al., 2018). The learning rate for the opti- miser has been adjusted every 7 epochs, multiplying it by a factor of 0.5. The initial learning rate was cross-validated using the training set and fixed to 10-3 for all the datasets. Also, the number of training epochs was adjusted in order to maximise the performance and minimise the computational time. Doing extra optimisation steps does not harm the performance, given that, at the end of the training process, the best model weights are selected taking into account the validation metrics. However, doing extra unneeded optimi- sation epochs can significantly increase the training time. In this way, the training epochs number was fixed to 25. It is worth noting that using the aforementioned pre-trained weights helps significantly reduce the required number of training epochs.\nFor the loss function considered during the optimisation process, different alternatives were tested:\n\u2022 Standard beta regularised CCE, proposed in Vargas et al. (2022).\n\u2022 Triangular regularised CCE, proposed in Vargas et al. (2023a)."}, {"title": "4.4. Performance metrics", "content": "With the aim to compare the proposed methodology against the previous alternatives which were proposed in the literature, six performance metrics are considered. Some of the are standard classification metrics (Accuracy), some others are intended for ordinal classification (WK, MAE and 1-off), and the other two are well-suited for unbalanced problems (MS and GMSEC). Specifically, the proposed GMSEC evaluates the performance in the extreme classes.\n\u2022 Weighted Kappa (WK) (de la Torre et al., 2018), which is based on the standard Kappa (\u043a) index. In this case, quadratic weights are used. This metric can be defined as follows:\n$\\kappa_\\omega = 1 - \\frac{\\sum_{ij} W_{ij} O_{ij}}{\\sum_{ij} W_{ij} E_{ij}},$ (34)\nwhere N is the number of samples, $w_{ij}$ are the elements of the penalisa- tion matrix (in this case, quadratic weights are considered, $W_{ij} = \\frac{(i-j)^2}{(J-1)^2},$ and MS metrics, maintaining competitive results in MAE, CCR and 1-off metrics.\nIn this way, the proposed methodology improves the sensitivities of the extreme classes without a significant detriment of the sensitivities of the intermediate classes for the six datasets considered in the experimental design."}, {"title": "6. Conclusions", "content": "In this work, a new unimodal regularisation approach for the loss function based on a generalised beta distribution was proposed. Also, a method to ob- tain the optimal distribution parameters for each class for a problem with any number of classes was presented. The main advantage of the proposed dis- tribution concerning the standard beta distribution lies on the enhancement of the classification performance of the extreme classes, while keeping a good classification performance for the rest of the categories. The proposed ap- proach was tested using 6 different datasets with variable number of classes. The average results of 30 executions for each dataset were compared with the results of the standard beta and other unimodal distributions proposed in previous works. The experimental results and the posterior statistical analysis showed that the proposed approach performed better than the other compared alternatives. Also, a more in-depth analysis was performed taking into account the standard beta and the proposed generalised beta to check whether the latter improved the sensitivity of the extreme classes. To do that, the GMSEC metric was introduced and the results shown that, effec-"}]}