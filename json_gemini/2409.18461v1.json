{"title": "Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration", "authors": ["Mahdi Morafah", "Vyacheslav Kungurtsev", "Hojin Chang", "Chen Chen", "Bill Lin"], "abstract": "Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes-from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) has rapidly gained traction as a promising approach to train machine learning models collaboratively across multiple devices, while preserving the privacy of user data. Standard federated learning methods, such as FedAvg [33], however, are primarily designed for unrealistic device-homogeneous scenarios, where all devices are assumed to have identical compute resource and can train the same neural network architecture [28, 33, 47, 21, 27, 46, 31]. Therefore, standard FL cannot support the participation of heterogeneous devices, all of which could significantly contribute to model training due to their unique and invaluable local datasets. To address this gap, knowledge distillation (KD) techniques have emerged as a promising approach to establish federation among heterogeneous device prototypes and facilitate knowledge transfer between them. In this approach, locally updated client models from different device prototypes, collectively termed as ensembles, serve as teachers to distill their knowledge into each device prototype's server student model using an unlabeled public dataset."}, {"title": "2 Related Works", "content": "Device Heterogeneous FL. Prior works on device heterogeneous FL have considered two distinct approaches with different objectives and settings. The first array of studies focuses on accommodating devices with varying compute resources, aiming to train a single global model. Techniques such as static and rolling-based partial model training allow devices to train a sub-model of the global model tailored to their compute resources [11, 18, 3, 1]. However, this approach does not fully reflect real-world scenarios. In practice, device prototypes such as IoTs and smartphones have unique neural network architectures designed for their specific configurations and underlying tasks, which may not support training varying neural architectures. This highlights a significant limitation in accommodating the full spectrum of device heterogeneity in this approach. The second array of studies addresses a more practical scenario where device prototypes with heterogeneous model architectures participate in FL to enhance their global model performance through mutual knowledge sharing [30, 41, 6]. In this context, KD techniques are used to transfer knowledge among prototypes, where locally updated client models, termed as ensembles, serve as teachers to distill their knowledge into each server's student model using an unlabeled public dataset. For example, FedDF [30] uses vanilla logit averaging, while Fed-ET [6] applies an uncertainty-weighted logit averaging, enhanced by a diversity regularization technique. However, existing works typically focus on settings where prototypes have similar capabilities\u2014both model and dataset sizes\u2014and thus neglecting the challenges in more diverse settings with varying capabilities. This oversight leaves their effectiveness in such settings largely unexplored. In this paper, we aim to study the underexplored diverse device settings. See Appendix A for a more detailed discussion on the related works.\nModel Editing via Task Arithmetic. Traditional methods for model editing often involve expensive joint fine-tuning across multiple tasks, which can limit scalability and democratization [60]. Recently, a promising technique called task arithmetic has emerged as a cost-effective and scalable method for updating pre-trained models with new information or refining undesired behavior [51, 37, 32]. The concept of \"task vectors\" introduced by Wortsman et al. [51] plays a pivotal role in these techniques. For any given task t, a task vector is derived by subtracting the model's pre-trained weights \\Theta_{pre} from its fine-tuned weights \\Theta_{ft} on task t, i.e. \\tau_{t} = \\Theta_{ft} - \\Theta_{pre}. These task vectors act as unique representations for specific tasks. Furthermore, researchers have demonstrated that by summing multiple task vectors {\\tau_{t}}_{t=1}^{T}, and integrating them into a pre-trained model via \\Theta = \\Theta_{pre} + \\sum_{t=1}^{T} \\lambda_{t} \\tau_{t}, one can effectively create a model capable of handling multiple tasks [51, 55]. To the best of our knowledge, this work is the first to extend the notion of task vectors to the federated learning setting, introducing a task arithmetic for knowledge distillation across diverse heterogeneous device prototypes."}, {"title": "3 Problem Statement: FL with Heterogeneous Device Prototypes", "content": "Consider a cross-device FL setup with a set of M distinct device prototypes \\mathcal{M}, i.e., M = |\\mathcal{M}|. Each device prototype m_{j} \\in \\mathcal{M} has a distinct neural network architecture f^{j}(.; \\theta^{j}_{k}) parameterized by \\theta^{j}_{k} \\in \\mathbb{R}^{n_{j}} and a set of clients C_{i}, with N_{i} = |C_{i}| clients in total. Each client c_{k} \\in C^{i} has a local private dataset \\mathcal{D}^{k}_{j} = {(x^{k}_{j}, y^{k}_{j})}, where n_{j,k} = |\\mathcal{D}^{k}_{j}|, and locally trains the parameters \\theta^{k}_{j} of the neural network architecture f^{j} on its local dataset. Furthermore, denote \\mathcal{D}^{j} = \\bigcup_{k \\in c_{i}} \\mathcal{D}^{k}_{j} to be the union of the private datasets for device prototype j. We assume \\mathcal{D}^{j} \\sim \\mathbb{D}_{j}, that is a subsample from the population distribution \\mathbb{D}_{j} and similarly \\mathcal{D} \\sim \\mathbb{D}. The union of the private datasets, i.e., \\mathcal{D} = \\bigcup_{j \\in \\mathcal{M}} \\mathcal{D}^{j}, is sampled from the entire population \\mathbb{D}, which is defined as an unknown mixture of the distributions each device prototype sampled its data from, i.e. generically non-i.i.d. We formalize this as a mixture of local clients data population, i.e., \\mathbb{D} = \\sum_{j}w_{j} \\mathbb{D}_{j} = \\sum_{j} \\sum_{k}w_{j,k} \\mathbb{D}^{k}_{j}, where 0 < w_{j,k} \\leq 1 and \\sum_{jk}w_{j,k} = 1, and w_{j,k} is unknown.\nThe ultimate objective is to minimize the test error and thus enable accurate inference for each device prototype j, aiming to obtain the optimal parameters for the population dataset:\n\\underset{\\theta^{avg}_{j} }{\\text{argmin}} \\underset{(x,y) \\sim \\mathbb{D} }{\\mathbb{E}} [l(f^{j}(x; \\theta^{avg}_{j}), y)] = \\underset{\\theta^{k}_{j} }{\\text{argmin}} \\sum_{j=1}^{M} \\sum_{k=1}^{N} w_{i,k} \\underset{(x,y) \\sim \\mathcal{D}^{k}_{j} }{\\mathbb{E}} [l(f^{j}(x; \\theta^{k}_{j}), y)]"}, {"title": "4 Background: Federated Ensemble Distillation", "content": "To address the limitations of standard FL in device heterogeneous settings, Lin et al. [30] proposed ensemble knowledge distillation to transfer knowledge between heterogeneous device prototypes in FL. This procedure consists of two stages: (1) local per-prototype FL, and (2) server-side vanilla ensemble distillation. The details of each stage discussed in the following paragraphs.\nLocal Per-Prototype FL. In this context, at each round r a subset of clients C from each device prototype j\\in \\mathcal{M} is randomly selected by the server and download their corresponding model initialization \\theta^{k}_{j}. Each client c_{k} \\in C, starting from this model initialization, locally train the model f^{j} on its local private data \\mathcal{D}^{k}_{j} by taking multiple steps of stochastic gradient descent. Then, they send back their updated parameters {\\theta^{k}_{j}}_{k\\in c_{i}} to the server. The server aggregates the received clients parameters, and computes \\theta^{avg}_{j} = \\sum_{k \\in c_{i}} \\frac{|\\mathcal{D}^{k}_{j}|}{\\sum_{k\\prime} |\\mathcal{D}^{k\\prime}_{j}|} \\theta^{k}_{j}. In classic federated learning formalism, the parameters \\theta^{avg}_{j} satisfy,\n\\theta^{avg}_{j} \\in \\underset{\\theta }{\\text{argmin}} \\sum_{k=1}^{N} \\underset{(x,y) \\sim \\mathcal{D}^{k}_{j} }{\\mathbb{E}} [l(f^{j}(x; \\theta), y)]\nVanilla Ensemble Distillation. In this stage, each server model f^{j} gets initialized with \\theta^{avg}_{j}, and undergoes updates using ensemble knowledge distillation. Here, heterogeneous client models from heterogeneous device prototypes, collectively termed as ensembles, serve as teachers, i.e. \\mathcal{T} := {f^{i}(.,\\theta^{k}_{i}) | i \\in \\mathcal{M}, k \\in C^{i}}, transferring their knowledge to each server student model, i.e. \\mathcal{S}_{i} := f^{j}(., \\theta). For simplicity, we drop the index for each server student model, denoting it as S. The ensemble distillation loss using a mini-batch of data from an unlabeled public dataset, i.e x \\in \\mathcal{D}_{public}, can be defined by the following equation:\nL_{ED} = KL\\Big(\\sigma \\big( \\frac{1}{|\\mathcal{T}|} \\sum_{f \\in \\mathcal{T}} F(x) \\big), \\sigma(S(x)) \\Big) \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } (AvgLogits)\nwhere \\sigma(.) is the softmax function. As illustrated in Eq. 3, vanilla ensemble distillation treats all heterogeneous device prototypes' ensembles equally by uniformly averaging their logits. This way of knowledge integration overlooks the individual strengths and informational value of each pro-totype's ensembles. As a result, the richer, more informative logits from stronger ensembles are diluted by less informative logits from weaker ensembles, leading to information loss. Furthermore,"}, {"title": "5 Task Arithmetic Knowledge Transfer and Integration", "content": "In this section, we introduce TAKFL, designed to overcome the fundamental limitations of previous approaches and enhance knowledge transfer across diverse heterogeneous device prototypes, which vary in size\u2014in terms of both model and dataset size. TAKFL consists of two main components: (1) individually transferring knowledge from each device prototype's ensembles, and (2) adaptively integrating knowledge via task arithmetic. Detailed descriptions of each component are provided in Section 5.1 and 5.2, respectively. An illustrative overview of TAKFL is presented in Figure 1b, and the full algorithm is detailed in Appendix B, Algorithm 1."}, {"title": "5.1 Knowledge Transfer from Individual Device Prototype", "content": "We begin by discussing our proposed knowledge transfer framework from each individual device prototype's ensembles. This process consists of two main components: ensemble knowledge transfer and self-regularization, each detailed in the subsequent paragraphs.\nEnsemble Knowledge Transfer. Vanilla ensemble distillation integrates the knowledge of vary-ing strength ensembles by uniformly averaging their logits. This approach can potentially trans-form or even degrade the overall quality of the knowledge being transferred, leading to suboptimal knowledge transfer. To effectively distill the unique knowledge and contributions of each pro-totype's ensembles, and to avoid dilution, information loss, and interference from other prototypes' ensembles, we propose transferring the knowledge from each prototype's ensembles separately and independently.\nSpecifically, let's consider \\mathcal{T}^{i} := {f^{i}(.,\\theta^{k}_{i}) | k \\in C^{i}} denotes the ensembles of device prototype i as teacher and S_{j} denotes the server student model of the device prototype j. Without loss of generality, we refer to each device prototype's server student model as just student denoted as S. Therefore, the knowledge distillation loss between the teacher ensembles \\mathcal{T}_{i} and server student S (\\mathcal{T}_{i} \\rightarrow S) is defined below:\n\\mathcal{L}^{CS}_{KD} = KL \\Big(\\sigma \\big(\\frac{1}{|\\mathcal{T}|} \\sum_{f \\in \\mathcal{T}} F(x) \\big), \\sigma(S(x)) \\Big)\nScaffolding Student from Noisy Ensemble Distillation. The ensemble distillation process may adversely impact the student, causing it to forget its own knowledge acquired through averaged locally updated parameters and be drifted into erroneous directions. This is primarily due to two key factors: (1) The ensemble distillation process introduces noise, mainly because the ensembles' logits are inferred on an unfamiliar public dataset they have not been trained on. These ensembles are originally trained on local private datasets, which usually differ from the unlabeled public dataset used for distillation. Moreover, other factors such as the presence of data heterogeneity within FL and insufficient training of some ensembles due to limited computational resources can exacerbate this noise, particularly in the initial rounds of federation. (2) The ensemble distillation process lacks supervision from the actual private datasets, which is the ultimate learning objective.\nTo scaffold the student models from the noisy and unsupervised distillation process, which may cause them to drift into erroneous directions and forget their invaluable self-knowledge, we introduce a KD-based self-regularization technique. Our self-regularization technique mitigates these issues by enforcing similarity between the logits of the student and its initial logits (when the student is initialized with averaged parameters) using KL divergence loss defined below:\nL_{self} = KL\\Big[\\sigma \\big(S(x;\\theta^{avg}_{j}) \\big), \\sigma(S(x))\\Big]\nOverall Knowledge Transfer Objective. The overall knowledge transfer objective from teacher ensembles \\mathcal{T}_{i} of device prototype i to the student S is the combination of the ensemble knowledge distillation loss \\mathcal{L}^{CS}_{KD} (Eq. 4) and the self-regularization loss L_{self} (Eq. 5) defined in the following:\n\\mathcal{L}^{T}_{i} = \\mathcal{L}^{CS}_{KD} + \\gamma L_{self}\nHere, \\gamma is a hyperparameter controlling the effect of self-regularization term. We associate the knowledge transfer from each device prototype i to a task T_{i} with the loss \\mathcal{L}^{T}_{i}."}, {"title": "5.2 Task Arithmetic Knowledge Integration", "content": "Herein, we delve into the details of our proposed method for customized integration of the separately distilled knowledge from heterogeneous ensembles. Drawing inspiration from recent advances in model editing via task arithmetic [51], where a pre-trained model's knowledge can be edited via task-specific vectors using arithmetic operation, we propose a novel customizable knowledge integration method via task arithmetic. To do so we extend the notion of task vector from centralized learning to federated learning. We conceptualize the averaged locally updated parameters, i.e. \\theta^{avg}, as a \u201cpre-trained\u201d, similar to those in centralized learning, and the parameters of the distilled model via knowledge transfer objective (Eq. 4), denoted as \\theta^{distilled}, as a \u201cfine-tuned\u201d version of the model (see Fig. 2). Consequently, the task vector \\tau_{i} associated with the knowledge transfer task \\mathcal{L}^{T}_{i} can be defined by subtracting the distilled parameters from the averaged locally updated parameters as follows:\n\\tau_{i} = \\Theta^{TS}_{i} - \\theta^{avg}.\nEssentially, task vectors serve as unique representations for the transferred knowledge from each prototype\u2019s ensembles to the student and encapsulate the distinct contributions of each prototype\u2019s ensembles to the student model. To selectively merge the knowledge of each prototype' ensembles into the student, we employ an adaptive task arithmetic operation as follows:\n\\Theta^{merged} = \\theta^{avg} + \\sum_{i \\in \\mathcal{M}} \\lambda_{i} \\tau_{i}\nwhere \\lambda_{i} denote the merging coefficient associated with task vector \\tau_{i}, and they sum to one, i.e. \\sum_{i \\in \\mathcal{M}} \\lambda_{i} = 1. The merging coefficients determine the extent of knowledge integration from each prototype's ensembles. Essentially, they enable the student to have customized knowledge inte-gration to achieve maximum performance. The student can determine these merging coefficients based on its own learning capacity and the relative knowledge and helpfulness of other device prototypes' ensembles. This approach provides an effective, low-cost, and scalable knowledge integration strategy in settings with diverse device heterogeneity. In our experiments, we considered this as a hyperparameter and tuned it manually or determined it using held-out validation sets which achieves similar results. More details can be found in Appendix F.3."}, {"title": "6 Theoretical Results", "content": "We present a theoretical understanding on the efficacy of knowledge distillation in device heteroge-neous FL. We argue that vanilla ensemble distillation (VED) diffuses the information from logits,which presents a notable disadvantage for solving (1). This effect is particularly pronounced when the teacher ensembles are from a device prototype of small capacity, and the student model is from a device prototype of large capacity. By contrast, our proposed method of task arithmetic knowledge integration, mitigates the drawbacks of VED and is able to simultaneously incorporate information from differently sized heterogeneous ensembles, efficiently filling up the capacity of each student with the most informative knowledge, achieving optimal knowledge transfer.\nAssumptions and Preliminaries. Standard practice, including the setting in consideration as well as the numerical experiments here, involves overparametrized neural networks, that is, the total number of weights far exceeds the training sample size. This implies that the set of weights that minimize the loss is non-unique, and moreover, it has been argued that they form a submanifold [8]. This submanifold structure of solution sets will provide the critical source of understanding the subsequent results. In particular, we shall consider knowledge distillation as filling up the capacity of device prototypes' models with basis vectors corresponding to submanifolds that minimize as many device prototypes' data distributions as possible.\nSince we are interested in server-side distillation across heterogeneous device prototypes, we assume optimal conditions at the local per-prototype FL level, meaning that the perfect solution for local per-prototype FL is achieved. The formal details of the assumptions and statements are presented in Appendix C."}, {"title": "7 Experiments", "content": "7.1 Main Experimental Setup\nDataset and Architecture. We evaluate our method on computer vision (CV) and natural language processing (NLP) tasks. For CV, we train image classification using CIFAR10/100 [24], CINIC-10 [9], and TinyImagenet [25]. For NLP, we fine-tune pre-trained models for text classification on MNLI [50], SST-2 [43], MARC [22], and AG News [58]. Our architectures include ResNet [17], VGG [42], and ViT [12] for CV, and small BERT variants [45] (-Tiny, -Mini, -Small) for NLP. We simulate a federated non-i.i.d setting using a Dirichlet distribution Dir(\\alpha), where a lower \\alpha indicates higher heterogeneity [27, 36]. Further details can be found in Appendix F.1 and F.2.\nImplementation Details. We use the Adam optimizer for both CV and NLP tasks. For CV, local training involves 20 epochs with a learning rate of 0.001, weight decay of 5e-5, and a batch size of 64. NLP training is conducted over 1 epoch with a learning rate of 3e-5, no weight decay, and a batch size of 32. For distillation, Adam is used with a learning rate of 1e-5 and weight decay of 5e-4 for CV, and 3e-5 with no weight decay for NLP. Batch sizes for distillation are 128 for CV and 32 for NLP. The softmax temperature is set at 3 for both tasks, with a temperature of 20 for self-regularization. Further details are provided in Appendix F.1 and F.2."}, {"title": "8 Conclusion and Discussion", "content": "In this work, we addressed a fundamental issue in standard federated learning: the lack of support for heterogeneous device prototypes. Existing KD-based methods often fall short in real-world scenar-ios, where device capabilities vary widely. To address this, we introduced TAKFL, a novel KD-based method that treats knowledge transfer from each prototype's ensembles as separate tasks and distills them independently. TAKFL susequently integrates the knowledge using an adaptive task arithmetic technique for optimized performance. We also introduced a KD-based self-regulation technique to mitigate issues arising from noisy and unsupervised ensemble distillation. The effectiveness of our method is substantiated by both theoretical results and extensive experimentation across CV and NLP tasks, using various datasets and models.\nLimitations remain, notably in real-world applicability. While TAKFL's effectiveness in an approximated real-world setup has been demonstrated, actual deployment on physical devices and in environments with extremely large models remains untested due to resource constraints. Experiencing TAKFL in genuine real-world settings could unveil additional challenges or limitations, providing further insights into its scalability and efficiency."}]}