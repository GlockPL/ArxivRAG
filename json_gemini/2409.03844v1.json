{"title": "MetaBGM: Dynamic Soundtrack Transformation\nFor Continuous Multi-Scene Experiences With\nAmbient Awareness And Personalization", "authors": ["Haoxuan Liu", "Zihao Wang", "Haorong Hong", "Youwei Feng", "Jiaxin Yu", "Han Diao", "Yunfei Xu", "Kejun Zhang"], "abstract": "This paper introduces MetaBGM, a groundbreaking framework for generating background music that adapts to dynamic scenes and real-time user interactions. We define multi-scene as variations in environmental contexts, such as transitions in game settings or movie scenes. To tackle the challenge of converting backend data into music description texts for audio generation models, MetaBGM employs a novel two-stage generation approach that transforms continuous scene and user state data into these texts, which are then fed into an audio generation model for real-time soundtrack creation. Experimental results demonstrate that MetaBGM effectively generates contextually relevant and dynamic background music for interactive applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Music, as a universal cultural artifact, transcends linguistic boundaries, serving as a potent medium of expression. Traditional music composition, however, demands considerable expertise and time. Recent advancements in datasets and audio generation models, exemplified by frameworks like MusicLM [1], TANGO [2], MusicGen [3], WavJourney [4], MusicLDM [5], and AudioLDM [6], have revolutionized this process, enabling rapid AI-driven music composition from text prompts.\nDespite these innovations, interactive music composition for dynamic multi-scene transitions remains underexplored. This is particularly crucial in automatic background music composition, where AI struggles to fluidly capture scene transitions and generate music that aligns with human expectations. Current models, which rely heavily on static music descriptions for text-to-music generation, falter in dynamic environments like film and video game scoring, where music must intricately synchronize with the narrative and atmosphere. In interactive contexts, such as gaming and interactive media, music must adapt seamlessly to real-time scene changes and user interactions, necessitating significant advancements in audio generation models to discern and respond to these shifts by producing coherent, contextually appropriate background music. Therefore, generating high-quality music descriptions becomes a critical research challenge.\nTraditionally, crafting these descriptions has been the purview of skilled musicians, a process both time-consuming and resource-intensive [7]. The advent of large language models (LLMs) presents new opportunities for generating music descriptions in complex scenarios [8]. These models can transform scene and interaction data into coherent, contextually appropriate textual descriptions. Within this framework, procedural narrative [9] generation converts continuous scene data into descriptive texts [10] [11], capturing dynamic shifts in scenes and character interactions [12]. For instance, SceneCraft [13] showcases this by generating adaptive dialogue paths that enhance the immersive quality of digital games. Automatic music description generation then translates these narratives into corresponding music texts, providing precise inputs for audio generation models. Projects like Noise2Music [14], MeLoDy [8], and LP_MusicCaps [15] leverage LLMs to generate rich music descriptions, yet most research remains focused on static descriptions, overlooking challenges in multi-scene transitions and real-time interactions. Moreover, despite advances in controllable music generation, the flexibility required for real-time adaptation remains insufficient.\nThis study introduces a novel method to transform continuous scene data into music descriptions interpretable by audio generation models, enabling the creation of adaptive background music. We propose an innovative algorithm for interactive, multi-scene music description generation, utilizing a two-stage pipeline to produce effective music descriptions. These descriptions are then used by audio generation models to create background music that seamlessly aligns with dynamic, continuous scenes in real time. Using Minecraft as a case study, we developed a real-time backend data collection algorithm and implemented a two-stage LLM-based generation process, encompassing both procedural narrative generation and music description synthesis. Additionally, we fine-tuned open-source LLMs to optimize music description outputs.\nOur contributions introduce the task of interactive real-time background music generation and propose the MetaBGM"}, {"title": "II. PROPOSED APPROACH", "content": "We employed the open-world continuous scene environment of Minecraft, celebrated for its intricate settings and dynamic interactions. Minecraft features over 62 distinct scenes and a plethora of player actions, offering a robust dataset for multi-scene and interaction analyses, alongside more than 50 adaptable music tracks.\nAs illustrated in Fig. 1, the framework operates by capturing real-time scene and user interaction data during each time segment [16], including context and actions. This data is incorporated into the model prompt and processed by the LLM to generate procedural narrative text. This narrative is then employed to create contextually aligned music description text. The audio generation model uses this description to produce real-time background music, with each piece serving as a melodic anchor for the next segment, ensuring fluid continuity across scene transitions. The interval between segments is adjustable.\nWe deployed a continuous scene client-server framework using official open-source code, with the client initializing the environment locally and the server handling real-time scene and user interaction data. To support this, we engineered a real-time backend data acquisition algorithm that captures the player's scene, status, and actions in JSON format at customizable intervals, typically every 10 seconds. The algorithm continuously aggregates data across 11 elements-biome, time, weather, temperature, health, hunger, action state, and combat state. A multi-threaded approach ensures seamless data collection, with the main thread acquiring data while auxiliary threads manage aggregation and parsing. Extracted data often includes superfluous details such as \"not on fire,\" \"not running,\" or \"not sneaking,\" which are only relevant in affirmative contexts and otherwise introduce unnecessary complexity. Furthermore, backend data is stored in double-precision floating-point format, imposing excessive precision that burdens the language model without enhancing insight. The significance of these data points fluctuates with context; for example, during combat, environmental data like weather is less critical, while the player's health and actions become paramount. Conversely, in non-combat exploration, the relevance of combat data diminishes, and environmental factors gain importance. To address these variances, we developed a data characterization algorithm that prioritizes relevant information based on context, filtering out redundant details and adjusting data precision as necessary. The detailed data characterization algorithm is illustrated in Algorithm 1."}, {"title": "C. Two-Stage Music Description Generation", "content": "After acquiring scene and user interaction data, we adopt a two-stage method for generating music descriptions. Initially, the JSON-formatted data is converted into narrative text, which is subsequently used to create the corresponding music description. This two-stage approach, rather than direct music description generation, is employed due to the lower readability and structured nature of JSON data compared to narrative language, which can yield less varied and rich music descriptions. Furthermore, in language modeling, the Chain of Thought method indicates that incorporating intermediate reasoning steps into the model's prompts can significantly enhance its reasoning capabilities [17]. Thus, by first generating narrative text, the model attains a better comprehension of the task, leading to more accurate and contextually relevant music descriptions.\nTo enhance the readability and comprehensibility of the characterized JSON-formatted data, we drew inspiration from procedural narrative generation techniques and transformed the JSON data into"}, {"title": "III. EXPERIMENT", "content": "This study necessitates paired datasets of Scene Data-Narrative Text and Narrative Text-Music Description Text. To construct the training set, two strategies were employed. First, leveraging chatGPT-3.5-Turbo, 433 feature-rich scenarios were synthesized by varying parameters such as scene, time, weather, and player status, yielding 433 Scene Data-Narrative Text and Narrative Text-Music Description Text pairs [21]. Second, by employing reverse generation, we curated music descriptions for 2,000 tracks from the Song Describer Dataset [22] and MusicCaps [1], subsequently matching them with suitable Minecraft scenes via chatGPT-3.5-Turbo, thereby generating an additional 1,539 Narrative Text-Music Description Text pairs.\nTo enhance narrative and music description generation while preserving the core capabilities of LLMs, the LoRA [23] method was employed for its computational efficiency and effectiveness. The model was fine-tuned using 1,972 pairs of Scene Data - Narrative Text and Narrative Text - Music Description Text from the Section III-A. These datasets were structured into an Instruction-Input-Output format for LoRA fine-tuning, optimizing performance during supervised learning.\nTo evaluate the model's efficacy, this study analyzed 13 original background music tracks from Minecraft, each tied to a specific scene. Using the LP_MusicCaps [15] model, these tracks were transformed into 304 descriptive segments at 10-second intervals, serving as a test set due to their scene-specific nature. For assessment, we utilized established bilingual translation metrics, including BLEU-1, BLEU-2, BLEU-3, and BLEU-4, alongside the METEOR metric for translation accuracy and recall, and ROUGE-L for comparing summary and reference similarity."}, {"title": "D. Results and Analysis", "content": "The baseline models encompass prominent LLMs such as Meta's LLaMA-2-7B, LLaMA-2-7B-Chat (unfine-tuned), LLaMA-2-13B [19] [20], Tsinghua University's ChatGLM-3-6B [24], and OpenAI's GPT-3.5-Turbo. The experiments initially compared direct music description generation with the proposed two-stage generation method, as shown in TABLE I. The two-stage approach, which first transforms JSON-formatted game data into scene descriptions before generating music descriptions, consistently outperformed direct generation across all metrics. This validates the hypothesis that incremental generation enhances logical reasoning and accuracy in large models, paralleling chain-of-thought reasoning in AI.\nTABLE II encapsulates the results for all baseline models. The MetaBGM method, employing a fine-tuned LLaMA-2-7B-Chat and the two-stage generation process, exhibited substantial improvements over baseline models, with the exception of GPT-3.5-Turbo. While GPT-3.5-Turbo combined with two-stage generation marginally surpassed our method, it is crucial to note that GPT-3.5-Turbo contains over 175B parameters\u201425 times more than LLaMA-2-7B-Chat. Despite this discrepancy in parameter count, the fine-tuned LLaMA-2-7B-Chat demonstrated superior performance, particularly in managing conversational inputs, validating its selection for this study."}, {"title": "IV. CONCLUSION", "content": "MetaBGM is an LLM-based framework that generates background music corresponding to continuous scenes and user interaction events through a two-stage music description generation method. The key idea involves using multi-threading to obtain contextual data from continuous scenes and user interactions, calculating features, and then converting them into music description text interpretable by the audio generation model. The evaluation results demonstrate the effectiveness of this background music generation method, with objective metrics indicating that MetaBGM can produce background music that aligns with scene transitions and user events at a relatively low cost."}], "equations": ["\\mathcal{L}_{PM}(Y \\mid x) = \\prod_{t=1}^{L} P_{\\theta_{M}}(y_{t} \\mid Y_{<t}, x)"]}