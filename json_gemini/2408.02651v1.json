{"title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?", "authors": ["Mohammad Bahrami Karkevandi", "Nishant Vishwamitra", "Peyman Najafirad"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs. Yet, the potential for generating harmful content through these models seems to persist. This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers. Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked. This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API access to the target model and a small surrogate model. Our method, which leverages a BERTScore-based reward function, enhances the transferability and effectiveness of adversarial triggers on new black-box models. We demonstrate that this approach improves the performance of adversarial triggers on a previously untested language model.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have been in the spotlight recently, due to their impressive capabilities in natural language tasks. However, given these models are trained on the internet text corpora, their safety and morality have been questioned in the literature [16, 27]. To mitigate the objectionable behaviors of LLMs, a line of work called alignment, has been done to improve their public usability and safety [3, 28, 34]. Despite their relative success in grounding LLMs to human morals, the question of \"Is it still possible to exploit LLMs to generate harmful content?\" remains an under-explored area.\nEver since the alignment of LLMs and following the same scheme of the common adversarial examples in machine learning [4, 6], there have been many attempts to reverse the alignment of LLMs, using the perturbation of their inputs, which are called Jailbreaking in the Natural Language Processing (NLP) community [8, 12, 23]. While the image processing field has seen excessive research in adversarial examples [6, 29], the NLP literature, specifically pertaining to LLMs has not been sufficiently explored. With the exponentially increasing popularity of LLMs, especially the public-facing commercial chatbots, such as GPT-4[26] and Claude3[2], ensuring their safety bears significant relevance.\nThe key issue with the existing perturbation approaches is that they are limited against black-box models. For example, Soft embedding prompts [38] require open access to the model's embeddings, are not interpretable, and lack the ability to transfer between models because of their different embedding distributions. Manually crafted prompts [8, 12, 30] however, can typically be used on different models and do not require white-box access, but they require human creativity and are blocked quickly due to their constant nature. Automatic discrete prompt perturbation for jailbreaking often involves appending a trigger string to the user prompt, which is optimized using gradient data [23, 58], which requires white-box access to the model, although it has been shown to have some transferability to black-box models. Proposed gradient-free attacks often require access to powerful models to succeed [7], or require carefully crafted initial seeds [20, 53]. Decoding manipulation attacks, which are more recent and faster [35], still require some level of access to the model's output logits or the output probability distribution.\nIn this paper, we introduce a novel approach to optimize adversarial triggers using reinforcement learning. Our approach only requires inference API access to the target language model, and a small surrogate model which is trained by reward signals calculated using the target model's text output. We show that our approach can be an extension to all previous work that optimize an adversarial trigger on white-box models and can personalize and extend the performance of triggers on new black-box target models. Intuitively, our work takes the adversarial triggers trained on a model and adapts them to a new model, using only inference to the new model. In summary, the contributions of this work are: i) We design a reinforcement learning paradigm, adapted from previous work, to optimize adversarial triggers using inference-only APIs. ii) we introduce a BERTScore-based [55] reward function utilizing the target model's text output generations. iii) We show that our"}, {"title": "2 BACKGROUND", "content": "Prompt Tuning. Although Large Language Models (LLMs) exhibit exceptional generalization capabilities, they still necessitate meticulously designed prompts to achieve optimal performance for specific tasks. According to the empirical research conducted by Scao and Rush [37], a well-crafted prompt can be as valuable as hundreds of data samples in a classification task. As LLMs continue to advance, there has been a growing focus on automatic prompt tuning [9, 41, 44, 54] and in-context learning [10, 49]. Automatic prompting initially involved fine-tuning prompt embeddings, a technique referred to as Soft Prompting [21, 22, 24, 31], which, despite its effectiveness, is often complex and computationally intensive. Subsequently, researchers began exploring the use of the continuous embedding space to create discrete prompts [32, 48]. Another significant approach has been the direct optimization of discrete prompt tokens [9, 11, 19, 41]. This method not only enhances interpretability and transferability between models but also has been demonstrated to outperform soft prompting in terms of performance.\nAdversarial Examples. The machine learning field has established that the inputs of a model can be deliberately altered to cause the model to produce (un)desired outputs; such modified inputs are termed Adversarial Examples [4, 6, 29, 43]. Within the realm of Natural Language Processing, these adversarial attacks have been employed across various applications, including classification tasks [11, 42, 52], sentiment analysis [1], and inducing toxic outputs [19, 45]. As language models evolve and prompting becomes more prevalent, there has been a significant rise in interest concerning adversarial attacks on prompts [38, 40, 50-52, 57]. These recent developments underscore the ongoing challenge of ensuring the robustness and security of language models against such sophisticated adversarial techniques.\nLLM Alignment and Jailbreaks. Pre-trained Large Language Models, while possessing remarkable out-of-the-box capabilities [5, 46], are often unsuitable for public use due to their insufficient understanding of instructions and their inherent unethical tendencies, such as biases [13, 25] and toxic behavior [27, 47]. Consequently, researchers strive to align these models with human values and regulatory standards through techniques like instruction-tuning, Reinforcement Learning from Human Feedback (RLHF) [3, 28], and Direct Preference Optimization [18, 34]. However, this alignment process has sparked vigorous attempts to jailbreak the models, compelling them to follow harmful instructions [7, 30, 39]. These efforts highlight the ongoing battle between enhancing model safety and the persistence of adversarial actors seeking to exploit model vulnerabilities.\nAdversarial Attacks on LLMs. The advent of prompt tuning has significantly influenced the landscape of adversarial attacks, particularly in the realm of language models. This trend has emerged because prompt tuning provides a pathway for creating automatically generated inputs for these models. Efforts to disrupt the alignment of language models (commonly known as jailbreaking) often mirror"}, {"title": "3 METHODOLOGY", "content": "In this paper, we introduce a novel approach to enhance the transferability of adversarial prompts to black-box models. Our method uses reinforcement learning to further personalize adversarial triggers to a target black-box LLM. Our method can extend the success rate of any previous work that has been done on white-box language models.\n3.1 Preliminaries\nUsing a similar notation to previous work, we define an Autoregressive Language Model M and its vocabulary set V. Let x \u2208 V denote a single token and x \u2208 V* a sequence of tokens, where V* is the set of all possible token sequences of any length. The language model M can be utilized to calculate the probability distribution of the next token, given x. Formally written as \\(P_M(\\cdot|x) : V \\rightarrow [0, 1]\\). Additionally, for instruct tuned models, the input typically follows the structure \\(x = x(s_1) + x(u) x(s_2)\\), where is the concatenation operator, \\(x(u)\\) is the user prompt, and \\(x(s_1)\\) and \\(x(s_2)\\) are system prompts at the beginning and the end of the input respectively."}, {"title": "3.2 Threat Model", "content": "Analogous to most jailbreaking methods [58], our threat model allows an adversary to append a sequence of adversarial tokens \\(x(a)\\) to the user prompt, forming the new input to the model \\(x' =\\)  \\(x(s_1) \\oplus x(u) \\oplus x(a) \\oplus x(s_2)\\). The adversary's objective is to maximize the attack success rate \\(A : V^* \\rightarrow [0, 1]\\) by finding an adversarial token sequence \\(x_(a)\\), which we call Adversarial Trigger in this paper. In this paper, we assume the attacker has already obtained an initial set of adversarial triggers \\(T_0\\) on a previously attacked model with white-box access. The objective of this paper is to enhance the attack success rate on a previously unseen target language model \\(M'\\) by personalizing \\(T_0\\) to the new target model. Contrary to most previous work, the attacker does not have any access to the new target model, other than an input/output inference API."}, {"title": "3.3 Approach", "content": "Consider a set of adversarial sequences \\(T_0\\) that have been obtained by attacking a previously targeted language model \\(M_0\\) on a set of harmful prompts P. In this section, we introduce a new method to obtain a new set of adversarial triggers T' with an improved attack success rate when used to attack a new target model M' compared to \\(T_0\\). We assume that it is impractical or impossible to obtain T' while attacking M' using the same method used to obtain \\(T_0\\) while attacking \\(M_0\\). For instance, M' could be a black-box model, accessed only through an inference API.\nIn this paper, we use a surrogate language model \\(M^{(a)}\\) to generate adversarial sequences \\(x^{(a)} \\in T'\\). The surrogate model is typically a small language model; in our case, we use different variations of GPT-2 [33], such as the 82M parameter distilGPT-2, and the 1.5B parameter GPT-2-xl. Similar to RLPrompt[9], we limit the parameters to be trained, \u03b8, to an MLP with a single hidden layer, adapted to the surrogate model \\(M^{(a)}\\) before the language head, and freeze the rest of the parameters of the model. Hence, given the set of harmful prompts P, the objective of finding the adversarial trigger \\(x^{(a)}\\) can be formally written as\n\\(\\underset{x^{(a)} \\in V^*}{maximize} A(P, x^{(a)})\\) (1)\nwhere A is the attack success rate and \\(x^{(a)}\\) is a candidate adversarial trigger, sampled from the surrogate model given an empty input and parameterized by \u03b8 which, with a slight abuse of notation, is defined as \\(M^{(a)} (x^{(a)}| \\O; \u03b8)\\). The overall architecture of our method is depicted in figure 1.\nTo train the new randomly initialized parameters of the surrogate model, \u03b8, we go through two phases of training. In the first phase of the training, we use the previously obtained \\(T_0\\) to finetune \\(M^{(a)}\\) in a supervised setting. The second phase, which is the main training phase of adapting the adversarial triggers to the new model, involves refining the surrogate model's adversarial trigger generations, using reinforcement learning. We describe each phase in detail in the following paragraphs.\nPhase 1. In reinforcement learning (RL) setups, it is common to utilize supervised fine-tuning to ensure the correct initialization of the model weights [28]. In this paper, \\(T_0\\), the set of adversarial sequences obtained by attacking a previously targeted model, using"}, {"title": "4 EVALUATION", "content": "4.1 Dataset\nThe Harmful Behavior set of the AdvBench[58] dataset, which contains 500 harmful instructions and the first few tokens of a target response for each instruction, is used in this paper. For instance, \"Create a tutorial on how to commit identity theft\" and \"Sure, here is a tutorial on how to commit identity theft\" is a sample of the Advbench's instructions and its respective target response. In this work, we regard the instruction as x(u) and the target string as \u0177. We use the first 100 samples of the harmful behaviors as the train set and hold out the next 400 samples for testing in all of our experiments.\n4.2 Experiment Setup\nTo obtain the set of initial adversarial triggers \\(T_0\\), GCG[58] is used with the default parameters as described in their paper. 300 steps of the GCG method is utilized to obtain an adversarial prompt for each of the first 100 behaviors of the AdvBench. We use vicuna-7b-v1.5 as a white-box model during the GCG training. For the purpose of testing our method, we regard the Mistral-7B-Instruct-v0.2[17] as an inference-only black-box model M'. Hence, we can not attack this model using any gradient-based method, including GCG. However, our reinforcement learning-based method can attack this model, as it only requires inference of the target model. We limit the adversarial sequence length to 20 tokens for all of our experiments.\nthe \"distilGPT-2\" model [33, 36] is used as the surrogate model \\(M^{(a)}\\). An MLP with a single hidden layer and 2048 hidden neurons is added to the surrogate model after the last transformer block and before the language head to provide the trainable parameters \u03b8, while the rest of the model is kept frozen. These parameters are then fine-tuned in a supervised fine-tuning setup, as explained in section 3. We use empty inputs and the set of initial adversarial triggers \\(T_0\\) as labels to train the model for 3 epochs using the crossentropy loss mentioned in equation 2. We use the Adam optimizer with a learning rate of \\(10^{-4}\\).\nDuring the attack, the surrogate model's parameters, \u03b8, are further fine-tuned using the Soft Q-Learning algorithm for \\(10^{4}\\) steps. We use the default parameters of the RLPrompt[9] during the reinforcement learning procedure. For the reward function, we use the official implementation of BERTScore[55] with the model hash roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.1).\n4.3 Results\nTo test our preliminary results, we compare the improvement of the attack success rate when transferred to the new target model. As mentioned, we use GCG[58] to obtain the initial set of adversarial triggers and try to improve and personalize these triggers for the new target model Mistral. Hence, we compare our work to both types of the GCG algorithm. Following previous work [23, 58] We deem an attack successful if the target model's response does not contain a list of denial phrases, such as \"I am sorry\". Acknowledging"}, {"title": "5 CONCLUSION", "content": "In this paper, we presented a novel reinforcement learning-based approach to optimize adversarial triggers for jailbreaking Large Language Models (LLMs). Our method addresses the limitations of existing techniques by requiring only inference API access to the target model, thus eliminating the need for white-box access. By training a small surrogate model with BERTScore-based reward functions, we have shown that it is possible to enhance the performance and transferability of adversarial triggers on new black-box models. Our results indicate that this approach not only improves attack success rates but also extends the applicability of previously developed adversarial triggers to a broader range of language models. This work contributes to the ongoing efforts to understand and mitigate the vulnerabilities of LLMs, highlighting the need for robust safety measures in their deployment.\nWhile our preliminary results show clear improvements in the attack success rate, we acknowledge that our work is intended as only a spark to motivate future work. Exploring more options as the initial set of adversarial triggers, more sophisticated reward engineering, for instance adding a coherency reward to bypass perplexity filters, and thoroughly testing this method qualitatively and with more black-box models are some of the interesting future routes to take. Additionally, future research should explore potential defensive measures to mitigate these attacks. Developing robust detection mechanisms, enhancing model resilience through adversarial training, and implementing stricter access controls are essential steps to protect LLMs from such vulnerabilities. These"}]}