{"title": "Simulating User Agents for Embodied Conversational-AI", "authors": ["Daniel Philipov", "Vardhan Dongre", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "abstract": "Embodied agents designed to assist users with tasks must possess the ability to engage in natural language interactions, interpret user instructions, execute actions to complete tasks and communicate effectively to resolve issues. However, collecting large-scale, diverse datasets of situated human-robot dialogues to train and evaluate such agents is expensive, labor-intensive, and time-consuming. To address this challenge, we propose building a large language model (LLM)-based user agent that can simulate user behavior during interactions with an embodied agent in a virtual environment. Given a specific user goal (e.g., make breakfast), at each time step during an interaction with an embodied agent (or a robot), the user agent may \"observe\" the robot actions or \"speak\" to either proactively intervene with the robot behavior or reactively answer the robot's questions. Such a user agent assists in improving the scalability and efficiency of embodied dialogues dataset generation and is critical for enhancing and evaluating the robot's interaction and task completion ability, as well as for future research, such as reinforcement learning using AI feedback. We evaluate our user agent's ability to generate human-like behaviors by comparing its simulated dialogues with the benchmark TEACh dataset. We perform three experiments: zero-shot prompting to predict the dialogue act from history, few-shot prompting, and fine-tuning on the TEACh training subset. Our results demonstrate that the LLM-based user agent can achieve an F-measure of 42% in mimicking human speaking behavior with simple zero-shot prompting and 43.4% with few-shot prompting. Through fine-tuning, we achieved similar success in deciding when to speak but much greater success in deciding what to say, from 51.1% to 62.5%. These findings showcase the feasibility and promise of the proposed approach for assessing and enhancing the effectiveness and reliability of robot task completion through natural language communication.", "sections": [{"title": "1 Introduction", "content": "Embodied agents or robots designed to assist users with tasks should be able to engage in natural language interactions and communicate effectively with their users to resolve issues that arise during task completion. It is costly and labor intensive to collect datasets to train such agents, as also supported by the few datasets available [Thomason et al., 2019, Shridhar et al., 2020, Gervits et al., 2021, Padmakumar et al., 2022, among others]. Furthermore, interactive datasets are needed to evaluate the task completion abilities of these agents.\nIn this work, we propose an LLM-based user proxy agent that simulates user behavior in human-robot interactions using a virtual environment, AI2Thor Kolve et al. [2017]. While the use of user simulators for task-oriented dialogue systems (TODS) is well-established (see Section 2), the application of conversational embodied AI user simulators that leverage LLMs is relatively unexplored. Addressing this gap is significant given the increasing capabilities of LLMs in generating natural and contextually appropriate dialogue. Embodied agents must interact with their users due to various reasons, such as"}, {"title": "2 Related Work", "content": "In this work, we represent the actions of the conversational user actions in terms dialogue acts. Annotations of dialogue acts are frequently found in task-oriented dialogue datasets and are often utilized to determine the next action for the agent in dialogue management or the next action for the user in user simulation. Gella et al. [2022] presents a dialogue act annotation schema for embodied task completion utilizing dialogues of the TEACh dataset and then extends it by fine-tuning language models to tag dialogue acts (see Appendix A for a list of dialogue acts used in this work and their explanations), predict the next dialogue act given a dialogue history, and guide the agent's non-dialogue behavior. Our user simulator approach works similarly, except, in addition to dialogue acts, we also predict the turn taking behavior of the user agent.\nMany previous studies proposed methods for building user simulators for TODS [Schatzmann and Young, 2009, G\u00fcr et al., 2018, Asri et al., 2016, among others]. For instance, Davidson et al. [2023]"}, {"title": "3 Approach", "content": "A session, $S = \\{(s_1, a_1), \\ldots, (s_t, a_T)\\}$, of interactions between a user and an embodied agent can be represented as a sequence of pairs, $(s_i, a_i)$, where $s_i$ represents the agent that performed an action at step i, $s_i \\in \\{robot, user\\}$, and $a_i$ represents the action that the agent $s_i$ performed at that step. The set of possible actions $A$ is the union of the set of possible dialogue acts, $D$, for conversational actions that the robot or the user can execute and the set of navigation and object manipulation actions, $P$, that the robot can execute. Then, building a user simulator involves predicting whether the user should not perform a conversational action and simply \"observe\" (i.e., the robot is performing an action at the next time step) or whether the user should perform a conversational action and what should be the dialogue act of that action, for each time step, given all the interaction history until that time. Hence, the input to the user simulator for step i, $x_i$, is the sequence $(s_1, a_1), ..., (s_{i-1}, A_{i-1})$, and the goal of simulation is to predict $y_i \\in \\{\\\"observe\\\"\\} \\cup D$. In this work, similar to Padmakumar et al. [2023], the user simulator produces dialogue acts (e.g., Instruction or Confirm) which can then be converted to natural language user responses using templates formed from the examples of the training dataset.\nThese steps in a session are not supposed to be regularly distributed over time, and their duration would differ depending on the action. To tackle this issue, if an \"observe\" action is predicted for the user, we expect the user simulator to wait until the robot performs an action, and after that, user simulator outputs its next action. However, this may result in an infinite loop, in case the robot also doesn't take an action. Hence, during inference, we introduce a maximum time to observe, and then the user simulator is forced to predict a conversational action. In this work, we experiment with zero- and few-shot methods that instruct an LLM to predict the next user action."}, {"title": "3.1 Instructing LLMs for Simulating Users", "content": "The first thing presented to the LLM is a description of the role it should play. In the description of the problem, dialogue acts are introduced and explained in order to be guide the LLM in the examples and the answers to the tasks given. Dialogue acts are explained using the descriptions in Gella et al. [2022] (See Appendix).\nIn experiments labeled \"FS,\" for few-shot, we included five example scenarios. Examples were selected as variable-length sequences from the randomly selected sessions of the training dataset. The examples all began at the beginning of that session, and continue for a random length of turns. Examples were re-drawn from the dataset if too many of the user turns (i.e., greater than 35%) had the \"OBSERVE\" answer, in an effort to represent more of the conversational turns to the LLM. In this case, up to two examples can be answered \"OBSERVE.\"\nEach example began with the user goal:\nGoal: make me a sandwich\nThen, continued with a sequence of \"COMMANDER\" and \"DRIVER\" action pairs, i.e.:\nCOMMANDER: <observe>\nDRIVER: <pickup Bread>\nNon-dialogue actions were surrounded by angle brackets, signifying that they are not text. Actions with a target, such as pickup actions, had it enclosed within the angle brackets. Dialogue actions were written in plain text, followed by the dialogue act(s) enclosed in pairs of angle brackets, i.e.:\nCOMMANDER: i would like the remote put on the side table \u00abInstruction>>>\nDRIVER: <observe>\nEach example ended with a request for the user agent's response, followed by the answer to the example problem, formatted like the following:\nCOMMANDER response:\nInstruction\nExperiments labeled \"ZS,\" for zero-shot, skipped directly from the dialogue acts to the task, and did not include examples. The task was then described to the LLM, where the LLM was instructed to respond with either \"OBSERVE\" or a dialogue act. After, the scenario was described to the LLM, in the same format as the examples, except the answer is omitted for the LLM to predict."}, {"title": "4 Experiments", "content": "For experimental validation, we use the TEACh dataset Padmakumar et al. [2022] of situated dialogues between human annotators playing the role of a user (Commander) or a robot (Follower/Driver). The annotators interact to complete a high level task from a given set of tasks in simulated household environments, such as making coffee or watering a plant. Each interaction session includes the definition of the high-level task (presented only to the Commander) and the sequence of actions performed by the two sides during that session. The robot engages in a dialogue with the user to learn the task to be completed, obtain information about objects involved in the task, and perform navigation or object manipulation actions in the simulated environment to achieve the task. The user can observe the actions that the robot is taking in the environment and interact with the robot in natural language. In this work, we use the dialogue acts for user and robot utterances annotated in Gella et al. [2022].\nTEACh dataset consists of about 3K interaction sessions split into 5 subsets: training, validation seen, validation unseen, test seen, and test unseen. Since the test sets are not publicly available, we report our results on the validation seen subset. This subset includes 181 sessions and 7923 steps. In these steps, either the user issues a conversational turn (17.6%), the robot issues a conversational turn (13.2%), or the robot issues a navigation or object manipulation action (69.2%)."}, {"title": "4.2 Experiment Details", "content": "Our experiments evaluated the user simulation model's capability across different prompting methods and configurations to replicate human-like interactions with embodied agents. We tested two primary prompting approaches: zero-shot, where the model predicts actions without prior example guidance, and few-shot, where selected in-context examples help orient the model's responses. Additionally, to assess the influence of non-verbal actions, we compared model performance on datasets both with and without move actions, examining how these configurations impacted turn-taking (Speak-F1) and Dialogue Act accuracy. A baseline model was established for comparison, reflecting simple reactive turn-taking and majority class assignments for dialogue acts. Fine-tuning was applied to Llama 3.1 8B and ROBERTa-base models over multiple epochs, providing a comparison to zero- and few-shot approaches and offering insight into performance gains achievable through model adaptation."}, {"title": "4.3 Metrics", "content": "To compute how well our models mimic the actual user behavior, we compute two metrics: the F-measure for the prediction of \"speak\" turns (Speak-F1) and the F-measure for the prediction of the dialogue actions (DA-F1). Speak-F1 aims to answer the question of whether the simulator can mimic the actual user's behavior to determine when to talk, and DA-F1 aims to answer the question of whether the simulator can mimic the user's behavior to determine what to say, assuming the model has spoken when expected to."}, {"title": "4.4 Results and Discussion", "content": "In experiments, we first computed the quality of the prediction of conversational user turns with the zero- and few-shot prompting of GPT4, as shown in Table 1. We compared these with a baseline inspired from TODS user simulators, where the user is predicted to speak every time after the robot issues a conversational turn. In addition to overall performance results, we also breakdown scores according to the previous robot action (e.g., \"P: R speak\" denotes the Speak-F1 results only after the robot utterances.) Our results indicate that both methods surpass this baseline and the GPT4 with few-shot examples perform the best, obtaining an Speak-F1 score of 43.4%.\nTable 2 shows the DA-F1 scores, where a simple baseline of assigning every predicted user turn the majority dialogue act class (i.e., Instruction) has been used. These results seem inferior to the full fine-tuning approach of Gella et al. [2022] that obtains an F-measure of 59.26% on this subset, however, ROBERTa has the advantage of being limited to these labels. It seemingly outclasses the other models in the DA-F1 measure. This advantage, however, limits the ability to generate a user-response."}, {"title": "4.5 Dialogue Act level Analysis", "content": "The distribution of F1 scores across dialogue acts as shown in Figure 3 reveals notable patterns in the performance of GPT-4 and Llama 3.1 models under zero-shot (ZS) and few-shot (FS) approaches. GPT-4 consistently outperforms Llama 3.1, with its few-shot implementation showing the highest F1 scores across most dialogue acts. This suggests that GPT-4 benefits significantly from task-specific examples. Both models demonstrate strengths in common dialogue acts such as 'Instruction' and 'InformationOnObjectDetails', indicating their proficiency in task-oriented communication. However, there's a marked performance drop for more nuanced or less frequent acts. 'AlternateQuestions', 'RequestForInstruction', and 'RequestForObjectLocationAndOtherDetails' prove particularly challenging, especially for Llama 3.1, highlighting the difficulty in capturing complex query structures. Interestingly, 'Acknowledge' and 'Greetings/Salutations' show high variance across models and training approaches, suggesting that seemingly simple acts can be context-dependent and thus harder to predict consistently. The acts 'RequestMore', 'RequestOtherInfo', and 'OtherInterfaceComment' have near-zero F1 scores for all models, as expected because these dialogue acts are typically reserved for the robot, as shown in Section A.0.1. The zero-shot performance of GPT-4 is notably robust, often rivaling its few-shot counterpart, which underscores its strong pre-training and generalization capabilities. This variance in performance across dialogue acts suggests that future improvements in dialogue systems may benefit from act-specific optimization strategies, particularly focusing on the more challenging and nuanced dialogue acts."}, {"title": "4.6 Impact of Move Actions", "content": "The impact of move actions on GPT-4's performance is strikingly illustrated in Figure 4. When move actions are included, the Speak F1 scores remain relatively low and consistent between zero-shot (27.04%) and few-shot (26.79%) approaches, indicating that the presence of move actions significantly hampers the model's ability to accurately predict speaking turns. However, the exclusion of move actions leads to a dramatic improvement in Speak F1 scores, jumping to 42.03% for zero-shot and 43.39% for few-shot scenarios. This substantial increase suggests that move actions introduce"}, {"title": "5 Conclusions and Future Work", "content": "We present an LLM-based user simulator for embodied AI research. Our approach leverages zero-shot and few-shot learning techniques, using LLMs to predict user actions during task-oriented dialogues. The experimental results demonstrate that the proposed method effectively simulates user behavior, achieving a Speak-F1 of 43.4% and a DA-F1 of 51.13% in the best-case scenario. This is a significant improvement over traditional baselines, where the user simulator only speaks when spoken to or defaults to a majority class for dialogue acts. We consider this paper as the first step towards building a more comprehensive model. For example, an open LLM like Llama-3 can be fine tuned with the TEACh data. The current model does not incorporate visual information, which is a crucial aspect of embodied AI. Future work could explore integrating visual cues into the user simulator, enabling it to initiate dialogue based on observations of the environment and the agent's actions. For instance, a visual LLM such as GPT-4V or LLaVa could enable the model to determine when to initiate dialogue based on visual cues, like observing the robot wandering around. Furthermore, we plan to plug in this simulator to a state of art embodied agent, such as the HELPER system Sarch et al. [2024], to enable researchers to perform many potential experiments for robot self-play."}, {"title": "6 Impact Statement", "content": "This work advances the field of embodied AI by demonstrating a scalable and efficient approach to simulate user interactions with embodied agents. The potential applications of this research are wide-ranging, from enhancing task efficiency in human-robot collaboration to providing robust test environments for developing embodied agents that can learn from AI-generated user feedback. Enhanced capabilities in LLM-based agents may inadvertently facilitate misuse, particularly in sensitive domains where autonomous actions could be manipulated for harmful purposes. We urge"}, {"title": "A Appendix", "content": "A.0.1 Dialogue act explanation\nTable 3: List of dialogue acts from the Dialogue Act Annotation done on TEACh by Gella et al. [2022].\nA.1 Example Prompt\nPrompts used were made of a few manually written components, and a few randomly generated components.\nA.1.1 Initial Instructions\nImagine you, the COMMANDER, are an embodied agent in a simulated world. Your purpose is to instruct a robot, named DRIVER, to do tasks for you by telling it what to do and interrupting it to give further instruction when necessary. Your job here is to predict when you should be giving instructions to the DRIVER based on turn history with the DRIVER. If there is nothing to do or say, you should just observe.\nA.1.2 Task to the user agent\nYour job is to respond to a given dialogue/action history with only one Dialogue act or OBSERVE.\nEither return the dialogue act, or return the OBSERVE action. Return only one word/phrase.\nGoal: Prepare coffee in a clean mug.\nCOMMANDER: <observe>\nDRIVER: hi,what should i do today?<\u00abGreetings/Salutations, Request ForInstruction>>>\nCOMMANDER: Add coffee to a mug \u00abInstruction>>>\nDRIVER: <observe>\nCOMMANDER: Mug is in the coffee maker already \u00abInformationOnObjectDetails>>>\nDRIVER: <observe>\nCOMMANDER: <observe>\nDRIVER: should i rinse the mug or not? \u00abAlternateQuestions>>>"}, {"title": "A.1.3 Example answers", "content": "Examples are randomly selected from the data.\nExample :\nGoal: Prepare breakfast.\nCOMMANDER: <observe>\nDRIVER: hello what are my tasks \u00abGreetings/Salutations, RequestForInstruction>>>\nCOMMANDER: hii \u00abGreetings/Salutations>>>\nDRIVER: <observe>\nCOMMANDER: prepare coffe in clean mug \u00abInstruction>>>\nDRIVER: <observe>\nCOMMANDER: <observe>\nDRIVER: <pickup Mug>\nCOMMANDER: <observe>\nDRIVER: <putdown CounterTop>\nCOMMANDER response:\nOBSERVE"}, {"title": "A.1.4 Variations", "content": "Variation of the dialogue history without Driver acts:\nGoal: Put all Book on any Furniture.\nCOMMANDER: <observe>\nDRIVER: hello how may i help\n<<<Greetings/Salutations, RequestForInstruction>>>\nCOMMANDER: hi \u00abGreetings/Salutations>>>\nDRIVER: <observe>\nCOMMANDER: put the cook on furniture \u00abInstruction>>>\nDRIVER: <observe>\nCOMMANDER: book \u00abInstruction>>\nDRIVER: <observe>\nCOMMANDER: <observe>\nDRIVER: <pickup Book>\nCOMMANDER: <observe>\nDRIVER: <putdown Desk>\nCOMMANDER: the book is in the small room \u00abInformationOnObjectDetails>>>\nDRIVER: <observe>"}, {"title": "A.2 To Speak or Observe? Impact of Action Dynamics on GPT-4's User Behavior Simulation", "content": "The confusion matrices illustrate the performance of GPT-4 in predicting when to speak and when to observe across four different experimental conditions, simulating user behavior in a human-robot interaction scenario. In the zero-shot setting with move actions, the model demonstrates a high rate of false positives (2107 instances), indicating a tendency to speak excessively even when it should be observing. This suggests that the model, without prior training examples, struggles to balance its actions, leading to inappropriate conversational behavior. Comparatively, in the zero-shot without move actions condition, there is a noticeable improvement in model performance, with a reduction in false positives (1408 instances) and an increase in true positives (752 instances). This indicates that the absence of move actions reduces the complexity of the decision-making process, allowing the model to identify appropriate moments to speak more accurately. When transitioning to the few-shot learning scenarios, the model shows mixed improvements. In the few-shot with move actions condition, while there is a slight increase in true positives (583 instances), the number of false positives (2352 instances) remains high, reflecting continued over-talkativeness. This suggests that few-shot learning alone may not be sufficient to mitigate the confusion introduced by move actions. On the other hand, in the few-shot without move actions condition, the model achieves its best performance, with the highest true positive rate (788 instances) and the lowest false negative rate (630 instances). This demonstrates that the combination of few-shot learning and the removal of move actions enables the model to better emulate user behavior, accurately deciding when to speak and when to remain silent.\nA.2.1 Selective Removal of Move Actions\nTo see if move actions provided meaningful context to the LLM despite often causing noise, we added a method of selective removal of move actions. The criteria for removing a move turn was whether it directly followed a question asked by the robot. In this case, the robot was expecting a response from the user, and in most cases the user responded. Often, however, the TEACh dataset contained a move turn directly following these questions, most likely caused by simple human error, but when the LLM sees this move, it decides not to answer after that, causing an error in evaluation for two moves."}]}