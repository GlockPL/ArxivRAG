{"title": "Survey and Improvement Strategies for Gene Prioritization with\nLarge Language Models", "authors": ["Matthew B. Neeley", "Guantong Qi", "Guanchu Wang", "Ruixiang Tang", "Dongxue Mao", "Chaozhong Liu", "Sasidhar Pasupuleti", "Bo Yuan", "Fan Xia", "Pengfei Liu", "Zhandong Liu", "Xia Hu"], "abstract": "Rare diseases can be difficult to diagnose due to limited patient data and broad genetic\ndiversity. Despite the advances in variant prioritization tools, many rare disease cases remain\nundiagnosed. While large language models (LLMs) have performed well in medical exams, their\naccuracy in diagnosing rare genetic diseases has not yet been evaluated.\nTo identify causal genes for genetic diseases, we benchmarked various LLMs for gene\nprioritization. We employed multi-agent and Human Phenotype Ontology (HPO) classification\napproaches to identify patient case groups based on phenotypes categorizing them into\ndifferent levels of solvability. To address LLM limitations in ranking large gene sets, we used a\ndivide-and-conquer strategy to break down the ranking task into smaller subsets. Mini-batching\ninputs and limiting the number of generated tokens improved efficiency.\nIn its vanilla form, GPT-4 consistently outperformed the other LLMs with an accuracy around\n30%. Multi-agent and HPO classification approaches aided us in distinguishing between\nconfidently-solved and challenging cases. In addition, we observed bias towards well-studied\ngenes and input order sensitivity which are drawbacks of LLMs in disease-causal gene\nprioritization. Our divide-and-conquer strategy enhanced accuracy by overcoming positional and\ngene frequency biases in literature. This framework, based on benchmarking insights and novel\ntechniques, significantly optimizes the overall process when identifying disease-causal genes", "sections": [{"title": "Conclusions", "content": "Using HPO classification and our novel multi-agent techniques, and our LLM divide-and-\nconquer strategy (1) highlighted the importance of accounting for differences in patient case\nsolvability and (2) yielded improved performance in identifying causal genes for rare diseases\nwhen compared to our baseline evaluation. We anticipate that our approach will streamline the\ndiagnosis of rare genetic disorders, facilitate the reanalysis of unsolved cases, and accelerate\nthe discovery of novel disease-associated genes."}, {"title": "Introduction", "content": "Rare diseases affect a small fraction of the population but collectively impose a significant\nburden upon individuals and society [1]. These rare conditions are frequently of genetic origin\nand classified as Mendelian diseases, and result from mutations in single genes. Their rarity\nand diversity considerably complicate diagnosis and treatment. A notable portion of these\nconditions remains undiagnosed due to the unpredictability and complexity of their genetic\nfoundations and clinical presentations, leading to lifetimes of difficulty and uncertainty for\npatients and their families [2]. There is an urgent need for enhanced diagnostic tools and\nmethodologies to evaluate patient data.\nIn response to this need, clinical sequencing, including whole genome sequencing (WGS) and\nexome sequencing (ES), has become an indispensable tool in the exploration of rare diseases.\nThese techniques allow clinicians and researchers to meticulously analyze a patient's genome\nand detect potentially causative genetic variants. While the vast array of candidate variants\npresents a challenge, variant prioritization tools effectively address this issue.\nLearning how to use gene prioritization to reliably identify the causal disrupted gene is critical for\naccurate diagnosis and understanding the genetic variants responsible for the disease [3,4].\nOther approaches to gene prioritization depended largely on structured databases that compile\nwell-known genetic data and predictions regarding the impact of various mutations [5,6].\nHowever, due to the rarity and distinct complexity of rare diseases [7], traditional approaches to\ngene prioritization are less likely to be effective, leaving many rare-disease cases unresolved.\nInnovative solutions in artificial intelligence have been primarily driven by the development of\nLLMs like GPT-4, Llama2[8], and Mixtral-8x7b[9]. These models are trained on a wide array of\ndata sources, including textual datasets, programming code bases, and other forms of digital\ncontent. Such training enables these models to not only emulate human-like communication, but\nalso to acquire a substantial understanding of specialized domains, including medical sciences\n[10,11,12,13]. Unlike simple data repositories, LLMs exhibit advanced reasoning capabilities,\nallowing them to process and interpret extensive and unstructured datasets effectively. This\nattribute is particularly valuable in exploring underutilized applications in genetic research and\nthe diagnosis of rare diseases [14]."}, {"title": "Methods", "content": "Because there is not currently a consensus strategy for benchmarking gene-based prioritization\nmethodology, synthetic datasets are often selected for evaluation [19]. We, however, used a\nlarge dataset of resolved patient cases: 1063 cases from a clinical diagnostic lab, Baylor\nGenetics (BG); 90 from the Undiagnosed Diseases Network (UDN); and 200 from Deciphering\nDevelopmental Disorders (DDD). The patient data consists of phenotypes from clinical\nelectronic health records (EHR) and genomic sequencing data.\nWe preliminarily filtered raw genomic data from each patient to keep variants with an allele\nfrequency less than 1%, or that were classified as pathogenic or likely pathogenic in ClinVar or\nHGMD, or that had a SpliceAl score greater than 0.8. The variants were converted to the gene\nlevel for gene-based prioritization. We then generated three gene sets of different sizes (5, 25,\nand 50 genes) for each patient by randomly sampling from their non-causal genes and adding\nthe causal gene. We shuffled the genes to randomize to the gene ordering for the prompt. We\nfinally converted patient clinical phenotypes from numerical HPO terms to their corresponding\ndefinitions because LLMs tend to hallucinate the true meaning of HPO terms (Fig. S10)."}, {"title": "Prompt Creation", "content": "We enumerated clinical phenotypes and candidate genes to structure the prompt. Then we\nincluded instructions for the model to assign a rank based on association between the specified\ngenes and the clinical phenotypes. We included further guidance regarding the use of gene\nfunction, expression sites, or analogous animal model information in the absence of direct"}, {"title": "Multi-Agent Classification", "content": "The Multi-Agent Classification method is an approach we developed for classifying patients by\nphenotypes via a multi-agent LLM pipeline that consists of two main steps (Fig. S4). In the first\nstep, the original prompts are given to an evaluator agent (GPT-4) to write a 100-word essay\nthat evaluates all the gene candidates in the prompt. In the second step, a summarizer agent\nsummarizes the output essay and distinguishes whether there is at least one gene directly\nlinked to the phenotypes by outputting \u201cYes\u201d or \u201cNo\u201d. The \u201cYes\u201d or \u201cNo\u201d output for each case is\nthen used to classify the cases into specific (Yes) or non-specific (No) groups for subsequent\nevaluation."}, {"title": "HPO Phenotype Classification", "content": "In this study, we adopt the HPO Phenotype Classification methodology (as detailed in the\nCohort Analyzer tool publication [20]) to evaluate HPO term specificity regarding the HPO\nhierarchy termed the dataset specificity index (DsI). This metric assesses whether the HPO\nterms used to phenotype the patients are general (closer to the root node in the hierarchy) or\nspecific (further from the root node). The Dsl calculation considers the distribution of HPO terms\nacross various levels of the HPO tree structure, penalizing general terms near the root while\nrewarding specific terms closer to the leaf nodes. For classification analyses, we randomly\nsampled 90 cases each from BG, UDN, and DDD for 270 total patient cases."}, {"title": "Divide-and-Conquer Strategy", "content": "LLMs struggle to effectively address problems involving numerous options [21]. To address this\nlimitation, we used a divide-and-conquer strategy that consists of three steps: First, all gene\ncandidates are randomly split into groups of five; second, LLMs estimate the probability of the\ngene candidates to be disease-causal within each group as their in-group probabilities; third, the\nfinal score for each gene is derived from the average of its in-group probability (Fig. S8). \u03a4\u03bf\nformulate this pipeline, we give the final score $S(g_j)$ for a gene $g_j$ in the following equation:\n$S(g_j) = \\frac{1}{N} \\sum_{i=1}^{N} Pr(g_j)$ \nHere, N is the total sampling number and $Pr(g_j)$ denotes the in-group probability of the gene\n$(g_j)$ provided by the LLMs. We divided the 25 gene candidates into five groups and the 50\ngene candidates into ten groups so that each group consists of 5 genes."}, {"title": "Ranking Strategy", "content": "We employ different strategies for interacting with proprietary and open-source LLMs due to\ntheir distinct capabilities. Specifically, GPT-3.5 and GPT-4 rank genes within their textual\noutputs. We selected to use a ranking strategy for open-source LLMs like Llama-2 and Mixtral-\n8x7B due to their ability to rank based on the probability of their output tokens, a feature not"}, {"title": "Benchmark of Large Language Models", "content": "We began our study by conducting a baseline evaluation on several LLMs as applied to three\ndatasets of patients with rare diseases from Baylor Genetics, Deciphering Developmental\nDisorders, and the Undiagnosed Diseases Network. The LLMs we examined included GPT-4\n[23], GPT-3.5 [23], Mixtral-8x7B [9], Llama-2-70B [8], and a specialist biomedical model called\nBioMistral [24]. These represent proprietary models, open-source models, a mixture of experts\nmodel, and a domain specialist model. For each patient case, the LLM evaluated lists of\ncandidate genes with lengths of 5, 25, and 50, along with the patient's phenotypes, to observe\nits performance with different input sizes. The output was a ranked list of the genes with\nprobabilities reflecting their likelihood of being associated with the phenotypes (Fig. 1A)."}, {"title": "Enhancing Diagnosis Accuracy through Multi-Agent Prompts and HPO Classification", "content": "Given the observed variability in patient presentations and datasets during the benchmark\nprocess, we wanted to classify the patient cases by phenotype and gene-phenotype-association\nspecificity. We used two approaches to accomplish this. We first used a multi-agent system in\nwhich an evaluator agent writes a 100-word essay assessing gene-phenotype associations and\na summarizer agent analyzes the essay to classify cases as having the direct presence of gene-\nphenotype connections (Fig. 2A). We next used HPO Phenotype Classification methodology,\nwhich calculates the dataset specificity index (Dsl) by considering HPO-term distribution across\nthe HPO hierarchy, to reward specific terms from deeper levels of the tree and penalize general\nterms from levels near the root. We applied both methods to three randomly sampled sets of 90\npatient cases from each of our three datasets. Together the multi-agent and HPO classification\nstrategies allowed for a refined analysis of cases based on their phenotypic and genotypic\ncharacteristics."}, {"title": "Literature and positional bias in LLMs", "content": "When considering the ranking abilities of LLMs it is crucial to evaluate potential biases that may\narise from the representation of genes in the literature and positional biases within the prompt. To\ninvestigate these factors, we conducted an analysis of LLM performance on the BG, UDN, and\nDDD datasets.\nOur analysis revealed a clear inverse correlation between the mean rank of a gene in these\ndatasets and the ClinVar submission count that correspond to the gene (Fig. 3B). This finding\nsuggests that LLMs may prioritize genes with greater representation in ClinVar, likely reflecting\nbiases in both the literature and ClinVar itself, as well as the increased representation of these"}, {"title": "Divide-and-conquer strategy addresses biases", "content": "To counteract previously mentioned biases, we introduced the divide-and-conquer strategy to\nhandle various gene options and orders when assigning gene rankings. This three-step pipeline\ninvolves randomly splitting gene candidates into groups of five, estimating in-group probabilities\nusing the GPT-3.5 model, and averaging these probabilities across sampling iterations to obtain\nfinal gene scores (Fig. S7).\nThe divide-and-conquer strategy effectively increases the performance of GPT-3.5 for each\ndataset across all ranks (Fig. 4A). We observed this strategy to be more effective when\nimplemented on ranking longer candidate gene lists (Fig. 4A, Fig. S9A). Causal genes\nconsistently exhibit high scores when grouped with the remaining genes. Conversely, non-"}, {"title": "Discussion", "content": "Our study provides a comprehensive analysis of the ways in which Large Language Models\n(LLMs) can enhance gene-based prioritization, particularly in the context of rare genetic\ndisorders. The performance benchmarks established for various LLMs, with GPT-4 as the\nfrontrunner, underscore the potential advanced transformer architectures have to assist experts\nin clinical genomics. This potential is likely due to the extensive and varied training datasets that\nequip LLMs with an advanced understanding of both structured and unstructured medical data.\nHowever, our findings also illuminate some challenges. Notably, certain cases were better\naddressed by LLMs than others, suggesting a variance in model efficacy that correlates with the\ncomplexity and specificity of the genetic and phenotypic data presented. This variability in\nperformance emphasizes the need for models that can adapt to the high heterogeneity inherent\nin rare disease diagnosis.\nA critical observation from our study is the apparent bias reflected in the existing literature and\nin genetic databases like ClinVar. Genes that are more frequently studied or reported tend to be\nprioritized by LLMs, potentially overshadowing less characterized but equally significant genes.\nThis bias underscores a fundamental challenge in employing Al in medical diagnostics: the\nquality of Al outputs can only be as good as the data inputs.\nTo mitigate these biases and enhance gene-based prioritization's reliability, we propose a\ndivide-and-conquer strategy. By dividing the prediction into multiple prioritization processes and\nintegrating the results, our approach not only diversifies the gene candidates considered but\nalso reduces the influence of skewed data distributions. This strategy is crucial for advancing\nthe utility of LLMs in clinical settings, where gene prioritization accuracy can directly influence\npatient outcomes.\nIn conclusion, while LLMs offer significant promise in revolutionizing genetic diagnostics, their\ndeployment must be carefully managed to address inherent biases and ensure equitable and\ncomprehensive genetic analyses. Future studies should focus on refining LLMs to handle the\ndiverse and complex nature of rare diseases more effectively, further bridging the gap between\nAl potential and clinical application."}, {"title": "Supplementary Materials", "content": "Take text directly from GPT models as output. From open-source models, we take the\nprobability. For the GPT models, this value is not available. We can only get text from the GPT\nmodels.\nWe have different methods to interact with open-source LLMs such as Llama-2 and Mixtral\ncompared with close-source LLMs such as ChatGPT. Open-source LLMs offer the probability of\neach output token, a feature unavailable in closed-source LLMs like ChatGPT. To illustrate, we\nutilize the following prompt when querying open-source LLMs. An example is provided below,\nwhere the placeholder [GENE NAME] represents the name of a gene.\n### Given the following phenotypes in a patient: Global developmental delay, Generalized\nhypotonia, Failure to thrive, Tetralogy of Fallot, Hypertrophic cardiomyopathy, Abnormal facial\nshape. ### Are these phenotypes caused by a mutation in the [GENE NAME] gene? ### Use\nany available information including gene function, reports of genetic variants, expression sites,\nor animal model studies if direct human data is insufficient. ### Just answer Yes/No.\nThe LLM outputs either 'Yes' or 'No' to indicate the causality of a gene gj to the phenotypes. We\nemploy maximum likelihood estimation [3] to determine the causality of genes to the\nphenotypes. Specifically, the log-likelihood ratio of 'Yes' and 'No' indicates the log-likelihood\nvalue $S(g_j)$ that the gene $g_j$ causes the phenotypes, as given in the following equation:\n$S(g_j) = log(\\frac{Pr(Yes)}{Pr(No)})$, \nwhere Pr(Yes) and Pr(No) denote the output probability of the 'Yes' and 'No' tokens. The\nranking of genes is determined based on their respective log-likelihood values."}, {"title": "Prompt Generation", "content": "For HPO data, we utilized the get_ontology() function from the ontologyIndex package to\nretrieve the HPO data. For gene data, we first extracted the gene symbols of causal variants\nfrom every patient case. Then, we used a sampling with pre-defined seeds to sample n-k gene\nsymbols of non-causal variants from the filtered gene pool (n is the pre-defined number for all\ninput genes and k is the number of gene symbols of causal variants of the given case). After the\nsampling, the causal genes and non-causal genes of every case will be combined in a shuffled\norder to reduce the positional bias. For investigating the effect of input order on the rank of\ncausal genes, 5 different seeds were used in the shuffling process to get varied input orders."}, {"title": "GPT Query", "content": "Queries of GPT-3.5 and GPT-4 are performed by calling API. The model used for GPT-3.5 is\ngpt-3.5-turbo-1106. The model used for GPT-4 is gpt-4-1106-preview. The temperature is set to\n1.0. For every patient case, 5 repeated queries with the same prompt will be performed for\nevery case to evaluate the baseline robustness of the GPT models. Based on the output of API\ncalling, another GPT-3.5 agent (gpt-3.5-turbo-1106, temperature = 1.0) will check and"}, {"title": "Result Summary", "content": "Based on the summarized output of LLMs, we matched the output gene names with the input\ngene names in the same case to assign ranks to every gene in the given case. Handling\nhallucinations, if the causal gene in the input list is not in the output list, the case will be\nabandoned for a further summary. Based on our observation, hallucinations with irrelevant\noutputs are rare. However, the output usually contains fewer genes compared with the input\ngene lists, especially when the input number of genes is 25 and 50. To measure the proportion\nof genes that can be matched during the result summary, we have causal_gene_output_ratio in\nthe supplementary tables to evaluate the proportion of causal genes that can be matched in\nevery batch of query and match_mean to evaluate the mean proportion of all input genes that\ncan be matched in every batch of query. After assigning ranks to every gene, the metrics \"% of\ncausal genes\" will be calculated by calculating the proportion of causal genes ranked \u2264 n among\nall the causal genes in the batch of queries. Therefore, the abandoned cases due to missing\ncausal genes will still be penalized as not within the given rank number."}, {"title": "Decreasing Inference/Generation Latency", "content": "To run larger open-source LLMs, we employ mini-batch inference and output clipping\ntechnologies. Specifically, mini-batch inference is a technique of feeding mini-batches of the\ninput sequence to LLMs, thereby accelerating the overall generation process by distributing the\ninference processes. Output clipping is a technique utilized to reduce inference time by limiting\nthe output token number, thereby shortening the output sequence. This approach is simply\nimplemented by setting the `max_new_tokens` argument parameter to a low value. It proves\nparticularly effective in tasks where concise answers are sufficient, such as confirming gene-\ndisease associations with simple affirmative or negative responses. The reduction in output\ntokens can effectively reduce the latency without compromising performance."}]}