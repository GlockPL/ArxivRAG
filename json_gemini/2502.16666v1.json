{"title": "SBSC: STEP-BY-STEP CODING FOR IMPROVING MATHEMATICAL OLYMPIAD PERFORMANCE", "authors": ["Kunal Singh", "Ankan Biswas", "Sayandeep Bhowmick", "Pradeep Moturi", "Siva Kishore Gollapalli"], "abstract": "We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. At each step/turn, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to solve it. This way, SBSC, sequentially navigates to reach the final answer. SBSC allows more granular, flexible and precise approach to problem-solving compared to existing methods. Extensive experiments highlight the effectiveness of SBSC in tackling competition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe SBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program generation based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME and 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark SBSC's greedy decoding against self-consistency decoding results of existing SOTA math reasoning strategies and observe performance gain by absolute 6.2% on AMC, 6.7% on AIME and 7.4% on MathOdyssey. Scripts & Data is uploaded at this link.", "sections": [{"title": "1 INTRODUCTION", "content": "Mathematical reasoning has emerged as a critical benchmark to measure the advanced reasoning and problem-solving abilities of the Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Achiam et al., 2023; Reid et al., 2024; Anthropic, 2023; OpenAI, June, 2024). This is due to the complex and creative nature of the numerous reasoning steps required to solve the problems.\nChain-of-Thought (Wei et al., 2022) and Scratchpad (Nye et al., 2021) prompting strategies helped LLMs to solve a problem using a step-by-step thought process. Program-Aided Language (PAL) (Gao et al., 2022) & Program-Of-Thought (POT) (Chen et al., 2022) introduced problem-solving via program generation where the answer is obtained by executing the generated program. Tool-Integrated Reasoning Agent (TORA) (Gou et al., 2023) & Mathcoder (Wang et al., 2023a) introduced tool-integrated math problem solving format where model outputs natural language reasoning followed by program generation to solve the entire problem using a single code block and incorporates code-interpreter output for either summarizing the program output to get the final answer and terminate; or re-attempt the problem in the subsequent turn using the same format. For brevity, let's call ToRA's defined way of tool-integrated reasoning (TIR) strategy as TIR-TORA.\nThe current generation of advanced LLMs such as GPT-40 (Achiam et al., 2023), Claude-3.5-Sonnet (Anthropic, 2023) and Gemini-ultra (Reid et al., 2024) have achieved high scores on elementary GSM8k (Cobbe et al., 2021) & high-school level MATH (Hendrycks et al., 2021) by leveraging these reasoning strategies via in-context learning (Brown et al., 2020; Chowdhery et al., 2022). Multiple studies (Yu et al., 2023b; Yue et al., 2023; Toshniwal et al., 2024; Gou et al., 2023; Wang et al., 2023a; Mitra et al., 2024; Beeching et al., 2024; Shao et al., 2024) have tried supervised fine-tuning (SFT) approach to distill these reasoning formats using a propriety models like GPT4 (Achiam et al., 2023). These studies show significant performance improvement over GSM8K and MATH benchmarks."}, {"title": "1.1 MOTIVATION", "content": "However, recent math specific competition and Olympiad-level benchmarking on Math Odyssey (Fang et al., 2024), OlymiadBench (He et al., 2024), and the American Invitational Mathematics Examination (AIME) & the American Mathematics Competitions (AMC) (Beeching et al., 2024; DeepSeek-AI et al., 2024; Reid et al., 2024) questions show that the state-of-the-art (SOTA), both generalist and specialist, LLMs continue to struggle with advanced math reasoning. These results highlights the limitation of the existing math prompting techniques. (Tong et al., 2024) highlights the severe bias towards easy problems that exists in the SOTA SFT datasets which originates primarily due to the ineffectiveness of the current prompting strategies in complex math problem-solving. Often, multiple chains are generated via self-consistency decoding (Wang et al., 2022) and majority voting is done to boost the accuracy which is unlike how humans solve problems.\nFundamentally, both PAL & TIR-TORA generate a single program block to solve the entire problem. Additionally, TIR-TORA framework allows the model to re-attempt the program generation in case of execution error. These approaches show improved performance over COT on elementary & high school level math problems. However, solving olympiad-level math problem requires coming up with complex and creative solution that constitutes of numerous elaborate intermediate steps which eventually leads to the answer. Often, it is not feasible to solve a complex problem entirely using a single program block and as a result, these prompting strategies fail to systematically address each detailed step of the problem-solving process. It tends to overlook specified constraints, edge cases or necessary simplifications, which are often encountered in Olympiad-level problems."}, {"title": "1.2 OUR CONTRIBUTION", "content": "Olympiad level math problem-solving can be viewed as solving/exploring an intermediate sub-task/key-concept in depth; and discovering + solving the next critical sub-task dynamically basis the accumulated knowledge of previous sub-tasks/key-concepts explorations. To this end, we propose Step-by-Step Coding framework (SBSC) which is a multi-turn math reasoning framework that leverages existing programming (Jain et al., 2024) and in-context learning skills (Brown et al., 2020) of the current generation of LLMs, particularly Claude-3.5-Sonnet (Anthropic, 2023) & GPT-40 (OpenAI, June, 2024). In each turn, it leverages code-interpreter results and knowledge of previous sub-tasks solutions or concept-explorations to define and programmatically solve the next sub-task. Thus it uses code generation as the reasoning strategy to solve an intermediate sub-task or explore an intermediate concept/step. Thus, providing detailed focus to each step of problem solving unlike PAL & TIR-TORA. SBSC allows an intermediate key-step to be discovered, and be explored and refined (if needed) before being appended to the chain of steps whereas in PAL & TIR-TORA all the intermediate steps are always stitched together.\nWe investigate the performance of SBSC on last 11 years of AIME & AMC-12 questions. We also benchmark on Olympiad-subset of MathOdyssey dataset along with math questions from OlympiadBench. We compare our method (greedy decoding) against greedy-decoding generation of existing reasoning strategies: COT, PAL & TIR-TORA. We also show SBSC (greedy decoding) effectiveness by benchmarking against self-consistency decoding results of COT, PAL & TIR-TORA. We conduct extensive ablations to understand the benefits of our approach such as sensitivity to exemplars, topic-wise analysis and measuring improvement in program refinement/debugging ability over TIR-TORA due to the granular nature of SBSC process."}, {"title": "2 SBSC: STEP-BY-STEP CODING FRAMEWORK", "content": "Solving complex math problems, such as competition or Olympiad-level ones, involves creative thinking, applying diverse mathematical knowledge, and dynamically creating subsequent strategies as new insights emerge. One must discover sub-tasks dynamically, rigorously explore intermediate concepts, and carefully handle constraints and edge-cases. Since PAL & TIR-ToRA generates single code block (even during self-correction step; incase of TIR-TORA) to solve a problem, they lack the flexibility or granularity to emulate this. To address this, we introduce SBSC.\nSBSC is a multi-turn, code-generation based math reasoning prompting strategy where at each turn: the model generates an intermediate sub-task and corresponding program to solve that sub-task by leveraging the outputs of the previous turns. At the end of each turn, code interpreter is used to"}, {"title": "2.1 SBSC EXEMPLAR DESIGN", "content": "To enable SBSC framework in LLMs, we rely on in-context learning abilities (Brown et al., 2020) of LLMs as explored by multiple previous works such as (Chen et al., 2022; Gao et al., 2022; Gou et al., 2023) etc. We also use a system prompt similar to previous works. With respect to exemplar design, to enable program generation, we borrow learning from PAL (Gao et al., 2022) & POT (Chen et al., 2022) to have meaningful variable names in the code and using natural language comments within programs(Chen et al., 2022). To enable intermediate tool (code interpreter) usage, we leverage the use of stop words similar to in (Gou et al., 2023). Sample SBSC exemplars can be found at A.11, A.12."}, {"title": "3 EXPERIMENT", "content": "We mainly use problems from 4 popular math competition datasets for benchmarking our performance: AIME, AMC, MathOdyssey (Fang et al., 2024) and OlympiadBench (He et al., 2024), covering multiple domains, mainly: Algebra, Combinatorics, Number Theory and Geometry. We use problems of last 11 years from AMC and AIME, obtaining questions and answers (Q&A) in LATEX format from the AoPS Wiki website. MathOdyssey (Fang et al., 2024), a popular benchmark for LLM math reasoning, consists of problems of varying difficulties. We include the 148 problems belonging to olympiad-level competitions. OlympiadBench is another challenging benchmark for LLMs containing olympiad-level multilingual scientific problems. We select only math related questions, in english language."}, {"title": "3.1.1 DATASET PROCESSING DETAILS:", "content": "First, we filter out all questions having reference images associated. Second, we process the questions to have integer type answers if they are already not in that format. All AIME problems have a unique integer answer ranging from 0 to 999, while AMC-12 problems are of Multiple Choice Question(MCQ) format. Similar to NuminaMath (Beeching et al., 2024), we remove all the answer choices from each AMC-12 question and modify the representation of the final answer for the question, wherever necessary, to ensure an integer answer. In case of OlympiadBench and MathOdyssey, we simply modify the question as needed. For this, we prompt GPT-40 to append an additional line at the end of each problem as suitable. Following is an example for demonstration:"}, {"title": "3.2 BASELINE & CONFIGURATIONS", "content": "We benchmark against three prompting/reasoning strategies: COT (Wei et al., 2022), PAL (Gao et al., 2022) & TIR-TORA (Gou et al., 2023). We use gpt-40-2024-05-13 and Claude-3.5-Sonnet as base LLMs for our experiments. For all datasets and all reasoning frameworks, we use 4-shot setting. Maximum number of turns (n) SBSC is set to 15. For greedy decoding inference, we use temperature=0 and max_tokens=1024 and also, we run 3 times and report average. For greedy decoding of TIR-ToRA, we keep n = 15 as well (Note: this is because although in TIR-TORA strategy the model attempts to solve the entire problem in the single turn, in case of execution error or readjustment it tries to re-attempt in subsequent turns). We also benchmark SBSC's greedy decoding results against self-consistency (SC) (Wang et al., 2022) decoding results (majority@7) of COT, PAL & TIR-TORA. We do this primarily for two reasons: First, SBSC takes multiple turns before arriving at the final answer (on average 6-7 turns per problem, Table 3 in Appendix A.1) and Secondly, to benchmark against the reliance of the current existing prompting strategies on majority voting for boosting accuracy. For SC decoding, we use temperature=0.7 and top_p=0.9.Note: we experimentally observe that for n > 4, there is insignificant increase in accuracy for TIR-TORA so we set n=4 for TIR-TORA during SC decoding.\nNote: PAL (Gao et al., 2022) work also reports a combined approach with Least-to-Most (L2M) prompting strategy (Wang et al., 2022), L2M-PAL that is essentially two stage. We implemented it as per the reported examples in the PAL work. We benchmark it on AMC + AIME dataset. We observe that L2M-PAL at best matches PAL or TIR-TORA scores. Detailed results available in appendix A.14. Hence for our main results, we stick to PAL & TIR-TORA along with self-consistency decoding due to resource optimisation and wider adaption of those prompting strategies for math-problem solving. For more discussion on L2M-PAL please check A.14."}, {"title": "3.3 PROMPTING/FEW-SHOT EXEMPLARS", "content": "For both AIME and AMC, we select 90 questions each, drawn from problems of years other than those included in the evaluation datasets. These questions were prompted with COT, PAL, TIR-TORA and SBSC to generate corresponding solutions in accurate format. For each dataset, we create a subset of 10 problems correctly solved by every method and finally select a combination of 4 exemplars among them. For MathOdyssey as well as Olympiad Bench, we use AIME exemplars as these datasets are of similar difficulty level. We provide the 4 chosen exemplars and system-prompts, used in the main experiments, for different methods in Appendix (A.9, A.10, A.11, A.12) & repository here."}, {"title": "4 RESULTS", "content": "We report the percentage accuracy of all the methods with different base LLMs and across all the benchmarking datasets in Table 1. On AMC dataset, SBSC shows an absolute improvement over TIR-TORA (greedy decoding) by roughly 11% using Claude-3.5-Sonnet and 7% using GPT-40. SBSC"}, {"title": "5 ABLATIONS & ANALYSIS", "content": null}, {"title": "5.1 SENSITIVITY TO EXEMPLARS", "content": "We study the effect of number/choice of examples in prompting on SBSC's performance using Claude-3.5-Sonnet on a subset of AIME and AMC data. As shown in Figure 2, we observe a notable increase in performance when increasing the examples from 2 to 4, which then starts to saturate as we further increase the number of examples to 6 and 8. This justifies our decision of using a 4-shot setting. To understand if the choice of exemplars affect the accuracy or not, we conduct a sensitivity analysis. We randomly sample 4 exemplars out of the already created pool of 10 exemplars three"}, {"title": "5.2 SBSC EXEMPLAR TUNING", "content": "Natural language comments present within a program have proven to be useful (Gao et al., 2022). So, in each of the SBSC exemplars, we provide suitable comments in natural language within the Python program for each turn to help guide the model."}, {"title": "5.3 CODE DEBUGGING ABILITY", "content": "We present the superior ability of our method to resolve an error related to code execution. If at any step of the trajectory chain, the program returns an execution error, we consider that to be an error step. We visually represent this, using Claude-3.5-Sonnet responses across AMC, AIME and MathOdyssey datasets in Figure 4, where we see that SBSC is able to recover from even multiple wrong steps and reach the correct final answer quite easily when compared to TIR-TORA whose performance drops steeply on increasing error steps. This can be attributed to the fact that SBSC, being precise and granular, tackles only a focused part of the problem and finds it easier to correct its mistakes compared to TIR-TORA which tries to correct the program at the problem level."}, {"title": "5.4 TOPIC-WISE ANALYSIS", "content": "We use GPT-40-mini (OpenAI, June, 2024) to classify problems from AIME and AMC, while MathOdyssey and OlympiadBench already contained topic labels. Our test set primarily comprised of: Algebra, Arithmetic, Combinatorics, Number Theory and Geometry. In this study, we benchmark the solutions obtained using Claude-3.5-Sonnet. As can be seen in Figure 5, our method outperforms COT & TIR-TORA (against both greedy and self-consistency decoding) in all the individual topics and across all the 4 datasets, thereby proving beneficial for all topics. This highlights the generalisation ability of our approach extending to different types and complexities of problems."}, {"title": "5.5 SBSC ACCURACY CORRELATION WITH CODING CAPABILITIES OF LLMS", "content": "We study the correlation of code related capabilities of the LLMs with respect to their success with SBSC. Since coding capabilities of a model is pivotal towards successfully following and executing our SBSC approach, we make a comparison involving LLMs with varying coding abilities. Figure 6 shows that the SBSC scores are correlated to the code generation abilities of the corresponding models for all cases that were evaluated on a subset of AIME and AMC data. The code-generation scores were taken from LiveCodeBench (Jain et al., 2024) benchmark."}, {"title": "5.6 SBSC + SELF-CONSISTENCY", "content": "Self-consistency (SC) decoding (Wang et al., 2022) has proven to be effective in boosting accuracy via sampling multiple chains and taking a majority voting. We employ SC decoding to assess the upper bound of our approach. For this study, we use temperature=0.7 and top_p=0.7."}, {"title": "6 RELATED WORK", "content": "Recently, significant advancements across various research directions have been made to enhance the mathematical capabilities of large language models (LLMs). One of the major ones has been along the prompting and thinking strategies such as Chain-of-Thought (COT) method (Wei et al., 2022; Kojima et al., 2022) that has shown to evoke multi-step thinking in LLMs before arriving at the answer. These methods struggle with complex and symbolic computations. For this, PAL (Gao et al., 2022) & POT (Chen et al., 2022) suggest making LLMs perform reasoning by writing program and offloading the computations to code interpreter. TIR-TORA (Gou et al., 2023) does rationale generation in one go beforehand and then it codes the entire solution to get the final answer using single code block. Additionally, in case of an error, it tries to re-attempt in similar format. Another line of research has been around pre-training and supervised fine-tuning (SFT). Multiple studies (Shao et al., 2024; Ying et al., 2024; DeepSeek-AI et al., 2024; Azerbayev et al., 2023; Lewkowycz et al., 2022; Paster et al., 2023; Taylor et al., 2022) have shown pre-training LLMs on high-quality maths tokens results in increased mathematical knowledge and reasoning abilities. Recent approaches (Yu et al., 2023b; Gou et al., 2023; Yue et al., 2023; Wang et al., 2023a; Shao et al., 2024; Toshniwal et al., 2024; Mitra et al., 2024; Beeching et al., 2024; Yin et al., 2024; Tong et al., 2024) have tried query/problem augmentation along with creating synthetic reasoning paths/trajectories using a teacher"}, {"title": "7 CONCLUSION", "content": "We introduce SBSC, a multi-turn math reasoning framework that tries to enable LLMs to solve complex math problems. SBSC pursues the solution, step-by-step with each turn dedicated to a step, and arrives at final answer via multiple turns. At each turn, an intermediate sub-task and its corresponding program solution is generated leveraging the execution outputs and solutions of all the previous sub-tasks. We show performance improvements of SBSC over TIR-ToRA, PAL & COT on challenging math problems. We also show that greedy-decoding results of SBSC outperforms self-consistency results of other prompting strategies."}, {"title": "8 FUTURE WORK", "content": "Given the detailed, dynamic and flexible step-wise nature of problem-solving along with the fact that its leverage program generation to conclude a key-intermediate step, we believe SBSC reasoning format could be highly useful for guided decoding strategies such as in Outcome-Supervised Value Model (Yu et al., 2023a), AlphaMATH (Chen et al., 2024), Q* framework (Wang et al., 2024). It would be well suited for step-wise preference optimisation for reasoning such as in (Lai et al., 2024). SBSC trajectories could be used also for imitation learning via SFT."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 NUMBER OF STEPS IN SBSC", "content": "In Table 3, we present the number of turns taken per question by SBSC responses obtained using Claude-3.5-Sonnet across the different datasets."}, {"title": "A.2 RESULTS ON JEE-BENCH AND OMNIMATH DATASETS", "content": "We present results on additional two benchmarks: JEE-Bench (Arora et al., 2023) and Omni-MATH (Gao et al., 2024). For JEE-Bench, we evaluate on 98 numerical answer type questions. For Omni-MATH, we filter out 576 questions out of that with all 26 Qs from calculus topics and 110 Qs randomly sampled from each one of the remaining topics. We use AIME exemplars for evaluation on these two datasets to also further show how generalizable SBSC is.\nFor JEE-Bench in Table 4, we observe significant improvements compared to other methods. For Claude 3.5 Sonnet, SBSC's greedy decoding takes an absolute lead (against greedy decoding) of 16% over TIR-TORA, 20% over PAL and over 30% COT. Similarly benchmarking against self-consistency decoding (majority@7) of other methods, SBSC takes an absolute lead of 11% over TIR-TORA, 16% over PAL and over 25% COT."}, {"title": "A.3 BENCHMARKING OPEN SOURCE MODEL: DEEPSEEKCODER V2.5", "content": "We performed benchmarking with DeepSeekCoder 2.5 (DeepSeek-AI, 2024) which is an open source model. Note: given the resource constraint and that most of the frontier open source LLMs such as DeepSeekCoder 2.5 require multiple gpus and days to host and run the experiments respectively, we leveraged this open source model via api provided by deepseek which was highly cost-effective. We present the results on 2 test-sets: AIME and AMC as used in the main results.\nWe can clearly observe in Table 6 that SBSC outperforms both greedy decoding and self-consistency decoding results of TIR-TORA, PAL & COT on both last 11 years AIME & AMC."}, {"title": "A.4 COST AND TOKEN COMPARISON ACROSS SBSC, TIR-TORA, PAL & COT", "content": "We also have done a study with Claude 3.5 Sonnet to compare the average cost per question. We have taken average over 50 AIME+AMC questions. We leveraged the prompt-caching of repeated input tokens while doing this experiment. Specifically, we cache the system prompt and exemplars for all the methods. The results are presented in the table below. As shown in the comparison in Table 7 SBSC is 3X of TIR-TORA, 4X of COT and 7X of PAL. When comparing average cost per question even in non-caching mode, from Table 8 we observe that SBSC is 6.2X of TIR-TORA, 7X of COT and 14.3x of PAL. Hence, Table 1 already covers fair comparison part as well even if we take"}, {"title": "A.5 EXEMPLAR SENSITIVITY ANALYSIS WITH LARGER POOL", "content": null}, {"title": "A.5.1 BRIEF ON EXEMPLAR CURATION & CRAFTING AND WHY IT DIDN'T REQUIRE MANUAL\nEFFORT", "content": "To enable the multi-turn reasoning for SBSC, we use combination of system prompt and few-shot prompting (exemplars) similar to what POT, PAL, TIR-TORA used. Similar to past works, our exemplar generation process didn't require any manual effort other than writing the system prompt and manually inspecting the final pool for format check.\nTo prepare the exemplars,"}, {"title": "A.5.2 ADDITIONAL EXEMPLAR SENSITIVITY & GENERALIZATION ANALYSIS WITH BIGGER\nPOOL", "content": "We conduct an additional study to expand the pool of exemplars for sensitivity analysis as suggested by the reviewer. We expand the pool to 30 AIME exemplars (prepared in the same manner as made initial 10 exemplars as mentioned above and in section 3.3 \"Prompting/Few-Shot Exemplars\") that were solved where we have 8 Algebra, 8 Number Theory, 8 Combinatorics and 6 Geometry questions. We randomly sample 4 exemplars from these 30 exemplars 9 times to create 9 sets of 4-shot prompts: v1, v2, v3, v4, v5, v6, v7, v8, v9. We test them on the same set of last 3 years AIME questions.\nFor further testament to our generalisation, we use AIME exemplars on MathOdyssey test-set.\nWe can clearly observe in Table 11 that the performance on AIME remains stable, as experienced in previous study in figure 3, with 9 exemplar-set randomly sampled from 30 AIME Qs. Also, from Table 12 we can see the performance on MathOdyssey stays stable with even 9 AIME exemplar sets.\nWe have utilized AIME exemplars for benchmarking SBSC on MathOdyssey 1, OlympiadBench 1, JEE-Bench 4 and Omni-MATH 5. We can observe on all these datasets, SBSC performances much better than other SOTA methods. This proves the generalization of SBSC and also proves that there has been no over fitting of exemplars done."}, {"title": "A.6 UNDERSTANDING SBSC IN DETAIL", "content": "In this section, we demonstrate some scenarios where SBSC has been successful while TIR-TORA has failed, with the help of some example questions and investigating the responses obtained from the two models.\nLet's consider the question in Example 1, involving a geometric progression of numbers written in logarithmic form, which TIR-TORA gets wrong.The method uses a binary search technique, which is not very precise when dealing with exact values required for mathematical problems, especially when fractions are involved.The solution uses a function to check whether the logarithms form a geometric progression which introduces additional complexity and potential inaccuracies because it involves comparing ratios that may not be exactly equal due to floating-point arithmetic.Also, this single-turn method tends to overlook specified constraints or necessary simplifications, which are often encountered in Olympiad level problems and instead makes false assumptions.\nThe question in Example 2 is an example scenario where TIR-TORA fails because it makes an incorrect assumption. It misinterprets the Lipschitz condition and incorrectly makes a simpler assumption that the difference $f (800) \\unicode{x2013} f(400)$ is equal to the maximum possible difference, which is 200. While the magnitude of the difference is bounded by 200, it does not mean that the actual difference will always be 200. Iterative solutions, as are often the only way out in single program based solutions, can sometimes lead to infinite loops, especially in cases where the stopping condition is not clearly defined or understood by the LLM.\nAs can be seen in Example 3, the single code is unable to take advantage of the factorization of $20^{20}$,"}, {"title": "A.7 COMPARISON ON SYMPY USAGE BETWEEN SBSC AND TIR-TORA", "content": "In this section, we conduct a study to understand the usage of SymPy library by SBSC and TIR-TORA for last 10 years of AIME and AMC questions, shown in Table 13. Specifically, we present the number of questions in which these methods utilized that library, and we present the accuracy for those set of questions. The number of questions is which both the methods utilized SymPy are similar. For both AIME and AMC problems from the last 10 years, SBSC based solutions have used SymPy only on 7 more questions in both test-sets. But when we look at the accuracy (among these filtered questions), we can observe that SBSC had over 10% absolute accuracy lead in both the datasets.\nAlso, we further filtered the common questions, for both the test-sets, where both TIR-TORA and SBSC made use of Sympy. We present this study in the table 14. We observed that more than 80% questions are common for both the methods across the test sets. In table 14, we compare the accuracy of both the methods over these common questionsand find that SBSC has 10% absolute improvement across both the test-sets."}, {"title": "A.8 STUDYING THE POTENTIAL OF SBSC TRAJECTORIES AND COMPARISON WITH 01", "content": "SBSC achieves SOTA but still room for improvement in accuracy: For frontier LLMs such as Claude 3.5 Sonnet, we showed that SBSC surpasses not only greedy-decoding but also self-consistency decoding results of TIR-TORA, PAL & COT. We also show topic wise analysis as well to show the improvement is across the topics. While we do achieve SOTA, accuracy but there is still room for improvement as evident from the scores. On last 11 years of AIME questions, despite being SOTA SBSC achieves 36% accuracy in pass@1. So there is a question on how much can SBSC trajectory really solve more.\n01 like complex reasoning methods achieves new SOTA for COT based math reasoning: Sophisticated reasoning systems like o1 (OpenAI, 2024) perform \"long thinking\" to do potentially tree-search/self-reflection/debate over natural-language reasoning chains to arrive at the final trajectory (Huang et al., 2024; Qin et al., 2024; OpenAI, 2024). They achieve significantly higher scores, on AIME and AMC, compared to previous COT scores by frontier LLMs such as Sonnet and GPT40 and use human like COT reasoning."}, {"title": "A.9 PAL EXEMPLARS", "content": "In this section, we provide the prompts for Program-Aided Language models (PAL) method. We initially used the default prompt as mentioned in the original PAL paper, but the results were poor. We noticed that the response often contained textual reasoning before or after the program, which isn't the desired format for PAL. Hence, we modify the instructions to confine the responses only to include Python program and subsequently, also notice improved accuracy.\nFor AIME\nLet's use python program to solve math problems.\nDO NOT USE ANY TEXTUAL REASONING.\nYour response must start with: \"\"python\nYour response must end with: print(result)\nHere are some examples you may refer to.\nExample Problem: A frog begins at Po = (0,0) and makes a sequence of jumps according to the following rule: from Pn = (xn, Yn), the frog jumps to Pn+1, which may be any of the points (xn + 7, Yn + 2), (xn + 2, Yn + 7), (xn - 5, Yn \u2013 10), or (xn \u2013 10, Yn 5). There are M points (x, y) with |x| + |y| \u2264 100 that can be reached by a sequence of such jumps. Find the remainder when M is divided by 1000.\nExample Problem: The AIME Triathlon consists of a half-mile swim, a 30-mile bicycle ride, and an eight-mile run. Tom swims, bicycles, and runs at constant rates. He runs fives times as fast as he swims, and he bicycles twice as fast as he runs. Tom completes the AIME Triathlon in four and a quarter hours. How many minutes does he spend bicycling?"}, {"title": "A.10 TIR-TORA EXEMPLARS", "content": "For AIME\nIntegrate step-by-step reasoning and Python code to solve math problems using the following guidelines:\n1)Analyze the question and write functions to solve the problem; the function should not take any arguments. 2)Present the final result in LaTeX using a \u2018\u25a1\u2018 without any units. 3)Utilize the \u2018pi\u02bb symbol and 'Rational\" from Sympy for \u03c0 and fractions, and simplify all fractions and square roots without converting them to decimal values.\nOnce you get the code output, just display the output answer within '' and terminate.\nHere are some examples you may refer to:\nExample Problem: A frog begins at Po = (0,0) and makes a sequence of jumps according to the following rule: from Pn = (xn, Yn), the frog jumps to Pn+1, which may be any of the points (xn + 7, Yn + 2), (xn + 2, Yn + 7), (xn - 5, Yn \u2013 10), or (xn \u2013 10, Yn \u2013 5). There are M points (x, y) with |x| + |y| \u2264 100 that can be reached by a sequence of such jumps. Find the remainder when M is divided by 1000."}, {"title": "A.1"}]}