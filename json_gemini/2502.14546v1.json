{"title": "Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks", "authors": ["Maya Bechler-Speicher", "Ben Finkelshtein", "Fabrizio Frasca", "Luis M\u00fcller", "Jan T\u00f6nshoff", "Antoine Siraudin", "Viktor Zaverkin", "Michael M. Bronstein", "Mathias Niepert", "Bryan Perozzi", "Mikhail Galkin", "Christopher Morris"], "abstract": "While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance. Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, relational databases, or chip design. Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases. Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights. These limitations have prevented the development of truly useful graph foundation models. This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning.", "sections": [{"title": "1. Introduction", "content": "Graphs are versatile mathematical structures capable of modeling complex interactions among entities across a wide range of disciplines, including the life sciences (Wong et al., 2023), social sciences (Easley & Kleinberg, 2010), and optimization (Cappart et al., 2021), underlining the need for specialized machine-learning methods to extract meaningful insights from graph-structured data. Hence, in recent years,"}, {"title": "Present work", "content": "In this position paper, we argue that graph learning must significantly revise its current datasets and benchmarking practices to remain impactful and relevant; see Figure 1 for an overview. Specifically, we\n\n1. discuss the current shortcomings in graph learning benchmarks, including the lack of transformative real-world problems, an overfocus on specific data modali-"}, {"title": "Related work", "content": "One of the first efforts towards more principled benchmarking of GNNs was taken by Dwivedi et al. (2020), who proposed a suite of real and synthetic graphs spanning a variety of node-, edge-, and graph-level tasks as well as an attempt to standardize evaluation protocols. However, the majority of the tasks either have a graph structure superimposed on the original dataset (such as graphs extracted from vision datasets like CIFAR10 which are long solved in the vision community) or focus on small synthetic graphs with a saturated performance. Another limiting factor is the strongly suggested model size below 500k parameters that was supposed to test models' inductive biases. While reasonable for the state of graph learning in 2020, such a manually set parameter count ceiling makes little sense in modern deep learning where scaling laws suggest model capabilities grow with both dataset size and parameter count (Hoffmann et al., 2022; Schaeffer et al., 2023; Wei et al., 2022).\n\nSoon after, Hu et al. (2020a) released the Open Graph Benchmark (OGB), a comprehensive suite of datasets encompassing various domains, tasks, and graph distributions. The authors proposed to gather results in a centralized, publicly visible leaderboard. The submission system requires researchers to provide test results, the corresponding validation performance, the number of learnable parameters, and some information about the tuning procedure. This effort goes in the direction of more informative and standardized benchmarking practices. Nevertheless, many datasets in the suite address (such as 2D molecular graphs or academic citation networks) are still a far cry from transformative real-world applications. As we discuss later in Sections 2 and 3, these graphs either fail to encode relevant information (e.g., 3D spatial arrangements of atoms) or induce a structural inductive bias that is of unclear advantage for downstream generalization performance. While we note that some (large-scale) more impactful benchmarks are exposed by OGB, the research community has focused on them with relatively lower priority. This is likely due to the inherent difficulty of scaling more sophisticated and expressive architectures to larger graphs or the interest drawn by more specific set-"}, {"title": "2. Missing transformative real-world applications and supporting benchmarks", "content": "We believe that the graph learning community has not yet identified benchmarks showcasing transformative real-world applications that genuinely exploit the benefits of machine learning on graphs. Unlike the computer vision or natural language domains, graph learning has no \u201cnatural\" application areas, as graphs usually abstract other data modalities featuring more or less evident relational structures.\n\nIn the past, graph learning primarily focused on benchmarking newly developed GNN architectures on datasets stemming from specific applications. The molecular domain, e.g., predicting properties of small 2D molecular graphs (Hu et al., 2020a; Morris et al., 2020) has been an area of particular interest. Meanwhile, meaningful small 2D molecular graphs only cover minor, niche sub-fields in chemistry or drug discovery, where it is more natural to relate a 3D structure and a property evaluated at a quantum mechanical level of theory. In addition, transforming raw chemical data obtained from, e.g., experiments or quantum mechanical calculations into 2D molecular graphs can be time-consuming; it often results in the loss of important information and, thus, fails to capture the relationship between spatial atomic arrangements and properties.\n\nIn addition to challenges in supervised graph learning, similar issues arise in graph generation. Most papers benchmark"}, {"title": "Suggested remedies", "content": "The community should shift focus from smaller, less relevant 2D molecular benchmarks to problems naturally represented as graphs. One promising area is combinatorial optimization, where graphs encode problem instances, such as in the vehicle routing problem (Toth & Vigo, 2002) or bipartite graphs in integer-linear programming (Schrijver, 1986), as discussed in Cappart et al. (2021). Combinatorial optimization benchmarks offer distinct advantages: (1) clear real-world applications, (2) easy generation of large datasets, and (3) ideal testbeds for studying size generalization.\n\nBeyond combinatorial optimization, other high-potential areas for GNNs include satisfiability solving (Biere et al., 2021), recommender systems (Wu et al., 2022), social networks (Newman, 2003), and power-flow networks (Owerko et al., 2020). Projects like RelBench (Robinson et al., 2024) and 4DBInfer (Wang et al., 2024) demonstrate GNNs' utility in automating machine learning on relational databases, while TpuGraphs (Phothilimthana et al., 2023) highlights their potential in computer systems. GNNs are also effective in automated chip design, such as in AlphaChip, where reinforcement-learning-based models leverage netlist embeddings (a hypergraph of circuit components and their connections) (Mirhoseini et al., 2021; 2024).\n\nIndustrial datasets like social networks often involve sensitive data, limiting accessibility. Better anonymization methods, such as generating anonymous graphs similar to real-world data (Yoon et al., 2023), could address this. While GNNs have been used for de-anonymization (Cre\u0163u et al., 2022), anonymized graph generation remains an open challenge.\n\nGraph generation also holds promise for design-related tasks, mainly through diffusion models (Liu et al., 2024). For example, these models create heat maps for sampling"}, {"title": "3. Graphs are not necessarily constructed in a meaningful way", "content": "As discussed above, graphs are higher-level abstractions of real-world phenomena or observables featuring relational structure. Hence, their effectiveness in tackling a specific task will inherently depend on how they are constructed and whether the relational information they encode predicts the problem (Halcrow et al., 2020). However, commonly adopted graph learning benchmarks often do not consider the meaningfulness, relevance, and completeness of the proposed constructed graphs; in fact, they sometimes either represent unsuitable formalisms for the data modality at hand, fail to encode important information, or do not correlate with the considered learning targets. We provide some examples in the following.\n\nA first exemplary case is that of the PASCALVOC-SP and COCO-SP datasets (Dwivedi et al., 2022b). Their graphs encode coarse-resolution images, with rag-boundary edges drawn to connect super-pixels corresponding to segmented regions. However, this modeling choice is not grounded on any theoretical or empirical justification. As such, it is unclear whether modeling images as graphs in this way is helpful for object detection or the vision domain in general.\n\nSpatiotemporal datasets, such as traffic networks, e.g., PEMS-BAY and METR-LA (Li et al., 2017) or air quality measurements. e.g., AQI (Zheng et al., 2015), rely on sensor readings taken at various locations. Subsequently, a thresholded Gaussian kernel is applied to the pairwise distances between these sensor locations to construct the graph structure, introducing structure to an otherwise fully connected weighted graph by imposing a threshold to decide which connections are retained. While this preprocessing step provides a relational structure that facilitates using graph-based methods, it is fundamentally arbitrary and may misrepresent the system's dynamics. For instance, the choice of the threshold value is often heuristic, potentially omitting meaningful connections, e.g., emphasizing short-range interactions, which may or may not be the right choice for the problem. This highlights the need for more principled, data-driven methodologies for constructing spatiotemporal"}, {"title": "Suggested remedies", "content": "Virtually any real-world phenomenon and system can potentially be modeled as a \"graph\" (Veli\u010dkovi\u0107, 2023), but this does not imply that any choice of relational structure is equally relevant or predictive or that a relational framework is a convenient modeling choice. Benchmarks should be designed in a way that accounts for these aspects systematically and quantitatively, openly and structurally considering the motto:\n\n\"Not everything that could be modeled as a graph should be modeled as a graph.\"\n\nWhen proposing a new benchmark, authors should discuss the advantages of adopting a \u201crelational\u201d modeling framework, articulating the advantages expected from processing data framed in graphs w.r.t. other possible modalities. In addition, they should discuss the choice of node and edge features and the rationale for how edges are determined in the first place. Crucially, authors should not only illustrate how graphs are constructed but expand on why the chosen approach is expected to be advantageous for the prediction task at hand.\n\nQuantitatively, we advise that benchmarks should always be accompanied by (adequately tuned) baselines such that comparisons with them will allow us to underscore the ad-"}, {"title": "4. Bad benchmarking culture", "content": "We believe inadequate benchmarking culture significantly hinders the graph learning community, irrespective of impactful applications (see Section 2) or the usefulness of underlying graphs (see Section 3). While poor benchmarking exists across machine learning (Herrmann et al., 2024), it is particularly problematic in graph learning. Even for standard datasets (Morris et al., 2020), inconsistent evaluation protocols and dataset splits result in highly variable performance reports (see Appendix B.4), with some papers overestimating performance by reporting validation metrics (Errica et al., 2020). Small datasets like MUTAG (Morris et al., 2020), with only 188 graphs, lead to large standard deviations and unreliable comparisons, while some suffer from misclassifications or insufficient class representation (Li et al., 2023; Platonov et al., 2023).\n\nNewly proposed architectures are often unfairly compared to outdated baselines, with hyperparameters fine-tuned on a small number of datasets but not for baselines. Theoretically motivated GNNS (Maron et al., 2019; Morris et al., 2019) frequently claim inflated performance gains by avoiding comparisons with state-of-the-art models.\n\nThe community often overlooks the relevance of minor improvements. For instance, ZINC (Dwivedi et al., 2020) tasks can be easily solved with standard chemoinformatics tools (Landrum, 2016), yet incremental improvements on such benchmarks are often highlighted. Additionally, limited molecular and material modeling domain knowledge prevents meaningful task understanding. For example, current state-of-the-art models often ignore critical relationships between 3D structure and molecular properties.\n\nIn 2D graph generation, datasets like QM9 (Wu et al., 2018) and ZINC250K (G\u00f3mez-Bombarelli et al., 2018) dominate despite near-perfect performance. More robust benchmarks, such as MOSES (Polykovskiy et al., 2020) and GUACAMOL (Brown et al., 2019), remain underutilized due to high computational demands. Benchmarking inconsistencies, such as differing dataset splits (Siraudin et al., 2024), inappropriate reliance on novelty for QM9 (Vignac"}, {"title": "Suggested remedies", "content": "To address these challenges, the graph learning community must develop practical tasks and robust evaluation frameworks. Unlike LMsys Arena's ELO-based evaluation (Zheng et al., 2023), graph learning lacks trusted benchmarks resistant to manipulation. While domain expertise poses challenges, creating expert-validated benchmarks can significantly improve model evaluation and adoption.\n\nA Kaggle-like competition with hidden test sets at the NeurIPS benchmark track could realistically assess models across domains like molecular prediction and combinatorial optimization. Addressing data quality issues requires larger, domain-relevant datasets such as ADMET BENCHMARK GROUP (Swanson et al., 2023) or PUBCHEMQC PM6 (Nakata et al., 2020), which provide diverse, real-world data. Multidisciplinary collaboration is essential for curating datasets and translating real-world problems into graph learning tasks (You et al., 2020).\n\nFor 2D molecule generation, benchmarks like MOSES and GUACAMOL should replace outdated ones like QM9 and ZINC250K for serious evaluations. Future efforts must focus on computational efficiency and improved benchmarks such as SPECTRE, incorporating larger datasets with diverse structural properties. Evaluations must include error bars and report ratios and prioritize combined metrics like MMD and VUN. We advocate for new benchmarks extending existing frameworks to effectively evaluate diverse, complex structures."}, {"title": "5. Implication: No true foundation model exists for graph learning", "content": "In deep learning, large pre-trained foundation models (Llama Team, 2024; Gemini Team, 2024) that unify multiple modalities (e.g., text, images, video, audio) excel at predictive and generative tasks, reshaping research and industry. However, similarly impactful graph foundation"}, {"title": "Suggested remedies", "content": "Despite these challenges, GFMs and robust real-world graph benchmarking are critical for advancing graph learning alongside progress in other deep learning areas. We propose shifting from one model for one dataset to one model for all datasets to provide a comprehensive view of model performance across diverse graphs. For example, instead of training a separate model for each task in a five-task benchmark, training one model for all tasks is preferable. For particularly non-trivial setups (e.g., combining classification with regression), we suggest an encoder-processor-decoder approach (Battaglia et al., 2018; Ibarz et al., 2022): pre-train a unified backbone model and fine-tune task-specific encoders and decoders. Finally, we advocate for creating large-scale, high-quality datasets of diverse graph structures (e.g., sparse, dense, homophilic, heterophilic, directed, multi-relational), addressing data gaps with synthetic data (Palowitch et al., 2022), and ensuring data decontamination by excluding known test sets from pre-training corpora."}, {"title": "6. Alternative views", "content": "Fields adjacent to graph learning, such as geometric deep learning (GDL) (Bronstein et al., 2017; 2021), are thriving and achieving remarkable successes. GDL has driven advancements in structural biology (Abramson et al., 2024; Jumper et al., 2021; Townshend et al., 2021) and materials science (Merchant et al., 2023; Reiser et al., 2022; Zeni et al., 2025). It also underpins state-of-the-art interatomic potentials for atomistic simulations at first-principles accu-"}, {"title": "7. Empirical evidence", "content": "Here, we support our claims made in the previous four sections with empirical evidence\u00b3."}, {"title": "7.1. Graphs not necessarily constructed in a meaningful way", "content": "In Section 3, we raised concerns about the lack of correlation between graph structures in commonly used benchmarks and the intended learning targets. In this subsection, we provide empirical evidence to support this claim further. Recently, Deac et al. (2022); Wilson et al. (2024) proposed a message-passing scheme in which, during every odd layer, the original graph is disregarded in favor of propagating information through a fixed-structure expander graph-specifically, a Cayley graph. Ablation studies pre-"}, {"title": "7.2. Reassessing simple baselines on PCQM4Mv2", "content": "In Section 4, we discussed problematic practices of empirical evaluations of novel GNN architectures. One common issue is the citation of old, outdated reference results to quantify the performance improvements of new architectures over simpler baselines. Often, these baseline results suffer from suboptimal hyper-parameters and are cited as-is for many years without reevaluation. As a consequence, the performance gains of newer architectures are commonly overestimated.\n\nHere, we demonstrate this issue on the commonly used PCQM4Mv2 dataset (Hu et al., 2021a), among the few large-scale datasets for graph-level learning tasks and is par-"}, {"title": "7.3. The meaningfulness of architectural changes", "content": "In addition, in response to Section 4, we further exemplify the GNN evaluations' brittleness in the node-prediction setting. Platonov et al. (2023) proposed a set of heterophilous graph datasets to evaluate the performance of various GNNs, including both baseline and heterophily-specific GNNs. In"}, {"title": "7.4. Multi-task pre-training with encoder-processor-decoder", "content": "In this section, as suggested in Section 5, we run a series of experiments to investigate multi-task pre-training/fine-tuning using an encoder-processor-decoder framework. For small molecules, similar settings have been explored recently in Kl\u00e4ser et al. (2024); Sypetkowski et al. (2024); Frasca et al. (2024). Here, we also want to study a cross-domain setting with data from vision, function-call graphs, large molecules, and social networks. The aim is to gather an initial signal on the suitability of this architectural pattern when pre-trained on a mix of vastly different graph tasks, even on a relatively small scale. As highlighted in Section 5, a large-scale, curated pre-training corpus is currently lacking, and we believe that positive results from our experiments could catalyze the community's efforts in building such a corpus, accompanied by standardized pre-training setups and evaluation procedures.\n\nWe train domain-specific encoders (e.g., embedding atom and bond types in molecules) and task-specific decoder MLPs. For the processor network, we both evaluate an MPNN based on the GINE architecture (Xu et al., 2018b) and a GT based on Graphormer (Ying et al., 2021) with a soft attention bias and RWSE structural encodings (Dwivedi et al., 2022a). For our experiments, we assemble two sets of datasets: the upstream mix used for pre-training and the downstream mix used for fine-tuning. We freeze the processor weights during fine-tuning and learn a new encoder and decoder. If the downstream task permits, we reuse the encoder from one of the datasets in the upstream mix. In addition, for each upstream or downstream dataset, we train baseline models with the same model architectures as our pre-trained models but trained from scratch in a single-task fashion. All experimental details are enclosed in Appendix B.3.\n\nOur upstream mix contains PCQM4Mv2 (Hu et al., 2021a), COCO-SP (Dwivedi et al., 2022b), and", "Architectures, training, and evaluation": "Architectures, training, and evaluation"}, {"title": "8. Conclusion", "content": "This paper highlights the need to rethink benchmarks and practices in graph learning. While GNNs have succeeded"}, {"title": "A. Extended related work", "content": "GRAPHWORLD (Palowitch et al., 2022) offered a synthetic perspective on graph benchmarking by showing that existing datasets cover a relatively narrow distribution of possible graphs and tuning common MPNN architectures is not indicative of their performance in other, less common domains. To alleviate the distribution issue, GRAPHWORLD suggested generating synthetic graphs using stochastic block models (Karrer & Newman, 2011) with more diverse connectivity patterns and probing GNNs on the synthetic datasets. Unfortunately, the dataset did not receive significant attention and adoption in the graph learning community partly due to the stated synthetic nature of the tasks.\n\nIn addition, Veli\u010dkovi\u0107 et al. (2022) proposed CLRS, an algorithmic reasoning benchmark modeling the simulation of 30 classical algorithms as graph tasks such as node- or edge-level prediction and evaluates in a difficult size generalization setting. Algorithmic reasoning, in particular in the size generalization setting, receives some interest in the broader machine learning community (Zhou et al., 2024a;b; McLeish et al., 2024) and is arguably highly relevant to the study and advancement of the reasoning capabilities of neural networks in general. At the same time, algorithmic reasoning methods are typically benchmarked on synthetic tasks aimed at studying reasoning and learning capabilities in a controlled setting. There is no suitable replacement for high-quality, real-world benchmarks with direct downstream applications.\n\nFurthermore, the benchmarking landscape for MPNNs remains constrained by the lack of large-scale and realistic graph datasets, particularly in domains like social networks. Commonly used datasets such as REDDIT (Hamilton et al., 2017) and FLICKR (Zeng et al., 2019) are often cited as representative of real-world social networks. However, these datasets fail to capture key characteristics of actual social networks, such as high-degree hubs and dense community structures, as their average node degree is significantly lower. This discrepancy makes them poor representatives for large, realistic graphs. Similarly, in social network-based datasets such as the Twitter retweet-induced subgraph dataset (Ribeiro et al., 2017), it is unclear whether the features and adjacency relationships of the sampled subgraph align with those of the full graph. Moreover, the structure of these subgraphs is constructed using a random walk-based crawler on the original graph. This sampling process further reduces the average node degree, making the dataset less representative of large-scale graphs, similar to REDDIT and FLICKR. We note here that the unavailability of real-world social network graph data is likely due to factors outside our field's control, e.g., privacy concerns and commercial relevance.\n\nAnother important aspect of many real-world graphs is their inherently dynamic nature. That is, nodes and edges and their features change over time. This aspect is often neglected in many datasets, including social networks. Recent efforts have introduced valuable benchmarking suites for learning on temporal graphs, e.g., the Temporal Graph Benchmark (TGB) (Huang et al., 2023; Gastinger et al., 2024a). Interestingly, on these temporal datasets, researchers have found that overlooked baselines and simple heuristics can be particularly predictive and outperform more sophisticated temporal GNNs (Gastinger et al., 2024a;b). This puts in question the relevance of some of the proposed benchmarks and the significance of the progress made by the community."}, {"title": "B. Additional experimental details", "content": "Here, we provide additional experimental details and results."}, {"title": "B.1. Graphs not necessarily constructed in a meaningful way", "content": "For each model among GraphConv, GIN, and GAT, we tuned the learning rate in { $10^{-3}, 5\\cdot10^{-3}$ }, number of layers in {3,5}, dropout in {0, 0.3}, hidden dimensions in {32, 64}, batch size in {16, 32}, early stopping with patience of 50 steps on the validation loss, and sum-pooling. We used ReLU activation and CrossEntropy loss.\n\nFor each dataset, we trained with seed 0 over all the hyper-parameter configurations and selected the best performing configuration on the validation set, according to the ROC-AUC scores. We then trained each model with its selected configuration with seeds 1 and 2. Finally, we report the mean and standard deviation of the ROC-AUC scores over the test set over these 3 seeds.\n\nWe consider the CGP propagation scheme from Wilson et al. (2024), where for each model, we utilize the Cayley graph in each layer and do not consider the original graph at all. For the DeepSet evaluation, we used the same architecture of GraphConv and fed it with empty graphs."}, {"title": "B.2. Reassessing simple baselines on PCQM4Mv2", "content": "We make minor changes to the layer configuration by using SiLU activation (Hendrycks & Gimpel, 2016) instead of ReLU for improved gradient flow. We also replace the BatchNorm with LayerNorm and apply it after the skip connection, similar to a standard transformer encoder layer. In each GINE layer, we use a 2-layer MLP as an update function with a hidden dimension of 1024 (double the embedding dimension (512)). After the final GINE layer, we apply sum-pooling followed by a 3-layer MLP, outputting a graph-level prediction.\n\nWe set the number of GINE layers to 20 and the latent embedding dimension to 512. In preliminary experiments, we found larger models to be overfitting, so we fixed this model configuration with approximately 20 million trainable parameters. This is comparable to the model sizes used in the graph transformer literature and about five times larger than the GINE model used to obtain the original results. Note that despite a model depth of 20 layers, we observed no performance degradation due to over-smoothing, trivially mitigated by following basic deep learning practices such as skip-connections and deep MLPs as update functions. It is known, of course, that these practices also prevent smoothing phenomena in transformers (Dong et al., 2021), and the same holds for MPNNs.\n\nWe train with the L1-loss for one million gradient descent steps using a batch size of 512 with the Adam optimizer. The learning rate warms up linearly for the first $10^4$ steps and follows a cosine decay schedule for the remainder of training towards a minimum rate of $10^{-6}$. No gradient clipping is used. We tune the remaining hyper-parameters through a grid search. Specifically, we tune the learning rate in { $2\\cdot10^{-4} ,1\\cdot10^{-4},5\\cdot10^{-5}$ }, the dropout rate in { $0,1\\cdot10^{-1} ,2\\cdot10^{-1}$ } and the weight decay in { $0,1\\cdot10^{-1}$ }. Tuning is done with RWSE features, and we reuse the same configuration for GINE without RWSE. Since the original validation split of PCQM4Mv2 is used to compare models in the literature, we create a separate holdout set by sampling 10K graphs uniformly at random from the training data and use this set for model selection and hyperparameter tuning. Each training run uses a single Nvidia H100 GPU and lasts approximately 8 hours. In total, hyperparameter tuning consumed less than 200 H100 hours of computing. The final hyper-parameters are provided in Table 6. For the final results reported in Table 2, we average the performance over three runs with different random seeds and also provide the corresponding standard deviation, which is relatively low."}, {"title": "B.3. Multi-task pre-training with encoder-processor-decoder", "content": "Here, we outline details for our experiments with the encoder-processor-decoder setup."}, {"title": "B.3.1. MODEL ARCHITECTURES", "content": "We consider an encoder-processor-decoder setup and two different processors, an MPNN with GINE (Hu et al., 2020b) layers and a graph transformer derived from Graphormer (Ying et al., 2021). As a common practice in the case of transformer architectures on graphs, we also experiment with injecting node-wise structural encodings, particularly RWSEs (Dwivedi et al., 2022a). In what follows, we detail the encoder-processor-decoder setup.\n\nUsing task-specific encoders, we embed node and edge features into a common embedding dimension $d \\in \\mathbb{N}+$ for both architectures. If no node or edge features are available, we use learnable vectors that we train jointly with the architecture. Following standard practice in (graph) transformer encoders (Ying et al., 2021), we add a [cls] token from which we read out graph-level representations.\n\nSubsequently, a processor network computes node- and graph-level representations from the embedded node, edge features, and graph structure. Given a graph G, both the MPNN and graph transformer update node representations $X \\in \\mathbb{R}^{n \\times d}$ at each layer as\n\n$X' \\leftarrow X + \\Phi( \\text{LayerNorm}(X), G)$,\n\n$X'' \\leftarrow X' + \\text{MLP}(\\text{LayerNorm}(X'))$,\n\nwhere MLP is a two-layer MLP with GELU non-linearity (Hendrycks & Gimpel, 2016) and $\\Phi( \\cdot, G)$ is either a graph convolution or attention, conditioned on G. Graph convolution is implicitly conditioned via message-passing over the local neighborhood.\n\nIn the case of attention, we add a graph-aware attention bias to the unnormalized attention matrix. Concretely, the graph transformer layer computes full multi-head scaled-dot-product attention over node-level tokens with a soft-attention bias computed from the edge features of the graph. The attention bias is a tensor $B \\in \\mathbb{R}^{L \\times L \\times h}$, where $L \\in \\mathbb{N}+$ is the number of tokens and $h \\in \\mathbb{N}+$ is the number of attention heads. In particular, we compute a separate attention bias for each attention head. For a graph with n nodes, we set $L := n + 1$ (accounting for the [cls] tokens). For simplicity, we write $i \\in \\mathbb{N}+$ to indicate the ith node in an arbitrary but fixed node ordering. We refer to the [cls] tokens as node $n + 1$. Further, only for the graph transformer, we use a maximum context size of 8192 and remove additional nodes that exceed this size. We then compute the attention bias B such that for all edges (i, j),\n\n$B_{ij} := W e_{ij}$,\n\nwhere $e_{ij} \\in \\mathbb{R}^{d}$ is the edge feature of (i, j) and $W \\in \\mathbb{R}^{d \\times h}$ is a learnable weight matrix. Again, we omit bias terms for clarity. If no edge exists between nodes i and j, we set $B_{ij}$ to all-zeros. For the [cls] token, we use learnable vectors $e_{in}, e_{out} \\in \\mathbb{R}^{d}$ as the attention bias for in- and out-coming edges, respectively, i.e., we set\n\n$B_{(n+1)j} := e_{in}$,\n\n$B_{i(n+1)} := e_{out}$.\n\nFinally, we add B as a soft bias to the unnormalized attention matrix, that is, before applying softmax.\n\nLastly, we apply a decoder network that makes task-specific predictions. In our experiments, we used the same MLP layout for all decoders. In particular, given a representation vector $x \\in \\mathbb{R}^{d}$, we define our decoder MLP as\n\n$W_2 \\text{LayerNorm} ( \\text{GELU} (W_1 x))$,\n\nDecoder"}, {"title": "B.3.2. MULTI-TASK PRE-TRAINING", "content": "Here, we outline details on the multi-task pre-training."}, {"title": "Training loop and optimization parameters", "content": "We perform multi-task pre-training by using data loaders for all tasks and accumulating gradients from each task at each iteration of the training loop, effectively simulating a \"heterogeneous batch\" of data from all available tasks. We train on bfloat16 with clipped gradients and a cosine learning rate scheduler."}, {"title": "Pretraining mix", "content": "As already mentioned in the main text, the pretraining mix is formed by the datasets described in Table 7, which differ in domain, task, and structural properties."}, {"title": "Hyperparameter tuning", "content": "We sweep the learning rate over {$4\\cdot10^{-5},7\\cdot10^{-5}, ..., 1\\cdot10^{-3}$} for graph transformers and {$4\\cdot10^{-5},7\\cdot10^{-5}, ..., 1\\cdot10^{-2}$} for GINE and train for 100K gradient steps. We pick the pre-trained checkpoint based on the best overall validation loss, which we compute as the sum of all three task losses."}, {"title": "B.3.3. SINGLE-TASK FINE-TUNING", "content": "Here, we outline details on the single-task fine-tuning."}, {"title": "Architectural details", "content": "Across all finetuning experiments, the prediction heads (i.e., the decoders) are initialized and trained from scratch, while the (pre-trained) processors are kept frozen. If a downstream task shares the same node and/or edge features with a pretraining dataset, we reuse the corresponding (pre-trained) encoders, which are also frozen during finetuning. Otherwise, a new encoder is initialized and trained from scratch. Note that downstream datasets with featureless nodes share identical (pre-trained) encoders; see, e.g., STARGAZERS below.\n\nIn all cases, we run a standard single-task finetuning on bfloat16 with clipped gradients and a cosine learning rate scheduler."}, {"title": "B.4. Variance of results reported on ENZYMES", "content": "In Section 4, we discussed problematic practices prevalent in experimental evaluations of GNNs. A common problem is using small, high-variance datasets without an established evaluation protocol. For some datasets, the numbers reported throughout the literature vary substantially, resulting in an inconsistent and confusing representation of model performance. Here, we illustrate this problem for the commonly used ENZYMES dataset (Morris et al., 2020) as an example of how extreme reported performance measurements vary."}]}