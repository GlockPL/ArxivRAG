{"title": "LLM Knows Geometry Better than Algebra:\nNumerical Understanding of LLM-Based Agents in A Trading Arena", "authors": ["Tianmi Ma", "Jiawei Du", "Wenxin Huang", "Wenjie Wang", "Liang Xie", "Xian Zhong", "Joey Tianyi Zhou"], "abstract": "Recent advancements in large language mod-\nels (LLMs) have significantly improved per-\nformance in natural language processing tasks.\nHowever, their ability to generalize to dynamic,\nunseen tasks, particularly in numerical reason-\ning, remains a challenge. Existing benchmarks\nmainly evaluate LLMs on problems with pre-\ndefined optimal solutions, which may not align\nwith real-world scenarios where clear answers\nare absent. To bridge this gap, we design\nthe Agent Trading Arena, a virtual numeri-\ncal game simulating complex economic sys-\ntems through zero-sum games, where agents\ninvest in stock portfolios. Our experiments re-\nveal that LLMs, including GPT-40, struggle\nwith algebraic reasoning when dealing with\nplain-text stock data, often focusing on local\ndetails rather than global trends. In contrast,\nLLMs perform significantly better with geo-\nmetric reasoning when presented with visual\ndata, such as scatter plots or K-line charts, sug-\ngesting that visual representations enhance nu-\nmerical reasoning. This capability is further\nimproved by incorporating the reflection mod-\nule, which aids in the analysis and interpreta-\ntion of complex data. We validate our findings\non NASDAQ STOCK dataset, where LLMS\ndemonstrate stronger reasoning with visual\ndata compared to text. Our code and data are\npublicly available at https://github.com/\nwekjsdvnm/Agent-Trading-Arena.git.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models\n(LLMs) have demonstrated exceptional proficiency\nacross various domains, achieving state-of-the-\nart performance in natural language processing\n(NLP) tasks such as translation (Koshkin et al.,\n2024), summarization (Yang et al., 2023), and\nreasoning (Kalyanpur et al., 2024; Qiao et al.,\n2023). While LLMs excel in language-based\ntasks, further progress in numerical and geo-\nmetric reasoning is essential to tackling com-\nplex, interdisciplinary challenges, particularly in\nfields like finance and scientific research. Bench-\nmarks such as GSM8K (Cobbe et al., 2021) and\nMATH (Hendrycks et al., 2021) have been de-\nveloped to assess and improve the mathematical\nproblem-solving abilities of LLMs through struc-\ntured datasets and standardized evaluation proto-\ncols. These benchmarks not only advance LLM\ndevelopment but also represent a crucial step in en-\nhancing the ability of artificial intelligence systems\nto solve mathematical reasoning problems.\nIn response, recent updates to LLMs have fo-\ncused on improving their mathematical abilities,\nwith models (Ahn et al., 2024; Romera-Paredes"}, {"title": "2 Related Works", "content": "2.1 Mathematical Benchmarks for LLMs\nMath word problems (MWPs) have been widely\nstudied, leading to the development of various\nbenchmarks for evaluating models' mathematical\nreasoning and problem-solving abilities. Early\ndatasets, such as MAWPS (Koncel-Kedziorski\net al., 2016), standardized existing problems to\nfacilitate consistent evaluation. Math23K (Wang\net al., 2017) introduced a large-scale collection\nof Chinese arithmetic problems that require struc-\ntured equation solving. To increase diversity,\nbenchmarks like ASDiv (Miao et al., 2020) and\nSVAMP (Patel et al., 2021) provide richer annota-\ntions and a broader range of problem types. More\nrecent benchmarks, including GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021),\nfocus on multi-step reasoning and advanced mathe-\nmatical concepts, broadening the scope of evalua-\ntion. Additionally, MathQA-Python (Austin et al.,\n2021), a Python variant of MathQA (Amini et al.,\n2019), emphasizes programmatic reasoning, while\nMGSM (Shi et al., 2023) extends these benchmarks\nto multilingual contexts. Despite these advance-\nments, current models (Lu et al., 2023b,a) primarily\nrely on memory-based answering strategies learned"}, {"title": "2.2 LLMs for Enhanced Mathematical Reasoning", "content": "Building on these benchmarks, LLMs have ad-\nvanced mathematical problem-solving by incor-\nporating specialized datasets into their training.\nModels such as Galactica (Taylor et al., 2022),\nPaLM-2 (Anil et al., 2023), Minerva (Lewkowycz\net al., 2022), and LLaMA-2 (Touvron et al., 2023)\nleverage extensive datasets during pre-training, im-\nproving their mathematical reasoning abilities and\nunderstanding of complex concepts. Fine-tuned\nmodels like MetaMath (Yu et al., 2024), MAm-\nmoTH (Yue et al., 2024), and WizardMath (Luo\net al., 2023) are specifically tailored for mathe-\nmatical tasks. These models undergo domain-\nspecific fine-tuning with carefully curated datasets,\nenabling them to tackle advanced reasoning chal-\nlenges and significantly enhance performance on\nspecialized benchmarks.\nHowever, existing methods (Naveed et al., 2023;\nYang et al., 2024a) often rely on large training\ndatasets, raising concerns about the true reason-\ning capabilities of LLMs. While these methods\nprimarily assess performance on established prob-\nlem types, the heavy reliance on extensive training\ndata suggests that models may achieve high perfor-\nmance through memorization and pattern recogni-\ntion, rather than genuine reasoning. Consequently,\nalternative evaluation paradigms are needed to\nmore accurately assess LLMs' ability to generalize\nmathematical principles to novel scenarios."}, {"title": "3 Proposed Method", "content": "To mitigate the influence of human prior knowledge\nand memory, we designed a closed-loop economic\nsystem (Guo et al., 2024) called the Agent Trad-\ning Arena, a zero-sum game simulating complex,\nquantitative real-world scenarios. The simulation\nworkflow is illustrated in Figure 2 and further de-\ntailed in Appendix A. In the Agent Trading Arena,\nagents can invest in assets, earn dividends from\nholding assets, and pay daily expenses using virtual\ncurrency. The agent with the highest total return\nwins the game."}, {"title": "3.1 Agent Trading Arena", "content": "Structure of Agent Trading Arena. To elimi-\nnate external knowledge biases, asset prices are de-\ntermined by a bid-ask system, reflecting the prices\nat which buyers and sellers are willing to transact.\nThe system evolves solely based on agents' actions\nand interactions, without external influences. This\ndesign ensures that the outcomes of agents' actions\nare not immediately apparent but unfold gradually,\ninfluenced by other agents' decisions.\nTo encourage active participation, a dividend\nmechanism is introduced. There are two primary\nsources of income in this system: capital gains\nfrom asset price differentials and dividends from\nholding assets. Dividends for each asset are dis-\ntributed according to a predefined ratio, serving as\nan implicit anchor for asset prices. Agents hold-\ning more low-cost assets receive higher dividends.\nTo prevent passive asset holding until the end of\nthe game, agents must pay a daily capital cost pro-\nportional to their total wealth. These expenses are\noffset by asset dividends, and only agents with suf-\nficient low-cost assets can cover costs. Under the\npressure of significant daily expenses, agents must\nact swiftly and strategically, triggering frequent\ntrades and price fluctuations to stimulate market\nactivity. This dynamic mechanism ensures fairness\nin the zero-sum game while preventing agents from\nrelying on fixed strategies to find optimal solutions.\nAgents Learn and Compete in Arena. The zero-\nsum game structure is crucial to eliminating the pos-\nsibility of a universally optimal strategy. In fixed\nscenarios with a static optimal solution, agents\ncould rely on predefined rules or memory-based\napproaches, bypassing adaptive decision-making.\nThe zero-sum game ensures that there is no uni-\nversally correct solution, with outcomes evolving\ndynamically based on agent interactions and com-\npetition. This design forces agents to continually\nadapt, learn from feedback, and develop context-\ndependent strategies, promoting deeper environ-\nmental exploration and preventing reliance on static\nor memory-driven solutions.\nIn the Agent Trading Arena, agents are unaware\nof implicit rules, except for the objective to maxi-\nmize their virtual wealth throughout the simulation.\nTo win this zero-sum game, agents must effectively\nlearn from experience, decipher hidden game rules,\nand develop strategies to counter competitors. This\nrequires the ability to comprehend numerical feed-\nback, formulate enduring strategies, and make in-\nformed decisions. Unlike other mathematical rea-\nsoning problems, the results of their actions unfold\ngradually and dynamically. Moreover, agents are\neasily misled by erroneous information from com-"}, {"title": "3.2 Types of Numerical Data Input", "content": "Limitations of Textual Numerical Data. In the\nAgent Trading Arena, the generated stock data is\nstored in numerical format. When used directly\nas input to an LLM, the models often struggle to\ninterpret numerical data accurately or make sound\ndecisions. To mitigate this, we convert the data\ninto textual formats (Hu et al., 2024; Zhang et al.,\n2024), enhancing semantic features and clarifying\noutput requirements to improve the models' under-\nstanding. During interactions, the LLMs process\nstock prices, trading volumes, and market indices\npresented as textual numerical data.\nHowever, this textual approach reveals signifi-\ncant limitations. While the data is presented clearly,\nLLMs tend to focus excessively on specific values\nrather than identifying long-term trends or global\npatterns. They also struggle with understanding\ncorrelative relations and percentage changes, limit-\ning their ability to assess differences and identify\nconnections between data points. When analyzing\ntime-series data with complex patterns, LLMs of-\nten fixate on individual data points, overlooking\noverarching relations. This issue is evident in the\nanalysis output in the top-right corner of Figure 3,\nwhere LLMs' focus on individual values impedes\ntheir ability to generalize, reducing their capacity\nto extract meaningful global insights.\nAdditionally, LLMs often overemphasize re-\ncent data while undervaluing historical information,\neven when prompted to consider its importance.\nThis prevents them from effectively integrating past\ndata and recognizing long-term patterns, complicat-\ning their understanding of numerical relations and\ntrends. These challenges highlight the need for im-\nproved mechanisms to process numerical relations,\nidentify global trends, and derive deeper insights\nfrom textual numerical data.\nPotential of Visual Numerical Data. Since tex-\ntual numerical data often leads LLMs to focus on\nlocal details while neglecting broader relations, we\ninvestigated whether visual representations, such as\nscatter plots, line charts, and bar charts, could help\nLLMs better understand overall trends, similar to\nhuman reasoning. Thus, we transition from textual\nnumerical data inputs to visualized formats (Yang\net al., 2025). As demonstrated in the bottom-right\ncorner of Figure 3, visual representations enable\nLLMs to more effectively grasp global trends, pat-\nterns, and relations that are often difficult to discern\nfrom textual numerical data alone.\nThese findings highlight the advantages of struc-\ntured, visual numerical data, indicating that this\nformat allows LLMs to more intuitively and com-"}, {"title": "3.3 Reflection Module", "content": "We propose a strategy distillation method, illus-\ntrated in Figure 4, that delivers real-time feedback\nto LLMs by analyzing both descriptive textual and\nvisual numerical data. This enables the generation\nof new strategies and optimization of action plans.\nThe approach allows agents to evaluate their results,\nrefine strategies, and adapt continuously based on\nfeedback. The process begins with assessing the\nday's trajectory memory and associated strategies\nusing an evaluation function. The strategic genera-\ntion process leverages contrastive analysis of peak\nand nadir performers from the evaluation phase,\ncreating bidirectional learning signals that inform\nsubsequent iterations. This iterative cycle ensures\ncontinuous strategy evolution, fostering sustained\nimprovement in decision-making.\nThe reflection module plays a crucial role in re-\nfining strategies by offering real-time feedback. It\nanalyzes both descriptive textual and visual numer-\nical data to generate new strategies and optimize"}, {"title": "4 Experimental Results", "content": "4.1 Experimental Setup\nDatasets. To evaluate the ability of LLMs to an-\nalyze and process data, we developed the Agent\nTrading Arena, a controlled environment that iso-\nlates external factors. The system's workflow is\nillustrated in Figure 2. Within this environment,\nagents discuss stock market trends, analyze stock\ndata, and engage in trading activities. Each agent\ncan execute multiple trades per day and reflect on\nall trades at the end of each trading session.The\nAgent Trading Arena allows for adjustable num-\nbers of agents and stocks; in our experiment, we\ndeployed at least nine agents and three stocks. All\nagents were provided with the same initial capital\nto ensure identical starting conditions. Detailed in-\nformation about each agent is presented in Table 1,\nand stock-specific data is available in Table 2. To\nfurther validate our findings, we selected a subset\nof NASDAQ STOCK dataset for portfolio invest-\nment. For more details, refer to Appendix B.\nEvaluation Metrics. Each agent was assigned\nvarying capital based on their roles, and perfor-\nmance was evaluated using the following metrics:\n1) Total Return (TR): Measures the overall per-\nformance of the strategy, calculated as: TR =\n(C1-Co)/Co, where Co is the initial asset value\nand C\u2081 is the final asset value."}, {"title": "4.2 Comparative Experiments", "content": "We conducted experiments to assess the real-time\ndata analysis and reasoning capabilities of LLM-\nbased agents, focusing on how textual and visual\nrepresentations influence decision-making. First,\nwe explored the impact of textual and visual repre-\nsentations in dynamic environments. Next, we in-\ncorporated the reflection module to enhance agents'\nreasoning and data interpretation, examining how\nreflective reasoning influences decision-making.\nFinally, we validated the model's effectiveness\nthrough stock investment simulations on NAS-\nDAQ STOCK dataset, assessing agents' adaptabil-\nity and decision-making in real-world scenarios.\nTrials with Textual or Visual Input. To enhance\nLLMs' understanding of complex data in the Agent\nTrading Arena, we transitioned from textual numer-\nical inputs to visualized formats, including scat-\nter plots, line charts, and bar charts. Three types\nof visualizations were used: daily K-line charts,\ntransaction histories, and quantities traded by each\nagent. The experimental setup for the visualiza-\ntion group is shown in Figure 3 and detailed in\nAppendix C. In the Arena, LLMs without image in-\nput capabilities received only text input and did not\nperform reflection. For image-enabled LLMs, the\nfirst agent received only visual input, the second re-\nceived both textual and visual input, and the others\nreceived only textual input. None of the agents had\nreflection capabilities. For details on the selected\nLLMs, please refer to Appendix E.\nWe conducted experiments across different\nLLMs and the results for Gemini-1.5 (Reid et al.,\n2024) and GPT-40 (Hurst et al., 2024) in Table 3\nshow that agents with visual numerical input out-"}, {"title": "4.3 Ablation Experiments", "content": "Impact of Modality on LLM Competitiveness.\nWe employed a relative evaluation method for this\nexperiment. The first and second agents used\nvarious LLMs in textual and visual settings, re-\nspectively, while the remaining agents were based\non LLaMa-3 (Dubey et al., 2024) as the baseline.\nThis setup aimed to explore the impact of differ-\nent agents and modalities on LLM performance.\nThe results are shown in Table 5. The findings in-\ndicate that DeepSeek (Liu et al., 2024) exhibited\nstronger competitive performance across different"}, {"title": "5 Conclusion", "content": "In this paper, we introduced the Agent Trading\nArena, a zero-sum game designed to simulate com-\nplex economic systems and evaluate LLMs on nu-\nmerical reasoning tasks. Our results show that\nwhile LLMs struggle with plain-text numerical data\n(algebraic reasoning), their performance signifi-\ncantly improves when presented with visual data\n(geometric reasoning). This highlights the advan-\ntage of visual representations in supporting numer-\nical reasoning, particularly in complex scenarios.\nThe integration of a reflection module further en-\nhances model performance, allowing LLMs to ana-\nlyze better and interpret data. We validated these\nfindings on NASDAQ STOCK dataset, demonstrat-\ning that LLMs excel in visual geometric reasoning\ntasks, suggesting that LLMs may perform better\nwith visual numerical data than with textual numer-\nical data. Overall, our work offers insights into the\nstrengths and limitations of LLMs in dynamic nu-\nmerical reasoning tasks, particularly in the context\nof geometry vs. algebra, and sets the foundation\nfor future research on improving their performance\nin real-world, interdisciplinary challenges."}, {"title": "Limitations", "content": "This study evaluates LLMs within a virtual stock\ntrading environment, focusing on their performance\nin visual geometric reasoning. While this con-\ntrolled setting limits the generalizability to other\ndomains, it provides valuable insights into LLMs'\ncapabilities. The reliance on high-quality visual-\nizations, reflection modules, and substantial com-\nputational resources may restrict applicability in\nresource-constrained environments. Future re-\nsearch can address these limitations by broadening\nthe scope to include diverse reasoning tasks, opti-\nmizing computational requirements, and exploring\nalternative modalities for more robust and general-\nizable assessments."}, {"title": "Acknowledgments", "content": "This work was supported in part by the National\nNatural Science Foundation of China under Grants\n62301213 and 62271361, the Hubei Provincial Key\nResearch and Development Program under Grant\n2024BAB039, and the Open Project Funding of\nthe Hubei Key Laboratory of Big Data Intelligent\nAnalysis and Application, Hubei University under\nGrant 2024BDIAA01."}, {"title": "A Agent Trading Arena", "content": "A.1 Agent Trading Arena Details\nAs illustrated in Figure 5, each agent's workflow\nintegrates LLMs for chat pool interactions, stock\nanalysis, decision-making, and reflection. In the\nstock analysis and decision-making modules, all\noutputs are validated for consistency with both\ncommon sense and operational requirements be-\nfore execution.\nA.1.1 Action Decision-Making\nAction generation follows the LLM framework.\nThe agent responsible for generating actions re-\nceives corresponding prompts via SQLite. Based\non these prompts and specified output formats, the\nagent decides whether to buy, sell, or hold stocks.\nThe action generation process is outlined in Equa-\ntion 1, with input prompts shown in Figure 6 and\nthe corresponding outputs displayed in the adjacent\nfigure.\n\\begin{equation}\n\\begin{cases}\nA_{date+1} = \\Psi (I_{ns}, Z_{date}, S_{date}), & \\text{if } t \\text{ is Iters,}\\\\\nA_{date}^{t+1} = \\Psi (I_{ns}, Z_{date}, S_{date}), & \\text{otherwise,}\n\\end{cases}\n\\end{equation}\nwhere Ins represents the environment introduction,\n$Z_{date}$ denotes the memory of the stock transaction\non day date retrieved from the database, and $S_{date}$\nis the strategy for day date generated via reflection.\nA.1.2 Environmental Interaction\nTo isolate external influences, we created a virtual\nsandbox environment where each agent is assigned\na unique ID, and their actions affect the environ-\nment. The function & facilitates environmental\ninteractions, as shown in algorithm 1, where \u201cOPS\u201d\nretrieves agent actions, \u201cdate\u201d refers to the trading\ndate, and \"Z\" represents the memory used for in-\nteraction with the environment. Through $, each\nagent's actions, such as buying or selling stocks,\ndetermine the current stock price and update the\ntrading platform, including stock prices and avail-\nable shares. Stock prices are independent of exter-\nnal factors and are influenced solely by the sand-\nbox's internal dynamics. The stock price is updated\nwith each transaction according to the following\nformula:\nPrice_{curr} = \\delta (Q, F, Price_{curr}, Price_{deal})\n\\begin{equation}\nPrice_{deal} = \\frac{Q \\cdot F + Price_{curr} \\cdot Q_{total}}{Q \\cdot F + Q_{total}}\n\\end{equation}\nwhere Q is the quantity of stock traded, F is the\nfluctuation constant, $Price_{curr}$ is the current stock\nprice, $Q_{total}$ is the total number of shares available,\nand $Price_{deal}$ is the price at which the trade occurs."}, {"title": "A.1.3 Memory", "content": "The superior performance of LLM-based agents\narises from the extensive internal knowledge ac-\nquired during pre-training. The large number of\nparameters in LLMs enables the retrieval of diverse\ninformation and supports logical and inferential rea-\nsoning. To further enhance knowledge retrieval\nacross various tasks, we incorporate a memory\nmodule that empowers LLM-based agents with self-\nimprovement capabilities. This memory module\nfacilitates strategy reflection through time-series\nfeedback. Unlike qualitative tasks, quantitative\nfeedback evolves incrementally with subtle differ-\nences, presenting a challenge for the generalization\nof existing LLM-based agents.\nTo minimize the influence of pre-existing knowl-"}, {"title": "A.1.4 Reflection", "content": "We propose a strategy distillation method that\ntransforms quantitative results into descriptive text,\nwhich is then used as prompts for LLMs. This ap-\nproach aids in the analysis of results and the genera-\ntion of actionable, qualitative summaries, enabling\nLLMs to derive new strategies. These strategies are\nimplemented, monitored, and evaluated over time,\nwhile underperforming strategies are archived for\nfuture review."}, {"title": "C Visualization Input", "content": "Figure 7 illustrates the system's input prompts and\ncorresponding outputs during the strategy update\nprocess. The input prompts consist of both tex-\ntual and visual components, including daily K-line\ncharts, transaction histories, and agent trading vol-\numes, all of which inform the strategy update."}, {"title": "D Simulation Process", "content": "In the Agent Trading Arena, the simulation process\nunfolds as follows: First, rumors are generated in\nthe chat pool based on the previous day's stock\nmarket analysis. Next, historical stock data is ana-\nlyzed, followed by decision-making and execution.\nShort-term memory is formed through interactions\nwith the environment. Finally, the system evaluates\nthis memory, updates the strategy, and consolidates\nit into long-term memory. This entire process is\nillustrated in Figure 8."}, {"title": "E Simulation Process", "content": "The experiments involved several LLMs, in-\ncluding LLaMa-3 (Dubey et al., 2024), GPT-\n40 (Hurst et al., 2024), DeepSeek (Liu et al.,"}]}