{"title": "SP2T: Sparse Proxy Attention for Dual-stream Point Transformer", "authors": ["Jiaxu Wan", "Hong Zhang", "Ziqi He", "Qishu Wang", "Ding Yuan", "Yifan Yang"], "abstract": "In 3D understanding, point transformers have yielded significant advances in broadening the receptive field. However, further enhancement of the receptive field is hindered by the constraints of grouping attention. The proxy-based model, as a hot topic in image and language feature extraction, uses global or local proxies to expand the model's receptive field. But global proxy-based methods fail to precisely determine proxy positions and are not suited for tasks like segmentation and detection in the point cloud, and exist local proxy-based methods for image face difficulties in global-local balance, proxy sampling in various point clouds, and parallel cross-attention computation for sparse association. In this paper, we present SP2T, a local proxy-based dual stream point transformer, which promotes global receptive field while maintaining a balance between local and global information. To tackle robust 3D proxy sampling, we propose a spatial-wise proxy sampling with vertex-based point proxy associations, ensuring robust point-cloud sampling in many scales of point cloud. To resolve economical association computation, we introduce sparse proxy attention combined with table-based relative bias, which enables low-cost and precise interactions between proxy and point features. Comprehensive experiments across multiple datasets reveal that our model achieves SOTA performance in downstream tasks. The code has been released in this url.", "sections": [{"title": "1. Introduction", "content": "The point transformer [28, 43, 44, 50] has recently emerged as a key research area in 3D perception [42] and other field [48], mainly using group attention [24, 25] to extract features. Although various point-transformer methods [43, 44] have recently advanced in expanding receptive fields, they remain constrained by the immense scale of point-cloud data. For example, the receptive field of the Point Transformer has expanded from 16 points in PTv2 [43] to 1024 points in PTv3 [44]. However, the total number of point clouds can range from 10,000 to 100,000 [1, 10], as shown in Fig. 1. Due to the constraints of group attention, further enlarging the receptive field in point transformer could be counterproductive. Thus, the receptive field must be broadened through alternative approaches.\nFor the extraction of language and image features, the proxy-based methods [11, 14, 16\u201318, 41] have emerged as a focus area, expanding the receptive field of neural networks while demanding less computation. These methods are generally divided into global proxy-based methods [11, 17, 18, 41] and local proxy-based methods [14, 16, 22]. Global proxy-based methods utilize proxies to exchange global information, thereby expanding the model's receptive field. However, the methods do not accurately account for the exact positioning of the proxies and are less suited to tasks focused on local details such as segmentation and detection. Conversely, local proxy-based methods utilize the proxy sampling method to determine the proxy's position and apply the association to depict the relationship between data and proxy, which expand the receptive field while ensure feature localization. However, the sampling and associations method for images [14, 16] proves to be challenging to implement for point clouds that exhibit diverse scales and size proportions. Moreover, when dealing with sparse point clouds, effectively parallelizing the association process based on cross-attention between proxies and points poses challenges in terms of time complexity and memory usage.\nFurthermore, networks that employ proxy methods must strike a balance between global and local information. In tasks like segmentation or detection, both global and local information is of equal significance. Global features with contextual information, on the one hand, improve classification results. Conversely, the feature of the local point cloud are crucial for refining segmentation and detection bounding boxes. Drawing inspiration from MobileFormer [5], we propose that a dual-stream Transformer architecture aids in harmonizing local and global information extraction.\nTo address the aforementioned issues, a local proxy-based dual-stream transformer, named SP2T is introduced for point cloud understanding. Our approach involves a dual-stream Point Transformer to address the challenge of integrating local details and global information in point clouds. In SP2T, both the proxy and the point cloud are fed into the network simultaneously, allowing the balance extraction of global and local information.\nSecondly, concerning proxy sampling in point clouds, we introduce a spatial-wise method employing a binary selection approach to determine the optimal proxy distribution. Subsequently, we propose a vertex-based point-proxy association method that speeds up the establishment of point-proxy associations by examining coordinate relationships through spatial-based proxy sampling.\nThirdly, to address the difficulty of efficiently computing cross-attention between the sparsely connected proxy and point, we present the sparse proxy attention (SPA). Specifically, SPA has changed from using the matrix-based attention method to using association lists, and it uses Map-Reduce [8] for parallel speed-up. Furthermore, a Table-Based Relative Bias is introduced to improve spatial awareness of SPA.\nIn general, the contributions of this paper include the following parts: (1) We introduce SP2T, a local proxy-based dual-stream point transformer, which effectively facilitates global receptive field while preserving a balance between global and local. (2) For the proxy sampling and association method in the sparse point cloud, a spatial-wise proxy sampling method with a vertex-based association is introduced, which ensures efficient and reliable sampling of the proxies and calculation of the association in the point cloud. (3) To address the issue of efficiently transferring information between points and proxies, Sparse Proxy Attention is introduced along with table-based relative bias. (4) Extensive experiments conducted on the many datasets [1, 3, 10] demonstrate that our model achieves state-of-the-art (SOTA) results in many downstream tasks."}, {"title": "2. Related Works", "content": "3D Point Cloud Understanding. Currently, methods oriented towards 3D Point Cloud Understanding can be divided into three main categories: 2D projection-based [4, 21, 23, 36], voxel-based [6, 12, 15, 26, 34, 39, 47, 51], and point cloud-based methods [29\u201331, 43, 44, 49, 50]. Initially, the 2D projection method utilizes the project to derive features from the point cloud using a 2D-backbone, and the voxel-based approach converts the point cloud into a structured voxel grid, which speeds up convolution or attention computations. Moreover, the point-cloud-based method processes the point-cloud directly using pooling-based or transformer architectures. SP2T is classified as a point-cloud-based network, which employs proxies to significantly expand the receptive field of the point-cloud network, thereby enhancing the model's proficiency in understanding 3D point clouds.\nProxy-based Methods. Recently, proxy-based methods for feature extraction in both imagery and linguistics have gained considerable attention. These methods are categorized mainly into global proxy-based approaches [11, 17, 18, 41] and local proxy-based approaches [14, 16, 22]. Initially, global proxy-based techniques employ proxies to gather general information, enhancing the model's receptive field. However, global proxies lack explicit modeling of proxy location and are not friendly to local-relevant tasks such as segmentation and detection. Secondly, local proxy-based methods focus solely on aggregating features around the proxy and incorporating information exchange between agents, thereby achieving the global receptive field. For the local proxy-based method, the sampling method and association strategy of the proxy are very important, which determine the spatial location of the proxy and the extraction range of the image. However, the sampling and association of the local proxy for point cloud data still require further investigation. SP2T employs a local proxy-based framework and examines the sampling and association method for point clouds, utilizing sparse proxy attention to expedite the computation of proxy-point interactions."}, {"title": "3. Method", "content": "3.1. Overall Architecture\nSP2T. As illustrated in Fig. 2, SP2T is a dual-stream encoder-decoder network specifically designed for point-cloud data. Formally, the input points $P_{point} \\in \\mathbb{R}^{N \\times 3}$ are processed through the point embedding module and the proxy initialization module to derive the feature of the point $F_{point} E \\mathbb{R}^{N \\times C}$ and the proxy $F_{proxy} \\in \\mathbb{R}^{M \\times C}$ and the position of the proxy $P_{point} \\in \\mathbb{R}^{M \\times 3}$, where N and M is the number of points and proxy, and C is the embed channel.\nAfterward, the positions and features of both the points and proxies are fed into a multi-stage encoder-decoder network. Each SP2TLayer is made up of several SP2TBlock for the exaction of features and ends with a downsampling or upsampling module for the point cloud and proxy to facilitate the encoding and decoding processes. Finally, the features produced by the last layer of the decoder are utilized for downstream tasks. For detailed network parameters, we report them in the support material.\nSP2TBlock. The SP2TBlock is the foundational module in SP2T, enabling distinct local feature aggregation for points and global feature aggregation for proxies. SP2TBlock contains four distinct modules for feature extraction: local fusion, global fusion, point-proxy interaction, and proxy-point interaction module.\nSpecifically, the local fusion module is crafted to seamlessly integrate the local features of point-cloud data, while the global fusion module exceeds local receptive field limitations for a comprehensive global context understanding. Subsequently, the point-proxy and proxy-point interaction modules combine information from points and proxies using the sparse proxy attention (SPA). These modules primarily enhance the exchange of information between the point cloud and proxy, enriching point cloud features with detailed semantic context while maintaining the integrity of the proxies' local feature representation."}, {"title": "3.2. Proxy Sampling and Association", "content": "Problem of FPS-based Proxy Sampling. FPS-based proxy sampling utilizes the Furthest Point Sampling algorithm [30] to select a constant number of reference points from the point cloud, as widely applied in PointNet[29] and other methods. But in our method, the FPS-based approach results in a decrease in precision. Using FPS-based sampling generates a scene-relevant proxy. The model might overfit to such proxy distribution during training, resulting in a segmentation output derived directly from the scene-relevant proxy rather than from the point feature.\nProblem of Grid-based Proxy Sampling Regarding grid-based proxy sampling methods, there are two primary approaches to sampling: one maintains a constant number of proxies, while the other regulates a specific proxy spacing. However, each of these methods has certain challenges. Firstly, the sampling method that keeps the number of proxies constant uses a predetermined quantity $Xvoxel \u00d7 Yvoxel \u00d7 Zvoxel$ within a uniform range. However, as illustrated in Fig. 3 (a), since the aspect ratio of the generated proxies mirrors that of the axis-aligned bounding box (AABB) of the point cloud, the proxies will appear stretched if the AABB has an extreme aspect ratio. This uneven distribution along different directions can adversely affect the uniformity of associations and feature fusion.\nSecondly, the spacing-fixed sampling method separates the aspect ratio of the proxy's spacing from the point cloud by fixing the size of the proxy's spacing, enabling the uniformity of the proxies' distribution in space. However, as illustrated in Fig. 3 (b), since the AABB's range of the point cloud can fluctuate, the resultant number of proxies generated also varies, introducing significant uncertainty regarding the FLOPs of the model.\nSpatial-wise Proxy Sampling. To address the aforementioned issues, we propose a spatial-wise proxy sampling method. Specifically, we introduced a binary search method for searching the proxy spacing. It is noted that, under an equal proxy spacing along different axis, the number of generated proxies monotonically decreases as the proxy spacing increases. Therefore, given a desired range of proxy numbers, the binary search method can be used to find an appropriate proxy spacing that brings the number of generated proxies as close as possible to the required range.\nIn practice, for a fixed number of proxies, our sampling method efficiently discerns the ideal proxy spacing by considering the AABB sizes of various point clouds. The method is designed to maintain the count of proxies within the bounds of $N_{max}$ and $N_{min}$, using a bisection approach to determine the optimal proxy spacing $L_p$. If the number of proxies exceeds $N_{max}$, the proxy spacing $L_p$ is reduced and the total number of proxies is recalculated. In contrast, $L_p$ increases until the number of proxies reaches the desired range from $N_{max}$ to $N_{min}$. As demonstrated in Fig. 3 (c), this method autonomously selects the best proxy spacing based on the predetermined proxy count for different point clouds depending on their AABB size. Compared to the other sampling methods in Fig. 3 (a) and (b), the spatial-wise proxy sampling method guarantees an appropriate aspect ratio for the proxy spacing and specifies a distinct number of proxies.\nVertex-based Association. A commonly adopted method for association uses the K-Nearest Neighbor (KNN) [37] method, which connects each data point with its k nearest proxies. This approach provides considerable flexibility in defining the degree of correlation between the proxies and the point cloud, thereby permitting modifications in the number of associations.\nDespite this, the KNN method requires evaluating the distances between all point clouds and proxies, which adds computational complexity. In KNN using the L-Infinity Norm with k = 8, every point is linked to eight proxies positioned at the corners of a cubic proxy within the AABB coordinate sphere. As the proxy is formed through grid sampling, its structural layout can be exploited to create a more efficient association method, thereby reducing the need for intricate distance computations.\nAs illustrated in Fig. 4, the initial transformation involves converting the coordinates of the points to their corresponding locations within the proxy grid. Then, by computing the floor and ceiling for each coordinate, the positions of the eight nearest proxies are determined. Finally, the compilation of association tuples is achieved by aggregating all associations into one comprehensive list."}, {"title": "3.3. SPA for Point-Proxy Interaction", "content": "The point-proxy interaction is the fundamental concept driving SP2T. Unlike other works such as SAFDNet [46], SP2T facilitates the exchange of information between proxies and points through attention mechanisms and introduces sparse proxy attention to speed up the attention computation.\nSparse Proxy Attention (SPA). Unlike traditional attention [38], the attention between the point and the proxy is sparse and discrete due to the vertex-based connections, leading to substantial computational demands with the typical attention operator. To address this issue, we implemented sparse proxy attention to speed up information exchange between proxies and points. In practice, sparse proxy attention utilizes either the point cloud or proxy features as the query feature, while the other acts as the key and value features. Thus, it enables the aggregation of information from the point to the proxy and the feedback from the proxy back to the point.\nTake the SPA that transfers features from proxy to point as an example. As shown in Fig. 5, to calculate the similarity between the proxy and the point, we denote the indices of the corresponding original points and proxy points in the i-th association tuple as $a_{rt}, a_{rx}$. The sparse attention module first computes the exponential similarity for the query of each association group and the key features of each head.\n$S^h_i = exp(dot(q^h_{apt}, k^h_{apt}) / \\sqrt{d})$ (1)\nwhere $S^h_i$ is the similarity of the i-th association of the head h, $q^h_{apt}$ and $k^h_{apt}$ is the query h of the head and key of $a_{rt}, a_{rx}$. d is the dimension of the feature.\nBuilding on this, we then employ a map-reduce algorithm to obtain the sum of exponentiated similarity over each query. Finally, a division is performed to complete the sparse softmax and obtain the weights for each value.\n$W^h_j = \\frac{S^h_i}{\\sum_j S^h_i}$ (2)\nwhere $S^h_i, S^h_j$ is the similarity of the i-th association and the j-th association of the same proxy. $W^h_j$ is the weight of the value after the sparse softmax.\nFinally, we adopt a similar map-reduce-based method to weight the corresponding value features by the weights and association indices to obtain the output features.\n$o^h_{apt} = \\sum_{j,a=i}W^h_j v^h_{apt}$ (3)\nwhere $v^h_{apt}$ is the association value $a_{rt}$ of the h th head and $o^h_j$ is the attention output of the h th head proxy i.\nTable-based Relative Bias. Inspire by the Swin Transformer [24], we found that integrating relative bias (RB) into the attention mechanism improves the network's proficiency in capturing the connections between the proxy and the point cloud. However, the relative bias in Swin Transformer is a learning parameter synchronized with the attention map, while in SPA, the relative bias must adapt to match the dimensions of the point cloud and proxy.\nTo address the aforementioned issues, we introduce Table-Based Relative Bias (TRB). As shown in Fig. 6, the TRB utilizes a lookup table $T_{rpe}$ characterized by dimensions $X_{rpe}, Y_{rpe}, Z_{rpe}$. Initially, the input relative positions are converted into normalized coordinates for the lookup table by applying a scaling factor $S_{rpe}$. The biases are then derived at these coordinates via trilinear interpolation. Relative distances that are beyond the permissible range are clamped to valid numerical values. In terms of form, the formula for TRB is as follows.\n$TRB(x) = TGS(T_{rpe}, clamp(s_{rpe}x, -1, 1))$ (4)\nwhere TGS is the function of the TrilinearGridSample and x is the distance between the proxy and the point. clamp is the function of the clamp number.\nFinally, the similarity formula with TRB is shown as follows.\n$S^h_i = exp((dot(q^h_{apt}, k^h_{apt}) / \\sqrt{d}) + TRB(\\frac{p_{pt}}{l_{pt}} - \\frac{p_{ps}}{l_{ps}}))$ (5)\nwhere $S^h_i$ is the similarity of the i-th association of the head h, $q^h_{apt}$ and $k^h_{apt}$ is the query of the head h and key of $a_{rt}, a_{rx}$ d is the dimension of the feature. $p_{pt}, p_{ps}$ is the position of the point and proxy.\nFurthermore, the feature dimension of the lookup table $T_{rpe}$ is configured to match the number of heads in the sparse attention mechanism. This setup allows parallel execution of interpolation sampling and integration into similarity for each head. Furthermore, during training, the initialization of $T_{rpe}$ used a Gaussian distribution with scaled variance, which encourages the network to initially focus on the features that are proximate.\nDuring the implementation phase, the point and proxy positions remain unchanged within the same layer. This means that for each instance of sparse attention in this layer, the relative position input provided to the relative position encoding module remains constant. Consequently, it is possible to compute the relative bias for a layer with a single invocation. Building on this optimization, we share all relative bias values across each layer, thus reducing model complexity and computational demand. Additionally, since the proxy positions are constant throughout the network, we also test sharing the relative bias amongst proxies across the entire network and across different layers.\nStructure of SPA. Fig. 7 (a) illustrates the SPA structure with Point-to-Proxy. Similarly to MHSA [38], SPA incorporates four additional linear layers designed to scale down and increase the features."}, {"title": "3.4. Local and Global Fusion", "content": "Local Fusion The local fusion in SP2T is followed by PTv3 [44]. In particular, SP2T employs point-cloud serialization along with self-attention within PTV3 to facilitate the local fusion of point-cloud data.\nGlobal Fusion Fig. 7 (b) illustrates the global fusion using a self-attention module with table-based relative bias. The proxy interaction module facilitates the exchange of information across all proxies, allowing each to have direct access to the global context."}, {"title": "4. Experiments", "content": "4.1. Result of Downstream Tasks\nIn this section, we will compare our model with other SOTA models in various downstream tasks. For detailed model and training settings, we report them in supplementary material.\nIndoor Semantic Segmentation. In Tab. 1, we present the validation and test results with other methods [7, 31, 40, 43, 44, 50] on the ScanNet v2 [10] and ScanNet200 [32] benchmarks, as well as Area 5 and the 6-fold cross-validation [29] conducted on S3DIS [1].\nOur model demonstrated a 1.3% / 1.3% improvement in the ScanNet Val / Test set and showed an 1.8% / 1.2% increase in performance in the ScanNet200 Val / Test set compared to PTv3 [44]. In S3DIS, our performance remained consistent with PTv3 in S3DIS area5, and has a 0.2% increase in performance in S3DIS 6-fold.\nAs noted in the PTv3 issues on GitHub, compared to the configuration in the val-set, the PTv3 test results could have used extra test time augmentation (TTA), such as incorporating data from the validation set for training, combining results from multiple training models [20], and applying over-segmentation methods [27, 33, 45]. These augmentations may not be open source in the code of PTv3. To ensure a fair comparison, our model compares the PTv3 test results with the TTA of the val set. Furthermore, we provide more details of the test result in the supplementary material.\nIndoor instance Segmentation. In Tab. 3, we provide validation outcomes for our model on the instance segmentation benchmarks of ScanNet v2 [10] and ScanNet200 [32]. Performance metrics displayed include mAP, mAP25, and mAP50, and are compared to various state-of-the-art (SOTA) backbones. Following PTv3 [44], we use the instance segmentation framework by utilizing PointGroup [19] in all evaluations, altering only the backbone. Compared to PTv3, we achieved enhancements of 1.1% in mAP25, 0.6% in mAP50, and 0.2% in mAP on ScanNet. Furthermore, in ScanNet200, there were gains of 1.1% in mAP25, 0.9% in mAP50, and 0.2% in mAP."}, {"title": "4.2. Ablation Study", "content": "Followed by PTv3, the model's ablation studies were conducted on the ScanNet validation set, where four metrics, including mIOU, mAcc, allAcc and latency. The latency is reported using a single 4090 GPU with 1 batch size.\nThe number of proxy and association. Tab. 4 presents the ablation study exploring the impact of the number of proxies and associations in SP2T. Experimental results indicate that increasing the number of proxies and associations does not always result in better performance. On the one hand, an excess of proxies leads to a shortage of corresponding point clouds, thereby restricting the model's receptive field. On the other hand, an abundance of associations generates numerous point clouds linked to the proxies, making it difficult for the proxy to accurately represent the point cloud. As a result, the ideal number of proxies and associations is essential to produce the best results.\nProxy Sampling Method and Table-based Relative Bias. Tab. 5 provides the results of the ablation study on proxy sampling methods and Table-based Relative Bias. The experiment indicates that the spatial-wise sampling method yields the best model accuracy among various sampling methods. Additionally, TRB is adaptable to all sampling methods and can generally enhance model performance. Particularly with respect to the sampling method, the Fix Number and Fix Size-based methods struggle with point clouds at different scales, resulting in reduced accuracy and failure training. In contrast, the spatial-wise sampling approach can flexibly determine the proxy spacing thereby improving performance. Regarding the FPS-based method, the analysis is provided in the Discussion regarding the low performance of the FPS-based sampling method.\nShare of Table Relative Bias. Tab. 6 shows a comparison of the accuracy of the model w/ and w/o the shared TRB. The experiment shows that the shared TRB both improves the accuracy of the model and reduces the inference time. Furthermore, Fig. 8 shows the MSE distance for TRB during point-proxy and proxy-proxy interaction in different layers. In total, TRB demonstrates stage-level similarity in the proxy-proxy interaction, while all TRB is similar in the point-proxy interaction. Consequently, the sharing of TRB improves model accuracy and reduces inference time.\nEmpty Proxies. Different from image, grid sampling in point cloud unavoidably results in empty proxies, which lack any point cloud association. Tab. 7 presents a comparison of SP2T accuracy both with and without considering empty proxies during global fusion. The experimental data indicate that empty proxies actually enhance the accuracy of the model. We believe that the experiment proves that 'nothing' is also a very important piece of information in the point cloud. Although empty proxies lack point-cloud data, the absence itself conveys spatial structure information, thereby contributing to an understanding of overall spatial feature for point cloud.\nModel efficiency. Tab. 8 illustrates the model's efficiency, evaluated in terms of accuracy and latency, for both indoor (ScanNet) and outdoor (nuScenes) datasets using a single RTX 4090. We compared our model with PTv3, PTv2, and MinkUnet. Although our model runs litter slower than PTv3, our model achieves the best balance of accuracy and latency in both indoor and outdoor datasets."}, {"title": "4.3. Discussion and Visualization", "content": "In this section, we discuss the limitations of the FPS sampling method and illustrate the attention map of SPA. More discussion and visualization are provided in support materials.\nOver-fitting due to FPS-based Sampling. Within the ablation study, the experiments show that FPS-based sampling performs poorly compared to alternative sampling methods. To investigate this further, we visualized the point-proxy attention maps for both FPS-based and Spatial-wise sampling methods, as illustrated in Fig. 9. The visualization illustrates that the attention map resulting from FPS-based sampling exhibits static and repetitive patterns, with its attention not influenced by the proxy's location. On the other hand, the attention map from spatial-wise sampling aligns with typical attention maps and takes the proxy location into account. Consequently, we contend that the sampling method based on FPS leads to significant overfitting of the model because of the scene leakage from FPS.\nVisualization of Latency. Fig. 10 (a) presents the latency tree map for our model. It reveals that SPA and global fusion contribute 17% to overall latency. The latency of local and global fusion include the latency of FFN. Additionally, Fig. 10 (b) & (c) display the latency tree map of each operator in SPA and global fusion. The findings indicate that SPA with global fusion significantly improves performance without imposing substantial delay.\nVisualization of SPA. Fig. 11 provides a depiction of the attention map in SPA, focusing on the interaction between the point and the proxy. Specifically, Fig. 11 (a) highlights the attention map from point to proxy, and Fig. 11 (b) showcases the return path from proxy to point. Examining the visualization reveals that the SPA's semantic mining capability is evident. In this visualization, as the chosen proxy approaches the bookshelf, both the point cloud and the proxy's focus show a stronger affinity for the bookshelf compared to the table, showcasing the SPA's semantic representation ability.\nBy processing the attention maps of point-to-proxy, proxy-to-proxy, and proxy-to-point, we can derive the corresponding point-to-point attention maps in SPA. Fig. 12 illustrates the point-to-point SPA attention map at different stages(with different downsampling rates). The visualization results indicate that SPA is capable of demonstrating semantic aggregation at different stages. This includes the focused attention of a point on one chair toward other chairs in Fig. 12 (a), the focus of a point on the wall towards the entire wall in Fig. 12 (b), and the attention of a point on the table towards other tables in Fig. 12 (c)."}, {"title": "5. Conclusion", "content": "In this paper, we introduce SP2T, a local proxy-based dual-stream point transformer that uses sparse proxy attention to achieve global receptive field. To address the proxy sampling challenges in point clouds, we propose a spatial-wise proxy sampling method with vertex-based association, which ensures efficient and reliable proxy sampling and association calculations. Moreover, we also address the point-proxy cross-attention problem by incorporating sparse proxy attention with table-based relative bias, which enhances precise interactions between proxy and point features. Comprehensive experiments across multiple datasets show that our model achieves state-of-the-art performance in various downstream tasks. For limitations, we claim that the implementation of SPA and network parameters can be further refined. Compared to PTV3, our method uses more time and memory, and we will continue this research to address these challenges."}]}