{"title": "Evaluating the quality of published medical research with ChatGPT", "authors": ["Mike Thelwall", "Xiaorui Jiang", "Peter A. Bath"], "abstract": "Evaluating the quality of published research is time-consuming but important for departmental evaluations, appointments, and promotions. Previous research has shown that ChatGPT can score articles for research quality, with the results correlating positively with an indicator of quality in all fields except Clinical Medicine. This article investigates this anomaly with the largest dataset yet and a more detailed analysis. The results showed that ChatGPT 40-mini scores for articles submitted to the UK's Research Excellence Framework (REF) 2021 Unit of Assessment (UoA) 1 Clinical Medicine correlated positively (r=0.134, n=9872) with departmental mean REF scores, against a theoretical maximum correlation of r=0.226 (due to the departmental averaging involved). At the departmental level, mean ChatGPT scores correlated more strongly with departmental mean REF scores (r=0.395, n=31). For the 100 journals with the most articles in UoA 1, their mean ChatGPT score correlated strongly with their REF score (r=0.495) but negatively with their citation rate (r=-0.148). Journal and departmental anomalies in these results point to ChatGPT being ineffective at assessing the quality of research in prestigious medical journals or research directly affecting human health, or both. Nevertheless, the results give evidence of ChatGPT's ability to assess research quality overall for Clinical Medicine, so now there is evidence of its ability in all academic fields.", "sections": [{"title": "Introduction", "content": "Research quality evaluation is important for departmental evaluations and academic career decisions. Unfortunately, the evaluators may not have time to fully read the work assessed and may instead rely on the reputation or Journal Impact Factor of the publishing journals, on the citation counts for individual articles, or on the reputation or career citations of the author. Whilst journal-based evidence is not optimal (Waltman & Traag, 2021), the main article-level indicator, citation counts, only directly reflects the scholarly impact of work and not its rigour, originality, and societal impacts (Aksnes, et al., 2019), all of which are relevant quality dimensions (Langfeldt et al., 2020). Moreover, article citation counts are ineffective for newer articles (Wang, 2013). In response, attempts to use Large Language Models (LLMS) to evaluate the quality of academic work have shown that ChatGPT quality scores are at least as effective as citation counts in most fields and substantially better in a few (Thelwall & Yaghi,\n2024). Medicine is an exception, however, with ChatGPT research quality scores having a small negative correlation with the mean scores of the submitting department in the Research Excellence Framework (REF) Clinical Medicine Unit of Assessment (UoA) (Thelwall, 2024ab; Thelwall & Yaghi, 2024). It is therefore important to find the reason for this anomaly and, if possible, create an effective LLM-based research quality assessment method for clinical medicine.\nAlthough ChatGPT is now widely used to support academic research (Eppler et al., 2024; Owens, 2023), and it has been systematically evaluated for natural language processing tasks like question answering, sentiment analysis, and text summarisation (Koco\u0144 et al., 2023) it has sometimes been applied to evaluative academic text processing tasks where a score must be given to a document based on complex criteria. Examples of this include pre- publication peer review recommendations (Zhou et al., 2024; Saad et al., 2024) and post- publication expert review quality scoring (Thelwall & Yaghi, 2024) as well as scoring impact case studies for reach and impact (Kousha & Thelwall, 2024). These show that ChatGPT works well with system instructions like those given to human experts, presumably because it is partly trained this way (Ouyang et al., 2022). It also seems to have a degree of effectiveness at translating authorial claims in abstracts into reasonable scores, perhaps also considering some wider context. For optimal results, the default parameters for the ChatGPT API work well and varying them may not help (Thelwall, 2024b). Nevertheless, scores can be improved by submitting the same query multiple times and averaging the results (Thelwall, 2024a). This seems to be a way of leveraging ChatGPT's internal probability distribution, which reveals the level of confidence of ChatGPT's model in its score.\nFor wider context, there seems to have been only one previous attempt to systematically provide post publication research quality scores for medical research, in the form of the Faculty Opinions system (formerly, Faculty of 1000, F1000, and F1000Prime) that gives expert post-publication scores (Chen et al., 2024) and tags (e.g., \u201cControversial\u201d, \u201cGood for Teaching\") to biomedical articles (Wang et al., 2020). An investigation into four medical journals found that higher rated articles tended to be more cited in three of the journals (Wang et al., 2020). The scores are available only to subscribers and probably only for a small fraction of biomedical research, however.\nGiven the promise shown by ChatGPT for a variety of evaluation tasks, as discussed above, and the positive results previously found for all fields except UoA 1 Clinical Medicine (Thelwall & Yaghi, 2024), it is important to check the Clinical Medicine results and to investigate the reason for the negative correlation. A methodological limitation of the previous study is that it sampled articles only from high and low scoring departments and if any of these had an unusual publishing strategy then this could influence the findings. The current paper therefore seeks a more comprehensive evaluation of UoA 1 as well as follow- up investigations to identify reasons for weak or negative correlations. In addition to article- level comparisons (RQ1) (as previously reported: Thelwall & Yaghi, 2024), departmental-level comparisons (RQ2) are useful because this is the level at which REF quality data is available and used in practice. Journal-level comparisons (RQ3) may give a different perspective and,\""}, {"title": "Methods", "content": "The research design was to obtain ChatGPT scores for as many as possible of the articles submitted to REF2021 in UoA 1 and then compare these scores with departmental REF mean for journal articles individually, by department, and by journal. The individual REF scores for journal articles are not known because only the number of articles generating each quality score is reported publicly, and the departmental REF mean is the best available quality score proxy."}, {"title": "Data", "content": "The REF outputs are available in a spreadsheet that can be downloaded from the REF website (results2021.ref.ac.uk). This set was filtered to exclude everything except the journal articles in UoA 1. These were then matched with corresponding records in Scopus by DOI, when available. Scopus records were needed to access the abstracts and citation counts of these articles, which were not in the REF data. This gave a set of 9,905 journal articles from UoA 1 matched with Scopus records. After excluding articles without abstracts, 9,872 remained for the main analysis. ChatGPT needs abstracts to score articles with the methods used here.\nDepartmental REF scores for articles: Each university submission to UoA 1 is called a \"department\u201d here although it may not map to an organisational unit or units with this name. Unfortunately, the REF scores for individual articles were never made public and were systematically deleted before the aggregate results were published. Instead, public information about departmental scores can be used to calculate departmental mean REF scores and these departmental scores can be used as approximate estimates of the REF quality of each of a department's articles. This proxy is not ideal but seems to be the only method to get any kind of quality estimate score for the articles. Unless there is a substantial departmental bias within ChatGPT, a positive correlation between ChatGPT scores and REF scores at the article level should translate into a weaker correlation between ChatGPT scores and departmental average REF scores at the article level.\nFor the departmental average REF score calculation, the departmental results could be downloaded from the official website (results2021.ref.ac.uk). For each department, the percentage of outputs scoring each one of the four quality levels (1*, 2*, 3* or 4*) is recorded and these percentages could be used to calculate the departmental mean REF scores. This is imperfect, however, because some journal articles are not in Scopus, some departmental outputs are not journal articles and some journal articles were deemed out of scope and not given a REF score. Thus, instead we used the departmental average from the valid journal article in Scopus submitted by each department. This information is not public but had been calculated for a previous project before the individual article level scores were deleted. Thus, the departmental REF averages used here were calculated from all valid Scopus-indexed articles from UoA 1 (unfortunately including the few without abstracts), and this is used as the REF score estimate for the articles.\nChatGPT Scores: Each of the 9,872 articles were submitted to ChatGPT 40-mini to obtain a score. Only the title and the abstract were submitted (with the prompt, \u201cScore this:\ntitle\\nAbstract\\nabstract\u201d) and the REF Panel A guidelines (i.e., the guidelines relevant to clinical medicine) were used as the system instructions for ChatGPT (as in a previous article:\nThelwall & Yaghi, 2024). ChatGPT's output is a report almost always containing a recommended score, and these scores were extracted by a series of text processing rules in Webometric Analyst (Al menu, Extract scores from ChatGPT reports option:\ngithub.com/MikeThelwall/Webometric_Analyst), with human input when the rules failed. Each article was submitted five times to ChatGPT, and the mean value used as the article's ChatGPT score. As previously shown, ChatGPT's scores can vary for the same prompt and this averaging process improves the accuracy of the score. About five times is enough before the additional iterations add a relatively small amount to the accuracy (Thelwall & Yaghi, 2024). Inputting legally accessed texts into artificial intelligence systems for academic research is legal in the UK and does not require copyright holder permission (Bristows, 2023). The ChatGPT API does not learn from the data ingested (OpenAI, 2024) and so there is no possibility of secondary copyright infringement.\nDepartmental and journal ChatGPT score: This is the mean of all ChatGPT scores for all articles associated with the department/journal from the 9,872 analysed.\nAlthough the Scopus data includes citation counts, as needed for RQ3, they need processing to be more useful because the articles are from different years (2014 to 2020) and fields (mostly related to Medicine, but not all) and citation rates vary naturally between fields and years. Sets of citation counts are also highly skewed, so a log transformation is needed before any averaging is applied. Thus, each citation count was transformed with Log(1+x) to reduce skewing, then divided by the mean of the Log(1+x) values for all articles published in Scopus in the same Scopus narrow field and year (including articles not submitted to the REF). The result is the Normalised Log-transformed Citation Score (NLCS), which is fair to compare between articles published in different fields and years (Thelwall, 2017).\nCitation rates for journals: The citation rate for each journal was calculated as the mean of the NLCS for all the articles associated with that department from the 9,872 analysed.\nThis is known as the Mean NLCS (MNLCS). This definition excludes all non-REF articles by design (these are only used for the normalisation procedure). Thus, even though the journal MNLCS is a bit like the Journal Impact Factor (JIF), it is based on a different set of articles."}, {"title": "Analysis", "content": "The primary statistical test was the Pearson correlation coefficient. This is more informative than a direct measure of accuracy (e.g., mean absolute deviation) because ChatGPT\u2019s score estimates can easily be scaled with a transformation. Pearson correlations were chosen in preference to Spearman because the data are not highly skewed, and Pearson is finer grained. Although some of the data analysed derives from ranks, the numbers correlated are not ranks and are often not integers due to averaging.\nFor RQ4, a Word Association Thematic Analysis (Thelwall, 2021) was used to identify themes related to articles with high ChatGPT scores by analysing the words found disproportionately often in the articles scoring at least the median ChatGPT score or more (ChatGPT average >= 3.6 n=5353) compared to articles with a below-median ChatGPT score (ChatGPT average <3.6, n= 4637). This method is conservative and statistically-oriented, designed to give evidence-based themes in sets of texts. For this, article titles and abstracts were fed into the text analytics software Mozdeh (github.com/MikeThelwall/Mozdeh) together with their ChatGPT scores and it was used to construct a list of terms occurring (statistically significantly) disproportionately often in the higher ChatGPT score set (for details see: Thelwall, 2021). The top 25 words were examined to find their typical context in the articles. Finally, the term contexts were clustered reflexively into themes by the first author. This was repeated for the lower scoring articles."}, {"title": "Results", "content": "The Pearson correlation between an article's ChatGPT score (i.e., the mean of 5 scores per article) and the REF score of its submitting department is 0.134 (95% confidence interval: 0.114, 0.153, n=9872). This is weak but positive and statistically significantly greater than 0, giving a positive answer to the first research question. The theoretical maximum correlation for this variable is the Pearson correlation between the individual article REF scores and the departmental mean values which can be calculated from the score distributions without knowing the individual article scores, and is 0.226 (at the R\u00b2 level, the 0.134 correlation is 35% of the maximum). Thus, the underlying correlation between the ChatGPT article is likely to be substantially higher and may be moderate rather than weak.\nThe ChatGPT scores tend to be above the departmental REF mean values, with the ChatGPT overall mean being 3.55 and the mean REF score (per article rather than per department) being 3.27. Thus, ChatGPT might be effectively rounding up scores between 3*\nand 4* to 4* that REF assessors rounded down to 3*.\nIn an attempt to bring the ChatGPT average closer to the REF average score, a number of different system prompting strategies were developed to encourage ChatGPT to be stricter, by explicitly telling it to be \u201cstrict\u201d, \u201cvery strict\u201d, or \u201cdraconian\u201d. These were tested on a tiny development set of 20 articles from The Lancet and eLife from 2024. The only strategy found that seemed to be effective at reducing the average scores was to change the start of the system instructions from \u201cYou are an academic expert, assessing academic journal articles\" to \"You are a draconian academic expert, harshly assessing academic journal articles\", and allowing it to use half points by adding, \u201cUse half points if a study is between two scores.", "added,": "esearch that directly affects human health is particularly valuable.\u201d The experiment was repeated with this revised prompt and the average ChatGPT score reduced slightly from 3.55 to 3.47 but this seemed to make the ChatGPT scores less informative because the correlation with departmental REF mean values reduced from 0.134 to 0.117. The original dataset alone was therefore used for the remainder of this paper."}, {"title": "RQ2: Department-level analysis", "content": "If the mean ChatGPT score of all of a department's articles is correlated against its mean REF score for journal articles then the correlation is 0.395 (95% CI: 0.047, 0.657, n=31), which is higher than at the article level, as expected. Nevertheless, this correlation is only modest, and a scatter plot reveals the existence of two substantial outliers (Figure 1).\n\u2022\nWarwick University (78 articles) has the second lowest REF score and the second highest ChatGPT score.\n\u2022\nLeicester University (81 articles) has the highest REF score and the 6th lowest mean\nChatGPT score.\nThe anomaly can be investigated by examining the journals mainly submitted by these two institutions. Whilst the most popular journals for Leicester where arguably the two most prestigious medical journals, the New England Journal of Medicine (NEJM, 13 articles) and The Lancet (10 articles), the single most popular journal for Warwick was eLife, a general biomedical and life sciences journal (11 articles). Thus, since the two departments seem to have different publishing patterns, the anomaly might be due to ChatGPT unduly favouring\nWarwick's topics or the journals that Warwick publishes in, relative to Leicester. The next section focuses on journals."}, {"title": "RQ3: Journal-level analysis", "content": "Altogether, 767 different journals had articles in the UoA 1 sample. The top journals in terms of explaining the results are those with the most articles, however. Thus, correlations were calculated overall and just for the top journals, using different cut-offs since there is not a natural choice. Including smaller journals adds noise to the data and tends to reduce the correlation, whereas only considering the top journals would be unrepresentative.\nFor the 50 largest journals for UoA 1 there was a moderately strong (r=0.517, n=50) Pearson correlation between the journal REF score (the mean REF score of the departments of the articles in the journal) and the journal ChatGPT score (the mean ChatGPT score of articles in the journal) (Table 1). Although there is also a positive correlation between journal\nREF score and journal mean citation rates (r=0.245, n=50), there is a negative correlation (r=-\n0.245, n=50) between journal ChatGPT scores and journal mean citation rates (MNLCS). Thus, and unexpectedly, ChatGPT tends to give lower scores to articles in more cited journals in\nUoA 1."}, {"title": "RQ4: Types of articles with higher ChatGPT scores", "content": "The Word Association Thematic Analysis to find article types attracting above median or below median ChatGPT scores revealed several clear patterns. Themes found in the higher\nscoring articles (Table 2) included genetics (e.g., words like: genetic, genome-wide), style (e.g.,\nhere, we, show, that), exploratory/theoretical (e.g., mechanism, complex, drive, pathway). In contrast, themes for the lower scoring articles (Table 3) included use of the past tense (e.g., were, was, had), structured abstract terms (e.g., methods, conclusion), patient/participants (e.g., patient, participant outcome, age), statistics (e.g., p mean, ci).\nOverall, this suggests that theoretical studies scored higher, perhaps by revealing more substantial results, whereas studies directly informing human health decisions scored\nlower. This is perhaps demonstrated most clearly by the terms \u201cpatient\u201d and \u201cparticipant"}, {"title": "Discussion", "content": "The results are limited by the nature of the sample used, which is from a single country and consists of the self-selected best work of the publishing academics. It may also exclude work from weaker departments if they chose to submit to a related UoA instead, or if individuals were submitted to a different UoA related to medicine (e.g., UoA2, Public Health, Health Services and Primary Care). The strength of the correlations reported is likely to differ between LLMs and between versions of an LLM. Finally, the positive correlations reported may well increase in strength for newer LLMs, although it is not clear if the negative correlations will become positive or stronger. ChatGPT was used to assess the quality of articles based on their titles and abstracts, since previous research had found this input to give the best results (Thelwall, 2024b), whereas REF panel members are expected to read and assess the whole paper.\nComparison with prior work\nThe finding of a statistically significant (the 95% confidence interval excludes 0) positive article-level correlation between ChatGPT scores and departmental mean REF scores contrasts with the negative correlation previously found (Thelwall & Yaghi, 2024). From Figure\n1, the reason for the negative correlation is that two of the departments chosen for the previous small-scale assessment (Leicester and Warwick) were anomalies against a general positive pattern. The results also contrast with a non-significant correlation found for ChatGPT 40 from a previous study of unpublished submissions to a medical journal (Saad et al., 2024), but this may have been due to a small sample size and the use of a single ChatGPT estimate per article.\nJournal-based anomalies\nThe results point to journal-based anomalies, so comparing extreme journals may reveal possible causes. Contrasting abstracts from NEJM and The Lancet with eLife, the main difference is that the former two journals are fact-based and therefore dry. The NEJM has structured abstracts, with the final section, Conclusions, used to summarise the results in non- statistical language without attempting to generalise or explicitly discuss their importance. For example, \u201cTreatment with rosuvastatin at a dose of 10 mg per day resulted in a significantly lower risk of cardiovascular events than placebo in an intermediate-risk, ethnically diverse population without cardiovascular disease.\u201d In contrast, the Lancet's structured abstracts end with an Interpretation section that has the same role but seems to have more scope for tentative generalisation or wider context setting (e.g., \u201cFunctional status 90 days after intracerebral haemorrhage did not differ significantly between patients who received tranexamic acid and those who received placebo, despite a reduction in early deaths and serious adverse events. Larger randomised trials are needed to confirm or refute a clinically significant treatment effect.\u201d: Sprigg et al., 2018). In contrast, eLife sometimes includes explicit claims for novelty and significance in its abstracts as well as general speculation about the importance of the results (e.g., \"This raises the general concept that\nproteins involved in cytoskeletal functions and appearing organism-specific, may have highly divergent and cryptic orthologs in other species\u201d: Dean et al., 2019). Given this difference it seems possible that ChatGPT is influenced by the explicit strength of claims in abstracts, and REF reviewers may be influenced by points made in the main part of the paper, even if the difference is less important in other UoAs. Medical articles seem to be short (3000-4000 words) and should be understandable to medical practitioners, so REF experts might be more willing to read the full text, and authors might be more willing to omit relevant context from abstracts. Four examples in the Appendix illustrate this, with assertions that seem likely to influence ChatGPT in bold.\nAn alternative possibility is that ChatGPT does not consider the importance of human health when evaluating medical research and so does not adequately assess the impact of the type of research typically found in The Lancet and NEJM. This could be a side-effect of medical\narticles not ever needing to spell out the importance of improving human health, so the\nsignificance of research is implicit rather than explicit. For example, the conclusion, \u201cTriple antiplatelet therapy should not be used in routine clinical practice", "The MTD of sorafenib when used with 30 Gy in 10 fractions was not established due to sorafenib-related systemic toxicity. Severe radiotherapy-related toxicities were also observed. These events suggest this concurrent combination does not warrant further study": "Murray et al., 2017). This not only did not report positive results but also acknowledged unwanted side effects. Other examples of negative results from low scoring articles include, \u201cOrally delivered PXD showed no evidence of clinical activity, when combined with weekly AUC2-carboplatin in PROC", "LY2495655 did not improve overall survival: the hazard ratio was 1.70 (90% confidence interval, 1.1-2.7) for 300 mg vs. placebo and 1.3 (0.82-2.1) for 100 mg vs. placebo (recommended doses)\" (Golan et al., 2018).\nWhilst it seems reasonable for ChatGPT to give lower scores to articles with negative findings since they are unlikely to have a \u201ctransformative effect\" they may be important research contributions since other clinicians will not need to repeat the same study. For the same reason, studies with serious side effects may be particularly important to report. In addition, negative results about existing treatments can also be transformative if it leads to their discontinuation. It is not clear whether REF assessors took these wider issues into account, but it seems possible that negative clinical results are more likely to be published\nthan negative theoretical results, and this would give an overall negative bias towards clinical studies in ChatGPT. Overall, this suggests that different (human expert) standards might apply\nto medical research involving patients or other participants, but that ChatGPT has not recognised this.\"\n    },\n    {\n      \"title\": \"Conclusion\",\n      \"content\": \"The results show for the first time that ChatGPT score estimates correlate positively with an indicator of article quality for clinical medicine journal articles, in this case the indicator being indirect in the form of the departmental mean REF scores. The practical value of this is that\nChatGPT score estimates could be used to support human judgements in contexts that citation data currently is, including for clinical medicine articles that are too new to have sufficiently mature citation data. For the best estimates, at least five iterations of ChatGPT\nshould be used for each article and a transformation should be applied to compensate for\nChatGPT's positive bias.\nThe main caveat to the above conclusion is that ChatGPT seems to systematically undervalue research in some prestigious medical journals, whether because of the journal's style or by overlooking the contribution of medical studies about human health. This should\nbe considered when using ChatGPT estimates, such as by estimating the quality of such articles in other ways or by not using ChatGPT-based indicators for them.\nFinally, the results, combined with those of a previous study (Thelwall & Yaghi, 2024),\ngive evidence that ChatGPT quality scores correlate positively with REF quality scores in all 34 UoAs, making it a universal quality indicator, albeit with its strength varying between fields.\"\n    },\n    {\n      \"title\": \"Appendix: Examples of ChatGPT scores for three medical journals\",\n      \"content\": \"A NEJM article that received a low score from ChatGPT was, \u201cEffects of serelaxin in patients with acute heart failure\", with an abstract that concluded, \u201cIn this trial involving patients who were hospitalized for acute heart failure, an infusion of serelaxin did not result in a lower incidence of death from cardiovascular causes at 180 days or worsening heart failure at 5 days than placebo.\" One of the ChatGPT reports giving this article a score of 2* concluded, \u201cThe article provides a well-executed trial, adding to the corpus of literature on acute heart failure treatment modalities. However, it fails to present novel insights that would elevate its status\nto an internationally excellent or world-leading level. The findings reaffirm existing knowledge rather than challenge or expand it significantly. Suggestions for future research could include exploration of different dosages, combination treatments with serelaxin, or investigations into specific patient subgroups that may respond differently to treatment. Additional focus\non long-term cardiovascular outcomes or quality of life measurements post-treatment could further enhance the study's impact and significance within the context of heart failure management.\"\nAn article in The Lancet with a low score from ChatGPT was, \u201cAntiplatelet therapy with aspirin, clopidogrel, and dipyridamole versus clopidogrel alone or aspirin and dipyridamole in patients with acute cerebral ischaemia (TARDIS): a randomised, open-label, phase 3\nsuperiority trial": "hich concluded, \u201cInterpretation: Among patients with recent cerebral\nischaemia, intensive antiplatelet therapy did not reduce the incidence and severity of recurrent stroke or TIA, but did significantly increase the risk of major bleeding. Triple\nantiplatelet therapy should not be used in routine clinical practice.\u201d A ChatGPT report\nrecommending a 3* score stated, \u201cIn conclusion, the article presents compelling evidence against the routine use of triple antiplatelet therapy in patients after acute cerebral\nischaemia, and while its originality, significance, and rigour are recognized as excellent, it\nstops short of achieving the highest standards of excellence. For improvement, the authors\ncould further explore long-term outcomes and potential variations in patient subgroups,\nwhich could enrich understanding and yield further insights into patient management\nstrategies.", "Curvature-induced expulsion of actomyosin bundles during cytokinetic ring contraction": "with an abstract that concluded, \"Strikingly, mechanical compression of actomyosin rings results in expulsion of bundles predominantly at regions of high curvature. Our work unprecedentedly reveals that\nthe increased curvature of the ring itself promotes its disassembly. It is likely that such a curvature-induced mechanism may operate in disassembly of other contractile networks.", "Overall, this article stands out as a world-leading contribution to the field, exemplifying high standards of originality,\nsignificance, and academic rigour. Its findings call for further exploration into the applications of PVCs [Photorhabdus Virulence Cassettes] not only in pest control but potentially in therapeutic areas involving human health and disease pathology as well. Future research\ncould build on this work to investigate potential implications and applications in greater depth, reinforcing the article's foundational contributions.\" For context, \u201cstrikingly": "as not\nappeared in the Lancet since 2013.\nAnother ChatGPT 4* article from eLife was, \u201cA conserved major facilitator superfamily member orchestrates a subset of O-glycosylation to aid macrophage tissue invasion", "We characterize for the first time the T and Tn glycoform O- glycoproteome of the Drosophila melanogaster embryo, and determine that Minerva increases the presence of T-antigen on proteins in pathways previously linked to cancer, most strongly on the sulfhydryl oxidase Qsox1 which we show is required for macrophage tissue entry. Minerva's vertebrate ortholog, MFSD1, rescues the minerva mutant's migration and T- antigen glycosylation defects. We thus identify a key conserved regulator that orchestrates O-glycosylation on a protein subset to activate a program governing migration steps important for both development and cancer metastasis.\"\"\n    }": ""}]}