{"title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer", "authors": ["Hao Chen", "Ze Wang", "Xiang Li", "Ximeng Sun", "Fangyi Chen", "Jiang Liu", "Jindong Wang", "Bhiksha Raj", "Zicheng Liu", "Emad Barsoum"], "abstract": "Efficient image tokenization with high compression ratios remains a critical challenge for training generative models. We present SoftVQ-VAE, a continuous image tokenizer that leverages soft categorical posteriors to aggregate multiple codewords into each latent token, substantially increasing the representation capacity of the latent space. When applied to Transformer-based architectures, our approach compresses 256x256 and 512\u00d7512 images using as few as 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and high-quality reconstruction, more importantly, it also achieves state-of-the-art and significantly faster image generation results across different denoising-based generative models. Remarkably, SoftVQ-VAE improves inference throughput by up to 18x for generating 256\u00d7256 images and 55x for 512\u00d7512 images while achieving competitive FID scores of 1.78 and 2.21 for SiT-XL. It also improves the training efficiency of the generative models by reducing the number of training iterations by 2.3x while maintaining comparable performance. With its fully-differentiable design and semantic-rich latent space, our experiment demonstrates that SoftVQ-VQE achieves efficient tokenization without compromising generation quality, paving the way for more efficient generative models. Code and model are released.", "sections": [{"title": "1. Introduction", "content": "Denoising-based generative modeling has witnessed remarkable progress with recent advances, such as Diffusion Transformers (DiT) [80], Scalable Interpolant Transformers (SiT) [72], and Masked Auto-Regressive models with diffusion loss (MAR) [60], to name a few. Denosing-based generative modeling not only presents impressive generation results on a wide range of modality, including natural language [32, 122], images [16, 25, 69, 83, 88], videos [6, 8, 42], and audios [24, 81], but also shows potential to unify the understanding and generation capabilities across modalities in multi-modal language models [107, 109, 110, 123].\nA core component of denoising-based generative models is the tokenizer [23, 38, 51, 56, 84, 104, 112, 117], which compresses the raw data of each modality into a set of latent tokens in either a discrete or continuous latent space. The compact latent space therefore allows for more efficient and better generative modeling [88]. Among previous efforts, Kullback-Leibler Variational Auto-Encoders (KL-VAE) [51] and Vector Quantized Variational Auto-Encoders (VQ-VAE) [23, 84, 104] stand out as representatives of tokenizers which introduce continuous and discrete latent spaces, respectively. The former constrains the latent space with a Gaussian distribution using re-parametrization [51] trick, and the latter makes its latent space a categorical discrete distribution with a codebook of finite vocabulary which requires straight-through estimation [7].\nAlthough both KL-VAE and VQ-VAE (and their variants) have been predominantly adopted in many generative models [10, 80, 88, 98, 123], they still present two major challenges restricting the efficiency and effectiveness of generative modeling: (1) the difficulty of achieving a higher compression ratio [13] and (2) the worse discriminative representations in their latent space than other self-supervised methods [14, 35, 109, 116]. The efficiency of downstream generative models, particularly Transformer-based architectures [105], is fundamentally constrained by their quadratic complexity to the number of latent tokens. Current image tokenizers [23, 88, 96] typically compress 256\u00d7256 images to at least 256 tokens and 512\u00d7512 images to at least 1024 tokens, creating a significant computational bottleneck for both training and inference of generative models [60, 72, 80]. Many efforts have been made to reduce the number of latent tokens on the generative model side, such as merging [62, 72, 80], pooling [58, 61, 94], and others [12, 63, 70]. More studies have recently emerged to fundamentally reduce the token number of the tokenizer [11, 30, 62, 115]. For example, TiTok [115] uses 128 tokens, achieving generation results comparable to 256 tokens by adding one extra decoder. DC-AE [11] compresses the initial 1024 tokens of 512\u00d7512 with 256 tokens. However, further increasing the compression ratio results in a significant degradation in the quality of reconstruction and therefore the generation [11]."}, {"title": "2. Preliminary", "content": "We present an overview of the tokenizers, i.e., KL-VAE [51] and VQ-VAE [104], and denoising-based generative models, i.e., DiT [80], SiT [72], and MAR [60], in this section."}, {"title": "2.1. Image Tokenizer", "content": "The architecture of image tokenizer generally resembles auto-encoders, consisting of two main components: an encoder $\\mathcal{E}$ and a decoder $\\mathcal{D}$, parameterized by $\\phi$ and $\\theta$, respectively. Given an input image $\\mathbf{x} \\in \\mathbb{R}^{H\\times W\\times 3}$, the encoder $\\mathcal{E}$ maps the high-dimensional $\\mathbf{x}$ to a lower-dimensional latent representation $\\mathbf{z} = \\mathcal{E}(\\mathbf{x}; \\phi)$. The latent representations can have varying shapes depending on the encoder architecture. We generally view the latent representation as a set of latent tokens $\\mathbf{z} = [\\mathbf{z}[0], \\mathbf{z}[1],...,\\mathbf{z}[L]] \\in \\mathbb{R}^{L\\times D}$, where $L$ is the number of tokens and $D$ is the dimension of the latent representation. For basic auto-encoders (AE), the decoder takes latent tokens as input and reconstructs the original signal $\\mathbf{x} = \\mathcal{D}(\\mathbf{z}; \\theta)$ by minimizing the reconstruction loss with $\\mathbf{x}$.\nVariational auto-encoders (VAE) [51] extend the basic AE by introducing a probabilistic perspective on the latent space. Specifically, VAE approximates a posterior distribution $p(\\mathbf{z}|\\mathbf{x})$ with a learned distribution $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ from the encoder, assuming that $\\mathbf{x}$ is generated by the unobserved latent $\\mathbf{z}$ with a prior distribution $p(\\mathbf{z})$. The decoder instead takes a set of sampled latent tokens $\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ from the posterior and aims to learn the marginal likelihood of data in a generative process by optimizing the evidence lower bound (ELBO): $\\max_{\\phi,\\theta} \\mathbb{E}_{q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}) - D_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))$. To make the optimization tractable, assumptions have been made on the prior, leading to different variants of VAE.\nKL-VAE [38, 51] parametrizes both the prior and posterior as Gaussians. The prior $p(\\mathbf{z})$ is assumed to be the isotropic unit Gaussian $\\mathcal{N}(0, \\mathbf{I})$. The continuous latent code is parameterized with posterior mean $\\mu_{\\phi}$ and variance $\\sigma_{\\phi}^2$ predicted by the encoder using the \u201cre-parametrization\u201d trick with a noise variable $\\epsilon$ from standard Gaussian:\nposterior: $q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z}; \\mu_{\\phi}(\\mathbf{x}), \\sigma_{\\phi}^2(\\mathbf{x}))$\nlatent: $\\mathbf{z} = \\mu_{\\phi}(\\mathbf{x}) + \\sigma_{\\phi}(\\mathbf{x}) \\epsilon, \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$\nkl: $L_{kl} = -(\\frac{1}{2} + \\frac{1}{2} \\log (\\sigma_{\\phi}^2(\\mathbf{x})) - \\frac{\\mu_{\\phi}^2(\\mathbf{x})}{2} - \\frac{\\sigma_{\\phi}^2(\\mathbf{x})}{2})$\n(1)\nWhile KL-VAE is widely used in diffusion models, it is known to have the \u201cposterior-collapse\u201d issue [13]. In addition, the KL loss weight usually imposes a trade-off between the quality of reconstruction and the smoothness of its latent space [100], preventing high compression ratio and the learning of semantics in the latent space of KL-VAE.\nVQ-VAE [23, 104], in contrast, generates latent code in a discrete space by taking the posterior as a $K$-way deterministic categorical distribution against a learnable codebook $\\mathcal{C} = [\\mathbf{c}[0],..., \\mathbf{c}[K]] \\in \\mathbb{R}^{K\\times D}$, with $\\mathbf{c}$ as the codeword:\n\\begin{cases}\n1 & \\text{if } k = \\arg \\min_j ||\\mathbf{z}_j - \\mathbf{c}[j] ||_2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\nlatent: $\\mathbf{z} = \\mathbf{c}[k]$, where $k = \\arg \\min_j ||\\mathbf{z}_j - \\mathbf{c}[j] ||_2\nkl: $L_{kl} = \\log K$\n(2)\nDue to the non-differentiable arg min, a \"straight-through\" trick [7] is adopted to approximate the gradient of the encoder output $\\mathbf{z}$ by directly copying gradient from the decoder input $\\mathbf{z}$. Since the reconstruction objective does not impose direct gradients on the codebook $\\mathcal{C}$, VQ-VAE additionally uses a codebook loss as $||sg[\\mathbf{z}] - \\mathbf{c}||^2_2$ to move codewords toward the encoder output, along with a commit loss $| | \\mathbf{z} - sg[\\mathbf{c}]||^2_2$ to prevent arbitrary growth of codewords, where $sg[\u00b7]$ denotes stop-gradient. The broken gradient hinders high compression ratio and latent space learning [27, 43]."}, {"title": "2.2. Denoising-based Generative Models", "content": "Denoising-based generative models synthesize images by progressively transforming Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ into tokenizer latent codes $\\mathbf{z}$ through a forward process [41, 92]:\n$\\mathbf{z}_t = \\alpha_t \\mathbf{z} + \\sigma_t \\epsilon,$\n(3)\nwhere $\\alpha_t$ and $\\sigma_t$ are a decreasing and increasing function of $t$, and $t \\in [0, T]$ [50]. In this paper, we adopt three approaches to implementing this denoising process as follows.\nDiT [80] is a diffusion-based model with a transformer architecture. It formulates the process through a forward-time stochastic differential equation (SDE), where $\\mathbf{z}_t$ converges to $\\mathcal{N}(0, \\mathbf{I})$ as $t\\rightarrow T$. Generation occurs via a reverse-time SDE [2], with the model trained to predict noise $\\epsilon$ at randomly sampled timesteps. Due to the long token length of the tokenizer [96] used in DiT, it has several variants to merge tokens at the input of the transformer backbone.\nSiT [72] instead uses stochastic interpolants [1] and performs denoising via probability flow ordinary differential equation (PF ODE). The model learns the ODE's velocity field [65] by minimizing mean-squared-error (MSE) with the interpolated noise. This velocity field defines a score function for sample generation, analogous to DiT's reverse-time SDE.\nMAR [60] combines diffusion with autoregressive generation. Unlike DiT and SiT's parallel denoising of all latent tokens, MAR adopts an encoder-decoder transformer architecture [35] that progressively denoises tokens in a \"next-set\" autoregressive manner [99], starting from masked tokens, similar to MaskGIT [10] and MAGE [59]."}, {"title": "3. Method", "content": "In this section, we present SoftVQ-VAE, a novel 1D continuous tokenizer with a high compression ratio and a semantic-rich latent space. We first introduce the architecture and the formulation of SoftVQ-VAE. Then, we show that SoftVQ-VAE can learn semantics easily by aligning its latent tokens with pre-trained features via its fully-differentiable property."}, {"title": "3.1. Architecture", "content": "We leverage Vision Transformer (ViT) [21, 111] as the architecture for the encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ of SoftVQ-VAE."}, {"title": "3.2. SoftVQ-VAE", "content": "As discussed above, the high compression ratio of both KL-VAE and VQ-VAE usually results in a significant degradation of quality in reconstruction and latent space. To overcome these limitations, we propose SoftVQ-VAE, a simple modification to VQ-VAE, which maintains the advantages of learning codewords to capture the data distribution while bridging it with more representation capacity as a continuous tokenizer. The core idea of SoftVQ-VAE is to allow each latent code to adaptively aggregate multiple codewords from the learnable codebook. Consequently, it imposes a soft categorical distribution on the posterior from the encoder:\nposterior: $q_{\\phi}(z|x) = Softmax(-\\frac{||z-C||^2}{T})$\nlatent: $z = \\mathbb{E}[q_{\\phi}(z|x)]C$\nkl: $L_{kl} = H(q_{\\phi}(z|x)) - H (\\mathbb{E}_{x~p(x)}q_{\\phi}(z|x))$\n(4)\nwhere $T$ is the temperature parameter controlling the sharpness of the softmax probability and is set to 0.07 by ablation. The derivation of $L_{kl}$ is shown in Appendix B.1.\nDespite this simple modification, SoftVQ-VAE can significantly reduce the latent token length while maintaining a high quality of the reconstruction and latent space. More importantly, with a soft categorical distribution, SoftVQ-VAE is fully-differentiable, and thus the encoder and codebook can be optimized directly from the reconstruction loss (and other losses). It also allows for easier regularization of various forms on the latent space that drastically improves its quality, and less hyper-parameter tuning without codebook and commit loss [43] in the reduced training objectives.\nInterpretation. The posterior and latent space of VQ-VAE can be interpreted with K-Means [67]. Hence, SoftVQ-VAE can be viewed as soft K-Means [22], with codewords as learnable prototypes in the latent space. This latent space can easily be extended to Gaussian Mixture Models (GMM) [86], which we refer to as GMMVQ-VAE. Instead of using fixed temperature, GMMVQ-VAE predicts data-dependent weights for the codeword prototypes, i.e., Gaussian means, to compute the soft categorical posterior: $q(z|x) = Softmax(-w(z) ||z-C||^2)$. Although prior work has explored GMM-based VAEs [17, 47], our GMMVQ-VAE differs by learning discrete prototypes rather than continuous latent variables. While providing the additional benefit of interpretable Gaussian components in the latent space, GMMVQ achieves reconstruction quality and downstream generation performance similar to SoftVQ in our experiments, and thus we mainly adopt the simpler SoftVQ throughout this paper. More details are in Appendix B.2."}, {"title": "3.3. Representation Alignment of Latent Space", "content": "While the reconstruction quality of tokenizer is important, learning a high-quality latent space is more crucial for downstream denoising-based generative modeling [116]. Several recent approaches have explored aligning the latent code with pre-trained features through a contrastive loss [30, 62, 109] and initializing the codebook with pre-trained features [124]. However, imposing effective regularization"}, {"title": "3.4. Final Training Objective", "content": "The training objective of SoftVQ-VAE combines the reconstruction loss, perceptual loss [20, 46, 54, 120] and adversarial loss [33, 44] as in VQ [23], and representation alignment:\n$\\mathcal{L} = \\mathcal{L}_{recon} + \\lambda_{1} \\mathcal{L}_{percep} + \\lambda_{2} \\mathcal{L}_{adv} + \\lambda_{3} \\mathcal{L}_{align} + \\lambda_{4} \\mathcal{L}_{kL},$\n(7)\nwhere $\\lambda_{1}$, $\\lambda_{2}$, $\\lambda_{3}$, and $\\lambda_{4}$ are hyper-parameters. Neither codebook nor commit loss as in VQ is needed. The perceptual loss $\\mathcal{L}_{percep}$ helps capture high-level perceptual features, $\\mathcal{L}_{adv}$ encourages the decoder to generate realistic images by removing the artifacts from the reconstruction loss only, $\\mathcal{L}_{align}$ guides the latent space to align with pre-trained features, and $\\mathcal{L}_{kL}$ as the KL divergence term in the ELBO."}, {"title": "4. Experiments", "content": "In this section, we validate the efficiency and effectiveness of SoftVQ-VAE with extensive experiments."}, {"title": "4.1. Experiments Setup", "content": "Implementation Details of SoftVQ-VAE. We use the LlamaGen codebase [98] to build our SoftVQ-VAE. We instantiate 4 configurations: SoftVQ-S, SoftVQ-B, SoftVQ-BL, SoftVQ-L, with a total of 45M, 173M, 391M, and 608M parameters, respectively. Each configuration has variants of latent codes $L = 64$ and $L = 32$. For the representation alignment, we choose DINOv2 [79] for the pre-trained features, similarly as Yu et al. [116] and Li et al. [62]. To learn a better semantic latent space, we initialize our encoder from the pre-trained DINOv2 weights. Alignment with other pre-trained features is explored in Sec. 4.4. We train the tokenizers on ImageNet [15] of resolution 256\u00d7256 and 512x512 for 250K iterations. Similarly to Tian et al. [99] and Li et al. [62], we found that the discriminator is very important for training tokenizer, and we adopted the same frozen DINO-S [9, 79] discriminator with a similar architecture to StyleGAN [48, 49]. In addition, we use DiffAug [121], consistency regularization [119], and LeCAM regularization [101] for discriminator training as in [99]. For the training objective, we set $\\lambda_{1} = 1.0$, $\\lambda_{1} = 0.2$, $\\lambda_{3} = 0.1$, and $\\lambda_{4} = 0.01$, following previous common practice. More training details are shown in Appendix C.1.\nImplementation Details of Generative Modeling. We select DiT [80], SiT [57], and MAR [60] for downstream denoising-based image generation tasks. For DiT and SiT, we set the patch size of DiT and SiT to 1 and use a 1D absolution position embedding. In our main experiments, we train DiT-XL and SiT-XL of 675M parameters for 3M steps, compared to 4M steps in REPA [116] and 7M steps vanilla version [72, 80]. We strictly follow their original training setup for other settings. For MAR-H, we train for 500 epochs and set the maximum learning rate to 2e-4 since a higher learning rate leads to NAN issues. For other experiments, we simply train SiT-L of 458M parameters for 400K steps. More experimental details are provided in Appendix C.2.\nEvaluation. We use reconstruction Frechet Inception Distance (rFID) [37] on ImageNet validation set to evaluate the tokenizer. To evaluate the performance of generation tasks, we report generation FID (gFID), Inception Score (IS) [90], Precision and Recall [52] (in Appendix D.4), with and without classifier-free guidance (CFG) [40]. We measure the efficiency of the generative models by GLOPs of the model's forward pass on the latent codes of tokenizers, and training and inference throughput for the models using floating point 32 and a batch size of 64 on a single AMD MI250."}, {"title": "4.2. Main Results", "content": "We present the main reconstruction and generation results on the ImageNet benchmark of resolution 256\u00d7256 and 512x512 in Tab. 1 and Tab. 2, respectively. We show that SoftVQ-VAE achieves reconstruction and generation performance (with generative models) comparable to leading"}, {"title": "4.3. Comparison of Tokenizers", "content": "We compare the proposed SoftVQ-VAE with the concurrent efficient image tokenizers, i.e., TiTok [115] and DC-AE [11]. To show the superiority of SoftVQ-VAE in achieving a high compression ratio, we additionally train VQ-VAE and AE using the small (S) configuration and the same training recipe of SoftVQ. To validate the generation performance, we train a SiT-L for 400K steps and report gFID and IS on 256 ImageNet without using CFG, as shown in Tab. 3.\nScalable performance. With a much smaller model size, i.e., 46M vs 390M, SoftVQ-S significantly outperforms both TiTok variants at 64 tokens, achieving better reconstruction quality with an rFID of 1.03 compared to 1.25, and better generation performance with a gFID of 11.24 and IS of 89.4 compared to the best TiTok gFID of 19.23 and IS of 61.8.\nNoteworthy is that the generation results SiT-L trained on SoftVQ with 32 tokens outperform those trained on KL tokenizer with 1024 tokens [72] by a large margin, i.e., 5.9 in gFID. Compared to DC-AE, SoftVQ-S demonstrates competitive scalability across token counts while maintaining significant generation quality even with fewer parameters.\nLess Lossy Property. SoftVQ-S exhibits remarkably consistent performance across different compression ratios. When reducing tokens from 256 to 32, SoftVQ-S maintains a minimal degradation in rFID from 0.80 to 1.24, significantly outperforming both VQ-S, from 1.45 to 10.97, and AE-S, from 1.15 to 2.01. The benefits of the fully-differentiable property of SoftVQ along with representation alignment are particularly evident in generation quality, where SoftVQ-S achieves substantially better gFID of 9.21-12.89, and IS scores of 79.5-93.6, compared to baselines, showing its robustness in preserving information at high compression ratios and a better latent space for generative models to be trained on."}, {"title": "4.4. Discussions on the Latent Space", "content": "We perform more analysis on the latent of SoftVQ here.\nAlignment with different initialization and target models. As shown in Tab. 4, our results demonstrate the effectiveness of alignment across various pre-trained models. The encoder initialization and alignment with DINOv2-B [79] achieve superior performance with rFID 0.88 and IS 103.4, compared to using either component alone. Further improvements are observed with CLIP-B [82] and EVA-02-B [26] on reconstruction. We reveal that a better rFID does not necessarily translate to a better gFID. Instead, the latent space quality, reflected by linear probing accuracy, is more closely related to the performance of the generative models."}, {"title": "4.5. Ablation Studies", "content": "We present a series of ablation studies in Appendix D.1, including SoftVQ variants with product quantization (PQ) [45], residual quantization (RQ) [55], and GMMVQ, codebook size, latent space size, and the temperature of SoftVQ. The results show the compatibility of SoftVQ with PQ and RQ, with slightly improved performance. GMMVQ presents results comparable to SoftVQ. Moreover, SoftVQ shows robustness to other parameters, such as codebook size and softmax temperature. We found that while increasing the dimension of the latent space can result in a significant improvement in reconstruction, it leads to deterioration in generation performance, possibly due to the learning difficulty of generative models with larger dimensions [92]."}, {"title": "5. Related Work", "content": "Image tokenization has emerged as a fundamental technique to bridge various vision tasks. Traditional approaches using autoencoders [39, 106] laid the groundwork by compressing images into latent representations. This field subsequently diverged into two main branches: generation-focused and understanding-focused approaches. Generation-oriented methods, such as VAE [84, 104] and VQ-GAN [23, 85], emphasized learning latent spaces for detail-preserving compression. These were further refined through variants [55, 73, 112, 124] that enhanced generation quality. Parallel developments in understanding-focused approaches leveraged Large Language Models (LLMs) [105] to create semantic representations for tasks like classification [21] and detection [125]. Recent work [62, 115] has demonstrated"}, {"title": "6. Conclusion", "content": "We presented SoftVQ-VAE, a continuous image tokenizer leveraging soft categorical posteriors to aggregate multiple codewords into each latent token. Our approach compresses images to just 32 or 64 tokens while maintaining high reconstruction quality and enabling state-of-the-art generation re-"}, {"title": "A. Posterior of KL-VAE and VQ-VAE", "content": "In this section, we provide the detailed derivation of the KL-divergence of KL-VAE and VQ-VAE used in Eq. (1) and Eq. (2), respectively.\nKL-VAE has the KL divergence of the posterior with a Gaussian prior:\n$\\mathcal{L}_{kl}(q_{\\phi}(z)||p(z)).$\n$= \\int q_{\\phi}(z) (\\log q_{\\phi}(z) - \\log p(z)) dz$\n$= \\frac{1}{2} \\sum_{j=1}^{D}(1 + \\log ((\\sigma_{\\phi}(j))^{2}) - (\\mu_{\\phi}(j))^{2} - (\\sigma_{\\phi}(j))^{2}),$\n(8)\nwhere $D$ is the latent space dimension.\nVQ-VAE assumes a uniform prior over the total $K$ codewords in the learnable codebook for the deterministic posterior, thus present a KL divergence as follows:\n$\\mathcal{L}_{kl}(q_{\\phi}(z)||p(z)).$\n$= \\int q_{\\phi}(z) (\\log q_{\\phi}(z) - \\log p(z)) dz$\n$= -(- \\log K)$\n$= \\log K$\n(9)"}, {"title": "B. Additional Details of SoftVQ-VAE and its Variants", "content": "In this section, we present more details on the posterior of SoftVQ-VAE, its variant GMMVQ-VAE with the latent space as a GMM model, and the compatibility of SoftVQ-VAE with improvement techniques of VQVAE."}, {"title": "B.1. Posterior of SoftVQ-VAE", "content": "In SoftVQ, we similarly assume that the prior is a uniform distribution over the $K$ learnable codewords as in VQ, except for we have the posterior as the softmax probability:\n$\\mathcal{L}_{kl}(q_{\\phi}(z)||p(z)).$\n$= \\int q_{\\phi}(z) (\\log q_{\\phi}(z) - \\log p(z)) dz$\n$= H(q_{\\phi}(z)) - H(q_{\\phi}(z), p(z)),$\n(10)\nwhere $H(q_{\\phi}(z)$ is the entropy for $H (q_{\\phi}(z), p(z))$ is the cross-entropy between $q_{\\phi}(z)$ and the uniform prior $p(z) \\sim U(0, K)$. In practice, $\\mathbb{H}(\\mathbb{E}_{x~p(x)} [q_{\\phi}(z)], p(z))$ we compute the $\\mathbb{E}_{x~p(x)} [q_{\\phi}(z)]$ instead, where $\\mathbb{E}_{x~p(x)} [q_{\\phi}(z)]$ is computed on the averaged batch data during training."}, {"title": "B.2. GMMVQ-VAE", "content": "As discussed in the main paper, the latent space of SoftVQ can be interpreted as soft K-Means, and it can be further extended to a latent space of Gaussian Mixture Model. We present more details of this extension here.\nIn GMMVQ-VAE, the encoder predicts two things: the embedding $\\mathbf{z}$ and the weights of the Gaussian component $w(\\mathbf{z})$. We can then formulate the posterior as:\nposterior: $q(z|x) = Softmax (-w(z) ||z - C||^2)$\nlatent: $z = \\mathbb{E}[q_{\\phi}(z|x)]C$\nkl: $L_{kl} = H(q_{\\phi}(z|x)) - H (\\mathbb{E}xp(x)q_{\\phi}(z|x)).$\n(11)\nThe difference between SoftVQ and GMMVQ in our formulation is the way in computing the posterior, i.e., using fixed temperature parameter versus learning the data-dependent temperature parameters by the encoder. Note that we still maintain the codebook and its codewords entries directly for the decoder input. It is possible to make the latent space formally a GMM, by treating the codewords as Gaussian means, and formulating the decoder input with the re-parametrization trick. However, we find that this formulation with re-parametrization will hinder the learning of the latent space. Thus, we adopted the simpler design to use the codebook directly for reconstruction, assuming fixed variance in the Gaussian components of GMM."}, {"title": "B.3. Compatibility of SoftVQ-VAE", "content": "Since SoftVQ-VAE maintains the learnable codebook, previous techniques to improve VQ-VAE are also compatible with it. Here, we follow ImageFolder [62] to show the combination of SoftVQ with product quantization [45] and residual quantization [55], as illustrated in Fig. 5.\nProduct Quantization (PQ) [45] divides the latent codes into $G$ groups, with each group having its own codebook:\n$\\mathbf{z} = [\\mathbf{z}^{(1)}, \\mathbf{z}^{(2)}, ..., \\mathbf{z}^{(G)}]$\n$q(\\mathbf{z}^{(g)}|x) = Softmax(-||\\mathbf{z}^{(g)} - C^{(g)} ||^2/T)$.\n(12)\nPQ can effectively increase the actual codebook size.\nResidual Quantization (RQ) [55] applies multiple layers of quantization to the residual errors of the encoder output:\n$\\mathbf{z}_l = \\mathbf{z}_{l-1} + SoftVQ(\\mathbf{r}_{l-1})$\n$\\mathbf{r}_l = \\mathbf{r}_{l-1} - \\mathbf{z}_l$\n$\\mathbf{z}_0 = 0$\n$\\mathbf{r}_0 = \\mathbf{z}$\n(13)\nwhere $\\mathbf{r}$ is the residual and $l$ is the layer index. RQ captures more fine-grained features and thus better reconstruction."}, {"title": "C. Additional Implementation Details", "content": "In this section, we provide more implementation details of the tokenizer training, the downstream generative models training, and the linear probing evaluation."}, {"title": "C.1. Implementation Details of SoftVQ-VAE", "content": "All tokenizer experiments (except the open-source ones) in this paper are trained using the same training recipe. We train the tokenizers on ImageNet [15", "108": ".", "68": "optimizer is used with $\\beta_1$ = 0.9, $\\beta_2$ = 0.95, a weight decay of 1e-4, a maximum learning rate of 1e-4, and cosine annealing scheduler with a linear warmup of 5K steps. We use a constant batch size of 256 is used for all models. For the training objective, we set $\\lambda_1$ = 1.0, $\\lambda_1$ = 0.2, $\\lambda_3$ = 0.1, and $\\lambda_4$ = 0.01, following previous common practice. We additionally linearly warmup the loss weight of perceptual loss, i.e., $\\lambda_1$ = 1.0, for 10K iterations, which we find beneficial to train with fewer latent tokens.\nSimilarly to Tian et al. [99", "62": "we found that the discriminator is very important for training the tokenizer. Instead of using the common PatchGAN and Style-GAN architecture, we adopted the frozen DINO-S [9, 79", "49": "as in [99", "121": "consistency regularization [119"}]}