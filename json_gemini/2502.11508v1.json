{"title": "Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities", "authors": ["Changchun Liu", "Kai Zhang", "Junzhe Jiang", "Zixiao Kong", "Qi Liu", "Enhong Chen"], "abstract": "Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.", "sections": [{"title": "1 Introduction", "content": "Chinese Spelling Correction (CSC) is a critical research area in Natural Language Processing (NLP), dedicated to identifying and correcting spelling errors in Chinese text. The recent explosion of user-generated content across social networks, e-commerce platforms, and online education has led to a corresponding surge in spelling errors, which can significantly degrade the performance of downstream NLP applications. Addressing these errors is crucial for enhancing the accuracy of information retrieval and recommendation systems in domains like social media and online shopping, as well as facilitating automated assessment and personalized learning in education. Therefore, advancements in CSC technology are of paramount importance.\nThe primary sources of errors in CSC typically stem from users' input method editors (IMEs) [Hu et al., 2024], as well as inaccuracies introduced by automatic speech recognition (ASR) and optical character recognition (OCR) systems. Based on the characteristics of these error sources, the IME and ASR originated errors are mainly related to the similar pronunciation (or pinyin) of Chinese characters, while OCR originated errors result primarily from similarities in the shapes of characters. In addition, statistical analyses reveal that 83% of errors are due to similar pronunciations, while 48% are attributed to similar shapes [Liu et al., 2010].\nEarly research in CSC relied heavily on rule-based and statistical methods. These methods employed predefined rules, large-scale corpora, and language models such as N-grams to rectify errors through statistical analysis. Subsequently, non-autoregressive pre-trained language models like BERT have emerged as the dominant paradigm in CSC, leading to a proliferation of related studies. Some research efforts [Li et al., 2022b; Liang et al., 2023] have focused on effectively extracting and utilizing the phonetic and visual features of Chinese characters through various methodologies, while striving to accurately interpret semantics within erroneous contexts. In parallel, the traditional detector-corrector framework [Huang et al., 2023; Wu et al., 2024] has been continually optimized and refined to get better performance.\nRecent advancements in large language models (LLMs), such as GPT-40 and DeepSeek, have garnered significant attention due to their remarkable semantic understanding and reasoning capabilities. However, only a limited number of studies have explored the integration of LLMs in traditional models [Liu et al., 2024] or fine-tuned LLMs [Li et al., 2024] for the CSC. This is mainly because LLMs, as autoregressive models, face fatal challenges in CSC tasks. Issues such as variable output length and tendencies toward overcorrection significantly impact their effectiveness. Consequently, the application of LLMs to the CSC tasks remains a largely untapped area, offering considerable potential for future research and development.\nDespite the extensive research conducted on CSC, a systematic review summarizing the findings, providing a comprehensive overview of the field, and identifying future research directions is currently lacking. Therefore, this survey aims to provide readers a thorough understanding of the fundamental concepts of the CSC task, current research trends, existing challenges, and promising future directions.\nTo facilitate understanding of CSC techniques for readers from diverse backgrounds, this survey is organized as follows: in Section 2, we introduce the formal definition of the CSC task and its key characteristics. Section 3 and 4 provide an in-depth discussion of the research methods applied to CSC across various stages of development, with a focus on model structure design and the extraction of Chi-"}, {"title": "2 Definition of the CSC", "content": "Chinese Spelling Correction (CSC) can be formally defined as follows: Given a Chinese sentence X = {x1,x2,..., Xn} of n characters that may include erroneous characters. We use Y = {Y1, Y2, ..., Yn } to represent the corresponding correct sentence. The sentence X and Y have the same length. The objective of the CSC is to detect and correct the erroneous characters by generating a prediction \u0176 = {Y1, Y2, ..., Yn} for the input X, where \u0177r is the character predicted for Xi. The primary mission of the CSC lies in accurately detecting the erroneous characters and predicting their correct counterparts in Y."}, {"title": "2.1 Formal Definition", "content": "Chinese Spelling Correction (CSC) can be formally defined as follows: Given a Chinese sentence X = {x1,x2,..., Xn} of n characters that may include erroneous characters. We use Y = {Y1, Y2, ..., Yn } to represent the corresponding correct sentence. The sentence X and Y have the same length. The objective of the CSC is to detect and correct the erroneous characters by generating a prediction \u0176 = {Y1, Y2, ..., Yn} for the input X, where \u0177r is the character predicted for Xi. The primary mission of the CSC lies in accurately detecting the erroneous characters and predicting their correct counterparts in Y."}, {"title": "2.2 Characteristics of the CSC Tasks", "content": "In this section, we provide a detailed description of the key features of the CSC. By dividing \"Chinese Spelling Correction\" into \"Chinese\u201d, \u201cSpelling\u201d, and \u201cCorrection\", we can clearly understand its essential characteristics.\nThe \"Chinese\" represents the language used in the CSC task is Chinese. Chinese is an ideographic language, where the shape of the characters is closely tied to their meaning. In contrast, English is a phonetic language, where letters represent distinct phonemes that are combined into words to convey meaning through pronunciation. Moreover, the grammatical structure of them is quite different. For instance, unlike English, Chinese does not employ spaces to divide words. Instead, words must be segmented according to context. As a result, despite numerous studies on correcting errors in English, these cannot be directly utilized for CSC, necessitating a model tailored specifically for Chinese.\nThe \"Spelling\u201d means that the CSC focuses on characters themselves rather than the overall text. The goal is to correct the misuse of individual Chinese character, rather than adjusting the entire sentence structure. It is worth noting that this does not imply a disregard for the overall meaning, but emphasizes the modification focusing on character errors. \"Spelling\" also pointed out that the error in the CSC study was an alignment error. That is, the sentence length remains unchanged before and after modification, with only \"replacement\" being used as a modification method, rather than \"addition\u201d or \u201cdeletion\u201d. From this perspective, CSC can be viewed as a sequence labeling task.\nThe \"Correction\" points directly to the primary target of the CSC: identifying and correcting errors in the sentence."}, {"title": "3 Methods Based on PLMS", "content": "During earlier periods, CSC research was relatively fragmented and limited. With the advancement of deep learning, the field of CSC has entered a revolutionary phase marked by rapid progress, with the most important achievements so far being based on PLMs. During this phase, substantial research efforts have focused on leveraging PLMs to extract semantic information from sentences as well as the phonetic and visual features of Chinese characters, optimizing model architectures and enhancing pretraining or finetuning methods.\nAlthough numerous studies have been conducted, most model structures can be categorized into two main types, models based on Information-Learning architecture (I-L) and models based on Detector-Corrector architecture (D-C), as illustrated in Figure 1. These two categories are not entirely independent, as some studies exhibit overlaps. In addition, for I-L architecture, various methods are also being developed to learn Chinese character information. Table 1 summarizes recent model architectures and their approaches to learning Chinese character information.\nThis section is divided into two parts for a detailed discussion. The first part examines how models learn character information and integrate them with sentence semantics, which constitutes the core of the I-L architecture. The second part introduces the development process of D-C architecture."}, {"title": "3.1 Learning and Utilization of Character Information", "content": "Since most sources of errors involve similar Chinese characters, it is essential to learn phonetic and visual information from characters and evaluate their similarity. Therefore, this part will introduce in detail how to learn and use the phonetic and visual information of Chinese characters.\nSince most sources of errors involve similar Chinese characters, it is essential to learn phonetic and visual information from characters and evaluate their similarity. Therefore, this part will introduce in detail how to learn and use the phonetic and visual information of Chinese characters."}, {"title": "I. Confusion Set", "content": "An effective approach to identify similar characters is to use a confusion set. The confusion set is a collection of characters that are easily confused, typically constructed based on similarities in phonetics and visuals. Despite its limitations, such as restricted coverage, lack of adaptability, and potential biases, confusion sets are still used in studies [Li et al., 2022b; Huang et al., 2023], such as during the pretraining phase, as once constructed, they eliminate the need for additional computations to determine character similarity."}, {"title": "II. Learning of Phonetic Information", "content": "Most models utilize the pinyin sequences of Chinese characters to represent their phonetic information, with only a few exceptions, such as Tacotron2 [Huang et al., 2021]. Pinyin effectively captures the phonetic characteristics in a simple and accurate manner. While some models [Liang et al., 2023] process the pinyin sequence as a whole, others adopt a more granular approach [Li et al., 2022b] by distinguishing between initials, finals, and tones."}, {"title": "III. Learning of Visual Information", "content": "There are various approaches to obtaining visual information of Chinese characters. Some studies [Cheng et al., 2020] use the strokes of Chinese characters directly as visual information. However, the same stroke sequence can form entirely different characters when arranged in different structural combinations. To address this, some research [Li et al., 2022a] incorporates an ideographic description sequence comprising structural components and strokes to obtain the"}, {"title": "IV. Utilization of Character Information", "content": "After extracting those information, utilizing it effectively is essential, which can be achieved through three primary approaches described in detail below."}, {"title": "Adjusting the masking strategy", "content": "[Zhang et al., 2021; Li et al., 2022a]. Unlike using the random replacement or the label \"[MASK]\" in the pretraining process of BERT-like models, characters are instead randomly replaced with others that share similar phonetics or visual features. These substitutions are sourced from a confusion set or determined by using similarity-based algorithms like Levenshtein Distance. This strategy enables the model to learn the relationships between phonetically or visually similar characters, thereby prioritizing the replacement of similar characters in potentially erroneous positions. Additionally, to mitigate overfitting, a certain proportion of characters is randomly replaced or masked during pretraining."}, {"title": "Specialized loss function", "content": "Some studies integrate Chinese character information into the loss function, allowing the model to learn relevant features during training. LEAD [Li et al., 2022c] integrates phonetic, shape, and meaning information into the contrastive function by creating positive and negative samples, and these samples optimize the CSC model within a unified contrastive learning framework. SCOPE [Li et al., 2022b] incorporates two parallel decoders: one for CSC and the other for auxiliary Chinese pronunciation pre-"}, {"title": "Embedded in models", "content": "Some studies integrate Chinese character information into various parts of the model, such as word embedding or classification part. Specifically, Spell-GCN [Cheng et al., 2020] integrates the phonetic and shape similarities of Chinese characters using a GCN. It constructs phonetic and shape similarity graphs and propagates features through the GCN, enabling similar Chinese characters to influence one another. In addition, PHMOSpell [Huang et al., 2021] and REALISE [Xu et al., 2021] leverage multimodal information to enhance CSC by integrating phonetic, visual, and semantic information. PHMOSpell employs text-to-speech tasks to learn latent representations of Chinese character pronunciation and utilizes VGG19 to extract glyph features. Additionally, it incorporates an adaptive gating mechanism to dynamically regulate the fusion of phonetic, visual, and semantic information. REALISE employs a hierarchical pinyin encoder to extract phonetic features at both the character and sentence levels while utilizing ResNet to extract glyph features and uses a selective modality fusion mechanism, which dynamically adjusts the fusion ratio of semantic, pinyin, and glyph information through gating units."}, {"title": "3.2 Detector-Corrector Architecture", "content": "The Detector-Corrector architecture (D-C) is a well-established framework for CSC, which follows a two-step process. First, the Detector identifies spelling errors in the input text, typically using Bi-GRU or a lightweight Transformer for binary classification. Second, the Corrector revises only the detected error locations, selecting the most probable character from the probability candidate set. However, the direct D-C architecture has its constraints, leading to ongoing research aimed at improving its efficiency.\nPrevious CSC methods are mainly based on the hard mask approach, which completely replaces detected erroneous characters, potentially discarding essential information of errors. To address these, Soft-Masked BERT [Zhang et al., 2020] incorporates a Bi-GRU for error detection and introduces a soft masking mechanism that applies weighted masking based on error probability rather than direct replacement. However, Soft-Masked BERT may diminish the visual and phonetic characteristics of errors during masking. To solve this, MDCSpell [Zhu et al., 2022] employs a late fusion strategy by directly using the original sentence as input to preserve the critical visual and phonetic features of typos. Furthermore, by integrating the hidden states of the detection and correction modules, it mitigates the misleading impact of typos on the context.\nIn addition, DR-CSC [Huang et al., 2023] introduces an enhanced D-C architecture, dividing the process into three steps: detection, reasoning, and search. First, misspelled characters in the text are identified through a detection task. Next, the reasoning task examines the error category. Finally, the outcomes of the detection and reasoning are used to construct a search matrix based on the confusion set, optimizing the search range for candidate characters.\nFurthermore, Bi-DCSpell [Wu et al., 2024] further optimizes D-C architecture. It uses a bidirectional interaction mechanism, enabling the detection and correction tasks to dynamically reinforce each other and employs a learnable control gate mechanism to regulate the intensity of information exchange between detection and correction modules."}, {"title": "4 Exploration of CSC on LLMs", "content": "In the current era, where LLMs exert a profound influence on NLP research, there remains a relative scarcity of studies focusing on their application to CSC. Meanwhile, research based on PLMs continues to advance. This disparity arises because LLMs, as generative models, exhibit critical limitations in addressing CSC tasks, which will be discussed in detail in Section 6.2. However, LLMs possess undeniable advantages over traditional CSC models in terms of domain adaptability and tolerance to diverse data [Li et al., 2023]. These attributes enable LLMs to perform context-sensitive adaptations rather than relying solely on rote memorization of fixed character pairs. Now, several studies try to address LLMs' issues in CSC, like over-correction or length change, and utilize LLMs' strengths.\n[Liu et al., 2024] proposes an innovative Alignment-and-Replacement Module (ARM). ARM employs an alignment method that incorporates a dynamic programming algorithm and Chinese character similarity calculations to closely align LLM outputs with the original sentences, minimizing discrepancies. To prevent the overcorrection of LLMs, a replacement strategy is applied. Notably, ARM requires no retraining or refine-tuning and can be directly integrated into existing PLMs based models.\n[Li et al., 2024] introduces C-LLM, a CSC method based on character-level segmentation. It addresses the limitations of LLMs in handling character length constraints and phonetic similarity. By redesigning the character-level segmentation approach, conducting continuous pre-training to incorporate new vocabulary, and performing supervised fine-tuning, C-LLM simplifies the correction process to character copying and replacement and enhances the model's performance across general and multi-domain datasets.\n[Zhou et al., 2024] proposes a method that uses a training-independent and prompt-free framework. The approach employs a LLM purely as a language model and integrates it with a minimum distortion model to ensure semantic fidelity through phonetic and shape similarity. Additionally, length-reward and fidelity-reward strategies are incorporated to enhance generation fluency and minimize over-correction."}, {"title": "5 Datasets and Evaluation Criteria", "content": "CSC has long faced challenges related to limited data volume and low-quality datasets, which hinder training performance and evaluation accuracy. To tackle these issues, numerous researchers explore diverse methods to construct various datasets. A summary of existing datasets is provided in Table 2, with detailed descriptions presented below."}, {"title": "5.1 Datasets of CSC", "content": "CSC has long faced challenges related to limited data volume and low-quality datasets, which hinder training performance and evaluation accuracy. To tackle these issues, numerous researchers explore diverse methods to construct various datasets. A summary of existing datasets is provided in Table 2, with detailed descriptions presented below.\nSIGHAN13/14/15 [Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015] are benchmarks for evaluating model performance. However, they present several significant issues: (1) These datasets consists of traditional Chinese characters, whereas the CSC task is conducted in a simplified Chinese characters. (2) All of them, especially SIGHAN13, contains numerous misuse of\u201c\u7684\u201d,\u201c\u5730\u201d, and \u201c\u5f97\u201d, which are easily confused auxiliary words that modify adjectives, nouns, and verbs. (3) Since these datasets are from Chinese beginners, the sentences are relatively simple and contain numerous semantic ambiguities, making them unsuitable as benchmarks. Wang271K [Wang et al., 2018] is developed to address the scarcity of training data. The authors aimed to generate Chinese character pairs with visual and phonetic similarities. OCR and ASR are applied to recognize blurred Chinese characters and Mandarin speech corpus. Misrecognitions are interpreted as evidence of similarity between the incorrectly recognized and the corresponding correct characters. Then, these pairs are randomly inserted into sentences selected from The People's Daily to complete the dataset.\nIn addition, ECSPell [Lv et al., 2023] encompasses three domains: law, medicine, and official documentation. It draws data from judicial examination, medical network consultation, and official documents. During annotation, volunteers introduced single-character and phrase-level spelling errors reflective of real-world scenarios, followed by manual proofreading to minimize inconsistencies. LEMON [Wu et al., 2023] is a multi-domain dataset, encompassing seven fields:"}, {"title": "5.2 Evaluation Criteria", "content": "The current standard for evaluating CSC involves calculating precision (P), recall (R), and the F1-index (F1). These metrics are further categorized into character-level and sentence-level, as well as detection and correction levels. Now, the majority of studies employ P, R, and F1 calculations for both error detection and correction at the sentence level. The calculation method of F1 is as follows:\nF1 = (2\u00b7P\u00b7R)/(P+R) (1)\nP represents the proportion of right detections or corrections among all sentences modified by the model, while R denotes the proportion of right detections or corrections among all error-containing sentences. Detection is deemed right if the identified position is accurate, whereas correction requires both the position and the corresponding label to be right."}, {"title": "6 Challenges and Opportunities", "content": "This section is segmented into three parts. The first part discusses key issues associated with PLMs and reviews relevant studies trying addressing these challenges. The second part examines the limitations of LLMs in CSC and proposes potential solutions. The third part highlights the shortcomings of existing datasets and explores future directions."}, {"title": "6.1 Challenges and Opportunities in PLMS", "content": "Currently, some studies have highlighted issues with PLM-based models, including over-correction, limited generalization ability, and ineffective correction of continuous errors. Various methods have been proposed to address these challenges; however, these issues have not been completely resolved, suggesting that future research can build upon existing work to further tackle these problems."}, {"title": "(1) Overcorrection and Low Generalization Ability", "content": "In the masked language model training paradigm, the model often substitutes the correct characters with similar ones, leading to overcorrection. This training approach also causes the model to memorize only the character pairs it has encountered during training, leading to an inability to handle novel situations and resulting in limited generalization capability. To address these issues, [Wang et al., 2019] proposes a model that integrates a pointer network with a copy mechanism, which can identify and copy correct characters directly from the input sentence. [Wu et al., 2023] suggests that masking 20% of non-error tokens randomly encourages the model to better utilize context and enhance generalization. [Wei et al., 2024] presents the prior knowledge guided teacher, using prior knowledge to produce soft labels alongside real ones to mitigate overfitting."}, {"title": "(2) Consecutive Errors", "content": "Consecutive errors introduce considerable noise, which disrupts semantic understanding and errors will affect each other, making the correction process more complicated. For this problem, Scholars conduct some research. [Wang et al., 2021] introduces a dynamic connected networks, which incorporates an attention mechanism to model dependencies between adjacent characters, effectively addressing the incoherence caused by consecutive errors. [Shulin et al., 2022] propose enhancing model robustness to multi-error texts by incorporating contextual noise modeling, which employs bidirectional KL divergence to constrain the distributional similarity between the original and the generated noisy context. [Li et al., 2022b] presents a straightforward and effective method employing a multiple-inference mechanism to address continuous errors."}, {"title": "6.2 Challenges and Opportunities in LLMs", "content": "Although LLMs outperform PLMs based models on many tasks, their performance in CSC remains inferior. This performance gap stems from several key limitations, which can be categorized into three issues, as illustrated in Table 3."}, {"title": "(1) Inability to Control Sentence Length", "content": "This is the most critical limitation, since CSC requires the input and output sentences to maintain the same lengths. However, as a generative model, LLMs always alter sentence length despite attempts to control the output through the prompt, which significantly impacts their performance on CSC. In sentence 1, the LLM replaced the correct word \u201c\u539f(originally)\u201d with synonym \u201c\u539f\u672c (initially)\u201d and added \u201c\u4fee\u6539\u540e\u7684\u53e5\u5b50\u662f:(Revised sentence:)\" at the beginning, significantly altering the length of the sentence.\""}, {"title": "(2) Overcorrection", "content": "Due to LLMs training methodology, LLMs frequently transform uncommon but correct expressions into more common alternatives. In sentence 2, despite being error-free, LLMs unnecessarily revise\u201c\u670d\u52a1\u751f(server)\u201d to the more frequently used synonym\u201c\u670d\u52a1\u5458(waiter)\". In addition to common synonyms, overcorrection also exists in proper nouns, punctuation, date formats, etc.\""}, {"title": "(3) Limited Understanding of Phonetic Information", "content": "Owing to LLMs' poor phonetic comprehension ability, LLMs often fail to replace Chinese characters with similar-pinyin alternatives, even though the primary source of errors are phonetic similar characters. In sentence 3, instead of changing"}, {"title": "6.3 Challenges and Opportunities in Datasets", "content": "Although the number of datasets is increasing, some of these datasets contain annotation errors to varying degrees. Beyond these errors, additional issues persist. The most prominent problems are summarized below, with specific examples provided in Table 4:"}, {"title": "(1) Grammatical Errors", "content": "Some sentences contain grammatical errors that cannot be corrected through only replacement operation. These errors often disrupt sentence semantics significantly, which in turn affects the model's understanding of sentences. In sentence 1, the repetition of two \u201c\u4f60(you)\u201d creates a grammatical error that spelling correction alone cannot resolve."}, {"title": "(2) Incomplete Contexts", "content": "Some sentences lack sufficient contextual information, making it impossible to determine the correct label based solely on the sentence. In sentence 2, the context does not clarify whether the third person refers to \u201c\u5979 (her)\u201d or \u201c\u4ed6 (him)\", so the right label cannot be got.\""}, {"title": "(3) Multiple Correct Labels", "content": "Certain errors in sentences can have more than one valid correction. In Sentence 3, \u201c\u6536 (charge)\" is also a right label because\u201c\u8981(need)\u201d and \u201c\u6536 (charge)\" are both phonetically and morphologically similar to \u201c\u54ac(bite)\u201d and they are in the same confusion set.\nThese issues negatively impact training effect and evaluation accuracy. Consequently, future research should prioritize developing higher-quality datasets or improving existing ones through technological or manual annotation methods."}, {"title": "7 Conclusion", "content": "As a subfield of NLP, Chinese spelling correction (CSC) has garnered significant research attention. In this survey, we systematically reviews the development of CSC, covering task characteristics, model architectures, datasets, and evaluation indicators. We also identify key issues in current research, including limitations in models and datasets and propose potential directions for future advancements. We hope this survey can provide valuable insights for researchers and further advance the field in the era of LLMs."}]}