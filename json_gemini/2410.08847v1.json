{"title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization", "authors": ["Noam Razin", "Sadhika Malladi", "Adithya Bhaskar", "Danqi Chen", "Sanjeev Arora", "Boris Hanin"], "abstract": "Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer No over Never can sharply increase the probability of Yes. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.", "sections": [{"title": "1 Introduction", "content": "To ensure that language models generate safe and helpful content, they are typically aligned based on pairwise preference data. One prominent alignment method, known as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), requires fitting a reward model to a dataset of human preferences, and then training the language model to maximize the reward via RL. While often effective for improving the quality of generated responses (Bai et al., 2022a; Achiam et al., 2023; Touvron et al., 2023), the complexity and computational costs of RLHF motivated the rise of direct preference learning methods such as DPO (Rafailov et al., 2023).\nGiven a prompt x, DPO and its variants (e.g., Azar et al. (2024); Tang et al. (2024); Xu et al. (2024a); Meng et al. (2024)) eschew the need for RL, by directly teaching a model \u03c0\u03b8 to increase the margin between the log probabilities of a preferred response y+ and a dispreferred response y\u00af. While intuitively these methods should increase the probability of y+ while decreasing that of y\u00af, several recent works observed that the probabilities of both y+ and y tend to decrease over the course of training (Pal et al., 2024; Yuan et al., 2024; Rafailov et al., 2024b; Tajwar et al., 2024; Pang et al., 2024; Liu et al., 2024). We term this phenomenon likelihood displacement see Figure 1.\nWhen the probability of y+ decreases, the probability of other, possibly undesirable, responses must increase. However, despite the prevalence of likelihood displacement, there is limited understanding as to why it occurs and what its implications are. The purpose of this work is to address these gaps. Through theory and experiments, we characterize mechanisms driving likelihood displacement, demonstrate that it can lead to surprising failures in alignment, and provide preventative guidelines. Our experiments cover models of different families and scales, including"}, {"title": "2 Preliminaries", "content": "Let V be a vocabulary of tokens. Modern language models consist of two parts: (i) a neural network (e.g., Transformer (Vaswani et al., 2017)) that intakes a sequence of tokens x \u2208 V* and produces"}, {"title": "2.1 Direct Preference Learning", "content": "Preference data. We consider the widely adopted direct preference learning pipeline, which relies on pairwise comparisons (cf. Rafailov et al. (2023)). Specifically, we assume access to a preference dataset D containing samples (x, y\u207a, y\u00af), where x is a prompt, y+ is a preferred response to x, and y is a dispreferred response to x. The preferred and dispreferred responses can be obtained by generating two candidate responses from the model (i.e. on-policy), and labeling them via human or AI raters (cf. Ouyang et al. (2022); Bai et al. (2022b)). Alternatively, they can be taken from some static dataset (i.e. off-policy). Our analysis and experiments capture both cases.\nSupervised finetuning (SFT). Preference learning typically includes an initial SFT phase, in which the model is finetuned via the standard cross-entropy loss. The sequences used for SFT can either be independent of the preference dataset D (Touvron et al., 2023) or consist of prompts and preferred responses from D, i.e. of {(x, y+) : (x, y\u207a, y\u00af) \u2208 D} (Rafailov et al., 2023).\nPreference learning loss. Aligning language models based on pairwise preferences is usually done by minimizing a loss of the following form:\n$\\pounds(\\theta) := \\mathbb{E}_{(x,y^+,y^-)\\sim D} [l_{x,y^+,y^-} (\\ln \\pi_{\\theta}(y^+|x) - \\ln \\pi_{\\theta}(y^-|x))],$\nwhere lx,y+,y: R \u2192 R\u2265o is convex and differentiable, for every (x,y+,y\u00af) \u2208 D. Denote by \u03b8init the parameters of the model prior to minimizing the loss L. To guarantee that minimizing L entails increasing the difference between In \u03c0\u03bf(y+|x) and ln \u03c0\u03bf(y\u00af|x), as expected from a reasonable preference learning loss, we make the mild assumption that lx,y+,y- is monotonically decreasing in a neighborhood of ln \u03c0\u03b8init (y+ |x) \u2013 ln init (y-x).\nThe loss L generalizes many existing losses, including: DPO (Rafailov et al., 2023), IPO (Azar et al., 2024), SLiC (Zhao et al., 2023), REBEL (Gao et al., 2024), and GPO (Tang et al., 2024) see Appendix B for details on the choice of lx,y+,y- corresponding to each loss.\u00b2 Notably, the common dependence on a reference model is abstracted through lx,y+,y-. Other loss variants apply different weightings to the log probabilities of preferred and dispreferred responses or incorporate an additional SFT regularization term (e.g., DPOP (Pal et al., 2024), CPO (Xu et al., 2024a), RPO (Liu et al., 2024), BONBON (Gui et al., 2024), and SimPO (Meng et al., 2024)). For conciseness, we defer an extension of our analysis for these variants to Appendix E."}, {"title": "2.2 Likelihood Displacement", "content": "We define likelihood displacement as the phenomenon where, although the preference learning loss is steadily minimized, the log probabilities of preferred responses decrease.\nDefinition 1. Let \u03b8init and \u03b8fin denote a language model before and after training with a preference learning loss L over the dataset D (Equation (2)), respectively, and suppose that the loss was successfully reduced, i.e. L(\u03b8fin) < L(\u03b8init). We say that likelihood displacement occurred if:\u00b3\n$\\frac{1}{|D|} \\sum_{(x,y^+,y^-)\\in D} \\ln \\pi_{\\theta_{fin}}(y^+|x) < \\frac{1}{|D|} \\sum_{(x,y^+,y^-)\\in D} \\ln \\pi_{\\theta_{init}}(y^+|x);$\nand that likelihood displacement occurred for (x, y\u207a, y\u00af) \u2208 D if ln \u03c0\u03b8\u03b5in (y+|x) < ln \u03c0\u03b8init (y+x)."}, {"title": "3 Catastrophic Likelihood Displacement in Simple Settings", "content": "Despite the prevalence of likelihood displacement (Pal et al., 2024; Yuan et al., 2024; Pang et al., 2024; Rafailov et al., 2024a; Liu et al., 2024), there is limited understanding as to why it occurs and where the probability mass goes. Prior work attributed this phenomenon to limitations in model capacity (Tajwar et al., 2024), the presence of multiple training samples or output tokens (Tajwar et al., 2024; Pal et al., 2024), and the initial SFT phase (Rafailov et al., 2024b). In contrast, we demonstrate that likelihood displacement can occur and be catastrophic independently of these factors, even when training over just a single prompt whose responses contain a single token each. The potential adverse effects of such displacement raise the need to formally characterize its underlying causes.\nSetting. The experiments are based on the Persona dataset (Perez et al., 2022), in which every prompt contains a statement, and the model needs to respond whether it agrees with the statement using a single token. We assign to each prompt a pair of preferred and dispreferred tokens (y+, y\u00af) from a predetermined set containing, e.g., Yes, Sure, No, and Never. Then, for the OLMo-1B, Gemma-2B, and Llama-3-8B models, we perform one epoch of SFT using the preferred tokens as labels, in line with common practices, and train each model via DPO on a single randomly selected prompt. See Appendix H.1 for additional details.\nLikelihood displacement is pervasive and can be catastrophic. Table 1 reports the decrease in preferred token probability, and notable tokens whose probabilities increase at the expense of y+. The probability of y+ dropped by at least 0.21 and up to 0.96 absolute value in all runs. Remarkably, when y+ and y\u00af are similar in meaning, the probability mass often shifts to tokens with meanings opposite to y+. Appendix G.1 reports similar findings for experiments using: (i) base models that did not undergo an initial SFT phase (Table 2); or (ii) IPO instead of DPO (Table 3)."}, {"title": "4 Theoretical Analysis of Likelihood Displacement", "content": "To uncover what causes likelihood displacement when minimizing a preference learning loss, we characterize how the log probabilities of responses evolve during gradient-based training. For a preference sample (x, y, y\u00af) \u2208 D, we identify the factors pushing ln \u03c0\u03bf(y+|x) downwards and those determining which responses increase most in log probability instead. Section 4.1 lays"}, {"title": "4.1 Technical Approach", "content": "Given a prompt x, the probability that the model \u03c0\u03bf assigns to a response z is determined by the hidden embeddings hx, hx,z<2, \u00b7 \u00b7 ., hx,z<|z| and the token unembeddings W (Equation (1)). Our analysis relies on tracking their evolution when minimizing the loss L (Equation (2)). To do so, we adopt the unconstrained features model (Mixon et al., 2022), which amounts to treating hidden embeddings as directly trainable parameters. Formally, the trainable parameters are taken to be 0 = {hz : z \u2208 V*}\u222a{W}. This simplification has proven useful for studying various deep learning phenomena, including neural collapse (e.g., Zhu et al. (2021); Ji et al. (2022); Tirer et al. (2023)) and the benefits of language model pretraining for downstream tasks (Saunshi et al., 2021). As verified in Sections 5 and 6, it also allows extracting the salient sources of likelihood displacement.\u2074\nLanguage model finetuning is typically done with small learning rates. Accordingly, we analyze the training dynamics of (stochastic) gradient descent at the small learning rate limit, i.e. gradient flow:\n$\\frac{d}{dt} \\theta(t) = - \\nabla L(\\theta(t)), \\quad t\\geq 0,$"}, {"title": "4.2 Overview of the Main Results", "content": "4.2.1 Single Training Sample and Output Token\nIt is instructive to first consider the case of training on a single sample (x, y\u207a, y\u00af), whose responses y+ \u2208 V and y\u00af \u2208 V contain a single token. Theorem 1 characterizes how the token unembedding geometry determines when Inne(t)(y+|x) is negative, i.e. when likelihood displacement occurs.\nTheorem 1 (Informal version of Theorem 4). Suppose that the dataset D contains a single sample (x, y, y), with y+ \u2208 V and y\u00af \u2208 V each being a single token. At any time t \u2265 0 of training, ln \u03c0\u03bf(t)(y+|x) is more negative the larger the following term is:\n$\\langle W_{y^+(t)}, W_{y^-(t)} \\rangle + \\sum_{z\\in V\\setminus\\{y^+,y^-\\} } \\pi_{\\theta(t)}(z|x) \\langle W_z(t), W_{y^+(t)} - W_{y^-(t)} \\rangle,$\nTwo terms govern the extent of likelihood displacement in the case of single token responses. First, $\\langle W_{y^+(t)}, W_{y^-(t)} \\rangle$ formalizes the intuition that likelihood displacement occurs when the preferred and dispreferred responses are similar. A higher inner product in unembedding space translates to a more substantial (instantaneous) decrease in In te(t) (y+|x). Second, is a term which measures the alignment of other token unembeddings with Wy+(t) \u2013 Wy-(t), where tokens deemed more likely by the model have a larger weight. The alignment of token unembeddings with Wy+(t) - Wy- (t) also determines which tokens increase most in log probability.\nTheorem 2 (Informal version of Theorem 5). Under the setting of Theorem 1, for any time t \u2265 0 of training and token z \u2208 V \\ {y+,y\u00ae}:\n$\\frac{d}{dt} \\ln \\pi_{\\theta(t)}(z|x) \\propto \\langle W_z(t), W_{y^+(t)} - W_{y^-(t)}\\rangle,$\nThe direction Wy+(t) \u2013 Wy- (t) can be decomposed into its projection onto Wy+ (t) and a component orthogonal to Wy+ (t), introduced by Wy\u2013 (t). Thus, tokens increasing in log probability can have unembeddings that mostly align with directions orthogonal to Wy+ (t), especially when the component orthogonal to Wy+(t) of Wy+(t) \u2013 Wy\u2013 (t) is relatively large (which we often find to be the case empirically; see Table 13 in Appendix G.1). Given that token unembeddings are known to linearly encode semantics (Mikolov et al., 2013; Arora et al., 2016; Park et al., 2024), this provides an explanation for why the probability mass can shift to tokens that are unrelated or opposite in meaning to the preferred token, i.e. why likelihood displacement can be catastrophic even in simple settings (as observed in Section 3)."}, {"title": "4.2.2 Responses with Multiple Tokens", "content": "We now extend our analysis to the typical case where responses are sequences of tokens. As shown below, the existence of multiple tokens in each response introduces a dependence on their (contextual) hidden embeddings.\nTheorem 3 (Informal version of Theorem 6). Suppose that the dataset D contains a single sample (x,y+, y\u00af), with y+ \u2208_V* and y\u00af \u2208 V*. At any time t \u2265 0 of training, in addition to the dependence on token unembeddings identified in Theorem 1, ln \u03c0\u03bf(t)(y+|x) is more negative the larger the following term is:\n$\\sum_{k=1}^{|y^+|}\\sum_{k'=1}^{|y^-|} a_{k,k'}(t) \\langle h_{x,y^+_{<k}}(t), h_{x,y^-_{<k'}}(t) \\rangle - \\sum_{k=1}^{|y^+|}\\sum_{k'=1}^{|y^+|} a^\\perp_{k,k'}(t) \\langle h_{x,y^+_{<k}}(t), h_{x,y^+_{<k'}}(t) \\rangle,$"}, {"title": "5 Identifying Sources of Likelihood Displacement", "content": "In Section 4 we derived the CHES score (Definition 2), which for a given model and preference sample (x, y, y\u00af), measures the similarity of y\u207a and y based on their hidden embeddings. Our theory indicates that samples with a higher CHES score lead to more likelihood displacement. Below, we affirm this prediction and show that the CHES score enables identifying which training samples in a dataset contribute most to likelihood displacement, whereas alternative similarity measures fail to do so. The following Section 6 then demonstrates that filtering out samples with a high CHES score can mitigate undesirable implications of likelihood displacement.\nSetting. We use the UltraFeedback and AlpacaFarm datasets and the OLMo-1B, Gemma-2B, and Llama-3-8B models. For every preference dataset and model, we compute the CHES scores of all samples. This requires performing a single forward pass over the dataset. Then, for each of the 0th, 25th, 50th, 75th, and 100th score percentiles, we take a subset of 512 samples centered around it. Lastly, we train the model via DPO on each of the subsets separately, and track the change in log probability for preferred responses in the subset \u2013 the more the log probabilities decrease, the more severe the likelihood displacement is. See Appendix H.2 for further details.\nBaselines. Preferences with low (normalized) edit distance where suggested in Pal et al. (2024) as a cause for likelihood displacement. Thus, we repeat the process outlined above while ranking the similarity of preferences using the (normalized) edit distance, where a lower edit distance between y+ and y corresponds to a higher similarity. To the best of our knowledge, no other property of a preference sample was linked with likelihood displacement in the literature. So we additionally"}, {"title": "6 Unintentional Unalignment in Direct Preference Learning", "content": "Direct preference learning has been successfully applied for improving general instruction following and performance on downstream benchmarks (e.g., Tunstall et al. (2023); Ivison et al. (2023); Jiang et al. (2024); Dubey et al. (2024)). This suggests that likelihood displacement may often be benign in such settings, and so does not require mitigation. However, in this section, we reveal that it can undermine the efficacy of safety alignment. When training a language model to refuse unsafe prompts, we find that likelihood displacement can unintentionally unalign the model, by causing probability mass to shift from preferred refusal responses to harmful responses. We then demonstrate that this undesirable outcome can be prevented by discarding samples with a high"}, {"title": "6.1 Setting", "content": "We train a language model to refuse unsafe prompts via the (on-policy) direct preference learning pipeline outlined in Rafailov et al. (2023), as specified below. To account for the common scenario whereby one wishes to further align an existing (moderately) aligned model, we use the Gemma-2B-IT and Llama-3-8B-Instruct models. Then, for each model separately, we create a preference dataset based on unsafe prompts from SORRY-Bench (Xie et al., 2024). Specifically, for every prompt, we generate two candidate responses from the model and label them as refusals or non-refusals using the judge model from Xie et al. (2024). Refusals are deemed more preferable compared to non-refusals, and ties are broken by the PairRM reward model (Jiang et al., 2023).\u2078 Lastly, we partition the datasets into training and test sets according to a 85%/15% split, and train the language models via DPO over their respective training sets. For brevity, we defer to Appendices G and H some implementation details and experiments using IPO, respectively."}, {"title": "6.2 Catastrophic Likelihood Displacement Causes Unintentional Unalignment", "content": "Since the initial models are moderately aligned, we find that they often generate two refusal responses for a given prompt. Specifically, for over 70% of the prompts in the generated datasets, both the preferred and dispreferred responses are refusals. This situation resembles the experiments of Section 3, where training on similar preferences led to catastrophic likelihood displacement (e.g., when y+ was No and y\u00af was Never, the probability of Yes sharply increased).\nAnalogously, we observe that as the DPO loss is minimized, likelihood displacement causes probability mass to shift away from preferred refusal responses (Table 16 in Appendix G.4 reports the log probability decrease of preferred responses). This leads to a significant drop in refusal rates. Specifically, over the training sets, DPO makes the refusal rates of Gemma-2B-IT and Llama-3-8B-Instruct drop from 80.5% to 54.8% and 74.4% to 33.4%, respectively (similar drops occur over the test sets). In other words, instead of further aligning the model, preference learning unintentionally unaligns it. See Appendix G.4 for examples of unsafe prompts from the training sets, for which initially the models generated two refusals, yet after DPO they comply with the prompts (Table 18).\nWe note that alignment usually involves a tradeoff between safety and helpfulness. The drop in refusal rates is particularly striking since the models are trained with the sole purpose of refusing prompts, without any attempt to maintain their helpfulness."}, {"title": "6.3 Filtering Data via CHES Score Mitigates Unintentional Unalignment", "content": "Section 5 showed that samples with a high CHES score (Definition 2) contibute most to likelihood displacement. Motivated by this, we explore whether filtering data via the CHES score can mitigate unintentional unalignment, and which types of samples it marks as problematic.\nAs discussed in Section 5, due to the embedding geometry of current models, CHES scores can correlate with the lengths of responses. To avoid introducing a length bias when filtering data, we apply a length-normalized variant of CHES (see Definition 3 in Appendix A). For comparison, we also consider adding an SFT term to the DPO loss, as suggested in Pal et al. (2024); Xu et al. (2024a); Pang et al. (2024); Liu et al. (2024), and training over \u201cgold\u201d responses from SORRY-Bench, which were generated from a diverse set of base and safety aligned models and labeled by human raters.\nFiltering data via CHES score mitigates unintentional unalignment. Figure 3 reports the refusal rates before and after training via DPO: (i) on the original dataset, which as stated in Section 6.2 leads to a substantial drop in refusal rates; (ii) with an additional SFT term on the original dataset; (iii) on the gold dataset; and (iv) on a filtered version of the original dataset that contains"}, {"title": "7 Related Work", "content": "Preference learning for language model alignment. There are two main approaches for aligning language models based on preference data. First, RLHF (or RLAIF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022b), which requires fitting a reward model to a dataset of human (or AI) preferences, and then training the language model to maximize the reward. While often effective for improving the quality of generated responses, RLHF is computationally expensive and can suffer from instabilities (Zheng et al., 2023; Ramamurthy et al., 2023; Razin et al., 2024).\nThis has led to the rise of direct preference learning, as popularized by DPO (Rafailov et al., 2023). Our analysis supports methods that maximize the log probability ratio of preferred and dispreferred responses (cf. Section 2.1), including DPO and many of its variants (e.g., Zhao et al. (2023); Azar et al. (2024); Gao et al. (2024); Tang et al. (2024); Pal et al. (2024); Xu et al. (2024a); Liu et al. (2024); Gui et al. (2024); Meng et al. (2024)). Investigating whether other variants, e.g., those pro-"}, {"title": "8 Conclusion", "content": "While direct preference learning has been widely adopted, there is considerable uncertainty around how it affects the model (cf. Xu et al. (2024b); Chen et al. (2024)). Our theory and experiments shed light on the causes and implications of one counter-intuitive phenomenon \u2013 likelihood displacement. We demonstrated that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning, which can result in unintentional unalignment when training a language model to refuse unsafe prompts. Intuitively, these failures arise when the preferred and dispreferred responses are similar. We formalized this intuition and derived the centered hidden embedding similarity (CHES) score (Definition 2), which effectively identifies samples contributing to likelihood displacement in a given dataset. As an example for its potential uses, we showed that filtering out samples with a high (length-normalized) CHES score can prevent unintentional unalignment. More broadly, our work highlights the importance of curating data with sufficiently distinct preferences. We believe the CHES score introduced by our theory may prove valuable in achieving this goal."}, {"title": "8.1 Limitations and Future Work", "content": "Theoretical analysis. Our theory focuses on the instantaneous change of log probabilities, and abstracts away which neural network architecture is used for computing hidden embeddings. Future work can extend it by studying the evolution of log probabilities throughout training and accounting for how the architecture choice influences likelihood displacement.\nOccurrences of catastrophic likelihood displacement. While our findings reveal that likelihood displacement can make well-intentioned training result in undesirable outcomes, we do not claim that this occurs universally. Indeed, direct preference learning methods have been successfully applied for aligning language models (Tunstall et al., 2023; Ivison et al., 2023; Jiang et al., 2024; Dubey et al., 2024). Nonetheless, in light of the growing prominence of these methods, we believe it is crucial to detect additional settings in which likelihood displacement is catastrophic.\nUtility of the CHES score. We demonstrated the potential of the (length-normalized) CHES score for filtering out samples that cause likelihood displacement. However, further investigation is necessary to assess its utility more broadly. For example, exploring whether data filtering via CHES scores improves performance in general instruction following settings, or whether CHES scores can be useful in more complex data curation pipelines for selecting distinct preferences based on a pool of candidate responses, possibly generated from a diverse set of models."}, {"title": "A Length-Normalized CHES Score", "content": "In Section 4 we derived the CHES score (Definition 2), which for a given model and preference sample (x, y, y\u00af), measures the similarity of y+ and y based on their hidden embeddings. Section 5 then demonstrated on standard preference learning datasets (UltraFeedback and AlpacaFarm) that samples with high CHES scores contribute most to likelihood displacement. However, as discussed in Section 5, due to the embedding geometry of current models, CHES scores often correlate with the lengths of responses. Thus, to avoid introducing a length bias when filtering data in Section 6.3, we apply the following length-normalized variant of CHES.\nDefinition 3. For a preference sample (x, y+, y\u00af) \u2208 D, we define the length-normalized CHES score of y+ and y with respect to a model \u03c0\u03bf by:\n$CHES_x(y^+,y^-) := \\frac{1}{|y^+||y^-|} (\\sum_{k=1}^{|y^+|} \\frac{h_{x,y^+_{<k}}}{\\left \\|h_{x,y^+_{<k}} \\right \\|}, \\sum_{k=1}^{|y^-|} \\frac{h_{x,y^-_{<k}}}{\\left \\|h_{x,y^-_{<k}} \\right \\|}),$"}, {"title": "B Common Instances of the Analyzed Preference Learning Loss", "content": "Let (x, y, y\u00af) \u2208 D be a preference sample. As discussed in Section 2.1, the preference learning loss L (Equation (2)) considered in our analysis generalizes many existing losses, which are realized by different choices of lx,y+,y-. The choice of lx,y+,y- corresponding to each loss is given below.\nDPO (Rafailov et al., 2023). The DPO loss can be written as:\n$l_{x,y^+,y^-} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)}) := - \\ln \\sigma(\\beta(\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)} - \\ln \\frac{\\pi_{ref}(y^+|x)}{\\pi_{ref}(y^-|x)})),$ where Tref is some reference model, \u03b2 > 0 is a regularization hyperparameter, and \u2642 : R \u2192 [0, 1] denotes the sigmoid function.\nIPO (Azar et al., 2024). The IPO loss can be written as:\n$l_{x,y^+,y^-} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)}) := ( - \\ln(\\pi_{\\theta}(y^+|x)) - \\ln \\frac{\\pi_{ref}(y^+|x)}{\\pi_{\\theta}(y^-|x)} - \\ln \\frac{\\pi_{ref}(y^-|x)} - \\tau)^2),$ where Tref is some reference model and \u03c4 > 0 is a hyperparameter controlling the target log probability margin.\nSLiC (Zhao et al., 2023). The SLiC loss can be written as:\n$l_{x,y^+,y^-} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)}) := \\max \\{ 0, \\delta - (\\ln \\pi_{\\theta}(y^+|x) - \\ln \\pi_{\\theta}(y^-|x)) \\},$ where d > 0 is a hyperparameter controlling the target log probability margin. We note that our assumption on lx,y+,y- being monotonically decreasing in a neighborhood of ln \u03c0\u03b8init (y+x) In teinit (yx) holds, except for the case where the loss for (x, y, y\u00af) is already zero at initial-ization (recall @init stands for the initial parameters of the model).\nREBEL (Gao et al., 2024). The REBEL loss can be written as:\n$l_{x,y^+,y^-} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)}) := (\\frac{1}{2} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)} - \\ln \\frac{\\pi_{ref}(y^+|x)}{\\pi_{ref}(y^-|x)} - \\eta (r(x,y^+) + r(x,y^-))),$ where Tref is some reference model, \u03b7 > 0 is a regularization parameter, and r is a reward model.\nGPO (Tang et al., 2024). GPO describes a family of losses, which can be written as:\n$l_{x,y^+,y^-} (\\ln \\frac{\\pi_{\\theta}(y^+|x)}{\\pi_{\\theta}(y^-|x)}) := f(\\ln \\frac{\\pi_{\\theta}(y^+|x) - \\ln \\frac{\\pi_{ref}(y^+|x)}{\\pi_{\\theta}(y^-|x)} - \\ln \\frac{\\pi_{ref}(y^-|x)}),$ where ref is some reference model and f : R \u2192 R is convex and monotonically decreasing in a neighborhood of ln \u03c0\u03b8init (y+ |x) \u2013 ln \u03c0\u03b8init (y\u00af|x) (recall @init stands for the initial parameters of the model)."}, {"title": "C Relation to Existing Claims on Likelihood Displacement", "content": "Throughout the paper, we specified how our results relate to existing claims regarding likelihood displacement. This appendix provides a concentrated account for the convenience of the reader.\nSimilarity of preferences. Tajwar et al. (2024) and Pal et al. (2024) informally claimed that samples with similar preferences are responsible for likelihood displacement. Our theoretical analysis (Section 4) formalizes this intuition, by proving that similarities between the token unembeddings and hidden embeddings of preferred and dispreferred responses drive likelihood displacement.\nDataset size and model capacity. Tajwar et al. (2024) also attributed likelihood displacement to the presence of multiple training samples in a dataset or a limited model capacity. Section 3 demonstrates that likelihood displacement can occur independently of these factors, even when training an 8B model on a single sample. Nonetheless, as we characterize in Section 4.2.3, having multiple training samples can contribute to the severity of likelihood displacement.\nPreferences with small edit distance. Pal et al. (2024) showed in controlled settings that preferences with a small edit distance can lead to likelihood displacement. However, as the experiments in Section 5 demonstrate, in more general settings edit distance is not indicative of likelihood displacement. In contrast, the CHES score (Definition 2), which measures similarity based on hidden embeddings, accurately identifies samples contributing to likelihood displacement.\nInitial SFT Phase. Rafailov et al. (2024b) suggested that likelihood displacement occurs due to the initial SFT phase in the direct preference learning pipeline (see Section 2). Our experiments and theory (Sections 3 and 4) refine this claim by showing that likelihood displacement can occur regardless of whether a model undergoes an initial SFT phase or not.\nPast sightings of catastrophic likelihood displacement. Prior work observed that DPO tends to degrade the performance on math and reasoning benchmarks (Pal et al., 2024; Yuan et al., 2024; Pang et al., 2024; Meng et al., 2024). This can be considered as an instance of catastrophic likelihood displacement. We note that, because in those settings only a few responses are correct, almost any likelihood displacement is catastrophic. In contrast, our work demonstrates that likelihood displacement can be catastrophic even in settings where there are many acceptable responses, and reveals its adverse effects for safety alignment."}, {"title": "D Formal Analysis of Likelihood Displacement", "content": "This appendix delivers the formal analysis overviewed in Section 4.2. Appendices D.1 to D.3 cover the results discussed in Sections 4.2.1 to 4.2.3, respectively. We refer the reader to Section 4.1 for the technical setting of the analysis.\nNotation. For any time t > 0, we use W(t), W\u2082(t), and h"}]}