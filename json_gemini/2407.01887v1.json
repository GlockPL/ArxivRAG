{"title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents", "authors": ["Fanzeng Xia", "Hao Liu", "Yisong Yue", "Tongxin Li"], "abstract": "In-context decision-making is an important capability of artificial general intelligence, which Large Language Models (LLMs) have effectively demonstrated in various scenarios. However, LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper investigates the performance of LLMs as decision-makers in the context of Dueling Bandits (DB). We first evaluate the performance of LLMs by comparing GPT-3.5 TURBO, GPT-4, and GPT-4 TURBO against established DB algorithms. Our results reveal that LLMs, particularly GPT-4 TURBO, quickly identify the Condorcet winner, thus outperforming existing state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle to converge even when explicitly prompted to do so, and are sensitive to prompt variations. To overcome these issues, we introduce an LLM-augmented algorithm, IF-ENHANCED LLM, which takes advantage of both in-context decision-making capabilities of LLMs and theoretical guarantees inherited from classic DB algorithms. The design of such an algorithm sheds light on how to enhance trustworthiness for LLMs used in decision-making tasks where performance robustness matters. We show that IF-ENHANCED LLM has theoretical guarantees on both weak and strong regret. Our experimental results validate that IF-ENHANCED LLM is robust even with noisy and adversarial prompts.", "sections": [{"title": "1 Introduction", "content": "In this paper, we are particularly interested in studying the ability of Large Language Model (LLM) agent to behave optimally in online decision-making settings, i.e., have low regret. In such settings, notable failure cases have been reported [1, 2], e.g., LLM agents being vulnerable to adversarial loss functions and suffer from high regret compared to classic algorithms such as Follow-The-Regularized-Leader (FTRL) [1], and failures in exploration for classic Multi-Armed Bandit (MAB) problems [2]. These results suggest that carefully designed prompts (particularly with external summarization) and non-trivial algorithmic interventions are needed to elicit desirable behavior of LLM agents.\n\nThe failure cases encountered by LLMs may be attributed to intrinsic difficulties of handling numerical rewards,\nand there has been a notable lack of emphasis on evaluating the relative comparisons among the decisions they\ngenerate. To disentangle the complexities introduced by numerical rewards, this paper focuses on the problem\nof dueling bandits (DB) [3] as an important variant of the MAB problem. The problem of DB [3, 4] extends the\nclassic MAB model [5] by querying for preference feedback between selected pairs of arms to identify the best one.\nIn DB, a decision-maker chooses a pair of arms every time, but only observes the binary outcome (win or lose)\nof a noisy comparison between the two selected arms. Figure 1 shows a toy example illustrating the in-context\ninteraction between an LLM agent and an environment. As a variant of the standard MAB problem, DB has\nattracted significant attention in recent years due to its applicability in information retrieval [6], recommendation\nsystems [7], and online ranker evaluation [4]. This setup is particularly useful when eliciting explicit feedback is\nchallenging or when the feedback is inherently comparative, like taste of food and product attractiveness [3]. We\nframe our investigation with the following question:\n\nAre LLMs effective in-context agents for solving the problem of dueling bandits?"}, {"title": "2 Preliminaries", "content": "In this section, we briefly introduce the problem of dueling bandits (DB) and establish the necessary notation for\nthis paper. Additional useful definitions can be found in Appendix A.2.1.\n\nDueling Bandits. In a $K$-armed dueling bandits problem, a learner interacts with the environment by selecting\ntwo arms $Arm_1(t)$ and $Arm_2(t)$ from a set of $K$ arms $\\{b_1,...,b_K\\}$ for a noisy comparison (a duel), at each round\n$t\\in\\{1,...,T\\}$ as Figure 1 illustrates. The outcome of a duel between two arms $(i, j)$ is probabilistic. More\nprecisely, the event that an arm $b_i$ wins against $b_j$ is a Bernoulli random variable with a parameter denoted by\n$Pr(b_i > b_j)$. For notational convenience, we normalize $Pr(b_i > b_j)$ such that $Pr(b_i > b_j) = \\epsilon(b_i,b_j) + 1/2$, where\n$\\epsilon_{ij} := \\epsilon(b_i,b_j) \\in (-1/2,1/2)$ is a measure of the distinguishability between arms $b_i$ and $b_j$, which is stationary\nover time and is symmetric such that $\\epsilon_{ij} = -\\epsilon_{ji}$ for all $i, j \\in [K] := \\{1,...,K\\}$ (see [3]). Finally, for notational\nconvenience, we define a preference matrix $P = [\\epsilon_{ij}]_{i,j\\in[K]}$.\n\nLLM Agents for Dueling Bandits. We consider an LLM agent with policy $\\pi_{ALLM}$ interacting with a $K$-armed\ndueling bandit environment. At each round $t \\in \\{1, ..., T\\}$, the LLM agent selects a pair of arms $(Arm_1(t), Arm_2(t))$\nfrom the set $\\{b_1,...,b_K\\}$ based on a natural language instruction $Prompt(C, H_t, R)$ (see Figure 6), consisting of\nthree parts:\n\n\u2022 Context $C$: a natural language description of the $K$-armed dueling bandit problem, including the number of\narms $K$, the time horizon $T$, and the task objective.\n\n\u2022 History $H_t$: an externally summarized interaction history up to round $t$, which includes a sequence of pairwise\ndueling results and the empirical probabilities.\n\n\u2022 Reasoning $R$: the zero-shot chain-of-thought (CoT) reasoning [15] that encourages the LLM agent to reason\nabout the problem in a structured manner."}, {"title": "3 Related Works", "content": "Dueling Bandits. The problem of dueling bandits was initially introduced in [3]. Various methods have been\nproposed to tackle the task since then. These methods can be broadly classified into two categories as Explore-\nThen-Exploit methods and Ongoing Regret Minimization methods according to [4]. Explore-Then-Exploit methods\nfocus on identifying the best arm with high confidence before exploiting it, such as Interleaved Filter (IF) [3] and\nBeat the Mean (BTM) [17], etc. In contrast, Ongoing Regret Minimization methods explicitly target the objective\nof minimizing cumulative regret, including Relative Upper Confidence Bound (RUCB) [18] and Self-Sparring [7],\netc. Dueling bandit problem and preference feedback in general has a wide variety of applications, including\nrecommendation systems [3], robotics [19], and most recently, the training algorithm of large language models,\nsuch as Reinforcement Learning from Human Feedback (RLHF) [20].\n\nLLM Agents for Multi-Armed Bandits. Several recent works have explored evaluating the capabilities of LLMs\nin bandit problems. For example, [21] proposed an approach to enhance contextual bandits by integrating LLMs as\nencoders. The LLMs' ability to capture rich semantic and syntactic information from textual contexts is leveraged\nto provide the algorithm with a more informative representation of the context. The LLM-augmented algorithm\ntransforms the raw context into a latent space vector using the LLM's encoding capabilities. This encoded context is\nthen used to guide the decision-making process. [2] investigates whether LLMs can engage in exploration in simple\nMAB environments without additional training. They compared various prompt designs and found that GPT-4\nwith zero-shot chain-of-thought (CoT) reasoning and an externally summarized interaction history performed the\nbest, while other configurations failed in exploration, either by never selecting the best arm after initial rounds or\nby selecting all arms nearly equally often [2]. Different from the previous results, in this work we go beyond the\nsettings of numeric rewards and investigate the capabilities of LLMs under preference feedback.\n\nIn-Context LLMs for Decision-Making. Beyond bandit problems, LLM agents have demonstrated strong\ncapabilities in complex reasoning across a wide range of in-context decision-making tasks [22, 23, 24, 25]. Various\nexisting works aim to understand LLM agents' capabilities for in-context decision-making, with notable examples\nincluding planning [26, 27]. Additionally, LLM agents have been shown to enhance embodied agents in various\nrobotic applications by providing advanced task planning abilities [28] and reward designing [29], further enabling\nthe development of lifelong learning agents [30]. Besides these empirical successes, the authors of [1] analyzed\nLLMs' interactions in online learning and game theory settings through the lens of the regret metrics. They\nidentified simple cases where LLMs fail to be no-regret. Another line of research incorporates LLMs into classic\ndecision-making frameworks to create LLM-augmented online decision-makers. For instance, Liu et al. [31]\nutilized LLMs to enhance the components of warm starting, sampling candidates, and surrogate modeling in\nBayesian optimization. Our work contributes to this broad area by integrating LLM agents with the Interleaved\nFilter algorithm to enhance the utilization of preference feedback."}, {"title": "4 LLMs as Standalone In-Context Decision-Makers", "content": "To evaluate the efficacy of LLMs for solving DB problems, in this section, we use LLMs as standalone in-context\ndecision-making agents and compare them with classic baseline algorithms. Our evaluation is two-fold: First, in\nFigures 2 and 8, we compare the performance of LLM agents and classic algorithms in terms of the strong and\nweak regret (see Eq. (2) and Eq. (3), with standard deviation). Second, we delve into the experimental results and\nanalyze the advantages and disadvantages of LLM agents.\n\n4.1 Implementation Details of Experiments\n\nPrompts and Configurations of LLMs. We employ an interactive zero-shot chain-of-thought (CoT) prompt\n$Prompt(C, H_t, R)$, as defined in Section 2, which describes the problem setting $C$ along with an externally summa-\nrized interaction history $H_t$ and reasoning instructions, denoted by $R$. We adopt the prompting template that leads\nto the best performance among all prompt variations explored in a recent study [2] for the classic MAB problem.\nThe LLM agents interact with dueling bandit environments in a round-based manner, with the prompt guiding\ntheir decision-making process. We conduct experiments with three LLMs: GPT-3.5 TURBO, GPT-4, and GPT-4\nTURBO through the OpenAI API with temperature = 0. The detailed prompt is provided in Appendix B.1.1.\n\nBaselines. We compare LLMs against eight well-established baseline algorithms to evaluate their efficacy. The\nbaselines include Interleaved Filter (IF2) [3], Beat the Mean (BTM) [17], Sensitivity Analysis of VAriables for\nGeneric Exploration (SAVAGE) [32], Relative Upper Confidence Bound (RUCB) [18], Relative Confidence\nSampling (RCS) [4], Relative Minimum Empirical Divergence (RMED) [33], Self-Sparring [7], and Double\nThompson Sampling (DTS) [16]. Each of these algorithms employs distinct strategies for selecting arms and\nestimating preferences, with the ultimate goal of efficiently identifying the Condorcet winner. We assess the\nperformance of LLMs and baseline algorithms using strong regret and weak regret metrics defined in Section 2.\n\nEnvironments. We evaluate the regret performance of LLMs and baselines across two stochastic environments,\neach characterized by a distinct preference matrix $P$. The preference matrices are constructed using the Bradley-\nTerry-Luce (BTL) model [34, 3], with a generalized form known as the Plackett-Luce model [35]. In this model,\neach arm is associated with a utility parameter $\\theta(i) > 0$, where $i$ represents the rank of the arm (i.e., $\\theta(1)$\ncorresponds to the best arm, $\\theta(2)$ corresponds to the second best arm, and so on). For any pair of arms $b_i$ and $b_j$,\nthe probability of $b_i$ being preferred over $b_j$ is determined by $P(i > j) = \\theta(i)/(\\theta(i) + \\theta(j))$. Setting the number of\narms $K = 5$, we randomize the order of the arms to prevent selection bias, resulting in the following arm ordering:\n$b_5 > b_3 > b_2 > b_1 > b_4$. We use two instances: Easy and Hard, with their respective $\\theta$ parameters given by:\n\n\u2022 Easy instance: $\\theta(1) = 1$, $\\theta(i) = 0.5 - (i - 1)/2K, \\forall i \\in [2, K]$.\n\n\u2022 Hard instance: $\\theta(i) = 1 - (i - 1)/K, \\forall i \\in [K]$.\n\nNote that the datasets generated in this way satisfy the Strong Stochastic Transitivity (SST) and Stochastic Triangle\nInequality (STI) properties [3] (see Appendix A.2.1 for more details). The settings of the BTL model used in our\nexperiments also imply the existence of a Condorcet winner.\n\nRandom Tests. The scale of our experiments is chosen to balance computational feasibility while preserving the\nability of obtaining meaningful conclusions. We set the time horizon to $T = 2000$ rounds, providing the LLMs and\nbaseline algorithms with sufficient opportunity to learn and adapt to the DB environments. Each experiment is\nreplicated $N = 5$ times for the LLMs and $N = 20$ times for the baseline algorithms, enabling an understanding of\ntheir average behaviors and reliable performance estimates.\n\n4.2 Experimental results\n\nFor brevity, we present our initial analysis focused on the Easy instance (Figure 2). The analysis is qualitatively\nsimilar for the Hard instance (Figure 8 in Appendix B.2). We use $\\gamma = 0.5$ for BTM, $f(K) = 0.3K^{1.01}$ for RMED,\n$\\eta = 1$ for Self-Sparring, and $\\alpha = 0.51$ for RUCB, RCS and DTS. We analyze the results in terms of the strong\nand weak regret defined in Section 2.\n\nStrong Regret. The strong regret results in Figure 2 (Left) reveal that among the LLMs, GPT-4 TURBO\ndemonstrates the best performance in both the Easy and Hard instances. Comparing GPT-4 TURBO with the\nstate-of-the-art baseline algorithms (DTS and Self-Sparring), we observe the following: (i) In the first 20 time"}, {"title": "5 Algorithm-Enhanced LLMs for Dueling Bandits", "content": "Classic DB algorithms such as Interleaved Filter 2 (IF2) [3] are known to be near-optimal, with matching regret\nupper and lower bounds up to multiplicative constants. To address the challenges identified in Table 1 of using\nLLM agents as standalone decision-makers for DB, we propose an LLM-augmented approach to demonstrate the\npossibility of taking advantage of both LLM agents and classic DB algorithms. Our algorithm, IF-ENHANCED\nLLM, enjoys both a regret guarantee and strong empirical performance.\n\n5.1 Algorithm Design of IF-ENHANCED LLM\n\nIn this section, we present the design intuitions of IF-ENHANCED LLM. We begin by discussing the limitations\nof a naive intervention approach and the desirable properties for an effective LLM augmentation framework.\nBased on these considerations, we identify IF2 [3] as the ideal candidate algorithm to serve as the foundation\nfor IF-ENHANCED LLM. Finally, we illustrate the components of IF-ENHANCED LLM and provide a detailed\nalgorithm description.\n\nLimitations of Naive Intervention. A straightforward approach to addressing the limitations of LLMs is to use a\nsimple if-else condition that forces the LLMs to converge when they appear to select two identical arms, which\nwe call the Convergence-Triggered (CT) intervention strategy. However, CT fails to guarantee the selection of\nthe true Condorcet winner and can reinforce local optima (see Figure 13 in Appendix B.2 for a failure example).\nThis suggests that relying on the LLM's internal convergence behavior for such a mechanism is not robust, as the\nLLM's exploration is largely driven by its inherent sampling noise rather than a structured exploration policy. Thus,\nhandling this limitation with theoretical guarantees remains challenging.\n\nDesirable Properties for LLM Augmentation. To address [Q1], we seek an algorithmic framework with the\nfollowing properties: (i) A clear, symbolic logical structure that allows for easy integration with LLM suggestions.\n(ii) A well-defined exploration-exploitation trade-off that leverages the LLMs' exploration behavior while ensuring\nconvergence. (iii) Strong theoretical guarantees to maintain robustness with various prompting scenarios.\n\nIF2 as an Ideal Candidate. Among classic DB algorithms, IF2 stands out as particularly well-suited for LLM\naugmentation:\n\n\u2022 Its Explore-Then-Exploit [4] structure naturally aligns with the LLMs' tendency to keep exploring without\nconverging (see Figure 11), allowing for effective leveraging the LLMs' exploration behavior while mitigating\ntheir exploration vulnerability and convergence instability (see Table 1).\n\n\u2022 Its symbolic representation of the algorithm's logic enables clear integration of LLM suggestions at specific\npoints without disrupting the overall structure and theoretical guarantees. In contrast, algorithms like\nSelf-Sparring [7] are less symbolic, making them less suitable for direct LLM augmentation.\n\n\u2022 Its strong theoretical guarantees, with an expected regret bound of $O((K/\\epsilon_{bad}) log T)$ matching the DB prob-\nlem's lower bound of $\\Omega((K/\\epsilon_{bad}) log T)$ up to constants (see Appendix A.2.1), and empirical performance\n(see Figures 2 and 8) provide a robust foundation, ensuring convergence and bounded regret.\n\n5.2 Theoretical Guarantees for IF-ENHANCED LLM\n\nIn this section, we present two main theoretical results. First, we characterize the vulnerability of using standalone\nLLM agents for dueling bandits in Assumption 1 and Theorem 5.1. Then, we provide the theoretical guarantees of\nIF-ENHANCED LLM in Theorem 5.2, demonstrating its robustness and efficacy.\n\nAssumption 1 (Worst-Case Behavior). Under the original prompt (see Figure 6), the worst-case behavior of an\nLLM agent in the dueling bandit setting is equivalent to a randomizer that selects actions uniformly at random.\n\nVulnerability of Standalone LLM Agents. Inspired by the adversarial corruptions framework introduced in [36]\nfor the classic MAB problem, we investigate the vulnerability of standalone LLM agents in the DB setting under\nadversarial prompts. We consider an attacker with a budget $\\Phi(T)$ who employs the following strategy: whenever\nthe LLM agent selects the optimal arm $b^*$ for comparison, the attacker manipulates the input prompt to the LLM to\neliminate $b^*$ from the duel with probability $p$ (where $0 < p < 1$ is a constant), subject to the constraint of performing\nat most $\\Phi(T)$ attacks over $T$ rounds. \n\nTheorem 5.1 (Vulnerability). For the dueling bandits problem with K arms and time horizon T, there exists a preference structure and an attacker strategy with budget $\\Phi(T)$, such that any standalone LLM agent, whose policy is represented by Eq.(1) and whose worst-case behavior under the original prompt satisfying Assumption 1, will suffer an expected regret of $\\Omega \\Big(\\min \\Big{\\Phi(T), \\frac{K}{\\epsilon}} \\Big\\Big)$.\n\n5.3 Empirical Evaluation of IF-ENHANCED LLM\n\nRegarding [Q2], we design a two-fold evaluation to assess efficacy and robustness. The evaluation is conducted\non the Easy instance, which provides higher distinguishability, allowing us to observe convergence and regret\ndifferences within a practical number of steps. First, we compare the strong regret of IF-ENHANCED LLM against\nstate-of-the-art baseline algorithms to validate its efficacy. Second, we investigate the robustness of IF-ENHANCED\nLLM with noisy and adversarial prompts.\n\n5.3.1 Efficacy Evaluation: Strong Regret and Weak Regret\n\nHyper-parameters. In our implementation of IF-ENHANCED LLM (see Algorithm 1), there are two hyper-\nparameters: the threshold parameter $t$, which controls the maximum number of comparisons between arms,\nand the confidence parameter $\\delta$, which determines the confidence level for pruning suboptimal arms. For the\nthreshold parameter $t$, we considered values from the set $\\{50,100,200\\}$, and for the confidence parameter $\\delta$, we\nexplored values from $\\{0.1,0.2,0.4\\}$. After fine-tuning, we found that setting $t = 50$ and $\\delta = 0.4$ provided the best\nperformance in terms of cumulative regret. These hyper-parameter values strike a balance between the number\nof comparisons required to identify the best arm and the confidence level for pruning suboptimal arms, enabling\nIF-ENHANCED LLM to efficiently explore and exploit the available arms in the dueling bandits setting."}, {"title": "6 Conclusion", "content": "This work evaluates LLM agents as in-context decision-makers for the problem of dueling bandits (DB) through\ncase studies. We find that the top-performing LLM (GPT-4 TURBO) exhibits strong capabilities in identifying the\nCondorcet winner and achieving low weak regret while maintaining minimal variance. Yet, stemming from the fact\nthat LLMs are primarily designed and trained for word token prediction rather than optimizing decision-making\nobjectives, GPT-4 TURBO struggles to consistently converge to a single optimal solution in the long run and\nis vulnerable to prompt design variations. To address these limitations, we propose IF-ENHANCED LLM, an\nLLM-augmented approach that takes advantages of both LLMs and classic DB algorithms, providing theoretical\nguarantees and robust empirical performance even with noisy and adversarial prompts. Our findings shed light on\nthe capabilities and challenges of LLMs as in-context decision-makers, and pave the way for the employment of\nLLMs in complex decision-making tasks.\n\nLimitations and Future Works. Due to computational constraints and financial costs, our case studies focus on a\nrelatively small number of arms ($K = 5$). To address this limitation, it is worth investigating the scalability of the\nproposed approach to larger decision spaces with more arms as certified by Theorem 5.2. In addition, while IF2 is\nemployed as a representative Explore-Then-Exploit [18] algorithm to design an LLM-augmented algorithm in this\nwork, utilizing other ongoing regret-minimization algorithms, such as RUCB [18], would be an interesting future\ndirection. Moreover, considering more complex preference structures, such as non-transitive preferences, would\nprovide insights on algorithm design. Investigating other variants of dueling bandits, including contextual dueling\nbandits, multi-dueling bandits, and adversarial dueling bandits, would further expand the applicability of this work.\nFinally, we have only experimented with some of the models available to us as of May 2024, selecting high-profile\nclosed-source models over open-source models. Exploring the performance of a wider range of LLMs, including\nopen-source models, would provide a more comprehensive understanding of the capabilities and limitations of\nLLMs in the dueling bandits setting.\n\nBroader Impacts. The ability of LLMs to encode complex contextual information enables their application in\nvarious decision-making scenarios beyond the scope of traditional DB algorithms. This could lead to more informed\nand context-aware decision-making in domains such as healthcare, finance, and recommender systems, where\nincorporating user feedback and adaptivity is crucial for handling dynamic relative preferences. However, the\nsubstantial computational resources required to train and deploy large-scale LLMs for decision-making tasks raise\nconcerns about energy consumption and environmental impact, especially when dealing with a large set of arms."}]}