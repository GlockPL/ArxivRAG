{"title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents", "authors": ["Fanzeng Xia", "Hao Liu", "Yisong Yue", "Tongxin Li"], "abstract": "In-context decision-making is an important capability of artificial general intelligence, which Large Language Models (LLMs) have effectively demonstrated in various scenarios. However, LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper investigates the performance of LLMs as decision-makers in the context of Dueling Bandits (DB). We first evaluate the performance of LLMs by comparing GPT-3.5 TURBO, GPT-4, and GPT-4 TURBO against established DB algorithms. Our results reveal that LLMs, particularly GPT-4 TURBO, quickly identify the Condorcet winner, thus outperforming existing state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle to converge even when explicitly prompted to do so, and are sensitive to prompt variations. To overcome these issues, we introduce an LLM-augmented algorithm, IF-ENHANCED LLM, which takes advantage of both in-context decision-making capabilities of LLMs and theoretical guarantees inherited from classic DB algorithms. The design of such an algorithm sheds light on how to enhance trustworthiness for LLMs used in decision-making tasks where performance robustness matters. We show that IF-ENHANCED LLM has theoretical guarantees on both weak and strong regret. Our experimental results validate that IF-ENHANCED LLM is robust even with noisy and adversarial prompts.", "sections": [{"title": "1 Introduction", "content": "In this paper, we are particularly interested in studying the ability of Large Language Model (LLM) agent to behave optimally in online decision-making settings, i.e., have low regret. In such settings, notable failure cases have been reported [1, 2], e.g., LLM agents being vulnerable to adversarial loss functions and suffer from high regret compared to classic algorithms such as Follow-The-Regularized-Leader (FTRL) [1], and failures in exploration for classic Multi-Armed Bandit (MAB) problems [2]. These results suggest that carefully designed prompts (particularly with external summarization) and non-trivial algorithmic interventions are needed to elicit desirable behavior of LLM agents.\n\nThe failure cases encountered by LLMs may be attributed to intrinsic difficulties of handling numerical rewards, and there has been a notable lack of emphasis on evaluating the relative comparisons among the decisions they generate. To disentangle the complexities introduced by numerical rewards, this paper focuses on the problem of dueling bandits (DB) [3] as an important variant of the MAB problem. The problem of DB [3, 4] extends the classic MAB model [5] by querying for preference feedback between selected pairs of arms to identify the best one. In DB, a decision-maker chooses a pair of arms every time, but only observes the binary outcome (win or lose) of a noisy comparison between the two selected arms. Figure 1 shows a toy example illustrating the in-context interaction between an LLM agent and an environment. As a variant of the standard MAB problem, DB has attracted significant attention in recent years due to its applicability in information retrieval [6], recommendation systems [7], and online ranker evaluation [4]. This setup is particularly useful when eliciting explicit feedback is challenging or when the feedback is inherently comparative, like taste of food and product attractiveness [3]. We frame our investigation with the following question:\n\nAre LLMs effective in-context agents for solving the problem of dueling bandits?"}, {"title": "2 Preliminaries", "content": "In this section, we briefly introduce the problem of dueling bandits (DB) and establish the necessary notation for this paper. Additional useful definitions can be found in Appendix A.2.1.\n\nDueling Bandits. In a K-armed dueling bandits problem, a learner interacts with the environment by selecting two arms $Arm_1(t)$ and $Arm_2(t)$ from a set of K arms $\\{b_1,...,b_K\\}$ for a noisy comparison (a duel), at each round $t\\in {1,...,T}$ as Figure 1 illustrates. The outcome of a duel between two arms (i, j) is probabilistic. More precisely, the event that an arm $b_i$ wins against $b_j$ is a Bernoulli random variable with a parameter denoted by $Pr(b_i > b_j)$. For notational convenience, we normalize $Pr(b_i > b_j)$ such that $Pr(b_i > b_j) = \\varepsilon(b_i,b_j) + 1/2$, where $\\varepsilon_{ij} := \\varepsilon(b_i,b_j) \\in (-1/2,1/2)$ is a measure of the distinguishability between arms $b_i$ and $b_j$, which is stationary over time and is symmetric such that $\\varepsilon_{ij} = -\\varepsilon_{ji}$ for all $i, j \\in [K] := {1,...,K}$ (see [3]). Finally, for notational convenience, we define a preference matrix $P = [\\varepsilon_{ij}]_{i,j\\in[K]}$.\n\nLLM Agents for Dueling Bandits. We consider an LLM agent with policy $\\pi_{LLM}$ interacting with a K-armed dueling bandit environment. At each round $t \\in {1, ..., T}$, the LLM agent selects a pair of arms $(Arm_1(t), Arm_2(t))$ from the set $\\{b_1,...,b_K\\}$ based on a natural language instruction $Prompt(C, H_t, R)$ (see Figure 6), consisting of three parts:\n\nContext C: a natural language description of the K-armed dueling bandit problem, including the number of arms K, the time horizon T, and the task objective.\nHistory $H_t$: an externally summarized interaction history up to round t, which includes a sequence of pairwise dueling results and the empirical probabilities.\nReasoning R: the zero-shot chain-of-thought (CoT) reasoning [15] that encourages the LLM agent to reason about the problem in a structured manner."}, {"title": "3 Related Works", "content": "Dueling Bandits. The problem of dueling bandits was initially introduced in [3]. Various methods have been proposed to tackle the task since then. These methods can be broadly classified into two categories as Explore-Then-Exploit methods and Ongoing Regret Minimization methods according to [4]. Explore-Then-Exploit methods focus on identifying the best arm with high confidence before exploiting it, such as Interleaved Filter (IF) [3] and Beat the Mean (BTM) [17], etc. In contrast, Ongoing Regret Minimization methods explicitly target the objective of minimizing cumulative regret, including Relative Upper Confidence Bound (RUCB) [18] and Self-Sparring [7], etc. Dueling bandit problem and preference feedback in general has a wide variety of applications, including recommendation systems [3], robotics [19], and most recently, the training algorithm of large language models, such as Reinforcement Learning from Human Feedback (RLHF) [20].\n\nLLM Agents for Multi-Armed Bandits. Several recent works have explored evaluating the capabilities of LLMs in bandit problems. For example, [21] proposed an approach to enhance contextual bandits by integrating LLMs as encoders. The LLMs' ability to capture rich semantic and syntactic information from textual contexts is leveraged to provide the algorithm with a more informative representation of the context. The LLM-augmented algorithm transforms the raw context into a latent space vector using the LLM's encoding capabilities. This encoded context is then used to guide the decision-making process. [2] investigates whether LLMs can engage in exploration in simple MAB environments without additional training. They compared various prompt designs and found that GPT-4 with zero-shot chain-of-thought (CoT) reasoning and an externally summarized interaction history performed the best, while other configurations failed in exploration, either by never selecting the best arm after initial rounds or by selecting all arms nearly equally often [2]. Different from the previous results, in this work we go beyond the settings of numeric rewards and investigate the capabilities of LLMs under preference feedback.\n\nIn-Context LLMs for Decision-Making. Beyond bandit problems, LLM agents have demonstrated strong capabilities in complex reasoning across a wide range of in-context decision-making tasks [22, 23, 24, 25]. Various existing works aim to understand LLM agents' capabilities for in-context decision-making, with notable examples including planning [26, 27]. Additionally, LLM agents have been shown to enhance embodied agents in various robotic applications by providing advanced task planning abilities [28] and reward designing [29], further enabling the development of lifelong learning agents [30]. Besides these empirical successes, the authors of [1] analyzed LLMs' interactions in online learning and game theory settings through the lens of the regret metrics. They identified simple cases where LLMs fail to be no-regret. Another line of research incorporates LLMs into classic decision-making frameworks to create LLM-augmented online decision-makers. For instance, Liu et al. [31] utilized LLMs to enhance the components of warm starting, sampling candidates, and surrogate modeling in Bayesian optimization. Our work contributes to this broad area by integrating LLM agents with the Interleaved Filter algorithm to enhance the utilization of preference feedback."}, {"title": "4 LLMs as Standalone In-Context Decision-Makers", "content": "To evaluate the efficacy of LLMs for solving DB problems, in this section, we use LLMs as standalone in-context decision-making agents and compare them with classic baseline algorithms. Our evaluation is two-fold: First, in Figures 2 and 8, we compare the performance of LLM agents and classic algorithms in terms of the strong and weak regret (see Eq. (2) and Eq. (3), with standard deviation). Second, we delve into the experimental results and analyze the advantages and disadvantages of LLM agents.\n\n4.1 Implementation Details of Experiments\n\nPrompts and Configurations of LLMs. We employ an interactive zero-shot chain-of-thought (CoT) prompt $Prompt(C, H_t, R)$, as defined in Section 2, which describes the problem setting C along with an externally summa- rized interaction history $H_t$ and reasoning instructions, denoted by R. We adopt the prompting template that leads to the best performance among all prompt variations explored in a recent study [2] for the classic MAB problem. The LLM agents interact with dueling bandit environments in a round-based manner, with the prompt guiding their decision-making process. We conduct experiments with three LLMs: GPT-3.5 TURBO, GPT-4, and GPT-4 TURBO through the OpenAI API with temperature = 0. The detailed prompt is provided in Appendix B.1.1.\n\nBaselines. We compare LLMs against eight well-established baseline algorithms to evaluate their efficacy. The baselines include Interleaved Filter (IF2) [3], Beat the Mean (BTM) [17], Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) [32], Relative Upper Confidence Bound (RUCB) [18], Relative Confidence Sampling (RCS) [4], Relative Minimum Empirical Divergence (RMED) [33], Self-Sparring [7], and Double Thompson Sampling (DTS) [16]. Each of these algorithms employs distinct strategies for selecting arms and estimating preferences, with the ultimate goal of efficiently identifying the Condorcet winner. We assess the performance of LLMs and baseline algorithms using strong regret and weak regret metrics defined in Section 2.\n\nEnvironments. We evaluate the regret performance of LLMs and baselines across two stochastic environments, each characterized by a distinct preference matrix P. The preference matrices are constructed using the Bradley- Terry-Luce (BTL) model [34, 3], with a generalized form known as the Plackett-Luce model [35]. In this model, each arm is associated with a utility parameter $\\theta(i) > 0$, where i represents the rank of the arm (i.e., $\\theta(1)$ corresponds to the best arm, $\\theta(2)$ corresponds to the second best arm, and so on). For any pair of arms $b_i$ and $b_j$, the probability of $b_i$ being preferred over $b_j$ is determined by $P(i > j) = \\theta(i)/(\\theta(i) + \\theta(j))$. Setting the number of arms K = 5, we randomize the order of the arms to prevent selection bias, resulting in the following arm ordering: $b_5>b_3>b_2>b_1>b_4$. We use two instances: Easy and Hard, with their respective $\\theta$ parameters given by:\n\nEasy instance: $\\theta(1) = 1$, $\\theta(i) = 0.5 - (i - 1)/2K$, $\\forall i \\in [2, K]$.\nHard instance: $\\theta(i) = 1 - (i-1)/K$, $\\forall i \\in [K]$.\nNote that the datasets generated in this way satisfy the Strong Stochastic Transitivity (SST) and Stochastic Triangle Inequality (STI) properties [3] (see Appendix A.2.1 for more details). The settings of the BTL model used in our experiments also imply the existence of a Condorcet winner.\n\nRandom Tests. The scale of our experiments is chosen to balance computational feasibility while preserving the ability of obtaining meaningful conclusions. We set the time horizon to T = 2000 rounds, providing the LLMs and baseline algorithms with sufficient opportunity to learn and adapt to the DB environments. Each experiment is replicated N = 5 times for the LLMs and N = 20 times for the baseline algorithms, enabling an understanding of their average behaviors and reliable performance estimates.\n\n4.2 Experimental results\n\nFor brevity, we present our initial analysis focused on the Easy instance (Figure 2). The analysis is qualitatively similar for the Hard instance (Figure 8 in Appendix B.2). We use $\\gamma = 0.5$ for BTM, $f(K) = 0.3K^{1.01}$ for RMED, $\\eta = 1$ for Self-Sparring, and $\\alpha = 0.51$ for RUCB, RCS and DTS. We analyze the results in terms of the strong and weak regret defined in Section 2.\n\nStrong Regret. The strong regret results in Figure 2 (Left) reveal that among the LLMs, GPT-4 TURBO demonstrates the best performance in both the Easy and Hard instances. Comparing GPT-4 TURBO with the state-of-the-art baseline algorithms (DTS and Self-Sparring), we observe the following: (i) In the first 20 time"}, {"title": "5 Algorithm-Enhanced LLMs for Dueling Bandits", "content": "Classic DB algorithms such as Interleaved Filter 2 (IF2) [3] are known to be near-optimal, with matching regret upper and lower bounds up to multiplicative constants. To address the challenges identified in Table 1 of using LLM agents as standalone decision-makers for DB, we propose an LLM-augmented approach to demonstrate the possibility of taking advantage of both LLM agents and classic DB algorithms. Our algorithm, IF-ENHANCED LLM, enjoys both a regret guarantee and strong empirical performance.\n\n5.1 Algorithm Design of IF-ENHANCED LLM\n\nIn this section, we present the design intuitions of IF-ENHANCED LLM. We begin by discussing the limitations of a naive intervention approach and the desirable properties for an effective LLM augmentation framework. Based on these considerations, we identify IF2 [3] as the ideal candidate algorithm to serve as the foundation for IF-ENHANCED LLM. Finally, we illustrate the components of IF-ENHANCED LLM and provide a detailed algorithm description.\n\nLimitations of Naive Intervention. A straightforward approach to addressing the limitations of LLMs is to use a simple if-else condition that forces the LLMs to converge when they appear to select two identical arms, which we call the Convergence-Triggered (CT) intervention strategy. However, CT fails to guarantee the selection of the true Condorcet winner and can reinforce local optima (see Figure 13 in Appendix B.2 for a failure example). This suggests that relying on the LLM's internal convergence behavior for such a mechanism is not robust, as the LLM's exploration is largely driven by its inherent sampling noise rather than a structured exploration policy. Thus, handling this limitation with theoretical guarantees remains challenging.\n\nDesirable Properties for LLM Augmentation. To address [Q1], we seek an algorithmic framework with the following properties: (i) A clear, symbolic logical structure that allows for easy integration with LLM suggestions. (ii) A well-defined exploration-exploitation trade-off that leverages the LLMs' exploration behavior while ensuring convergence. (iii) Strong theoretical guarantees to maintain robustness with various prompting scenarios.\n\nIF2 as an Ideal Candidate. Among classic DB algorithms, IF2 stands out as particularly well-suited for LLM augmentation:\n\nIts Explore-Then-Exploit [4] structure naturally aligns with the LLMs' tendency to keep exploring without converging (see Figure 11), allowing for effective leveraging the LLMs' exploration behavior while mitigating their exploration vulnerability and convergence instability (see Table 1).\nIts symbolic representation of the algorithm's logic enables clear integration of LLM suggestions at specific points without disrupting the overall structure and theoretical guarantees. In contrast, algorithms like Self-Sparring [7] are less symbolic, making them less suitable for direct LLM augmentation.\nIts strong theoretical guarantees, with an expected regret bound of $O((K/\\varepsilon_{bad}) \\log T)$ matching the DB prob- lem's lower bound of $\\Omega((K/\\varepsilon_{bad}) \\log T)$ up to constants (see Appendix A.2.1), and empirical performance (see Figures 2 and 8) provide a robust foundation, ensuring convergence and bounded regret.\n\n5.2 Theoretical Guarantees for IF-ENHANCED LLM\n\nIn this section, we present two main theoretical results. First, we characterize the vulnerability of using standalone LLM agents for dueling bandits in Assumption 1 and Theorem 5.1. Then, we provide the theoretical guarantees of IF-ENHANCED LLM in Theorem 5.2, demonstrating its robustness and efficacy.\n\nAssumption 1 (Worst-Case Behavior). Under the original prompt (see Figure 6), the worst-case behavior of an LLM agent in the dueling bandit setting is equivalent to a randomizer that selects actions uniformly at random.\n\nVulnerability of Standalone LLM Agents. Inspired by the adversarial corruptions framework introduced in [36] for the classic MAB problem, we investigate the vulnerability of standalone LLM agents in the DB setting under adversarial prompts. We consider an attacker with a budget $\\Phi(T)$ who employs the following strategy: whenever the LLM agent selects the optimal arm $b^*$ for comparison, the attacker manipulates the input prompt to the LLM to eliminate $b^*$ from the duel with probability $p$ (where $0 < p < 1$ is a constant), subject to the constraint of performing at most $\\Phi(T)$ attacks over T rounds. This adversarial strategy compels the LLM agent to select suboptimal arms, resulting in poor performance, as formalized in the following theorem with Assumption 1.\n\nTheorem 5.1 (Vulnerability). For the dueling bandits problem with K arms and time horizon T, there exists a preference structure and an attacker strategy with budget $\\Phi(T)$, such that any standalone LLM agent, whose policy is represented by Eq.(1) and whose worst-case behavior under the original prompt satisfying Assumption 1, will suffer an expected regret of $\\Omega(\\min \\{\\Phi(T), \\frac{K}{\\varepsilon}\\})$ .\n\n5.3 Empirical Evaluation of IF-ENHANCED LLM\n\nRegarding [Q2], we design a two-fold evaluation to assess efficacy and robustness. The evaluation is conducted on the Easy instance, which provides higher distinguishability, allowing us to observe convergence and regret differences within a practical number of steps. First, we compare the strong regret of IF-ENHANCED LLM against state-of-the-art baseline algorithms to validate its efficacy. Second, we investigate the robustness of IF-ENHANCED LLM with noisy and adversarial prompts.\n\n5.3.1 Efficacy Evaluation: Strong Regret and Weak Regret\n\nHyper-parameters. In our implementation of IF-ENHANCED LLM (see Algorithm 1), there are two hyper- parameters: the threshold parameter $\\tau$, which controls the maximum number of comparisons between arms, and the confidence parameter $\\delta$, which determines the confidence level for pruning suboptimal arms. For the threshold parameter $\\tau$, we considered values from the set $\\{50,100,200\\}$, and for the confidence parameter $\\delta$, we explored values from $\\{0.1,0.2,0.4\\}$. After fine-tuning, we found that setting $\\tau = 50$ and $\\delta = 0.4$ provided the best performance in terms of cumulative regret. These hyper-parameter values strike a balance between the number of comparisons required to identify the best arm and the confidence level for pruning suboptimal arms, enabling IF-ENHANCED LLM to efficiently explore and exploit the available arms in the dueling bandits setting."}, {"title": "6 Conclusion", "content": "This work evaluates LLM agents as in-context decision-makers for the problem of dueling bandits (DB) through case studies. We find that the top-performing LLM (GPT-4 TURBO) exhibits strong capabilities in identifying the Condorcet winner and achieving low weak regret while maintaining minimal variance. Yet, stemming from the fact that LLMs are primarily designed and trained for word token prediction rather than optimizing decision-making objectives, GPT-4 TURBO struggles to consistently converge to a single optimal solution in the long run and is vulnerable to prompt design variations. To address these limitations, we propose IF-ENHANCED LLM, an LLM-augmented approach that takes advantages of both LLMs and classic DB algorithms, providing theoretical guarantees and robust empirical performance even with noisy and adversarial prompts. Our findings shed light on the capabilities and challenges of LLMs as in-context decision-makers, and pave the way for the employment of LLMs in complex decision-making tasks.\n\nLimitations and Future Works. Due to computational constraints and financial costs, our case studies focus on a relatively small number of arms (K = 5). To address this limitation, it is worth investigating the scalability of the proposed approach to larger decision spaces with more arms as certified by Theorem 5.2. In addition, while IF2 is employed as a representative Explore-Then-Exploit [18] algorithm to design an LLM-augmented algorithm in this work, utilizing other ongoing regret-minimization algorithms, such as RUCB [18], would be an interesting future direction. Moreover, considering more complex preference structures, such as non-transitive preferences, would provide insights on algorithm design. Investigating other variants of dueling bandits, including contextual dueling bandits, multi-dueling bandits, and adversarial dueling bandits, would further expand the applicability of this work. Finally, we have only experimented with some of the models available to us as of May 2024, selecting high-profile closed-source models over open-source models. Exploring the performance of a wider range of LLMs, including open-source models, would provide a more comprehensive understanding of the capabilities and limitations of LLMs in the dueling bandits setting.\n\nBroader Impacts. The ability of LLMs to encode complex contextual information enables their application in various decision-making scenarios beyond the scope of traditional DB algorithms. This could lead to more informed and context-aware decision-making in domains such as healthcare, finance, and recommender systems, where incorporating user feedback and adaptivity is crucial for handling dynamic relative preferences. However, the substantial computational resources required to train and deploy large-scale LLMs for decision-making tasks raise concerns about energy consumption and environmental impact, especially when dealing with a large set of arms."}, {"title": "Appendix", "content": "This appendix provides supplementary information and additional experimental results to support the main text. The content is organized into two main parts:\n\nTheoretical Part: Algorithm Design and Analysis of IF-ENHANCED LLM\nAppendix A.1 describes the IF-ENHANCED LLM algorithm stated in Section 5.1, detailing its key features and implementation remarks.\nAppendix A.2.1 presents the necessary assumptions and lemmas for the theoretical analysis of IF- ENHANCED LLM in Section 5.2.\nAppendix A.2.2 proves Theorem 5.1 and 5.2, establishing IF-ENHANCED LLM's regret bounds.\nExperimental Part: Prompt Design and Supplementary Results\nAppendix B.1.1 illustrates the prompt design and prompt perturbations logic.\nAppendix B.1.2 provides a five consecutive time steps example of GPT-4 TURBO to showcase its behavior.\nAppendix B.2 presents supplementary experimental results, providing further insights into the performance and behavior of the algorithms in Sections 4 and 5.\n\nA Algorithm Design and Analysis of IF-ENHANCED LLM\n\nIn this section, we detail the design principles and implementation of the IF-ENHANCED LLM algorithm. We also provide a rigorous proof of Theorem 5.1 and 5.2, establishing the theoretical guarantees of IF-ENHANCED LLM under the assumptions outlined in Appendix A.2.1.\n\nA.1 Detailed Procedure Description\n\nIn Procedure 1 below, we describe the MATCH ARMS procedure used in IF-ENHANCED LLM (see Algorithm 1 and Figure 3).\n\nA.2 Theoretical Analysis\n\nA.2.1 Useful Assumptions and Lemmas for Dueling Bandits\n\nWe introduce the useful assumptions and lemmas for Dueling Bandits that are necessary for the theoretical analysis of our proposed algorithm.\n\nAssumption 2 (Total Ordering). The preference matrix $P = (\\varepsilon_{ij})$ satisfies the Total Ordering (TO) property such that for all $i, j \\in [K], i > j$ implies $\\varepsilon_{ij} > 1/2$.\n\nWith the TO property satisfied, we assume the preference matrix P further satisfies the following two standard properties [6, 17, 3].\n\nAssumption 3 (Strong Stochastic Transitivity). The preference matrix $P = (\\varepsilon_{ij})$ satisfies the Strong Stochastic Transitivity (SST) such that for any arms $i, j,k \\in [K]$ such that $i > j > k$ under the total order >, we have $\\varepsilon_{ik} > \\max \\{\\varepsilon_{ij}, \\varepsilon_{jk}\\}$.\n\nAssumption 4 (Stochastic Triangle Inequality). The preference matrix $P = (\\varepsilon_{ij})$ satisfies the Stochastic Triangle Inequality (STI) such that for any arms $i > j > k$, we have $\\varepsilon_{ik} \\leq \\varepsilon_{ij} + \\varepsilon_{jk}$.\n\nNote that the Bradley-Terry-Luce (BTL) model [34] used in our experiments 4.1 satisfies Assumption 3 and 4. We restate the following theoretical guarantees for IF2 that is useful in the proof of Theorem 5.2. Let $\\varepsilon_{bad} := \\min_{b \\neq b^*} \\varepsilon(b,b^*)$.\n\nLemma 1 (Theorem 2 in [3]). Assuming the preference matrix P satisfies the SST and STI, then IF2 has its expected regret (both weak and strong) bounded from above by\n\n$E[SR(IF2)] \\leq O \\Big( \\frac{K}{\\varepsilon_{bad}} \\log T \\Big)$\n\nThe following expected regret bound achieved by IF2 is tight up to multiplicative constants, as indicated by the lower bound (Theorem 4) in [3] such that any algorithm ALG for DB satisfies $E[SR(ALG)] = \\Omega \\Big( (K/\\varepsilon_{bad}) \\log T \\Big)$.\n\nA.2.2 Theoretical Guarantees of IF-ENHANCED LLM\n\nPart I: Vulnerability of Standalone LLM Agents\n\nProof of Theorem 5.1. Consider the following Dueling Bandit instance with K > 3 arms {b1,...,bk} and prefer- ence matrix P:\n$\\Pi_{i,j}=\\begin{cases}\n0.5+\\epsilon, & \\text{if } b_i=b^* \\text{ and } b_j \\neq b^*, \\\\\n0.5-\\epsilon, & \\text{if } b_i \\neq b^* \\text{ and } b_j = b^*, \\\\\n0.5, & \\text{otherwise}.\n\\end{cases}$"}]}