{"title": "RV4Chatbot: Are Chatbots Allowed to Dream of Electric Sheep?", "authors": ["Andrea Gatti", "Viviana Mascardi", "Angelo Ferrando"], "abstract": "Chatbots have become integral to various application domains, including those with safety-critical\nconsiderations. As a result, there is a pressing need for methods that ensure chatbots consistently\nadhere to expected, safe behaviours. In this paper, we introduce RV4Chatbot, a Runtime Verification\nframework designed to monitor deviations in chatbot behaviour. We formalise expected behaviours\nas interaction protocols between the user and the chatbot. We present the RV4Chatbot design and\ndescribe two implementations that instantiate it: RV4Rasa, for monitoring chatbots created with the\nRasa framework, and RV4Dialogflow, for monitoring Dialogflow chatbots. Additionally, we detail\nexperiments conducted in a factory automation scenario using both RV4Rasa and RV4Dialogflow.", "sections": [{"title": "Introduction", "content": "On November 30th, 2022, ChatGPT was unveiled [40] deeply shaking the industry and academic worlds.\nThe impression, at that time, was that ChatGPT and chatbots based on Large Language Models (LLM)\nwould have irreversibly changed the way chatbots were designed and built, wiping away any pre-existing\ntechnology.\nAfter almost two years, a more balanced view on the future is emerging, with the shared feeling that\nthere is still room for chatbots that do not rely on generative AI techniques.\nThere are many ways to classify chatbots based for example on the knowledge domain, the service\nprovided, the goals, the response generation method [2], the locus of control (chatbot- or user-driven)\nand duration of the interaction (short or long) [23], or their affordances and disaffordances [32, 35]. For\nthe purposes of this paper, the simplest and most suitable classification of text-based chatbots divides\nthem into conversational AI and generative AI ones [18].\nDialogFlow [28,42], Rasa [11,41], Wit.ai [38, 39], just to name a few, are text-based conversational\nAI chatbots, also referred to as intent-based chatbots. They can understand the users questions, no matter\nhow they are phrased, thanks to Natural Language Understanding (NLU) capabilities that allow them\nto detect the user's intent and further contextual information. The NLU component exploits machine\nlearning techniques for the intent classification and performs well even with a small amount of training\nsentences. The answer to be provided to the user is not autonomously generated by the chatbot, but is\ndesigned by the chatbot's developer. Conversational AI chatbots can remember conversations with users\nand incorporate contextual information into their interactions.\nChatGPT, Gemini [29], Jasper Chat [31] are examples of generative AI chatbots. They go far beyond\nconversational AI chatbots thanks to their capability of generating new content as their answer in form\nof high-quality text, images and sound based on LLMs they are trained on. This impressive power,\nhowever, does not come without pitfalls. Besides religious bias [1], gender bias and stereotypes [33],\nand hallucinations [48], major privacy concerns are associated with LLMs.\nIn March 2023, Italy's data regulator imposed a temporary ban on ChatGPT due to concerns related\nto data security. During its development, in November 2023, an open letter was signed by nine Italian\nscientific associations including the Italian Association for AI and the Italian Association for Computer\nVision, Pattern Recognition and Machine Learning, and by around 500 scientists, asking the Italian\ngovernment to guarantee that strict rules for the use of generative AI were included in the European AI\nAct [21].\nScientific studies on LLM privacy leakage are so recent to be still unpublished at the time of writing,\nbut many pre-prints by academic scholars show that the problem is real [17,46,47]. Personal Identifiable\nInformation (PII) protection can only be complied with by organisations able to have a private installation\nof a LLM within a private cloud or on premise [30]. The resources needed to implement this solution\nmake it not affordable for most companies and universities.\nThe global LLM market size (that includes the generative AI chatbot market plus a wide range of\nother applications) is projected to reach 259,886 Million USD revenue by 2029 [26], while the conver-\nsational AI market is expected to reach 29,800 Million USD by 2028 [37]: the market forecasts and the\nprivacy, ethical, and economical issues of LLM suggest that traditional conversational AI chatbots will\nstill be needed and used by many players in the next few years.\nAlthough more controllable than their generative evolution, the behaviour of conversational AI chat-\nbots can also be unsafe. In a factory automation scenario, where an intent-based chatbot provides a\nnatural language interface between the user and a virtual representation of a factory, a conversation be-\ncomes unsafe if the user requests to position an object where another object has already been placed, or\nif the distance between objects is insufficient. Similarly, a conversation is unsafe if the chatbot provides\nthe coordinates of an object that the user never inserted.\nTo cope with safety issues in conversational AI chatbots, we present an approach to verify at runtime\nthe conversation between the user and the chatbot. Runtime Verification (RV) [8] is a formal verification\ntechnique used to analyse the runtime behaviour of software and hardware systems concerning specific\nformal properties. A RV monitor emits boolean verdicts that state whether the property is satisfied or\nnot by the currently observed events. The default functioning is to state that something went wrong\nwhen it just went wrong, and trigger recovery actions. In some cases, the monitor may intervene before\nthe wrong event is generated or the unsafe action is done, hence allowing for prevention. With respect\nto other formal verification techniques, such as Model Checking [19] and Theorem Provers [36], RV\nis more dynamic and lightweight and shares some similarities with software testing, being focused on\nchecking how the system behaves while it is running.\nTo perform RV of chatbots, we have designed a general and formalism-agnostic framework named\nRV4Chatbot. We show RV4Chatbot versatility by instantiating it for RV of chatbots created with Rasa,\nwidely used to develop chatbots in local environments, and Dialogflow, used in cloud-based applications.\nWe demonstrate how our engineering decisions render RV4Chatbot a highly practical methodology and\nhow it can seamlessly integrate with existing chatbot frameworks. It is essential to note that our utilisation\nof Rasa and DialogFlow serves only to illustrate potential applications of our approach. Our ultimate aim\nis to encompass any chatbot development framework.\nThe paper is structured as follows. After overviewing the related work in Section 2, Section 3 intro-\nduces one example that motivates the need of RV4Chatbot. After that, Section 4 describes the architec-\nture and the data and control flow of RV4Chatbot. Sections 5 and 6 describe, respectively, RV4Rasa\nand RV4Dialogflow, the two concrete instantiations of the RV4Chatbot logical architecture. Section 7\ndiscusses the formalisation of some relevant safety properties in the motivating scenario, using a highly"}, {"title": "Related Work", "content": "RV of interaction protocols attracted the attention of researchers starting from the beginning of the mil-\nlennium. The first interaction protocols to be verified at runtime involved web services [34], cloud\napplications [44], cryptography [9]. RV of interactions among autonomous software agents followed\nsoon [3,7]\u00b9.\nDespite the large interest in RV of interactions and the pressing need to monitor what chatbots say\nand do, to the best of our knowledge no studies on RV of human-chatbot interactions exist, if we exclude\nthe very recent works where we were involved.\nApart from [22] which serves as the foundation for this paper but is tailored for a specific chatbot\nframework, the only other work in the literature that deals with the formal verification of chatbot systems\nis [20], introducing a framework known as RV4JaCa. In that work we integrated RV within the multiagent\nsystem (MAS) domain and demonstrated how to monitor agent interaction protocols within that context.\nThe focus there was not on the chatbot itself, but on the software agents interacting with it. The main\ncontribution was hence in the MAS domain, although applied in a scenario where messages for agents\nare generated by a chatbot.\nExpanding the boundaries of our investigation, we can mention a recent proposal that approaches\nformal verification of chatbots from a static perspective, instead of at runtime as we do. In [25] the\nauthors introduce a strategy for verifying chatbot conversational flows during the design phase using the\nUPPAAL tool [10], a well-known model checker. The approach is tested by designing a hotel booking\nchatbot and receiving feedback from developers. The strategy is found to have an acceptable learning\ncurve and potential for improving chatbot development. In contrast to our approach, the work presented\nin [25] focuses on abstracting the chatbot using a model and subsequently verifying it through model\nchecking. Due to the distinct inherent natures of these two verification approaches, we envision the\npossibility of integrating them to harness their respective strengths. Specifically, our technique could\nenhance the visibility of [25] by providing information that is only available at runtime. Conversely,\nthe exhaustiveness of [25] could be leveraged by our approach to simplify the properties for monitoring,\nthanks to prior knowledge of the chatbot's behavioural model.\nIf we further expand our search and give up formality, hence resorting to software testing of chatbots,\nsome works from J. Bozic's research group can be mentioned. The paper [14] introduces a planning-\nbased testing approach for chatbots, focusing on functional testing, specifically in the context of tourism\nchatbots for hotel reservations. Planning is used to generate test scenarios, and a testing framework\nautomates the execution of test cases. The results show success in testing chatbots, but some issues,\nsuch as intent recognition errors, need further attention. Metamorphic testing is illustrated in [15], where\nmetamorphic relations are used instead of traditional test oracles due to the unpredictable nature of AI\nsystems. On a similar line of research, the work [13] introduces an approach that leverages ontologies to\ngenerate test cases and addresses the absence of a test oracle by using a metamorphic testing approach.\nThe method is demonstrated on a real tourism chatbot.\nA methodology that automates the generation of coherence, sturdiness, and precision tests for chat-\nbots and leverages the test results to enhance their precision is presented in [16]. The methodology is"}, {"title": "Motivating Example", "content": "Chatbots can be exploited for achieving three main goals: providing specific information stored in a\nfixed source (information chatbots); holding a natural conversation with the user (chat-based chatbots);\nand understanding the tasks that the user wants to perform, hence executing functions to perform them\n(task-based chatbots) [2].\nUsually, information chatbots provide an answer to one questions and go back to a state \u2013 the only\nstate they can be \u2013 where they are ready to answer a new question. There is no need for them to keep\nmemory of what the user already asked or said, and to carry out a coherent and fluent conversation. The\ncorrectness of the chatbot is related with the correctness of the search engine in its backend. Given that\nwe aim at verifying the conversation flow rather than the quality of the retrieved information, RV of\ninformation chatbots following our approach is out of our scope.\nChat-based and task-based chatbots, on the other hand, engage into conversations that should evolve\nin different ways depending on what the user utters. For example, a chat-based chatbot may show\ndifferent reactions to the very same request from the user, depending on how much the user insists upon\nit. In a task-based chatbot, the possibility for the user to ask for some task to be performed may depend\non the fact that some prerequisite task had been asked, and hence performed, before.\nWithout loss of generality, the following motivating example focuses on a task-based chatbot. The\napplication of RV4Chatbot to chat-based chatbots is left for future work, as it would mainly require\nadapting the types of properties to be verified, rather than altering the verification methodology. In\nessence, RV4Chatbot is not restricted to task-based chatbots; its architecture is sufficiently flexible to\nsupport the verification of any intent-based chatbot.\nA Task-based Chatbot in the Factory Automation Domain. This example, presented in the VOR-\nTEX 2023 workshop [22] and briefly summarised here, is set in the field of robotics and involves the\ndevelopment of a task-based chatbot assisting in the creation of a simulated factory work floor. The\nchatbot's role is to guide users through this process, taking into account both the users' requirements\nand the factory regulations\u00b2 concerning what can or cannot be added or removed from the factory work"}, {"title": "RV4Chatbot: The Foundation", "content": "Figure 1 illustrates the operation of an intent-based chatbot. RV4Chatbot specifically focuses on\nverifying intent-based chatbots, leaving the verification of non-intent-based chatbots for future work, as\nmentioned in both the introduction and conclusion sections.\nA human user, or more generally, a sentence generator, produces a sentence in natural language (1).\nThis sentence is categorised by a classifier based on its intent, and its parameters are extracted. The\nintent classifier is responsible for Natural Language Understanding (NLU). The recognised intent, along\nwith its parameters (2), is then passed to a decision maker that determines the chatbot's response (7)\nto the input sentence. This decision-making process is typically hard-coded and integrated directly into\nthe chatbot framework. Finally, the chatbot generates the response to be delivered to the user (8). The\nmodule responsible for this generation process is termed the 'actuator'.\nRV4Chatbot introduces a decision wrapper, depicted in red with right angles inside the chatbot ar-\nchitecture, to manage actions numbered (3) to (6) on the right side of the figure. The decision wrapper\nextends the decision maker to allow the data characterising a chatbot's lifecycle-user's intents and chat-\nbot's actions, both with optional parameters\u2014to be sent to an external monitor where Runtime Verifica-\ntion (RV) occurs. Depending on the chatbot's framework and its modularity, implementing the decision\nwrapper may vary in complexity and intrusiveness. The decision wrapper instruments the \u2018System Under\nScrutiny' (the chatbot in this application), using standard RV terminology. In RV4Chatbot, instrumenta-\ntion is confined solely to this module.\nThe decision wrapper sends the recognised intents and parameters (3) to the monitor. Regardless\nof its implementation and the language used for modeling properties to verify, the monitor observes one\nevent at a time and emits a boolean verdict indicating whether the event complies with the property (4). If\nthe verdict is true (or inconclusive\u00b3), control returns to the decision maker, which decides the subsequent\naction. Before executing the action, the decision wrapper sends it to the monitor (5), which again verifies\nits compliance with the property and emits a verdict (6).\nA true (or inconclusive) verdict from the monitor does not alter the chatbot's standard execution\nflow. A false verdict-whether originating from an unexpected user intent or a disallowed action by the\nchatbot-returns the chatbot to a listening state, displaying a message explaining the failure to the user.\nIn both cases, no unsafe actions are performed.\nNumerous intent-based chatbot frameworks are documented in the literature. Although their imple-\nmentations may vary significantly, their main components and functionalities are accurately represented\nin Figure 1. Similarly, many RV monitors exist. Regardless of the monitor used, it must at least be able\nto observe events from the System Under Scrutiny and output a verdict that is either true, false, or incon-\nclusive. This is the only assumption we make regarding the RV monitor's function, and it is satisfied by\nthe definition of a monitor. Thus, the RV4Chatbot logical architecture is parametric in both the chatbot\nframework and the monitor.\nTo automate the experiments presented in Section 7.3, we developed a piece of software capable of\nreading natural language sentences from a file and sending them to the chatbot using the APIs provided\nby the chatbot frameworks considered in this paper. Although our primary interest lies in RV, we soon\nrealised that the files of simulated user sentences could be seen as test cases, and that the software\ncomponent named \u2018sentence generator' in Figure 1 (left side) could be used to run batches of tests. We\nre-engineered this component and elevated it to the status of one of the RV4Chatbot components. This\napproach allows testing the chatbot during its development by exploiting the monitor as an offline test\nengine. The advantage of this method is that once the chatbot has been tested offline and then deployed,\nthe monitor can continue to function at runtime, in line with its primary objective. No code changes are\nrequired in the monitor or the instrumented chatbot when switching from offline testing to RV; only the\nsource of sentences changes, becoming a human user in the latter case.\nNow that we have completed the introduction of RV4Chatbot, we can focus on its two instantiations\nfor the RV of Rasa and Dialogflow chatbots."}, {"title": "RV4Rasa", "content": "\"Rasa Open Source is an open source conversational AI platform allowing developers to understand and\nhold conversations, and connect to messaging channels and third party systems through a set of APIs.\"4\nRasa [11] is composed by two different tools: Rasa NLU and Rasa Core. When a message is received\nfrom the user, Rasa NLU extracts the intent and the entities (namely, structured pieces of information"}, {"title": "The RV4Rasa instantiation of RV4Chatbot", "content": "The Rasa architecture aligns closely with that of RV4Chatbot, as shown in Figure 2. Each element of the\nRV4Chatbot architecture maps directly to a specific component in the RV4Rasa instantiation, without\nrequiring any modifications to Rasa's original design.\nThe Sentence Generator can be either the human user using the shell provided by Rasa or a script\nthat sends messages as POST requests to the Rasa server provided by Rasa. The Intent Classifier is the"}, {"title": "Challenges in the RV4Rasa design and development", "content": "The main effort required by the RV4Rasa development was understanding how policies work, and im-\nplementing the monitorPolicy. In fact, whereas Rasa's documentation is extensive and well-assorted\nfor a basic usage, it is almost completely absent when policies come into play. The policy is added in the\nconfig file as follows:\npolicies:\n- name: policies.monitorPolicy.MonitorPolicy\npriority: 6\nerror_action: \"utter_error_message\"\nIn its implementation the main class, monitorPolicy, inherits Rasa's Policy class; in particular, it\ninherits and redefines:\n\u2022 __init__, the initialisation method, here the error action provided by the user is saved or set to a\ndefault value if needed;\n\u2022 predict_action_probabilities, called every time the policy runs and returning the list of\nprobabilities. This method may also return no value at all, and this is exactly the way we use it, to\nkeep the conversation flowing as if no RV were performed, if no errors occur.\nNote that, the priority assigned to the policy is user-defined and ensures that Rasa gives precedence to\nthe monitorPolicy over other custom policies. This higher priority is crucial since the monitorPolicy\naddresses safety aspects, which must take precedence in the chatbot's decision-making process."}, {"title": "Source Code", "content": "To instantiate RV4Rasa there are only two additions to be made in the chatbot:\n1. monitor_policy.py: 150 lines of code;\n2. config.yml: 3 further lines should be added to the configuration file, to turn Rasa into RV4Rasa.\nThe code of RV4Rasa is available at https://github.com/driacats/RV4Chat/tree/main/Rasa."}, {"title": "RV4Dialogflow", "content": "Dialogflow [27] is a lifelike conversational AI platform developed by Google that enables users to create\nvirtual agents equipped with intents, entities (similar to those in Rasa), and fulfillment. Fulfillment\nrefers to the capability of these agents to interact with external systems or APIs to retrieve dynamic\nresponses, process data, or execute specific actions based on the user's input, going beyond pre-defined\nstatic responses.\nDialogflow performs NLU using intents, defined via a name and a set of training example sentences.\nDialogflow trains a model able to identify, for each user message sent on the chat, the nearest intent and\nthe confidence score. Training sentences may also contain entities, namely pieces of information that\nmay be significant for the conversation and that should be extracted from the text. For each intent, a\nbunch of possible answers may be displayed. However, some messages cannot be answered from inside\nDialogflow, as they require to process data or execute operations. In this case, users can use the fulfillment\nfor sending a message to an external server that will execute the correct actions, and provide an answer\nback."}, {"title": "The RV4Dialogflow instantiation of RV4Chatbot", "content": "RV4Dialogflow is structured as shown in Figure 3. To instantiate RV4Chatbot in Dialogflow, we had to\nadd a brand new component to the Dialogflow architecture. This made the design and implementation of\nRV4Dialogflow much more complex than the RV4Rasa one.\nThis additional component can be generated directly from an exported Dialogflow agent using an\ninstrumentation script that we developed. We call this brand new component policy, for analogy with\nRV4Rasa.\nThe instrumentation script has two outputs: a .zip file containing the new Dialogflow agent, and the\npolicy python script. In the new Dialogflow agent, every message is forwarded to the policy that controls"}, {"title": "Challenges in the RV4Dialogflow design and development", "content": "The RV4Dialogflow policy works with events as shown in Figure 4 (reported as ev). There are two\nmain types of events: user and bot events.\nThe policy can receive messages both from the user or the bot. When it receives a message it creates\nan event. An event in this domain can be of five main types: (1) a user message with all its features; (2)\na bot message with all its features; (3) a plain answer to be sent as answer; (4) an action to be performed\nby the webhook; (5) the error action.\nThe event is then consumed: it is sent to the monitor and then if it is a plain answer it is sent on\nthe chat, if it is an action it is performed. Obviously, if the monitor claims an error the action is set\nimmediately to the error one.\nWhen the event is consumed the policy computes the next one. For examples, if the event is a user\nmessage the next event is the answer. The policy consumes and computes next event until the next event\nis None, in this case it listens for new inputs."}, {"title": "Source Code", "content": "To instantiate RV4Dialogflow the needed files are:\n1. instrumenter.py: 190 lines of code;\n2. policy.py: (generated by the instrumenter) from 150 lines of code;\nThe code of RV4Dialogflow is available at http://github.com/driacats/RV4Chat/tree/main/\nDialogflow."}, {"title": "Experiments", "content": "The formalism we use to model properties to be verified at runtime is named Runtime Monitoring Lan-\nguage (RML) and has been selected for its high expressive power that goes beyond Linear Temporal"}, {"title": "Runtime Monitoring Language", "content": "The Runtime Monitoring Language (RML [4, 6]) is a Domain-Specific Language (DSL) for specifying\nhighly expressive properties in RV such as non context-free ones. We chose to use RML in this work\nbecause of its support of parametric specifications and its native use for defining interaction protocols.\nSince RML is just a means for our purposes, we only provide a condensed view of its syntax and\ndenotational semantics in terms of the represented traces of events. A detailed explanation of some of its\noperators is provided in Section 7.2 where RML specifications are provided. The complete presentation\ncan be found in [6].\nIn RML, a property is expressed as a tuple (t,ETs), with t a term and ETs = { ET1,...,ETn } a set\nof event types. An event type ET is represented as a set of pairs { k\u2081 : v1,...,kn : vn }, where each\npair identifies a specific piece of information (ki) and its value (vi). An event Ev is denoted as a set\nof pairs { k\u2081 : v\u2081,..., km: vm }. Given an event type ET, an event Ev matches ET if ET \u2286 Ev, which\nmeans \u2200(ki : vi) \u2208 ET \u00b7\u2203(kj : vj) \u2208 Ev\u00b7ki = kj > vi = vj. In other words, an event type ET specifies the\nrequirements that an event Ev has to satisfy to be considered valid.\nAn RML term t, with t1, t2 and t' as other RML terms, can be:\n\u2022 ET, denoting a set of singleton traces containing the events Ev s.t. ET \u2286 Ev;\n\u2022 t1 t2, denoting the sequential composition of two sets of traces;\n\u2022 t1 t2, denoting the unordered composition of two sets of traces (also called shuffle or interleaving);\n\u2022 t1 t2, denoting the intersection of two sets of traces;\n\u2022 t1 V t2, denoting the union of two sets of traces;\n\u2022 { let x; t' }, denoting the set of traces t' where the variable x can be used (i.e., the variable x can\nappear in event types in t', and can be unified with values);\n\u2022 t'*, denoting the set of chains of concatenations of traces in t.\nEvent types can contain variables (we use the terms argument and variable interchangeably). In\nRML, recursion is modelled by syntactic equations involving RML terms, such as t = ET\u2081 t / ET2,\nmodeling a finite (possibly empty) sequence of events matching the event type ET\u2081 ended by one event\nmatching ET2, or the infinite trace including only events matching ET\u2081."}, {"title": "Factory Automation Domain properties", "content": "The three properties to be verified in the factory automation domain have been presented in the VORTEX\n2023 paper [22]. We report one of them to better clarify the use of RML and the kind of protocols we\nare interested in verifying at runtime.\nThe first property aims at ensuring that the user does not add an object in an already taken position.\nThe corresponding RML specification is reported in the following (as in [22]).\nAddObject = { let x, y;\n(msg_user_to_bot / add_object(x,y))\n(msg_bot_to_user / object_added)\n(not_add_object(x,y)* \\AddObject) }\nETs = { msg_user_to_bot,\nmsg_bot_to_user,add_object(x,y),\nobject_added}\nmsg_user_to_bot = { sender : \u201cuser\u201d, receiver : \u201cbot\" }\nmsg_bot_to_user = { sender : \u201cbot\u201d, receiver : \u201cuser\u201d }\nadd_object(x,y) = { intent : { name: \u201cadd_object\" },\nslots: {horizontal : x,vertical : y } }\nobject_added = { last_action : \u201cutter_add_object\" }\nAs an example, the user's request \u201cAdd a robot in position (3, 5)\u201d is safe only if the position (3, 5) is\nempty. But, the position is empty if the user did not already ask to put objects there. Hence, the history\nof the previous interactions must be taken into account, to verify the feasibility of a new object addition.\nThe property is parametric w.r.t. coordinates and is defined recursively; it involves the definition of four\nevent types and exploits the let, A, and * RML operators.\nFigure 5 presents screenshots of the simulated environment in which the Rasa (and Dialogflow)\nchatbots operate. These screenshots specifically demonstrate how, by interacting with the chatbot, the\nuser can add new objects to the simulated factory floor. This interaction occurs under the scrutiny of the\nRML monitor, which checks, among other things, the previously mentioned property.\nThe second property, whose RML encoding is more complex than the previous one, deals with the\naddition of an object in a position which is relative to another object in the simulation. The user's request\nmay be \"Add a robot on the left of Robot3\". In order to be a safe request, Robot3 should have been\npreviously positioned somewhere, hence previous messages involving added and removed objects, their\nname, and their position in the simulation must be taken into account. The property is parametric w.r.t.\ncoordinates and objects names, and defined recursively; it involves eight event types and exploits the\n| and V RML operators, besides let, ^, and *. The | operator is used, for example, to cope with the\ninterleaving of future additions relative to the currently added object and additions of objects that are\nnot relative to the newly added one. Disjunction is used to discriminate between the situation where one\nadded object is then removed, and hence no further references to it are allowed, and the situation where\nno removal takes place, and references are safe.\nThe third property exploits the RML feature of constraining values in event types. It checks that any\nobserved message has the value associated with its confidence field \u2013 as returned by the NLU component\nof the chatbot \u2013 greater than 60%."}, {"title": "Performance Evaluation", "content": "All the experiments can be tested using the code provided here https://github.com/driacats/\nRV4Chat/tree/main/Examples. In particular there are three important scripts for each experiment:\n\u2022 start_service.py: this script allows the user to select the platform (Rasa, Dialogflow), the\nmonitor (no monitor, dummy monitor, real monitor), and the scenario (factory automation), and\nstarts the service;\n\u2022 run_test.py: this script launches the test conversation. The input messages are stored in the\ntest_input.txt file, and the conversation is iterated a certain number of times for each combi-\nnation of platform and monitor. For each iteration, a file is created to store the response times for\neach message sent;\n\u2022 chat.py: this script provides a chat interface to test the chatbot. It takes as argument the platform\nand manages the connection automatically. To test the program it is sufficient to start the service\nand then launch the chat with the same platform.\nThe tests have been performed using RV4Rasa and RV4Dialogflow with three different monitoring levels:\n1. without a monitor;\n2. with a dummy monitor that replies always True;\n3. with a real monitor that checks the properties discussed in the previous sections.\nFor this experiment the chatbot can identify three intents and five entities. The three intents are\n(1) add an object (2) add a object with a reference to another object (3) remove an object, while the\nentities are (1) object to add or remove (2) vertical position (3) horizontal position (4) relative position\n(5) reference object.\nThe Dialogflow WebHook in this case is more complex and manages the addition and the removal\nof objects inside a real virtual environment. The implementation of this experiment in Rasa, with a real\nVirtual Environment in the backend and a Multi-Agent System in the middle, has been presented in [22].\nFor the tests presented here, the API calls that in [22] accessed the virtual environment are instead sent\nto a dummy script that provides a terminal based representation of a virtual space and simulates the\nexecution. No virtual environment implementation is involved in this experiment, which is aimed at\ntesting the performance of the RV mechanism."}, {"title": "Conclusions and Future Work", "content": "This paper introduces RV4Chatbot, a framework for verifying the behaviour of conversational AI chat-\nbots. RV4Chatbot achieves this in a versatile manner, imposing minimal constraints on both the chatbot\ncreation framework and the monitors deployed at runtime for formal verification. To demonstrate its\nefficacy, this paper presents two implementations of RV4Chatbot: RV4Rasa and RV4Dialogflow. The\nengineering and experimental outcomes of these implementations are detailed, particularly when applied\nto safety-critical case studies in domains such as factory automation.\nThe experimental findings underscore RV4Chatbot's generality, efficiency, and lightweight nature in\nterms of the overhead introduced by its monitoring components.\nLooking ahead, our plans involve further exploration and experimentation with RV4Chatbot, includ-\ning its application to more complex case studies that can better challenge the framework's robustness\nand performance. This will allow us to assess how the performance overhead of RV4Chatbot is im-\npacted when applied to larger, real-world conversational systems with increased message volumes, more\ncomplex dialogue flows, and higher interaction frequencies. Additionally, while our current focus is on\nconversational AI chatbots, we plan to evaluate the scalability of RV4Chatbot to understand how it per-\nforms as the number of monitored properties, intents, and concurrent conversations grows. Preliminary\nintuition suggests that the framework's modularity may support scaling to moderately large applications,\nbut this hypothesis needs to be tested empirically.\nFurthermore, the insights and experiences gained from this work may facilitate future developments\nfor handling generative chatbots. In such scenarios, where intents may be unavailable and decision-\nmaking is based on machine learning techniques, we aim to refine and expand RV4Chatbot to integrate\nmore dynamic monitoring approaches that can accommodate the unpredictability and complexity of\ngenerative AI."}]}