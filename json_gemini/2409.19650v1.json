{"title": "Grounding 3D Scene Affordance From Egocentric Interactions", "authors": ["Cuiyu Liu", "Wei Zhai", "Yuhang Yang", "Hongchen Luo", "Sen Liang", "Yang Cao", "Zheng-Jun Zha"], "abstract": "Grounding 3D scene affordance aims to locate interactive regions in 3D environments, which is crucial for embodied agents to interact intelligently with their surroundings. Most existing approaches achieve this by mapping semantics to 3D instances based on static geometric structure and visual appearance. This passive strategy limits the agent's ability to actively perceive and engage with the environment, making it reliant on predefined semantic instructions. In contrast, humans develop complex interaction skills by observing and imitating how others interact with their surroundings. To empower the model with such abilities, we introduce a novel task: grounding 3D scene affordance from egocentric interactions, where the goal is to identify the corresponding affordance regions in a 3D scene based on an egocentric video of an interaction. This task faces the challenges of spatial complexity and alignment complexity across multiple sources. To address these challenges, we propose the Egocentric Interaction-driven 3D Scene Affordance Grounding (Ego-SAG) framework, which utilizes interaction intent to guide the model in focusing on interaction-relevant sub-regions and aligns affordance features from different sources through a bidi-rectional query decoder mechanism. Furthermore, we introduce the Egocentric Video-3D Scene Affordance Dataset (VSAD), covering a wide range of common interaction types and diverse 3D environments to support this task. Extensive experiments on VSAD validate both the feasibility of the proposed task and the effectiveness of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "THE concept of \u201caffordance\u201d is termed the \u201cpossibilities for interaction\" in an environment by James Gibson [1]. For embodied agents operating in a 3D space, understanding environmental affordance and identifying interactive regions are fundamental to effective perception, interaction, reasoning, and learning [2], [3]. Developing this capability is essential for transforming a passive perception system into an intelligent system capable of actively engaging with its environment, which is particularly critical in fields such as embodied in-telligence [4], environmental perception [5], action prediction [6], [7], and robot navigation [8], [9]. To achieve a higher level of environmental comprehension and interaction, despite reasoning and identifying various types of interactions within a 3D scene, an agent should precisely ground the affordance regions involved in these interactions, enhancing its ability to operate in complex environments.\nRecently, increasing attention has been given to under-standing affordance in 3D scenes. Among them, one type of approach involves directly mapping semantics to 3D in-stances based on geometric structure and appearance [10]\u2013[12], enabling agents to identify interactive regions within a scene from semantic instructions. While this semantic mapping offers a structured and intuitive way to analyze the environ-ment, it fundamentally depends on passively recognizing and classifying specific objects. Moreover, due to the limitations of predefined semantic concepts, these methods often fail to accurately capture the diverse interaction targets in complex real-world environments, leading to inaccurate affordance per-ception. Another paradigm leverages reinforcement learning [13]\u2013[15], where agents interact with simulated 3D environ-ments to learn scene affordance through active exploration. However, this method requires numerous trials to converge, making it inefficient. Additionally, the sim-to-real gap further hinders the effectiveness of these methods.\nThese limitations motivate us to explore a new paradigm for understanding 3D scene affordance. In cognitive science, studies show that humans learn complex interaction skills by observing and imitating others' interactions with their sur-roundings, then applying these skills across different contexts [16]. The recent availability of affordable head-mounted equip-ment [17]\u2013[19] and egocentric data offers a more accessible and efficient way to analyze human-environment interactions, making it feasible to construct a model that learns from observations. By focusing on key elements and minimizing redundant visuals, this approach provides richer interaction clues for 3D scene affordance perception. Building on this, we propose grounding 3D scene affordance from egocentric interaction videos, as illustrated in Fig. 1, which lies in a more human-like fashion for embodiment simulation, enhancing agents' understanding of 3D spaces by reducing their reliance on reinforcement learning's trial-and-error processes.\nTo achieve this objective, the primary challenge lies in addressing two key aspects: (1) Spatial Complexity: In 3D en-vironments, the inherent complexity of spatial structures often renders most regions non-essential for interaction, introducing ambiguity in the process of grounding scene affordance. Functional sub-regions within a scene are typically designed based on human interaction patterns and tailored to meet specific functional requirements. For example, kitchen sinks are commonly integrated into countertops to support tasks such as washing. By modeling the relationship between interaction intent observed in videos and the spatial layout of 3D scene sub-regions, it becomes possible to pinpoint the areas most crucial to specific interactions. (2) Alignment Complexity: Variations in user habits, object appearances, and background settings can lead to the same interaction being portrayed differently across videos. At the same time, the corresponding affordance regions within different scenes may also vary significantly in size, position, and structure. These variations create inconsistencies within the feature space, causing com-plexity in aligning affordance features. However, underlying affordance knowledge shared across these interactions can be harnessed to develop a unified representation that aligns regions with common affordance characteristics.\nTo tackle these complexities, we propose Ego-SAG, an innovative framework for Egocentric Interaction-driven 3D Scene Affordance Grounding, designed to extract and align in-teraction information from diverse sources to anticipate scene affordance accurately. This framework consists of two inte-gral sub-modules: the Interaction-Guided Spatial Significance Allocation Module (ISA) and the Bilateral Query Decoder Module (BQD). The ISA, designed to handle spatial complex-ity, is incorporated into each decoding layer of a 3D U-Net. It uses a sampling and grouping strategy to extract features from local sub-regions while modeling the relationship between interaction intent and sub-region layout through the muti-head cross-attention mechanism. This bottom-up approach prioritizes regions most relevant to specific interactions. On the other hand, the BQD constructs a dynamic affordance map across modalities through a bilateral query decoder mecha-nism, which progressively extracts affordance-related features and optimizes high-dimensional alignment to reveal explicit 3D scene affordance.\nFurthermore, existing datasets [20], [21] fail to fully address the specific requirements of the research task proposed in this paper. To overcome this limitation, we introduce a new dataset named Video-3D Scene Affordance Dataset (VSAD), which provides an extensive collection of egocentric inter-action video-3D scene affordance pairs. VSAD encompasses 17 common affordance categories and 16 different interac-tion targets, featuring 3,814 egocentric videos and 2,086 3D indoor scenes. These scenes include 7,690 interactive areas that directly correspond to the interaction videos. The dataset covers a wide range of frequently encountered interaction types and various diverse 3D environments, making it highly representative of real-world scenarios. With its comprehensive scope, VSAD serves as a robust and reliable benchmark for both model training and testing.\nOur contributions are summarized as follows:\n1) This paper presents a 3D scene affordance grounding task from the egocentric view and establishes a large-scale VSAD benchmark to facilitate the research for empowering the agent to capture affordance features from egocentric interactions.\n2) We propose the Ego-SAG framework, which lever-ages interaction intent to guide the model to focus on interaction-relevant sub-regions within the scene and aligns features of affordance regions between the video and 3D scene through a bidirectional query mechanism, thereby revealing explicit 3D scene affordance.\n3) Experiments conducted on the VSAD dataset demon-strate that Ego-SAG significantly outperforms other representative methods across several related fields and can serve as a strong baseline for future research."}, {"title": "II. RELATED WORKS", "content": "A. Visual Affordance Grounding\nAffordance grounding is crucial for agents to understand their interactions with objects and environments. Various ap-proaches have been explored to achieve visual affordance grounding. Some studies predict object affordance from 2D sources, i.e., images and videos [22]\u2013[26], while others utilize natural language to infer the parts of objects where human interaction occurs within 2D data [27], [28]. However, ex-tending these methods to the 3D physical world remains non-trivial due to the inherent gap between 2D representations and 3D environments. With the introduction of several 3D object datasets, some researchers explore affordance grounding using 3D data. Certain approaches directly map semantic affordance to specific parts of 3D object regions [29]\u2013[31]. However, the absence of real-world interaction can hinder the generalization capability, especially when object structures do not correspond neatly to specific affordance. Other methods attempt to infer 3D object affordance and human contact areas through inter-actions in images [32], [33]. However, the limited interaction information in a single image restricts the effectiveness of these methods. While these works mainly focus on single objects, scaling to room-sized environments requires robots to efficiently and effectively explore large-scale 3D spaces for meaningful interactions. Ego-TOPO [34] decomposes space into a topological map derived from egocentric activity to obtain environment affordance, but it is constrained to 2D space. Similarly, reinforcement learning approaches, such as [13]\u2013[15], explore 3D scene affordance, but they are computa-tionally expensive due to the trial-and-error nature of learning. In contrast, unlike prior work, by leveraging the dynamic and contextual interaction information in egocentric videos, our approach enables agents to learn affordance in real-world 3D environments more efficiently, bridging the gap between 2D visual input and 3D scene understanding."}, {"title": "B. Egocentric Video Understanding", "content": "Egocentric vision technology captures human activities from the unique perspective of the camera wearer, providing a distinct action viewpoint [35]. With the introduction of several large-scale Egocentric Video datasets in recent years [36], [37], this field has rapidly developed, leading to numerous research tasks such as action recognition [38]\u2013[40], action prediction [41], [42], interactive object prediction [43], [44], visual question answering [45], [46], and interaction intention anticipation [47]\u2013[49]. While research on these tasks has sig-nificantly advanced machine vision, understanding 2D visual content alone is insufficient for agents operating in the human world to overcome challenges in a 3D environment due to the lack of spatial perception. However, the task proposed in this paper aims to estimate the corresponding fine-grained affordance regions in the 3D environments by identifying and understanding the interactions provided in egocentric videos and the geometric structure in 3D scenes, thereby bridging the gap between 2D and 3D."}, {"title": "C. 3D Spatial Reasoning", "content": "Recently, research on 3D spatial reasoning has garnered widespread attention, significantly advancing agents' abilities to perceive and comprehend the physical world. Some stud-ies improve agents' understanding of objects within specific semantic categories by segmenting semantics and instances in 3D scenes [50]\u2013[53], though generalizing to unknown cate-gories remains a challenge. 3D visual grounding tasks [54]\u2013[56] further deepen spatial reasoning by using bounding boxes to ground objects in 3D scenes based on text descriptions of spatial relationship or object appearance. However, they lack a focus on interactivity within environments. Others [57]\u2013[59] leverage large language model (LLM) to map rich semantic features to 3D scenes, enabling agents to understand object functions, geometric structures and even answer 3D visual questions. However, these methods primarily focus on static scene perception based on given instructions, overlooking the dynamic interactions that scenes afford. In this paper, we explore a novel approach by grounding affordance in 3D scenes through 2D interactive clues, addressing the gap between static perception and dynamic interaction."}, {"title": "III. METHOD", "content": "The pipeline of Ego-SAG is shown in Fig. 2, includ-ing extracting modality-wise features (Sec.III-B), modeling the intrinsic relationship between interaction intent and sub-region layout features to highlight interaction-relevant areas (Sec.III-C) and achieving feature alignment between modali-ties to reveal explicit 3D scene affordance (Sec.III-D).\nA. Preliminaries\nGiven a sample {S, V, y}, where S \u2208 RN\u00d76 is a 3D scene represented by point cloud with three-dimensional coordinates x, y, z and reflection characteristics such as color: r,g,b, V\u2208 R3\u00d7T\u00d7H\u00d7W represents a egocentric interaction video clip with T frames of size H \u00d7 W, and y is the video affordance category. Our goal is to learn a model f\u03b8 that anticipates the fine-grained affordance mask Ma \u2208 RN\u00d71 on the 3D scene that corresponds to the interaction in egocentric video, along with a video affordance logits \u0177, expressed as Ma, y = f(S, V; \u03b8), where \u03b8 is the parameters.\nB. Modality-wise Feature Extraction\nVideo Encoder: To extract features from the input ego-centric video, we utilize the video encoder from Lavila [60]. Specifically, it downsamples the input video clip to 16 frames and then passed through TimeSFormer for feature extraction, which is then projected by a video projector fv to obtain video features Fv \u2208 RC\u00d7TH1W1, where C denotes the feature dimension, and H1, W1 represent the height and width. Fv is directly utilized as input for subsequent network modules."}, {"title": "Scene Encoder:", "content": "We voxelize the point cloud for regular input and utilize a U-Net style backbone that employs submanifold sparse convolution (SSC) and sparse convolution (SC) to extract point-wise scene features Fs \u2208 RC\u00d7N in a bottom-up manner. Within this framework, we design an Interaction-guided Spatial Significance Allocation Module, integrated into each decoding layer of the 3D U-Net and introduced in the following subsection."}, {"title": "C. Interaction-Guided Spatial Significance Allocation Module", "content": "In a 3D scene, extensive non-interactive areas adversely impact the efficiency and accuracy of affordance grounding. To mitigate this, the Interaction-guided Spatial Significance Allocation Module (ISA) firstly captures layout features of these sub-regions through a sample-and-group strategy [61] at each 3D U-Net decoding layer. It then leverages a muti-head cross-attention mechanism to model the intrinsic relationship between interaction intent and sub-region layout features, thereby highlighting interaction-relevant areas and refining the model's capacity for scene understanding.\nSpecifically, each layer of the 3D U-Net is indicated as i \u2208 {1,2,3,4,5} in a bottom-up direction. Due to the sparse and irregular distribution of point clouds, extracting geometric and semantic features of scene sub-regions is challenging. In response to this, ISA firstly applies Farthest Point Sam-pling (FPS) to downsample the i-th decoding layer feature Di \u2208 RCi\u00d7Ni from Ni points to Nc, with these Nc points serving as local centroids. Next, k-nearest neighbor (k-NN) is used to select k nearest neighbors within a radius r around each centroid, aggregating across the k dimension to capture the geometric structure features of uniformly distributed sub-regions, denoted as Gi \u2208 RCi\u00d7Nc. These features are further refined through an MLP layer. The process is expressed as:\nGi = MLP(k-NN(FPS(Di, Nc), k, Di)).\nWe derive the interaction intention in egocentric videos by applying a convolutional projector fi to Fv, followed by average pooling over TH1W1, yielding a unified video-level intention representation Fi \u2208 RC\u00d71. Subsequently, a muti-head cross-attention layer fn is employed to model the intrin-sic correlation between Gi and F\u2081, formulated as:\nFi = fn(Gi, F1),\nwhere F \u2208 RCixNe is the joint feature that emphasizes the interaction-significant sub-regions features. In order to restore the resolution of F to match the Di, the features are propagated [62] from each sub-region's centroid back to its corresponding points through weighted interpolation, resulting in F. Finally, a residual gate \u03b4 is introduced to learn a weight matrix, which further filters out areas irrelevant to the interaction intent, and then combines these features with the original point cloud features, formulated as D\u2081 = Di + \u03b4(Fj) \u2208 RCixNi, which is used to as input of next decoding layer.\nMoreover, to reduce the computational burden of subsequent operations and enhance the entire network's representational capability, we pre-sample the 3D scene into M superpoints (SP) using the method described in [63]. After obtaining the full-resolution feature map Fs of the scene, point-level fea-tures within these superpoints are aggregated into superpoint-level features Fsp fp(Fs,SP) \u2208 RC\u00d7M through a superpoint pooling layer fp. The aggregated Fsp is then fed into the Bilateral Query Decoder module to further explore and align affordance-related contexts between Fsp and Fv."}, {"title": "D. Bilateral Query Decoder Module", "content": "The vast differences between the visual appearance of objects in videos and their geometry counterparts in 3D scenes lead to substantial divergence in the distribution of video and scene features within feature space. To generate accurate and consistent representations of the 3D scene affordance re-gions alongside the corresponding interaction in the egocentric video, Bilateral Query Decoder Module (BQD) leverages a bilateral query mechanism that facilitates interaction between the two modalities, progressively unearthing their consistency, thereby achieving effective feature alignment.\nSpecifically, BQD consists of L basic layers. Before the first layer, Fsp is projected into affordance features Fa and mask features Fm using the MLP-based projectors fa and fm respectively, enabling separate processing of affordance and mask information in subsequent layers. Meanwhile, BQD randomly initialize a set of learnable bilateral query vectors Qo \u2208 RC\u00d7N. At the l-th layer, Q captures affordance infor-mation from Fa and Fv through transformer-based Geometry Cross-Attention block and Interaction Cross-Attention block, respectively. In these blocks, the input Q is initially projected to generate the shared query Q = QW1. Simultaneously, the features Fa and Fv are projected into distinct sets of keys and values, defined as K/Vs = FaW2/3 and K/Vu = FvW4/5, where W1~5 denote the projection matrices. Subsequently, cross-attention is applied to aggregate these features, thereby extracting affordance-related contexts, formulated as:\nQs/v = \u03c3(Q \u00b7 K/v/\u221ad) \u00b7 K/vVs/\u03c5,\nwhere Q \u2208 Rd\u00d7K, K/Vu \u2208 Rd\u00d7M,K/V \u2208 Rd\u00d7TH1W1, d is the dimension of projection. Following is a self-attention layer and a feed-forward network to model the intra-connections between affordance clues extracted by Q in respective modalities, the process is expressed as: Qs/v = FFN(\u0393(Qs/v)). Subsequently, Qs and Quare merged to update the bilateral query vectors Ql+1 for the next layer, as described by: Ql+1 = fm([Qs; Qv]), where fm is an MLP and [;] denotes concatenation.\nTo speed up convergence, we incorporate a prediction head at each layer of BQD. In lth layer, Ql+1 generated for next layer is pooled to computer \u0177. Leveraging the mask-aware feature Fm, we directly multiply it by Qs, followed a sigmoid function to generate superpoint mask Msp \u2208 RM\u00d71 \u2208 [0,1]. Furthermore, as accurate affordance predictions typically oc-cupy only a small fraction of the overall 3D scene, with the majority constituting background, it is necessary to rank and filter the prediction results. To this end, we calculate the dense cosine similarity between each refined scene query vector Q and the pooled video feature Fv, employing this similarity as a quality score to assess and refine the predicted masks, formulated as:\nwi = Qs,iF'v||Qs,i||||F'v||\u2208 RN\u00d71,\nwhere wi is the quality score of corresponding ith superpoint-wise affordance mask Msp,i. And point-wise affordance mask Ma can be obtained by: Ma = Msp[SP].\nE. Training Objectives\nDuring training, we formulate ground truth label assignment as an optimal assignment problem. Hungarian matching [64] is employed to pair predictions Ma with GT affordance masks Ma by minimizing a cost, which consists of a binary cross-entropy (BCE) loss and a dice loss [65], expressed as:\nCi,j = 1(Ma,i, Ma,j) +2(Ma,i, Ma,j),\nDice(Ma, Ma) = 1 \u2212 2|MaMa|+1|| Ma|| + || Ma|| +1,\nwhere C represents the cost of matching the ith predicted affordance mask with the jth GT mask, 1, 2 are hyper-parameters, set to 2 and 5, respectively.\nAfter the assignment, we compute the final loss between the matched pairs, which comprises four parts: LCE, Lmask, LKL, Lcon. LCE computes the cross-entropy loss between y and \u0177, implicitly optimizing the alignment process. To improve the quality of Ma, we introduce a Lmask, expressed as: Lmask = lbce + ldice + lscore, where lscore = MSE(w,wgt) and the score wgt is the IoU between Ma and Ma, used to supervise the quality score of Ma. Intuitively, a higher IoU indicates better mask quality. To enable the network focus on the alignment of affordance regions, we apply KL Divergence (KLD) [66] to constrain the distribution between Q and Qs, formulated as: LKL = D(Q | Qs). Since Q reflects the affordance distribution related to video interac-tions, constraining the distribution of Qs with LKL guides it to focus on the corresponding affordance features in the scene, allowing the alignment and extraction of affordance to optimize mutually. Lcon is a contrastive loss that guides the model to learn more discriminative scene affordance feature representations. Q with IoU greater than 0.5 are treated as positive examples, while the rest are considered negative. Given that a scene may contain multiple affordance regions described by interaction video, resulting in multiple positive examples, we adopt a multi-positive example contrastive loss referred to [67], defined as:\nLcon = -log1N+N+Ni=1exp(w+i)exp(w+i)+N\u2212i=1exp(wi\u2212),\nwhere w+i and wi\u2212 represent the scores of positive and negative pairs while N+ and N\u2212 denote the number of positive and negative affordance predictions within a scene. Ultimately, the total loss is formulated as:\nLtotal = X1LCE + X2Lmask + X3LKL + X4Lcon,\nwhere 1, 2, 3 and 4 are hyper-parameters to balance the total loss.\nDuring inference, Ego-SAG directly predicts K affordance masks with the highest scores and filters out those with scores less than the threshold T, which we set as 0.5."}, {"title": "IV. DATASET", "content": "Collection Details. We constructed the Video-3D Scene Affordance Dataset (VSAD), which comprises paired ego-centric video and 3D scene affordance data. The egocentric interaction videos are primarily sourced from the Ego4D [37] and EPIC-100 [68] datasets, while the 3D indoor scenes are derived from the leading 3D scene understanding datasets such as ScanNetV2 [20] and Matterport3D [21]. The selection criteria are detailed as follows: the egocentric videos must ex-plicitly depict interactions between humans and objects within a 3D scene. For instance, if the 3D scene includes an object like a \"chair\" with the affordance \"move,\" the video must demonstrate the process of a person moving the chair from an egocentric perspective. Ambiguous and outdoor videos are removed by hand, while videos with incorrect annotations are manually corrected. Ultimately, we compiled 3,814 interac-tion video clips covering 17 affordance categories and 16 interaction targets. All scenes from ScanNetV2 are included. To mitigate the long-tail effect and enhance dataset diversity, we select and add additional scenes from Matterport3D as supplements. The final dataset comprises 2,086 3D indoor scenes, encompassing 7,690 interactive areas corresponding to video interactions. The training set consists of 3,063 video clips and 1,649 3D scenes, while the validation set includes 751 video clips and 437 3D scenes. The accompanying Fig. 3 (a) shows paired data examples.\nStatistic Analysis. To provide a more comprehensive and thorough understanding of the VSAD dataset, we present its statistical characteristics in Fig. 3, which offers valuable and insightful information about the dataset's distribution. It is important to emphasize that the egocentric videos and scenes are sampled from different sources, meaning there is no requirement for a strict one-to-one correspondence between them. In practical, a single 3D scene can be linked to multiple egocentric video clips, with the number of pairings varying based on the complexity and diversity of the interactions within each scene. Fig. 3 (b) presents a detailed analysis of the number and distribution of objects across various affordance categories within the video clips, underscoring the diversity of affordance and objects, and highlighting the dataset's ability to capture a wide range of interactions. Furthermore, Fig. 3 (c) illustrates the proportion of videos and scenes associated with each object category, providing key insights into the balance between video content and 3D scene representation, which further demonstrates the dataset's comprehensiveness."}, {"title": "V. EXPERIMENT", "content": "A. Benchmark Setting\nEvaluation Metrics. To provide a comprehensive and effec-tive evaluation, we integrate several advanced achievements in the fields of affordance grounding and 3D scene understanding and selecte the following evaluation metrics: mAP, AP50, AP25, mRC, RC50, and RC25. Specifically, mean Average Precision (mAP) and mean Recall (mRC) are calculated by averaging the scores over IoU thresholds ranging from 50% to 95% in 5% increments. AP50 and RC50 represent the scores at an IoU threshold of 50%, while AP25 and RC25 correspond to an IoU threshold of 25%. These metrics effectively assess the precision and coverage of affordance prediction masks across different IoU thresholds.\nModular Baselines. Similar research to our methodolog-ical paradigm typically uses text to locate target regions in a scene. To highlight the advantages of our approach, we compare it with several state-of-the-art open-vocabulary 3D scene understanding methods [12], [69], [70]. These methods perform well on instance segmentation task and can map semantics to specific structures using an additional text en-coder, enabling region grounding in the scene through open-vocabulary queries such as object category, color, material, and other characteristics. For consistency, we use the category names of the target objects interacted within the videos as queries. For the baseline, we remove the ISA and BQD modules, treat a cross-attention layer before the decoder as a cross-fusion module, and add an MLP to predict quality scores. Furthermore, to demonstrate that egocentric videos with richer interaction clues are more effective for learning scene affordance than a single image, we use only a center frame from the video as the 2D input and employ ResNet50 to extract image features.\nImplementation Detial. Our model is implemented in PyTorch and trained with the AdamW optimizer [71]. The input videos are downsampled to 16 frames and each frame is randomly clipped from 256 \u00d7 256 to 224 \u00d7 224 with random horizontal flipping. For the input 3D scenes, we first extract its superpoints using the method in [63], and then preprocess it using the same method as ScanNetv2. We train the model for 250 epochs on four NVIDIA RTX3090 GPUs with an initial learning rate of 1e-4. The hyperparameters \u51651, 12, 13, and A4 are set to 1, 0.5, 0.5, and 0.5, respectively, with L and N set to 6 and 50."}, {"title": "B. Comparison Results", "content": "Quantitative Results. The results of the objective metrics, as presented in Tab. I, demonstrate that our method consis-tently outperforms all other approaches across all evaluated metrics. The baseline model, which produces poor results, suggests that simple cross-attention mechanism is insufficient to capture the intricate correlations between 2D and 3D sources. Notably, our approach exceeds all methods relying on fixed semantic-geometric mappings to anchor corresponding regions. For instance, compared to Openins3d, our method achieves improvements of +2.304 in mAP, +8.243 in AP50 and +13.474 in AP25. By exploring the relationship between interaction intent and scene layout, our method effectively reduces false affordance mask predictions in non-interactive areas. Furthermore, the use of the BQD mechanism further enhances the alignment of affordance clues between different modalities, contributing to the superior performance of our model. The results from the model trained on single interaction images also highlight the difficulty of capturing complex inter-actions within a single frame, further validating the superiority of our design."}, {"title": "Qualitative Results.", "content": "We present a visual comparison of the affordance maps generated by our method and other ap-proaches, as illustrated in Fig. 4. Our method consistently ex-cels at precisely identifying affordance regions within a scene. For instance, our method accurately grounds the most relevant affordance region for the interaction, while the ISA module effectively ignores irrelevant areas. In more complex scenes, such as the fifth example \"operate washing machine,\" where multiple regions offer the same affordance, our approach pro-duces a more comprehensive affordance maps. Unlike methods that rely on fixed semantic-structure mappings, our approach adeptly predicts scene affordance by considering not only geometry but also interaction properties, avoiding confusion with objects of similar structure. These results reinforce the validity of our method and highlight its superiority."}, {"title": "C. Ablation Study", "content": "In this section, we conduct a comprehensive ablation study to investigate the effect of different framework design, loss function design and hyper-parameter settings.\nFramework Design. The framework ablation results are shown in Tab. II. It reports the model performance without ISA, which captures the correlations of interaction intent and sub-region layout. Without this module, the network struggles to identify significant interaction regions, resulting in incorrect predictions in irrelevant areas and a marked decline in both performance and efficiency. Similarly, excluding BQD hinders the model's ability to extract and align affordance contexts across modalities, leading to a significant drop in overall performance. We also evaluate the impact of the prediction head, which generates predictions at each layer of BQD. Removing these heads and computing the loss solely from the final prediction disrupts progressive supervision, making it harder for the model to converge and significantly reducing its overall performance. To visually assess the effectiveness of our design, we employ t-SNE [72] to illustrate the clustering of affordance for the baseline model (without any modules) and our enhanced model, as shown in Fig. 5. The results indicate that our method establishes more distinct discrimi-native boundaries, enabling clearer differentiation of various affordance features. Additionally, we visualize the relevance maps of Qr to the original video and Qs to the original scene at layer 1 and layer 6 in the BQD module. The visualizations reveal that as the layers deepen, Q and Qs increasingly focus on affordance-related contexts in the video and scene features, confirming the validity and effectiveness of our module design.\nLoss Function Design. We investigate the impact of differ-ent loss functions, with the results presented in Tab. III. Since other loss functions have minimal effect without the supervi-sion of Lmask, all results are from experiments that include Lmask. It indicates that using LCE implicitly optimizes the alignment process, enhancing model performance. Notably, the improvement achieved by Lcon is more significant than that of LKL, suggesting that enhancing the separation between positive and negative examples more effectively boosts the model's ability to perceive and identify distinct affordance regions. Additionally, the incorporation of LKL, compared to using only LCE, substantially improves prediction accuracy by constraining the model to align affordance-related features more precisely. The above results highlight the effectiveness of our loss function design.\nHyper-parameter Settings. We conduct experiments on several key hyper-parameters, including the model dimension C, the number of bilateral query vectors N, and the number of layers in BQD L. The results are presented in Fig. 8. As shown in Fig. 8 (a), C significantly impacts model precision and has a smoothing effect on coverage. A smaller C may yield poor results, as limited dimensionality reduces the network's ability to express complex patterns. Conversely, excessively increasing C may complicate optimization and degrade per-formance. A larger number of channels may increase the complexity of the optimization and lead to a decrease in model performance. Fig. 8 (b) illustrates the effect of N, where each query corresponds to an affordance mask for the final prediction. The optimal performance is achieved at N = 50. Larger values of N tend to worsen performance, likely due to query redundancy, with multiple queries focusing on the same affordance region or interaction-independent region, leading to an increase in false positive. Fig. 8 (c) demonstrates the impact of L, showing a generally positive effect on model performance as L increases from 1 to 6. This trend suggests that the BQD module progressively captures affordance clues in 2D interactive videos and 3D scenes, which plays a critical role in affordance prediction. However, when L becomes too large, the network becomes overly complex, reducing efficiency and increasing the rate of incorrect predictions. Finally, balancing efficiency and performance, we set C, N, and L to 512, 50, and 6, respectively."}, {"title": "D. Performance Analysis", "content": "Different Videos with Same Affordance w.r.t. Single Scene: In real-world scenarios, multiple videos may depict the same affordance despite variations in interaction styles, object appearance and background environment. Consequently, our model is designed to consistently identify the same affor-dance regions within identical 3D scenes, regardless of these variations. As illustrated in Fig. 7 (a) and (b), despite being described by different videos, the affordance region in the corresponding 3D scene is correctly detected. These results demonstrate that our model is robust to these variations and effectively captures the common affordance knowledge shared among the videos.\nDifferent Videos with Different Affordance w.r.t. Single Scene. To better understand the surrounding environment, when given multiple interaction videos with different af-fordance, our model is expected to correctly localize the corresponding affordance regions in the 3D scene according to the specific interaction depicted in each video. Examples in Fig. 7 (c) and (d) show that our model accurately aligns the 2D interaction clues with 3D scene information, even when the affordance differs across videos. This further illustrates its capability to adapt to varying interaction clues while maintaining precise scene interpretation.\nLimitations. Our model has certain limitations, shown in Fig. 9. In complex 3D environments with numerous regions offering similar affordance, the fixed query number strategy employed in the BQD module may restrict the maximum number of affordance masks that can be predicted. Further-more, when affordance regions overlap with other objects, the method may struggle to delineate boundaries accurately, leading to over-prediction. This issue likely stems from the model's limited capacity to capture fine-grained geometric de-tails. To address these challenges, we plan to explore dynamic and adaptive strategies, as well as more advanced geometric modeling techniques in future work."}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "In this paper, we address the complex task of grounding 3D scene affordance from egocentric interactions, providing valuable interactive insights into the environment that can enhance embodied intelligence and applications like AR and VR. We first introduce a new dataset VSAD, curated from existing datasets, which pairs egocentric video with 3D scene affordance data, creating a pioneering benchmark for this"}]}