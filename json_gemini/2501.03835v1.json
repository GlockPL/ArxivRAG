{"title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification", "authors": ["Yindu Su", "Huike Zou", "Lin Sun", "Ting Zhang", "Haiyang Yang", "Liyu Chen", "David Lo", "Qingheng Zhang", "Shuguang Han", "Jufeng Chen"], "abstract": "Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendations, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity to the item embedding. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial scenarios. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Moreover, it has been successfully deployed in a real-world e-commerce platform, processing millions of product listings daily while supporting dynamic, large-scale attribute taxonomies.", "sections": [{"title": "1 Introduction", "content": "Product attribute values are critical components that underpin the functionality of e-commerce platforms. They provide essential structural information, aiding customers in making informed purchasing decisions and enabling various services, such as product listing (Chen et al., 2024), recommendation (Truong et al., 2022; Sun et al., 2020), retrieval (Huang et al., 2014), and question answering (Kulkarni et al., 2019; Gao et al., 2019).\nHowever, seller-provided attribute values are often incomplete or inaccurate, undermining the effectiveness of these applications. As a result, the automatic identification of product attribute values has become a critical research challenge.\nResearchers have explored Product Attribute Value Extraction (PAVE), which extracts spans from product profiles using Named Entity Recognition (NER) (Zheng et al., 2018) or Question Answering (QA) (Wang et al., 2020) models. The upper part of Figure 1 illustrates an example of NER-based PAVE. While this paradigm effectively extracts value spans, the outputs are raw subsequences. To produce standardized values, a normalization step (Putthividhya and Hu, 2011; Zhang et al., 2021) is required to map these spans to predefined formats, as shown in the lower part of Figure 1. However, implicit values, such as 6GB, cannot be directly extracted and must instead be inferred from context or prior knowledge."}, {"title": "2 Related Work", "content": "PAVE as Named Entity Recognition. PAVE can be formulated as NER by identifying subsequences in product texts as entity spans and linking them to attributes as entity types. Early methods, such as OpenTag (Zheng et al., 2018), trained individual models for each category-attribute pair. Subsequent efforts generalized this approach to support multiple attributes or categories. For instance, SUOpenTag (Xu et al., 2019) incorporated attribute embeddings into an attention layer to handle multiple attributes, while AdaTag (Yan et al., 2021) used attribute embeddings to parameterize the decoder. TXtract (Karamanolakis et al., 2020) introduced a category encoder and a category attention mechanism to tackle various categories effectively. Additionally, M-JAVE (Zhu et al., 2020) jointly modeled attribute prediction and value extraction tasks while also incorporating visual information. More recently, Chen et al. (2023) scaled BERT-NER by expanding the number of entity types to support a broader range of attributes.\nPAVE as Question Answering. The QA framework can also be adapted for PAVE by treating the product profile as context, attributes as questions, and value spans extracted from the context as answers. Wang et al. (2020) first introduced AVEQA for QA-based PAVE. Subsequent work extended this framework by incorporating multi-source information (Yang et al., 2022), multi-modal feature (Wang et al., 2022), and trainable prompts (Yang et al., 2023). Moreover, the question can be extended by appending candidate values as demonstrated by (Shinzato et al., 2022). Combining NER and QA paradigms, Ding et al. (2022) proposed a two-stage framework, which first identifies candidate values and then filters them.\nWhile NER- and QA-based paradigms have proven effective for PAVE, they struggle to identify implicit attribute values. Additionally, both paradigms rely on post-extraction normalization to standardize values, using either string-based methods (Putthividhya and Hu, 2011) or semantic-based techniques (Zhang et al., 2021). Furthermore, QA-based methods require processing a single product multiple times to handle multiple target attributes, leading to inefficiencies in large-scale settings. These limitations underscore the need for novel paradigms integrating extraction and normalization while addressing implicit values."}, {"title": "2.2 Product Attribute Value Identification", "content": "A straightforward approach is to frame PAVI as a multi-label classification problem over a finite set of values. Chen et al. (2022) trained a unified classification model that masks invalid labels based on the product category. However, a significant limitation of this classification-based paradigm is its inability to recognize OOD values not included in the training set. This limitation reduces its practicality in dynamic e-commerce environments, where new categories and values frequently emerge.\nGeneration-Based PAVI. Recent advancements in LLM have spurred the exploration of generation-based PAVI methods (Sabeh et al., 2024b). Some methods (Roy et al., 2021; Nikolakopoulos et al., 2023; Blume et al., 2023) construct attribute-aware prompts to generate values for each attribute individually. In contrast, others generate values for multiple attributes simultaneously, either in a linearized sequence format (Shinzato et al., 2023) or as a hierarchical tree structure (Li et al., 2023). Multimodal information has also been integrated into LLMs to identify implicit attribute values from product images (Lin et al., 2021; Khandelwal et al., 2023). More recently, Brinkmann et al. (2024) explored the use of LLMs for both the extraction and normalization of attribute values. Additionally, Zou et al. (2024) introduced the learning-by-comparison technique to reduce model confusion, and Sabeh et al. (2024a) investigated Retrieval-Augmented Generation (RAG) technologies for PAVI.\nAlthough generation-based methods can infer implicit and OOD attribute values from product profiles, they face several challenges in real-world scenarios. A key issue is the potential for the LLMs to produce uncontrollable or hallucinated outputs, a known limitation of LLMs (Huang et al., 2024). Additionally, these methods often rely on large, computationally intensive models to achieve strong performance, making them inefficient and costly for large-scale industrial deployment."}, {"title": "3 Taxonomy-Aware Contrastive Learning Retrieval", "content": "This section defines the PAVI task with an attribute taxonomy (\u00a73.1) and presents our retrieval-based paradigm for PAVI (\u00a73.2). We then detail the use of contrastive training with taxonomy-aware negative sampling (\u00a73.3) and an adaptive inference mechanism with dynamic thresholds (\u00a73.4). Figure 3 provides an overview of the approach."}, {"title": "3.1 PAVI Task Definition", "content": "PAVI is grounded in an attribute taxonomy that encompasses numerous product categories. For each category c, the taxonomy specifies a set of attributes $A_c = \\{a_1, a_2, . . . \\}$ relevant to products in that category, and for each attribute $a \\in A_c$, it provides a predefined set of standard values $V_a = \\{v_1, v_2, ...\\}$. Figure 2 illustrates this structure. For a given product item i, with its title t and description d, the item is assigned to a specific category e with associated attributes Ac. The objective of the PAVI task is to identify a relevant set of values $V \\subseteq V_a$ for each attribute $a \\in A_c$. The set V can take one of three forms: a singleton (\\{v\\}), multiple values (\\{v_1, v_2, . . . \\}), or an empty set (\u00d8) if no information about a is available in the product profile. Notably, a standard value may not always appear explicitly as a text span in t or d; it may be conveyed in other forms. When a value is not explicitly mentioned, such cases are referred to as implicit values."}, {"title": "3.2 Retrieval-Based PAVI", "content": "In a standard information retrieval setting, given a query, the objective is to retrieve a list of relevant documents from a corpus. Similarly, for PAVI, we treat the input item as the query and the attribute\ntaxonomy as the corpus, aiming to retrieve relevant attribute values as the output documents.\nTo achieve this, we use encoders to generate embeddings for both the item and its candidate values. The cosine similarity between the item embedding and each candidate value embedding is computed and normalized to the range [0, 1] to measure relevance. For each attribute, the candidate values are ranked based on their similarity scores, and the most relevant values are selected as the output set.\nTo effectively encode both the item and candidate values, we preprocess them as textual inputs and use a shared text encoder. For each item, we concatenate its title t and description d as the input text. Each candidate value v under an attribute a within category c is represented as a text prompt, such as \"A [phone (c)] with [brand (a)] being [Apple (v)]\". 1 We explore the impact of various value prompt templates in \u00a75.3."}, {"title": "3.3 Contrastive Training", "content": "Inspired by CLIP (Radford et al., 2021), we employ contrastive learning to train the shared encoder. Rather than relying on in-batch negatives, we compare each positive value with hard negative values from the same category and attribute in the taxonomy, providing a more challenging and precise training signal.\nFormally, the subset of values matched with the item is referred to as the ground truth value set, $V^+ \\subseteq V_a$. If no matched values exist for a given attribute, i.e., $V^+ = \\emptyset$, we assign a specific null value $v_0$ for this attribute as the positive value, i.e. $v^+ = v_0$. Otherwise, a positive value is ran-"}, {"title": "3.4 Adaptive Inference", "content": "During retrieval, relevance scores are assigned to every candidate values. To filter output values, a static threshold T can be applied to these scores. However, in real-world e-commerce platforms with a vast number of category-attribute pairs, using a single threshold across all pairs is often suboptimal. Moreover, defining a unique threshold for each pair is tedious or even impractical.\nTo address this, we introduce an adaptive inference method that uses dynamic thresholds to make cutoff decisions. As discussed in \u00a73.3, we add an explicit null value $v_0$ for each category-attribute pair, with its embedding learned during training. In the inference phase, we compute the similarity $s(i, v_0)$ between the item and the null value, using it as a dynamic threshold $T_a$ to filter out values with scores lower than that of the null value:\n$V^{pred} = \\{v | s(i, v) > T_a\\}$.\nSince most category-attribute pairs have exclusive\nvalues, meaning that each product can have at most"}, {"title": "4 Experiment Settings", "content": "To evaluate PAVI under the settings described in \u00a73.1, we benchmark our proposed method against baselines using both proprietary and public datasets with normalized values.2 Table 2 presents statistics of the attribute taxonomies and the datasets.\nEcom-PAVI. This dataset, derived from a real-world e-commerce platform, is designed to evaluate the scalability and generalization of PAVI methods. The attribute taxonomy in the e-commerce platform comprises 8,803 product categories, 26,645 category-attribute pairs, and 6.3 million category-attribute-value tuples. For our experiments, we sampled 1 million products for training, 10,000 for validation, and 10,000 for testing, ensuring that the samples span different time periods to reflect real-world scenarios. To ensure data quality, annotators manually verified the assigned product categories, discarded incorrectly categorized products, and selected the corresponding attribute-value pairs from the taxonomy as the ground truth.\nWDC-PAVE (Brinkmann et al., 2024). This dataset consists of products distributed across 5 categories. The training set includes 1,066 products and 8,832 product-attribute pairs, of which 3,973 have null values. The test set contains 354 products and 2,937 product-attribute pairs, with 1,330 null pairs."}, {"title": "4.2 Metrics", "content": "Since most attributes in the taxonomy are exclusive (i.e., each product can have at most one value per attribute), we evaluate PAVI methods using micro-averaged Precision@1,Recall@1, and F1@1 scores.\nFor each attribute, the ground truth is a set of values V from the taxonomy. If the ground truth is empty (\u00d8), a correct prediction (True Negative, TN) occurs when the model also predicts an empty set; otherwise, it is a False Positive (FP). When the ground truth is not empty, the model's top-1 output is a True Positive (TP) if it matches any ground truth value. Predicting an empty set in this case results in a False Negative (FN), while mismatched predictions are both False Positives (FP) and False Negatives (FN), as it simultaneously introduces an error and misses the correct value.3 Table 3 summarizes these outcomes. Final precision, recall, and F1 scores are computed by aggregating TP, FP, and FN counts across the dataset for a comprehensive performance evaluation."}, {"title": "4.3 Baselines", "content": "We evaluate our retrieval-based method TACLR against classification and generation baselines.4\nFor implementation details, refer to Appendix A.\nBERT-CLS. This baseline frames PAVI as a multi-label classification task, treating each category-attribute-value tuple as an independent label. The model is fine-tuned to predict matches, with label masking applied to exclude irrelevant labels for each category, following (Chen et al., 2022). The model outputs a probability distribution over values and selects the highest probability value for each attribute. If no probability exceeds a specified threshold, the prediction is set to be empty.\nLLMs. For generation-based baselines, we utilize state-of-the-art open-source LLMs, including Llama3.1-7B (Llama Team, 2024) and Qwen2.5-7B (Qwen Team, 2024). These models are initially evaluated in zero-shot and few-shot settings using a template adapted from (Brinkmann et al., 2024), which incorporates the category, attribute, and product profile along with detailed value normalization guidelines. We also fine-tune the LLMs on task-specific data to predict attribute values in JSON format. A greedy decoding strategy is applied to ensure reproducibility."}, {"title": "5 Results", "content": "Table 4 presents the performance comparison between our retrieval-based method TACLR and classification- and generation-based baselines on Ecom-PAVI and WDC-PAVE. On Ecom-PAVI, TACLR achieves the highest F1 score of 86.2, surpassing the fine-tuned Llama3.1, which obtains an"}, {"title": "5.2 Inference Efficiency", "content": "Table 5 compares the inference efficiency of PAVI methods under identical evaluation conditions. We employed a naive PyTorch implementation without speed optimizations and used the largest batch size that avoids out-of-memory errors. All experiments were conducted on a machine equipped with one NVIDIA V100 GPU.\nTACLR achieves a strong balance between performance and efficiency, with an inference time of 12.7 ms and a throughput of 63.2 samples per second. In contrast, generation-based methods, such as Llama3.1 (few-shot) and Qwen2.5 (few-shot), exhibit significantly longer inference times (124.8 ms and 98.4 ms, respectively) and lower throughputs (6.4 and 9.1 samples per second), highlighting the computational overhead of LLMs in high-load scenarios. While BERT-CLS delivers the fastest inference time and highest throughput, its inability to handle OOD values and limited capacity restrict its effectiveness in practical applications."}, {"title": "5.3 Analysis", "content": "Figure 4 compares the proposed taxonomy-aware sampling (\u00a73.3) with in-batch sampling across different sample sizes. As the number of sampled"}, {"title": "6 Conclusion", "content": "In this work, we present TACLR, a novel approach for retrieval-based PAVI. By formulating PAVI as an information retrieval task, TACLR inherently supports generalization to OOD values, infers implicit values, and produces normalized outputs. Building on this framework, TACLR employs contrastive training with taxonomy-aware sampling and adaptive inference with dynamic thresholds to enhance retrieval performance and scalability. Comprehensive experiments on proprietary and public datasets demonstrated TACLR's superiority over classification- and generation-based baselines. Notably, TACLR achieved an F1 score of 86.2% on the large-scale Ecom-PAVI dataset. Efficiency analysis further highlighted its advantage, achieving significantly faster inference speeds than generation-based methods. TACLR has been successfully deployed on a real-world e-commerce platform, processing millions of product listings daily and seamlessly adapting to dynamic attribute taxonomies, making it a practical solution for large-scale industrial applications."}, {"title": "7 Limitations", "content": "While TACLR demonstrates effectiveness in processing textual product profiles, it does not currently leverage multimodal information, such as images or videos. Multimodal data could provide valuable complementary context for attributes that are challenging to infer from text alone (e.g., visual attributes like color or texture). Incorporating multimodal capabilities may further enhance the model's ability to identify attribute values with greater accuracy and comprehensiveness."}, {"title": "A Implementation Details", "content": "We utilize pre-trained RoBERTa-base models (Liu et al., 2019; Cui et al., 2020), augmented with a linear projection layer, to encode both the item profile and the value prompt. The embedding dimension is set to 256. For each product-attribute pair, we sample up to 128 values, which include a null value, an optional positive value (no positive value is selected when absent for the product-attribute), and negative values sampled from the same category-attribute"}, {"title": "B Deployment", "content": "The proposed TACLR has been successfully integrated into key functionalities of an e-commerce platform, including product listing, search, recommendation, and price estimation. The system is designed to be highly scalable and efficiently process millions of products daily.\nIn the product listing process, TACLR automatically identifies attribute-value pairs from user-provided titles and descriptions, significantly reducing manual input and errors while improving the quality of structured information.\nFor product search and recommendation, the enhanced structured data provided by TACLR enables more accurate item retrieval, improving product matching with user queries and personalized recommendations.\nIn the area of price estimation, TACLR identifies key attributes that influence pricing, leading to more accurate price predictions. This provides both sellers and buyers with reliable, market-aligned information."}]}