{"title": "CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt", "authors": ["Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi"], "abstract": "Text classification is a fundamental task in natural language processing (NLP), and large language models (LLMs) have demonstrated their capability to perform this task across various domains. However, the performance of LLMs heavily depends on the quality of their input prompts. Recent studies have also shown that LLMs exhibit remarkable results in code-related tasks. To leverage the capabilities of LLMs in text classification, we propose the Code Completion Prompt (CoCoP) method, which transforms the text classification problem into a code completion task. Co-CoP significantly improves text classification performance across diverse datasets by utilizing LLMs' code-completion capability. For instance, CoCoP enhances the accuracy of the SST2 dataset by more than 20%. Moreover, when CoCoP integrated with LLMs specifically designed for code-related tasks (code models), such as CodeLLaMA, this method demonstrates better or comparable performance to few-shot learning techniques while using only one-tenth of the model size. The source code of our proposed method will be available to the public upon the acceptance of the paper.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) show commendable performance in both classification and generation tasks. However, to enhance their performance, fine-tuning task-specific data is imperative. The fine-tuning process requires collecting substantial, high-quality data pertinent to the domain task. Also, fine-tuning is expensive and may not be suitable for all situations (Kaddour et al., 2023). On the other hand, in-context learning techniques, such as few-shot learning, offer more flexibility (Brown et al., 2020). Unlike fine-tuning, these techniques do not require extensive domain-specific data and work only with a few examples (Brown et al., 2020). It makes them accessible for anyone to use during inference. It is important to emphasize that the effectiveness of these methods relies on the context and examples given as input.\nLLMs are trained on massive datasets that include diverse input styles. LIMA (Zhou et al., 2024) claims that if a model can learn the style of interaction with the user, it can reach superior performance in alignment and effectively communicate its knowledge to the user. This concept works in the reverse direction as well. When users interact with LLMs using prompt structures familiar to LLMs, the model demonstrates better performance. The efficacy of a prompt format for the model is contingent on its training data. Code-related datasets exist on the internet and are among the most critical resources for the pre-training phase of LLMs. For instance, LLaMA (Touvron et al., 2023a) is pre-trained on a substantial 328 GB dataset sourced from GitHub, constituting 4.5% of the overall model training data. Additionally, Google has mentioned that code-related data are one of the most important parts of their pre-training data for the Gemini model (Team et al., 2023). Choosing code-related datasets highlights the importance of teaching LLMs about coding style and code creation. Moreover, LLMs show good performance in code-related benchmarks. For instance, both LLaMA and LLaMA2 (Touvron et al., 2023b) have reported commendable results in code-related benchmarks. The proficiency of LLMs in code-related tasks can be leveraged for tasks beyond coding.\nTo enhance LLMs performance in classification tasks, we introduce the Code Completion Prompt (CoCoP) method in this paper. Our suggested method uses LLMs' capabilities in code-related tasks, particularly in code completion. Additionally, we employ the in-context learning in this method. CoCoP creates incomplete-code which consists of a few demonstrations in code format. Next, the LLM completes the code to determine the proper label for a user query. Thus, CoCoP is founded on the strengths of LLMs' capabilities in code completion and in-context learning.\nThe effectiveness of our method is evaluated across various classification datasets using LLaMA2 and CodeLLaMA models (Roziere et al., 2023), revealing superior performance compared to the few-shot learning method in classification tasks. Also, our experiments indicate that LLMs designed for code-related tasks (code models) demonstrate superior performance with this method compared to other LLMs. The source code of our proposed method will be available to the public upon the acceptance of the paper.\nOur research contributions include:\n\u2022 Introduced a novel text classification method using LLMs leveraging their code completion capability.\n\u2022 Outperformed the traditional few-shot learning method in text classification with LLMs.\n\u2022 Achieved comparable or superior performance using smaller code models like 7B and 13B model size compared to larger models such as the 70B model size across various text classification benchmarks through our approach.\nThe rest of this paper is organized as follows. Section 2 describes our proposed method. Next, Section 3 shows results of CoCoP performance and impact of each part of it. Section 4 describes related works. Limitations and future work are discussed in Section 6. Finally, Section 5 concludes the paper."}, {"title": "Method", "content": "Our approach, called CoCoP, revolves around transforming the classification task into a code completion task. We use it to improve LLMs's classification performance. An overview of this method is illustrated in Figure 1 (a). The few-shot learning technique requires examples, so we employ examples for the CoCoP method.\nThese examples are passed to Incomplete-Code Generator along with the query it should classify. The Incomplete-Code Generator is the main module of our method. This module accepts examples, query, and function name as inputs and produces incomplete-code as output. The function name is a hyperparameter that the user should pass to this module.\nIncomplete-Code Generator module performs two primary roles. First, it converts examples and their labels into code, transforming each example into one or more variable assignments. Then, utilizing a function call within the code, the label of each example is applied to it. This function call uses a function name provided by the user. Also, the name of the used function is crucial as it indicates the task process and guides the LLM in achieving better performance. So, this method can be customized for various classification tasks by modifying the function name and incomplete-code structure. Second, this module transforms the query into an incomplete function call. This function call has the variable assigned with a query as a first argument, and the second argument, the label, is not passed to it. The LLM is expected to complete this function call with a correct label.\nGiven the LLM's proficiency in code completion, it is expected to complete the code following the provided examples, producing an output with a format similar to those examples. We also expect the generated label to come from the labels provided in the examples. Then, the LLM's output is passed to the Label Extractor module. This module identifies a suitable label from the list of all labels at a particular position within the generated output. If the label Extractor finds an acceptable label, it is chosen as the output label; otherwise, the output indicates that no proper label is generated. For instance, Figure 1 shows the process for the sentiment analysis task.\nThe sentiment analysis task (SST2) (Wang et al., 2018) includes more than 67,000 training data and about 1800 test data for labelling. Three of them are randomly selected from the training data. These examples and the query are passed to the Incomplete-Code Generator module (Figure 1 (b)), which generates incomplete-code as shown in Figure 1 (c). In the incomplete-code, each example is a sentence transformed into a string variable assignment. Additionally, this module utilizes an apply_sentence_sentiment() function to apply the label to each sentence. The apply_sentence_sentiment as the function's name is a hyperparameter that the user passes. This function takes two arguments: the variable corresponding to the example sentence as the first argument and the example's label as the second argument. Similarly, this module processes the query but does not complete the apply_sentence_sentiment() function with the target label which does not exist. Hence, the generated code is incomplete (see Figure 1 (c)). Furthermore, in cases where multiple sentences exist for each input, such as in Natural Language Inference (NLI) tasks, each sentence can be expressed in a separate variable assignment. The simplicity and adaptability of this method make it suitable for a diverse set of classification tasks.\nFinally, the output of the Incomplete-Code Generator is passed to the LLM. LLM completes this code by adding a label as a second argument to the apply_sentence_sentiment function call, as illustrated in Figure 1 (d). Then, the output of LLM is passed to the Label Extractor. This module extracts the assigned label for target input and presents it as the result of classification (see Figure 1 (e)). To validate the efficacy of this method, we assess its performance across various classification datasets."}, {"title": "Results", "content": ""}, {"title": "Evaluation setup", "content": "We use LLaMA2-chat (Touvron et al., 2023b) and CodeLLaMA-Instruct (Roziere et al., 2023) as our baseline models. We use these models with temperature 0 to make our results reproducible. Furthermore, we assess the efficacy of our method across various classification datasets. We examine the GLUE (Wang et al., 2018) dataset for text classification, selecting SST2, CoLA, and MRPC for binary classification tasks. For the multi-class classification task, we use the SNLI (Bowman et al., 2015) dataset. To generate few-shot examples, we use instances from the training data of these datasets. We use two examples from each class within SST2, CoLA, and MRPC datasets to generate few-shot examples in incomplete-code. Also, for SNLI, we utilize a single example for each class to generate a few-shot examples of incomplete-code generation for this task."}, {"title": "Method performance", "content": "In this subsection, we evaluate the CoCoP method performance in different datasets and assess its capability to improve classification tasks. We also present SoTA model results which have been trained on training data from selected datasets. Therefore, it is expected that SoTA models show better results than using general-purpose LLMs."}, {"title": "Few-shot learning vs. CoCoP", "content": "We used two methods for classification: few-shot learning as a base method and CoCoP. We used LLaMA2-chat with 7B, 13B, and 70B variations for evaluation. Table 1 shows the results of this experiment. In LLaMA2-7B-chat, CoCoP only performed better in MRPC and SNLI, but in LLaMA2-13B-chat, it performed better in SST2, CoLA, and SNLI. Also, LLaMA2-70B-chat showed better performance in SST2, CoLA, and SNLI and similar performance in MRPC. These results show that CoCoP can improve performance in classification tasks. Also, this impact becomes more remarkable when the model size increases because it improves performance in the larger models."}, {"title": "Code models vs. base models", "content": "Considering our methodology, which emphasizes leveraging the capabilities of LLMs for code completion, we specifically concentrate on code models. These models are fine-tuned on tasks related to coding and are trained with substantially larger amounts of code data (Roziere et al., 2023; Luo et al., 2024). So, we used CodeLLaMA-Instruct as a code model for the CoCoP approach. This model is based on LLaMA2 and improves the coding capability of the base model. Subsequently, we conducted a comparative analysis between the CodeLLaMA-based models and the baseline LLaMA2 model. We report results for the 7B, 13B, and 34B code models.\nLLaMA2 and CodeLLaMA models are available in 7B and 13B sizes, allowing for a comparative analysis of CoCoP's impact on their performance. Table 1 presents the results of this evaluation. For the 7B size, CodeLLaMA with CoCoP outperformed LLaMA2 when employing CoCoP and few-shot learning techniques across all four datasets. Notably, using CoCoP with CodeLLaMA significantly enhanced accuracy, exceeding 20% for SST2 and SNLI datasets, compared to using LLaMA2 with CoCoP and few-shot learning methods. In the 13B model size, CodeLLaMA with Co-CoP outperformed LLaMA2 with CoCoP and few-shot learning in SST2, CoLA, and SLI. However, LLaMA2 with few-shot learning showed slightly better performance with a 0.8% improvement in MRPC. These results show that using CoCoP with a code model has the potential to improve performance for text classification tasks significantly. This method also works well with small code models. Note that each experiment was conducted ten times with a different set of examples for both the few-shot and CoCoP methods, and the standard deviations were reported accordingly. The results demonstrated that the CoCoP with code model exhibited lower standard deviations than the other approaches. This finding suggests that CoCoP is more robust to variations in example contexts and exhibits greater reliability."}, {"title": "Few-shot learning vs. CoCoP in code models", "content": "This subsection evaluates the difference between CoCoP and few-shot learning methods in CodeLLaMA as a code model. Table 1 shows the results of these evaluations. CoCoP performed better in all four datasets in all model sizes, including 7B, 13B, and 34B. These results mean that performance improvement through CoCoP with CodeLLaMA is independent of this model's capability to do classification or learning through demonstrations and comes from the CoCoP method. Another surprising result from this experiment is that few-shot learning performance decreases in CodeLLaMA by increasing model size."}, {"title": "Models size", "content": "In Subsection 3.2.1, we observed that employing the CoCoP method shows better accuracy than the few-shot technique when using LLaMA2 models of the same size. Additionally, Subsection 3.2.2 highlighted that utilizing the CodeLLaMA-Instruct model shows better accuracy results than the LLaMA2 model when using the CoCoP method, despite both being in the same model size. This subsection underscores the assessment that employing the CoCoP method with a smaller code model can yield better results than larger LLaMA2 models without CoCoP. As the model size grows, the inference cost increases. Therefore, it is essential to attain improved results by using smaller models.\nAs can be seen in Table 1, using the CodeLLaMA-34B-Instruct with the CoCoP method yielded better results than utilizing the LLaMA2-70B-chat model with the few-shot method across SST2, COLA, and MRPC datasets. The results obtained using the CodeLLaMA-34B-Instruct in the SST2 and CoLA datasets exhibited about 10% improvement in classification accuracy. Additionally, it demonstrated comparable performance in the SNLI dataset. This suggests that the CoCoP method with the CodeLLaMA model can achieve better or comparable results than the traditional few-shot method despite using a twice-as-small model.\nAnother unexpected finding is that when employing CodeLLaMA-7B-Instruct and CodeLLaMA-13B-Instruct with the CoCoP method, we observed better results in the SST2 and COLA datasets than utilizing LLaMA2-70B-chat with a few-shot technique. Moreover, these models exhibit results comparable to those of the LLaMA2-70B-chat with a few-shot method in the MRPC and SNLI datasets with only about 4% difference accuracy. These observations suggest that achieving high performance in classification tasks is feasible with smaller LLMs. These results suggest a trend towards developing prompt techniques for smaller language models that can maintain performance levels comparable to larger models."}, {"title": "Providing additional information in the prompt", "content": "Additional information can be added to the generated prompt based on CoCoP's prompt generation in code format. For instance, CoCoP can show all possible labels for classification with a list of strings in the code format. We assess the impact of adding additional information to generate incomplete-code in CoCoP.\nTo guide the LLM in determining the label, we consider two types of additional information: the names of all possible labels and the task's name. In the first scenario, the Incomplete-Code Generator adds a line at the beginning of the prompt. This line shows a variable assignment that assigns a list of strings to a variable. Each string represents a possible label. For the second scenario, the Incomplete-Code Generator module adds the name of each task at the beginning of the prompt as a code comment. Figure 2 shows the results.\nResults showed that adding a list of labels slightly improved SST2, MRPC, and SNLI, but accuracy decreased in CoLA. On the other hand, adding a task's name decreased accuracy in all datasets. These results show that adding extra information has the potential to improve CoCoP performance, but the type of extra information is critical."}, {"title": "Ablation Study", "content": "Our proposed method incorporates certain characteristics which may influence its performance. To assess the impact of these characteristics, we conducted a series of experiments using CodeLLaMA-7B and CodeLLaMA-13B models on the MRPC and SST2 datasets. The choice of these code models was motivated by the demonstrated efficacy of the CoCoP method in enhancing their performance. Specifically, we selected SST2 as a dataset where CoCoP has shown significant improvements, while MRPC represents a task where CoCoP's performance gains are not as substantial, albeit still present. Our analysis focused on two key characteristics: the structure of the incomplete code and the type of the examples used in the incomplete code prompt. We conducted separate experiments to isolate and evaluate the impact of each of these characteristics."}, {"title": "CoCoP prompt structure", "content": "The CoCoP method uses the concept of few-shot learning by using demonstrations. These demonstrations can be arranged in different orders. To investigate this, we conducted experiments using two positive examples and two negative examples with varying orders. Figure 3 illustrates the results for both datasets with two model sizes. These results showed that CodeLLaMA-7B is less sensitive to example orders than CodeLLaMA-13B. Specifically, in MRPC, the order of examples has a greater influence on the F1-score of the negative class than accuracy and the positive class. Hence, this ordering can significantly affect the performance of the negative class."}, {"title": "CoCoP examples type", "content": "The type of demonstrations can impact performance in few-shot learning (Min et al., 2022). Therefore, we conducted experiments to evaluate the impact of different types of examples in the CoCoP method. By Keeping the incomplete-code structure fixed, we varied the type of examples used. This entails maintaining the order of examples based on their labels while allowing for changes in the sentences themselves. Seven different types of example sentences were employed:\n\u2022 Normal: Basic version.\n\u2022 Reverse: Replaces negative class with a positive sentence and positive class with a negative sentence.\n\u2022 Without examples: Only shows labels without any accompanying sentences.\n\u2022 Full random: Replaces sentences with random ones from other datasets.\n\u2022 Random: Replaces sentences with random ones from its own dataset.\n\u2022 Only positive: Replaces all sentences with positive ones.\n\u2022 Only negative: Replaces all sentences with negative ones.\nFigure 4 shows the results for MRPC and SST2. In SST2, CodeLLaMA-7B was less sensitive to the type of examples than CodeLLaMA-13B. However, in MRPC, experiments with \u201cWithout Examples\" and \"Full random\" types indicated that the F1-score for the negative class is close to zero. This shows that the model lacks sufficient context about the negative class when no examples are provided, and the type of examples becomes crucial. Notably, employing examples from its dataset yields improved results."}, {"title": "Related Work", "content": ""}, {"title": "Text classification by LLMs", "content": "The introduction of the transformer model (Vaswani et al., 2017) has led to the development of numerous models based on this architecture. One of the most influential models is BERT, which is pre-trained on a vast amounts of data and can be fine-tuned for downstream tasks, such as classification (Kenton and Toutanova, 2019). Following the BERT model, the advent of LMs like GPT-2 (Brown et al., 2020) and T5 (Raffel et al., 2020b) has further expanded the capabilities of transformer-based models in various downstream tasks.\nTwo primary approaches have emerged for leveraging these models in classification tasks: fine-tuning and prompt engineering (Kaddour et al., 2023). Fine-tuning involves further training the pre-trained LLM on a specific task and dataset, allowing the model to adapt its parameters to the target domain. Prompt engineering, on the other hand, involves carefully crafting natural language prompts to elicit desired responses from the frozen LLM without further fine-tuning. Given the large parameter sizes of LLMs such as GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), and LLaMA2 (Touvron et al., 2023b), we employ the concept of prompt engineering. This approach enables seamless adaptation to various LLMs without necessitating additional resources for fine-tuning."}, {"title": "LLMs for code (code models)", "content": "Some LLMs are fine-tuned on specific tasks to enhance their performance in specialized domains. One of the most significant tasks for LLMs is code-related tasks, such as code generation and code completion. For instance, Codex by OpenAI (Chen et al., 2021) and PalmCoder by Google (Chowdh-ery et al., 2023) are examples of proprietary, closed-source code models. On the other hand, open source code models, such as CodeLLaMA (Roziere et al., 2023), CodeGemma (Team, 2024), Wazir Coder (Luo et al., 2024), and InCoder (Fried et al., 2022), are also available. We call these models code models. While code models are primarily employed for code-related tasks, the CoCoP method leverages their capabilities beyond the conventional scope of code processing. Specifically, we harness the models\u2019 prowess in code-related tasks to perform a classification task, thereby extending their utility to a broader range of applications."}, {"title": "Performing tasks with LLMs through code format", "content": "Several previous works have leveraged the capabilities of LLMs in code-related tasks to tackle problems beyond the coding domain. For instance, Code as Policy (CaP) (Liang et al., 2023) employed LLMs to generate code policies for robots. Another example is TidyBot (Wu et al., 2023), which utilized LLMs\u2019 code completion capabilities to perform robotic household cleanup tasks. Both approaches presented their problems as incomplete code to the LLM, which then completed the code to accomplish the desired tasks.\nAdditionally, there are works like Chain of Code (Li et al., 2023) and Program of Thought (Chen et al., 2023) that use the Chain-of Thought (CoT) (Wei et al., 2022) concept and apply it to LLMs\u2019 code-related capabilities to enhance the models\u2019 reasoning performance. The CoCoP leverages the capabilities of LLMs in code-related tasks, similar to previous works . However, our method applies this capability to text classification problem."}, {"title": "Conclusion", "content": "Based on LLMs\u2019 excellent performance in code-related tasks, we proposed the Code Completion Prompt (CoCoP) method. This method transforms classification tasks into code completion task and leverages few-shot learning. Our evaluation of this method across many classification datasets demonstrated its efficacy in enhancing classification results. For instance, CoCoP can improve the accuracy of the LLaMA2-70B-chat model in SST2 and CoLA benchmarks about 10%.\nFurthermore, we employed pre-trained and fine-tuned LLMs on code-related datasets (code models), and these models exhibited superior performance compared to basic models when using the CoCoP method. CodeLLaMA-34B-Instruct with CoCoP outperformed LLaMA2-70B-chat with about 10% accuracy improvement in SST2 and CoLA and 2% improvement in MRPC. Remarkably, smaller models like CodeLLaMA-7B-Instruct, when combined with the CoCoP method, attained comparable results to that of the LLaMA2-70B-chat model using the few-shot technique. For instance, CodeLLaMA-7B-Instruct with CoCoP outperformed LLaMA2-70B-chat in SST2 and CoLA and also showed comparable accuracy in MRPC and SNLI only with less than 4% different accuracy. These results showed better or comparable accuracy with one-tenth the model size compared to basic models"}, {"title": "Limitations and Future Work", "content": "This study uses the capability of LLMs for code completion in classification tasks. Much research work can be done based on this research. While we precisely assess the CoCoP method in some important text classification datasets, it is conceivable that the proposed idea can be applied to a broader spectrum of text classification tasks. Furthermore, several other classification tasks could be reconfigured as text classification problems and addressed using this method.\nBased on the success of the CoCoP method in the text classification tasks, additional research is required to evaluate the effectiveness of this method in generation tasks. Future work can focus on transforming the reasoning and generation tasks into a code completion problem and solving them using the CoCoP method.\nThis study has demonstrated that LLMs designed for specific tasks, such as code generation, can leverage their capabilities in domains beyond their original intended purpose. The findings suggest that future research endeavors could explore the potential of repurposing domain-specific LLMs for novel applications. By adapting and fine-tuning these models on relevant datasets, it may be possible to harness their strengths and transfer their knowledge to new domains, thereby expanding the frontiers of their utility."}]}