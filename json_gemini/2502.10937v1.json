{"title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention", "authors": ["Chengshuai Zhao", "Zhen Tan", "Chau-Wai Wong", "Xinyan Zhao", "Tianlong Chen", "Huan Liu"], "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively Simulates Content Analysis via Large language model (LLM) agents. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.", "sections": [{"title": "Introduction", "content": "Content analysis is a critical research method in various disciplines. It breaks down complex and unstructured text into numeric categories based on theory-driven rules, offering a systematic and quantitative approach to interpreting sophisticated information. Particularly, in social science, content analysis is usually labor-intensive and time-consuming. It often requires a team of researchers to manually annotate sizable datasets, conduct comprehensive discussion, and iteratively refine coding rules (a.k.a. codebook) in multiple rounds to ensure reliability and validity of findings, as illustrated in Figure 1. This artificial process, while rigorous, presents two challenges: First, it relies heavily on domain-specific knowledge and individual scientists, potentially introducing subjectivity and limiting generalizability. Second, the substantial human resources demanded by content analysis tasks make it difficult to scale, especially as the volume of digital data expands exponentially.\nRecent years have witnessed significant progress in artificial intelligence (AI), especially with the advent of large language models (LLMs). LLM agents emerge as a versatile tool across a wide range of domain-specific tasks. However, content analysis tasks present unique challenges, requiring a fine-grained understanding of social science principles, human-like collaborative interaction, and rule-base iterative refinement, which limits the effectiveness of agent systems derived from other tasks.\nIn this paper, we propose a novel multi-agent"}, {"title": "Related Work", "content": "Content Analysis. Content analysis has long been a foundational method in the social sciences and humanities, providing a structured approach to converting qualitative text into quantitative data. Recently, content analysis has significantly advanced the understanding of complex social issues, ranging from political polarization to emotional contagion and group dynamics. These traditional methods rely on manual annotation by human coders, who use predefined rules in the codebook to categorize text, often iteratively refining their coding schemes in multiple rounds of discussions. Although manual content analysis provides robust and theory-driven insights, it remains labor-intensive, time-consuming, and prone to subjectivity. Furthermore, as the volume of digital text increases, scaling traditional methods to accommodate larger datasets has become increasingly challenging. The advent of powerful AI and LLM offers an automated and more scalable solution.\nMulti-agent Systems for Social Science. Multi-agent systems (MAS) have become increasingly prevalent in computational social science, modeling social phenomena through agent individuals or groups with predefined behaviors or decision-making rules. Recent MAS explore to simulate human-like deliberation for more nuanced decision-making such as data interpretation. However, existing systems often lack dy-"}, {"title": "Traditional Content Analysis in Social Science: A Preliminary", "content": "Social scientists conduct content analysis by manually annotating textual data to uncover potential patterns and insights. A group of (two or more) social scientists S first develop a codebook C that contains a set of coding rules grounded in relevant social science theories and contextualized within the given text corpus. Guided by the codebook, each social scientist then independently labels a small set (e.g., 10\u201320) of text entries T: \\(Y = S_{code}(T, C)\\), where Y \u2208 {0,1,2, ...} are labels. Later, they meet to discuss and resolve inconsistencies \\(y^{(i+1)} = S_{Discuss} (C^{(i)}, y^{(i)})\\), where an updated codebook with well-refined and more specific coding rules may be proposed for better annotation: \\(C^{(i+1)} = S_{Refine}(C^{(i)}, y^{(i)})\\). This whole process iterates for multiple (e.g., 3\u20135) rounds until the discussion converges. The finalized codebook is applied by each social scientist to code all text entries in the corpus D. In general, content analysis in social science centers on two objectives: (I) precisely annotating all text entries and (II) crafting a well-honed codebook with distinct coding rules."}, {"title": "The Proposed Framework: SCALE", "content": "We introduce SCALE as a framework that mirrors the key phases of real-world content analysis\u2014text coding, collaborative discussions, and dynamic codebook evolution. The method unfolds in four primary steps.\nCoder Simulation. Prior to the content analysis task, we set up both the LLM agents and the corresponding codebook. As illustrated in Figure 2a, we begin by configuring N LLM agents \\(A = \\{a_i\\}_{i=1}^N\\), each emulating a seasoned social scientist through a system prompt that incorporates N distinct personas, \\(P = \\{p_i\\}_{i=1}^N\\). These personas\u2014derived from real-world social scientists aside from their names\u2014ensure authentic role-playing. Depending on the specific content analysis task (detailed in Section 5.1), we initialize a corresponding codebook C that either starts with N' human-expert predefined rules (shown in red) \\(C = \\{r_i\\}_{i=1}^{N'}\\) or as an empty set \u00d8 prompting agents to propose and iteratively refine the codebook from scratch. For simplicity, each rule is tailored to cover a single scenario, enabling the categorization of text into a discrete class.\nBot Annotation. LLM agents convert text entries into numerical categories by applying theory-informed rules from the codebook. Each agent is assigned an identical batch of B text entries from the text dataset and works autonomously to classify each entry into a discrete category. Mimicking the independent coding approach of human researchers, these LLM agents adhere strictly to the codebook guidelines, which is facilitated by a prompt.\nAgent Discussion. In this phase, agents engage in collaborative discussions to resolve discrepancies in their coding outputs. Due to the initial ambiguity of the evolving codebook and the distinct personas embodied by each agent, it is not uncommon for agents to generate differing annotations for the same text, which actually mirrors the subjective nature of real-world content analysis. Whenever an agent's coding diverges from the consensus, the agents initiate a structured and up-to-K-round discussion. During each round, they update their annotations along with explanations based on peer opinions until they converge on a unanimous decision or reach the maximum number of discussion rounds. Once a text entry is finalized, the agents will move to the next entry.\nCodebook Evolution. In this stage, agents refine the codebook by incorporating insights from their discussions. As noted earlier, the initial rules can be ambiguous, overlapping, or insufficiently detailed to cover all categories. To address these issues, we introduce an iterative codebook evolution process grounded in domain expertise. Specifically, the evolution approach offers two strategies: one enriches existing rules by adding clarifying examples and explanations, while the other allows for adding, removing, or modifying rules to dynamically adjust the set of categories. In practice, agents first propose an updated codebook, then engage in multiple rounds of discussion to refine it until consensus is reached. The finalized codebook subsequently guides the next task cycle. Importantly, agents may also retain the current codebook if no"}, {"title": "Human Intervention", "content": "We further augment SCALE framework by integrating diverse human intervention modes that empower domain experts to provide targeted feedback and instructions. Depending on the scope that human experts can control, human intervention can categorized as targeted and extensive intervention. Further, human intervention can be formulated as collaborative or directive intervention based on the role the human experts play.\n* Targeted intervention. The scope of intervention is limited to the agent discussion phase. The process while with less human oversight.\n* Extensive intervention. Human intervention can be applied to both discussion and codebook evaluation. It may slow automation and raise costs, but it ensures that AI discussion and workflow remain closely aligned with expert insights.\n* Collaborative intervention. Human experts are involved as collaborators. LLM agents may either accept or reject feedback and suggestions from human experts, which fosters an interactive and cooperative discussion loop.\n* Directive intervention. Under this mode, human experts serve as absolute authority. LLM agents must adhere to every instruction provided, thereby establishing a highly prescriptive and unequivocally top-down approach.\nIt is noted that, in practice, we can combine various scopes and roles of expert control to enable custom interventions (e.g., targeted-collaborative and extensive-directive intervention). Different human interventions are implemented by delicately crafted prompts elaborated in Appendix D.5."}, {"title": "Experiments", "content": "Our experiments leverage five real-world datasets, each meticulously annotated and validated by social science experts. These datasets are organized into seven distinct tasks that encompass both multi-class and multi-label classification challenges. Table 1 summarizes their key characteristics, with further details provided in Appendix B.1."}, {"title": "Experiment Settings & Metrics", "content": "Experiment Settings. Our multi-agent system is built on GPT-4O and GPT-4O-mini. While alternative backbones (e.g., Gemini, Claude) are available, our focus is on assessing whether LLM agents can simulate sophisticated social science tasks rather than comparing various models. For each backbone, we explore four prompting strategies: vanilla, chain-of-thought (COT), tree-of-thought (TOT), and self-consistency.\nThe prompts for CoT and ToT can be found in Appendix D.6. The identifiers for GPT-4O and GPT-4O-mini are gpt-40-2024-05-13 and gpt-4o-mini-2024-07-18, separately. We simulate a real-world scenario in content analysis by setting agent number N = 2, text batch size B = 20, and discussion rounds K = 3. We consider and discuss more hyper-parameters in Section 5.4.1.\nMetrics. We define the following evaluation metrics for our content analysis tasks. For various classification tasks, we employ standard multi-class classification accuracy and define the multi-label classification accuracy as \\(ACC = \\frac{1}{Hamming Loss}\\). Furthermore, we use the agreement rate-defined as the proportion of text entries where all agents concur\u2014to assess the level of consensus during discussions. All experiments were conducted over 10 independent runs, with the average results reported to ensure robustness. Further metric details are provided in Appendix B.2."}, {"title": "Superior Performance of SCALE", "content": "We first assess the performance of automatic content analysis without human intervention. (I) As presented in Table 2, SCALE achieves satisfactory results with an average accuracy of 0.701 across a diverse range of tasks and models. (II) Notably, different prompting techniques offer distinct benefits: compared to the vanilla model, self-consistency and tree-of-thought (TOT) prompts boost labeling accuracy by 2.31% and 6.51%, respectively. However, in certain cases (e.g., CN-NP), chain-of-thought (COT) prompts lead to a significant performance drop. We attribute this decline to the challenging nature of tasks with ambiguous coding rules that introduce greater subjectivity, where the"}, {"title": "Human-Intervened Content Analysis", "content": "Further, SCALE can be augmented with four types of human interventions, as detailed in Table 3. (I) The labeling results with human intervention achieve an average accuracy of 0.872, demonstrating superior performance. When compared to automatic content analysis performance, the model with human intervention shows an average improvement of 12.6%, validating the effectiveness of our proposed human intervention method. (II) Moreover, interventions in the directive mode prove more effective than those in the collaborative mode, leading to a 13.1% increase in coding accuracy. Similarly, models with extensive interventions generally outperform ones with targeted interventions, yielding a 15% average improvement. This aligns with intuition, as more domain knowledge from human experts can be involved by intervening in the large scope (i.e., extensive intervention) of agent behaviors in a mandatory manner (i.e., directive mode). (III) The benefits of human intervention also vary by task. For example, the CSE task sees a 20% improvement, while the PIS task shows a 5.4% relative gain likely because the CSE task is more domain-specific and subjective, thus more responsive to expert insights. Additional results"}, {"title": "Extra Investigations and Case Studies", "content": "To answer Q1, we analyze how SCALE enhances content analysis tasks by considering the number of texts, discussion rounds, and agents.\nNumber of texts. We first evaluate the influence of the number of texts B (as shown in Figure 3a). Our findings indicate that a moderate B (e.g., 10 and 20) produces the best accuracy. When B is small (e.g., 1), agents frequently refine the codebook, resulting in unstable coding outcomes. However, when B is large (e.g., 40), results become more stable, but the overall performance decreases due to less frequent codebook evolution.\nNumber of discussion rounds. Next, we examine the effect of the number of discussion rounds K, as illustrated in Figure 3b. We observe that SCALE achieves better performance with higher rounds (e.g., 3, 4, or 5), as more rounds of discussion enhance the consensus between agents, thereby improving coding accuracy. Importantly, setting K to 0 (i.e., no discussion phase) results in a significant drop in accuracy for several tasks (e.g., BCD-D and CN-NP in Appendx C.3), highlighting the importance of inter-agent discussions.\nNumber of agents. Finally, we assess the impact of the number of agents N, as depicted in Figure 3c. Generally, increasing the number of agents improves coding accuracy, as more agents bring diverse perspectives, fostering more comprehensive discussions. When N is set to 1, SCALE degrades as a single-agent system, where a single agent performs the coding task without collaboration. As expected, this setup yields the worst performance, verifying the effectiveness of multi-agent design in the proposed SCALE framework."}, {"title": "How does the discussion between LLM agents impact coding results?", "content": "To answer Q2, we conducted a discussion analysis using both qualitative statistics and illustrative examples. Our findings reveal that inter-agent discussions substantially boost consensus\u2014improving the average agreement rate by 41.1% across all seven tasks and enhancing overall content analysis accuracy by 15.4%, as shown in Figure 4a. Similar trends were observed for GPT-4O-mini agents, as illustrated in Figure 7 and Appendix C.4.\nA practice example of sentimental analysis (multi-class classification task) can be found in the PIS dataset: a tweet such as \"Hey @SamsungMobileUS, bf has a recalled #GalaxyNote7. Can't find a replacement S7 Edge in Orlando, FL area. Any ideas or help please?\" initially resulted in conflicting sentiment annotations (neutral versus negative). After three rounds of collaborative discussion, both LLM agents agreed on a neutral sentiment-aligning with the ground truth. The complete example is showcased in Appendix E.1.\nHowever, the benefits of discussion can be marginal when agents remain firmly entrenched in their views. In some datasets (e.g., FWPE and"}, {"title": "How reliable are the codebooks proposed by LLM social scientists?", "content": "One of the key objectives of our method is to refine the codebook for annotating large-scale text. To answer Q3, we analyze the codebook evolution in SCALE. We observe that LLM agents can enhance codebooks in non-structural ways by adding clarifying details and examples. For example, during the PIS codebook update (shown in Appendix E.3), Agent #1 proposed incorporating examples for each sentiment category (positive, neutral, negative) to ensure consistent interpretation, while Agent #2 initially preferred the original version. After discussion, the final codebook merged Agent #1's detailed examples with Agent #2's simplicity, achieving a balance of clarity and reliability. This iterative process mirrors core content analysis practices by fostering convergence in agent judgments.\nHowever, the agents were less adept at adjusting"}, {"title": "To what extent can LLM agents simulate content analysis?", "content": "To answer Q4, we examine the complete workflow of SCALE on the NES (multi-label classification) task. Here, two agents mimic social scientists to classify cancer narrative events into multiple categories (e.g., prevention, detection, treatment, survivorship), which is reported in Appendix E.4.\nText coding. Both agents independently applied the codebook rules to annotate the presence of one or more cancer narratives. For example, when annotating the text \u201cWhen I hear that some women feel too afraid to go for a mammogram...\", both agents agreed on labeling it as detection. In contrast, for the text \u201c...After that I will have 25 days of radiation... But through it all, I have had great support from my family and friends,\" Agent #1 focused solely on treatment, while Agent #2 identi-\""}, {"title": "Conclusion", "content": "In this paper, we propose SCALE, a novel multi-agent framework to simulate the rigorous practice of content analysis in social science via LLMs. Guided by domain knowledge and social science theory, SCALE is delicately crafted and augmented with human interventions. Comprehensive experiments demonstrate that SCALE enables large-scale and high-quality annotations while producing rational codebooks, offering an innovative potential for future social science research. Future work will explore methods to inject domain knowledge into off-the-shelf LLM frameworks."}, {"title": "Limitations", "content": "While SCALE demonstrates strong potential in automating content analysis, there are several limitations that present opportunities for future research.\nAlgorithmic Bias and Fairness. Despite incorporating human intervention, which may help to mitigate bias, LLMs remain prone to perpetuating biases present in the training data. This may compromise content analysis outcomes and raise ethical concerns in social science applications. Future work could explore advanced bias mitigation strategies\u2014such as fairness-aware training or the integration of demographic and behavioral data\u2014to potentially enhance model impartiality.\nInter-agent Discussion Overhead. The collaborative discussions among LLM agents, while effective in harmonizing divergent outputs, incur considerable computational overhead. This inefficiency becomes especially problematic when agents fail to reach consensus after the maximum number of discussion rounds. Streamlining the process by reducing the number of agents involved or limiting discussion rounds might alleviate the computational burden with the sacrifice of performance.\nHuman Expertise Bottleneck. Although diverse human interventions boost overall performance, reliance on human experts, particularly under extensive intervention mode, creates a scalability bottleneck. A promising direction is to enable LLM agents to learn from human behavior and logic underlying the interventions, thereby reducing the dependency on expert input over time."}, {"title": "Ethical Statement", "content": "Our research involves the use of large language models (LLMs) to simulate social scientist role-play. We are acutely aware of the ethical challenges inherent in AI systems, particularly regarding bias, data privacy, and transparency. To address these concerns, we have taken the following actions: (I) Bias mitigation. We acknowledge that LLMs can inadvertently reproduce or amplify biases present in their own training data. To counteract this, we incorporate structured inter-agent discussions and human oversight, ensuring that diverse expert perspectives inform the refinement of our outputs. (II) Data privacy. Demographic information used to create the role-playing personas was derived from real-world data but was fully anonymized. All personally identifiable details\u2014including names, addresses, and workplace information\u2014were re-"}, {"title": "Use of Generative AI", "content": "To enhance clarity and readability, we utilized OpenAI o1 exclusively as a language polishing tool. Its role was confined to proofreading, grammatical correction, and stylistic refinement-functions analogous to those provided by traditional grammar checkers and dictionaries. This tool did not contribute to the generation of new scientific content or ideas, and its usage is consistent with standard practices for manuscript preparation."}]}