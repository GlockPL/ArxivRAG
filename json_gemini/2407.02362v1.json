{"title": "Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA", "authors": ["Xuqi Zhu", "Huaizhi Zhang", "JunKyu Lee", "Jiacheng Zhu", "Chandrajit Pal", "Sangeet Saha", "Klaus D. McDonald-Maier", "Xiaojun Zhai"], "abstract": "Modern Neural Network (NN) architectures heavily rely on vast numbers of multiply-accumulate arithmetic operations, constituting the predominant computational cost. Therefore, this paper proposes a high-throughput, scalable and energy-efficient non-element-wise matrix multiplication unit on FPGAs as a basic component of the NNs. We firstly streamline interlayer and intra-layer redundancies of MADDNESS algorithm, a LUT-based approximate matrix multiplication, to design a fast, efficient scalable approximate matrix multiplication module termed \"Approximate Multiplication Unit (AMU)\". The AMU optimizes LUT-based matrix multiplications further through dedicated memory management and access design, decoupling computational overhead from input resolution and boosting FPGA-based NN accelerator efficiency significantly. The experimental results show that using our AMU achieves up to 9x higher throughput and 112\u00d7 higher energy efficiency over the state-of-the-art solutions for the FPGA-based Quantised Neural Network (QNN) accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent years have been notable progress in the field of hardware-based inference accelerators implementing Neural Networks (NNs) for AI [1]. As such AI application scenarios continue to broaden, addressing diverse performance-oriented requirements for varying tasks emerges as a new challenge [2], [3], [4]. To handle the diversity of AI applications and the sophistication of task scenarios, researchers are exploring the design of performance-scalable multipliers in NN accelerators executing AI applications at varying scales and performance requirements for simplification. For example, an IoT application for facial recognition utilising Floor of Log achieves high efficiency in terms of energy utilisation and storage while maintaining an acceptable decrement in accuracy. On the other hand, some health and activity monitoring applications require high accuracy at the cost of high energy consumption in inferring user activities by utilizing sophisticated sensors, features, and complex classification algorithms that lead to higher energy consumption. Another example is the use of any-precision NN that prioritises accuracy at the cost of an increased execution time or energy consumption [5], [6], [7]. Together, these sources highlight the complex trade-offs that NNs undergo to balance model complexity, accuracy, execution time, and energy consumption. This emphasises the necessity of thorough analysis and optimisation to strike the right balance for particular applications and environmental factors.\nNumerous studies have explored resource-efficient, high-throughput solutions on NN accelerators for diverse range of AI applications and complex task scenarios [8]. Since matrix multiplication is a major computational overhead in NN inference tasks, an optimized matrix multiplication unit can improve the overall accelerator performance. Several studies used quantised matrices to simplify multiplication computation and memory requirements. Authors in [9] proposed FP-BNN, a binarized neural network, by replacing the exact multiplication with XNOR and Pop-count operation. Similarly, authors in [10], [11] substituted the matrix multiplication operation with a computational unit, a.k.a. Matrix Vector Activation Unit (MVAU), using XNOR and Pop-count operations. By sacrificing a bit of accuracy performance, the MVAU can achieve much faster processing speed and lower resource utilization. Since bit operations are well-suited to FPGA hardware logic, MVAU has been recongnized as an effective solution for accelerating BNN inference [12]. Wang et al. [13] introduced LUT-Net, aiming to enhance the logic density of XNOR gate-based matrix multiplication. Zhang et al. [14] developed FracBNN, a novel BNN architecture designed to mitigate accuracy degradation caused by low-precision quantization in XNOR gate-based matrix multiplication. Blalock et al. [15] proposed the MADDNESS, a LUT-based matrix multiplication algorithm, to speed up matrix multiplications while using fewer compute resources. MADDNESS leverages a form of Product Quantization (PQ) [16], [17] that organizes datasets into a fixed number of clusters and learns substitutes for the sample vectors within each cluster. This approach achieves notable acceleration with a minor loss of accuracy in NN inference tasks. This method has been further leveraged in NN inference works [18], [19].\nEfficient accelerators depend not only on individual matrix multiplication units but also on the design of the overall computing architectures. For example, FINN [10], [11], an FPGA-based end-to-end deployment tool for QNN accelerator, leverages streamlined technique [20] to achieve a certain level of customization on parallelism and resource utilization by introducing matrix folding strategies. Similarly, HLS4ML [21], FlexCNN [22], Angel-Eye [23], DNNBuilder [24] and HybridDNN [25] provided alternative frameworks for an FPGA-oriented end-to-end deployment tool for NN accelerator. Beyond targeting FPGA platforms, HEP-BNN [26] is a BNN accelerator framework, that can automatically generate an efficient layer-to-device mapping for GPUs and CPUs. The studies in [18], [19], [27] illustrated an ASIC accelerator for NN inference applications on x86 server CPU.\nHowever, the solutions proposed in the aforementioned studies did not address the challenges posed by varying problem sizes in matrix multiplication operations, leading to degraded accelerator throughput and energy efficiency, especially with larger problem sizes. First, element-wise arithmetic operations serve as the primary performance bottleneck for matrix multiplications, leading to $O(nm)$ computational complexity, where nm is the matrix size. Although some methods, such as folding strategy [20], offer a scalable performance through tuning parallelism and resource utilization, XNOR gate-based matrix multiplication still requires a considerable amount of element-by-element arithmetic operations. This will then result in inevitable computation and data access cost, affect the efficiency and prevent matrix multiplication units such as MVAU [10], [11], from achieving better Pareto optimal on throughput and resource utilization. Secondly, the previous approaches did not address software-hardware co-design optimization sufficiently for the specific algorithm's atypical logic behaviours. The authors in [18], [19] used an approximate matrix multiplication algorithm having a completely different logic behaviour from the exact matrix multiplication operations, which introduced a significantly larger parameter size and atypical memory access behaviours to the NN models and NN accelerator. These pose a challenge in designing an efficient hardware accelerator.\nThe limitations of prior works motivated us to design a multiplier that can decouple the multiplication computation overhead from the size of the input feature map, enabling flexible tuning of the resources, accuracy and energy efficiency of the multiplier, ultimately improving NN accelerator performance. In this paper, we thus design an FPGAbased vector-matrix multiplication unit, termed the Approximate Multiplication Unit (AMU), which forsakes element-by-element arithmetic operations to decouple the multiplication computation overhead from the resolution of the input feature map (i.e., the problem size). This approach allows the AMUbased NN accelerator to achieve significant gains in throughput and energy efficiency, compared to the typical element-wise matrix multipliers. The AMU is an alternative arithmetic operation of MVAU, which can be used to build a resourceaccuracy-efficiency scalable and latency invariant accelerator substituting the element-by-element arithmetic operation with the assistance of LUT-based storage.\nThe contributions of this paper are summarised as follows:\n\u2022 We firstly streamline inter-layer and intra-layer redundancies of the MADDNESS-based matrix multiplication to enable a fast, efficient scalable approximate matrix multiplication on an FPGA-based QNN accelerator.\n\u2022 We eliminate element-by-element arithmetic operations of matrix multiplication by introducing three optimization strategies (i.e., I/O data pruning, feature map reorganisation, and parameter compression) on MADDNESSbased matrix multiplication to decouple the multiplication computation overhead from the problem size, which significantly improves the performance of the MADDNESSbased matrix multiplication.\n\u2022 We explore the hardware design space on FPGA to optimize atypical memory allocation and access arising from the unstructured pruning on MADDNESS-based matrix multiplication.\n\u2022 The proposed AMU-based NN inference accelerator achieves up to 9.9\u00d7 throughput and 112\u00d7 energy efficiency (GOPS/W) over the state-of-the-art solutions for FPGA-based Quantised Neural Network (QNN) accelerator with the same level of NN model complexity.\nThe overview of the proposed AMU is discussed in Section III, while Section IV presents the performance comparing between AMU and prior works, Section V discusses the conclusion and future challenge of our work."}, {"title": "II. APPROXIMATE MULTIPLICATION: MADDNESS [15]", "content": "This section firstly introduces the approximate multiplication method MADDNESS [15], preparatory to discussing our algorithm. MADDNESS is a LUT-based approximate multiplication algorithm, requiring offline training as shown in Fig. 1. (b). In the offline training, MADDNESS generates codebooks which will be used in the online multiplication stage."}, {"title": "A. Approximate multiplication setting for neural networks", "content": "The problem domain we are addressing in approximate multiplications on the two matrices, A and W, is outlined as follows:\n$\\begin{equation}\nf(A,W) = A \\times W + E,\n\\end{equation}$\nwhere $A \\in R^{n\\times m}$, $W \\in R^{m\\times d}$, and the error matrix $E \\in R^{n\\times d}$. The $||E||_2$ should be small enough according to the accuracy requirements.\nFor neural networks, A can be represented as follows:\n$\\begin{equation}\nA^T \\triangleq [a_1, a_2,..., a_n],\n\\end{equation}$\nwhere $a_i \\in R^n$. Each $a_i$ can be considered as the neuron values of the input layer in a fully connected layer. We utilize a pre-trained neural network (i.e., the weights W are known).\nFig. 1. (a) describes a fully connected layer in a neural network, where $w_{j,k}$ represents a weight linking the $j^{th}$ neuron in the previous layer to the $k^{th}$ neuron in the current layer. The signal $s_j$ entering each ReLU unit in Fig. 1.(a) follows Eq. (3):\n$\\begin{equation}\ns_j = \\sum_i(w_{i,j} \\times a_i) = (W^T a_{in})_j,\n\\end{equation}$\nwhere $a_{in}$ is the neuron values in the input layer, $(\\cdot)_j$ is the j-th component of a vector, $n_{in}$ is the number of neurons at the input layer (e.g., the length of the vector a), $W =[W_1,W_2,\\dots, W_{n_{out}}]$, where $w_i=[W_{i,1}, W_{i,2}, ..., W_{i,n_{in}}]$, and $N_{out}$ is the number of neurons at the output layer. The signals $s_j$ are input signals for activation functions (i.e., ReLU). We name a series of input neurons $[a_1,a_2,\\dots, a_m]^T$ as an input feature vector $a_{in}$ in this paper. Therefore, our approximate multipliers are focused on the multiplications between neurons and weights in a fully connected layer based on Eq. (3)."}, {"title": "B. Offline training", "content": "We refer to \"offline training\" in this paper as the process used to compute partial dot product values in LUTs, distinct from the typical training method involving backpropagation used for training neural networks. In the first stage of the offline training, MADDNESS generates codebooks. Each codebook consists of a set of prototypes learned from training data. For example, using the i-th training sample, $a_i \\in R$ can be divided into $C \\in R^{n/C}$ dimensional data, where C is an integer that allows n/C = $d_{sub}$ to be an integer. In this case, there exist C codebooks according to C sub-dimensions of training data. We use the notation $a_i^{(c)}$ for the c-th sub-dimensional vector in the i-th training sample:\n$\\begin{equation}\na_i^{(c)} = [a_{1}^{(c)}, ..., a_{d_{sub}(i)}^{(c)}].\n\\end{equation}$\nTo simplify without loss of generality, let us denote a $a_i^{(c)}$ for $a_i^{(c)}$ in this sub-section. MADDNESS uses a heuristic clustering strategy based on a decision tree to find a series of prototypes in each codebook. The clustering utilizes the hyperplane bisection method. For instance, the 1-st hyperplane generates $2^1$ sub-spaces. Each sub-space is divided by its own hyperplane. Therefore, utilizing the h-th $2^{h-1}$ hyper-planes I create $2^h$ sub-spaces in the c-th codebook to classify the c-th sub-dimensional vectors across all training samples into $2^h$ sub-spaces. Considering all past buckets, the number of buckets in total becomes $n_b = \\sum_{i=1}^{I} 2^i$, where I is the number of split indexes. Numbering buckets follows from left to right. For example, the first hyperplane generates the left-side subspace (i.e., $a_{j1} \\le v_1$, 1st bucket) and the right-side sub-space (i.e., $a_{j1} > v_1$, 2nd bucket). We denote jj', for the j'-th split index.\nThe prototype for each bucket in a codebook can be considered as a representative vector for all sub-dimensional vectors assigned to that bucket. The prototype is found using the least squares algorithm. We use the notation $p_k^{(c)}$, to indicate the prototype for the k-th bucket in the c-th codebook generated by the split index jj'.\nFor example, in Fig. 1. (b), the second element in sub-dimensional vectors, $a_2$, is chosen for a hyperplane bisection. The hyperplane, $a_2$ = $v_0$ (Split value), splits sub-dimensional vectors into two sub-spaces across all training samples (i.e., the space where $a_2 > v_0$ and the other space where $a_2 < v_0$.). Each split index divides the space into two sub-spaces. For example, the first split index splits the space into two subspaces and the next index divides each subspace into two sub-spaces, generating four sub-spaces in total. Similarly, $n_h$ dimensions generate $2^{n_h}$ sub-spaces. Each subspace is referred to as a bucket in this paper. The $2^{j'-1}$ split values for the j'-th split index are determined heuristically so that they minimize the sum of squared errors between the $2^{j'}$ chosen prototypes and sub-dimensional vectors of training samples residing in the $2^{j'}$ buckets split by $2^{j'-1}$ split values in $V_{j'} = [v_1, v_2, ..., v_{2^{j'} -1}]^T$.\nThis process is performed in order from k = 1 to k = $n_b$. For example, $V_1 = [v_1]$ is found for k = {1,2} (i.e., Buckets 0 (k=1) and 1 (k=2) in Fig. 1.(b).). After that, the first element in $v_2$ is found for k = {3,4} and the second element is found when k = {5,6}, and so on. In Fig. 1.(b), the first element in $v_4$ is found for k = {15, 16} (Buckets 0 and 1 in the last level of the tree) and the 8-th element is found for k = {29,30} (Buckets 14 and 15 in the last level of the tree). The binary decision tree finds suitable values that can split two children's buckets from a father's bucket with the lowest accumulated Sum of Squared Error (SSE) in MADDNESS. The initial prototypes are found by averaging the sub-dimensional vectors residing in the associated buckets. The least squares algorithm is used to find the final prototypes $p_k^{(c)}$, by refining the initial prototypes to minimize residuals further.\nOnce all prototypes are found, MADDNESS generates the LUTs containing dot-product values. Each row in a LUT represents a dot-product value between a prototype and a sub-dimensional known vector (e.g., a sub-dimensional weight vector). For example, in Fig. 1. (b), all possible partial dot product results between every prototype in each codebook and the sub-vectors of each dimension on the weight matrix (i.e., Sub-Wi,j) are computed and recorded in LUTs offline. Therefore, a LUT consists of a table having the size of $2^I \\times C$. If the number of split indexes I = 4, each row element contains the dot product value between the prototype in one of the last level buckets and one of the C sub-dimensional weight vectors."}, {"title": "C. Online multiplications (Inference)", "content": "Online multiplications assume that the patterns in the test dataset are similar to the training dataset. Under this assumption, we can utilize the LUTs generated from the offline training stage for the online multiplications.\nIn the online multiplication stage, a test sample is first divided into C sub-dimensional vectors. A partial dot product between a sub-dimensional test vector and a sub-dimensional weight vector is immediately found by referring to an LUT. For example, each grey cell in Fig. 1. (c) represents the partial dot product value between a prototype and a subdimensional weight vector. Since the first sub-dimensional test vector (i.e., the first block in Input Fmap) is mapped with the first prototype in CodeBooko and uses \u20181' stored in the LUT for a partial product. Only the values at the four split indexes in a sub-dimensional test vector are referred to for the mapping procedures in Fig. 1. (c). Likewise, the second subdimensional vector is then mapped with the third prototype in the second codebook, and so on. Since a LUT contains a partial dot product value associated with each sub-dimension, the final dot product value can be thus found by adding the values in all grey boxes.\nThe right side in Fig. 1. (c) shows how to find the best prototype for each sub-dimensional vector in a test sample using the values in the split indexes. The binary decision tree generated in the offline training stage is leveraged to seek the best prototype. For example, the decision tree has the split index information and their values. At the outset, the first split index and its corresponding value within a sub-dimensional test sample determine the assignment of that vector to one of two buckets (k = 1 or 2). If the value at the split index of the test sample is less than the split value, it belongs to the left child bucket (k = 1). Subsequently, the second split index is assessed. If the value at the second index of the test sample exceeds the split value, it is assigned to the right bucket in the second level of the tree (k = 4). This binary tree search process persists until reaching the leaf nodes. Hence, I binary decisions are necessary for mapping. This can be realized by utilizing I bits as a LUT address. In Fig. 1. (c), for instance, the binary decisions derived from the split indexes of the first sub-dimensional vector are \u201cXXXX\u201d (equivalent to \u201c0000\u201d), while those from the last sub-dimensional vector are \u201cOOOO\u201d (equivalent to \u201c1111\u201d), serving as the LUT address for a partial dot product value. This prototype search is called encoding [15]. Therefore, MADDNESS can approximate matrix multiplications in the inference stage using LUTs, each representing a series of partial dot products."}, {"title": "III. APPROXIMATE MULTIPLICATION UNIT", "content": "To make the accelerator performance independent from the resolution of the input feature map (i.e., problem size), we designed a matrix multiplier, AMU, which improves MADDNESS [15] using the following three optimisation strategies: Input/Output data pruning, reorganising feature maps, and compressing LUT bespoke to hardware."}, {"title": "A. Challenges posed by MADDNESS", "content": "Although the original MADDNESS algorithm streamlines processing by eliminating the need for online matrix multiplication operation, it still requires element-by-element operations to produce all output elements, thereby introducing unnecessary inter-layer information transfer and redundant intra-layer parameters. For example, a matrix-vector multiplication in Eq. (3) requires $n_{out}$ dot products (i.e., MADDNESS is required to compute all $s_j$ in Eq. (3).), failing to alleviate the time and space complexities associated with the input feature map resolution (i.e., problem size). However, we note that if multiple MADDNESS matrix-vector multiplication units are used sequentially, the number of dot products required can be significantly reduced, hence improving computational speed significantly. This leads to our I/O data pruning scheme.\nFurthermore, the significant volume of redundant input/output feature maps and LUT content impedes the implementation of MADDNESS on FPGA-based accelerators with limited resources. As shown in Fig.1. (c), each column of the weight matrix has a corresponding LUT with $2^I \\cdot C$ cells, where I indicates the number of split indexes and C is the number of codebooks. However, not all LUTs can be used for obtaining the key information (i.e., the value on the split index) for the next layer. Therefore, the original MADDNESS algorithm executes numerous superfluous operations and generates massive amounts of redundant data that negligibly contribute to NN inference while occupying a huge volume of storage.\nIn order to completely decouple the time and space complexities from the input feature map resolution, and mitigate the redundancy in the generated results within layers and resource occupancy, we introduce three specific optimisation strategies: Input/output data pruning, Feature map reorganisation and Parameters compression in Section III-B1, III-B2 and III-B3 respectively, which makes AMU possible to achieve much lower latency and fewer operations for NN inference acceleration. At the same time, these optimisations also decouple the resource utilisation and Initiation Interval (II) of AMU from the problem size. In other words, by tuning the number of codebooks C and the number of prototypes $2^I$ for each codebook we can improve resource utilisation, accuracy and efficiency but without additional latency cost."}, {"title": "B. Algorithm Optimisation", "content": "We apply three design optimisation strategies to make AMU more efficient and compact: 1) Input/Output (I/O) data pruning, 2) Feature map reorganisation and 3) parameter compression, which brings two direct benefits: faster processing speed and fewer parameters for storage as shown in Fig. 2. Furthermore, they dispel the need to compute all $s_j$ in Eq. (3), thereby eliminating the element-by-element operations in MADDNESS. This helps minimise the impact of the problem size on resource utilisation and the pipeline throughput (II) of the AMU.\n1) I/O data pruning: In Fig. 1. (c), the original MADDNESS employs only I \u00d7 C split indexes and their split values to identify prototypes from C codebooks within a given input feature map. This means that a majority of the feature map elements remain unused for approximate matrix multiplication. Our I/O data pruning technique leverages this property to conserve FPGA resources. The I/O pruning technique retains solely the vital information including the split indexes and split values at the split indexes (e.g., coloured blocks in the codebooks as depicted in Fig. 2) prior to computing approximate matrix multiplication. In the context of inference tasks, outputting entire feature maps becomes redundant using MADDNESS since approximate matrix multiplication at the subsequent layers only requires information on split indexes to encode prototype IDs. Therefore, our pruning strategy considers the feature map structure from both the current and next approximate matrix multiplications, as all split indexes and their spilt values are predetermined during the offline training stage.\nNow, suppose the first AMU receives an input feature map with N codebooks (each codebook has I split indexes) and the next AMU is required to receive a feature map with M codebooks (each codebook has O split indexes) as Fig. 3 (a). In the first AMU, I \u00d7 N block readings are required to search optimal prototypes across N codebooks, and O \u00d7 M block writings are required to write dot-product outcomes to one of O blocks across M codebooks at next AMU so that the N encoded codebooks can be used for aggregating O \u00d7 M block in next AMU. The blocks in unused indexes of the input and output feature map (marked as white blocks in Fig. 2) are ignored. Due to fewer blocks required to output, the AMU only processes the approximate dot product between the input feature map and a row of weight O \u00d7 M times. The associated compression ratios with a required LUT size are shown in Eq. (5) to (7).\n$\\begin{equation}\nInputCompressionRatio_i = \\frac{I_i N_i}{U_{i-1}},\n\\end{equation}$\n$\\begin{equation}\nOutputCompressionRatio_i = \\frac{O_i M_i}{U_i},\n\\end{equation}$\n$\\begin{equation}\nLUTsize_i = N_i \\cdot 2^{I_i},\n\\end{equation}$\nwhere $U_{i-1}$ and $U_i$ represent the number of block readings and the number of block writings of the i-th layer using unpruned MADDNESS, respectively. Assuming the i-th layer has an input feature map (1, $U_{i-1}$ = 512) and a ($U_{i-1}$ = 512, $U_i$ = 512) weight, the original MADDNESS shown in Fig. 2 need computes $U_i$ elements for output and next layer also need read $U_i$ elements, while this optimisation can theoretically reduce 93.75% read/write and processing operations according to Eq. (5) and (6). However, the AMU leveraging I/O data pruning with $I_i = O_i = 4$ and $N_i = M_i = 8$ in i-th layer, merely 32 blocks are received and sent.\n2) Feature map reorganisation: I/O pruning fragments the entire feature map, dispersing valid information (i.e., the values of the split indexes) unevenly across each codebook. This fragmented feature map poses challenges for designing the high parallelism in AMUs. To overcome this issue, we restructure the pruned feature map. As shown in Fig. 3. (a), the values on the split index used in i-th encoding are assembled into the i-th package. Each package comprises N blocks from the N corresponding codebook. Additionally, the output feature map needs to be restructured to match the input structure of the next AMU, facilitating the cascading of the AMUs to implement a dataflow-based accelerator.\n3) Parameters compression: According to Fig. 2, the original MADDNESS algorithm needs to load all U LUTs for computing approximate matrix multiplication between the input feature map and U weight vectors of the weighing matrix to obtain the entire output feature map. However, because of the I/O pruning, the AMU only needs O \u00d7 M LUTs, where the O \u00d7 M \u00ab U. As shown in Fig. 2, Wo can not be used to obtain the values on the desired split indexes of the output feature map, in contrast, Wi and Wu are employed to compute the required value on split indexes of the output feature map, therefore, LUTo that holds the percomputed partial dot-product of the input feature map and Wo (i.e., N Codebooks \u00d7 Wo) is abandoned, while LUTi and LUTu are loaded. According to Eq. (6), for the AMU with $I_i = O_i = 4$ and $N_i = M_i = 8$ in i-th layer, and i-th layer has ($U_{i-1}$ = 512, $U_i$ = 512) weight matrix, the original MADDNESS will generate 512 LUTs, each of them owning $M \\times 2^{I} = 128$ cells. However, only $O_i \\times M_i = 32$ LUTs are required when applying parameter compression, which can reduce 93.75% of the LUTs. To parallel compute M blocks and send O package sequentially, the used LUTs are reshaped and allocated to O \u00d7 M array in practice."}, {"title": "C. AMU Architecture Overview", "content": "AMU is based on MADDNESS architecture consisting of an allocator, encoder, and aggregator. AMU, however, optimizes each computing block according to the three proposed optimization techniques. The three processing units provide allocating blocks, finding prototypes, fetching partial results and packing the multiplication result function respectively. In the following subsections, we will introduce the three processing units respectively."}, {"title": "1) Allocator:", "content": "Fig. 3.(a) illustrates how an AMU layer receives a pruned input feature map with I \u00d7 N elements from the previous layer via I packets, where N is the number of codebooks at the input feature map, and sends O packets (each containing M blocks) as a pruned output feature map with O \u00d7 M pixel to next layer, where N (or M) and I (or O) denotes that there are N (or M) codebooks in input (or output) feature map (i.e., Fmap) and each codebook contains I (or O) split indexes. The main function of the allocator is to unpack the received package and allocate them to N parallel encoders. As shown in Fig. 3.(b), due to feature map reorganisation in Section III-B2, I Q \u00d7 N-bits-packages (i.e., a pkg) will be arrived successively. For each package, the allocator employs a bitwise operator to take out N Q-bitsvalues (i.e., N blocks). Here Q determines the quantisation level of activation or weight in the inference networks."}, {"title": "2) Encoder:", "content": "The N blocks are unpacked to N corresponding encoders, each encoder is assigned to encode the ID of a certain codebook (e.g. CodeBooki), which is used to look up the dot product result of the corresponding prototype as well as the weight sub-matrix in the aggregator. As shown in Fig. 3(c), the encoder reads blocks serially, for each round, the encoder takes the current block from the input package and the corresponding split value to identify the next round ID. Here the $2^{I-1}$ split values are allocated to a sequential array. In the $j^{th}$ round the split value on $Address = ID_{j-1}+Bias$ is taken to compare with the current block, where $ID_{j-1}$ indicates the last round ID and $Bias = 2^{j-1}$. Once the encoder has processed all blocks, it can obtain an ID indicating the prototype address of CodeBooki."}, {"title": "3) Aggregator:", "content": "In Fig. 3.(d), the N = 8, I = 4 and O = 4 case is demonstrated. The aggregator is on standby until all IDs are ready. For each CodeBook, there is a distinct prototype ID ranging from 0b0000 to 0b1111 ($2^I$ prototypes in total). Consider the ID 1101 is the prototype address of the 7th codebook (i.e., CodeBook6), the cells on all LUTs located in (13, 6) are picked as the partial dot product result of the input feature map and weight. All picked cells (marked as grey cells) are summed up and bias is added if required. They are then passed through successive thresholding (i.e., a quantised NN operator, equivalent to scaling, batch normalisation, and uniform-quantised activation) [20] to obtain O \u00d7 M blocks and pipelined to send O packages to the next AMU. The aggregator contains O \u00d7 M LUTs obtained from the offline training stage, each LUT has I\u00d7N approximate partial product results. By tuning O/I and M/N we can scale the LUT of the aggregator to achieve the desired resource-efficiency-accuracy for the AMU-based NN inference acceleration."}, {"title": "D. Hardware Design Space Exploration", "content": "Unlike GEMM using element-by-element arithmetic operations, the AMU uses unstructured pruning and utilises atypical memory accesses to achieve matrix multiplications. The sparse matrices derived from the pruning result in atypical memory access behaviour, posing an additional challenge for designing efficient accelerators. As shown in Fig. 3, the main memory accesses occur in the encoding and aggregating stage. Meanwhile, to store the partial dot-product of all prototypes and weight matrix, the LUT can largely occupy the main memory resource. Therefore, the memory allocation of LUTs and the design of aggregator memory accesses are essential to improve the resource utilisation and throughput of AMUs.\n1) Memory allocation design: To properly leverage the storage resources of the device, we analysed the array structure of the LUT. As depicted in Fig. 4, each codebook can only be identified as a specific prototype, requiring only one partial dot product to be read from each column to compute a single pixel of the output feature map (i.e., there is only one grey cell in each column of the LUT). Based on this feature, we allocate the LUTs to various dual-port ROMs (termed as 'Group' partition), each containing every $2 \\times S \\times O \\times E$-columns partial dot product from LUTs, where $S \\in \\{1,2,3,... [N/2]\\}$ and $E \\in \\{1,2,3,... M\\}$. For example, an AMU with N input codebooks and M output codebooks can be assigned $M \\times N/S \\times E$ dual-port ROM. While larger S or E enhances memory utilisation by minimizing unused regions in each ROM, it incurs $a \\times O \\times S \\times E$ latency in fetching all dot product results from a dual-port ROM. Here, we suppose a is the average clock cycle delay of an operation in Fig. 4.\n2) Memory accesses pipeline design: From the other perspective, the throughput is always desired for designing an efficient accelerator. A well-designed memory access architecture can provide the promised II and consequently can significantly improve the throughput of the AMU. As shown in Fig. 4, the II of the AMU is the time delay in cycles, between the launch of processing successive input feature maps. Two crucial bottlenecks prevent AMUs from achieving lower II (i.e., the red double arrows in Fig. 4). Specifically, (1) sequentially allocating and encoding packages results in a \u00d7 I clock cycle delay, and (2) aggregating partial dot product from $\\alpha \\times M \\times N/S \\times E$ dual-port ROM results in a \u00d7 S \u00d7 E clock cycle delay. The final II of the AMU is the maximum delay between two bottlenecks.\nTo address the bottleneck of (1), where data dependencies exist not only between allocating and encoding packages but also between encoding packages of an input feature map, we can assemble all packages and transmit them within a single operation. This enables the encoder to process successive input feature maps in a pipeline. For the bottleneck of (2), caused by dual-port ROM read blocking, there is $a \\times O \\times S \\times E$ clock cycle delay between two successive input feature maps. To compress this delay, we can decrease S or E. In an extreme scenario, we can allocate every column or even every cell of LUTs to distributed ROMs or separated registers (termed as 'Complete' partition) so that all the grey cells can be fetched at once without read conflicts, which enables fully unroll aggregating processes to achieve the best throughput."}, {"title": "E. AMU-based QNN accelerator", "content": "1) Design Principles: Based on the study from the [19], it was found that the accuracy of a model decreases as more layers are replaced by approximate matrix multiplication as in MADDNESS [15] and Vanilla PQ-based multiplication [19]. The accumulating errors through these approximate multiplications are difficult to eliminate. Even when using a differentiable implementation as shown in Halutmatmul [18], the accuracy of Halutmatmul-based NN decreases statistically as the network gets deeper. However, both authors [19], [18] found that using exact matrix multiplication for the first hidden layer results in a considerable accuracy gain in accuracy for the NN model that uses approximate matrix multiplication. For clarification, we use the term \u201cexact matrix multiplication\" for the matrix multiplication using single precision arithmetic (i.e., 32-bit floating point arithmetic) in the paper. Thus, the configuration of the first layer in the AMU-based NN can greatly impact the accuracy of the NN inference.\nTo quantify the impact of the first hidden layer configuration on the accuracy of the AMU-based NN inference, we investigate the degradation of accuracy with increasing NN depth for different first-hidden layer settings of AMU-based MLP on the MNIST dataset. The MLP contains repeated hidden layers (a hidden layer includes: 256 \u00d7 256 linear transfer, batch normalization, activation, and dropout) and a classification layer (256 \u00d7 10 linear transfer). As shown in the left part of Fig. 5, all hidden and classification layers are replaced into (I = 4, N = 16) AMU, except the first hidden layer with 4 different settings: using more codebooks (I = 4, N = 32), more prototypes (I = 5, N = 32), exact matrix multiplication (the up boundary of the AMU when N = 28 \u00d7 28, $I \\to \\infty$) and default AMU (N = 16, I = 4).\nThe AMU-based MLP using exact matrix multiplication for the first hidden layer shows the best accuracy, compared to the other AMUs. The AMU with larger LUT (based on Eq. (7)) exhibits superior accuracy. For example, Fig. 5 shows the accuracy ranges: 74.6% - 65.4% for I = 5, N = 16 vs 69.1% - 58.8% I = 4, N = 16 and 78.5% - 70.5% for I = 4, N = 32 vs 69.1% - 58.8% for I = 4, N = 16. Additionally, given a LUT size budget, using more codebooks (78.5% - 70.5% for I = 4, N = 32, LUT size = 16\u00d732 items) can own better accuracy"}, {"title": "AMUNet-0:", "content": "to achieve high efficiency, we assign fewer prototypes (i.e., $I_i = 3$, where $i \\in [0,3]$ and $N_0 = 64$, $N_1 = 32$, $N_2 = 16$ and $N_3 = 8$) for each layer to restrict the power consumption."}, {"title": "AMUNet-1:", "content": "to achieve balanced accuracy and efficiency, we assign regular prototype numbers and codebook numbers (i.e., $I_i = 4$, where $i \\in [0, 3]$ and $N_0 = 64$, $N_1 = 32$, $N_2 = 16$ and $N_3 = 8$) for each layer to strike a balance between accuracy and efficiency performance."}, {"title": "AMUNet-2:", "content": "to achieve high accuracy, we assign more codebooks (i.e., $I_i = 4$, where $i \\in [0,3]$ and $N_0 = 72$, $N_1 = 36$, $N_2 = 36$ and $N_3 = 18$) for each layer to improve the accuracy.\nThe number of output package $O_i$ and the number of blocks in each output package $M_i$ is the same as $I_{i+1}$ and $N_{i+1}$, where $i < 2$ represents the $i^{th}$ AMU in the NN accelerator. All AMU units in three accelerators apply the 'Complete' partition and all accelerators will generate $O_3 = 10$ output package and each output package has M = 1 block (i.e., 10 16-bit values for final soft-max operation). In addition, a FINN-generated MVAU-based accelerator [10] and a LUTNetbased SFC accelerator [29] are implemented as the baseline:"}, {"title": "MVAU-based NN accelerator (MVAU):", "content": "this accelerator has $SIMD_0 = 392$, $SIMD_1 = 128$,i \u2208 [1,3], and $PE_i = 128$, i \u2208 [0,2], $PE_3 = 5$. The total fold F = 4 according to Eq. (9) for each layer i. Theoretically, it will have the same level throughput $FPS = 2 \\times 10^7$ (according to Eq. 8) as an AMU-based accelerator when the clock frequency $F_{clk} = 100 MHz$."}, {"title": "Unrolled LUTNet (LUTNet):", "content": "this accelerator has K = 4 and P = 0, where K indicates the accelerator can perform an arbitrary Boolean operation on up to 4 inputs, and P 0 means unrolled architecture [29].\nAll accelerators are tested on a ZCU104 board with the clock frequency 100 MHz. As shown in Fig. 9 and Table II, with increasing the LUTs utilisation, the AMU-based accelerator gains 5.2% accuracy improvement. However, the growth in resource occupancy introduces more power consumption, resulting in a 5.5 \u00d7 106 FPS/W drop in efficiency from AMUNet-0 to AMUNet-2. The comparison among AMU-based NN accelerators reveals that adjusting the hyperparameters I and N for each AMU enables us to achieve diverse efficiency-accuracy trade-offs. This capability enables the AMU to achieve superior efficiency while maintaining acceptable accuracy or to attain higher accuracy within the constraints of the available LUT resources. For example, the AMUNet-2 achieves 9.1\u00d7 and 13.7\u00d7 efficiency over LUTNet and MVAU within the same level resource cost, while introducing < 2.6% accuracy loss. This observation indicates that forsaking element-by-element arithmetic operations release the potential of the AMU-based NN accelerator for achieving significant gains in efficiency, compared with the traditional element-wise arithmetic-based matrix multipliers."}, {"title": "D. Performance comparison", "content": "To validate the performance of the proposed AMU module in the context of NN inference acceleration, we implemented AMU-based NN accelerators for the SFC model (i.e., a fourlayer MLP for MNIST) and convolution layers of CNV model (i.e a VGG inspired CNN for CIFAR10) on ZCU104 and XCZU19EG respectively, together with 7 prior works implemented on different platforms with varying frequencies and validated by different task complexities (i.e., the total computing operations of the neural network). In order to make AMU compatible with convolution computation, the convolution operations are transformed into a series of matrix multiplications by using the img2col function. We substituted three successive convolution layers in the CNV model progressively, which takes 50%, 69% and 94% of total computational cost (i.e., 57.8, 79.4 and 109 MOPs), with AMUs to illustrate the AMU-based accelerator performance when it encountering with the convolution operation. As shown in Table III, the peak throughput and peak efficiency are recorded as the key criteria for demonstrating the performance of each accelerator in this table. The resource utilization of each accelerator is measured by the volumes of LUT, BRAM, FF, and DSP utilisation.\nAs illustrated in Table III, the proposed work, AMUbased NN accelerator, for the SFC (0.6 MOPs) at 100 MHz outperforms MVAU-based [10] NN accelerators at 200 MHz by 9x in throughput and 112\u00d7 in energy efficiency, while introducing 2.1\u00d7 LUT occupancy. However, the growing complexity of convolution NNs (from 57.8 MOPs to 79.4 MOPs) introduces a 1.4\u00d7 throughput of the AMU-based NN accelerator, which also results in 3\u00d7 LUT and 3.5\u00d7 Flip Flop occupancy because of increasing demands for larger parameter storage and maintaining high throughput. To cope with resource shortage, the AMU-based NN accelerator applies a 'Group' partition strategy for the 109 MOPs CNV convolution layers, achieving 3 \u00d7 105 and 1.7 \u00d7 105 per million parameter growth in LUT and FF occupancy. Less than 5.8 \u00d7 106 and 3.7 \u00d7 106 per million parameter growth in LUT and FF occupancy compared with the other two CNV implementations using the 'Complete' partition strategy. As shown in Table III, with growth in CNV model complexity and parameter size, the AMU-based accelerators own increasing throughput and energy efficiency, which indicates that the proposed work has attractive scalability.\nFrom another perspective, AMU-based NN accelerators for the CNV convolution layers achieve higher energy efficiency, ranged from 6456 GOPS/W to 8906 GOPS/W. This surpasses state-of-the-art NN accelerators including FINN (MVAUbased accelerator) [10], Angel-Eye [23], DNNBuilder [24] and TCS II'20 [30]. Angel-Eye and DNNBuilder are FPGAoriented End-to-End CNN deployment frameworks, while the TCS II'20 proposed a general solution for accelerating matrix-matrix multiplication on FPGA [30]. Those works are validated on a face recognition CNN model (552 MOPs), Alexnet (2900 MOPs) and Shufflenet (137 MOPs), respectively. Although the complexity and parameter size of our works are different from these works which results in different throughput and resource utilization, the proposed memory allocation and access strategies suggest that the AMU-based accelerator has better scalability, which can potentially achieve better energy efficiency at a higher level of complexity.\nIn addition, our method achieved 4.7\u00d7 throughput to Stella Nera [18] in a similar level of complexities (from 109 MOPS to 574.16 MOPs), where the Stella Nera using Halutmatmul, a matrix multiplication method based on MADDNESS, is an ASIC-based accelerator (14 nm) for Resnet-9. Our work owning a comparable performance to ASIC-based accelerator indicates that the proposed three optimization strategies and dedicated hardware design for atypical memory allocation and access are capable of facilitating MADDNESS-based matrix multiplication to achieve better performance on ASIC-based accelerators.\nThe analysis of Table III suggests that eliminating elementwise arithmetic operation together with software-hardware codesign optimisation for atypical logic behaviours releases the potential of the AMU-based NN accelerator, achieving significant gains in throughput (up to 9\u00d7) and energy efficiency (up to 112x), compared to the accelerator using element-wise arithmetic-based matrix multipliers in the same complexity inference task."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a novel approximate multiplier unit: AMU, a scalable, energy-efficient and element-wise arithmetic operation-free matrix multiplication unit that aims to build fast, and efficient dataflow-type accelerators for the quantised NNs on FPGA. Furthermore, by replacing elementby-element arithmetic operation with encoding, retrieval and summation, the proposed AMU-based dataflow-type accelerator achieves 9\u00d7 throughput and 112\u00d7 energy efficiency than FINN-generated MVAU-based accelerator that uses bit-wise and pop-count operation on the quantised elements for the NN inference task with the same complexity level, while sacrificing acceptable accuracy loss and resource burden. The pruning and compression optimisation for the MADDNESS algorithm together with dedicated memory allocation and memory access design for FPGA enables our approach to achieve competitive performance compared to an ASIC accelerator using the original MADNNESS-based matrix multiplication. However, the original MADDNESS introduces an inevitable accuracy loss in an AMU-based accelerator, and this issue arises from the clustering strategy used in the approximate matrix multiplication algorithm, where each AMU is only responsible for its output and disregards its connection to the final network inference results. Additionally, our studies thus suggest that instead of increasing I and N for every layer, using more codebooks in the first few hidden layers can provide better accuracy and better effect on alleviating accuracy loss with increasing depth of NN, while introducing less resource cost. However, the proposed pruning and compression optimisation for MADDNESS algorithm may degrade performance when encountering complex NNs, especially for the NNs that have a convolution layer, encouraging us to explore the effective optimisation for those commonly appeared operations in deeper NNs for future work."}]}