{"title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Casual Significance and Consistency", "authors": ["Kangsheng Wang", "Xiao Zhang", "Zizheng Guo", "Tianyu Hu", "Huimin Ma"], "abstract": "Chain-based reasoning methods like chain of thought (CoT) play a rising role in solving reasoning tasks for large language models (LLMs). However, the causal illusions between a step of reasoning and corresponding state transitions are becoming a significant obstacle to advancing LLMs' reasoning capabilities, especially in long-range reasoning tasks. This paper proposes a non-chain-based reasoning framework for simultaneous consideration of causal significance and consistency, i.e., the Causal Significance and Consistency Enhancer (CSCE). We customize LLM's loss function utilizing treatment effect assessments to enhance its reasoning ability from two aspects: causal significance and consistency. This ensures that the model captures essential causal relationships and maintains robust and consistent performance across various scenarios. Additionally, we transform the reasoning process from the cascading multiple one-step reasoning commonly used in Chain-Based methods, like CoT, to a causal-enhanced method that outputs the entire reasoning process in one go, further improving the model's reasoning efficiency. Extensive experiments show that our method improves both the reasoning success rate and speed. These improvements further demonstrate that non-chain-based methods can also aid LLMs in completing reasoning tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have shown promising potential in solving both structured logical reasoning problems [1]\u2013[3] and unstructured mathematical reasoning problems [4]\u2013[6]. Previous research indicates that although Chain-Based methods, represented by Chain of Thought (CoT) [7], can effectively enhance LLMs' reasoning capabilities, they cannot avoid the inherent issue in the answer generation mechanism of LLMs-namely, the causal illusion between a step of reasoning and corresponding state transitions, where the causal relationship is not significant [8]. Contrary to the intuitive expectation that a correct CoT always leads to the correct answer (and an incorrect CoT always leads to an incorrect answer), our tests reveal a disproportionately high frequency of correct answers following incorrect CoTs (and vice versa). We believe the Chain-Based reasoning approach has inherent attentional-defocus flaws that are difficult to resolve. Therefore, we have designed a causally enhanced method based on Treatment Effect assessments to solve LLMs' reasoning problems without relying on Chain-Based approaches, i.e., CSCE. This method leverages causal inference metrics, Individual Treatment Effect (ITE) [9]\u2013[14], by incorporating its absolute expectation and variance into the model's training loss function, thereby enhancing the causal significance and consistency of the model's inner inference process. Furthermore, we tested our approach against three Chain-Based methods-CoT [7], RoT [15], and RAP [16]-on the Blocksworld [17], GSM8K [18], and Hanoi Tower [19] datasets as shown in Fig.1. The main contributions of this paper are as follows:\n1. We propose a framework for training LLMs with causal enhancements based on Treatment Effect assessments, addressing the issue of causal illusions featuring the weak causal relationship between a step of reasoning and corresponding state transitions.\n2. We effectively integrate LLMs' traditional cross-entropy loss function with the absolute expectation and variance of the ITE, resulting in outputs with higher causal significance and consistency.\n3. We provide a theoretical analysis of introducing Treatment Effect-related methods from the field of causal inference during the unfrozen training of large models, revealing that causal consistency is also crucial in LLM reasoning in addition to causal significance.\n4. Experiments demonstrate that our method achieves state-of-the-art (SOTA) performance in reasoning success rate and outperforms comparison methods in reasoning speed."}, {"title": "II. RELATED WORK", "content": "Current LLM reasoning approaches can be broadly categorized into frozen and unfrozen models. Frozen models refer to those where the model is not typically trained further, but instead, external methods are used to guide the model's output, such as CoT [7], RoT [15], RAP [16], ToT [20], and GoT [21]. These methods, called Chain-Based methods, break down the problem into a series of cascading reasoning processes to query the model. On the other hand, Unfrozen models enhance the model through fine-tuning on top of the pre-trained model rather than relying solely on external guidance. Examples include ICL [22], SFT [23], and RLHF [24]. Both approaches can be used to enhance the reasoning capabilities of LLMs. Our method belongs to the unfrozen model category."}, {"title": "B. Causal inference method for LLM training", "content": "Causality is initially divided into three levels [25], with the Treatment Effect [26] widely used as an evaluation metric in machine learning. By leveraging causal analysis, a method used to identify and understand the reasons and effects of different behaviors or decisions [27], [28], we can assist in training LLMs for reasoning tasks. This involves examining the reasons or causes behind specific events and the potential outcomes they may produce. By analyzing the differences between observational and interventional distributions, as well as evaluating the causal significance and consistency between a step of reasoning and corresponding state transitions that guide the correct answer output [29], we achieve Causal Enhancement of LLM reasoning without relying on Chain-Based strategies."}, {"title": "III. METHOD", "content": "This section will discuss how to implement Causal Enhancement in the LLM reasoning process based on the Treatment Effect and the theoretical analysis behind this approach. We will also briefly compare it with Chain-Based methods."}, {"title": "A. Framework Overview", "content": "The framework we designed is shown in Fig.2. It differs from Chain-Based methods like CoT, although they may appear similar. Our model generates the entire sequence of steps in one go during the reasoning validation phase without relying on externally provided prompts from a Chain. This framework demonstrates how the model's state evolves during training, with each reasoning step causing a state transition. Specifically, we designed a loss function that better controls the training process by evaluating the causal significance and consistency between adjacent reasoning steps and state transitions. This is a real-time adjustment process during training, and the enhanced pre-trained model will use the same loss function and parameters during the reasoning validation phase. The detailed design of the loss function will be elaborated in Section C."}, {"title": "B. Theoretical Analysis", "content": "In causal inference, ITE measures the difference in outcomes for an individual with and without a specific treatment. A larger ITE typically indicates a stronger causal relationship between random variables. Its definition is as follows:\n$$ITE_i = Y_i(W = 1) \u2013 Y_i(W = 0)$$\nwhere $Y_i(W=1)$ and $Y_i(W = 0)$ are the potential treated/control outcomes of sample i. W represents the treatment assignment."}, {"title": "C. Loss Function Design", "content": "We use ITE to quantitatively estimate the causal relationship between each reasoning step and the corresponding state transitions. Next, we incorporate ITE into the training loss function alongside cross-entropy, enhancing these state transitions' causal significance and consistency. The cross-entropy loss governs the reasoning path selection, while the ITE loss is focused on suppressing hallucinations. By jointly considering both causal significance -|E(ITE)| and causal consistency Var(ITE), we achieve a balance that improves model performance. Additionally, we use perplexity (PPL) as a performance metric for LLMs, with lower values indicating better predictive accuracy.\nGiven binary variables P and Q, which represent the correctness of each reasoning step and the corresponding state transitions respectively, where P, Q ~ B(0,1) and P = 1 (or Q = 1) indicates correctness, we begin by calculating the cause-effect interventions between P and Q. Subsequently, we modify the distribution of Q by intervening in P. From a statistical correlation perspective, if P and Q are correlated, Q can be predicted using P. However, if there is no causal relationship between P and Q, intervening in P will not alter the distribution of Q. Therefore, even if P and Q are correlated but lack a causal connection, manipulating or intervening in P will not affect the distribution of Q. This distinction is essential in statistical analysis and experimental design, as it addresses the common misconception that correlation implies causation.\nWe aim to establish a one-to-one correspondence between a step of reasoning and corresponding state transitions. It is important to note that this one-to-one correspondence does not necessarily imply that the combination of each reasoning step and its corresponding state transition is correct. During the training process of a large model, the model's own cross-entropy loss function also plays a crucial role. Similar to methods like RAP and CoT, which freeze the model, the cross-entropy loss function can partially ensure consistency. Moreover, since our model is not frozen, its output consistency improves with each training epoch.\nITE does not operate in isolation but works in conjunction with the cross-entropy loss function. The dimensions of cross-entropy loss and ITE are different. In the early stages of model training, the changes in cross-entropy loss (or PPL) are much more significant than the changes in ITE, making ITE's influence minimal at this stage. However, in the later stages of training, as the cross-entropy loss decreases and its rate of change approaches that of ITE, ITE plays a more significant role, assisting the model in further optimization.\nConsequently, we incorporate the ITE into the loss function, as is shown in (2) and (3), $p_{1|p}$ and $p_{0|p}$ denote the conditional probabilities of Q being 1 and 0, respectively, given the state of P.\n$$L_{CrossEntropy} = - [Q log(p_{1|p}) + (1 \u2212 Q) log(p_{0|p})]$$\n$$L_{Loss} = L_{Cross Entropy}-\u03b1|E(ITE)|+\u03b2Var(ITE) = ln(PPL)$$"}, {"title": "IV. EXPERIMENT", "content": "In our experiments, we utilized three key datasets: Blocksworld [17], GSM8K [18], and a custom-made Hanoi Tower dataset, which are briefly introduced in Fig.1. Blocksworld involves stacking n blocks in a specified order, with the LLM performing actions like picking up, putting down, unstacking, and stacking blocks, all while manipulating only one block at a time. GSM8K comprises 1,319 grade school math problems requiring multi-step calculations, which we address by decomposing each problem into smaller sub-questions. Our Hanoi Tower dataset, constructed with random initial and goal states, is more complex than Blocksworld. It incorporates a classical solving algorithm that translates solution paths into textual data, similar to Blocksworld, but with an added complexity of stack order judgment. Errors in the stack order result in solving failures, making the task more challenging. Unlike Blocksworld, where solution steps vary, the steps in our Hanoi Tower dataset are always odd and aligned with the minimum number of required moves. The dataset size used in our experiments is consistent with Blocksworld."}, {"title": "B. Baseline", "content": "In our study, we utilized several pre-trained models as baselines, including LLAMA-2-7B [33], Phi-2-7B [34], Mistral-7B [35] and Mixtral-8x7B [36]. The training primarily involved 7B models on a single NVIDIA A100 GPU, with models loaded in 4-bit. We also evaluated three techniques to enhance reasoning capabilities: RAP, which employs Monte Carlo Tree Search (MCTS) for strategic exploration [16], transforming LLMs into reasoning agents and world models; CoT [7], which improves reasoning by generating intermediate reasoning steps; and RoT [15], a framework enhancing tree-search-based prompting methods, leveraging past experiences to improve reasoning and planning."}, {"title": "C. Results", "content": "We acknowledge that our experiments were primarily conducted on 7B parameter models due to computational constraints and availability. Despite this, our method achieved high accuracy rates across the datasets categorized as Structured Problems, including Blocksworld and Hanoi Tower, consistently outperforming baseline methods. The choice of a 7B model increases the effectiveness of our approach, as demonstrated by its superior success rates in these tasks, as is shown in Table I. In the Blocksworld tasks, our method exhibited strong inference capabilities. In contrast, our model maintained higher accuracy than other models on the more complex Hanoi Tower tasks, even under stricter stacking order requirements. This robustness across different structured problem types underscores the reliability of our approach.\nOur approach significantly shortens the time required to complete long-range reasoning tasks compared to benchmarks like CoT, RoT, and RAP, as shown in Fig.4. The main reason for our higher inference speed is that, unlike other methods, our approach performs simultaneous multi-step reasoning and outputs the answer in one go. In contrast, RAP relies on Monte Carlo search tree inference, where each node in the tree requires a separate inference step, leading to slower overall performance. CoT and RoT models, which rely on chained stacked inference, require multiple short inductions for a single long process, further slowing down the inference. Our method eliminates the need for multiple inferences, making it more efficient, especially in longer reasoning scenarios."}, {"title": "V. CONCLUSION", "content": "This study introduced a novel approach to enhance LLMs in handling structured and unstructured reasoning tasks, demonstrating significant improvements in accuracy and efficiency across various problem types. Our method proved effective in structured problem scenarios like Blocksworld and Hanoi Tower, addressing challenges such as causal hallucinations and large search spaces. However, in scenarios with strict order requirements, like Hanoi Tower, the accuracy could have been higher. It also showed strong performance in non-structured mathematical tasks, such as those found in the GSM8K dataset, further highlighting its versatility. Future work will refine the method to enhance scalability and efficiency across complex problem-solving tasks, including structured and unstructured scenarios."}]}