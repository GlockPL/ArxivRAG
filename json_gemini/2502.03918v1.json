{"title": "Adaptation of Task Goal States from Prior Knowledge", "authors": ["Andrei Costinescu", "Darius Burschka"], "abstract": "This paper presents a framework to define a task\nwith freedom and variability in its goal state. A robot could use\nthis to observe the execution of a task and target a different goal\nfrom the observed one; a goal that is still compatible with the\ntask description but would be easier for the robot to execute. We\ndefine the model of an environment state and an environment\nvariation, and present experiments on how to interactively\ncreate the variation from a single task demonstration and how\nto use this variation to create an execution plan for bringing\nany environment into the goal state.", "sections": [{"title": "I. INTRODUCTION", "content": "The state of the scene that defines the goal of a specific\ntask allows several possible variations in object positions and\ntheir internal state; variations that can still be considered\nvalid configurations for a task.\nMany robotic systems try to accurately copy and imitate\nthe goal configuration from human demonstration [1], [2],\n[3]. But they often struggle to do so because of additional\nchallenges on the manipulator kinematics and, by imitating,\nthey cost unnecessary additional time for adjustments of\npositions and internal states of objects, e.g. a cup's content\nlevel. Existing geometric task models [4] need to be extended\nby additional ontology information about possible variations\nin their execution, which this paper addresses.\nIn non-assembly tasks, e.g., household environments, it\nis rare that fixed values are desired for object states. When\ncleaning the table after dinner, the washed cutlery does not\nhave a fixed 3d pose in the cutlery drawer. Rather, there is an\nassigned space, a zone, a range of values where the knives,\nforks, and spoons are located, but not fixed values. Similarly,\nwhen we want to empty a cup, i.e. set its content to 0, we\ndo not make sure there is no water atom left in the cup.\nThus, representing task goal states as (only) fixed values\ndoes not model reality.\nThe option to model ranges of values is important for\nrobots to make sense of the world: to learn task goal states\nfrom the persons with whom they interact and to execute\nskills to bring an environment into a particular goal state.\nThis is a challenging problem, as illustrated by Figure 1. The\nuser, moving the cup on the front table, has a goal definition\nin mind that he is trying to achieve. As illustrated on the fig-\nure's right side, there are many possibilities for the intended\ngoal. A method of disambiguation and navigation through\nthe possibility space is required for task goal specification.\nIn this paper, we propose a way of representing task goal\nstates not via fixed states but via value ranges of agent and\nobject properties. This is based on a hierarchical, conceptual\ndefinition of objects and agents that includes (both intrinsic\nand extrinsic) properties with their respective co-domain,\nwhich we call ValueDomain [5].\nFollowing the structured definition of environment states,\nwe define a task's goal state as a subset of the whole\nenvironment ValueDomain. We call a subset of a Value-\nDomain a variation. This structured environment definition\nenables computing differences between values of the same\nValueDomain and between values and defined variation.\nThis becomes important when trying to solve a task: i.e.\ndetermining the Skills for agents to execute to change the\nenvironment into the task's goal state. Creating the variation\nof an environment is also important to the model. We propose\nan interactive method based on one user demonstration of a\ntask. We create the task goal state by computing the differ-\nences between the pre- and post-demonstration environments\nand by asking the user questions to solve ambiguities in the\nvariation selection process.\nTask goal modeling was developed as part of classical\nartificial intelligence (AI) tools. Initially, logical predicates\nand statements were used to define states and (fixed) goal\nconditions. This definition led to the development of con-\nstraint satisfaction problems [6], in which variables could be\nconstrained to ranges of values, not only fixed ones. The\npossible value ranges were also extended from Boolean to\nnumerical values.\nVersion Spaces [7] is another classic AI tool for modeling\nvariable states. Though initially not developed for modeling\ntask goals, Version Spaces represent a set of hypotheses\nexplaining a supervised data set. It can represent a most\ngeneral hypothesis, accepting any data, a most specific hy-\npothesis, accepting only one data point, and all other possible\ncombinations of data points. One can view Version Spaces\nas representing subsets of a set: the set itself, single-element\nsets, and all element combinations in between. Inspired by\nthis set-like view on Version Spaces, we have developed the"}, {"title": "II. MODEL", "content": "Task goal representations in household environments can\nspecify fixed values for entity properties, a range of allowed\nvalues, or be indifferent to the property value. In the example\nof Figure 1, the goal state could be that the drinking mug\nmust be half full, and its accepted location should be close to\nits current location on the front table. In this goal example,\nthere is a fixed value for the amount of content of the\ndrinking mug, a range of values for its location, and any\npossible value for the table's location, because it was not\nspecified in the goal state. A model of a task's goal must\nrepresent all these possibilities. Our approach is to introduce\nvariations of values, described in II-B.\nTo turn an environment into a desired goal state, the\nsystem must first assess the current state of an environ-\nment, described in II-A. In the environment state, values\nof entity properties and the whole range of allowed values\nare represented. This range of allowed values is the basis\nof variations, which can be seen as subsets of the whole\ndomain of values.\nIn the second step, the differences between the current\nand goal environment states must be computed, see II-C,\nand solved to get to the desired task goal state.\n##### A. Model of the world: What is an Environment State?\nWe model the state of an environment as the collection\nof states of entity instances (i.e. agents and objects) in the\nenvironment, see the left side of Figure 2. In the environment\non the left side of Figure 1, there are four entity instances:\nthe person doing the demonstration, the drinking mug that\nhas changed location, and the two tables, one in the back\nand the other in the front, on which the mug is now located.\nThe state of an entity instance comprises the concepts of\nthe entity and the collection of properties defined by the\nentity's concepts [5]. Concepts also define their properties'\nallowed range of values (incl. value type). Subconcepts in-\nherit properties. Instances, not concepts, define the values of\ntheir properties. The state of the drinking mug thus comprises\nall parent concepts, e.g. mug, liquid container, container,\nobject, physical entity, concept, and all properties defined by\nthese concepts, including the location, mass, maximal vol-\nume that can be contained (contentVolume), volume currently\ncontained (contentLevel), contained instances, color, etc.\nThus, the environment state contains all the property\nvalues of its entity instances.\nThe property values are elements of a ValueDomain,\ni.e. a set of values. For example, the set of values of the\ncontentVolume property is a non-negative real number, and\nthe set of values of the location property is a tuple of the\nreference entity and the pose delta to that reference's origin.\nTo represent subsets of ValueDomains, we define variations.\n##### B. What is a Variation?\nA variation of a ValueDomain represents a subset of\nvalues from that ValueDomain. Thus, a variation can be\nempty, it can be a fixed value, it can be a range of values\n(Range Variation), it can be a conjunction and/or a disjunc-\ntion of variations, or it can be the whole ValueDomain itself.\nFor example, the variation of an Integer value could be the\nempty set, the number 4, the set of prime numbers, the union"}, {"title": "C. Comparison between ValueDomains", "content": "In our work, a Comparison represents the contrast be-\ntween two ValueDomains. It contains a target and a value to\nbe checked for equality with the target.\nValues and targets of different ValueDomains are different.\nValues and targets of the same ValueDomain are compared\nfor equality. If different and the ValueDomain has sub-data,\nComparisons of the sub-data are also created. For example,\nas per Figure 2, a Location is represented as a Pose delta to\na reference Instance entity. Thus, the Location has a Pose\nand an Instance as sub-data. If the Location target and value\nare different, Comparisons of the Pose and Instances are also\ncreated. This helps pinpoint the exact reason for the values\nbeing different. Similarly, EnvironmentData has a Collection\nof Instances as sub-data. Collections have an Integer size\nand elements as sub-data. Instances have a ConceptValue\nand ConceptParameters as sub-data.\nComparisons of sub-data are saved as additional informa-\ntion in the parent data Comparison so that, e.g., the Location\nComparison knows there is a difference between the Pose\nsub-data of the Location.\nOur system can also model Comparisons between a Val-\nueDomain and a variation of the same Value Domain. In this\ncase, the Comparison is equivalent to checking if the value is\ninside the variation. If not, the additional information stored\nin the comparison are the reasons why the value is not in the\ntarget variation. In the example of an Integer-Comparison,\nif the target variation is the interval [6,10] and the value is\n4, the reason is the Boolean-Comparison of the LessEqual\nfunction between the interval's lower bound 6 and the value\n4, which is false but is supposed to be true when the value\nis inside the interval.\nThe third Comparison type is when the value stems from\nan Instance's concept properties. Besides the additional info\nwhen the value is different from the target, this Comparison\ntype has information about the instance, which the system\ncan use to determine which Actions and Skills (see III-A)\nshould be used to change the value of this concept property.\nFigure 3 illustrates a solution to bring the contentLevel\nproperty of the WhiteMugInstance inside the variation range."}, {"title": "III. RESULTS", "content": "Figure 4 shows our proposed framework to define a\ntask goal, i.e. an environment goals state, and to turn a\ngiven environment into this goal state. The system visually\nobserves a task execution by a user and segments this single\ndemonstration into Skills. Actions and Skills are defined in\nIII-A. The demonstration changed one or several properties\nof entities in the environment; environment which is now\nin the goal state. This information and the differences in\nentity properties from the start and end environment states\nare used to represent the task goal state. More on that in\nIII-B. To turn a new environment into the defined goal state,\na planning problem must be solved. This entails computing\nthe differences between the environment's current state and\nthe goal state, finding Actions that solve these differences,\ninstantiating Skills that implement the Actions in the environ-\nment, selecting the Skills to execute by minimizing a given\nmetric, and finally, sending the Skills to the agents in the\nenvironment to execute. This process is detailed in III-C.\n##### A. Actions and Skills\nA change in the environment is modeled using Actions,\ni.e. what has happened, and Skills, i.e. how did the change\nhappen [5]. Like in STRIPS [8] and PDDL [9], we represent\nActions by their effects on entity properties and Skills by\ntheir preconditions and effects. Actions do not need pre-\nconditions because they only describe the what part of a\nchange, not which conditions must be satisfied to perform\nthe change. Besides preconditions and effects, Skills have a\nlist of checks that tell our system if the Skill is executed in\nthe environment. These checks allow the creation of a Skill\nrecognition program, like the one presented in [5]. Using the\nSkill recognition output, we capture the changes from a task\ndemonstration.\nA Skill is thus the physical enactment of an abstract\nAction in an environment. Hence, Skills are correlated with\nActions via their effects. A Skill can have more effects than\na corresponding Action. For example, the Skill of scooping\njam from a jar with a spoon implements the Action of\nTransferring Contents, but it also Dirties the spoon.\n##### B. How To Parameterize The Model\nCreating a new goal state should be easier than manually\nspecifying all variations wanted from the goal state. Doing\nso requires programming knowledge, which should not be\nneeded to define goal states. One can let the system, which\nknows how to represent goal states, question the user about\nthe desired state of the environment. However, this tedious\nprocess requires many questions from the system, also lead-\ning to decreased system usability.\nTherefore, our approach is to let the user turn a given\nenvironment into a desired goal state and analyze the dif-\nferences between the initial and final environment state to\ncreate the goal state representation. This single demonstra-\ntion highlights the entity property values that were not in the\ndesired goal state before being changed by the user.\nWe capture the demonstration via an Intel Realsense 3D\ncamera [19], analyze the human skeleton via the OpenPose\nhuman pose estimation method [20], and determine the 3d\npose of objects with AprilTag markers [21].\nOne demonstration contains the initial environment, not\nin the task goal state, and the final environment, in the goal\nstate. The final environment state alone is not enough to\ncreate the environment variation. Thus, additional questions,\nguided by the differences between the two environment\nvalues, are posed by the system to the user to determine\nthe desired variation in the environment state.\nIn a demonstration in which the user pours milk into a\nbowl, as shown in the top of Figure 4, the initial question\nposed to the user is which entities that have changed prop-\nerties are relevant for the goal state. If the goal state is to\nhave more milk in the bowl, the milk carton is irrelevant; it\nis a means to achieve the goal state but not relevant to the\ngoal itself. The bowl is thus selected as a relevant entity.\nNext, the list of relevant modified properties must also be\ndetermined for each relevant entity. It could have happened\nthat during pouring of the milk into the bowl, the bowl's\nlocation also changed, e.g. touched accidentally by the user.\nThus, not all modified properties could be relevant to the\ntask. After selecting the relevant properties, the system\nknows from the knowledge base [5] their ValueDomain and\nthe list of implemented variations for that ValueDomain.\nThus, the user parametrizes a selected variation from the\nlist: choosing either a fixed value, a ValueDomain-specific\nRange Variation that must be parametrized, a conjunction or\ndisjunction of Range Variations, or the whole ValueDomain.\nIn the example above, the user chooses the contentLevel\nproperty as relevant. The system knows this property's\ndefined set of values: a non-negative real number, and the\npossible range variation types: an open interval, a closed\ninterval, an open-closed or closed-open interval, an intersec-\ntion or union of intervals, etc. The user chooses a closed\ninterval of [0.28,0.32] around the final contentLevel value\nof 0.3L. The user also specifies a variation for the entity's\nconcept. It is generalized from that specific bowl instance to\na LiquidContainer.\nAfter each modified property of each entity has a repre-\nsented variation, the system automatically collects the enti-\nties into a variation of type A, see II-B, which is the assigned\nvariation for the collection of entities in the environment.\nThus, the environment variation is determined in\nO(nxmxp) questions to the user, where n is the number\nof entities in the environment, m is the maximal number"}, {"title": "C. How To Use The Model", "content": "Assuming the representation of a task's goal state is given,\ni.e. an environment variation, we detail our procedure (see\nFigure 6) to turn the current environment into the goal state.\nFirst, a Comparison between the environment and the goal\nvariation is computed. This leads, as described in II-C, to a\nlist of reasons why the environment is not in the variation.\nThese reasons, i.e. differences $\\delta$ of concept properties p,\nmust be fixed to turn the environment into the goal state.\nFor an EnvironmentData-Variation $\\mathit{venv}$ that defines a\nCollection-RangeVariation of type $A$, see II-B, computing\nthe Comparison between an EnvironmentData $\\mathit{env}$ and this\ntarget $\\mathit{venv}$ leads to a list of reasons for each entity $\\mathit{eenv}$ in\nthe entity collection of $\\mathit{env}$, why $\\mathit{eenv} \\notin v, v \\in A$. This can be\nseen in Figure 6, where for each entity of LiquidContainer\nconcept in the environment, there is a list of differences, i.e.\nComparisons, created for why the respective entity does not\nmatch the defined variation on the top-right.\nThe second step of the procedure is to turn the list of\ndifferences into a list of Actions that can fix them. In notation,\nAction $A_x$ solves a difference in the concept property $p_x$. The\nsystem knows which properties Actions modify by analyzing\nthe definition of their effects. Thus, Actions are created\n(parametrized) to fix the differences in entity properties.\nIn the third step, each Action $A_x$ is converted into an\nexecution plan $P_x$ that implements solving the difference\n$\\delta_{px}$ in the environment. It is also possible that there is no\npossibility to implement the Action $A_x$ in the environment;\nthis is represented as an execution plan $P_x = \\emptyset$. An execution\nplan $P_x$ is otherwise, in its simplest form, a set of Skill\nalternatives $\\{S_y\\}$, where the Skill $S_y$ implements the Action\n$A_x$. There is the case to consider that the Skill $S_y$ has\npreconditions that are not met. And so, before executing the\nskill $S_y$, a different execution plan $P_{S_y}$, has to be computed and\nexecuted to allow the Skill $S_y$ to solve the property difference\n$\\delta_{px}$. It is also possible that one single Skill $S_y$ is not enough\nto implement the Action $A_x$. Consider the case where the\nenvironment contains three cups with 0.1L of water, and the\ngoal is to have one cup with 0.3L of content. One single\nPouring Skill is not enough to fulfill the goal; two Pouring\nSkills must be executed. Thus, in the most general form, an\nexecution plan $P_x = [\\{S_{iy}, P_{S_{iy}}\\}]$ is a list of skill alternatives\n$\\mathit{\\{S_{iy}, P_{S_{iy}}\\}}$ , that possibly contain other execution plans $P_{S_{iy}}$\nto solve the skill's preconditions.\nOur procedure to parameterize the Skills $S_y$ that implement\nthe Action $A_x$ is a custom solution for each property $p_x$.\nOne could backtrack through all possible parameter values\nof all possible skills to create a general solution that works\nfor all properties. Another idea is to invert Skill effects\nand thus guide the Skill parameter search from the target\nvariation to the value. However, both approaches would be\ncomputationally intense and would not create execution plans\nin a reasonable time.\nThe procedure to solve an entity $\\mathit{e}$'s contentLevel property\ndifference searches for other Container object instances\nin the environment, sorts them according to their content\nvolume, and iterates through them in ascending order if\n$\\mathit{e}$.contentLevel $< \\mathit{target}$.contentLevel; otherwise, in descend-\ning order. If a Skill $\\mathit{S}$ can be executed with the two ob-\njects, that reduces the difference between $\\mathit{e}$.contentLevel\nand $\\mathit{target}$.contentLevel, the Skill is added to the execu-\ntion plan. If, after checking all objects, $\\mathit{e}$.contentLevel $\\nless \\mathit{target}$.contentLevel, there is no solution to solve this prop-\nerty difference.\nThus, the result of the third step is an execution plan $P_x$\nfor each entity property difference.\nFourth, after having the execution plans $P_x$ per entity-"}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we presented a model to represent the desired\nvariation in a task's goal, i.e., the variation in an environment\nstate. We also showed how to build this model from a single\nuser demonstration of the task and how to use the model of\nthe task's variation to create an execution plan to change a\ngiven environment into the goal state or determine if the goal\nstate is unattainable with the Skills than an agent possesses.\nWith such a model, an agent would not need to imitate\nobserved task execution trajectories, but could optimize the\nexecution plan to its kinematic structure and attained Skills.\nvariation and entity, a solution selector scores all solutions\naccording to defined metrics and then, via a maximal match-\ning algorithm, selects the solutions to execute to satisfy\nall variations of the Collection-RangeVariation of type A.\nThe edges in the maximal matching have the cost of the\nsolution score. For this paper, the scoring metric by the\nsolution selector is the number of steps of the execution plan.\nThe fifth and final step is to pass the execution plan to\nthe agent(s) to execute in the environment. Figure 7 presents\nthe flow of data through the five steps. We have used the\nFranka Emika Panda robot in CoppeliaSim [22] to perform\nthe computed execution plan.\nThe experiments aim to compute solution plans for solving\nthe difference of the contentLevel property of Container\nobjects. For this, we consider the following criteria. C1:\nvariation type = $\\{$fixed, interval, interval union$\\}$. C2: tar-\nget relative to content = $\\{\\{t < cL <cV\\}, \\{cL < t < cV\\},\\{cL <t \\ni cV\\}, \\{cL \\le cV <t\\}\\}$, where t is the variation\nvalue and cL and CV are the contentLevel and contentVolume\nproperties respectively. C3: achievable in environment\n$\\{$yes, no$\\}$. Figure 8 presents planning results for different en-\nvironments and the criteria described above. The lower table\nshows cases where the computed solution does not match the\nactual solution. This only happens when multiple instance\nvariations are defined. The reason is that the implemented\nprocedure to turn the list of differences into an execution plan\ntreats each difference independently. Thus, dependencies\nbetween two variations are not accurately solved.\nIn the upper table of Figure 8, there are two solutions for\nC1.3, C2.3, C3.1: one with the bowl B as the instance in\nthe variation V1, the other with M. The solution when B is\nthe matched instance has three steps: 1) pouring 0.1L from\nM into B, 2) pouring 0.1L from C1 into B, and, finally, 3)\npouring 0.02L from C2 into B. This plan is sent to the robot\nin simulation and is executed as shown in Figure 9.\n##### A. Limitations\nThe procedure detailed in III-C works for one instance\nvariation. When multiple instance variations are defined in\nthe environment variation, the procedure does not consider\nthat to solve one instance variation, other instances will be\nmodified. And thus, determined solution plans do not bring\nthe whole environment into the goal state; just parts of\nit. This limitation is overcome by improving the planning\nprocedure, e.g., by using a PDDL solver.\nAnother limitation of the framework is the missing pro-\ncedure to fix differences in Skill-preconditions, so that, e.g.,\nif the milk carton is closed, the solution to open it and pour\nfrom it is determined by the system. The solution is to make\nthe difference-solving procedure recursive and parameterize\nit with the differences found in Skill preconditions.\n##### B. Future Work\nImmediate future work is overcoming the limitations as\npresented above, followed by improving the scoring of Skills\nwhen selecting execution plans to consider the abilities of\nagents and, e.g., energy cost or path distance or time to\ncompletion of Skills. Furthermore, we plan to extend the\nvariations to exclude certain ranges of values. Including nega-\ntions of ranges besides unions and intersections, would give\nthe variations full expressiveness over the ValueDomains."}]}