{"title": "Leveraging Multimodal Models for Enhanced Neuroimaging Diagnostics in Alzheimer's Disease", "authors": ["Francesco Chiumento", "Mingming Liu"], "abstract": "The rapid advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have shown great potential in medical diagnostics, particularly in radiology, where datasets such as X-rays are paired with human-generated diagnostic reports. However, a significant research gap exists in the neuroimaging field, especially for conditions such as Alzheimer's disease, due to the lack of comprehensive diagnostic reports that can be utilized for model fine-tuning. This paper addresses this gap by generating synthetic diagnostic reports using GPT-40-mini on structured data from the OASIS-4 dataset, which comprises 663 patients. Using the synthetic reports as ground truth for training and validation, we then generated neurological reports directly from the images in the dataset leveraging the pre-trained BiomedCLIP and T5 models. Our proposed method achieved a BLEU-4 score of 0.1827, ROUGE-L score of 0.3719, and METEOR score of 0.4163, revealing its potential in generating clinically relevant and accurate diagnostic reports.", "sections": [{"title": "I. INTRODUCTION", "content": "Degenerative diseases are conditions that gradually damage and destroy parts of the cells of the nervous system, particularly in areas such as the brain. These diseases typically develop slowly, and the effects and symptoms generally manifest in the later stages. Among the most common degenerative diseases is Alzheimer's, which is estimated to currently affect 6.9 million Americans aged 65 or older, and remains the fifth-leading cause of death [1]. Alzheimer's disease (AD) develops in different stages, and is the most common cause of dementia, accounting for 60-80% of all cases [2]. Currently, there is no definitive cure; however, early diagnosis of this condition can lead to a slowdown in its progression and an improvement in the patient's quality of life. To meet the need for fast and accurate diagnoses, there is an increasing reliance on automatic diagnostic systems based on machine learning methods. Diagnostic reports provide essential textual descriptions and are important for the early diagnosis and treatment of the disease. The interpretation of these reports can indeed influence patient outcomes. However, interpreting biomedical images to generate diagnostic reports can take a considerable amount of time, even for the most experienced clinicians [1], [3], [4].\nDeep learning methods, particularly those based on Convolutional Neural Networks (CNNs) and transformer archi-tectures, have shown great potential in the detection of Alzheimer's disease, thanks to their ability to learn complex patterns and representations from large-scale datasets [5]. However, the use of CNNs presents several limitations, such as the inability to capture long-term dependencies and the absence of an attention mechanism. Furthermore, these models are frequently criticized for their lack of interpretability. The hierarchical and non-linear nature of their processing can make it challenging to understand how they arrive at a particular decision [6]. Additionally, one of the main challenges is the efficient integration of medical images with structured data. Despite the potential benefits of combining these data, their integration can affect the quality of classification performance. To address these limitations, recent research has explored the potential of more advanced language models and multimodal approaches. These models can process and integrate various data types, showing promising results in terms of accuracy and interpretability for Alzheimer's disease [7]. Transformers leverage their attention mechanisms to capture global contextual information, which can make them more interpretable in decision-making processes particularly for tasks that require understanding long-range dependencies or a comprehensive view of the context [8]\u2013[10].\nTo the best of our knowledge, the generation of synthetic diagnostic reports for Alzheimer's disease using multimodal approaches has not been previously explored in the literature. Specifically, our research question in this work is to determine how synthetic diagnostic reports can bridge the gap between existing neuroimaging datasets and the training requirements of Visual Language Models (VLMs) and Large Language Models (LLMs) for Alzheimer's diagnosis. With this in mind, the main contributions of our work are as follows:\n\u2022 We generate synthetic diagnostic reports to address the lack of textual data in neuroimaging, facilitating the fine-tuning of multimodal models for Alzheimer's diagnosis;\n\u2022 We propose a framework using BiomedCLIP and T5 to combine visual features from MR images with clinical descriptions, extending multimodal model applications to neuroimaging datasets with a particular focus on the OASIS;\n\u2022 We integrate MR images and clinical data to analyze both"}, {"title": "II. RELATED WORK", "content": "LLMs and VLMs, trained on vast datasets, have recently shown incredible performance in diagnostic report generation, significantly enhancing accuracy and consistency [7], [11]. The development of foundation models has been crucial, impacting various applications in language processing and computer vision. Foundation models such as CLIP (Contrastive Language-Image Pretraining) and PaLM are particularly notable for their ability to be adapted to various tasks without requiring significant modifications to their parameters [4]. These models utilize enormous amounts of data and computational resources during training; this enables them to generalize effectively across various domains. The integration of multimodal data, such as text and images, has been particularly effective, allowing these models to generate coherent text across different modalities. For example, models like CLIP create visual representations associated with linguistic descriptions, making it possible to perform complex image-text alignment tasks [12].\nIn the biomedical domain, several models that employ these advancements have been developed. For example, Biomed-CLIP [13] combines textual and visual information to map medical images and text into a common representation space. Based on the CLIP architecture, BiomedCLIP employs two separate encoders: a PubMedBERT-based textual encoder and an advanced version of a Vision Transformer (ViT) pretrained on the PMC-15M dataset, a large collection of 15 million image-text pairs extracted from scientific articles in PubMed Central (PMC). This multimodal approach allows Biomed-CLIP to perform well in the alignment of image-text pairs specifically in the medical field. In addition, Med-PaLM 2 has shown excellent results in extracting relevant clinical information and generating detailed medical reports, achieving 86.5% accuracy on the MedQA dataset, a benchmark based on the USMLE style dataset [4], [11]. Despite these advance-ments, LLM-based chatbots often generate responses that, while promising, are not reliable enough for real-world clinical settings, highlighting the need for further refinement [11]. One of the most prominent LLMs explored in this study is the T5 model (Text-to-Text Transfer Transformer) [14], [15]. T5 is a unified framework for natural language processing tasks, treating every NLP problem as a text-to-text task. It utilizes a modified encoder-decoder transformer, and its architecture was trained on the Colossal Clean Crawled Corpus (C4) [16] dataset using a span corruption objective. In this paper, several model variants were tested: T5-small (60 million parameters), T5-base (220 million parameters), and T5-large (770 million parameters).\nIn terms of VLM applications, studies have been conducted on the use of VLMs in biomedical image analysis by integrating LLMs with Computer-Aided Diagnosis (CAD) networks for clinical applications. For instance, Wang et al. combined existing LLMs with CAD networks, demonstrating how these models can be applied to diagnostic tasks [17]. Similarly, Yan et al. explored the performance of ChatGPT-4V on simple medical Visual Question Answering (VQA) tasks. Although the results are promising, they find it unsuitable for real-world diagnostic scenarios due to its inability to efficiently handle complex medical visual tasks [18]."}, {"title": "B. Challenges of AI Models in Neuroimaging", "content": "To date, the application of these models in the field of neuroimaging remains underexplored. Neuroimaging data are characterized by high complexity and variability, making it challenging to directly transfer the methodologies developed for other biomedical fields to this domain. Although models like LLaVA-Med, Biomed-GPT, and Geneformer have been successfully applied in fields such as radiology, pathology, and general medical imaging analysis, their use in specialized areas like Alzheimer's disease diagnosis has not yet been thoroughly investigated. For instance, Med-PaLM 2, one of the leading generalist AI models, has achieved excellent results in broader biomedical tasks such as MedQA, but its application in specific domains like Alzheimer's disease diagnosis remains unexplored. Therefore, the performance of these models in such diagnostic contexts could be poor [4], [11], [17]."}, {"title": "C. Relevant Datasets", "content": "The most commonly used datasets for training Vision-Language Models (VLMs) in automatic diagnosis and di-agnostic report generation are MIMIC-CXR, IU-Xray, and CXR-PRO. These datasets primarily focus on chest radio-graphic images and include comprehensive textual diagnostic reports [19]. In the field of Alzheimer's disease, notable datasets include the Alzheimer's Disease Neuroimaging Ini-tiative (ADNI) [20], Open Access Series of Imaging Stud-ies (OASIS) [21], and Australian Imaging, Biomarkers and Lifestyle (AIBL) [22]. These datasets contain a significant amount of multimodal data, such as magnetic resonance imaging, MRI, and positron emission tomography (PET) images, alongside structured data related to cognitive tests, disease status evaluations, genetic biomarkers and volumetric seg-mentations extracted using FreeSurfer [23]. However, unlike radiology datasets, such as MIMIC-CXR and IU-Xray, these comprehensive neuroimaging datasets do not include com-plete diagnostic reports associated with the medical images, an essential element for fine-tuning VLMs and subsequent validation. This is precisely one of the main challenges we aim to address in this work.\nIn summary, the main differences between our paper and the related work lie in three aspects. First, we propose the generation of synthetic diagnostic reports to address the lack of textual descriptions in neuroimaging datasets, particularly for the training of advanced language models. Second, our approach emphasizes the integration of visual transformer and language transformer models, which enhances the diagnostic capabilities through a more effective multimodal data fusion. Finally, we introduce a methodology for selecting relevant structured data from datasets such as OASIS-4, enabling the generation of clinically relevant and accurate synthetic reports."}, {"title": "III. OASIS DATASETS", "content": "The Open Access Series of Imaging Studies (OASIS) dataset was selected for its accessibility and the completeness of the data it contains. Developed through research initiatives conducted at the Washington University Knight Alzheimer Disease Research Center, this dataset includes comprehensive clinical and neuroimaging information and comprises four cohorts: OASIS-1 and OASIS-2, which mainly contain MR images and basic demographic information without detailed structured data. OASIS-3 and OASIS-4, instead, also include clinical and cognitive assessments, as well as FreeSurfer segmentations, in addition to MR images [24]."}, {"title": "A. OASIS-1", "content": "OASIS-1 [24] is a cross-sectional dataset comprising 416 subjects, aged between 18 and 96 years. Each subject un-derwent 3 to 4 T1-weighted MRI scans, acquired in a single scanning session. Among these subjects, 100 individuals aged over 60 have been diagnosed with very mild to moderate Alzheimer's disease. A subgroup of 20 patients underwent a second imaging session within 90 days of the initial visit. The dataset includes 434 MRI T1w sessions along with basic clinical and demographic data such as age, gender, and cognitive status. The images were preprocessed and segmented using the FreeSurfer software to extract brain volumes."}, {"title": "B. OASIS-2", "content": "OASIS-2 [25] is a longitudinal dataset comprising 150 sub-jects, aged between 60 and 96 years. Each subject underwent multiple scans in two or more visits, at least one year apart, for a total of 373 imaging sessions. Among these subjects, 72 remained nondemented throughout the study, 64 were diagnosed as demented at the initial visit (including 51 with mild to moderate Alzheimer's disease), and 14 transitioned from a nondemented state to a diagnosis of dementia in subsequent visits. The dataset includes detailed clinical and cognitive data, such as neuropsychometric and diagnostic evaluations like Clinical Dementia Rating (CDR) and Mini-Mental State Examination (MMSE), enabling the monitoring of disease progression over time."}, {"title": "C. OASIS-3", "content": "The OASIS-3 [26] dataset is a retrospective collection of data from 1379 participants gathered over the course of 30 years. The cohort includes 755 cognitively normal adults and 622 individuals with various degrees of cognitive decline, aged 42 to 95 years.\n\u2022 Imaging: OASIS-3 contains 2842 MRI sessions with various sequences, such as T1w, T2w, FLAIR, ASL, SWI, DTI, and resting-state BOLD. Additionally, there are 2157 PET sessions, using tracers like PIB, AV45, and FDG to study brain metabolism and amyloid accumula-tion;\n\u2022 Clinical and cognitive data: The clinical evaluations include standardized measures such as the CDR and cognitive tests like the MMSE. Genetic data, such as APOE status, are also included;\n\u2022 Processing: MR imaging data were processed using the FreeSurfer software to obtain volumetric segmentations, including the hippocampus, amygdala, and other brain regions involved in neurodegenerative diseases [27]."}, {"title": "D. OASIS-4", "content": "The OASIS-4 [28] dataset includes 663 participants aged 21 to 94 years, evaluated for memory disorders or dementia. OASIS-4 represents an independent dataset focused on clinical, cognitive, and neuroimaging data.\n\u2022 Imaging: OASIS-4 includes 676 MRI sessions for struc-tural brain analysis, but unlike OASIS-3, it does not include PET sessions;\n\u2022 Clinical and cognitive data: Clinical evaluations include neuropsychometric tests and biomarker measurements. Diagnostic tools such as the CDR and MMSE are also used in this cohort;\n\u2022 Processing: The images of the patients in this dataset have been processed using FreeSurfer to extract volumet-ric segmentations of areas involved in neurodegenerative diseases.\nThe most informative datasets for model training are OASIS-3 and OASIS-4. In the development of this work, particular reference will be made to the OASIS-4 dataset, as OASIS-3 is more suitable for longitudinal studies."}, {"title": "IV. METHODOLOGY", "content": "This section illustrates the methodology adopted to address the research questions. The workflow is organized into three phases: data preprocessing, model training, and validation. The process starts with the preprocessing of structured clinical data and neuroimaging data, followed by the generation of synthetic reports from these data, which will be used as ground truth for model training and validation. Subsequently, state-of-the-art VLMs and LLMs are integrated for fine-tuning. Finally, the generated reports are validated using natural language gener-ation metrics to ensure clinical relevance and accuracy. Fig. 1 shows the data preparation process that will subsequently be used by the state-of-the-art VLM BiomedCLIP and the LLM T5, while Fig. 2 provides a complete overview of the proposed methodology for model training, validation, and testing."}, {"title": "A. Preprocessing of Structured Data", "content": "The structured data from the OASIS-4 dataset required extensive preprocessing to address missing values and nor-malize variables. In some cases, patients had incomplete entries in the structured data, while in others, corresponding FreeSurfer volumetric measurements were missing. To ensure data consistency and reliability, only patients with complete clinical and imaging data, obtained within one year of the MRI visit, were selected, resulting in a final cohort of 468 patients. The clinical, neuropsychometric, and imaging datasets were then merged using the patient ID as a key, with values aligned according to visit days. Subsequently, files containing clinical, neuropsychometric, and cognitive scores were used to create a cohesive dataset for the subsequent generation of synthetic diagnostic reports. The structured data selected for preliminary tests includes values such as CDR, Sumbox (sum of boxes), MMSE, and final diagnosis, providing detailed clinical information on the cognitive status of patients. The created dataset also includes volumetric measurements of brain areas such as the left hippocampus and right hippocampus. The combination of cognitive, clinical, and anatomical data offers a comprehensive overview of each patient's health status, making it possible to generate accurate synthetic reports."}, {"title": "B. Generation of Synthetic Diagnostic Reports", "content": "Since the available neuroimaging datasets do not provide complete textual reports associated with MR images, it was necessary to generate synthetic reports to serve as ground truth for the training and validation of the models. To this end, the GPT-40-mini API was used in combination with the Bio_ClinicalBERT model to generate clinically relevant texts. Bio_ClinicalBERT is a variant of the BERT (Bidirectional En-coder Representations from Transformers) model, trained on corpora of clinical texts to improve its effectiveness in under-standing medical domain concepts. In combination with GPT-40-mini, which facilitates text generation, Bio_ClinicalBERT enriched the descriptions with relevant clinical details based on the available structured data [29], [30]. The choice of GPT-40-mini as the report generation model was due to a compromise between the cost of the API usage and the quality of the synthetic reports generated.\nThe generation process involves several key steps. First, the structured data underwent a phase of normalization and feature extraction. Specifically, predefined ranges were set to catego-rize clinical variables such as MMSE, CDR, and hippocampal volumes into qualitative descriptions to ensure consistency in report generation. Following this preprocessing phase, the structured data were tokenized using the Bio_ClinicalBERT tokenizer. These tokenized inputs were then passed through Bio_ClinicalBERT to extract relevant features. A specific prompt was created to guide GPT-40-mini in generating syn-thetic texts. The prompt instructed the model to generate reports of 100-150 words, emphasizing appropriate medical terminology, with a temperature setting of 0.7 to better mimic real clinical scenarios. We note that while clinical validation of the synthetic reports by expert clinicians is crucial for improving the model's performance, it is not the key focus of this paper but will be part of our future work."}, {"title": "C. Preprocessing of T1-Weighted MR Images", "content": "The T1-weighted MR images were preprocessed to provide visual input to the BiomedCLIP model. Preprocessing was applied to the entire 3D volume of each MRI scan. The images were initially converted to the RAS (Right, Anterior, Superior) orientation for standardization. Subsequently, the intensity values were normalized to fall within the range of 0 to 1 using the 2nd and 98th percentiles to clip intensity values and reduce outliers [31]. A Gaussian filter with a value of 0.5 was used to reduce noise in the images. This value was chosen to balance noise reduction with the preservation of relevant anatomical details. Higher values compromise the definition of brain structures, while lower values are less effective in noise reduction. Additionally, a gamma correction with an exponent of 0.8 was applied to enhance the contrast of the images. Finally, background noise was removed by setting intensity values below the 1st percentile threshold to zero, retaining only the diagnostically significant regions of the image. After preprocessing, 2D slices were extracted from the 3D volumes along the axial, coronal, and sagittal planes, selecting a fixed number of central slices for each orientation to ensure comprehensive brain representation."}, {"title": "D. Training and Validation of the Models", "content": "The training process for automatic text report generation consists of two main phases executed on workstation equipped with an NVIDIA GeForce RTX 4090 GPU:\n\u2022 Fine-tuning of the BiomedCLIP model to generate embeddings from the MRI slices of the patients;\n\u2022 Fine-tuning of the T5 model to generate textual diagnos-tic reports based on the numerical embeddings previously extracted.\n1) Fine-Tuning and Validation of the BiomedCLIP Multi-modal Model: The model was adapted to the OASIS dataset and synthetic reports through the PEFT (Parameter-Efficient Fine-Tuning) library [32]. Specifically, the LoRA (Low-Rank Adaptation) [33] technique was used, which allows for effi-cient model fine-tuning by modifying only a small portion of the parameters. Specifically, LoRA was applied to the model's q_proj and v_proj modules. The LoRA con-figurations used for fine-tuning include an adaptation factor $r = 10$, a parameter $lora\\_alpha = 32$, and a dropout value of $lora\\_dropout = 0.3$.\nFor training, three distinct datasets were created using scikit-learn's train_test_split function: a training set (70%), a validation set (20%), and a test set (10%), generated from MR images and synthetic reports of the OASIS dataset. The multiple MRI slices of each subject are associated with the same synthetic report, allowing the model to learn to associate various brain regions with the overall medical description. The multimodal inputs (images and text) are preprocessed using the BiomedCLIP processor, necessary for tokenizing the reports and converting the images into tensors. During the training phase, which was conducted for 100 epochs with a batch size of 64, the embeddings generated for images and text were compared using a CosineEmbeddingLoss function. This loss function is based on cosine similarity and helps the model learn to align the image and text embeddings. The similarity target value was set to 1 to maximize the similarity between the image and text embeddings. An attention-weighted mechanism is also used to compute the weighted average of embeddings for each patient, aggregating information obtained from mul-tiple MRI slices. This process enables the model to learn multimodal relationships between brain images and textual reports, giving more importance to slices that are more relevant to the report content. Model optimization is carried out using the AdamW algorithm, with an initial learning rate of 1e - 5 and a weight decay of 0.01. Early stopping with a patience of 10 epochs and gradient clipping with a maximum norm of 1.0 were also implemented to prevent overfitting and stabilize the training. Validation is performed using the same loss function, and attention-weighted mechanism, calculating the similarity score between the embeddings for each patient. The learning rate is dynamically adjusted using the ReduceLROnPlateau scheduler, which reduces the learning rate when the validation loss reaches the plateau. Finally, the aggregated embeddings of images and text are extracted and saved for each patient to be subsequently used by the T5 model.\n2) Fine-Tuning and Validation of the T5 Model: Below is the procedure for using the large variant of the T5 model. The methodology was also tested with the small and base variants by selecting the corresponding model size during the configu-ration phase. The fine-tuning of the T5 model was performed using the multimodal embeddings generated by BiomedCLIP and diagnostic reports. The final objective is to generate complete clinical reports based on MRI representations and combined textual information. The optimization process of T5 consists of several phases. To improve the model's robustness and generalization, data augmentation techniques were applied during fine-tuning. Data augmentation is applied to both image embeddings and text reports, as follows:\n\u2022 Embedding augmentation: random Gaussian noise (with a standard deviation of 0.1) is added to the image embeddings to handle small fluctuations in the input data;\n\u2022 Text augmentation: 10% of the words in the reports are replaced with synonyms from the NLTK WordNet database [34], [35], selected using a random sampling strategy.\nInitially, the embeddings extracted from BiomedCLIP are combined with the synthetic textual reports of each patient. Before integration into T5, a projection network was intro-duced to adapt the dimensionality of the embeddings produced by BiomedCLIP (512 dimensions) to the dimensions required by T5 large (1024 dimensions). The projection layer consists of a linear layer, followed by a LayerNorm layer and a dropout layer (rate = 0.2) to prevent overfitting and normalize the projected inputs. The diagnostic reports are tokenized and encoded with the T5 tokenizer, maintaining a maximum length of 512 tokens.\nDuring fine-tuning the training data is organized in batches of 4 samples. To prevent overfitting, early stopping criteria is applied, terminating the training if no improvement in the validation loss is observed for 15 consecutive epochs. The CrossEntropyLoss function is used to compare the reports generated by the model with the reference reports. For model optimization, the AdamW algorithm is used with a learning rate of 1e-4 and a weight decay of 0.05. The ReduceLROn-Plateau scheduler is used to dynamically reduce the learning rate based on validation loss values with a patience of 10 epochs and a factor of 0.5. Additionally, gradient clipping with a maximum norm of 1.0 is applied to stabilize training. The report generation phase uses sampling techniques such as"}, {"title": "E. Testing and Evaluation of the T5 Model", "content": "The model performance evaluation was carried out using natural language generation metrics. Specifically, Bilingual Evaluation Understudy (BLEU), Metric for Evaluation of Translation with Explicit Ordering (METEOR), and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) were selected [7], [19].\n1) BLEU Score: This metric was originally used to evaluate the quality of machine-generated translations by comparing them to one or more reference translations. The BLEU score calculates a precision-based metric by counting the number of n-grams (continuous sequences of n items) in the generated text that match any reference translation. In the evaluation of the reports, precision was calculated for n-grams with values ranging from 1 to 4 (BLEU-1, BLEU-2, BLEU-3, BLEU-4):\n$Precision(n) = \\frac{\\text{# overlapping n-grams}}{\\text{# all n-grams (model-generated)}}$\nThe BLEU-n score formula is reported below:\n$BLEU-n = BP \\times \\exp{\\frac{1}{n} \\sum_{k=1}^n log (Precision(k))}$\nWhere BP refers to the brevity penalty and is calculated as:\n$BP = \\begin{cases}\n1 & \\text{if } c > r \\\\\n e^{(1-\\frac{r}{c})} & \\text{if } c \\leq r\n\\end{cases}$\nWhere c is the length of the model-generated text, and r is the length of the reference text. The BLEU score ranges from 0 to 1, with values closer to 1 indicating better agreement with the reference text. In the code implementation, the BLEU score was calculated using the NLTK library, specifically by using the sentence_bleu function [34]. The BLEU score for 1 to 4 n-grams was computed both for each individual patient with respect to the reference text and at the corpus level.\n2) ROUGE Score: ROUGE is a set of metrics used to evaluate the overlap between the model-generated text and the human reference text, where ROUGE-n measures the overlap of n-grams between the two. ROUGE captures both precision and recall, providing a more balanced evaluation. The metric is calculated as follows:\n$ROUGE-n = \\frac{\\text{# overlapping n-grams}}{\\text{# all n-grams in the reference text}}$\nROUGE-L, on the other hand, focuses on measuring the longest common subsequence between the model-generated text Y and the reference text X. It is calculated using the following formula:\n$ROUGE-L = \\frac{(1 + \\beta^2) \\times R \\times P}{(R + P \\times \\beta^2)}$\nWhere $R = \\frac{LCS(X,Y)}{m}$ and $P = \\frac{LCS(X,Y)}{n}$. Here, m is the length of X, n is the length of Y, and LCS(X, Y) is the length of the longest common subsequence between X and Y. The parameter $\\beta$ controls the weight given to precision (P) and recall (R) based on the specific task and their relative importance. The ROUGE scores range from 0 to 1, where 1 indicates a perfect similarity between the generated text and the reference text. In the code implementation, the RougeScorer function from the rouge_score library was used."}, {"title": "V. RESULTS AND DISCUSSION", "content": "This section presents the preliminary results obtained using the three versions of the T5 model: small, base, and large, evaluated on the OASIS-4 dataset as described in Section III. Specifically, the corpus-level scores (Table II) for the following metrics are reported: BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L (using the F1 score), and METEOR.\nThe results show similar performance across the T5 model variants, with some slight differences. The T5-small model achieved higher BLEU scores compared to T5-base and T5-large, particularly for BLEU-1 (0.5617) and BLEU-4 (0.1858). This suggests that the smaller model might be more effective at generating n-grams that match the reference text. On the other hand, the T5-large model showed superior performance in terms of ROUGE-1 (0.5908), ROUGE-2 (0.2565), and ROUGE-L (0.3719). These results indicate that the larger model might produce outputs that better capture the overall structure and coherence of the text, generating longer se-quences that align more closely with the reference text. As for the METEOR score, the T5-small and T5-large models achieved similar results (0.4167 and 0.4163, respectively), while at the corpus level, the T5-base model outperformed the other two (0.4271).\nThe boxplot in Fig. 3 illustrates the quartile distribution of the evaluation metrics, and Table III provides a detailed analysis of the main metrics. For the BLEU-4 score, although the median values are comparable across all three models, the 75th percentile is slightly higher for T5-Small (0.2207) compared to the other models. However, as highlighted in the boxplot 3, T5-Small shows greater variability than T5-Base and T5-Large, indicating less stable performance. Regarding ROUGE-1 and ROUGE-L, the median and third quartile values for T5-Large (0.5951 and 0.6324 for ROUGE-1, 0.3784 and 0.4092 for ROUGE-L) demonstrate the model's ability to generate text that better aligns with the structure and coherence of the reference text. These results are further confirmed by Fig. 3, where T5-Large shows narrower interquartile ranges and fewer extreme values, leading to more consistent and robust performance compared to T5-Small and T5-Base.\nThe Table IV shows a comparison of the performance of the T5 model in its three variants (small, base, and large) based on different dataset splitting ratios (Train/Validation/Test). The results indicate that the 70/20/10 split provides the best overall results for the BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR metrics, followed by the 60/30/10 configuration and finally the 80/10/10 configuration, which demonstrates the worst performance among the three ratios. The 70/20/10 split configuration provides overall better results as shown by the following metrics: BLEU-4 (T5-Large: 0.1827), ROUGE-1 (T5-Large: 0.5908), ROUGE-2 (T5-Large: 0.2565), ROUGE-L (T5-Large: 0.3719), and METEOR (T5-Base: 0.4271). This indicates that a split with a higher percentage of training data and a balanced proportion of validation data leads to more promising results. In contrast, a smaller amount of validation data, as seen in the 80/10/10 configuration, hinders overfitting control, reducing the model's generalization capabilities. In conclusion, the table highlights that the 70/20/10 configuration is optimal for all T5 model variants, as it ensures a better balance between training and validation data, resulting in improved report accuracy and consistency.\nFig. 4 shows an example of a text report generated using the T5-large model, which achieved the best results among its outputs in terms of ROUGE-1 (0.6854), ROUGE-2 (0.4076), ROUGE-L (0.5540), and BLEU-4 (0.3245) scores. Comparing the generated report with the reference text, several obser-vations can be made. The overall structure of the generated report closely aligns with the ground truth, adhering to a format similar to that of the synthetic reports. In terms of diagnostic accuracy, the generated report correctly identifies the diagnosis of Alzheimer's Disease Dementia, as reported in the reference text. However, there are some differences. For instance, the model generated the phrase \"very mild cognitive impairment\", whereas the ground truth indicates the condition as \"mild\". Additionally, the Sumbox assessment is described as \"very mild\" in the generated text, while in the ground truth it is reported as having a \u201cmoderate impact\u201d. There are also differences in the description of hippocampal atrophy. In fact, the generated report mentions \u201cmild atrophy", "severe atrophy\". A lexical inaccuracy was also observed, as the model generated the non-existent word \\\"hippopulation\\\", which likely resulted from an unintended fusion of two words.\nIn summary, the generated report shows an overall structure that is consistent with the ground truth; however, discrepancies and clinical details highlight the need for further refinement to improve its overall accuracy.\"\n    },\n    {\n      \"title\"": "VI. CONCLUSION AND FUTURE WORK"}, {"content": "This study proposes an innovative approach to address the limitations of VLMs and LLMs in the field of neuroimaging diagnostics by using structured clinical data and MR images to generate diagnostic reports. BiomedCLIP and T5 were utilized with OASIS-4 data, demonstrating the potential of creating a framework capable of generating diagnostic reports for Alzheimer's disease. Our preliminary results indicate that the proposed methodology can effectively integrate visual information and textual data to produce clinically detailed de-scriptions for neurodegenerative diseases such as Alzheimer's. However, there are some limitations that should be noted for an accurate interpretation of the results and to guide future research. One key limitation is the sample size used in this study. The study only employed data from the OASIS-4 cohort, resulting in a relatively small sample size of 468 patients after preprocessing. This sample size is significantly lower than the datasets typically used for training VLMs and LLMs, which often require thousands of samples to achieve robust generalization [4", "11": [17], "38": "."}]}