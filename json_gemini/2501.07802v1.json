{"title": "Visual Language Models as Operator Agents in the Space Domain", "authors": ["Alejandro Carrasco", "Marco Nedungadi", "Enrico M. Zucchelli", "Amit Jain", "Victor Rodriguez-Fernandez", "Richard Linares"], "abstract": "This paper explores the application of Vision-Language Models (VLMs) as operator agents in the space domain, focusing on both software and hardware operational paradigms. Building on advances in Large Language Models (LLMs) and their multimodal extensions, we investigate how VLMs can enhance autonomous control and decision-making in space missions. In the software context, we employ VLMs within the Kerbal Space Program Differential Games (KSPDG) simulation environment, enabling the agent to interpret visual screenshots of the graphical user interface to perform complex orbital maneuvers. In the hardware context, we integrate VLMs with robotic systems equipped with cameras to inspect and diagnose physical space objects, such as satellites. Our results demonstrate that VLMs can effectively process visual and textual data to generate contextually appropriate actions, competing with traditional methods and non-multimodal LLMs in simulation tasks, and showing promise in real-world applications.", "sections": [{"title": "I. Introduction", "content": "Large Language Models (LLMs) possess extensive domain-specific knowledge derived from their extensive pre-training, establishing themselves as invaluable tools across various fields. Since the emergence of the LLM trend, initiated with the first release of ChatGPT [1], these systems have undergone continuous development and have evolved into multimodal architectures. Multimodal models, such as GPT-40 [2], LLaMA 3.2 [3] and Claude with its latest 3.5 Sonnet model [4], integrate language understanding with non-language capabilities, including vision and audio processing. This progression unlocks new opportunities for developing intelligent agents capable of recognizing and interpreting patterns not only at a semantic level but also through components that can incorporate other types of unstructured data into prompts, significantly expanding their potential applications and impact.\nExtending these capabilities, Vision-Language Models (VLMs) build on multimodal principles by integrating visual reasoning into the LLM framework. By introducing new tokens into the prompts to process image frames, VLMs enable simultaneous semantic and visual reasoning. This enhancement is particularly valuable in dynamic applications like robotics, where the integration of vision and language reasoning enables systems to generate environment-responsive actions. Such actions, often described as descriptive policies, translate reasoning into meaningful, executable commands. Language models able to generate such commands are usually referred to as \"agentic\". In particular, models such as OpenVLA [5] further advance this paradigm by incorporating two specialized tokenizers: one for spatial reasoning and another for characteristic reasoning, solidifying VLMs as the cornerstone of interactive and adaptable systems [6].\nThe transformation of LLMs into highly capable LLM-based agents (LLMAs) marks a pivotal step toward achieving more human-like behavior in autonomous systems [7]. A key proving ground for these agents lies in video games, which simulate complex and dynamic open-world environments, offering an ideal testbed for interaction and evaluation"}, {"title": "II. Background", "content": "This section provides the necessary background on the foundational technologies and tools relevant to this work, including large language models (LLMs), vision-language models (VLMs), and the Kerbal Space Program Differential Games (KSPDG)."}, {"title": "A. Large Language Models (LLMs)", "content": "Large Language Models (LLMs) are advanced neural networks trained on extensive text corpora. During pre-training, LLMs are taught to complete sentences. After that, with limited fine-tuning, LLMs can be aligned to perform a wide range of natural language processing (NLP) tasks. These models, typically based on transformer architectures, have demonstrated remarkable capabilities in tasks such as reasoning, summarization, translation, and autonomous decision-making. The scalability of LLMs, coupled with their ability to generalize knowledge across diverse domains, has rendered them indispensable for applications in autonomous systems. In these circumstances, LLMs can efficiently process complex inputs and produce coherent and interpretable output.\nThe versatility of LLMs is often unlocked through prompt engineering, a technique that structures model inputs to elicit desired behaviors. By carefully designing prompts, users can guide LLMs to perform complex reasoning tasks or exhibit high levels of contextual understanding without requiring explicit additional training."}, {"title": "1. Prompt Engineering", "content": "Prompt engineering is a key methodology for leveraging the capabilities of LLMs. It involves crafting input prompts to guide the model's behavior effectively. This approach is particularly powerful in scenarios where models are deployed in dynamic or resource-constrained environments, allowing users to achieve task-specific outcomes with minimal or no additional training. Two prominent paradigms in prompt engineering are zero-shot and few-shot prompting.\nZero-shot Prompting Zero-shot prompting involves presenting an LLM with a query or task description without providing any examples. The model relies on its extensive training to infer the appropriate response. For example, a zero-shot prompt for translation might look like:\nTranslate the following sentence into French: \"The weather is lovely today\".\nThis approach is highly efficient for general-purpose tasks, as it does not require additional context or fine-tuning.\nFew-shot Prompting Few-shot prompting provides the model with a small number of examples within the prompt itself to establish context. This method enhances task-specific performance by grounding the model's output in the examples provided. For instance, a few-shot prompt for arithmetic reasoning could be structured as:\nQ: What is 3 + 5?\nA: 8\nQ: What is 7 + 9?"}, {"title": "Advanced Prompting Paradigms", "content": "To further enhance reasoning capabilities, advanced prompting paradigms such as\nChain of Thought (CoT) [16] and ReAct (Reasoning and Acting) [17] have been developed:\n\u2022 Chain of Thought (CoT): This paradigm encourages the model to produce intermediate reasoning steps before arriving at the final output. By breaking down complex problems into smaller, interpretable steps, CoT improves the model's accuracy and transparency in reasoning. For example, the following prompt can be included before the user input to activate CoT in the language model:\nQ: If a train travels 50 miles per hour for 3 hours, how far does it travel?\nLet's think step by step.\nA: First, calculate the distance per hour: 50 miles/hour.\nNext, multiply by the time traveled: 50 * 3 = 150 miles.\nTherefore, the train travels 150 miles.\n\u2022 ReAct (Reasoning and Acting): ReAct combines reasoning with decision-making to enable LLMs to handle interactive or agent-based tasks. The ReAct paradigm integrates logical deductions with contextual actions, making it suitable for autonomous systems and complex problem-solving. For example, the following prompt can be included before the user input to activate ReAct in the language model:\nQ: The room is dark, and I need to find a flashlight. What should I do?\nA: Reasoning: A flashlight is often stored in a drawer or cabinet.\nAction: Search the drawer for a flashlight.\nThese paradigms enhance the utility of LLMs in scenarios requiring high-skilled reasoning or agent-driven applications, making them integral to the advancement of AI-powered autonomous systems.\nStructured Outputs Modern LLMs support structured outputs, including JSON and other machine-readable formats, making them well-suited for programmatic applications. These capabilities allow users to guide the model in generating outputs that integrate seamlessly with APIs or software systems.\nFor example:\nTask: Provide a structured JSON response for the following user information:\nName: Alice, Age: 30, Location: New York.\nOutput:\n{\n\"name\": \"Alice\",\n\"age\": 30,\n\"location\": \"New York\"\n}\nBy clearly specifying the desired structure, users can ensure that the model's outputs are directly usable for downstream processes.\nFunction Calling Function calling builds on the structured JSON paradigm by enabling language models to interact with external codebases in an API-like manner. By specifying the desired tool or function in a structured JSON format, users can instruct the model to execute specific tasks or access external functionalities. For instance, a function call to retrieve weather information might look like this:"}, {"title": "B. Vision-Language Models (VLMs)", "content": "Vision-Language Models (VLMs) extend the capabilities ofLLMs by integrating visual information. By aligning textual and visual representations in a shared embedding space, VLMs can perform tasks that require understanding of both modalities. This multimodal capability makes them particularly suitable for simulation environments, where textual observations can be augmented with visual snapshots to improve situational awareness and decision making.\nRecent advances in VLMs have connected the gap between computer vision and NLP. These models typically combine pre-trained vision encoders with language models to create powerful multimodal architectures. VLMs gained significant attention in 2021 with the introduction of CLIP [18], which pioneered contrastive learning for aligning vision and language modalities, enabling zero-shot transfer to a range of visual tasks. As visual-language models matured, frameworks like Flamingo [19] and BLIP [20] refined multimodal interactions, further improving the merge between vision and language models. More recently, LLaVA [21] showcased how instruction-following language models, integrated with CLIP-based visual encoders, could effectively handle complex multimodal tasks. Today, multimodal LLMs, such as GPT-40 [2] and Claude 3.5 Sonnet [4], demonstrate unparalleled versatility, seamlessly integrating vision and language capabilities to serve as VLMs while excelling across diverse applications.\nOnce VLMs were established, their adaptation to robotics opened new frontiers. RoboFlamingo, for example, leverages pre-trained VLMs like OpenFlamingo, introducing fine-tuning strategies for effective language-conditioned robot control [6]. Similarly, OpenVLA [5] combines advanced visual encoders and LLMs to achieve high-performance robot manipulation, highlighting the potential of multimodal systems in robotics. These advancements underscore how vision-language models are reshaping the landscape of autonomous systems and robotics.\nVLMs have shown remarkable capabilities in various tasks, including:\n\u2022 Zero-shot image classification\n\u2022 Image-text retrieval\n\u2022 Visual question answering\n\u2022 Object localization\n\u2022 Multimodal reasoning\nThe architecture of modern VLMs often involves:\n1) A pretrained vision encoder (e.g., CLIP, DINOv2, SigLIP)\n2) A pretrained language model (e.g., Llama 2, Vicuna)\n3) A mechanism to align visual and textual representations (e.g., linear projections, attention mechanisms)\nBy leveraging large-scale pretraining on diverse datasets, these models can generalize to a wide range of vision-language tasks, making them valuable tools for multimodal artificial intelligence (AI) applications, including simulations and robotics [22].\nIt is crucial to ensure that these tasks cannot be performed effectively by an LLM. As demonstrated in [11], LLMs continue to outperform VLMs when they can comprehend their reward structure or scoring criteria."}, {"title": "C. Kerbal Space Program Differential Games (KSPDG)", "content": "Kerbal Space Program Differential Games(KSPDG) [23] is a simulation environment based on the Kerbal Space Program (KSP), developed by MIT Lincoln Laboratories as part of the space-gym library. Designed for reinforcement learning and differential game challenges, it offers a dynamic platform to navigate complex scenarios.\nIn KSPDG, agents operate in discrete steps, interpreting observations, such as relative positions, velocities, and fuel levels, and executing 4D actions: thrusts along the X-, Y-, and Z-axes, plus thrust impulse duration. Performance is evaluated on metrics such as approach time and distances to targets and obstacles. This process promotes adaptive decision making, enabling agents to optimize maneuvers in real time.\nKSPDG 2025 features two main scenario categories: Pursuer-Evader and Target Guarding (Lady-Bandit-Guard), detailed below."}, {"title": "1. Pursuer-Evader Scenarios", "content": "In the Pursuer-Evader scenarios, participants design autonomous agents to control a pursuer spacecraft to minimize the distance to an evading spacecraft. The scenarios differ in the evader's strategies:\n\u2022 E1: No evasive maneuvers by the evader.\n\u2022 E2: Random short-duration evasive maneuvers when the pursuer is within range.\n\u2022 E3: Structured full-thrust maneuvers to escape within a distance threshold.\n\u2022 E4: Constant prograde thrust aligned with the orbital velocity vector."}, {"title": "2. Lady-Bandit-Guard Scenarios", "content": "The Lady-Bandit-Guard (LBG) scenarios are 1-v-2 target guarding problems. Participants control the Bandit spacecraft, which aims to minimize the distance to the Lady spacecraft while maximizing the distance to the Guard spacecraft.\nScenarios vary in Lady and Guard policies as well as initial orbital configurations:\n\u2022 Policy Environment Identifiers (lg):\n1g0: Lady and Guard are passive.\nlg1: Passive Lady, Guard pursues Bandit using a heuristic target-zero_vel-target maneuver.\n1g2: Lady evades Bandit with out-of-plane burns, Guard pursues using heuristic target-zero_vel-target maneuvers.\nIg3: Code-obfuscated environment, passive Lady, advanced Guard algorithms.\n\u2022 Initial Orbit Environment Identifiers (i):\ni1: Lady and Guard in a circular orbit, Guard 600m prograde of Lady, Bandit in an elliptical orbit with an upcoming tangential conjunction at apoapsis.\ni2: Lady, Bandit, and Guard in the same circular orbit, Guard 600m retrograde of Lady, Bandit 2000m retrograde of Guard.\nThese scenarios are assessed using a fine-grained scoring equation defined by:\nscore = dm_lb\u00b2 + \\frac{a}{dm_bg^2 + b} \\tag{1}\nwhere:\n\u2022 dm_lb: Closest approach distance between the agent and the Lady (meters).\n\u2022 dm_bg: Closest approach distance between the agent and the Guard (meters).\na = 106: A scaling factor ensuring that approximately 100 meters of dm_lb is as beneficial as 100 meters of dm_bg is harmful.\n\u2022 b = 0.1: A positive offset to prevent division by zero and to balance the penalties and rewards appropriately."}, {"title": "III. VLMs as Software operators - use case in Kerbal Space Program", "content": "To explore a distant spacecraft, satellite, or orbital debris, an agent must match its orbit to that of its target by executing a complex set of orbit correction maneuvers to reduce their relative position and velocity to zero. This process of approaching a target is commonly referred to as rendezvous. Once the agent achieves proximity within a sufficiently close range, it can transition to preparation for docking with the target to carry out the intended task. These operations demand precise trajectory planning and real-time adjustments to account for orbital mechanics, ensuring a safe and efficient approach.\nFigure 1 depicts an overview diagram of our new VLM agent interaction. In the previous challenge edition, an LLM was designed to only utilize text user prompts in real time leveraged with a fine-grained system prompt. The resulting content was aligned with few-shot examples, generating a concise response and providing function calls in the proper format. For this year's challenge, we extended the workflow by assigning specific observations to either the vision or language components of the VLM, allowing visual data to be processed through the vision module and textual data through the language module, leveraging each component's strengths."}, {"title": "A. Prior Prompt Engineering", "content": "Our new agent retains similarities to its predecessor, which relied solely on language for operation [12]. The original KSPDG action space uses continuous numbers, which many models find challenging to interpret effectively [24], and these values were discretized in the same manner as in our approach. Similarly, the observation space, composed of distances and velocities, also relies on floating point numerical data, posing additional challenges for LLMs. To address these limitations, we augmented the observations with the prograde vector, as will be explained at the end of this subsection. Additionally, models specialized in mathematical reasoning, such as OpenAI's 01[25], exhibit latency levels that are impractical for the real-time missions addressed in this research. We discretize the action space into 9 distinct actions, encompassing various combinations of throttle levels and directional adjustments. Specifically, we include two throttle settings full throttle and full reverse throttle - for each axis of motion (X, Y, and Z), as well as a no-throttle option.\nThe following actions represent movement along the three axes of the spacecraft's reference frame:\n\u2022 X-axis: Left and right adjustments.\n\u2022 Y-axis: Forward and backward adjustments.\n\u2022 Z-axis: Up and down adjustments.\nEach axis allows for three possible actions: positive thrust, negative thrust, or no thrust. This configuration results in a total of 3 \u00d7 3 \u00d7 3 = 27 possible permutations of combined actions, enabling the agent to freely move in the 3D space.\nThe previous prompting strategy emphasized providing the agent with explicit telemetry details and a contextual mission goal from the observation space. It included structured information, detailed below:\n\u2022 Relative positions and velocities between the spacecraft, the target, and optionally, the guard to evade.\n\u2022 Spacecraft status such as fuel usage.\n\u2022 Augmented observations (not included by default in the ones given by the KSPDG challenge organizers) including the prograde vector.\nWhile effective in many scenarios, relying uniquely on textual prompts constrained the previous agent's capacity to comprehend the spatial environment, especially in dynamically evolving situations. To address this, we incorporate visual inputs and redesign the prompt engineering process, introducing examples that explicitly describe the game's dashboard graphic user interface (GUI) and most important mission metrics. Our previous research included several new metrics derived from data augmentation. Due to the vision capabilities, this study only augments observations by providing the prograde, an important component for mission control.\nThe prograde p can be calculated by:\np = R^{-1} \\frac{V_p-V_e}{||V_p - V_e||} \\tag{2}\nwhere:\n\u2022 R: Rotation matrix that transforms coordinates from vessel to celestial body reference frame.\n\u2022 Vp, ve: pursuer's and evader's velocity in celestial body reference frame."}, {"title": "B. Vision Few-Shot Prompting", "content": "Both system and user prompts are crucial for the model's performance, guiding the LLM to interpret the interface much the same way a human relies on a manual for reference. By incorporating visual data through in-game screenshots, the vision agent can dynamically observe key elements that are essential to completing the task. These visual clues enhance the textual inputs, allowing the agent to form a deeper understanding of the mission state and respond more effectively to changing conditions.\nThe few-shot examples in Fig. 2 demonstrate how to process and act upon this visual information in conjunction with telemetry. These example demonstrations highlight:\n\u2022 How to interpret the navball to align with the prograde or retrograde vectors.\n\u2022 How to use the visual representation of the spacecraft's orientation to adjust maneuvers.\n\u2022 How to combine visual screenshots with textual telemetry to generate accurate action commands.\nThe actual telemetry read by the model receives less observations than our previous agent from [12], reading only relative positions and prograde values collected through data augmentation."}, {"title": "C. Results", "content": "As mentioned earlier, this paper is a preliminary development and evaluation of our proposed agents. Therefore, this section only includes a simple approach metric, the distance from the Lady vessel as well as the distance to the Guard spacecraft, which the Bandit, our spacecraft, must avoid in this mission. Finally, we present the scoring metric defined in Section II.C.2 along with the latency metrics for each VLM model.\nAs shown in Table 1, both the LLM and the VLM agents* outperform most traditional methods. Furthermore, we observe that LLMs demonstrate strong reasoning capabilities, as reflected in their ability to balance the competing objectives of minimizing the distance to the Lady while maintaining a safe distance from the Guard.\nThis scoring function rewards approaches that minimize dm_lb while penalizing those that reduce dm_bg excessively. As such, it reflects the dual objectives of the task: approaching the Lady while keeping a safe distance from the Guard."}, {"title": "IV. VLMs as Hardware operators - Robotics use case", "content": "This section explores the integration of Vision-Language Models (VLMs) with robotic systems, focusing on their application in space hardware inspection. By leveraging real-time image processing and model-driven decision-making, the system aims to enhance autonomous robotics for complex tasks in dynamic environments. The following details the interaction between the robot, its camera, and the VLM in diagnosing potential issues within space hardware."}, {"title": "A. VLM - Robot Interaction", "content": "Historically, robots have been highly specialized, excelling at narrowly defined tasks such as assembly line work or material handling. While those robots are effective within their specific domains, they lack the versatility to handle a wide range of activities. Typically, each new task would require the development of a separate robot, creating inefficiencies and increasing the need for specialized infrastructure. However, a growing trend in the robotics industry is the shift toward more generalized robots\u2014like humanoid forms\u2014that can adapt to a broad spectrum of tasks. This trend has been particularly evident in industries such as automotive manufacturing, where humanoid robots are now used not only for repetitive tasks like welding but also for more complex operations such as quality inspections and maintenance. The ability to use the same tools as human workers and switch between diverse tasks reduces the need for multiple specialized machines, making operations more efficient and cost-effective.\nThe proposed system employs an xArm 7 robotic arm [26] equipped with an Intel RealSense camera [27] to inspect and diagnose potential issues with space hardware. The hardware setup allows the robot to capture red, green, blue, depth (RGBD) images and stream them as frames to the VLM, enabling real-time interaction between the robot and the environment. In the system that would ultimately be deployed, a LIDAR sensor would be used instead of this camera to achieve higher precision.\nThe overall pipeline, illustrated in Fig. 3, operates by dividing the video stream from the RealSense camera into individual image frames, which are ingested by the VLM one at a time. Each frame is processed sequentially, and the model reasons about its content to determine the next control action. The generated action includes positional changes (\u2206x), rotational adjustments (\u2206\u03b8), and a boolean flag (x) to decide whether to capture an annotated image. This can be expressed mathematically as:\nat = F(It, St, P) \\tag{3}\nat = (\\Delta\\chi_t, \\Delta\\theta_t, \\chi_t) \\tag{4}\nwhere\nrepresents the action generated at timestep t:\n\u2022 Ax\u2081: Positional change,\n\u2022 \u0394\u03b8\u2081: Rotational adjustment,\nXt: Boolean flag indicating whether to capture an annotated image.\nHere, It denotes the input image frame at time t, St is the robot's corresponding state (e.g., Cartesian position and joint angles), and P is the system prompt containing task-specific instructions. Frepresents the Vision-Language Model (VLM) that processes these inputs to generate the action. Importantly, the system ensures that no subsequent frames are processed until the current action is executed, maintaining a synchronized pipeline between perception and control.\nInference is the most time-consuming component of LLM processing. To address this limitation, we implement advanced optimization strategies, including refined prompting techniques and LLM-specific mechanisms such as Flash Attention [28] or open-source models; for proprietary models, we consider strategies to reduce the output prompt size via few-shot prompting examples or user-side prompt caching. These measures aim to significantly minimize response times, ensuring efficient and responsive performance for mission-critical real-time operations.\nOnce an action is completed, the robot updates its position and orientation, and the pipeline resumes processing the next frame. This iterative approach ensures structured decision making and efficient data collection, with images and associated descriptions stored for post-mission analysis."}, {"title": "B. Methodology and Preliminary Results", "content": "The latest robotic trends have led us to explore OpenVLA [5], a VLA model that unifies \u201ccontroller\u201d and image capture decisions through a 7-dimensional action space (a 6D Cartesian vector plus the boolean snapshot decision, x). This model demonstrates great results compared to other VLMs and state-of-the-art diffusion models, and also offers a generalizable framework for several robotics tasks.\nA model used for a complex scenario such as the one visualized in Table 3 must be either trained or fine-tuned to ensure effectiveness, requiring domain-specific datasets with a variety of desired tasks. In this work, we fine-tune OpenVLA on a curated set of training samples, tracking standard performance metrics (e.g., accuracy and loss curves) to evaluate convergence. Furthermore, we develop a real-time teleoperation interface that records the robot's states - including Cartesian positions, joint angles, and efforts - via keystrokes. This setup serves both as a mechanism for data collection and a means for interactive validation of the fine-tuned model.\nOur fine-tuning process uses an HDF5 dataset, a hierarchical format that organizes data into groups and datasets. Each session in the dataset represents a training episode with multiple frame groups. These frame groups include image data, robot states, and a single instruction for the entire episode. By shuffling the frames within each session, the model learns to predict actions for each timestep independently, avoiding reliance on the sequence of frames and improving its generalization.\nTable 3 summarizes the performance metrics from a preliminary fine-tuning on a 90-10 split dataset with only 10 episodes, significantly fewer than the recommended 50 for effective training."}, {"title": "V. Perspectives", "content": "Recent advancements in Visual Language Models (VLMs) show great promise for transforming the way we operate spacecraft and robots in space. This emerging technology offers significant potential for a wide range of applications, from satellite inspection and servicing to in-orbit assembly and collaborative missions. Unlike traditional, deterministic approaches that rely on well-defined dynamics, kinematics, and mission objectives, VLMs provide a versatile framework capable of adapting to the complexities of real-world space environments. These environments are often characterized by uncertain parameters such as the mass or moment of inertia of target spacecraft, dynamic lighting conditions (especially in low Earth orbit), and rapidly changing mission objectives. VLMs are well-suited for tackling these challenges, as they allow for online decision-making and flexible responses to unanticipated scenarios.\nTraditional approaches have had success in more limited, well-defined scenarios such as cooperative rendezvous missions-where the spacecraft's actions and objectives are relatively well-defined. However, these methods are increasingly inadequate for more complex and dynamic tasks, such as those required for satellite inspection, servicing, and cislunar missions. In such cases, VLMs enable spacecraft and robotic systems to operate autonomously or with human assistance, dynamically adjusting to changing conditions and evolving mission goals. Initial tests, such as those performed on the UFactory xArm 7 and humanoid form factors, have shown promise in demonstrating the potential for this technology. The humanoid design is especially valuable for its versatility, providing a flexible platform for a variety of space-based tasks, from collaborative operations to assisting human operators in real-time. As this technology continues to develop, it is expected to play a crucial role in overcoming the increasingly difficult challenges of space exploration and operations."}, {"title": "VI. Conclusions", "content": "In conclusion, Large Language Models (LLMs) have evolved significantly from their initial form, becoming increasingly capable and versatile tools in a wide range of domains. The advent of multimodal models, such as GPT-40 and LLaMA 3.2, has expanded the ability of LLMs to process and integrate diverse types of unstructured data, enhancing their utility in complex tasks that require both language and non-language reasoning. This transformation is particularly evident in Vision-Language Models (VLMs), which integrate visual and linguistic processing to enable more sophisticated reasoning, particularly in dynamic environments like robotics. As these models continue to advance, they pave the way for intelligent agents capable of executing actions that are both contextually aware and adaptable.\nThe transition from LLMs to LLM-based agents (LLMAs) marks a significant leap toward creating more autonomous, human-like systems. Video games, with their complex and interactive environments, have become an important proving ground for these agentic models. However, despite the considerable progress in VLMs, challenges remain, particularly in visual reasoning capabilities, which still require refinement to reach the level of performance seen in purely language-based agents.\nOur research builds on these foundational developments by exploring the application of LLMs and VLMs in the space domain, with a focus on space control tasks. We propose an innovative end-to-end space control framework that spans both software-based spacecraft control and hardware-oriented robotic inspection of space objects. This dual approach offers valuable insights into the potential of LLMs for both simulation and real-world applications in space exploration and satellite maintenance. As we move forward, our work contributes to the ongoing evolution of LLMs and their integration into autonomous systems across diverse fields, with the space domain serving as an exciting and challenging frontier for future research."}]}