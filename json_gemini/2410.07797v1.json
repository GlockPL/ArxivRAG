{"title": "Rewriting Conversational Utterances with Instructed Large Language Models", "authors": ["Elnara Galimzhanova", "Cristina Ioana Muntean", "Franco Maria Nardini", "Raffaele Perego", "Guido Rocchietti"], "abstract": "Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models' most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user's requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "Since their introduction, Large Language Models (LLMs) have impressed with their capabilities in dealing with tasks such as question answering, text summarization, coding, and translation, with performances that are comparable to those of human annotators. Thanks to their ability to perform tasks via few-shot learning, LLMs can learn from just a few examples, considerably expanding the range of applications supported and lowering the effort needed for targeting novel tasks. This feature has been successfully exploited to train Instructed LLMs, where methods from reinforcement learning with human feedback (RLHF) are used to directly instruct the model to act following the user's intention [1].\nAs a result, we assisted a new gold rush for part of the major tech companies to show their new intelligent systems. At first, we witness the introduction of ChatGPT, powered by a GPT-3.5 model. Then, we witness the release of a novel version of Bing search powered by GPT-4. The availability of the GPT-4-powered Bing search engine sets a definitive shift from a search paradigm based on \"ten blue links\" returned as an answer to a user query to a natural-language answer that is then returned to the user. Such an autonomous system automatically chooses the most relevant documents and extracts and elaborates the relevant information that is then presented to the user in the form of an answer to her/his query. This novel paradigm that exploits the dialogue to interact between the user and the search system can indeed provide a more friendly and natural way of interacting with the search service. In this paper, we move a step forward in an orthogonal direction by studying the ability of instructed LLMs to improve the retrieval effectiveness of a state-of-the-art search engine in a conversational setting [2]\u2013[4]. We aim to answer two main research questions:\nRQ1 Can an instructed LLM improve conversational search effectiveness by automatically rewriting the users' ut- terances to allow the search engine to retrieve more precise and relevant results?\nRQ2 Which prompting template performs best in order to generate rewritten queries that enhance retrieval performance?\nWe investigate the research questions above by adopting the Conversational Assistance Track (CAsT) framework provided by TREC for training and evaluating models in open-domain information-centric conversational dialogues [2].\nThe characteristics of conversational utterances, i.e., missing context from previous questions, topic shifts [5], [6], and implied concepts from previous answers, pose new challenges to deal with, which are a direct consequence of the paradigm shift introduced by conversational search. They heavily impact the performance of standard information retrieval techniques. Query rewriting techniques applied on a per-utterance level answer these challenges as they help propagate the context throughout the conversation and deal with possible topic shifts. The novel contributions of this work are thus the following:\n\u2022\n\u2022\nWe investigate utterance rewriting in conversational search using an instructed LLM and specifically designed prompt- ing templates. Given an utterance and its context, we prompt the model asking to generate a rewriting of the utterance with the goal of enhancing the retrieval effectiveness of a state-of-the-art information retrieval system. This approach allows us to evaluate the ability of an instructed LLM to deal with the context of a conversation and possible topic shifts that may occur. At the same time, we inspect its ability to rewrite natural language utterances containing ambiguities, coreferences, omissions, acronyms, and colloquial grammar misuses.\nWe present five different prompting templates to rewrite the utterances. Each prompt has been evaluated in an end-"}, {"title": "II. RELATED WORK", "content": "a) Conversational search: Query rewriting is central in modern web search as it better models the user's information need and enhances retrieval effectiveness [7]. Similar challenges arise in conversational search, since utterances, like queries, may be ambiguous or poorly formulated.\nConversational utterance rewriting aims to reformulate a concise request in a conversational context to a fully specified, context-independent query dealing with anaphoras, ellipses, and other linguistic phenomena [5], [8]. These techniques aim at identifying terms previously mentioned in the conversation to expand the current utterance profitably [6], [9]\u2013[11]. In this line, Aliannejadi et al. propose a novel neural utterance relevance model based on BERT that helps identify the utterances relevant to a given turn [9]. Voskarides et al. [10] model query rewriting for conversational search as a binary term classification task and introduce QuReTeC, a Bi-LSTM model that selects the valuable terms in context to enrich the query.\nOther approaches rewrite the utterances by exploiting a fine- tuned neural model [12]\u2013[15]. Yu et al. presents CQR, a few shot generative approach to solve coreference and omissions in conversational query rewriting [12]. The authors propose two methods to solve coreference and omissions to generate weak supervision data that are then used to fine-tune GPT-2 to rewrite conversational queries. Results show that on the TREC CAST Track a weakly-supervised finetuning of GPT-2 improves the ranking accuracy by 12%.\nVakulenko et al. [14] approach the problem by tackling conversational question answering. The authors propose a question-rewriting technique that translates ambiguous requests into semantically-equivalent unambiguous questions.\nIn more recent works, several papers exploit pre-trained language models to represent queries and documents in the same dense latent vector space and then use the inner product to compute the relevance score of a document to a given query. In conversational search, the representation of a query can be computed in two different ways. In one case, a stand-alone contextual query understanding module reformulates the user query into a rewritten query, exploiting the context history [16], and then a query embedding is computed, e.g. using embedding models such as ANCE [17] or STAR [18]. Alternatively, the learned representation function is trained to receive as input the query together with its context history and to generate a query embedding that is more similar to the manual query embeddings [19]. In both cases, dense retrieval methods are used to compute the query-document similarity by deploying efficient nearest neighbor techniques over specialized indexes, such as those provided by the FAISS toolkit [20].\nb) Large Language Models: LLMs based on transformer architectures such as GPT are trained on large corpora of text data to comprehend and produce natural language [21], [22]. The pre-trained models produced with unsupervised training [23] can be easily fine-tuned for various tasks in a supervised setting. InstructGPT, based on GPT-3, has been fine- tuned using human feedback to make it better at following user intentions [1]. Bidirectional and Auto-Regressive Transformer (BART) integrate the strengths of two established models, i.e., BERT and GPT-2, and are trained using a denoising autoen- coder approach to understand text structure and semantics, as well as generate fluent and coherent text [24]. Another instructed LLM model of the GPT family is ChatGPT\u00b9, which is explicitly tailored for conversational applications [25].\nInstructed LLMs such as ChatGPT are easily adaptable to new tasks and domains, making them very useful in various tasks. Wei et al. [26] propose ChatIE, a framework that employs ChatGPT to perform zero-shot Information Extraction (IE) tasks via multi-turn question-answering and claim that their method can achieve impressive results and surpass some full-shot models across three IE tasks. Sun et al. [27] found that ChatGPT can perform as well as, or better than, supervised methods in information retrieval relevance ranking when guided by domain-specific guidelines. The models mentioned earlier achieve impressive results in many NLP tasks, and their applications are many, from medicine to finance and beyond. With proper instructions, these models can solve a vast variety of tasks, making them valuable tools for researchers and developers alike. ChatGPT is the instructed LLM we use in our experiments.\nLately Mao et al. [28] conducted a work that studies the impact of LLMs. They focus on capturing the contextual conversational search intent through the use of GPT-3. The authors evaluate their findings in an ad-hoc dense retrieval scenario, using ANCE embeddings [17] for computing the similarity scores between documents and queries. We use their best-performing prompt in our experimental setting to see its effectiveness in our framework.\nOur Contribution. This work contributes to the line of rewriting conversational utterances with generative models. Differently from previous works, we assess the capabilities of"}, {"title": "III. METHODOLOGY", "content": "Our goal is to understand to which extent a state-of-the- art instructed LLM can be used to improve conversational search effectiveness. To this respect, this work assesses with reproducible experiments the rewriting capabilities of ChatGPT (RQ1) and investigates the impact of different prompts and instructions on the effectiveness of a two-stage conversational search pipeline (RQ2). In Table I, we introduce the notation used to describe our task. Our rewriting system O, based on an instructed LLM, can take as input many of the elements described in the table in order to perform the rewriting of the current utterance u\u1d62 into a rewritten version \u00fb\u1d62. More formally, a typical rewriting request consists of the following:\n$\\Theta(s, \\mathcal{E}, \\mathcal{C}, p, u_i) = \\hat{u}_i,$\nwhere s represents the scope, i.e., the general task instruc- tions of how we want the system to behave, \\mathcal{E} is a conversation example different from the current one, \\mathcal{C} is the context of u\u1d62, and p is the prompt accompanying u\u1d62, which explicitly instructs detailing the rewriting request by adding specific desired characteristics, e.g., \u201cconcise\u201d, \u201cverbose\u201d, and \u201cself- explanatory\u201d.\n(1)"}, {"title": "A. Instructed LLM", "content": "We employ ChatGPT as the instantiation of \u0398. Specifically, we employ the gpt-3.5-turbo model. As indicated in the ChatGPT API description\u00b2, the model takes \u201ca series of messages as input and returns a model-generated message as output\u201d. Since the model does not provide memory or session retention, in each interaction, we enclose the interaction history of previous turns of the conversation into the current request. This leads to having a conversational-style request, similar to an actual dialog.\nWe adapt our utterance rewriting requests to the input structure of the gpt-3.5-turbo model. The requests are composed of three main elements: system, user, and assistant. The \u201csystem\u201d content is provided at the start of the session to specify the scope of the following interactions, in our case s. The \u201cuser\u201d and \u201cassistant\u201d, on the other hand, indicate the interactions between the user and ChatGPT, as a series of user instructions/requests consisting of prompt and current original utterance (p, ui), and the corresponding assistant response containing the rewritten utterance \u00fbi.\nTo better understand what the best way of prompting the system is, we experiment with different ways of providing ChatGPT with the prompt and the context."}, {"title": "B. Prompting ChatGPT", "content": "We present five different prompts p to ask the instructed LLM to rewrite the utterances of a conversation \\mathcal{U}.\nThe typical request submitted through the ChatGPT APIs\u00b3 contains the elements detailed in Eq. 1, namely, scope, example, context, prompt, and current utterance. For all five prompts the example \\mathcal{E} consists of an exemplary conversation, chosen randomly from the dataset and not related with \\mathcal{U}, where the user inputs are the original utterances, and the assistant inputs are instead the same utterances rewritten manually. Moreover, the context \\mathcal{C} consists of the previous utterances of \\mathcal{U}, where the user inputs are the original utterances u\u2081,..., u\u1d62\u2212\u2081, and the assistant inputs are instead the same utterances rewritten by the model, \\hat{u}_1, ..., \\hat{u}_{i\u22121}. The only exception to this request template is the prompt P1, where the context consists of the previous utterances u\u2081,..., u\u1d62\u2212\u2081, and the assistant inputs consist of the previous utterances rewritten by the model \\hat{u}_1, ..., \\hat{u}_{i\u22121} together with the generated answers r\u2081, ..., r\u1d62\u2212\u2081.\nThe above structure of the requests allows us to assess not only the rewriting capabilities of the model but also its proficiency in retaining and exploiting the context information fed to the system. We now detail the five prompts with their specific characteristics and the intuition behind each of them."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To assess the utterance rewriting quality, we submit \\hat{u}_i as a query to a two-stage information retrieval pipeline. We evaluate the effectiveness of the different rewriting strategies using the TREC CAST framework [2]\u2013[4], which allows us to perform an objective evaluation by comparing our results to those obtained by state-of-the-art competitors4."}, {"title": "A. Conversational Datasets", "content": "Our experiments are based on the TREC Conversational Assistant Track (CAsT) 2019 and 20205 datasets. The CAsT 2019 [2] dataset consists of 20 human-assessed test conversa- tions, while CAST 2020 [3] includes 25 conversations, with an average of 10 turns per conversation. The CAsT 2019 and 2020 datasets include relevance judgments at the passage level. Conversations are provided with original and manually- rewritten utterances. The manually-rewritten utterances are the same conversational utterances as the original ones, where human assessors resolve missing keywords or references to previous topics. Relevance judgments have a three-point graded scale and refer to passages of the TREC CAR (TREC Complex Answer Retrieval), the MS-MARCO (MAchine Reading COm- prehension) and the WaPo (TREC Washington Post Corpus) collections for CAST 2019 and 2020 for a total of 38,636,520 passage. In these datasets, questions within a conversation are characterized by anaphora and ellipses. They imply a big part of the context and miss explicit references to the current topic."}, {"title": "B. Baselines", "content": "We assess the retrieval effectiveness of original, manually- rewritten, and automatically-rewritten utterances. In detail, we consider the following rewriting methods and baselines:\n\u2022 Original utterances: raw utterances provided by TREC CAST.\n\u2022\nManual utterances: manually-rewritten utterances by hu- man annotators provided by TREC CAST.\n\u2022 QuReTeC [10]: utterances are rewritten with a BiLSTM sequence to sequence model trained for query resolution.\n\u2022 CQR self-learn cv [12]: utterances are generated in two steps, first with a GPT-2 model trained with self-supervised learning to generate contextual utterances containing few information presented in previous utterances. The second step is performed with a GPT-2 model fine-tuned on manual rewrites via five-fold cross-validation.\n\u2022 CQR rule-based cv [12]: utterances are generated in two steps, first with a rule-base approach that deals with omissions and coreference and successively rewritten with a GPT-2 model fine-tuned on manual rewrites via five-fold cross-validation.\n\u2022 Prompt E [28]: although the results by Mao et al. are achieved on a different generative model, i.e., GPT-3, we use their prompt in our experimental framework to compare its retrieval performance against ours."}, {"title": "C. Two-stage Retrieval", "content": "To evaluate and compare the different utterance rewritings, we index the TREC CAST collections by removing stopwords and applying Porter's English stemmer. We use PyTerrier [29] to build the information retrieval pipeline, which is composed of two stages:\n\u2022\nThe first stage performs document retrieval on the indexed collection with the DPH weighting model [30], using the raw, manually, and automatically-rewritten utterances;\n\u2022 The second stage performs reranking of the top-1000 candidates retrieved by the first stage by using the MonoT5 model [31] made available in PyTerrier6.\nWe measure the retrieval effectiveness of the first stage and of the second stage using the following metrics: Mean Reciprocal Rank (MRR), Precision@1 (P@1), Normalized Discounted Cumulative Gain@3 (NDCG@3), and Recall@500 (R@500). MRR and NDCG@3 are standard metrics used for evaluation purposes in the TREC CAsT framework while the others are included to provide a more comprehensive evaluation of the retrieval capabilities of the first-stage (R@500) and the reranking capabilities of the second-stage (P@1)."}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we discuss the experimental results on CAST 2019 and 2020 datasets to assess the various rewriting strategies and compare them with the baselines."}, {"title": "A. First-stage Retrieval", "content": "In Table III, we report the results obtained when performing document retrieval using the DPH weighting model [30]. Results refer to the first-stage retrieval pipeline on both the CAST 2019 and CAsT 2020 datasets. We also experiment with other weighting models, i.e., BM25 [32]. We do not report them as their results are worse than those achieved by DPH.\nThe performance of our methods and baselines range between the ones obtained for the original and the manually-rewritten utterances. Considering CAsT 2019, P5 is the best-performing prompt when looking at MRR while P1 is the best-performing prompt in terms of Precision@1 and NDCG@3. Regarding R@500, the QuReTec baseline is the best-performing method. When performing the statistical significance evaluation using a two-paired t-test (p-value < 0.05) with the Bonferroni correction [33], the results achieved by our prompts are not statistically different from the state-of-the-art baselines, except for R@500 for P2, P3, and E.\nImproved results are achieved when rewriting the utterances of the CAST 2020 evaluation dataset. The best-performing rewriting method is based on P1, where all metrics show considerable gains over the QuReTec baseline. For P@1 and MRR, the improvement achieved by P1 is statistically significant when compared to the QuReTec baseline, with a 21.6% gain in MRR and 31.7% in P@1. NDCG@3 and R@500 increase by 17.1% and 10.6%, respectively. We remind the reader that P1 also considers the generated answers to the previously rewritten questions to produce the current rewriting. In fact, it is worth noting that, compared to CAST 2019 where most relevant concepts could be found in the previous utterances, for CAST 2020, some missing relevant concepts that fill out the context, can be found only in the responses and not in the utterance history. Results show that by generating the answers to the user requests and instructing the model to use them in the rewriting phase, we obtain improved results.\nThe fact that, independently of the dataset considered, our few-shot rewriting system obtains results as good as\u2014or better than\u2014state-of-the-art techniques should be further exploited in future work."}, {"title": "B. Second-stage Retrieval", "content": "In Table IV, we report the end-to-end results obtained with CAST 2019 and 2020 when performing document re-ranking using the MonoT5 model in the second-stage retrieval pipeline.\nOur intuition is that because our rewriting techniques produce verbose and well-formed utterance rewritings, it would be beneficial to use a LLM-based model such as T5, so as to effectively exploit the information added by the gpt-3.5-turbo model. We can see that the performance obtained by the generated rewritings achieves higher results than those obtained by the CQR and QuReTec competitors for prompts such as P1, P5 for CAsT 2019, and for all prompts for CAST 2020.\nThe winning method for CAST 2019 is P5, with an MRR of 0.8119 (3.3% increase), P@1 of 0.7283 (5.9% increase), NDCG@3 of 0.5343 that is slightly better than the one provided by QuRETec, i.e., 0.5330. Consistent with the first stage, also in the second-stage retrieval, the results are better with respect to the QuReTec baseline, except for R@500, although not statistically significant.\nWhen considering the CAST 2020 evaluation dataset, our rewriting methods show significant improvements after rerank- ing. In this case, we have a clear winner, i.e., P1, for which all metrics improve over QuReTec in a statistically-significant way. The MRR increases by 25.2%, the P@1 by 31.7%, the NDCG@3 by 27.0%, and the R@500 by 11.5%. Also, for P2, we have a statistically-significant improvement of 22.17% in terms of NDCG@3.\nEven in the second stage of retrieval, we obtain results as good as or better than\u2014state-of-the-art competitors, confirm- ing that instructed LLMs are effective in rewriting utterances in a multi-turn conversational setting."}, {"title": "C. Answering our Research Questions", "content": "RQ1. We affirm that using an instructed LLM to rewrite utterances helps the effectiveness of the retrieval system. In fact, we can observe that for the CAST 2020 dataset, we obtain significant improvements over the QuReTeC baseline, while for the CAST 2019 we achieve the same results, and in some cases, we outperform QuReTeC and the two CQR competitors.\nThe results achieved also show that, although the LLM has not been fine-tuned explicitly for utterance rewriting, it provides competitive results compared to the state of the art. This confirms the ability of these models to perform a variety of tasks via few-shot learning, thus lowering the effort needed for targeting novel tasks. In fact, custom-made models for utterance rewriting in conversational search, i.e., QuReTec, reach worse results on CAST 2020 than an instructed LLM with well-designed prompts. We explain these results as a consequence of the capability of an LLM to deal with different datasets and domains, keeping a rewriting quality higher than other systems trained on limited data and thus characterized by a lower generalization power.\nRQ2. For what concerns the best way of prompting the LLM, the best results are obtained with P1 for CAST 2020, while with P1 and P5 for CAsT 2019. While for some of the prompts discussed we clearly explicit the scope of the rewriting (e.g.,", "system[...]": "n P2), in both P1 and P5 this information is not explicit, suggesting that this kind of instruction is not useful to obtain better rewritings.\nMoreover, in both cases, there is a clear indication of how to exploit examples and context from the previous interactions. The difference is that P1 explicitly asks the model to also add previously generated answers to the context and use all the information for generating the rewriting \\hat{u}_i. This proved particularly effective in the case of CAsT 2020. This could also be the reason why QuReTec underperforms as, by design, it only focuses on the previous utterance and does not integrate the content of the answers for generating the rewriting. Therefore, after establishing the best-performing prompts and observing that they both make use of the context, we can conclude that providing examples can have a significant impact on the model's capabilities in performing the chosen task."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we proposed several methods for using an instructed LLM for the conversational utterance rewriting task.\nWe focused on assessing if such type of model is suitable for this task and if it is competitive with the current state-of-the-art rewriting techniques, which use models specifically fine-tuned for the task. We also studied different prompting techniques to assess the most effective ways to instruct the model using 5 prompt formulations.\nWe evaluate our proposals on the publicly-available TREC CAST 2019 and CAST 2020 datasets. We provide a com- prehensive experimental evaluation of our proposed five ways of prompting the instructed LLM and state-of-the-art conversational rewriting baselines by assessing their retrieval effectiveness in a two-stage retrieval pipeline.\nExperiments show that, in most cases, our proposed rewriting methods outperform the baselines. The largest gain is achieved for CAST 2020 with increases in MRR by 25.2%, in P@1 by 31.7%, in NDCG@3 by 27.0%, and in R@500 by 11.5%. These results are obtained using prompt P1, in which the system is also required to consider previous answers when rewriting the current utterance. We can conclude that using an instructed LLM is beneficial for the utterance rewriting task in conversational search. These models can become a useful tool to further expand rewriting approaches and set new state-of-the-art standards.\nFuture Work. As future work, we are interested in studying how instructed LLMs can be used to generate synthetic data that can be exploited in other tasks of conversational search or even for enriching conversational datasets with weak supervision labels. The limited number of assessed conversations is in fact one of the main limitations in the conversational search domain. Moreover, we are interested in assessing the sensibility of prompting, i.e., how the utterance rewriting changes with respect to variations in the prompt and how it influences the retrieval performance, in a systematic and comprehensive way."}]}