{"title": "Understanding Artificial Neural Network's Behavior\nfrom Neuron Activation Perspective", "authors": ["Yizhou Zhang", "Yang Sui"], "abstract": "This paper explores the intricate behavior of deep neural networks (DNNs) through\nthe lens of neuron activation dynamics. We propose a probabilistic framework that\ncan analyze models' neuron activation patterns as a stochastic process, uncovering\ntheoretical insights into neural scaling laws, such as over-parameterization and the\npower-law decay of loss with respect to dataset size. By deriving key mathematical\nrelationships, we present that the number of activated neurons increases in the form of\n\\(bN(1-(\\frac{Do+b}{D+N})^b)\\), and the neuron activation should follows power-law distribution. Based\non these two mathematical results, we demonstrate how DNNs maintain generalization\ncapabilities even under over-parameterization, and we elucidate the phase transition\nphenomenon observed in loss curves as dataset size plotted in log-axis (i.e. the data\nmagnitude increases linearly). Moreover, by combining the above two phenomenons\nand the power-law distribution of neuron activation, we derived the power-law decay\nof neural network's loss function as the data size scale increases. Furthermore, our\nanalysis bridges the gap between empirical observations and theoretical underpinnings,\noffering experimentally testable predictions regarding parameter efficiency and model\ncompressibility. These findings provide a foundation for understanding neural network\nscaling and present new directions for optimizing DNN performance.", "sections": [{"title": "Introduction", "content": "Deep neural networks (DNNs) have achieved unprecedented success across a wide range of applica-\ntions, including natural language processing, computer vision, and scientific discovery (11,18,7,3).\nDespite these advancements, understanding the underlying mechanisms that govern the behavior\nof DNNs remains a fundamental challenge.\nRecent empirical studies have revealed consistent scaling laws in neural networks, highlighting\ntwo key phenomena: over-parameterization (1, 4) and the power-law decay of loss with respect to\ndataset size (10, 8) . Over-parameterization refers to the observation that DNNs with parameter\nsizes growing faster than dataset sizes can achieve remarkable generalization performance (10).\nConcurrently, the loss function of DNNs decreases approximately as a power-law function of the\ndataset size (8). While these scaling laws have been validated empirically, theoretical explanations\nfor their emergence remain limited and often lack generality or consistency with experimental\nobservations.\nFrom the generalization error perspective, classical statistical learning theory suggests that the\ngeneralization error decreases with the size of the dataset, often inversely proportional to D or\n\\(\\sqrt{D}\\) (2). Some researchers attempt to extend these ideas to high-dimensional spaces by correlating\nthe power-law exponent with the intrinsic dimensionality of the data. While this offers a plausible\nexplanation, it falls short in explaining why such scaling laws are predominantly observed in neural\nnetworks and not in other models, such as random forests, which share similar statistical properties.\nOn the other hand, the capability quanta perspective, inspired by concepts from quantum\nmechanics, proposes that learning involves acquiring discrete units of task-solving ability, termed\n\"capability quanta.\u201d (16) This theory effectively explains the observed power-law relationships\nbut relies on counterintuitive assumptions, such as the existence of discrete generalization status,\nwhich violate the classical understanding to continuous generalization error bound, and power-law\ndistribution of capability quanta.\nIn contrast, our approach focuses on the activation patterns of neurons and models their evolution\nas a stochastic process. By doing so, we derive theoretical results that not only explain key aspects\nof neural scaling laws, such as over-parameterization and the power-law decay of loss, but also align\nwith experimental findings that are seldom discussed or newly reported. Moreover, our framework"}, {"title": "", "content": "bridges the gap between empirical observations and theoretical explanations, offering new avenues\nfor understanding the behavior of neural networks. Specifically, we analyze the evolution of neuronal\nactivation as a stochastic process and study its relationship with dataset expansion and parameter\ngrowth. Through this approach, we derive theoretical results that align with key experimental\nfindings:\n\u2022 The neural networks in over-parameterization, where the parameter size N grows faster than\nthe dataset size D, i.e. N > O(D), can still generalize.\n\u2022 We can observe a phase transition in the generalization process of neural network if we plot\nthe data size in log-axis plot. This finding is implied by the figure 2 in the first neural scaling\nlaw paper from OpenAI (10), but seldom discussed by previous researches.\n\u2022 The power-law decay of loss L as a function of D, expressed as L = O(D\u00af), where s > 0.\nFurthermore, we extend our framework to make experimentally testable predictions regarding\nthe role of noise-to-signal parameter ratios and the compressibility of large models. These predic-\ntions not only offer new insights into neural network scaling but also provide a foundation for future\nexperimental validation.\nThe remainder of this paper is structured as follows. Section 2 presents the notations and\nassumptions of our probabilistic framework. In Section 3, we propose our central hypothesis,\nwhose form is simple and clear, which assumes that when the training data size is increasing, the\nneuron activation distribution is steady in linear axis. In Section 4, based on our central hypothesis,\nwe pointed out that the neuron activation distribution is a power-law distribution and acquired\nthe growth function of activated neurons. In Section 5, based on the derived growth function,\nwe present why neural networks under over-parameterization can still generalize, and why there\nexist a clear phase transition if we plot the data size in log-axis. Then using the phase transition\neffect and the power-law distribution, we derived the power-law decay of loss L as a function of D.\nAfter that, in Section 6, we provides new predictions, mainly about the parameter efficiency and\npruning for neural network, based on our derived conclusions and theory framework. In Section\n7, we review related work on neural scaling laws and existing theoretical interpretations. Finally,\nSection 8 concludes with a discussion of our limitation and potential future research directions."}, {"title": "Notations and Preliminary", "content": "In this paper, we consider a single layer of neurons in a deep neural network. The notation used\nthroughout are as follows:\n\u2022 Number of neurons: Let N denote the total number of neurons in the layer, and the index of\neach neuron is denoted by i \u2208 {1,2,..., N}.\n\u2022 Training dataset: The size of the training dataset is D.\n\u2022 Activation Function: We consider a neural network that uses ReLU as activation function:\n\nReLU(x) = \\begin{cases}\nx, & \\text{if } x \\geq 0 \\\\\n0, & \\text{if } x < 0\n\\end{cases}\n\nSuch a function satisfy two important properties: (1) only activated neurons contribute to\nthe final loss of the model; and (2) only activated neurons can receive gradients in backward\npropagation, which means that a sample is learnt by a neuron only if the neuron is activated.\n\u2022 Activation of neurons: During training on dataset D, not every neuron is activated by every\nsample (Otherwise, the neural network will lose non-linearity). Let Di represent the number\nof samples that activate neuron i. Since a neuron can only learn (i.e., be back-propagated)\nwhen it is activated, the Di also represent the sample number that neuron i has learnt.\n\u2022 Expect of Activated Neuron Number: Modern deep neural networks often include nor-\nmalization layers (e.g., batch normalization, layer normalization, or group normalization\n(9, 12, 21)), which constrain the number of neurons activated by each sample to be approx-\nimately constant across different samples, denoted as c. The c should be only relevant to\nhyper-parameter setting and the neuron number N, and intuitively it should only increase\nwhen N increases."}, {"title": "Central Hypothesis", "content": "In this paper, we consider a steady process that introduce a new sample into an existing dataset, i.e.,\nmoving from D \u2192 D + 1. This new sample is assumed to be drawn independently and identically"}, {"title": "", "content": "distributed (i.i.d.) with the existing samples. Based on the above defined notations, we provide our\ncentral hypothesis: Stable Activation Probability Hypothesis:\n\u2022 Based on the i.i.d property of the new sample, the probability of neuron i being activated by\nthe new sample is proportional to Di + b, where Di is the number that neuron i is activated\nby samples in D, and b is a small positive constant ensuring that even unactivated neurons\nhave a non-zero probability of being activated.\nBased on the above assumption, the probability of neuron i being activated by the new sample\nis given by:\n\npi(\u03c3(i)|D1, D2, . . ., DN) = \\frac{cDi + cb}{\\sum_{j=1}^{N}(Dj + b)} = \\frac{cDi + cb}{cD + bN} = \\frac{Di + b}{D + \\frac{b}{c}N}\n\nwhere b is a constant ensuring that unactivated neurons have a non-zero activation probability, c is\nthe approximate number of neurons activated per sample, and cD = \\sum_{j=1}^{N} Dj is the total number\nof activations across all neurons."}, {"title": "The dynamic of neuron activation", "content": "In this section, we analyze how the distribution of Di evolves as D increases. Neurons are divided\ninto two categories: \"working neurons\u201d, which have been activated by at least one sample, and\n\"free neurons\u201d, which have not been activated. By analyzing the stochastic process of neuron\nactivation, we present two conclusions:\n\u2022 The number of working neuron, denoted as K(D), increase as a function of D, in the form\nof \\(N(1-(\\frac{Do+b}{D+N})^b)\\).\n\u2022 The distribution of activation number in the working neuron, denoted as P(Di = k), approx-\nimately follows power-law distribution: P(Di = k) \u221d k\u00af\u00ba, where 0 < a < 1"}, {"title": "The dynamic of working neuron number", "content": "We consider the transition from free to working neurons. Since the transition from free to working\nneurons is unidirectional, let K denote the number of working neurons. Then, K(D) is a monotoni-\ncally increasing function of D. Based on the above probability definition, we derive the recurrence"}, {"title": "", "content": "relation:\n\nK(D + 1) \u2013 K(D) = \\frac{(N \u2212 K(D))b}{D+\\frac{b}{c}N}\n\nBy approximating the discrete difference as a continuous change, we rewrite the relation in its\ndifferential form:\n\n\\frac{dK}{dD} = \\frac{(N \u2212 K(D))b}{D + \\frac{b}{c}N}\n\nSolving the above differential equation yields the following solution for the number of working\nneurons K(D):\n\nK(D) = N \u2013 \\frac{(N-Ko)}{(D+\\frac{b}{c}N)^{b}}\\left(\\frac{D_o+\\frac{b}{c}N}{D+\\frac{b}{c}N}\\right)^b\n\nwhere Ko and Do represent the initial conditions. Solutions can be found in the Appendix. This\nsolution describes how the number of working neurons evolves with the growth of the dataset size\nD. For large model on large dataset (i.e. N, D >> K0, D0), K(D) approximately follows the form\nof \\(N(1-(\\frac{Do+b}{D+N})^b)\\)."}, {"title": "The distribution of working neuron's activation number", "content": "We now consider the distribution of activation counts Di at a total sample size D, denoted as\nP(Di = k | D). When D is sufficiently large, the distribution reaches a steady state, where\nP(Di = k | D) \u2248 P(Di = k | D + 1).\nAt steady state, the clusters with Di = k have a probability proportional to k + b of transitioning\nto Di = k + 1, while the clusters with Di = k + 1 have a probability proportional to k + 1 + b of\ntransitioning back to Di = k. This leads to the equilibrium equation:\n\n(k + b)P(Di = k) = (k + 1 + b)P(Di = k + 1).\n\nExpanding this recurrence relation back to P(Di = 1), we find that P(Di = k) is proportional\nto \\frac{D_{min}+b}{k+b}P(D_i = D_{min}) = constant. This distribution approximates a power-law distribution P(Di =\nk) \u221d k\u00af\u00ba,where 0 < a < 1. This range is because \\frac{k+1+b}{k+b} < \\frac{k+1}{k}, which implies P(Di = k) decrease\nas k increases but slower than \\frac{1}{k}. Therefore, the P(Di) forllows:\n\nP(D_i) = \\frac{D_i^{-\\alpha}}{\\sum_{1}^{D_{max}} K^{-\\alpha}} \\approx \\frac{D_i^{-\\alpha}}{\\int_{1}^{D_{max}} K^{-\\alpha} dK} = \\frac{(1-\\alpha)D_i^{-\\alpha}}{D_{max}^{1-\\alpha} - 1} \\approx \\frac{(1-\\alpha)D_i^{-\\alpha}}{D_{max}^{1-\\alpha}}"}, {"title": "", "content": "where Dmax is the maximum of Di. Since we know \\(\\sum D_i = cD\\), we have:\n\ncD = \\sum D_i = K(D)E(Di) \u2248 K(D)(1-a) \\frac{\\int_{1}^{D_{max}} K^{1-\\alpha} dK}{D_{max}^{1-\\alpha}} = K(D) \\frac{D_{max}^{2-\\alpha}-1}{2-\\alpha} D_{max}^{1-\\alpha} = O(K(D)D_{max})\n\nThus, we have Dmax = O(\\frac{cD}{K(D)})."}, {"title": "Understanding Deep Neural Network Behavior", "content": "In this section, we apply the above two conclusions to understand three key empirical phenomenons:\n\u2022 The generalization of over-parametrized neural network\n\u2022 The phase transition of neural network generalization\n\u2022 The power-law decay of neural network loss regarding the increase of data size scale."}, {"title": "The generalization of over-parametrized neural network", "content": "From the above results, we can understand why overparamterized neural network (i.e. the neural\nnetwork with N > O(D)) highly generalize. Given that every sample activates c neurons, we\nconsider the average sample number of every base learner (working neuron), which is defined as:\n\n\\frac{cD}{K(D)} = \\frac{c}{\\frac{D_o+\\frac{b}{c}N}{(1-(\\frac{Do+b}{D+N})}}\\frac{D+N}{bN})\n\nSuppose that N grows with D with linearity and N, D >> Do, i.e. N=kD=O(D), then the \\frac{cD}{K(D)}\nwill become:\n\n\\frac{cD}{K(D)} = \\frac{c}{C_1 + \\frac{1}{\\sqrt{N}}} \u2248 O(c)\n\nwhich grows when D and N grow. Thus, it means that if N grows with D with linearity, then\nthe average sample of each base learner will grow together with D, finally leading to underfitting\nin some settings. Thus, N should grow a little bit faster then D, which aligns with experimental\nobservation that N^{0.74} \u221d D."}, {"title": "The phase transition of neural network generalization", "content": "In (10), particularly the figure 2, the authors reported the curves about how neural language model's\nperformance change with data size D when the parameter size is given. They reported the curves\nof model with various parameter sizes. And it can be observed that the validation loss of all models\nshows a phase transition phenomenon. More specifically, at very beginning, the model's loss is very\nsteady, which looks approximately as a flat line. Then as data increases, the model will get into a\ntransition phase, in which the model's loss dramatically reduces. After that, the model's loss again\nbecome a approximately steady flat line.\nSuch phenomenon is very counterintuitive for machine learning area, in which the generalization\nerror bound of a model should be a continuous function that decrease as data size increase (5).\nHowever, in this section, we are going to explain why such phase transition process generally\nhappens for different models.\nGiven a series of neurons, K(D) satisfies the following approximation:"}, {"title": "", "content": "\nlog(K(D)) = \\begin{cases}\nconstant, & \\text{if } log(D) \\geq log(\\frac{b}{c}N) + \\epsilon \\\\\nlog(D) + constant, & \\text{if } log(D) \\leq log(\\frac{b}{c}N) - \\epsilon \\\\\n\\text{Inapproximable} & \\text{otherwise}\n\\end{cases}\n\nWe provide the theoretic analysis to the above approximation. To give more intuitive justification,\nwe put a figure (Fig. 1) showing how log(K(D)) change with log(D) when N = 10^{11} (i.e. 10B, the\nnormal scale of large language models), c = 10^{5} (approximately the square root of N) and b = 10^{3}.\nFrom the figure, we can see that across multiple magnitudes that is lower than log \\frac{b}{c}N, the linear\napproximation fits very well. And when the magnitude crosses log \\frac{b}{c}N, it dramatically change to a\nconstant.\nWhen the log(D) < log(\\frac{b}{c}N)-\\epsilon, i.e. cD << bN, we see log(K(D)) = log(D)+constant, which\nimplies K(D) = O(D) << N'. This means that the working neuron number much fewer than free\nneurons, implying the output of the whole network is dominant by a random noise generated by a\nfree noise with initial parameters. Thus, the loss should be close to the loss of a randomly initialized\nneural network, denoted as Lnoise. Meanwhile, when log(D) > log(\\frac{b}{c}N) + \\epsilon, i.e., bN < cD, from\nthe over-parameterization nature of deep neural network, we can know that the sample numbers is\nsufficiently enough for the network to learn parameters that are close to optimal. We denote this\nloss as Lopt. It is obviously that there exists a clear gap between Lopt and Lnoise. Thus, we can\nobserve a phase transition phenomenon when we apply log axis"}, {"title": "The neural scaling law", "content": "In this section, we will discuss how to approximately derive neural scaling law by analyzing the\nabove dynamics and distributions that we acquired.\nSince the activation probabilities of the \"working neurons\u201d in our theory also follows power-\nlaw distribution, it is very natural to relate the \"working neuron\" with the 'ability quanta' in (16).\nMeanwhile, when bN << cD (i.e., log(D) > log(\\frac{b}{c}N) + \\epsilon), the probability that a free neuron is\nactivated is close to 0:\n\nK(D + 1) \u2013 K(D) = \\frac{(N \u2212 K(D))b}{D+\\frac{b}{c}N} < \\frac{bN}{D} \u2248 0."}, {"title": "", "content": "Thus, when D is sufficiently large, the overall loss of the neural network is only contributed by the\n\"working neurons\" that are activated. In other words, the overall loss can be represented by the\nexpect of the loss contributed by the activated neuron:\n\nL(D) = E_iL_i(D_i) = \\sum^{K(D)} \\sum p_iL_i(D_i) = \\sum^{K(D)} p(\\sigma(i)|D_1, D_2, ..., D_N)L_i(D_i)\n= \\sum^{K(D)}\\frac{D_i + b}{D + \\frac{b}{c}N}L_i(D_i) \u2248  \\sum^{K(D)}\\frac{D L_i(D_i)}{D+\\frac{b}{c}N}\n\nwhere i represent the neuron index and pi = p(\u03c3(i)|D1, D2, . . ., DN) is the probability that the\ni-th neuron is activated.\nSince a neuron's output is actually the output of the sub-network that consists of its prede-\ncessor neurons and itself, then each working neuron's loss should also satisfy the phase transition\nphenomenon that we previously discussed, i.e. :\n\nlog(K(D_i)) = \\begin{cases}\nconstant, & \\text{if } log(D_i) \\geq log(\\frac{b'}{c'}N_i) + \\epsilon \\\\\nlog(D) + constant, & \\text{if } log(D_i) \\leq log(\\frac{b'}{c'}N_i) - \\epsilon \\\\\n\\text{Inapproximable} & \\text{otherwise}\n\\end{cases}\n\nDue to the symmetry of neural network architecture, for two neuron's in the same layer,\ntheir predecessor layers are the same, and thus share same configuration of bi, ci, Ni. These same\nconfiguration also implies same random initialized loss Lnoise and optimal loss L'opt of these\nsub-networks, which implies:\n\nL_i = \\begin{cases}\nL'opt & \\text{if } log(D_i) \\geq log(\\frac{b'}{c'}N_i) + \\epsilon \\\\\nL'noise, & \\text{if } log(D_i) \\leq log(\\frac{b'}{c'}N_i) - \\epsilon \\\\\nL_t(D_i) & \\text{otherwise}\n\\end{cases}\n\nwhere Lt(Di) means the transition phase loss. We denoted the shared configuration as b', c', N'.\nTherefore, we have:\n\nL(D) = E_iL_i(D_i) =L'_{noise}P(log D_i < log(log(\\frac{b'}{c'}N') - \\epsilon) \n+ L'_{opt}P(log D_i > log(log(\\frac{b'}{c'}N') + \\epsilon)\n+ (E L_i)P(log(\\frac{b'}{c'}N') - \\epsilon < log D_i < log(log(\\frac{b'}{c'}N') + \\epsilon)"}, {"title": "", "content": "where ELi refers to the expect of loss from those neurons in transition phase.\nMeanwhile, as we derived previously in Section 4, the distribution of Di follows Da:\n\nP(D_i) = \\frac{(1-\\alpha)D_i^{-\\alpha}}{D_{max}^{1-\\alpha}}\n\nwhere 0 < a < 1 is the shape parameter of Di's distribution.\nThen we have:\n\nP(log(\\frac{b'}{c'}N') - \\epsilon < log D_i < log(\\frac{b'}{c'}N') + \\epsilon) = \\frac{(\\frac{b'N'}{c'})^{1-\\alpha}(e^{\\epsilon} - e^{-\\epsilon})}{D_{max}^{1-\\alpha}} = \\frac{(\\frac{b'N'}{c'})^{1-\\alpha}(e - e^{-\\epsilon})}{c'D_{max}^{1-\\alpha}}\n\nSince e \u2192 0, we can do Taylor expansion to e to acquire e^{\u03b5} \u2212 e^{\u2212\u03b5} = 2\u03b5. Thus P(log(\\frac{b'}{c'}N')-\\epsilon <\nlog D_i <log(\\frac{b'}{c'}N') + \\epsilon) = O(\\frac{\\epsilon}{D_{max}}). When e \u2192 0 and b'N' << c'Dmax, we have P(log(\\frac{b'}{c'}N')-\\epsilon <\nlog Di < log(\\frac{b'}{c'}N') + \u03b5) \u2192 0\nTherefore, we have:\n\nL(D) = E_iL_i(D_i) \u2248 L'_{noise} P(log D_i < log(log(\\frac{b'}{c'}N') - \\epsilon) + L'_{opt} P(log D_i > log(log(\\frac{b'}{c'}N') + \\epsilon)\n\u2248 L'_{opt} + (L'_{noise} - L'_{opt}) P(log D_i < log(log(\\frac{b'}{c'}N') - \\epsilon)\n\u2248 L'_{opt} + (L'_{noise} - L'_{opt}) P(log D_i < < log(log(\\frac{b'}{c'}N'))\n= L'_{opt} + (L'_{noise} - L'_{opt}) \\frac{(\\frac{b'}{c'}N')^{1-\\alpha} - 1}{c'D_{max}^{1-\\alpha}}\n\nFrom the expression of K(D), we know that K(D) monotonically increases and K(D) < N. Thus,\nwhen D is sufficiently large, K(D) = O(1). Thus, Dmax = O(cD) have:\n\nL(D) \u2248 L'_{opt} + (L'_{noise} - L'_{opt})  \\frac{(\\frac{b'}{c'}N')^{1-\\alpha} - 1}{(c' cD)^{1-\\alpha}}\n\nAs we discussed, 0 < a < 1. Thus, the L(D) is actually a power-law function of D and N"}, {"title": "Predictions of Unknown Phenomena", "content": "In the previous sections, we demonstrated how the proposed model explains two key phenomena\nin scaling laws: over-parameterization and the power-law decay of loss with dataset size. In this\nsection, we extend the analysis to predict some previously unexplored phenomena based on our\nmodel."}, {"title": "", "content": "We focus on the noise-to-signal parameter ratio, which quantifies the relationship between the\nmodel's total parameters N and the scale of free neurons N \u2013 K(D). Particularly, this ratio can help\nus understand the optimal or sub-optimal ratio of pruning and compressing a neural network (13,6),\nwhich shows significance in many areas (14). Using the formula for K(D), we can derive:\n\n\\frac{N-K(D)}{N} = \\frac{(\\frac{b}{c}N)^b}{(cD+\\frac{b}{c}N)^b} = \\frac{(\\frac{b}{c}N)^b}{(D + \\frac{b}{c}N)^b}\n\nWhen N grows faster than D, this ratio approaches 1 as N increases. This result implies that the\nproportion of ineffective parameters in a large model increases as both the model size N. When N\ngrows faster than linearity of D, we also see that when datasize increases, the ineffective parameter\nratio increases too. Based on this observation, we propose several predictions:\n1. Larger models are inherently more compressible.\n2. With the same architecture, models trained on larger datasets achieve a lower compression\nratio compared to those trained on smaller datasets. The ratio should approximately decrease as a\npower-low function of D in the form of (\\frac{cD}{N})^{C_1} C_2\n3. With the same dataset, larger models generally achieve higher compression ratios than smaller\nmodels. The ratio should approximately grows as a power-low function in the form of (\\frac{N}{D})^{C_1} C_2\n4. Over time, naively pre-trained large models will exhibit lower capability density compared\nto smaller models trained with more data.\nIt is worth noting that the second conclusion has been empirically verified in recent work.\nSpecifically, the concept of capability density, introduced in (22), highlights that the effective\nparameter size of a model relative to its actual parameter size is increasing over time. This aligns\nwith our theoretical prediction that the density of useful parameters in larger, naively pre-trained\nmodels diminishes relative to smaller, efficiently trained models. The study shows that the capability\ndensity of open-source models has doubled approximately every three months, supporting the idea\nthat smaller, well-trained models can achieve comparable or even superior performance with fewer\nparameters."}, {"title": "Related Work", "content": "This work builds upon and extends previous studies in neural scaling laws (10, 8), which examine\nthe relationship between compute, parameter size, dataset size, and the loss of neural networks.\nSince compute can be expressed as a function of parameter size and dataset size, we primarily\nfocus on the relationships between parameter size N and dataset size D, as well as the relationship\nbetween dataset size D and neural network loss L.\nHere, we define:\n\u2022 Dataset size D: The number of training samples used to optimize the neural network.\n\u2022 Parameter size N: The total number of trainable parameters in the neural network.\n\u2022 Loss L: The objective function value that the neural network minimizes during training, often\nrepresenting the prediction error on the dataset."}, {"title": "Generalization under Overparameterization", "content": "One critical aspect of neural scaling laws is the phenomenon of overparameterization. As neural\nnetworks scale, the parameter size N increases faster than the dataset size D. Mathematically, this\ncan be expressed as:\n\nN > O(D),\n\nindicating that the parameter size grows superlinearly with respect to the dataset size. Empirically,\nN^{0.74} \u221d D (10), indicating that the parameter size N scales sub-linearly with respect to the dataset\nsize D. This overparameterization has been shown to improve generalization, despite the apparent\nrisk of overfitting. Recent studies have empirically and theoretically validated this relationship\nacross various architectures and tasks, highlighting that sufficiently large models are capable of\ncapturing complex patterns in data, even with finite datasets."}, {"title": "Power-Law Relationship between Loss and Dataset Size", "content": "Another key observation in neural scaling laws (10) is the power-law decay of loss L with increasing\ndataset size D. Specifically, the loss can be expressed as:\n\nL(D) = O (D^{-s}),\n\nwhere s > 0 is a positive constant. This power-law relationship has been consistently observed in\nboth language and vision models, suggesting that increasing the dataset size leads to diminishing\nreturns in reducing loss. The exponent s depends on the model architecture, the quality of the data,\nand the task, but its existence reflects a universal trend in neural scaling.\nThe insights from these two aspects of neural scaling laws provide a foundation for the theo-\nretical analysis in this paper, particularly the derivation of scaling behavior in loss and the role of\noverparameterization in improving model performance."}, {"title": "Interpretations of Neural Scaling Laws", "content": "Existing works attempting to interpret neural scaling laws generally follow two primary approaches.\n1. Generalization Error Perspective. From the perspective of classical statistical learning\ntheory, the generalization error is typically inversely proportional to either the dataset size D or\nthe square root of D (15, 5). Based on this, some researchers have proposed that the power-law\nexponent observed in scaling laws could be explained as the intrinsic dimensionality of the data in\nhigh-dimensional spaces. However, this explanation encounters a critical limitation: theoretically,\nit should also apply to other machine learning models, such as random forests. In practice, however,\nthe power-law relationships are predominantly observed in neural networks and not in other machine\nlearning models. This discrepancy casts doubt on the universality of this approach.\n2. Capability Quanta Perspective. Another approach, inspired by ideas from quantum mechan-\nics, has been proposed by researchers with a background in physics (16). This approach hypothesizes\nthe existence of \"capability quanta,\u201d analogous to energy quanta in quantum mechanics. Each ca-\npability quantum represents a discrete unit of task-solving ability. When a model acquires a new\ncapability quantum, it unlocks a specific ability. Under this framework, the distribution of capability\nquanta follows a power-law distribution, and the data and parameter requirements for learning each\nquantum also exhibit power-law growth."}, {"title": "", "content": "While this theory successfully explains the observed power-law relationships between loss,\ndataset size, and parameter size, it has several significant limitations:\n\u2022 Counterintuitive Assumptions: The theory assumes the existence of capability quanta.\nThese quanta has discrete status, i.e. \"learnt\" and 'non-learnt'. And the losses contributed\nby quantas with different status have a clear gap. As admitted by the authors themselves in\ntheir discussion of paper limitation, this assumption is very counterintuitive, and violates\nthe observation that neural networks are optimized gradually. In machine learning area, the\ngeneralization error bound of a model is usually characterized as a continuous function of\nsample number D.\n\u2022 Specific Distribution Assumptions: It further assumes that the distribution of capability\nquanta follows a power-law distribution. Other distributions, such as exponential or Poisson\ndistributions with appropriate parameters, are not considered, making the framework appear\noverly tailored to fit the power-law observations.\nThese limitations highlight the challenges in fully explaining neural scaling laws and suggest\nthe need for more robust and empirically consistent theoretical frameworks.\nIn this paper, we propose a new perspective to understand the behavior of neural networks. By\nanalyzing the activation patterns of neurons and modeling their evolution as a stochastic process,\nwe study how neural networks scale with increasing dataset size D and parameter size N. Through\nthis approach, we derive conclusions consistent with existing experimental results, including the\nbenefits of over-parameterization and the power-law decay of loss with D. Furthermore, within this\ntheoretical framework, we provide experimentally testable predictions."}, {"title": "Conclusion, Limitation and Future Work", "content": "This paper provides a probabilistic framework for understanding the impact of individual neu-\nron activation patterns on the behavior of deep neural networks. The derived results align with\nexperimental observations and offer testable predictions that can guide future research.\nConcurrently", "to": "n\u2022 Our stable"}]}