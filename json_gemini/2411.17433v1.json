{"title": "LC-SVD-DLinear: A low-cost physics-based hybrid machine learning model for data forecasting using sparse measurements", "authors": ["Ashton Hetherington", "Javier L\u00f3pez Leon\u00e9s", "Soledad Le Clainche"], "abstract": "This article introduces a novel methodology that integrates singular value decomposition (SVD) with a shallow linear neural network for forecasting high resolution fluid mechanics data. The method, termed LC-SVD-DLinear, combines a low-cost variant of singular value decomposition (LC-SVD) with the DLinear architecture, which decomposes the input features\u2014specifically, the temporal coefficients\u2014into trend and seasonality components, enabling a shallow neural network to capture the non-linear dynamics of the temporal data. This methodology uses under-resolved data, which can either be input directly into the hybrid model or downsampled from high resolution using two distinct techniques provided by the methodology. Working with under-resolved cases helps reduce the overall computational cost. Additionally, we present a variant of the method, LC-HOSVD-DLinear, which combines a low-cost version of the high-order singular value decomposition (LC-HOSVD) algorithm with the DLinear network, designed for high-order data. These approaches have been validated using two datasets: first, a numerical simulation of three-dimensional flow past a circular cylinder at $Re = 220$; and second, an experimental dataset of turbulent flow passing a circular cylinder at $Re = 2600$. The combination of these datasets demonstrates the robustness of the method. The forecasting and reconstruction results are evaluated through various error metrics, including uncertainty quantification. The work developed in this article will be included in the", "sections": [{"title": "1. Introduction", "content": "Computational fluid dynamics is generally used to resolve complex problem defined by high-dimensional systems, which can range up to large quantities of spatio-temporal flow scales. Solving realistic problems, capturing the highly complex underlying physics of the dynamic system being studied, comes with a proportional computational expense, limiting the problem-solving capabilities based on the amount of available resources. This has caused experts to branch out into new research paths, with the objective being to reduce the computational cost while minimizing the effect on the accuracy of results. There are two main sources which contribute to high computational cost, these being the spatial resolution used to define a singular solution, and the temporal resolution, which is the number of equidistantly spaced solutions generated during a certain period of time in order to study the evolution of the dynamic system.\nStarting off with the spatial resolution, this has been an ongoing problem in the field of fluid mechanics given the size of the databases. Spatial dimensionality is important if the goal is to generate data with high granularity. Experts in industry and researchers are sometimes forced to downscale their problems to fit their available computational resources, search for alternative solutions which can ingest large amounts of data, or to apply high performance computing techniques, such as data parallelization, in order to be able to process their data in a distributed manner, to speed up direct numerical simulation solvers [1], or to employ non-linear optimization on large scale sparse systems [2].\nFocusing on data dimensionality reduction, which is the most popular and affordable technique, in recent work, low-cost singular value decomposition [56] was developed as a highly accurate reconstruction technique via data assimilation, capable of increasing the resolution of any dataset, independently of its complexity (two- or three-dimensional, laminar or turbulent,"}, {"title": "2. Methodology", "content": "The following section explains the details behind this novel methodology. First, a brief breakdown of the LC-SVD algorithm is provided, since an in-depth explanation can be found in the original work [56]. Following up, the novel LC-HOSVD methodology is described, which follows the same principles as LC-SVD, but seems more suitable for high-dimensional data. Next, the DLinear architecture is explained. After presenting both parts of this novel methodology (optimal data resolution enhancement algorithms and the time series forecasting shallow neural network), the final hybrid models, LC-SVD-DLinear and LC-HOSVD-DLinear, are presented. This section finishes off with a description on how the prediction and reconstruction errors have been analysed, using commonly used error evaluation metrics and statistics."}, {"title": "2.1. Data organization", "content": "A matrix-form dataset consists of a ordered group of K snapshots \u03c5k =\nv(tk), where te is the time at instant k. For convenience, the snapshots are\ncollected in the following snapshot matrix\n$V_1^K = [v_1, v_2, . . ., v_k, v_{k+1},\u06f0\u06f0\u06f0, v_{K\u22121}, v_K]$.\n(2.1)\nwhere $V_1^K$ refers to the snapshot matrix, and each one of the K temporal\nsamples vk is known as snapshot. Each snapshot is formed by a vector"}, {"title": "2.2. Low-cost singular value decomposition (LC-SVD)", "content": "Low-cost singular value decomposition is an extension of SVD, which\nwas first presented in [56], and is used to analyse reduced datasets and to\nreconstruct them to a higher resolution. This method allows users to collect\nminimum data during experiments, or to perform numerical simulations in\na reduced grid, using less computational resources, since by using LC-SVD\nthe data resolution can be enhance with minimum computational cost, using\nSVD to capture the underlying physics while eliminating any noise. SVD is\nimplemented as follows:\n\u2022 Step 1: apply standard SVD to the under-resolved data. SVD is applied\nto the reduced dimension snapshot matrix eq. (2.2) as\n$V_1^K ~ W\u00c9T$,\n(2.8)\nwhere $W^TW = TT^T =$ are unit matrices of dimension \u00d1 \u00d7 \u00d1. The\nnumber of retained SVD modes, \u00d1, is defined by the user, and mode\nreduction is implemented by applying matrix slicing on the matrices\nfrom eq. (2.8) in the dimension that contains the number of modes.\n\u2022 Step 2: normalize the spatial SVD modes. Matrix \u03a3 can be ill-conditioned\nwhen small singular values are retained. This can cause SVD modes\ncalculated in W to be slightly non orthogonal due to round-off er-\nrors. QR factorization is applied to re-orthonormalize these modes as\nW = QW RW, leading to\n$W = W(R_W)^{-1}$,\n(2.9)\nwhere $R_W \u2208 R^{N\u00d7K}$. Notice that, similar to SVD, only \u00d1 modes are\nretained.\n\u2022 Step 3: normalization of the temporal coefficients. Similar to Step 2,\nthe temporal coefficients calculated in T could also be slightly non or-\nthogonal so, to ensure they are orthogonal, QR factorization is applied"}, {"title": "2.3. Low-cost high-order singular value decomposition (LC-HOSVD)", "content": "The HOSVD algorithm was first introduced by Tucker in 1966 [65], and\nhas gained popularity in recent years, particularly due to its implementation\nby de Lathauwer et al. [66, 67].\nHOSVD decomposes high-order datasets (also referred to as tensors in-\nterchangeably in this work) by applying SVD to each fiber of the tensor. For\ninstance, HOSVD of the fifth-order tensor defined in eq. (2.5) is presented\nas\n$V_{j1j2j3j4k}~= \u03a3 \u03a3 \u03a3 \u03a3 \u03a3 W_1 W_2 W_3 W_4  S_{p1P2P3p4nTkn}$,\n(2.15)\nHere, $S_{p1P2P3P4n}$ is the core tensor, a fifth-order tensor which contains the\nsingular values, while the columns of $W^(1), W^(2), W^(3), W^(4)$ and T are the\nmodes of the decomposition.\nThe columns of W^(l) for l = 1, 2, 3, 4 represent the spatial HOSVD modes,\ncorresponding to the database components and spatial variables, while the\ncolumns of T represent the temporal HOSVD modes, corresponding to the\ntime variable. The singular values of the decomposition is now formed by\nfive sets of values,\n$\u03c3_{p1}^{(1)}, \u03c3_{p2}^{(2)}, \u03c3_{p3}^{(3)}, \u03c3_{p4}^{(4)}, and \u03c3_{\u03b7}^{t}$\n(2.16)\nwhich are also sorted in descending order. Truncation in HOSVD, similar\nto SVD, retains only the most relevant modes. While the full HOSVD is\nexact, selecting a reduced number of modes helps reduce noise, or even lower\ndimensionality, based on the application's requirements. This number is\nselected by the user. After truncation, HOSVD (2.15) is written as:\n$V_{j1j2j3j4k} = \u03a3 W_{j1j2j3j4n} Vk_n$\n(2.17)\nwhere $W_{j1j2j3j4n}$ and $Vk_n$ are the spatial and temporal modes, and N is the\nspatial complexity or number of retained HOSVD modes. The modes are\ndefined as:\n$W_{j1j2j3J4n} = \u03a3 \u03a3 \u03a3 S_{P1P2P3P4n} W_DW_2W_3W_4 \u03c3_kn.$,\n(2.18)"}, {"title": "2.4. Data preprocessing", "content": "Before moving on to the DLinear model, it is important to cover the data\npreprocessing required for this model to operate. First, the input tempo-\nral coefficients matrix is divided into train, validation and test sets, with\nproportions 0.7, 0.15 and 0.15, respectively.\nAfter splitting the data, the temporal coefficients are normalized using\nmin-max scaling, with the data range being [-1,1], since DLinear solves a\nsimple linear regression where negative values are possible. Min-max scaling\nis applied as follows:\n$T_{scaled} = a +\n\n(T \u2212 T_{min})(b \u2212 a)\nT_{max} - T_{min}$\n,\n(2.20)\nwhere $T_{max}$ and $T_{min}$ are the maximum and minimum values of T, while a\nand b are the bottom and top limits of the data range, so -1 and 1, respec-\ntively. An example of the split normalized temporal coefficients can be seen\nin fig. 1"}, {"title": "2.5. Decomposition linear neural network (DLinear)", "content": "The neural network used in this work was first presented in [57], where\nits performance was compared to the achieved by a variety of transformers\nused for long time series forecasting (LTSF). The model is a shallow neural\nnetwork, since it only has a depth of one layer with trainable parameters,\nbut varies in width (based on the number of modes N).\nwith $T_\u2208 R^{L\u00d7N}$. This\nprocesses decomposes each mode into trend and seasonality components.\nGiven an input sequence $T_z$, the average pooling layer extract the se-\nquence trend as so:\n\n$T_{avg} = \\frac{1}{L} \u03a3 T_z (i)$\n(2.22)\nwhere $T_z$ and $T_{avg}$ represent the input data and the average value, re-\nspectively, while i denotes the temporal position. After calculating the mean\nvalue of the sequence, the trend matrix is formed:\n$T_t = [T_{avg}, T_{avg}, T_{avg}, ..., T_{avg}]$,\n(2.23)\nwith $T_\u2208 R^{L\u00d7N}$. The trend T\u2081 represents the overall level of the sequence,\ngiving a low-frequency approximation of the data. The seasonality, which can\nalso be referred to as the residual, can be calculated as follows:\n$T_s = T_ - T_t$.\n(2.24)"}, {"title": "2.6. Hybrid models: LC-SVD-DLinear and LC-HOSVD-DLinear", "content": "The combination of the methodologies described in this section allow\nfor the fusion of the advantages of both, resulting in the forecast of new\nhigh-resolution snapshots from an input under-resolved dataset, by using a\nsize-variant sequence of data."}, {"title": "2.7. Error analysis", "content": "A variety of error metrics have been carefully selected to analyse the ro-\nbustness and precision of the methodology. The first two metrics presented in\nthis section are applied to the ground truth temporal coefficients (y) and the\nforecast values (\u0177) generated by the model. The remaining metrics are used\nto evaluate the error of the reconstructed snapshots. This is, the difference\nbetween the ground truth snapshots (V\u2081K) and the reconstructed snapshots,\n(VK), generated using the forecast temporal coefficients (\u0177).\nStarting off with the evaluation of the temporal coefficients predictions,\nthe Mean Squared Error (MSE) is used to measure the average of the squared\ndifferences between the predicted and observed values, and is expressed as\nfollows:\n$MSE = \\frac{1}{n} \u03a3 (\\hat{Y_i} - Y_i)^2$,\n(2.25)\nwhere \u0177 are the predicted temporal coefficient values, y are the observed\n(ground truth) values, and n is the number of observations. The MSE error\nis robust to outliers, meaning that larger errors are heavily penalized with\nthe square operator. Due to this advantage, this error metrics is used to train\nthe neural network. The MSE units are the observed value units squared."}, {"title": "3. Test cases", "content": "The precision, robustness and computational cheapness of LC-SVD-DLinear\nhas been demonstrated by using diverse databases. The selected test cases\nare a mixture between two- and three-dimensional, laminar and turbulent,\nexperimental and numerical fluid dynamics datasets. The first dataset con-\nsists of a three-dimensional cylinder at $Re = 220$, used as an initial bench-\nmark, and an experimental circular cylinder at $Re = 2600$, to demonstrate\nthe methods robustness when applied to real turbulent datasets.\nAs previously mentioned, these test cases are fluid dynamics datasets and,\ntherefore, are governed by the Navier-Stokes equations. These equations for\na viscous, incompressible and Newtonian flow are:\n$\u2207 \u00b7 V = 0,$\n(3.1)\n$\\frac{\u2202V}{\u2202t} + (V \u00b7 \u2207)V = \u2212\u2207p + \\frac{1}{Re} \u0394V ,$\n(3.2)\nwhere V is the velocity vector, p is the pressure, and $Re$ is the Reynolds\nnumber, which varies for each test case. These equations are non-dimensionalised\nusing the characteristic length L and time L/U, where U is the characteristic\nor free stream velocity for each case."}, {"title": "3.1. Three-dimensional numerical laminar cylinder (laminar cylinder)", "content": "The first dataset consists in a numerical database solving a three-dimensional\nflow passing a circular cylinder, which is presented in Ref. [38]. This dataset\nis commonly used as a benchmark problem to validate methodologies, given\nits simplicity. The cylinder dynamics are closely related to the Reynolds\nnumber concept, defined with the cylinder diameter D. The initial Reynolds\nnumber is low and, therefore, the flow is steady. Upon reaching Re \u2248 46, a\nHofp bifurcation that produces an unsteady flow, which is conducted by a\nvon Karman vortex street [39], emerges. After reaching Re \u2248 189 the oscil-\nlations transition from two- to three-dimensional, for specific wavelengths in\nthe spanwise direction [40], with the development of a second bifurcation.\nNumerical simulations have been carried out using the open-source solver\nNek5000 [41] to solve the incompressible Navier-Stokes equations which de-\nfine the behaviour of the flow. This solver uses spectral elements methods as\nspatial discretization.\nThe boundary conditions configured in the simulation for the cylinder\nsurface are Dirichlet for velocity (u = v = w = 0) and Neumann for pressure.\nThe conditions in the inlet, upper and lower boundaries of the domain are\nthe same: u = 1, v = w = 0 for the streamwise, normal and spanwise\nvelocities, respectively, and Neumann condition for pressure. The conditions\nin the outlet are Dirichlet for pressure and Neumann for velocity. The domain\nof the computational simulations is composed by 600 rectangular elements,\neach one of these is discretized using the polynomial order \u03a0 = 9. The\ndimensions of the computational domain are non-dimensionalized with the\ndiameter of the cylinder. The size of the domain in the normal direction is\nconstant Ly = 15D, and extends in the streamwise direction from Lx = 15D\nupstream of the cylinder to Lx = 50D downstream.\nOnly the last 200 snapshots have been selected, since they correspond\nto the saturated regime of the numerical simulation, where the flow is fully\nthree-dimensional. The snapshots are equidistant in time with step size \u2206t =\n1. The flow field velocity components of this dataset are defined by U\u2081 for\nthe streamwise velocity, U2 for the normal velocity, and U3 for the spanwise\nvelocity (Ncomp = 3), which are enclosed in a domain of Nx = 100 points\nin the streamwise direction, N\u2081 = 40 in the normal direction, and N\u2082 = 64\nin the spanwise direction."}, {"title": "3.2. Experimental turbulent cylinder (turbulent cylinder)", "content": "This second dataset has been extracted from Ref. [50], and consists in\na turbulent flow passing a circular cylinder, which is D = 5mm in diameter\nand L = 20cm in length, at two different Reynolds numbers with a transient\nstate in between. For this case, the second steady state, where the flow\nreaches Re = 2600 has been selected.\nThe experiment was conducted in the low-speed wind tunnel at the Von\nKarman Institute. The dataset captures a turbulent flow over a circular\ncylinder at two distinct Reynolds numbers, along with the transition between\nthese states. The Reynolds numbers are Re \u2248 4000 and Re = 2600, with\nvortex shedding frequencies of 450 Hz and 303 Hz, respectively, yielding a\nStrouhal number of approximately St = fd/U\u00a3 \u2248 0.19 in both regimes. The\nfull range of Reynolds numbers explored falls within the domain of three-\ndimensional vortex shedding [51]. During the test, the free stream velocity\nU shifts between two steady-state conditions, specifically from Ux = 12.1\u00b1\n3% to U\u221e = 7.9\u00b13% m/s, with the transition occurring smoothly over about\n1 second.\nThe experimental domain is defined by N\u2082 = 301 points in the streamwise\ndirection and Ny = 111 in the normal direction. The flow velocity field\nconsists of two velocity components (Ncomp = 2): streamwise velocity U\u2081 and\nnormal velocity U2, which were measured during 5200 snapshots, with time\nstep At = 0.33."}, {"title": "4. Results", "content": "The following section gathers the results obtained after applying both LC-\nSVD-DLinear and LC-HOSVD-DLinear to the test cases, using an optimal\nnumber of POD modes, and down-sampling the datasets according to the\noptimal number of sensors calculated in Ref. [56]. These are, N\u2083 = 45\nsensors for the three-dimensional laminar cylinder, and N\u00b0 = 40 for the\nturbulent cylinder. The data compression rates are C\u2081 = 17066 and C\u2081 =\n835, respectively, meaning that the original data of each test case dataset is\nreduced by this proportion.\nThe first part of the results section illustrates the results obtained when\napplying both models to the laminar cylinder test case, while the second part\nis dedicated to results achieved when applied these to the turbulent cylinder.\nThis way, a clear comparison can be made between the performance of both\nmodels."}, {"title": "4.1. Laminar cylinder forecast", "content": "The application of the LC-SVD-DLinear forecasting model on the nu-\nmerical three-dimensional laminar cylinder test case starts with the optimal\ndownsampling of the dataset, reducing the spatial dimensionality down to\nN\u2083 = 45 points. After this, noise is filtered out by selecting the first N = 12\nmodes, and the reduced tensor is then reconstructed to a clean high reso-\nlution version. After this, the reconstructed temporal coefficients, Trec, are\nprepared for the forecasting task."}, {"title": "4.2. Turbulent cylinder forecast", "content": "When it comes to the turbulent cylinder at Re = 2600 test case, LC-SVD-\nDLinear starts by optimally down-sampling, de-noising and reconstructing\nthe dataset using N\u2083 = 40 sensors and N = 6 SVD modes. By selecting a low\nnumber of singular values, therefore, keeping only the robust modes, we are"}, {"title": "5. Conclusions", "content": "This article has presented a new methodology, which consists of two hy-\nbrid neural networks product of the combination of the data resolution en-\nhancement techniques LC-SVD and LC-HOSVD with the DLinear model, to\nforecast high-resolution snapshots using an under-resolved (or high-resolution)\ninput dataset. The hybrid models input data with any type of resolution.\nThey create a high-resolution denoised version of the input data using one\nof two previously mentioned SVD-based data assimilation methods, with the\ntemporal coefficients matrix being used by the DLinear model to forecast fu-\nture values. The architecture is optimal, since all calculations are performed\non the low-resolution data, drastically reducing the computational cost, and\nonly at the end of the pipeline is the newly predicted data reconstructed into\nhigh-resolution snapshots using the denoised data created at the beginning.\nBased on the ease for the model to identify patterns in the temporal coeffi-\ncients, the output sequence length can vary considerably. For example, most\nof the temporal coefficients extracted from turbulent data contain chaos and,\ntherefore, reduce the model capability to produce a large forecast sequence\nbefore diverging to the trend of each mode while, in the case of the temporal\ncoefficients from laminar datasets, periodic trends are clearly identifiable in\nthe most relevant modes, so the forecast sequence can be up-to 30 times the\nsize of the input sequence.\nThe previous statement has been demonstrated by using the model to\nforecast future snapshots of the two test cases, which consist of a numerical\nsimulation of a laminar flow passing a three-dimensional cylinder, and an\nexperimental turbulent flow passing a circular cylinder. The effects of the\nnumber of retained modes on the models performance accuracy has also been\ntested proving that, for laminar datasets, the models performance is consis-\ntent, independently to the number of retained modes while, for turbulent\ndata, the models accuracy is inversely proportional to the number of modes.\nBy analyzing the frequencies, it has been possible to find the relationship\nbetween the models forecast performance of the main modes, relating peak\nresults to periodicity (and unique frequencies) in the data.\nThe hybrid model presented in this article will be incorporated into a fu-\nture release of ModelFLOWs-app, which can be downloaded from the official\nModelFLOWs-app software website [53]."}]}