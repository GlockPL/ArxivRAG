{"title": "SAIL: Sample-Centric In-Context Learning for Document Information Extraction", "authors": ["Jinyu Zhang", "Zhiyuan You", "Jize Wang", "Xinyi Le"], "abstract": "Document Information Extraction (DIE) aims to extract structured information from Visually Rich Documents (VRDs). Previous full-training approaches have demonstrated strong performance but may struggle with generalization to unseen data. In contrast, training-free methods leverage powerful pre-trained models like Large Language Models (LLMs) to address various downstream tasks with only a few examples. Nonetheless, training-free methods for DIE encounter two primary challenges: (1) understanding the complex relationship between layout and textual elements in VRDs, and (2) providing accurate guidance to pre-trained models. To address these challenges, we propose SAmple-centric In-context Learning (SAIL). SAIL introduces a fine-grained entity-level textual similarity to facilitate in-depth text analysis by LLMs and incorporates layout similarity to enhance the analysis of layouts in VRDs. Moreover, SAIL formulates a unified In-Context Learning (ICL) prompt template for various sample-centric examples, enabling tailored prompts that deliver precise guidance to pre-trained models for each sample. Extensive experiments on FUNSD, CORD, and SROIE benchmarks with various base models (e.g., LLMs) indicate that our SAIL outperforms training-free baselines, even closer to the full-training methods, showing the superiority and generalization of our method.", "sections": [{"title": "1 Introduction", "content": "Document Information Extraction (DIE) focuses on extracting structured information from Visually Rich Documents (VRDs) such as receipts, forms, and invoices (Park et al. 2019; Huang et al. 2019; Jaume, Ekenel, and Thiran 2019). Previous works, including LayoutLMv3 (Huang et al. 2022), primarily concentrate on full-training methodologies that demand extensive task-specific labeled data. While these models have achieved notable success on the trained dataset, they often struggle to generalize effectively to unseen data, especially when the test data distribution significantly diverges from that of the training data. To address this challenge, training-free DIE methods (He et al. 2023) leverage powerful pre-trained models like Large Language Models (LLMs) that can generalize to unseen data given only a few examples, and thus begin to attract more research interests.\nOne of the primary challenges in the training-free DIE task is understanding the complex relationship between the document layout and its textual entities using only a few examples. VRDs possess discrete textual elements alongside flexible, inherently structured layouts, complicating the establishment of relationships between textual entities and the extraction of implicit layout information. Even the advanced multi-modal LLMs like GPT-40 (OpenAI 2023b) demonstrate limited effectiveness in performing DIE task. As illustrated in Figure 1(c), GPT-40 misidentifies three entity texts and labels three entity texts incorrectly, highlighting the challenges inherent in the training-free DIE task.\nAnother significant challenge is providing a clear and effective guidance to pre-trained models (e.g., LLMs). Although these models possess extensive knowledge and capabilities, they necessitate appropriate instructions for optimal performance on specific downstream tasks. Recent research has incorporated In-Context Learning (ICL) within LLMs to enhance the DIE performance (He et al. 2023). This approach involves selecting a few textually similar examples and carefully crafting the in-context prompts with diverse demonstrations for the entire dataset. While this method shows promising results in GPT-3.5 (Brown et al. 2020), the fixed in-context examples fail to effectively guide different LLMs, leading to a significant performance decline when transitioning across different LLMs, as detailed in Table 1.\nTo address these challenges, we propose a Sample-centric In-context Learning (SAIL) method. Our method follows two core principles: (a) To enhance LLMs' understanding of the complex interplay between layout and text within VRDs, the provided prompts must analyze the question from different angles in depth. (b) To ensure precise guidance, it is essential to develop a customized prompt for each test sample. Regarding the first principle, previous methods (He et al. 2023) only adopted rough document-level textual similarity for example selection, which inadequately supports LLMs in understanding textual information in lengthy documents. Consequently, we propose a refined entity-level text similarity for in-depth text analysis. Additionally, we incorporate layout similarity to identify examples that enjoy similar layouts, facilitating LLMs in comprehending complex layout information in VRDs. The three distinct examples are illustrated in figure 1(b). For the second principle, we select distinct examples for each test sample and integrate them into a unified prompt template with clear instructions to devise a tailored sample-centric in-context prompt.\nEquipped with these designs, our proposed SAIL demonstrates versatility across various LLMs on multiple benchmarks. SAIL not only stably surpasses all training-free baselines, but even achieves comparable performance to many fully-trained models when implemented with GPT-4. Overall, our main contributions can be summarized as follows:\n\u2022 We introduce layout similarity and entity-level text similarity, each highlighting unique facets of VRDs, resulting in a thorough and in-depth analysis of VRDs.\n\u2022 To form sample-centric in-context prompts, we propose a unified ICL prompt template applicable to various examples. With clear instructions, LLMs enhance their attention to specific information in the examples.\n\u2022 We conduct extensive experiments on multiple benchmarks including FUNSD, CORD, and SROIE with various base LLMs. Our SAIL achieves superior performance than training-free baselines, even closer to the performance of full-training methods."}, {"title": "2 Related Works", "content": "Document Information Extraction (DIE). Traditional DIE methods primarily rely on extensive datasets for model pre-training and subsequent fine-tuning on downstream tasks. These methods can be classified into four main categories. The first category consists of grid-based methods (Katti et al."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Training-free DIE leverages pre-trained models (e.g., LLMs) to extract specified categories of text information (e.g., company, address, and date (Huang et al. 2019)) from VRDs. Specifically, given a document image I, the goal is to label all entities within I. First, entity texts T = {t1,t2,..., tne} and their corresponding boxes B = {b1, b2,..., bne} are recognized from I by an OCR system, where ne is the total number of entities in the document image. To effectively utilize LLMs, in-context prompts C are designed to convey the extraction intention. For ICL-based DIE, C is constructed by selecting several examples demonstrating how to solve DIE tasks. With these in-context prompts as illustrations, LLMs are tasked with generating labels Ypred for all detected entities. The process is achieved by maximizing the conditional probability P(Y|T, B) while incorporating the prompts C as an additional condition:\n$$P(Y|T, B) = \\frac{1}{ne}\\sum_{k=1}^{ne} PLM (1k/C, (T, B)),$$ (1)\nwhere PLM is the conditional probability predicted by the LLMs, and denotes the operation of converting the entity texts and boxes into a textual format suitable for LLMs' input. In training-free DIE, the construction of effective in-context prompt C is crucial, which is the primary focus of this work. Finally, the predicted labels Ypred are evaluated using F1 scores against the ground truth labels Ygt."}, {"title": "3.2 Overview Framework", "content": "To maximize P(Y|T, B) with the in-context prompt C, we propose SAIL, a sample-centric in-context prompt construction method for DIE. SAIL focuses on designing C for individual samples by automatically selecting tailored layout examples, document-level text similarity examples, and entity-level text similarity examples based on the test sample, subsequently leveraging these examples to generate C.\nThe overall architecture, illustrated in Figure 2, comprises five steps. Firstly, the test document image and m training document images are processed through OCR to extract entity texts T and boxes B. Secondly, T are transformed into entity-level text embeddings Eent and document-level text embeddings Edoc. B are used to construct layout image \u2160. Thirdly, Eent, I and Edoc are used to select textually similar entities, layout similar documents, and textually similar documents for the test sample. Then, these selections are substituted into the prompt template to form a tailored in-context prompt C. Finally, LLM performs inference with C and question (T, B) to generate predicted labels Ypred."}, {"title": "3.3 Document-Level Text Similarity Examples", "content": "To improve the capability of ICL, we employ text semantic search to select the nearest training document examples for a given test sample (Liu et al. 2022). The entity texts T extracted from a document image are concatenated into a single sentence and encoded with Sentence-BERT (Reimers and Gurevych 2019), resulting in a text semantic embedding Edoc for the document. We determine the nearest training examples by computing the document-level text similarity Tsim_doc between the test embedding Etest and m training embeddings Etrain using the cosine similarity score:\n$$Tsim_doc = \\frac{Etest. Etrain}{|| Etest || || Etrain ||}$$(2)"}, {"title": "3.4 Entity-Level Text Similarity Examples", "content": "The document-level text similarity Tsim_doc between a lengthy text document and the found text-similar documents is notably low. To facilitate LLMs in generating text with more relevant examples for learning, we propose entity-level text similarity examples, as shown in Figure 2.\nEntity texts T = {t1,t2,...,tne} recognized by OCR are filtered to exclude texts consisting solely of numbers, which provide minimal semantic content. Subsequently, the filtered mf training entity texts and nf test entity texts are encoded using Sentence-BERT to derive the semantic embedding Eent. The entity-level text similarity Tsim_ent is computed from the semantic embedding Eent by employing the cosine similarity score, defined as follows:\n$$Tsim_ent = \\frac{Etest. Etrain}{|| Etest || || Etrain||},$$ (3)\nWe select ns textually similar entities for each test entity by nearest neighbor search and obtain nf \u00d7 ns examples."}, {"title": "3.5 Layout Similarity Examples", "content": "To identify documents with similar layouts, we introduce a layout similarity assessment methodology, illustrated in Figure 3. Firstly, all b\u00bf from boxes B = {b1, b2, ..., bn } are rendered as black rectangles on a blank image. Subsequently, we define the information area as the minimal region that contains all entity texts and crop the layout image to maintain a 10-pixel margin between the information area and the image borders. Next, we standardize the layout image dimensions through resizing. Finally, we select ng layout similar documents by calculating the layout similarity Lsim between the training layout image \u012btrain and the test layout image [test using Mean Square Error (MSE) loss:\n$$Lsim = \\frac{1}{n1}MSE (U \u2013 V)T(U \u2013 V),$$ (4)\nwhere U, V are the pixel matrix of \u012btrain and \u012btest, and n\u2081 is the total number of pixels in the layout image.\nMoreover, to enhance the understanding of layouts by LLMs, we substitute the boxes from the cropped image B' for all documents in the prompt instead of using B."}, {"title": "3.6 Sample-Centric ICL Prompt Template", "content": "To construct C for an individual test sample, we propose an adaptive sample-specific ICL prompt template. The template is comprised of 5 parts: candidate labels illustration Ccl, entity-level text demonstrations Cet, layout demonstrations C\u2081, document-level text demonstrations Cat and test question (T, B), as shown on the right of Figure 2.\nCandidate labels illustration Cel enumerates all potential labels for the DIE task. For abbreviated labels, a corresponding natural language description is appended.\nEntity-level text demonstrations Cet present textually similar entities. The prompt pe \"Sample text and corresponding label:\" in conjunction with the labels of the selected ns textually similar entity examples Yet, formulates the entity-level text similarity demonstrations:\n$$Cet = CONCAT[pe, Yet].$$(5)\nLayout demonstrations C\u2081 aim to facilitate LLMs in analyzing the layout of the test document. After obtaining ns layout similar documents, we introduce a layout analysis step. This step enables LLMs to comprehend the overall document structure and the relationship between layout and label selection. The layout analysis prompt pa is defined as: \"These are the information extracted from the document through OCR, and the Box is the position of the text in the document. Please analyze where each label is generally located in the document.\", which can apply to any dataset. The labels of layout-similar documents Y\u2081 are input into LLMs together with pa, allowing LLMs to analyze the layout information in layout-similar documents by themselves. The resulting output from the LLM is denoted as A\u2081. A layout similarity demonstration C\u2081 is formulated as follows:\n$$C\u2081 = CONCAT[Y1, Pa, A1].$$(6)\nDocument-level text demonstrations Cat showcase textually similar documents in question-answer format, guiding LLMs to produce answers in a specific format. The textually similar documents Xdt, the ground truth answer Yat and the DIE instruction pq such as \"What are the labels for these texts?\" form the Document-level text demonstration prompt:\n$$Cat = CONCAT[Xdt, Pq, Yat].$$(7)\nFinally, the test question (T, B) for the test sample is:\n$$(T, B) = CONCAT[T, \u0392', pq].$$(8)"}, {"title": "3.7 Inference", "content": "After selecting a diverse set of examples, ICL prompts facilitate LLMs in generating entity labels Ypred. This process is mathematically represented as follows:\n$$P(Y|T, B) = \\frac{1}{Ne}\\sum_{k=1}^{ne} PLM (1k/Ccl, Cet, C1, Cat, (T, B)).$$(9)\nSubsequently, entity labels Ypred are extracted from the generated output. We assess the accuracy of Ypred against the ground truth labels Yet utilizing the F1 score."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets, Metrics, and Details", "content": "FUNSD (Jaume, Ekenel, and Thiran 2019) is a dataset for understanding the content of tables in scanned documents. It contains 149 tables and 7,411 entities in the training set, and 50 tables and 2,332 entities in the test set. In the DIE task, the candidate labels of the FUNSD dataset include \"Header\", \"Question\u201d, \u201cAnswer\", and \"Other\".\nSROIE (Huang et al. 2019) is another scanned receipt understanding dataset, containing 626 receipts in the training set and 347 in the test set. The DIE task needs to extract \"company\", \"date\", \"address\", and \"total\" information.\nCORD (Park et al. 2019) is a receipt understanding dataset that contains 800 training data, 100 test data, and 100 validation data. This dataset features 30 detailed and hierarchical labels, much more than the above two datasets.\nMetrics. Following previous works (He et al. 2023), we adopt entity-level F1 score, precision and recall as metrics.\nDetails. We evaluate our method using three LLMs: the open-source ChatGLM3 (THUDM 2023) and the closed-source GPT-3.5 (OpenAI 2023a) and GPT-4 (OpenAI 2023b). Specifically, we use the chatglm3-6b-32k version for ChatGLM3, gpt-3.5-turbo API version for GPT-3.5, and gpt-40 API version for GPT-4. For GPT-3.5 and GPT-40, we set the temperature parameter to 0 to enhance the reproducibility. In the case of GPT-40, we only provide text prompts as input, while also testing its multimodal capabilities by providing document images and clear task instructions. In our experiments, for each test document, we select four textually similar documents and four layout-similar documents as examples due to the limitation of prompt token number. Furthermore, for each filtered test entity, we choose four textually similar entity examples."}, {"title": "4.2 Results on DIE Benchmarks", "content": "Baselines. We compare our SAIL against baseline models including BERT (Devlin et al. 2019), LiLT (Wang, Jin, and Ding 2022), BROS (Hong et al. 2022), XYLayoutLM (Gu et al. 2022), LayoutLM (Gu et al. 2022), LayoutLMv2 (Xu et al. 2021), and LayoutLMv3 (Huang et al. 2022) in both full-training and few-shot settings. We borrow their metrics from (He et al. 2023). Training-free methods including standard ICL and ICL-D3IE (He et al. 2023) are also compared. ICL-D3IE only reports the performance of standard ICL and ICL-D3IE with GPT-3.5, so we evaluate their performance with GPT-4 and ChatGLM3 using their official repositories.\nQuantitative results are presented in Table 1. First, overall, our method stably outperforms ICL-D3IE across different LLMs on all datasets. Second, when switching the LLM from GPT-3.5 to ChatGLM3, the performance drop of ICL-D3IE is significantly larger than our SAIL (e.g., -73.8% vs.-12.73% in SROIE), demonstrating that our method has better robustness and adaptability to various LLMs. Third, the performance of ICL-D3IE degrades slightly when transitioning from GPT-3.5 to the more advanced GPT-4 on FUNSD and SROIE datasets, further indicating its incompatibility with new LLMs. However, in all datasets, our method achieves better performance on more advanced GPT-4 than on GPT-3.5, which is intuitive and reasonable. These results demonstrate the advantages of our method.\nQualitative results are illustrated in Figure 4. ICL-D3IE incorrectly predicts the entities on the three left green boxes as \"answer\", while our SAIL accurately identifies them as"}, {"title": "4.3 Comparison with Multi-modal LLMS", "content": "Baselines. Recent years have witnessed the rapid development of multi-modal LLMs (MLLMs) represented by GPT-40 (OpenAI 2023b). To further validate the effectiveness of our method, we also compare our SAIL with MLLMs including open-source LLaVA-1.5 (Liu et al. 2024a) and proprietary GPT-40. We provide these MLLMs with explicit and detailed instructions to inform the task definition.\nQuantitative results are provided in Table 2. The open-source LLaVA exhibits limited DIE capabilities, resulting in a low F1 score (e.g., 0.7% in FUNSD). The proprietary GPT-40 significantly outperforms LLaVA (50.72% vs 0.7% in FUNSD), yet still falls short when compared to specialized DIE methods. Therefore, despite their rapid evolution, MLLMs still underperform in the DIE task, highlighting the importance and contribution of our proposed work."}, {"title": "4.4 Ablation Studies", "content": "Effect of Adaptive Example. We assess the influence of adaptive examples by employing both fixed and adaptive examples to construct in-context prompts within the same prompt template. The base LLM is selected as GPT-3.5, and the results are illustrated in Table 3. The utilization of adaptive examples results in superior F1 scores, confirming the"}, {"title": "5 Conclusions and Limitations", "content": "In this work, we propose SAIL, a sample-centric ICL method for training-free DIE task. Our SAIL leverages layout similarity and entity-level text similarity in combination with a unified prompt template, constructing tailored prompts for each test sample, showcasing superiority over baselines on three DIE benchmarks with different LLMs."}, {"title": "Appendix", "content": ""}, {"title": "A Implementation Detail", "content": "We test our SAIL with ChatGLM3 on an RTX 3090 GPU. For FUNSD, we set the maximum token output to 2500; For CORD and SROIE, we set the maximum token output to 1500. Due to the randomness of the generation of LLMs, we have set some simple checks, such as whether there is a \"{\", and if the generated result does not meet the requirements, it is regenerated. In the FUNSD dataset, we set the number of examples to 2, and if the number of tokens exceeds the limit, the number of examples is changed to 1. In the SROIE dataset, we set the number of examples to 4, and if the number of tokens exceeds the limit, the number of examples is changed to 2."}, {"title": "B Additional Ablation Studies", "content": "Results on Synthetic Data. We explore two data-synthetic methods.\n\u2022 Replace text. Randomly replace the text with other texts of the same label while keeping the original layout.\n\u2022 Replace layout. Keep the text unchanged and replace the layout with those from other documents."}]}