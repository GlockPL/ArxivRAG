{"title": "HACMan++: Spatially-Grounded Motion Primitives\nfor Manipulation", "authors": ["Bowen Jiang", "Yilin Wu", "Wenxuan Zhou", "Chris Paxton", "David Held"], "abstract": "Although end-to-end robot learning has shown some\nsuccess for robot manipulation, the learned policies are often not\nsufficiently robust to variations in object pose or geometry. To im-\nprove the policy generalization, we introduce spatially-grounded\nparameterized motion primitives in our method HACMan++.\nSpecifically, we propose an action representation consisting of\nthree components: what primitive type (such as grasp or push)\nto execute, where the primitive will be grounded (e.g. where the\ngripper will make contact with the world), and how the primitive\nmotion is executed, such as parameters specifying the push\ndirection or grasp orientation. These three components define a\nnovel discrete-continuous action space for reinforcement learning.\nOur framework enables robot agents to learn to chain diverse\nmotion primitives together and select appropriate primitive\nparameters to complete long-horizon manipulation tasks. By\ngrounding the primitives on a spatial location in the environment,\nour method is able to effectively generalize across object shape\nand pose variations. Our approach significantly outperforms\nexisting methods, particularly in complex scenarios demanding\nboth high-level sequential reasoning and object generalization.\nWith zero-shot sim-to-real transfer, our policy succeeds in chal-\nlenging real-world manipulation tasks, with generalization to\nunseen objects. Videos can be found on the project website:\nhttps://sgmp-rss2024.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite recent progress in training manipulation policies\nwith reinforcement learning (RL), it remains challenging to\nscale RL training to longer-horizon problems with broader\ntask variations [6, 10, 36, 17]. A significant limitation is\nthat most robot manipulation policies reason over the space\nof granular robot-centric actions, such as gripper or joint\nmovements [15, 32, 37, 35]. These action spaces are highly\ninefficient for longer-horizon tasks due to exploration, credit\nassignment, and training stability challenges in deep reinforce-\nment learning [6, 17].\nInstead of learning policies over low-level timesteps, the\nrobot should reason about long-horizon manipulation problems\nwith general, reusable primitives. For example, to make coffee,\nthe robot may segment the task into picking up a mug and\nthen placing it under the coffee machine. This process involves\ndecomposing the task into a \u201cgrasping\u201d stage followed by a\n\"placing\" stage. With a similar idea, prior work has proposed\napplying a hierarchical structure in robot decisions, such as\noptions or skill primitives [3, 16, 30]. These methods decouple\nthe high-level decisions of \"what\" to do from the low-level"}, {"title": "II. RELATED WORK", "content": "Hierarchical Reinforcement Learning. Prior work has\nintegrated a hierarchical structure into reinforcement learning\nto reduce the challenge of long-horizon reasoning for RL\nalgorithms [27]. In hierarchical reinforcement learning, a high-\nlevel policy will communicate with one or more low-level\npolicies to finish the task. However, it can be difficult to\njointly optimize both the high and low-level policies [7].\nAlternatively, prior work has proposed to first learn a set of\nlow-level skills from an offline dataset [18, 2, 22, 21, 25].\nInstead, we follow prior work in the robotics domain and\ndefine the low-level policies as commonly used primitives such\nas grasping, placing, and pushing [3, 16]. We compare our\nmethod to other methods that use \"skill libraries,\" including\nsome hierarchical RL methods, explained below.\nSkill Libraries. Prior work [9] has specifically designed a\nset of primitives including approach, contact, fit, align, and\ninsertion, as a skill library. However, this set of primitives\nis not generalizable to other tasks. Furthermore, it assumes\na pre-specified order of primitives to be executed to finish\na given task. Another line of work defines the action space\nof the RL policy based on a more general set of pre-defined\nparametrized primitives such as RAPS [3], MAPLE [16], and\nParameterized DQN [30]. The RL policy learns to automat-\nically chain different primitives together to achieve a task\nwithout assuming a fixed order of primitives. This also means\nthat the agent can re-execute primitives when a failure occurs.\nOur method inherits the benefits of those work in RL policy\nwith parameterized primitives and also differs from them in\nthat we spatially ground the primitives to improve spatial\nreasoning. We compare our method to these prior methods\nin the experiments and demonstrated significantly improved\nperformance.\nSpatial Action Maps. Spatial action maps connect a dense\naction representation with visual inputs using segmentation-\nstyle models [34, 20, 23, 13, 31, 29]. Our method pro-\nposes a novel combination of motion primitives with spatial\naction maps to incorporate both temporal abstraction and\nspatial reasoning. Most prior work on spatial action maps has\nlimitations on requiring expert demonstrations for imitation\nlearning [34, 20, 23] or is limited to one-step decisions without\nsequential reasoning [13, 31, 29]. Our work is the most related\nto [36, 4]; however, Zhou et al. [36] is limited to one non-\nprehensile skill (pushing) while we use a set of heterogeneous\nskills that can be combined to achieve more complex tasks.\nFeldman et al. [4] uses 2 skills, grasp and shift, and their"}, {"title": "III. BACKGROUND", "content": "A stochastic sequential decision problem can be formal-\nized as a Markov Decision Process (MDP), characterized by\n(S, A, P, r, \u03b3). Here, S denotes the states, A represents the\nactions, P(St+1|St, at) is the likelihood of transitioning from\nstate st to state st+1 given action at, and r(st, at, St+1) is the\nreward obtained at time t. The goal within this framework is to\noptimize the return Rt, which is the sum of discounted future\nrewards, expressed as \\(R_t = \\sum_{i=0}^\\infty \\gamma^i r_{t+i}\\). Under a policy \u03c0,\nthe expected return for taking action a in state s is described\nby the Q-function Q\u03c0(s, a) = E\u03c0[Rt|St = s, at = a].\nHACMan++ leverages Q-learning-based algorithms for con-\ntinuous action spaces [11, 5]. These methods are characterized\nby a policy \u03c0\u03b8 with parameters \u03b8, and a Q-function Q\u03c6 with\nparameters \u03c6. Training involves collecting a dataset D of state\ntransitions (st, at, St+1), with the Q-function's loss formulated\nas:\n\\(L(\\phi) = E_{(s_t,a_t,s_{t+1}) \\sim D}[(Q_\\phi(s_t, a_t) - y_t)^2]\\),\nwith yt being the target value, determined by:\n\\(y_t = r_t + \\gamma Q_\\phi(s_{t+1}, \\pi_\\theta(s_{t+1})).\\)\nThe optimization of the policy \u03c0\u03b8 is described by the loss\nfunction:\n\\(J(\\theta) = -E_{s_t \\sim D}[Q_\\phi(s_t, \\pi_\\theta(s_t))].\\)"}, {"title": "IV. METHOD", "content": "Assumptions. We assume that the robot agent records a\npoint cloud observation of a scene X, which may be obtained\nfrom one or more calibrated depth cameras. We further assume\nthat this point cloud is segmented into object Xobj and\nbackground Xb components. See Appendix A for details.\nTo address the challenges of long-horizon manipulation\ntasks, our method uses a set of parameterized motion primi-\ntives, and learns how to both 1) chain these primitives together\nto achieve a task and 2) select appropriate parameters for\nthe execution of each primitive. Section IV-A defines the\nstructure of the proposed action representation. Section IV-B\nlists the specific choices of parameterized motion primitives.\nSection IV-C describes how we train the policy with the\nproposed action space with the RL algorithm.\nA. Action Representation\nOur action representation comprises three key elements:\nthe primitive type aprim, the primitive location aloc, and\nthe primitive parameters am. These components collectively\ndefine the \"What\", \"Where\", and \"How\" of each sequential\nskill execution.\nPrimitive Type aprim determines the type of primitive the\nrobot will execute, such as poking, grasping, or placing (see\nthe full list in Section IV-B). The robot policy aims to\nlearn to perform different tasks by chaining the primitives\nin appropriate order based on the observations. Each type of\nprimitive is uniquely parameterized to allow for variations in\nexecution, adapting to the specific demands of the task. Once\nthe parameters are specified, these primitives are executed with\na low-level controller.\nPrimitive Location aloc is a selected point of interaction\nin the scene, chosen from the observed point cloud X. The"}, {"title": "B. Parameterized Motion Primitives", "content": "We use five distinct and generic motion primitives, that\ncollectively satisfy the needs of a wide range of manipu-\nlation tasks, following the primitive designs from previous\nwork [3, 16]. Each primitive has its own specific parameters\ndescribed below. More details of the motion parameters for\neach primitive can be found in Appendix A.\nPoke: This primitive applies a non-prehensile poking motion\nto the target object [36, 33, 4, 1]. The robot moves the fingertip\nof the gripper to the selected primitive location aloc on the\nobject as the initial contact point (see Appendix A for details).\nThe motion parameters am consists of two parts: 1) the 2D\ngripper orientation while approaching the initial contact point,\nand 2) parameters that describe the poking motion after the\ngripper reaches the initial contact point on the object, defined\nas a 3D vector of gripper translation.\nGrasp: This primitive grasps the target object and then lifts it\n[14, 26, 4]. The primitive location aloc under the grasp\nprimitive type defines a grasping point on the object. The\nmotion parameters am detail the 2D gripper orientation while\napproaching the grasping point. Upon reaching the grasping\npoint, the gripper closes to grasp the object. It then lifts up by\na pre-specified distance (see Appendix A).\nMove to:\nThis primitive moves the gripper to a location that is defined\nrelative to a point aloc selected from the background point\ncloud Xb. The primitive parameters am contain two parts: 1)\nthe 2D gripper orientation when approaching the location, and\n2) a 3D vector defining an offset from the selected location\naloc; the target point for the gripper to move to is given by\nthe selected location aloc plus this offset. The selected location\naloc grounds this motion on the point cloud, whereas the added\noffset gives the robot more flexibility in where to move. To\nspeed up exploration, we restrict the primitive location aloc to\nbe selected from the background points and we only execute\nthis primitive when the gripper is already grasping an object.\nOpen Gripper: This primitive opens the gripper. The selected\nlocation aloc has no influence on the action, and this primitive\ndoes not require any motion parameters.\nMove delta: To account for any nuanced movements that are\ndifficult to achieve with the above primitives, we include the\n\"Move delta\" primitive to move the gripper by a 3D delta\nmovement and 2D orientation. Motion parameters for this\nprimitive specify a delta translation and rotation of the gripper."}, {"title": "C. Hybrid RL Algorithm", "content": "HACMan++ integrates a multi-primitive approach with ex-\nisting Q-learning-based RL algorithms [5, 8, 11]. Our action\nspace includes both discrete and continuous components: the\nprimitive type aprim is selected from K primitives; for each\nprimitive type, the primitive location aloc is selected from N\npoints from the observed point cloud; whereas the motion\nparameters am are a vector of continuous values.\nThe overall architecture of our approach is shown in Fig-\nure 2. The agent receives as input a point cloud observation of\nsize N. We first use a segmentation-style point cloud network\nto output per-point actor features fa for each point xi. These\nfeatures are shared across the K different primitives. We then\ninput each of these features into a per-primitive MLP to output\nmotion parameters am for each point xi and each primitive\nk. We refer to these outputs as an \"Actor Map\".\nOur method also extracts per-point critic features fi for each\npoint 2 through a segmentation-style point cloud critic feature\nextractor. These features are shared across the K different\nprimitives. The per-point motion parameters am are then\nconcatenated with per-point critic features fi and input into a\nmulti-layer perceptron (MLP) to calculate per-point Q values\nQi,k for each point xi and each primitive k; this Q-value\nrepresents the effectiveness of executing the kth primitive with\nthe motion parameters am at the primitive location xi. The\nabove procedure generates a \"Critic Map\" with a total of KN\nQ-values across all points and all primitives (see Figure 2).\nThe optimal action is chosen by selecting the highest\nQ-value Qmax from the critic map, which corresponds to\nprimitive type k, primitive location xi, and motion parameters\na. During training, the policy selects primitive types and\nlocations by sampling from a softmax distribution over Q-\nvalues to balance exploration and exploitation, formalized as:\ndiscrete(k, xis) = \\( \\frac{exp(Q_{i,k}/\\beta)}{\\sum_{K}\\sum_{N} exp(Q_{i,k}/\\beta)}\\)\nwhere \u03b2 is a temperature parameter modulating the softmax\nfunction, guiding the agent's exploratory behavior.\nThe Q-function is updated according to the Bellman equa-\ntion (Equation 1) following TD3 [5]. To update the primitive\nparameters amk, we similarly follow the TD3 algorithm [5]: If\nwe define the actor \\(a_{\\theta}^{m}\\)(s) as the function parameterized by \u03b8\nthat maps from the observation s to the action parameters ak\nfor a given point xi and primitive k, then the loss function for\nthis actor is given by:\nJ(\u03b8) = -Q\u03c6(fi,amk) = -Q\u03c6(fi, \u03c0i,k(s)),\nwhere Q\u03c6 is the critic network and fi is the critic feature of\nthe point xi.\nTo assist the network in understanding the relationship\nbetween the observation and the goal, we compute the corre-"}, {"title": "V. EXPERIMENTAL SETUP", "content": "We evaluate our method on three ManiSkill tasks\n(Sec. V-A), two Robosuite tasks (Sec. V-B), as well as a\nDoubleBin task (Sec. V-C) as illustrated in Figure 3. This\nsection outlines the setup, objective, and reward function for\neach task.\nA. ManiSkill Tasks\nWe evaluate our method with three tasks from Man-\niSkill [15] (Figure 3, Left). For these tasks, we train the hybrid\nactor-critic map with the default reward functions defined in\nthe ManiSkill benchmark [15].\nLift Cube: The agent is tasked with picking up a cube and\nlifting it to a specified height threshold. The initial cube\nposition and orientation are randomized.\nStack Cube: This task involves stacking a red cube on top\nof a green cube, requiring precision in alignment. The initial\nposition and orientation of both cubes are randomized.\nPeg Insertion: This task involves inserting a peg horizontally\ninto a hole in a box. As the original ManiSkill paper [15]"}, {"title": "VI. SIMULATION RESULTS", "content": "In our simulation experiments, we aim to answer the fol-\nlowing questions:\n\u2022\n\u2022\n\u2022 Does the learned policy generalize to unseen objects?\nThe comparison between our method and baselines over six\ntasks is reported in Figure 4. The details of the training and\nevaluation procedures can be found in Appendix B.\nEffect of Spatial Grounding. To demonstrate the benefits of\nspatial grounding, we compare our method to two baselines,"}, {"title": "VII. REAL-WORLD EXPERIMENTS", "content": "We perform evaluations on the real world DoubleBin task\nwith the policy trained in simulation as discussed in the\nprevious sections. At the beginning of each episode, we place\nthe object at a random pose in a randomly chosen bin. We also\nspecify a goal SE(3) transformation, which can be either in the\nsame bin as the initial object pose or in the opposite side bin.\nAmong all the objects we are testing, Rubik's Cube, Bowl,\nCup, Tennis are evaluated with the translation goals because\nof their rotation-symmetric shape. At each step, we perform\npoint cloud registration to compute the correspondence from\nthe current observation to the goal (see details in Appendix D).\nSimilar to our simulation environment, an episode is deemed\na success when the mean distance between each observation"}, {"title": "VIII. CONCLUSION", "content": "In this work, we present spatially grounded motion prim-\nitives for robot manipulation tasks, leveraging hybrid actor-\ncritic maps with reinforcement learning. Our agent learns to\nchain different spatially-grounded primitives with appropri-\nately selected primitive parameters to complete a task. Our"}, {"title": "APPENDIX", "content": "A. Algorithm and Implementation Details\n1) Observations:\nThe robot agent first perceives a point cloud X \u2208 RM\u00d73\nfor the entire scene by stacking multiple cameras' views\ntogether, where M is the number of points in the raw point\ncloud observations. Our method assumes that we pre-process\nthe scene by segmenting the object point cloud Xobj from\nthe background point cloud X'; details of the segmentation\nprocess are listed in Appendix B1 and Appendix D1. After\nsegmentation, we downsample Xobi and Xb with voxel sizes\nof 1cm and 2cm respectively. We then randomly sample 400\nand 1000 points from Xobi and Xb respectively.\nTo make our policy also goal-conditioned, we append the\ngoal information into the observation as \"goal flow\" in which\nwe compute the per-point correspondence from the current\nobject point cloud to the goal object point cloud. Specifically,\nfor each point xi in the object point cloud, the goal flow is\n\u2206xi = x-xi, where x is a corresponding point of xi in the\ngoal point cloud. In the simulation, we use the ground-truth\npoint correspondence given the object pose and the goal pose.\nIn the real world, we use point cloud registration to align the\nobservation to the goal (see Appendix D2).\nTherefore, the entire observation space (Op, Og, Om) of our\nrobot agent includes three parts: the point cloud op represent-\ning the 3D position (x, y, z) of the points (3-dimensions per\npoint), the goal flow og indicating the flow from the current\nobject point cloud to the point cloud of the object in the goal\npose (3-dimensions per point), and the segmentation mask om\n(1-dimension per point).\n2) Primitive Implementation Details:\nWe have five generic motion primitives that can be used\nstrategically and collectively to solve long-horizon manip-\nulation tasks. The details of the actual execution of those\nprimitives are listed below.\n=\n=\nPoke: The Poke primitive is parameterized by a location\nparameter aloc \u2208 Xobj and a motion parameter am\n(xm, ym, , zm zm, 0x, 0y) \u2208 (\u22121,1)5, described below. The gripper\nfirs"}]}