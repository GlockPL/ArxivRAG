{"title": "Large Language Models and the Rationalist-Empiricist Debate", "authors": ["David King"], "abstract": "To many Chomsky's debates with Quine and Skinner are an updated version of the Rationalist-Empiricist debates in the 17th century. The consensus being that Chomsky's rationalism was victorious. This dispute has reemerged with the advent of Large Language Models. With some arguing that LLMs vindicate rationalism because of the necessity of building in innate biases to make them work. The necessity of building in innate biases is taken to prove that empiricism hasn't got the conceptual resources to explain linguistic competence. Such claims depend on the nature of the empiricism one is endorsing. Externalized empiricism has no difficulties with innate apparatus once they are determined empirically (Quine 1969). Thus, externalized empiricism isn't refuted because of the need to build in innate biases in LLMs. Furthermore, the relevance of LLMs for the rationalist-empiricist debate in relation to humans is dubious. For any claim about whether LLMs learn in an empiricist manner to be relevant to humans it needs to be shown that LLMs and humans learn in the same way. Two key features distinguish humans and LLMs. Humans learn despite a poverty of stimulus and LLMs learn because of an incredibly rich stimulus. Human's linguistic outputs are grounded in sensory experience and LLMs are not. These differences in how the two learn indicates that they both use different underlying competencies to produce their output. Therefore, any claims about whether LLMs learn in an empiricist manner are not relevant to whether humans learn in an empiricist manner.", "sections": [{"title": "Introduction.", "content": "This paper will evaluate the rationalist-empiricist debate in relation to Large Language Models\u00b9. Theorists have argued that because LLMs such as ChatGPT demonstrate surprisingly effective linguistic output, they provide evidence about human linguistic capacities (Piantadosi, 2023). It has also been suggested that LLMs are data which can be used to adjudicate between rationalist-empiricist debate (Buckner, C. 2018, Childers et al 2023, Long 2024). The paper will first consider the recent historical debate between rationalists and empiricists centring on the dispute between Chomsky, Skinner and Quine. This historical debate will illustrate what was traditionally at stake in the dispute. Once this is done the paper will evaluate whether the analogy between LLMs and human linguistic output is a good one; demonstrating that it is not an accurate analogy. And hence LLMs have little to tell us about the rationalist/empiricist debate pertaining to human cognitive capacities.\nFrom about 1950 there was a resurgence of interest in the Rationalist/Empiricist debate. With people viewing Chomsky as an updated rationalist carrying on the traditions of Leibniz and Descartes. While Quine and Skinner were viewed as modern day empiricists carrying on the traditions of Locke and Hume. And a consensus emerged that Chomsky's rationalism won out over the empiricism of Quine and Skinner (Lyons1970, Hornstein 2005, Boeckx 2006, Laka 2009). In recent years with the rise of LLMs some have argued that their architecture is data driven and hence they are an existence proof that empiricist learning is a viable way of modelling the mind (Milliere & Buckner. 2024a). Prompting debate where others argue that these models don't vindicate any kind of empiricism as they rely on innate architecture (Childers et al 2023).\nIt will be demonstrated that LLMs are too dissimilar to human linguistic capacities to be used as a model for them. LLMs acquire their linguistic ability through being trained on linguistic data orders of magnitude higher than what humans require to acquire their native language (Milliere, R & Buckner C. 2024a). Furthermore, while human language is sensorily grounded, LLMs are not (Harnard 2024). These key differences demonstrate that LLMS and humans learn through different underlying competencies therefore any claims as to whether LLMs learn in an empiricist manner has no significance for how humans learn.\nIn the final section it will be argued that given that human cognition is currently the best example on the planet of general intelligence, if we want to build AGI, then it would be appropriate to try to model them on human cognitive systems. But even here given that externalized empiricists have no difficulty with using innate architecture if it can be determined experimentally (Quine 1968). There seems to be less and less use for the distinction between architecture proposed by rationalists and empiricists. Any attempt to model an AGI system on human cognitive capacities should stick closely to the capacities postulated by our best science and avoid getting bogged down in arcane philosophical debates on empiricism vs rationalism."}, {"title": "Chomsky and the Rationalist-Empiricist Debate.", "content": null}, {"title": "Chomsky & Skinner.", "content": "Famously the rationalist-empiricist debate of the seventeenth century was revived in the 1960s. Noam Chomsky's 1959 review of Skinner's book Verbal Behaviour was the beginning of this revival. His review was critical of Skinner's empiricist model of Verbal Behaviour. Chomsky entitled his 1966 book \u2018Cartesian Linguistics: A Chapter in the History of Rationalist Thought', thus setting himself up as the heir apparent to Descartes rationalism. While Skinner was viewed by many as the heir to the mantle of the empiricist kingdom\u00b2.\nIn Skinner's Verbal Behaviour he divided language up into seven Verbal Operants which he argued are controlled using his three-term contingency of antecedent behaviour and consequence. Chomsky criticized Skinner for taking concepts from the laboratory based on experiments with rats and pigeons and extending them into areas where there was no such experimental evidence\u00b3. He charged Skinner of either using the concepts literally in which case his views were false, or metaphorically in which case the concepts offered no more insight than our own vague concepts derived from folk-psychological intentional discourse (Skinner 1959).\nIn his 1965 book \u2018Aspects of a Theory of Syntax' Chomsky made his famous distinction between competence and performance. Chomsky argued that the only substantive theory of performance which would be possible would come through an understanding of underlying competence (Chomsky, 1965). And he illustrated this point through showing how elements from our underlying grammatical competence could predict and explain aspects of our linguistic performance. Since behaviourism was obviously primarily concerned with behaviour many viewed Chomsky's competence performance distinction as an attack on behaviourism. Many scientists influenced by Chomsky argued that behaviourism not having a competence-performance distinction meant that it couldn't be taken seriously as a science (Jackendoff 2002, Collins 2007).\nChomsky (1972, 1986) poverty of stimulus argument was deemed a further dent in the behaviourist project (Hornstein 2005, Lyons 1970). Chomsky used the structure dependence of language, exemplified through auxiliary inversion, as an example of syntactic knowledge which a person acquired despite a poverty of stimulus. He argued that a person could go through much or all their life without ever encountering evidence for the construction. The reasoning being that if the child learned the construction despite never encountering evidence for its structure. And the child didn't engage in trial- and-error learning where he tried out incorrect constructions which were systematically corrected by his peers until he arrived at the correct one (Crain and Nakayama 1987,"}, {"title": "Quine and Chomsky.", "content": "The debate between Chomsky and Skinner was primarily focused on issues in linguistics, and psychology. In philosophy the rationalist-empiricist debate played out in a debate between Quine and Chomsky. Quine billed himself as an externalized empiricist whose primary aim was to explain how humans go from stimulus to science in a naturalistic manner. His entire project centred on naturalising both epistemology and metaphysics. On the epistemological side of things his need to explain how we go from stimulus to science would involve psychological speculations on how we acquire our language, and how we develop the ability to refer to objects. Quine was explicit in these speculations that any linguistic theory was bound to be behaviourist in tone since we acquire our language through intersubjective mouthing of words in public settings (Quine 1960). This commitment to behaviourism set Quine at odds with Chomsky.\nIn 1969 Chomsky's wrote a criticism of Quine called 'Quine's Empirical Assumptions'. This criticism noted that Quine's notion of a pre-linguistic quality space wasn't sufficient to account for language acquisition. That Quine's Indeterminacy of Translation Argument was trivial and amounted to nothing more than ordinary Underdetermination. And that Quine's invocation of the notion of the probability of a sentence being spoken was meaningless.\nSkinner never replied to Chomsky\u2074 arguing that Chomsky so badly misunderstood his position that further dialogue was pointless. But Quine (1968) did reply; in his reply he charged Chomsky with misunderstanding his position and of attacking a strawman. On the issue of a prelinguistic quality space he argued that it was postulated as a necessary condition of acquiring the ability to learn from induction or reinforcement; he never thought it was a sufficient condition of our acquiring language (ibid). Quine argued that \"the behaviourist was knowingly and cheerfully up to his neck in innate apparatus\" (Quine 1976 p. 57). He further argued that indeterminacy of translation was additional to underdetermination and revealed difficulties with linguists and philosophers' uncritical usage of meanings, ideas and propositions. And finally, he noted that Chomsky misunderstood Quine's discussion of the probability of a sentence being spoken. Quine wasn't speaking about the absolute probability of a sentence being spoken, rather he was concerned with the probability of a sentence being spoken in response to queries in an experimental setting (Ibid).\nThis led to a series of back-and-forth exchanges between Chomsky and Quine. In his (1970) 'Methodological Reflections on Current Linguistic Theory', Quine criticized"}, {"title": "Artificial Intelligence and Empiricism.", "content": "In recent years with Al getting more and more sophisticated; philosophers, psychologists, and linguists have begun to explore what these Al systems tell us about the rationalist-empiricist debate. With some theorists arguing that empiricist architecture is responsible for the success of recent Al systems (Buckner 2018, Long, 2024). While others have argued that because the Al's architecture require substantial in-built biases, they in fact support rationalism (Childers et al 2023).\nBuckner (2018) argued that Deep Convolutional Neural Networks are useful models of mammalian cognition. And he further argued that these DCNNs use of \"transformational abstraction\u201d, vindicated Hume's empiricist conception of how humans acquire abstract ideas. Childers et al (2023) have hit back at this view and have argued both that LLMs and DCNNs require built in biases for them to be successful. And they further argue that the need for built in biases in Al is analogous to the way Quine"}, {"title": "Large Language Models and Human Linguistic Competence.", "content": "Theorists have argued that the similarities between LLMs output and human linguistic output make LLMs and the way they learn directly relevant to theoretical linguistics. Thus, Piantadosi (2023), has argued that LLMs refute central claims made by Chomsky et al in the generative grammar tradition about language acquisition. This comparisons of LLMs to actual human cognition has been challenged in the literature (Chomsky et al 2023, Kodner et al 2024, Katzir, R 2023). In this section I will consider various disanalogies between LLMs and Human linguistic cognition which makes any comparison between them problematic. And in the final section I will consider the relevance of these disanalogies towards considering work in Al as being pertinent to debates about Rationalism versus Empiricism."}, {"title": "Poverty of Stimulus Arguments, Artificial Intelligence, & Human Linguistic Capacities.", "content": "A clear disanalogy to human linguistic abilities and LLMs is that humans acquire their language despite a poverty of stimulus, while LLMs learn because of a richness of stimulus"}, {"title": "Competence & Performance in LLMs & Humans.", "content": "The divergence on linguistic data which LLMs and humans are trained on is a clear indicator that they work of different underlying competencies. Other differences emerge in terms of the materials they use. In terms of computation considerations chips are faster than neurons (Long 2024). To the degree that outputs are similar that doesn't demonstrate that they are implementing the same underlying competencies (Firestone 2020, Kodner et al 2024, Milliere & Buckner 2024b). Kodner et al give the example of two watches both of which keep time accurately but one of which is digital, and the other is mechanical. Despite similar performances they achieve it through different underlying competencies (Kodner et al 2024)."}, {"title": "But Are Human and LLM Outputs Analogous?", "content": "The question of whether Humans and LLM's output are analogous is obviously vital if we want to understand whether they operate using the same underlying competencies. We have already seen that the two systems seem to learn differently one despite the poverty of stimulus and one because of the richness of stimulus. This points towards different underlying innate competencies. Different underlying competencies aside the next section will demonstrate that the performances of each system are very different.\nAt a superficial level it LLMs and Human outputs appear very similar. Chat GPT can to some degree fool a competent reader into thinking that a human produced the outputted sentences. Clark et al (2021) studied human created stories, news articles, and texts and got LLMs (GPT 2 and GPT 3) to create similarly sized stories. 130 participants were tested, and 50% of the participants couldn't tell the human written text apart from the LLM models outputs. (Scwitzgebel et al 2023). Scwitzgebel, et al (2023) Created a Large Language Model which was able to simulate Daniel Dennett's writing style and though experts were able to distinguish amongst them at rates above chance, it was surprising how close run the thing was given the fact was that it was scholars who were experts on Dennett who were being probed (ibid).\nWhile a LLM can construct sentences which appear to be analogous to ordinary human sentences there are obviously a lot of disanalogies. While LLMs can reliably produce syntactically sound sentences and sentences which are semantically interpretable. The words the LLM use have no meaning to the LLMs only to the humans that interpret their output. The reason that they have no meaning is because they are not grounded in sensory experience for the LLM. Whereas for humans they obviously are (Harnard 2024). The LLM unlike the human isn't talking about any state of affairs in the world, rather it is merely grouping together tokens according to how the tokens are fed into it in its training data."}, {"title": "Some Push Back", "content": "The idea that humans and LLMs differ because humans are referentially grounded and LLMs are not has been challenged by several theorists (Sogaard 2022, Molo & Milliere 2023, Mandelkern & Linzen 2024). One of the key points they make is that LLMs have outputs which are isomorphic with aspects of the mind independent world. Thus Sogaard 2022 makes the following claim:\n\u201cWords that are used together, tend to refer to things that, in our experience, occur together. When you tell someone about your recent hiking trip, you are likely to use words like mountain, trail, or camping. Such words, as a consequence, end up close in the vector space of a language model, while being also intimately connected in our mental representations of the world. If we accept the idea that our mental organization maps (is approximately isomorphic to) the structure of the world, the world-model isomorphism follows straight-forwardly (by closure of isomorphisms) from the distributional hypothesis.\u201d (Sogaard 2022 p.443).\nWhile it is acknowledged by all that mere isomorphism alone obviously isn't sufficient to provide referential grounding. Some theorists argue that there are other factors which can be appealed to along with isomorphism which show that LLMs are referentially grounded.\nMolo & Milliere (2023) have claimed that there is a degree of referential grounding the output of LLMS can attain. They aren't arguing that the LLMs themselves ground the symbols, rather they claim that the outputs themselves are grounded. They then note that there are two key criteria necessary to achieve referential grounding (1) causal informational relations between representations and the world, (2) Historical relations which imbue the systems with a type of normativity. And they maintain that with LLMs 1 is provided by human linguistic structures partially mirroring reality, and 2 is provided by reinforcement learning, specifically, that part of the training which involve training to represent reality accurately.\nWhen discussing causal informational relations between LLMs outputs and the world they make the following claim:\n\u201c...we should expect LLMs that learn to encode word meanings based on co-occurrence statistics to acquire linguistic representations whose structure in the high-dimensional vector space can be mapped to the structure of the world, at least in limited domains.\u201d (Molo & Milliere 2023 \u0440. 18).\nThey justified this claim by appealing to recent experimental work which demonstrated isomorphisms between LLM outputs and spatial features of the world in areas such as absolute spatial representations of cities, which cities shared boarders, the geometry of colour outputs etc (ibid p. 18). These results do indicate a causal informational relational relation between LLMs and the world which is to be expected given that the LLMs are trained on human linguistic outputs.\nMolo & Milliere then argue that the LLMs meet the normative criteria because LLMs are trained using reinforcement training. This reinforcement training has two cores an ethical core and an epistemic core. The training consists in rewarding outputs that meets ethical standards using a rating system, and punishing outputs that don't meet the"}, {"title": "Replying to the Push-Back", "content": "Originally, we have noted that much of human language is not referential hence arguments that LLMs are not referentially grounded aren't as impactful as sometimes believed. But it was noted via a discussion of Quine's notion of stimulus meaning that nonetheless humans differ from LLM because humans learn their first words through intersubjectively keying them to stimulus meanings, and LLMs are not capable of doing this. But in the last section we considered the rejoinder that LLMs are referentially grounded because they have outputs which are causally and informationally related to historical baptismal events, and that their outputs are shaped by normative considerations emphasizing accuracy which is implemented through reinforcement learning.\nBut Quine's notion of stimulus meaning is more fundamental than referential relations. Quine correctly notes that when we are learning the stimulus meaning we do so before we acquire the capacity to use objective reference:\n\u201cBut the mother, red, and water are for the infant all of a type: each is just a history of sporadic encounter, a scattered proportion of what goes on. His first learning of the three words is uniformly a matter of learning how much of what goes on about him counts as the mother, or as red or as water. It is not for the child to say in the first case \"Hello Mama! Again,\u201d in the second case \u201cHello! another red thing,\u201d and in the third case \u201cHello! More water. They are all on a par: Hello! more mamma, more red, more water.\u201d (Quine 1969 \u0440. 7).\nOn Quine's conception the child hasn't yet learned the syntax of quantification which requires using quantifiers, relative terms, pronouns etc. Therefore, the child is not capable of making ontological distinctions which would be necessary for the child to use objective reference prior acquiring this developmental capacity. Nonetheless the child is still associating the sounds with stimulus meaning derived from his perceptual environment. And here we can see that the human and the LLM are entirely distinct. Even if we grant the externalist his argument that the LLM is capable of referential grounding the LLM unlike the human is not engaging in sensorial grounding. Hence, the competency that the LLM is exhibiting is very different than the competency that the human exhibits."}, {"title": "Al and the Relevance of Rationalism and Empiricism.", "content": "Above we discussed some disanalogies between LLMs and human linguistic capacities. Two main differences were noted (1) Differences in the stimulus needed for the respective agents to acquire language, which indicates different underlying competencies. (2) LLMs language are not sensorily grounded, and human language is sensorily grounded; and this difference is a result of the different ways in which they acquire their language, humans begin by being responsive to the world in a triangular relation with others, while the LLM acquires their language through statistical grouping of text they are trained on.\nSo given that humans and LLM's outputs are the result exposure to different quantities, and types of data, which indicate different underlying competencies, a question arises as to the relevance of Al to understanding human linguistic capacities. Earlier we discussed the debate between Chomsky and Skinner & Quine and its relation to current debates on the nature of LLMs. But given the disanalogies we have noted between LLMs and humans it is questionable whether they have anything to tell us about the rationalist-empiricist debate at all.\nThe debate between the rationalists and empiricists was never centred on whether innate apparatus was necessary for a creature to learn a particular competency. All sides of the debate agreed that some innate apparatus was necessary to explain competencies, and the degree of the innate apparatus which need to be postulated is to be determined empirically. The empiricist position was one which argued that humans learned primarily through data driven learning (supported by innate architecture), while the rationalist argued that humans learned through innate domain specific competencies being triggered by environmental input.\nHowever, given the differences between LLMs and Human's linguistic competencies their relevance to each other on the rationalist-empiricist debate is in doubt. Even if we can conclusively demonstrate that a LLM learns in an empiricist manner, this will not tell us anything about whether humans learn in an empiricist manner. Because human competencies are so different than a LLMs it is simply irrelevant to the rationalist-empiricist debate whether LLMs learn in an empiricist manner or not.\nIt is theoretically possible that a philosopher could argue a la Kant that any form of cognition will a priori need to implement innate domain specific machinery to arrive at its steady state. And one could offer empirical data to support this a priori claim by showing that very different forms of cognition e.g. human and LLMs both learn by implementing innate domain specific architecture. But this isn't how the debate has been played out in the literature. Typically, the literature argues that LLMs are largely empiricist, and this fact vindicates empiricism in general, or it is argued that they are largely rationalist, and this fact vindicated rationalism. I have argued here that given the very different nature of LLMs and humans it is irrelevant to the question of how humans acquire their knowledge whether LLMs are rationalist or empiricist.\nBut this isn't to say that it is unimportant whether LLMs or other forms of Al learn in a rationalist or an empiricist manner. There are still practical issues in engineering as to whether one is more likely to be successful in building things like Artificial General Intelligence using empiricist architecture or not. Thus, people like Marcus (2020) argue that while we may be able to build AGI using empiricist principles, we will not be able to build AGI unless we build in substantial innate domain specific knowledge into the system. Marcus even argues that the best way to understand what innate architecture is necessary to be built into our Al models we"}, {"title": "Conclusion", "content": "This paper evaluated the rationalist empiricist debate between Chomsky, Quine and Skinner and its relation to current debates about the architecture used by LLMs. It showed that LLMs and Humans learn differently indicating different underlying competencies. It therefore argued that whether LLMs learn in a rationalist, or an empiricist manner is irrelevant to whether humans are rationalist or empiricist learners. The paper concluded by arguing that since humans are the paradigm example of general intelligence learners. If engineers want to build AGI, they should do so by modelling it on human cognition. But doing this involves sticking as close as possible to the science of human cognition and behaviour and moving away from arcane debates such as rationalism vs empiricism."}]}