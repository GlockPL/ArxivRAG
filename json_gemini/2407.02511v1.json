{"title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning", "authors": ["Silin Meng", "Yiwei Wang", "Cheng-Fu Yang", "Nanyun Peng", "Kai-Wei Chang"], "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs. This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding.", "sections": [{"title": "Introduction", "content": "Path planning is the computational process of determining a path from an initial point to a destination point that adheres to specific criteria, such as avoiding obstacles, minimizing travel distance or time, and satisfying other constraints (LaValle, 2006; Hart et al., 1968b; Karaman and Frazzoli, 2011). This problem is crucial across several fields, such as robotics, autonomous vehicle navigation, industrial automation, and virtual environment navigation due to its direct impact on the efficiency, safety, and feasibility of operational systems (Thrun et al., 2005; Karaman and Frazzoli, 2011; Fiorini and Shiller, 1998; Fox et al., 1997).\nExisting path planning algorithms are capable of effectively completing planning tasks and ensuring the validity of their paths. However, as the environment and map scale up, algorithms like A* and its variants (Hart et al., 1968b; Korf"}, {"title": "Related Work", "content": "Traditional Algorithms in Path Planning.\nPathfinding has been pivotal in artificial intelligence, robotics, and computer graphics, with numerous algorithms developed to address various challenges. Among the foundational methods, the A* algorithm, introduced by Hart, Nilsson, and Raphael in 1968, stands out for its use of a heuristic to estimate the cost from the current to the goal node, balancing greedy best-first search with uniform-cost search for efficient pathfinding (Hart et al., 1968a). Similarly, Pearl's Best First Search (BFS), proposed in 1984, prioritizes nodes based on heuristic values but can lead to longer paths due to its focus on the most promising nodes (Pearl, 1984).\nExtensions of A* have aimed to enhance its efficiency and adaptability. Korf's Iterative Deepening A* (IDA*), from 1985, combines depth-first search with A*'s heuristic to create a memory-efficient approach (Korf, 1985). Korf also introduced Learning Real-time A* (LRTA*) in 1990, incorporating real-time learning to dynamically update heuristic values, improving performance in changing environments (Korf, 1990). Russell's Simplified Memory Bounded A* (SMA*), from 1992, addresses memory constraints by selectively forgetting less promising paths, making it suitable for resource-limited applications (Russell, 1992).\nFurther enhancements include Stentz's Dynamic A* (D*) from 1994, which recalculates paths as the environment changes, proving effective for navigation in unknown or evolving terrains (Stentz, 1994). Koenig et al.'s Lifelong Planning A* (LPA*), introduced in 2004, incrementally updates paths in dynamic and large-scale environments (Koenig et al., 2004). Harabor and Grastien's Jump Point Search (JPS), proposed in 2011, optimizes A* for only grid-based maps by identifying key \"jump points\", reducing the number of expanded nodes (Harabor and Grastien,"}, {"title": "Methodology", "content": "3.1 A* Algorithm\nThe A* algorithm is a widely used pathfinding and graph traversal algorithm. It seeks to find the shortest path from a start node $s_0$ to a goal node $s_g$ by combining the strengths of Dijkstra's Algorithm and Greedy Best-First Search.\nA* employs a heuristic function $h(s)$ to estimate the cost from a node $s$ to the goal, and a cost function $g(s)$ to track the exact cost from the start to $s$. The total cost function $f(s)$, defined as $f(s) = g(s) +h(s)$, guides the search towards the goal. The algorithm operates as follows:\nInitialization: Place the start node $s_0$ in the OPEN list with $f(s_0) = g(s_0) + h(s_0)$, and initialize the CLOSED list as empty.\nSearch: Continuously select the node $s$ from the OPEN list with the lowest f-cost, expand its neighbors, and update their costs. If a neighbor $s_n$ offers a cheaper path than previously recorded, update its cost and parent node. Repeat until the goal node $s_g$ is reached or the OPEN list is empty.\nPath Reconstruction: Once $s_g$ is reached, reconstruct the path by tracing back from $s_g$ to $s_0$ via parent nodes.\nThe heuristic $h(s)$ should be admissible, meaning it does not overestimate the true cost to reach the goal. This ensures the path optimality of A*.\n3.2 LLM-A* Algorithm\nLLM-A* integrates the global insights provided by Large Language Models (LLMs) with the A* algorithm's optimal local search mechanism, where achieves a balance between the efficiency of the pathfinding process and optimality. The pseudocode for LLM-A* is shown in Figure 2, and it closely resembles the original A* algorithm.\nLLM-A* accepts the same inputs as A*, with the addition of an obstacle state variable, denoted"}, {"title": "Prompt Techniques", "content": "Few shot Learning. In the methodology we termed \"Few Shot Learning\" or \"Vanilla Prompting,\" our initial approach involves directly presenting the Large Language Model (LLM) with ground-truth sequences of actions as prompts. This method is informed by previous studies which have demonstrated that the performance of such models can vary significantly based on the volume of task-specific examples provided (Cao et al., 2019; Razeghi et al., 2022). To investigate this further, we employed a few-shot learning technique, wherein we provides five demonstrations (See Table 2 in Appendix) presented to the LLM. This approach aimed to determine the optimal number of examples that would enhance the model's accuracy and learning efficiency.\nChain of Thought. The Chain-of-Thought (CoT) methodology, as proposed by (Wei et al., 2022), introduces a technique that encourages a Large Language Model (LLM) to engage in a sequential, step-by-step reasoning process. This approach has demonstrated substantial efficacy in tasks necessitating multiple layers of reasoning and decision-making. In light of its proven effectiveness, we have adapted the CoT strategy (See Table 3 in Appendix) to the specific requirements of path planning.\nRecursive Path Evaluation. The Recursive Path Evaluation (RePE) methodology (See Table 4 in Appendix) is designed to guide Large Language Models (LLMs) in generating paths incrementally, with a particular emphasis on evaluating each step in the process. This approach gains its effectiveness from deconstructing the path planning problem into three distinct sub-problems: environment analysis, path generation, and path evaluation. By"}, {"title": "Experiments", "content": "4.1 Dataset\nOur dataset consists of 100 manually selected 50 \u00d7 30 maps from a randomly generated collection, each with 10 different start and goal positions. Therefore, there are 1000 samples in total (see Figure 1 for sample visualization). Our data conform to the standard of search-based algorithm environments in a continuous space. Each map includes the following parameters:\nx_range: The minimum and maximum x-coordinates of the environment boundary range as [x_min, x_max].\ny_range: The minimum and maximum y-coordinates of the environment boundary range as [y_min, y_max].\nhorizontal_barriers: List of horizontal barriers, each represented as [y, x_start, x_end].\nvertical_barriers: List of vertical barriers, each represented as [x, y-start, y_end].\nstart_goal: List of 10 unique start and goal positions for each map.\nThese parameters define the structure and constraints of each map, ensuring consistency and relevance to the standard experimental environment conditions for search-based algorithms. Meanwhile, the map environment is able to scale properly for scalability experiment."}, {"title": "Experimental Setup", "content": "Large Language Model. We employ GPT-3.5-TURBO and LLAMA3-8B-16bit for their balance of robustness and cost-effectiveness in validating the LLM-A* algorithm. Prompts include simple instructions, standard 5-shot examples, chain of thought with 3-shot, and recursive path evaluation with 3-shot for in-context learning (see Section 3.3).\nExperiment Environment. Our system allows search-based pathfinding in a continuous environment with modules for environment management, agent control, and visualization (see Section 4.1).\nEnvironment Management: Configures the environment and provides feedback.\nAgent Control: Customizes the agent's operations using the algorithm and model.\nVisualization: Offers real-time and final visual outputs for analysis.\nExperiment Subject. Our experiments focus on two critical aspects: efficiency and scalability. For efficiency, we assess the number of operations and the storage required for the pathfinding process, defined as time and space complexity, respectively. Additionally, we evaluate the generated path length to assess path efficiency. These metrics are used to compute a composite efficiency score, as presented in Table 1. Larger environments and maps are employed to better illustrate algorithm efficiency, as they offer a more comprehensive reflection of the algorithm's performance"}, {"title": "Evaluation Metrics", "content": "We assess LLM-A* against A* using metrics for operation efficiency, storage efficiency, and path quality. Performance is summarized by the geometric mean of performance ratios between LLM-A* and A* for operation, storage, path length, offering a balanced view less affected by outliers.\nOperation and Storage Ratios. We compute the geometric mean of the ratios of operations and storage used by LLM-A* relative to A* $(\\frac{LLM-A*}{A*})$. A lower score indicates better efficiency, e.g., a 50% score means LLM-A* uses 50% of the resources compared to A*.\nRelative Path Length. Path quality is evaluated by comparing the path lengths from LLM-A*, A* and LLM-only approach to the optimal path. The geometric mean of these ratios indicates how close LLM-A* paths are to optimal.\nValid Path Ratio. This metric measures the pro- portion of successful pathfinding attempts, often indicating that the generated path is collision-free"}, {"title": "Quantitative Analysis", "content": "Table 1 presents a comparative analysis of three pathfinding methodologies: the classical A* algorithm, an LLM-only approach, and our proposed LLM-A* approach. The A* algorithm serves as the baseline, with an index value of 100 indicating performance equivalent to A*, as outlined in Section 4.3. The methodologies are evaluated on maps 50 \u00d7 30 of original map sizes.\nThe results demonstrate that LLM-A* significantly enhances both operation and storage efficiencies compared to A*. Specifically, when utilizing the LLM-A* model, GPT-3.5 achieves a 57.39% score in operations and a 74.96% score in storage, with a modest 2.44% increase in relative path length. Superior, with the LLAMA3 model, LLM-A* reduces operations by 44.59% and storage by 64.02%, accompanied by a slight 2.47% increase in relative path length. These results highlight that LLM-A* not only reduces resource consumption but also maintains path validity, consistently achieving a valid path ratio of 100% across all scenarios. The observed increase in path length remains relatively low compared to the optimal path.\nMeanwhile, the LLM-only approach underperforms compared to LLM-A* and A* algorithms in terms of both path efficiency and validity. When used in isolation, LLMs may struggle with comprehensive path planning due to their lack of heuristic guidance, which is provided by LLM-A*, or the deterministic guarantees inherent in A*. The integration of LLM insights in LLM-A* significantly enhances its operational and storage efficiencies, surpassing the performance of A*.\nAblation Analysis. Notably, the Recursive Path Evaluation (RePE) prompting method achieves the smallest increases in relative path length in LLM-A*, with increments of 2.41% for the GPT-3.5 models, respectively. This suggests that RePE's step-by-step progression and intrinsic reasoning"}, {"title": "Qualitative Analysis", "content": "From the visualization in Figure 1, LLM-A* identifies the optimal path with only 140 operations, less than one-fifth the 859 operations required by A*, as well as the storage reduction. Both algorithms utilize a priority queue that stores the f-cost of each reached state, with the state having the lowest f-cost selected for exploration. The fundamental distinction between the two algorithms lies in their calculation of the f-cost or heuristic values.\nAs illustrated in Figure 4, LLM-A* leverages heuristic values derived from LLM-generated waypoints in addition to standard heuristic from A*, resulting in a dynamic heuristic that changes as the algorithm progresses. This dynamic adjustment is achieved through switching to the next target state during search when the current target state is reached. Each time the target state changes, the heuristic values for all previously reached states are recalculated. This allows LLM-A* to steer the search direction towards areas deemed more favorable by the large model at various stages of the search.\nIn contrast, A* employs a static heuristic for each state, which remains unchanged throughout the search. This static approach can lead to extensive exploration of non-optimal paths, including dead-end areas in the environment."}, {"title": "Conclusion", "content": "In this work, we propose a novel path planning algorithm, LLM-A*, which outperforms traditional algorithms like A* in terms of both computational and memory efficiency, as well as LLM-only approach in path robustness and optimality. LLM-A* integrates heuristic values derived from LLM-generated waypoints (serves as global insight), with the deterministic guarantees in the A* algorithm. This hybrid approach addresses the shortcomings of both LLM-only approach and the A* algorithm by combining their respective strengths. Furthermore, the methodology of LLM-A* retains the general applicability of A*, making it suitable for pathfinding tasks in a wide range of environments. Thus, LLM-A* serves as an effective alternative to A* algorithm for path planning, especially in large-scale scenarios."}, {"title": "Limitations", "content": "Although around 90% of the paths generated by LLM-A* are optimal, our algorithm does not guarantee optimal path. While these cases are relatively few, they indicate that the algorithm may sometimes yield paths that are not the shortest or most efficient. Future improvements could focus on enhancing the optimality of the generated paths to ensure more consistent performance. Our experiments mainly utilized GPT-3.5-TURBO and LLAMA3-8B-16bit with basic prompt techniques. Although these models and prompts were adequate to validate the robustness of the LLM-A* algorithm, we did not explore a wider array of models or advanced prompt engineering strategies. Further testing with additional models and varied prompting methods could provide more comprehensive insights into the algorithm's performance across different scenarios."}, {"title": "Admissible Heuristic and Optimality", "content": "In path planning algorithms such as A*, a heuristic function $h(n)$ is deemed admissible if it never overestimates the cost to reach the goal from any given node $n$. This ensures that the estimated cost from $n$ to the goal does not exceed the actual lowest possible cost, thereby providing a lower bound on the true cost. An admissible heuristic guarantees that the A* algorithm will find an optimal solution, as it always explores the least costly path first.\nThe standard A* heuristic is often the Euclidean distance or straight-line distance between the current node and the goal, which is both admissible and consistent. This heuristic function accurately reflects the minimum possible cost in scenarios where there are no obstacles or other constraints that might alter the cost path.\nHowever, the LLM-A* algorithm integrates an additional heuristic component, influenced by insights from large language models (LLMs), into the traditional A* heuristic function. Specifically, LLM-A* incorporates a modified heuristic $h_{LLMA^*}(n)$ which includes an additional cost term that estimates the difficulty of transitioning from the current state to the target state, based on the learned patterns from the LLM. This adjustment effectively amplifies the traditional heuristic by adding a factor derived from the LLM's assessment of the state-space complexity and the likely transitions required.\nLet $h_{A^*}(n)$ represent the conventional heuristic, and $c_{LLM}(n)$ represent the cost component derived from the LLM insights. The modified heuristic can be expressed as:\n$h_{LLMA^*}(n) = h_{A^*}(n) + c_{LLM}(n)$\nThe term $c_{LLM}(n)$ may include factors such as predicted transition costs, obstacle avoidance strategies, or other environmental complexities inferred by the LLM, through selected target states in target list. Consequently, the heuristic function $h_{LLMA^*}(n)$ provides a more nuanced estimate of the cost to reach the goal, potentially guiding the search more effectively by leveraging the LLM's understanding of the domain.\nWhile this enhanced heuristic expedites the search process by prioritizing paths that the LLM identifies as promising, it introduces a deviation from admissibility. By incorporating the additional cost $c_{LLM}(n)$, the heuristic may overestimate the true cost to the goal, particularly if the LLM-derived costs are overly conservative or based on non-optimal path predictions. This overestimation violates the admissibility condition because the total estimated cost $g(n) + h_{LLMA^*}(n)$ could exceed the actual optimal path cost, where $g(n)$ is the cost from the start to the current node.\nThe implications of this non-admissibility are significant: while the LLM-A* heuristic can potentially lead to faster convergence towards the goal by focusing the search in promising regions of the state space, it compromises the guarantee of finding the optimal path. The trade-off between search efficiency and optimality must be carefully considered in the application of LLM-A*. In scenarios where the heuristic insights from the LLM offer substantial benefits in reducing search time and computational resources, the potential loss of optimality may be justified. However, for applications where finding the absolute optimal path is crucial, relying solely on an admissible heuristic might be preferable."}, {"title": "Prompts in LLMs", "content": "This appendix outlines the prompting techniques used in our LLM-A* algorithm to generate paths between start and goal points while navigating around obstacles. We employed different prompting strategies to evaluate their effectiveness in guiding the model. Below are the details of each technique along with the templates used.\nB.1 Standard 5-Shot Demonstration\nIn the standard 5-shot demonstration in Table 2, the model is provided with five examples (or demonstrations) to guide the generation of the path. Each example includes start and goal points, along with horizontal and vertical barriers. The model is prompted to generate a path by following the pattern observed in the examples.\nB.2 Chain of Thought (CoT) Prompting\nThe chain of thought prompting technique in Table 3 provides a sequence of reasoning steps that the model follows to arrive at the final path. This technique includes a detailed thought process and evaluation for each step, helping the model to understand the rationale behind the path generation.\nB.3 Recursive Path Evaluation (RePE)\nIn the recursive path evaluation technique shown Table 4, the model iteratively evaluates the path"}, {"title": "Details of Dataset Construction", "content": "The dataset for A* path planning is generated using a custom Python script, leveraging several key packages for randomization, geometric manipulation, visualization, and data management. The process involves the following steps:\nInitialization: The script initializes with specified map dimensions (x and y boundaries) and parameters (number of barriers and obstacles) for the number of unique environments and start-goal pairs.\nEnvironment Creation: For each map configuration, do the following:\nRandom obstacles, horizontal barriers, and vertical barriers are generated within defined x and y ranges using the shapely.geometry.LineString for line segments.\nStart and goal points are randomly"}, {"title": "Evaluation Metric", "content": "In this study, we evaluate the performance of our algorithm using the geometric mean of ratios. This metric provides a robust measure for comparing the efficiency and effectiveness of different path planning algorithms. Below, we outline the rationale for choosing this metric, the calculation procedure, and its advantages.\nRationale\nThe geometric mean of ratios is used in this study to assess the relative performance of different path planning algorithms or approaches. It provides a balanced evaluation by aggregating multiple performance ratios, ensuring that no single extreme value disproportionately affects the overall metric. This is particularly useful in scenarios where the distribution of ratios can be skewed, and a simple arithmetic mean might be misleading.\nCalculation Procedure\nLet $R_i$ represent the ratio of performance measures (such as path length, computation time, or any other relevant metric) between the proposed algorithm and a baseline or reference algorithm for the i-th test case. The geometric mean $G$ of N ratios is calculated as follows:\n$G = (\\prod_{i=1}^{N} R_i)^{\\frac{1}{N}}$\nThe geometric mean $G$ provides a multiplicative average, effectively normalizing the ratios and providing a single representative value that reflects the overall performance across all test cases.\nAdvantages\nUsing the geometric mean of ratios offers several benefits in the context of evaluating path planning algorithms:\nSensitivity to Relative Changes: The geometric mean is sensitive to the relative differences between performance measures, making it suitable for comparing ratios.\nMitigation of Outliers: Unlike the arithmetic mean, the geometric mean minimizes the impact of extreme values or outliers, providing a more stable and representative metric.\nInterpretability: The geometric mean allows for easy interpretation of performance improvements or deteriorations. A geometric mean greater than 1 indicates that, on average, the proposed algorithm performs better than the baseline, while a value less than 1 suggests poorer performance.\nScalability: The geometric mean naturally scales with multiplicative factors, making it appropriate for comparing algorithms across different scales or units of measurement."}]}