{"title": "Query languages for neural networks", "authors": ["Martin Grohe", "Christoph Standke", "Juno Steegmans", "Jan Van den Bussche"], "abstract": "We lay the foundations for a database-inspired approach to interpreting and understanding neural network models by querying them using declarative languages. Towards this end we study different query languages, based on first-order logic, that mainly differ in their access to the neural network model. First-order logic over the reals naturally yields a language which views the network as a black box; only the input-output function defined by the network can be queried. This is essentially the approach of constraint query languages. On the other hand, a white-box language can be obtained by viewing the network as a weighted graph, and extending first-order logic with summation over weight terms. The latter approach is essentially an abstraction of SQL. In general, the two approaches are incomparable in expressive power, as we will show. Under natural circumstances, however, the white-box approach can subsume the black-box approach; this is our main result. We prove the result concretely for linear constraint queries over real functions definable by feedforward neural networks with a fixed number of hidden layers and piecewise linear activation functions.", "sections": [{"title": "1 Introduction", "content": "Neural networks [11] are a popular and successful representation model for real functions learned from data. Once deployed, the neural network is \"queried\" by supplying it with inputs then obtaining the outputs. In the field of databases, however, we have a much richer conception of querying than simply applying a function to given arguments. For example, in querying a database relation Employee (name, salary), we can not only ask for Anne's salary; we can also ask how many salaries are below that of Anne's; we can ask whether no two employees have the same salary; and so on.\nIn this paper, we consider the querying of neural networks from this more general per- spective. We see many potential applications: obvious ones are in explanation, verification, and interpretability of neural networks and other machine-learning models [29, 2, 24]. These are huge areas [31, 7] where it is important [28, 21] to have formal, logical definitions for the myriad notions of explanation that are being considered. Another potential application is in managing machine-learning projects, where we are testing many different architectures and training datasets, leading to a large number of models, most of which become short-term legacy code. In such a context it would be useful if the data scientist could search the repository for earlier generated models having certain characteristics in their architecture or in their behavior, which were perhaps not duly documented."}, {"title": "2 Query languages for neural networks", "content": "The idea of querying machine learning models with an expressive, declarative query language comes naturally to database researchers, and indeed, Arenas et al. already proposed a language for querying boolean functions over an unbounded set of boolean features [3]. In the modal logic community, similar languages are being investigated [25, references therein]. In the present work, we focus on real, rather than boolean, functions and models, as is indeed natural in the setting of verifying neural networks [2].\nThe constraint query language approach A natural language for querying real functions on a fixed number of arguments (features) is obtained by simply using first-order logic over the reals, with a function symbol F representing the function to be queried. We denote this by FO(R). For example, consider functions F with three arguments. The formula $|F(a, b, c) \u2013 F(a,b', c)| < \u0454 expresses that the output on (a, b, c) does not depend strongly on the second feature, i.e., F(a, b', c) is e-close to F(a, b, c) for any b'. Here, a, b, c and e can be real constants or parameters (free variables).\nThe language FO(R) (also known as FO + POLY) and its restriction FO(Rlin) to linear arithmetic (aka FO + LIN) were intensively investigated in database theory around the turn of the century, under the heading of constraint query languages, with applications to spatial and temporal databases. See the compendium volume [20] and book chapters [23, chapter 13], [13, chapter 5]. Linear formulas with only universal quantifiers over the reals, in front of a quantifier-free condition involving only linear arithmetic (as the above example formula), can already model many properties considered in the verification of neural networks [2]. This universal fragment of FO(Rlin) can be evaluated using linear programming techniques [2].\nFull FO(R) allows alternation of quantifiers over the reals, and multiplication in arith- metic. Because the first-order theory of the reals is decidable [5], FO(R) queries can still be effectively evaluated on any function that is semi-algebraic, i.e., itself definable in first-order logic over the reals. Although the complexity of this theory is high, if the function is pre- sented as a quantifier-free formula, FO(R) query evaluation actually has polynomial-time data complexity; here, the \"data\" consists of the given quantifier-free formula [17].\nFunctions that can be represented by feedforward neural networks with ReLU hidden units and linear output units are clearly semi-algebraic; in fact, they are piecewise linear. For most of our results, we will indeed focus on this class of networks, which are widespread in practice [11], and denote them by ReLU-FNN."}, {"title": "3 A black-box query language", "content": "First-order logic over the reals, denoted here by FO(R), is, syntactically, just first-order logic over the vocabulary of elementary arithmetic, i.e., with binary function symbols + and * for addition and multiplication, binary predicate <, and constant symbols 0 and 1 [5]. Constants for rational numbers, or even algebraic numbers, can be added as an abbreviation (since they are definable in the logic).\nThe fragment FO(Rlin) of linear formulas uses multiplication only for scalar multipli- cation, i.e., multiplication of variables with rational number constants. For example, the formula $y = 3x_1 - 4x_2 + 7$ is linear, but the formula $y = 5x_1 \\cdot x_2 - 3$ is not. In practice, linear queries are often sufficiently expressive, both from earlier applications for temporal or spatial data [20], as well as for querying neural networks (see examples to follow). The only caveat is that many applications assume a distance function on vectors. When using distances based on absolute value differences between real numbers, e.g., the Manhattan distance or the max norm, we still fall within FO(Rlin).\nWe will add to FO(R) extra relation or function symbols; in this paper, we will mainly consider FO(R, F), which is FO(R) with an extra function symbol F. The structure on the domain R of reals, with the arithmetic symbols having their obvious interpretation, will be denoted here by R. Semantically, for any vocabulary of extra relation and function symbols, FO(R, \u03c4) formulas are interpreted over structures that expand R with additional relations and functions on R of the right arities, that interpret the symbols in T. In this way, FO(R, F) expresses queries about functions $F: R^m \u2192 R$.\nThis language can express a wide variety of properties (queries) considered in inter- pretable machine learning and neural-network verification. Let us see some examples."}, {"title": "4 Weighted structures and FO(SUM)", "content": "Weighted structures are standard abstract structures equipped with one or more weight functions from tuples of domain elements to values from some separate, numerical domain. Here, as numerical domain, we will use $R_\\perp = R \\cup {\\perp}$, the set of \"lifted reals\" where $\\perp$ is an extra element representing an undefined value. Neural networks are weighted graph"}, {"title": "5 White-box querying", "content": "For any natural numbers m and n, we introduce a vocabulary for neural networks with m inputs and n outputs. We denote this vocabulary by $\\Upsilon_{net(m, n)}$, or just $\\Upsilon_{net}$ if m and n"}, {"title": "6 Model-agnostic queries", "content": "We have already indicated that FO(R, F) is \"black box\" while FO(SUM) is \"white box\". Black-box queries are commonly called model agnostic [29]. Some FO(SUM) queries may,"}, {"title": "7 From FO(Rlin) to FO(SUM)", "content": "In practice, the number of layers in the employed neural network architecture is often fixed and known. Our main result then is that FO(SUM) can express all $FO(R_{lin})$ queries."}, {"title": "7.1 FO(SUM) translations", "content": "Let Y and $\\Gamma$ be vocabularies for weighted structures, and let n be a natural number. An n-ary FO(SUM) translation $\\varphi$ from Y to $\\Gamma$ consists of a number of formulas and weight terms over Y, described next. There are formulas $dom(\\vec{x})$ and $\\varphi_=(\\vec{x_1}, \\vec{x_2})$; formulas $\\varphi_R(\\vec{x_1},...,\\vec{x_k})$ for every k-ary relation symbol R of $\\Gamma$; and formulas $\\varphi_f(\\vec{x_0}, \\vec{x_1},...,\\vec{x_k})$ for every k-ary standard function symbol f of $\\Gamma$. Furthermore, there are weight terms $\\varphi_w(\\vec{x_1},...,\\vec{x_k})$ for every k-ary weight function k of $\\Gamma$.\nIn the above description, bold $\\vec{x}$ denote n-tuples of distinct variables. Thus, the formulas and weight terms of $\\varphi$ define relations or weight functions of arities that are a multiple of n.\nWe say that $\\varphi$ maps a weighted structure A over Y to a weighted structure B over $\\Gamma$ if there exists a surjective function h from $dom(A) \\subseteq A^n$ to B such that:\n*   $h(\\vec{a_1}) = h(\\vec{a_2}) \\Leftrightarrow A \\models \\varphi_=(\\vec{a_1}, \\vec{a_2})$;\n*   $(h(\\vec{a_1}),...,h(\\vec{a_k})) \\in R^B \\Leftrightarrow A \\models \\varphi_R(\\vec{a_1},..., \\vec{a_k})$;\n*   $h(\\vec{a_0}) = f^B(h(\\vec{a_1}), ..., h(\\vec{a_k})) \\Leftrightarrow A \\models \\varphi_f (\\vec{a_0}, \\vec{a_1},..., \\vec{a_k})$;\n*   $w^B(h(\\vec{a_1}),..., h(\\vec{a_n})) = (\\varphi_w)^A(\\vec{a_1}, ..., \\vec{a_n})$.\nIn the above, the bold $\\vec{a}$ denote n-tuples in dom(A).\nFor any given A, if $\\varphi$ maps A to B, then B is unique up to isomorphism. Indeed, the elements of B can be understood as representing the equivalence classes of the equivalence relation $\\varphi_=(A)$ on dom(A). In particular, for B to exist, $\\varphi$ must be admissible on A, which means that $\\varphi_=(A)$ is indeed an equivalence relation on dom(A), and all relations and all functions $\\varphi_R(A)$, $\\varphi_f(A)$ and $\\varphi_\\omega(A)$ are invariant under this equivalence relation."}, {"title": "7.2 Proof of Theorem 7.1", "content": "We sketch the proof of Theorem 7.1. For clarity of exposition, we present it first for single inputs, i.e., the case m = 1. We present three Lemmas which can be chained together to obtain the theorem.\nPiecewise linear functions We can naturally model piecewise linear (PWL) functions from R to R as weighted structures, where the elements are simply the pieces. Each piece p is defined by a line y = ax + b and left and right endpoints. Accordingly, we use a vocabulary $\\Upsilon_{pwl}$ with four unary weight functions indicating a, b, and the x-coordinates of the endpoints. (The left- and rightmost pieces have no endpoint; we set their x-coordinate to $\\perp$).\nFor m = 1 and l = 2, the proof of the following Lemma is based on the same ideas as in the proof sketch we gave for Lemma 5.3. For m > 1, PWL functions from $R^m$ to R are more complex; the vocabulary $\\Upsilon_{pwl}^m$ and a general proof of the lemma will be described in Section 7.3."}, {"title": "7.3 Extension to multiple inputs", "content": "For m > 1, the notion of PWL function $f: R^m \u2192 R$ is more complex. We can conceptualize our representation of f as a decomposition of $R^m$ into polytopes where, additionally, every polytope p is accompanied by an affine function $f_p$ such that $f = \\cup_p f_p|_p$. We call $f_p$ the component function of f on p. Where for m = 1 each pies of piece f was delineated by just two breakpoints, now our polytope in $R^m$ may be delineated by many hyperplanes, called breakplanes. Thus, the vocabulary $Y_{pwl}^m$ includes the position of a polytope relative to the breakplanes, indicating whether the polytope is on the breakplane, or on the positive or negative side of it. We next sketch how to prove Lemma 7.3 in its generality. The proof of Lemma 7.6 poses no additional problems.\nWe will define a PWL function $f_u$ for every neuron u in the network; the final result is then $f_{out}$. To represent these functions for every neuron, we simply add one extra relation"}]}