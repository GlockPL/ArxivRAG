{"title": "Query languages for neural networks", "authors": ["Martin Grohe", "Christoph Standke", "Juno Steegmans", "Jan Van den Bussche"], "abstract": "We lay the foundations for a database-inspired approach to interpreting and understanding neural network models by querying them using declarative languages. Towards this end we study different query languages, based on first-order logic, that mainly differ in their access to the neural network model. First-order logic over the reals naturally yields a language which views the network as a black box; only the input-output function defined by the network can be queried. This is essentially the approach of constraint query languages. On the other hand, a white-box language can be obtained by viewing the network as a weighted graph, and extending first-order logic with summation over weight terms. The latter approach is essentially an abstraction of SQL. In general, the two approaches are incomparable in expressive power, as we will show. Under natural circumstances, however, the white-box approach can subsume the black-box approach; this is our main result. We prove the result concretely for linear constraint queries over real functions definable by feedforward neural networks with a fixed number of hidden layers and piecewise linear activation functions.", "sections": [{"title": "1 Introduction", "content": "Neural networks [11] are a popular and successful representation model for real functions learned from data. Once deployed, the neural network is \"queried\" by supplying it with inputs then obtaining the outputs. In the field of databases, however, we have a much richer conception of querying than simply applying a function to given arguments. For example, in querying a database relation Employee (name, salary), we can not only ask for Anne's salary; we can also ask how many salaries are below that of Anne's; we can ask whether no two employees have the same salary; and so on.\nIn this paper, we consider the querying of neural networks from this more general per- spective. We see many potential applications: obvious ones are in explanation, verification, and interpretability of neural networks and other machine-learning models [29, 2, 24]. These are huge areas [31, 7] where it is important [28, 21] to have formal, logical definitions for the myriad notions of explanation that are being considered. Another potential application is in managing machine-learning projects, where we are testing many different architectures and training datasets, leading to a large number of models, most of which become short-term legacy code. In such a context it would be useful if the data scientist could search the repository for earlier generated models having certain characteristics in their architecture or in their behavior, which were perhaps not duly documented."}, {"title": "2 Query languages for neural networks", "content": "The idea of querying machine learning models with an expressive, declarative query language comes naturally to database researchers, and indeed, Arenas et al. already proposed a language for querying boolean functions over an unbounded set of boolean features [3]. In the modal logic community, similar languages are being investigated [25, references therein]. In the present work, we focus on real, rather than boolean, functions and models, as is indeed natural in the setting of verifying neural networks [2].\nThe constraint query language approach A natural language for querying real functions on a fixed number of arguments (features) is obtained by simply using first-order logic over the reals, with a function symbol F representing the function to be queried. We denote this by FO(R). For example, consider functions F with three arguments. The formula $\\sqrt{b'} |F(a, b, c) \u2013 F(a,b', c)| < \\epsilon$ expresses that the output on (a, b, c) does not depend strongly on the second feature, i.e., F(a, b', c) is e-close to F(a, b, c) for any b'. Here, a, b, c and e can be real constants or parameters (free variables).\nThe language FO(R) (also known as FO + POLY) and its restriction FO(Rlin) to linear arithmetic (aka FO + LIN) were intensively investigated in database theory around the turn of the century, under the heading of constraint query languages, with applications to spatial and temporal databases. See the compendium volume [20] and book chapters [23, chapter 13], [13, chapter 5]. Linear formulas with only universal quantifiers over the reals, in front of a quantifier-free condition involving only linear arithmetic (as the above example formula), can already model many properties considered in the verification of neural networks [2]. This universal fragment of FO(Rlin) can be evaluated using linear programming techniques [2].\nFull FO(R) allows alternation of quantifiers over the reals, and multiplication in arith- metic. Because the first-order theory of the reals is decidable [5], FO(R) queries can still be effectively evaluated on any function that is semi-algebraic, i.e., itself definable in first-order logic over the reals. Although the complexity of this theory is high, if the function is pre- sented as a quantifier-free formula, FO(R) query evaluation actually has polynomial-time data complexity; here, the \"data\" consists of the given quantifier-free formula [17].\nFunctions that can be represented by feedforward neural networks with ReLU hidden units and linear output units are clearly semi-algebraic; in fact, they are piecewise linear. For most of our results, we will indeed focus on this class of networks, which are widespread in practice [11], and denote them by ReLU-FNN.\nThe SQL approach Another natural approach to querying neural networks is to query them directly, as graphs of neurons with weights on the nodes and edges. For this purpose one represents such graphs as relational structures with numerical values and uses SQL to query them. As an abstraction of this approach, in this paper, we model neural networks as weighted finite structures. As a query language we use FO(SUM): first-order logic over weighted structures, allowing order comparisons between weight terms, where weight terms can be built up using rational arithmetic, if-then-else, and, importantly, summation.\nOriginally introduced by Gr\u00e4del and Gurevich [12], the language FO(SUM) is compa- rable to the relational calculus with aggregates [18] and, thus, to SQL [22]. Logics close to FO(SUM), but involving arithmetic in different semirings, were recently also used for unifying different algorithmic problems in query processing [35], as well as for expressing hypotheses in the context of learning over structures [36]. The well-known FAQ framework [27], restricted to the real semiring, can be seen as the conjunctive fragment of FO(SUM).\nTo give a simple example of an FO(SUM) formula, consider ReLU-FNNs with a single input unit, one hidden layer of ReLU units, and a single linear output unit. The following"}, {"title": "M. Grohe, C. Standke, J. Steegmans, and J, Van den Bussche", "content": "formula expresses the query that asks if the function evaluation on a given input value is positive:\n$0 <b(out) + \\sum_{x:E(in,x)} w(x, out) \\cdot ReLU(w(in, x) \\cdot val + b(x)).$\nHere, E is the edge relation between neurons, and constants in and out hold the input and output unit, respectively. Thus, variable x ranges over the neurons in the hidden layer. Weight functions w and b indicate the weights of edges and the biases of units, respectively; the weight constant val stands for a given input value. We assume for clarity that ReLU is given, but it is definable in FO(SUM).\nJust like the relational calculus with aggregates, or SQL select statements, query evalua- tion for FO(SUM) has polynomial time data complexity, and techniques for query processing and optimization from database systems directly apply.\nComparing expressive powers Expressive power of query languages has been a classical topic in database theory and finite model theory [1, 23], so, with the advent of new models, it is natural to revisit questions concerning expressivity. The goal of this paper is to understand and compare the expressive power of the two query languages FO(R) and FO(SUM) on neural networks over the reals. The two languages are quite different. FO(R) sees the model as a black-box function F, but can quantify over the reals. FO(SUM) can see the model as a white box, a finite weighted structure, but can quantify only over the elements of the structure, i.e., the neurons.\nIn general, indeed the two expressive powers are incomparable. In FO(SUM), we can express queries about the network topology; for example, we may ask to return the hidden units that do not contribute much to the function evaluation on a given input value. (For- mally, leaving them out of the network would yield an output within some e of the original output.) Or, we may ask whether there are more than a million neurons in the first hidden layer. For FO(R), being a black box language, such queries are obviously out of scope.\nA more interesting question is how the two languages compare in expressing model ag- nostic queries: these are queries that return the same result on any two neural networks that represent the same input-output function. For example, when restricting attention to networks with one hidden layer, the example FO(SUM) formula seen earlier, which evaluates the network, is model agnostic. FO(R) is model agnostic by design, and, indeed, serves as a very natural declarative benchmark of expressiveness for model-agnostic queries. It turns out that FO(SUM), still restricting to networks of some fixed depth, can express model- agnostic queries that FO(R) cannot. For example, for any fixed depth d, we will show that FO(SUM) can express the integrals of a functions given by a ReLU-FNNs of depth d. In contrast, we will show that this cannot be done in FO(R) (Theorem 6.1).\nThe depth of a neural network can be taken as a crude notion of \"schema\". Standard relational query languages typically cannot be used without knowledge of the schema of the data. Similarly, we will show that without knowledge of the depth, FO(SUM) cannot express any nontrivial model-agnostic query (Theorem 6.2). Indeed, since FO(SUM) lacks recursion, function evaluation can only be expressed if we known the depth. (Extensions with recursion is one of the many interesting directions for further research.)\nWhen the depth is known, however, for model-agnostic queries, the expressiveness of FO(SUM) exceeds the benchmark of expressiveness provided by FO(Rlin). Specifically, we show that every FO (Rlin) query over functions representable by ReLU-FNNs is also ex- pressible in FO(SUM) evaluated on the networks directly (Theorem 7.1). This is our main"}, {"title": "4 Weighted structures and FO(SUM)", "content": "Weighted structures are standard abstract structures equipped with one or more weight functions from tuples of domain elements to values from some separate, numerical domain. Here, as numerical domain, we will use $R_\\perp = R\\cup{\\perp}$, the set of \"lifted reals\" where $\\perp$ is an extra element representing an undefined value. Neural networks are weighted graph"}, {"title": "M. Grohe, C. Standke, J. Steegmans, and J, Van den Bussche", "content": "structures. Hence, since we are interested in declarative query languages for neural networks, we are interested in logics over weighted structures. Such logics were introduced by Gr\u00e4del and Gurevich [12]. We consider here a concrete instantiation of their approach, which we denote by FO(SUM).\nRecall that a (finite, relational) vocabulary is a finite set of function symbols and relation symbols, where each symbol comes with an arity (a natural number). We extend the notion of vocabulary to also include a number of weight function symbols, again with associated arities. We allow 0-ary weight function symbols, which we call weight constant symbols.\nA (finite) structure A over such a vocabulary $\\Upsilon$ consists of a finite domain A, and functions and relations on A of the right arities, interpreting the standard function symbols and relation symbols from $\\Upsilon$. So far this is standard. Now additionally, A interprets every weight function symbol w, of arity k, by a function $w^A : A^k \\rightarrow R_\\perp$.\nThe syntax of FO(SUM) formulas (over some vocabulary) is defined exactly as for stan- dard first order logic, with one important extension. In addition to formulas (taking Boolean values) and standard terms (taking values in the structure), the logic contains weight terms taking values in $R_\\perp$. Weight terms t are defined by the following grammar:\n$t ::= \\perp | \\omega(s_1,..., s_n) | r(t, ..., t) | if \\varphi then t else t | \\sum_{x:\\varphi}t$\nHere, w is a weight function symbol of arity n and the $s_i$ are standard terms; r is a rational function applied to weight terms, with rational coefficients; $\\varphi$ is a formula; and x is a tuple of variables. The syntax of weight terms and formulas is mutually recursive. As just seen, the syntax of formulas is used in the syntax of weight terms; conversely, weight terms $t_1$ and $t_2$ can be combined to form formulas $t_1 = t_2$ and $t_1 < t_2$.\nRecall that a rational function is a fraction between two polynomials. Thus, the arith- metic operations that we consider are addition, scalar multiplication by a rational number, multiplication, and division.\nThe free variables of a weight term are defined as follows. The weight term $\\perp$ has no free variables. The free variables of $\\omega(s_1,..., s_n)$ are simply the variables occurring in the $s_i$. A variable occurs free in $r(t_1,..., t_n)$ if it occurs free in some $t_i$. A variable occurs free in 'if $\\varphi$ then $t_1$ else $t_2$' if it occurs free in $t_1$, $t_2$, or $\\varphi$. The free variables of $\\sum_{x:\\varphi}t$ are those of $\\varphi$ and t, except for the variables in x. A formula or (weight) term is closed if it has no free variables.\nWe can evaluate a weight term t(x1,...,xk) on a structure A and a tuple a \u2208 Ak providing values to the free variables. The result of the evaluation, denoted by $t^{A,a}$, is a value in $R_\\perp$, defined in the obvious manner. In particular, when t is of the form $\\sum_{y:\\phi}t'$, we have\n$t^{A,a} = \\sum_{b:A\\vDash\\varphi(a,b)}t'^{A,a,b}$\nDivision by zero, which can happen when evaluating terms of the form $r(t, ...,t)$, is given the value $\\perp$. The arithmetical operations are extended so that x + $\\perp$, q\u00b7$\\perp$ (scalar multiply), $x\\perp$, and x/$\\perp$ and $\\perp$/x always equal $\\perp$. Also, $\\perp <a$ holds for all $a \\in R$.\nWhite-box querying For any natural numbers m and n, we introduce a vocabulary for neural networks with m inputs and n outputs. We denote this vocabulary by $\\Upsilon_{net(m, n)}$, or just $\\Upsilon_{net}$ if m and n"}, {"title": "5 White-box querying", "content": "are understood. It has a binary relation symbol E for the edges; constant symbols in1, ..., inm and out1, ..., outn for the input and output nodes; a unary weight function b for the biases, and a binary weight function symbol w for the weights on the edges.\nAny ReLU-FNN N, being a weighted graph, is an $\\Upsilon_{net}$-structure in the obvious way. When there is no edge from node $u_1$ to $u_2$, we put $w^N (u_1, u_2) = \\perp$. Since inputs have no bias, we put $b^N (u) = \\perp$ for any input u.\nDepending on the application, we may want to enlarge $\\Upsilon_{net}$ with some additional param- eters. For example, we can use additional weight constant symbols to provide input values to be evaluated, or output values to be compared with, or interval bounds, etc.\nThe logic FO(SUM) over the vocabulary $\\Upsilon_{net}$ (possibly enlarged as just mentioned) serves as a \"white-box\" query language for neural networks, since the entire model is given and can be directly queried, just like an SQL query can be evaluated on a given relational database. Contrast this with the language FO(R, F) from Section 3, which only has access to the function F represented by the network, as a black box.\nWhile the language FO(R, F) cannot see inside the model, at least it has direct access to the function represented by the model. When we use the language FO(SUM), we must compute this function ourselves. At least when we know the depth of the network, this is indeed easy. In the Introduction, we already showed a weight term expressing the evaluation of a one-layer neural network on a single input and output. We can easily generalize this to a weight term expressing the value of any of a fixed number of outputs, with any fixed number m of inputs, and any fixed number of layers. Let val\u2081, ..., valm be additional weight constant symbols representing input values. Then the weight term $ReLU(b(u) + w(in_1, u) \\cdot val_1 + \u00b7\u00b7\u00b7 + w(in_m, u) \\cdot val_m)$ expresses the value of any neuron u in the first hidden layer (u is a variable). Denote this term by $t_1(u)$. Next, for any subsequent layer numbered l > 1, we inductively define the weight term $t_l(u)$ as\n$ReLU(b(u) + \\sum_{x:E(x,u)} w(x, u) t_{l-1}(x)).$\nHere, $ReLU(c)$ can be taken to be the weight term if c > 0 then c else 0. Finally, the value of the jth output is given by the weight term $eval_j := b(out_j) + \\sum_{x:E(x,out_j)} w(x, out_j) t_l(x)$, where l is the number of the last hidden layer.\nWe can also look for useless neurons: neurons that can be removed from the network without altering the output too much on given values. Recall the weight term evalj from the previous example; for clarity we just write eval. Let z be a fresh variable, and let eval' be the term obtained from eval by altering the summing conditions E(x, u) and E(x, out) by adding the conjunct x \u2260 z. Then the formula |eval \u2013 eval'| < \u0454 expresses that z is useless. (For $c\\perp$ we can take the weight term if c > 0 then c else -c.)\nAnother interesting example is computing integrals. Recall that F(m, l) is the class of networks with m inputs, one output, and depth l.\nLet m and l be natural numbers. There exists an FO(SUM) term t over $\\Upsilon_{net (m, 1)}$ with m additional pairs of weight constant symbols mini and maxi for i \u2208 {1,...,m}, such that for any network N in F(m,l), and values $a_i$ and $b_i$ for the mini and maxi, we have $t^{N, a_1,b_1,...,a_m,b_m} = \\int_{a_1}^{b_1} ... \\int_{a_m}^{b_m} F^N dx_1...dx_m$.\nProof (sketch). The proof for the full result is delayed to Appendix G, since it needs results and notions from Section 7. However, we can sketch here a self-contained and elementary"}, {"title": "M. Grohe, C. Standke, J. Steegmans, and J, Van den Bussche", "content": "proof for m = 1 and l = 2 (one input, one hidden layer). This case already covers all continuous piecewise linear functions R \u2192 R.\nEvery hidden neuron u may represent a \"quasi breakpoint\" in the piecewise linear func- tion (that is, a point where its slope may change). Concretely, we consider the hidden neurons with nonzero input weights to avoid dividing by zero. Its x-coordinate is given by the weight term breakx(u) := \u2212b(u)/w(in\u2081, u). The y-value at the breakpoint is then given by breaky(u) : = eval\u2081(break(u)), where eval\u2081 is the weight term from Example 5.1 and we substitute break(u) for val1.\nPairs ($u_1,u_2$) of neurons representing successive breakpoints are easy to define by a formula succ(u1,u2). Such pairs represent the pieces of the function, except for the very first and very last pieces. For this proof sketch, assume we simply want the integral between the first breakpoint and the last breakpoint.\nThe area (positive or negative) contributed to the integral by the piece ($u_1,u_2$) is easy to write as a weight term: $area(u_1, u_2) = (breaky(u_1) + breaky(u_2))(breakx(u_2) \u2013 breakx(u_1))$. We sum these to obtain the desired integral. However, since different neurons may rep- resent the same quasi breakpoint, we must divide by the number of duplicates. Hence, our desired term t equals $\\sum_{u_1, u_2:succ(u_1,u_2)} area(u_1, u_2)/(\\sum_{u'_1,u'_2:\\gamma} 1)$, where $\\gamma$ is the formula $succ(u_1,u_2) \\wedge breakx(u_1) = breakx (u'_1) \\wedge breakx(u_2) = breakx(u'_2)$.\nA popular alternative to Example 3.3 for measuring the contribution of an input feature i to an input y = ($y_1, ..., y_m$) is the SHAP score [26]. It assumes a probability distribution P on the input space and quantifies the change to the expected value of FN caused by fixing input feature i to $y_i$ in a random fixation order of the input features:\n$SHAP(i) = \\sum_{I \\subset {1,...,m}\\{i}} \\frac{|I|!(m-1-|I|)}{m!} (E(F_N(x) | x_{I\\cup {i}} = y_{I\\cup {z}})-E(F_N(x) | x_I = y_I)).$\nWhen we assume that P is the product of uniform distributions over the intervals (aj, bj), we can write the conditional expectation $E(F_N(x) | x_J = y_J)$ for some $J \\subset {1,...,m}$ by setting ${1,...m} \\setminus J = : {j_1, . . ., j_r}$ as follows.\n$E(F_N(x) | x_J = y_J) = \\prod_{k=1}^r \\frac{1}{b_{j_k} - a_{j_k}}\\int_{a_{j_1}}^{b_{j_1}}...\\int_{a_{j_r}}^{b_{j_r}} F_N (X|_{x=y_J}) dx_{j_r}... dx_{j_1}$\nwhere $X|_{x=y_J}$ is a short notation for the variable obtained from x by replacing $x_j$ with $y_j$ for all $j \\in J$. With lemma 5.3, this conditional expectation can be expressed in FO(SUM) and by replacing J with I or IU {i} respectively, we can express the SHAP score.\nOur main result will be that, over networks of a given depth, all of FO(Rlin, F) can be expressed in FO(SUM). So the examples from Section 3 (which are linear if a Manhattan or max distance is used) apply here as well. Moreover, the techniques by which we show our main result readily adapt to queries not about the final function F represented by the network, but about the function $F_z$ represented by a neuron z given as a parameter to the query, much as in Example 5.2. For example, in feature visualization [29] we want to find the input that maximizes the activation of some neuron z. Since this is expressible in FO(Rlin, F), it is also expressible in FO(SUM)."}, {"title": "6 Model-agnostic queries", "content": "We have already indicated that FO(R, F) is \"black box\" while FO(SUM) is \"white box\". Black-box queries are commonly called model agnostic [29]. Some FO(SUM) queries may,"}, {"title": "Query languages for neural networks", "content": "and others may not, be model agnostic.\nFormally, for some l \u2265 1, let us call a closed FO(SUM) formula $\\varphi$, possibly using weight constants $c_1,...,c_k$, depth-l model agnostic if for all m > 1 all neural networks N,N' \u2208 $\\bigcup_{i=1}^{\\infty}F(m, i)$ such that FN = FN', and all $a_1,...,a_k \\in R$ we have $N, a_1,...,a_k \\vDash \\varphi \\Leftrightarrow N', a_1, ..., a_k \\vDash \\varphi$. A similar definition applies to closed FO(SUM) weight terms.\nFor example, the term of Example 5.1 evaluating the function of a neural network of depth at most l is depth-l model agnostic. By comparison the formula stating that a network has useless neurons (cf. Example 5.2) is not model agnostic. The term t from Lemma 5.3, computing the integral, is depth-l model agnostic.\nThe query $\\int_0^1 f = 0$ for functions $f \\in PL(1)$ is expressible by a depth-2 agnostic FO(SUM) formula, but not in FO(R, F).\nWe have already seen the expressibility in FO(SUM). We prove nonexpressibility in FO(R, F).\nConsider the equal-cardinality query Q about disjoint pairs ($S_1, S_2$) of finite sets of reals, asking whether |S\u2081| = |S\u2082|. Over abstract ordered finite structures, equal cardinality is well-known not to be expressible in order-invariant first-order logic [23]. Hence, by the generic collapse theorem for constraint query languages over the reals [20, 23], query Q is not expressible in FO(R, S1, S2).\nNow for any given $S_1$ and $S_2$, we construct a continuous piecewise linear function $f_{S_1,S_2}$ as follows. We first apply a suitable affine transformation so that $S_1 \\cup S_2$ falls within the open interval (0,1). Now $f_{S_1,S_2}$ is a sawtooth-like function, with positive teeth at elements from $S_1$, negative teeth (of the same height, say 1) at elements from $S_2$, and zero everywhere else. To avoid teeth that overlap the zero boundary at the left or that overlap each other, we make them of width min{m, M}/2, where m is the minimum of $S_1 \\cup S_2$ and M is the minimum distance between any two distinct elements in $S_1 \\cup S_2$.\nExpressing the above construction uniformly in FO(R, S1, S2) poses no difficulties; let $\\psi(x, y)$ be a formula defining $f_{S_1,S_2}$. Now assume, for the sake of contradiction, that $\\int_0^1 F = 0$ would be expressible by a closed FO(R, F) formula $\\varphi$. Then composing $\\psi$ with $\\varphi$ would express query Q in FO(R, S1, S2). Indeed, clearly, $\\int_0^1 f_{S_1,S_2} = 0$ if and only if |S\u2081| = |S\u2082|. So $\\varphi$ cannot exist.\nIt seems awkward that in the definition of model agnosticity we need to bound the depth. Let us call an FO(SUM) term or formula fully model agnostic if is depth-l model agnostic for every l. It turns out that there are no nontrivial fully model agnostic FO(SUM) formulas.\nLet\\phi be a fully model agnostic closed FO(SUM) formula over $\\Upsilon_{net (m, 1)}$. Then either $N \\vDash \\varphi$ for all $N \\in \\bigcup_{l>1}F(m, l)$ or $N \\nvDash \\phi$ for all $N \\in \\bigcup_{l\\geq 1} F(m, l)$.\nThe proof is in Appendix B. The idea is that FO(SUM) is Hanf-local [22, 23]. No formula can distinguish a long enough structures consisting of two chains where the middle nodes"}, {"title": "11", "content": "are marked by two distinct constants $c_1$ and $c_2$, from its sibling structure where the markings are swapped. We can turn the two structures into neural networks by replacing the markings by two gadget networks $N_1$ and $N_2$, representing different functions, that is supposed to distinguish. However, the construction is done so that the function represented by the structure is the same as that represented by the gadget in the left chain. Still, FO(SUM) cannot distinguish these two structures. So, is either not fully model-agnostic, or $N_1$ and $N_2$ cannot exist and $\\varphi$ is trivial.\nThe FO(R, F) query F(0) = 0 is not expressible in FO(SUM)."}, {"title": "7 From FO(Rlin) to FO(SUM)", "content": "In practice, the number of layers in the employed neural network architecture is often fixed and known. Our main result then is that FO(SUM) can express all FO (Rlin) queries.\nLet m and l be natural numbers. For every closed FO(Rlin, F) formula there exists a closed FO(SUM) formula $\\psi$ such that for every network N in F(m, l), we have $R, F^N \\vDash \\psi$ iff N = $\\varphi$.\nThe challenge in proving this result is to simulate, using quantification and summation over neurons, the unrestricted access to real numbers that is available in FO (Rlin). Thereto, we will divide the relevant real space in a finite number of cells which we can represent by finite tuples of neurons.\nThe proof involves several steps that transform weighted structures. Before presenting the proof, we formalize such transformations in the notion of FO(SUM) translation, which generalize the classical notion of first-order interpretation [15] to weighted structures."}, {"title": "7.1 FO(SUM) translations", "content": "Let $\\Upsilon$ and $\\Gamma$ be vocabularies for weighted structures, and let n be a natural number. An n-ary FO(SUM) translation $\\varphi$ from $\\Upsilon$ to $\\Gamma$ consists of a number of formulas and weight terms over $\\Upsilon$, described next. There are formulas $dom(\\bold{x})$ and $\\varphi_=(\\bold{x_1}, \\bold{x_2})$; formulas $\\varphi_R(\\bold{x_1},...,\\bold{x_k})$ for every k-ary relation symbol R of $\\Gamma$; and formulas $\\varphi_f(\\bold{x_0}, \\bold{x_1},...,\\bold{x_k})$ for every k-ary standard function symbol f of $\\Gamma$. Furthermore, there are weight terms $\\varphi_w(\\bold{x_1},...,\\bold{x_k})$ for every k-ary weight function k of $\\Gamma$.\nIn the above description, bold x denote n-tuples of distinct variables. Thus, the formulas and weight terms of $\\varphi$ define relations or weight functions of arities that are a multiple of n.\nWe say that $\\varphi$ maps a weighted structure A over $\\Upsilon$ to a weighted structure B over $\\Gamma$ if there exists a surjective function h from $dom(A) \\subseteq A^n$ to B such that:\n$\\rhd h(\\bold{a_1}) = h(\\bold{a_2}) \\Leftrightarrow A \\vDash \\varphi_=(\\bold{a_1}, \\bold{a_2});$\n$\\rhd (h(\\bold{a_1}),...,h(\\bold{a_k})) \\in R^B \\Leftrightarrow A \\vDash \\varphi_R(\\bold{a_1},..., \\bold{a_k});$\n$\\rhd (h(\\bold{a_0}) = f^B(h(\\bold{a_1}), ..., h(\\bold{a_k})) \\Leftrightarrow A \\vDash \\varphi_f (\\bold{a_0}, \\bold{a_1},..., \\bold{a_k});$\n$\\rhd w^B(h(\\bold{a_1}),..., h(\\bold{a_m})) = \\varphi_w(\\bold{a_1}, ..., \\bold{a_n})$.\nIn the above, the bold a denote n-tuples in dom(A).\nFor any given A, if $\\varphi$ maps A to B, then B is unique up to isomorphism. Indeed, the elements of B can be understood as representing the equivalence classes of the equivalence relation $=(A)$ on dom(A). In particular, for B to exist, $\\varphi$ must be admissible on A, which means that =(A) is indeed an equivalence relation on dom(A), and all relations and all functions $\\varphi_R(A)$, $\\varphi_f(A)$ and $\\varphi_w(A)$ are invariant under this equivalence relation."}, {"title": "11", "content": "If K is a class of structures over $\\Upsilon$, and T is a transformation of structures in K to structures over $\\Gamma$, we say that $\\varphi$ expresses T if $\\varphi$ is admissible on every A in K, and maps A to T(A).\nThe relevant reduction theorem for translations is the following:\nLet $\\varphi$ be an n-ary FO(SUM) translation from $\\Upsilon$ to $\\Gamma$, and let $(\\psi_1,..., \\psi_k)$ be a formula over $\\Gamma$. Then there exists a formula $\\phi(x_1,...,x_k)$ over $\\Upsilon$ such that whenever $\\varphi$ maps A to B through h, we have $B \\vDash \\psi(h(a_1),...,h(a_k))$ iff $A \\vDash \\phi\\psi(\\alpha_1,...,\\alpha_k)$.\nFurthermore, for any weight term t over $\\Gamma$, there exists a weight term $\\varphi_t$ over $\\Upsilon$ such that $t^B(h(a_1),...,h(a_k)) = \\varphi_t^A(\\alpha_1,...,\\alpha_k)$.\nAs this result is well known and straightforward to prove for classical first- order interpretations, we only deal here with summation terms, which are the main new as- pect. Let t be of the form $\\sum_{y:\\phi} t'$. Then for $4_t$ we take $\\sum_{x:\\phi_{\\varphi}} \\phi_{t'} (X_1,..., X_k, x)/(x':\\phi_{=}(x,x') 1).$"}, {"title": "7.2 Proof of Theorem 7.1", "content": "We sketch the proof of Theorem 7.1. For clarity of exposition, we present it first for single inputs, i.e., the case m = 1. We"}]}