{"title": "Geometry Informed Tokenization of Molecules for Language Model Generation", "authors": ["Xiner Li", "Limei Wang", "Youzhi Luo", "Carl Edwards", "Shurui Gui", "Yuchao Lin", "Heng Ji", "Shuiwang Ji"], "abstract": "We consider molecule generation in 3D space using language models (LMs), which\nrequires discrete tokenization of 3D molecular geometries. Although tokenization\nof molecular graphs exists, that for 3D geometries is largely unexplored. Here,\nwe attempt to bridge this gap by proposing the Geo2Seq, which converts molec-\nular geometries into SE(3)-invariant 1D discrete sequences. Geo2Seq consists\nof canonical labeling and invariant spherical representation steps, which together\nmaintain geometric and atomic fidelity in a format conducive to LMs. Our exper-\niments show that, when coupled with Geo2Seq, various LMs excel in molecular\ngeometry generation, especially in controlled generation tasks.", "sections": [{"title": "Introduction", "content": "The generation of novel molecules with desired properties is an important step in drug discovery.\nSpecifically, the design of three-dimensional (3D) molecular geometries is particularly important\nbecause 3D information plays a critical role in determining many molecular properties. Different\ngenerative models have been used for 3D molecule generation. Early studies such as G-SchNet [27]\nuse autoregressive generative models to generate 3D molecules by sequentially placing atoms in 3D\nspace. It was observed that these models often yield results with low chemical validity. Recently,\ndiffusion models [34, 89] achieve better performance in 3D molecule generation tasks. However,\nthey typically need thousands of diffusion steps, resulting in long generation time.\nLanguage models (LMs) [77, 16, 10, 30], with their streamlined data processing and powerful\ngeneration capabilities, have shown success across various domains, particularly in natural language\nprocessing (NLP). Recently, large language models (LLMs) [97] show extraordinary capabilities in\nlearning complex patterns [93] and generating meaningful outputs [74, 1, 14]. Despite their potential,\nthe application of LLMs to the direct generation of 3D molecules is largely under-explored. This\nis primarily due to the fact that geometric graph structures of molecular data are fundamentally\ndifferent from texts. However, 3D geometric information is crucial in molecular tasks, since different\nconformations of the same molecule topology have different properties, such as per-atom forces. This\ngap reveals a unique challenge of how to make use of the powerful pattern recognition and generative\ncapabilities of LLMs to handle complicated molecular graph structures, especially geometries.\nIn this work, we bridge this gap by applying LMs to the task of 3D molecule generation. We\nemploy a novel approach translating the intricate geometry of molecules into a format that can be\neffectively processed by LMs. This is achieved by our proposed tokenization method Geo2Seq, which\nconverts 3D molecular structures into SE(3)-invariant one-dimensional (1D) discrete sequences.\nThe transformation is based on canonical labeling, which allows dimension reduction with no\ninformation loss outside graph isomorphism groups, and invariant spherical representations, which\nguarantees SE(3)-invariance under the equivariant global frame. By doing so, we harness the\nadvanced sequence-processing capabilities and efficiency of LMs while retaining essential geometric\nand atomic information.\nNote that since Geo2Seq operates solely on input data, our method is agnostic to the subsequent LMs\nused. When combined with powerful modern LLMs, Geo2Seq can achieve highly accurate modeling\nof 3D molecular structures. In addition, Geo2Seq can benefit conditional generation by including\nreal-world chemical properties in sequences because modern LLMs are capable of capturing long-\ncontext correlations to comprehend global structure and information in sequences. Our experimental\nresults demonstrate these advantages. We show that using different LMs with Geo2Seq can reliably\nproduce valid and diverse 3D molecules and outperform the strong diffusion-based baselines by a\nlarge margin in conditional generation. These results validate the feasibility of using LMs for 3D\nmolecule generation and highlight their potential to aid in the discovery and design of new molecules,\npaving the way for applications such as drug development and material science."}, {"title": "Preliminaries and Related Work", "content": "In this work, we study the problem of generating 3D molecules from scratch. Note that this problem\nis different from the 3D molecular conformation generation problem studied in the literature [53,\n71, 28, 86, 87, 69, 26, 88, 38], where 3D molecular conformations are generated from 2D molecular\ngraphs. We represent a 3D molecule with n atoms in the form of a 3D point cloud (i.e., a set of\npoints with different positions in 3D Euclidean space) as $G = (z, R)$. Here, $z = [z_1,\\cdots, z_n] \\in \\mathbb{Z}^n$\nis the atom type vector where $z_i$ is the atomic number (nuclear charge number) of the $i$-th atom,\nand $R = [r_1,\\ldots, r_n] \\in \\mathbb{R}^{3 \\times n}$ is the atom coordinate matrix, where $r_i$ is the 3D coordinate of the\n$i$-th atom. Note that 3D atom coordinates $R$ are commonly called 3D molecular conformations or\ngeometries in chemistry. We aim to solve the following two generation tasks in this work:\n\u2022 Random generation. Given a 3D molecule dataset $G = \\{G_j\\}_{j=1}^N$, we aim to learn an unconditional\ngenerative model $p_\\Theta(\\cdot)$ on $G$ so that the model can generate valid and diverse 3D molecules.\n\u2022 Controllable generation. Given a 3D molecule dataset $G = \\{(G_j, s_j)\\}_{j=1}^N$ where $s_j$ is a certain\nproperty value of $G_j$, we aim to learn a conditional generative model $p_\\Theta(\\cdot|s)$ on $G$ so that for a\ngiven $s$, the model can generate 3D molecules whose quantum property values are $s$.\nA major technical challenge of 3D molecule generation lies in maintaining invariant to SE(3)\ntransformations, including rotation and translation. In other words, ideal models should assign the\nsame probability to $G = (z, R)$ and $G' = (z, R')$ if $R' = QR+b1^\\top$, where $1$ is an $n$-dimensional\nvector whose elements are all one, $b \\in \\mathbb{R}^3$ is an arbitrary translation vector, and $Q \\in \\mathbb{R}^{3 \\times 3}$ is\na rotation matrix satisfying $QQ^\\top = I, |Q| = 1$. To achieve SE(3)-invariance in 3D molecule\ngeneration, existing studies have proposed various strategies. Early studies propose to generate 3D\natom positions by SE(3)-invariant features, such as interatomic distances, angles and torsion angles.\nThey construct 3D molecular structures through either atom-by-atom generation [27, 52] or generating\nfull distance matrices [33] in one shot. Recently, more and more studies have applied generative\nmodels to generate 3D atom coordinate directly. These studies include E-NFs [66] and EDM [34],\nwhich combine equivariant atom coordinate alignment process with equivariant EGNN [67] model\nfor 3D molecule generation. Following EDM, many other studies have proposed to improve diffusion-\nbased 3D molecule generation frameworks by stochastic differential equation (SDE) based diffusion\nmodels [83, 7] or latent diffusion models [89]. Besides, some recent studies [63] have explored\ngenerating 3D molecules through generating and connecting fragments first, then aligning atom\ncoordinates with software like RDKit. We refer readers to Du et al. [18], Zhang et al. [95] for a\ncomprehensive review.\nWhile generating 3D molecules in the form of 3D point clouds have been well studied, few studies\nhave tried applying powerful language models to this problem. In this work, different from mainstream"}, {"title": "Chemical Language Model", "content": "LMs have catalyzed significant advancements across a spectrum of fields. Recently, LLMs have\nrevolutionized the landscape of NLP and beyond [74, 1, 14]. Drawing inspiration from NLP method-\nologies, chemical language models (CLMs) have emerged as a competent way for representing\nmolecules [9, 37, 6, 93]. Due to the superiority LMs show in generation tasks, most CLMs are\ndesigned as generative models. Variants of LMs have been adapted for molecular science, pro-\nducing a variety of works such as MolGPT [5], MolReGPT [42], MolT5 [19], MoleculeGPT [94],\nInstructMol [11], DrugGPT [43], and many others.\nCLMs learn the chemical vocabulary and syntax used to represent molecules, as well as the condi-\ntional probabilities of character occurrence at given positions of sequences depending on preceding\ncharacters. This vocabulary covers all characters from the adopted molecule representation. All\ninputs including chemical structures and properties should be converted into sequence form and tok-\nenized for compatibility with language models. Commonly, SMILES [81] is used for this sequential\nrepresentation, although other formats like SELFIES [39], atom type strings, and custom strings with\npositional or property values are also viable options. To learn representations, CLMs are usually\npre-trained on extensive molecular sequences through self-supervised learning. Subsequently, models\nare fine-tuned on more focused datasets with desired properties, such as activity against a target\nprotein. Generative CLMs generally adopt an autoregressive training approach of next token predic-\ntion, i.e., iteratively predicting each subsequent token in a sequence based on the preceding tokens.\nTraditional autoregressive models use the Transformer architecture with causal self-attention [10]\ndue to its superior efficacy, while other sequence models like recurrent neural networks (RNNs) and\nstate space models (SSMs) [30, 61, 60] also show considerable functionality.\nGiven a dataset of sequences, $U = \\{U_1,U_2,\\ldots,U_N\\}$, where $U_i$ is transformed from the rep-\nresentation, property conditions and/or descriptions of a molecule $G_i$ with $n_i$ nodes, let $U_i =$\n$\\{u_1, u_2,..., u_{n_i} \\}$ and all tokens $u_i$ belong to vocabulary $V$. An autoregressive CLM has param-\neters $\\Theta$ encoding a distribution with conditional probabilities of each token given its predecessors,\n$p(U_i; \\Theta) = \\prod_{j=1}^{n_i} p(u_j|u_0 : u_{j-1}; \\Theta)$. The optimization process involves maximizing the probabilities\nof the entire dataset $p(U; \\Theta) = \\prod_{i=1}^{N}P(U_i; \\Theta)$. Each conditional distribution $p(u_j|u_0 : u_{j-1};\\Theta)$ is\na categorical distribution over the vocabulary size $|V|$; thus the loss for each term aligns with the\nstandard cross-entropy loss. To generate new sequences, the model samples each token sequentially\nfrom these conditional distributions. To introduce randomness and control into generation, the sam-\npling process is typically modulated with Top-K (k) and temperature ($\\tau$) hyperparameters, enabling\na balance between adherence and diversity.\nMost existing CLM works consider chemical structures as well as other modalities such as natural\nlanguage captions [5, 42, 43, 19, 85, 13, 75, 91, 12, 62, 48, 80], while some focus on pure text of\nchemical literature [50] or molecule strings [31, 55, 8, 57, 21, 40, 36, 92, 82, 54]. Notably, all these\nworks solely consider 2D molecules for representation learning and downstream tasks, overlooking\n3D geometric structures which is crucial in many molecular predictive and generative tasks. For\nexample, different conformations of the same 2D molecule have different potentials and per-atom\nforces. In order to use pivotal 3D information, another line of work incorporate geometric models\nsuch as GNNs in parallel with the CLM [84, 94, 11, 44, 46, 24], which requires additional design\nand training techniques to mitigate alignment issues. Some works extend the architecture of CLM\nto include 3D-geometric-model-like modules in the attention block [25, 70, 45, 72, 51, 56, 76, 96],\ncapturing 3D information as positional encodings with considerable computations and framework\ndesign. In contrast, Flam-Shepherd and Aspuru-Guzik [23] make an initial attempt showing language\nmodels trained directly on contents of XYZ format chemical files can generate molecules with\nthree coordinates, implying pure LMs' potential to directly explore 3D chemical space. In this\nwork, we propose an invariant 3D molecular sequencing algorithm, Geo2Seq, to empower CLMs\nwith structural completeness and geometric invariance, showing LMs' capabilities of understanding\nmolecules precisely in 3D space. We extend beyond the conventional Transformer architecture of\nCLMs and additionally employ SSMs as LM backbones. Furthermore, Geo2Seq operates solely\non the input data, which allows independence from model architecture and training techniques and\nprovides reuse flexibility."}, {"title": "Tokenization of 3D Molecules", "content": "A fundamental difference between LMs and other models is that LMs use discrete inputs, i.e., tokens.\nIn this section, we introduce our tokenization method to map input 3D molecules with atomic\ncoordinates to discrete token sequences appropriate for LM learning.\nA main challenge in tokenization design is to develop bijective mappings between 3D molecules\nand token sequences, i.e., obtaining the same token sequence for the same input 3D molecule, while\nobtaining different sequences for different inputs. In this section, we present our solutions to tackle\nthis challenge. We first reorder the atoms in the input molecule to a canonical order (Section 3.1),\nsuch that any two isomorphic graphs result in the same canonical form, and any non-isomorphic\ngraphs yield different canonical forms. We then convert 3D Cartesian coordinates to SE(3)-invariant\nspherical representations, including distances and angles (Section 3.2). Combining them together,\nwe obtain our geometry informed tokenization method Geo2Seq (Section 3.3). We provide rigorous\nproof of all theorems supporting the bijective mapping relation in Appendix B."}, {"title": "Serialization via Canonical Ordering", "content": "As the first step in 3D molecule tokenization, we need to transform a graph to a 1D sequential\nrepresentation. We resort to canonical labeling as a solution for dimension reduction without\ninformation loss.\nCanonical labeling (CL), in the context of graph theory, is a process to assign a unique form to each\ngraph in a way that two graphs receive the same canonical form only if they are isomorphic [59]. The\ncanonical form is a re-indexed version of a graph, which is unique for the whole isomorphism class\nof a graph. The new indexes naturally establish the order of nodes in the graph. The order, which\nwe refer to as canonical labels, is not necessarily unique if the graph has symmetries and thus has\nan automorphism group larger than 1. However, all canonical labels are strictly equivalent when\nused for serialization. The canonical label essentially re-assigns an index $l_i$ to each node originally\nindexed with i in graph G. Since canonical labeling can precisely distinguish non-isomorphic graphs,\nit fully contains the structure information of a graph G. Thus, by arranging nodes with attributes\nin the labeling order $l_1, l_2, ..., l_n$, we obtain a sequential representation of attributed graphs with all\nstructural information preserved.\nThe Nauty algorithm [58], tailored for CL and computing graph automorphism groups, presents a rig-\norous implementation of CL. In this paper, we adopt the Nauty algorithm for CL calculation, while all\nanalyses and derivations apply to other rigorous algorithms. The bijective mapping relation between\nCL-obtained sequential representation and graphs can be be proved based on graph isomorphism. We\nfirst formally define graph isomorphism as below. Due to the geometric needs in our case, we move a\nstep forward and define the isomorphism problem for attributed graphs.\nDefinition 3.1. [Graph Isomorphism] Let $G_1 = (V_1, E_1, A_1)$ and $G_2 = (V_2, E_2, A_2)$ be two graphs,\nwhere $V_i$ denotes the set of vertices, $E_i$ denotes the set of edges, and $A_i$ denotes the node attributes\nof $G_i$ for $i = 1,2$. Let attr(v) denote the node attributes of vertex v. The graphs $G_1$ and $G_2$ are said\nto be isomorphic, denoted as $G_1 \\cong G_2$, if there exists a bijection $b: V_1 \\rightarrow V_2$ such that for every"}, {"title": "Invariant Spherical Representations", "content": "In this section, we describe how to incorporate 3D\nstructure information into our sequences. One main\nchallenge here is to ensure the SE(3)-invariance\nproperty described in Section 2.1. Specifically,\ngiven a 3D molecule, if it is rotated or translated\nin the 3D space, its 3D representation should be\nunchanged. Another challenge is to ensure no in-\nformation loss [47, 79]. Specifically, given the 3D\nrepresentation, we can recover the given 3D structure.\nIf two 3D structures cannot be matched via a SE(3)\ntransformation, the representations should be differ-\nent. This property is important to the discriminative\nability of models.\nWe address these challenges by spherical representa-\ntions, i.e., using spherical coordinates to represent 3D\nstructures. Compared to Cartesian coordinates, spher-\nical coordinate values are bounded in a smaller re-\ngion, namely, a range of $[0, \\pi]$ or $[0, 2\\pi]$. This makes\nspherical coordinates advantageous in discretized rep-\nresentations and thus easier to be modeled by LMs.\nGiven the same decimal place constraints, spherical\ncoordinates require a smaller vocabulary size, and\ngiven the same vocabulary size, spherical coordinates\npresent less information loss. This is also supported\nby empirical results and analysis when using different methods to represent 3D molecular structures,\nas detailed in Appendix C.\nWe propose to maintain SE(3)-invariance while ensuring no information loss. Given a 3D molecule\n$G$ with atom types $z$ and atom coordinates $R$, we first build a global coordinate frame $F = (x, y, z)$\nbased on the input. Specifically, as shown in Figure 1, the frame is built based on the first three\nnon-collinear atoms in the canonical ordering $L(G)$. Let $l_1, l_2, \\text{and } l_F$ be the indices of these three\natoms. Then the global frame $F = (x, y, z)$ is calculated as\n$\\begin{aligned}\nx &= \\text{normalize}(r_{l_2} - r_{l_1}), \\\\\ny &= \\text{normalize} ((r_{l_F} - r_{l_1}) \\times x), \\\\\nz &= x \\times y.\n\\end{aligned}$\nHere normalize(\u00b7) is the function to normalize a vector to unit length. Note that the global frame\nis equivariant to the rotation and translation of the input molecule, as shown in Figure 2 and\nAppendix B.2. After obtaining the global frame, we use a function $f(.)$ to convert the coordinates\nof each atom to spherical coordinates $d, \\theta, \\phi$ under this frame. Specifically, for each node $l_i$ with\ncoordinate $r_{l_i}$, the corresponding spherical coordinate is\n$\\begin{aligned}\nd_{l_i} &= ||r_{l_i} - r_{l_1}||_2, \\\\\n\\theta_{l_i} &= \\text{arccos} ((r_{l_i} - r_{l_1})\\cdot z/d_{l_i}), \\\\\n\\phi_{l_i} &= \\text{atan2} ((r_{l_i} - r_{l_1}) \\cdot y, (r_{l_i} - r_{l_1}) \\cdot x) .\\\\\n\\end{aligned}$\nThe spherical coordinates show the relative position of each atom in the global frame $F$. As shown\nin Figure 2, if the input coordinates are rotated by a matrix $Q$ and translated by a vector $b$, the\ntransformed spherical coordinates remain the same, so the spherical coordinates are SE(3)-invariant.\nNext, we demonstrate that there is no information loss in our method. We show that given our\nSE(3)-invariant spherical representations, we can recover the given 3D structures. For each node $l_i$,\nwe convert the spherical coordinate $[d_{l_i}, \\theta_{l_i}, \\phi_{l_i}]$ to coordinate $r'$ in 3D space as\n$\\begin{aligned}\n[d_{l_i} \\sin(\\theta_{l_i}) \\cos(\\phi_{l_i}), d_{l_i} \\sin(\\theta_{l_i}) \\sin(\\phi_{l_i}), d_{l_i} \\cos \\theta_{l_i}].\n\\end{aligned}$\nNote that our reconstructed coordinate $r'$ may not be exactly the same as the original coordinate $r_{l_i}$.\nHowever, there exists a SE(3)-transformation $g$, such that $g(r_{l_i}) = r'$, for all i. Note that the same\ntransformation $g$ is applied to all nodes. Formally, by applying the function $f(\\cdot)$ to the 3D coordinate\nmatrix $R$, we can demonstrate the following properties of spherical representations.\nLemma 3.3. Let $G = (z, R)$ be a 3D graph with node type vector $z$ and node coordinate matrix $R$.\nLet $F$ be the equivariant global frame of graph $G$ built based on the first three non-collinear nodes\nin $L(G)$. $f(\\cdot)$ is our function that maps 3D coordinate matrix $R$ of $G$ to spherical representations $S\nunder the equivariant global frame $F$. Then for any 3D transformation $g \\in SE(3)$, we have $f(R) =$\n$f(g(R))$. Given spherical representations $S = f(R)$, there exist a transformation $g \\in SE(3)$, such\nthat $f^{-1}(S) = g(R)$.\nLemma 3.3 indicates that our spherical representation is SE(3)-invariant, and we can reconstruct\n(a transformation of) the original coordinates. Therefore, our method can convert 3D structures\ninto SE(3)-invariant representations with no information loss. Detailed proofs are provided in\nAppendix B."}, {"title": "Geo2Seq: Geometry Informed Tokenization", "content": "In this section, we describe the process and properties of our 3D tokenization method, Geo2Seq.\nEquipped with canonical labeling that reduces graph structures to 1D sequences with no information\nloss regarding graph isomorphism, and SE(3)-invariant spherical representations that ensure no\n3D information loss, we develop Geo2Seq, a reversible transformation from 3D molecules to 1D\nsequences. Figure 1 shows an overview of Geo2Seq. Specifically, given a graph G with n nodes,\nGeo2Seq concatenates the node vector $[z_i, d_i, \\theta_i, \\phi_i]$ of every node in G to a 1D sequence by its\ncanonical order, $l_1,..., l_n$. To formulate the properties of Geo2Seq, we extend the concept of graph\nisomorphism in Definition 3.1 to 3D graphs.\nDefinition 3.4. [3D Graph Isomorphism] Let $G_1 = (z_1, R_1)$ and $G_2 = (z_2, R_2)$ be two 3D graphs,\nwhere $z_i$ is the node type vector and $R_i$ is the node coordinate matrix of the molecule $G_i$. Let\n$V_i$ denote the set of vertices, $A_i$ denote node attributes, and no edge exists. Two 3D graphs $G_1$\nand $G_2$ are 3D isomorphic, denoted as $G_1 \\approx_{3D} G_2$, if there exists a bijection $b : V_1 \\rightarrow V_2$ such\nthat $G_1 \\cong G_2$ given $A_i = [z_i, R_i]$, and there exists a 3D transformation $g \\in SE(3)$ such that\n$r_1^{b(i)} = g(r_i)$. If a small error $\\epsilon$ is allowed such that $|r_1^{b(i)} - g(r_i)| \\leq \\epsilon$, we call the two 3D\ngraphs $\\epsilon$-constrained 3D isomorphic.\nConsidering Lemma 3.2, we specify $G = (V, E, A)$ with $A = [z, R]$ and define the CL function for\n3D molecules as $L_m$, which extends the equivalence of Lemma 3.2 to $L_m$ with 3D isomorphism.\nWe formulate Geo2Seq and our major theoretical derivations below.\nTheorem 3.5. [Bijective Mapping between 3D Graph and Sequence] Following Definition 3.4, let\n$G_1 = (z_1, R_1)$ and $G_2 = (z_2, R_2)$ be two 3D graphs. Let $L_m(G)$ be the canonical label for 3D"}, {"title": "3D Molecule Generation", "content": "Training and Sampling. Now that we have defined a canonical and robust sequence representation\nfor 3D molecules, we turn to the method of modeling such sequences, $U$. Here, we attempt to train\na model M with parameters $\\theta$ to capture the distribution of such sequences, $p_\\Theta(U)$, in our dataset.\nAs this is a well-studied problem within language modeling, we opt to use two language models,\nGPT [64] and Mamba [29], which have shown effective sequence modeling capabilities on a range\nof tasks. Both models are trained using a standard next-token prediction cross-entropy loss $l$ for all\nelements in the sequence:\n$\\begin{aligned}\n\\min_\\Theta \\mathbb{E}_{U} [\\sum_{i=1}^{u-1} l (M_\\Theta(U_1,\\cdots, U_i), Ui), U_{i+1})]\n\\end{aligned}$\nTo sample from a trained model, we first select an initial atom token by sampling from the multinomial\ndistribution of first-tokens in the training data (we note that in almost all cases this is 'H'). We then\nperform a standard autoregressive sampling procedure by iteratively sampling from the conditional\ndistribution $p_\\Theta (U_{i+1}|U_1,\\ldots, U_i)$ until the stop token or max length is reached. We sample from this"}, {"title": "Experimental Studies", "content": "In this section, we evaluate the method of generating 3D molecules in the form of our proposed\nGeo2Seq representations by LLMs. We show that in the random generation task (see Section 2.1),\nthe performance of Geo2Seq with GPT [64] or Mamba [29] models is better than or comparable with\nstate-of-the-art 3D point cloud based methods, including EDM [34] and GEOLDM [89]. In addition,\nin the controllable generation task (see Section 2.1), we show that Geo2Seq with Mamba models\noutperform previous 3D point cloud based methods by a large margin."}, {"title": "Random Generation", "content": "Data. We adopt two datasets, QM9 [65] and GEOM-DRUGS [4], to evaluate performances in the\nrandom generation task. The QM9 dataset collects over 130k 3D molecules with 3D structures\ncalculated by density functional theory (DFT). Each molecule in QM9 has less than 9 heavy atoms\nand its chemical elements all belong to H, C, N, O, F. Following [3], we split the dataset into train,\nvalidation and test sets with 100k, 18k and 12k samples, separately. The GEOM-DRUGS dataset\nconsists of over 450k large molecules with 37 million DFT-calculated 3D structures. Molecules in\nGEOM-DRUGS has up to 181 atoms and 44.2 atoms on average. We follow Hoogeboom et al. [34]\nto select 30 3D structures with the lowest energies per molecule for model training.\nSetup. On the QM9 dataset, we set the training batch size to 32, base learning rate to 0.0004, and\ntrain a 12-layer GPT model and a 26-layer Mamba model by AdamW [49] optimizers. On the\nGEOM-DRUGS dataset, we set the training batch size to 32, base learning rate to 0.0004, and train\na 14-layer GPT model and a 28-layer Mamba model by AdamW optimizers. See Appendix D for\nmore information about hyperparameters and other settings. When model training is completed, we\nrandomly generate 10,000 molecules, and evaluate the performance on these molecules. Specifically,\nwe first transform 3D molecular structures to 2D molecular graphs using the bond inference algorithm\nimplemented in the official code of EDM. Then, we evaluate the performance by atom stability,\nwhich is the percentage of atoms with correct bond valencies, and molecule stability, which is the\npercentage of molecules whose all atoms have correct bond valencies. In addition, we report the"}, {"title": "Controllable Generation", "content": "Data. In the controllable generation task, we train our models on molecules and their property\nlabels in the QM9 [65] dataset. Specifically, we try taking a certain quantum property value as the\nconditional input to LLMs, and train LLMs to generate molecules with the conditioned quantum\nproperty values. Following Hoogeboom et al. [34], we split the training dataset of QM9 to two\nsubsets where each subset has 50k samples, and train our conditional generation models and an\nEGNN [67] based quantum property prediction models on these two subsets, respectively. We\nconduct the controllable generation experiments on six quantum properties from QM9, including"}, {"title": "Discussion", "content": "Geo2Seq showcases the potential of pure LMs in revolutionizing molecular design and drug discovery\nwhen geometric information is properly transformed. The framework has certain limitations, particu-\nlarly in the generalization abilities across the continuous domain of real numbers. Due to the discrete\nnature of vocabularies, LMs rely on large pre-training corpus, fine-grained tokenization or emergent\nabilities for better generalization, as a trade-off to high precision and versatility. Future works\npoints towards several directions, such as expanding on conditional tasks and exploring advanced\ntokenization techniques."}, {"title": "Broader Impacts and Limitations", "content": "Our work demonstrates the significant potential of pure language models (LMs) in revolutionizing\nmolecular design and drug discovery by effectively transforming geometric information. The chal-\nlenge of molecule design is particularly daunting when scientific experiments are cost-prohibitive or\nimpractical. In many real-world scenarios, data collection is confined to specific chemical domains,\nyet the ability to generate molecules for broader tasks where experimental validation is difficult\nremains crucial. Traditional diffusion-based models fall short in terms of efficiency, scalability, and\nthe ability to learn from extensive databases or transfer knowledge across different tasks. In contrast,\nLMs exhibit inherent advantages in these areas. We envision the development of efficient, large-scale\nmodels trained on vast chemical databases that can function across multiple datasets and molecular\ntasks. By introducing LMs into the 3D molecule generation field, we unlock substantial potential for\nbroad scientific impact.\nOur research adheres strictly to ethical guidelines, with no involvement of human subjects or potential\nprivacy and fairness issues. This work aims to advance the field of Machine Learning and AI for\ndrug discovery, with no immediate societal consequences requiring specific attention. We foresee\nno potential for malicious or unintended usage beyond known chemical applications. However, we\nrecognize that all technological advancements carry inherent risks, and we advocate for ongoing\nevaluation of the broader implications of our methodology in various contexts.\nWe admit certain limitations, including that rounding up numerical values to certain decimal places\nbring information loss and discretized numbers impair generalization abilities across the continuous\ndomain of real numbers. However, this is a trade-off betweeen advantages brought by our model-\nagnostic framework. Due to the discrete nature of vocabularies, LMs depend on extensive pre-training\ncorpora, fine-grained tokenization, or emergent abilities for better generalization, balancing high\nprecision and versatility. Geo2Seq operates solely on the input data, which allows independence from\nmodel architecture and training techniques and provides reuse flexibility. This also means that we\ncan effortlessly apply Geo2Seq on the latest generative language models, making seamless use of\ntheir capabilities. Future work points towards expanding on conditional tasks and exploring advanced\ntokenization techniques to enhance the model's performance and applicability."}, {"title": "Proofs", "content": "Lemma (Colored Canonical Labeling for Graph Isomorphism). Let $G_1 = (V_1", "L": "G \\rightarrow L$ be a\nfunction that maps a graph $G \\in G$", "holds": "n$L(G_1) = L(G_2) \\Leftrightarrow G_1 \\cong G_2$\nwhere $G_1 \\cong G_2$ denotes that the graphs $G_1$ and $G_2"}]}