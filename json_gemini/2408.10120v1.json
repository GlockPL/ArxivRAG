{"title": "Geometry Informed Tokenization of Molecules for Language Model Generation", "authors": ["Xiner Li", "Limei Wang", "Youzhi Luo", "Carl Edwards", "Shurui Gui", "Yuchao Lin", "Heng Ji", "Shuiwang Ji"], "abstract": "We consider molecule generation in 3D space using language models (LMs), which requires discrete tokenization of 3D molecular geometries. Although tokenization of molecular graphs exists, that for 3D geometries is largely unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which converts molecular geometries into SE(3)-invariant 1D discrete sequences. Geo2Seq consists of canonical labeling and invariant spherical representation steps, which together maintain geometric and atomic fidelity in a format conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various LMs excel in molecular geometry generation, especially in controlled generation tasks.", "sections": [{"title": "Introduction", "content": "The generation of novel molecules with desired properties is an important step in drug discovery. Specifically, the design of three-dimensional (3D) molecular geometries is particularly important because 3D information plays a critical role in determining many molecular properties. Different generative models have been used for 3D molecule generation. Early studies such as G-SchNet [27] use autoregressive generative models to generate 3D molecules by sequentially placing atoms in 3D space. It was observed that these models often yield results with low chemical validity. Recently, diffusion models [34, 89] achieve better performance in 3D molecule generation tasks. However, they typically need thousands of diffusion steps, resulting in long generation time.\nLanguage models (LMs) [77, 16, 10, 30], with their streamlined data processing and powerful generation capabilities, have shown success across various domains, particularly in natural language processing (NLP). Recently, large language models (LLMs) [97] show extraordinary capabilities in learning complex patterns [93] and generating meaningful outputs [74, 1, 14]. Despite their potential, the application of LLMs to the direct generation of 3D molecules is largely under-explored. This is primarily due to the fact that geometric graph structures of molecular data are fundamentally different from texts. However, 3D geometric information is crucial in molecular tasks, since different conformations of the same molecule topology have different properties, such as per-atom forces. This gap reveals a unique challenge of how to make use of the powerful pattern recognition and generative capabilities of LLMs to handle complicated molecular graph structures, especially geometries.\nIn this work, we bridge this gap by applying LMs to the task of 3D molecule generation. We employ a novel approach translating the intricate geometry of molecules into a format that can be effectively processed by LMs. This is achieved by our proposed tokenization method Geo2Seq, which"}, {"title": "Preliminaries and Related Work", "content": null}, {"title": "3D Molecule Generation", "content": "In this work, we study the problem of generating 3D molecules from scratch. Note that this problem is different from the 3D molecular conformation generation problem studied in the literature [53, 71, 28, 86, 87, 69, 26, 88, 38], where 3D molecular conformations are generated from 2D molecular graphs. We represent a 3D molecule with n atoms in the form of a 3D point cloud (i.e., a set of points with different positions in 3D Euclidean space) as $\\mathcal{G} = (\\mathbf{z}, \\mathbf{R})$. Here, $\\mathbf{z} = [z_1,\\cdots, z_n] \\in \\mathbb{Z}^n$ is the atom type vector where $z_i$ is the atomic number (nuclear charge number) of the $i$-th atom, and $\\mathbf{R} = [\\mathbf{r}_1,\\dots,\\mathbf{r}_n] \\in \\mathbb{R}^{3\\times n}$ is the atom coordinate matrix, where $\\mathbf{r}_i$ is the 3D coordinate of the $i$-th atom. Note that 3D atom coordinates $\\mathbf{R}$ are commonly called 3D molecular conformations or geometries in chemistry. We aim to solve the following two generation tasks in this work:\n\u2022 Random generation. Given a 3D molecule dataset $\\mathcal{G} = {\\mathcal{G}_j}_{j=1}^N$, we aim to learn an unconditional generative model $p_{\\mathcal{G}}(\\cdot)$ on $\\mathcal{G}$ so that the model can generate valid and diverse 3D molecules.\n\u2022 Controllable generation. Given a 3D molecule dataset $\\mathcal{G} = {(\\mathcal{G}_j, s_j)}_{j=1}^N$ where $s_j$ is a certain property value of $\\mathcal{G}_j$, we aim to learn a conditional generative model $p_{\\mathcal{G}}(\\cdot|s)$ on $\\mathcal{G}$ so that for a given $s$, the model can generate 3D molecules whose quantum property values are $s$.\nA major technical challenge of 3D molecule generation lies in maintaining invariant to SE(3) transformations, including rotation and translation. In other words, ideal models should assign the same probability to $\\mathcal{G} = (\\mathbf{z}, \\mathbf{R})$ and $\\mathcal{G}' = (\\mathbf{z}, \\mathbf{R}')$ if $\\mathbf{R}' = Q\\mathbf{R}+\\mathbf{b}\\mathbf{1}^T$, where $\\mathbf{1}$ is an $n$-dimensional vector whose elements are all one, $\\mathbf{b} \\in \\mathbb{R}^3$ is an arbitrary translation vector, and $Q \\in \\mathbb{R}^{3\\times 3}$ is a rotation matrix satisfying $QQ^T = I,|Q| = 1$. To achieve SE(3)-invariance in 3D molecule generation, existing studies have proposed various strategies. Early studies propose to generate 3D atom positions by SE(3)-invariant features, such as interatomic distances, angles and torsion angles. They construct 3D molecular structures through either atom-by-atom generation [27, 52] or generating full distance matrices [33] in one shot. Recently, more and more studies have applied generative models to generate 3D atom coordinate directly. These studies include E-NFs [66] and EDM [34], which combine equivariant atom coordinate alignment process with equivariant EGNN [67] model for 3D molecule generation. Following EDM, many other studies have proposed to improve diffusion-based 3D molecule generation frameworks by stochastic differential equation (SDE) based diffusion models [83, 7] or latent diffusion models [89]. Besides, some recent studies [63] have explored generating 3D molecules through generating and connecting fragments first, then aligning atom coordinates with software like RDKit. We refer readers to Du et al. [18], Zhang et al. [95] for a comprehensive review.\nWhile generating 3D molecules in the form of 3D point clouds have been well studied, few studies have tried applying powerful language models to this problem. In this work, different from mainstream"}, {"title": "Chemical Language Model", "content": "LMs have catalyzed significant advancements across a spectrum of fields. Recently, LLMs have revolutionized the landscape of NLP and beyond [74, 1, 14]. Drawing inspiration from NLP methodologies, chemical language models (CLMs) have emerged as a competent way for representing molecules [9, 37, 6, 93]. Due to the superiority LMs show in generation tasks, most CLMs are designed as generative models. Variants of LMs have been adapted for molecular science, producing a variety of works such as MolGPT [5], MolReGPT [42], MolT5 [19], MoleculeGPT [94], InstructMol [11], DrugGPT [43], and many others.\nCLMs learn the chemical vocabulary and syntax used to represent molecules, as well as the conditional probabilities of character occurrence at given positions of sequences depending on preceding characters. This vocabulary covers all characters from the adopted molecule representation. All inputs including chemical structures and properties should be converted into sequence form and tokenized for compatibility with language models. Commonly, SMILES [81] is used for this sequential representation, although other formats like SELFIES [39], atom type strings, and custom strings with positional or property values are also viable options. To learn representations, CLMs are usually pre-trained on extensive molecular sequences through self-supervised learning. Subsequently, models are fine-tuned on more focused datasets with desired properties, such as activity against a target protein. Generative CLMs generally adopt an autoregressive training approach of next token prediction, i.e., iteratively predicting each subsequent token in a sequence based on the preceding tokens. Traditional autoregressive models use the Transformer architecture with causal self-attention [10] due to its superior efficacy, while other sequence models like recurrent neural networks (RNNs) and state space models (SSMs) [30, 61, 60] also show considerable functionality.\nGiven a dataset of sequences, $\\mathcal{U} = {\\mathcal{U}_1,\\mathcal{U}_2,\\dots,\\mathcal{U}_N}$, where $\\mathcal{U}_i$ is transformed from the representation, property conditions and/or descriptions of a molecule $G_i$ with $n_i$ nodes, let $\\mathcal{U}_i = {u_1, u_2,\\dots, u_{n_i} }$ and all tokens $u_j$ belong to vocabulary $\\mathcal{V}$. An autoregressive CLM has parameters $\\theta$ encoding a distribution with conditional probabilities of each token given its predecessors, $p(\\mathcal{U}_i; \\theta) = \\prod_{j=1}^{n_i} p(u_j|u_0: u_{j-1}; \\theta)$. The optimization process involves maximizing the probabilities of the entire dataset $p(\\mathcal{U}; \\theta) = \\prod_{i=1}^{N}P(\\mathcal{U}_i; \\theta)$. Each conditional distribution $p(u_j|u_0: u_{j-1};\\theta)$ is a categorical distribution over the vocabulary size $|\\mathcal{V}|$; thus the loss for each term aligns with the standard cross-entropy loss. To generate new sequences, the model samples each token sequentially from these conditional distributions. To introduce randomness and control into generation, the sampling process is typically modulated with Top-K ($k$) and temperature ($\\tau$) hyperparameters, enabling a balance between adherence and diversity.\nMost existing CLM works consider chemical structures as well as other modalities such as natural language captions [5, 42, 43, 19, 85, 13, 75, 91, 12, 62, 48, 80], while some focus on pure text of chemical literature [50] or molecule strings [31, 55, 8, 57, 21, 40, 36, 92, 82, 54]. Notably, all these works solely consider 2D molecules for representation learning and downstream tasks, overlooking 3D geometric structures which is crucial in many molecular predictive and generative tasks. For example, different conformations of the same 2D molecule have different potentials and per-atom forces. In order to use pivotal 3D information, another line of work incorporate geometric models such as GNNs in parallel with the CLM [84, 94, 11, 44, 46, 24], which requires additional design and training techniques to mitigate alignment issues. Some works extend the architecture of CLM to include 3D-geometric-model-like modules in the attention block [25, 70, 45, 72, 51, 56, 76, 96], capturing 3D information as positional encodings with considerable computations and framework design. In contrast, Flam-Shepherd and Aspuru-Guzik [23] make an initial attempt showing language models trained directly on contents of XYZ format chemical files can generate molecules with three coordinates, implying pure LMs' potential to directly explore 3D chemical space. In this work, we propose an invariant 3D molecular sequencing algorithm, Geo2Seq, to empower CLMs with structural completeness and geometric invariance, showing LMs' capabilities of understanding molecules precisely in 3D space. We extend beyond the conventional Transformer architecture of CLMs and additionally employ SSMs as LM backbones. Furthermore, Geo2Seq operates solely on the input data, which allows independence from model architecture and training techniques and provides reuse flexibility."}, {"title": "Tokenization of 3D Molecules", "content": "A fundamental difference between LMs and other models is that LMs use discrete inputs, i.e., tokens. In this section, we introduce our tokenization method to map input 3D molecules with atomic coordinates to discrete token sequences appropriate for LM learning.\nA main challenge in tokenization design is to develop bijective mappings between 3D molecules and token sequences, i.e., obtaining the same token sequence for the same input 3D molecule, while obtaining different sequences for different inputs. In this section, we present our solutions to tackle this challenge. We first reorder the atoms in the input molecule to a canonical order (Section 3.1), such that any two isomorphic graphs result in the same canonical form, and any non-isomorphic graphs yield different canonical forms. We then convert 3D Cartesian coordinates to SE(3)-invariant spherical representations, including distances and angles (Section 3.2). Combining them together, we obtain our geometry informed tokenization method Geo2Seq (Section 3.3). We provide rigorous proof of all theorems supporting the bijective mapping relation in Appendix B."}, {"title": "Serialization via Canonical Ordering", "content": "As the first step in 3D molecule tokenization, we need to transform a graph to a 1D sequential representation. We resort to canonical labeling as a solution for dimension reduction without information loss.\nCanonical labeling (CL), in the context of graph theory, is a process to assign a unique form to each graph in a way that two graphs receive the same canonical form only if they are isomorphic [59]. The canonical form is a re-indexed version of a graph, which is unique for the whole isomorphism class of a graph. The new indexes naturally establish the order of nodes in the graph. The order, which we refer to as canonical labels, is not necessarily unique if the graph has symmetries and thus has an automorphism group larger than 1. However, all canonical labels are strictly equivalent when used for serialization. The canonical label essentially re-assigns an index $l_i$ to each node originally indexed with $i$ in graph $G$. Since canonical labeling can precisely distinguish non-isomorphic graphs, it fully contains the structure information of a graph $G$. Thus, by arranging nodes with attributes in the labeling order $l_1, l_2, ..., l_n$, we obtain a sequential representation of attributed graphs with all structural information preserved.\nThe Nauty algorithm [58], tailored for CL and computing graph automorphism groups, presents a rigorous implementation of CL. In this paper, we adopt the Nauty algorithm for CL calculation, while all analyses and derivations apply to other rigorous algorithms. The bijective mapping relation between CL-obtained sequential representation and graphs can be be proved based on graph isomorphism. We first formally define graph isomorphism as below. Due to the geometric needs in our case, we move a step forward and define the isomorphism problem for attributed graphs.\nDefinition 3.1. [Graph Isomorphism] Let $G_1 = (V_1, E_1, A_1)$ and $G_2 = (V_2, E_2, A_2)$ be two graphs, where $V_i$ denotes the set of vertices, $E_i$ denotes the set of edges, and $A_i$ denotes the node attributes of $G_i$ for $i = 1,2$. Let $\\text{attr}(v)$ denote the node attributes of vertex $v$. The graphs $G_1$ and $G_2$ are said to be isomorphic, denoted as $G_1 \\cong G_2$, if there exists a bijection $b: V_1 \\rightarrow V_2$ such that for every"}, {"title": "Invariant Spherical Representations", "content": "In this section, we describe how to incorporate 3D structure information into our sequences. One main challenge here is to ensure the SE(3)-invariance property described in Section 2.1. Specifically, given a 3D molecule, if it is rotated or translated in the 3D space, its 3D representation should be unchanged. Another challenge is to ensure no in-formation loss [47, 79]. Specifically, given the 3D representation, we can recover the given 3D structure. If two 3D structures cannot be matched via a SE(3) transformation, the representations should be different. This property is important to the discriminative ability of models.\nWe address these challenges by spherical representations, i.e., using spherical coordinates to represent 3D structures. Compared to Cartesian coordinates, spherical coordinate values are bounded in a smaller region, namely, a range of $[0, \\pi]$ or $[0, 2\\pi]$. This makes spherical coordinates advantageous in discretized representations and thus easier to be modeled by LMs. Given the same decimal place constraints, spherical coordinates require a smaller vocabulary size, and given the same vocabulary size, spherical coordinates present less information loss. This is also supported by empirical results and analysis when using different methods to represent 3D molecular structures, as detailed in Appendix C.\nWe propose to maintain SE(3)-invariance while ensuring no information loss. Given a 3D molecule $G$ with atom types $\\mathbf{z}$ and atom coordinates $\\mathbf{R}$, we first build a global coordinate frame $F = (\\mathbf{x}, \\mathbf{y}, \\mathbf{z})$ based on the input. Specifically, as shown in Figure 1, the frame is built based on the first three non-collinear atoms in the canonical ordering $L(G)$. Let $l_1, l_2, \\text{and } l_F$ be the indices of these three atoms. Then the global frame $F = (\\mathbf{x}, \\mathbf{y}, \\mathbf{z})$ is calculated as\n$\\begin{aligned}\n\\mathbf{x} &= \\text{normalize}(\\mathbf{r}_{l_2} - \\mathbf{r}_{l_1}), \\\\\n\\mathbf{y} &= \\text{normalize}((\\mathbf{r}_{l_F} - \\mathbf{r}_{l_1}) \\times \\mathbf{x}), \\\\\n\\mathbf{z} &= \\mathbf{x} \\times \\mathbf{y}.\n\\end{aligned}$"}, {"title": "Geo2Seq: Geometry Informed Tokenization", "content": "In this section, we describe the process and properties of our 3D tokenization method, Geo2Seq. Equipped with canonical labeling that reduces graph structures to 1D sequences with no information loss regarding graph isomorphism, and SE(3)-invariant spherical representations that ensure no 3D information loss, we develop Geo2Seq, a reversible transformation from 3D molecules to 1D sequences. Figure 1 shows an overview of Geo2Seq. Specifically, given a graph $G$ with $n$ nodes, Geo2Seq concatenates the node vector $[z_i, d_i, \\theta_i, \\phi_i]$ of every node in $G$ to a 1D sequence by its canonical order, $l_1,..., l_n$. To formulate the properties of Geo2Seq, we extend the concept of graph isomorphism in Definition 3.1 to 3D graphs.\nDefinition 3.4. [3D Graph Isomorphism] Let $G_1 = (\\mathbf{z}_1, \\mathbf{R}_1)$ and $G_2 = (\\mathbf{z}_2, \\mathbf{R}_2)$ be two 3D graphs, where $\\mathbf{z}_i$ is the node type vector and $\\mathbf{R}_i$ is the node coordinate matrix of the molecule $G_i$. Let $V_i$ denote the set of vertices, $A_i$ denote node attributes, and no edge exists. Two 3D graphs $G_1$ and $G_2$ are 3D isomorphic, denoted as $G_1 \\approx_{3D} G_2$, if there exists a bijection $b : V_1 \\rightarrow V_2$ such that $G_1 \\cong G_2$ given $A_i = [\\mathbf{z}_i, \\mathbf{R}_i]$, and there exists a 3D transformation $g \\in SE(3)$ such that $\\mathbf{r}_{l}^{G_1} = g(\\mathbf{r}_{b(i)}^{G_1})$. If a small error $\\epsilon$ is allowed such that $|\\mathbf{r}_{l}^{G_1} - g(\\mathbf{r}_{b(i)}^{G_1})| \\leq \\epsilon$, we call the two 3D graphs $\\epsilon$-constrained 3D isomorphic.\nConsidering Lemma 3.2, we specify $G = (V, E, A)$ with $A = [\\mathbf{z}, \\mathbf{R}]$ and define the CL function for 3D molecules as $L_m$, which extends the equivalence of Lemma 3.2 to $L_m$ with 3D isomorphism. We formulate Geo2Seq and our major theoretical derivations below.\nTheorem 3.5. [Bijective Mapping between 3D Graph and Sequence] Following Definition 3.4, let $G_1 = (\\mathbf{z}_1, \\mathbf{R}_1)$ and $G_2 = (\\mathbf{z}_2, \\mathbf{R}_2)$ be two 3D graphs. Let $L_m(G)$ be the canonical label for 3D"}, {"title": "3D Molecule Generation", "content": "Training and Sampling. Now that we have defined a canonical and robust sequence representation for 3D molecules, we turn to the method of modeling such sequences, $\\mathcal{U}$. Here, we attempt to train a model $M$ with parameters $\\theta$ to capture the distribution of such sequences, $p_{\\theta}(\\mathcal{U})$, in our dataset. As this is a well-studied problem within language modeling, we opt to use two language models, GPT [64] and Mamba [29], which have shown effective sequence modeling capabilities on a range of tasks. Both models are trained using a standard next-token prediction cross-entropy loss $\\ell$ for all elements in the sequence:\n$\\min_{\\theta} \\mathbb{E}_{\\mathcal{U}} [\\sum_{i=1}^{|\\mathcal{U}|-1} \\ell (M_{\\theta}(\\mathcal{U}_1,\\cdots, \\mathcal{U}_i), \\mathcal{U}_{i+1}) ]$\nTo sample from a trained model, we first select an initial atom token by sampling from the multinomial distribution of first-tokens in the training data (we note that in almost all cases this is 'H'). We then perform a standard autoregressive sampling procedure by iteratively sampling from the conditional distribution $p_{\\theta} (\\mathcal{U}_{i+1}|\\mathcal{U}_1,\\dots,\\mathcal{U}_i)$ until the stop token or max length is reached. We sample from this"}, {"title": "Experimental Studies", "content": "In this section, we evaluate the method of generating 3D molecules in the form of our proposed Geo2Seq representations by LLMs. We show that in the random generation task (see Section 2.1), the performance of Geo2Seq with GPT [64] or Mamba [29] models is better than or comparable with state-of-the-art 3D point cloud based methods, including EDM [34] and GEOLDM [89]. In addition, in the controllable generation task (see Section 2.1), we show that Geo2Seq with Mamba models outperform previous 3D point cloud based methods by a large margin."}, {"title": "Random Generation", "content": "Data. We adopt two datasets, QM9 [65] and GEOM-DRUGS [4], to evaluate performances in the random generation task. The QM9 dataset collects over 130k 3D molecules with 3D structures calculated by density functional theory (DFT). Each molecule in QM9 has less than 9 heavy atoms and its chemical elements all belong to H, C, N, O, F. Following [3], we split the dataset into train, validation and test sets with 100k, 18k and 12k samples, separately. The GEOM-DRUGS dataset consists of over 450k large molecules with 37 million DFT-calculated 3D structures. Molecules in GEOM-DRUGS has up to 181 atoms and 44.2 atoms on average. We follow Hoogeboom et al. [34] to select 30 3D structures with the lowest energies per molecule for model training.\nSetup. On the QM9 dataset, we set the training batch size to 32, base learning rate to 0.0004, and train a 12-layer GPT model and a 26-layer Mamba model by AdamW [49] optimizers. On the GEOM-DRUGS dataset, we set the training batch size to 32, base learning rate to 0.0004, and train a 14-layer GPT model and a 28-layer Mamba model by AdamW optimizers. See Appendix D for more information about hyperparameters and other settings. When model training is completed, we randomly generate 10,000 molecules, and evaluate the performance on these molecules. Specifically, we first transform 3D molecular structures to 2D molecular graphs using the bond inference algorithm implemented in the official code of EDM. Then, we evaluate the performance by atom stability, which is the percentage of atoms with correct bond valencies, and molecule stability, which is the percentage of molecules whose all atoms have correct bond valencies. In addition, we report the"}, {"title": "Controllable Generation", "content": "Data. In the controllable generation task, we train our models on molecules and their property labels in the QM9 [65] dataset. Specifically, we try taking a certain quantum property value as the conditional input to LLMs, and train LLMs to generate molecules with the conditioned quantum property values. Following Hoogeboom et al. [34], we split the training dataset of QM9 to two subsets where each subset has 50k samples, and train our conditional generation models and an EGNN [67] based quantum property prediction models on these two subsets, respectively. We conduct the controllable generation experiments on six quantum properties from QM9, including"}, {"title": "Discussion", "content": "Geo2Seq showcases the potential of pure LMs in revolutionizing molecular design and drug discovery when geometric information is properly transformed. The framework has certain limitations, particularly in the generalization abilities across the continuous domain of real numbers. Due to the discrete nature of vocabularies, LMs rely on large pre-training corpus, fine-grained tokenization or emergent abilities for better generalization, as a trade-off to high precision and versatility. Future works points towards several directions, such as expanding on conditional tasks and exploring advanced tokenization techniques."}, {"title": "Broader Impacts and Limitations", "content": "Our work demonstrates the significant potential of pure language models (LMs) in revolutionizing molecular design and drug discovery by effectively transforming geometric information. The challenge of molecule design is particularly daunting when scientific experiments are cost-prohibitive or impractical. In many real-world scenarios, data collection is confined to specific chemical domains, yet the ability to generate molecules for broader tasks where experimental validation is difficult remains crucial. Traditional diffusion-based models fall short in terms of efficiency, scalability, and the ability to learn from extensive databases or transfer knowledge across different tasks. In contrast, LMs exhibit inherent advantages in these areas. We envision the development of efficient, large-scale models trained on vast chemical databases that can function across multiple datasets and molecular tasks. By introducing LMs into the 3D molecule generation field, we unlock substantial potential for broad scientific impact.\nOur research adheres strictly to ethical guidelines, with no involvement of human subjects or potential privacy and fairness issues. This work aims to advance the field of Machine Learning and AI for drug discovery, with no immediate societal consequences requiring specific attention. We foresee no potential for malicious or unintended usage beyond known chemical applications. However, we recognize that all technological advancements carry inherent risks, and we advocate for ongoing evaluation of the broader implications of our methodology in various contexts.\nWe admit certain limitations, including that rounding up numerical values to certain decimal places bring information loss and discretized numbers impair generalization abilities across the continuous domain of real numbers. However, this is a trade-off betweeen advantages brought by our model-agnostic framework. Due to the discrete nature of vocabularies, LMs depend on extensive pre-training corpora, fine-grained tokenization, or emergent abilities for better generalization, balancing high precision and versatility. Geo2Seq operates solely on the input data, which allows independence from model architecture and training techniques and provides reuse flexibility. This also means that we can effortlessly apply Geo2Seq on the latest generative language models, making seamless use of their capabilities. Future work points towards expanding on conditional tasks and exploring advanced tokenization techniques to enhance the model's performance and applicability."}, {"title": "Proofs", "content": null}, {"title": "Proof of Lemma 3.2", "content": "Lemma (Colored Canonical Labeling for Graph Isomorphism). Let $G_1 = (V_1, E_1, A_1)$ and $G_2 = (V_2, E_2, A_2)$ be two finite, undirected graphs where $V_i$ denotes the set of vertices, $E_i$ denotes the set of edges, and $A_i$ denotes the node attributes of the graph $G_i$ for $i = 1,2$. Let $L : G \\rightarrow \\mathcal{L}$ be a function that maps a graph $G \\in \\mathcal{G}$, the set of all finite, undirected graphs, to its canonical labeling $L(G) \\in \\mathcal{L}$, the set of all possible canonical labelings, as produced by the Nauty algorithm. Then the following equivalence holds:\n$L(G_1) = L(G_2) \\Leftrightarrow G_1 \\cong G_2$\nwhere $G_1 \\cong G_2$ denotes that the graphs $G_1$ and $G_2$ are isomorphic.\nThe Nauty algorithm, tailored for CL and computing graph automorphism groups, presents rigorous mathematical underpinnings to guarantee the CL properties. Here we leave out the proof of Nauty algorithm's rigor for canonical labeling, which is detailed in the work of McKay and Piperno [58]. The key is the refinement process ensuring that the partitioning of the graph's vertices is done in such a way that any two isomorphic graphs will end with the same partition structure."}, {"title": "Proof of Lemma 3.3", "content": "Lemma. Let $G = (\\mathbf{z}, \\mathbf{R})$ be a 3D graph with node type vector $\\mathbf{z}$ and node coordinate matrix $\\mathbf{R}$. Let $F$ be the equivariant global frame of graph $G$ built based on the first three non-collinear nodes in $L(G)$. $f(\\cdot)$ is our function that maps 3D coordinate matrix $\\mathbf{R}$ to spherical representations $S$ under the equivariant global frame $F$. Then for any $SE(3)$ transformation $g$, we have $f(\\mathbf{R}) = f(g(\\mathbf{R}))$. Given spherical representations $S = f(\\mathbf{R})$, there exist a $SE(3)$ transformation $g$, such that $f^{-1}(S) = g(\\mathbf{R})$.\nProof. Let $l_1, l_2, \\text{and } l_F$ be the indices of the first three non-collinear atoms in $G$. Then the global frame $F = (\\mathbf{x}, \\mathbf{y}, \\mathbf{z})$ is\n$\\begin{aligned}\n\\mathbf{x} &= \\text{normalize}(\\mathbf{r}_{l_2} - \\mathbf{r}_{l_1}) \\\\\n\\mathbf{y} &= \\text{normalize}((\\mathbf{r}_{l_F} - \\mathbf{r}_{l_1}) \\times \\mathbf{x}) \\\\\n\\mathbf{z} &= \\mathbf{x} \\times \\mathbf{y}\n\\end{aligned}$\nFor a $SE(3)$ transformation $g$, let $\\mathbf{R}' = g(\\mathbf{R}) = Q\\mathbf{R} + \\mathbf{b}$. Then the global frame $F' = (\\mathbf{x}', \\mathbf{y}', \\mathbf{z}')$ is\n$\\begin{aligned}\n\\mathbf{x}' &= \\text{normalize}(\\mathbf{r}_{l_2}' - \\mathbf{r}_{l_1}') = \\text{normalize}(g(\\mathbf{r}_{l_2}) - g(\\mathbf{r}_{l_1})) = Q\\mathbf{x} \\\\\n\\mathbf{y}' &= \\text{normalize}((\\mathbf{r}_{l_F}' - \\mathbf{r}_{l_1}') \\times \\mathbf{x}_2) = \\text{normalize}((g(\\mathbf{r}_{l_F}) - g(\\mathbf{r}_{l_1})) \\times \\mathbf{x}_2) = Q\\mathbf{y} \\\\\n\\mathbf{z}' &= \\mathbf{x}' \\times \\mathbf{y}' = (Q\\mathbf{x}) \\times (Q\\mathbf{y}) = Q\\mathbf{z}\n\\end{aligned}$\nThus $F' = QF$. Here $\\text{normalize}(\\cdot)$ is the function to normalize a vector to the corresponding unit vector. Then $\\forall i$, the spherical representations $f(\\mathbf{R})_i$ is\n$\\begin{aligned}\nd_i &= ||\\mathbf{r}_i - \\mathbf{r}_{l_1} ||_2 \\\\\n\\theta_i &= \\text{arccos} ((\\mathbf{r}_i - \\mathbf{r}_{l_1})\\cdot \\mathbf{z}/d_i) \\\\\n\\phi_i &= \\text{atan2} ((\\mathbf{r}_i - \\mathbf{r}_{l_1}) \\cdot \\mathbf{y}, (\\mathbf{r}_i - \\mathbf{r}_{l_1})\\cdot \\mathbf{x})\n\\end{aligned}$\nSimilarly, the spherical representations $f(\\mathbf{R}')_{i}$ is\n$\\begin{aligned}\nd_i' &= ||\\mathbf{r}_i' - \\mathbf{r}_{l_1}' ||_2 = ||g(\\mathbf{r}_i) - g(\\mathbf{r}_{l_1}) ||_2 = d_i \\\\\n\\theta_i' &= \\text{arccos} ((\\mathbf{r}'_i - \\mathbf{r}'_{l_1})\\cdot \\mathbf{z}'/d_i) = \\text{arccos} ((g(\\mathbf{r}_i) - g(\\mathbf{r}_{l_1}))\\cdot \\mathbf{z}'/d_i) = \\theta_i \\\\\n\\phi_i' &= \\text{atan2} ((\\mathbf{r}'_i - \\mathbf{r}'_{l_1}) \\cdot \\mathbf{y}', (\\mathbf{r}'_i - \\mathbf{r}'_{l_1}) \\cdot \\mathbf{x}') = \\text{atan2} ((g(\\mathbf{r}_i) - g(\\mathbf{r}_{l_1})) \\cdot \\mathbf{y}', (g(\\mathbf{r}_i) - g(\\mathbf{r}_{l_1})) \\cdot \\mathbf{x}') = \\phi_i\n\\end{aligned}$\nTherefore, we show that $f(\\mathbf{R"}]}