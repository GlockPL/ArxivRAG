{"title": "Weight Clipping for Deep Continual and Reinforcement Learning", "authors": ["Mohamed Elsayed", "Qingfeng Lan", "Clare Lyle", "A. Rupam Mahmood"], "abstract": "Many failures in deep continual and reinforcement learning are associated with increasing magnitudes of the weights, making them hard to change and potentially causing overfitting. While many methods address these learning failures, they often change the optimizer or the architecture, a complexity that hinders widespread adoption in various systems. In this paper, we focus on learning failures that are associated with increasing weight norm and we propose a simple technique that can be easily added on top of existing learning systems: clipping neural network weights to limit them to a specific range. We study the effectiveness of weight clipping in a series of supervised and reinforcement learning experiments. Our empirical results highlight the benefits of weight clipping for generalization, addressing loss of plasticity and policy collapse, and facilitating learning with a large replay ratio.", "sections": [{"title": "1 Introduction", "content": "Deep learning and reinforcement learning methods face many challenges when learning online or continually. These challenges include loss of plasticity (Lyle et al. 2023, Dohare et al. 2023a), failure to achieve further improvement (e.g., Sokar et al. 2023, Lyle et al. 2023, 2021), gradual performance decreases (e.g., Dohare et al. 2023a, Abbas et al. 2023, Elsayed & Mahmood 2024), and even loss of generalization (Ash & Adams 2019). One common feature among many instances of learning failures is their association with increasing weight magnitudes. An unbounded weight growth can make it increasingly harder for the learners to adjust the weight further (Lyle et al. 2024), causing loss of plasticity (Dohare et al. 2023a) or policy collapse (Dohare et al. 2023b). Moreover, large weight magnitudes can be harmful to the optimization dynamics (see Lyle et al. 2023 and Wortsman et al. 2023) and are often associated with overfitting (Zhang et al. 2021), leading to performance decrease and potentially explaining the learning difficulties.\nWhile many methods exist to address these learning difficulties, these methods are predominantly complex or require significant changes to the learning systems. For example, several methods require a change of the optimization method (Dohare et al. 2023a, Elsayed & Mahmood 2024, Sokar et al. 2023), use an auxiliary objective (Lan et al. 2023), change the architecture (e.g., Lyle et al. 2023, Nikishin et al. 2023, Lan & Mahmood 2023), or even require a new reinforcement learning estimator that maintains exploration (Garg et al. 2022). There are simpler techniques of weight regularization that directly promote small"}, {"title": "2 Problem Formulation", "content": "In this section, we describe the two problem formulations we use in this paper and what metrics we use to evaluate learners in each of them."}, {"title": "2.1 Streaming Supervised Learning", "content": "In streaming supervised learning, the data samples are presented to the learner as they come, one sample at each time step. Each sample is processed once by the agent, then the learner is evaluated based on some evaluation metric, and after that, the sample is discarded immediately (Hayes et al. 2019). This setup mirrors animal learning (Hayes & Kanan 2022) and is important for various applications such as on-device learning. The target function $f_t$ generating these data samples is typically non-stationary, producing non-independently and identically distributed (non-i.i.d.) samples such that $y_t = f_t(x_t)$. For simplicity, we assume that the target function is locally stationary in time, not arbitrarily non-stationary, but instead changes frequently (e.g., due to task change). The learner is expected to process the input $x_t \\in \\mathbb{R}^d$ and produce a prediction $\\hat{y}_t \\in \\mathbb{R}$, after which it is evaluated based on the metric $E(y_t, \\hat{y}_t)$. The goal of the learner is to maximize the average online metric (see Kumar et al. 2023b, Elsayed & Mahmood 2024) given by $\\frac{\\Sigma_{t=1}^{T}E(Y_t, \\hat{y}_t)}{T}$, where $T$ is the total number of time steps."}, {"title": "2.2 Reinforcement Learning", "content": "The sequential decision process of the agent and the interaction with the environment is modeled as a Markov decision process (MDP). In this paper, we consider episodic interactions between the agent and the environment in which its episodic MDP is denoted by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma, d_0, H)$, where $\\mathcal{S}$ is the set of states, $\\mathcal{A}$ is the set of actions, $\\mathcal{R} \\subset \\mathbb{R}$ denotes the set of reward signals, $\\mathcal{P} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S} \\times \\mathcal{R})$ denotes the transition dynamics model in which $\\Delta(X)$ is a distribution over the set $X$, $d_0$ is the starting state distribution, $\\gamma \\in [0,1]$ is the discount factor, and $H$ is the set of terminal states. The agent interacts with the environments using a policy $\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$ that outputs a distribution over actions conditioned on the state (Sutton & Barto 2018). Each episode of interaction begins after the environment samples a state from the starting state distribution $S_0 \\sim d_0$. At each time step $t$, the policy receives the state $S_t$ and produces the action $A_t \\sim \\pi(.|S_t)$, and then the environment samples a new state and reward signal using the transition dynamics as follows: $S_{t+1}, R_{t+1} \\sim p(., .|S_t, A_t)$. The interaction continues until the agent ends up in one of the terminal"}, {"title": "3 Method", "content": "In this section, we introduce weight clipping and show how it can be used with existing optimization methods. We propose a clipping scheme that uses the boundaries given by a uniform distribution at initialization (e.g., He et al. 2015). Consider a neural network $f$ parameterized by the set of weights $\\mathcal{W} = \\{W_1, W_2, ..., W_L\\}$ and the set of biases $\\{b_1,b_2,..., b_L\\}$. Specifically, given that the entries of the weight matrix $W_l, \\forall l$ and the bias vector $b_l, \\forall l$ are initialized from the uniform distribution $U[-\\delta_l, \\delta_l], \\delta_l \\in \\mathbb{R}^+, \\forall l$, we propose clipping any value outside the range $[-\\kappa \\delta_l, \\kappa \\delta_l]$, where $\\kappa \\in \\mathbb{R}^+$ is a hyper-parameter defining the size of the constraint weight space. Algorithm 1 shows how weight clipping can be integrated into optimization methods.\nOne natural question is whether we reduce the expressivity of the network by weight clipping. Typically, the larger the neural network, the smaller the weight change is to reduce the loss (Ghorbani et al. 2019, Geiger et al. 2020). For example, in over-parameterized networks, the weight change is usually tiny to represent any function, which is referred to as lazy training (see Chizat et al. 2019). Thus, we suspect weight clipping would not reduce much of the expressivity of over-parameterized networks due to tiny weight changes and a relatively large clipping range. We leave studying the effect of weight clipping on neural network expressivity to future work.\nNext, we investigate the effect of weight clipping on smoothing the neural network. Liu et al. (2022) showed that reducing the Lipschitz constant (e.g., via Lipschitz regularization) increases the smoothness of the functions represented by neural networks and, hence, improves generalization (Yoshida et al. 2017), stabilizes Wasserstein generative adversarial networks (Arjovsky et al. 2017), and protects against adversarial attacks (Li et al. 2019). We start our analysis by showing that weight clipping leads to bounding the Lipschitz constant. In Theorem 1, we show that weight clipping bounds the Lipschitz constant (see proof in Appendix A). For simplicity, we consider fully connected networks and 1-Lipschitz activation functions (e.g., ReLU, Leaky ReLU, Tanh), but the results can be extended to other networks (see Gouk et al. 2021) and other activation functions.\nTheorem 1. Smoothness of Clipped Networks. Consider a fully-connected neural network $f_W: \\mathcal{X} \\rightarrow \\mathcal{Y}$ parametrized by the set of augmented weight matrices (include biases) $\\mathcal{W}_{Aug} = \\{W_1,...,W_L\\}$. If the activation function $\\sigma$ used is 1-Lipschitz (e.g., ReLU), then the clipped network $f_{W_{clipped}}$ is Lipschitz continuous. That is, $\\exists k \\geq 0$ such that $|| f_{W_{clipped}}(x_1) - f_{W_{clipped}}(x_2)||_1 \\leq k||x_1-x_2||_1, \\forall x_1, x_2 \\in \\mathcal{X}$.\nThis result suggests that weight clipping adds an upper bound to the sharpness level of the network. In contrast, while other regularization methods can also improve smoothness (e.g., Liu et al. 2022),"}, {"title": "4 Experiments", "content": "In this section, we study the effectiveness of weight clipping in 1) improving generalization, 2) maintaining network plasticity, 3) mitigating policy collapse, and 4) facilitating learning with large replay ratios. We start by considering the warm start setup (see Ash & Adams 2019) and show that weight clipping can reduce loss of generalization. We then evaluate weight clipping using non-stationary streaming learning problems introduced by Elsayed and Mahmood (2024). Specifically, we use the Input-Permuted MNIST, Label-Permuted EMNIST, and Label-Permuted mini-ImageNet problems. Finally, we evaluate the effectiveness of weight clipping in addressing policy collapse in PPO (Schulman et al. 2017) and improving the performance of DQN (Mnih et al. 2015) and Rainbow (Hessel et al. 2018) with a large replay ratio.\nThe performance is evaluated based on the test accuracy in the warm-starting problem, the average online accuracy for the streaming learning problems, and the non-discounted episodic return for the reinforcement learning problems. We perform a hyper-parameter search (see Appendix B) for each method and use the best-performing configuration to plot in the following experiments. Our criterion for the best-performing configuration is the one that maximizes the area under the accuracy curve in supervised learning problems and the area under the non-discounted episodic return for the reinforcement learning problems."}, {"title": "4.1 Weight Clipping for Improved Generalization", "content": "Here, we study the role of weight clipping (WC) in improving generalization. We use the warm starting setup proposed by Ash and Adams (2019) with the CIFAR-10 dataset (Krizhevsky 2009). Two variations of SGD are trained from scratch using ResNet18 (He et al. 2016), one using 100% of the training and another in two stages: on 50% of the data, followed by the other 50%. In Fig. 2, when SGD is trained in two stages, we observe that its test accuracy is lower than if it was trained on the full data in one stage, which gives the same loss of generalization phenomenon demonstrated by Ash and Adams (2019). We introduce weight clipping in two ways: 1) clip once after the training on the first half of the data and 2) clip every time step. We observe that weight clipping only once removes the generalization gap and improves the test accuracy significantly. On the other hand, weight clipping every time step improves generalization in both stages of learning. These results show that large weights can cause overfitting and reduction in performance, which is alleviated by weight clipping that can improve generalization. The results are averaged over 10 independent runs."}, {"title": "4.2 Weight Clipping in Streaming Learning", "content": "Now, we study weight clipping in addressing loss of plasticity using the Input-permuted MNIST problem. The input-permuted MNIST problem is a standard problem for studying loss of plasticity in neural networks since the learned features become irrelevant to the next task after each permutation. We permute the inputs every 5000 time step. A new task starts when a permutation is performed. Next, we use the label-permuted EMNIST and label-permuted mini-ImageNet problems where loss of plasticity is intertwined with catastrophic forgetting (Elsayed & Mahmood 2024). In both label-permuted problems, we permuted the labels each 2500 time step.\nWe compare SGD and Adam (Kingma & Ba 2014) along with their variations with two regularization methods, Shirnk & Perturb (S&P) and L2 Init, against weight clipping with SGD and Adam. In previous works (Kumar et al. 2023a, Dohare et al. 2023a), L2-Init and S&P have been shown to be effective in maintaining plasticity when combined with SGD but have not been studied when combined with Adam. In addition, we compare against the Madam optimizer (Bernstein et al. 2020), which uses weight clipping to reduce its exponential weight growth but has not been studied in non-stationary settings before. In the streaming classification problems, the learner is required to maximize the online average accuracy using a stream of data, one sample at a time. Each learner is presented with 1M samples and uses a multi-layer (300 \u00d7 150) network with leaky-relu units.\nFig. 3a shows that almost all methods except for SGD and Adam maintain their performance throughout, but by different degrees. We plot the average online accuracy against the number of tasks, where each point in the figure represents the percentage of correct predictions within the task since the sample online prediction is 1 for correct predictions and 0 for incorrect ones.\nIn Fig. 4, we characterize the solutions of each method using diagnostic statistics. Specifically, we show the online loss, $l_2$-norm of weights, and $l_2$-norm of gradients. In addition, we show the average online plasticity of each method using the sample plasticity metric (Elsayed & Mahmood 2024), which is given by $p(\\mathcal{Z}) = max(\\frac{L(W,\\mathcal{Z})}{max(L(W,\\mathcal{Z}),\\epsilon)}, 0)$ $\\in [0,1]$, where $\\mathcal{Z}$ is the sample, $W^{\\dagger}$ is the set of weights after performing the update and $\\epsilon$ is a small number for numerical stability. We observe a gradual increase of the $l_2$ norm of the weights of SGD and Adam compared to other methods.\nNext, we use the label-permuted problems to evaluate the role of weight clipping in not biasing the weights towards some point. The label-permuted problems involve label permutation, which means the learned representation by the learner does not need to change, and the learner can continually improve upon them instead of overwriting and then re-learning. Fig. 3b and 3c show that while all methods addressing loss of plasticity can maintain their performance, only weight clipping can keep improving its performance, likely due to not biasing the weights toward a specific point. We defer the diagnostic statistics on these two problems to Appendix C."}, {"title": "4.3 Weight Clipping Against Policy Collapse", "content": "In this section, we study the role of weight clipping in mitigating policy collapse (Dohare et al. 2023b). Dohare et al. (2023b) demonstrated that the performance of PPO can collapse if trained for longer periods of time. We use CleanRL's implementation (Huang et al. 2022) with the default hyper-parameters as suggested by Dohare et al. (2023b). The network used is multi-layered (64 \u00d7 64) with tanh activations. Fig. 5 illustrates the phenomenon of policy collapse where the performance of PPO with Adam drops gradually with time in a number of MuJoCo (Todorov et al. 2012) environments. Weight clipping is effective in mitigating policy collapse, allowing for continual improvement. Here, we only show the performance in the MuJoCo environments with which policy collapse happens and exclude other environments where PPO with Adam experiences no policy collapse. We further investigate why policy collapse happens using the approximate KL given by (1 - r) \u2013 logr as our diagnostic metric, where r is the ratio in PPO between the current policy and the old policy. Fig. 6 shows the approximate KL throughout learning using Adam against Adam+WC. We observe that the approximate KL with Adam increases, indicating that the current policy deviates a lot from the old policy, in contrast to Adam+WC, which maintains small values, stabilizing learning in PPO. Next, we show additional diagnostic statistics in Fig. 7 to characterize the solutions found by Adam and Adam+WC. Notably, we observe that weight clipping reduces the $l_2$ norm of the weights and reduces the percentage of saturated units. We defer diagnostic statistics on the rest of the environment in Appendix C."}, {"title": "4.4 Weight Clipping with Large Replay Ratios", "content": "The replay ratio (RR) is the number of gradient updates performed per environment step. An increase of RR helps a learning agent extract as much information as possible from transitions, resulting in a higher sample efficiency. However, in practice, when the RR is too high, the learning agent would overfit to a small amount of data and lose the plasticity to learn new information due to aggressive parameter updates, thus reducing the learning performance (Nikishin et al. 2022).\nIn this section, we show that by incorporating weight clipping, we can prevent the agent from aggressive parameter updates and improve sample efficiency by a large amount under a high RR"}, {"title": "5 Related Works", "content": "Biologically plausible NNs. Physical and biological systems usually have bounded outputs, as components or elements within these systems often have inherent limits or constraints. For example, in neurobiological processes, synaptic weights are assumed to have a maximum value (Michiels van Kessenich et al. 2016). Weight clipping can be viewed as a biologically plausible mechanism that improves plasticity in artificial neural networks."}, {"title": "6 Conclusion", "content": "In this paper, we introduced weight clipping as a simple mechanism that helps with learning under non-stationarity. Weight clipping can be used besides existing methods without any major change to the optimizer or the network used. Our results show that weight clipping can help mitigate loss of plasticity in streaming learning, alleviate policy collapse, and improve performance when learning with large replay ratios. Future work can perform adaptive weight clipping that does not require any hyper-parameter tuning or develop Lipschitz regularization methods that guarantee smoothness."}]}