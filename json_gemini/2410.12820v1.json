{"title": "A TRANSFORMER-BASED DEEP REINFORCEMENT LEARNING APPROACH TO SPATIAL NAVIGATION IN A PARTIALLY OBSERVABLE MORRIS WATER MAZE", "authors": ["Marte Eggen", "Inga Str\u00fcmke"], "abstract": "Navigation is a fundamental cognitive skill extensively studied in neuroscientific experiments and has lately gained substantial interest in artificial intelligence research. Recreating the task solved by rodents in the well-established Morris Water Maze (MWM) experiment, this work applies a transformer-based architecture using deep reinforcement learning \u2013 an approach previously unexplored in this context \u2013 to navigate a 2D version of the maze. Specifically, the agent leverages a decoder-only transformer architecture serving as a deep Q-network performing effective decision making in the partially observable environment. We demonstrate that the proposed architecture enables the agent to efficiently learn spatial navigation strategies, overcoming challenges associated with a limited field of vision, corresponding to the visual information available to a rodent in the MWM. Demonstrating the potential of transformer-based models for enhancing navigation performance in partially observable environments, this work suggests promising avenues for future research in artificial agents whose behavior resembles that of biological agents. Finally, the flexibility of the transformer architecture in supporting varying input sequence lengths opens opportunities for gaining increased understanding of the artificial agent's inner representation of the environment.", "sections": [{"title": "Introduction", "content": "Navigation is a fundamental cognitive skill extensively studied in neuroscientific experiments such as the Morris Water Maze (MWM), a well-established behavioral test for assessing spatial learning in rodents [1, 2, 3]. In this task, a rodent is placed in a circular pool filled with opaque water and tasked with locating a hidden, submerged platform. Over the course of repeated trials, the rodent learns to navigate to the platform using distal visual cues [3]. In artificial intelligence (AI) research, solving navigation tasks has lately gained substantial interest, specifically in the exploration of replicating neural firing patterns found in parahippocampal areas such as the entorhinal cortex [4]. Particularly, recent work has investigated the navigation strategies of a deep reinforcement learning agent in a square version of the MWM task [5]. This study employs an actor-critic network where a shared gated recurrent unit (GRU) is followed by two parallel sets of fully connected layers, providing the policy and value estimate of the current state. The proximal policy optimization algorithm is adapted for training, with auxiliary tasks integrated to improve the agent's performance. The study showed that a recurrent reinforcement learning agent indeed can learn to locate the hidden platform in the simulated MWM, and that the learning process benefited from introducing auxiliary tasks not directly related to the main navigation objective.\nIn order to realistically recreate neuroscientific experiments with artificial agents representing living organisms, the environment should be partially observable, to reflect rodents' limited field of vision. However, agents operating with incomplete information often lead to aliasing of multiple states into the same observation [6]. To address this challenge, which generally occurs in partially observable environments, recurrence has commonly been incorporated through GRU and long short-term memory units, to retain historical context from previous steps [5, 7]. In recent years, the transformer architecture has demonstrated exceptional success in handling sequential data, outperforming other neural network"}, {"title": "Background", "content": ""}, {"title": "The transformer architecture", "content": "The transformer is a deep neural network architecture, first introduced in [8] for sequence-to-sequence NLP modeling. The model leverages an attention mechanism to effectively capture long-range dependencies and correlations within sequential data. Specifically, self-attention allows each element of a sequence to attend to all other elements, enabling the model to learn higher-order relations across the entire input. The transformer operates on an embedded input sequence, where each embedding is a learned vector representation of dimension d corresponding to an input observation. Positional encodings are added to preserve information about the observations' relative positions. The resulting embedded sequence X = {x1,..., Xn} \u2208 Rn\u00d7d is projected through three learnable weight matrices We \u2208 Rd\u00d7dk,\nWK \u2208 Rd\u00d7dk, and WV \u2208 Rd\u00d7dv to produce queries Q, keys K and values V, respectively. The attention computation proposed in [8], referred to as scaled dot product attention, is\nQ = XWQ, K = XWK, V = XWV\nAttention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{dk}})V. (1)\nThe concept of multi-head attention, shown to be more beneficial than a single attention function, computes h attention heads in parallel [8]. Each head i \u2208 {1, ..., h} uses separate sets of queries, keys, and values, obtained from respective learned linear projections W? \u2208 Rd\u00d7dk,WK \u2208 Rd\u00d7dk, and WV \u2208 Rd\u00d7dv with dk = d\u2081 = d/h. The resulting output from all heads is concatenated and followed by a final learned linear transformation W\u00b0 \u2208 [Rhd\u300f\u00d7d. The multi-head attention function is defined as\nMultiHead(Q, K, V) = Concat(head1, ..., head\u0127)W\u00b0\nwhere head\u2081 = Attention(QW, KWK,VWV). (2)\nThe transformer model in [8] consists of both an encoder and a decoder, and offers a flexible architecture easily modified to accommodate a variety of machine learning tasks. The Generative Pre-trained Transformer (GPT) family of models represents a specific adaptation, only leveraging the decoder part of the original architecture in an auto-regressive manner [10]. The decoder comprises multiple layers, each containing a masked multi-head self-attention mechanism and a feed-forward network, in addition to layer normalization and residual connections. Masked self-attention ensures that each sequence element attends only to previous elements, maintaining the model's auto-regressive property. Among other modifications from the original GPT architecture [10] to its successor, GPT-2, the layer normalization has been moved to the input of the attention and feed-forward modules, with an additional layer normalization at the final output [11]. In this work, the reinforcement learning agent is constructed using the same decoder-only transformer architecture as in GPT-2."}, {"title": "Deep Q-networks", "content": "Q-learning is a reinforcement learning algorithm enabling an agent to learn the optimal policy for solving a task in a Markov decision process (MDP) defined by the tuple (S, A, P, R), consisting of states S, actions A, transition"}, {"title": "Method", "content": "In accordance with [5], the MWM environment is modeled as a partially observable MDP (POMDP), represented by the tuple (S, A, P, R, \u03a9, O). The distinction between the POMDP and the MDP from section 2.2 is that, at time step t, the agent receives a partial observation ot \u2208 \u03a9 determined by O : S \u2192 \u03a9 when in a state st \u2208 S. The hidden platform requires the agent to navigate using visual cues, which in our simulated version of the MWM are represented by a fixed segment of the circumference in a contrasting color to the rest of the environment. Similar to [5], our agent is provided with 12 evenly spaced sight lines spanning a fixed 1-radian field of view, aligned with the agent's facing direction (refer to Fig. 1). Each sight line is represented by two numbers: the distance to the intersecting environment boundary and the associated color, encoded as a unique integer. This results in an observation ot \u2208 R24 at each time step t.\nThe agent's experiences are stored in a replay buffer with a maximum size of 50,000, where the oldest element is removed each time a new element is inserted after the buffer reaches its capacity. The neural network input is a sequence of n consecutive observations ot:t+n = {0t, ..., Ot+n}, each embedded through a learned linear transformation into the dimension d = 128, and subsequently added to a learned positional encoding. The agent architecture is a stacked decoder-only transformer, equivalent to the structure of GPT-2 [11], with 2 layers and 8 attention heads, following the hyperparameters in [6]. The output from the final decoder layer is projected through a linear layer dimensioned for the action space to yield Q-values, akin to the proposed deep transformer Q-network in [6]. The model generates a Q-value per action for each observation in an input sequence, which, at each time step, relies exclusively on preceding observations via the masked self-attention mechanism. This approach diverges from that of [5] by removing the need for recurrence in the agent architecture.\nDuring evaluation, the selected action corresponds to the highest Q-value from the most recent time step in the input sequence, representing the current state. However, training with intermediate Q-values has been found to enable an efficient training regime and faster learning [6]. Following [6], the loss associated with an input sequence is computed by summing the individual losses according to (4) for every sequence element.\nThe action space comprises four actions: no action, a forward movement of 1 unit, and a left or right rotation of the agent's faced angle by 0.2 radians. The circular environment has a radius of 10 units, and a landmark, in the form of a different wall color, covers 1/8 of the total circumference. The maze contains a hidden platform with a radius of 0.75 units, fixed at a randomly selected position within a 5-unit radius. An episode terminates when the agent successfully reaches the platform, or after 500 steps. At the beginning of each episode, the agent is randomly positioned along the environment boundary, facing the center. This is equivalent to positioning the agent in the same location in each episode while rotating the environment. During training, the agent receives a reward of 1 for locating the platform, -0.3 for colliding with the environment boundary, and -0.0003 per step, motivating goal-directed behavior.\nThe training procedure employs the epsilon-greedy strategy with an exploration rate starting at a value of 0.95 which decays to 0.05 over 10,000 training steps. During training, the transformer dropout probability is 0.4 to prevent overfitting, the learning rate is 0.0001, the discount factor in (4) is 0.99, and the batch size is 64. A target network is updated every 10,000 training steps to ensure stable learning, see (4)."}, {"title": "Results and analysis", "content": "The agent was trained for 3, 000 episodes, varying the input sequence lengths to assess performance given different amounts of historical information about previous steps. Fig. 2a illustrates the total reward per episode, with the associated standard deviation computed across five independent runs, each corresponding to a different platform location. Similarly, Fig. 2b presents the number of steps per episode with standard deviations for the same sequence lengths. The figure includes a selection of the tested sequence lengths, showing that longer sequences tend to yield more efficient navigation. However, we observed diminishing returns in performance improvements beyond a sequence length of approximately 45 observations.\nTo further investigate navigation strategies, we analyzed the trained agent's behavior using a sequence length of 45 observations over 100 episodes. The agent developed a consistent navigation pattern, where it typically proceeds in a direct trajectory toward the center of the environment, and subsequently rotates until detecting the visual cue to adjust the navigation path towards the platform, as exemplified in Fig. 1. While we observe the described behavior in the majority of cases, we find that in a small subset of cases, the agent gets stuck in a repetitive oscillating movement. In"}, {"title": "Discussion", "content": "The capability of sequential processing is a particularly prominent feature of the transformer architecture, as demonstrated by our agent's efficient learning. The transformer architecture's ability to handle sequential input provides the agent with a history of previous observations, removing the need for recurrence in the network architecture. While we demonstrate the aptness of this architecture and input for solving the navigation task, the unexpected oscillating behavior suggests that the sequential processing capabilities are not fully exploited by our model configuration, or even the GPT-2 architecture. Thus, an interesting direction for further study is to understand how different aspects of the task are internalized with varying sequence lengths.\nFurthermore, we aim to utilize techniques from the field of explainable AI (XAI), suited for transformer architectures, to gain a better understanding of the agent's inner representations, i.a. how the agent internalizes information about the environment and solution strategies. While our study is limited to visual inspection of the agent's navigation strategies, the application of XAI might provide deeper and more detailed insights, primarily regarding the extent to which the agent utilizes the landmark for navigation. Additionally, such analysis could facilitate more precise optimization of the agent's behavior, and the agent's robustness could be assessed through more challenging platform locations. Finally, it is possible that XAI techniques could inform how different sequence lengths influence the agent's decision making, both in the cases of efficient navigation, and particularly in cases of \u201cstuck behavior\".\nAnother direction for future research involves refining the selection of experiences to add to the replay buffer, which functions as the agent's selective, task-specific memory. We aim to investigate whether a biology-inspired approach for filling the replay buffer could improve the relevance of the input sequences. Understanding which past experiences are most critical for a rodent's spatial learning could inform the design of a corresponding algorithmic experience selection criterion for artificial agents, potentially improving the learning process."}, {"title": "Conclusion", "content": "This work presents the successful application of a transformer-based architecture serving as a deep Q-network in an environment replicating the circular MWM, a well-established neuroscientific experiment for spatial learning in rodents. The simulation is intended to preserve the natural challenges faced by rodents in navigation tasks, ensuring the artificial agent's learning experience aligns with the physical constraints experienced by rodents. The agent operates in a partially observable environment to reflect a rodent's visual experience in navigating the maze, and we find that the transformer architecture is well-suited for this class of decision making tasks."}]}