{"title": "mmE5: Improving Multimodal Multilingual Embeddings via\nHigh-quality Synthetic Data", "authors": ["Haonan Chen", "Liang Wang", "Nan Yang", "Yutao Zhu", "Ziliang Zhao", "Furu Wei", "Zhicheng Dou"], "abstract": "Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.", "sections": [{"title": "1 Introduction", "content": "Multimodal embedding models encode multimedia inputs, such as images and text, into latent vector representations. They have demonstrated effectiveness across diverse downstream tasks, including classification (Deng et al., 2009), visual question answering (VQA) (Singh et al., 2019), and cross-modal retrieval (Hu et al., 2023). Prior studies have focused on training multimodal embedding models using simple text-image pre-trained models such as CLIP (Radford et al., 2021). More recently, researchers have turned to multimodal large language models (MLLMs), including LLaVA (Liu et al., 2023a) and Phi (Abdin et al., 2024), to develop universal embedding models.\nThese vision-language models (VLMs) mostly rely on high-quality human-labeled datasets to achieve robust embedding capabilities. Such datasets suffer from data scarcity because they require high costs of multimodal annotations. To address this, researchers have leveraged the advanced language modeling capabilities of large language models (LLMs) and MLLMs to synthesize datasets for fine-tuning multimodal embedding models (Zhang et al., 2024a; Zhou et al., 2024a; Zhang et al., 2024b). However, existing works lack a comprehensive exploration into the quality of synthetic embedding data. Typically, most data generated by them are limited to specific modality types of English retrieval tasks, harming the generalization capabilities of the embedding models.\nAfter analyzing common application scenarios of multimodal embedding models, we identify three key criteria and introduce a data synthesis framework guided by these principles: (1) Broad scope. Multimodal embedding models are commonly employed in tasks such as classification, visual question answering (VQA), and retrieval, which requires understanding various input combinations of text and images. Additionally, multilingual contexts are increasingly popular in daily scenarios. As shown in Figure 1, our framework synthesizes datasets covering three tasks, seven modality combinations, and 93 languages, ensuring that models trained on it generalize effectively across diverse scenarios. (2) Robust cross-modal alignment. In multimodal tasks, models must understand and align information across different modalities to generate meaningful representations. Without accurate cross-modal alignment, embeddings may fail to capture the underlying relationships, leading to poor performance in downstream tasks. To synthesize data of robust cross-modal alignment, our framework incorporates a deep thinking process. Specifically, for each sampled image, we first employ an MLLM to interpret it from four perspectives before generating data: general information, object-level description, contextual background information, and task-specific brainstorming, i.e., how the image relates to the given task. Additionally, the entire data synthesis process is executed within a single pass of an MLLM. By this, the MLLM can \"see\" the images at the whole time, avoiding potential information loss that might occur due to multiple I/O steps in previous works (Zhou et al., 2024a; Zhang et al., 2024b). (3) High fidelity. The individual quality of each modality (e.g., real images, high-quality instructions, queries, and hard negatives) determines the overall usefulness of the dataset. To enhance fidelity, our framework uses real images sampled from an open-source corpus (LAION-400m (Schuhmann et al., 2021)) as the input images. We also apply a series of quality control measures, such as self-evaluation and refinement, ensuring that the synthetic components accurately reflect real-world distributions and maintain strong cross-modal alignment.\nWith the synthesized data ready, we train a multimodal multilingual E5 model (mmE5). It achieves state-of-the-art performance on the 36 datasets of MMEB (Jiang et al., 2024b), using 45 times less training data than the previous SOTA model MMRet (Zhou et al., 2024a) (560K compared to 26M) in a zero-shot setting. After incorporating labeled data, mmE5 still demonstrates the best performance. Besides, mmE5 achieves the best results on the multilingual benchmark XTD (Aggarwal and Kale, 2020), demonstrating its superior multilingual capabilities.\nIn summary, our contributions are as follows:\n\u2022 Based on our analysis of common scenarios for multimodal embedding models, we identify three key criteria of high-quality synthetic data: broad scope, robust cross-modal alignment, and high fidelity.\n\u2022 We introduce a data synthesis framework guided by the proposed principles. This framework leverages an MLLM to produce high-quality synthetic datasets that cover a wide range of tasks, modality combinations, and languages. It ensures robust cross-modal alignment through a comprehensive multi-aspect interpretation process and maintains high fidelity by employing self-evaluation and refinement mechanisms.\n\u2022 Compared to the previous leading model, mmE5 achieves SOTA performance on the MMEB benchmark while using 45\u00d7 less synthetic data in both zero-shot and supervised settings. mmE5 also demonstrates superior multilingual capabilities on the XTD benchmark."}, {"title": "2 Related Work", "content": "Multimodal Embedding Previous studies, such as CLIP (Radford et al., 2021), Align (Jia et al., 2021), BLIP (Li et al., 2022), and CoCa (Yu et al., 2022), have employed large-scale weakly supervised data to learn separate multimodal representations through pre-training. Some works attempt to obtain universal embeddings for texts and images utilizing existing CLIP-like models (Wei et al., 2024; Liu et al., 2023b; Zhou et al., 2024b,c). For instance, UniIR (Wei et al., 2024) integrates separate embeddings from different modalities into unified features. Recent approaches finetune MLLMs to leverage their multimodal reasoning capabilities for obtaining universal representations (Jiang et al., 2024a,b; Zhang et al., 2024b; Zhou et al., 2024a; Lin et al., 2024). For example, VLM2Vec (Jiang et al., 2024b) utilizes instruction-tuning to transform MLLMs into embedding models.\nSynthetic Data The generation of synthetic data has been extensively explored for text embedding tasks (Wang et al., 2024a; Chen et al., 2024; Li et al., 2024b). With the recent emergence of MLLMs like Phi-3.5-V (Abdin et al., 2024) and LLaVA (Liu et al., 2023a), along with diffusion models like Stable Diffusion (Rombach et al., 2022), researchers have been focused on synthesizing data to address the scarcity of multimodal instruction-tuning datasets. For example, MagicLens (Zhang et al., 2024a) utilizes co-existing images from the same webpage and an LLM to create multimodal data triplets (query image, instruction, relevant image), i.e., IT\u2192I paradigm. MegaPairs (Zhou et al., 2024a) aims to synthesize more diverse data triplets by retrieving relevant images from different perspectives. GME (Zhang et al., 2024b) employs an LLM and a diffusion model to generate a fused modality dataset that includes both T\u2192IT and IT\u2192IT types. Table 1 presents a comparison of the synthesized data in this study with that of previous works."}, {"title": "3 Methodology: mmE5", "content": "In this section, we present our method, which synthesizes high-quality multimodal data for the further finetuning of our embedding model mmE5. As shown in Figure 2, our method consists of five stages: (1) Initially, for each data sample to be synthesized, we configure the specifics of the task, modality combination, language, and input images. (2) We employ an MLLM to generate multi-grained descriptions for the input images, ensuring that the synthesized texts are well-aligned with the images. (3) Utilizing this MLLM, we synthesize text data based on both the images and their descriptions. (4) The MLLM then evaluates its synthesized data from multiple perspectives, offering revised data to enhance cross-modal alignment and fidelity. (5) Finally, the synthesized texts and images are used to finetune an MLLM specifically for embedding tasks. To minimize potential information loss, stages (2), (3), and (4) are executed within a single pass of the MLLM."}, {"title": "3.1 Preliminaries", "content": "An MLLM can accept text, image, or text-image pairs as input, allowing both the query side $q$ and the document side $d$ to be multimodal. Inspired by existing works on synthetic text embedding data (Wang et al., 2024a; Chen et al., 2024), each data sample we generate is a quadruple of (task instruction, query, positive document, hard negative document), denoted as $(t, q, d_+, d^-)$. For each data piece, we first sample images from the large-scale open-source image corpus LAION-400M (Schuhmann et al., 2021) as the query image, positive image, and hard negative image $(q_i, d_i^+, d_i^-)$. Then, with these three images as input, an MLLM $\\pi_\\theta$ can synthesize a multimodal embedding data sample $y \\sim \\pi_\\theta(y \\vert q_i, d_i^+, d_i^-)$, where $y = (t, q_t, d_t^+, d_t^-)$. As a result, the synthetic data can have a maximum of seven elements: $\\{t, (q_t, q_i), (d_t^+, d_i^+), (d_t^-, d_i^-)\\}$. More data examples can be found in Appendix D."}, {"title": "3.2 Data Synthesis Framework", "content": "Guided by the principles of high-quality synthetic multimodal data, i.e., broad scope, robust cross-modal alignment, and high fidelity, we introduce a data synthesis framework. This framework is designed to synthesize high-quality data that transforms an MLLM for downstream embedding tasks."}, {"title": "3.2.1 Data Configuration", "content": "To prepare for the data synthesis process, we configure the input data from three aspects:\nTask and Modality Combination We aim to synthesize data with a broad scope by generating beyond simple retrieval data of IT\u2192IT and T\u2192IT types. Our data cover three key multimodal embedding tasks identified by previous work (Jiang et al., 2024b): classification, VQA, and retrieval. After selecting a task for synthesis, we will sample a modality combination with respect to the specific task, such as choosing from seven possible combinations for the retrieval task type. Note that we only synthesize data of modality types that are included in the MMEB benchmark (Jiang et al., 2024b), which can cover most scenarios.\nImage Despite the powerful multimodal capabilities of modern MLLMs (e.g., GPT-4o, Llama-3.2 (Meta, 2024), and Llava-1.6), most cannot generate images, and those that can often produce low-fidelity images (Zhou et al., 2024b). Following previous works (Zhang et al., 2024a; Zhou et al., 2024a), we sample real images from the LAION-400M corpus (Schuhmann et al., 2021). First, we will sample a query image from the corpus ($q_i \\in I$). Then, for the modality types involving images on the document side (e.g., IT\u2192IT), we use a small embedding model, jina-clip-v2 (Koukounas et al., 2024), to retrieve a similar positive image $d^+$ and a hard negative image $d^-$ efficiently.\nLanguage Most existing models only focus on high-source languages like English, harming the multilingual ability of embedding models. To synthesize multilingual data, we sample languages from the language list of XLM-R (Conneau et al., 2020) during configuration. In order to facilitate the common usage scenarios, we give high-source languages higher weights. Note that the generated task instruction will always be in English for effective instruction tuning."}, {"title": "3.2.2 One-pass Generation with MLLM", "content": "With the data configuration ready, we introduce a deep thinking process that involves interpreting input images, generating data, and performing self-evaluation. To ensure that the MLLM always takes the image context into account, we execute this entire process in a single pass.\nMulti-aspect Visual Interpretation To obtain a comprehensive understanding of the images, the MLLM $\\pi_\\theta$ first analyzes them from multiple perspectives: (1) the general information, (2) detailed description of the objects present, (3) contextual background information, and (4) potential connections between the image and the text that may be synthesized. The deep understanding of the images enables $\\pi_\\theta$ to produce texts that are closely aligned with the visual content, thereby enhancing the cross-modal alignment.\nSynthesizing Data Using the images and their descriptions as input, we prompt $\\pi_\\theta$ to synthesize texts $(t, q_t, d_t^+, d_t^-)$. Specifically, the text instruction $t$ is expected to connect $q_i$ with $d_i^+$.\nThe query and document texts should be relevant to their respective images. Note that the input and output formats for the synthetic data may vary depending on the combination of modalities. For example, for I\u2192IT and T\u2192IT types, there can be no query text and image, respectively.\nSelf-evaluation In order to further enhance the quality of the synthetic data, $\\pi_\\theta$ evaluates the data it synthesizes from: (1) the relevance of the texts to their corresponding images, (2) the plausibility of hard negatives, (3) the clarity of $t$, and (4) the"}, {"title": "3.3 Finetuning Embedding Model mmE5", "content": "Following previous works of instruction-tuned text embedding models (Xiao et al., 2024; Li et al., 2024a) and multimodal embedding models (Jiang et al., 2024b), we apply an instruction template on each query: [IMAGE] {t} \\n {qt} {qi}, where \u201c[IMAGE]\u201d is the image token that varies from different MLLMs. We then append an \u201c[EOS]\u201d token to each query and document. The representation of each input in an MLLM is derived from the output of the \u201c[EOS]\u201d token from the final layer.\nWe utilize the InfoNCE loss (van den Oord et al., 2018) to perform the standard contrastive learning objective on our synthetic data D:\n$L = - \\log \\frac{\\exp(\\phi(q, d^+))}{\\exp(\\phi(q, d^+)) + \\sum_{d^- \\in N} \\exp(\\phi(q, d^-))}$,\nwhere $q$ is the encoded multimodal query, $d$ represents the encoded document, and $N$ denotes the set of negative documents. The function $\\phi(\u00b7) = \\exp(\\cos(\u00b7)/\\tau)$, where $\\cos(\u00b7)$ denotes cosine similarity, and $\\tau$ is a temperature hyperparameter."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nWe synthesize a total of 560K multimodal embedding data samples. The MLLM utilized for data synthesis is GPT-4o-2024-08-06. The backbone model for mmE5 is Llama-3.2-11B-Vision2. For finetuning mmE5, we employed LoRA (Hu et al., 2022) with a rank of 8. We evaluate the general embedding performance in terms of Precision@1 on the MMEB benchmark (Jiang et al., 2024b). This benchmark comprises 36 multimodal embedding tasks across four categories: classification (10), VQA (10), retrieval (12), and visual grounding (4). Our synthetic dataset is distributed among classification, VQA, and retrieval tasks in a 1:1:2 ratio. We synthesize more retrieval data since this type contains more kinds of modality combinations. We do not synthesize visual grounding data since they are relatively simpler for MLLM based on the MMEB results. To evaluate multilingual multimodal capabilities, we conducted tests using the"}, {"title": "4.2 Results on MMEB", "content": "The overall results on the MMEB benchmark are presented in Table 2. mmE5 achieves the best performance on both zero-shot setting (with synthetic data only) and supervised setting (with IND training datasets of MMEB). This demonstrates the quality of our synthetic data and the effectiveness of our multimodal embedding model. Furthermore, we can make the following observations: (1) mmE5 generalizes well on all four kinds of tasks. This demonstrates the broad scope of our synthetic multimodal embedding data in terms of task types. (2) With only 560K synthetic data, mmE5 manages to perform better than MMRet which uses 26M data. This proves the quality of our synthetic data again. (3) Intriguingly, mmE5 underperforms MMRet on retrieval tasks in a zero-shot setting. This is because MMRet is trained on 26M pure retrieval data, which makes it perform well on retrieval tasks, but generalizes poorly on other task types."}, {"title": "4.3 Multilingual Performance on XTD", "content": "We synthesize a multilingual multimodal dataset that consists of 93 languages, in order to train our embedding model mmE5 to generalize across more languages. The language distribution of our dataset is presented in Figure 3. Notably, the dataset primarily consists of English data samples, facilitating common usage scenarios. For the 75 low-resource languages, we evenly synthesize data samples to obtain a balanced multilingual dataset that supports comprehensive cross-linguistic generalization.\nTo evaluate the multilingual capability of mmE5, we conduct experiments across seven languages on a text-to-image retrieval benchmark XTD. As presented in Table 3, mmE5 outperforms other models in terms of overall performances on all languages, demonstrating its superior multilingual multimodal embedding capability. The following observations can be made: (1) The multilingual performance of multimodal embedding models is largely dependent on their foundational models. For example, jina-clip-v2 and M-CLIP outperform VLM2Vec-LLaVA, despite VLM2Vec\u2019s strong performance on MMEB. GME exhibits robust performance on XTD, which can be attributed to the powerful multilingual MLLM, Qwen2-VL (Wang et al., 2024b). (2) The performance of mmE5 declines when labeled data is omitted, indicating that general multimodal capabilities remain essential for multilingual retrieval tasks. (3) In a zero-shot setting, mmE5 trained on multilingual synthetic data (mmE5 w/ synthetic data only) outperforms mmE5 with the same amount of English synthetic data (mmE5 w/ english synthetic data). This suggests that the extensive language coverage provided by our synthetic data enhances the multilingual capabilities of embedding models."}, {"title": "4.4 Application to Other Base MLLM", "content": "We train mmE5 based on the powerful MLLM LLaMA-3.2-Vision, which is instruction-tuned and effective in interpreting multimodal inputs. Notably, our synthetic data and training paradigm can effectively transform other foundation MLLMs into embedding models. We use both our synthetic data and labeled data to train LLaVA-1.6 and Phi-"}, {"title": "4.5 Discussions of Data Synthesis Process", "content": "In this section, we will further investigate the data synthesis process via zero-shot experiments."}, {"title": "4.5.1 Ablation Studies", "content": "To evaluate each component of our data synthesis framework, we conduct ablation studies of mmE5:\nDeep Thinking Process To synthesize high-quality data, we introduce a deep thinking process to boost data synthesis. As presented in Table 5, the performance of mmE5 declines when the Visual Interpretation and Self-evaluation components are excluded. For example, mmE5 performs worse when utilizing the original data compared to revised data. This indicates that the self-evaluation mechanism can enhance data fidelity, facilitating the training of a more robust embedding model.\nEmbedding Task Types In order to expand the scope of data, we synthesize data across three task types: classification, VQA, and retrieval. The performance of mmE5 decreases after each type of multimodal embedding data is omitted, demonstrating that our diverse synthetic data can facilitate model generalization. Intriguingly, the performance drops the least after removing the retrieval data, which is inconsistent with previous research (Jiang et al., 2024b). One possible explanation is that our backbone, Llama-3.2 Vision, inherently exhibits more robust retrieval capabilities than Phi-3.5-V."}, {"title": "4.5.2 Scaling Effect", "content": "The scaling effect is an important aspect of synthetic data generation for multimodal embedding models (Zhang et al., 2024b; Zhou et al., 2024a). It explores how the performance of the model varies with the size of synthetic datasets. Besides, the data synthesis and training processes demand significant computational resources and time. Therefore, studying the scaling effect allows us to identify the point of diminishing returns, ensuring that resources are utilized efficiently without overproducing redundant data.\nIn this section, we further investigate the performance of mmE5 using synthetic datasets of varying sizes. Specifically, we conduct zero-shot experiments on MMEB to analyze the scaling effect. As illustrated in Figure 4, mmE5 consistently achieves better performance with increased training data, demonstrating the high quality of our synthetic data again. This paradigm also indicates a linear-log relationship between the model performance and data size, consistent with previous works of text embedding (Chen et al., 2024) and dense retrieval (Fang et al., 2024). This finding facilitates the balancing of the cost and the multimodal embedding model performance for future works."}, {"title": "4.6 Hyperparameter Analysis", "content": "In order to analyze the training process of our multimodal embedding model, we perform experiments with mmE5 using various training settings. For efficiency, we report zero-shot results for mmE5 trained with 280K synthetic data. Note that we tune these hyperparameters on evaluation datasets comprising 1K samples from each training set. However, for consistency with previous experiments, we present results on the MMEB test sets.\nLORA Rank denotes the rank of the additional low-rank matrices in LoRA. This parameter influences the number of parameters added into the original model, balancing the model\u2019s capacity and computational efficiency. As shown in the left part of Figure 5, the performance of mmE5 initially improves then drops. This demonstrates a trade-off: a lower rank reduces memory and computation but may lead to underfitting if r is too small, whereas a higher rank risks harming the pre-trained multimodal reasoning capabilities of MLLM.\nTraining Batch Size In contrastive learning, batch size plays a critical role because it directly affects the number of negative samples available for training. As presented in the middle part of Figure 5, the performance of mmE5 consistently increases with larger batch size. However, large batches demand significantly more GPU memory, i.e., more computational resources.\nTemperature The temperature parameter \\tau in the InfoNCE loss (Equation 1) influences the separation between positive and negative samples in the embedding space. We can observe that mmE5\u2019s performance first improves then declines with larger temperature. This pattern suggests a trade-off: a low \\tau forces the model to strongly penalize near-positive negatives which can lead to overfitting, while a high \\tau leads to a more uniform distribution of embeddings which may hinder the effective separation of positive and negative samples."}, {"title": "5 Conclusion", "content": "In this work, we synthesize high-quality multimodal multilingual data to train the model mmE5. We first define high-quality multimodal synthetic data based on three criteria: broad scope, robust cross-modal alignment, and high fidelity. Then, we develop a data synthesis framework guided by these principles. Finally, we train a multimodal multi-"}, {"title": "Limitations", "content": "Our work has several limitations that we intend to resolve in future research:\n1. Our model currently relies on the proprietary MLLM GPT-4o for synthesizing multimodal data. Future work should explore aligning smaller MLLMs with the knowledge from GPT-like models to achieve more efficient data synthesis.\n2. mmE5 focus on text and image modalities. Future models should aim to extend coverage to additional modalities, such as audio and video.\n3. Due to the cost limitation and the observed scaling effect, we limited the amount of data produced for model training. Future research may consider increasing data size while preserving diversity to optimize model performance."}, {"title": "Appendix", "content": "A Details about Synthetic Data\nIn this study, we introduce a synthetic multimodal multilingual embedding dataset designed to facilitate model learning. This section delves into the details of our synthetic dataset. The dataset is comprised of three distinct tasks and seven modality combinations, totaling 560K data samples. Table 6 provides a detailed statistical overview of our synthetic data, categorized by tasks and modalities.\nB Implementation Details"}, {"title": "B.1 Data Synthesis", "content": "For the data synthesis process, we employ the MLLM GPT-4o-2024-08-06 model to generate data samples. Both the temperature and top-p parameters are set to 1.0 to ensure diverse and coherent outputs. Our image corpus is sourced from LAION-400m (Schuhmann et al., 2021), from which we exclude images that are either corrupted or have inaccessible URLs. Each synthetic data sample incorporates one image sampled from this corpus as the query image. For modality combinations that include images on the document side, we utilize the jina-clip-v25 model to retrieve a similar image, along with a hard negative image, to serve as additional inputs."}, {"title": "B.2 Finetuning Embedding Model", "content": "We train mmE5 using the open-source MLLM, Llama-3.2-11B-Vision. The training is conducted on 64 NVIDIA A100 GPUs, each equipped with 40GB of memory. To optimize GPU memory usage, we employ gradient checkpointing and set the gradient accumulation steps to 4. The model is trained with a learning rate of 2e-5 for one epoch, utilizing both synthetic and labeled data. LORA (Hu et al., 2022) is applied to the MLLM with a rank of 8. Each training sample incorporates one hard negative document. Hard negatives are mined for each subset of MMEB using VLM2Vec-LORA7, with the 70th position in the ranking list selected as the hard negative sample.\nMore implementation details can be found in https://github.com/haon-chen/mmE5."}, {"title": "C Prompts", "content": "We use different prompts of data synthesis for different tasks. For retrieval task, we design two prompts for modality combinations that involve images on the document side or not. Let us take the prompt of generating classification data for an example to illustrate the prompt design.\nFirst, we sample a modality combination from {image-to-text, (image,text)-to-text}. If the query side does not include texts, the \"input_text\" of the classification data sample will be an empty string. Similarly, for modalities of retrieval task that do not include document texts, the \u201cpositive_document\u201d and \u201chard_negative_document\u201d will be empty. Following previous works of synthesizing text embedding data (Wang et al., 2024a; Chen et al., 2024), we will randomly select a clarity and difficulty setting to enhance diversity.\nThen, for the multi-aspect visual description process, we ask the MLLM to explicitly include four perspectives of description. Besides, for the data synthesis process, we also ask the MLLM to follow some specific guidelines. Furthermore, the MLLM will evaluate the initially generated data from several aspects and provide \u201cpossible_improvements\u201d. Finally, the revised version of data will be used as the output data sample. Note that there are no task instructions generated for the VQA task, since they are all fixed as \"Represent the given image with the following question:\"."}, {"title": "D Data Examples", "content": "In this section, we present the examples of the synthetic multimodal embedding data for Retrieval"}]}