{"title": "Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment", "authors": ["Allison Huang", "Yulu Niki Pi", "Carlos Mougan"], "abstract": "We explore how large language models (LLMs) can be influenced by prompting them to alter their initial decisions and align them with established ethical frameworks. Our study is based on two experiments designed to assess the susceptibility of LLMs to moral persuasion. In the first experiment, we examine the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally ambiguous scenarios and observing how a Persuader Agent attempts to modify the Base Agent's initial decisions. The second experiment evaluates the susceptibility of LLMs to align with predefined ethical frameworks by prompting them to adopt specific value alignments rooted in established philosophical theories. The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length. Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion. Code is available at https://github.com/acyhuang/moral-persuasion.", "sections": [{"title": "Introduction", "content": "As the capabilities of Large Language Models (LLMs) continue to advance, their potential application extends to more complex and ethically challenging tasks that can involve morally ambiguous scenarios. These scenarios often entail situations where decisions are not straightforward, and multiple courses of action can be justified depending on the ethical framework employed. Additionally, the growing development of more autonomous models-capable of using tools, making decisions, and operating independently-highlights the importance of understanding how these models might influence each other, especially in situations with ethical implications.\nPrevious research has predominantly focused on the safety considerations of LLM usage, particularly regarding the potential for misuse or the adverse social impacts these models might generate (Hendrycks et al., 2023; Phuong et al., 2024; Bommasani et al., 2021). However, an area that remains underexplored is the influence of persuasion between LLMs in moral scenarios where both actions might be deemed beneficial. To the best of our knowledge, there is no prior research that studies how susceptible LLMs are to persuasion from other models in moral contexts.\nAdditionally, morality is highly complex and dependent on context \u2013 human morals vary between individuals and across political and social groups. Philosophers and psychologists have proposed various ways to break down morality into distinct foundations or values to explain differences in human behaviour. Our work investigates how persuasion utilizing normative approaches to morality (classical moral philosophies and Gert's rules of common morality (Gert, 2004)) impacts LLMs, as"}, {"title": "Related Work", "content": "Moral Evaluation of LLMS Researchers have introduced methods to evaluate the moral values in LLMs across several dimensions: Hendrycks et al. (2021) measures knowledge of different moral philosophies. Pan et al. (2023) measures how models trade off between rewards and morally acceptable behavior. Bonagiri et al. (2024) measures the consistency of an LLM's moral beliefs, and Huang et al. (2023) measures toxicity, bias, and value-alignment. In our work we will focus on two: Scherrer et al. (2023) moral ambiguity evaluations and the Moral Foundation Theory (Graham et al., 2013).\nScherrer et al. (2023) introduces a statistical method for eliciting and evaluating beliefs encoded in large language models (LLMs) through a survey of moral scenarios. The authors design a large-scale survey comprising high and low-ambiguity moral scenarios based on the rules of common morality (Gert, 2004). The first experimental part of our work is designed as an LLM to LLM persuasion to change the original response on highly ambigous scenarios.\nMoral Foundations Theory (MFT) (Graham et al., 2013) provides a framework for understanding the diverse moral reasoning that underlies human behavior across cultures and political orientations (Haidt and Joseph, 2004; Graham et al., 2009). It posits that human morality is rooted in several innate psychological foundations, shaped by both evolutionary and cultural influences. MFT has been applied to LLMs by Abdulhai et al. (2023) and Simmons (2023), who have measured how well LLMs represent different political orientations through the five foundations. Additionally, Ji et al. (2024) builds a larger dataset meant for LLMs based on MFT. While this approach is valuable for categorizing moral values, MFT does not offer a structured methodology for decision-making in moral dilemmas where multiple values are in conflict. In our work, we aim to align LLMs with three of the most influential ethical theories-deontologist, utilitarianism, and virtue ethics-providing a clear and structured framework for evaluating their moral alignment using MFT metrics.\nPersuasion in LLMs Several studies have investigated the effect of LLM-generated persuasive text on humans (Salvi et al., 2024; Durmus et al., 2024; Phuong et al., 2024). For example, Ji et al. (2024) develops a method to quantify the persuasiveness of text by training a regression model on a dataset of ranked pairs of persuasive text. On the other hand, Xu et al. (2023) and Heitkoetter et al. (2024) have studied persuasion as a method of convincing LLMs of misinformation. Zeng et al. (2024) uses persuasion to produce harmful answers from an LLM. Payandeh et al. (2024) investigates whether LLMs are more susceptible to logical reasoning or logical fallacies.\nIn contrast to using persuasion to provoke unsafe behaviour in LLMs, we employ it as a methodology to study moral ambiguity within LLMs. Moreover, our approach involves minimal prompting to prime each LLM. While this approach controls for fewer variables in the generated text, it enables testing over multiple conversation turns, potentially offering partial generalizability to real-world interactions between LLM agents."}, {"title": "LLM-on-LLM Persuasion in Morally Ambiguous Decision-Making Scenarios", "content": "This experiment aims to investigate the influence of a (Persuader Agent) LLM on another LLM's decision-making process (Base Agent) when confronted with morally ambiguous scenarios. Specifically, the experiment aims to assess how susceptible the Base Agent is to persuasion, particularly when there is no clear morally correct action."}, {"title": "Methods", "content": "The experiment consists of two main stages:\nStage 1: Baseline evaluation The Base Agent is assessed using the moralchoice dataset (Scherrer et al., 2023) to establish an initial score on key metrics which later will serve to compare. Following the methodology outlined by Scherrer et al. (2023), we present each scenario in six semantically equivalent forms to account for model sensitivity to word phrasing and question forms. These results are placed in Appendix A.\nStage 2: Susceptibility to persuasion evaluation The two models engage in conversation for each scenario in the dataset. Both models are provided with the context and possible actions for the scenario. The Persauder Agent is specifically tasked with convincing the Base Agent to change its initial decision to the other action. The Base Agent is evaluated again on the dataset, but this time, the conversation history from the previous stage is included to assess any changes in its responses. See Table1 for prompts. We test the impact of two variables using this experiment:(i) Number of turns in conversation: We test whether increasing the length of the conversation by allowing each agent to contribute additional messages increases the impact of persuasiveness; (ii) LLM for agents: We evaluate how different LLMs perform as both the Persuader Agent and the Base Agent.\nData We use the moralchoice dataset from Scherrer et al. (2023), containing 680 high-ambiguity and 687 low ambiguity moral scenarios, each of which include context and two potential actions. Each scenario is based on a generation rule where one action violates the rule and the other action does not (\"Do not deceive\" and \"Do not kill\" (Gert, 2004)). However, both actions may violate other rules. The dataset contains auxiliary tags where annotators have labeled whether each action violates each rule (\u201cYes\u201d, \u201cNo\u201d, \u201cNo Agreement\"). We used 100 of the high ambiguity scenarios for this experiment.\nMetrics We have a dataset of survey questions, $D = {x_i}_{i=1}^K$, where each question $x_i = {d_i, A_i}$ consists of a scenario description $d_i$ and a set of action descriptions $A_i = {a_{i,k}}_{k=1}^K$. The \"survey respondent\" is an LLM (a probabilistic model) parameterized by $\\theta$, represented as $p_\\theta$, that generates a sequence of tokens $s$ in response to a given scenario $x_i$, with the probability distribution over these token sequences denoted by $p_\\theta (s | x_i)$.\nFollowing Scherrer et al. (2023), we define action likelihood as the probability of an LLM preferring action $a_{i,k}$ in scenario $x_i$. When presented with a description and two possible actions, the LLM returns a sequence $p_\\theta(s | x_i)$, which must be mapped to a corresponding action $a_{i,k}$. The action likelihood of model $p_\\theta$ on scenario $x_i$ is defined as:\n$p_\\theta(a_{i,k} | x_i) = \\sum_{s \\in C(a_{i,k})}p_\\theta(s | x_i), \\forall a_{i,k} \\in A_i,$"}, {"content": "where $C(a_{i,k})$ denotes the set of all token sequences that semantically encode a preference for action $a_{i,k}$. This mapping is achieved via a semantic equivalence relation, allowing the aggregation of probabilities for sequences with the same meaning (Kuhn et al., 2023). Following Scherrer et al. (2023) we sample M token sequences {$s_1,...,s_M$} from $p_\\theta(s | x_i)$ and map each to an action using a deterministic function $g : (x_i, s) \\rightarrow A_i$. The action likelihood is then approximated by:\n$p_\\theta(a_{i,k} | x_i) = \\frac{1}{M}\\sum_{m=1}^M\\mathbb{I}[g(s_m) = a_{i,k}], s_m \\sim p_\\theta(s | x_i).$\nThe mapping function $g$ is operationalized via stem matching with a set of potential answers.\nDefinition 3.1 (Change in Action Likelihood). Let $p_\\theta(a_{i,k} | x_i)$ and $p_\\theta(a_{i,k} | x'_i)$ denote the approximated action likelihoods for action $a_{i,k}$ in scenarios $x_i$ and $x'_i$, respectively. The Change in Action Likelihood (CAL) between these two scenarios for action $a_{i,k}$ is defined as:\n$CAL = \\frac{1}{N}\\sum_{(x_i,x'_i) \\in P} |p_\\theta(a_{i,k} | x_i) - p_\\theta(a_{i,k} | x'_i)|.$"}, {"content": "Where P is the set of all question pairs and N is the total number of such pairs.\nDefinition 3.2 (Decision Change Rate). Let $a_{i,k}^{pre}$ and $a_{i,k}^{post}$ denote the actions chosen by the LLM before and after the persuasive conversation for scenario $x_i$, respectively. A decision change occurs if $a_{i,k}^{pre} \\neq a_{i,k}^{post}$. The Decision Change Rate (DCR) is defined as the fraction of scenarios where a decision change occurs out of the total number of scenarios evaluated. Formally, it is given by:\n$DCR = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}[a_{i,k}^{pre} \\neq a_{i,k}^{post}],$\nwhere $\\mathbb{I}[x]$ is the indicator function that returns 1 if the condition inside is true and 0 otherwise\nDefinition 3.3 (Rule Violation Rate). The Rule Violation Rate (RVR) is calculated by translating the labels assigned to each action into specific values: {\"Yes\": 1.0, \"No\" : 0.0, \"No Agreement\" : 0.5}. For each scenario, we add up these values for both the initial and final actions taken by the model. The metric is then normalized using the highest possible total (i.e. if the model violated that rule every time it had the option to)."}, {"title": "Results", "content": "This experiment assesses the impact of conversation length on the action likelihood of the Base Agent. We conducted tests across conversations with 2, 4, 6, 8, and 10 turns. Our findings indicate that both the change in action likelihood and the percentage of decision changes increase slightly with additional turns (Figure 1). The smallest model (mistral-7b-instruct) does not follow this trend, and transcripts suggest that this is due to the agents deviating from their roles over longer conversations. Based on these results, we selected a four-turn conversation for the final evaluation, as further turns yielded only marginal improvements."}, {"title": "Evaluating Effectiveness and Susceptibility to Persuasion in LLMS", "content": "In this section, we explore the effectiveness of different LLMs combinations as both Base Agent and Persuader Agent. To quantify these effects, we analyze CAL and DCR across high-ambiguity scenarios of moralchoice dataset using the Persuader and Base Agent combinations of eight different LLMs. We find that LLMs are susceptible to persuasion in morally complex scenarios, with the effectiveness of persuasion varying significantly based on the model.\nFigure 3.2.2 indicates that claude-3-haiku and llama-3.1-8b are the most susceptible to persuasion. In aggregate, these two models change their original actions in almost half of the scenarios. In contrast, other models demonstrate greater resistance to persuasion and tend to maintain their initial decisions. To further validate these findings, we conducted additional tests using a low-ambiguity dataset, where persuasion proved to be largely ineffective. We tested a strong Persuader Agent,"}, {"title": "Influencing Moral Foundations Through Alignment Prompting", "content": "To explore how different ethical frameworks influence the models' moral foundations, we designed specific prompts aligned with three major ethical theories: utilitarianism, deontology, and virtue ethics. This follows a similar prompting strategy than Abdulhai et al. (2023) but with moral principles instead of political orientations. Each prompt directs the model to adopt a particular moral perspective when responding to the MFQ-30. See prompts in Table 2."}, {"title": "Results", "content": "In Figure 3, we observe the MFQ scores across different philosophical prompts for gpt-40, claude-3-haiku, and mistral-7b-instruct. We lack data for other models because they refused to provide answers for significant portions of the survey. Without any alignment prompts:\ngpt-40 exhibits a broader distribution across the moral foundations without any specific alignment prompts, indicating a well-rounded and generalized response pattern. When aligned with specific ethical frameworks, particularly utilitarianism, gpt-40 shows significant deviations in its moral foundation scores, suggesting a heightened responsiveness to utilitarian cues that strongly influence its moral reasoning.\nIn contrast, claude-3-haiku demonstrates relatively consistent scores across all prompts, reflecting a stable response pattern. The minimal variation across different ethical frameworks indicates that this model may be less sensitive to changes in ethical alignment prompts, suggesting a less flexible underlying moral reasoning compared to gpt-40."}, {"title": "Discussion", "content": "LLMs exhibit varying levels of susceptibility to persuasion in morally ambiguous scenarios. Models like claude-3-haiku and llama-3.1-8b were more easily influenced, while gpt-40 and claude-3.5-sonnet showed more resistance. Additionally, our study reveals that LLMs show much greater variation in how easily they can be persuaded than in their ability to persuade others. This finding underscores the complexity of persuasion dynamics between LLMs and suggests that certain models may be particularly vulnerable, raising important considerations for their use in morally sensitive applications. However, we lack clear predictors for susceptibility to persuasion. No model family is significantly stronger at persuasion or less susceptible to persuasion, and it is unclear whether model size correlates with susceptibility to persuasion. While the largest models are the least susceptible to persuasion, claude-3-haiku is by far the most susceptible model and its size is unknown, to the best of our knowledge.\nWe find that persuasion can change how models trade off between different moral values, though this occurs to different degrees for each model. claude-3.5-sonnet is the most consistent with respect to the rules of common morality, whereas claude-3-haiku and llama-3.1-70b see the largest changes in Rule Violation Rate. While we don't see clear patterns across models yet, this method may be promising for future work investigating how LLMs approach morally ambiguous scenarios.\nThe experimental results indicate that LLM can indeed be influenced to align with specific moral foundations through targeted ethical prompting. This finding is significant as it opens the possibility of LLMs to reflect specific ethical frameworks, depending on the requirements of the application domain. However, this also raises ethical concerns about the potential for bias and manipulation in the alignment process."}, {"title": "Prompts", "content": "Table 1: System prompts for each agent of Section 3. LLM-on-LLM Persuasion in Morally\nAmbiguous Decision-Making Scenarios"}, {"title": "Extended Experiments", "content": "We begin by evaluating the baseline action likelihood of the LLMs across various scenarios, aiming to reproduce the results from Scherrer et al. (2023) and establish the distribution of Action Likelihood prior to any persuasion attempts.\nIn both the handwritten and generated scenarios, we find that all models consistently prefer action1.\nMore important, we compare the distributions of both actions \u2013P(action1) and P(action2)- and\nobserve significant differences in these distributions for the generated scenarios see Figure 4. In\ntable 3 we provide the statistical testing whether if the distributions between action1 and action2\nare statistically significant."}]}