{"title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records", "authors": ["Daeun Kyung", "Junu Kim", "Tackeun Kim", "Edward Choi"], "abstract": "Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals to assess patient conditions and monitor changes over time. Generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic X-rays. However, these models mainly focus on conditional generation using single-time-point data, i.e., typically CXRs taken at a specific time with their corresponding reports, limiting their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. We demonstrate that our framework generates high-quality, realistic future images that capture potential temporal changes, suggesting its potential for further development as a clinical simulation tool. This could offer valuable insights for patient monitoring and treatment planning in the medical field.", "sections": [{"title": "I. INTRODUCTION", "content": "Chest X-ray imaging (CXR) is a crucial diagnostic tool employed in hospitals to evaluate a patient's health status and to track changes over time. Due to its cost-efficiency and low radiation dose, CXR is the most frequently performed imaging technique in hospitals. This widespread use makes building large-scale datasets relatively easy compared to other medical imaging methods, leading many studies to focus on training AI models using these X-ray datasets. However, existing datasets often suffer from issues such as class imbalances and noisy labels.\nGenerative models offer the potential to address these challenges by generating CXRs depicting specific diseases or modeling desired pathology locations and severity. The recent success of high-quality image generation in the general domain has inspired studies to create realistic synthetic CXRs in the medical field. Initial efforts [1]-[3] used Generative Adversarial Networks (GANs) [4] or VQ-VAE [5], which were state-of-the-art (SOTA) models for image generation. Recently, stable diffusion [6] models like RoentGAN [7] and Cheff [8] have been explored to generate high-fidelity medical images.\nExisting CXR generation models primarily focus on generating images with under-represented labels using conditional generation based on CXR labels [1], [2] or paired CXR reports [3], [7]-[10]. However, these methods are limited to synthesizing realistic CXRs at a single time point and cannot provide future images of specific patients. Thus, they only serve as data augmentation tools for addressing class imbalances, without leveraging the capability of CXRs to track the progression of a patient's condition over time. Given that CXR imaging is crucial not only for assessing current health status but also for monitoring disease progression, a new framework that incorporates temporal changes is needed.\nRecently, BiomedJourney [11] proposed a counterfactual medical image generation framework. Given two CXR reports taken at different time points for a specific patient, GPT-4 is used to synthesize the disease progression between the two CXRs. Then, BiomedJourney generates high-quality counterfactual images by using the first CXR image and the progression description from the text. However, this approach serves a different purpose, since it is not considering the temporal progression of patient status (i.e., generating the most likely future CXRs), but rather generating CXRs given an arbitrary progression scenario (e.g., what would the patient's CXR look like if he was to develop a new nodule?).\nElectronic Health Records (EHRs) are large-scale multimodal databases that encompass a patient's comprehensive medical history, including structured records such as diagnoses, procedures, and medications, along with imaging modalities like chest X-rays (CXR). Given the multimodal nature of EHRs, and the fact that doctors in clinical practice rely on both patient images and EHR tabular data for the decision-making process [12], [13], it is natural as well as potentially useful to predict changes in a patient's condition by combining previous images with subsequent medical history.\nIn this paper, we introduce a novel task focused on predicting future CXRs by combining previous CXRs with subsequent events from the EHR table data, and present EHRXDiff, the first latent diffusion-based model to perform this task. This approach offers a dynamic view of a patient's temporal changes, tracking their condition based on medical events such as medication or treatment starting from the initial status of the patient (i.e., the previous CXR image). We comprehensively evaluate the quality of the predicted CXRs of our proposed model. Specifically, our framework shows potential as a simulation tool for healthcare professionals by outperforming baselines in tracking patients' changing states over time."}, {"title": "II. METHODOLOGY", "content": "A. Task Definition\nGiven the previous CXR image $I_{prev}$ and a sequence of medical events $S_{event} = (e_1, e_2, ..., e_{n_{event}})$, the model predicts the target CXR image $I_{trg}$. We work with the triple $(I_{prev}, S_{event}, I_{trg})$, where $I_{prev}, I_{trg} \\in R^{3\\times H \\times W}$, and $S_{event}$ consists of table events $e_i$ (e.g., prescriptions, lab tests) occurring at time $t_i$ between the two imaging times. Here, $H$ and $W$ are the input image resolution, and $i \\in \\{1, ..., N_{event}\\}$, with $n_{event}$ indicating the total number of events. Our approach is more challenging than the previous works [7], [8], [10] since it incorporates the patient's medical history rather than relying on simple text descriptions. The model must recognize the initial state from the previous image and predict the patient's status based on the provided history.\nB. Encoder Modules for CXR and Medical events Integration\nTo predict future CXR images based on previous CXR images and consecutive medical events, the model must (1) produce realistic, high- quality CXR images and (2) accurately reflect the potential clinical status based on prior images and medical histories. To achieve this, the framework includes three encoders: a VAE encoder, and two CLIP"}, {"title": "C. Diffusion Models for EHR based CXR prediction", "content": "We use the state-of-the-art image generation model, Latent Diffusion Model (LDM) [6], as our backbone. The LDM consists of the following two components: 1) Variational autoencoder (VAE) with encoder $E_{VAE}$ and decoder $D_{VAE}$, which projects the image to a lower-dimensional latent representation $z$ and then reconstructs the image from $z$. 2) Conditional denoising U-Net $\\epsilon_{\\theta}$ that iteratively denoises an initially randomly generated latent vector. The objective function of a text-conditioned LDM is defined as:\n$L_{LDM} = E_{z_t \\sim N(0,1), t} [|| \\epsilon_t - \\epsilon_{\\theta}(z_t, t, C_{text}) ||^2]$   (1)"}, {"title": "III. EXPERIMENTS", "content": "A. Experimental Settings\nImplementation Details. We implemented our LDM model based on Weber et al. [8]. We used a VAE encoder pretrained with MaCheX [8]. For the CLIP encoder, we employed ViT-B/32 [14] for images and a 2-layer transformer encoder for tables. The table transformer is configured with a maximum input length of 1024, a feature dimension of 1536, and 24 attention heads. The input image resolution is 256 \u00d7 256. We set $C_z$, $H_f$, and $W_f$ to 3, 64, and 64,\n*Both datasets are publicly accessible through the PhysioNet platform (https://physionet.org/)"}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduce EHRXDiff, a novel framework designed to predict future CXR images by integrating previous CXRs with subsequent medical events. We comprehensively validate EHRXDiff's performance, demonstrating its potential to capture temporal changes in CXR findings, as well as its ability to generate high-quality, realistic CXR images. These findings suggest the potential for further development as a clinical simulation tool, providing valuable insights for patient monitoring and treatment planning in the medical field. However, there is still room for improvement compared to the GT, especially in retaining fine-grained details. In future work, we aim to improve performance through more precise feature sampling guided by medical experts or by incorporating other modalities, such as text."}]}