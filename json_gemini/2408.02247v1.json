{"title": "Contrastive Learning and Abstract Concepts: The Case of Natural Numbers", "authors": ["Daniel N. Nissani (Nissensohn)"], "abstract": "Contrastive Learning (CL) has been successfully applied to classification and other downstream tasks related to concrete concepts, such as objects contained in the ImageNet dataset. No attempts seem to have been made so far in applying this promising scheme to more abstract entities. A prominent example of these could be the concept of (discrete) Quantity. CL can be frequently interpreted as a self-supervised scheme guided by some profound and ubiquitous conservation principle (e.g. conservation of identity in object classification tasks). In this introductory work we apply a suitable conservation principle to the semi- abstract concept of natural numbers by which discrete quantities can be estimated or predicted. We experimentally show, by means of a toy problem, that contrastive learning can be trained to count at a glance with high accuracy both at human as well as at super-human ranges.. We compare this with the results of a trained-to-count at a glance supervised learning (SL) neural network scheme of similar architecture. We show that both schemes exhibit similar good performance on baseline experiments, where the distributions of the training and testing stages are equal. Importantly, we demonstrate that in some generalization scenarios, where training and testing distributions differ, CL boasts more robust and much better error performance.", "sections": [{"title": "INTRODUCTION AND RELATED WORK", "content": "Contrastive Learning (CL) is a self supervised scheme which has attracted much attention in recent years. In the visual modality realm it maps a visual input (e.g. objects to be classified) to linearly separable representations which achieve classification accuracy rates competitive with those of supervised learning (SL) networks of similar architecture (Chen et al., 2020) in challenging datasets such as ImageNet (Deng et al., 2009). In spite of its impressive success in the space of concrete concepts (Chen et al., 2020), neither CL nor its variants (Grill et al., 2020; He et al., 2020; Chen & He, 2020) have been apparently applied so far to the learning and prediction of abstract or semi- abstract entities. In a recent work (Nissani (Nissensohn), 2023) has shown that CL can (unlike SL) build \"hyper-separable\" representations which are useful not only to predict an object identity but also to indicate the existence (or absence) of selected attributes of interest of this object; this might be seen as a first modest step away from the concrete and towards the abstract. Another prominent example of the learning, 'grounding', or in-depth 'understanding' of such an abstract entity could be that of the concept of natural numbers (equivalently, discrete quantities). This work is a preliminary and introductory step forward in this direction.\nCL exploits, in the concrete visual modality, a profound principle of conservation: that distinct views of an object preserve the identity of said object. To create such distinct views suitable transformations should (and generally can) be designed (Tian et al., 2020). Analog principles of conservation have been applied in physics (e.g. conservation of energy, of momentum, etc. under suitable reference frames transformations) with extraordinary success during the last two centuries. To apply a relevant and useful transformation to our (discrete) quantity prediction challenge within a CL scheme we can imagine the following thought experiment: that we have objects which we wish to count at a glance (that is \"count without counting\","}, {"title": null, "content": "see ahead), that these objects lay at the bottom of a closed box with transparent cover. Shaking the box, our transformation, will randomly change the layout of the objects inside the box, but the total number of objects on the box floor will be conserved (since the box is closed, etc.).\nOur CL optimization goal l(i, j) will then consist of minimizing the normalized distance (Wang & Isola, 2020) between the neural network so called projection (i.e. last) layer (Chen et al., 2020) representations $z_i$ and $z_j$ of the pre- and post-shaking views ('positive samples' in the CL jargon) while simultaneously maximizing the distances between these representations and the representations of other samples randomly gathered in a mini-batch ('negative samples'). Formally, our goal will then be (see Chen et al., 2020 for details):\n$\\begin{equation} l(i, j) = \\frac{\\text{sim}(z_i, z_j)}{\\tau} \\end{equation}$\n$\\begin{equation} + \\log \\left( \\sum_{k=1}^{2N} \\textbf{1}_{k \\neq i} \\text{exp} \\left( -\\frac{\\text{sim}(z_i, z_k)}{\\tau} \\right) \\right) \\end{equation}\n$\\begin{equation} L = - \\sum_{k=1}^{N} [l(2k - 1, 2k) + l(2k, 2k - 1)] \\end{equation}\nwhere sim(.,.) is the normalized inner product (i.e. the cosine similarity), $\\tau$ is a system temperature, N is the mini-batch size, $\\textbf{1}_{k \\neq i}$ is a binary indicator function which vanishes when k = i and equals 1 otherwise, and L is the overall loss, i.e. l(i, j) summed over all samples in the mini-batch. At the end of the optimization process we freeze the neural network learned parameters, fetch an interior layer and define its output to be our linearly separable representation vectors. These are then fed into a (usually supervised, single layer) linear classifier.\nWe humans are able to estimate at a glance, with high precision and with no explicit enumeration, a relatively small (up to between 4 to 7) number of objects in view (Trick & Pylyshyn, 1994). This capability, for which the special term 'subitizing' was coined (Kaufman et al., 1949) has motivated in recent years a few groups of neural networks practitioners (Chattopadhyay et al, 2017; Acharya et al., 2018) to explore the application of supervised learning (SL) schemes to a similar challenge.\nWe are not aware of any similar work on natural numbers under the umbrella of CL, nor of the application of CL schemes to other non-tangible concepts.\nAs a preliminary introduction to these ideas we implement a toy problem and corresponding datasets by means of which CL and SL networks of similar architecture are trained to subitize and predict the quantity of identical objects present in an image. We compare the error performance of these two schemes at both a 'baseline' regime, where training and testing data originate from identical distributions, as well as at various generalization regimes, where training and testing data distributions differ. CL is trained by a more profound, potentially 'grounding', criterion than SL, a criterion that is intimately related to the concept of Quantity itself. We thus may well suspect that it will exhibit error performance better than that of SL, at least in some of the forementioned generalization regimes.\nare:\nThe main contributions of this introductory work\na. We demonstrate through the example of the Quantity (equivalently, Natural Numbers) non-tangible concept, that CL can learn abstract concepts whenever transformations which conserve their defining properties can be identified and implemented.\nb. We show that both CL and SL can learn to subitize, both at human range (order of 10 objects) and super-human range (order of 100). Moreover, that the error performance of CL and SL are similarly good under the forementioned baseline regime scenarios.\nc. We show that CL exhibits significant error performance superiority over SL under certain generalization regime scenarios, possibly corroborating, as suspected, the more profound and grounded nature of learning by means of a conservation principle (relative to SL learning which consists of merely forcing representations to adjust to arbitrary labels).\nIn Section 2 the toy problem and test setup which we introductorily employ to demonstrate these ideas are presented. Section 3 details our simulation results, and Section 4 provides concluding remarks and outlines potential future lines of research."}, {"title": "EXPERIMENTAL SETUP", "content": "We implement a toy problem to conduct experiments, demonstrate our ideas and probe into our CL superiority conjecture. We generate (practically infinite) sequences of random synthetic images of dimension d x d pixels, with d = 22 or 28. Each image contains a number of identical white objects laid out over a black background. For each experiment we select a training dataset distribution {O, S, R} and a testing dataset distribution {O', S', R'} so we designate an experiment by the composite triplet {O/O', S/S', R/R'}.\nBaseline experiments consist of triplets where O = O', S = S' and R = R'; otherwise they are Generalization experiments wherein the training and testing distributions differ.\nO is the maximal number of objects in an image and took in our experiments the values 10 (to emulate human range), 60 or 80 (for super-human); the number of objects in an image is uniformly randomly selected within a subset of [1, O].\nS defines the shape of all objects within an image and, for the sake of some shape diversity, can take the values F, V, C, D or X (see Figure 1 for details).\nR can take the values A (for All), E (for Evens) or O (for Odds) where A means that the number of objects in an image can be any in the range [1, O] while E or O restricts this number to the subset of even or odd values respectively.\nA typical experiment may consist of {O/O', S/S', R/R'} = {80/80, D/D, A/A} which means identical training and testing distribution (i.e. a Baseline experiment) with between 1 to 80 objects in each image, each object of Dot shape, and no Evens nor Odds only restriction.\nThe objects are randomly laid out within the image. We do not allow objects to occlude nor touch each other. Each sample image is generated along with its label which describes the number of objects in that sample.\nWe train a simple fully connected multi layer neural network by CL with architecture [d\u00b2 400 400 100] where the last layer, of dimension 100, is the (so called) projection layer (Chen et al., 2020) and the penultimate layer, of dimension 400 is our representation layer.\nWe use for SL an identical architecture, except for the last layer which for SL contains a softmax activation function of dimension O. Note that we could instead have chosen to implement a regression (e.g. linear) scheme for SL. We opted for the former since it supports a more apples to apples' comparative study.\nIt is not so simple to physically emulate the \"shaking of our box\" (of our thought experiment above) in order to create a new image sample with the same number of objects within it but with a different random objects layout. Instead, we opt for a surrogate: we take the label attached to each image (which describes the number of objects in that image) and use it to generate another (random layout) image with this specified number of objects."}, {"title": null, "content": "This may seem at first glance a cheating perversion by which we convert our CL (unsupervised learning) scheme into a supervised learning one (since we are now using labels for our 'transformation'). After some reflection however, it should be easy to conclude that this is immaterial to our purpose. The same results we are going to show can be exactly replicated by a true physical shaking of our box.\nTraining of CL or SL proceeded for a number of samples until no further visible convergence progress is observed in the Loss goal (Eqns. (1) and (2) above for CL) or in the training Mean Squared Error goal (for SL). Once training is halted we freeze the CL (or SL) parameters, and train a single layer supervised linear classifier (Oord et al., 2018) with the CL (or SL) network representations generated by the testing dataset distribution. After this linear classifier is trained, we evaluate error performance with samples generated by means of this same selected test dataset distribution (as mentioned above our datasets are practically infinite so we are not, by no means and as properly prohibited, re-using samples for both training and testing).\nWe set the CL hidden layers neural units activation to ReLU and the last layer units to tanh. CL temperature ($\\tau$ of Equation 1) was set to 1, and batch size (N of Equation 2) was set to 10\u00b3. We used ADAM (Kingma & Ba, 2015) gradient descent with fixed learning rate $\\eta$ = 10\u207b\u00b3. The number of training samples ranged from 1.8 x 10\u2076 (equivalent to e.g. 30 MNIST epochs) to 3.6 x 10\u2076 depending on the running experiment.\nSL units were here again set to ReLU for hidden layers but to softmax activation for the last layer. We used ADAM with fixed learning rate $\\eta$ = 5 x 10\u207b\u2074 for the initial stage followed by plain gradient descent with learning rate typically descending from 10\u207b\u2075 to 10\u207b\u2077. We found this protocol necessary to achieve our best error performance, possibly testifying the navigation within a deep narrow valley (Martens, 2010). The number of SL training samples ranged from 4.2 x 10\u2076 to 10.8 x 10\u2076.\nFinally, the linear classifier was trained by ADAM followed by plain SGD, with similar optimization protocol as that of SL above and number of samples typically ranging from 2.4 x 10\u2076 to 28.8 x 10\u2076.\nAcross all three above schemes ADAM parameters were set to $\\beta_1$ = 0.9, $\\beta_2$ = 0.999 and $\\epsilon$ = 10\u207b\u2078.\nTo facilitate replication of our results a simulation package will be provided by the author upon request."}, {"title": "SUBITIZING EXPERIMENTAL RESULTS", "content": "In this Section we will be comparing CL vs. SL subitizing error performance. To probe whether our forementioned conjecture, which states that CL will boast superior performance w.r.t. SL in at least some of the Generalization scenarios (where training and test distributions differ) we define a set of appropriate experiments. We also provide baseline results (where training and testing distributions are identical) for reference.\nGeneralization here can be applied along the shape dimension, or the quantity dimension, or both at a time (which we do not pursue herein). For convenience we list below the experiments (generalized item bold and underlined; please refer to Section 2 for triplets legend):\nShape Dimension (human range; train with mixed images, test with Dots):\nBaseline: {O/O', S/S', R/R'} =\n{10/10, X/X, A/A}\nGeneralization: {O/O', S/S',\nR/R'} = {10/10, X/D, A/A}\nQuantity Dimension, Range extension (super human range; train with up to 60 objects, test with up to 80):\nBaseline: {O/O', S/S', R/R'} =\n{60/60, D/D, A/A}\nGeneralization: {O/O', S/S',\nR/R'} = {60/80, D/D, A/A}\nQuantity Dimension, within Range (super human range; train with up to 80 objects, Even values only, test with up to 80, All values):\nBaseline: {O/O', S/S', R/R'} =\n{80/80, D/D, E/E}\nGeneralization: {O/O', S/S',\nR/R'} = {80/80, D/D, E/A}\nQuantity Dimension, within Range (super human range; train with up to 80 objects, Even values only, test with up to 80, Odd values only):\nBaseline: {O/O', S/S', R/R'} =\n{80/80, D/D, E/E}\nGeneralization: {O/O', S/S',\nR/R'} = {80/80, D/D, E/O}"}, {"title": "Shape Dimension (human range; train with mixed images, test with Dots)", "content": "We report $Pr^{CL, B}{error}$ = 0.042 and $Pr^{SL, B}{error}$ = 0.007 for Baseline test (denoted by B in super- script), i.e. {O/O', S/S', R/R'} = {10/10, X/X, A/A}, for CL and SL respectively.\nThat SL yields better performance than CL under baseline tests with similar networks architecture is no surprise: it was noticed in prior works, see e.g. (Chen et al. 2020) where in order to achieve similar error performance a deeper and wider architecture (on CL network relative to SL) was required.\nInterestingly, errors are not uniformly distributed across ground truth values at both CL and SL (see Figure 2): as might be intuitively expected error rate steadily increases from small to large ground truth values (with 10 a possible curious exception; this may be an artifact of our setup: since we set a hard limit of 10 through our softmax function, errors at this ground truth value are contributed, unlike at other values, by one side only). A qualitatively similar phenomenon was observed in subitizing experiments conducted with human subjects (Kaufman et al., 1949). We did not observe however a similar trend with our experiments at super human range.\nWe next turn to the corresponding Shape Generalization test, i.e. {O/O', S/S', R/R'} = {10/10, X/D, A/A} where training is conducted with mixed images (which, as described above, contain either F or V or C objects but do not contain Dots) but testing is done with Dots images. Error probability significantly degrades, with similar degradation at CL and SL: $Pr^{CL, G}{error}$ = 0.31 and $Pr^{SL, G}{error}$ = 0.30 (superscript G here denotes Generalization).\nMore important perhaps however in a quantity estimation task than this gross error measure is the conditional distribution of Distances (conditioned on error events), where Distance = |Ground Truth value - Predicted value : in practical situations to predict a value of 8 instead of a ground truth value 9 is forgivable while to predict a 1 in the same case is not.\nThis more suitable metric can be observed in Figure 3: we notice that in spite the significant forementioned $Pr{error}$ degradation, the distribution of Distance in both CL and SL is remarkably concentrated in the very low values, with $Pr^{CL, G}{Distance > 1 | error}$ = 0.036 and $Pr^{SL, G}{Distance > 1 | error}$ = 0.041, again pretty similar to each other.\nTo summarize, in this Shape Generalization experiment CL and SL perform similarly; they both degrade significantly in terms of Pr{error}, but their more tolerant (and perhaps relevant) Pr{Distance > 1 | error} metric is still pretty good.\nWe have provided this Figure to contrast vs. next experiments, as we will immediately see."}, {"title": "Quantity Dimension, Range Extension (super human range; train up to 60 objects, test with up to 80)", "content": "Here we report $Pr^{CL, B}{error}$ = 0.029 and $Pr^{SL, B}{error}$ = 0.004 for the Baseline test, i.e. {O/O', S/S', R/R'} = {60/60, D/D, A/A}, for CL and SL respectively. Since the Distance distribution metrics is apparently of more significant relevance in subitizing tasks we also report for the Baseline test $Pr^{CL, B}{Distance > 1 | error}$ = 0.035 and $Pr^{SL, B}{Distance > 1 | error}$ = 0.002.\nthe contrastive learning conservation principle, compared to the merely forced mapping of input images to arbitrary labels in SL. The concept of Numbers seems to become 'grounded' in CL but not so in SL.\nThe corresponding Quantity Generalization test is {O/O', S/S', R/R'} = {60/80, D/D, A/A} where training is conducted with up to 60 Dots images and testing is done with up to 80 Dots images. Again, as in the Shape Generalization experiment, error probability significantly degrades, with similar degradation at CL and SL: $Pr^{CL, G}{error}$ = 0.24 and $Pr^{SL, G}{error}$ = 0.23.\nConditional Distance distributions of CL and SL, however, are totally different. Please refer to Figure 4. While CL Distance distribution is concentrated in the very low values (as in our earlier experiments) and $Pr^{CL, G}{Distance > 1 | error}$ = 0.12, SL distribution practically explodes with $Pr^{SL, G}{Distance > 1 | error}$ = 0.80.\nBriefly summarizing SO far, in shape generalization experiments (Subsection 3.1 above) both CL and SL behave similarly: this is reasonable since it is not shape what CL learns in depth and thus they should not differ much from each other, just as in baseline tests.\nIn contrast, in quantity generalization scenarios, CL and SL respond extremely differently and CL significantly outperforms SL. This can be attributed in our view to the profoundness to which CL learns the abstract concept of \"numberness\" by means of"}, {"title": "Quantity Dimension, within Range (super human range; train with up to 80 objects, Even values only, test with up to 80, All values)", "content": "Next we report $Pr^{CL, B}{error}$ = 0.019 and $Pr^{SL, B}{error}$ = 0.0004 as well as $Pr^{CL, B}{Distance > 2 | error}$ = $Pr^{SL, B}{Distance > 2 | error}$ = 0 for the Baseline test of this experiment, i.e. {O/O', S/S', R/R'} = {80/80, D/D, E/E}, for CL and SL respectively. Notice that conditional Distance distribution results here are reported w.r.t. a threshold valued 2 (rather than 1 as before) since Odds \"do not exist\" in this Baseline scenario.\nThe corresponding Quantity Generalization test is {O/O', S/S', R/R'} = {80/80, D/D, E/A} where training is conducted with even quantities (i.e. 2, 4, 6...80) of up to 80 Dots images, and testing is done with all quantities (i.e. 1, 2, 3....80) of up to, again, 80 Dots images. SL degrades its error probability w.r.t. baseline here, while CL this time exhibits even better performance than its own baseline: $Pr^{SL, G}{error}$ = 0.045 and $Pr^{CL, G}{error}$ = 0.00003.\nEvaluating again with our conditional Distance distribution metrics (please refer to Figure 5), CL significantly outperforms SL here too with $Pr^{CL}{Distance > 1 | error}$ = 0 and $Pr^{SL, G}{Distance > 1 | error}$ = 0.285, supporting once again our motivating conjecture."}, {"title": "Quantity Dimension, within Range (super human range; train with up to 80 objects, Even values only, test with up to 80, Odd values only)", "content": "Our baseline setup here is identical to that of the last experiment, i.e. {O/O', S/S', R/R'} = {80/80, D/D, E/E}, for CL and SL respectively, and so are our results. The conditional Distance distributions results here (for both baseline and generalization tests) are once again reported w.r.t. a threshold valued 2 (rather than 1) for the same reason as in Subsection 3.3 above.\nThe corresponding Quantity Generalization test is {O/O', S/S', R/R'} = {80/80, D/D, E/O} where training is conducted with even quantities (i.e. 2, 4, 6...) of up to 80 Dots images and testing is done with odd quantities (i.e. 1, 3, 5...79) of up to 79 Dots images. Both CL and SL exhibit good error probability with $Pr^{CL, G}{error}$ = 0.026 and $Pr^{SL, G}{error}$ = 0.026 and both show excellent Distance conditionals with $Pr^{CL, G}{Distance > 2 | error}$ = $Pr^{SL, G}{Distance > 2 | error}$ = 0.\nIt appears that once CL and SL learn the Evens the discriminative resolution of the resulting representations is good enough to predict the Odds with good error performance, For the SL scheme however, this holds only provided that the Evens are omitted from the testing dataset, so that they do not 'confuse' the SL network as apparently occurred in our previous Subsection 3.3 experiment."}, {"title": "DISCUSSION AND TOPICS FOR FUTURE RESEARCH", "content": "We have provided preliminary demonstrative evidence, through our selected example of quantity estimation at a glance, that contrastive learning methods can deal not only with concrete, tangible concepts. Our choice however of natural numbers is not casual. When dealing with concrete objects it is not difficult to identify transformation sets, with random properties, which efficiently span the distribution of a dataset; examples include crop and color distortion for ImageNet (Chen et al., 2020) and elastic distortion (Simard et al, 2003) for EMNIST (Nissani (Nissensohn), 2023). This is possibly not the case for abstract concepts in general, and identifying viable abstract sets and their corresponding spanning random transformations is a research challenge. It would be of interest to see"}]}