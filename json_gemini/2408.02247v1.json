{"title": "Contrastive Learning and Abstract Concepts:\nThe Case of Natural Numbers", "authors": ["Daniel N. Nissani (Nissensohn)"], "abstract": "Contrastive Learning (CL) has been successfully applied to classification and other downstream tasks\nrelated to concrete concepts, such as objects contained in the ImageNet dataset. No attempts seem to have\nbeen made so far in applying this promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted as a self-supervised scheme\nguided by some profound and ubiquitous conservation principle (e.g. conservation of identity in object\nclassification tasks). In this introductory work we apply a suitable conservation principle to the semi-\nabstract concept of natural numbers by which discrete quantities can be estimated or predicted. We\nexperimentally show, by means of a toy problem, that contrastive learning can be trained to count at a\nglance with high accuracy both at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural network scheme of similar\narchitecture. We show that both schemes exhibit similar good performance on baseline experiments, where\nthe distributions of the training and testing stages are equal. Importantly, we demonstrate that in some\ngeneralization scenarios, where training and testing distributions differ, CL boasts more robust and much\nbetter error performance.", "sections": [{"title": "INTRODUCTION AND\nRELATED WORK", "content": "Contrastive Learning (CL) is a self supervised\nscheme which has attracted much attention in recent\nyears. In the visual modality realm it maps a visual\ninput (e.g. objects to be classified) to linearly\nseparable representations which achieve\nclassification accuracy rates competitive with those\nof supervised learning (SL) networks of similar\narchitecture (Chen et al., 2020) in challenging\ndatasets such as ImageNet (Deng et al., 2009).\nIn spite of its impressive success in the space of\nconcrete concepts (Chen et al., 2020), neither CL nor\nits variants (Grill et al., 2020; He et al., 2020; Chen\n& He, 2020) have been apparently applied so far to\nthe learning and prediction of abstract or semi-\nabstract entities. In a recent work (Nissani\n(Nissensohn), 2023) has shown that CL can (unlike\nSL) build \"hyper-separable\" representations which\nare useful not only to predict an object identity but\nalso to indicate the existence (or absence) of selected\nattributes of interest of this object; this might be seen\nas a first modest step away from the concrete and\ntowards the abstract. Another prominent example of\nthe learning, 'grounding', or in-depth\n'understanding' of such an abstract entity could be\nthat of the concept of natural numbers (equivalently,\ndiscrete quantities). This work is a preliminary and\nintroductory step forward in this direction.\nCL exploits, in the concrete visual modality, a\nprofound principle of conservation: that distinct\nviews of an object preserve the identity of said\nobject. To create such distinct views suitable\ntransformations should (and generally can) be\ndesigned (Tian et al., 2020). Analog principles of\nconservation have been applied in physics (e.g.\nconservation of energy, of momentum, etc. under\nsuitable reference frames transformations) with\nextraordinary success during the last two centuries.\nTo apply a relevant and useful transformation to\nour (discrete) quantity prediction challenge within a\nCL scheme we can imagine the following thought\nexperiment: that we have objects which we wish to\ncount at a glance (that is \"count without counting\","}, {"title": null, "content": "see ahead), that these objects lay at the bottom of a\nclosed box with transparent cover. Shaking the box,\nour transformation, will randomly change the layout\nof the objects inside the box, but the total number of\nobjects on the box floor will be conserved (since the\nbox is closed, etc.).\nOur CL optimization goal l(i, j) will then consist\nof minimizing the normalized distance (Wang &\nIsola, 2020) between the neural network so called\nprojection (i.e. last) layer (Chen et al., 2020)\nrepresentations $z_i$ and $z_j$ of the pre- and post-shaking\nviews ('positive samples' in the CL jargon) while\nsimultaneously maximizing the distances between\nthese representations and the representations of other\nsamples randomly gathered in a mini-batch\n('negative samples'). Formally, our goal will then be\n(see Chen et al., 2020 for details):\n$\\displaystyle l(i, j) = \\frac{\\text{sim}(z_i, z_j)}{\\tau}$\n$\\displaystyle  + \\log \\sum_{k=1}^{2N} \\exp{\\big(-\\frac{\\text{sim}(z_i, z_k)}{\\tau}\\big)}$ (1)\n$\\displaystyle L = - \\sum_{k=1}^{N}[l(2k - 1, 2k) + l(2k, 2k - 1)]$ (2)\nwhere sim(.,.) is the normalized inner product (i.e.\nthe cosine similarity), t is a system temperature, N\nis the mini-batch size, $1_{k\\neq i}$ is a binary indicator\nfunction which vanishes when k = i and equals 1\notherwise, and L is the overall loss, i.e. l(i, j)\nsummed over all samples in the mini-batch. At the\nend of the optimization process we freeze the neural\nnetwork learned parameters, fetch an interior layer\nand define its output to be our linearly separable\nrepresentation vectors. These are then fed into a\n(usually supervised, single layer) linear classifier.\nWe humans are able to estimate at a glance, with\nhigh precision and with no explicit enumeration, a\nrelatively small (up to between 4 to 7) number of\nobjects in view (Trick & Pylyshyn, 1994). This\ncapability, for which the special term 'subitizing'\nwas coined (Kaufman et al., 1949) has motivated in\nrecent years a few groups of neural networks\npractitioners (Chattopadhyay et al, 2017; Acharya et\nal., 2018) to explore the application of supervised\nlearning (SL) schemes to a similar challenge.\nWe are not aware of any similar work on natural\nnumbers under the umbrella of CL, nor of the\napplication of CL schemes to other non-tangible\nconcepts.\nAs a preliminary introduction to these ideas we\nimplement a toy problem and corresponding datasets\nby means of which CL and SL networks of similar\narchitecture are trained to subitize and predict the\nquantity of identical objects present in an image. We\ncompare the error performance of these two schemes\nat both a 'baseline' regime, where training and\ntesting data originate from identical distributions, as\nwell as at various generalization regimes, where\ntraining and testing data distributions differ. CL is\ntrained by a more profound, potentially 'grounding',\ncriterion than SL, a criterion that is intimately\nrelated to the concept of Quantity itself. We thus\nmay well suspect that it will exhibit error\nperformance better than that of SL, at least in some\nof the forementioned generalization regimes.\nare:\nThe main contributions of this introductory work\na. We demonstrate through the example of the\nQuantity (equivalently, Natural Numbers)\nnon-tangible concept, that CL can learn\nabstract concepts whenever transformations\nwhich conserve their defining properties can\nbe identified and implemented.\nc.\nb. We show that both CL and SL can learn to\nsubitize, both at human range (order of 10\nobjects) and super-human range (order of\n100). Moreover, that the error performance\nof CL and SL are similarly good under the\nmentioned baseline regime scenarios.\nWe show that CL exhibits significant error\nperformance superiority over SL under\ncertain generalization regime scenarios,\npossibly corroborating, as suspected, the\nmore profound and grounded nature of\nlearning by means of a conservation\nprinciple (relative to SL learning which\nconsists of merely forcing representations to\nadjust to arbitrary labels).\nIn Section 2 the toy problem and test setup which\nwe introductorily employ to demonstrate these ideas\nare presented. Section 3 details our simulation\nresults, and Section 4 provides concluding remarks\nand outlines potential future lines of research."}, {"title": "EXPERIMENTAL SETUP", "content": "We implement a toy problem to conduct\nexperiments, demonstrate our ideas and probe into\nour CL superiority conjecture. We generate\n(practically infinite) sequences of random synthetic\nimages of dimension d x d pixels, with d = 22 or 28.\nEach image contains a number of identical white\nobjects laid out over a black background. For each\nexperiment we select a training dataset distribution"}, {"title": "SUBITIZING EXPERIMENTAL\nRESULTS", "content": "In this Section we will be comparing CL vs. SL\nsubitizing error performance. To probe whether our\nforementioned conjecture, which states that CL will\nboast superior performance w.r.t. SL in at least some\nof the Generalization scenarios (where training and\ntest distributions differ) we define a set of\nappropriate experiments. We also provide baseline\nresults (where training and testing distributions are\nidentical) for reference.\nGeneralization here can be applied along the\nshape dimension, or the quantity dimension, or both\nat a time (which we do not pursue herein). For\nconvenience we list below the experiments\n(generalized item bold and underlined; please refer\nto Section 2 for triplets legend):\n\u2022\nShape Dimension (human range; train\nwith mixed images, test with Dots):\n\u039f\nBaseline: {O/O', S/S', R/R'} =\n{10/10, \u03a7/\u03a7, \u0391/\u0391}\nGeneralization: {O/O', S/S',\nR/R'} = {10/10, X/D, A/A}\nQuantity Dimension, Range extension\n(super human range; train with up to 60\nobjects, test with up to 80):\n\u2022\n\u039f\n\u2022\nBaseline: {O/O', S/S', R/R'} =\n{60/60, D/D, A/A}\nGeneralization: {O/O', S/S',\nR/R'} = {60/80, D/D, A/A}\nQuantity Dimension, within Range\n(super human range; train with up to 80\nobjects, Even values only, test with up\nto 80, All values):\n\u039f\n\u2022\n\u039f\nBaseline: {O/O', S/S', R/R'} =\n{80/80, D/D, E/E}\nGeneralization: {O/O', S/S',\nR/R'} = {80/80, D/D, E/A}\nQuantity Dimension, within Range\n(super human range; train with up to 80\nobjects, Even values only, test with up\nto 80, Odd values only):\n\u039f\nBaseline: {O/O', S/S', R/R'} =\n{80/80, D/D, E/E}\nGeneralization: {O/O', S/S',\nR/R'} = {80/80, D/D, E/O}"}, {"title": "Shape Dimension (human range;\ntrain with miXed images, test with\nDots)", "content": "We report PrCL, B {error} = 0.042 and PrSL, B{error} =\n0.007 for Baseline test (denoted by B in super-\nscript), i.e. {O/O', S/S', R/R'} = {10/10, X/X, A/A},\nfor CL and SL respectively.\nThat SL yields better performance than CL under\nbaseline tests with similar networks architecture is\nno surprise: it was noticed in prior works, see e.g.\n(Chen et al. 2020) where in order to achieve similar\nerror performance a deeper and wider architecture\n(on CL network relative to SL) was required.\nInterestingly, errors are not uniformly distributed\nacross ground truth values at both CL and SL (see\nFigure 2): as might be intuitively expected error rate\nsteadily increases from small to large ground truth\nvalues (with 10 a possible curious exception; this\nmay be an artifact of our setup: since we set a hard\nlimit of 10 through our softmax function, errors at\nthis ground truth value are contributed, unlike at\nother values, by one side only). A qualitatively\nsimilar phenomenon was observed in subitizing\nexperiments conducted with human subjects\n(Kaufman et al., 1949). We did not observe however\na similar trend with our experiments at super human\nrange.\nWe next turn to the corresponding Shape\nGeneralization test, i.e. {O/O', S/S', R/R'} = {10/10,\nX/D, A/A} where training is conducted with mixed\nimages (which, as described above, contain either F\nor Vor Cobjects but do not contain Dots) but\ntesting is done with Dots images. Error probability\nsignificantly degrades, with similar degradation at\nCL and SL: PrCL, G{error} = 0.31 and PrSL, G{error} =\n0.30 (superscript G here denotes Generalization).\nMore important perhaps however in a quantity\nestimation task than this gross error measure is the\nconditional distribution of Distances (conditioned on\nerror events), where Distance = |Ground Truth value\n- Predicted value : in practical situations to predict a\nvalue of 8 instead of a ground truth value 9 is\nforgivable while to predict a 1 in the same case is\nnot.\nThis more suitable metric can be observed in\nFigure 3: we notice that in spite the significant\nforementioned Pr{error} degradation, the\ndistribution of Distance in both CL and SL is\nremarkably concentrated in the very low values,\nwith PrCL G{Distance > 1 | error} = 0.036 and\nPrSL G{Distance > 1 | error} = 0.041, again pretty\nsimilar to each other.\nTo summarize, in this Shape Generalization\nexperiment CL and SL perform similarly; they both\ndegrade significantly in terms of Pr{error}, but their\nmore tolerant (and perhaps relevant) Pr{Distance > 1\n| error} metric is still pretty good.\nWe have provided this Figure to contrast vs. next\nexperiments, as we will immediately see."}, {"title": "Quantity Dimension, Range\nExtension (super human range;\ntrain up to 60 objects, test with up\nto 80)", "content": "Here we report PrCL, B{error} = 0.029 and\nPrSL, B {error} = 0.004 for the Baseline test, i.e.\n{O/O', S/S', R/R'} = {60/60, D/D, A/A}, for CL\nand SL respectively. Since the Distance distribution\nmetrics is apparently of more significant relevance\nin subitizing tasks we also report for the Baseline\ntest PrCL B {Distance > 1 | error} = 0.035 and\nPrSL B {Distance > 1 | error} = 0.002."}, {"title": null, "content": "the contrastive learning conservation principle,\ncompared to the merely forced mapping of input\nimages to arbitrary labels in SL. The concept of\nNumbers seems to become 'grounded' in CL but not\nso in SL.\nThe corresponding Quantity Generalization test\nis {O/O', S/S', R/R'} = {60/80, D/D, A/A} where\ntraining is conducted with up to 60 Dots images and\ntesting is done with up to 80 Dots images. Again, as\nin the Shape Generalization experiment, error\nprobability significantly degrades, with similar\ndegradation at CL and SL: PrCL, G{error} = 0.24 and\nPrSL, G{error} = 0.23.\nConditional Distance distributions of CL and SL,\nhowever, are totally different. Please refer to Figure\n4. While CL Distance distribution is concentrated in\nthe very low values (as in our earlier experiments)\nand PrCL G{Distance > 1 | error}\n= 0.12, SL\ndistribution practically explodes with\nPrSL G{Distance > 1 | error} = 0.80.\nBriefly summarizing SO far, in shape\ngeneralization experiments (Subsection 3.1 above)\nboth CL and SL behave similarly: this is reasonable\nsince it is not shape what CL learns in depth and\nthus they should not differ much from each other,\njust as in baseline tests.\nIn contrast, in quantity generalization scenarios,\nCL and SL respond extremely differently and CL\nsignificantly outperforms SL. This can be attributed\nin our view to the profoundness to which CL learns\nthe abstract concept of \"numberness\" by means of"}, {"title": "Quantity Dimension, within Range\n(super human range; train with up\nto 80 objects, Even values only, test\nwith up to 80, All values)", "content": "= 0.019 and\nNext we report PrCL, B{error}\nPrSL, B{error} = 0.0004 as well as PrCL B{Distance >\n2 | error} = PrSL B {Distance > 2 | error} = 0 for the\nBaseline test of this experiment, i.e. {O/O', S/S',\nR/R'} = {80/80, D/D, E/E}, for CL and SL\nrespectively. Notice that conditional Distance\ndistribution results here are reported w.r.t. a\nthreshold valued 2 (rather than 1 as before) since\nOdds \"do not exist\" in this Baseline scenario.\nThe corresponding Quantity Generalization test is\n{O/O', S/S', R/R'} = {80/80, D/D, E/A} where\ntraining is conducted with even quantities (i.e. 2, 4."}, {"title": null, "content": "6...80) of up to 80 Dots images, and testing is done\nwith all quantities (i.e. 1, 2, 3....80) of up to, again,\n80 Dots images. SL degrades its error probability\nw.r.t. baseline here, while CL this time exhibits even\nbetter performance than its own baseline:\nPrSL G{error} = 0.045 and PrCLG {error} = 0.00003.\nEvaluating again with our conditional Distance\ndistribution metrics (please refer to Figure 5), CL\nsignificantly outperforms SL here too with PrCL\n{Distance > 1 | error} = 0 and PrSL G{Distance > 1 |\nerror} = 0.285, supporting once again our motivating\nconjecture."}, {"title": "Quantity Dimension, within Range\n(super human range; train with up\nto 80 objects, Even values only, test\nwith up to 80, Odd values only)", "content": "Our baseline setup here is identical to that of the last\nexperiment, i.e. {O/O', S/S', R/R'} = {80/80, D/D,\nE/E}, for CL and SL respectively, and so are our\nresults. The conditional Distance distributions\nresults here (for both baseline and generalization\ntests) are once again reported w.r.t. a threshold\nvalued 2 (rather than 1) for the same reason as in\nSubsection 3.3 above.\nThe corresponding Quantity Generalization test\nis {O/O', S/S', R/R'} = {80/80, D/D, E/O} where\ntraining is conducted with even quantities (i.e. 2, 4.\n6...) of up to 80 Dots images and testing is done\nwith odd quantities (i.e. 1, 3, 5...79) of up to 79\nDots images. Both CL and SL exhibit good error\nprobability with PrCL G{error}"}, {"title": "DISCUSSION AND TOPICS\nFOR FUTURE RESEARCH", "content": "We have provided preliminary demonstrative\nevidence, through our selected example of quantity\nestimation at a glance, that contrastive learning\nmethods can deal not only with concrete, tangible\nconcepts. Our choice however of natural numbers is\nnot casual. When dealing with concrete objects it is\nnot difficult to identify transformation sets, with\nrandom properties, which efficiently span the\ndistribution of a dataset; examples include crop and\ncolor distortion for ImageNet (Chen et al., 2020) and\nelastic distortion (Simard et al, 2003) for EMNIST\n(Nissani (Nissensohn), 2023). This is possibly not\nthe case for abstract concepts in general, and\nidentifying viable abstract sets and their\ncorresponding spanning random transformations is a\nresearch challenge. It would be of interest to see"}]}