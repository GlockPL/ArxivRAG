{"title": "Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines", "authors": ["Junyu Lai", "Jiahe Xu", "Yao Yang", "Yunpeng Huang", "Chun Cao", "Jingwei Xu"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing and reasoning tasks. However, their performance in the foundational domain of arithmetic remains unsatisfactory. When dealing with arithmetic tasks, LLMs often memorize specific examples rather than learning the underlying computational logic, limiting their ability to generalize to new problems. In this paper, we propose a Composable Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute step-by-step computations by emulating Turing Machines, thereby gaining a genuine understanding of computational logic. Moreover, the proposed framework is highly scalable, allowing composing learned operators to significantly reduce the difficulty of learning complex operators. In our evaluation, CAEF achieves nearly 100% accuracy across seven common mathematical operations on the LLaMA 3.1-8B model, effectively supporting computations involving operands with up to 100 digits, a level where GPT-40 falls short noticeably in some settings.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have made significant strides in recent years, showcasing extraordinary capabilities across a range of natural language processing (NLP) tasks [Dubey et al., 2024, Jiang et al., 2024, Chowdhery et al., 2023], and in some cases, even surpassing human performance in specific benchmarks [Achiam et al., 2023]. However, despite these advancements, LLMs still face significant challenges in performing arithmetic. Current research indicates that when presented with arithmetic problems, LLMs often rely on memorizing specific expressions and their corresponding outcomes rather than grasping the fundamental logic of arithmetic operations [Wu et al., 2023]. This inherent limitation poses a substantial barrier to their effective application in fields that demand essential computational skills.\nTo enhance the performance of LLMs in solving arithmetic problems, two primary approaches have been developed. The first approach positions the LLM as an agent that relies on an external calculator to perform computations [Hao et al., 2024, Ruan et al., 2023]. In this setting, the LLM's role is limited to providing the operands and invoking the appropriate operations. Although this method effectively simplifies the challenge of arithmetic for LLMs, it misses the opportunity for the models to learn computational logic, preventing LLMs from comprehending the underlying principles of arithmetic. Given that arithmetic serves as the foundation of mathematics, the lack of understanding may significantly impede the LLM's ability to grasp more complex mathematical concepts. The second approach focuses on stimulating the LLM's intrinsic capabilities, employing prompt engineering or fine-tuning techniques to enable the model to master arithmetic computations"}, {"title": "2 Approach: Framework Design", "content": "Computational logic is fundamental to arithmetic. To truly master arithmetic, the LLM should learn and execute the underlying computational logic of arithmetic operations rather than merely memorizing examples of arithmetic expressions. For scalability, the LLM should be capable of constructing new operators by combining existing operators. For example, after learning Addition operation, the LLM could construct Multiplication by understanding the computational logic of repeated addition could achieve multiplication.\nTherefore, we need a framework that enables LLM to model arithmetic operators by learning to execute their underlying computational logic. In the field of automata, the Turing machine provides a suitable framework for describing this logic. Numerous well-established Turing machines exist for common arithmetic operations, which we can reference to create adequate datasets of execution steps for LLM training. Furthermore, the Turing machine inherently supports the combination of multiple Turing machines, making it ideal for constructing complex operations from existing ones. By emulating Turing machines, LLM can be designed to integrate multiple models, enabling it to execute more intricate arithmetic tasks."}, {"title": "2.2 LLM executes as Turning machine", "content": "A Turing machine can be formally defined as a septuple $T = (Q, \\Sigma, \\Gamma, b, q_0, F, \\delta)$, where Q is a finite set of states, $\\Sigma \\subseteq \\Gamma$ is a finite alphabet for input, $\\Gamma$ is a finite tape alphabet, $b \\in \\Gamma$ is the blank symbol, $q_0 \\in Q$ is the initial state, $F \\subseteq Q$ is a set of final states, and $\\delta$ is the transition function. When a Turing machine is in a non-halting state, the next action is determined by both the current symbol on the tape and the machine's current state. In each action, the machine updates the symbol on the tape, transitions to a new state, and moves the tape head either left or right. This process repeats iteratively until the machine reaches a halting state, at which point the computation is complete, and the result is saved on the tape.\nLLM is the generative model for text-based language, so how to transfer all information from a Turing machine to the LLM effectively is challenging. A tailored representation system is necessary for LLMs to accurately understand computational logic. To facilitate this transfer, the system must incorporate states analogous to those of the Turing machine, such as the machine state and tape state, to indicate the current status of the computation, in other words, the step in the execution process. Additionally, the system should include commands that specify the actions to be executed based on the current state to ensure correct transitions to the next state. Thus, CAEF provides a text-based"}, {"title": "2.3 Representation design", "content": "Computational logic typically operates within a structuralized abstract representation. As illustrated in Figure 2, representation of the arithmetic problem includes: 1) a status indicating the current step of the computation, and 2) a \"tape\" that records all operands and essential information, such as the number of digits processed, any carryover during addition, and other intermediate results. To facilitate the LLM's learning of the execution process, the representation in CAEF explicitly includes the commands e required for execution. These commands involve the call to the next state s and other detailed actions, such as carrying over or moving the pointer. All the above elements are represented in text, which is friendly to LLM to understand. Then, to make LLM execute based on the representation, CAEF structures the input as < si, Ci > for current step, while the output is < Si+1, Ci+1 > for the next stepi+1.\nBesides modeling the representation, representation translation is another critical part of CAEF. In general, the original input of an arithmetic problem does not include the initial state or the first command to execute. Moreover, upon completing the computation, the raw output remains in the representation format. Thus, CAEF incorporates an aligner to manage the bidirectional translation between the original input/output and the representation. The aligner can also be implemented by fine-tuning a specific LoRA adapter. Notice that one key feature of the aligner should learn the ability to convert the left-to-right (L2R) representation of numbers into a right-to-left (R2L) format, as R2L is evaluated more effectively for LLM to operate the operands [Lee et al., 2023]."}, {"title": "3 Approach: Implementation", "content": "Building on the conceptual design of CAEF, we present the detailed implementation of Equation 1, highlighting two key derived components: basic executors and executor composers with examples."}, {"title": "3.1 Fine-tuning process and implementation design", "content": "CAEF offers a framework to enhance the arithmetic capabilities of LLMs. To implement Equation 1 for a specific arithmetic task, CAEF involves the following steps: 1) step 1: design a state machine and implement the representation < si, Ci > for the arithmetic task, and 2) step 2: sample pairs of input and output to create a dataset, which is then used to fine-tune the LLM for one-step execution.\nStep 1. Designing a state machine can draw from existing Turing machines or other relevant state machines for the task. To implement state si and commands c\u2081 in the representation, we transform the structured representation into plain text following two main guidelines: 1) numbers are formatted in R2L order, separated by l, and 2) each command is expressed in the format {[CMD] action}. For example, for the addition task 45 + 67 = where the current step involves adding the tens digits with a carry of 1 from the units place, the representation < Si, Ci > may include several pointers: two HEADs pointing to the digits, a carry C for the carry value, and OUTPUT to record the results. All these pointers are moved using the RIGHT command. The representation is written as follows:\nsi: ADD, q1, | 5[HEAD1]| 4 | 7[HEAD2]| 6 [C]1 | 2[OUTPUT]\nci: CMD: [C] 1, [OUTPUT] 1, [OUTPUT] RIGHT, [HEAD1] RIGHT, [HEAD2] RIGHT, q1\nwhere q1 indicates the current status, and all pointers are presented in uppercase, enclosed in brackets with the pointed value on their right.\nStep 2. To fine-tune the LLM, the dataset, including input and output representation pairs used for learning one-step execution. Continuing with the example, we create the representation < Si+1, Ci+1 > for the output of the one-step execution based on the above < Si, Ci >:\nSi+1: ADD, q1, | 5| 4[HEAD1] | 7| 6[HEAD2] [C]1 | 2| 1[OUTPUT]\nCi+1: CMD: [OUTPUT] 1, [OUTPUT], [C], qH\nwhere qH is the halting status. The details of the dataset refer to Section 4.1.\nOne efficient way for LLM to learn for one-step execution is LoRA fine-tuning. Since we target to learn +, -, \u00d7, \u00f7, >, <, and == arithmetic operators, implementing multiple LLM instances leads to significant memory overhead. To mitigate this, we use a single base LLM model with multiple LoRA adapters that serve as learned executable arithmetic operators. Switching LoRA adapters based on function calls generated by the LLM can efficiently perform various operations, optimizing memory usage while maintaining flexibility in handling different arithmetic computations.\nTo implement a specific computational task, CAEF introduces two types of executors (i.e., basic executor and the executor composer) to learn to execute under the proposed representation. The basic executor is designed to handle tasks with well-defined computational logic, while the executor composer acts as a higher-level controller that orchestrates the process by calling other basic executors. In the following, we introduce the two types of executors through illustrative examples."}, {"title": "3.2 Basic executors", "content": "We use addition to illustrate the design of a basic executor. A natural way to implement addition is to imitate the accumulator, performing the addition of two corresponding digits from the operands once"}, {"title": "3.3 Executor composers", "content": "Executor composer designs to orchestrate the basic executors into intricate computational logic. Instead of performing computations directly, the executor composer \"calls\" other basic executors in a specific sequence to accomplish more complex tasks.\nMultiplication is a typical example of the executor composer, which can be implemented by calling the + and < basic executors. CAEF uses two accumulators (+ involved) to implement a \u00d7 b. The first accumulator increments by 1 with each loop iteration, while the second adds a during each iteration. This process continues until the first accumulator reaches b (< involved), and then the value in the second accumulator represents the final result. LLM is fine-tuned to execute control flow and loops, by calling the < executor and, based on its result, either halts or continues the loop. Figure 4 illustrates the computation process for 89 \u00d7 2 using our implementation. Since the executor composer decouples the computational logic into several executors, the fine-tuning process could be done separately for each executor, showing the ability of executor composition.\nBesides multiplication, we also design subtraction (considering only non-negative results) and division (floor division) executor composers using similar methodologies. Specifically, we draw inspiration from how subtraction is handled in CPUs to construct the subtraction executor composer and the detailed implementation can be found in Appendix A.4."}, {"title": "4 Evaluation", "content": null}, {"title": "4.1 Setting", "content": "Models. We utilize the LLaMA 3.1-8B pretrained model (non-instruct version) as the base model. During LoRA fine-tuning, all linear modules in the decoder layer are involved in training, with the hyperparameters fixed at r = 8, a = 16, and a learning rate of 5 \u00d7 10\u20135. The fine-tuning process is conducted in two stages. In the first stage, we introduce an exhaustive explanation in the prompt, detailing the computation goal of an executor, the required input/output format, and providing an example. This explanation is followed by the actual sample, as illustrated in Appendix A.1. In the second stage, we remove the long explanation from the prompt and present only the sample, expecting the model to predict the next state and the subsequent commands directly. We use batch sizes of 8 and 16 for the first and second stages, respectively. All experiments are conducted on a server equipped with six H800 GPUs. The code and the models are available2.\nBaseline. We compare our approach against three baselines on +, -, \u00d7, \u00f7, ==, >, and < operators. The first is a LLM fine-tuned with LoRA on LLaMA 3.1-8B (non-instruct version). Additionally, we include two original LLMs, GPT-40 and LLaMA 3.1-8B Instruct, both of which directly generate the computational results based on the arithmetic expressions. The prompts used for these models are in Appendix A.5.\nDataset. In CAEF, an operator requires an executor and an aligner, each supported by a specific LORA adapter. To generate training datasets for these adapters, we implement a Turing machine prototype for each operator. For the executor, we generate random arithmetic expressions and run the Turing machine from its initial state until it halts, recording states and commands before and"}, {"title": "4.2 Main Results", "content": "Table 1 presents the evaluation results of our method and baseline models across the seven operators. Compared to the baselines, the proposed approach performs stably on all operators with high accuracy. Specifically for tasks with long numbers, such as 100-digit addition, LLM with CAEF effectively learns the computational logic to execute the addition process.\nTo further explore the actual performance of the executor and aligner during the computation process, we separately evaluate their accuracy on the same dataset. As the results shown in Table 2, we observe"}, {"title": "5 Related Work", "content": "LLMs in Mathematical Contexts. Prior research has focused on enhancing LLM performance in mathematical tasks, often relying on external tools for calculations and primarily addressing math word problems rather than pure arithmetic. A common external tool is a calculator, as exemplified by Schick et al. [2024], which introduces a self-supervised method where the model learns when to call external tools via API access. Similar strategies can be found in Gou et al. [2023] and He-Yueya et al. [2023]. Another tool is a programming language interpreter, such as Python, where the model generates code and an external interpreter executes it to obtain the result. A representative example is Lu et al. [2024], which treats the LLM as a planner that generates code and submits it to an external Python executor to handle math problems in tabular contexts. Wang et al. [2023] employs supervised fine-tuning to improve code-based problem-solving, while Zhou et al. [2023] proposes a zero-shot prompting method to enable code-driven self-verification, thereby improving mathematical performance.\nLLMs in Arithmetic Scenarios. Another series of work focuses solely on arithmetic, which we consider directly related to our research. The common characteristic of these studies is their effort to teach LLMs computational logic and improve calculation accuracy through step-by-step processes. Among these works, Nye et al. [2021] is an early and far-reaching study, predating the popular Chain-of-Thought (CoT) approach. It introduces a similar idea in the arithmetic domain, where the language model outputs intermediate steps to a buffer called a \"scratchpad,\" significantly improving performance in integer addition. Hu et al. [2024] observes that transformers tend to approach arithmetic problems using \"case-based reasoning\" and proposes a Rule-Following Fine-Tuning technique that guides the model to execute calculations step by step. Zhou et al. [2024] combines four techniques (i.e., FIRE position encodings, Randomized position encodings, Reversed format (R2L format), and Index hints) to develop a new model that achieves a 2.5\u00d7 improvement in length generalization for two-integer addition."}, {"title": "6 Limitations", "content": "Prone to errors with repeated digit patterns. Both the executor and the aligner tend to generate incorrect steps when encountering patterns of repeated digits, such as sequences like \"999...\" where a single digit repeats, or \"456456...\" where multiple digits repeat. These errors typically manifest as extra or missing repetitions of the pattern. While this issue might be mitigated by intentionally generating more such expressions to increase the representation of similar samples in the training set, we believe the root cause lies in limitations inherent to generative LLMs.\nEfficiency Issue. In our method, completing a single computation requires generating the full sequence of intermediate steps, which essentially means repeatedly calling the model.generate() function. For computations involving hundreds of steps, this process can be extremely time-consuming. One potential solution lies in optimizing the use of the KV cache. In our approach, the input to the LLM at two consecutive steps is highly similar. However, since different parts of the input shift position, the KV cache from the previous step cannot be effectively reused. The KV cache functions like a ROM. If we could transform it into a RAM-like structure that supports simple editing operations, such as swapping adjacent tokens while maintaining the correct tokens and positional embeddings, this could significantly improve computational efficiency.\nImplementation of the Turing machine prototype. When generating the training set for the executor, CAEF wants to ensure the correctness of the samples and enable the executor to learn key computational steps, such as carrying over or exiting loops. A practical approach is to construct"}, {"title": "7 Conclusion", "content": "This paper proposes a framework that enables LLMs to learn to execute step-by-step arithmetic computational logic by imitating the behavior of a Turing machine. This approach significantly enhances LLMs' computational capability without relying on any external tools. Moreover, the framework is highly scalable, allowing the construction of complex executors by composing learned basic executors, reducing the difficulty of understanding the complex logic. We hope that our work provides a new perspective for enabling LLMs to learn rule-based computation."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Example of Samples in Training Set of Executor and Aligner", "content": null}, {"title": "A.1.1 Addition", "content": "Addition executor:\nInput:\nThe following is a state paired with a command to be executed of a Turing Machine that\nperforms addition.\nThe state includes the current operator, the current state of the machine, the current tape\ncontents, and the current head positions.\nThere are three states in the machine: q0, q1, and qH. The machine starts in state q0 and\nhalts when it reaches state qH. q1 is the state where the machine does the addition and\ncalculates the carry out.\nThe head positions are represented by [HEAD1] and [HEAD2], which indicate the positions\nof the heads on the two operands.\nThe carry out is represented by [C].\nThe output position is represented by [OUTPUT].\nThe command includes a series of actions to be executed by the machine and they are\nseparated by commas.\n- [OUTPUT] <number>: Write the number to the output position.\n[OUTPUT] <direction>: Move the output head to the direction.\n- [C] <number>: Write the number to the carry out register.\n[HEAD1] <direction>: Move the head on the first operand to the direction.\n- [HEAD2] <direction>: Move the head on the second operand to the direction.\n<state>: Move the machine to the state.\nThe machine performs addition by reading the digits from the two operands and writing the\nsum to the output tape.\nBased the current state and the command, predict the next state of the machine and next\ncommand to be executed.\nADD, q0, [HEAD1] |5|4[HEAD2] |7|6 [C] [OUTPUT]\nCMD: [C] 0, [HEAD1] RIGHT, [HEAD2] RIGHT, q1\nOutput:\nADD, q1, [HEAD1]|5|4 [HEAD2]|7|6 [C]0 [OUTPUT]\nCMD: [C] 1, [OUTPUT] 2, [OUTPUT] RIGHT, [HEAD1] RIGHT, [HEAD2] RIGHT, q1\nAddition aligner:\nInput:\nThe following is an input to a Turing Machine or an output of a Turing Machine.\nThe task is doing an adaptation:\n- If it is an input, adapt the original input to the format that the Turing Machine can understand.\n- If it is an output, adapt the original output to the format that represents the final result."}, {"title": "A.1.2 Subtraction", "content": "Subtraction executor:\nInput:\nThe following is a input to be executed of a Turing Machine that performs subtraction.\nTo solve a subtraction problem by the machine, the machine is required to provide the initial\nstate and command for other basic machines, including addition, reflection and left mask.\nFor example, for 47819 - 12345 = 35474, the machine will perform the following steps:\n- step 1: call reflection, 99999 - 12345 = 87654\n- step 2: call addition, 47819 + 87654 = 135473\n- step 3: call addition, 135473 + 1 = 135474"}, {"title": "A.1.3 Multiplication", "content": "Multiplication executor:\nInput:\nThe following is a input to be executed of a Turing Machine that performs multiplication.\nTo solve a multiplication problem by the machine, the machine is required to provide the\ninitial state and command for other basic machines, including addition and less_than machines.\nFor example, for 4513 * 3 = 13539, the machine will perform the following algorithm:\n- step 1: cnt = 1, sum = 4513(oprand1)\n- step 2: call less_than, determine whether cnt <3(oprand2), if yes, go to step 3, otherwise, go\nto step 5\n- step 3: call addition, sum = sum + 4513(oprand1)\n- step 4: call addition, cnt = cnt + 1, go to step 2\n- step 5: current machine halts\nThe input includes at least two lines and may have two more lines.\nThe first line is the current state of the machine.\nThe second line is the command to be executed.\nWhen there are two more lines:\nThe third line and the fourth line are halt state of another machine which is called by the\nmultiplication machine at previous step."}, {"title": "A.1.4 Division", "content": "Division executor:\nInput:\nThe following is a input to be executed of a Turing Machine that performs division.\nTo solve a division problem by the machine, the machine is required to provide the\ninitial state and command for other basic machines, including addition and greater_than\nmachines.\nFor example, for 4513 // 1504 = 3, the machine will perform the following algorithm:\n- step 1: output = 0, cnt = 1504(oprand2)\n- step 2: call greater_than, determine whether cnt >4513(oprand1), if yes, go to step 5,\notherwise, go to step 3\n- step 3: call addition, output = output + 1\n- step 4: call addition, cnt = cnt + 1504, go to step 2\n- step 5: current machine halts, output is the result\nThe input includes at least two lines and may have two more lines.\nThe first line is the current state of the machine.\nThe second line is the command to be executed.\nWhen there are two more lines:\nThe third line and the fourth line are halt state of another machine which is called by the\ndivision machine at previous step."}, {"title": "A.1.5 Greater_than", "content": "Greater than executor:\nInput:\nThe following is a state paired with a command to be executed of a Turing Machine that\ndetermines whether the first operand is greater than the second operand.\nThe state includes the current operator, the current state of the machine, the current\ntape contents, and the current head positions.\n- There are three states in the machine: q0, q1, and qH. The machine starts in state q0 and\nhalts when it reaches state qH. q1 is the state where the machine does the comparison.\n- The head positions are represented by [HEAD1] and [HEAD2], which indicate the positions\nof the heads on the two operands.\nThe output position is represented by [OUTPUT].\nThe command includes a series of actions to be executed by the machine and they\nare separated by commas.\n[OUTPUT] <number>: Write the number to the output position.\n[OUTPUT] <direction>: Move the output head to the direction.\n[HEAD1] <direction>: Move the head on the first operand to the direction.\n- [HEAD2] <direction>: Move the head on the second operand to the direction.\n<state>: Move the machine to the state.\nThe machine performs comparison by reading the digits from the two operands and\nwriting the result to the output tape.\nBased on the current state and the command, predict the next state of the machine\nand next command to be executed."}, {"title": "A.1.6 Less_than", "content": "Less_than executor:\nInput:\nThe following is a state paired with a command to be executed of a Turing Machine that\ndetermines whether the first operand is less than the second operand.\nThe state includes the current operator, the current state of the machine, the current\ntape contents, and the current head positions.\n- There are three states in the machine: q0, q1, and qH. The machine starts in state q0 and\nhalts when it reaches state qH. q1 is the state where the machine does the comparison.\n- The head positions are represented by [HEAD1] and [HEAD2], which indicate the positions\nof the heads on the two operands."}, {"title": "A.1.7 Equal", "content": "Equal executor:\nInput:\nThe following is a state paired with a command to be executed of a Turing Machine that\nperforms equality comparison.\nThe state includes the current operator, the current state of the machine, the current\ntape contents, and the current head positions.\nThere are three states in the machine: q0, q1, and qH. The machine starts in state q0 and halts\nwhen it reaches state qH. q1 is the state where the machine does the equality comparison.\nThe head positions are represented by [HEAD1] and [HEAD2], which indicate the positions\nof the heads on the two operands.\nThe output position is represented by [OUTPUT].\nThe command includes a series of actions to be executed by the machine and they\nare separated by commas.\n[OUTPUT] <number>: Write the number to the output position.\n[OUTPUT] <direction>: Move the output head to the direction.\n[HEAD1] <direction>: Move the head on the first operand to the direction.\n- [HEAD2] <direction>: Move the head on the second operand to the direction.\n<state>: Move the machine to the state.\nThe machine performs equality comparison by reading the digits from the two operands and\nwriting the result to the output tape.\nBased on the current state and the command, predict the next state of the machine\nand next command to be executed."}, {"title": "A.2 Arithmetic Expression Template", "content": "Templates in Table 3 are used for generate arithmetic expressions in our experiment."}, {"title": "A.3 Full Computation Process of the Examples", "content": "The followings are the full computation process of the examples in 3.2 and 3.3."}, {"title": "A.3.1 Addition", "content": "Step 1 (aligner):\n45+67=\nStep 2 (executor):\nstate: ADD, q0, [HEAD1] |5|4[HEAD2] |7|6 [C] [OUTPUT]\ncommando: CMD: [C] 0, [HEAD1] RIGHT, [HEAD2] RIGHT, q1\nStep 3 (executor):\nstate1: ADD, q1, [HEAD1]|5|4 [HEAD2]|7|6 [C]0 [OUTPUT]\ncommand1: CMD: [C] 1, [OUTPUT] 2, [OUTPUT] RIGHT, [HEAD1] RIGHT, [HEAD2]\nRIGHT, q1\nStep 4 (executor):\nstate2: ADD, q1, |5[HEAD1]|4 |7[HEAD2]|6 [C]1 |2[OUTPUT]\ncommand2: CMD: [C] 1, [OUTPUT] 1, [OUTPUT] RIGHT, [HEAD1] RIGHT, [HEAD2]\nRIGHT, q1\nStep 5 (executor):\nstate3: ADD, q1, |5|4[HEAD1] |7|6[HEAD2] [C]1 |2|1[OUTPUT]\ncommand3: CMD: [OUTPUT] 1, [OUTPUT], [C], qH\nStep 6 (executor):\nstate4: ADD, qH, |5|4[HEAD1] |7|6[HEAD2] [C]1 |2|1|1\ncommand 4: No command to execute. Halt state."}, {"title": "A.3.2 Multiplication", "content": "Step 1 (aligner):\n89*2=\nStep 2 (executor):\nstate: MUL", "HEAD1": 9, "HEAD2": 2, "COUNT": ["OUTPUT"], "ncommand": "CMD [COUNT", "OUTPUT": 0, "executor)": "nstate1: MUL"}, {"HEAD1": 9, "HEAD2": 2, "COUNT": 0, "OUTPUT": 0, "ncommand1": "CMD [CALL", "q2\ncallee_state": "LESS_THAN"}, {"HEAD1": 0, "HEAD2": 2, "OUTPUT": "ncallee_command): CMD [HEAD1"}, {"HEAD2": "RIGHT", "OUTPUT": false, "executor)": "nstate1: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 0, "OUTPUT": 0, "ncommand1": "CMD [CALL", "H": "LESS_THAN", "0[HEAD1": 2}, {"HEAD2": true, "ncallee_command": "No command to execute. Halt state.\nStep 4-1", "executor)": "nstate2: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 0, "OUTPUT": 0, "ncommand2": "CMD [CALL", "q3\ncallee_state": "ADD", "HEAD1": 9, "8[HEAD2": 0, "C": ["OUTPUT"], "ncallee_command)": "CMD: [C"}, {"HEAD1": "RIGHT", "HEAD2": "RIGHT", "executor)": "nstate2: MUL"}, {"HEAD1": 9, "HEAD2": 2, "COUNT": 0, "OUTPUT": 0, "ncommand2": "CMD [CALL", "q3\ncallee_state": "ADD", "9|8[HEAD1": 0}, {"HEAD2": ["C"], "9|8\ncallee_command": "No command to execute. Halt state.\nStep 5-1", "executor)": "nstate3: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 0, "OUTPUT": 9, "8\ncommand3": "CMD [CALL", "q1\ncallee_state": "ADD", "HEAD1": 0}, {"HEAD2": 1, "C": ["OUTPUT"], "ncallee_command)": "CMD: [C", "HEAD1": "RIGHT"}, {"HEAD2": "RIGHT", "executor)": "nstate3: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 0, "OUTPUT": 9, "8\ncommand3": "CMD [CALL", "q1\ncallee_state": "ADD", "0[HEAD1": 1}, {"HEAD2": ["C"], "1\ncallee_command": "No command to execute. Halt state.\nStep 6-1", "executor)": "nstate4: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 1, "OUTPUT": 9, "4": "CMD [CALL", "q2\ncallee_state": "LESS_THAN", "HEAD1": 1}, {"HEAD2": 2, "OUTPUT": "ncallee_command): CMD [HEAD1"}, {"HEAD2": "RIGHT", "OUTPUT": false, "executor)": "nstate 4: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 1, "OUTPUT": 9, "4": "CMD [CALL", "H": "LESS_THAN", "1[HEAD1": 2}, {"HEAD2": true, "ncallee_command": "No command to execute. Halt state.\nStep 7-1", "executor)": "nstate5: MUL", "HEAD1": 9}, {"HEAD2": 2, "COUNT": 1, "OUTPUT": 9, "8\ncommand5": "CMD [CALL", "q3\ncallee_state": "ADD", "HEAD1": 9, "8[HEAD2": 9, "C": ["OUTPUT"], "ncallee_command": "CMD: [C"}, {"HEAD1": "RIGHT", "HEAD2": "RIGHT", "executor)": "nstate5: MUL"}, {"HEAD1": 9, "HEAD2": 2, "COUNT": 1, "OUTPUT": 9, "8\ncommand5": "CMD [CALL", "q3\ncallee_state": "ADD", "9|8[HEAD1": 9, "8[HEAD2": ["C"], "871\ncallee_command": "No command to execute. Halt state.\nStep 8-1", "executor)": "nstate6: MUL, q3, [HEAD1"}]}