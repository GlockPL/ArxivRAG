{"title": "Reinforcement learning to learn quantum states for Heisenberg scaling accuracy", "authors": ["Jeongwoo Jae", "Jeonghoon Hong", "Jinho Choo", "Yeong-Dae Kwon"], "abstract": "Learning quantum states is a crucial task for realizing the potential of quantum information technology. Recently, neural approaches have emerged as promising methods for learning quantum states. We propose a meta-learning model that employs reinforcement learning (RL) to optimize the process of learning quantum states. For learning quantum states, our scheme trains a Hardware efficient ansatz with a blackbox optimization algorithm, called evolution strategy (ES). To enhance the efficiency of ES, a RL agent dynamically adjusts the hyperparameters of ES. To facilitate the RL training, we introduce an action repetition strategy inspired by curriculum learning. The RL agent significantly improves the sample efficiency of learning random quantum states, and achieves infidelity scaling close to the Heisenberg limit. We showcase that the RL agent trained using 3-qubit states can be generalized to learning up to 5-qubit states. These results highlight the utility of RL-driven meta-learning to enhance the efficiency and generalizability of learning quantum states. Our approach can be applicable to improve quantum control, quantum optimization, and quantum machine learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning quantum states has emerged as a significant problem in the field of quantum computing and machine learning [1, 2]. As a tool for analyzing and benchmark-ing quantum systems, learning quantum states is essen-tial for realizing quantum information technology [3]. The purpose of learning quantum states is to estimate quan-tum states with high fidelity based on samples obtained by measuring copies of states. Quantum state tomogra-phy (QST) is the standard method for learning quantum states [4, 5], but this method becomes impractical for large systems as the number of measurements required scales exponentially with the size of system. This limi-tation fuels research directions to devise various strate-gies for QST using matrix product state [6, 7], permuta-tion invariance of state [8], shadow tomography [9], and Bayesian estimation [10].\nWith the advent of deep learning, there has been a growing body of work using a neural network-based model for learning quantum states [11\u201319]. Generative models provide sample-efficient methods for learning many-body quantum states based on restricted Boltz-mann machine [20], variational autoencoder [21], and re-current neural network [22]. These models are trained with unsupervised learning. Using supervised learning, a model of feed-forward neural network enhances QST by removing errors [23]. The power of these approaches stems from the expressivity of neural network, the ability to represent intricate features of quantum states in terms of a small number of parameters [24]. In the framework of quantum machine learning, a model based on a quan-tum neural network has also been employed for learning quantum systems [25\u201329]. The quantum model can be implemented with parameterized quantum gates [30, 31], making it well suited for learning states of quantum com-puters. Moreover, it is known that a quantum neural net-work can have higher expressive power than classical neu-ral networks of comparable size [32, 33].\nMeanwhile, the generalizability, the ability to gener-alize to unexperienced tasks, is one of the hallmarks of machine learning model. However, some of the models for learning quantum states are difficult to generalize as each model is tailored to learning a specific quantum state. Learning models with the generalizability can be found in attempts to improve a quantum optimization algorithm using a deep learning technique [34\u201337]. These works employ a hybrid approach that leverages a classi-cal model to learn a quantum model. Specifically, Ver-don et al. propose a model based on a recurrent neural network to initialize the parameters of quantum approx-imate optimization algorithm (QAOA) circuit [35]. Sim-ilar work by Wilson et al. uses a recurrent neural net-work model to learn the gradient of objective function of QAOA [36]. Khairy et al. show that a feed-forward neural network model can be used to learn the hyper-parameters of QAOA with reinforcement learning [37]. They show that the hybrid models can be generalized to solving problems of higher dimension than the dimension of problems for training. The deep learning technique in the previous works can be categorized as meta-learning, also called learning to learn, which is employed to enable a model to adapt to unexperienced tasks [38, 39]. Meta-learning methods for learning quantum states have also been proposed, where a classical model is used to learn another classical model [40] or Bayesian estimation algo-rithm [41]. For the task of learning quantum states, the validity and generalizability of meta-learning models in the hybrid setting remains an area of exploration.\nIn this work, we propose a meta-learning approach which leverages reinforcement learning (RL) to learn quantum states. The meta-learning model is based on the quantum and classical neural networks. We consider a Hardware efficient ansatz as the quantum neural net-work model [42]. The quantum model learns a quantum"}, {"title": "II. META-LEARNING: RL TO LEARN", "content": "The schematic of meta-learning model is illustrated in Fig. 1 (a). The environment contains an evolution strat-egy and a quantum system. As shown in Fig. 1 (b), the quantum system consists of a quantum state $\\ket{\\psi}$, a Hard-ware efficient ansatz (HEA) $\\hat{U}(\\theta)$, and a measurement. The evolution strategy trains the HEA to learn an in-put quantum state based on the measurement. The RL agent learns the hyperparameters of evolution strategy using two classical models in Fig. 1 (c), called actor and critic. We describe our RL scheme with a Markov deci-sion process.\nMarkov decision process (MDP) is a framework to de-scribe a sequential interaction that takes place between an environment and an agent [46]. The agent takes an ac-tion $a$ based on observation $o$ about the environment. As a result of an action, the state of environment is changed, and the agent receives a reward $r$ from the environment. A purpose of agent is to learn the policy of action $\\pi$ to maximize cumulative rewards (see Appendix A for a de-tailed description).\nIn determining approach of agent for formulating and solving a MDP, observability of the environment is a sig-nificant factor to consider. When the elements of MDP are fully known, that is, assuming full observability of en-vironment, an agent can derive the exact policy based on Bellman's principle of optimality [47]. However, in many scenarios, an agent cannot completely specify the state of environment and its dynamics due to limited (or partial) observability. Partially observable MDP (POMDP) is a generalized framework accommodating such limited ob-"}, {"title": "A. Environment", "content": "Learning a quantum state. Consider an unknown $N$-qubit state $\\ket{\\psi}$ as the input of HEA $\\hat{U}(\\theta)$. We measure the quantum state transformed by the HEA, $\\hat{U}(\\theta) \\ket{\\psi}$, with a measurement $M = \\{M_s, M_f\\}$ prepared in known bases, where\n$M_s = \\ket{s}\\bra{s}, M_f = I - M_s.$\n(1)\nThe outcome $s$ stands for success and $f$ does fail. We perform the measurement until a fail outcome appears, and count the number of success outcomes before the fail outcome. If a fail outcome appears, we change the pa-rameters of quantum circuit based on the number of suc-cess outcomes. Otherwise, we retain the parameters. This measurement scheme is the same as that of single-shot measurement learning (SSML) [25, 26]. While SSML uses a random search algorithm to update the parameters, we use the evolution strategy which will be explained later.\nWe refer to the number of consecutive success out-comes as success count. The success count is deter-mined by the probability of success outcome $p_s = \\bra{\\psi}\\hat{U}^{\\dagger}(\\theta) M_s \\hat{U}(\\theta)\\ket{\\psi}$. Thus, the chance to obtain $C$ success count is governed by a geometric distribution\n$p(C) = p_s^C (1 - p_s)$.\n(2)\nThe average value of success count is then given by $\\langle C \\rangle = \\sum_{C=0}^{\\infty} C p(C) = p_s / (1 - p_s)$. If $p_s \\rightarrow 1$, $\\langle C \\rangle \\rightarrow \\infty$, and, if"}, {"title": "Optimizer", "content": "To train the HEA, we employ an evolu-tion strategy (ES) [58]. An ES consists of following steps: (i) assuming a probability distribution for sampling, (ii) sampling parameters around a current parameter, (iii) evaluating the parameter samples, and (iv) updating the current parameter based on the evaluations. In our prob-lem, the ES uses a multivariate Gaussian distribution to sample parameters [43], and the evaluation corresponds to measuring the success count.\nThe purpose of ES is to find the parameters of HEA $\\theta$ to achieve the halting condition $C(\\theta) > C_{\\text{target}}$. For this purpose, we consider the expectation of success count as an objective function of ES\n$J(\\theta) := \\frac{1}{C_{\\text{target}}} \\mathbb{E}_{p(\\theta)}[C(\\theta)],$\n(5)\nwhere the target success count $C_{\\text{target}}$ is introduced in the denominator to normalize the objective function. $p(\\theta)$ is the multivariate Gaussian distribution $\\mathcal{N}(\\theta, \\sigma^2 I)$ of mean $\\theta$ and covariance matrix $\\sigma^2 I$, where $\\sigma \\in (0,\\infty)$ is a hy-perparameter to determine the range of sampling over the space of parameters, and $I$ is the identity matrix. To increase the expected success count, the parameters"}, {"title": "B. RL agent", "content": "The RL agent consists of an actor and a critic, both implemented by feed-forward neural networks [see Fig. 1 (c)]. The both networks use three hidden layers. For each hidden layer, the critic has hundred nodes, and the actor has fifty nodes. We use ReLu for activation functions. The actor is to learn a policy of action. We construct the action space $A$ by selecting $m$ values from each of the hyperparameter spaces $A_{\\sigma} = [0, \\infty)$ and $A_{\\eta} = [0,\\infty)$. So the number of actions the agent can choose from is $m^2 = |A|$. The output of actor is the policy $\\pi_{\\phi_2} = p(\\sigma, \\eta|o, \\phi_2)$, the probability distribution defined over the action space $A$. The critic outputs an estimate of observation-value function $Q_{\\phi_1}(o)$.\nWe train the agent to recommend the hyperparmaeters $\\sigma$ and $\\eta$ that enables the learning quantum states to be finished quickly. This is accomplished by giving a penalty of -1 for each time step, and no penalty for the time step when the learning quantum states is finished. In other words, the reward is given by\nr_t = \\begin{cases}\n0 & \\text{if } t = t_h \\\\\n-1 & \\text{otherwise}.\n\\end{cases}\n(8)\nThe empirical cumulative reward is then given by\nR_t = \\sum_{t}^{t_h} r_t = t - t_h.\n(9)\nFor the training, we use the Actor-Critic algorithm, and collect training datasets using a tree search guided by the policy. See Appendix C for detailed description."}, {"title": "III. RESULTS", "content": "We apply our meta-learning scheme to learning ran-dom pure states. The random states are prepared by act-ing unitary operators $V_s$ drawn from Haar measure to a computational basis state $\\ket{0}$ [67]. To measure these states, we perform a measurement in the computational basis, and consider that the basis $\\ket{0}\\langle 0|^{\\otimes N}$ is the basis of"}, {"title": "B. Action repetition strategy", "content": "To obtain these results, we use a method, called action repetition strategy (ARS), that repeats an action given by the RL agent for a certain period of time steps $t_{\\text{rep}}$. The schematic of ARS is illustrated in Fig. 3 (a). For each RL episode $T$, $t_{\\text{rep}}$ is given by\nt_{\\text{rep}} = \\text{max}\\left(t_l, \\frac{t_u - t_l}{T_{\\text{th}}} T, t_u\\right),\n(11)\nwhere $t_u$ and $t_l$ are the upper and lower value of $t_{\\text{rep}}$, respectively, and $t_u > t_l$. When $T=0$, the action repeti-tion time is $t_u$, and it decreases until the RL episode $T$ reaches $T_{\\text{th}}$. After $T_{\\text{th}}$, $t_{\\text{rep}}$ saturates to $t_l$."}, {"title": "C. The scaling of infidelity", "content": "Using the RL agents trained with the target success count $C_{\\text{target}} = 10^4$, we perform the learning a hundred random pure states by varying the target success counts from $10^1$ to $10^4$. The results are shown in Fig. 4 (a)-(c). We compare these results to the baseline values ob-tained by an action $a_{\\text{base}}$ (see Appendix D). Fig. 4 (d) shows the amount of total success counts saved by the RL agent according to the increase of the number of qubits $N$ and target success count $C_{\\text{target}}$. The amount of to-tal success counts saved is represented by $\\Delta \\langle C_{\\text{total}}\\rangle := \\langle C_{\\text{total}} \\rangle_{\\text{ES}} - \\langle C_{\\text{total}} \\rangle_{\\text{ES+RL}}$, where $\\langle C_{\\text{total}}\\rangle_{\\text{ES}}$ is obtained using $a_{\\text{base}}$ without the RL agent, and $\\langle C_{\\text{total}}\\rangle_{\\text{ES+RL}}$ is"}, {"title": "D. Generalization of RL agent", "content": "We apply the RL agent trained with the random 3-qubit states to learning a hundred random 4- and 5-qubit states. The results are illustrated in Fig. 4 (e). For the target success count $C_{\\text{target}} = 10^4$, the aver-age of total success count and infidelity are given by $\\langle C_{\\text{total}} \\rangle \\approx 7.261 \\times 10^6$ and $f \\approx 6.561 \\times 10^{-4}$ for 4-qubits, and $\\langle C_{\\text{total}} \\rangle \\approx 1.291 \\times 10^7$ and $f \\approx 9.311 \\times 10^{-4}$ for 5-qubits. The scaling factor of infidelity achieves $\\beta = -1.189$ and $\\beta = -0.829$ for 4- and 5-qubit, respec-tively. For the multi-qubit cases including 2- and 3-qubit results, our method gives the similar results to the self-guided quantum state tomography (SGQT) which shows the scaling factors in the range $\\beta \\in (0.80, 1.05)$ up to 10-qubits [76]."}, {"title": "IV. DISCUSSION & CONCLUSION", "content": "The single-shot measurement scheme used in this work is proposed by the single-shot measurement learning (SSML) [25, 26]. Notably, the SSML addresses following problems of the standard approach: (i) The way to recon-struct a learned state is not known. (ii) The learned state can have negative eigenvalues [5]. (iii) The scaling of infi-delity is limited to $O(n^{-3/4})$ for $n$ copies of states without additional information about quantum state [73]. Our method also does not suffer from these problems. The trained HEA immediately provides a way to reconstruct the estimated state, and the reconstructed state always satisfies the conditions of quantum state as the state re-construction is given by the quantum circuit. Also, the average infidelity nearly achieves the scaling of statisti-cal limit [45]. While the SSML shows that the optimal scaling in the learning up to 6-dimensional states, our"}, {"title": "Appendix A: Markov decision process", "content": "For finite time steps $t = 1, 2, ..., T$, a markov decision process (MDP) is represented by a tuple $(S, A, T,R)$, where $S$ is the space of state $s_t$, $A$ is the space of action $a_t$, $T$ is the space of transition probability $p(s_{t+1}|s_t,a_t)$ from a state $s_t$ to $s_{t+1}$ by an action $a_t$, and $R$ is the space of reward $r_t$. In the description of MDP, a trajectory of decision process $\\tau$, e.g., $s_1, a_1, r_1, s_2, a_2, r_2 ..., s_{T-1}, a_{T-1}, r_{T-1}, s_T, r_T$, is deemed to satisfy Markov condition, which assumes that a current state and reward only depend on a previ-ous state and action. Specifically, the assumption implies that a probability distribution which governs the trajec-tory $\\tau$ can be written as\np(\\tau) = p(s_1, a_1, r_1, s_2, a_2, r_2 ..., s_{T-1}, a_{T-1}, r_{T-1}, s_T, r_T)\n= p(s_1) \\prod_{t=1}^{T-1} p(s_{t+1}, r_{t+1}|s_t, a_t) p(a_t|s_t),\n(A1)\nwhere the probability $p(a_t|s_t)$ is called policy. The pur-pose of agent is to learn the optimal policy so as to max-imize the expected cumulative reward $G_t$ at each time step $t$, where\nG_t := \\mathbb{E}_{\\tau \\sim p(\\tau)} \\sum_t^{T} r_t.\n(A2)\nFinally, the RL problem that the agent solves is an opti-mization to find a policy such that\n\\pi^* = \\text{argmax } G_0.\n{\\pi = p(a_t|s_t)}\n(A3)"}, {"title": "Appendix B: Elements of POMDP", "content": "Our RL problem can be formulated with a POMDP defined by a tuple $(S, A, T, R, \\Omega, O)$:\n1.  $S$ is the Hilbert space of $N$-qubit states. The state $s_t$ is a quantum state $\\hat{U}(\\theta_t) \\ket{\\psi}$ transformed by the HEA in Fig. 1 (b)."}, {"title": "Appendix C: Actor-Critic algorithm", "content": "Actor-Critic algorithm proceeds with iteration of gen-erating training datasets, value evaluation, and policy improvement [87, 88]. We generate the rollout datasets $\\{(O_t, a_t, r_t, O_{t+1})\\}_{t=1}^{H-1}$ by using a tree search guided by the policy and store them in the data buffer. We ran-domly sample a datasets $B$ from the data buffer [89], and, based on the sampled datasets $B$, the critic network evaluates the observation-value function $Q_{\\phi_1}$. We take the empirical cumulative reward (9) as an estimator of the observation-value function:\nQ(O_t) = \\frac{R_t}{t_{\\text{max}}},\n(A1)\nwhere $t_{\\text{max}}$ is the maximum time step of ES which is set to $3 \\times 10^3, 10^4, and 2 \\times 10^4$ for 1-, 2-, and 3-qubit, respec-tively. We introduce $t_{\\text{max}}$ in the denominator to normal-ize the empirical cumulative reward. The loss function of critic network is given by the difference between the output of critic network and the estimator of observation-value function, i.e.,\n\\text{loss}_{\\text{critic}}(\\phi_1) := \\frac{1}{2} (Q_{\\phi_1}(O_t) - \\hat{Q}(O_t))^2\n(A2)\nWe update the parameters of critic network $\\phi_1$ by using a gradient descent method. Based on the critic network, we improve the policy $\\pi_{\\phi_2}$ by training the actor network. The loss function of actor network is given by\n\\text{loss}_{\\text{actor}}(\\phi_2) := A(O_t) \\log \\pi_{\\phi_2}(a_t|O_t),\n(A3)"}]}