{"title": "Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI", "authors": ["Isadora Krsek", "Anubha Kabra", "Yao Dou", "Tarek Naous", "Laura A. Dabbish", "Alan Ritter", "Wei Xu", "Sauvik Das"], "abstract": "In pseudonymous online fora like Reddit, the benefits of self-disclosure are often apparent to users (e.g., I can vent about my in-laws to understanding strangers), but the privacy risks are more abstract (e.g., will my partner be able to tell that this is me?). Prior work has sought to develop natural language processing (NLP) tools that help users identify potentially risky self-disclosures in their text, but none have been designed for or evaluated with the users they hope to protect. Absent this assessment, these tools will be limited by the social-technical gap: users need assistive tools that help them make informed decisions, not paternalistic tools that tell them to avoid self-disclosure altogether. To bridge this gap, we conducted a study with N = 21 Reddit users; we had them use a state-of-the-art NLP disclosure detection model on two of their authored posts and asked them questions to understand if and how the model helped, where it fell short, and how it could be improved to help them make more informed decisions. Despite its imperfections, users responded positively to the model and highlighted its use as a tool that can help them catch mistakes, inform them of risks they were unaware of, and encourage self-reflection. However, our work also shows how, to be useful and usable, Al for supporting privacy decision-making must account for posting context, disclosure norms, and users' lived threat models, and provide explanations that help contextualize detected risks.", "sections": [{"title": "1 INTRODUCTION", "content": "Every day, millions of people log onto pseudonymous online fora like Reddit to access a seemingly safe space to vent, seek support, and connect with like-minded others without the additional baggage of their offline identity. Unsurprisingly, prior work has also found that users in these pseudonymous fora are more likely to disclose highly sensitive information [98]. While the benefits"}, {"title": "2 RELATED WORK", "content": "We consider three categories of related work: the current burden of balancing the benefits of self-disclosure with the risks, common approaches from prior work to reduce the risk of online self- disclosure (both non-AI-based and AI-based), and the need for user engagement in the development of AI technology."}, {"title": "2.1 The Tension Between the Need for Self-Disclosure and the Risks", "content": "Online pseudonymous communities, like Reddit, offer internet users a safe, seemingly anonymous space to seek opportunities to engage with others free from the baggage of their identity. These interactions often stem from a desire for informational or emotional support, and in seeking support people disclose many personal details publicly [98]. Self-disclosure is an important key to establishing a sense of solidarity and community even disclosing negative experiences has been shown to lead users to feel a sense of support and increased confidence in themselves [75]. Online self-disclosure specifically can establish a sense of belonging, and support the development of new relationships [16, 47] \u2013 and is especially valuable for users seeking support on sensitive topics that they might not otherwise be able to receive in their physical communities due to concerns over factors like social stigma, or embarrassment [101]. Anonymity when engaging in public self-disclosures can provide users a cover for more intimate and open conversations [9, 80].\nWhile self-disclosure fulfills many core social needs and despite users' need to continuously make online self-disclosure decisions based on their intended audiences [98], prior work has shown that people do not always accurately anticipate who might see their content [93]. Balancing informational and emotional support needs with the abstract risks associated with their disclosures (e.g., re-identification, stalking, identity theft, blackmail) is challenging [36, 93]. Unlike face-to- face disclosures, online disclosures leave a digital trace that can be screen-captured and linked to other posts to re-identify and ultimately harm the original posters. To mitigate their risk of re-identification, users employ tactics such as the use of \u201cthrowaway\u201d or \u201cburner\" accounts [5, 50], but can still end up disclosing enough information about themselves to be re-identified [3, 67].\nWork on re-identifying users from anonymous datasets shows that few attributes are needed to sufficiently re-identify individuals using incomplete datasets: for example, using 15 demographic attributes, around 99.98% of Americans can be identified; using just 3, around 83% can still be identified [74]. Information as simple as age, gender, and ethnicity can be enough to re-identify an individual when combined with a medical diagnosis [55]. Re-identification risks are highly relevant to users of online pseudonymous platforms, like Reddit, with communities dedicated to specific medical diagnoses such as r/Cushings, or r/Marfans wherein users experience many benefits from personal disclosure in seeking support (e.g., earlier detection and treatment of medical issues that would have otherwise gone unaddressed) [44, 64].\nRelatedly, users' disclosure is guided by their perception of risk and the sensitivity of the information that they are disclosing. It follows that prior research on anonymous public self-disclosure suggests users feel better able to express themselves when posting anonymously, [9, 80], as anonymity allows users to avoid the face-to-face stigma of embarrassing or uncomfortable topics [24, 26]. Anonymity affords users the opportunity to be honest, unfiltered, and vulnerable; characteristics they might not be able to exhibit through in-person interactions. Conversely, users' heightened feelings of anonymity can also result in benign disinhibition, wherein users disclose more personal information as they feel more secure and less identifiable [49].\nThus providing users with more awareness of risks to their privacy while maintaining the benefits of disclosures remains a longstanding challenge for security and privacy practitioners. We extend this body of work by exploring the design considerations, utility, and intrusiveness of"}, {"title": "2.2 Past Approaches to Alleviating the Risks of Self-Disclosure", "content": "Non AI-based Tools. Researchers and practitioners in security & privacy have explored approaches to helping users navigate the risks of self-disclosures with varying success. One ubiquitous, built-in approach to mitigating users' risks of online self-disclosure is the employment of customize-able privacy preferences [2, 69]. While important, and helpful for users of social networking sites like Facebook, the use of granular privacy settings is not entirely relevant to mitigating the harmful self-disclosure risks pseudonymous online community members face since users are intentionally posting information publicly. Furthermore, despite the protective intention of this, more granularity in privacy settings can actually encourage self-disclosure. In their study, Brandimarte et al. showed that in the context of an online social network, participants who were offered finer-grain privacy controls disclosed more personal information compared to those who were offered weaker controls [11]. Finer-grain control has also been found to reduce users' privacy concerns in other areas as well such as personalization [82]. Research has also explored intervention-based strategies for mitigating a user's level of disclosure at the time of posting through the use of framing and presentation. For example, Wang et al. explored the effect of a modified Facebook interface that provided feedback about a post's audience and also allowed users to cancel their post within 10 seconds of making it [92]. Prior work from Braunstein et al. also showed that changing the wording in a survey question to remind users that they are revealing sensitive information impacts how much they're willing to reveal [12]. While these specific techniques are slightly difficult to apply to an online community like Reddit where most posts will be made in public subreddits, they may still be useful for privacy intervention tools. Tools that detect potentially risky self-disclosures for example, may benefit in utility by incorporating additional context for these detections. As such, in this study, we explore the utility of different framing for suggestions by presenting participants with two versions of a disclosure detection model that vary in granularity.\nAl-based Tools/existing NLP Approaches to detecting self-disclosure. With the advancement of AI technology, over the course of the past few years, we've also started to see numerous natural language processing (NLP) based intervention tools emerge as aids for the prevention of privacy leaks from self-disclosure [14, 37, 57]. A core benefit offered by NLP-based self-disclosure interven- tions compared to the aforementioned ones is that they take a more targeted approach to potential privacy leaks by explicitly pinpointing disclosures made by users in their posts. This offers users actionable areas for improvement and the agency to control the dissemination of their sensitive information. Multiple recent works have introduced NLP-based self-disclosure detection models that are targeted towards specific disclosure categories such as medical conditions [87, 89, 102], personal opinions [18], employment history [84], or stories of sexual harassment [19]. Other works developed detection models that consider any type of personal information revealed by users as a single category [10, 73, 97]. A few studies also developed models capable of more fine-grained detection of multiple self-disclosure categories at once [4, 52]. A major limitation in past work is that it mainly centers around operational changes for enhancing model performance in detecting self-disclosures, such as improving their accuracy or expanding their granularity of categories. However, studying the interaction of users with NLP-based self-disclosure detection models and analyzing user experiences is key in identifying the next steps for the real-world adoption of NLP models in privacy-preserving tools. To our knowledge there has been no prior work on user's reactions to these models, or whether the classifiers used are perceived as being helpful by users."}, {"title": "2.3 The Need for User Engagement in the Design of Al Technology", "content": "Al technologies have profound social and economic implications across various domains. When predictive modeling is employed in critical decision-making processes such as credit offers, in-surance assessments, hiring, and parole decisions [20, 35], the application of AI can exacerbate social inequality as these technologies have the potential to shape opportunities, leaving certain individuals marginalized due to data or model bias [8, 20, 28, 65]. Additionally, most AI-based technologies are developed from data collected from (biased) human decision-makers, or through training processes which can also introduce bias [20, 81].\nIt is imperative that the people impacted by AI systems' deployment be substantively engaged in providing feedback and design direction, and that these suggestions form a feedback loop that can directly influence AI systems' development and broader policy frameworks which are imperative for ensuring fair and unbiased outcomes [46, 81]. Engaging users in the design of AI technology can help mitigate the negative impacts of AI on society and promote ethical and responsible AI development [20, 81]. Park et al. [62] shows the importance of integrating user feedback into the development process of consumer-oriented systems as there is a potential disparity between user expectations and technological functionalities. Recognizing that user preferences and demands can evolve rapidly, the paper advocates for an adaptive approach that places user feedback at the forefront of system design. On a similar note, [21] talks about the necessity of ongoing updates based on user feedback to prevent errors, particularly in dynamic and evolving scenarios. In contrast to static systems, those that are continually updated and refined based on user interactions demonstrate a heightened ability to adapt to new information, circumstances, and user expectations. Several works have delved into growing privacy concerns associated with AI-based technologies. The impact of AI on privacy interests is discussed by Kerry [45], emphasizing how advances in AI amplify concerns regarding the use of personal information [7, 58, 59, 91].\nThe work of Dou et al. [27] is a first step in assessing user preferences for AI-assisted self- disclosure detection, though it only reports high-level takeaways as to what participants liked and didn't like, and the percentage of users who reported that they would continue to use the model. The goal of Dou et. al.s paper was to introduce a state-of-the-art disclosure detection model \u2013 the user feedback reporting was primarily to affirm the desirability of their technical contribution to users. In contrast, our goal centers user needs and preferences by aiming to examine: (i) how users make sense of and decide to accept or reject AI-detected self-disclosure risks in their content, (ii) whether and how these AI-detected self-disclosure risks help users make more informed self-disclosure decisions, and (iii) where the model falls short or could be improved in being of greater use to users when they must make these self-disclosure decisions. Our study contributes an exploration of the usefulness, usability, and utility of AI-supported privacy decision-making despite its imperfections. Our work ultimately aims to fill this void by examining user perspectives on disclosure and privacy in the area of online communication, contributing knowledge to enhance technical safeguards, and aligning AI systems with user expectations in the rapidly evolving landscape of digital interactions."}, {"title": "3 SELF-DISCLOSURE DETECTION MODEL", "content": "Prior works on self-disclosure detection mostly focused on sentence or post-level classification [17, 88] that simply determines if the given text contains any disclosure. However, it doesn't pinpoint the specific spans (i.e., segments of consecutive words) of self-disclosures, providing"}, {"title": "4 METHODOLOGY", "content": "To explore how AI-assisted identification of self-disclosure spans might impact users' awareness of self-disclosure risks and their writing behaviors, we conducted a 75-minute interview study with 21 Reddit users. We asked participants to share two of their self-authored Reddit posts with researchers which were subsequently run through our NLP model. We then asked participants questions, using the NLP model outputs as a guide, to determine how the model affected users' awareness of potential self-disclosure risks (RQ1), how feedback from the model should be framed to maximize utility to participants (RQ2), and where these tools might be altered to better align with users' preferences (RQ3)."}, {"title": "4.1 Recruitment", "content": "We recruited a total of 21 Reddit users through Prolific, an on-demand participant recruitment platform. Participants were screened to ensure that they: had a Reddit account at the time of the study, made at least three posts on Reddit, resided in the U.S., and were 18 years of age or older. Eligible participants were subsequently asked to provide links to two text-based Reddit posts they had made; one with which they had privacy concerns and another with which they did not. From about 158 who took the pre-study survey, we invited 33 Reddit users to participate in the interview based on the Reddit posts they shared with which they had privacy concerns. Participants were only invited to participate after researchers ensured their shared posts were majority text-based, and contained personal disclosures, and after ensuring that the model we used was able to detect potentially identifying disclosures in the post. More specifically, participants were filtered out of the study if the posts that they shared were inaccessible, removed, contained no text (e.g. only an image), or if they reported not having made a post that meets those criteria (e.g., if they answered with \u201cN/A\u201d or \u201cI have not done this\u201d when asked to share a Reddit post they drafted over which they had privacy concerns). While we did reach out to participants who shared invalid links, none of them responded to our request for an alternative post. From the total 158 users who took the pre-study survey, we invited 33 Reddit users to participate in the interview based on the Reddit posts they shared with which they had privacy concerns. Of those 33 participants, 21 followed up on our request to schedule interviews. Given the average sample size for studies within the CHI community is 12 participants [13], and the rich body of work demonstrating that anywhere between 6-12 participants is enough for high saturation [32, 38-40, 56, 60], we did not run any additional recruitment attempts.\nThe collected demographic data is displayed in the Appendix (see Table A in section A). The sample was slightly skewed female, with 12 participants identifying as female. 16 of the 21 partici- pants were below the age of 50, and 15 participants held a bachelor's degree or higher certificate of education. Data collection occurred in the summer of 2023, and participants received $17 compen- sation for their participation in the interview and pre-study survey. Our study design was approved by an institutional review board."}, {"title": "4.2 Pre-study Survey", "content": "Participants were first directed to an online survey that included screening questions to ensure eligibility. Those who didn't meet the requirements were automatically removed from the survey at this stage. Afterward, participants were presented with a consent form in the survey. Those who agreed to participate provided their preferred email address for future communication and to schedule a Zoom-based interview. Participants were then requested to share links to two Reddit posts they had made; one with which they had privacy concerns and another with which they did not. As mentioned prior, participants were filtered out of the study if the posts that they shared were inaccessible, removed, contained no text (e.g., had only an image), or if they reported not having made a post that met our criteria.\nIn the final section of the pre-study survey, participants answered questions about their Reddit usage and behaviors, their perception of self-anonymity on Reddit, as well as their tendency to disclose information publicly using validated scales adapted from prior work on perceived anonymity in online social support communities [99]. Additionally, participants were asked about their demographics and their general attitudes towards security and privacy using the SA-13 scale [30]. We collected these data because prior research has found that differences in end-user security attitudes, demographic characteristics, and perceived audiences when sharing content online can shape users' online S&P decision-making [15]."}, {"title": "4.3 Interview", "content": "User Assessment of Model Output on Their Posts. We conducted 75-minute semi-structured in- terviews to gather feedback on the model's outputs and understand participants' views on online anonymity risks and how those risks are influenced by subreddit community norms. The interviews began with general questions about participants' Reddit usage and the disclosure norms they followed for the subreddits in which they were active. We also asked about their posting behaviors and differences in disclosure norms across subreddits in order to account for posting practices that might interfere with the outputs of the model (RQ1, RQ3). Prior to showing participants the model outputs on their posts, we discussed the posts participants shared with us in the pre-study survey. We started with the post they reported feeling less comfortable sharing with people they know in the physical world, probing them on what personal disclosures they thought they included in the post, as well as the threats they perceived to their anonymity, who they were concerned might identify them, and why. We then asked participants to either (a) share a post from a burner account they had privacy concerns with, or (b) draft a post that they'd previously hesitated to share on Reddit for reasons related to privacy or sensitivity. After doing this, we then probed participants on all the same questions as the first post. As opposed to the first post, asking them to share/draft an additional post allowed us to assess the model's utility in providing feedback on posts that users hesitated to share from their main accounts.\nIn the next stage of the interview, we analyzed participants' posts using both the binary and categorical disclosure versions of the disclosure detection model (RQ2). Participants first saw the output of the binary disclosure model on both of their posts and assessed those. We then showed participants the output of the categorical disclosure model on their posts for assessment. When evaluating the model's outputs on self-authored posts, we also asked participants to assess each of the disclosure spans detected by the model; we asked whether they agreed or disagreed that the detected disclosure span contained self-disclosure and why, as well as whether and how they would want to change their post as a result. Participants then rated each detected disclosure span on four aspects using a 5-point Likert scale (from \u201cnot at all...\u201d (1) to \u201cvery...\u201d(5)): 1) the \u201chelpfulness\u201d of the disclosure span in surfacing a privacy risk, 2) the \u201cimportance\u201d of disclosing the information in the disclosure span for communicating the context of the post, 3) their perceived \u201csensitivity\u201d of the information in the disclosure span, and 4) the \u201criskiness\u201d of disclosing that information in their online post. Because these scales were delivered verbally in during the interview, we also probed participants on the reasoning behind their rankings. We present our analysis of their reasoning in the results section alongside rankings. Finally, we asked participants whether they would feel comfortable posting the modified second post to their main Reddit account based on the model outputs, and any changes they would make as a result (RQ1, RQ2, RQ3)."}, {"title": "4.4 Data Analysis", "content": "We used an inductive thematic analysis approach to coding our interviews with participants [66], leveraging affinity diagramming to uncover themes within the work. This analysis was conducted by two researchers who actively collaborated in reviewing 5 transcripts to develop a codebook with 40 codes, containing a mixture of general reactions to the model, disclosure detection preferences, and their reasons for each. The same two researchers then used this codebook to identify emergent patterns and themes across the full set of interview transcripts. The researchers also collaborated in affinity diagramming the codes in order to reveal sub-themes in the work. For a subset of the study sessions (23% of the data) which were coded independently by two researchers, we achieved"}, {"title": "5 RESULTS", "content": "To answer RQ1, our findings indicate that users found value in using our model to detect self- disclosure risks, with the majority (17/21) expressing a desire to use it outside of the study or recommend it to others. In particular, users appreciated the self-reflection the model encouraged, finding value in its ability to: (i) catch risky disclosures they were previously unaware of or missed in the process of writing, (ii) broaden their conception of risky disclosure, and (iii) help them come to a decision on information they were unsure about disclosing. However, we also found that users required support in navigating how to rephrase and de-risk the content that they wrote in a manner that preserved the semantic meaning of their original post. To answer RQ2, our results suggest that the granularity of information offered alongside text that the model detected as risky may have had an impact on users' acceptance of disclosure detection spans (the text that the models determined may contain self-disclosure risks). Users described the categorical disclosure version of the model's category labels as helping them understand why a disclosure span was detected as a privacy risk. Finally, to answer RQ3, our results indicate that participants found the model least useful when its outputs failed to account for their own risk mitigation strategies (e.g., hypothetical situations, intentional lies users made to preserve their anonymity), conflicted with the disclosure norms and requirements of their posting context, and surfaced risks that were misaligned with the threat models they most cared about. We also highlight categories of risky disclosure that the model failed to capture, but which participants felt were important to detect."}, {"title": "Descriptive Statistics on Users' Reactions to the Self-Disclosure Detection Spans Surfaced by the Model", "content": "Among the 851 total self-disclosure spans that the model detected across all our participants, participants accepted 58% (495 total) as containing risky self-disclosures, and rejected the other 41.8% (356 total) (see Table 3). In response to seeing the detected disclosures, about 15% (127/851) of all disclosure spans were ultimately altered by participants, while 3.5% (30/851) left participants undecided on what to do; the remaining 81.5% (694/851) were not altered by participants.\nWhen only looking at the disclosure spans that were accepted by users (495), the alteration rate of disclosure spans was only slightly higher (114/495 = 23%), with the majority still deciding not to alter the disclosure span (357/495 = 72%); participants were unsure whether or how they would alter the remaining accepted disclosure spans (23/495 = 5%).\nTo explore whether participants' decisions to alter the content of detected disclosure spans were influenced by mistakes made by the model, we further examined the 72% of non-altered disclosure spans that participants accepted as containing self-disclosures (357). For these detected disclosures,"}, {"title": "5.1 RQ1: How Interaction with an NLP Model Can Benefit Users' Awareness of Risky Self-Disclosures and Mitigation Behaviors", "content": "While the model was clearly imperfect, when reflecting on their overall opinion of the tool, partici- pants' reactions to the model were largely positive: The vast majority of participants (17/21) either expressed that they would want to use it personally when making online posts (14/21), or that they would want to recommend its use to others they know 2/21. That latter group felt like they already had appropriate strategies for mitigating self-disclosure risks in their posts without the model, but suggested it might be especially useful for users with less experience in mitigating disclosure risks online. P19, for example, recounted: \u201cI don't know that I necessarily need it. I do think it'd be a good idea for like tweens and teens, like people who are new to the internet. Just kind of give them an idea of stuff that they really shouldn't be sharing.\u201d\nUsers perceived several benefits from their interaction with the model, with most describing its value as a tool for reflection (12/21), even for circumstances where the model's outputs made little sense in isolation. Participant's responses revealed that a core value of the model lay in its potential for catching mistakes and double-checking one's writing for potential privacy risks: \u201cWhat it grabs prompts you to think more thoroughly about whether you actually want to include that information, and whether that information could be used to identify you. Specifically, in terms of"}, {"title": "5.2 RQ2: The Impact of Classification Granularity", "content": "To assess how the granularity of classification labels might impact the utility of the disclosure detection spans from the model to users, we also assessed the differences in users' reactions to a binary disclosure model (where text spans are classified as risky self-disclosures or not) versus as a categorical disclosure model (where these spans were also labeled with a category describing the kind of sensitive disclosure made).\nOverall, we found that higher-granularity classifications resulted in greater user acceptance of model outputs: outputs from the binary disclosure model were rejected (54%) more often than they were accepted (46%), while outputs from the categorical disclosure model were accepted (65%) more often than they were rejected (35%).\nHowever, greater acceptance of disclosure spans did not necessarily result in markedly different disclosure behaviors. We define the \"alteration rate\" of a model as the number of detected disclosures that users decided to alter, divided by the total number of disclosures detected by that model. The alteration rates for both the categorical disclosure (91/557 = 16.4%) and binary disclosure (36/289 =\n12.5%) models were both similar ($\\chi^2(1, N = 846) = 1.9, p = 0.16$) and low overall. Note, however,"}, {"title": "5.3 RQ3: Where Al-powered Disclosure Detection Tools Can Fall Short And Be Improved", "content": "We next analyzed our data to identify where participants felt our models fell short in order to identify opportunities for improvement. Our results surfaced many ways that automated disclosure detection tools \u2013 like the models we developed in this work \u2013 could be adjusted to better accommodate users' needs.\nMisalignment of risk conception. One of the most common reasons users rejected model outputs was that participants disagreed that a disclosure detected by the model was actually risky (see Table 3), and didn't mind disclosing that information resulting in a misalignment with user preference. This preference misalignment was the most common reason for rejection among nine of the seventeen disclosure categories detected by our model: i.e., \u201crelationship status\u201d, \u201cpet\u201d, \u201cfinance\u201d,"}, {"title": "6 DISCUSSION", "content": "Our findings highlight both the promise and peril of using AI-assisted self-disclosure detection tools to help users make informed decisions about what to disclose when sharing personal information online. Whereas prior work in this space has focused on the technical challenge of developing effective self-disclosure classification models, we show the problem is inherently socio-technical and emblematic of Ackerman's social-technical gap [1]. An effective solution cannot stop at achieving a high F\u2081 score; it will require significant consideration of, e.g., users' lived threat models, posting context, risk mitigation strategies, and understanding of risk. To that end, we culminate with a discussion on how such tools can be designed in a manner that is not only effective at detecting risky disclosures but useful in helping users make informed decisions.\nTo summarize our findings, the majority of participants responded positively to our model \u2013 particularly the categorical disclosure variant describing its utility in facilitating self-reflection and in catching user's mistakes. Users also anticipated it being helpful in making decisions about content they might be uncertain about sharing. We also found that users appreciated the presence of coarse explanations provided by the categorical disclosure version of our model: since users' conceptions of risk may have differed from what the model was trained to identify, they were more likely to accept model outputs when given the \u201ccategory\u201d of the detected disclosure versus just the fact that a given span of text had some kind of disclosure. Finally, our findings also surface many weaknesses of existing models that highlight directions for future modeling and design work.\nFor future modeling work, our findings identify new categories of disclosure users desired the model to detect, the need to distinguish between factual and non-factual disclosures, to account for a user's history of disclosures, and the need to help users de-risk disclosures while preserving semantic meaning. For future design work, our findings suggest a need to design effective explanations, account for posting context and disclosure norms, and prioritize disclosures that are particularly important for users to heed so as to avoid habituation effects from over-warning users. Below, we center and embellish upon a few promising design implications for AI tools that aim to help users navigate self-disclosure risks online."}, {"title": "6.1 Practical Implications for Deploying Self-Disclosure Detection Tools", "content": "Our study raises important considerations for any practitioners or researchers hoping to apply the use of AI-assisted self-disclosure detection to existing systems, and showcases its potential utility across a multitude of contexts. Beyond being helpful as a standalone tool users could use across social media sites, we could also envision this being integrated into the design of platforms themselves, presented to users at the time of drafting a post. For example, such a tool could be smoothly integrated into existing features Reddit has released for making sure users adhere to community"}, {"title": "7 LIMITATIONS", "content": "The disclosure spans detected by the binary and categorical disclosure variants of the model we used as a technology probe did not surface the exact same disclosure spans, reducing our ability to make direct comparisons between the two. We also did not counterbalance the appearance of the models users were always presented with the binary disclosure version of the model first. Thus reactions to the models may be subject to order effects. As such, we avoided making any direct statistical comparisons between the two models, instead opting to focus on why users preferred one model's outputs over the other from their interview responses. Future work comparing the"}, {"title": "7.1 Model & Study Design", "content": "The gender demographics of our exploratory user study are significantly skewed toward people who identify as women. This gender distribution is not representative of the general population, nor is it representative of Reddit user gender statistics [53]. Prior literature has well-established gender differences in privacy perceptions and behaviors [41], and as such our results may not be widely generalizable. Our participant demographics are also skewed white, with a majority of participants below the age of 50, though these skews are more representative of Reddit demographics [53]. Furthermore, our sample was conducted solely with participants residing in the United States. Though we attempted to recruit participants from the Reddit platform, we ultimately were only able to recruit participants from Prolific. Because we had used a newly created lab account to make recruitment posts, our posts were removed as our account \u201cdid not have enough karma\u201d. \u201cKarma\u201d is a form of social credit that is built on Reddit through commenting on others' posts, receiving upvotes, and awards; it is used to maintain the contribution quality of content posted to the site. While all interviewees were verified Reddit users, it's possible that we introduced additional biases into our sample as crowd-sourced participants are accustomed to participating and volunteering in research, and on the whole tend to be more tech-savvy than the general population [63, 71]. Moreover, it's possible that those who responded to our recruitment call were more comfortable with sharing content that included personal disclosures online than others. Finally, our work is highly specific to the Reddit context and, as such, our findings may not generalize to other pseudonymous or partially pseudonymous online fora (e.g., X, Mastodon)."}, {"title": "7.2 Recruiting & Sample Representation", "content": "When posting personal information in pseudonymous online fora, like Reddit, it is difficult for users to balance the tangible benefits of self-disclosure (e.g., providing context when seeking support) with its comparatively abstract risks (e.g., de-anonymization by institutions or third parties). Prior work has explored the use of NLP-based self-disclosure identification tools to help users make more informed decisions by surfacing explicit disclosure risks in the content they plan to share online [14, 37, 57]. Building on this line of work, we contribute the first comprehensive evaluation of these tools with the users they aim to protect. Through an in-depth interview study with 21 Reddit users, using a state-of-the-art self-disclosure detection model as a technology probe [27], we explore if and how these models might be useful and usable beyond their F\u2081 scores. Our findings suggest"}]}