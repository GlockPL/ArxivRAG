{"title": "Prioritize Alignment in Dataset Distillation", "authors": ["Zekai Li", "Ziyao Guo", "Wangbo Zhao", "Tianle Zhang", "Zhi-Qi Cheng", "Samir Khaki", "Kaipeng Zhang", "Ahmad Sajed", "Konstantinos N Plataniotis", "Kai Wang", "Yang You"], "abstract": "Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information. This simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms. Furthermore, built on trajectory matching, PAD achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Dataset Distillation (DD) [46] aims to compress a large dataset into a small synthetic dataset that preserves important features for models to achieve comparable performances. Ever since being introduced, DD has gained a lot of attention because of its wide applications in practical fields such as privacy preservation [6, 49], continual learning [29, 36], and neural architecture search [13, 33].\nRecently, matching-based methods [7, 45, 51] have achieved promising performance in distilling high-quality synthetic datasets. Generally, the process of these methods can be summarized into two steps: (1) Information Extraction: an agent model is used to extract important information from the target dataset by recording various metrics such as gradients [54], distributions [53], and training trajectories [1], (2) Information Embedding: the synthetic samples are optimized to incorporate the extracted information, which is achieved by minimizing the differences between the same metric calculated on the synthetic data and the one recorded in the previous step.\nIn this work, we first reveal both steps will introduce misaligned information, which is redundant and potentially detrimental to the quality of the synthetic data. Then, by analyzing the cause of this misalignment, we propose alleviating this problem through the following two perspectives.\nTypically, in the Information Extraction step, most distillation methods allow the agent model to see all samples in the target dataset. This means information extracted by the agent model comes\nfrom samples with various difficulties (see Figure 1(a)). However, according to previous study [11], information related to easy samples is only needed when the compression ratio is high. This misalignment leads to the sub-optimal of the distillation performance.\nTo alleviate the above issue, we first use data selection methods to measure the difficulty of each sample in the target dataset. Then, during the distillation, a data scheduler is employed to ensure only data whose difficulty is aligned with the compression ratio is available for the agent model.\nIn the Information Embedding step, most distillation methods except DM [53] choose to use all parameters of the agent model to perform the distillation. Intuitively, this will ensure the information extracted by the agent model is fully utilized. However, we find shallow layer parameters of the model can only provide low-quality, basic signals, which are redundant for dataset distillation in most cases. Conversely, performing the distillation with only parameters from deep layers will yield high-quality synthetic samples. We attribute this contradiction to the fact that deeper layers in DNNs tend to learn higher-level representations of input data [28, 38].\nBased on our findings, to avoid embedding misaligned information in the Information Embedding step, we propose to use only parameters from deeper layers of the agent model to perform distillation, as illustrated in Figure 1(b). This simple change brings significant performance improvement, showing its effectiveness in aligning information.\nThrough experiments, we validate that our two-step alignment strategy is effective for distillation methods based on matching gradients [54], distributions [53], and trajectories [1]. Moreover, by applying our alignment strategy on trajectory matching [1, 11], we propose our novel method named Prioritize Alignment in Dataset Distillation (PAD). After conducting comprehensive evaluation experiments, we show PAD achieves state-of-the-art (SOTA) performance."}, {"title": "2 Misaligned Information in Dataset Distillation", "content": "Generally, we can summarize the distillation process of matching-based methods into the following two steps: (1) Information Extraction: use an agent model to extract essential information from the target dataset, realized by recording metrics such as gradients [54], distributions [53], and training trajectories [1], (2) Information Embedding: the synthetic samples are optimized to incorporate the extracted information, realized by minimizing the differences between the same metric calculated on the synthetic data and the one recorded in the first step."}, {"title": "2.1 Misaligned Information Extracted by Agent Models", "content": "In the information extraction step, an agent model is employed to extract information from the target dataset. Generally, most existing methods [1, 7, 51, 54] allow the agent model to see the full dataset. This implies that the information extracted by the agent model originates from samples with diverse levels of difficulty. However, the expected difficulty of distilled information varies with changes in IPC: smaller IPCs prefer easier information while larger IPCs should distill harder one [11].\nTo verify if this misalignment will influence the quality of synthetic data, we perform the distillation where hard/easy samples of target dataset are removed with various ratios. As the results reported in Figure 2, pruning unaligned data points is beneficial for all matching-based methods. This proves the misalignment indeed will influence the distillation performance and can be alleviated by filtering out misaligned data from the target dataset."}, {"title": "2.2 Misaligned Information Embedded by Metric Matching", "content": "Most existing methods use all parameters of the agent model to compute the metric used for matching. Intuitively, this helps to improve the distillation performance, since in this way all information extracted by the agent model will be embedded into the synthetic dataset. However, since shallow layers in DNNs tend to learn basic distributions of data [28, 38], using parameters from these layers can only provide low-level signals that turned out to be redundant in most cases."}, {"title": "3 Method", "content": "To alleviate the information misalignment issue, based on trajectory matching (TM) [1, 11], we propose Prioritizing Alignment in Dataset Distillation (PAD). PAD can also be applied to methods based on matching gradients [54] and distributions [53], which are introduced in Appendix A.1."}, {"title": "3.1 Preliminary of Trajectory Matching", "content": "Following the two-step procedure, to extract information, TM-based methods [1, 11] first train agent models on the real dataset DR and record the changes of the parameters. Specifically, let {\u03b8t} be an expert trajectory, which is a parameter sequence recorded during the training of agent model. At each iteration of trajectory matching, \u03b8t and \u03b8t+N are randomly selected from expert trajectories as the start and target parameters.\nTo embed the information into the synthetic data, TM methods minimize the distance between the expert trajectory and the student trajectory. Let \u0398t denote the parameters of the student agent model trained on synthetic dataset Ds at timestep t. The student trajectory progresses by doing gradient descent on the cross-entropy loss l for N steps:\n$\\theta_{t+i+1} = \\theta_{t+i} - \\alpha\\nabla l(\\theta_{t+i},D_s),$   (1)\nFinally, the synthetic data is optimized by minimizing the distance metric, which is formulated as:\n$L = \\frac{||\\Theta_{t+N} - \\theta_{t+M}||}{||\\theta_{t+M} - \\theta_t||}.$   (2)"}, {"title": "3.2 Filtering Information Extraction", "content": "In section 2.1, we show using data selection to filter out unmatched samples could alleviate the misalignment caused in Information Extraction step. According to previous work [11], TM-based methods prefer easy information and choose to match only early trajectories when IPC is small. Conversely, hard information is preferred by high IPCs and they match only late trajectories. Hence, we should use easy samples to train early trajectories, while late trajectories should be trained with hard samples. To realize this efficiently, we first use the data selection method to measure the difficulty of samples contained in the target dataset. Then, during training expert trajectories, a scheduler is implemented to gradually incorporate hard samples into the training set while excluding easier ones.\nDifficulty Scoring Function Identifying the difficulty of data for DNNs to learn has been well studied in data selection area [17, 18, 30, 41]. For simplicity consideration, we use Error L2-Norm (EL2N) score [34] as the metric to evaluate the difficulty of training examples (other metrics can also be chosen, see Section 4.3.2). Specifically, let x and y denote a data point and its label, respectively. Then, the EL2N score can be calculated by:\n$\\mathcal{X}_t(x, y) = \\mathbb{E}||p(W_t, x) - Y||_2.$   (3)\nwhere $p(w_t, x) = \\sigma(f(w_t,x))$ is the output of a model f at training step t transformed into a probability distribution. In consistent with [41], samples with higher EL2N scores are considered as harder samples in this paper.\nScheduler The scheduler can be divided into the following stages. Firstly, the hardest samples are removed from the training set, ensuring that it exclusively comprises data meeting a predetermined initial ratio (IR). Then, during training expert trajectories, samples are gradually added to the training set in order of increasing difficulty. After incorporating all the data into the training set, the scheduler will begin to remove easy samples from the target dataset. Unlike the gradual progression involved in adding data, the action of reducing data is completed in a single operation, since now the model has been trained on simple samples for a sufficient time. (Please refer to Appendix A.2 for experimental comparisons)"}, {"title": "3.3 Filtering Information Embedding", "content": "To filter out misaligned information introduced by matching shallow-layer parameters, we propose to add a parameter selection module that masks out part of shallow layers for metric computation. Specifically, parameters of an agent network can be represented as a flattened array of length L that stores weights of agent models ordered from shallow to deep layers (parameters within the same layer are sorted in default order). The parameter selection sets a threshold ratio \u03b1 such that the first k = L\u03b1 parameters are not used for distillation. Then the parameters used for matching can now be formulated as:\n$\\Theta_{t+N} = \\{\\cancel{\\theta_0, \\theta_1,..., \\theta_{k-1}}, \\theta_k, \\theta_{k+1},..., \\theta_L\\}.$   (4)\nIn practice, the ratio \u03b1 should vary with the change of IPC. For smaller IPCs, it is necessary to incorporate basic information thus \u03b1 should be lower. Conversely, basic information is redundant in larger IPC cases, so \u03b1 should be higher accordingly."}, {"title": "4 Experiments", "content": "We compare PAD with several prominent dataset distillation methods, which can be divided into two categories: matching-based approaches including DC [54], DM [53], DSA [52], CAFE [45], MTT [1], FTD [7], DATM [11], TESLA [5], and kernel-based approaches including KIP [32], FRePo [55], RCIG [27]. The assessment is conducted on widely recognized datasets: CIFAR-10, CIFAR-100[19], and Tiny ImageNet [21]. We implemented our method based on DATM [11]. In both the distillation and evaluation phases, we apply the standard set of differentiable augmentations commonly used in previous studies [1, 7, 11]. By default, networks are constructed with instance normalization unless explicitly labeled with \"-BN,\" indicating batch normalization (e.g., ConvNet-BN). For CIFAR-10 and CIFAR-100, distillation is typically performed using a 3-layer ConvNet, while Tiny ImageNet requires a 4-layer ConvNet. Cross-architecture experiments also utilize LeNet [22], AlexNet [20], VGG11 [40], and ResNet18 [12]. More details can be found in the appendix."}, {"title": "4.2 Main Results", "content": "CIFAR and Tiny ImageNet We conduct comprehensive experiments to compare the performance of our method with previous works. As the results presented in Table 1, PAD outperforms previous matching-based methods on three datasets except for the case when IPC=1. When compared with kernel-based methods which use a larger network to perform the distillation, our technique exhibits superior performance in most cases, particularly when the compression ratio exceeds 1%. As can be observed, PAD performs relatively better when IPC is high, suggesting our filtering out misaligned information strategy becomes increasingly effective as IPC increases."}, {"title": "4.3 Ablation Study", "content": "To validate the effectiveness of each component of our method, we conducted ablation experiments on modules (section 4.3.1) and their hyper-parameter settings (section 4.3.2 and section 4.3.2)."}, {"title": "4.3.1 Modules", "content": "Our method incorporates two separate modules to filter information extraction (FIEX) and information embedding (FIEM), respectively. To verify their isolated effectiveness, we conduct an ablation study by applying two modules individually. As depicted in Table 3(b), both FIEX and FIEM bring improvements, implying their efficacy. By applying these two modules, we are able to effectively remove unaligned information, improving the distillation performance. More ablation results can be found in Table 8(b)."}, {"title": "4.3.2 Hyper-parameters of Filtering Information Extraction", "content": "Initial Ratio and Data Addition Epoch To filter the information learned by agent models, we initialize the training set with only easy samples, and the size is determined by a certain ratio of the total size. Then, we gradually add hard samples into the training set. In practice, we use two hyper-parameters to control the addition process: the initial ratio (IR) of training data for training"}, {"title": "4.3.3 Ratios of Parameter Selection", "content": "It is important to find a good balance between the percentage of shallow-layer parameters removed from matching and the loss of information. In Table 4(b), we show results obtained on different IPCs by discarding various ratios of shallow-layer parameters. The impact of removing varying proportions of shallow parameters on the distilled data and its relationship with changes in IPC is consistent with prior conclusions. For small IPCs, distilled data requires more low-level basic information. Thus, removing too many shallow-layer parameters causes a negative effect on the classification performance. By contrast, high-level semantic information is more important when it comes to large IPCs. With increasing ratios of shallow-layer parameters being discarded, we can ensure that low-level information is effectively filtered out from the distilled data."}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Distilled Images with Filtering Information Embedding", "content": "To see the concrete patterns brought by removing shallow-layer parameters to perform the trajectory matching, we present distilled images obtained by discarding various ratios of shallow-layer parameters in Figure 4. As can be observed in Figure 4(a), without removing any shallow-layer parameters to filter misaligned information, synthetic images are interspersed with substantial noises. These noises often take the form of coarse and generic information, such as the overall color distribution and edges in the image, which provides minimal utility for precise classification.\nBy contrast, images distilled by our enhanced methodology (see Figure 4(b) and Figure 4(c)), which includes meticulous masking out shallow-layer parameters during trajectory matching according to the compressing ratio, contain more fine-grained and smoother features. These images also encapsulate a broader range of semantic information, which is crucial for helping the model make accurate classifications. Moreover, we observe a clear trend: as the amount of the removed shallow-layer parameters increases, the distilled images exhibit clearer and smoother features."}, {"title": "5.2 Rationale for Parameter Selection", "content": "In this section, we analyze from the perspective of trajectory matching why shallow-layer parameters should be masked out. In Figure 5, we present the changes in trajectory matching loss across different layers as the distillation progresses. Compared to the deep-layer parameters of the agent model, a substantial number of shallow-layer parameters exhibit low loss values that fluctuate during the matching process (see Figure 5). By contrast, the loss values of the deep layers are much higher but consistently decrease as distillation continues. This suggests that matching shallow layers primarily conveys low-level information that is readily captured by the synthetic data and quickly saturated. Consequently, the excessive addition of such low-level information produces noise, reducing the quality of distilled datasets."}, {"title": "5.3 Parameter Selection Strategy", "content": "In the previous section, we observed a positive correlation between the depth of the model layers and the magnitude of their trajectory-matching losses. Notably, the loss in the first layer of the ConvNet was higher compared to other shallow layers. Consequently, we further compared different parameter alignment strategies, specifically by sorting the parameters based on their matching losses and excluding a certain proportion of parameters with lower losses. Higher loss values indicate greater discrepancies in parameter weights; thus, continuing to match these parameters can inject more information into the synthetic data. As shown in Table 4(c), sorting by loss results in an improvement compared with no parameter alignment, but filtering based on parameter depth proves to be more effective."}, {"title": "5.4 Other Methods with Data Selection", "content": "To further demonstrate the effectiveness of our FIEX, we compare ours with BLiP [47], which also uses a data selection strategy before distillation. It proposes a data utility indicator to evaluate if samples are 'useful' given an IPC setting, and then samples with low utility are pruned. As shown in Table 5(a), PAD brings better performance improvements on IPC1/10/50. Under a given data-dropping ratio, PAD's improvements over BLiP get larger as the IPC increases. This supports our conclusion that difficulty misalignment between IPCs and real data used is more harmful. PAD's data selection module is more effective in removing such misaligned information."}, {"title": "5.5 Generalization to Other Methods", "content": "In Section 2, we show that the two filtering modules of PAD can be generalized to existing matching-based DD algorithms. Here, we combine PAD with a more recent DD method that achieves great success on high-resolution datasets, SRe\u00b2L [48], to show that PAD has good generalizability. As shown in Table 5(c), by filtering out misaligned information extracted in the squeeze stage, the performance of SRe\u00b2L improves on both small and large IPC settings. This further supports our claim that PAD is beneficial to matching-based methods for performance improvement."}, {"title": "6 Related Work", "content": "Introduced by [46], dataset distillation aims to synthesize a compact set of data that allows models to achieve similar test performances compared with the original dataset. Since then, a number of studies have explored various approaches. These methods can be divided into three types: kernel-based, matching-based, and using generative models [50]."}, {"title": "7 Conclusion", "content": "In this work, we find a limitation of existing Dataset Distillation methods in that they will introduce misaligned information to the distilled datasets. To alleviate this, we propose PAD, which incorporates two modules to filter out misaligned information. For information extraction, PAD prunes the target dataset based on sample difficulty for different IPCs so that only information with aligned difficulty is extracted by the agent model. For information embedding, PAD discards part of shallow-layer parameters to avoid injecting low-level basic information into the synthetic data. PAD achieves SOTA performance on various benchmarks. Moreover, we show PAD can also be applied to methods based on matching gradients and distributions, bringing improvements across various IPC settings.\nLimitations Our alignment strategy could also be applied to methods based on matching gradients and distributions (see Appendix A.1). However, due to the limitation of computing resources, for methods based on matching distributions and gradients, we have only validated our method's effectiveness on DM [53] and DC [54], and SRe\u00b2 L [48] (see Table 5(c), Table 6 and Table 7)."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Filtering Misaligned Information in DC and DM", "content": "Although PAD is implemented based on trajectory matching methods, we also test our proposed data alignment and parameter alignment on gradient matching and distribution matching. The performances of enhanced DC and DM with each of the two modules are reported in Table 6 and Tabl 7, respectively. We provide details of how we integrate these two modules into gradient matching and distribution matching in the following sections.\nGradient Matching We use the official implementation\u00b3 of DC [54]. In the Information Extraction step, DC uses an agent model to calculate the gradients after being trained on the target dataset. We employ filter misaligned information in this step as follows: When IPC is small, a certain ratio of hard samples is removed from the target dataset so that the recorded gradients only contain simple information. Conversely, when IPC becomes large, we remove easy samples instead.\nIn the Information Embedding step, DC optimizes the synthetic data by back-propagating on the gradient matching loss. The loss is computed by summing the differences in gradients between each pair of model parameters. Thus, we apply parameter selection by discarding a certain ratio of parameters in the shallow layers.\nDistribution Matching We use the official implementation of DM [53], which can be accessed via the same link as DC. In the Information Extraction step, DM uses an agent model to generate embeddings of input images from the target dataset. Similarly, filtering information extraction is applied by removing hard samples for small IPCs and easy samples for large IPCs.\nIn the Information Embedding step, since DM only uses the output of the last layer to match distributions, we modify the implementation of the network such that outputs of each layer in the model are returned by the forward function. Then, we perform parameter selection following the same practice as before."}, {"title": "A.2 Data Scheduler", "content": "To support the way we design the data scheduler to remove easy samples at late trajectories directly, we compare direct removal with gradual removal. The implementation of gradual removal is similar to the hard sample addition. Experimental results are shown in Table 8(a) on CIFAR-10 and CIFAR-100. Only large IPCs are tested because only large IPCs match late trajectories. As can be observed, compared with gradually removing easy data, deleting easy samples in one operation performs better. This supports our conclusion that after being trained on the full dataset for some epochs, it is more effective for the model to focus on learning hard information rather than easy information by removing easy samples directly."}, {"title": "A.3 Experiment Settings", "content": "We use DATM [11] as the backbone TM algorithm and our proposed PAD is built upon. Thus, our configurations for distillation, evaluation, and network are consistent with DATM.\nDistillation. We conduct the distillation process for 10,000 iterations to ensure full convergence of the optimization. By default, ZCA whitening is applied in all the experiments.\nEvaluation. We train a randomly initialized network on the distilled dataset and evaluate its per-formance on the entire validation set of the original dataset. Following DATM [11], the evaluation networks are trained for 1000 epochs to ensure full optimization convergence. For fairness, the experimental results of previous distillation methods in both low and high IPC settings are sourced from [11].\nNetwork. We employ a range of networks to assess the generalizability of our distilled datasets. For scaling ResNet, LeNet, and AlexNet to Tiny-ImageNet, we modify the stride of their initial convolutional layer from 1 to 2. In the case of VGG, we adjust the stride of its final max pooling layer from 1 to 2. The MLP used in our evaluations features a single hidden layer with 128 units.\nHyper-parameters. Hyper-parameters of our experiments on CIFAR-10, CIFAR-100, and TinyIm-ageNet are reported in Table 9. Hyper-parameters can be divided into three parts, including FIEX, FIEM, and trajectory matching (TM). For FIEX, the ratio of easy samples removed for all IPCs is 10%. Soft labels are applied in all experiments, we set its momentum to 0.9.\nCompute resources. Our experiments are run on 4 NVIDIA A100 GPUs, each with 80 GB of memory. The amount of GPU memory needed is mainly determined by the batch size of synthetic data and the number of steps that the agment model is trained on synthetic data. To reduce the GPU usage when IPC is large, one can apply TESLA [5] or simply reducing the synthetic steps N or the synthetic batch size. However, the decrement of hyper-parameters shown in Table 9 could result in performance degradation."}]}