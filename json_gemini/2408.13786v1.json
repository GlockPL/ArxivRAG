{"title": "Localization of Synthetic Manipulations in Western Blot Images", "authors": ["Anmol Manjunath", "Viola Negroni", "Sara Mandelli", "Daniel Moreira", "Paolo Bestagini"], "abstract": "Recent breakthroughs in deep learning and generative systems have significantly fostered the creation of synthetic media, as well as the local alteration of real content via the insertion of highly realistic synthetic manipulations. Local image manipulation, in particular, poses serious challenges to the integrity of digital content and societal trust. This problem is not only confined to multimedia data, but also extends to biological images included in scientific publications, like images depicting Western blots. In this work, we address the task of localizing synthetic manipulations in Western blot images. To discriminate between pristine and synthetic pixels of an analyzed image, we propose a synthetic detector that operates on small patches extracted from the image. We aggregate patch contributions to estimate a tampering heatmap, highlighting synthetic pixels out of pristine ones. Our methodology proves effective when tested over two manipulated Western blot image datasets, one altered automatically and the other manually by exploiting advanced AI-based image manipulation tools that are unknown at our training stage. We also explore the robustness of our method over an external dataset of other scientific images depicting different semantics, manipulated through unseen generation techniques. We release our experimental code and the manipulated datasets at https://github.com/polimi-ispl/western-blot-synthetic-manipulation-localization.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent times, synthetic content generation methods have been gaining a lot of popularity. The rapid advancements of Artificial Intelligence (AI) over the past decade have incredibly simplified the creation of images exhibiting high quality and realism, easily fooling the human eye [1]\u2013[6]. Moreover, the newest image generation techniques allow users to alter images locally by combining parts of real and fake content, producing incredibly realistic manipulations [3], [7]. While opening the doors to exciting scenarios, the widespread use of AI-based image manipulation techniques poses significant challenges to the authenticity of digital content and public trust [8]. Indeed, AI-based image manipulation techniques can be easily exploited for malicious purposes such as spreading misinformation, performing identity theft and promoting fraudulent items and goods [9], [10].\nNo less important, it has been proved that image tampering extends beyond digital multimedia content and it is increasingly affecting scientific images, i.e., images included in scientific publications [11]\u2013[13]. This issue poses a significant threat to the integrity and reliability of scientific research. An analysis of the scientific literature revealed that approximately 6% of published papers contain manipulated images [14]. Another study, which examined a large dataset of more than 20,000 scientific papers, found that around 4% of them included manipulated figures [15].\nThe latter study demonstrated that the manipulation issue is quite prevalent in scientific images depicting Western blots [15]. Western blots, also known as western blotting or immunoblotting, are experimental techniques used to detect and quantify target proteins [16]. They are extensively used in biomedical literature, particularly in molecular biology and immunogenetics, given their high sensitivity and precision.\nStandard manipulations of Western blots include the selective removal or addition of bands, targeted adjustments to brightness or contrast, and the elimination of background elements [17]. This issue is significant considering that Western blots have been used in approximately 8 to 9% of protein-related publications over the past 30 years [18].\nVisual inspection is still the most common method for detecting manipulations in Western blot images. As a matter of fact, forensic techniques aimed at identifying local image tampering often struggle to detect manipulations in scientific images. This difficulty is often due to their lower pixel resolution and the numerous processing operations used to create realistic forgeries [19], [20]. Moreover, if the analyzed image has been synthetically manipulated, it would be almost impossible to detect the tampering traces through visual inspection [12]. Within some preliminary experiments, the authors of [12] confirmed that conventional image generation methods that use Generative Adversarial Networks (GANs) could produce Western blots that were nearly indistinguishable from real ones, even to expert observers [21], [22]. These findings highlight the critical need for increased vigilance to ensure the integrity of such images.\nIn this work, we tackle the task of image tampering localization in the context of Western blot images. We build upon the work of [20], where the authors addressed the detection of synthetically generated Western blots through GANs and Denoising Diffusion Probabilistic Models (DDPMs). Here, we propose a method that is capable of localizing the region of insertion of synthetic content in pristine Western blot images. Our proposed approach works by analyzing small squared patches extracted from the query image. For a given image, we sequentially extract image patches and feed each patch to a detector trained to distinguish between real and synthetic content. By aggregating the scores from all patches, we"}, {"title": "II. LOCALIZATION OF SYNTHETIC MANIPULATIONS", "content": "In this section, we formulate the tackled task and we detail the proposed pipeline to address it.\nThis paper proposes a method to localize the presence of synthetically generated content within pristine scientific images. Let us consider a manipulated image I with size $R \\times C$, that presents a synthetically generated tampered region T. We can associate I with a tampering mask M of size $R \\times C$, where M is a 2D binary matrix in which each pixel takes a value of 0 if the corresponding pixel in I is pristine and a value of 1 if the pixel is synthetically generated. Formally, we define the pixel of M with coordinates $(r, c)$ as\n$$M(r, c) = \\begin{cases} 1 & \\text{if } (r, c) \\in T, \\\\ 0 & \\text{otherwise.} \\end{cases}$$\nOur aim is to estimate a real-valued tampering heatmap H as shown in Fig. 1, which differentiates between real and synthetic regions, providing for each pixel the probability of it being synthetically generated. Thresholding H gives an estimate of M.\nOur goal is to localize synthetically generated regions within scientific images. To do so, we propose to work in a patch-wise fashion, extracting and analyzing multiple patches from a single image under analysis. A sketch of the proposed methodology is shown in Fig. 2. Our pipeline is composed of three main steps:\n1) Patch extraction, in which we sequentially extract small squared patches from the image under analysis.\n2) Convolutional Neural Network (CNN)-based detection, in which every patch is analyzed by a CNN-based detector trained to discriminate between genuine and synthetic content.\n3) Heatmap estimation, in which the patch scores are as-sembled to estimate a real-valued heatmap to tell real and synthetic pixels apart.\nGiven an image I under analysis, we sequentially extract small squared patches of P \u00d7 P pixels from it. Every patch is defined as $P_{i,j}$, (i,j) being the coordinates of its top-left corner pixel from the whole image I. We consider an overlap between one patch and the next one, extracting patches with a specific stride of S \u00d7 S pixels in both dimensions. Notice that the analyzed image I can be of any size greater than P\u00d7 P (which is very small), thus our proposed methodology can be potentially applied to both tiny and large-sized images.\nEvery extracted patch is analyzed by a CNN-based synthetic image detector which differentiates between real and synthetic content. To do so, we select a straightforward detector that has been successfully exploited in the state-of-the-art for deepfake detection tasks [25]\u2013[27]. In particular, we exploited the EfficientNet-B0 architecture which allows for a fast and lightweight training process. The detection network is trained on image patches $P_I$ and outputs a real score $p_{i,j} \\in [0,1]$, where 0 means pristine and 1 synthetic (i.e., $p_{i,j}$ represents the likelihood of a patch being synthetically generated).\nTo enhance robustness against post-processing operations, we employ strong data augmentations as recommended in several state-of-the-art studies on synthetic image detection [27]. These augmentations include horizontal and vertical flips, random 90-degree rotations, histogram equalization, random blurring, and random adjustments in brightness, contrast, color, and saturation. We also employ random downscaling and upscaling, and JPEG compression with quality factors randomly selected between 40 and 100. Each augmentation has a 50% probability of being applied, except for JPEG compression, which has an 80% probability. The parameters utilized are based on those outlined in [28].\nWe rearrange the detection scores according to their associated patch positions in the analyzed image, obtaining an estimated tampering heatmap H. We associate each score $p_{i,j}$ with a patch of the heatmap $P_H$, placed in the same location of the analyzed patch $P_I$ in the query image. The patch $P_H$ holds constant pixel values all equal to $p_{i,j}$. During the reconstruction procedure, heatmap"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Here, we present the datasets employed within this study and describe the training setup we used for the proposed real versus synthetic detector.\nWe select scientific images from the Western blots dataset released in [20]. This dataset contains 14K real Western blot images and 24K synthetic images generated by four generative models (6K images per generator), namely CycleGAN [29], Pix2pix [21], StyleGAN2-ADA [30], and one DDPM [31]. All images have size 256 \u00d7 256 pixels. Some examples of the real and synthetic images are shown in Fig. 3 and Fig. 4 respectively. We use 28K images from this dataset equally split between real and synthetic image types to train and evaluate our detector.\nThe dataset comprises 5690 tampered Western blot images and has been built on top of the evaluation set (see Section III-B for more details). We created the spliced images by inserting a synthetic squared patch of 64 \u00d7 64 pixels into a real host image at a random location. We extracted the patches from randomly chosen synthetically generated images belonging to the evaluation set. The amount of synthetic patches used is equally balanced between CycleGAN, Pix2pix, StyleGAN2-ADA and DDPM.\nThis dataset consists of 456 manipulated Western blot images. As the automatically manipulated dataset, it is built upon the evaluation set of the Western blots dataset. We manipulated by hand pristine images in this dataset to emulate real-world forgery scenarios. To do so, we employed three different tampering tools, i.e., DALL-E2 [3], Cleanup [7] and GIMP [23].\nIn all scenarios, we selected the area of tampering to perform insertion or deletion of Western blots. On GIMP this process was completely user-driven: we selected the pristine area and substituted it with synthetic content from the synthetic evaluation set to create realistic forgeries. DALL-E2 and Cleanup tools are based on advanced diffusion models and enable the user to select a pixel region and substitute it with synthetically generated content.\nA single image can contain one or multiple local manipu-lations. On average, the images in this dataset are tampered by 4.12%, 13.93%, and 16.71% for GIMP, Cleanup, and DALL-E2 respectively. The percentages correspond to the number of tampered with pixels over the total pixels in the given image.\nThis dataset consists of medical 3D Computed Tomography (CT) lung images with local manipulations. It consists of 8577 images tampered using GANs and Diffusion models working in 3D. Manipulations in M3Dsynth are performed by injecting or removing lung cancer nodules in real CT scans. Images in this dataset are tampered with by extracting a 3D cube of size 32mm from a real image, modifying the inner sides of the cube synthetically, and re-inserting it back in the real image. The authors exploited three different generation models to create the synthetic portions, namely a CycleGAN designed to work with 3D data [33], a modified DDPM [34], and CT-GAN [11].\nFor our experiments, we consider the evaluation set of this dataset. This partition includes 205 synthetically generated 3D images, each composed of multiple 2D slices with dimensions of 512 x 512 pixels.\nThe EfficientNet-B0 we employ within this study is pre-trained on the ImageNet dataset [35]. We fine-tune it for the task of synthetic image detection on the Western blots dataset, splitting the dataset into training (64%), validation (16%) and test (20%) partitions. We use batches of size 250, balanced in terms of real and synthetic samples, cross entropy loss, Adam optimizer, and a learning rate of $1 \\times 10^{-3}$. We employ a scheduler to reduce the learning rate when the validation loss plateaus, with a minimum learning rate of $1 \\times 10^{-8}$. A patience value of 50 is used as a threshold for early stopping based on no improvements of the validation loss."}, {"title": "IV. RESULTS", "content": "In this section, we present the results achieved by our proposed detection methodology. We inspect the patch extraction parameters that best suit our synthetic detection task. We report our results on the automatically manipulated Western blots and on the realistically tampered ones. We perform an analysis of the false alarms returned by our detector on pristine Western blot images. We conclude this section by investigating the robustness of our methodology against the M3Dsynth dataset, which represents a challenging test set including different semantic content and different synthetic generation traces."}, {"title": "A. Parameters Selection for Synthetic vs Real Detection", "content": "In the patch extraction phase, we select patches of size P\u00d7 P pixels, investigating different values P\u2208 [32, 64, 96, 128]. This results in four synthetic versus real detectors, each having a different input patch size. To define the best patch size for our task, we test the four versions of our synthetic detector on the test partition of the Western blots dataset (extracting all the non-overlapping patches of P \u00d7 P pixels from the testing images)."}, {"title": "B. Performance on Automatically Manipulated Western blots", "content": "In this section, we assess the performance of the model in detecting and localizing the tampering region in automatically manipulated Western blot images. A few examples of these images are shown in the second column of Fig. 5. These are not meant to be realistic images (as the other datasets we are using), but help providing sufficient statistical results. As specified before, we always extract patches of size 32 \u00d7 32 pixels from the image under analysis. We experiment with various stride values of S \u00d7 S pixels, considering S \u2208 [4, 8, 16, 32].\nResults show that lower stride values lead to more accurate heatmap estimates, which help in localizing tampered regions. This outcome was expected since lower strides allow to exploit multiple patches contributions for estimating the heatmap"}, {"title": "C. Performance on Realistically Tampered Western blots", "content": "Here, we test our detector on realistically tampered Western blot images. Some examples are shown in Fig. 7, for all three different tampering methods.\nTo validate our results, we benchmark against a state-of-the-art model recently developed for the localization of image ma-nipulations. The selected baseline is the TruFor method [24], which is based on a transformer network trained on a huge amount of images, undergone many different post-processing. It has demonstrated to achieve excellent performances for"}, {"title": "D. Handling False Alarms", "content": "We conduct an additional experiment to assess the behavior of our detector in absence of manipulations, i.e., we test it over pristine Western blots and we check if it is keen to return false alarms. To this purpose, we compute the threshold associated with the maximum BA achieved over the realistically tam-pered Western blots dataset, which corresponds to heatmap values equal to 0.736. Pixels with detection scores below the threshold are classified as real, while those with scores greater than or equal to the threshold are classified as synthetic.\nAs a test set, we consider a subset of pristine Western blots from the evaluation partition that have not been used to generate realistically tampered images. This subset comprises 1950 genuine images. Considering the threshold computed above, we compute the correct detection rate as the number of pixels correctly classified as real vs the total amount of pixels, obtaining a final value of 99.3%. These findings suggest that we can guarantee a low rate of false alarms when our detector is presented with authentic images."}, {"title": "E. Performance on M3Dsynth", "content": "In this section, we test our proposed method in a challeng-ing scenario, assessing our generalization capabilities on the M3Dsynth dataset.\nWhile the detector preserves some robust-ness, results show an evident drop in its performance with respect to previous experiments. This was indeed an expected outcome for two main reasons. First, M3Dsynth images and Western blots completely differ in terms of semantics. Second, the synthetic generation techniques are different from those included in our training set: some of the generators have been seen only in their 2D versions (CycleGAN and DDPM), others are completely unknown (CT-GAN)."}, {"title": "V. CONCLUSIONS", "content": "In this work, we address the task of localizing synthetic tampering regions within Western blot images. To tackle this issue, we propose a method working in a patch-wise fashion: we iteratively extract small image patches from the image under analysis, and we pass each patch through a synthetic vs real detector providing the likelihood of every patch being synthetically generated. Eventually, we aggregate patches' scores and build a tampering heatmap that reveals the image area in which the manipulation has been performed.\nTo assess our performances, we created two different evaluation datasets. One dataset encompasses automatically generated spliced images, while the other contains manu-ally performed manipulations using advanced AI-based image manipulation tools that were not known at training phase. Both dataset have been publicly released to allow for future comparisons and investigations by the forensic community.\nThe proposed method achieves excellent performances on both automatically and realistically tampered with Western blots, exhibiting an extremely reduced false alarms rate. We also compare our results with a state-of-the-art methodology, discussing advantages and limitations of both solutions. Ad-ditional testing for robustness on the M3Dsynth dataset [32], which contains completely different semantics and manipula-tions, yields promising results."}]}