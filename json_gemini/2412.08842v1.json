{"title": "KAJAL: Extracting Grammar of a Source Code Using Large Language Models", "authors": ["Mohammad Jalili Torkamani"], "abstract": "Understanding and extracting the grammar of a domain-specific language (DSL) is crucial for various software engineering tasks; however, manually creating these grammars is time-intensive and error-prone. This paper presents KAJAL, a novel approach that automatically infers grammar from DSL code snippets by leveraging Large Language Models (LLMs) through prompt engineering and few-shot learning. KAJAL dynamically constructs input prompts, using contextual information to guide the LLM in generating the corresponding grammars, which are iteratively refined through a feedback-driven approach. Our experiments show that KAJAL achieves 60% accuracy with few-shot learning and 45% without it, demonstrating the significant impact of few-shot learning on the tool's effectiveness. This approach offers a promising solution for automating DSL grammar extraction, and future work will explore using smaller, open-source LLMs and testing on larger datasets to further validate KAJAL'S performance.", "sections": [{"title": "1 Introduction", "content": "Grammars have many applications in engineering and computer science [6]. They play an essential role in defining the structure that code must follow to be syntactically correct and interpretable by compilers [3, 18] or interpreters [11]. Grammars provide developers with a standardized way to write and read code, ensuring reliability in compilation, execution, and maintenance [9]. Moreover, they facilitate the creation of domain-specific languages (DSLs) [2, 13], enabling tailored solutions for specific tasks in software and enhancing both productivity and code quality. Understanding and defining the grammar of such languages is crucial for enabling advanced tooling, such as parsers, syntax highlighters, and transpilers.\nHowever, extracting the grammar of an arbitrary DSL poses a significant challenge due to the complexity and diversity of these languages, which often deviate from the conventions of more familiar programming languages. For example, JSX, a DSL widely used with JavaScript, illustrates this complexity. Listing 1 shows an example code snippet for this language. Although JSX shares some features with JavaScript, it allows developers to write HTML-like syntax directly within their JavaScript code. In contrast, standard JavaScript typically requires using the document.createElement method to create HTML elements (Listing 2). Consequently, the grammar of JSX differs significantly from that of JavaScript.\nIn addition to their syntactical differences, the underlying rules governing these languages are not always well-documented or consistent, especially in legacy or ad-hoc DSLs used in specific industries [12]. Real-world scenarios, such as the intricate rules of a trading DSL for finance [16] or a markup language customized for medical records [17], exemplify this complexity. As a result, manually extracting and formalizing grammar in such cases would be prohibitively time-consuming and error-prone.\nTo resolve this issue, several traditional approaches are employed for extracting the grammar of programming languages, including manual grammar specification, lexical and syntactic analysis [7], statistical [4], or machine learning-based grammar induction [8], and grammar mining from existing code. While manual specification allows for precise control over grammar, it is labor-intensive and depends heavily on expert knowledge. Lexical and syntactic analysis can generate grammars from existing codebases but often results in incomplete or overly complex outputs. Statistical methods, while scalable, require large amounts of labeled data for supervised learning and may not yield grammars that are easily interpretable by humans. Grammar mining focuses on analyzing existing source code but may produce grammars tailored to specific codebases, lacking generalizability. Thus, a feasible approach to extracting the grammar of arbitrary DSLs would streamline the construction of these languages systematically, aiding in the design of parsers, transpilers, and static analysis tools. Such automated methods would also enhance the accessibility and integration of DSLs across various fields and domains.\nLarge language models (LLMs) [1] present a promising approach for grammar extraction and offer a flexible and adaptive method. LLMs can learn from a diverse range of programming examples and natural language descriptions, enabling them to infer complex grammar patterns. Additionally, they can be fine-tuned with minimal labeled data, facilitating efficient grammar extraction across various contexts while reducing the manual effort required.\nIn this paper, we introduce KAJAL as an innovative approach to extract the grammar of domain-specific languages. By employing prompt engineering techniques and providing relevant examples through few-shot learning, KAJAL assists LLMs in extracting grammar from relatively complex source code. Our approach is automated and does not require implementing LLMs locally; instead, it leverages their Application Programming Interface (APIs) for inference, resulting in a more powerful and cost-effective solution in terms of inference time and resource requirements. Furthermore, it eliminates the need for labeled data or training, making it more convenient than traditional machine learning-based approaches. To evaluate our method, we collected a dataset that the LLM has not encountered before. We divided this dataset into two distinct subsets: one for few-shot learning and the other for evaluation. We employed various quantitative and qualitative metrics to assess KAJAL's accuracy and will discuss its strengths and weaknesses throughout the paper."}, {"title": "2 Example", "content": "In this section, we present a practical example to demonstrate the process of generating grammar for a DSL using KAJAL. This example provides an overview of KAJAL's purpose, outputs, and operational approach, as well as the benefits of our proposed method.\nKAJAL is designed as an end-to-end tool that generates grammar for a given code snippet extracted from the dataset. This means that it takes two datasets as input and produces an output JSON file containing the inferred grammar for each dataset record, along with evaluation metrics that indicate performance and accuracy. The approach incorporates prompt engineering, dynamically generating a prompt for the LLM based on the input code snippet to effectively produce grammar. Additionally, it performs few-shot learning by providing three similar code snippets and their corresponding grammar to provide the LLM with some contextual insight.\nA key component of KAJAL's operation is a feedback-driven iterative process along with few-shot prompting-a technique in prompt engineering that enhances inference accuracy through iterative prompt refinement. Recent research [14, 20] has demonstrated the effectiveness of few-shot prompting in increasing inference accuracy. In KAJAL, this is implemented by creating a prompt from a code snippet, which the LLM uses to generate grammar in a specified output format. After the initial inference, the inferred grammar is tested by parsing the input code snippet. If parsing succeeds, the record is marked as a correct inferred grammar, and the process moves to the next dataset record. If parsing fails, KAJAL performs few-shot prompting. This means that it extracts the error message from the parsing library and feeds it back to the LLM as additional context for prompt correction. This feedback loop continues for up to ten iterations, providing the error message for each unsuccessful inference until a correct grammar (which can parse the given code snippet from the dataset) is generated or the attempt is marked as failed.\nTo illustrate this process, we consider a custom code snippet comprising a set of statements forming an arbitrary DSL where each statement starts with a capital letter K. As this DSL is unique and unfamiliar to the LLM, the LLM has no prior exposure to its syntax or grammar. Therefore, the LLM must derive patterns from the code snippet itself rather than relying on its own training data."}, {"title": "3 Technical Details", "content": "In this section, we provide a comprehensive description of KAJAL's implementation, operational workflow, and the types of information it processes. Additionally, we include a schematic diagram to visually represent the workflow, aiding in a clearer understanding of KAJAL's functionalities.\nOverall, KAJAL is an end-to-end tool pipeline implemented in Python, which can be executed using the same language.\nIt requires two input datasets: one for evaluation and another for few-shot learning. The end-to-end feature ensures that the tool processes the inputs and produces outputs automatically. The first input dataset, the evaluation dataset, is used to assess the performance of KAJAL. The second input dataset, the few-shot learning dataset (FSL dataset), is used to extract similar records. This process will be elaborated later. Upon execution, the pipeline generates an output JSON file containing the inferred grammar for the input dataset's records, along with quantitative results computed for the overall inference. Moreover, KAJAL employs a feedback-driven iterative approach, which means that it iteratively refines its inference by providing the LLM with the latest error messages from the parser as feedback when parsing the input record with the inferred grammar.\nThe pipeline comprises five sequential components. Each component processes its inputs and passes the results to the next component. Figure 1 illustrates the KAJAL workflow.\nAs shown in Figure 1, the pipeline iterates over the evaluation dataset. For each record, the following process occurs:\nThe DSL code snippet is extracted from the evaluation dataset. This snippet is passed to the Similar Grammar Extractor component, which identifies the three most similar code snippets from the FSL dataset along with their associated grammars. To compute similarity during prompt engineering, KAJAL vectorizes both the candidate snippet and each snippet in the FSL dataset, padding them to ensure uniform sizes. It then applies the cosine similarity formula, sorts the results in descending order, and uses a minimum similarity threshold of 0.5:\ncosine similarity(A, B) =  \\frac{A \\cdot B}{||A|| x ||B||}\nThis threshold balances between identical records, and dissimilar records, ensuring a fair selection of snippets.\nThe extracted similar code snippets, along with the original input snippet, are then passed to the Prompt Builder component. This component constructs a prompt for the next component, the LLM. To construct the prompt, the system field (illustrated in Figure 2) is used. This field contains instructions guiding the LLM in understanding the input and generating the required output in the desired format. For the user field, the source snippet and, optionally, the feedback message (along with the most recent invalid grammar) are included. In few-shot learning experiments, similar code snippets and their grammar are passed to the LLM as returned to the LLM for refinement. This feedback serves as additional context for the LLM. The process is repeated for up to ten iterations. Regardless of success or failure, all details are logged in the output JSON file.\nDespite the advantages of our work over related works (described in Section 5), KAJAL has limitations. First, it does not ensure semantic accuracy in inferred grammars, potentially leading to ambiguities. This limitation arises because grammar extraction often requires domain-specific knowledge. Second, KAJAL lacks mechanisms to validate the quality of feedback or mitigate biases in its iterative approach. Finally, for large codebases requiring extensive token usage, KAJAL may face limitations depending on the adopted LLM's capabilities and constraints."}, {"title": "4 Evaluation", "content": "The purpose of this section is to demonstrate how KAJAL performs when applied to our evaluation dataset. We aim to showcase our evaluation methodology, establish clear evaluation goals, and introduce the metrics that allow us to assess KAJAL's efficiency through qualitative and quantitative measures."}, {"title": "4.1 Dataset Collection", "content": "To evaluate KAJAL's effectiveness, the selection of an appropriate dataset is essential. As discussed in Section 1, we use the GPT-3 model to maintain consistency, ensuring its cut-off date does not shift over time. For a fair evaluation, a selected dataset ideally should not be seen by the LLM during training, ensuring that KAJAL's grammar inference is unbiased and based solely on the input code. We also collected an additional dataset (independent of release dates), to support few-shot learning. This secondary dataset is excluded from the evaluation dataset but provides context for the LLM.\nTo the best of our knowledge, by the time we wrote this paper, we could not find any appropriate datasets released after the cut-off date. Therefore, we used the ChatGPT-40 model to create 23 code snippets. Ten of these code snippets are similar to well-known programming languages but contain slight differences in syntax. The remaining snippets cover various domains, including networking, data transformation, user permissions, task management, mathematical expressions, and more. These code snippets are stored in a single JSON file. We then split this dataset into two distinct subsets: an evaluation set (containing 20 samples) with release dates after the LLM's cut-off, and a few-shot learning dataset (containing 3 samples) with no cut-off restrictions. By connecting these datasets to our end-to-end tool, KAJAL can sequentially process each record in the evaluation set, perform inference, evaluate the results, and store them in a comprehensive JSON file."}, {"title": "4.2 Methodology", "content": "We conduct the experiments in two distinct variants. The first is our baseline, where we run KAJAL without any few-shot learning, relying solely on the LLM's default inference capability using prompt engineering. This approach reveals the base performance of our chosen LLM. We will compare the results of the second variant with this baseline to gauge improvement.\nIn the second variant, we adopt a few-shot learning approach, where similar code snippets along with their grammar are fed to the LLM, enabling the LLM to have more accurate inference and generate a valid grammar that can parse the given source code."}, {"title": "4.3 Evaluation Metrics", "content": "To assess KAJAL's accuracy and performance, we defined a set of metrics that help us gauge the effectiveness of our approach and tool. These metrics also assist in identifying areas for improvement. The metrics are as follows:\n\u2022 Parsing Accuracy Percentage (PAP): This metric reflects the percentage of code snippets for which KAJAL inferred the correct grammar. It indicates the accuracy and efficiency of our approach in generating grammars that can parse the given code snippet. PAP is calculated as follows:\nPAP = \\frac{Number of Correctly Parsed Code Snippets}{Total Number of Code Snippets} \u00d7 100\n\u2022 Grammar Validity Index (GVI): This metric measures whether the inferred grammar (and generally, output) is structurally valid and can be processed by a specific parsing tool (e.g., lark), regardless of the code snippet. It evaluates the structural correctness of the generated grammar, helping us understand the LLM's capabilities in generating grammars compatible with the parsing library. GVI is calculated as follows:\nGVI =  \\frac{Number of Structurally Valid Grammars}{Total Number of Code Snippets} \u00d7 100\n\u2022 Iteration Success Rate of Grammar Correctness in the nth Iteration (ISRGC-n): This measures the success rate of generating correct grammar by the nth iteration. It helps us understand the effectiveness of the feedback-driven approach in producing correct grammar.\nWe will also conduct manual evaluations to assess the qualitative aspects of the generated grammar. This analysis will help categorize any failures and identify areas for improvement, guiding future enhancements."}, {"title": "4.4 Results and Analysis", "content": "After conducting the experiments, the following statistics have been achieved:\nThe results presented in Table 1 and Figures 4 and 5 highlight the impact of incorporating few-shot learning into the grammar extraction process for a DSL using LLMs. Without few-shot learning, the system achieved a validity accuracy of 45.0% and a correctness accuracy of 45.0%, reflecting a relatively low rate of valid and correct inferences. However, when few-shot learning was introduced, both validity and correctness accuracy improved significantly to 60.0%. This increase suggests that few-shot learning helps the model generalize better from limited examples, leading to more accurate inferences and a reduction in the number of invalid or incorrect outputs."}, {"title": "4.4.1 ISRGC / With Few-Shot Learning", "content": "The iterative approach with few-shot learning demonstrates a clear trend of generating correct inferences in the early iterations. In the first two iterations, the model produces 5 and 4 correct grammars, respectively, showing that it is able to refine its grammar generation effectively. However, as iterations continue, the number of correct inferences declines, converging to 0 by iteration 5. Despite this decline, the model still generates some correct inferences in later iterations (1 correct inference in iteration 6), which suggests that the iterative feedback approach helps the model gradually refine the grammar over time. In other words, the iterative process still provides opportunities for it to generate correct grammar in the next iterations."}, {"title": "4.4.2 ISRGC / Without Few-Shot Learning", "content": ": Without few-shot learning, the model shows a slightly different behavior. The first iteration produces 5 correct inferences, but there is a sharp drop to 1 correct inference in the second iteration, and no correct inferences are generated in subsequent iterations, with the exception of iterations 5 and 7, where 1 and 2 correct inferences appear respectively. Although this fluctuation shows the possibility of inferring correct grammars in later iterations, having low PAP shows the impact of few-shot learning in the KAJAL's total accuracy.\nAs a comparison of the described variants, the iterative approach proves to be essential in both scenarios, but especially in the case without few-shot learning. While the model's performance declines over time in both cases, the iterative process allows it to continue making corrections and refining its grammar, ensuring that some correct inferences are generated even in later iterations. Without this iterative feedback loop, the model would not have been able to make further inferences for each record, hindering its ability to improve or adjust its grammar generation. Therefore, the iterative approach plays a crucial role in increasing the KAJAL's accuracy over time, particularly when few-shot learning is not present.\nThe experiments for the attached datasets took less than 2 minutes to complete for each variant.\nAs a manual analysis of the experiment results, and according to our released artifacts, our analysis shows that the most frequent errors across all variants when making inferences are as follows:\n\u2022 Rule [RULE] used but not defined.\n\u2022 Unexpected token [TOKEN].\n\u2022 No terminal matches [VARIABLE] in the current parser context.\nThe error messages observed in the experiment results reveal critical insights into the challenges faced when inferring grammars with Lark. The most frequent issue, \"Rule [RULE] used but not defined,\" indicates that the inferred grammar references rules that have not been declared, suggesting incomplete or inconsistent rule generation during inference. The error \"Unexpected token [TOKEN]\" points to scenarios where the parser encounters input sequences that deviate from the expected syntax, often due to incorrect token definitions or misplaced rules in the grammar. Finally, \"No terminal matches [VARIABLE] in the current parser context\" highlights mismatches between the input and the defined grammar's terminal symbols, signaling either gaps in coverage or conflicts in the tokenization process. Together, these errors underline the complexity of achieving fully functional and syntactically sound grammar, emphasizing the need for more robust inference strategies and systematic validation of generated rules."}, {"title": "5 Related Works", "content": "Grammar induction has been an active area of research, with various approaches proposed to infer the structure of programming languages or domain-specific languages. These methods range from traditional algorithms to modern machine learning techniques, each addressing specific challenges such as balancing precision and generality or handling complex syntax. In this section, we discuss some of the most relevant research works that have explored grammar inference through computational methods, highlighting their contributions and limitations in comparison to our approach, which leverages Large Language Models (LLMs) to extract grammars from source code.\nJain et al. proposed an incremental approach [5] for extracting the grammar from a source code (or the dialects) written in C, C++, and COBOL in an interactive way. Their approach starts with an approximate grammar and iteratively refines it by parsing source programs, identifying constructs not covered by the current grammar, and extending the grammar with rules sourced from a knowledge base or manually provided by the user. This work differs from our research in several ways. While their method relies on user interaction and a pre-defined knowledge base for refining the grammar, our research focuses on leveraging LLMs to extract grammar automatically. This shift eliminates the need for extensive manual intervention or pre-existing knowledge bases, enabling a more scalable and automated process. Additionally, KAJAL addresses not only dialects of existing languages but also explores the potential of LLMs to infer new syntactic patterns from examples, extending the scope beyond legacy systems to novel grammar extraction challenges.\nSaha et al. proposed a programming language grammar inference system called Gramin [15], which adapts grammar inference techniques traditionally used in the Natural Language Processing (NLP) domain to the programming language domain. Their approach includes an algorithm named Focus, designed to narrow the search space to non-terminals associated with parsing errors. Unlike KAJAL, which uses a feedback-driven iterative approach to refine the inferred rules, Gramin incrementally modifies the candidate grammar by adding new rules to resolve parsing errors. Gramin's methodology relies heavily on heuristics to prioritize rule selection and utilizes Prolog's natural backtracking for efficient search and ranking of possible grammar rules. This makes Gramin less scalable compared to KAJAL's approach, where the use of LLMs can facilitate broader grammar extraction with potentially higher adaptability and automation.\nIn another research conducted by L\u00e4mmel et al. [10], a semi-automated approach was proposed for extracting grammars of programming languages from existing artifacts such as compilers and language reference manuals. This method is particularly valuable for facilitating software renovation efforts, enabling the processing of legacy code where detailed grammar specifications might not be readily available. Furthermore, as most programming languages evolve, this approach supports the incremental extension of grammar definitions. The methodology involves multiple steps, starting with raw grammar extraction, followed by static error resolution, such as fixing unconnected non-terminals, and test-driven correction, where sample code is parsed to identify and address issues in the extracted grammar. Additional steps include modularization, beautification, and adaptation for specific use cases such as software renovation. A key distinction from KAJAL is that L\u00e4mmel et al.'s approach relies on pre-existing structured artifacts and adapts them for renovation and analysis. In contrast, KAJAL leverages LLMs to directly extract grammar from source code, potentially streamlining grammar acquisition in scenarios where artifacts are incomplete or unavailable.\nThe latest related work presented within this paper is a research conducted by Mernik et al., exploring the challenge of grammar induction using genetic programming for Domain-Specific Languages. The authors present a novel evolutionary algorithm that employs grammar-specific genetic operators for tasks like crossover and mutation, achieving grammar inference. The approach is validated through experiments, demonstrating its feasibility in generating parsers for DSLs. While the paper acknowledges the difficulty of inferring grammars for general-purpose programming languages, it successfully applies its method to DSLs, addressing syntax generation and, to some extent, semantics via attribute grammars. This work differs significantly from KAJAL, which leverages LLMs for extracting grammars from source code. Unlike the evolutionary algorithm approach used in this paper, which relies on structured genetic programming and predefined fitness criteria, KAJAL utilizes LLMs' contextual understanding and pattern recognition capabilities. Thus, our research enables a broader application scope, including complex and general-purpose languages, without requiring extensive manually designed evaluation functions."}, {"title": "6 Conclusion", "content": "In this paper, we introduce KAJAL as a novel approach for extracting the grammar of domain-specific languages. Unlike prior methods, particularly those based on machine learning, our approach leverages large language models as grammar generators through APIs, making it independent of their underlying implementations. Our experiments show the effectiveness of our approach, with 60% accuracy when using few-shot learning and 45% accuracy without it, highlighting the impact of few-shot learning on increasing the accuracy of KAJAL. In the future, we plan to use smaller, open-source LLMs and test our tool on larger datasets to further understand the effectiveness of our approach."}, {"title": "7 Data Availability", "content": "All artifacts including the source codes and experiments' results are available online 1."}]}