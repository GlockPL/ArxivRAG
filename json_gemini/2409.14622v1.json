{"title": "LatentQGAN: A Hybrid QGAN with Classical Convolutional Autoencoder", "authors": ["Alexis Vieloszynski", "Soumaya Cherkaoui", "Ola Ahmad", "Jean-Fr\u00e9d\u00e9ric Laprade", "Olivier Nahman-L\u00e9vesque", "Abdallah Aaraba", "Shengrui Wang"], "abstract": "Quantum machine learning consists in taking advantage of quantum computations to generate classical data. A potential application of quantum machine learning is to harness the power of quantum computers for generating classical data, a process essential to a multitude of applications such as enriching training datasets, anomaly detection, and risk management in finance. Given the success of Generative Adversarial Networks in classical image generation, the development of its quantum versions has been actively conducted. However, existing implementations on quantum computers often face significant challenges, such as scalability and training convergence issues. To address these issues, we propose LatentQGAN, a novel quantum model that uses a hybrid quantum-classical GAN coupled with an autoencoder. Although it was initially designed for image generation, the LatentQGAN approach holds potential for broader application across various practical data generation tasks. Experimental outcomes on both classical simulators and noisy intermediate scale quantum computers have demonstrated significant performance enhancements over existing quantum methods, alongside a significant reduction in quantum resources overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "Generative Adversarial Networks [1] (GANs) have become widely recognized as an effective approach for data generation [2], anomaly detection [3] as well as other common applications in machine learning. It learns in an unsupervised way to generate new data with the similar statistics to those of the training set. A GAN consists of two neural networks, a generator and a discriminator. The purpose of the generator is to generate new (fake) data that imitates the distribution of the training dataset, while the purpose of the discriminator is to distinguish between real data and the generated fake ones. The two neural networks are trained in competition, with the generator trying to fool the discriminator and the discriminator trying to distinguish between true and fake data.\nIn recent theoretical studies, it has been suggested that quantum generative models could demonstrate an advantage compared to classical equivalents [4]\u2013[6]. With the improvement of quantum computers, much work has been done by the quantum computing community towards implementing Quantum Generative Adversarial Networks (QGANs) [7], [8]. Among the existing implementations, the ability to effectively run the training process on real quantum computers and that to represent the high-dimensional data distribution are among the main challenges, the latter being known as a scalability issue. Moreover, existing QGANs suffer from lack of ability to generate diverse data that adheres to the desired data distribution, which is known as mode collapse [9].\nMotivated by these challenges, we propose in this paper a new model, named LatentQGAN, to address scalability and mode collapse issues in an attempt to generate high dimensional data. We focused our effort on facilitating the training and model evaluation on real quantum computers, which resulted in a significant improvement over previous works. The contributions of this paper are as follows:\n(i) We propose a hybrid quantum-classical model for data generation, applied on images. The new model integrates an autoencoder and maps images into a latent space with reduced dimensionality which makes the new representation more compatible with the quantum generator.\n(ii) By learning a compressed representation of the original dataset, the new model maximizes the efficiency of quantum circuits while minimizing the utilization of quantum resources such as the quantum circuit depth or the number of qubits. This allows us to circumvent the limitations of noisy intermediate scale quantum (NISQ) computers such as decoherence and limited qubit connectivity, enabling training of the model on quantum computers.\n(iii) Our experiments, conducted both on simulators and on real quantum machines, show that LatentQGAN outperforms the existing implemented QGANs, and classical counterparts with the same number of parameters. The model has been trained on the MNIST dataset [10], which is commonly used in both classical and quantum machine learning evaluations [1], [11]."}, {"title": "II. BACKGROUND", "content": "GANs [1] are characterized by a dueling interplay between two neural networks: the generator G and the discriminator D. This dynamic is encapsulated in a minimax game scenario, where the generator strives to produce synthetic data samples x from random noise z, aiming to minimize the likelihood of being discerned as fake by the discriminator. Conversely, the discriminator endeavors to distinguish between real and synthetic samples, seeking to maximize its classification accuracy. D has to classify the fake data G(z) as 0 to indicate it is fake, and real data x as 1 to indicate it is real. Depending on the output, the parameters are updated, constituting a single iteration in the training process. This adversarial process is governed by a loss function L(D, G), defined as:\n$\\min_{G}\\max_{D} L(D, G) = E_{x \\sim P_{data} (x)}[log D(x)] + E_{z \\sim p_{z}(z)} [log(1 \u2013 D(G(z)))],$ \nwhere $P_{data}(x)$ represents the distribution of real data and $p_{z}(z)$ denotes the distribution of the noise. The loss function drives the networks to optimize their respective objectives: the discriminator's loss $L_{D}$ and the generator's loss $L_{G}$.\nAutoencoders [12] represent a cornerstone in the domain of unsupervised learning, particularly in the realm of image processing and computer vision. Derived from traditional autoencoders, convolutional autoencoders (CAEs) leverage convolutional layers to capture spatial hierarchies and extract meaningful features from input data [13]. At their core, CAEs consist of two main components: the encoder and the decoder. The encoder utilizes convolutional layers that reduce spatial dimensions of the input data while extracting relevant features into what is called a latent space. Conversely, the decoder employs transposed convolutional layers (also known as deconvolution or upsampling layers) to reconstruct the original input from the encoded representation.\nThe training objective of CAEs involves minimizing a loss function that measures the discrepancy between the input and the reconstructed output. Typically, the mean squared error (MSE) loss is employed for this purpose. CAEs find widespread applications in various domains, including image denoising [14], compression [15] along with other widespread applications in artificial intelligence. Moreover, they serve as a fundamental building block for more advanced techniques such as image generation and anomaly detection.\nQuantum computing has witnessed a surge in interest in recent years, mainly due to three factors: scientific and engineering breakthroughs in hardware design and fabrication [16], the availability of quantum processors through cloud access, and the development of variational algorithms which provide a framework to experiment with the limited resources of NISQ quantum devices. Variational quantum algorithms [17] (VQAs) proceed by optimizing a cost function $C(\\theta)$ which encodes the problem of interest. Here, $\\theta$ is a set of real parameters, and the solution is obtained by a minimization process,\n$\\theta^* = arg \\min C'(\\theta)$.\nThe optimization routine is performed on a classical computer while the evaluation of the cost function is offloaded to a quantum computer. We can express this cost function as the expectation value of some observable O, with respect to a parameter-dependent state,\n$\\langle O \\rangle = Tr [OU (\\theta) pU^{\\dagger} (\\theta)]$.\nHere, p is any initial state, generally taken as the zero state of N qubits, $p = |0\\rangle^{\\otimes N} \\langle 0|^{\\otimes N}$, and $U(\\theta)$ is a parameterized unitary transformation.\nMachine learning is particularly well suited to the VQA framework, giving rise to the field of quantum machine learning (QML). For tasks such as data classification [18], it is common to express the parameterized quantum circuit as a composition of two unitaries: a data embedding circuit $V_{x}(x, \\phi)$ which depends on a data sample \u00e6 and may include tunable parameters \u03c6, as well as an ansatz U(\u03b8) which is trained to identify a separating hyperplane in the Hilbert space. For generative learning [19], $V(x, \\phi)$ can be used to initialize a quantum state with some random noise and U(\u03b8) is expected to learn the data distribution model."}, {"title": "III. RELATED WORKS", "content": "LatentGAN [20] is a deep learning method for generating new data structures by combining an autoencoder with a GAN. First the autoencoder is trained to understand the dataset and create a compressed representation. Then, this compressed representation is used as a target for the generator of the GAN in its training process. LatentGAN can generate data resembling those in the dataset while also creating novel structures with comparable characteristics, making it a useful tool for generating diverse and new data structures. Training models on reduced-dimensional data is a well-established technique, enhancing computational efficiency and mitigating overfitting [21].\nA recent development in quantum GANs is the Quantum Patch GAN (QPatchGAN) [7] which introduces a novel hybrid architecture where the generator is implemented with a quantum model, while the discriminator remains classical. Following an approach inspired by theoretical studies, [4]-[6], QPatchGAN aims to bridge the gap between quantum advantage and the limitations of current quantum computers such as the lack of qubits, and short decoherence times. The quantum generator consists of multiple circuits, each generating a part of the image (data), with parameters updated based on the discriminator's output. Although QPatchGAN shows promising results on the 8 \u00d7 8 MNIST dataset, it struggles with scalability and mode collapse issues [8], on data with higher dimensions.\nTo address the aforementioned issues, a recent work, MosaiQ [8] provides an alternative approach. Instead of working"}, {"title": "IV. LATENTQGAN", "content": "In this section, we describe our LatentQGAN method that offers a new approach to data generation with greater scalability compared to QPatchGAN. Moreover, LatentQGAN can be run on contemporary quantum computers. As illustrated in Fig. 1, our model is composed of a convolutional autoencoder which is trained first on all classes together, and a QGAN which is trained subsequently on the latent representation of each class separately. To make the latent representation compatible with the quantum generator, we normalize it line by line, following the final layer of the encoder. After the completion of the training of the autoencoder, we train our hybrid QGAN by injecting random noise vectors into the generator. Once the GAN's training is finished, the generator's output is given as an input for the decoder, to create new images. The following two sections are dedicated to describing in detail how the QGAN and the autoencoder of LatentQGAN work."}, {"title": "B. Quantum Generative Adversarial Networks (QGAN)", "content": "Inspired by previous work [7], the QGAN in LatentQGAN is hybrid with the generator being a series of quantum circuits, and the discriminator being a classical fully connected neural network.\n1) Quantum Generator: The quantum generator comprises a set containing a number T of quantum circuits, each designed to generate and reproduce its respective portion of the data, which is divided into T parts. The data is split into T parts, and each generator is used to reproduce a part of the data. Each generator is a parametrized quantum circuit (PQC) as in the quantum generator part of Fig.1, while each circuit is composed of a number of qubits, denoted by N, with an input layer of rotation gates that encode the random noise. The output is denoted by the state $ |z\\rangle$, where $ |z\\rangle = \\bigotimes_{i=1}^{N} R_{y}(\\alpha_{i})$ and \u03b1 is the vector of random noise sampled uniformly. We employ L parametrized layers, which are composed of rotation gates on each qubit, with controlled-Z gate that are sequentially applied between each pair of consecutive qubits, such that each qubit controls the next qubit $U_{i} = \\bigotimes_{N} R_{y}(\\theta_{i}) CZ_{s}$, with $CZ_{s} = \\bigotimes_{i=1}^{N-1} CZ(i,i+1)$ and \u03b8 being the parameters to optimize."}, {"title": "C. The Autoencoder", "content": "As mentioned before, for each sub-generator $t \\in [1,T]$ output, there is a constraint formulated as follows:\n$\\sum_{j=0}^{2N_{G}-1} P_{t}(J = j) = 1$\nWe have to make the latent representation of the encoder compatible with the generator's set of constraints. We use a normalization per line on the latent representation, named \"Normalize\" on Fig. 2, which is formulated as follows: let h and $\\hat{h}$ be the latent representation respectively before and after the normalize process. We represent h by the matrix $h = (h_{i,j})$ where $i \\in [1,T]$ and $j \\in [1,2^{N_{G}}]$. After the \"Normalize\u201d process, we can represent $\\hat{h}$ by:\n$\\hat{h} = (\\hat{h}_{i,j}) = (\\frac{h_{i,j}}{\\sum_{j=1}^{2^{N_{G}}} h_{i,j}})$,\nwhich satisfies $\\sum_{j=1}^{2^{N_{G}}} \\hat{h}_{i,j} = 1$.\nIn the decoder, the initial linear layers take the latent representation produced by the encoder and begin to reconstruct it into a form compatible with the original data. The transpose convolutional layers reverse the encoding process, gradually expanding the latent representation to reconstruct the original data in its original dimensions. This demonstrates another contribution of using an autoencoder which is that it is able to turn constrained output of the quantum generator into generalized data. ReLU activation function, while the sigmoid activation function in the final layer ensures that the reconstructed values fall within the [0,1] range, consistent with the scale of the original data. By combining the encoder and decoder, the model aims to learn a compressed representation of the input data in the latent space while being able to faithfully reconstruct the original data from this representation."}, {"title": "V. EXPERIMENTAL DETAILS AND RESULTS", "content": "We benchmark LatentQGAN on the MNIST dataset, which consists of 28 \u00d7 28 gray scale images depicting handwritten digits ranging from 0 to 9. The encoder compresses the information contained in the images into a normalized latent space of dimension 5 \u00d7 8. To match this dimension, the quantum generator is composed of five quantum circuits, one for each row of the latent space, with $N_{G} = 3$ and $N_{A} = 1$ for the generator and ancillary qubits respectively. In our experiments, the variational circuits we used were composed of L = 7 layers, for a total of 140 parameters. These values are the result of a careful search in hyperparameter space and represent an optimal trade-off between maximizing performance of the model and minimizing computational time on the quantum machine. Employing five circuits of four qubits each to form an array of size eight is computationally efficient, as they can be parallelized on a single 127-qubit\nIBM eagle quantum processor, making this model trainable on contemporary quantum computers.\nOn the other hand, the discriminator is built using a neural network with FCNNs, ReLU activation after convolutional layers and a sigmoid function for the output layer. The FCNN has an input layer of 40 neurons, followed by two hidden layers with 64 and 16 neurons, and an output layer with 1 neuron, totalling 3681 parameters. To compute the gradients of the generator, we employ the parameter-shift rule [23]. LatentQGAN uses the stochastic gradient descent optimizer and the binary cross entropy loss, described in (1) for the shared loss of the generator and discriminator and the mean squared error loss for the autoencoder. The autoencoder's learning rate is 0.05, with a batch size of 20 and its training is done on 100 epochs, which we have determined empirically and led to optimal results. The generator learning rate is 0.3 and the discriminator learning rate is 0.01 as recommended in [7], [8] with a batch size of 1 to limit the training time. The training is done on a number of iteration that depends on the class, based on our experiments, the optimal number of iterations for all 10 MNIST classes is 490.\nIn this study, model evaluation was performed based on the Fr\u00e9chet Inception Distance (FID) metric, which is commonly used to evaluate QGANs [7], [8]. The FID metric serves as a measure of the similarity between two distributions of data by computing the Fr\u00e9chet Distance between their corresponding multivariate Gaussian distributions. Mathematically, FID is calculated as follows:\n$FID = ||\\mu_{r} - \\mu_{g}||^{2} + Tr(\\Sigma_{r} + \\Sigma_{g} \u2013 2(\\Sigma_{r}\\Sigma_{g})^{\\frac{1}{2}})||$,\nwhere $\u00b5_{r}$ and $\u00b5_{g}$ are the mean vectors, and $\u03a3_{r}$ and $\u03a3_{g}$ are the covariance matrices of the real and generated data distributions, respectively. A lower FID score indicates a higher resemblance between the generated samples and the real data distribution. While useful for gauging the similarity between generated and real data distributions, the FID does not always correspond to human evaluations [24] as shown in Fig.6. Thus, though valuable, it should be supplemented"}, {"title": "VI. CONCLUSION", "content": "The LatentQGAN proposed in this paper allows the QGAN model to learn more effectively a latent representation of data generated by an encoder from an autoencoder. This method enables dealing with high-dimensional data and training of the QGAN on real quantum computers, and demonstrates significant improvement in the quality of data generation both on quantum simulators and real quantum computers. Using autoencoders to reduce input data dimensionality could enable more effective use of quantum machine learning models.\nExploring this approach could prove valuable, as it might enhance the versatility of quantum systems in tackling diverse tasks. Extending the model to a full quantum implementation is worth exploring. This work will also be utilized as a basis for our further investigations into generation of complex data types, in particular time series data generation and anomaly detection, where a major challenge will be the non-stationarity of data."}]}