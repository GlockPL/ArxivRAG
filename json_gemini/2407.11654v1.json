{"title": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models", "authors": ["Aladin Djuhera", "Vlad C. Andrei", "Xinyang Li", "Ullrich J. M\u00f6nich", "Holger Boche", "Walid Saad"], "abstract": "Split federated learning (SFL) is a compute-efficient paradigm in distributed machine learning (ML), where compo- nents of large ML models are outsourced to remote servers. A significant challenge in SFL, particularly when deployed over wireless channels, is the susceptibility of transmitted model parameters to adversarial jamming that could jeopardize the learning process. This is particularly pronounced for word embedding parameters in large language models (LLMs), which are crucial for language understanding. In this paper, rigorous insights are provided into the influence of jamming LLM word embeddings in SFL by deriving an expression for the ML training loss divergence and showing that it is upper-bounded by the mean squared error (MSE). Based on this analysis, a physical layer framework is developed for resilient SFL with LLMs (R-SFLLM) over wireless networks. R-SFLLM leverages wireless sensing data to gather information on the jamming directions-of-arrival (DoAs) for the purpose of devising a novel, sensing-assisted anti-jamming strategy while jointly optimizing beamforming, user scheduling, and resource allocation. Extensive experiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness, achieving close-to-baseline performance across var- ious natural language processing (NLP) tasks and datasets. The proposed methodology further introduces an adversarial training component, where controlled noise exposure significantly enhances the LLM's resilience to perturbed parameters during training. The results show that more noise-sensitive models, such as RoBERTa, benefit from this feature, especially when resource allocation is unfair. It is also shown that worst-case jamming in particular translates into worst-case model outcomes, thereby necessitating the need for jamming-resilient SFL protocols.", "sections": [{"title": "I. INTRODUCTION AND MOTIVATION", "content": "Future 6G networks are anticipated to introduce a substan- tial leap toward highly integrated and intelligent connectivity at the edge, enabled by artificial intelligence (AI) [1] and machine learning (ML) [2]. However, in this envisioned hyper- connected and AI-assisted network, important questions and stringent requirements on resilience and trustworthiness arise, both from a user and network perspective [3]. This imposes significant design challenges for emerging technologies, such as distributed and collaborative ML (DCML) [4], which may be targeted by adversarial attacks over the wireless medium. For instance, the distributed training of large language models (LLMs) faces unique challenges in ensuring data integrity and model robustness due to highly sensitive word embedding parameters. Much research has focused on addressing these challenges via adversarial AI training methods and model- based solutions [5], [6], and in [7], the authors closely in- vestigated such attack and defense strategies in 6G networks, exposing critical vulnerabilities across all network layers. However, a number of important research questions remain underexplored, such as how these attacks can be orchestrated in practice, and how current and next-generation wireless network architectures, systems, and technologies can introduce proactive defense mechanisms, ideally by-design."}, {"title": "A. Adversarial Poisoning in Wireless Federated LLM Training", "content": "Motivated by the increasing importance of end-user data privacy and associated privacy protection laws [8], DCML has been gradually shifting toward mechanisms such as fed- erated learning (FL) [9] and split FL (SFL) [10]. These have proven to be privacy-preserving as only the respective model parameters, some parts of it or model gradients need to be exchanged. SFL in particular has emerged as a compute- efficient federated protocol, suitable for distributed training of large ML model architectures. Unlike traditional FL, in which the entire model is trained on each client, SFL splits the model, allowing for more compute-intensive parts to be outsourced to a remote server. This approach is particularly advantageous for LLM architectures that cannot be entirely processed at resource-constrained edge devices due to computational and memory limitations [11]. However, the practical realization of (S)FL over wireless networks faces challenges from the inherently unreliable wireless medium, limited bandwidth, and suboptimal resource allocation [12]. In addition, malicious actors such as jammers could target privacy and security aspects of (S)FL systems by intentionally poisoning data and models through adversarial noise. The particular importance of studying adversarial jamming attacks in SFL with LLMs is motivated by recent results from natural language processing (NLP) research [13], [14]. The authors in [13] study the susceptibility of LLMs to word embedding poisoning caused by noisy perturbations and show that by altering even a single word embedding vector, an adversary can subtly manipulate a"}, {"title": "B. Proactive and Resilient-by-Design Anti-Jamming in SFL", "content": "To preemptively safeguard LLM parameter transmissions in SFL against adversarial jamming attacks, proactive and in particular resilient-by-design approaches are needed [17]. This requires a simultaneous co-design of AI, resilience, beam- forming, user scheduling, and resource allocation, thereby integrating resilience from a bottom-up approach, by design. In addition, such proactive defense mechanisms need to be agnostic of the respective jamming capabilities, including physical and spatial features. This is not the case for some more recent works, which tend to impose strong assumptions on the adversary's knowledge and setup. For instance, [18] and [19] only consider single- or few-antenna jammers with common secrets being exchanged between legitimate parties. This essentially excludes so-called worst-case jammers with extensive system knowledge and capabilities [20], [21]. In order to develop universally applicable defense strategies for a wide range of jamming scenarios in SFL, system performance needs to be guaranteed for the worst-case. Thus, we need to generalize toward intelligent and reconfigurable worst-case jammers. In our prior work in [22], we motivated the study of how sensing-assisted network information can be harnessed to enhance existing mitigation schemes without the need for otherwise precise jamming statistics. Therein, we have shown that information on the jamming signal directions-of-arrival (DoAs) can be used to devise MIMO-OFDM anti-jamming strategies with exceptional performance. However, we did not discuss whether such sensing-assisted defense strategies can be straightforwardly applied to enhance the resilience in SFL over wireless networks. In particular, the impact of worst-case jamming needs to be quantified in order to study its influence on LLM model poisoning as compared to conventional jam- mers. In the subsequent sections, we provide thorough insights on these aspects, including an analysis on the minimum system rate that guarantees a reliable and resilient SFL training."}, {"title": "C. Contributions", "content": "The main contribution of this paper is an analysis and framework for resilient SFL over wireless networks that will help close the gap between adversarial jamming attacks in SFL and LLM model poisoning. We provide insights into how jamming LLM word embeddings affects the global model training and how the latter can be efficiently safeguarded by MIMO signal processing at the availability of sensing-assisted information. In summary, our key contributions include:\n\u2022 We derive an analytical expression for the ML training loss divergence based on a relaxed $(L_0, L_1)$-smoothness assumption for LLM transformer architectures in the case of corrupted word embeddings. We show that its upper bound depends on the communication mean squared error (MSE), thereby motivating a wireless approach to resilience in SFL.\n\u2022 We provide a novel analysis on the minimum system rate which guarantees a robust and reliable SFL training over the wireless network, thereby characterizing minimum network conditions based on the outage rate caused by the jammer.\n\u2022 We develop R-SFLLM, a novel, sensing-assisted anti- jamming framework for resilient SFL with LLMs, which leverages the jamming signal's DoAs to devise an anti-jamming strategy formulated as a joint optimization problem for beamforming, user scheduling, and resource allocation while maximizing the sum rate of the SFL participants. In this problem, any explicit knowledge about the jamming statistics is replaced by a surrogate expression that depends only on the jamming DoAs. We provide an efficient solution to the problem using an iterative water-filling approach [23].\n\u2022 In order to benchmark R-SFLLM against worst-case conditions in SFL, we utilize the worst-case jamming strategy in our prior work [22], which minimizes the sum rate instead.\n\u2022 We provide extensive simulations for BERT and ROBERTa models on various datasets, demonstrating near-optimal per- formance when anti-jamming is enabled and significantly worse outcomes for unprotected scenarios. We show that R- SFLLM introduces an additional adversarial training com- ponent as word embeddings are exposed to controlled noise since jamming cannot be mitigated perfectly. This exposure further helps improve the model robustness by teaching it to learn effectively even in the presence of interference [24]."}, {"title": "II. SYSTEM MODEL AND ADVERSARIAL ANALYSIS", "content": "We consider an SFL setup in which a set Q of Q legitimate clients cooperatively train transformer-based LLMs, which consist of embedding, attention and head layers. A natural choice in SFL with LLMs is to partition the model according to these blocks, assigning the embedding layer to the client and the compute-intensive attention and head layers to the server as outlined in Figure 1. This particular partitioning alleviates the computational load at the client while ensuring that word embeddings are processed close to the raw data, thereby enhancing privacy. Further partitioning the embedding block and transmitting intermediate layers instead increases the risk of sensitive information being exposed to adversarial attacks, such as model inversion [25]. During training, each"}, {"title": "A. Wireless R-SFLLM System Model", "content": "We further model $H_{qnk}$ and $G_{nk}$ as beamspace channels:\n$H_{qnk} = \\sum_{l=1}^{L_{H}} b_{H_{qnk,l}} a_{N_R}(\\theta_{H_{q,l}}) a_{N_{T_q}}^H(\\psi_{H_{q,l}}) e^{j 2 \\pi w_{nk}(v_{H_{q,l}},\\tau_{H_{q,l}})} (5)$\n$G_{nk} = \\sum_{l=1}^{L_{G}} b_{G_{nk,l}} a_{N_R}(\\theta_{G,l}) a_{N_J}^H(\\psi_{G,l}) e^{j 2 \\pi w_{nk}(v_{G,l},\\tau_{G,l})}. (6)$\nHere, $L_{H_q}$ and $L_G$ are the number of resolvable paths for each channel, $b_{.,l}$ is the path gain for each resolvable path $l$, $a_{N_x}(\\theta)$ is the steering vector at each terminal with $N_x$ antennas, $\\theta_{.,l}$ is direction-of-arrival, $\\psi_{.,l}$ is direction-of- departure, and $w_{nk}(v,\\tau) = kvT_s - n\\tau\\Delta f$ is the phase shift caused by the Doppler shift $v$ and propagation delay $\\tau$, with $T_s$ and $\\Delta f$ being the symbol period and subcarrier spacing. Furthermore, the power $P_q$ for each user is limited across all resource elements and the jamming signal equally adheres to a jamming power constraint $P_J$, i.e.\n$\\sum_{(n,k)\\in R_q} ||x_{qnk}||^2 \\leq P_q, \\sum_{(n,k)\\in R_q} tr(C_{unk}) \\leq P_J. (7)$\nWe further assume that the legitimate parties have precise channel state information (CSI), encompassing the wireless link parameters defined by the set $\\Omega_q = \\{Q_{qnk}, P_{qnk}, w_{qnk}, v_{qnk}, H_{qnk}, \\sigma^2\\}$. Additionally, the SFL participants are provided with the DoAs of the jamming signal, i.e. $\\Omega_G = \\{\\theta_1, ..., \\theta_{L_G}\\}$. This may be enabled by advanced wireless sensing technologies in future 6G networks, such as integrated sensing and communication (ISAC) and reflective intelligent surfaces (RIS) [26]\u2013[28], to name a few. Further, we assume no restrictions on the particular jamming strategy, hence the jammer is assumed to be in the so-called jammer- dominant regime [20] with more transmit power and antennas than any legitimate party, i.e. $P_J \\gg P_q$ and $N_J > N_{T_q}, N_R$. In addition, the adversary may possess full system knowledge, including $S_q$. This represents a worst-case jammer assumption. The adversarial jamming introduces corruption not only at the symbol level but also at the decoded message, such that jamming can be modeled as the post-decoding error as follows:\n$\\hat{e}_q = e_q + \\epsilon$, where $\\epsilon \\sim \\mathcal{N} (0, C_{\\epsilon}), (8)$\n$tr(C_{\\epsilon}) = MSE(s_q), (9)$\n$MSE(s_q) = E [||s_q - \\hat{s}_q||^2]$\n$= \\sum_{(n,k) \\in R_q} \\alpha_{qnk} \\cdot E [|s_{qnk} \u2013 \\hat{s}_{qnk}|^2] . (10)$\nUpon receiving the jammed signal $y_{nk}$, the server slice continues processing the LLM attention and head layers using the corrupted word embeddings $\\hat{e}_q \\neq e_q$. At the end of the forward propagation pass, the server slice computes the training loss metric $L : \\mathbb{R}^E \\rightarrow \\mathbb{R}$, which yields the corrupted loss $L(\\hat{e}_q)$ and its gradient $\\nabla L(\\hat{e}_q)$, using which the backpropagation process is initiated. This procedure is repeated for each transmission of the word embeddings across all global training rounds. We also assume that the jammer is not active in the downlink as the perturbation of gradients has been studied in various federated setups, for which corresponding defense mechanisms exist [29]. Similarly, the client- and server-side model aggregation after each global round are assumed to be unaffected by the adversary as corresponding secure aggregation strategies exist as well [30]. Note that FedAvg [9] is used in this work. Thus, the consideration of only the uplink trans- mission suffices to study the jamming impact on LLM word embeddings in this setup. Anti-jamming can then be directly applied if necessary conditions are fulfilled, including the assumption that maximizing the signal-to-interference-plus- noise-ratio (SINR) implies maximizing the ML performance. This assumption is verified next. Figure 2 shows the R-SFLLM system architecture, where the SFL protocol is augmented by sensing-assisted jamming DoA information, a necessary component for our anti-jamming framework in Section III."}, {"title": "B. Adversarial Jamming Impact on LLM Training in SFL", "content": "Previous works in FL typically assume the loss function to be convex, twice differentiable, and Lipschitz smooth. While these assumptions may hold true for simpler neural networks as in [12] and [31], more involved architectures such as transfomers generally do not exhibit these properties [32]. The assumption that $L$ is Lipschitz smooth is particularly far-reaching as this implies bounded gradients during backprop- agation. In [33], it is shown that the standard Lipschitz as- sumption introduces a large variability along the optimization trajectory. Thus, a relaxed $(L_0, L_1)$-smoothness needs to be assumed, which generalizes to more complex models, such as LLMs. Based on this generalization, we derive upper bounds on the loss divergence, caused by jammed word embeddings, and show how these relate to the communication MSE.\n1) Assumptions on the Loss Function:\nAssumption 1. The loss function $L : \\mathbb{R}^E \\rightarrow \\mathbb{R}$ is twice- differentiable and bounded from below with infimum $L^*$.\nAssumption 2. $L$ is $(L_0, L_1)$-smooth coordinate-wisely, i.e. there exist coefficient vectors $L_0, L_1 \\in \\mathbb{R}^E$ such that for any $x, y \\in \\mathbb{R}^E$ with $||x - y||_2 \\leq ||L_1||$ it holds for all $j \\in [E] = \\{1,..., E\\}$ that\n$\\begin{aligned} \\left|\\frac{\\partial L(y)}{\\partial x_j} - \\frac{\\partial L(x)}{\\partial x_j}\\right| \\leq \\frac{L_{0,j}}{2} + L_{1,j} \\cdot \\frac{||y - x||_2^2}{\\sqrt{E}}.\\end{aligned}$\nThis is a generalization of the scalar $(L_0, L_1)$-smoothness:\nDefinition 1. $L$ is called $(L_0, L_1)$-smooth if there exist scalars $L_0, L_1 \\in \\mathbb{R}$ such that for all $x \\in \\mathbb{R}^E$ it holds that\n$||\\nabla^2L(x)|| \\leq L_0 + L_1||\\nabla L(x)||$.\nThe coordinate-wise $(L_0, L_1)$-smoothness implies that smoothness may vary for each coordinate of the input space. This particularly pertains to LLMs as it has been shown in [32] that variance can be observed across mostly every transformer layer, such that each layer coordinate $j$ satisfies an own $(L_{0,j}, L_{1,j})$ pair. Thus, if the coefficients $L_{1,j}$ are non-zero, smoothness is potentially unbounded. In contrast, if all $L_{1,j}$ are strictly zero, the original Lipschitz smoothness is recovered. In [32], the following Lemma has been established, relating the coordinate-wise smoothness to the loss divergence.\nLemma 1. [32] Let $L$ be $(L_0, L_1)$-smooth coordinate-wisely. Then for any $x, y \\in \\mathbb{R}^E$ with $||x - y||_2 \\leq ||L_1||$, we have\n$\\begin{aligned} L(y) &\\leq L(x) + (\\nabla L(x), y - x) + \\sum_{j=1}^{E} \\left(\\frac{L_{0,j}}{2} + \\frac{L_{1,j}}{\\sqrt{E}} \\left|\\frac{\\partial L(x)}{\\partial x_j}\\right|\\right) |y_j - x_j| \\newline &+ \\frac{1}{2} \\left( \\sum_{j=1}^{E} \\frac{L_{0,j}}{2} + \\frac{L_{1,j}}{\\sqrt{E}} \\left|\\frac{\\partial L(x)}{\\partial x_j}\\right|\\right) |y_j - x_j|. \\end{aligned} (11)$\n2) Upper Bound on the LLM Loss Divergence: We utilize Lemma 1 to derive the loss divergence upper bound as follows.\nLemma 2. For $x, y \\in \\mathbb{R}^E$, the loss divergence is bounded by\n$|L(y) - L(x)| \\leq ||\\nabla L(x)||_2 \\cdot ||y - x||_2 \\newline + ||L_0 + L_1 \\odot |\\nabla L(x)|||_2 \\cdot ||y \u2013 x||_2. (12)$\nIn (12), the loss gradient $||\\nabla L(x)||_2$ might be unbounded, particularly when several coordinates need to be considered. However, common practice in deep learning suggests to bound gradients manually by means of gradient clipping [33] using a clipping threshold $\\tau > 0$, thereby preventing exploding gradients, i.e.\n$\\nabla_{\\tau}L(x) = \\begin{cases} \\nabla L(x), & \\text{if } ||\\nabla L(x)||_2 \\leq \\tau \\newline \\frac{\\tau}{||\\nabla L(x)||_2} \\nabla L(x) & \\text{otherwise} \\end{cases} (13)$"}, {"title": "C. Minimum System Rate for Reliable SFL with LLMs", "content": "To characterize the minimum system rate, under which the communication link can support SFL reliably, we need to identify outage conditions caused by jamming. To this end, we provide three remarks. In Remark 1, we first derive a lower"}, {"title": "III. R-SFLLM ANTI-JAMMING FRAMEWORK", "content": "In this section, we develop the R-SFLLM anti-jamming component. To this end, we first define the anti-jamming opti- mization problem and provide insights into the role of sensing- assisted DoA information. Then, we solve the optimization problem using an iterative water-filling solution. Finally, we provide an analytical expression for the worst-case jamming strategy as a benchmark for our SFL resilience framework."}, {"title": "A. Anti-Jamming Strategy and Optimization Problem", "content": "As a result of Proposition 2, jammed SFL word embeddings $\\hat{e}_q$ lead to a deviation from the ground truth in the deterministic loss function $L$. Thus, the anti-jamming objective can be generally formulated as the minimization of the expected loss divergence, i.e. $min E [|L(e_q) \u2013 L(\\hat{e}_q)|]$. Using Corollary 2, we can instead minimize $J(s_q, \\hat{s}_q) = \\sqrt{E [||s_q - \\hat{s}_q||^2]} + E [||s_q - \\hat{s}_q||^2]$, which is only dependent on the MSE and where we imply using gradient clipping during training. This problem can be equivalently interpreted as the maximization of the SINR for each user $q \\in Q$, respectively, or more generally, as the maximization of the sum rate. To this end, we derive an expression for the achievable sum rate as follows:\n$\\begin{aligned} R &= \\sum_{(n,k) \\in R_q \\forall q} I(Y_{nk}; \\{S_{qnk}\\}_{q \\in Q}) \\newline &= \\sum_{(n,k) \\in R_q \\forall q} \\log \\left(1 + \\frac{\\sum_{q \\in Q} @_{qnk}P_{qnk}}{\\Upsilon_{qnk}(C_{z_{nk}})}\\right) (34) \\end{aligned}$\nIn this setup, $\\Upsilon_{qnk}(C_{z_{nk}})$ represents the SINR of user $q$ for the composite noise covariance matrix $C_{z_{nk}}$ and allocated resource elements $(n,k) \\in R_q$, i.e.\n$\\Upsilon_{qnk}(C_{z_{nk}}) = w_{qnk}^H H_{qnk}C_{z_{nk}}^{-1}H_{qnk}^Hw_{qnk}. (35)$\nIn order to incorporate anti-jamming in SFL by-design, we need to jointly optimize over beamforming, user scheduling, and resource allocation constraints, thus introducing resilience proactively at the bit level. To this end, we pose the following"}, {"title": "B. Role of Sensing-Assisted Jamming DoA Information", "content": "We have shown in [34] that such a surrogate covariance can be approximated using the jamming signal DoAs as follows:\n$C_{z_{nk}} = \\eta A(\\Theta_G)A(\\Theta_G)^H + \\sigma^2 I_{N_R} \\approx C_{z_{nk}} (37)$\nwith the array manifold evaluated at the known DoAs, i.e.\n$A(\\Theta_G) = [a_{N_R}(\\Theta_{G,1}) ... a_{N_R}(\\Theta_{G,L_G})]. (38)$\nThis was motivated by showing that the true SINR $\\gamma$ can be lower bounded by an approximate expression $\\hat{\\gamma}$, which is dependent on a scaling parameter $\\eta$ and the DoAs as follows:\n$\\gamma(w, v) \\ge \\frac{w^H H_{qnk}^H H_{qnk} w}{\\hat{v}^H (\\eta A_{R_x} (\\Theta_G) A_{R_x}(\\Theta_G)^H + \\sigma^2 I) v} = \\hat{\\gamma}(w, v). (39)$\nBy inserting (37) into (34), we hence obtain a lower bound on $R$. Note that in general $\\eta$ is unknown since it depends on the unknown jamming setup. Thus, we consider it as a hyper- parameter, which controls the resilience level of our system. In [34], we showed that by choosing $\\eta$ to be much larger than the noise level $\\sigma^2$, i.e. $\\eta \\gg \\sigma^2$, we coincide with the case where the jammer setup is known, that is where $\\gamma \\approx \\hat{\\gamma}$. In this case, the SINR can be maximized by maximizing the lower bound instead. Thus, $C_{z_{nk}}$ constitutes a conservative approximation of $C_{z_{nk}}$, ensuring it does not underestimate the impact of noise and adversarial jamming, indicated by the L\u00f6wner order $\\succeq$ in (37). We carefully validated this conjecture in [22] for $\\eta = 10$ in several jamming scenarios for a range of power budgets $P_J < \\infty$. Consequently, the availability of the DoAs alleviates the need to know the exact jamming statistics."}, {"title": "C. Iterative Water-Filling Solution", "content": "We have further shown in [22] that the NP-hard problem in (36) can be iteratively solved using a water-filling approach, as described in Algorithm 1. The proposed method adapts the original water-filling for MAC and MIMO channels [35] to incorporate user scheduling. To this end, each user $q \\in Q$ determines its update on the matrix $X_{qnk}$ and computes an optimal set of the wireless system design parameters $@_{qnk}, P_{qnk}, w_{qnk}$, as outlined in steps (6)-(11), where for each user the following optimization problem is solved:\n$\\max_{\\@_{qnk}, P_{qnk}, \\text{w}_{qnk}} \\sum_{(n,k) \\in R_q} @_{qnk} \\log \\left(1 + \\frac{P_{qnk}}{\\Upsilon_{qnk}(X_{qnk})}\\right). (40)$\nAlgorithm 1 effectively circumvents the need to deal with the NP-hardness of the problem as the overall sum rate is indirectly maximized by maximizing the sum rate $R_q$ of the strongest users individually. In each iteration, we use the power iteration method for computing eigenvalues and eigenvectors, and perform water-filling for power allocation. Both of these methods are known to exhibit rapid convergence. Consequently, Algorithm 1 inherits these convergence properties, which we have verified in extensive experiments, where our method on average takes less than five iterations to converge. With $n_{iter}$ representing the number of iterations required for convergence, the overall complexity of the algorithm is\n$O(n_{iter}QNKN_R N_T^3). (41)$"}, {"title": "D. Worst-Case Jamming Strategy", "content": "In Proposition 2, we have seen that jamming LLM word embeddings affects the training performance. However, the impact still remains to be investigated for worst-case condi- tions and in particular how this affects the global performance in SFL after aggregating such corrupted models. To this end, we need to derive the worst-case jamming strategy, which we use to benchmark R-SFLLM. In contrast to anti-jamming, we"}, {"title": "IV. EXPERIMENTS, SIMULATION RESULTS AND ANALYSIS", "content": "In this section, we discuss our simulation results for apply- ing R-SFLLM to the distributed training of LLMs. We provide insights into the sensitivity of SFL to poisoned LLM aggre- gations, the impact of the worst-case jammer as compared to barrage jamming, the effectiveness of sensing-assisted anti- jamming, as well as the role of different LLM architectures."}, {"title": "A. Experimental Setup", "content": "We refer to the R-SFLLM setup in Figure 2. Therein, Q = 3 legitimate clients participate in fine-tuning BERT [38] and ROBERTa [39] base models for two distinct NLP tasks: Sequence classification (SC) and named entity recognition (NER). SC assigns a category to a sequence of words or tokens, while NER identifies named entities, such as persons or organizations within a text. For SC, the binary classifi- cation datasets SST2 [40] and QNLI [41], as well as the ternary dataset MNLI [42] are considered, while for NER the CONLL2003 [43] and WNUT17 [44] datasets are used. Each dataset is divided equally among the clients, ensuring a unique and private portion of the data. For each NLP task, the pre-trained LLMs are fine-tuned for $N_{epochs} = 10$ epochs and $N_{rounds} = 10$ global SFL rounds. In this setup, worst- case adversarial jamming is encountered during each uplink transmission of the LLM word embeddings in the MIMO- OFDM MAC. All participating parties employ uniform linear antenna arrays with $N_{T_q} = 8$ and $N_R = 16$ legitimate transmit and receive antennas. The adversary is assumed to be in the jammer-dominant regime and employs the previous worst-case strategy to maximally corrupt the LLM word embbeddings. The corresponding user and jamming DoAs are determined as\n$\\theta_{H_{q,l}} = \\theta_{H_q} + \\phi_{H_{q,l}}, \\text{ and } \\theta_{G,l} = \\theta_G + \\phi_{G,l} (43)$\nwhere the central angles $\\theta_{H_q}$ and $\\theta_G$ are set to $0\u00b0$ and $20\u00b0$, respectively. The disturbance in form of the angle spread $\\phi_{.,l}$ for each antenna $l$ is drawn uniformly via $\\mathcal{U}(.)$ according to\n$\\phi_{H_{q,l}} \\sim \\mathcal{U}[-10^\\circ, 10^\\circ] \\text{ and } \\phi_{G,l} \\sim \\mathcal{U}[-5^\\circ, 5^\\circ]. (44)$\nThe considered communication protocol employs 5G New Radio (NR) slots with $K = 14$ symbols per slot and $N = 64$ subcarriers. The maximum number of resource allocation blocks for each user q is given by $B_q = \\frac{NK}{Q} = 298$.\nWe choose the scale parameter of the sensing-assisted R- SFLLM anti-jamming to be $\\eta = 10$, which is similar to previous experiments in [22] and hence much larger than the background noise $\\sigma^2 = -3$ dBm. We further resample the jamming statistics after each uplink transmission, i.e. after each training batch, to simulate movement and jamming vari- ance. The following four scenarios are studied as benchmarks:\n1) SFL Baseline: SFL performance without wireless model.\n2) Gaussian: No adversarial jamming, only AWGN.\n3) No Protection: Worst-case jamming without R-SFLLM.\n4) Protection: Worst-case jamming with R-SFLLM."}, {"title": "B. R-SFLLM Simulation Results", "content": "1) Sequence Classification: As shown in Table II, for all three SC datasets, R-SFLLM is able to consistently safeguard the distributed training in general, leading to robust global models with classification accuracies near-identical or very"}, {"title": "APPENDIX A\nPROOF OF PROPOSITION 1", "content": "First, we adapt the original proof for Lemma 1 in [32] and reformulate its initial statement toward the loss divergence:\n$\\begin{aligned} L(y) \u2013 L(x) &= \\nabla L(x) (y \u2013 x) \\newline &+ \\int_{0}^{1} (\\nabla L(x + u(y - x)) - \\nabla L(x), y - x)du. (45)\\end{aligned}$\nSecond, applying the norm |. | and triangle inequality yields\n$\\begin{aligned} |L(y) \u2013 L(x)| &\\leq |\\nabla L(x) (y \u2212 x)| \\newline &+ \\int_{0}^{1} |(\\nabla L(x + u(y \u2013 x)) \u2013 \\nabla L(x), y \u2212 x)du|. (46)\\end{aligned}$\nThird, we use the upper bound on this integral from the proof in [32] to further obtain\n$\\begin{aligned} |L(y) \u2013 L(x)| &\\leq |\\nabla L(x) (y \u2013 x)| \\newline &+ \\sum_{j=1}^{E} \\left( \\frac{L_{0,j}}{2} + \\frac{L_{1,j}}{\\sqrt{E}} \\left|\\frac{\\partial L(x)}{\\partial x_j}\\right|\\right) |y_j - x_j| \\cdot ||y - x||_2 . (47)\\end{aligned}$\nBy defining the vectors $u = [u_1 ... u_E]^T$ with $u_j = \\frac{L_{0,j}}{2} + \\frac{L_{1,j}}{\\sqrt{E}} \\left|\\frac{\\partial L(x)}{\\partial x_j}\\right|$ and $v = [v_1 ... v_E]^T$ with $v_j = |y_j - x_j|$, above expression can be denoted via scalar products, i.e.\n$|L(y) \u2013 L(x)| \\leq |\\nabla L(x)^T (y \u2013 x)| + |u^T v \\cdot ||y - x||_2| . (48)$\nNext, applying the Cauchy-Schwarz inequality on the right- hand side and identifying $||v||^2 = ||y - x||^2$ yields\n$|L(y) \u2013 L(x)| \\leq ||\\nabla L(x)||_2 \\cdot ||y - x||_2 \\newline + ||u||_2 \\cdot ||y - x||_2 \\cdot ||y - x||_2. (49)$\nThe vector $u$ can be further reformulated as $u = L_0 + L_1 \\odot |\\nabla L(x)|$, where $\\odot$ denotes the Hadamard product and $\\nabla L(x)$ is the gradient vector of $L$. This eventually yields the statement in Lemma 2."}, {"title": "APPENDIX B\nPROOF OF PROPOSITION 2", "content": "Plugging in $e_q, \\hat{e}_q$ into the upper bound on the loss divergence in (12) and taking the gradient w.r.t. $e_q$ ($\\nabla_{e_q}$) gives\n$\\begin{aligned} |L(e_q) \u2013 L(\\hat{e}_q)| &\\leq ||\\nabla_{e_q}L(e_q)||_2 \\cdot ||e_q - \\hat{e}_q||_2 \\newline &+ ||L_0 + L_1 \\odot |\\nabla_{e_q}L(e_q)|||_2 \\cdot ||e_q \u2013 \\hat{e}_q||_2. (5```json\nntinue generating json\n40)\\end{aligned}$\nFurther, substituting by u(eq) from (21) and taking the expectation over the joint distribution of $\\{S_{qnk}\\}_{(n,k) \\in R}$, yields\n$\nE [|L(e_q) \u2013 L(\\hat{e}_q)|] \\leq E [||\nabla_{e_q}L(e_q)||_2 \\cdot ||e_q - \\hat{e}_q||^2\n] \\\n+ E [||u(e_q)||_2 \\cdot ||e_q - \\hat{e}_q ||^2]. (51)\n\nWith non-negative expectations, the Cauchy-Schwarz in- equality, i.e. E[A\\cdot B] \\leq \\sqrt{E[A^2]} \\cdot \\sqrt{E[B^2]}, can be applied. Further, as $\\nabla_{e_q} L(e_q)$ and $u(e_q)$ are independent of $S_{qnk}$ and with $E [||e_q - \\hat{e}_q||^2 = E [||s_q - \\hat{s}_q||^2]$ from Proposition 1, we obtain the following statement of Proposition 2:\n$\\begin{aligned}E [|L(e_q) - L(\\hat{e}_q)|] &\\leq ||\\nabla_{e_q} L(e_q) ||_2 \\cdot \\sqrt{E [||s_q - \\hat{s}_q||^2]} \\newline &+ ||u(e_q) ||_2 \\cdot E [||s_q - \\hat{s}_q||^2]. (52)\\end{aligned}$"}]}