{"title": "Survey: Transformer-based Models in Data Modality Conversion", "authors": ["ELYAS RASHNO", "AMIR ESKANDARI", "AMAN ANAND", "FARHANA ZULKERNINE"], "abstract": "Transformers have made significant strides across various artificial intelligence domains, including natural language processing, computer vision, and audio processing. This success has naturally garnered considerable interest from both academic and industry researchers. Consequently, numerous Transformer variants (often referred to as X-formers) have been developed for these fields. However, a thorough and systematic review of these modality-specific conversions remains lacking. Modality Conversion involves the transformation of data from one form of representation to another, mimicking the way humans integrate and interpret sensory information. This paper provides a comprehensive review of transformer-based models applied to the primary modalities of text, vision, and speech, discussing their architectures, conversion methodologies, and applications. By synthesizing the literature on modality conversion, this survey aims to underline the versatility and scalability of transformers in advancing Al-driven content generation and understanding.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is inspired by human perceptions, such as vision, hearing, and reading, and seeks to replicate these abilities [67]. Typically, a modality is linked to a particular sensor that creates a distinct communication channel, such as sight, speech, and written language. Humans possess a fundamental process in sensory perception that allows them to efficiently engage with the world in dynamic and unconstrained situations by integrating data from several sensory modalities. Each modality functions as a separate source of information that is distinguished by its own specific statistical features. A photograph depicting \"elephants playing in the water\" delivers visual information through numerous pixels, whereas a similar verbal description conveys this sight using distinct words. Similarly, voice can communicate the same occurrence using spectrograms or speech characteristics. A data conversion Al system must receive input from a specific modality, process, understand, and reproduce its content in a different modality, imitating human-like perception. Modality Conversion (MC) is a broad methodology for constructing artificial intelligence models that can extract and transform information from one modality of representation to another [67]."}, {"title": "2 Related surveys", "content": "Numerous surveys have explored TB models across text processing [3], computer vision [75], and speech processing [91] domains. Each of these surveys typically reviews papers focusing on a single"}, {"title": "3 Architecture of TB models", "content": "The transformer is a type of architecture that has revolutionized the field of natural language processing (NLP) and has subsequently made significant impacts across various domains of artificial intelligence (AI). Introduced by Vaswani et al. in 2017 [158], the TB model deviates from earlier sequence-to-sequence architectures by eschewing recurrent (RNN) or convolutional layers. Instead, it relies entirely on a mechanism known as self-attention to weigh the significance of different parts of the input data. The vanilla transformer model consists of two main components: an encoder and a decoder, each composed of a stack of identical layers (Fig. 4).\nSelf-attention mechanism: A key breakthrough in the Transformer model is the self-attention mechanism. This feature enables the model to assess the significance of various segments of the input sequence while handling each token. The mechanism functions concurrently across all positions within the sequence.\nModel architecture: The Transformer's structure includes an encoder and a decoder, each comprising several layers of multi-head attention and feed-forward neural networks. The self-attention mechanism enables the model to focus on diverse parts of the input sequence, efficiently capturing long-range dependencies. Fig. 4 depicts the basic architecture of the vanilla transformer.\n\u2022 Encoder-decoder: The Transformer model is commonly employed in sequence-to-sequence tasks, where the encoder processes the input sequence and the decoder produces the output sequence.\n\u2022 Layer stacking: Both the encoder and decoder are composed of multiple identical layers stacked vertically. Each of these layers includes self-attention mechanisms and feed-forward neural networks."}, {"title": "4 Natural Language Processing", "content": "In the rapid development of NLP, Pretrained Language Models (PLMs) [66] have established new benchmarks in performance across a range of linguistic tasks. In this section, we will first illustrate the architecture of NLP transformers and highlight prominent models. Subsequently, we will study various downstream tasks in NLP. Finally, we will show the application of TB models in converting textual data to visual or speech modalities."}, {"title": "4.1 TB Architecture in NLP", "content": "In NLP, three types of TB models namely Encoder-only, Decoder-only, and Encoder-Decoder, which will be explained."}, {"title": "4.1.1 Encoder-only", "content": "Encoder-only architectures within the domain of PLMs endeavor to encapsulate the entirety of semantic and contextual data present within a text corpus, subsequently transforming this information into a condensed feature vector representation. The most popular encoder-only architecture is BERT, proposed by Devlin et al. [38]. It represents a paradigm shift as a pre-trained model that leverages the TB's architecture and is subsequently fine-tuned to excel in various NLP applications, including sentiment analysis, entity recognition, and question-answering. BERT's pre-training involves predicting masked tokens in a sentence, allowing the model to learn deep bidirectional context. This methodology enables BERT to capture nuanced word relationships, substantially improving performance across diverse NLP tasks. RoBERTa (A Robustly Optimized BERT Pretraining Approach) [109] developed by Facebook AI and builds upon the foundation laid by BERT, employing dynamic masking patterns and eliminating the next sentence prediction objective. It further refines the pre-training process through larger mini-batches and more data, resulting in improved performance on downstream tasks. A smaller, faster, cheaper, and lighter version of BERT, DistilBERT [139] is trained by distilling BERT's knowledge into a smaller model that retains most of its predecessor's performance but with significantly reduced size and complexity. ALBERT (A Lite BERT) [88] is another variant of BERT that aims to reduce the model size drastically while maintaining performance. Another PLM is Electra [32] which uses a masked language model for pretraining and uses a setup where it replaces some of the tokens in an input sequence with incorrect ones and trains the model to distinguish between the \"real\" and \"fake\" tokens."}, {"title": "4.1.2 Decoder only", "content": "Decoder-only PLMs are designed primarily for generating text, capable of producing coherent and contextually relevant sequences of text based on a given prompt. The introduction of GPT (Generative Pretrained Transformer) by Radford et al. [126] marked another significant stride in transformer evolution. As a generative model pre-trained on extensive textual data, GPT's objective is to forecast subsequent tokens based on the preceding context. Exhibiting proficiency in text synthesis, language modeling, and question-answering, GPT differs from BERT in its generative training objective, which endows it with a broader, more holistic grasp of linguistic patterns. Another extension of the original TB model, Transformer-XL [34] introduces a mechanism to handle long-term dependencies, enabling the model to remember information from much earlier in the text than standard TB models. XLNet's [182] architecture allows it to function effectively in a generative capacity as well. It combines the best of both worlds: the bidirectional context modeling of BERT and the generative capabilities of models like GPT. CTRL (Conditional Transformer Language Model) [79] is developed by Salesforce, CTRL is a decoder-only model that generates text conditioned on control codes that specify domain, style, topic, dates, and other attributes. Moreover, the Reformer model [86] introduces efficiency improvements that enable the processing of very long documents, significantly reducing memory usage and computation time without sacrificing the quality of text generation."}, {"title": "4.1.3 Encoder-Decoder", "content": "Encoder-decoder PLMs are designed to handle a wide array of complex NLP tasks that involve both understanding input text (encoding) and generating new text based on that understanding (decoding). These models have been pivotal in advancing the capabilities of NLP applications, from machine translation to summarization and question-answering. Facebook AI developed BART (Bidirectional and Auto-Regressive Transformers) [97]. BART combines bidirectional encoding (similar to BERT) with autoregressive decoding (similar to GPT), making it particularly effective for text generation tasks that require a deep understanding of context, such as summarization and translation. mBART (multilingual BART) [29], an extension into multilingual contexts, is a sequence-to-sequence model pre-trained on large-scale monolingual corpora across multiple languages. This pre-training gives mBART the deep understanding of linguistic subtleties it needs to do translation work, even in languages with few resources. This makes the benefits of advanced NLP models available to everyone, regardless of language. Google introduced T5 or Text-to-Text Transformer [116]. It redefines the paradigm by framing all NLP tasks as a text-to-text problem. The model handles every task, from translation to summarization, by converting one type of text into another using a consistent approach. This innovative perspective has simplified the application of transfer learning in NLP. In addition, BigBird [97] is an encoder-decoder model that proposes a sparse attention mechanism, which reduces complexity and time consumption for tasks such as question answering and summarization."}, {"title": "4.2 NLP downstream tasks", "content": "NLP has many real-world applications. We will discuss language modeling, question answering, machine translation, text classification, and text summarization. Table 1 categorizes these NLP tasks using transformer-based models, detailing the attention mechanism, transformer variant, and underlying model for each method."}, {"title": "4.2.1 Language Modeling", "content": "Language Modeling (LM) is a key task in NLP focused on predicting the next word in a text sequence based on the context of preceding words[87]. It is essential for many NLP applications like ma-chine translation, speech recognition, and text generation. Notable advancements in LM include Autoprompt[142], Transformer-XL[34], and Dynamic Evaluation [87]. Autoprompt enhances knowledge extraction by automating prompt generation. Transformer-XL improves the handling of long-term dependencies with a recurrence mechanism and new positional encoding. Dynamic Evaluation adapts the model parameters dynamically to better suit domain-specific or stylistically varied content."}, {"title": "4.2.2 Question Answering", "content": "In the dynamic field of question-answering (QA) models [200], several key architectures have made significant contributions. SDNet (Semantic Decoding Network) [200] incorporates semantic parsing for better question comprehension, as demonstrated on the CoQA dataset [132]. XLNet [182], introduced by Yang et al., uses a permutation-based training strategy to capture bidirectional context, outperforming earlier models on benchmarks like SQUAD [129]. DIALOGPT [193], built on the GPT-2 architecture, is fine-tuned on extensive dialogue data to generate coherent conversational responses. The Reformer model [86] optimizes attention mechanisms for processing long sequences, aiding QA tasks with extensive contexts. TANDA (Transfer and Adapt) [49] improves pre-trained models"}, {"title": "4.2.3 Machine Translation", "content": "Machine translation (MT) involves the automatic translation of text from one language to another. MT models typically use an encoder-decoder structure that features a bidirectional encoder for effective context understanding and a decoder that produces text of variable lengths, based on the foundational design of the Transformer-based (TB) architecture. Elaffendi et al. introduced PIA [45], which converts natural language sentences into unique binary attention context vectors, capturing semantic context and word dependencies. Dongxing et al. [98] refined TB for MT by introducing an interacting-head attention mechanism, overcoming the low-rank bottleneck by optimizing the number of attention heads and promoting extensive interactions among them through computations in low-dimensional subspaces across all tokens."}, {"title": "4.2.4 Text Classification", "content": "Text classification is an essential task in the field of natural language processing as it forms the baseline upon which other methodologies are constructed. TB models have emerged as the leading approach for text classification, boasting considerable success in recent years [153]. One notable model, SCIBERT [8], leverages the BERT framework, pre-trained on a broad array of scientific literature, to overcome the challenges posed by the scarcity of high-caliber labeled data in the scientific domain. SCIBERT's pre-training enables enhanced performance on specialized scientific NLP tasks. Similarly, ClinicalBert [68] applies BERT's bidirectional capabilities to the analysis of clinical notes, achieving superior results in predicting hospital readmission and discovering medical concept relationships. Moreover, BioBERT [95] is a domain-specific representation model pre-trained on biomedical texts, which surpasses BERT and other leading models across various biomedical text mining tasks. Beyond these domains, TB models find intriguing applications in cybersecurity, as seen with MalBERT [127], which leverages BERT's pre-trained model for malware classification using textual features extracted from application source codes. Additionally, Murat et al. [153] introduced the BiTransformer, a novel model utilizing dual Transformer encoder blocks with bidirectional position encoding to enhance text classification tasks by refining the attention mechanisms."}, {"title": "4.2.5 Text Summarization", "content": "The field of text summarization has significantly progressed by adapting Transformer-based (TB) models to manage various text lengths and contexts [52]. Recent efforts have focused on modifying existing text-to-text models for extended narratives. A key development in this area is the Longformer [9], a TB model optimized for long-document processing with an efficient attention mechanism capable of handling larger contexts. Xiao et al. [177] introduced PRIMERA, a pyramid-based pretraining technique for multi-document summarization that uses masked sentence pretraining to improve summary coherence and informativeness. For query-focused summarization, Xu et al. Additionally, Ghalandari et al. [52] explored the use of reinforcement learning to fine-tune TB models for sentence compression, enhancing summarization efficiency by emphasizing brevity and content salience."}, {"title": "4.2.6 Sentiment Analysis", "content": "Sentiment analysis in NLP involves determining the emotional tone or attitude expressed in a piece of text. Dimple et al. [20] proposed KEAHT, a knowledge-enriched attention-based hybrid TB model for social sentiment analysis (SA), which enhances explicit knowledge using lexicalized domain ontology and latent Dirichlet allocation (LDA) topic modeling. BERT was utilized to train the corpus. This method effectively addresses complex text issues and incorporates an attention mechanism. Zhang et al.'s \"TextGT\" [191] introduces a double-view graph transformer for aspect-based sentiment analysis, incorporating both syntactic and semantic structures for comprehensive sentiment understanding. Chen et al. combine RoBERTa and LSTM in \"RoBERTa-LSTM [20],\" utilizing the strengths of"}, {"title": "4.2.7 Named Entity Recognition", "content": "Named Entity Recognition (NER) involves identifying and classifying entities within text into predefined categories, such as businesses, locations, dates, numbers, and people [108]. Li et al. [99] introduced FLAT, a TB model for Chinese NER, which converts lattice structures into flat spans where each span represents a character or latent word along with its position in the original lattice. Zhang et al. [183] developed FinBERT-MRC, a BERT-based financial NER model within the machine reading comprehension framework. Jarrar et al. [74] presented Wojood, an Arabic NER corpus recognized using BERT, employing the pre-trained ARaBERT to train a nested NER model through multi-task learning. Liu et al. [108] proposed a two-stage fine-tuning method for BERT tailored for NER in the geological domain, resulting in GeoBERT, which was initially fine-tuned on a pre-trained BERT model and then on a small dataset for geological reports. Kezhou et al. [133] introduced an ALBERT-based model, combining it with BiLSTM and Conditional Random Field (CRF) to create the ALBERT-BiLSTM-CRF model."}, {"title": "4.3 Text to Vision", "content": "Text-to-vision TB models aim to take text input and produce a corresponding image or video. There are two main types of text-to-vision transformers dual-encoder and cross-attention. The dual-encoder methods, which involve separately mapping text and vision into a shared embedding space, are appealing for their scalability in retrieval and efficiency in handling billions of images through approximate nearest-neighbor searches. Fast models, referred to as dual encoders (as shown on the left side of Fig. 5), evaluate the input image and text separately to calculate a similarity score using a single dot product. This score may be efficiently indexed, allowing for large-scale search. Conversely, slow models, which are also referred to as cross-attention models (as shown on the right side of Fig. 5), simultaneously analyze the input image and text using cross-modal attention in order to calculate a similarity score.\nThere are three different applications of text-to-vision TB models: story visualization, text-to-image, and text-to-video. The summary of all the methods for these applications is provided in Table 2."}, {"title": "Story Visualization", "content": "Several recent methods have been proposed to enhance consistency and semantic matching in story-based image generation. StoryGAN [104] employed a story-level discriminator to improve global consistency,"}, {"title": "4.4 Text to Speech", "content": "Speech-to-text technologies convert written text into spoken words, usually producing mel-spectrum and phonemes. These technologies have numerous applications, including chatbots and voice assistants. Various neural architectures have been used for this purpose, but we will focus solely on transformer-based architecture. We will discuss Tacotron 1 & 2 in section 6 in detail. These two methods have been designed for speech-to-text tasks. In addition to Tacotron 1 & 2, FastSpeech [136] is another popular method for this problem. FastSpeech uses a parallel setting for mel-spectrogram generation based on transformer blocks, which reduces the inference time significantly. It also focuses on increasing the robustness of the generation. In the past, an autoregressive setting caused propagated errors in generation, possibly due to incorrect attention alignments between text and speech. To address this, FastSpeech utilizes a phoneme duration predictor to ensure accurate alignment between text and speech, thereby increasing robustness.\nFastSpeech was tested on the LJSpeech dataset [71]. It matches the quality of autoregressive models while significantly accelerating mel-spectrogram generation by 270 times and end-to-end speech synthesis by 38 times. FastSpeech 2 [135] is a natural extension of FastSpeech. It uses a more straightforward training pipeline to reduce the training time. FastSpeech2 incorporates more information, including pitch and energy, to improve quality and accuracy. They also introduced FastSpeech 2s [135], the first system to convert waveform from text. Both FastSpeech 2 and 2s perform better than FastSpeech 2 and autoregressive models.\nFASTPITCH [89] is also based on the FastSpeech model. FASTPITCH aims to improve synthesized speech quality by integrating conditioning based on fundamental frequency estimation for each input symbol, eliminating the need for knowledge distillation of the mel-spectrogram. In addition to the methods mentioned above, there are other TB approaches, such as Durian [184], MultiSpeech [24], and s-Transformer [168], that have been utilized for this task. A summary of the discussed methods is presented in Table 3."}, {"title": "5 Computer Vision", "content": "After the success of the vanilla transformer in NLP tasks, the Vision Transformer (ViT) was introduced, catering specifically to image-based modalities [44]. This adaptation of transformers for image processing marked a significant shift in the approach to computer vision problems. An overview of the vision Transformer has been shown in (Fig 7a)."}, {"title": "5.1 Vision Transformer", "content": "The Vision Transformer reimagines an image not as a grid of pixels but as a sequence of flattened 2D patches. These patches are analogous to tokens (words) in NLP tasks. Unlike the vanilla transformer, where embeddings are directly related to the tokens of a textual sequence, in ViT, the embeddings represent the information contained in each image patch.\nAn input image $x \\in R^{H\\times W\\times C}$ is first divided into a set of fixed-size patches $x_p \\in R^{N\\times (P^2\\cdot C)}$. Here, $(H, W)$ denotes the original image resolution, C is the number of channels, (P, P) is the size of each patch, and $N = HW/P^2$ is the number of patches [44]. These patches are then linearly embedded into a D-dimensional space to create patch embeddings. This embedding process is different from the embedding mechanism in the vanilla transformer, reflecting the adaptation required for image data. The sequence of patch embeddings then serves as the input for the subsequent layers of the TB model. The TB models utilize a multi-head self-attention mechanism. In the context of ViT, this mechanism is applied to the sequence of patch embeddings. Each head in the multi-head attention module can focus on different parts of the image, capturing diverse features from the patch embeddings. This approach allows the model to consider both local and global information from the image. ViT also explores a hybrid architecture where the input sequence is derived from the feature maps of a CNN [90]. In this configuration, the patch embedding projection, which is typically applied to raw image patches, is instead applied to patches extracted from CNN feature maps. In a particular instance, these patches can be as small as 1x1, indicating that the input sequence is created by flattening the spatial dimensions and then projecting them into the Transformer dimension, as shown in Figure 6. This hybrid approach allows for the"}, {"title": "5.2 Image Processing", "content": "In the domain of Image Processing, TB models are emerging as a transformative force to reshape how visual data is interpreted and utilized. The advancements in transformer architectures, such as Vision Transformer (ViT) [44], Data- efficient Image Transformer (DeiT) [155], Pyramid Vision Transformer (PVT) [167], Cross-Shaped Window Transformer (CSWin) [43], and Cross-Covariance Image Transformer (XCIT) [2] have provided robust frameworks that significantly outperform traditional convolutional and recurrent networks [80] [143]. These transformer-based models, originally excels in natural language processing, and pave the way for sophisticated image processing applications, from image classification to object detection and beyond [155] [65].\nA well-known innovation in this domain is the development of the Image Processing Transformer (IPT) [21], which leverages the power of pre-training on large-scale datasets. The IPT model utilizes the representation ability of TB models, enhanced with multi-heads, multi-tails, and further augmented by contrastive learning to adapt to different image processing tasks efficiently. The IPT's ability to generalize across tasks, even with limited task-specific data, addresses the challenges of dataset variability and the need for multiple processing modules. With this approach, a single pre-trained model can be fine-tuned to outperform state-of-the-art methods across various low-level benchmarks.\nThis section aims to discuss the role of TB architectures in image processing domain and study their implementation in various sub-domains. The following sections will further explore the specific contributions and implementations of TB models in image processing tasks."}, {"title": "5.2.1 Edge Detection", "content": "The Edge Detection TransformER (EDTER) [124] use a novel approach for edge detection and addresses the limitation of traditional convolutional neural networks (CNNs). EDTER mitigate this limitation by combining the transformer's proficiency in capturing long-range dependencies with a two-stage process that preserves detailed local cues. The first stage of EDTER employs a global transformer encoder to take in long-range global context from coarse-grained image patches. The second stage refines this with a local transformer encoder that targets short-range local cues from fine-grained patches. EDTER produces crisp and less noisy edge maps. EDTER outperforms state-of-the-art methods on benchmarks like BSDS500 [4], NYUDv2 [115], and Multicue [114]."}, {"title": "5.2.2 Semantic Segmentation", "content": "The Swin Transformer [110], with its hierarchical structure utilizing shifted windows, demonstrates exceptional adeptness in modeling at various scales, and achieves a new state-of-the-art performance with significant margins. Segmenter [149] is built upon the Vision Transformer (ViT) that leverages global context from the first layer, and uses a mask transformer decoder or a point-wise linear decoder to translate patch embeddings directly into class labels. SegFormer [178] introduces a hierarchically structured Transformer encoder and a lightweight MLP decoder to create a system that balances efficiency with high accuracy. SeMask [72] further refines this approach by infusing semantic context into pre-trained TB backbones during fine-tuning. It also enhances the performance with minimal additional computational cost."}, {"title": "5.2.3 Object Detection", "content": "The End-to-End Object Detection with Transformers (DETR)'s architecture incorporates a transformer encoder-decoder and a novel set-based global loss enforced by bipartite matching. It is designed to output a unique set of predictions in parallel to address the challenge of duplicate predictions inherent in traditional methods [38, 118]. It also captures the intricate relationships between objects and their context within the image to simplify the detection pipeline. DETR demonstrates comparable, if not superior, performance to well-established detection systems like Faster R-CNN [134] on benchmarks such as COCO [105]."}, {"title": "5.2.4 Image resolution enhancement", "content": "The Learning Texture Transformer Network for Image Super-Resolution (TTSR) [181] introduces a novel approach to image super-resolution (SR) by transferring high-resolution (HR) textures from reference images to low-resolution (LR) ones. This novel Texture Transformer Network (TTSR) utilizes a set of interconnected modules designed for image generation tasks by facilitating the transfer and synthesis of textures through attention mechanisms. Building upon the foundations laid by TB applications in SR, the Dual Aggregation Transformer for Image Super-Resolution (DAT) [28] further innovates by merging spatial and channel dimensions within a TB framework for"}, {"title": "5.3 Image to Text", "content": "The integration of vision and language has fostered a novel paradigm for understanding and generating rich textual descriptions from images, commonly known as \"Image to Text\" translation. The Contrastive Captioner (CoCa) a foundational model for image caption generation is anexample of such a model [185]. By training jointly with contrastive and captioning losses, it adeptly merges the capabilities of both generative and contrastive methods which leads to state-of-the-art performances across a multiple tasks like visual recognition, cross-modal retrieval, and image captioning. Notably, on ImageNet, CoCa's architecture excludes cross-attention in the initial decoder layers to focus on unimodal text representations and achieved a remarkable 86.3% zero-shot top-1 accuracy. When fine-tuned, it further cemented its dominance with a new peak of 91.0% top-1 accuracy [185].\nAnother innovative approach, NLIP (Noise-robust Language-Image Pre-training), aims to counteract the inherent noise in web-crawled data, such as incorrect or irrelevant content [69]. It introduces a principled framework to stabilize pre-training through two schemes: noise-harmonization and noise-completion. With noise-harmonization, it adjusts the cross-modal alignments by estimating noise probability. While noise-completion enhances text with missing object information, guided by visual concepts. NLIP thus promises an efficient mitigation of noise impacts during pre-training [69].\nThe PaLI model [27] further exemplifies the synergy of large-scale encoders and decoders from language [158] and vision [44] pathways. By processing both modalities, it performs a broad array of tasks in many languages with its simplicity, modularity, and scalability. PaLI notably benefits from scaling the vision and language components jointly,"}, {"title": "5.4 Video to Text", "content": "Video to text generation is a pivotal area in the intersection of computer vision and natural language processing that focuses on interpreting and translating visual content into descriptive language. This task not only encapsulates the challenge of understanding visual cues from static images and dynamic sequences but also demands the generation of coherent and contextually relevant textual descriptions. Within this domain, two key tasks have emerged as significant research avenues: Temporal Action Localization and Description (TALD) and Dense Video Captioning.\nWang et al. present an innovative approach in Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning [165]. They tackle the challenges of utilizing both past and future video contexts for accurate event proposal predictions and constructing informative inputs for the decoder to generate natural event descriptions. This bidirectional proposal method, exploits past and future contexts with a novel context gating mechanism that dynamically balances contributions from the current event and its surrounding contexts. By fusing hidden states attentively, it outperforms state-of-the-art models on the ActivityNet Captions dataset, and shows significant improvement in dense video captioning tasks.\nZheng et al. introduce the Stacked Multimodal Attention Network (SMAN) for context-aware video captioning [198]. They focus on integrating historical visual and textual information into the caption generation process, which has often been overlooked in previous models. By utilizing a stacked architecture to gradually process different features and applying reinforcement learning for refinement, SMAN leverages historical context effectively to enhance captioning results.\nLei et al. propose the Channel-wise Temporal Attention Network (CTAN) for video action recognition [96]. Although it primarily targets action recognition, CTAN's methodology is notable for its focus on channel-wise attention generation and its exploitation of interactions across video frames. The paper assumes that enhancing fine-grained informative features via channel-wise attention could be beneficial for TALD as well, hinting at broader applications for their proposed network.\nLastly, A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer [70] addresses the untapped potential of audio cues in dense video captioning. By integrating a novel Bi-modal Transformer that processes both audio and visual modalities, the authors demonstrate superior performance on the ActivityNet Captions dataset. Their work suggests that leveraging multimodal inputs can lead to richer and more descriptive video captions."}, {"title": "5.5 Vision to Speech", "content": "The evolution of artificial intelligence has brought about significant advancements in the translation of visual data into spoken descriptions, a task referred to as \"Vision to Speech.\" This section explores the methodologies and innovations in the domain of generating speech from images to highlight the advancements made by various TB models.\nOne of the foundational study in this domain is the \"Image2speech\" model, which aims to generate spoken descriptions from images [57]. This model utilizes a sequence-to-sequence architecture with attention mechanisms to convert image features extracted from the VGG16 network into sequences of speech units. These units are then synthesized into audio using Clustergen. The innovation here lies in the use of different segmentation methods, including words, first-language phones, pseudo-phones, and second-language phones, to facilitate speech generation in both languages with and without known orthography. The model's performance is validated using BLEU scores and token error rates, demonstrating its ability to produce intelligible and coherent spoken sentences [57].\nBuilding on this concept, the \"Text-Free Image-to-Speech Synthesis Using Learned Segmental Units\" model takes a novel approach by eliminating the need for text as an intermediate representation [64]. This method connects image captioning and speech synthesis through a set of discrete, sub-word speech units discovered via a self-supervised visual grounding task. The model employs ResDAVEnet-VQ to learn these units and generates spoken audio captions directly from images. This approach is validated on the Flickr8k and MSCOCO datasets, showing that the generated captions capture diverse visual semantics effectively [64].\nThe \"Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-Modal Tokens\" model introduces a powerful and efficient framework for image-to-speech captioning [84]. It leverages a pre-trained vision-language model, GiT, to transfer rich image comprehension and language modeling knowledge into the image-to-speech task. The model generates discretized speech units using HuBERT and converts images into compressed image units through vector quantization. This method significantly reduces data storage and computational requirements, setting new state-of-the-art performance on the COCO and Flickr8k datasets [84].\nFinally, the \"IMAGEBIND: One Embedding Space To Bind Them All\" model aims to learn a joint embedding across multiple modalities, including images, text, audio, depth, thermal, and IMU data [53]. By using images to bind these modalities together, IMAGEBIND enables zero-shot capabilities across new modalities without requiring extensive paired data for training. This model demonstrates emergent capabilities such as cross-modal retrieval and zero-shot recognition, outperforming specialist supervised models in various tasks. IMAGEBIND leverages the natural pairing of modalities with images to create a unified embedding space, showcasing strong few-shot recognition results and setting a new benchmark for evaluating vision models across both visual and non-visual tasks [53]."}, {"title": "6 Speech Processing", "content": "Sequence processing is one of the areas where TB architectures [157] have shown great impact. In speech processing tasks such as automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS), algorithms deal with sequences, making TB architectures attractive for integration. While long short-term memory (LSTM) has been the go-to sequence processing module in the past, recent studies have highlighted the superiority of transformers over recurrent-based approaches [101, 170, 186]. The TB processing mechanism allows for smoother information flow through sequential steps, leading to improved gradient flow and streamlined training. Moreover, the parallel calculation capability of transformers accelerates the training process significantly [186].\nIn [101, 170, 186], researchers have shown how TB and recurrent-based approaches performed across various speech processing tasks and datasets. The empirical results demonstrate the superiority of TB approaches. In [78], Karita et al. have provided instructions for the community to make utilization of TB approaches easier (an attempt for the community to use TB models). Additionally, they work on an open, community-driven project for end-to-end speech applications using both TB and RNN models. Albert Zeyer et al. [186] validate the efficiency of gradient flow and parallel computing, asserting that TB models require less training time than a similarly performing LSTM model. However, the TB models are prone to overfitting. To address this issue, the authors demonstrate the effectiveness of data augmentation. In [25, 186], they have investigated the performance of the combined architecture. The forthcoming section will discuss this domain's most popular TB architecture alongside various prominent tasks."}, {"title": "6.1 Speech TB models", "content": "wav2vec 2.0. This framework uses self-supervised learning to convert raw audio data into a latent vector space. It consists of feature encoding and contextual representation learning. The raw waveform is fed to the encoder, which includes temporal convolutional neural networks and quantization [6", "54": "utilized [7", "55": "aims to capture global and fine-grained features by integrating TB and convolution networks. Convolutional neural networks excel at extracting detailed features, but requiring multiple layers can lead to computational challenges during training and inference. TB models were used to understand the global context better, but they may need assistance identifying detailed local feature patterns. Consequently, Anmol Gulati et al. adopted a collaborative approach with convolution to leverage the strengths of both techniques. The method they proposed demonstrated superiority over the state-of-the-art models in the LibriSpeech [120", "13": "benchmarks, as indicated by the Word Error Rate (WER) metric.\nHuBERT. HuBERT (Hidden Unit BERT) [63"}]}