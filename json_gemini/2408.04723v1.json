{"title": "Survey: Transformer-based Models in Data Modality Conversion", "authors": ["ELYAS RASHNO", "AMIR ESKANDARI", "AMAN ANAND", "FARHANA ZULKERNINE"], "abstract": "Transformers have made significant strides across various artificial intelligence domains, including natural language processing, computer vision, and audio processing. This success has naturally garnered considerable interest from both academic and industry researchers. Consequently, numerous Transformer variants (often referred to as X-formers) have been developed for these fields. However, a thorough and systematic review of these modality-specific conversions remains lacking. Modality Conversion involves the transformation of data from one form of representation to another, mimicking the way humans integrate and interpret sensory information. This paper provides a comprehensive review of transformer-based models applied to the primary modalities of text, vision, and speech, discussing their architectures, conversion methodologies, and applications. By synthesizing the literature on modality conversion, this survey aims to underline the versatility and scalability of transformers in advancing AI-driven content generation and understanding.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) is inspired by human perceptions, such as vision, hearing, and reading, and seeks to replicate these abilities [67]. Typically, a modality is linked to a particular sensor that creates a distinct communication channel, such as sight, speech, and written language. Humans possess a fundamental process in sensory perception that allows them to efficiently engage with the world in dynamic and unconstrained situations by integrating data from several sensory modalities. Each modality functions as a separate source of information that is distinguished by its own specific statistical features. A photograph depicting \"elephants playing in the water\" delivers visual information through numerous pixels, whereas a similar verbal description conveys this sight using distinct words. Similarly, voice can communicate the same occurrence using spectrograms or speech characteristics. A data conversion AI system must receive input from a specific modality, process, understand, and reproduce its content in a different modality, imitating human-like perception. Modality Conversion (MC) is a broad methodology for constructing artificial intelligence models that can extract and transform information from one modality of representation to another [67]."}, {"title": "2 Related surveys", "content": "Numerous studies have investigated TB models in the areas of text [187], vision [92], speech processing [76], and multi-modality [11] (Fig. 3). This section reviews these survey papers by categorizing them according to their respective modalities."}, {"title": "Surveys in NLP", "content": "In recent years, TB pre-trained models have revolutionized the field of natural language processing (NLP), enabling significant advancements in various applications. Comprehensive surveys such as those by Ammus et al. [3] (2021) and Zhang et al. [187] have meticulously cataloged the evolution and efficacy of these models across numerous NLP tasks, highlighting their versatility and superior performance. Detailed visualizations by Lee et al. [93] provide intuitive insights into the inner workings and interpretability of transformers, facilitating a deeper understanding of their mechanisms. Furthermore, the applicability of these models in less-resourced languages, as discussed by Rahman et al. [128], underscores the transformative impact of TBs in global linguistic contexts. Practical deployments of these models in real-world scenarios, examined by Smith et al. [146], demonstrate their utility and effectiveness in diverse, real-life applications, reinforcing the importance of continuous innovation in transformer architectures for advancing NLP."}, {"title": "Surveys in Computer Vision", "content": "The application of TB models to computer vision has been extensively explored, with numerous surveys providing comprehensive overviews of their evolution, applications, and effectiveness. Surveys such as those by Johnson et al. [75] and Lee et al. [92] detail the foundational aspects and developments in vision transformers, while Kim et al. [82] and Park et al. [121] offer insights into the specific architectural advancements and model variations. Further, Liu et al. [106] and Wang et al. [164] examine the utility of transformers in low-level computer vision tasks, and Smith et al. [147] focus on the integration of TB models with generative adversarial networks (GANs). Practical applications and the impact of these models in real-world scenarios are highlighted by Brown et al. [12] and Chen et al. [22], while Davis et al. [36] and Evans et al. [46] delve into the strategies for model compression and efficiency improvements, ensuring their scalability and performance optimization. Collectively, these surveys"}, {"title": "Surveys in Speech Processing", "content": "Recent studies have highlighted the transformative impact of TB models in speech processing. Comparisons with RNNs reveal transformers' superior performance and efficiency (Lee et al., [91]). Extensive surveys detail their advantages in speech applications (Johnson et al., [76]) and innovations like the TEASEL model for integrating speech and language processing (Kim et al., [81]). Additionally, the multilingual capabilities of transformers in automatic speech recognition are emphasized (Chen et al., [18]), and non-autoregressive transformers show promise for faster, accurate speech-to-text conversion (Park et al., [122]). These studies collectively underscore transformers' advancements in model performance, efficiency, and multilingual support."}, {"title": "Surveys in multi-modal processing", "content": "TB models have shown significant advancements in multimodal learning, as highlighted by recent surveys. The comprehensive review by Smith et al. [145] on TB multimodal pre-trained models emphasizes the integration of different data modalities, enhancing model versatility and performance. Brown et al. [11] provides an in-depth survey on video-language pre-training with transformers, demonstrating their capability to learn rich representations from synchronized video and textual data. Lee et al. [94] examine various techniques for transformer compression, addressing the need for efficient and scalable models in resource-constrained environments. Furthermore, the survey by Davis et al. [35] on multimodal learning with transformers explores the synergistic use of multiple modalities, showing substantial improvements in tasks that require comprehensive understanding from diverse data sources. These studies collectively illustrate the transformative potential of TB models in multimodal and pre-training contexts."}, {"title": "3 Architecture of TB models", "content": "The transformer is a type of architecture that has revolutionized the field of natural language processing (NLP) and has subsequently made significant impacts across various domains of artificial intelligence (AI). Introduced by Vaswani et al. in 2017 [158], the TB model deviates from earlier sequence-to-sequence architectures by eschewing recurrent (RNN) or convolutional layers. Instead, it relies entirely on a mechanism known as self-attention to weigh the significance of different parts of the input data. The vanilla transformer model consists of two main components: an encoder and a decoder, each composed of a stack of identical layers (Fig. 4).\nSelf-attention mechanism: A key breakthrough in the Transformer model is the self-attention mechanism. This feature enables the model to assess the significance of various segments of the input sequence while handling each token. The mechanism functions concurrently across all positions within the sequence.\nModel architecture: The Transformer's structure includes an encoder and a decoder, each comprising several layers of multi-head attention and feed-forward neural networks. The self-attention mechanism enables the model to focus on diverse parts of the input sequence, efficiently capturing long-range dependencies. Fig. 4 depicts the basic architecture of the vanilla transformer.\n\u2022 Encoder-decoder: The Transformer model is commonly employed in sequence-to-sequence tasks, where the encoder processes the input sequence and the decoder produces the output sequence.\n\u2022 Layer stacking: Both the encoder and decoder are composed of multiple identical layers stacked vertically. Each of these layers includes self-attention mechanisms and feed-forward neural networks."}, {"title": "Positional encoding", "content": "Unlike RNNs or CNNs, the Transformer lacks built-in positional information. To address this, it adds positional encodings to the input embeddings, providing data about the position of each token in the sequence [158]."}, {"title": "Multi-head attention", "content": "Rather than using a single attention mechanism, the Transformer utilizes multi-head attention. This technique computes attention multiple times in parallel, each with distinct learned linear projections, allowing the model to focus on various parts of the input sequence for different tasks."}, {"title": "Position-wise feed-forward networks", "content": "Following the multi-head attention mechanism, a position-wise feed-forward network is applied independently to each position within the sequence."}, {"title": "Residual connections and layer normalization", "content": "To combat the vanishing gradient problem, residual connections are employed. Additionally, layer normalization is applied after each sub-layer within each layer."}, {"title": "4 Natural Language Processing", "content": "In the rapid development of NLP, Pretrained Language Models (PLMs) [66] have established new benchmarks in performance across a range of linguistic tasks. In this section, we will first illustrate the architecture of NLP transformers and highlight prominent models. Subsequently, we will study various downstream tasks in NLP. Finally, we will show the application of TB models in converting textual data to visual or speech modalities."}, {"title": "4.1 TB Architecture in NLP", "content": "In NLP, three types of TB models namely Encoder-only, Decoder-only, and Encoder-Decoder, which will be explained."}, {"title": "4.1.1 Encoder-only", "content": "Encoder-only architectures within the domain of PLMs endeavor to encapsulate the entirety of semantic and contextual data present within a text corpus, subsequently transforming this information into a condensed feature vector representation. The most popular encoder-only architecture is BERT, proposed by Devlin et al. [38]. It represents a paradigm shift as a pre-trained model that leverages the TB's architecture and is subsequently fine-tuned to excel in various NLP applications, including sentiment analysis, entity recognition, and question-answering. BERT's pre-training involves predicting masked tokens in a sentence, allowing the model to learn deep bidirectional context. This methodology enables BERT to capture nuanced word relationships, substantially improving performance across diverse NLP tasks. RoBERTa (A Robustly Optimized BERT Pretraining Approach) [109] developed by Facebook AI and builds upon the foundation laid by BERT, employing dynamic masking patterns and eliminating the next sentence prediction objective. It further refines the pre-training process through larger mini-batches and more data, resulting in improved performance on downstream tasks. A smaller, faster, cheaper, and lighter version of BERT, DistilBERT [139] is trained by distilling BERT's knowledge into a smaller model that retains most of its predecessor's performance but with significantly reduced size and complexity. ALBERT (A Lite BERT) [88] is another variant of BERT that aims to reduce the model size drastically while maintaining performance. Another PLM is Electra [32] which uses a masked language model for pretraining and uses a setup where it replaces some of the tokens in an input sequence with incorrect ones and trains the model to distinguish between the \"real\" and \"fake\" tokens."}, {"title": "4.1.2 Decoder only", "content": "Decoder-only PLMs are designed primarily for generating text, capable of producing coherent and contextually relevant sequences of text based on a given prompt. The introduction of GPT (Generative Pretrained Transformer) by Radford et al. [126] marked another significant stride in transformer evolution. As a generative model pre-trained on extensive textual data, GPT's objective is to forecast subsequent tokens based on the preceding context. Exhibiting proficiency in text synthesis, language modeling, and question-answering, GPT differs from BERT in its generative training objective, which endows it with a broader, more holistic grasp of linguistic patterns. Another extension of the original TB model, Transformer-XL [34] introduces a mechanism to handle long-term dependencies, enabling the model to remember information from much earlier in the text than standard TB models. XLNet's [182] architecture allows it to function effectively in a generative capacity as well. It combines the best of both worlds: the bidirectional context modeling of BERT and the generative capabilities of models like GPT. CTRL (Conditional Transformer Language Model) [79] is developed by Salesforce, CTRL is a decoder-only model that generates text conditioned on control codes that specify domain, style, topic, dates, and other attributes. Moreover, the Reformer model [86] introduces efficiency improvements that enable the processing of very long documents, significantly reducing memory usage and computation time without sacrificing the quality of text generation."}, {"title": "4.1.3 Encoder-Decoder", "content": "Encoder-decoder PLMs are designed to handle a wide array of complex NLP tasks that involve both understanding input text (encoding) and generating new text based on that understanding (decoding). These models have been pivotal in advancing the capabilities of NLP applications, from machine translation to summarization and question-answering. Facebook AI developed BART (Bidirectional and Auto-Regressive Transformers) [97]. BART combines bidirectional encoding (similar to BERT) with autoregressive decoding (similar to GPT), making it particularly effective for text generation tasks that require a deep understanding of context, such as summarization and translation. mBART (multilingual BART) [29], an extension into multilingual contexts, is a sequence-to-sequence model pre-trained on large-scale monolingual corpora across multiple languages. This pre-training gives mBART the deep understanding of linguistic subtleties it needs to do translation work, even in languages with few resources. This makes the benefits of advanced NLP models available to everyone, regardless of language. Google introduced T5 or Text-to-Text Transformer [116]. It redefines the paradigm by framing all NLP tasks as a text-to-text problem. The model handles every task, from translation to summarization, by converting one type of text into another using a consistent approach. This innovative perspective has simplified the application of transfer learning in NLP. In addition, BigBird [97] is an encoder-decoder model that proposes a sparse attention mechanism, which reduces complexity and time consumption for tasks such as question answering and summarization."}, {"title": "4.2 NLP downstream tasks", "content": "NLP has many real-world applications. We will discuss language modeling, question answering, machine translation, text classification, and text summarization. Table 1 categorizes these NLP tasks using transformer-based models, detailing the attention mechanism, transformer variant, and underlying model for each method."}, {"title": "4.2.1 Language Modeling", "content": "Language Modeling (LM) is a key task in NLP focused on predicting the next word in a text sequence based on the context of preceding words[87]. It is essential for many NLP applications like machine translation, speech recognition, and text generation. Notable advancements in LM include Autoprompt[142], Transformer-XL[34], and Dynamic Evaluation [87]. Autoprompt enhances knowledge extraction by automating prompt generation. Transformer-XL improves the handling of long-term dependencies with a recurrence mechanism and new positional encoding. Dynamic Evaluation adapts the model parameters dynamically to better suit domain-specific or stylistically varied content."}, {"title": "4.2.2 Question Answering", "content": "In the dynamic field of question-answering (QA) models [200], several key architectures have made significant contributions. SDNet (Semantic Decoding Network) [200] incorporates semantic parsing for better question comprehension, as demonstrated on the CoQA dataset [132]. XLNet [182], introduced by Yang et al., uses a permutation-based training strategy to capture bidirectional context, outperforming earlier models on benchmarks like SQUAD [129]. DIALOGPT [193], built on the GPT-2 architecture, is fine-tuned on extensive dialogue data to generate coherent conversational responses. The Reformer model [86] optimizes attention mechanisms for processing long sequences, aiding QA tasks with extensive contexts. TANDA (Transfer and Adapt) [49] improves pre-trained models like BERT for specific QA tasks through a two-step fine-tuning process. TOD-BERT [176], designed for task-oriented dialogue systems, enhances performance by fine-tuning on diverse dialogue datasets. SOLOIST [123] combines language generation and task completion in a unified framework, improving dialogue system robustness and accuracy."}, {"title": "4.2.3 Machine Translation", "content": "Machine translation (MT) involves the automatic translation of text from one language to another. MT models typically use an encoder-decoder structure that features a bidirectional encoder for effective context understanding and a decoder that produces text of variable lengths, based on the foundational design of the Transformer-based (TB) architecture. Elaffendi et al. introduced PIA [45], which converts natural language sentences into unique binary attention context vectors, capturing semantic context and word dependencies. Dongxing et al. [98] refined TB for MT by introducing an interacting-head attention mechanism, overcoming the low-rank bottleneck by optimizing the number of attention heads and promoting extensive interactions among them through computations in low-dimensional subspaces across all tokens."}, {"title": "4.2.4 Text Classification", "content": "Text classification is an essential task in the field of natural language processing as it forms the baseline upon which other methodologies are constructed. TB models have emerged as the leading approach for text classification, boasting considerable success in recent years [153]. One notable model, SCIBERT [8], leverages the BERT framework, pre-trained on a broad array of scientific literature, to overcome the challenges posed by the scarcity of high-caliber labeled data in the scientific domain. SCIBERT's pre-training enables enhanced performance on specialized scientific NLP tasks. Similarly, ClinicalBert [68] applies BERT's bidirectional capabilities to the analysis of clinical notes, achieving superior results in predicting hospital readmission and discovering medical concept relationships. Moreover, BioBERT [95] is a domain-specific representation model pre-trained on biomedical texts, which surpasses BERT and other leading models across various biomedical text mining tasks. Beyond these domains, TB models find intriguing applications in cybersecurity, as seen with MalBERT [127], which leverages BERT's pre-trained model for malware classification using textual features extracted from application source codes. Additionally, Murat et al. [153] introduced the BiTransformer, a novel model utilizing dual Transformer encoder blocks with bidirectional position encoding to enhance text classification tasks by refining the attention mechanisms."}, {"title": "4.2.5 Text Summarization", "content": "The field of text summarization has significantly progressed by adapting Transformer-based (TB) models to manage various text lengths and contexts [52]. Recent efforts have focused on modifying existing text-to-text models for extended narratives. A key development in this area is the Longformer [9], a TB model optimized for long-document processing with an efficient attention mechanism capable of handling larger contexts. Xiao et al. [177] introduced PRIMERA, a pyramid-based pretraining technique for multi-document summarization that uses masked sentence pretraining to improve summary coherence and informativeness. For query-focused summarization, Xu et al. Additionally, Ghalandari et al. [52] explored the use of reinforcement learning to fine-tune TB models for sentence compression, enhancing summarization efficiency by emphasizing brevity and content salience."}, {"title": "4.2.6 Sentiment Analysis", "content": "Sentiment analysis in NLP involves determining the emotional tone or attitude expressed in a piece of text. Dimple et al. [20] proposed KEAHT, a knowledge-enriched attention-based hybrid TB model for social sentiment analysis (SA), which enhances explicit knowledge using lexicalized domain ontology and latent Dirichlet allocation (LDA) topic modeling. BERT was utilized to train the corpus. This method effectively addresses complex text issues and incorporates an attention mechanism. Zhang et al.'s \"TextGT\" [191] introduces a double-view graph transformer for aspect-based sentiment analysis, incorporating both syntactic and semantic structures for comprehensive sentiment understanding. Chen et al. combine RoBERTa and LSTM in \"RoBERTa-LSTM [20],\" utilizing the strengths of"}, {"title": "4.2.7 Named Entity Recognition", "content": "Named Entity Recognition (NER) involves identifying and classifying entities within text into predefined categories, such as businesses, locations, dates, numbers, and people [108]. Li et al. [99] introduced FLAT, a TB model for Chinese NER, which converts lattice structures into flat spans where each span represents a character or latent word along with its position in the original lattice. Zhang et al. [183] developed FinBERT-MRC, a BERT-based financial NER model within the machine reading comprehension framework. Jarrar et al. [74] presented Wojood, an Arabic NER corpus recognized using BERT, employing the pre-trained ARaBERT to train a nested NER model through multi-task learning. Liu et al. [108] proposed a two-stage fine-tuning method for BERT tailored for NER in the geological domain, resulting in GeoBERT, which was initially fine-tuned on a pre-trained BERT model and then on a small dataset for geological reports. Kezhou et al. [133] introduced an ALBERT-based model, combining it with BiLSTM and Conditional Random Field (CRF) to create the ALBERT-BiLSTM-CRF model."}, {"title": "4.3 Text to Vision", "content": "Text-to-vision TB models aim to take text input and produce a corresponding image or video. There are two main types of text-to-vision transformers dual-encoder and cross-attention. The dual-encoder methods, which involve separately mapping text and vision into a shared embedding space, are appealing for their scalability in retrieval and efficiency in handling billions of images through approximate nearest-neighbor searches. Fast models, referred to as dual encoders (as shown on the left side of Fig. 5), evaluate the input image and text separately to calculate a similarity score using a single dot product. This score may be efficiently indexed, allowing for large-scale search. Conversely, slow models, which are also referred to as cross-attention models (as shown on the right side of Fig. 5), simultaneously analyze the input image and text using cross-modal attention in order to calculate a similarity score.\nThere are three different applications of text-to-vision TB models: story visualization, text-to-image, and text-to-video. The summary of all the methods for these applications is provided in Table 2.\nStory Visualization: Several recent methods have been proposed to enhance consistency and semantic matching in story-based image generation. StoryGAN [104] employed a story-level discriminator to improve global consistency,"}, {"title": "Text-to-image generation", "content": "a subset of story visualization, has traditionally focused on enhancing semantic relevance and resolution. Recent advances in text-driven image creation have been achieved through extensive training data and large-scale models like DALL-E [131], its successor DALL-E2 [130], CogView [41], and Make-A-Scene [47], which incorporate sketch input. Despite their ability to produce high-quality images, text-to-image models sometimes struggle to encode context, particularly with metaphorical phrases across multiple sentences. Furthermore, utilizing advanced models can be computationally challenging due to their large size. For instance, diffusion-based models such as Imagen [138] and DALL-E2 [130] contain 2 billion and 3.5 billion parameters, respectively, limiting their use in resource-constrained inference scenarios."}, {"title": "Text to video", "content": "Creating videos from text involves producing multiple frames based on textual input, like story visualization. Recent advancements have led to sophisticated video generation models like GODIVA [174] and NUWA [175]. Moreover, contemporary research has succeeded in creating high-resolution videos with sequential frames of superior quality [62, 144]. Typically, cutting-edge text-to-video models (CogVideo [62] and Make-A-Video [144]) create videos from a single sentence, often featuring uniform backgrounds. The Phenaki method [159] now allows for video generation based on extensive paragraphs. This method, while capable of generating longer-duration videos, demands a very large model trained on extensive data and a detailed paragraph with closely timed scene descriptions."}, {"title": "4.4 Text to Speech", "content": "Speech-to-text technologies convert written text into spoken words, usually producing mel-spectrum and phonemes. These technologies have numerous applications, including chatbots and voice assistants. Various neural architectures have been used for this purpose, but we will focus solely on transformer-based architecture. We will discuss Tacotron 1 & 2 in section 6 in detail. These two methods have been designed for speech-to-text tasks. In addition to Tacotron 1 & 2, FastSpeech [136] is another popular method for this problem. FastSpeech uses a parallel setting for mel-spectrogram generation based on transformer blocks, which reduces the inference time significantly. It also focuses on increasing the robustness of the generation. In the past, an autoregressive setting caused propagated errors in generation, possibly due to incorrect attention alignments between text and speech. To address this, FastSpeech utilizes a phoneme duration predictor to ensure accurate alignment between text and speech, thereby increasing robustness.\nFastSpeech was tested on the LJSpeech dataset [71]. It matches the quality of autoregressive models while significantly accelerating mel-spectrogram generation by 270 times and end-to-end speech synthesis by 38 times. FastSpeech 2 [135] is a natural extension of FastSpeech. It uses a more straightforward training pipeline to reduce the training time. FastSpeech2 incorporates more information, including pitch and energy, to improve quality and accuracy. They also introduced FastSpeech 2s [135], the first system to convert waveform from text. Both FastSpeech 2 and 2s perform better than FastSpeech 2 and autoregressive models.\nFASTPITCH [89] is also based on the FastSpeech model. FASTPITCH aims to improve synthesized speech quality by integrating conditioning based on fundamental frequency estimation for each input symbol, eliminating the need for knowledge distillation of the mel-spectrogram. In addition to the methods mentioned above, there are other TB approaches, such as Durian [184], MultiSpeech [24], and s-Transformer [168], that have been utilized for this task. A summary of the discussed methods is presented in Table 3."}, {"title": "5 Computer Vision", "content": ""}, {"title": "5.1 Vision Transformer", "content": "After the success of the vanilla transformer in NLP tasks, the Vision Transformer (ViT) was introduced, catering specifically to image-based modalities [44]. This adaptation of transformers for image processing marked a significant shift in the approach to computer vision problems. An overview of the vision Transformer has been shown in (Fig 7a)."}, {"title": "An input image", "content": "x \u2208 RH\u00d7W\u00d7C is first divided into a set of fixed-size patches xp \u2208 RN\u00d7(P\u00b2\u22c5C). Here, (H, W) denotes the original image resolution, C is the number of channels, (P, P) is the size of each patch, and N = HW/P\u00b2 is the number of patches [44]. These patches are then linearly embedded into a D-dimensional space to create patch embeddings. This embedding process is different from the embedding mechanism in the vanilla transformer, reflecting the adaptation required for image data. The sequence of patch embeddings then serves as the input for the subsequent layers of the TB model. The TB models utilize a multi-head self-attention mechanism. In the context of ViT, this mechanism is applied to the sequence of patch embeddings. Each head in the multi-head attention module can focus on different parts of the image, capturing diverse features from the patch embeddings. This approach allows the model to consider both local and global information from the image. ViT also explores a hybrid architecture where the input sequence is derived from the feature maps of a CNN [90]. In this configuration, the patch embedding projection, which is typically applied to raw image patches, is instead applied to patches extracted from CNN feature maps. In a particular instance, these patches can be as small as 1x1, indicating that the input sequence is created by flattening the spatial dimensions and then projecting them into the Transformer dimension, as shown in Figure 6. This hybrid approach allows for the incorporation of CNN's strengths in extracting local features and hierarchical representations while still leveraging the global self-attention mechanism of the transformer."}, {"title": "Data-efficient image transformers & distillation through attention (DeiT)", "content": "DeiT [155] specifically tackles the challenges of training efficiency and resource demands, issues that have been significant hurdles for the application of earlier TB models in image processing tasks. The main achievement of DeiT is demonstrating the feasibility of training high-performing, convolution-free transformers solely on the ImageNet dataset. This training is remarkably efficient, requiring only a single computer and less than three days to complete. This contrasts sharply with previous models that relied on pre-training with hundreds of millions of images and substantial computational resources, limiting their applicability."}, {"title": "Pyramid Vision Transformer (PVT)", "content": "Following the trail of the Vision Transformer (ViT) [44], the Pyramid Vision Transformer (PVT) [167] emerges as a versatile backbone for various dense prediction tasks in computer vision to address the limitations of ViT in this domain. The architecture of PVT introduces a pyramid structure that generates multi-scale feature maps and incorporates a progressive shrinking pyramid. PVT has shown substantial performance in downstream tasks. When combined with RetinaNet, it achieves a 40.4 AP on the COCO dataset with a parameter count comparable to ResNet50+RetinaNet."}, {"title": "Swin Transformer", "content": "The Swin Transformer [110] has emerged as a successor to the Vision Transformer. The Swin Transformer introduces a hierarchical design with shifted windows which enhances the efficiency by limiting self-attention to local windows while enabling cross-window connection. This architecture allows for scalable representation at various resolutions by maintaining linear computational complexity regardless of image size. Swin Transformer outperforms the state-of-the-art models across multiple downstream task such as image classification, object detection, and semantic segmentation."}, {"title": "CSwin Transformer", "content": "The CSWin Transformer [43] represents a significant advancement in the domain of TB backbones for vision tasks. Its novel Cross-Shaped Window self-attention mechanism computes self-attention across horizontal and vertical stripes in parallel to form a cross-shaped window. This approach outperforms existing encoding schemes by better handling local positional information, empowers the CSWin Transformer to support various input resolutions. CSWin Transformer achieves new state-of-the-art performances on benchmarks like ImageNet-1K, COCO, and ADE20K without relying on extra data or labels. It also exhibits superior speed-accuracy trade-offs compare to prior architectures like Swin Transformer."}, {"title": "Cross-Covariance Image Transformers (XCiT)", "content": "The Cross-Covariance Image Transformer (XCiT) [2] introduces an innovative approach to address the computational inefficiencies of TB models. XCiT implements a novel cross-covariance attention (XCA) mechanism that operates across feature channels rather than tokens. This leads to linear complexity with respect to the number of tokens and facilitates efficient processing of high-resolution images. XCiT delivers excellent results on diverse benchmarks, including image classification, object detection, and semantic segmentation. The strategic combination of cross-covariance attention with other models marks a significant advancement in applying TB models to high-resolution image processing."}, {"title": "5.2 Image Processing", "content": "In the domain of Image Processing, TB models are emerging as a transformative force to reshape how visual data is interpreted and utilized. The advancements in transformer architectures, such as Vision Transformer (ViT) [44], Data-efficient Image Transformer (DeiT) [155], Pyramid Vision Transformer (PVT) [167], Cross-Shaped Window Transformer (CSWin) [43], and Cross-Covariance Image Transformer (XCiT) [2] have provided robust frameworks that significantly outperform traditional convolutional and recurrent networks [80] [143]. These transformer-based models, originally excels in natural language processing, and pave the way for sophisticated image processing applications, from image classification to object detection and beyond [155] [65].\nA well-known innovation in this domain is the development of the Image Processing Transformer (IPT) [21], which leverages the power of pre-training on large-scale datasets. The IPT model utilizes the representation ability of TB models, enhanced with multi-heads, multi-tails, and further augmented by contrastive learning to adapt to different image processing tasks efficiently. The IPT's ability to generalize across tasks, even with limited task-specific data, addresses the challenges of dataset variability and the need for multiple processing modules. With this approach, a single pre-trained model can be fine-tuned to outperform state-of-the-art methods across various low-level benchmarks.\nThis section aims to discuss the role of TB architectures in image processing domain and study their implementation in various sub-domains. The following sections will further explore the specific contributions and implementations of TB models in image processing tasks."}, {"title": "5.2.1 Edge Detection", "content": "The Edge Detection TransformER (EDTER) [124] use a novel approach for edge detection and addresses the limitation of traditional convolutional neural networks (CNNs). EDTER mitigate this limitation by combining the transformer's proficiency in capturing long-range dependencies with a two-stage process that preserves detailed local cues. The first stage of EDTER employs a global transformer encoder to take in long-range global context from coarse-grained image patches. The second stage refines this with a local transformer encoder that targets short-range local cues from fine-grained patches. EDTER produces crisp and less noisy edge maps. EDTER outperforms state-of-the-art methods on benchmarks like BSDS500 [4], NYUDv2 [115], and Multicue [114]."}, {"title": "5.2.2 Semantic Segmentation", "content": "The Swin Transformer [110], with its hierarchical structure utilizing shifted windows, demonstrates exceptional adeptness in modeling at various scales, and achieves a new state-of-the-art performance with significant margins. Segmenter [149] is built upon the Vision Transformer (ViT) that leverages global context from the first layer, and uses a mask transformer decoder or a point-wise linear decoder to translate patch embeddings directly into class labels. SegFormer [178] introduces a hierarchically structured Transformer encoder and a lightweight MLP decoder to create a system that balances efficiency with high accuracy. SeMask [72] further refines this approach by infusing semantic context into pre-trained TB backbones during fine-tuning. It also enhances the performance with minimal additional computational cost."}, {"title": "5.2.3 Object Detection", "content": "The End-to-End Object Detection with Transformers (DETR)'s architecture incorporates a transformer encoder-decoder and a novel set-based global loss enforced by bipartite matching. It is designed to output a unique set of predictions in parallel to address the challenge of duplicate predictions inherent in traditional methods [38, 118]. It also captures the intricate relationships between objects and their context within the image to simplify the detection pipeline. DETR demonstrates comparable, if not superior, performance to well-established detection systems like Faster R-CNN [134] on benchmarks such as COCO [105]."}, {"title": "5.2.4 Image resolution enhancement", "content": "The Learning Texture Transformer Network for Image Super-Resolution (TTSR) [181] introduces a novel approach to image super-resolution (SR) by transferring high-resolution (HR) textures from reference images to low-resolution (LR) ones. This novel Texture Transformer Network (TTSR) utilizes a set of interconnected modules designed for image generation tasks by facilitating the transfer and synthesis of textures through attention mechanisms. Building upon the foundations laid by TB applications in SR, the Dual Aggregation Transformer for Image Super-Resolution (DAT) [28] further innovates by merging spatial and channel dimensions within a TB framework for"}, {"title": "5.3 Image to Text", "content": "The integration of"}]}