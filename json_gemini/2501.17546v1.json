{"title": "Is Conversational XAI All You Need? Human-Al Decision Making\nWith a Conversational XAI Assistant", "authors": ["Gaole He", "Nilay Aishwarya", "Ujwal Gadiraju"], "abstract": "Explainable artificial intelligence (XAI) methods are being proposed\nto help interpret and understand how AI systems reach specific pre-\ndictions. Inspired by prior work on conversational user interfaces,\nwe argue that augmenting existing XAI methods with conversa-\ntional user interfaces can increase user engagement and boost user\nunderstanding of the Al system. In this paper, we explored the\nimpact of a conversational XAI interface on users' understanding\nof the Al system, their trust, and reliance on the AI system. In com-\nparison to an XAI dashboard, we found that the conversational XAI\ninterface can bring about a better understanding of the AI system\namong users and higher user trust. However, users of both the XAI\ndashboard and conversational XAI interfaces showed clear over-\nreliance on the AI system. Enhanced conversations powered by\nlarge language model (LLM) agents amplified over-reliance. Based\non our findings, we reason that the potential cause of such over-\nreliance is the illusion of explanatory depth that is concomitant\nwith both XAI interfaces. Our findings have important implications\nfor designing effective conversational XAI interfaces to facilitate\nappropriate reliance and improve human-AI collaboration.", "sections": [{"title": "1 Introduction", "content": "In recent years, deep learning-based Al systems have brought about\ntremendous possibilities to change and affect our daily life [86, 104].\nDue to the intrinsic opaqueness of such systems, automating criti-\ncal decision making by using Al systems is far from reliable [27].\nHowever, leveraging such powerful Al systems to assist and em-\npower human decision makers is an alternative that has gained\nprominence [62]. In such a collaborative decision making process,\nexplanations are incorporated to increase intelligibility and ensure\nthat decision makers can make informed decisions [22]. Post-hoc\nexplainable AI (XAI) methods are typically used to help explain AI\npredictions from deep learning-based AI systems.\nTo realize the goal of complementary team performance, users of\nan Al system are expected to rely appropriately on Al advice [101].\nSuch appropriate reliance requires a comprehensive understand-\ning of the Al system and its underlying rationale alongside the\nAI advice [14, 67, 102], which play important roles in calibrating\nuser trust and reliance behaviors [116, 130]. According to several\nempirical studies in human-AI collaboration [62, 119, 130], most\nXAI methods are not as helpful as expected and are even harm-\nful at times (e.g., causing over-reliance). The reasons behind this\nare multi-fold: (1) Most existing XAI methods can only provide\nspecific types of information [68] (e.g., feature importance [72],\ncounterfactual reasoning [127]). (2) In practice, there are diverse\nstakeholders of AI systems [66, 88] (e.g., developers, experts, and\nlaypeople) having different levels of domain expertise and AI lit-\neracy. (3) The information needs of diverse stakeholders can vary\ngreatly. Thus, a specific type of XAI method can seldom address\nvarying information needs, resulting in a lack of understanding of\nthe Al system.\nBased on folk concepts in the theory of mind literature, Jacovi\net al. [51] argue that successful explanations can provide users\nwith the necessary components to build a coherent mental model.\nWe extrapolate that to make critical decisions with AI assistance,\nusers need to build a relatively more complete and coherent mental\nmodel by exploring different explanations provided by XAI methods.\nHowever, such a process can be complex-it requires processing\ninformation based on a variety of aspects, depending on the X\u0391\u0399\nmethods. When presenting tailored explanations for specific audi-\nences, designers need to trade off the simplicity and completeness\nof the explanations [48]. Instead of selecting a single specific expla-\nnation, an XAI dashboard enables users to explore their information\nneeds by providing them access to their desired explanations on\ndemand. Such an interactive interface can bring forth the advan-\ntages of both simplicity and completeness and has been increasingly\nrecognized as an effective design [81, 126]. However, not all users\nhave the necessary Al knowledge and experience to understand\nor benefit from such explanations [68]. Nor can all users articulate\ntheir information needs and find suitable XAI methods to address\ntheir concerns [109]. Therefore, we need a more flexible, dynamic,\nand personalized approach to resolving users' explanation needs.\nConversational user interfaces can provide a human-like interac-\ntion [78] and simplify complex tasks with filtered information [12],"}, {"title": "2 Related Work", "content": "This paper focuses on exploring the impact of an XAI dashboard\nand a conversational XAI interface on user understanding of an AI\nsystem (RQ1), which may further affect user trust and appropriate\nreliance (RQ2). Thus, we position our work in the following realms\nof related literature: human-AI decision making (\u00a72.1), explainable\nAI (\u00a72.2), and conversational user interfaces (\u00a72.3)."}, {"title": "2.1 Human-AI Decision Making", "content": "While predictive Al systems are powerful, they are seldom per-\nfect [58]. Transparency and accountability issues prevent deep\nlearning-based Al systems from automation in high-stakes applica-\ntions like medical diagnosis [21]. In comparison, human workers\n(e.g., medical doctors) show strong reliability and accountability\nfor their work outcomes and decisions, which serve as the foun-\ndation for customers to trust their services. With these concerns,\nhuman-AI collaborative decision making is regarded as a promising\napproach to taking advantage of both humans and AI to achieve\nmore accurate and reliable decision outcomes.\nComplementary team performance is an important goal for\nhuman-Al decision making [6, 33], and will continue to be vital\nin the age of LLMs [3, 10, 45]. To achieve complementary team\nperformance, users of Al systems are expected to rely on AI advice\nappropriately [101]. To this end, users are expected to follow AI\nadvice when the AI system is more capable than them, and not rely\non AI advice when the AI system is less capable. When users fail\nto calibrate their trust in the Al system, they may misuse or disuse\nthe AI advice, resulting in over-reliance and under-reliance, respec-\ntively. The causes for unexpected reliance behaviors are complex.\nFor example, algorithm aversion [23, 31] and algorithm apprecia-\ntion [129] can cause under-reliance and over-reliance, respectively.\nExisting work has extensively explored how confidence [19, 130],\nrisk perception [38, 40], performance feedback [71, 92], and expla-\nnations [32, 94, 119] can affect human-Al decision making.\nPrior studies found that human factors like expertise and domain\nknowledge [18, 83] and cognitive bias [7, 46] can greatly affect user\ntrust [117] and appropriate reliance [101] on the AI system. To\nmitigate the negative impact of some human factors, researchers\nhave proposed tutorial interventions [15, 18, 46, 63], cognitive forc-\ning functions [13, 43, 70], and improving transparency of the AI\nsystem [64, 71, 119]. Chiang and Yin [18] found that a tutorial in-\ntervention to reveal the limitations of the AI system can effectively\nreduce over-reliance. Others have explored the role of task factors\nsuch as task complexity and uncertainty in shaping trust and re-\nliance in human-AI decision-making [98, 100]. Bu\u00e7inca et al. [13]\nproposed cognitive forcing functions to compel people to engage\nmore thoughtfully with explanations along with AI advice. They\nfound that such interventions can effectively mitigate over-reliance.\nIn previous work, researchers [9, 16, 119, 120] explored how dif-\nferent XAI methods may affect user understanding of an AI system,\ntrust, and reliance. It is still unclear how the interaction interfaces\nto present XAI methods will substantially affect user understanding"}, {"title": "2.2 Explainable AI", "content": "While deep learning-based Al systems have been recognized as\npowerful predictive toolkits, explainability has been a primary\nconcern that prevents them from becoming widespread practice.\nAccording to GDPR, users of Al systems have the right to obtain\nmeaningful explanations along with Al predictions [105]. Under\nsuch circumstances, researchers have proposed a diverse set of XAI\nmethods like feature attribution explanations [72, 93], counterfac-\ntual explanations [124], and contrastive explanations [53, 128]. For\na more comprehensive review of existing XAI methods and criteria\nto evaluate XAI methods, we encourage readers to refer to recent\nwork by Arrieta et al. [2], Nauta et al. [80].\nAs humans have diverse information needs, there is no one-size-\nfits-all solution [69]. With a proposal of putting users/humans at the\ncenter of technology design [28, 118], more and more researchers\nhave started to explore human-centered XAI [30, 69]. In such line of\nliterature, researchers focus on the function of explanation - how\nexplanations affect user understanding and what characteristics\nmake explanations effective [1, 125]. The mental model [56] denotes\nhow one person build an internal representation of the external\nreality, and plays an important role for analyzing human-centered\n\u03a7\u0391\u0399 [5, 60, 61, 95]. Through empirical user studies, researchers\nfound that many properties of explanations like simplicity [1], com-\npleteness [61] will substantially affect user mental model and the\neffectiveness of explanations.\nAccording to Jacovi et al. [51], effective explanations should\nproduce coherent mental models (i.e., communicate information\nwhich generalizes to contrast cases), be complete to avoid mis-\nunderstanding and be interactive to address contradictions. We\nrecognize that conversational XAI interfaces can satisfy all the\nabove key properties for providing effective explanations. Thus,\nwe argue that a conversational XAI interface may benefit users\nwith a better understanding of the Al system, which can further\nfacilitate user trust and appropriate reliance. Existing work has\nexplored conversational XAI interfaces in the contexts of collabo-\nrative scientific writing [106] and decision support with a focus on\nteam performance [108]. None of the existing works, however, have\nsystematically explored the impact of conversational XAI interfaces\non trust and appropriate reliance. To fill this knowledge and em-\npirical gap while complementing existing efforts, we designed a\ncontrolled study with loan approval tasks to analyze the impact of\na conversational XAI interface on human-AI decision making."}, {"title": "2.3 Conversational User Interfaces", "content": "A conversational user interface (CUI) is a user interface for com-\nputers that emulates a conversation with a real human [122]. CUIs\nhave been studied widely across multiple disciplines, such as natu-\nral language processing, human-computer interaction, and artificial\nintelligence. Since the famous Turing Test [114], the capability to\nconduct human-like conversation has for long been recognized as"}, {"title": "3 Task, Method, and Hypotheses", "content": "In this section, we describe the loan approval task and present our\nhypotheses, which have been preregistered before data collection."}, {"title": "3.1 Loan Approval Task", "content": "The basis for our experimental setup is a task where participants\nhave to decide whether a loan application is Credit Worthy or\nNot Credit Worthy using the publicly available loan prediction\ndataset. The rationale for selecting the loan approval task as a\ntest bed is three-fold. Firstly, this task was chosen as a critical deci-\nsion making scenario for human-AI collaboration, where there is a\nclear risk and a benefit when adopting AI advice. Secondly, most\nlaypeople are familiar with this context and can make informed de-\ncisions based on their knowledge. Thirdly, It has also been adopted\nby existing research in behavioral economics [8] and human-AI\ncollaboration [39, 44].\nIn the loan approval task, participants are presented with eleven\nfeatures (including loan amount, income, and the absence or pres-\nence of credit history) in both table format and text description\n(as shown in Figure 1). Based on the application profile (composed\nof the eleven features), participants are asked to decide whether\nthe loan applicant is credit worthy to get the loan approved. This\nsimulates a realistic scenario where participants interact with an\nAl system and may rely on AI advice and XAI methods due to the\ninherent complexity in decision-making [99]. As the selected loan\napproval task is one where decision making is fully based on the\neleven features, it would be easier to assess users' decision criteria\nbased on the top-ranked features explicitly specified by the users\nthemselves.\nTwo-stage Decision Making. In our study, we adopted a two-\nstage decision making process for each loan approval task. Every\nparticipant in our study is first asked to work on the loan approval\ntask without any assistance from the AI system. After that, they\nwere given a second chance to alter their initial choice according\nto the AI advice (i.e., AI prediction) and AI explanations (e.g., X\u0391\u0399\ndashboard, according to different experimental conditions). This\nsetup is similar to the update condition in work by Green and\nChen [39]. This setup is apt for analyzing user incorporation of\nsystem advice and user trust in the AI system [24, 38]. It is a widely\nadopted setup in empirical studies exploring human-AI decision\nmaking [18, 46, 71, 119]. To assess user decision criteria, we ask"}, {"title": "3.2 Design of XAI Interfaces", "content": "XAI methods. Our selection of XAI methods is informed by the\ntaxonomy of XAI methods regarding user information needs [68,\n80, 119]. Following the XAI question bank [68], we selected six user\ninformation needs associated with the rationale of AI advice: how\n(global model-wide explanation), why, why not, how to be that (a\ndifferent prediction), how to still be this (current prediction), and\nwhat if. These user information needs can be addressed with five\nwidely-used XAI methods (correspondence summarized in Table 1).\nThese are (1) A global explanation method \u2013 PDP (i.e., partial de-\npendency plot) [36], which visualizes how one feature globally\nimpacts the model prediction, (2) Feature importance attribution\nmethod - SHAP [72]. Based on Shapley values, the SHAP method\nprovides feature importance to indicate how each feature supports\nor opposes the current model prediction. (3) Counterfactual ex-\nplanation method \u2013 MACE [127]. MACE will inform users of the\nminimum changes in the applicant profile required to flip model\nprediction. (4) Widely adopted interactive XAI toolkit \u2013 WhatIf. Based on the WhatIf toolkit, users can modify the applicant profile\nand obtain the model prediction for the new profile. (5) Decision\ntree-based explanation. This is one popular XAI method, which\nmakes decisions based on a tree-structure decision criteria. In our\nimplementation, we provide the decision path to reach the AI ad-\nvice. We implemented all these XAI methods by using the OmniXAI\nlibrarMore details can be found in supplementary materials.\nXAI Dashboard. Following existing standards, the XAI dashboard\nis an interactive interface that provides users with XAI responses\non demand when accessed through the navigation tab. Users can explore all XAI methods by focusing on one at\na time, which ensures both simplicity and complete coverage of the\navailable five XAI methods."}, {"title": "Conversational XAI Interface", "content": "Templating conversational inter-\nactions via a rule-based agent [41] can be an effective method to\nguide users in exploring their information needs and understanding\nthe model decisions. Thus, we adopted a rule-based conversational\nagent to power the conversational XAI interface. By referring to the\nXAI question bank [68], we first set up five user intents which can be answered with the corresponding XAI responses.\nTo provide a smooth conversational experience, we curated the\nfive user intents into three categories: about AI advice (SHAP,\nMACE, Decision Tree \u2013 XAI responses required no user input),\nAI advice for modified applicant profile (WhatIf, where users need\nto revise the applicant profile), and the global impact of a specific\nfeature (PDP, where users need to specify a feature of interest). At\nthe beginning of the conversation, users are guided to select one\ncategory among the three and then specify one query to check or\nspecify user input. After users receive one XAI response, we repeat\nthe aforementioned process. All user intents are wrapped into an\niterative loop, and users can stop the conversation after receiving at\nleast two different XAI responses. All the conversations are guided\nby empowering participants to select options using custom buttons\nand commands (i.e., dropdown selection for PDP or feature input\nfor WhatIf, shown in Figure 2(a)). Such designs have been widely\nadopted in domains such as conversational crowdsourcing [89, 90],\nor customer service chatbots and proven to be effective in address-\ning user information needs and are easy to use for laypeople [57].\nEvaluative Conversational XAI Interface for Decision Sup-\nport. Based on the collected user decision criteria in the initial\ndecision, we further adapted the conversation to guide users to"}, {"title": "3.3 Hypotheses", "content": "Our experiment was designed to answer questions surrounding\nthe impact of conversational XAI interfaces on user understand-\ning, trust, and reliance on AI systems. XAI dashboards, which can\nswitch between different XAI methods with a navigation bar, have\nbeen recognized as a promising interactive interface to present\nexplanations towards model decisions [25, 109, 110]. Considering\nits wide application for model explainability, we consider it a strong\nbaseline in our study. As shown in prior work, conversational user\ninterfaces have the advantages of more human-like interaction [78]\nand simplified understanding of complex tasks with filtered infor-\nmation [12] over graphical user interfaces. Compared with the XAI\ndashboard (where users interact with the dashboard in a uni-lateral\nfashion), the conversational XAI interface has the potential to in-\ncrease user engagement, and provides a more natural bi-directional\nway for users to explore their information needs and develop an\nunderstanding of the Al system. As a result, users with a conversa-\ntional XAI interface may develop a better understanding of the AI\nsystem. Thus, we hypothesize that:\n(H1): Compared to the XAI dashboard, the conversational\nXAI interface creates a better understanding of the Al system\namong users.\nPrior work has highlighted that humans show higher trust when\ninteracting with intelligent systems using a conversational inter-\nface compared to conventional web interfaces [42]. Further, con-\nversational user interfaces have been shown to increase worker\nengagement in microtask crowdsourcing [89] compared to a tradi-\ntional GUI. Such increased engagement can potentially help users\ndeliberate, reflect, and thereby make better decisions, relying on\nthe AI system more critically. Conversational XAI interfaces can\nhelp users explore and address different information needs, which\nmay bring a higher trust in the AI system. Thus, we hypothesize:\n(H2): Compared to the XAI dashboard, the conversational XAI\ninterface will help users exhibit a relatively higher trust in the\nunderlying Al system.\n(H3): Compared to the XAI dashboard, the conversational XAI\ninterface will help users exhibit a relatively more appropriate\nreliance on the underlying AI system.\nEvaluative decision support in the XAI interface may further\nhelp users reassess their initial thoughts about the AI system and\nAI advice. By revealing the difference among their decision criteria\nand providing explanations for the AI system's advice, users can\nobtain a better understanding of the Al system and make more\ncritical decisions [75]. This can in turn facilitate critical thinking\nabout the AI system, leading to a potential calibration of user trust\nand increased appropriate reliance on the Al system. Thus, we\nhypothesize that:\n(H4): Adaptive steering of conversations for evaluative deci-\nsion support in the conversational XAI interface will increase\nuser trust and appropriate reliance on an Al system."}, {"title": "4 Study Design", "content": "This section describes our experimental conditions, variables, and\nprocedures related to our study. This study was approved by the\nhuman research ethics committee of our institution."}, {"title": "4.1 Experimental Conditions", "content": "The main aspects of our research questions and hypotheses concern\nthe effect of different XAI interfaces. In our study, all participants\nworked on the loan approval tasks with a two-stage setup (described\nin Section 3.1), where AI advice is provided in the second stage of\ndecision making. The only difference is the nature of the interface\nthrough which Al advice is explained. Considering this factor as\nthe sole independent variable in our study, we designed a between-\nsubjects study with five experimental conditions:\n\u2022 Control: no XAI interface.\n\u2022 Dashboard: with XAI dashboard interface (as described in\nSection 3.2)."}, {"title": "4.2 Measures and Variables", "content": "Our hypotheses mainly considered five types of dependent vari-\nables: user understanding, user trust, performance, reliance, and\nappropriate reliance on the AI system.\nUser Understanding of the AI System. This work focuses on\nanalyzing the impact of the XAI interfaces instead of evaluating the\nquality of explanations [49]. In our study, user understanding of\nthe AI system is a function of interactive exploration with the XAI\ninterfaces, which can evolve while working on tasks. Note that we\nconsider and describe perceived explanation utility as a separate\nconstruct below. Based on existing literature [11, 103, 107, 112],\nwe synthesized and adopted four dimensions to assess user under-\nstanding of the Al system. As a result of practice through our study,\nusers can potentially learn across tasks and understand the system.\nWe aim to capture this through the dimensions of Perceived Feature\nUnderstanding, Learning Effect across tasks, and Understanding of\nthe System. All questionnaires used to assess user understanding\ncan be found in supplementary materials. To objectively quantify\nuser understanding of the features, we calculated nDCG [55] of\nusers' top-3 features and the SHAP feature importance ranking\nas Objective Feature Understanding. For the relevance scores, we\nadopted a decreasing relevance for the SHAP feature order (based\non the abstract value of SHAP values) with an interval of 1. Thus\nthe relevance scores range from [1, 11] for the 11 features we used.\nBesides, Perceived Feature Understanding is also used as an indicator\nof perceived user understanding.\nExplanation Utility. Alongside user understanding, the perceived\nexplanation utility is an important aspect identified in the existing\nliterature on human-centered XAI [29, 30, 69, 95]. We synthesized\nand adopted four dimensions based on existing literature to evaluate\nthe explanation utility provided in conditions with XAI interface.\nAccording to Jacovi et al. [51], effective explanations can provide\nusers with a coherent and complete mental model to explain the\ncurrent Al prediction. Thus, we adopted the dimensions of Expla-\nnation Completeness and Explanation Coherence in our post-task\nquestionnaires. According to Hsiao et al. [50], perceived Explanation\nClarity and Explanation Usefulness are also important dimensions\nfor assessing perceived explanation goodness.\nUser Trust. Mohseni et al. [77] showed that understandability\nand predictability are desired properties for trustworthy intelligent\nsystems. Moreover, the perceived competence of the AI system\n(i.e., users' confidence about the system's capabilities) and reli-\nability of the Al system (i.e., the extent to which the system is\nperceived not suffer from unexpected errors) are also identified\nas essential constructs to establish trust [97, 115]. In addition to\ncapturing these attributes, we also captured subjective trust of users\nby adopting three validated subscales from the trust in automation"}, {"title": "4.3 Procedure", "content": "4.3 Procedure"}, {"title": "5 Experimental Results", "content": "In this section, we present the results of our empirical study. In\naddition to the main results, we carried out exploratory analyses\nto draw nuanced interpretations of our key insights. Readers can\nrefer to the appendix. Our code and data can be found at Github."}, {"title": "5.1 Descriptive Statistics", "content": "To ensure the reliability of our results and interpretations, we only\nconsider participants who passed all attention checks. Finally, the\nparticipants considered for analysis were distributed in a balanced\nmanner across the four experimental conditions: 61 (Control), 61\n(Dashboard), 62 (CXAI), 61 (ECXAI), 61 (LLM Agent). On average,\neach task consumes 13 API calls to obtain responses in LLM Agent\ncondition, including generating reply messages and XAI usage. The\naverage time (mins) spent across conditions are: 22 (Control), 34\n(Dashboard), 52 (C\u03a7\u0391\u0399), 45 (ECXAI), 62 (LLM Agent). With Kruskal-\nWallis H-tests and post-hoc Mann-Whitney test, we confirmed\nsignificance: Control < Dashboard < CXAI, ECXAI < LLM Agent.\nDistribution of Covariates. The covariates' distribution is as fol-\nlows: ML Background (22.5% with machine learning background\nknowledge, 77.5% without machine learning background knowl-\nedge), ATI (M = 3.99, SD = 0.90; 6-point Likert scale, 1: low, 6: high),\nTiA-Propensity to Trust (M = 2.88, SD = 0.71; 5-point Likert scale,\n1: tend to distrust, 5: tend to trust), and TiA-Familiarity (M = 2.67,\nSD = 1.10; 5-point Likert scale, 1: unfamiliar, 5: very familiar).\nPerformance Overview. On average across all conditions, par-\nticipants achieved an accuracy of 64.5% (SD = 0.11), which is still\nlower than the AI accuracy (70%). The agreement fraction is 0.847\n(SD = 0.16), and the switching fraction is 0.522 (SD = 0.41). With\nthese measures, we confirm that when users disagree with AI ad-\nvice, they do not always blindly rely on AI advice. As all dependent\nvariables are not normally distributed, we used non-parametric\nstatistical tests to verify our hypotheses."}, {"title": "5.2 Hypothesis Tests", "content": "For the convenience of the readers", "H1": "effect of XAI interfaces on user understanding. To ana-\nlyze the main effect of the XAI interfaces on user understanding of\nthe Al system, we conducted an Analysis of Covariance (ANCOVA)\nwith the experimental condition as between-subjects factor and TiA-\nPropensity to Trust, TiA-Familiarity, ATI, and ML Background as\ncovariates. While our data may not be normally distributed, we\nstill adopted AN(C)OVAs for analysis because these analyses have\nbeen shown to be robust to Likert-type ordinal data. For this\nanalysis, we considered all participants across three experimental\nconditions with XAI (i.e., Dashboard, CXAI, and ECXAI). We found\nno significant differences resulting from the different XAI inter-\nfaces (i.e., experimental condition). However, the TiA-Propensity\nto Trust showed a significant impact on all dimensions of user\nunderstanding. For the objective feature understanding (continu-\nous value, non-normal distribution), we conducted Kruskal-Wallis\nH-tests by considering different XAI interfaces. A significant dif-\nference (H = 16.19, p = .001) was found between participants with\ndifferent XAI interfaces. Through post-hoc Mann-Whitney U test,\nwe found that LLM Agent condition achieved significantly worse ob-"}]}