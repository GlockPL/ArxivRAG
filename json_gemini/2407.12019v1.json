{"title": "DIM: Dynamic Integration of Multimodal Entity Linking with Large Language Model", "authors": ["Shezheng Song", "Shasha Li", "Jie Yu", "Shan Zhao", "Xiaopeng Li", "Jun Ma", "Xiaodong Liu", "Zhuo Li", "Xiaoguang Mao"], "abstract": "Our study delves into Multimodal Entity Linking, aligning the men-tion in multimodal information with entities in knowledge base. Existing methodsare still facing challenges like ambiguous entity representations and limited im-age information utilization. Thus, we propose dynamic entity extraction usingChatGPT, which dynamically extracts entities and enhances datasets. We alsopropose a method: Dynamically Integrate Multimodal information with knowl-edge base (DIM), employing the capability of the Large Language Model (LLM)for visual understanding. The LLM, such as BLIP-2, extracts information rele-vant to entities in the image, which can facilitate improved extraction of entityfeatures and linking them with the dynamic entity representations provided byChatGPT. The experiments demonstrate that our proposed DIM method outper-forms the majority of existing methods on the three original datasets, and achievesstate-of-the-art (SOTA) on the dynamically enhanced datasets (Wiki+, Rich+,Diverse+). For reproducibility, our code and collected datasets are released onhttps://github.com/season1blue/DIM.", "sections": [{"title": "1 Introduction", "content": "Multimodal entity linking (Koml\u00f3si & Waldbuesser, 2015) involves linking mentionedentity (i.e. mention) in natural language texts to their corresponding entity (Zhao, Hu,Cai, & Liu, 2021) in a knowledge base (Fu et al., 2020). These entities could be individ-uals, locations, organizations in the real world, or specific entities within a knowledgebase. MEL could help computers better understand user semantics, integrate variousinformation sources, resolve ambiguities, and enhance user experience. It plays an im-portant role in search engines (P. Wu & Xie, 2024), recommendation systems (Zhou,2023), information retrieval(Ma, Zhou, et al., 2023), and knowledge graph construc-tion (Ma, Chen, et al., 2023; Zhao, Hu, Cai, & Liu, 2021). Through MEL, systems canprovide more relevant search results, more personalized recommendations, more com-prehensive information retrieval, richer knowledge graphs, and smarter conversationand text processing capabilities."}, {"title": "2 Dynamic Building Entity Representation", "content": "Multimodal entity linking(MEL) significantly influences and facilitates a profound un-derstanding and cognition of information for humans. MEL serves as a crucial meansto unify human cognition with structured knowledge repositories: (1) Assurance of se-mantic consistency: MEL ensures semantic consistency by aligning entities mentionedin the cognition with knowledge base. It helps in eliminating ambiguity and ensuringthat the interpretation of specific entities remains clear despite contextual variations. (2)Enhancement of cognitive information: MEL offers individuals a richer and deepercognitive experience. By associating entities with background knowledge, individualscan comprehensively grasp the meaning of entities, thereby elevating their cognitiveawareness of information. (3) Integrated knowledge acquisition: This contributes to"}, {"title": "2.1 Existing Entity Representation", "content": "In our investigation and study, we analyze the existing Entity Linking datasets and theirmethods of entity representation:\nWikimel and Richpedia (Zhou, 2021) employ concise attributes from Wikidata.This representation lacks representativeness for entities, as many entities share sim-ilar attributes. It is easy to link accurate cognition to the wrong entity incorrectly.\nWikiperson (Sun, 2022), similar to Wikimel, uses attributes of individuals as rep-resentatives, but in a more simplified manner. In this example, the attribute \"Amer-ican President\" is inadequate as a representative of Joe Biden, given that there havebeen multiple American Presidents.\nWeibo (Zhang, 2021) utilizes individuals from Weibo as entities, using user-authoredpersonal bios as entity representations. These bios, relying on user-generated con-tent, may contain biases or errors and do not accurately reflect the broader public'sunderstanding of the entity.\nWikidiverse (Wang, 2022) uses images collected from Wikipedia as entity rep-resentation. However, images can deviate from a person's true appearance due tofactors like angles and time, lacking real-time accuracy."}, {"title": "2.2 Leverage Chatgpt to Dynamic Connect", "content": "ChatGPT (OpenAI, 2023) is a powerful large model trained on massive amounts of webdata and is continuously updated over time. We plan to utilize the interface provided byChatGPT to inquire about entities, so as to subsequently link with the entities in theimplicit knowledge base of ChatGPT. Candidate entities are input into ChatGPT for in-quiries using the prompt: \u201cYou are a helpful assistant designed to give a comprehensiveintroduction about people. Who is this one?\u201d The generated response from ChatGPTis shown in Table 1. For experimental convenience and dataset quality enhancement,we collect the response of ChatGPT to enhance the dataset. In detail, we constructentity representations for 17391, 17804, and 57007 entities from Wikimel, Richpedia,Wikidiverse, respectively. The newly built entity representations better reflect the gen-eral public's understanding of entities, align closely with their inherent semantics, andfacilitate a unified approach to cognition and knowledge base.\nTaking the Wikimel (Zhou, 2021) as an example, out of the 17474 entities collectedfrom the dataset, 131 entities did not return any results, and 220 inquiries returned\"Sorry, I cannot provide an introduction to this entity.\" Besides, ChatGPT providesspeculative information based on cultural, regional, or other contextual cues for 462entities. For example, \"John Abbott is a common English given name and surname,\" butdid not provide specific representation. Additionally, 2997 entities require additionalinformation for verification. For instance, \u201cIt is possible that Edward J. Livernash isa private individual without any notable achievements.\u201d Furthermore, 599 entities arespectulated to be fictional names, such as \u201cJohn McDuffie is a fictional name, so thereis no information.\" The specific reasons and their proportions are as shown in Figure 2.In summary, out of 17391 entities, 5517 entities were unable to be enhanced throughChatGPT. For these entities, we continue to use the original entity representations."}, {"title": "3 Dynamically Integrate Multimodal Information", "content": "To evaluate the effectiveness of our enhanced dataset, we introduce a correspondingbaseline, a method to Dynamically Integrate Multimodal information(DIM). DIM wasexperimented not only on the original Wikimel, Richpedia, and Wikidiverse datasets butalso on the enhanced Wiki+, Rich+, and Diverse+ datasets to assess the effectiveness ofthe enhanced datasets.\nIn detail, our DIM employs CLIP (Radford et al., 2021) for feature encoding andutilizes BLIP-2 (Li et al., 2023) as an expert to extract useful information from images,serving as supplementary information for feature extraction by CLIP. This approachwas designed to enhance the representation and understanding of entities in our experi-ments. The performance comparisons across the original and enhanced datasets aimed"}, {"title": "3.1 Feature Extractor", "content": "Given a sentence $x_t$, mention $x_m$ and Wikipedia description of entity $x_e$, we follow CLIP(Radford et al., 2021) to tokenize it into a sequence of word embeddings. Then the spe-cial tokens startoftext and endoftext are added at the beginning and end positions ofword embeddings. As a result, with $N$ sentences and $N_e$ candidate entities, we feedsentence representation $t \\in \\mathbb{R}^{N \\times d}$, mention representation $m \\in \\mathbb{R}^{N \\times d}$ and entity repre-sentation $e \\in \\mathbb{R}^{N_e \\times d}$ into model. Similarly, we feed image into visual encoder of CLIPto get visual feature $v \\in \\mathbb{R}^{N \\times d}$. $d$ is the hidden size of textual and visual features."}, {"title": "3.2 Expert Supplementary Information", "content": "We employ BLIP-2 as the expert for extracting information from images, employingvarious approaches to process BLIP-2 images: 1) Image Captioning: We extract corre-sponding captions $c_1$ for images, such as \u201cA man and a woman on the red carpet.\" 2)Prompt-based Inquiry: We utilize prompts to ask the detailed information $c_2$ about theimages, with specific prompt designs such as \"Question: Who are the characters in thepicture? Answer: \"\nWe concatenate the image-related information obtained through these two methodsto obtain expert information $c = [CLS]c_1[SEP]c_2$, where '[CLS]' and '[SEP]' are spe-cial characters used to indicate the beginning and separation of text, respectively. $c$ isfed into the text encoder of CLIP to get expert feature $f_c$."}, {"title": "3.3 Architecture", "content": "We combine the extracted image and text features with the supplementary informationprovided by the expert, concatenating them to form the final feature representation.To manage the expression of features, we employ multi-head attention for selection.Through multi-head attention, $c$ will interact separately with text feature $t$ and imagefeature $v$ to extract useful information and control noise.\n$$f_t = softmax(\\frac{(W_q^Qf_c)^T(W_v^Kt)}{\\sqrt{d}})(W^V t)$$\n$$f_v=softmax(\\frac{(W_q^Qf_c)^T(W_v^Kv)}{\\sqrt{d}})(W^V v)$$\nwhere $W^Q \\in \\mathbb{[]}^{d \\times d_q}, W^K \\in \\mathbb{[]}^{d \\times d_k}, W^V \\in \\mathbb{R}^{d \\times d_v}$ are randomly initialized projection ma-trices. We set $d_q = d_k = d_v = d/h$. $h$ is the number of heads of attention layer.\nOn one hand, text information, image information, and expert features are fused toform the fused feature $g$.\n$$g = f_v+f_c+f_t$$\nOn the other hand, the textual representations of $N_e$ candidate entities in the knowl-edge base also encoded by CLIP's text encoder to obtain $e$. During this process, the"}, {"title": "3.4 Training Loss", "content": "We utilize Npairloss (Sohn, 2016) as our optimization training objective to enhance thelearning process. NpairLoss is applied to networks with multiple outputs, where eachoutput corresponds to a specific task. Its purpose is to optimize the network by maxi-mizing the similarity of sample pairs within the same category, thereby enhancing theeffectiveness of multi-task learning. Specifically, for each sample, NpairLoss definesthe loss by comparing the similarity between positive sample pairs (belonging to thesame category) and negative sample pairs (belonging to different categories). It encour-ages the network to make sample pairs within the same category more similar whileensuring that sample pairs from different categories are more dissimilar.\nThe mathematical formulation of NpairLoss $L_{\\gamma}$ is typically expressed as the sumof losses over each sample pair. The similarity scores of positive sample pairs are max-imized, while those of negative sample pairs are minimized. This helps effectively bal-ance the trade-offs between different tasks in multi-task learning, thereby improving thegeneralization performance of network.\n$$L= \\sum_{i=1}^{N}(\\frac{sim(g_i, p_i)}{\\sum_{j=1}^{K}esim(g_i,n_j)}+log\\sum_{j=1}^{K} esim(g_i,n_j)$$\nwhere $g_i$ is the fused feature of the i-th sample. $p_i$ is the representation of the positivesample corresponding to the i-th sample pair. $n_i$ is the representation of the j-th negativesample corresponding to the i-th sample pair. $sim(a,b)$ denotes the similarity measurebetween two representations, typically cosine similarity."}, {"title": "4 Expreriment", "content": "DIM method is not only tested on the original dataset, with results presented in Table3, but also on the dynamically enhanced dataset we introduced in Table 4. For compar-ative analysis, we reproduce several classic baselines on the enhanced dataset, such asBERT (Devlin et al., 2019), GHMFC (P. Wang, 2022) and CLIP (Radford et al., 2021)."}, {"title": "4.1 Dataset", "content": "Our experimental dataset comprises authoritative datasets in the entity linking domain,including Richpedia, WikiMEL, Wikidiverse. We conduct a comprehensive analysis ofthe augmented dataset, and the statistics are presented in Table 2."}, {"title": "4.2 Baseline", "content": "We select several representative methods from the current research community as ourbaseline: (1) BLINK (L. Wu et al., 2019) is a two-step entity linking model based onthe BERT model. (2) BERT (Devlin et al., 2019) is a deep learning model based onthe attention mechanism and Transformer architecture (3) ARNN (Eshel et al., 2017)utilizes the attention-RNN structure to establish the relationship link between entitiesand input information. (4) DZMNED (Moon et al., 2018) focuses on utilizing a multi-modal attention mechanism to analyze information related to mentions in both imagesand text. (5) JMEL (Adjali et al., 2020) leverage fully connected layers to project mul-timodal features into a shared latent space facilitating the representation of features.(6) MEL-HI (Zhang et al., 2021) employs multiple attention mechanisms to focus ondifferent aspects of multimodal information and decrease the effects of noisy images.(7) HieCoAtt (Lu et al., 2016) is a multimodal fusion mechanism, using alternatingco-attention and three textual levels (tokens, phrases, and sentences) to calculate rela-tionship. (8) GHMFC (P. Wang, 2022) takes the gated multimodal fusion and novelattention mechanism to link entities in knowledge base. (9) MMEL (Yang et al., 2023)is a joint feature extraction module to learn the representations of context and entitycandidates, from both the visual and textual perspectives. (10) CLIP-text (Radford etal., 2021) only uses textual information and focuses on the ability to build textual re-lationships between text and entity. (11) CLIP (Radford et al., 2021) take both textualand visual features into consideration. The model concatenates multimodal features andcalculates the similarity between fused features and entities."}, {"title": "4.3 Metrics", "content": "For metric evaluation, we adopted the T@1, 5, 10, and 20 metrics as employed inGHMFC (P. Wang, 2022). These metrics represent the ranking of the similarity scoresfor candidate entities, where the linked entity's similarity score is within the top 1, 5,10, and 20 positions, respectively. Following the previous approach (P. Wang, 2022;Yang et al., 2023), calculations are conducted among 100 candidate entities.\nTo be specific, following the definition in DWE (Song et al., 2023), the formula isas follows:\n$$ACC_{top-k} = \\frac{1}{N} \\sum_{i=1}^{N}\\eta{I(cos(g,g_t), cos(g,C_e)) \\leq k}$$"}, {"title": "4.4 Implement Details", "content": "Following previous work(Zhou, 2021; Wang, 2022; Song et al., 2023), we select 100potential entities as candidates. In the Wiki+, Rich+, and Diverse+ datasets, we utilizefuzzy matching 3 technology to recognize candidate entities that resemble the particularmentions.\nOur experiments are conducted on RTX 3090 using PyTorch 2.0. ChatGPT is basedon the GPT-3.5-turbo version. The version of CLIP employed is Vit-base-patch16-224-in21. The training consisted of 300 epochs, with both image and text hidden layer di-mensions set to 512, and the output layer dimension set to 512. We utilized the AdamWoptimizer with a learning rate of 5e-5, and the batch size was set to 64."}, {"title": "4.5 Experiment on Original Dataset", "content": "We conduct experiments with the DIM method on the datasets before dynamic enhance-ment to validate its effectiveness. As shown in Table 3, the performance of the DIM"}, {"title": "4.6 Experiment on Enhanced Dataset", "content": "In Table 2, to illustrate the distinctions between the enhanced datasets and the originaldatasets, we conducted feature statistics on the datasets and a series of experiments.As shown in Table 4, not only did we experiment with the DIM method on the en-hanced datasets, but for comparison, we also replicated several classic baselines, in-cluding BERT, CLIP, and GHMFC. Our approach outperforms most existing modelson the original dataset without dynamic enhancement, showcasing the effectivenessof our proposed DIM method. Furthermore, on our enhanced dataset(Rich+, Wiki+,and Diverse+), our method continues to demonstrate robust performance, validatingthe effectiveness of our enhancement approach. The improved entity representation byChatGPT aligns more coherently with entities in the knowledge base, achieving bettersemantic consistency."}, {"title": "5 Conclusion", "content": "Our study on multimodal entity linking introduces an impactful solution to key chal-lenges. We leverage ChatGPT's rapid learning to enhance datasets (Wiki+, Rich+, Di-verse+), addressing ambiguous entity representations. Furthermore, the dynamicallyintegrate multimodal information with knowledge base (DIM) method validates ef-ficacy and improves information extraction from images, overcoming existing limi-tations. These innovations contribute to a deeper understanding of human cognitionand knowledge bases, advancing natural language processing and artificial intelligence.Experiments show that our DIM not only outperforms most methods on the originaldataset(Wikimel, Richpedia, Wikidiverse) but also achieves optimal performance onthe newly enhanced dataset(Wiki+, Rich+, Diverse+).\nThe dataset we collect relies on ChatGPT's understanding of the knowledge baseand the world. Although this allows for dynamic entity information linking, it can lead"}]}