{"title": "DIM: Dynamic Integration of Multimodal Entity Linking with Large Language Model", "authors": ["Shezheng Song", "Shasha Li", "Jie Yu", "Shan Zhao", "Xiaopeng Li", "Jun Ma", "Xiaodong Liu", "Zhuo Li", "Xiaoguang Mao"], "abstract": "Our study delves into Multimodal Entity Linking, aligning the mention in multimodal information with entities in knowledge base. Existing methods are still facing challenges like ambiguous entity representations and limited image information utilization. Thus, we propose dynamic entity extraction using ChatGPT, which dynamically extracts entities and enhances datasets. We also propose a method: Dynamically Integrate Multimodal information with knowledge base (DIM), employing the capability of the Large Language Model (LLM) for visual understanding. The LLM, such as BLIP-2, extracts information relevant to entities in the image, which can facilitate improved extraction of entity features and linking them with the dynamic entity representations provided by ChatGPT. The experiments demonstrate that our proposed DIM method outperforms the majority of existing methods on the three original datasets, and achieves state-of-the-art (SOTA) on the dynamically enhanced datasets (Wiki+, Rich+, Diverse+). For reproducibility, our code and collected datasets are released on https://github.com/season1blue/DIM.", "sections": [{"title": "1 Introduction", "content": "Multimodal entity linking (Koml\u00f3si & Waldbuesser, 2015) involves linking mentioned entity (i.e. mention) in natural language texts to their corresponding entity (Zhao, Hu, Cai, & Liu, 2021) in a knowledge base (Fu et al., 2020). These entities could be individuals, locations, organizations in the real world, or specific entities within a knowledge base. MEL could help computers better understand user semantics, integrate various information sources, resolve ambiguities, and enhance user experience. It plays an important role in search engines (P. Wu & Xie, 2024), recommendation systems (Zhou, 2023), information retrieval(Ma, Zhou, et al., 2023), and knowledge graph construction (Ma, Chen, et al., 2023; Zhao, Hu, Cai, & Liu, 2021). Through MEL, systems can provide more relevant search results, more personalized recommendations, more comprehensive information retrieval, richer knowledge graphs, and smarter conversation and text processing capabilities."}, {"title": "2 Dynamic Building Entity Representation", "content": "Multimodal entity linking(MEL) significantly influences and facilitates a profound understanding and cognition of information for humans. MEL serves as a crucial means to unify human cognition with structured knowledge repositories: (1) Assurance of semantic consistency: MEL ensures semantic consistency by aligning entities mentioned in the cognition with knowledge base. It helps in eliminating ambiguity and ensuring that the interpretation of specific entities remains clear despite contextual variations. (2) Enhancement of cognitive information: MEL offers individuals a richer and deeper cognitive experience. By associating entities with background knowledge, individuals can comprehensively grasp the meaning of entities, thereby elevating their cognitive awareness of information. (3) Integrated knowledge acquisition: This contributes to breaking down information silos, enabling people to easily cross different domains, texts, and knowledge sources to acquire information, promoting an overall improvement in cognitive levels."}, {"title": "2.1 Existing Entity Representation", "content": "In our investigation and study, we analyze the existing Entity Linking datasets and their methods of entity representation:\nWikimel and Richpedia (Zhou, 2021) employ concise attributes from Wikidata. This representation lacks representativeness for entities, as many entities share similar attributes. It is easy to link accurate cognition to the wrong entity incorrectly.\nWikiperson (Sun, 2022), similar to Wikimel, uses attributes of individuals as representatives, but in a more simplified manner. In this example, the attribute \"American President\" is inadequate as a representative of Joe Biden, given that there have been multiple American Presidents.\nWeibo (Zhang, 2021) utilizes individuals from Weibo as entities, using user-authored personal bios as entity representations. These bios, relying on user-generated content, may contain biases or errors and do not accurately reflect the broader public's understanding of the entity.\nWikidiverse (Wang, 2022) uses images collected from Wikipedia as entity representation. However, images can deviate from a person's true appearance due to factors like angles and time, lacking real-time accuracy.\nIn summary, existing multimodal entity linking methods suffer from the limitation that entity representation fails to effectively represent entities. More importantly, these representations are manually collected from Wikipedia or other knowledge bases and can only represent the entity's state at a specific time. Human understanding of entities changes over time and events. For instance, Donald Trump is no longer the President of the United States in 2023. In such cases, rigid and less adaptable entity representations can lead to errors. Additionally, when a mentioned entity is not in the dataset, there is no corresponding entity representation, causing potential issues in entity linking."}, {"title": "2.2 Leverage Chatgpt to Dynamic Connect", "content": "ChatGPT (OpenAI, 2023) is a powerful large model trained on massive amounts of web data and is continuously updated over time. We plan to utilize the interface provided by ChatGPT to inquire about entities, so as to subsequently link with the entities in the implicit knowledge base of ChatGPT. Candidate entities are input into ChatGPT for inquiries using the prompt: \u201cYou are a helpful assistant designed to give a comprehensive introduction about people. Who is this one?\u201d The generated response from ChatGPT is shown in Table 1. For experimental convenience and dataset quality enhancement, we collect the response of ChatGPT to enhance the dataset. In detail, we construct entity representations for 17391, 17804, and 57007 entities from Wikimel, Richpedia, Wikidiverse, respectively. The newly built entity representations better reflect the general public's understanding of entities, align closely with their inherent semantics, and facilitate a unified approach to cognition and knowledge base.\nTaking the Wikimel (Zhou, 2021) as an example, out of the 17474 entities collected from the dataset, 131 entities did not return any results, and 220 inquiries returned \"Sorry, I cannot provide an introduction to this entity.\" Besides, ChatGPT provides speculative information based on cultural, regional, or other contextual cues for 462 entities. For example, \"John Abbott is a common English given name and surname,\" but did not provide specific representation. Additionally, 2997 entities require additional information for verification. For instance, \u201cIt is possible that Edward J. Livernash is a private individual without any notable achievements.\u201d Furthermore, 599 entities are speculated to be fictional names, such as \u201cJohn McDuffie is a fictional name, so there is no information.\" The specific reasons and their proportions are as shown in Figure 2. In summary, out of 17391 entities, 5517 entities were unable to be enhanced through ChatGPT. For these entities, we continue to use the original entity representations."}, {"title": "3 Dynamically Integrate Multimodal Information", "content": "To evaluate the effectiveness of our enhanced dataset, we introduce a corresponding baseline, a method to Dynamically Integrate Multimodal information(DIM). DIM was experimented not only on the original Wikimel, Richpedia, and Wikidiverse datasets but also on the enhanced Wiki+, Rich+, and Diverse+ datasets to assess the effectiveness of the enhanced datasets.\nIn detail, our DIM employs CLIP (Radford et al., 2021) for feature encoding and utilizes BLIP-2 (Li et al., 2023) as an expert to extract useful information from images, serving as supplementary information for feature extraction by CLIP. This approach was designed to enhance the representation and understanding of entities in our experiments. The performance comparisons across the original and enhanced datasets aimed to highlight the impact of our dataset augmentation on the efficacy of the proposed DIM model."}, {"title": "3.1 Feature Extractor", "content": "Given a sentence $x_t$, mention $x_m$ and Wikipedia description of entity $x_e$, we follow CLIP (Radford et al., 2021) to tokenize it into a sequence of word embeddings. Then the special tokens startoftext and endoftext are added at the beginning and end positions of word embeddings. As a result, with N sentences and $N_e$ candidate entities, we feed sentence representation $t \\in \\mathbb{R}^{N \\times d}$, mention representation $m \\in \\mathbb{R}^{N \\times d}$ and entity representation $e \\in \\mathbb{R}^{N_e \\times d}$ into model. Similarly, we feed image into visual encoder of CLIP to get visual feature $v \\in \\mathbb{R}^{N \\times d}$. d is the hidden size of textual and visual features."}, {"title": "3.2 Expert Supplementary Information", "content": "We employ BLIP-2 as the expert for extracting information from images, employing various approaches to process BLIP-2 images: 1) Image Captioning: We extract corresponding captions $c_1$ for images, such as \u201cA man and a woman on the red carpet.\" 2) Prompt-based Inquiry: We utilize prompts to ask the detailed information $c_2$ about the images, with specific prompt designs such as \"Question: Who are the characters in the picture? Answer: \"\nWe concatenate the image-related information obtained through these two methods to obtain expert information $c = [CLS]c_1[SEP]c_2$, where '[CLS]' and '[SEP]' are special characters used to indicate the beginning and separation of text, respectively. c is fed into the text encoder of CLIP to get expert feature $f_c$."}, {"title": "3.3 Architecture", "content": "We combine the extracted image and text features with the supplementary information provided by the expert, concatenating them to form the final feature representation. To manage the expression of features, we employ multi-head attention for selection. Through multi-head attention, c will interact separately with text feature t and image feature v to extract useful information and control noise.\n$f_t = softmax(\\frac{(W^Qf_c)^T(W^Kt)}{\\sqrt{d}})(W^V t)$ (1)\n$f_v=softmax(\\frac{(W^Qf_c)^T(W^Kv)}{\\sqrt{d}})(W^V v)$ (2)\nwhere $W^Q \\in []^{\\mathbb{R}^{d \\times d_q}}, W^K \\in []^{\\mathbb{R}^{d \\times d_k}}, W^V \\in \\mathbb{R}^{d \\times d_v}$ are randomly initialized projection matrices. We set $d_q = d_k = d_v = d/h$. h is the number of heads of attention layer.\nOn one hand, text information, image information, and expert features are fused to form the fused feature g.\n$g = f_v+f_c+f_t$ (3)\nOn the other hand, the textual representations of $N_e$ candidate entities in the knowledge base also encoded by CLIP's text encoder to obtain e. During this process, the"}, {"title": "3.4 Training Loss", "content": "We utilize Npairloss (Sohn, 2016) as our optimization training objective to enhance the learning process. NpairLoss is applied to networks with multiple outputs, where each output corresponds to a specific task. Its purpose is to optimize the network by maximizing the similarity of sample pairs within the same category, thereby enhancing the effectiveness of multi-task learning. Specifically, for each sample, NpairLoss defines the loss by comparing the similarity between positive sample pairs (belonging to the same category) and negative sample pairs (belonging to different categories). It encourages the network to make sample pairs within the same category more similar while ensuring that sample pairs from different categories are more dissimilar.\nThe mathematical formulation of NpairLoss $\\mathcal{L}_{y}$ is typically expressed as the sum of losses over each sample pair. The similarity scores of positive sample pairs are maximized, while those of negative sample pairs are minimized. This helps effectively balance the trade-offs between different tasks in multi-task learning, thereby improving the generalization performance of network.\n$\\mathcal{L}=\\sum_{i=1}^{N}\\left[-\\frac{s i m\\left(g_{i}, p_{i}\\right)}{\\sum_{j=1}^{K} e^{s i m\\left(g_{i}, n_{j}\\right)}}+\\log \\left(\\sum_{j=1}^{K} e^{s i m\\left(g_{i}, n_{j}\\right)}\\right)\\right]$ (4)\nwhere $g_i$ is the fused feature of the i-th sample. $p_i$ is the representation of the positive sample corresponding to the i-th sample pair. $n_j$ is the representation of the j-th negative sample corresponding to the i-th sample pair. $sim(a,b)$ denotes the similarity measure between two representations, typically cosine similarity."}, {"title": "4 Expreriment", "content": "DIM method is not only tested on the original dataset, with results presented in Table 3, but also on the dynamically enhanced dataset we introduced in Table 4. For comparative analysis, we reproduce several classic baselines on the enhanced dataset, such as BERT (Devlin et al., 2019), GHMFC (P. Wang, 2022) and CLIP (Radford et al., 2021)."}, {"title": "4.1 Dataset", "content": "Our experimental dataset comprises authoritative datasets in the entity linking domain, including Richpedia, WikiMEL, Wikidiverse. We conduct a comprehensive analysis of the augmented dataset, and the statistics are presented in Table 2."}, {"title": "4.2 Baseline", "content": "We select several representative methods from the current research community as our baseline: (1) BLINK (L. Wu et al., 2019) is a two-step entity linking model based on the BERT model. (2) BERT (Devlin et al., 2019) is a deep learning model based on the attention mechanism and Transformer architecture (3) ARNN (Eshel et al., 2017) utilizes the attention-RNN structure to establish the relationship link between entities and input information. (4) DZMNED (Moon et al., 2018) focuses on utilizing a multimodal attention mechanism to analyze information related to mentions in both images and text. (5) JMEL (Adjali et al., 2020) leverage fully connected layers to project multimodal features into a shared latent space facilitating the representation of features. (6) MEL-HI (Zhang et al., 2021) employs multiple attention mechanisms to focus on different aspects of multimodal information and decrease the effects of noisy images. (7) HieCoAtt (Lu et al., 2016) is a multimodal fusion mechanism, using alternating co-attention and three textual levels (tokens, phrases, and sentences) to calculate relationship. (8) GHMFC (P. Wang, 2022) takes the gated multimodal fusion and novel attention mechanism to link entities in knowledge base. (9) MMEL (Yang et al., 2023) is a joint feature extraction module to learn the representations of context and entity candidates, from both the visual and textual perspectives. (10) CLIP-text (Radford et al., 2021) only uses textual information and focuses on the ability to build textual relationships between text and entity. (11) CLIP (Radford et al., 2021) take both textual and visual features into consideration. The model concatenates multimodal features and calculates the similarity between fused features and entities."}, {"title": "4.3 Metrics", "content": "For metric evaluation, we adopted the T@1, 5, 10, and 20 metrics as employed in GHMFC (P. Wang, 2022). These metrics represent the ranking of the similarity scores for candidate entities, where the linked entity's similarity score is within the top 1, 5, 10, and 20 positions, respectively. Following the previous approach (P. Wang, 2022; Yang et al., 2023), calculations are conducted among 100 candidate entities.\nTo be specific, following the definition in DWE (Song et al., 2023), the formula is as follows:\n$A C C_{t o p-k} = \\frac{1}{N} \\sum_{i=1}^{N} \\eta{\\{I(\\cos(g,g_t),\\cos(g,C_e)) \\leq k\\}}$ (5)"}, {"title": "4.4 Implement Details", "content": "Following previous work(Zhou, 2021; Wang, 2022; Song et al., 2023), we select 100 potential entities as candidates. In the Wiki+, Rich+, and Diverse+ datasets, we utilize fuzzy matching 3 technology to recognize candidate entities that resemble the particular mentions.\nOur experiments are conducted on RTX 3090 using PyTorch 2.0. ChatGPT is based on the GPT-3.5-turbo version. The version of CLIP employed is Vit-base-patch16-224-in21. The training consisted of 300 epochs, with both image and text hidden layer dimensions set to 512, and the output layer dimension set to 512. We utilized the AdamW optimizer with a learning rate of 5e-5, and the batch size was set to 64."}, {"title": "4.5 Experiment on Original Dataset", "content": "We conduct experiments with the DIM method on the datasets before dynamic enhancement to validate its effectiveness. As shown in Table 3, the performance of the DIM"}, {"title": "4.6 Experiment on Enhanced Dataset", "content": "In Table 2, to illustrate the distinctions between the enhanced datasets and the original datasets, we conducted feature statistics on the datasets and a series of experiments. As shown in Table 4, not only did we experiment with the DIM method on the enhanced datasets, but for comparison, we also replicated several classic baselines, including BERT, CLIP, and GHMFC. Our approach outperforms most existing models on the original dataset without dynamic enhancement, showcasing the effectiveness of our proposed DIM method. Furthermore, on our enhanced dataset(Rich+, Wiki+, and Diverse+), our method continues to demonstrate robust performance, validating the effectiveness of our enhancement approach. The improved entity representation by ChatGPT aligns more coherently with entities in the knowledge base, achieving better semantic consistency."}, {"title": "5 Conclusion", "content": "Our study on multimodal entity linking introduces an impactful solution to key challenges. We leverage ChatGPT's rapid learning to enhance datasets (Wiki+, Rich+, Diverse+), addressing ambiguous entity representations. Furthermore, the dynamically integrate multimodal information with knowledge base (DIM) method validates efficacy and improves information extraction from images, overcoming existing limitations. These innovations contribute to a deeper understanding of human cognition and knowledge bases, advancing natural language processing and artificial intelligence. Experiments show that our DIM not only outperforms most methods on the original dataset(Wikimel, Richpedia, Wikidiverse) but also achieves optimal performance on the newly enhanced dataset(Wiki+, Rich+, Diverse+).\nThe dataset we collect relies on ChatGPT's understanding of the knowledge base and the world. Although this allows for dynamic entity information linking, it can lead to biases or omissions in data collection due to ChatGPT's potential hallucinations or unavailability. We will continue to explore and refine methods for entity data collection based on large models to enhance accuracy and completeness."}]}