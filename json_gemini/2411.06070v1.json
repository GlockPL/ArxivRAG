{"title": "GFT: Graph Foundation Model with Transferable Tree Vocabulary", "authors": ["Zehong Wang", "Zheyuan Zhang", "Nitesh V Chawla", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there haven't been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees \u2013 i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.", "sections": [{"title": "1 Introduction", "content": "Foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs) keep reshaping our view of the world [7, 100, 51, 112, 50]. These models are designed to be general-purpose, adaptable across various tasks and domains through fine-tuning or prompting, such as GPT-4 [1] in Natural Language Processing (NLP) and Sora [46] in Computer Vision (CV). Research attributes the success of foundation models to the uniformity of tasks and a general vocabulary that defines basic transferable patterns across tasks [98, 76, 112, 3, 50]. For example, LLMs [1, 112] treat language tasks as question-answering or next-word prediction and deconstruct sentences using a word vocabulary. Similarly, LVMs [100, 98, 3] reformulate image tasks as image question-answering, converting images into discrete tokens using a vision vocabulary. Inspired by the success of LLMs"}, {"title": "2 Rethinking Transferable Patterns on Graphs", "content": "Transferability refers to a model's capability to extract patterns from source tasks and apply this knowledge to enhance performance on related target tasks [5, 33, 90]. Understanding transferable"}, {"title": "2.1 Transferability on GNNS", "content": "patterns is essential for developing graph foundation models. Early research focuses on analyzing transferability through the perspectives of graph spectrum [41, 42] and subgraphs/substructures [114], defining transferability as model invariance to minor permutations on the graph. A more recent study [50] investigates the transferable vocabulary in graphs by identifying key substructures relevant to various tasks. For instance, they find that triadic closure, homophily, and heterophily are vital for node classification; local and global structural proximities are crucial for link prediction; and certain motifs [103], such as triangles, k-cliques, and stars, serve as fundamental components for graph classification. Another line of research [62, 8, 69] incorporates graphon theory to provide a theoretical basis for transferability. Specifically, Ruiz et al. [62] establish a bound on the embeddings of two graphs sampled from the same graphon. Cao et al. [8] expand this to include pre-training and fine-tuning scenarios, assessing the distance between graphs based on their alignment within the graphon space. However, the stringent assumptions of graphon theory limit its practical application in the design of graph foundation models.\nWe identify two primary limitations in analyzing transferable patterns on graphs: (1) While certain domains [110, 87, 43, 108] or tasks [114, 111, 19] exhibit transferable patterns, the challenge of identifying universally transferable substructures is difficult. (2) More critically, basic message-passing GNNs, constrained by the 1-WL test [94, 52], fail to recognize certain subgraphs (or motifs) [20, 10, 103], such as stars, conjoint cycles, and k-cliques, as well as heterophily patterns [113]. This limitation in recognizing substructures impedes using subgraphs as transferable tokens in graph vocabulary [68, 30, 45]. More related works are elaborated in Appendix A."}, {"title": "2.2 Computation Tree as Transferable Pattern", "content": "In this paper, we rethink the transferable pattern in graphs as the computation tree a specialized subtree pattern that emerges from unfolding the message-passing process [12]. This pattern is demonstrably effective at capturing critical localized information within the graph [20, 52, 94, 12]. Treating computation trees as tokens within a graph vocabulary offers two distinct advantages: (1) computation trees preserve the essential structural information of the graph, which is learnable through message-passing GNNs, and (2) the computation tree structure occurs across various graph-based tasks. These tasks can be unified as computation tree classification by integrating a virtual node, as shown in Figure 1.\nBefore diving into transferability analysis, we first establish the necessary notations. Consider a graph $G = (V, E)$ composed of node set $V$ and edge set $E$. Each node $v \\in V$ is associated with a feature vector $x \\in \\mathbb{R}^d$ and a computation tree $T$ with $L$ layers. A GNN encoder $\\phi$ processes these computation trees as inputs, producing embeddings for root nodes $z = \\phi(T_v) \\in \\mathbb{R}^{d'}$.\nDefinition 2.1 (Computation Trees [12]). Given a graph $G = (V,E)$, define $T_v^0 = v$ and $T_v^L$ as the $L$-layer computation tree. This tree is constructed by recursively integrating the subtrees of neighborhoods. The multiset of $L$-layer computation trees on graph $G$ is denoted by $\\mathbb{T}_G := \\{T_v^L\\}_{v \\in V}$.\nFigure 1 demonstrates the construction of computation trees across various graph tasks, including node-, link-, and graph-level tasks. These trees capture essential localized subtree patterns within the graphs [55, 64, 12]. If the $L$-layer computation trees for two nodes are similar, it indicates that these nodes share similar neighborhoods, suggesting they represent analogous phenomena [42]. Thus, it is rational to assess transferability of computation trees by examining the stability of GNNs in producing analogous embeddings for similar trees [62, 42].\nTheorem 2.2 (Transferability of Computation Tree). Given two $L$-layer computation trees $T_{v_1}, T_{v_2}$ derived from the graph $G$ and a GNN encoder $\\phi$, the Euclidean distance between the tree embeddings $\\triangle \\leq ||\\phi(T_{v_1}) - \\phi(T_{v_2})||_2$ is bounded as follows:\n$\\triangle \\leq C_1||x_{v_1} - x_{v_2}||_2 + C_2 \\sum_{l=1}^{L} \\underset{j \\in N(v)}{\\triangle^{l-1}_{v_1,v_2,j}} \\leq 2B_x\\frac{C_1 + C_2D_1}{1-C_2d} \\leq 2B_x(C_1 + C_2D_1)$ (1)\nwhere $\\underset{v_1,v_2,j}{\\triangle^{l-1}}$ represents the distance between the $(L - 1)$-layer subtrees of the $j$-th children of nodes $v_1$ and $v_2$, $C_1, C_2$ are constants, and $B_x$ denote bounded norm of $x$. The variable $d_l$ indicates the number of children in the $l$-layer subtrees, and $D_l = d_ld_{l-1}...d_1$."}, {"title": "3 GFT: Graph Foundation Model with Transferable Tree Vocabulary", "content": "We develop GFT, a cross-domain and cross-task graph foundation model that leverages computation trees as transferable patterns within graph vocabulary. As illustrated in Figure 4, GFT undergoes"}, {"title": "3.1 Pre-training with Computation Tree Reconstruction", "content": "The pre-training stage focuses on learning general computation tree patterns on graphs, facing two primary challenges: (i) obtaining transferable patterns, and (ii) comprehensively capturing computation tree knowledge. For the first challenge, we learn a discrete tree vocabulary by quantizing the embedding space of computation trees [77]. For the second challenge, we introduce a computation tree reconstruction task that considers multiple aspects of computation trees.\nLearning Tree Vocabulary. The idea of learning a discrete computation tree vocabulary originates from the principles of sparse distributed memory in cognitive science [37, 38], which stores and retrieves memory in a distributed manner. By adopting these principles, the tree vocabulary maintains a set of tokens that are reusable and adaptable across various tasks, improving model transferability.\nWe adopt the Vector Quantization (VQ) [77] to develop the tree vocabulary. Given a graph database$^2$ $\\mathbb{D} = \\{G_i\\}_{i=1}^n$, we randomly extract a set of computation trees $\\mathbb{T} = \\{T_i\\}_{i=1}^m$ and employ a GNN encoder $\\phi$ to generate the tree embeddings $\\mathbb{Z} = \\{z_i\\}_{i=1}^m$. We define the computation tree vocabulary as a set of learnable tokens $\\mathbb{C} = \\{c_1, ..., c_k\\}$. The tree embedding space is quantized by assigning each embedding to the nearest token, resulting in quantized tree embeddings $q_i = c_j$, where $j = \\text{arg} \\underset{i}{\\text{min}}||z_i - c_j||_2$. We optimize this projection by back-propagating the reconstruction error to the tree vocabulary $\\mathbb{C}$ and applying a straight-through gradient estimator [6] to the encoder $\\phi$. In particular, we jointly optimize vocabulary loss and commitment loss [77], along with tree reconstruction loss (discussed later), where the former updates the token vectors $c_i$ using the fixed quantization $q_i$, and the latter ensures alignment between the tokens in the vocabulary and the quantized tree embeddings, serving as a regularizer. The pre-training objective is thus defined as:\n$\\mathcal{L}_{pretrain} = \\mathcal{L}_{tree} + \\frac{1}{m} \\sum_{i=1}^{m} || \\text{sg}[z_i] - c_i||_2 + \\beta_1 \\frac{1}{m} \\sum_{i=1}^{m} || z_i - \\text{sg}[c_i]||_2$\n (2)\nwhere sg[.] denotes the stop-gradient operator and $\\beta_1$ is the weight."}, {"title": "Computation Tree Reconstruction", "content": "We introduce a computation tree reconstruction task designed to enable a deep understanding of the structural and semantical attributes of computation trees [36]. We use the tree tokens to reconstruct the original computation tree, retaining general knowledge while discarding irrelevant details. Specifically, we develop three reconstruction tasks: (i) reconstructing the features of the root node $\\mathcal{L}_{feat}$, (ii) reconstructing the connectivity among nodes in the computation trees $\\mathcal{L}_{topo}$, and (iii) reconstructing the overall semantics of the computation trees $\\mathcal{L}_{sem}$:\n$\\mathcal{L}_{feat} = \\frac{1}{m} \\sum_{i=1}^{m} || \\hat{x}_i - x_i||_2, \\hspace{1cm} \\mathcal{L}_{sem} = \\frac{1}{m} \\sum_{i=1}^{m} || 1 - \\frac{(\\hat{z}_i^Tz_i)}{\\| \\hat{z}_i\\|\\|z_i\\|} ||_2,$ (3)\n$\\mathcal{L}_{topo} = \\frac{1}{\\|E\\|| + ||\\hat{E}\\|} \\sum_{(i,j) \\in E} \\text{log}(\\sigma(\\hat{q}_i^T \\hat{q}_j)) + \\frac{1}{\\|E\\|| + ||\\hat{E}\\|} \\sum_{(i,j') \\in \\hat{E}} \\text{log} (1 - \\sigma(\\hat{q}_i^T \\hat{q}_{j'}))$,\nwhere $\\hat{z}_i = \\phi(T_i)$ represents the semantics of the original computation trees, and $\\Omega$ is updated through a moving average of the tree encoder $\\phi$. The quantized tree embedding $q$ is projected via different decoders defined by MLP, $\\hat{x}_q = d_x(q), \\hat{z}_q = d_q(q)$, $\\gamma$ is the scaling factor, and $E$ and $\\hat{E}$ represent sets of existing and non-existing connections in computation trees, respectively. $e_{ij}$ denotes the edge embedding between nodes $i$ and $j$. By jointly optimizing these tasks, we establish a comprehensive reconstruction objective:\n$\\mathcal{L}_{tree} = \\beta_2\\mathcal{L}_{feat} + \\beta_3 \\mathcal{L}_{sem} + \\beta_4 \\mathcal{L}_{topo},$ (4)\nwhere $\\beta_i$ indicates the weights of respective losses. The philosophies under these loss functions separately correspond to existing works [39, 26, 74, 86]. For example, Kipf and Welling [39] reconstruct the graph structure, aligning to the philosophy of $\\mathcal{L}_{topo}$, Hou et al. [26] reconstruct node feature that is similar to $\\mathcal{L}_{feat}$, and Thakoor et al. [74], Wang et al. [86] employ contrastive learning to maximize the alignment between two views, aligning to $\\mathcal{L}_{sem}$. Unlike existing methods that typically focus on reconstructing a single aspect of computation trees, GFT integrates multiple facets [85] to learn a general and transferable tree vocabulary.\nEnhancing the Quality of Tree Vocabulary. The effectiveness of GFT is correlated to the quality of the tree vocabulary, which should be both comprehensive and expressive. A comprehensive vocabulary is inclusive enough to accommodate new patterns, while an expressive vocabulary ensures that different tree tokens do not overlap in representation [50]. To enhance comprehensiveness, we augment the computation trees during pre-training, increasing the variety of observed computation trees through node feature augmentation and structural augmentation, as described by [115]. To improve expressiveness, we regularize the tree vocabulary space by intentionally increasing the distance between distinct tokens [65]. Specifically, we introduce an orthogonal regularizer designed to maintain tree tokens orthogonal to each other, effectively expanding the tree token space:\n$\\mathcal{L}_{ortho} = \\frac{\\lambda}{K^2} ||\\mathbb{C}\\mathbb{C}^T - \\mathbb{I}_K||_F^2, \\hspace{0.5cm} \\mathbb{C} = [c_1, ..., c_K]^T \\in \\mathbb{R}^{K \\times d'},$ (5)\nwhere $c_i$ is tree token, $I_K$ is the identity matrix for $K$ dimensions, and $||\\cdot||_F$ denotes the Frobenius norm. The orthogonal loss $\\mathcal{L}_{ortho}$ is integrated with Equation 2. More analysis is in Appendix C.2."}, {"title": "3.2 Fine-tuning with Computation Tree Classification", "content": "The pre-training stage encodes general knowledge into the tree vocabulary, while the fine-tuning phase adapts this knowledge to specific tasks. This adaptation is challenging because identical patterns can have different meanings across domains and tasks [8]. For example, a triangular structure indicates stable relationships in social networks (node classification) but denotes unstable chemical properties in molecular networks (graph classification). To this end, we propose computation tree classification that utilizes the tree vocabulary to unify graph tasks as the tree-level task, ensuring the adaptation is applicable across diverse tasks and domains.\nReformulate Graph Tasks as Computation Tree Classification. Graph-related tasks can be represented by task-specific computation trees, as illustrated in Figure 1. Specifically, for node classification, the task-specific computation tree, denoted as $T_{node} = T_i$, is derived directly from the node itself, resulting in the embedding $z = \\phi(T_i)$. For link prediction, the computation tree, $T_{link} = Combine(T_s, T_t)$, merges the computation trees of two nodes of the edge, with the embedding"}, {"title": "Prototype Classifier", "content": "$z = \\text{mean}(\\phi(T_s), \\phi(T_t))$. For graph classification, the task-specific computation tree $T_{graph} = Combine(\\{T_v\\}_{v \\in V})$ integrates the computation trees of all nodes within the graph, and computes the embedding as $z = \\text{mean}(\\{\\phi(T_v)\\}_{v \\in V})$. Subsequently, the embeddings of these task-specific trees are used to query the tree vocabulary and then make predictions, adapting the general knowledge encoded in the vocabulary to various tasks and domains.\nThe prototype classifier $f_{proto}$ constructs class prototypes using tokens from the tree vocabulary. Given a set of task-specific computation trees $\\{(T_i, y_i)\\}_{i=1}^n$ with $|\\mathbb{C}|$ classes, we employ the pre-trained GNN encoder $\\phi$ to generate tree embeddings $\\mathbb{Z} = \\{z_i\\}_{i=1}^n$. These embeddings are then used to query the tree vocabulary and produce quantized embeddings $\\mathbb{Q} = \\{q_i\\}_{i=1}^n$. Subsequently, we construct a class-wise memory bank $\\mathbb{S} = \\{\\mathbb{S}_1,...,\\mathbb{S}_{|\\mathbb{C}|}\\}$, where $\\mathbb{S}_k = \\{q_i \\in \\mathbb{Q} | y_i = k\\}$, to store tree tokens of the same class. The memory bank typically includes all instances from the training set. From this, we derive a set of prototypes for each class $\\{p_k\\}_{k=1}^{|\\mathbb{C}|}$, calculated as $p_k = (1/|\\mathbb{S}_k|) \\sum_{q \\in \\mathbb{S}_k} q_i$. These prototypes are then used for predictions:\n$p(y = k|z) = \\frac{\\text{exp}(-\\text{sim}(z, p_k)/\\tau)}{\\sum_{c}^{\\mathbb{C}} \\text{exp}(-\\text{sim}(z, p_c)/\\tau)},$ (6)\nwhere sim(.) denotes the cosine distance and $\\tau$ is a temperature scaling factor. We optimize the cross-entropy loss between the classifier's output and the ground truth to update the encoder $\\phi$."}, {"title": "Linear Classifier", "content": "Different from the prototype classifier, which utilizes class prototypes to adapt to target tasks, the linear classifier $f_{lin}$ directly applies the knowledge encoded in each tree token. Specifically, given a task-specific computation tree $T_i$, we use the encoder to generate tree embeddings $z_i$ and then query the tree vocabulary to retrieve $q_i$. These embeddings are used for predictions as:\n$p(y = k|z) = \\frac{\\text{exp}(l_{in}(q_i)/\\tau)}{\\sum_{c}^{\\mathbb{C}} \\text{exp}(l_{in}(q_i)/\\tau)},$ (7)\nWe optimize the cross-entropy loss between the prediction $f_{lin}(z)$ and the ground truth to update the parameters of the encoder and the linear classifier. During inference, predictions from both the prototype and linear classifiers are combined to form the final output. It is important to note that the tree vocabulary remains fixed during fine-tuning to preserve the integrity of the encoded knowledge."}, {"title": "3.3 Additional Analysis", "content": "Tree Vocabulary Learns Generalizable Tokens. Learning tree vocabulary via VQ involves clustering within the embedding space of computation trees, utilizing a margin-aware classifier [14] that assigns each computation tree to a specific cluster. Assuming that each computation tree $T$ is associated with an underlying clustering label $y$, and that each pair $(T_i, y_i)$ is sampled from the distribution $P_T$, we derive the following theorem:\nTheorem 3.1. Given a set of computation trees $\\{(T_i, y_i)\\}_{i=1}^n$ sampled from the distribution $P_T$, the VQ process functions as a margin-aware prototype classifier $f$ that predicts the class of computation trees via a distance measure. The risk $\\mathcal{R}(f)$ of classifier $f$ can be bounded with probability $1 - \\delta$:\n$\\mathcal{R}(f) \\leq \\hat{\\mathcal{R}}(f) + \\frac{20 \\cdot C \\cdot \\rho( \\rho - 1) \\cdot \\sqrt{n}}{n \\rho} + \\sqrt{\\frac{\\text{ln}(2/\\delta)}{2n}},$ (8)\nwhere $\\hat{\\mathcal{R}}(f)$ is the empirical risk, $\\rho$ denotes the number of tokens, $C$ is a constant, and $\\rho$ acts as the margin, serving as a penalty factor in evaluating the distance between computation trees and tokens.\nRemark 3.2. The generalizability of tokens within the vocabulary highly correlates to the margin $\\rho$, the number of observed computation trees $n$, and the number of tokens $\\rho$. (i) A larger margin $\\rho$ results in a tighter bound by ensuring higher inter-cluster distances and lower intra-cluster distances. This supports the use of an orthogonal regularizer (Equation 5) that explicitly pushes tokens apart, enhancing cluster distinction. (ii) An increased number of observed computation trees $n$ leads to a tighter generalization bound, which supports the use of augmentations to increase the diversity of computation trees. (iii) More tokens $\\rho$ may loose the upper bound of the generalization error, potentially due to a higher risk of overfitting. This aligns with our experimental findings that more tokens do not necessarily lead to improved performance (Section 4.4)."}, {"title": "Complexity Analysis", "content": "A comprehensive complexity analysis of GFT is provided in Appendix B. Notably, GFT employs a single GNN to decompose and encode computation trees, taking $O(L \\cdot (|E| \\cdot d + |V| \\cdot d^2))$. In contrast, subgraph-based GFMs [30, 45] require the explicit extraction of subgraphs for each node, taking additional $O(|V|^3)$ using adjacency matrix-based BFS. This contrast highlights the efficiency of using computation trees as transferable patterns in terms of time complexity. More discussions are in Appendix C."}, {"title": "Tree Vocabulary Mitigates Negative Transfer", "content": "Negative Transfer (NT) occurs when the pre-training process degrades model performance on a target task. This issue often results from misalignment between the pre-training and fine-tuning tasks [89, 8]. Following the approach in [89], we characterize the NT gap, $\\mathcal{R}(S,T) \u2013 \\mathcal{R}(\\emptyset, T)$, as the risk gap on task T with ($\\mathcal{R}(S,T)$) and without ($\\mathcal{R}(\\emptyset), T$)) pre-training on task S, where a smaller NT gap indicates improved transferability. As illustrated in Figure 5, employing the learned tree vocabulary to align the tree reconstruction task in pre-training and tree classification task in fine-tuning can significantly mitigate negative transfer."}, {"title": "4 Experiments", "content": "We employ cross-domain and cross-task graph datasets to evaluate the effectiveness of GFT. For node-level tasks, we utilize citation networks such as Cora, PubMed, Arxiv, and the web link network WikiCS. For edge-level tasks, we include two Knowledge Graphs (KGs), WN18RR and FB15K237. For graph-level tasks, we use molecule networks, including HIV, PCBA, and ChEMBL. All preprocessing steps follow [45]. We take various baselines, encompassing MLP, supervised GNNs such as GCN [40], GAT [78], GIN [94], and self-supervised methods like BGRL [74], GraphMAE [26], GIANT [11], and GFMs including Prodigy [30] and OFA [45]. We replicate each experiment ten times and report the average performance to minimize the influence of randomness. Further details on experimental settings are available in Appendix F."}, {"title": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets", "content": "Pre-training and Fine-tuning. Table 2 demonstrates the model performance across cross-domain and cross-task datasets in pre-training and fine-tuning setting. We evaluate the effectiveness of graph foundation models [30, 45] in the following few-shot setting due their distinctive training mechanisms, such as in-context pre-training [30] and fully supervised training [45]. For supervised baselines, models are trained directly on the target graph; for self-supervised methods, we pre-train across all datasets before adapting to the specific target graph. Our approach demonstrates a substantial performance improvement, exceeding the best baseline by an average of over 6%. Specifically, our method outperforms the best baseline by 2% across three datasets and by 5% across another three datasets. This underscores the effectiveness of using computation trees as transferable patterns.\nFew-shot Learning Table 3 presents the few-shot learning performance of GFT compared to self-supervised methods [74, 26, 11] and graph foundation models [30, 45, 25]. We randomly select k samples per way from the training set for fine-tuning\u00b3. This method is similar to Prodigy [30], and is much more label-efficient than OFA [45] with supervised pre-training. Despite the extremely limited labels for fine-tuning, GFT significantly surpasses existing methods, showing the fast adaptation capability. Appendix H shows more fine-tuning instances can significantly improve performance."}, {"title": "4.3 Transferability", "content": "Table 5 shows the impact of different pre-training datasets under the pre-training and fine-tuning setting, where comprehensive results (including the following ablation studies) are available in Appendix I. The performance for specific tasks (node-, link-, graph-level) represent the average across all involved datasets. We examine three scenarios with distinct pre-training datasets: (i) all"}, {"title": "4.4 Ablation Study", "content": "Tree Reconstruction and Classification. Table 4 shows the impact of various reconstruction tasks in pre-training and tree classifiers in fine-tuning. All reconstruction tasks enhance model performance compared to models without pre-training. Notably, semantic reconstruction is most effective for node-level and graph-level tasks due to its comprehensive consideration of node features and graph structures. Feature reconstruction is particularly beneficial for link-level tasks, as it preserves the original node semantics, which are crucial for KGs. The optimal performance is achieved when three tasks are jointly optimized, aligning with findings in Ju et al. [36]. Similarly, the combination of prototype and linear classifiers in tree classification leads to superior performance. Furthermore, removing strategies designed to enhance the quality of the tree vocabulary results in model degradation across all settings (Appendix 1.3)."}, {"title": "5 Conclusion", "content": "Conclusion. In this paper, we rethink the transferable pattern in graphs as computation trees and validate their transferability both empirically and theoretically. Building on this insight, we propose a cross-domain and cross-task GFM named GFT. This model leverages computation tree reconstruction to acquire general graph knowledge from cross-domain datasets and uses computation tree classification to facilitate adaptation to various target tasks. In future work, we aim to explore its capabilities for in-context learning and zero-shot learning.\nLimitations. In this paper, we focus primarily on message-passing GNNs, as message-passing can be naturally unrolled as a tree-like structure. However, our analysis excludes graph transformers and expressive GNNs with specialized computational architectures. We plan to extend our analysis to understand the transferable patterns of these advanced learning algorithms in future work. Additionally, message-passing GNNs may lack the expressiveness needed to address isomorphism problems in graphs. One can apply advanced techniques [105] to handle link isomorphism and use advanced expressive GNNs [103] to tackle graph isomorphism. Moreover, the deployment of GFT in real-world applications may encounter efficiency issues, which can be mitigated by techniques like [106, 88].\nBoarder Impact. The proposed GFT is a cross-domain and cross-task graph foundation model, designed for rapid adaptation to target tasks with extremely limited labels. We wish our research can support applications where label acquisition is challenging and model training is time-consuming, such as in molecular discovery and financial fraud detection."}]}