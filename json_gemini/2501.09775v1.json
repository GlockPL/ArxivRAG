{"title": "MULTIPLE CHOICE QUESTIONS: REASONING MAKES LARGE\nLANGUAGE MODELS (LLMS) MORE SELF-CONFIDENT EVEN\nWHEN THEY ARE WRONG", "authors": ["Tairan Fu", "Gonzalo Mart\u00ednez", "Javier Conde", "Mar\u00eda Grandury", "Pedro Reviriego", "Mar\u00eda Grandury"], "abstract": "One of the most widely used methods to evaluate LLMs are Multiple Choice Question (MCQ) tests.\nMCQ benchmarks enable the testing of LLM knowledge on almost any topic at scale as the results\ncan be processed automatically. To help the LLM answer, a few examples called few shots can be\nincluded in the prompt. Moreover, the LLM can be asked to answer the question directly with the\nselected option or to first provide the reasoning and then the selected answer, which is known as chain\nof thought. In addition to checking whether the selected answer is correct, the evaluation can look\nat the LLM-estimated probability of its response as an indication of the confidence of the LLM in\nthe response. In this paper, we study how the LLM confidence in its answer depends on whether the\nmodel has been asked to answer directly or to provide the reasoning before answering. The results\nof the evaluation of questions on a wide range of topics in seven different models show that LLMs\nare more confident in their answers when they provide reasoning before the answer. This occurs\nregardless of whether the selected answer is correct. Our hypothesis is that this behavior is due to\nthe reasoning that modifies the probability of the selected answer, as the LLM predicts the answer\nbased on the input question and the reasoning that supports the selection made. Therefore, LLM\nestimated probabilities seem to have intrinsic limitations that should be understood in order to use\nthem in evaluation procedures. Interestingly, the same behavior has been observed in humans, for\nwhom explaining an answer increases confidence in its correctness.", "sections": [{"title": "1 Introduction", "content": "The evaluation of Large Language Models (LLMs) is challenging, as their answers are in natural language and they\nhave to be evaluated on their performance on a large number of topics and tasks [1].\nA potential approach is human evaluation, so people evaluate LLM responses. However, this does not scale to tens\nof thousands of questions for each model, with new models appearing every day. To address this issue, initiatives\nsuch as Chatbot Arena [2] resort to the community to assess human preferences. However, questions, answers, and\nparticipants are not controlled, so the results provide a comparative ranking of models, but not a detailed analysis of\ntheir specific capabilities. A more scalable alternative would be to use an LLM to evaluate other LLMs [3]. This method\nhas limitations, as the LLM that is judging may have biases towards its own content or toward long responses [4, 5], and"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Procedure", "content": "In our evaluation, we consider two different prompts when asking the question to the model. In the first, the model is\nasked to answer directly:\n\"Please respond with only the letter of the solution, in the format {'sol': 'solution'}. Do not respond with any other\ninformation. Here is an example:\nInput: A car travels 60 kilometers per hour for 2 hours and then 80 kilometers per hour for 3 hours. What is the average\nspeed of the car for the entire trip? a) 70 km/h, b) 72 km/h, c) 75 km/h, d) 74 km/h\nOutput: {'sol': 'b'}\"\nIn the second prompt, as per CoT, the model is asked to provide step-by-step reasoning before selecting an option:\n\u201cPlease think step by step before answering, considering at least three steps. Once you have the solution, end the response\nonly with the letter of the solution, in the format {\u2018sol': \u2018solution'}. Here is an example:\nInput: A car travels 60 kilometers per hour for 2 hours and then 80 kilometers per hour for 3 hours. What is the average\nspeed of the car for the entire trip? a) 70 km/h, b) 72 km/h, c) 75 km/h, d) 74 km/h\nOutput: First, I need to calculate the total distance traveled. For the first part of the trip, the car travels at 60 km/h for\n2 hours, so the distance is 60 * 2 = 120 kilometers. Next, for the second part of the trip, the car travels at 80 km/h for 3\nhours, so the distance is 80 * 3 = 240 kilometers. The total distance traveled is 120 + 240 = 360 kilometers. Now, I"}, {"title": "2.2 LLMs", "content": "In order to ensure that the results are representative of the current LLMs, we select open and proprietary models from\ndifferent companies and sizes. In more detail, we evaluate the following LLMs.\n\u2022 Two models from Meta: LLama3.1-8B and LLama3.2-11B [15].\n\u2022 One model from Mistral: Mistral-7B [16].\n\u2022 One model from Google: Gemma-2-9B [17].\n\u2022 One model from 01.AI: Yi-1.5-9B [18].\n\u2022 Two models from OpenAI: GPT-40-mini and GPT-40 [19]."}, {"title": "2.3 Tests", "content": "The benchmark selected for our experiments is the Massive Multitask Language Understanding (MMLU) [8] as it\ncovers a wide range of topics and we are interested in evaluating if the self-confidence in the results depends on the\nnature of the question. The dataset has 57 categories and more than 15,000 questions in total."}, {"title": "3 Evaluation results", "content": "The 57 categories of MMLU questions were run on the selected models with the direct and CoT prompts described in\nthe previous section\u00b9. First, we look at the aggregated results in terms of accuracy. The accuracies with both prompts\nare shown in Figure 1 for the different models. It can be seen that accuracy increases when the models reason before\nselecting the option, as reported in the literature [20]."}, {"title": "4 Discussion", "content": "The results in Figure 6 show that when LLMs generate incorrect answers, the frequency of the \"wrong and confident\"\nscenario significantly exceeds that of the \"wrong and not confident\" scenario, particularly when LLMs are required to\nreason, which is consistent with human behavior when answering MCQ [21]. Furthermore, as shown in Figure 7, for\nquestions such as those related to history or moral disputes, which require minimal reasoning, the impact of reasoning\non the accuracy of LLMs is negligible, and in many cases, accuracy actually decreases. However, simultaneously, the\nconfidence of the model in providing incorrect answers increases significantly. This suggests that LLMs generating\nmore reasoning information may actually be harmful. Experiments on MCQ in medical exams [22] have shown that\nnon-analytical reasoning, which relies on intuition to quickly answer questions, led to the correct answer even more\neffectively than analytical reasoning. This is primarily because non-analytical reasoning can more efficiently utilize the\ntest taker's prior experience with similar questions, whereas reasoning processes may cause this experience to become\nineffective, thus leading to incorrect choices. This provides a potential explanation for our findings. For questions\ninvolving common sense, LLMs undoubtedly possess vast amounts of experience during their training process. When\nreasoning is required before answering, the influence of this experience is diminished, causing the model to potentially\nrely on erroneous reasoning based on faulty premises, resulting in incorrect answers. Although this effect is not as\npronounced as in humans, it suggests that, for certain categories of questions, allowing LLMs to rely on intuition\nmight be a more reliable approach. In such cases, applying logprobs to evaluate the model's performance may not be\nappropriate.\nThe increase in LLM self-confidence when it provides reasoning before answering can be related to the auto-regressive\nnature of these models that predict the next token based on the previous ones. This means that if the reasoning is\nconvincing and supports the selection of a given option, the model would tend to assign it a larger probability as the next\ntoken. In fact, this behavior has been consistently observed in humans, when they explain the answer, their confidence\nin their response increases, as stated in [23], \"explaining is believing\". The same observation applies to the limited\ncorrelation between confidence and accuracy, with incorrect answers having many times higher confidence values,\nwhich has also been reported in studies with humans [24]. Therefore, it seems that more studies are needed to see if the\nconfidence of the models follows the same patterns as in humans. If that is the case, it could provide insight into how\nLLMs work.\nFinally, the results show how the confidence of the model is, as in humans, highly dependent on several factors and\ntherefore should be used with caution as a tool to evaluate LLM performance. More research is needed to understand\nwhen confidence is a valid performance indicator. Existing studies on human behavior with regard to confidence provide\nvaluable information that should be used in these research efforts."}, {"title": "5 Conclusion", "content": "This paper has studied how the self-confidence of LLMs in their answers to multiple choice questions depends on\nwhether the models answer directly with the selected options or if they provide first step-by-step reasoning and then\nselect an option. The results for the 57 subjects of the MMLU benchmark and in seven different LLMs show that the\nestimated probability of the selected response increases when the models provide reasoning before answering. This\noccurs regardless of whether the option selected by the LLM is correct. In fact, the increase in self-confidence is larger\nwhen the selected option is incorrect. These results are consistent with human studies on confidence, and suggest that\nfurther research is needed to understand when and how LLM confidence estimates can be used for evaluation."}]}