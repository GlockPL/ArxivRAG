{"title": "Multi-stream deep learning framework\nto predict mild cognitive impairment with Rey Complex Figure Test", "authors": ["Junyoung Park", "Eun Hyun Seo", "Sunjun Kim", "SangHak Yi", "Kun Ho Lee", "Sungho Won"], "abstract": "Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive\nfunctions such as visuospatial skills and memory, making them valuable tools for detecting\nmild cognitive impairment (MCI). Despite their utility, existing predictive models based on\nthese tests often suffer from limitations like small sample sizes and lack of external validation,\nwhich undermine their reliability. We developed a multi-stream deep learning framework that\nintegrates two distinct processing streams: a multi-head self-attention based spatial stream\nusing raw RCFT images and a scoring stream employing a previously developed automated\nscoring system. Our model was trained on data from 1,740 subjects in the Korean cohort and\nvalidated on an external hospital dataset of 222 subjects from Korea. The proposed multi-\nstream model demonstrated superior performance over baseline models (AUC = 0.872,\nAccuracy = 0.781) in external validation. The integration of both spatial and scoring streams\nenables the model to capture intricate visual details from the raw images while also\nincorporating structured scoring data, which together enhance its ability to detect subtle\ncognitive impairments. This dual approach not only improves predictive accuracy but also\nincreases the robustness of the model, making it more reliable in diverse clinical settings. Our\nmodel has practical implications for clinical settings, where it could serve as a cost-effective\ntool for early MCI screening.", "sections": [{"title": "KEYWORDS", "content": "Rey Complex Figure Test, Mild cognitive impairment prediction, Multi-stream deep learning,\nConvolutional Neural Network, Multi-head self-attention."}, {"title": "INTRODUCTION", "content": "Drawing tests have been well-documented for their comprehensive assessment\ncapabilities which include evaluating visuospatial skills, visual memory and executive function,\nand they are commonly used within the elderly population as a cognitive screening tool for\ndementia, both in clinical and research fields [1]. Among the most prominent drawing tests are\nthe Pentagon Drawing Test (PDT), the Clock Drawing Test (CDT), and the Rey Complex\nFigure Test (RCFT). The PDT, for example, requires participants to draw two intersecting\npentagons with scoring typically binary (fail or success) [2]. The CDT assesses executive\nfunction and visuospatial skills by having subjects draw a clock face set to a specific time, with\nscoring methods varying significantly \u2013 from a binary system to detailed point assignments\nbased on accuracy of contour, number sequence, and hand placement [3-5]. The RCFT,\ndefined by Rey [6], challenges participants to copy and recall a complex figure, with a widely\nused 36-point scoring system developed by Osterrieth [7].\nRecent advancements have seen the application of machine learning approaches to\nenhance the predictive accuracy of cognitive status from these tests. This is particularly\nvaluable because of the simplicity of administering drawing tests, which could be useful for\nscreening early stages of dementia in clinical fields. For example, deep-learning approaches\nhave been utilized for the digitized PDT[8], CDT [9] and RCFT[10] to predict MCI and CN\npatients. Additionally, multi-dimensional kinematic parameters extracted from a digital pen\nand tablet during RCFT were analyzed using logistic regression [11].\nHowever, there are some limitations in previous studies. Primarily, most of these\nstudies had small samples sizes and lacked an external test set, which undermined the reliability\nof model performances. Even in cases where sample sizes were not small, the performance of"}, {"title": null, "content": "models was not sufficiently robust for screening early stages of dementia. This could be\nattributed to the challenges inherent in utilizing image data in deep learning models. For\ninstance, image data often contains a vast amount of information but can also be prone to noise\ndue to its high dimensionality [12, 13]. Moreover, image data encompasses diverse patterns\nand features, making it challenging for models to learn effectively, especially when sample\nsizes are not significantly large [14].\nIn this paper, we proposed a novel multi-stream deep learning network that combines a\nspatial stream with raw image data and a scoring stream utilizing an automated scoring system\ndeveloped in a previous study [15]. The proposed model was implemented by using a total\n1,740 subjects (CN 947, MCI 793) to train a deep learning model for distinguishing MCI\npatients from CN subjects. Additional 222 subjects (CN 106, MCI 116) were utilized as an\nexternal dataset to improve the reliability of the model performance."}, {"title": "MATERIALS AND METHODS", "content": "Datasets\nGARD cohort\nWe enrolled 1,740 subjects from the Gwangju Alzheimer's and Related Dementia (GARD)\ncohort registry at Chosun University in Gwangju, Korea during 2015-2019. The diagnostic\ncriteria for CN and MCI have been described in Seo et al. [16]. Briefly, CN subjects were\nincluded if they were aged 60 or older, had a Clinical Dementia Rating (CDR) score of 0, and\nexhibited normal cognitive function, with all neuropsychological test z-scores above -1.5 \u00d7\nstandard deviation (SD) based on age, education, and gender norms. MCI patients were aged\n60 or older, had a CDR score of 0.5, and met the MCI criteria established by [17].\nWUH cohort"}, {"title": null, "content": "The Wonkwang University Hospital (WUH) cohort includes 106 CN subjects and 116 MCI\npatients enrolled between 2017 and 2022. In alignment with our training set criteria, subjects\nwere classified based on their CDR scores: a CDR score of 0 indicated a CN diagnosis, while\na score of 0.5 indicated MCI."}, {"title": "Deep learning architecture", "content": "Figure 1A provides an overview of the proposed method. Our model predicts the probability\nof an individual being classified as a MCI patients using three pre-processed RCFT images\nalong with age, sex and years of education. The pre-processing method for the RCFT images\nfollows the protocol outlined by Park et al. [15]. Our prediction model employs a dual-stream\narchitecture: a spatial stream and a scoring stream. Both streams process data through softmax\nfunctions, and their outputs are merged using average fusion to yield the final classification\nprobability. In the spatial stream, each 512x512 image is input into a CNN model that uses\nEfficientNet [18] as its backbone. We selected EfficientNet-B2 for its efficiency and suitability\nin medical applications, given its lower parameter count and adequate performance with limited\ndatasets. EfficinetNet-B2 incorporates a 3x3 convolution layer followed by multiple 3x3 and\n5x5 mobile inverted bottleneck convolution (MBConv) blocks, a design borrowed from\nMobileNet [19] (Figure 1B). Post-CNN, the feature map are flattened, and a multi-head self-\nattention layer is applied, enhancing the model's focus on significant spatial region. The multi-\nhead self-attention mechanism, as defined by [20], combines multiple self-attention layers to\ncapture diverse features, expressed as:\nMultiHead(Q, K,V) = Concat(head\u2081, ..., head\u04bb)W\u00b0,\nhead\u2081 = Attention(QW, KW, VW)"}, {"title": null, "content": "where Q, K, V are the query, key and value matrix, respectively, and we use four attention\nheads (h=4). The outputs from multi-head self-attention layers are integrated and processed\nthrough two fully connected (FC) layers followed by a softmax function.\nConversely, the scoring stream uses a previously developed deep learning model [15]\nto predict RCFT scores. The scores for three images, along with demographic data, are\nconcatenated and passed through an FC layer with a softmax function. It is important to note\nthat the scoring model's weights remain fixed during training, preventing updates."}, {"title": "Baseline models", "content": "The proposed model was evaluated against four baseline models: three logistic regression\nmodels and one deep learning model. The first baseline model utilized MMSE scores. The\nsecond and third models used three RCFT scores, scored by trained experts and a previous AI\nscoring system, respectively. The final baseline was a deep learning model, which solely\nutilized the spatial stream network. All baseline models included age, sex and years of\neducation as covariates."}, {"title": "Scoring validation", "content": "To mitigate human errors in scoring, scanning and digitizing, we tailored our Al scoring system\nspecifically for the external test set to enhance data quality. For images where the difference\nbetween the human expert scores and AI-generated scores exceeded ten points, we conducted\na re-examination by trained human experts. Following this, we compared the AI-generated\nscores with these newly corrected scores to ensure accuracy and reliability."}, {"title": "Experiments", "content": "We conducted prediction model building and performance evaluation using data from GARD\nand WUH cohort. GARD cohort was employed to construct the prediction model. Throughout\nthe training process, we utilized the binary cross-entropy as the loss function and the Adam\noptimizer was adopted to minimize the loss function. To prevent overfitting, we reduced the\ninitial learning rate to 10% every five epochs and implemented early stopping if there was no\nimprovement in validation loss after 30 epochs, ensuring that the final model weights selected\ncorresponded to the lowest validation loss.\nTo evaluate our model's performance, GARD cohort was randomly divided into\ntraining, validation and test sets with 6:2:2 ratio. This division process was repeated fifty times.\nExternal validation was performed using WUH cohort. Model performance was assessed using\nthe area under receiver operating characteristics (AUC), the accuracy (ACC), sensitivity (SEN)\nand specificity (SPE).\nAll experiments were conducted using the Pytorch library (v 2.0.0) in Python (v 3.8.8)\nwith NVIDIA 1080ti GPUs with 48 GB of memory per GPU."}, {"title": "RESULTS", "content": "Characteristics\nsummarizes the clinical characteristics of subjects in the GARD and WUH cohort\ndatasets. In the GARD dataset, the average ages were 71.8 (\u00b16.1) years for CN subjects and\n73.5 (\u00b16.4) years for MCI patients (P<0.01). Education levels and MMSE scores also\nsignificantly differed between CN subjects (education level: 10.4\u00b14.6; MMSE score: 27.5\u00b12.1)\nand MCI patients (9.8\u00b14.7; 25.5\u00b13.1) (P<0.01). Similarly, sex ratios exhibited comparable"}, {"title": null, "content": "trends in both groups. Conversely, the WUH dataset revealed no significant differences in the\naverage ages between CN (69.9\u00b17.7) subjects and MCI (71.4\u00b18.3) patients (P>0.05), nor were\nthere differences in education levels between CN (8.7\u00b14.2) and MCI (9.2\u00b14.5) groups\n(P>0.05). Comparing the two datasets, the external test set consistently showed lower age,\neducation level, and RCFT scores across both groups, with the exception of the education level\nand RCFT copy score in CN group of the GARD dataset."}, {"title": "Scoring validation", "content": "The initial correlation (R\u00b2) between scores by AI and those by experts was 0.81, with a mean\nabsolute error (MAE) of 3.0 point (Figure 2A). Discrepancies exceeding 10 points between the\nground truths and predicted scores were identified in 30 images. Upon validation, scores for 26\nof these images were corrected. After these adjustments, the correlation improved significantly\nto an R\u00b2 of 0.95 with an MAE = 2.0 (Figure 2 B)."}, {"title": "Comparison of model performance via internal test using GARD cohort", "content": "We evaluated the classification performances of five models, including three that incorporated\nthe proposed method. These models are: 1) logistic regression using MMSE scores; 2) logistic\nregression using RCFT scores assessed by experts; 3) logistic regression using RCFT scores\npredicted by the AI model; 4) deep learning model utilizing only spatial stream network; 5)\ndeep learning model employing multi stream networks. The mean performances of those\nmodels are shown in Table 2 (A)."}, {"title": null, "content": "The logistic regression model with MMSE scores demonstrated the lowest performance,\nwith an AUC of 0.714 [95% confidence interval: 0.706-0.712], an ACC of 0.660 [0.652-0.667],\nSEN of 0.625 [0.613-0.636] and SPE of 0.694 [0.685-0.704]. The logistic regression model\nusing expert-assessed RCFT scores recorded an AUC of 0.776 [0.768-0.782], an ACC of 0.705\n[0.699-0.712], an SEN of 0.700 [0.689-0.711] and an SPE of 0.71 [0.700-0.722]; the\nperformance of the model using AI-predicted RCFT scores was similar, with an AUC of 0.777\n[0.770-0.783], ACC of 0.710 [0.703-0.717], SEN of 0.699[0.689-0.709] and SPE of 0.721\n[0.710-0.731].\nPerformance improvements were evident with the spatial stream network model, which\nachieved an AUC of 0.803 [0.768-0.837], ACC of 0.731 [0.702-0.761], SEN of 0.701 [0.661-\n0.741] and SPE of 0.762[0.720-0.804]. Finally, our proposed deep learning model using the\ntwo-stream network outperformed all baseline models across all metrics, with an AUC of 0.852\n[0.837-0.869], ACC of 0.771 [0.755-0.787], SEN of 0.742 [0.718-0.767] and SPE of 0.800\n[0.774-0.823]."}, {"title": "External validation using WUH cohort", "content": "Performance metrics for the trained models on this set are detailed in Table 2 (B). The logistic\nregression model using expert-rated RCFT scores from the initial dataset demonstrated an AUC\nof 0.750 [0.750-0.751], ACC of 0.709 [0.707-0.712], SEN of 0.832 [0.829-0.835] and SPE of\n0.575 [0.571-0.579]. With the validated dataset based on the re-rated RCFT scores, the model's\nperformance improved to an AUC of 0.813 [0.812-0.814], ACC of 0.750 [0.748-0.753], SEN\nof 0.799 [0.718-0.767] and SPE of 0.800 [0.774-0.823]. The logistic model with AI-predicted\nRCFT scores displayed comparable performance to that of human experts (AUC=0.804[0.803-\n0.805], ACC=0.722[0.721-0.725], SEN=0.799[0.797-0.802] and SPE=0.639[0.634-0.722])."}, {"title": null, "content": "The deep learning model employing the spatial stream network achieved a higher AUC\n(0.837[0.814-0.860]), ACC (0.744[0.719-0.768]) and SPE (0.745[0.697-0.792]) but had a\nlower SEN (0.743[0.690-0.800]). Our proposed deep learning method using the two-stream\nnetwork outperformed all baseline models, showing superior performance across all metrics:\nAUC=0.872[0.862-0.882], ACC=0.781[0.768-0.795], SEN=0.836[0.807-0.864]\nSPE=0.722[0.687-0.757]."}, {"title": "DISCUSSION", "content": "In this article, we developed a multi-stream deep learning network to differentiate between\nMCI patients and CN subjects. Our approach surpasses previous methods utilizing drawing test\n(PDT, CDT and RCFT) by leveraging a larger sample size and an external test set, thereby\nenhancing the robustness and performance of the model. Notably, our model outperformed\nexisting studies, achieving the highest recorded performance metrics.\nOur multi-stream network combines both the scoring stream and spatial stream. The\nscoring stream incorporates an Al scoring system for RCFT, which save time and human\nresources while proactively preventing human errors, thus improving accuracy. This\nimprovement was evidenced by results showing that the model, when AI scoring was used for\nQC, exhibited much higher performance compared to the model performance using the initial\nexpert-assessed RCFT scores without QC. Furthermore, while it takes approximately 5 minutes\nfor an expert to score one subject, our AI scoring system takes only 10 seconds. The spatial\nstream of our model utilizes raw RCFT images as input, and extracts subtle details within the\nimages, such as pen thickness and shape, which are not captured by the human scoring system"}, {"title": null, "content": "(ranging from 0-36 points). This leads to substantial improvement in performance compared to\nmodels that rely solely on scoring. However, although raw image data is rich with information,\nit also includes considerable noise; therefore, the integration of multi-head self-attention layers\nhelps the model to prioritize crucial spatial regions within the feature map, boosting\nperformance. However, models that rely solely on raw images have shown higher SDs in\nperformance compared to logistic models utilizing scores, and the performance of the spatial\nstream network may be compromised due to differences in resolution between existing training\nimages and new test images. By combining the advantages of both scoring stream network,\nwhich utilizes human scoring systems, and the spatial stream network, which processes images,\nour proposed method achieves high and robust performance.\nThe proposed method offers a cost-effective and efficient screening tool for MCI\npatients at the medical check-up centers. Currently, the MMSE is the most popularly utilized\nscreening tool, known for its simplicity and quick administration time of approximately 5-10\nminutes [2]. However, our results indicate that MMSE is less informative for predicting MCI\nand lacked accuracy in distinguishing between CN subjects and MCI patients (AUC = 0.714).\nAnother study reported MMSE performance with an AUC of 0.733 (N=2,577) [8]. In contrast,\ncomprehensive cognitive function tests such as Neuropsychological Test Battery are more\ntime-consuming, taking up 2 hours to administer [21] and pose challenges in examining\nmultiple subjects due to the additional time required for scoring and interpretation. Although\nthe RCFT requires more times than the MMSE, approximately 30 minutes including a 20-\nminute delay interval [22], our model based on the RCFT significantly outperformed that of\nthe MMSE (AUC>0.85). Furthermore, since our model does not necessitate additional time for\nexpert scoring, it is highly efficient compared to other cognitive function tests that rely on\nexpert scoring."}, {"title": null, "content": "Despite the flexibility of the proposed method, our study had some limitations and areas\nfor future development. First, we did not incorporate additional ancillary information beyond\nthe raw images. Recent studies have shown that kinematic data such as pressure, velocity, time\nwhich cannot be captured by traditional paper-and-pencil drawing tests but recorded by tabled-\nbased tests revealed significant differences between case and control groups. These parameters\nsuggest potentially useful covariates to enhance the performance of prediction models [11, 23].\nWe have developed a tablet-based application that administers the RCFT, records the drawing\nprocess and extracts kinematic parameters. By incorporating this information, further\nimprovement may be possible. Second, verbal tests have also played a crucial role in\nneuropsychological evaluation [24]. Recent advancements in automatic speech recognition\ntechnology, such as BERT [25], have enabled the exploration of speech-based methods for AD\ndetection [26, 27]. For future work, we plan to develop tablet-based, fully automated memory\ntests that integrate both visual and verbal assessments.\nIn conclusion, our multi-stream deep learning network outperformed previous studies\nin distinguishing MCI patients from CN subjects. By integrating human scoring systems and\nimage-based information, our model demonstrated robust performance across internal and\nexternal datasets. Our findings suggest potential clinical utility as a time-efficient screening\ntool for cognitive impairment."}]}