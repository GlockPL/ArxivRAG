{"title": "Learning Soft Driving Constraints from Vectorized Scene Embeddings while Imitating Expert Trajectories", "authors": ["Niloufar Saeidi Mobarakeh", "Behzad Khamidehi", "Chunlin Li", "Hamidreza Mirkhani", "Fazel Arasteh", "Mohammed Elmahgiubi", "Weize Zhang", "Kasra Rezaee", "Pascal Poupart"], "abstract": "The primary goal of motion planning is to generate safe and efficient trajectories for vehicles. Traditionally, motion planning models are trained using imitation learning to mimic the behavior of human experts. However, these models often lack interpretability and fail to provide clear justifications for their decisions. We propose a method that integrates constraint learning into imitation learning by extracting driving constraints from expert trajectories. Our approach utilizes vectorized scene embeddings that capture critical spatial and temporal features, enabling the model to identify and generalize constraints across various driving scenarios. We formulate the constraint learning problem using a maximum entropy model, which scores the motion planner's trajectories based on the similarity to the expert trajectory. By separating the scoring process into distinct reward and constraint streams, we improve both the interpretability of the planner's behavior and its attention to relevant scene components. Unlike existing constraint learning methods that rely on simulators and are typically embedded in reinforcement learning (RL) or inverse reinforcement learning (IRL) frameworks, our method operates without simulators, making it applicable to a wider range of datasets and real-world scenarios. Experimental results on the InD and Traffic Jams datasets demonstrate that incorporating driving constraints not only enhances model interpretability but also improves closed-loop performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Motion planning involves generating safe and feasible trajectories for vehicles to navigate complex traffic scenarios. These trajectories must strictly adhere to traffic regulations and avoid safety-critical incidents, such as collisions with other vehicles, pedestrians, or maintaining unsafe proximity to road boundaries [1]-[3]. A key challenge in this domain is ensuring that the planned trajectories are safe, especially in scenarios with unpredictable traffic participants. Imitation learning (IL) techniques have gained prominence in addressing this challenge by focusing on replicating human driving behaviors based on real-world data [4]-[6]. However, IL approaches often lack transparency, making it difficult to interpret how and why the model makes certain decisions. This lack of interpretability can be problematic when deploying autonomous driving systems in safety-critical settings, where understanding model behavior is crucial for trust and validation. One promising approach to overcome this limitation is to derive driving constraints directly from observed human behaviors. These constraints\u2014such as maintaining appropriate distances, obeying speed limits, and adhering to right-of-way rules are intuitive for human drivers but are challenging to encode explicitly in autonomous systems. Human drivers naturally internalize these constraints through experience and context-awareness, whereas autonomous systems require more formalized representations. Capturing these implicit driving rules and encoding them into autonomous systems could enhance both the safety and interpretability of motion planning models, improving their closed-loop performance in real-world scenarios.\nSeveral studies have been conducted to explore the constraint learning problem [7]\u2013[16]. In [10], Scobee et al. reformulate an Inverse Reinforcement Learning (IRL) on Markov decision processes (MDPs) to estimate state, action, and feature constraints motivating agent behavior. Utilizing the maximum entropy IRL framework of [17], they proposed an iterative method to infer the maximum likelihood constraints. These constraints are then used to explain the model behavior. Malik et al. [11] extend this by focusing on learning hard constraints that a constraint-abiding agent follows. They introduced an algorithm to acquire such constraints. A key distinction between their work and [10] is that Malik's approach can learn arbitrary Markovian constraints in high-dimensional environments, a capability not supported by the former. In [12], Gaurav et al. propose an Inverse Constraint Learning (ICL) framework to learn soft constraints from demonstrations, assuming that the reward function is given. This approach is similar to IRL, except it focuses on learning the constraints rather than the reward. Similarly, Kim et al. [13] extend the ICL to multi-task settings to infer shared safety constraints across multiple tasks from demonstrations. The authors in [7] develop a benchmark for Inverse Constrained Reinforcement Learning (ICRL) algorithms through synthetic and real-world inspired tasks. These tasks are designed to challenge the ability of ICRL algorithms to accurately infer environmental constraints from observed expert behaviors. The proposed algorithms are tested in a variety of environments to feature a variety of constraint types, including navigation and motion constraints that mimic real-world limitations.\nFor safety-critic applications like autonomous driving, it is crucial to imitate human drivers and learn the implicit rules that human drivers follow [18]. Several studies tried to learn a constraint from human demonstrations and integrate it into autonomous driving frameworks [7], [8], [15], [19], [20]. For instance, Liu et al. [7] propose an inverse constraint reinforcement learning benchmark that evaluates constraint inference on a highway driving scenario. Lindner et al. [19] propose convex constraint learning for reinforcement learning (CoCoRL) and evaluate their method on a 2D driving simulator [21]. In contrast to [12] that assumes a given reward, they learn constraints from demonstrations with unknown rewards. Rezaee et al. [8] extend the work of [10] by training a variational auto-encoder (VAE) based model that predicts driving constraints. In [15], the authors introduced a dual-constraint mechanism to handle long-term and short-term safety concerns to ensure robust safety during training and deployment.\nExisting approaches predominantly rely on interactions with simulators or the availability of reward signals, as they are typically formulated within reinforcement learning (RL) or inverse reinforcement learning (IRL) frameworks. In these paradigms, agents optimize their actions by interacting with the environment. However, developing accurate and robust simulators is particularly challenging, especially in safety-critical domains such as autonomous driving. This complexity arises from the need to model highly dynamic, multi-agent environments with diverse traffic scenarios, rare safety-critical events, and unpredictable behaviors of human drivers. Simulators must also accurately capture the intricate physics of vehicle dynamics, weather conditions, and sensor noise, which can be difficult to replicate in a virtual setting. In contrast, we integrate constraint learning directly within the imitation learning framework, thereby obviating the need for intricate simulator design or access to reward signals. Our contribution in this work can be summarized as"}, {"title": "II. PROBLEM DESCRIPTION AND METHODOLOGY", "content": "In this section, we present the constraint learning problem. Before delving into its details, we provide a concise overview of our planning pipeline, as we intend to integrate the constraint learning problem into our planning framework.\n The objective of the planning problem is to find a policy through imitating human driving behaviors. Fig. 1 illustrates the architecture of our planner, which is adapted from [23], originally designed for trajectory prediction. To tailor this framework for planning purposes, we incorporate several modifications and enhancements to suit closed-loop decision-making in autonomous driving. Our model utilizes VectorNet [24] to encode various input features, which include dynamic traffic objects and static map elements. For dynamic features, we consider the ego vehicle and surrounding traffic agents, capturing their spatial and kinematic attributes such as position, velocity, orientation, and size. The map-based static features include centerlines, lane types, speed limits, and other high-definition map data, which are critical for contextual awareness in driving scenarios. The VectorNet architecture is composed of two core components: polyline subgraphs and a global interaction graph. The polyline subgraphs are designed to encode local features from the road and agent elements, such as individual lanes or vehicle trajectories, by representing them as a sequence of connected line segments. This allows the network to capture fine-grained structural information from the environment. On the other hand, the global interaction graph aggregates the information from all polyline subgraphs to model interactions between agents and their surrounding road structures. This is essential for understanding the broader driving context, such as interactions at intersections or lane changes influenced by neighboring vehicles. Once the input features are encoded, the output of the global graph is passed to a target selection module. This module predicts a set of potential future positions for the ego vehicle within a predefined time horizon, using a mixture of learned behaviors and map-based constraints. The targets represent feasible goals for the vehicle, such as specific points on the road or future locations within traffic. Subsequently, these predicted target positions are processed by a motion estimation module. Here, we generate a diverse set of candidate trajectories for the ego vehicle, each representing a potential path from its current position to one of the predicted target locations. To ensure that the trajectories are both feasible and physically plausible, we employ a kinematic bicycle model, which simulates the dynamics of a vehicle during motion planning. This model takes into account the physical constraints of the vehicle, such as turning radius and acceleration limits, and applies a smoothing filter to adjust the trajectory parameters. As a result, the final set of trajectories adheres to both the vehicle's motion constraints and environmental requirements.\nAs discussed earlier, the objective of our planner is to generate trajectories that closely resemble those of an expert driver. To achieve this, we need a scoring mechanism that assigns higher score values to trajectories exhibiting higher proximity to the expert trajectory. The existing planner architectures take into account score values exclusively based of their resemblance to the expert's trajectory (Fig. 1(a)), without offering insights into the interpretable driving behavior associated with these trajectories. To tackle this limitation, we divide our scoring into two components: reward and constraint, as depicted in Fig. 1(b).\nLet $T$ represent the set of all possible trajectories generated by the motion estimation module. We express the reward function as $r:T \\rightarrow R$. Also, we denote the constraint function as $c: T \\rightarrow [0, 1]$. Using this notation, a constraint value of 0 indicates that the corresponding trajectory fails to exhibit the desired behavior, violating established driving metrics. For instance, if we define the constraint with respect to collisions, $c(T) = 0$ indicates that the trajectory $T$ results in a collision. Conversely, a constraint value of 1 asserts that the corresponding trajectory adheres to driving requirements, constituting a safe trajectory. As mentioned in section II-A, our motion estimation generates a set of trajectories at each time. Let $N$ denote the number of these trajectories. Also, let $\\pi(\\cdot)$ show the deployed policy by the planner. To score these trajectories, the original implementation in [23] uses a maximum entropy model, where the probability of selecting trajectory is given by\n$P(\\tau_{\\pi}) = \\frac{e^{r(T_{\\pi})}}{\\sum_{i=1}^{N}e^{r(T_i)}}$   (1)\nHowever, this scoring scheme does not provide any insight into the driving behavior of associated trajectories. To improve interpretability of the model, we modify (1) such that it considers measure of safety in addition to resemblance to the expert trajectory when scoring the trajectories. Given that $c(T) \\in [0,1], \\forall \\tau \\in T$, a natural approach to integrate the constraints into the planning problem is by weighting the selection probabilities of trajectories based on their constraint values. When a trajectory exhibits a low constraint value, it implies undesired performance, requiring a reduction in its selection probability. Conversely, trajectories exhibiting higher quality should correspond to elevated constraint values, necessitating an adjustment in the probability formulation as outlined in (1). As a result, following [11], we can write the probabilities as\n$P(\\tau_{\\pi}) = \\frac{C(T)e^{r(T_{\\pi})}}{\\sum_{i=1}^{N}C(T_i)e^{r(T_i)}}$ (2)\nTo train the constraint model effectively, it is imperative to establish a suitable loss function. This begins with defining a metric for similarity between two trajectories. Let $s(T_1, T_2)$ represent the similarity function between two trajectories $T_1$ and $T_2$, defined as\n$S(T_1, T_2) = ||T_1 - T_2||^2$.\nUtilizing this metric, we identify the trajectory closest to the ground truth trajectory as\n$T_{best} = arg \\min_{T\\in T} s(T, T_{gt})$,  (3)\nwhere $T$ denotes the set of $N$ trajectories generated by motion estimation, and $T_{gt}$ represents the ground truth trajectory. Additionally, let $T'$ denote the set of trajectories in $T$ that contravene driving rules. Our objective is to learn constraint values such that trajectories similar to the ground truth tend towards a constraint value of 1, while the constraint values of violating trajectories in $T'$ converge to 0. To achieve this, trajectories violating driving rules are labeled as 0. The trajectory most similar to the ground truth is assigned a label of 1. Furthermore, all other trajectories meeting driving rules are not labeled. Fig. 2 illustrates the labeling scheme in three different cases, where collision, out-of-map, and stuck serve as the constraint metrics. For the collision and out-of-map, we mark trajectories leading to a future collision or out-of-map behaviour with label of 0. For the stuck scenario, it is a bit tricky. We record the amount of time the vehicle stays stationary. Once this time exceeds $T$, we assess the available trajectories at time $t+T$. If there exists at least one safe and non-stationary trajectory, we label all stuck trajectories with label of 0. This is to ensure that cases like staying in traffic jams or behind the red light traffic are not labelled as invalid trajectories. Using this scheme, we define the constraint loss as\n$\\mathcal{L}(\\Theta) = \\frac{1}{ |T'| + 1 } [\\sum_{T_{i} \\in T'} \\gamma(c_{\\Theta}(T_{i}), 0) + \\gamma(c_{\\Theta}(T_{best}), 1) ]$,  (4)\nwhere $\\Theta$ is the parameter of the constraint network and $\\gamma(x, y)$ is the cross-entropy loss. Training the constraint model using $\\mathcal{L}(\\Theta)$ enables the learning of the implicit constraint function. Algorithm I outlines the detailed steps for learning the constraint function."}, {"title": "III. EVALUATION AND RESULTS", "content": "Dataset: We trained and evaluated our proposed constraint learning method using the InD dataset [22] and the TrafficJams dataset. The InD dataset contains several hours of measurement data collected at various unsignalized intersections, including trajectories of over 11,500 road users, such as vehicles, cyclists, and pedestrians. The TrafficJams dataset focuses on highway driving scenarios, specifically emphasizing congestion and stop-and-go traffic conditions.\nTraining: For the VectorNet and target selection modules, we used the same implementation as introduced in [23]. For the motion estimation, reward, and constraint networks, we use multi-layer perceptrons (MLPs) with two hidden layers of size 64, followed by the ReLU activation function. Layer normalization is also applied after each hidden layer. The motion estimation calculates the necessary acceleration and rate of turn for the ego vehicle to reach its intended targets. The resulting output from motion estimation is then fed into a bicycle filter to produce the corresponding trajectories. The reward and constraint modules take these trajectories along with the scene embedding to calculate associated reward and constraint values. In order to keep the constraints in the range of 0 and 1, the output of constraint network is fed into a Sigmoid activation function. For the constraint labels, we consider all three metrics, including collision, out of map, and stuck. We trained our model for 70 epochs on Nvidia RTX 2080 Ti GPU. The batch size is 128 and each training takes approximately 16 hours.\nThe evaluation metrics we employ include progress rate, success rate, collision rate, outside road, MDBC, and risk factor. The progress rate expresses the ratio of the distance driven by the vehicle towards the goal compared to the ground truth. Success rate denotes the proportion of scenarios where the ego vehicle successfully reaches its destination without any collisions. Collision rate represents the ratio of scenarios resulting in a collision. Outside road represents the percent of trajectories that invade the road boundaries, and MDBC represents the mean drive between collisions. We also use the risk factor model introduced in [25], which maps a scenario to a risk score based on the number of unsafe decisions available to the vehicle by the planner. Specifically, the risk factor is calculated as the mean ratio of collision trajectories to total trajectories for each scenario in closed-loop evaluation. This metric offers deeper insights into the safety performance of the system under various conditions.\nOne advantage of using constraints over traditional scoring methods, as outlined in [23], is that the constraint value provides a direct and interpretable measure of the safety associated with a given trajectory. This is particularly important in safety-critical systems, where understanding and controlling the risk of a trajectory is crucial. In contrast, score values, which are often used to rank trajectories based on various performance metrics, fail to exhibit consistent or reliable patterns across different scenarios, especially when safety is considered. Fig. 3 illustrates instances where trajectories with higher scores lead to unsafe outcomes such as collisions. The inconsistency arises because the scoring does not inherently account for safety. As a result, score values alone are unable to distinguish between safe and unsafe trajectories reliably. On the other hand, constraint values effectively distinguish between the trajectories that violate safety conditions and those that do not. Consequently, since the planner typically selects trajectories with the highest scores for future planning, it often lacks the necessary mechanisms to evaluate and ensure the safety of the selected trajectories. By incorporating a constraint module into the planner, the system gains the ability to assess the safety implications of its chosen actions more accurately.\nTable I presents the closed-loop performance of our model after integrating various constraint modules for both InD and TrafficJams datasets. Initially, we introduced a collision avoidance constraint, and subsequently, we incorporated two additional constraints: the out-of-map constraint and the stuck constraint. The out-of-map constraint was designed to prevent the vehicle from crossing lane boundaries, ensuring that it stays within the intended driving area. This constraint acts as a safeguard, ensuring that the vehicle does not drift off the road or enter restricted zones. Similarly, the stuck constraint was introduced to address situations where the vehicle becomes stationary and unable to proceed, which could cause it to halt prematurely, thereby failing to complete the scenario. The experimental results highlight several positive outcomes from integrating these constraints. For example, the inclusion of the out-of-map constraint completely eliminated the issue of vehicles veering off the road, reducing such incidents to zero. Additionally, the overall success rate of the planner improved, as did its ability to avoid collisions. This collision avoidance improvement is also reflected in the risk factor values, as they count for the number of unsafe trajectories generated by the planner. This indicates a significant enhancement in the planner's capability to select safer and more reliable trajectories. Moreover, the MDBC metric showed notable improvement, reflecting the model's stronger adherence to safe driving behaviors, even in challenging or unfamiliar scenarios. This improvement underscores the model's ability to navigate complex situations while maintaining safety.\nIt is important to note that we have only compared our constraint learning model with the baseline. Existing approaches [7], [8], [10], [12], [15], [19], [20] typically require interaction with the environment to learn constraint values, which is a key limitation in our context. Unlike these methods, our approach does not assume access to a precise autonomous driving simulator that faithfully replicates the characteristics of the collected dataset. Since such simulators are unavailable, a direct comparison with these methods would be neither feasible nor meaningful in evaluating our model's effectiveness.\nTo understand why the model trained with constraint learning modules performs better, we observed that these modules significantly enhance the model's ability to focus on key elements in the scene. Specifically, the model can assign more of its attention to the causal agents\u2014those with the greatest impact on the outcome of the scenario. Fig. 4 provides sample snapshots from both the InD and TrafficJams scenarios. In these examples, as shown in row 1 and row 3, the base model struggles to allocate sufficient attention to the causal agents. Particularly, it gives its attention to road elements or non-causal components of the scene, resulting in collisions. In contrast, the model trained with constraint modules (row 2 and row 4) successfully identifies these agents and assigns them the appropriate level of attention, allowing it to navigate the scene without issues. This improvement demonstrates the effectiveness of constraint learning in improving the model's attention. Additional examples are available in the supplementary video"}, {"title": "IV. CONCLUSIONS", "content": "In this work, we have integrated constraint learning directly into the imitation learning framework. Our approach utilizes vectorized scene embeddings that capture encode critical spatial and temporal relationships between scene components, such as vehicle positions, road features, obstacle locations, etc. Using these embeddings, our model learns to identify and generalize constraints across different driving scenarios. Our approach is improves the interpretability of the planner's behavior, resulting in enhanced safety for autonomous vehicles. A key advantage of our method is that it does not depend on complex simulators or reward-based reinforcement learning techniques, which are often required by state-of-the-art methods. We validated our algorithm using two public datasets, InD and TrafficJams. The results demonstrate that decoupling the scoring module into two distinct components\u2014reward and constraint-significantly enhances the model's ability to detect causal agents in the scene, leading to safer trajectory planning. Our findings also suggest that constraint values offer a more reliable measure of safety compared to traditional score-based methods."}]}