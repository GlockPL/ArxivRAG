{"title": "PREDICATE HIERARCHIES IMPROVE\nFEW-SHOT STATE CLASSIFICATION", "authors": ["Emily Jin", "Joy Hsu", "Jiajun Wu"], "abstract": "State classification of objects and their relations is core to many long-horizon\ntasks, particularly in robot planning and manipulation. However, the combinatorial\nexplosion of possible object-predicate combinations, coupled with the need to adapt\nto novel real-world environments, makes it a desideratum for state classification\nmodels to generalize to novel queries with few examples. To this end, we propose\nPHIER, which leverages predicate hierarchies to generalize effectively in few-shot\nscenarios. PHIER uses an object-centric scene encoder, self-supervised losses that\ninfer semantic relations between predicates, and a hyperbolic distance metric that\ncaptures hierarchical structure; it learns a structured latent space of image-predicate\npairs that guides reasoning over state classification queries. We evaluate PHIER\nin the CALVIN and BEHAVIOR robotic environments and show that PHIER\nsignificantly outperforms existing methods in few-shot, out-of-distribution state\nclassification, and demonstrates strong zero- and few-shot generalization from\nsimulated to real-world tasks. Our results demonstrate that leveraging predicate\nhierarchies improves performance on state classification tasks with limited data.", "sections": [{"title": "1 INTRODUCTION", "content": "State classification of objects and relations is essential for a plethora of tasks, from scene understand-\ning to robot planning and manipulation (Migimatsu & Bohg, 2022; Chen et al., 2024). Many such\nlong-horizon tasks require accurate and varied state predictions for entities in scenes. For example,\nplanning for \"setting up the table\" requires classifying whether the cup is Next To the plate,\nwhether the utensils are OnTop of the table, and whether the microwave is Open. The\ngoal of state classification is to precisely answer such queries about specific entities in an image, and\ndetermine whether they satisfy particular conditions across a range of attributes and relations.\nHowever, the combinatorial space of objects (e.g., cup, plate, microwave) and predicates (e.g.,\nNextTo, OnTop, Open) gives rise to an explosion of possible object-predicate combinations that\nis intractable to obtain corresponding training data for. In addition, real-world systems operating\nin dynamic environments must generalize to queries with novel predicates, often with only a few\nexamples (Bendale & Boult, 2015; Joseph et al., 2021; Ha & Song, 2022). For instance, we may"}, {"title": "2 RELATED WORK", "content": "State classification. The ability to understand object states and relationships is essential for a wide\nrange of tasks in computer vision and robotics, including scene understanding, robotic manipulation,\nand high-level planning (Yao et al., 2018; Yuan et al., 2022). Earlier works that focus on a similar\ntask of visual relationship detection learn to extract object-centric representations from raw images\nand make predictions based on them (Gkioxari et al., 2018; Yao et al., 2018; Ma et al., 2022; Yuan\net al., 2022). A more recent approach by Yuan et al. (2022) specifically addresses state classification\nby extracting object-centric embeddings from RGB images and feeding them into trained networks to\nclassify a set of predefined predicates. However, their approach relies on input images of the objects\nof interest and is limited to predicates seen during training, as it requires a separate network for each\npredicate. In contrast, our method can few-shot generalize to unseen predicates given only the input\nscene and query, without any additional annotations or specific object images.\nRecent advancements in robotics simulators have additionally enabled scalable training of state\nclassification in simulation and subsequent transfer to real-world settings. Simulators such as\nCALVIN (Mees et al., 2022) and BEHAVIOR (Li et al., 2023a) offer varying levels of realism and\nare widely used in the robotics community to generate large and diverse datasets, supporting data\ngeneration across a broad range of states (Ge et al., 2024). We train our model on such simulators\nand show that, compared to prior works, PHIER is significantly more effective at zero- and few-shot\ngeneralization to real-world state classification tasks.\nQuestion-answering approaches. Several key advancements in visual question answering (VQA)\nhave been driven by the development of pretrained large vision-language models (VLMs). These\nmodels are trained on extensive image and text data, leading to a unified visual-language represen-\ntation that can be applied to perform various downstream tasks (Shen et al., 2021; Li et al., 2023b;\nOpenAI, 2023). Approaches such as Viper-GPT (Sur\u00eds et al., 2023) further leverage the power of\nfoundation models by composing LLMs to generate programs that pretrained VLMs can execute.\nHowever, despite their strong general-purpose capabilities, these models still struggle with questions\ninvolving visual and spatial relationships (Tong et al., 2024).\nOn the other hand, a separate class of VQA methods are models trained directly for the VQA task.\nThese approaches follow a general framework of extracting visual and textual features, combining\nthem into a multimodal representation, and learning to predict the answer. Convolutional neural\nnetworks (CNNs) and transformers are widely used for feature extraction, with various techniques\nfor fusing the features. FiLM (Perez et al., 2018) is an early, modular approach that applies a feature-\nwise transformation to condition the image features on the text features, while BUTD (Anderson\net al., 2018) and Re-Attention (Guo et al., 2020) are representative attention-based methods that\nlocalize important regions based on the question. Furthermore, many approaches, including modular,\ngraph-based, or neurosymbolic approaches, introduce more explicit reasoning to better model the\nrelationships and interactions between objects (Andreas et al., 2016; Yi et al., 2018; Ma et al., 2022;\nNguyen et al., 2022; Wang et al., 2023). Our work lies in this latter class of methods, designed and\ntrained for state classification. In contrast to prior works, we not only encode visual features and\ntextual features of predicates but also learn to capture the inherent predicate hierarchy in a joint\nimage-predicate latent space.\nHyperbolic representations for hierarchy. In recent years, several works have explored the\nbenefits of using hyperbolic space to model hierarchical relationships, as it is particularly well-\nsuited for capturing hierarchical structures (Ganea et al., 2018; Nickel & Kiela, 2018). In computer\nvision, prior works have focused on learning hyperbolic representations for image embeddings,\ndemonstrating their effectiveness across various tasks such as semantic segmentation and object\nrecognition (Khrulkov et al., 2020; Liu et al., 2020; Atigh et al., 2022; Ermolov et al., 2022; Ge et al.,\n2023). Hyperbolic embeddings allow models to naturally represent hierarchical relationships between\nvarious entities, such as objects and scenes or different categories, leading to improved performance\nand generalization in such tasks (Weng et al., 2021; Ge et al., 2023). Several approaches further\nincorporate self-supervised learning to learn the underlying hierarchical structure without the need\nfor explicit labels, instead leveraging proxy tasks such as contrastive learning (Hsu et al., 2021; Ge\net al., 2023; Yue et al., 2023). Recently, Desai et al. (2023) proposed a contrastive method to learn\njoint representations of vision and language in hyperbolic space, yielding a representation space that\ncaptures the underlying structure between images and text. Inspired by these works, PHIER learns a\npredicate hierarchy in hyperbolic space informed by language and the pairwise relations between\npredicates, and uses it to conduct few-shot generalization to unseen state classification queries."}, {"title": "3 METHOD", "content": "In this section, we describe PHIER, our model for generalizable state classification. We define\nPHIER's task as a binary state classification problem: given a 2D RGB image and a text query\ndescribing a state (e.g., NextTo(cup, plate), the goal is to predict whether the predicate\nNext To holds True or False for the objects, cup and plate, in the input image. PHIER enables\nefficient generalization to unseen predicates, by way of inferring a structured latent space of image-\npredicate pairs to perform reasoning over (See Figure 2). At the core of our method is a predicate\nhierarchy that captures the semantic relationships between predicates. This hierarchical structure is\ninferred through three key components:\n1. An object-centric scene encoder that localizes regions corresponding to relevant objects;\n2. Self-supervised losses that inject pairwise relations of predicates into the latent space;\n3. A hyperbolic distance metric and an encoder that model hierarchical representations.\nIn Section 3.1, we describe PHIER's base architecture of object and predicate conditioning. In\nSection 3.2, we introduce self-supervised losses that encourage representations to cluster based on\npairwise predicate relationships. In Section 3.3, we detail how PHIER learns hierarchical representa-\ntions by embedding representations in hyperbolic space."}, {"title": "3.1 OBJECT-CENTRIC IMAGE ENCODER", "content": "PHIER extracts an object-centric scene representation by conditioning the input image I on the\nquery. Assume the input query is Next To (cup, plate). PHIER first parses it into its objects\n0={cup, plate} and predicate P = Next To. Then, PHIER separately conditions the latent\nspace on both of these components (See Figure 2). By disentangling the full state classification query,\nwe ensure that PHIER faithfully identifies the relevant entities; it then learns to focus on their key\nfeatures for the given predicate's classification.\nObject conditioning. The goal of object conditioning is to localize regions of the image that\ncorrespond to the relevant objects in the query. This allows PHIER to focus on the objects of interest\nfor predicting the downstream predicate condition. To encode an input image I on the objects O, our\nencoder $E_{img}$ generates an object-conditioned image mask M(I, O) that highlights image regions\nbased on the relevant entities, following Zhou et al. (2022). At a high level, we extract features\nfor the image and object texts, align the image features with each object text's features to generate\nindividual object masks, and then encode the image based on these masks. Concretely, for each object\no \u2208 O, we use a CLIP image encoder V and text encoder T (Radford et al., 2021) to obtain image\nfeatures V(I) \u2208 $R^{P}$ and object text features T(0) \u2208 $R^{D}$, where $D_{1}$, $D_{2}$ correspond to the visual and\nlanguage embedding dimensions, respectively."}, {"title": "3.2 SELF-SUPERVISED LEARNING", "content": "PHIER learns a joint image-predicate space that encodes an inferred predicate hierarchy with self-\nsupervised losses. In particular, a specific predicate such as OnLeft is encouraged to be close to a\nrelated, more general predicate such as Next To, as features important to closer predicates tend to be\nmore alike. At the same time, OnLeft should have a larger norm than Next To, to reflect its lower\nposition in the hierarchy. This ensures that the features that are learned to be relevant to higher-level\npredicates remain useful when reasoning about their lower-level children.\nWe introduce two self-supervised losses to encourage such pairwise relationships: a predicate triplet\nloss based on the similarity between predicates and a norm regularization loss based on the hierarchy\nbetween predicates. For both losses, we sample triplets with unique corresponding predicates and then\nextract knowledge from an LLM to determine the underlying relationships between the predicates.\nPredicate triplet loss. Given a predicate triplet with an anchor a, positive p, and negative n sample,\nthe triplet loss encourages the distance between the anchor and negative predicate to be at least some\nmargin \u03bb greater than the distance between the anchor and positive predicate. In Figure 2, we see\nthat Next To is the anchor predicate, OnLeft is the positive sample, and Inside is the negative\nsample. The proper assignment of the samples is critical, as it directly affects how faithfully the\nmodel learns the relationship between predicate pairs. To determine the anchor, positive, and negative\nsample for any given predicate triplet, we query an LLM based on the semantic meanings and scene\ndetails described by each query. More concretely, for each triplet, we prompt the LLM to assess the\nunderlying relationships between the predicates. One predicate in the triplet is randomly chosen as\nthe anchor. The LLM is asked to determine which of the other two predicates is more similar to the\nanchor. The anchor and the more similar predicate form a positive pair, while the anchor and the less\nsimilar predicate form a negative pair. We provide the prompt template provided in Appendix D. By\nextracting knowledge from an LLM, we leverage the LLM's explicit and extensive understanding of\npredicate relationships to produce meaningful triplets and guide the model toward a semantically rich\nimage-predicate latent space.\nOur formulation of the triplet loss is based on the distance between representations:\n$L_{triplet, \\lambda}(a, p, n) := max(0, d(a,p) \u2013 d(a, n) + \\lambda)$,\nwhere d is the distance metric used in the latent space. We describe our choice of a hyperbolic\ndistance metric in the following section. With this loss and chosen distance metric, the resulting\nrepresentation space captures the similarity between predicates via their distance in the latent space."}, {"title": "3.3 HYPERBOLIC IMAGE-PREDICATE LATENT SPACE", "content": "Finally, PHIER effectively learns this inferred hierarchy of predicates through hyperbolic repre-\nsentations. While PHIER's self-supervised losses inject semantic knowledge of predicates into its\nrepresentations, PHIER further encodes the hierarchical nature of the predicates in hyperbolic space.\nIn hyperbolic space, we can more easily visualize these relationships forming a natural predicate\nhierarchy. In Figure 2, we visualize a learned predicate hierarchy in PHIER's latent space.\nHyperbolic space is a non-Euclidean space characterized by constant negative curvature, which\nallows hierarchical structure to be easily embedded. Due to hyperbolic space's curvature, the area of\na disc increases exponentially with its radius, analogous to the exponential branching of trees. This\nproperty makes hyperbolic space well-suited for modeling hierarchies, as it provides a continuous\nrepresentation of discrete trees. Furthermore, hyperbolic space is differentiable, making it easy to\nintegrate with our model. Hence, we propose using a hyperbolic distance metric for our predicate\ntriplet loss and norm regularization loss to more effectively encode the predicate hierarchy. These\nhierarchical representations enable PHIER to generalize effectively to novel predicates, by inferring\ntheir representations based on their relationships to learned predicates in the hyperbolic latent space.\nPoincar\u00e9 ball model. In this work, we use the Poincar\u00e9 ball model of hyperbolic space. The Poincar\u00e9\nball is an open d-dimensional ball of radius 1, equipped with the metric tensor $g_p = (\\lambda_d(x))^2 g_e$. Here,\n|| \u00b7 || is the Euclidean norm, $\\lambda_d(x) = \\frac{2}{1 - ||x||^2}$ is the conformal factor, and $g_e$ is the Euclidean metric\ntensor (i.e., the Euclidean dot product). This induces the distance $d_p$ between two points x, y on the\nPoincar\u00e9 ball as\n$d_p(x, y) = \\cosh^{-1}(1+2\\frac{||x - y||^2}{(1 - ||x||^2)(1 - ||y||^2)})$.\nOn the Poincar\u00e9 ball, the distance between two points captures the degree of similarity between them,\nwhile the relative norm of two points reflects their hierarchical structure. Thus, the Poincar\u00e9 ball is a\nsuitable space to represent the underlying predicate hierarchy, and PHIER's self-supervised losses\nuse such metrics to embed image-predicates onto the Poincar\u00e9 ball.\nHyperbolic encoder. To obtain the hyperbolic image-predicate representation, we use the exponential\nmap to lift the representation from Euclidean space onto the Poincar\u00e9 ball and pass it through a small\nhyperbolic linear network (Ganea et al., 2018). For more details on these functions, we refer the\nreader to Appendix C."}, {"title": "3.4 TRAINING LOSS", "content": "After the hierarchical representation is learned in hyperbolic space, we apply the logarithmic map to\nproject it back to Euclidean space, where it is passed through a small MLP for state classification.\nWe train PHIER with a binary cross entropy loss based on the ground truth labels (True or False),\nalong with the predicate triplet loss and the norm regularization loss. Our overall loss is defined\nas $L_{total} := L_{sup} + \\alpha L_{triplet,\\lambda} + \\beta L_{reg, \\gamma}$, where \u03b1, \u03b2 are coefficients that control the strength of the\ntriplet and regularization losses, and \u03bb, \u03b3 are the margins for the two losses, respectively."}, {"title": "4 DATASET", "content": "We evaluate PHIER on established robotics environments, with three state classification datasets\ndesigned to test the following key aspects of performance: a faithful understanding of entities and\nrelations between them, few-shot generalization to out-of-distribution queries, and zero- and few-shot\ntransfer to a real-world setting. See Figure 3 for examples from each of the environments.\nSimulator dataset generation. In order to evaluate our method's state classification performance on\nrobotic environments, we generate datasets of varying levels of realism with two widely used robotics\nsimulators, CALVIN (Mees et al., 2022) and BEHAVIOR (Li et al., 2023a). Both are known for their\nease of use and customizability, allowing us to generate diverse data for various states with different\nobjects under various lighting, camera angle, and object pose conditions. As shown in Figure 3,\nCALVIN is visually simplistic while BEHAVIOR is more realistic and complex. By evaluating on\ndata from these two simulators, we assess how well various methods understand the semantics of\ndifferent predicates.\nSimulator dataset details. To evaluate the effectiveness of our inferred abstraction hierarchy, we\ndefine sets of in-distribution and out-of-distribution states, featuring both unary and binary relations.\nThe out-of-distribution states involve unseen predicate-object combinations and novel predicates. See\nAppendix E for the states in each dataset. We train on a balanced dataset of 200 examples (100 True,\n100 False) for each in-distribution state. We then evaluate on balanced test sets of 50 examples for\neach state under both in-distribution and out-of-distribution settings.\nReal-world dataset. In addition, we evaluate on BEHAVIOR Vision Suite (Ge et al., 2024) (see\nFigure 3), a complex real-world benchmark that consists of diverse scenes and distractor objects.\nSpecifically, compared to our train data, this one consists of 10 unseen combinations and 10 novel\npredicates, with 337 total examples. We use this dataset to test our method's ability to perform zero-\nand few-shot real-world transfer after training on simulated datasets alone."}, {"title": "5 RESULTS", "content": "We evaluate PHIER on the three datasets and compare against 10 state-of-the art models, with our\nevaluation metric as binary state classification accuracy. Our experiments show that PHIER's learned\npredicate hierarchy leads to significantly improved performance, especially in the challenging settings\nof few-shot, out-of-distribution generalization as well as zero- and few-shot, real-world transfer."}, {"title": "5.1 IMPLEMENTATION", "content": "PHIER. For our model, we use the CLIP image encoder, CLIP text encoder, and BERT text encoder\nas our image, object text, and predicate text encoders, respectively. Our hyperbolic encoder consists\nof two hyperbolic linear layers with output dimensions of 256 and 128, and the final small MLP is\na single layer. We use a = 0.05 as our triplet loss coefficient, \u03bb = 10.0 as our triplet loss margin,\n\u03b2 = 1.0 as our regularization loss coefficient, and y = 0.1 as our regularization margin. We train\nall models for 50 epochs using the AdamW optimizer with a learning rate of 1e-4 using a gradual\nwarmup scheduler and cosine annealing decay. For the few-shot setting, we provide 5 examples of\neach novel predicate and train for 20 epochs with the same optimizer and learning rate.\nBaselines. We use 10 state-of-the-art methods as baselines, ranging from supervised methods to\npretrained large vision language models (VLMs). Of the supervised methods, RelViT (Ma et al., 2022)\nand SORNet (Yuan et al., 2022) are recent methods designed with a focus on state classification, while\nRe-Attention (Guo et al., 2020), Coarse-to-Fine (Nguyen et al., 2022), BUTD (Anderson et al., 2018),\nfinetuned CLIP (Shen et al., 2021), and FiLM (Perez et al., 2018) are top-performing general VQA\nmethods. The supervised models are all trained on the same data as PHIER, while the pretrained large\nVLMS, BLIP-2 (Li et al., 2023b), GPT-4V(OpenAI, 2023), and ViperGPT (Sur\u00eds et al., 2023), are\nrun inference-only. All methods are evaluated on both in-distribution and out-of-distribution queries,\nexcept for SORNet as its architecture does not allow classification of unseen states. Additional details\non our baselines are provided in Appendix B."}, {"title": "5.2 COMPARISON TO PRIOR WORK", "content": "Few-shot generalization accuracy. In Table 1, we show comparisons of PHIER and prior work\non CALVIN and BEHAVIOR datasets, in the 5-shot generalization setting. We split prior works\ninto trained supervised methods (T) and inference-only pretrained VLMs (I). While PHIER yields\ncomparable performance to top-performing prior works on the in-distribution test set, PHIER signifi-\ncantly outperforms all methods on the out-of-distribution test set. PHIER demonstrates a 22.5 percent"}, {"title": "5.3 ABLATIONS", "content": "We ablate the components of PHIER in Table 4. Specifically, we begin by reporting the out-of-\ndistribution results of a supervised model for state classification. We then test variants of PHIER,\nprogressively adding each component: our object-centric encoder, predicate triplet loss, norm\nregularization, and finally the full model with the hyperbolic distance metric.\nWe see that each component encourages a structured and semantically relevant latent space for\nout-of-distribution generalization. The object-centric encoder localizes relevant objects in the scene,\nimproving performance by 15.4 and 13.7 percent points on CALVIN and BEHAVIOR, respectively.\nAdding the predicate triplet loss helps PHIER encode pairwise relationships between predicates,\nimproving performance by 13.1 and 8.2 percent point on CALVIN and BEHAVIOR. The addition of\nthe norm regularization loss introduces hierarchical structure into the latent space, further improving\nperformance by 6.5 and 4.7 percent points. Finally, we highlight the full PHIER equipped with the\nhyperbolic metric, which further enforces a tree-like hierarchy to emerge in latent space and yields\nthe strongest generalization performance."}, {"title": "5.4 DISCUSSION", "content": "We propose PHIER as a framework for incorporating predicate hierarchies into the latent space of\nstate classification models. One important design decision is how explicitly the hierarchy should be\nenforced-PHIER softly encourages this structure with self-supervised losses, but does not impose\nhard constraints on the model's forward pass. We designed PHIER in this way, in order to allow\nhierarchical structure to emerge in the hyperbolic latent space based on the data, and capture more\nnuanced structure than a strict, discrete predicate hierarchy could. Quantitatively, we see that PHIER\nretains the ability to perform well on generalization tests, while qualitatively, we can visualize\nPHIER's learned predicate hierarchy in the latent space.\nWe note that PHIER is potentially limited by the accuracy of the language model in determining\npairwise predicate relations. In this paper, we assume that language itself can differentiate the\nrelationship between predicates, but there may be cases where visual cues from data also matter.\nEmpirically, on the datasets we tested, we see that the LLM's predictions match our expectations of\nwhat the predicate hierarchy should be. Additionally, as PHIER's enforcement of this hierarchy is not\nexplicit, it is still possible for PHIER to learn from data when the language model is incorrect. As\na future direction, exploring environments where the dataset for state classification yields a unique\npredicate hierarchy, which we can encode through explicit enforcement in the model's forward\npass, would showcase the effect of an explicitly hierarchical version of PHIER in generalization.\nIn addition, exploring ways of training a model to infer the pairwise predicate relations with weak\nsupervision, instead of injecting relation priors through a language model, could potentially give rise\nto a fully emergent and discovered predicate hierarchy."}, {"title": "6 CONCLUSION", "content": "PHIER tackles the challenge of few-shot out-of-distribution state classification by encoding predicate\nhierarchies into its latent space. Our proposed model, PHIER, learns language-informed image-\npredicate representations to generalize to novel predicates with few examples. Our experiments on\nCALVIN, BEHAVIOR, and a real-world test set demonstrate that PHIER significantly improves upon\nexisting methods, particularly in highly difficult generalization cases. We show that using predicate\nhierarchies is a promising approach to enable more robust and adaptable state classification."}, {"title": "A PHIER RESULTS AND DETAILS", "content": ""}, {"title": "A.1 MODEL DETAILS", "content": "PHIER's image and text encoders are initialized with pretrained CLIP (Radford et al., 2021) and\nBERT (Devlin, 2018) weights, respectively. The hyperbolic linear layers are initialized following the\napproach of Shimizu et al. (2020), with the weights drawn from a normal distribution centered at\nzero with a standard deviation (2nm)\u00af\u00bd, where m and n are the input and output sizes of the layer,\nand the biases set to the zero vector. The linear layer in the small MLP is initialized by the standard\nKaiming initialization. All of the parameters in PHIER are trainable and updated during training.\nPHIER disentangles the conditioning of the image on the full state classification query into two\ndistinct ones: one that identifies the relevant objects and another that focuses on key features for\nthe given predicate. While we use MaskCLIP to identify the relevant entities, PHIER's contribution\nlies in the decomposition of the query into object and predicate components, enabling it to faithfully\nidentify the relevant entities and extract features based on the predicate."}, {"title": "A.2 COMPARISON ON MANUALLY COLLECTED REAL-WORLD DATASET", "content": "We collect a small real-world dataset with 100 examples, consisting of 4 examples for each of the\nout-of-distribution BEHAVIOR states, to test our method's ability to perform zero-shot real-world\ntransfer after training on simulated datasets alone. See Figure 4 for examples. In Table 5, we\nobserve similar trends as in the BEHAVIOR Vision Suite evaluation in the main text, even with a\nsimpler dataset. PHIER significantly outperforms prior supervised baselines. However, as expected,\npre-trained models trained on large-scale real-world data outperform PHIER."}, {"title": "A.3 ABLATION STUDY ON EXAMPLE COUNT", "content": "We study the effect of varying the number of examples used in the few-shot setting. We added\nnew ablation experiments with 0, 1, 2, 3, 4, 5, and 10-shot generalization performance on both\nCALVIN and BEHAVIOR environments. The results in Figure 5 show that PHIER consistently\noutperforms prior works across all numbers of examples. Notably, in the CALVIN environment,\nPHIER's performance plateaus as the number of examples increases, indicating that the method\nrequires only a few examples to adapt effectively to unseen scenarios."}, {"title": "A.4 ABLATION STUDY WITH REMOVED COMPONENTS", "content": "We add an ablation study that evaluates the impact of removing individual components of PHIER to\nevaluate their contributions. We compare PHIER with four variants, (1) without the object-centric\nencoder, (2) without the hyperbolic latent space, (3) without the norm regularization loss, and (4)\nwithout the predicate triplet loss. We report results in Table 6. We see that without our object-\ncentric design, performance drops significantly in both ID and OOD settings, emphasizing the\nimportance of object-centric encoders for improved representation and reasoning. In addition, we\nshow that removing each of the self-supervised losses leads to much weaker generalization capability.\nFinally, we observe reduced generalization performance without PHIER 's hyperbolic latent space and\nhyperbolic norm regularization loss, demonstrating that the hyperbolic space facilitates better handling\nof hierarchical relationships. These results validate that each component contributes meaningfully to\nPHIER's performance, particularly in improving OOD generalization."}, {"title": "A.5 FEW-SHOT GENERALIZATION TO NOVEL OBJECTS", "content": "We expand our CALVIN and BEHAVIOR experiments to evaluate accuracy on few-shot generalization\non novel objects in Table 7. The queries with these novel objects are listed in Table 8. As in our\nexperiments on unseen combinations and novel predicates, we observe that PHIER significantly\noutperforms prior baselines on unseen objects. Specifically, PHIER improves upon the top-performing\nprior work by 21.8 percent points on CALVIN and 13.5 percent point on BEHAVIOR. These results\ndemonstrate that PHIER improves generalization to both novel objects and predicates, further\nhighlighting the benefit of our object-centric encoder and inferred predicate hierarchy."}, {"title": "A.6 VISUALIZATIONS ON THE INFERRED PREDICATE HIERARCHY", "content": "In Figure 6, we visualize the joint image-predicate space for BEHAVIOR on the Poincar\u00e9 disk,\nhighlighting the hierarchical semantic structure captured by PHIER's embeddings. By grouping the\njoint image-predicate embeddings by predicate, we uncover the inferred predicate hierarchy. For\ninstance, we see that embeddings for Next To are positioned closer to the origin compared to those\nfor OnLeft, accurately reflecting their hierarchical relationship\u2014\u2014OnLeft is a more specific case\nof Next To. Furthermore, embeddings for Touching are nearest to the origin, consistent with its\nrole as the most general predicate. For example, when one object is Inside or On Top of another,\nthey are inherently Touching. Similarly, objects that are Next To or OnLeft are also frequently\nTouching. This visualization demonstrates that PHIER captures not only semantic structure but\nalso nuanced hierarchical relationships between predicates.\nWe further analyze the embeddings for novel predicates after few-shot learning with only five\nexamples. Notably, even with such limited data, PHIER successfully integrates these novel predicates"}, {"title": "A.7 IN-DISTRIBUTION PERFORMANCE", "content": "Here, we discuss the in-distribution performance of PHIER in Table 1 of the main text. We note\nthat in the in-distribution (ID) setting of CALVIN, PHIER outperforms all prior works except Re-\nAttention, with only a small margin of 1.4%. In the out-of-distribution (OOD) setting, which is\nour primary focus, PHIER outperforms Re-Attention by a significant 22.5%. Similarly, on ID\nBEHAVIOR, PHIER performs comparably to top-performing prior works, surpassing all except\nRelViT by 0.7%; however, in the OOD setting we focus on, PHIER outperforms RelViT by 8.3%.\nWe highlight that PHIER performs comparably to top-performing prior works in the ID setting, while\nsignificantly improving the OOD performance. We focus on the few-shot generalization task and\ndesign our method to enforce bottlenecked representations (via a joint image-predicate space), while\nacknowledging that this might include tradeoffs on ID performance to avoid overfitting to the train\ndistribution.\nWe also analyze specific cases where PHIER underperforms on ID examples. For instance, in\nCALVIN, we hypothesize that PHIER may struggle with tasks that the baselines may memorize\ndue to their less constrained representations. We show an example in Figure 7, and note that for\nthe ID query, TurnedOn (lightbulb), Re-Attention correctly predicts True, while PHIER\npredicts False. However, for the out-of-distribution query, TurnedOff(lightbulb), which is\nl"}]}