{"title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features", "authors": ["Michael Tschannen", "Alexey Gritsenko", "Xiao Wang", "Muhammad Ferjad Naeem", "Ibrahim Alabdulmohsin", "Nikhil Parthasarathy", "Talfan Evans", "Lucas Beyer", "Ye Xia", "Basil Mustafa", "Olivier H\u00e9naff", "Jeremiah Harmsen", "Andreas Steiner", "Xiaohua Zhai"], "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe\u2014this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).", "sections": [{"title": "1. Introduction", "content": "Contrastive image-text embedding models trained on billion-scale datasets, as pioneered by CLIP [50] and ALIGN [28], have become the mainstream approach for high-level, semantic understanding of visual data. These models enable fine-grained, zero-shot classification rivaling the quality of supervised methods and enable efficient text-to-image and image-to-text retrieval. Furthermore, they lead to excellent vision-language understanding capabilities when combined with Large Language Models (LLMs) to build Vision-Language Models (VLMs).\nDeveloping on the success of CLIP, several improvements have been proposed such as re-captioning images [38], adding image-only self-supervised losses [38, 45], and training with a small decoder for auxiliary tasks such as caption-ing and localization [32, 62, 67]. At the same time, several groups have released model check-points for the open-source community [19, 27, 50, 57, 70]. However, these releases do not in-clude the full breadth of latest improvements into a single model, as they all relatively closely follow CLIP's original approach. Here, building on the SigLIP training recipe [71], we incorporate sev-eral improvements from prior work and release a new family of open models\u00b9 that both excel on CLIP's core capabilities\u2014zero-shot classification, retrieval, and feature extraction for VLMs\u2014and improve areas where vanilla CLIP-style models lag behind, including localization and extracting dense, semantic representations.\nIn summary, SigLIP 2 models provide the fol-lowing:\n\u2022 Strong multilingual vision-language en-coders: SigLIP 2 shows excellent perfor-mance on English-focused vision-language tasks while providing strong results on mul-tilingual benchmarks with a single model. This enables use in a wide range of languages and cultural contexts.\n\u2022 Dense features: We incorporate self-"}, {"title": "2. Training recipe", "content": "We combine the original SigLIP training recipe [71] with decoder-based pretrain-ing [60, 62], in addition to self-distillation and masked prediction as in the DINO line of work [9, 47] (see Fig. 1 for an overview). Pretraining an image encoder with a language decoder for captioning and referring expression comprehension was shown to improve OCR capabilities and localization [62], whereas self-distillation and masked prediction leads to better features for dense prediction tasks, zero-shot classification and retrieval [38, 45]. Rather than combining all these techniques in a single run we follow a staged approach as outlined below to manage the computational and memory overhead compared to SigLIP training."}, {"title": "2.1. Architecture, training data, optimizer", "content": "For the architecture, we follow SigLIP [71] so that existing users can simply swap out the encoder weights. Specifically, the fixed-resolution variant relies on the standard ViT architecture [15] with learned positional embedding. We use the same architecture for the image and text tower, except for the g-sized vision encoder which is paired with an So400m-sized [1] text encoder. Vision and text representations are pooled using a MAP head (at-"}, {"title": "2.2. Training with Sigmoid loss and decoder", "content": "In the first step of pretraining, we combine SigLIP [71] with LocCa [62] by simply combin-ing the two losses with equal weight. Unlike CLIP [50], which relies on a contrastive loss, SigLIP creates binary classification problems by combining every image embedding with every text embedding in the mini-batch and trains the embeddings to classify matching and non-matching pairs via logistic regression (sigmoid loss). We use the original implementation and refer to [71] for details.\nFor LocCa, we attach a standard transformer decoder with cross-attention to the un-pooled vision encoder representation (before applying the MAP head). The decoder follows the shapes of the text encoder except that we add cross-attention layers and reduce the number of lay-ers by a factor of two. Besides image captioning, LocCa also trains for automatic referring expres-sion prediction and grounded captioning. The former amounts to predicting bounding box co-"}, {"title": "2.3. Training with self-distillation and masked prediction", "content": "Following SILC [45] and TIPS [38], we aug-ment the training setup described in Sec. 2.2 with local-to-global correspondence learning with self-distillation and masked prediction losses [9, 47, 75] to improve the local semantics of the (un-pooled) feature representation. This repre-sentation is typically used for dense prediction tasks like segmentation, depth estimation etc. Concretely, we add two terms to the losses de-scribed in Sec. 2.2 as detailed next.\nThe first term is the local-to-global consistency loss from [45], in which the vision encoder be-comes the student network, which gets a par-tial (local) view of the training image, and is trained to match the teacher's representation, de-rived from the full image. This auxiliary matching task is performed in a high-dimensional feature space computed with a separate MLP head. As is common in the literature, the teacher parameters are obtained as an exponential moving average"}, {"title": "2.4. Adaptation to different resolutions", "content": "To obtain fixed-resolution checkpoints at multi-ple resolutions, we resume the checkpoints (with sequence length 256 and patch size 16) at 95% of training, resize the positional embedding to the target sequences length (and in some cases the patch embedding from patch size 16 to 14 with the pseudoinverse (PI)-resize strategy from [6]), and resume the training at the target resolu-tion with all losses. We opt for this approach as the common strategy of fine-tuning the final checkpoint with smaller learning rate and with-"}, {"title": "2.4.2. Variable aspect and resolution (NaFlex)", "content": "NaFlex combines ideas from FlexiViT [6], i.e. sup-porting multiple, predefined sequence lengths with a single ViT model, and NaViT [12], namely processing images at their native aspect ratio. This enables processing different types of images at appropriate resolution, e.g. using a larger res-olution to process document images, while at the same time minimizing the impact of aspect ratio distortion on certain inference tasks, e.g. on OCR.\nGiven a patch size and target sequence length, NaFlex preprocesses the data by first resizing the input image such that the height and width after resizing are multiples of the patch size, while 1) keeping the aspect ratio distortion as small as possible and 2) producing a sequence length of at most the desired target sequence length. The resulting distortion in width and height is at most $(patch\\_size-1)/width$ and $(patch\\_size-1)/height$, respectively, which tends to be small for common resolutions and as-pect ratios. Note that NaViT incurs the same type of distortion. After resizing, the image is split into a sequence of patches, and patch coordinates as well as a mask with padding information is added (to handle the case where the actual sequence length is smaller than the target length).\nTo process different sequence lengths (and as-pect ratios) with a ViT, we bilinearly resize (with anti-aliasing) the learned positional embedding to the target, non-square patch grid for the resized input image. We set the length of the learned positional embedding to 256, assuming a 16\u00d716 patch grid before resizing. When the sequence length after resizing is smaller than the target sequence length, the attention layers (including the MAP head) are masked to ignore the extra padding tokens.\nAs for the fixed-resolution, adapted variants, we start from the default checkpoints trained with the setup described in Sec. 2.2, i.e. with non-aspect preserving resize to 256px, resulting in a sequence length of 256. We take the checkpoint at 90% training completion, then switch to aspect-"}, {"title": "2.5. Distillation via active data curation", "content": "To maximize performance of the smallest fixed-resolution models (ViT-B/16 and ViT-B/32), we distill knowledge from a teacher (reference)"}, {"title": "3. Experiments and results", "content": "In Table 1 we report the performance of SigLIP 2 along with baselines on common zero-shot classi-fication (ImageNet [13] ObjectNet [4], ImageNet-v2 [53], ImageNet ReaL [5]) and image-text re-trieval benchmarks. SigLIP 2 performs better than SigLIP and other (open-weight) baselines across the board, despite supporting many languages unlike the baselines (except mSigLIP [71]). Note that DFN [19], which comes closest to SigLIP 2 on these benchmarks, uses a network fine-tuned on ImageNet, COCO, and Flickr (i.e. the main benchmarks in Table 1) as a filter to improve data quality. SigLIP 2's improvements over the base-lines are particularly significant for the B-sized models owing to distillation (Sec. 2.5). More-over, we observe the common scaling trends as a function of image resolution and model size."}, {"title": "3.1.1. NaFlex variant", "content": "Fig. 3 compares the fixed-resolution square-aspect ratio (standard) SigLIP 2 with the aspect-preserving NaFlex variant (one checkpoint for all sequence lengths) as a function of the se-quence length. In addition to the retrieval bench-marks listed in the previous section, we add a range of OCR/document/screen-focused image-text benchmarks, namely TextCaps [55], Hier-Text [36], SciCap [26] and Screen2Words [63]. The NaFlex variant outperforms the standard vari-ant on the majority of these retrieval benchmarks,"}, {"title": "3.2. SigLIP 2 as a vision encoder for VLMs", "content": "A popular use case for vision encoders like CLIP and SigLIP is to extract visual representations for VLMs [3, 7, 32, 35, 39, 48, 59]. The common paradigm combines a pretrained vision encoder with a pretrained LLM and does multimodal train-ing on a rich mixture of vision language tasks. To evaluate the performance of SigLIP 2 in this application, we develop a recipe similar to that of PaliGemma 2 [56]. Concretely, we combine"}, {"title": "3.3. Dense prediction tasks", "content": "We adopt the evaluation protocol from [38] and probe the frozen SigLIP 2 representation, either with a linear layer or with a DPT decoder [52], on six benchmarks spanning semantic segmen-tation, monocular depth estimation, and surface normal estimation (see [38, Sec. 4.1] for details on the protocol and hyper parameters). Note, we make one (necessary) change: where the original method concatenates the CLS token to each of the patch feature vectors, we concatenate the out-put embedding of the MAP head instead, as we use a MAP head instead of a CLS token. The re-sults in Table 2 indicate that SigLIP 2 outperforms several previous open, CLIP-style vision encoders, including SigLIP, often by a significant margin."}, {"title": "3.3.2. Open-vocabulary segmentation", "content": "Open-vocabulary segmentation aims to develop models that can segment any novel classes be-yond a fixed training vocabulary. Here, we evaluate SigLIP 2's performance on this task. We use Cat-Seg [11] as a framework and com-pare performance across different models as pro-posed in [45]. We train Cat-Seg on COCO-Stuff-164k [8] with 172 classes and then test it on various representative datasets with differ-ent vocabularies: ADE20k [73, 74] with 847 or 150 classes (A-847/A-150), Pascal Context (PC-459/PC-59) [43], and Pascal VOC (VOC-20/VOC-21) [17]. The results can be found in Table 3. We observe that the SigLIP 2 at L/16 improves on SigLIP and even surpasses the much bigger OpenCLIP G/14 model [27]."}, {"title": "3.4. Localization tasks", "content": "To probe the referring expression comprehension capabilities of SigLIP 2 on different RefCOCO vari-ants [29, 68] we apply the evaluation protocol from [62]. We attach a 6-layer transformer de-coder to the un-pooled, frozen vision encoder representation via cross-attention and train it from scratch on a mix of all RefCOCO variants (see [62] for details). The results in Table 5 show that SigLIP 2 outperforms SigLIP as well as CLIP and pretraining via image captioning (Cap) by a large margin, across resolutions and model sizes. This can be attributed to the decoder-based pre-training, as described in Sec. 2.2. SigLIP 2 is only outperformed LocCa, which we hypothesize"}, {"title": "3.5. Cultural diversity and fairness", "content": "Besides the improvement in model quality in SigLIP 2 compared to its predecessor, SigLIP 2 is also more inclusive in two aspects. First, we follow the recommendations of [49] and utilize a training mixture comprising both English and multilingual data to enhance cultural diversity. Second, to address potential societal biases in the training data, we integrate the data de-biasing techniques from [2]. These techniques are ap-plied to mitigate biases in both first-order statis-tics, such as disparities in gender representation, and second-order statistics, such as biased associ-ations between gender and occupation. Next, we present the evaluation results."}, {"title": "5. Conclusion", "content": "In this work, we introduced SigLIP 2, a family of open-weight multilingual vision-language en-coders that builds on the success of SigLIP. By in-corporating a combination of techniques such as decoder-based pretraining, self-supervised losses, and active data curation, SigLIP 2 achieves sig-nificant improvements in zero-shot classification, transfer performance as a vision encoder in VLMs, and in localization and dense prediction tasks. Furthermore, thanks to training on multilingual data and applying de-biasing filters, SigLIP 2 at-tains more balanced quality across culturally di-verse data. Finally, the NaFlex variant enables the model to support multiple resolutions with a sin-gle model checkpoint, while preserving the native image aspect ratio. We hope that our SigLIP 2 release will enable many exciting applications within the open-source community."}]}