{"title": "Fundus Image-based Visual Acuity Assessment with PAC-Guarantees", "authors": ["Sooyong Jang", "Kuk Jin Jang", "Hyonyoung Choi", "Yong-Seop Han", "Seongjin Lee", "Jin-hyun Kim", "Insup Lee"], "abstract": "Timely detection and treatment are essential for maintaining eye health. Visual acuity (VA), which measures the clarity of vision at a distance, is a crucial metric for managing eye health. Machine learning (ML) techniques have been introduced to assist in VA measurement, potentially alleviating clinicians' workloads. However, the inherent uncertainties in ML models make relying solely on them for VA prediction less than ideal. The VA prediction task involves multiple sources of uncertainty, requiring more robust approaches. A promising method is to build prediction sets or intervals rather than point estimates, offering coverage guarantees through techniques like conformal prediction and Probably Approximately Correct (PAC) prediction sets. Despite the potential, to date, these approaches have not been applied to the VA prediction task. To address this, we propose a method for deriving prediction intervals for estimating visual acuity from fundus images with a PAC guarantee. Our experimental results demonstrate that the PAC guarantees are upheld, with performance comparable to or better than that of two prior works that do not provide such guarantees.", "sections": [{"title": "1. Introduction", "content": "In eye health management, early detection and timely treatment are crucial. Deep learning-based models have the potential to support large-scale screening programs, helping to detect abnormalities, support clinicians, and enable earlier diagnosis for individuals. However, the inherent uncertainty in machine learning predictions challenges the effectiveness of such models in safety-critical applications like large-scale screening and clinician decision-support systems.\nA common approach to addressing uncertainty is to predict a set (or interval) of labels with a Probably Approximately Correct (PAC) guarantee."}, {"title": "2. Related Work", "content": "In this section, we review the relevant literature. We begin by discussing two key studies focused on visual acuity prediction from fundus images. Following this, we discuss prior work on prediction guarantees, with an emphasis on conformal prediction and PAC prediction sets."}, {"title": "2.1. VA prediction from fundus images", "content": "The Visual acuity (VA) prediction task is to estimate VA from fundus images. VA can be represented in various ways, such as a decimal number (e.g., 0.5, 1.0), a fraction (e.g., 20/40, 20/20), or a letter score (e.g., 60, 35).\nThe VA prediction has been explored in two recent studies. In, the problem was formulated as a classification task. The original eleven visual acuity levels (ranging from 0.0 to 1.0 in increments of 0.1) were mapped into four levels based on an ophthalmologist's guidelines. They then employ a two-stage approach, with different classifiers at each stage. In contrast, Paul et al. (2023) treat the problem as a regression task, leveraging the ordinality of visual acuity labels to predict visual acuity letter scores using various models such as ResNet50, ConvNeXt, Efficient NetV2, and Swin Transformers. Both approaches demonstrate strong performance in their evaluations. However, neither study provides any guarantees on the reliability of their results."}, {"title": "2.2. Conformal Prediction and PAC Guarantees", "content": "One approach to providing guarantees on model predictions is to construct prediction intervals that, with high probability, include the true label. This can be achieved using methods such as Conformal Prediction and PAC Prediction sets . The two methods are similar, but offer different types of guarantees. Conformal prediction generates a prediction interval that contains the ground truth for a test data point with high probability. In contrast, the PAC prediction set offers guarantees conditioned on the training data.\nIn VA prediction, an important consideration when providing guarantees on prediction intervals is the need for varying interval widths depending on the difficulty of each example. Ideally, easy examples (for the model) should have narrow prediction intervals, while more difficult examples should have wider intervals. There has been extensive research in conformal prediction to address this need, using techniques such as scalar estimated uncertainty and locally adaptive conformal prediction. These approaches has been employed across various machine learning applications.\nWhen applying these techniques to the VA prediction task, a critical aspect is constructing intervals that have clinically useful widths. Specifically, the interval width must be narrow enough to be practical for VA prediction tasks. Although these techniques aim to minimize the interval width (or set size) while ensuring coverage, they do not address the practical requirements for interval width in clinical settings."}, {"title": "3. Method", "content": "Our approach first trains a regression model for predicting VA from fundus images. The output is used to derive a prediction interval of the estimate with PAC guarantees. The overall process is illustrated in Figure 2."}, {"title": "3.1. Background - PAC Prediction Interval", "content": "The PAC prediction interval (or set) constructs a prediction interval, C(x) with a Probably Approximately Correct (PAC) guarantee, as represented in Equation (1). The PAC guarantee implies that the prediction error is small (\"Approximately Correct\", described by the inner probability with e), and holds with a high probability (\"Probably\", represented by the outer probability with d) as long as the test data follows the same distribution D as the training and calibration data. In our context, \"Approximately\" means that the prediction interval includes the true visual acuity with a coverage rate of at least 1 \u2013 \u0454, while \"Probably\" indicates that this coverage rate bound generally holds as long as all data\u2014training, calibration, and test-are drawn from the same distribution, D."}, {"title": "3.2. Regression model learning", "content": "Let X be the set of fundus images. We train a model f: X \u2192 R\u00b2, to predict visual acuity for a given fundus image. We model the VA prediction with a Gaussian distribution, where the mean represents the predicted visual acuity, and the standard deviation indicates the uncertainty of the prediction. The two model output values correspond to the mean (\u03bc\u2208R) and standard deviation (\u03c3\u2208 R>0). That is, for the ith fundus image xi, f(xi) = (f\u00b5(xi), fo(xi)), where fu(xi) and fo(xi) predict the mean and standard deviation, respectively.\nTo train this model, we use the Negative Log Likelihood (NLL) loss. The standard deviation represents the prediction uncertainty and will be used to construct the prediction intervals, as illustrated in the following section."}, {"title": "3.3. Prediction Interval with PAC guarantee", "content": "From the output, we build a prediction interval for VA, C(xi) = [C1, Cu], that should contain the true value with high probability, i.e., Yi \u2208 C(xi). Our goal is to derive the prediction interval that contains the ground truth with the minimum width, i.e., high coverage and narrow width, that satisfies the following PAC guarantee (Equation (1)).\nWith a constant c which controls the prediction interval width satisfying Equation (1), we derive a prediction interval Cc(xi) for (xi, Yi),\nCc(xi) = [Cc,l(Xi), Cc,u(Xi)],\nwhere Cc,l(xi) = f(xi) - c \u00d7 fo (xi), Cc,u(xi) = fu(xi) + c \u00d7 fo(xi), with a constant c which satisfies Equation (1). Our interval uses the estimated standard deviation (fr(\u00b7)) for each example, allowing each example to have a different prediction interval based on its estimated uncertainty (standard deviation).\nWe determine c by solving the following optimization problem as described in:\nc* = arg min c subject to c \u2265 1 \u2212 6,\nC\nwhere [c,c] is the Clopper-Pearson interval for W = {1(Yi \u2208 Cc (xi)) | (xi, Yi) \u2208 Z} with the significance level d. The prediction interval Ce* satisfies the PAC guarantee."}, {"title": "4. Experiment", "content": "To evaluate our approach, we train models according to Section 3.2 and compute prediction intervals satisfying the PAC guarantee. The experiments are repeated five times with different seeds, and we report the mean and standard deviations across the repetitions."}, {"title": "4.1. Dataset", "content": "For training and evaluation, we use a fundus dataset obtained from existing work. The dataset consists of 54,781 fundus images labeled with visual acuity levels ranging from 0.0 to 1.0 in increments of 0.1, which were obtained through a visual acuity assessment. The dataset uses categorized integer values from 0 to 10 (0, 1, 2, ..., 10) to represent visual acuity levels ranging from 0.0 to 1.0, considering visual acuity prediction as a classification task. We utilize these categorized labels for our regression task. The details of the data distribution and related discussion are provided in Appendix A.\nThe dataset is randomly divided into training, validation, and test sets in a 6:2:2 ratio. To evaluate the robustness, we create five different dataset splits of the dataset using different random seeds."}, {"title": "4.2. Models", "content": "We employ four different base models: Simple-CNN, ResNet18, ResNet50, and Efficient NetV2-S. All models have the final fully connected layer with two output nodes for the two Gaussian distribution parameters. The Simple-CNN model that we implemented comprises two convolutional layers followed by three fully connected layers. All layers use ReLU activation functions, except for the final layer. For the other models, we use pre-implemented versions available in PyTorch, modifying the final output layers to match the required dimensions. The ResNet and Efficient Net models are initialized with pre-trained weights provided by PyTorch, while the Simple-CNN is randomly initialized. After the initialization, the models are trained using NLL loss as described in Section 3."}, {"title": "4.3. Results", "content": "First, we present the performance of our regression model and with respect to the prediction intervals. In the subsequent section, we compare our results to prior work."}, {"title": "4.3.1. REGRESSION MODEL", "content": "The mean absolute error (MAE) of point predictions is displayed in Figure 3. Based on the result, Efficient NetV2-S shows the best result with an average of 1.54, and the Simple-CNN shows the worst with an average of 1.88. While these MAEs might be acceptable considering the inherent inaccuracies in the (manual) assessment of visual acuity, it is important to note that the error can vary significantly for individual subjects because the models do not provide any guarantees."}, {"title": "4.3.2. PAC PREDICTION INTERVALS", "content": "We compute a prediction interval with PAC guarantee according to Section 3. We utilize a range of \u20ac \u2208 [0.2,0.3,0.4] for the coverage guarantee (i.e. Coverage bound: 1 \u2013 6) with a significance level d= 0.001%, and compute the coverage and the average interval width as shown in Figure 4. Our method consistently satisfies the coverage bound for all e values. All models show higher coverage than the given bound for all configurations.\nBased on the results, Efficient NetV2-S generates the narrowest prediction intervals, while Simple-CNN produces the widest. This variation may be partially due to differences in model complexity. More complex models, like Efficient NetV2-S, tend to learn intricate patterns in the data, allowing them to make more confident predictions with lower estimated standard deviation. As a result, these models produce narrower intervals. In contrast, simpler models, such as Simple-CNN, may have difficulty capturing such patterns, leading to greater uncertainty and wider prediction intervals.\nSpecifically, the average width for Efficient NetV2-S is around 3.0, when coverage rate bound is 70% (\u20ac = 0.3). However, for practical usage with this 70% coverage, we require a slightly narrower width, around 2, which aligns with the variability in VA measurement by humans."}, {"title": "4.4. Comparison to additional baselines", "content": "We implement a Bayesian Neural Network (BNN) for ResNet18 and ResNet50 based on the code from the repository and vanilla conformal prediction as other baselines.\nFirst, we compare the BNN performance for the point estimates with models from Section 3.2. We compute the macro-averaged MAE to account for the dataset imbalance and compare the point estimate performance between our base model and the BNNs. This metric represents the average MAE for each ground truth value. As shown in Table 1, both BNN versions show similar point estimate performance compared to their deterministic counterparts. However, it is essential to note that the BNN approach does not provide a probabilistic coverage guarantee like our method. To compare prediction interval performance, we perform Monte Carlo sampling to generate multiple predictions and compute the interval based on the 0.5 and 99.5 quantiles. This enables us to then calculate coverage and interval widths. Since intervals from the BNN model do not ensure a coverage rate bound, the empirical coverage rate is not relevant to the expected quantile percentage (99%).\nWe also implement a vanilla conformal prediction (VCP) with the nonconformity score, \u0177i - Yi. The results are presented in Table 2. BNN interval widths are comparable to those of our models; however, they do not provide the guaranteed coverage. VCP, designed to ensure coverage, does not always exceed the target rate of 70% due to small fluctuations, which is expected. While the average interval width is comparable to our model's, this approach produces the same interval width for all examples. In contrast, our method provides adaptive widths based on the estimated standard deviation."}, {"title": "4.5. Comparisons to previous studies", "content": "We compare our approach with other methods for estimating visual acuity, as described in. These two approaches differ in their label scheme and the dataset, making a fair comparison challenging. Nevertheless, we have made every effort to ensure the most accurate comparison possible."}, {"title": "4.5.1. COMPARISON WITH (KIM ET AL., 2022)", "content": "There are four main differences in the settings, a) Class scheme (4-level vs. 11-level), b) Dataset split (balanced test-set vs. imbalanced test-set), c) Prediction Type (Point vs. Interval), and d) Repetition (single experiments vs. repeated experiments). Considering the differences, we first map the visual acuity levels into 4 categories, as described by the authors in their paper (see Table 3). Next, we compute the macro-average accuracy (MA-ACC) of our prediction intervals on our imbalanced test set and compare it with the accuracy of their point predictions on the balanced test set. For our MA-ACC calculation, we consider the interval correct when it contains a true label. In addition, we set e = 0.2 for our intervals as per their final accuracy (82.4%).\nThe result is shown in Table 4. Our MA-ACC of prediction intervals is comparable with with a width of around 4.37. In addition to comparable performance, we have the advantage of providing a guarantee on the coverage. However, we note that a fair comparison is challenging due to differences in settings.\nIt should be noted that our MA-ACC in the table may be below 80% (1 - \u20ac), because the guarantee is not targeted for this metric. This metric is computed by macro-averaging classwise accuracy for the comparison to the, while the prediction interval is computed to satisfy the coverage bound on the whole dataset. Because of this difference, some of the computed MA-ACC in the table can be lower than the guaranteed coverage. However, an appropriate interval could be computed if targeting the same metric."}, {"title": "4.5.2. COMPARISON WITH (PAUL ET AL., 2023)", "content": "We note two major differences from our settings in terms of a) Dataset and b) Visual Acuity Score System (Letter score vs. Fraction of Snellen chart). Although the datasets are different, we compare the two results based on the error distributions reported in their paper. In, they present the distribution of errors by computing the percentage of test examples that fall into three error ranges in the VA letter score: between 0 and 5 ([0, 5]), between 6 and 10 ([6, 10]), and greater than 10 ([11,]). We also compute the errors of our predictions using the same VA letter score and compare the error distributions across these ranges. The difference in visual acuity scores was addressed using the conversion formula.\nAs shown in Table 5, our models have more examples in the small error range([0,5]) compared to the model in. Again, based on these results, our model is at least comparable to or better than the model presented in while providing coverage guarantees."}, {"title": "4.6. Discussion", "content": "As shown in Section 4.3.2, our results indicate that the more complex model (Efficient NetV2-S) demonstrates better accuracy and produces narrower prediction intervals compared to less complex models. This suggests that more complex models may offer improved performance in terms of prediction interval width. We believe that models like EfficientNetV2-L and RETFound are promising candidates for future work due to their higher complexity. In particular, as RETFound is a foundation model specifically designed for retinal images, it may be an especially suitable option.\nAnother point to improve the prediction intervals in terms of the their width narrower width, we examine the estimated standard deviation (fo(xi)) of examples as our prediction interval width is derived from the standard deviations (|Cc(xi)| = 2x cx fo(xi)). If the estimated standard deviation for each example are lower, we can achieve narrower prediction intervals.\nThe figure reveals a positive correlation between prediction error and standard deviation, indicating that examples with lower errors has the lower standard deviations. This observation suggests that for the subset of examples where the given model performs accurately, lower standard deviations can be achieved, resulting in narrower prediction intervals, because the prediction intervals are based on the predicted standard deviation. It may be possible to identify such subsets of examples with low prediction errors, enabling us to provide narrower prediction intervals for the subset.\nOne significant challenge in applying Conformal Prediction and PAC based approaches in practice is the potential violation of their distributional assumptions. Specifically, the training (or calibration) data and test data may come from different distributions, a phenomenon known as dataset shift. To address this, several strategies have been proposed. Detection algorithms can be employed to identify such shifts, enabling model retraining. Alternatively, adaptation algorithms can adjust the model to account for the shifted distribution. These approaches can be applied to the visual acuity prediction problem. For instance, the adaptation algorithms in PAC prediction sets could be incorporated into our approach.\nPotential applications. Our approach can be extended to applications beyond visual acuity prediction. It can be applied to any regression task. Additionally, since the PAC interval can be used in classification tasks, this approach can be adapted for tasks such as diabetic retinopathy detection and glaucoma detection. In such classification tasks, the predictions will be sets of labels rather than intervals. For both regression and classification tasks, we can provide PAC guarantees for coverage.\nEthical Considerations. Guidelines for this approach can be established, such as specifying the appropriate interval width based on the variability in traditional visual acuity measurements (around 2 within the 0-10 label range). If the predicted interval exceeds this width, its clinical usefulness may be diminished. Furthermore, previous studies have demonstrated that prediction intervals with probabilistic guarantees help clinicians make more accurate decisions. By engaging in meaningful discussions with clinicians about our approach, we believe that the effectiveness of our methods can be further maximized.\nAdditional Analysis. We perform qualitative and robustness analyses in Appendix B. We visualize samples categorized by accurate vs. incorrect predictions and narrow vs. wide intervals, along with their activation maps. We also show that our algorithm remains robust under mild blurring conditions."}, {"title": "5. Conclusion", "content": "In this work, we developed a regression model that predicts visual acuity, modeled as a Gaussian distribution. From the output, our approach derives prediction intervals with a PAC guarantee on coverage while varying the prediction interval widths. Our empirical results demonstrate that the approach meets the probabilistic guarantee, with an average interval width of 3.04 for a coverage bound of e = 0.3. These outcomes are comparable to, or surpass, those of prior studies. Importantly, our method provides a PAC guarantee on coverage, which is particularly advantageous for clinical applications - an aspect previously unaddressed. This represents a significant step toward creating more trustworthy models for visual acuity prediction.\nFuture work could further enhance these models by: 1) employing more advanced base architectures, as our findings suggest that increased model complexity yields lower errors, and 2) leveraging the relationship between prediction error and estimated standard deviation, as discussed in Section 4.6. These future directions have the potential to lead to more accurate and clinically useful prediction intervals."}, {"title": "Appendix A. Dataset Imbalance", "content": "The distribution of the data across each class is provided in Table A.6, and the dataset is imbalanced, with most of the data corresponding to good visual acuity (label = 9 or 10).\nAlthough the dataset exhibits class imbalance, we did not apply oversampling or weighting techniques, as the PAC coverage still holds. The trade-off is that the intervals may be wider for minority classes due to the model's lower performance on these classes. For comparison, we apply a resampling technique based on class frequency and evaluate the coverage rate and interval width of the EfficientNetV2-S model against those achieved with the vanilla technique. As shown in Table A.7, we do not observe significant improvements from resampling and therefore it is not included in the final version."}, {"title": "Appendix B. Additional Experiment Results", "content": "We conduct a qualitative analysis of our model's results, specifically sampling images where the model's prediction interval either contains the ground truth or does not, as well as images with both wide and narrow intervals. Additionally, we apply EigenGradCAM to analyze the model's prediction behavior.\nAdditionally, we plot the activation maps from the neural network layers for deeper analysis.\nFigure B.7 shows activation maps generated by EigenGradCAM for the sample images drawn in the previous figures. These maps indicate that the model mainly focuses on the macula region. Furthermore, when the model evaluates a wider region, the prediction interval tends to include the ground truth, and the intervals are wider.\nWe conduct experiments with multiple repetitions using different dataset splits based on various seeds, reporting the average and standard deviation. We believe that this experimental setup demonstrates the robustness of our approach across different participants. Additionally, the PAC interval algorithm provides a conditional guarantee based on the training data (as indicated in Equation (1), where the calibration set Zn is sampled), ensuring coverage regardless of the calibration set.\nIn terms of image quality, our approach may yield wider intervals for low-quality images, which might not be clinically helpful. Achieving accurate predictions in such cases is quite challenging. However, we believe this issue can be addressed by utilizing automatic image quality detection algorithms ."}]}