{"title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge", "authors": ["Ravi Raju", "Swayambhoo Jain", "Bo Li", "Jonathan Li", "Urmish Thakkar"], "abstract": "Large Language Models (LLMs) have revolutionized the landscape of machine learning, yet current benchmarks often fall short in capturing the diverse behavior of these models in real-world applications. A benchmark's usefulness is determined by its ability to clearly differentiate between models of varying capabilities (separability) and closely align with human preferences. Existing frameworks like Alpaca-Eval 2.0 LC (Dubois et al., 2024a) and Arena-Hard v0.1 (Li et al., 2024a) are limited by their focus on general-purpose queries and lack of diversity across domains such as law, medicine, and multilingual contexts. In this paper, we address these limitations by introducing a novel data pipeline that curates diverse, domain-specific evaluation sets tailored for LLM-as-a-Judge frameworks. Our approach leverages a combination of manual curation, semi-supervised learning to generate clusters, and stratified sampling to ensure balanced representation across a wide range of domains and languages. The resulting evaluation set, which includes 1573 samples across 14 categories, demonstrates high separability (84%) across ten top-ranked models, and agreement (84%) with Chat-bot Arena and (0.915) Spearman correlation. The agreement values are 9% better than Arena Hard and 20% better than AlpacaEval 2.0 LC, while the Spearman coefficient is 0.7 more than the next best benchmark, showcasing a significant improvement in the usefulness of the benchmark. We further provide an open-source evaluation tool that enables fine-grained analysis of model performance across user-defined categories, offering valuable insights for practitioners. This work contributes to the ongoing effort to enhance the transparency, diversity, and effectiveness of LLM evaluation methodologies.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have dramatically changed the landscape of machine learning research and have been incorporated in products for the past few years. Along with their rise, a multitude of benchmarks and frameworks (Liang et al., 2023) have been proposed to assess the capabilities of LLMs which include knowledge tasks such as MMLU (Hendrycks et al., 2021a), reasoning tasks like GSM8k (Cobbe et al., 2021) and more standard NLP tasks (Zellers et al., 2019; Narayan et al., 2018). However, these benchmarks fail to capture the behavior that a user experiences in a chat/generative applications. Typically, human evaluations are seen as a gold standard to determining which LLM responses are preferable over others in a chat setting but is time-consuming and expensive to conduct (Chiang et al., 2024).\nTo address this shortcoming, Zheng et al. introduced the concept of LLM as a judge as an automatic evaluator alternative, which uses another LLM the judging of model completions to another LLM such as GPT-4 or GPT-40 (Zheng et al., 2023b; OpenAI et al., 2024). Alpaca-Eval is another benchmark designed under the paradigm of LLM as an evaluator where a target LLM's completions are compared against a reference LLM's output (the default being GPT-4 Turbo) and assigned a winrate against the reference (Li et al., 2023). It has seen widespread adoption since it is cheap, fast, and mitigates length bias (Chiang et al., 2024). Similarly, Arena-Hard v0.1 is recent benchmark which fo-"}, {"title": "2. Related Work", "content": "At their core, benchmarks are tool to estimate LLM capabilities. There are many different flavors of benchmarks, spanning either across domains or various tasks. Some popular benchmarks include: Boolq (Clark et al., 2019), MMLU (Hendrycks et al., 2021a), GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), XSUM (Narayan et al., 2018), Hellaswag (Zellers et al., 2019), and MGSM (Shi et al., 2022). An expanded framework of static benchmark is AutoBencher which automatically creates new benchmarks which finds holes in knowledge of current SOTA LLMs (Li et al., 2024b).\nThese types of benchmarks have ground-truth references and compare how closely the LLM's completion aligns with those references. An inherent limitation with static benchmarks is that they are hosted on the internet and thus are susceptible test leakage contamination (Sainz et al., 2023; Yang et al., 2023). The other style of benchmarking relies on constructing a human evaluation trials on a set of evaluation prompts. Due to the expensive nature of human evaluation, a recent, cheaper alternative is to use SOTA LLMs to evaluate model completions either through single score or pairwise comparison with a reference answer, popularly referred to as LLM-as-a-Judge (Li et al., 2023; Zheng et al., 2023b; Li et al., 2024a; Dubois et al., 2024b; Verga et al., 2024).\nThis motivates the need for \"live, refreshable\" benchmarks so that the integrity of the benchmark can be maintained. LiveBench is a framework which sources data from arXiv papers, news articles, and datasets to periodically replace the stale prompts (White et al., 2024). Chatbot arena is an open platform that allows online users to send prompts to two different models and compare/contrast the models' response (Chiang et al., 2024). Users can then vote on which completion was superior. Other live benchmarks include DynaBench (Kiela et al., 2021), LiveCodeBench (Jain et al., 2024), and R2E (Jain et al.). Our work lies in the intersection between LLM-as-a-Judge and live benchmarks as our data pipeline enables periodic refreshing of the evaluation set from existing clusters. Furthermore, our data pipeline is fairly general as it can consume a variety of diverse datasets (relative to Arena-Hard v0.1 and Alpaca-Eval), consists of using open-source models, and is flexible enough to work on the user's desired data."}, {"title": "3. Methodology", "content": "In this section, we describe our approach to creating novel evaluation set using LLM-as-a judge. We enumerate the datasets that we source from to create our unlabeled corpus and subsequently describe our data pipeline for generating the evaluation set."}, {"title": "3.1. Data Sources", "content": "We use data sources from a variety of source to ensure we cover a variety of domains as well as languages. The domains we target can be broadly classified as the following: medical, law, finance, mathematics and coding. The languages we cover are standard but also more esoteric: Japanese (ja), Arabic (ar), Thai (th), Hungarian (hu), Russian (ru), Serbian (sr), Slovenian (sl), and Turkish (tr). Prompts that don't neatly fit into these groups fall into a catch-all general category. A complete list of all the data we use can be found in Table 4 in the Appendix."}, {"title": "3.2. Data pipeline", "content": "Our data pipeline can be divided into 3 distinct steps, as shown in Figure 5. We first take the data corpus and use an embedding model to generate their corresponding embedding. Each embedding encapsulates some level of semantic understanding of its associated prompt, and nearby embeddings typically encode similar semantic information.\nTo generate the labels for the unlabeled data, we take inspiration from semi-supervised learning (Hady & Schwenker, 2013). We manually define a set of categories, curate a seed set of prompts which fall into those categories (assigning"}, {"title": "4. Experimental Setup", "content": "In this section, we discuss finer details about the data pipeline we mentioned in the prior section. experimental setup on a set of ten highly rated models\u00b9 as well as defining the metrics which determine the quality of the benchmark."}, {"title": "4.1. Data pipeline details", "content": "For the data pipeline, we use semi-supervised learning via a k-NN classifier. We consider 13 categories comprising of domains: finance, law, medical, maths, coding and languages: Arabic, Russian, Serbian, Hungarian, Japanese,Thai and Slovenian. We follow usual supervised training and via hyperparameter sweep over validation set yield k = 40 as the best value of k.\nTo generate the embeddings of the unlabeled data collected, we use the e5-mistral-7b-instruct embedding model(Wang et al., 2024) for its strong performance on the Massive Text Embedding Benchmark (MTEB) Leaderboard (Muennighoff et al., 2022) and multilingual capability. If the k-NN encounters a sample which it is not familiar with or uncertain to label, we want those samples to be classified as general prompts. We use entropy of k-NN classifier probabilities of various categories for a given prompt as the measure of uncertainty. If entropy if too high entropy of the output of the classifier is too high, we bucket the sample into the default/general category (Settles, 2010). We set the entropy threshold to be 1.5 based on careful error analysis on the validation set.\nAfter labeling with k-NN, we conducted stratified sampling within each cluster, selecting 100 samples for curation. We then filtered out excessively long prompts (longer than 5000 words) that could overwhelm the judge's context window. Additionally, we reviewed the remaining prompts to eliminate those that were nonsensical or of low quality. During the evaluation, we observed that categories with a small number of examples had a significant impact on the category's win rate. The inherent variability of the LLM-as-a-Judge evaluation, even with a fixed random seed and temperature set to 0.0, made it challenging to discern which model performed better in those categories. To mitigate this uncertainty, we ensured that any category with fewer than 90-100 examples was supplemented with additional data, enabling us to obtain meaningful and interpretable results. Our final evaluation set comprises 1573 examples."}, {"title": "4.2. LLM-as-a-Judge Details", "content": "We follow a similar scoring setup as Arena-Hard (Li et al., 2024a) and Alpaca-Eval (Dubois et al., 2024a) where we use GPT-40 as a judge model and GPT-40 as a reference model as well. For each model we want to test, we obtain the completions and ask GPT-40 to record which model responses is better for the input prompt. In order to mitigate positional bias, we swap the completions between the model we are evaluating and the reference on a coin flip.\nFor the judge prompt, we used the default prompt from the MT-Bench work with one notable change (Zheng et al., 2023b). When we evaluated multilingual prompts with LLM-as-a-judge, the judge at times incorrectly awards wins to models which don't necessarily follow instructions. Given the sentence \"Please respond 'How does the economy work?' in Hungarian,\" two models might respond differently: 1) one provides a detailed English response with bulleted lists, while 2) the other responds concisely in Hungarian. The judge model will rate the model answering in the incorrect language higher, which is clearly not a measure of the model's multilingual capability (Marchisio et al., 2024). In order to reduce these incorrect decisions, we modified the judge prompt to specifically penalize responses that respond to the prompt in the incorrect language.\nIn addition to issues with multilingual queries, we also note specifically for coding that GPT-4o seems to prefer models which provide detailed explanations to the code even if the code provided is of lower quality compared to a model which has better code quality but is not as verbose. This leads to scenarios where models that have chat but lower benchmark performance (e.g. HumanEval (Chen et al., 2021)) obtain higher winrate than models which are objectively better on coding prompts. To circumvent this issue, we explicitly prompt GPT-40 that it should focus on the correctness of the response as opposed to the style of the response. Our judge template can be found in the Appendix."}, {"title": "4.3. Obtaining Confidence Intervals", "content": "We follow the setup outlined in Li et. al (Li et al., 2024a; Chiang et al., 2024). We use the Bradley-Terry model in order to model the preference distribution between models on the leaderboard and the reference model (GPT-40 in our case). We aggregate preference pairs between models and perform 100 rounds of bootstrapping to obtain 95% confidence intervals for each model ranking.\nWe conduct the same analysis with annotations, denoting for each prompt which model response was preferred, from the Alpaca-Eval repo to obtain mean ELO rankings and 95% confidence intervals according to their leaderboard. Since similar artifacts (model preference comparisons) are not updated on Arena-Hard v0.1, we take the model winrates (ELO scores not listed) and 95% confidence intervals from their repo. For Chatbot Arena, we do the same thing and took model winrates/ELO scores as well as the confidence intervals from the website as a source of ground truth."}, {"title": "4.4. Metrics", "content": "There are four different metrics we use to judge the efficacy of a benchmark. The first of these is Spearman's correlation coefficient, which measures the rankings order between the two benchmarks. The other metrics are: separability, agreement with Confidence Interval (CI), and Brier Score. Separability refers to how well the benchmark can separate various models with high confidence. In particular, if on benchmark A model M1 has a higher ELO/winrate than model M2 and CM refers to the confidence intervals of model M, S is a binary variable indicating if benchmark A is able to separate\nbetween model M1 and M2, S = 1 if CM1 \u2229 CM2= \u2205, and S=0 otherwise. The separability is then calculated as a ratio over all possible model pairs. Agreement with CI measures how well benchmarks A and B confidently distinguish between two models with the same ordering. The Brier Score evaluates an LLM benchmark's ability to predict the ranking of a pair of competing models, rewarding confidence in accurate predictions and penalizing confidence in incorrect ones. More details behind these metrics can be found in (Li et al., 2024a). Ultimately, we want our benchmark to align with Chatbot Arena as that is seen as an oracle for modeling human preferences."}, {"title": "5. Results", "content": "Our main results can be found in Table 1. With the exception of Chatbot Arena, our benchmark's separability is 84.4% compared to other baselines like Arena-Hard v0.1 (80%) and Alpaca-Eval 2.0 LC (73.33%), which shows that our benchmark can better differentiate amongst different models.\nOne interesting datapoint regarding separability is Chatbot Arena's score of 100% which may be attributed to a combination of two factors: 1) Chatbot Arena has more battles than any of the benchmarks listed in Table 1 and 2) Chatbot Arena includes battles between many different models rather than fixing a reference model like the other benchmarks. By providing the Bradley-Terry model bootstrapping process with more varied battles, Chatbot Arena is able to produce tighter confidence intervals, suggesting a future avenue for investigation is whether confidence estimation should include multiple reference answers during judging to more closely simulate Chatbot Arena.\nOur benchmark showed an 84.44% agreement with CI with respect to Chatbot Arena, which is higher than Arena-Hard v0.1's 75.50% and Alpaca-Eval 2.0 LC's 64.44%. This demonstrates that our benchmark has higher alignment with respect to Chatbot Arena which is supposed to be approximation of human preferences. In addition, our benchmark has a Spearman's correlation coefficient of 0.915, indicating a strong correlation in rankings order compared to Alpaca-Eval 2.0 LC's 0.2969. While our leaderboard ranking consists of 10 models, the pool of models we have included"}, {"title": "5.1. Separability, Agreement with CI (95%), Pair Brier Score", "content": ""}, {"title": "5.2. Diversity", "content": "Due to our data sources being quite diverse rather than simply just ChatBot Arena (Chiang et al., 2024), we are able to have more diversity in our evaluation set. To demonstrate this, we label Arena-Hard v0.1 with our kNN model using the entropy threshold to get a distribution of categories in that evaluation set. As shown in Figure 3, there is an over-representation of coding prompts, which comes from a byproduct of their data pipeline filtering for the hardest, highest quality which skews towards coding. Similarly, Alpaca-Eval's prompt distribution shown in Figure 2 demonstrates that there is a large emphasis on general chat queries, along with some coding and math prompts while medical and law prompts are relatively underrepresented.\nOur evaluation set breakdown in Figure 6(a) which covers more domains than the baseline, such languages like Arabic, Japanese, Hungarian and more. The close to equal distribution amongst the categories is likely due to the effect stratified sampling. We compare how our evaluation set category breakdown compared with LM-SYS Conversations (using our k-NN labeling approach) (Zheng et al., 2023a) in Figure 6(b), which is a snapshot of cleaned Chatbot Arena conversations from April to June 2023. In Figure 6(b), \"Other\" refers to the languages our k-NN classifier recognizes but groups them together collectively. We note that this distribution looks similar to Alpaca-Eval and the general category may contain additional languages not recognized by the classifier so it may have exceeded the entropy threshold."}, {"title": "5.3. Category Separability", "content": "Due to our unique ability to categorize the prompts, we can compute category separability for all the various categories in our evaluation set. Across 14 different categories, we do the same bootstrapping procedure on the category data to obtain the mean winrate/ELO and 95% CI, shown in Table 2. In general, there is a drop in separability when we look both"}, {"title": "5.4. Using different judges", "content": "We conduct an ablation of judge models on our evaluation, as we want to understand the effect of judge models on separability, Agreement with CI (95%) and Brier Score. We consider GPT-40 mini as one of the judges to be a small-closed source foil to GPT-40. The other judges that we consider are open source models such as: Llama 3.1 405B instruct (using SambaNova's developer API)\u2074 and Llama 3.1 70B Instruct-Turbo. We follow the same setup as gpt-40 with these other judge models.\nOur results are shown in Table 3. In terms of separability, GPT-40-mini and 405B get 82.2% and 70B get 84% separability, comparable to GPT-40's separability. 405B and GPT-40-mini attain similar Agreement with CI (95%) close to 76% while 70B is almost 10 points lower; GPT-40 is the clear winner having the highest agreement with CI (95%). With the exception of 70B, all models get similar Brier Scores indicating that the Bradley-Terry models used to generate the rankings on confidence intervals for each judge are similarly confident. 70B's high Brier score (relative to other judges), in addition to Agreement with CI, indicates that it poor judge than the other listed in Table 3.\nThe Spearman's correlation coefficient (with respect to ChatBot Arena rankings) seems to indicate that GPT-40-mini, Llama 3.1 405B, and 70B are poor judges getting a correlation of only 0.0787 vs. GPT-40's 0.915. Looking at Figure 7, it seems this aberration comes from both judges rating"}, {"title": "6. Limitations/Future Work", "content": "There are certain limitations to our work. Currently, the categories we enumerate in our data pipeline is manually specified by humans and significant curation is done to ensure high quality prompts; for future work, we want to expand to using LLMs as category generators as well as quality checkers to automate the human effort out of this pipeline. For improving our leaderboard, we wish to add more models to be more representative of the entire spectrum of other leaderboards and futhur increasing the quality of the Bradley-Terry models we use to obtain the model's confidence intervals. In order to improve category separability, we look to creating a methodology on figuring out the minimum number of samples required to improve separability.\nThe other aspect of future work relies to details regarding LLM-as-a-judge evaluation. Typically, the judge models are ablated but less explored is the quality of the reference answer and whether one can use a weaker model instead of a stronger one to see if metrics are maintained. Current metrics define how separable a benchmark is and how much it aligns with human preferences but fails to account for the composition and diversity of the underlying data. For future work, we seek to quantify the diversity of each benchmark to understand how many capabilities/domains it spans."}, {"title": "7. Conclusion", "content": "We introduce a data pipeline that leverages via semi-supervised learning with a k-NN to enable practitioners to create benchmarks on their own data for targeted domains. Through evaluations of ten various closed and open-sourced models, we demonstrated that our benchmark achieves higher separability and agreement with CI with respect to Chatbot Arena, nearly 5 and 10 percentage points higher than the next best baseline, respectively. Our benchmark covers a wide variety of topics such as finance, medicine, legal and different languages absent in other LLM as a judge benchmarks. We hope that LLM developers can use our data pipeline to create their own benchmarks to evaluate their models for their particular use-case."}, {"title": "8. Appendix", "content": ""}, {"title": "8.1. Data Sources", "content": ""}, {"title": "8.2. Judge Template", "content": "Below is our judge template that we used for our LLM-as-a-judge evaluation:"}, {"title": "8.3. Evaluation Tool", "content": "With the notion of self-defined categories and using the LLM-as-a-judge framework, we create an evaluation tool which loads an internal leaderboard from a csv file and breaks down the winrate into several categories the user defined. The UI shows the leaderboard in a dataframe and shows the winrates in set of bar plots across different categories. A screenshot of the tool can be seen in Figure 8.\nThere is also a feature which enables the user to view completions on the evaluation from both the model the user is interested in, the reference model, and the judge model to examine its reasoning. This tool enables the user to examine where the model they are developing is performing better than other competitors and areas where improvement is required."}]}