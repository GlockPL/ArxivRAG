{"title": "SCIENCEAGENTBENCH:\nTOWARD RIGOROUS ASSESSMENT OF LANGUAGE\nAGENTS FOR DATA-DRIVEN SCIENTIFIC DISCOVERY", "authors": ["Ziru Chen", "Shijie Chen", "Yuting Ning", "Qianheng Zhang", "Boshi Wang", "Botao Yu", "Yifei Li", "Zeyi Liao", "Chen Wei", "Zitong Lu", "Vishal Dey", "Mingyi Xue", "Frazier N. Baker", "Benjamin Burns", "Daniel Adu-Ampratwum", "Xuhui Huang", "Xia Ning", "Song Gao", "Yu Su", "Huan Sun"], "abstract": "The advancements of language language models (LLMs) have piqued growing in-\nterest in developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about the true ca-\npabilities of such agents. In this work, we argue that for an agent to fully automate\nscientific discovery, it must be able to complete all essential tasks in the workflow.\nThus, we call for rigorous assessment of agents on individual tasks in a scien-\ntific workflow before making bold claims on end-to-end automation. To this end,\nwe present ScienceAgentBench, a new benchmark for evaluating language agents\nfor data-driven scientific discovery. To ensure the scientific authenticity and real-\nworld relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed\npublications in four disciplines and engage nine subject matter experts to vali-\ndate them. We unify the target output for every task to a self-contained Python\nprogram file and employ an array of evaluation metrics to examine the generated\nprograms, execution results, and costs. Each task goes through multiple rounds\nof manual validation by annotators and subject matter experts to ensure its anno-\ntation quality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompt-\ning, OpenHands CodeAct, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let alone\nend-to-end automation for scientific research.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have shown remarkable capabilities beyond text generation, includ-\ning reasoning (Wei et al., 2022; Yao et al., 2023), tool learning (Schick et al., 2023; Wang et al.,\n2024a), and code generation (Chen et al., 2021; Yang et al., 2024a). These abilities have piqued\nsignificant research interests in developing LLM-based language agents to automate scientific dis-\ncovery end-to-end. For instance, Majumder et al. (2024a) urge the community to build automated\nsystems for end-to-end data-driven discovery, an increasingly important workflow in many disci-\nplines (Hey et al., 2009) that leverages existing datasets to derive new findings. More recently, Lu\net al. (2024) claim to have built The AI Scientist, an agent that is capable of automating the entire re-\nsearch workflow, from generating ideas to running experiments and writing papers. This ambitious\nclaim has sparked both excitement and skepticism about the true capabilities of such agents."}, {"title": "SCIENCEAGENTBENCH", "content": "In this work, we contend that for a language agent to fully automate data-driven discovery, it must\nbe able to complete all essential tasks in the workflow, such as model development, data analysis,\nand visualization. Thus, we advocate careful evaluations of the agents' performance on these tasks,\nbefore claiming they can automate data-driven discovery end-to-end. Such an assessment strategy\nhelps grasp a more solid understanding of an agent's strengths and limitations than purely relying\non end-to-end evaluations, e.g., using an LLM-based reviewer to assess generated papers (Lu et al.,\n2024). Yet, high-quality benchmarks focusing on individual tasks in real-world scientific workflows\nare lacking for objective assessment and continued development of agents for data-driven discovery.\nTo this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for\ndata-driven discovery. The construction of ScienceAgentBench follows three key design principles.\n(1) Scientific authenticity through co-design with subject matter experts: We ensure the authen-\nticity of tasks in our benchmark by directly extracting them from peer-reviewed publications and\nengaging nine subject matter experts (incl. senior Ph.D. students and professors) from the respec-\ntive disciplines to validate them. This approach also minimizes the generalization gap for agents\ndeveloped on our benchmark to real-world scenarios. In total, we curate 102 diverse tasks from\n44 peer-reviewed publications in four disciplines: Bioinformatics, Computational Chemistry, Geo-\ngraphical Information Science, and Psychology & Cognitive Neuroscience (Figure 1). (2) Rigorous\ngraded evaluation: Reliable evaluation for language agents is notably difficult due to the open-\nendedness and complexity of data-driven discovery tasks. We first unify the target output for every\ntask as a self-contained Python program, and then employ an array of evaluation metrics that ex-\namine the generated programs, execution results (e.g., rendered figures or test set predictions), and\ncosts. We also provide step-by-step rubrics specific to each task to enable graded evaluation. (3)\nCareful multi-stage quality control: Each task goes through multiple rounds of manual validation\nby annotators and subject matter experts to ensure its quality and scientific plausibility. We also\npropose two effective strategies to mitigate data contamination concerns due to LLM pre-training.\nWe comprehensively evaluate five open-weight and proprietary LLMs, each with three frameworks:\ndirect prompting, OpenHands CodeAct (Wang et al., 2024c), and self-debug. Surprisingly, with-\nout expert-provided knowledge, Claude-3.5-Sonnet using self-debug can successfully solve 10.8%\nmore tasks than using OpenHands CodeAct while costing 17 times less API fees. This result res-\nonates with recent findings that agent designs should jointly consider costs and performance to\nmaximize their practical utility (Kapoor et al., 2024). Still, given three attempts for each task, the\nbest agent can only solve 32.4% of the tasks independently and 34.3% of them with expert-provided\nknowledge. These results also suggest language agents cannot yet automate essential tasks in data-\ndriven discovery nor the research pipelines end-to-end, in contrast to claims in recent work such as\nLu et al. (2024).\nDespite their current mediocre performance, we believe language agents hold significant potential\nin augmenting human scientists' productivity: For each task in our benchmark, it takes a trained"}, {"title": "PROBLEM FORMULATION", "content": "Given a natural language instruction, a dataset, and some optional expert-provided knowledge, an\nagent shall generate a program to complete the assigned task and save it to Python source code file.\nEach instance in our benchmark contains four components (Figure 2):\n(a) Task Instruction, which describes the goal of an essential task in data-driven discovery and its\noutput requirements. To resemble real-world settings, we keep the instructions concise and avoid\nunnecessary details when describing task goals. This setup also retains the open-endedness of data-\ndriven discovery and encourages the development of practical agents that do not rely on prescriptive\ndirections from scientists. We provide example task instructions in Appendix C for each discipline.\n(b) Dataset Information, which contains the dataset's directory structure and a preview of its con-\ntent. For agents without file navigation tools, they need such information to correctly use the dataset\nin their generated programs. For agents that can navigate file systems, it also helps them save a few\nturns of interactions to read datasets from the programming environment.\n(c) Expert-Provided Knowledge, which includes explanations for scientific terms, formulas to con-\nduct analysis, and example usages of programming tools. These pieces of knowledge are provided\nby subject matter experts, including senior Ph.D. students and professors, and are optional inputs to"}, {"title": "DATA COLLECTION", "content": "Task Annotation. We start by forming a group of nine graduate students to annotate the tasks\nin four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science,\nand Psychology & Cognitive Neuroscience. Within each discipline, we search for peer-reviewed\npublications that release their code and data under permissive licenses (Appendix I). Then, we follow\nfive steps to annotate each task: (1) Identify a reasonably documented code example that is self-\ncontained and convert it into a task in our benchmark. (2) Collect and preprocess datasets used in\nthe code. (3) Annotate the reference program by revising the referred code to analyze datasets in\nour benchmark. (4) Implement task-specific success criteria as an executable script and use GPT-40\nto draft fine-grained rubrics for evaluation. (5) Write the instruction and dataset information for\nthis task. We gathered 110 tasks initially but discarded four because their programs require long\nexecution time or nontrival environment setup. This leaves us with 106 tasks for validation.\nData Contamination and Shortcut Mitigation. In our preliminary studies, we have noticed that\nsome agents, such as OpenHands, may take shortcuts to solve a task. For example, when asked to\ndevelop a machine learning model, they may directly read and report the ground-truth labels in the\ntest set without writing the training code. Such perfect results are actually cheating and will hurt\nevaluation validity. In addition, because datasets and programs in our benchmark are open-sourced,\nthey are subject to data contamination in LLM training. To mitigate these issues, we devise two\nstrategies to modify the datasets: (1) For each dataset, we randomly remove five data points from\nits test set. If an LLM-generated program uses automatic data loaders that appeared in the training\ncorpora, it will produce results misaligned to our setup and fail the success criteria. In some cases,\nwe have to skip this step if it would break the completeness of a dataset, e.g., if it results in an\nincomplete geographical map. (2) For tasks involving model development, we re-split the dataset,\nkeep the test set labels only for evaluation, and replace them with dummy values, such as -1 for\nclassification tasks. These two strategies effectively mitigate data contamination and agent shortcut\nconcerns by failing agents that recite memorized code or attempt to directly report test set labels.\nSee Appendix E.2: Example E.4 for a case study.\nExpert Validation. We engage nine subject matter experts, including senior Ph.D. students and pro-\nfessors from the four involved disciplines, to validate each task and provide additional knowledge.\nFor each task, we present to experts with its instruction, dataset information, annotated program, and\ntask rubrics. The experts are asked to validate the tasks by completing a questionnaire (Appendix\nF), which can be summarized as four steps: (1) Validate if an annotated task represents a realistic\ntask in their data-driven discovery workflow. (2) Review whether a task instruction gives an accurate\nhigh-level description of the program and uses professional languages in their disciplines. (3) Pro-\nvide up to three pieces of knowledge that might be needed for solving each task. (4) Make necessary\nrevisions to the rubrics for grading the program. Then, following the experts' feedback, we revise 41\ntask instructions and remove three tasks that are not representative enough for scientific workflows\nin their disciplines. With 103 tasks remaining, our publication-oriented annotation strategy is shown\nto be effective in collecting real-world tasks.\nAnnotator Verification. To ensure data quality, we work with the nine annotators for another round\nof task verification. We ask the annotators to verify tasks that are not composed by themselves and\nexecute programs to reproduce the results. During this process, we refine 29 task annotations and\ndiscard one more task whose result is hard to replicate with the same program due to randomness.\nWe finalize ScienceAgentBench with 102 high-quality tasks for data-driven scientific discovery."}, {"title": "EVALUATION", "content": "While it is a preferable feature, the open-endedness of tasks in our benchmark introduces a cru-\ncial evaluation challenge. Specifically, our evaluation strategy has to accommodate diverse setup"}, {"title": "EXPERIMENTAL SETUP", "content": "We experiment with three open-weight LLMs, Llama-3.1-Instruct-70B, 405B (Dubey et al., 2024),\nand Mistral-Large-2 (123B) (MistralAI, 2024), and two proprietary LLMs, GPT-40 (OpenAI, 2024)\nand Claude-3.5-Sonnet (Anthropic, 2024). For all experiments, we use the same hyperparameters,\ntemperature = 0.2 and top_p = 0.95, and perform 0-shot prompting via the APIs. The prompts are\nincluded in Appendix H. We evaluate the LLMs under three different (agent) frameworks:\nDirect Prompting. Direct prompting is a simple framework that does not interact with any program-\nming environment. Given the task inputs, it prompts an LLM to generate a corresponding program\nin one pass. We use this framework to show the basic code generation capability of each LLM.\nOpenHands CodeAct. OpenHands (Wang et al., 2024c) is a generalist agent development frame-\nwork for code generation and software engineering. It provides three kinds of tools in the environ-\nment and defines different actions for an agent to interact with the tools: Python code interpreter,\nbash shell, and web browser. Among the agents developed with OpenHands, we use the best-\nperforming CodeActAgent v1.9 (Wang et al., 2024b) in our experiments. This agent unifies all\nactions in OpenHands, including the agent-computer interface commands (Yang et al., 2024a) to\nread and edit local files, into a large action space of different Python API calls. We experiment with\nthe CodeActAgent v1.9 using different LLMs to test the effectiveness of OpenHands' framework\ndesign for code generation tasks in data-driven discovery. For simplicity, we shorten the name of\nthis agent framework as OpenHands CodeAct."}, {"title": "RESULTS AND ANALYSIS", "content": "Through comprehensive experiments (Table 3), we show that the latest LLMs and agents can only\nachieve low-to-moderate task success rates. Given three attempts for each task, Claude-3.5-Sonnet\nwith self-debug demonstrates the best performance (34.3% SR) when using expert-provided knowl-\nedge. This result underline that LLM-based agents are not yet capable of fully addressing realistic\nand challenging data-driven discovery tasks, such as those in ScienceAgentBench."}, {"title": "MAIN RESULTS", "content": "Direct Prompting vs. Self-Debug: Execution feedback is necessary for LLMs to generate use-\nful programs. As shown in Table 3, directly prompting LLMs cannot unleash their full potential in\nprogramming for data-driven discovery tasks. Without executing its code, even the best performing\nLLM, Claude-3.5-Sonnet, can only solve 16.7% of the tasks independently and 20.6% with addi-\ntional knowledge. For most failed tasks, we share similar findings with Liang et al. (2024) that\nLLM-generated programs have correct high-level structures but implementation-level errors, such\nas missing steps or wrong API usage. Compared to direct prompting, self-debug can nearly double\nClaude-3.5-Sonnet's success rate (16.7 \u2192 32.4; 1.94\u00d7) without extra knowledge. With expert-\nprovided knowledge, Claude-3.5-Sonnet using self-debug also shows decent improvement over di-\nrect prompting. It achieves 13.7 absolute gains on SR (20.6 \u2192 34.3; 1.67\u00d7) and 45.1 absolute gains\non VER (41.2 \u2192 86.3; 2.09\u00d7). These results highlight the effectiveness of the simple self-debug\nframework and the importance of enabling LLMs to execute and revise their code for complex tasks.\nOpenHands CodeAct vs. Self-Debug: Agent designs should consider costs and capabilities\nof LLMs. For four of the five LLMs evaluated, self-debug demonstrates better performance than\nOpenHands CodeAct, with GPT-40 as the only exception (Table 3). By examining the trajectories,\nwe find that GPT-40 is better at leveraging tools in OpenHands than other LLMs. For instance, it\nis the only LLM that search for more details about the provided knowledge with the web browser.\nIn contrast, other LLMs are still struggling with specialized bash commands in OpenHands to edit\nprograms correctly (Example in Appendix E.1). We hypothesize that GPT-40 may have been trained\nto better follow instructions for language agents and to better use complex tools like a web browser.\nWhen it comes to self-debug, which has a more straightforward design, GPT-40 loses its advantage\nand underperforms Mistral-Large-2 and Claude-3.5-Sonnet, both of which are trained for better code\ngeneration according to their reports (MistralAI, 2024; Anthropic, 2024). Most surprisingly, without\nthe help of expert-provided knowledge, Claude-3.5-Sonnet using self-debug can successfully solve\n10.8% more tasks (21.6 \u2192 32.4 SR) than using OpenHands while costing 17 times less API fees\n($0.958 \u2192 $0.057), which is a critical factor to consider for practical applications. Overall, our\nresults resonate with recent findings on agent design (Kapoor et al., 2024; Xia et al., 2024): (1)"}, {"title": "HUMAN EVALUATION", "content": "Evaluation Setup. To further investigate the performance of Claude-3.5-Sonnet with self-debug\n(the best-performing agent), we conduct a rubric-based human evaluation of all the 102 programs\ngenerated using expert-provided knowledge. With the task-specific rubrics validated by experts\n(examples in Appendix G) and gold programs as references, each generated program is rated by two\ndifferent evaluators who participated in data collection. To reduce possible noises in ratings, the\nevaluators only mark whether a rubric item is met by the LLM-generated program. For each stage,\nwe add up points for satisfied rubric items and normalize them by total available points to the range\nof 0-100. Similarly, we calculate the overall score considering all items. The final score of each\nprogram is the average of two evaluators' ratings.\nAdditionally, one purpose of this human evaluation is to assign partial credits to the generated pro-\ngram even if it is not correct (Section 2.3). Therefore, we do not provide the evaluators with program\nexecution results and hide task success outcomes. Although this setup encourages evaluators to ex-\namine LLM-generated programs carefully, it also introduces some noise. For example, there are\ntasks where both a feed-forward neural network and a random forest model can achieve satisfying\nperformance on the test set. While the gold program implements the neural network, the agent\nchooses to use random forest. Since each rubric is derived from a gold program and reflect its\nimplementation, there are chances that the evaluator overlooks such equivalence. Also, for output\nformatting, we observe some subjective variance when judging the formats of figures, such as colors,\nscales, and text labels, according to the rubrics and gold programs. As a result, successful programs\nwould not always receive a perfect human rating.\nResults and Analysis. As shown in Figure 4, data loading and processing, the first two stages in\ndata-driven discovery tasks, can distinguish successful programs from failed ones. Except for a few\noutliers, almost all successful programs receive a perfect human rating for data loading. In contrast,\n25% of the failed programs have their rating below 50 in the first stage. For data processing, the\nrating distribution of successful programs skews toward the full score, while that of failed programs\nskews toward a score between 20 and 50. These human evaluation results correspond to an intuitive\nexplanation: If the dataset were not loaded or processed correctly, it would be impossible to solve a\ntask successfully, regardless of the code implementation for consequent stages."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We introduce ScienceAgentBench, a new benchmark to evaluate language agents for data-driven\nscientific discovery. We compile 102 diverse, real-world tasks from 44 peer-reviewed publica-\ntions across four scientific disciplines and engage nine subject matter experts to ensure data quality.\nThrough comprehensive experiments on five LLMs and three frameworks, we show that the best-\nperforming agent, Claude-3.5-Sonnet with self-debug, can only solve 34.3% of the tasks when using\nexpert-provided knowledge. Our results and analysis suggest that current language agents cannot\nyet automate tasks for data-driven discovery or a whole research pipeline.\nBy introducing ScienceAgentBench, we advocate the use of language agents to assist human sci-\nentists with tedious tasks in their workflows and call for more rigorous assessments of such agents.\nWe plan to continually expand our benchmark into more disciplines and facilitate future research in\ntwo ways: (1) ScienceAgentBench will serve as a necessary testbed for developing future language\nagents with stronger capabilities to process scientific data or to utilize expert-provided knowledge.\n(2) ScienceAgentBench will help future research to design new automatic graded metrics, such as\nan LLM judge based on task-specific rubrics, to assess language agents for data-driven discovery."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Z. Chen led the project, formulated the benchmark, organized data collection and human evalua-\ntion, implemented programs and evaluation scripts for experiments, conducted all experiments on\nGPT-40/Claude/Mistral, and wrote the manuscript. S. Chen designed and implemented rubric-based\nevaluation, and helped to run direct prompting, self-debug, and OpenHands experiments on Llama\n3.1 70B/405B, optimize the evaluation scripts, and revise the manuscript. Y. Ning helped to run\nsome experiments on OpenHands with Llama 3.1 70B/405B. Z. Chen, S. Chen, Y. Ning, Q. Zhang,\nB. Wang, B. Yu, Y. Li, Z. Liao, and C. Wei worked as the student annotators to collect the bench-\nmark, verify the tasks, and evaluate generated programs based on rubrics. Z. Lu, V. Dey, M. Xue,"}, {"title": "LIMITATIONS", "content": "Capabilities and Evaluation of Language Agents for Science. In this work, we have developed\na benchmark focusing on tasks in data-driven discovery and formulate them as code generation\nproblems due to two reasons. (1) Data-driven discovery is an increasingly important workflow\nfor science (Hey et al., 2009). While plenty of computational tools (Cao, 2017) and AI models\n(Wang et al., 2023) have been developed, the sheer amount and heterogeneity of data are already\noverwhelming for scientists (Bell et al., 2009), not to mention the programming efforts to access\nthese tools and models for processing, analyzing, and visualizing scientific data. A language agent\nthat can automate such tedious tasks in data-driven discovery would help to save hours of effort for\nscientists. (2) We aim to rigorously assess the capabilities of existing language agents as science co-\npilots that can write code to process, analyze, and visualize data. Hence, we formulate each task as\na code generation problem, whose output shall be easily verifiable using well-established automatic\nmetrics and directly usable by a scientist without additional efforts to modify or implement.\nAs a result, we only focus on the code generation capability of language agents. We encourage future\nstudies to carefully examine the agents' other capabilities that can help with scientific discovery,\nsuch as summarizing literature (Lin et al., 2024), suggesting ideas (Si et al., 2024), or planning\nexperiments (Boiko et al., 2023). Specifically, we advocate rigorous, comprehensive assessments\nof one such capability at a time, as we need to deeply understand the strengths and limitations of\ncurrent language agents for each aspect of scientific discovery. In addition, while we only use well-\nestablished evaluation methods in our benchmark, such as CodeBERTScore (Zhou et al., 2023) and\nGPT-40 judge for figures (Wu et al., 2024; Yang et al., 2024b), we acknowledge that they are not\nperfect yet. Future research may leverage the diverse set of tasks in our benchmark to develop better\nautomatic evaluation metrics or human evaluation protocols for data-driven discovery tasks and code\ngeneration problems.\nDiversity of Tasks, disciplines, and Programs. Although we strive to include a diverse set of\ntasks and programs from different scientific disciplines in ScienceAgentBench, we devise several\ncompromises to make data collection more practical. First, when collecting publications, we have\nindeed found more with programs written in R, Stata, or Matlab. However, because our annotators\nare not familiar with these programming languages, we focus on collecting Python programs, which\nall annotators can adapt confidently. Second, for evaluation efficiency, we only collect programs\nthat can accomplish the task within 10 minutes. As a result, the final benchmark includes relatively\nfewer tasks that process large-scale data and develop complex methods. Finally, we choose the\nfour representative disciplines considering their abundance of open-source data and the availability\nof experts we can easily contact. With these limitations in mind, we have designed a principled,\nextensible data collection process and expert validation protocol. Future work is encouraged to\nexpand ScienceAgentBench with programs in other languages and tasks in other disciplines.\nEthical and Safety Considerations. Our benchmark is constructed by adapting open-source code\nand data, to which we respect their creators' ownership and intellectual property. In Appendix I, we\nhave made our best effort to cite the original papers, list the repositories, and provide their licenses.\nStill, we acknowledge that two repositories are copyrighted and believe their terms for use are com-\npatible with our research purpose (Table I.4, I.5). We welcome requests from the original authors to\nmodify or remove relevant tasks if needed. Meanwhile, agents developed with ScienceAgentBench\nshould consider potential safety issues in deployment, such as adversarial attacks and confidential\ndata leakage. Instead of solely relying on language agents for scientific discovery, it is important for\nusers to have control over the agents through intervention and feedback mechanisms."}, {"title": "RELATED WORK", "content": "AI for Science. Since deep learning unlocks the power of data, AI algorithms and models have been\nincreasingly used to accelerate scientific discovery (Wang et al., 2023). One of the most prominent\nexamples is AlphaFold (Jumper et al., 2021), which can predict protein structures with high accuracy\nand save biologists months to years of effort. More recently, a tremendous number of language\nmodels has been developed for different disciplines, including math (Yue et al., 2024), chemistry\n(Yu et al., 2024), biology (Labrak et al., 2024), geography (Li et al., 2023), and so on. To automate\ndata-driven discovery end-to-end, it is necessary for language agents to write code to access these AI\nmodels and other computational tools (Cao, 2017). Our work aims to develop language agents with\nthis essential ability, which can help scientists save hours of programming effort, and rigorously\nevaluate such agents to grasp a more solid understanding of their strengths and limitations.\nAgents and Benchmarks for Task Automation. Developing agents for task automation is a long-\nestablished challenge in AI research (Russell & Norvig, 2010). Built upon LLMs, a new generation\nof agents has shown new promise to automatically perform many tasks in web navigation (Deng\net al., 2023; He et al., 2024; Koh et al., 2024; Zheng et al., 2024; Zhou et al., 2024), software\ndevelopment (Jimenez et al., 2024; Wang et al., 2024c; Yang et al., 2024a), or scientific discovery\n(Boiko et al., 2023; Zheng et al., 2023; Lu et al., 2024).\nTo evaluate the performance of these new agents, many benchmarks have been recently proposed.\nFor example, TaskBench (Shen et al., 2024) is one of the first benchmarks for evaluating language\nagents with large-scale synthetic tasks. The output of these tasks are formatted as JSON API tool\ncalls, which hinders the generalization of agents developed on this benchmark to constantly chang-\ning tools or new ones in practice. To resemble real-world use cases with more flexibility, many\nbenchmarks formulate their tasks as code generation and unifies their outputs as Python programs,\nsuch as SWE-Bench (Jimenez et al., 2024), BioCoder-Py (Tang et al., 2024b), ML-Bench (Tang\net al., 2024a), and MLAgentBench (Huang et al., 2024b). Yet, these benchmarks only consist of\ntasks found on secondary sources, such as GitHub and Kaggle.\nTo fill the gap in evaluating language agents for scientific tasks, a few benchmarks start to use sci-\nentific publications as their task sources, including SciCode (Tian et al., 2024), BLADE (Gu et al.,\n2024), and DiscoveryBench-Real (Majumder et al., 2024b). Among them, DiscoveryBench-Real\nis the most similar to our ScienceAgentBench. However, DiscoveryBench-Real asks the agents to\noutput abstract steps to complete a task in natural language, which is hard to evaluate rigorously and\nmaybe practically less useful. With ScienceAgentBench, we advocate careful evaluations of lan-\nguage agents' performance on individual tasks, instead of purely relying on end-to-end evaluations\nof these agents, e.g., using an LLM-based reviewer to assess generated papers (Lu et al., 2024). In\nthe long run, ScienceAgentBench will serve as a high-quality benchmark focusing on essential tasks\nthat involve code generation in real-world data-driven discovery workflows for objective assessment\nand continued development of future language agents."}, {"title": "EXAMPLE TASK INSTRUCTIONS", "content": "Table C.1: Example instructions of Bioinfomatics and Compuational Chemistry tasks (Section 2.2).\nDisciplines\nBioinformatics\nTask Instructions\nTrain a cell counting model on the BBBC002 datasets containing\nDrosophila KC167 cells. Save the test set predictions as a single\ncolumn \"count\" to \"pred_results/cell-count_pred.csv\".\nTrain a drug-target interaction model using the DAVIS dataset to\ndetermine the binding affinity between several drugs and targets.\nThen use the trained model to predict the binding affinities between\nantiviral drugs and COVID-19 target. Rank the antiviral drugs based\non their predicted affinities and save the ordered list of drugs to\n\"pred_results/davis_dti_repurposing.txt\", with one SMILES per line.\nPlot the Tanimoto similarities of the fingerprint between the frames.\nSpecifically, the interaction fingerprints between a selected ligand\nand protein for the first 10 trajectory frames. Save the png file\ninto pred_results/ligand_similarity_pred.png.\nTrain a VAE model on the given data and perform a 1-vs-all\ndifferential expression test for each cell type. Extract top markers\nfor each cell type using the results. Visualize them as a dotplot with\nthe cell types organized using a dendrogram. Save the figure to\npred_results/hca_cell_type_de.png.\nTrain a multitask model on the Clintox dataset to predict a drug's\ntoxicity and FDA approval status. Save the test set predictions,\nincluding the SMILES representation of drugs and the probability\nof positive labels, to \"pred_results/clintox_test_pred.csv\".\nGenerate features for the given diffusion data based on material\ncomposition and use the SHAP feature selection approach to select\n20 features. Save the selected features as a CSV file\n\"mat_diffusion_features.csv\" to the folder \"pred_results/\".\nComputational Chemistry\nFilter the compounds in \"hits.csv\" and save the SMILES represen-\ntations of the left ones. Compounds to be kept should have no PAINS\nor Brenk filter substructures and have a maximum tanimoto similarity\nof less than 0.5 to any of the active compounds in \"train.csv\". Save\nthe SMILES of left compounds to \"pred_results/compound_filter_results\n.txt\", with each one in a line.\nTrain a graph convolutional network on the given dataset to predict\nthe aquatic toxicity of compounds. Use the resulting model to compute\nand visualize the atomic contributions to molecular activity of the\ngiven test example compound. Save the figure as\n\"pred_results/aquatic_toxicity_qsar_vis.png\"."}, {"title": "EXAMPLE INSTRUCTIONS", "content": "Table C.2: Example instructions of Geographical Information Science and Psychology & Cognitive\nNeuroscience tasks (Section 2.2).\nDisciplines\nGeo Information Science\nTask Instructions\nAnalyze and visualize Elk movements in the given dataset. Esti-\nmate home ranges and assess habitat preferences using spatial\nanalysis techniques. Identify the spatial clusters of Elk move-\nments. Document the findings with maps and visualizations.\nSave the figure as \"pred_results/Elk_Analysis.png\".\nAnalyze the impact of land subsidence on flooding based on\nfuture elevation data of the study area. Identify flood-prone\nareas and estimate potential building damage to support urban\nplanning and mitigation strategies. Save the results to\n\"pred_results/flooding_analysis.png\".\nCalculate the deforestation area percentage in the Brazilian\nstate of Rond\u00f4nia within the buffer zone of 5.5km around\nroad layers. Save the percentage result in a CSV file named\n\"pred_results/deforestation_rate.csv\" with a column title\npercentage_deforestation.\nLoad North America climate data in NetCDF file and extract\ntemperature data along the time series, then perform a\nquadratic polynomial fit analysis on the temperature data,\nand output the fitting results by year in\n'pred_results/polynomial_fit_pred.csv'.\nProcess and visualize the given ECG data by perform R\npeak detection and outlier correction. Plot an overview of\nthe data and save the final figure as\n\"pred_results/ecg_processing_vis1_"}]}