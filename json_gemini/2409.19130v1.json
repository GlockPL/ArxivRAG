{"title": "Multi-modal Cross-domain Self-supervised Pre-training for fMRI and EEG Fusion", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Nancy B. Carlisle", "Hua Xie", "Gregory A. Fonzo", "Yu Zhang"], "abstract": "Neuroimaging techniques including functional magnetic resonance imaging (fMRI) and electroencephalogram (EEG) have shown promise in detecting functional abnormalities in various brain disorders. However, existing studies often focus on a single domain or modality, neglecting the valuable complementary information offered by multiple domains from both fMRI and EEG, which is crucial for a comprehensive representation of disorder pathology. This limitation poses a challenge in effectively leveraging the synergistic information derived from these modalities. To address this, we propose a Multi-modal Cross-domain Self-supervised Pre-training Model (MCSP), a novel approach that leverages self-supervised learning to synergize multi-modal information across spatial, temporal, and spectral domains. Our model employs cross-domain self-supervised loss that bridges domain differences by implementing domain-specific data augmentation and contrastive loss, enhancing feature discrimination. Furthermore, MCSP introduces cross-modal self-supervised loss to capitalize on the complementary information of fMRI and EEG, facilitating knowledge distillation within domains and maximizing cross-modal feature convergence. We constructed a large-scale pre-training dataset and pretrained MCSP model by leveraging proposed self-supervised paradigms to fully harness multimodal neuroimaging data. Through comprehensive experiments, we have demonstrated the superior performance and generalizability of our model on multiple classification tasks. Our study contributes a significant advancement in the fusion of fMRI and EEG, marking a novel integration of cross-domain features, which enriches the existing landscape of neuroimaging research, particularly within the context of mental disorder studies.", "sections": [{"title": "1. Introduction", "content": "Functional magnetic resonance imaging (fMRI) and Electroencephalography (EEG) are widely-used neuroimaging techniques for investigating brain disorders. Both of these two modalities offer insights into brain function across spatial, temporal, and spectral domains. In each modality, the three domains possess distinct characteristics. For example, fMRI measures low-"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Deep Learning for Brain Networks Learning", "content": "Various methods have been developed for neuroimaging analysis, encompassing both fMRI and EEG. A noteworthy trend is the recent surge in deep learning and graph convolutional networks (GCNs). Regarding fMRI signals, BrainNetCNN pioneered the use of Convolutional Neural Networks (CNNs) in brain network analysis, employing CNNS to extract deep semantic features from fMRI. BrainGNN explored brain network information by modeling the brain as a graph with nodes and edges, leveraging GCN to extract spatial features of the brain networks. Brain Network Transformer (BNT) and Dynamic BNT improved Graph"}, {"title": "2.2. Multi-modal Fusion for Neuroimaging Integration", "content": "In neuroscience, the existence of various neuroimaging modalities, such as fMRI and EEG, underlines the complexity and depth of brain study. These modalities, each offering unique insights into brain activity and/or structure, are crucial for advancing our understanding and treatment of brain disorders. The integration of multi-modal data is pivotal for improving diagnosis and prognosis of brain disorders. Increasing research efforts have been dedicated to achieving this goal. For instance, a CNN-LSTM framework was proposed to conduct the fusion of large volume time-series data like EEG. [23] improved the performance of mild cognitive impairment diagnosis by integrating both MRI and PET modalities via GCN. Cross-GNN integrated fMRI and DTI to capture inter-modal dependencies through dynamic graph and mutual learning. A Multi-Modal and Multi-Atlas In-tegrated Framework was proposed to fuse resting-state fMRI (rs-fMRI)"}, {"title": "2.3. Self-supervised Pre-training and Multi-domain Fusion", "content": "Self-supervised learning explores latent data relationships by constructing self-supervised loss functions, a widely embraced practice in various fields. This method facilitates pre-training or optimizing training through inherent data characteristics without labeled examples.\nRecent advancements in the field include a self-supervised pre-training model, trained on large-scale neuroimaging data, showcasing its potential in brain science. Additionally, a pre-training pipeline utilizing self-supervised learning techniques was introduced, aiming to regularize consistency loss across different domains (spatial and temporal) for time series prediction, encompassing EEG and MEG signals. Similarly, a framework leveraging principles of self-supervised learning was developed, focusing on contrastive regularization between time series signals like EEG. Furthermore, another model delved into learning pretext information to enhance spatiotemporal representations for emotion recognition from ECG signals.\nWhile some of these methods consider constructing self-supervised in-"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overall Architecture", "content": "As illustrated in Fig. 1, our proposed MCSP model is designed to handle multi-modal and multi-domain inputs, specifically fMRI and EEG, for comprehensive feature extraction from brain signals across spatial, temporal, and frequency domains. During the pre-training phase, based on contrastive self-supervised pre-training fashion , we introduce Cross-domain self-supervised loss (CD-SSL) and Cross-modal self-supervised loss (CM-SSL) to maximize the similarity of features from different domains and modalities."}, {"title": "3.2. Domain-specific Data Construction", "content": "Before feeding the data into domain-specific encoders, we first preprocess and construct the data for each domain. In the spatial domain, after data preprocessing, we compute connectivity from both fMRI and EEG source estimates, each containing time series signals extracted from 100 regions of interest (ROIs). The data preprocessing and connectivity calculation methods are described in Section 4.1. For fMRI, following [15], we construct functional connectome using the Pearson Correlation Coefficients as node features and adopt the correlation as edge connections. For EEG, we compute the power envelope connectivity and use power envelope coefficients as node features. Similarly, we adopt the correlation as edge connections. For time series signals, we standardize the fMRI time series with varying lengths to the unified $E_{fMRI} \\in \\mathbb{R}^{N \\times l}$. Here, $N$ represents the number of ROIs, and $l$ represents the unified length of sequences. Specifically, we set $N = 100$ and $l = 200$ for fMRI time series. As for EEG data, given that EEG time series are typically much longer than fMRI time series due to a higher sampling rate and contain more precise information due to a higher resolution, we initially downsample and unify the EEG time series for each subject to $F_{EEG}^{T} \\in \\mathbb{R}^{N \\times l'}$ with unified length, where $l' = 25000$. Subsequently, we divide the entire time series sequence into 125 segments $F_{EEG}^{T} \\in \\mathbb{R}^{N \\times l}$, each having the same length of $l = 200$, to align with the time series of fMRI data. In addition, for frequency-domain sequences, we apply the same processing"}, {"title": "3.3. Domain-specific Encoders and Projectors", "content": ""}, {"title": "3.3.1. Modality-agnostic Domain-specific Encoders", "content": "We utilize modality-agnostic domain-specific encoders to extract unique features for each domain. In the spatial domain, we employ a Graph Transformer model with positional embeddings [18] as the encoder to capture spatial information by treating each ROI as a node, thereby capturing the spatial distribution of fMRI and EEG, as well as connectivity features between ROIs. For the temporal and frequency domain, Transformer models are used to extract time series features while considering dependencies between long-distance time points.\n\n$e_{m}^{D} = Encoder_{m}^{D}(E_{M}^{D}), E_{m}^{D} \\in \\mathbb{R}^{K \\times c}$\n\nwhere $M$ means modality and $M \\in {MRI,EEG}$. $D$ represents domain and $D \\in {Spatial,Temporal, Frequency}$. $m$ means type of encoder model and $m \\in {GraphTransformer,Transformer,Transformer}$ for three domains, respectively. For spatial domain, $K$ refers to the number of nodes, while $c$ represents the dimension of the node feature. For temporal and frequency domains, $K$ means the number of brain ROIs, while $c$ denotes the length of time and frequency sequences. After extracting features via domain-specific encoders, we can obtain six kinds of hidden-layer feature"}, {"title": "3.3.2. Modality-aware Projection Heads", "content": "Projectors are employed to obtain contrastive embeddings, which are crucial for calculating the contrastive loss function and conducting classification via fully-connected layers. Due to the lengthy nature of the time and frequency sequences in EEG, in order to align them with fMRI, we segment long sequences from each subject into shorter, equal-length segments (length = 200) during the data construction process. As shown in Fig. 2, for these segments, we concatenate them before inputting them into the projector. Then, we project them into a high-dimensional space with unified dimensions to obtain consistent embeddings $h_{M}^{D}$.\n\n$h_{M}^{D} = Projector_{m}^{D}(e_{m}^{D}), h_{M}^{D} \\in \\mathbb{R}^{N \\times 128}$"}, {"title": "3.4. Cross-domain Self-supervised Loss", "content": "Self-supervised learning enables the exploration of intrinsic relationships between features, uncovering deep dependencies within the data. The critical aspect of self-supervised learning lies in the construction of effective self-supervised loss functions and embeddings for loss computation.\nTo fully explore the latent interaction across different domains among a single modality, we propose a Cross-domain Self-supervised Loss, denoted as CD-SSL, which consists of an Intra-domain Cross-view Consistency Loss $L_{ID}^{M}$ and a Cross-domain Consistency Loss $L_{CD}^{M}$. As shown in Fig. 3, for each domain, we utilize domain-specific augmentation techniques to generate pairwise contrastive embeddings. Inspired by graph augmentation in graph contrastive learning (GCL) , for the spatial domain, we leverage graph augmentation to generate spatially augmented views while keep semantic invariance of the brain network graph. For structure-based graph augmentation, there are two options. First, we randomly remove 20% to 50% of the weakest edges' connection strength, resulting in graph views with the same node distribution and node features but sparser edges. Second,"}, {"title": "3.5. Cross-modal Self-supervised Loss", "content": "For capturing the inner interaction across different modalities during the pre-training or the online training, we propose a cross-modal self-supervised loss, named CM-SSL, which consists of an Intra-domain Cross-modal Distillation Loss $L_{IM}^{Mu}$ and a Cross-modal Consistency Loss $L_{CM}^{Mu}$. As illustrated in\n\n$L_{IM}^{Mu}(h_{i}^{Mx},h_{i}^{My}) = -log\\frac{exp(sim(z_{i}^{Mx}, z_{i}^{My})/\\tau)}{\\sum_{j=0}^{size-1} exp((sim(z_{i}^{Mx}, z_{j}^{Mx}) + sim(z_{i}^{My}, z_{j}^{Mx}))/\\tau)} + KL(z_{i}^{My}||z_{i}^{Mx})$\n\nwhere $D$ means domain and $D \\in {Spatial,Temporal, Frequency}$. $z_{i} = softmax(h_{i})$. $M_{x}$ and $M_{y}$ denote two different modalities. $x$ and $y \\in {fMRI, EEG}$ and $x \\neq y$. $KL(.)$ stands for Kullback-Leibler divergence. $exp(.)$ represents the exponential function. $sim(.)$ denotes cosine similarity. $\\tau$ indicates the temperature coefficient, and here we set it to 0.2. Note that when in the spatial domain (i.e., $D = Spatial$), $x = EEG$ and $y = fMRI$, this means that in the spatial domain, we expect knowledge from fMRI to distill into EEG, and EEG features should learn and approach those of fMRI. Conversely, when in the temporal or frequency domain (i.e., $D = Temporal or Frequency$), $x = fMRI$ and $y = EEG$, we expect knowledge from EEG to distill into fMRI, and fMRI features should learn and approach those of EEG."}, {"title": "3.6. Cross-model Distillation across Domains", "content": "As illustrated in Fig. 4, we also apply an overall distribution regularization across the domain-integrated fusion embedding of fMRI, EEG and their fused feature.\n\n$L_{CM}^{Mu}(h_{i}^{M_f}, h_{i}^{M_e}) = KL(z_{i}^{M_f}||z_{i}^{M_e}) + KL(z_{i}^{M_e}||z_{i}^{M_f}) + KL(z_{i}^{M_i}||z_{i}^{M_f})$\n\nwhere $z_{i} = softmax(h_{i})$.\nFinally, we combine the Intra-modal regularization $L_{IM}^{Mu}(h_{i}^{Ma}, h_{i}^{My})$ and the Cross-modal regularization $L_{CM}^{Mu}(h_{i}^{Ma}, h_{i}^{My})$ to form our $L_{CM-SSL}^{Mu}$ with a balance coefficient $\\alpha = 0.8$.\n\n$L_{CM-SSL}^{Mu} = \\alpha \\cdot L_{IM}^{Mu} + (1 - \\alpha) \\cdot L_{CM}^{Mu}$\n\nThe proposed self-supervised loss functions are designed to fully explore the latent interaction among different modalities and domains, especially leveraging the complementary information between domains of fMRI and EEG modalities. During the re-training for the downstream classification tasks, the Cross Entropy (CE) loss $L_{CE}$ is adopted as the loss function. Additionally, as illustrated in Fig.5, another potential pre-training and fine-tuning paradigm is cross-model distillation across domains, showing the ability to distill the knowledge from a large teacher model, fused with knowledge from multiple domains, into a smaller student model focusing on a single domain. The key lies in defining an appropriate loss function, which includes both soft target loss and hard target loss as shown below\n\n$L_{soft_i} = - \\sum_i^{classes} p_{i}^{STF} log(q_{i}^{e}), L_{hard_i} = - \\sum_i^{classes} y_{i} log(q_{i}^{e})$"}, {"title": "4. Data Acquisition and Preprocessing", "content": ""}, {"title": "4.1. Datasets", "content": "We evaluated the efficacy of the proposed model on multiple independent datasets, including ADHD-200 [32], Autism Brain Imaging Data Exchange (ABIDE I [33] and ABIDE II [34]), Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC)[35], and Healthy Brain Network (HBN)."}, {"title": "4.1.1. ADHD-200", "content": "The ADHD-200 dataset [32] is a collection of rs-fMRI data for studying ADHD. It aggregates data from various research institutions, aiming to assist researchers in gaining deeper insights into the brain dysfunction of ADHD."}, {"title": "4.1.2. ABIDE", "content": "The ABIDE [33, 34] is a collection of rs-fMRI data for studying ASD. This dataset comprises two phases: ABIDE I and ABIDE II. The ABIDE I consists of rs-fMRI data from 555 ASD patient runs and 598 matched HC runs while the ABIDE II has collected data from 581 ASD patient runs and 733 HC runs. We evaluated our model for ASD prediction on the ABIDE I and II, respectively, to confirm its robustness against different study phases of the dataset."}, {"title": "4.1.3. EMBARC", "content": "The EMBARC [35] is a multi-modal dataset containing both fMRI and EEG collected from 265 MDD patients. We utilized this dataset to examine multi-modal fusion and conduct two different prediction tasks: Depression Grading and Sex Classification. For Depression Grading: We categorized all MDD patients into two groups based on their symptom severity as assessed by the 17-item Hamilton Depression Rating Scale (HAMD17) scores. According to an established clinical criteria[36], 138 patients with baseline HAMD17 < 17 before treatment were categorized as a mild depression group while 127 as moderate to severe depression group. We tested our model for classifying these two groups. For Sex Classification: We also tested our model on discriminating sex in MDD patients (103 females and 162 males)."}, {"title": "4.1.4. HBN", "content": "The HBN [37], created by the Child Mind Institute, is an open resource for transdiagnostic research on children's mental health and learning disorders. It includes rich multimodal data such as structural and functional MRI, EEG, cognitive-behavioral assessments, and genomic data. The dataset covers a range of pediatric mental health conditions like ADHD, autism, depression, and anxiety. HBN's transdiagnostic approach enables researchers to explore commonalities and differences across diagnoses. In our study, we used fMRI and EEG time series data from the HBN dataset. For the fMRI data, HBN includes 2,282 subjects, with each subject having two fMRI runs. For the EEG data, HBN contains eye-open EEG time series from 1,594 subjects and eye-closed EEG time series from 1,744 subjects. To match subjects with both fMRI and EEG data, we performed a selection process and identified 1,029 subjects who had both fMRI and EEG data available. In subjects who have undergone modality matching, we conducted two downstream tasks: the identification of MDD and the identification of ASD. And we quantified the labeled data for both disorders. Specifically, among the subjects with MDD, 119 were diagnosed as healthy, while 77 were diagnosed as MDD patients. In the case of ASD subjects, there were 119 healthy individuals and 138 diagnosed with ASD."}, {"title": "4.2. Unified Pre-training Dataset Construction", "content": "As shown in Table 1, we compiled statistics on the number of fMRI and EEG scans across the five datasets. The ADHD-200 and ABIDE datasets contain only fMRI data, whereas the EMBARC and HBN datasets provide both fMRI and EEG data. In these multimodal datasets, fMRI data for each subject includes two runs, and EEG data comprises two runs corresponding to eye-open and eye-closed conditions. However, due to missing modalities in several subjects, the number of usable fMRI-EEG pairs for pre-training is relatively limited, potentially compromising the effectiveness of pre-training. To mitigate this issue, we applied a cross-matching strategy, pairing different runs of fMRI and EEG from the same subject. This approach allows each subject to generate up to 22 pairs, thereby significantly increasing the number of available pre-training samples."}, {"title": "4.3. Data preprocessing", "content": "All rs-fMRI data utilized in this study were preprocessed using the fMRIPrep pipeline[38]. The T1-weighted image underwent intensity non-uniformity correction and skull stripping, followed by spatial normalization through nonlinear registration[39]. Brain features were segmented using FSL, and fieldmap information was used for distortion correction. The BOLD reference was transformed to the T1-weighted image, addressing remaining distortion with nine degrees of freedom. Head-motion parameters were estimated, and BOLD signals were corrected and resampled into standard space. ICA-AROMA was applied for artifact removal[40]. Regional time series were then extracted from the preprocessed rs-fMRI using the Schaefer atlas of 100 ROIS, followed by calculating functional connectivity using Pearson's correlation. Based on these connectivity measures, we constructed brain network graphs for the subsequent analysis.\nIn the EMBARC and HBN datasets, rs-EEG signals were preprocessed using an automated artefact rejection pipeline [41], involving removing line noise, drifts, and artefacts, rejecting bad epochs and channels, interpolating bad channels, and re-referencing to the common average. The preprocessed EEG were filtered into four bands: theta (4-7 Hz), alpha (8-12 Hz), beta (13-30 Hz), and gamma (31-50 Hz). EEG source localization and power envelope"}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Implementation details", "content": "Our experimental analyses were implemented using the PyTorch framework and trained on an Nvidia RTX 4090 GPU. The total number of training epochs was set to 50. We employed dynamic learning rates and Adam optimizer for a better model training. We conducted 10 runs of 10-fold cross validation. For each run, we adopted different random seeds to construct training, validation and testing sets. We fixed random seeds for all the models but adopted random seeds to split dataset in each run. For pre-training, ensuring the effectiveness of contrastive learning requires both a substantial data volume and a sufficiently large batch size. Consequently, we set the batch size to 128 during this phase. For fine-tuning, however, due to the limited availability of labeled data for specific tasks, we reduced the training batch size to 32. The learning rate was set to 0.0005."}, {"title": "5.2. Overall comparison with other methods", "content": "We compared our model with a wide array of state-of-the-art approaches on the ADHD-200, ABIDE I, ABIDE II, EMBARC and HBN datasets for"}, {"title": "5.3. Ablation analysis of cross domains and modalities", "content": "To investigate the influence of different domains and modalities on the model, we conducted comprehensive ablation experiments both quantitatively and qualitatively. As shown in Table 5, for fMRI, the spatial domain"}, {"title": "5.4. Brain biomarker interpretation", "content": "As shown in Figs. 7 and 8, We ranked the brain regions according to their importance scores and visualized the top 10 most important brain ROIs identified from each domain and modality for the Depression Grading task on the EMBARC dataset, providing an interpretation of brain areas associated with depression severity. Our results indicate that the Lingual Cortex, Superior Parietal Cortex, Superior Frontal Cortex and Inferior Temporal Cortex are most important for predicting depression severity. These findings echo the results of numerous previous studies. For instance, the Lingual Cortex's dysfunction has been noted for its strong linkage to MDD severity and related anxiety, emphasizing its critical role in mood regulation [47]. The Superior Parietal and Frontal Cortex, involved in spatial attention and sensory integration, has been implicated in emotional processing, particularly in regulating attention to emotional stimuli. In addition, the Inferior Temporal Cortex, associated with visual object recognition and emotional valence, is intricately connected to the limbic system, influencing emotion regulation and cognitive functions. Disruptions in neural circuits involving these regions have suggested to underlie the emotionl and cognitive symptoms observed in"}, {"title": "5.5. Ablation study on the proposed self-supervised loss", "content": "As shown in Table 6, we investigated the effects of the proposed cross-domain and cross-modal self-supervised loss functions, comparing a baseline model that used only cross-entropy loss for classification. The incorporation of self-supervised pretraining, leveraging both CD-SSL and CM-SSL markedly enhanced the model's performance. It is worth noting that these proposed loss functions not only effectively serve as the basis for our pre-training procedure but also demonstrate comparable efficiency in improving model outcomes. A key advantage of employing this pretraining approach is its versatility, allowing the pretrained model to be adeptly adapted and applied to a variety of downstream tasks as needed."}, {"title": "5.6. Analysis of the potential universal pre-training abilities", "content": "As illustrated in Fig. 9, our model demonstrates the capability of universal pretraining on fMRI and EEG across four scenarios. 1) Cross Modality: The model was pretrained on fMRI and fine-tuned on EEG, and vice versa. The universal pretraining strategy, utilizing the proposed cross-domain loss,"}, {"title": "5.7. Cross-model knowledge distillation across domains", "content": "A notable application of our model lies in its adeptness at facilitating cross-model distillation across diverse domains. Initially, we constructed a comprehensive teacher model by integrating data from spatial, temporal, and spectral domains, serving as a repository of consolidated knowledge. We then employed distillation techniques to transfer the insights gathered by this teacher model to smaller, domain-specific student models. This process"}, {"title": "5.8. Comparison among the parameters of different models", "content": "To validate the efficiency of our model, we compared the performance and parameter counts of different models. As shown in Figure 10, we compared our model's performance and parameter counts with various methods. The results show that although our model is composed of three smaller sub-models and has a slightly higher parameter count than previous methods, its performance significantly surpasses that of other models, especially after undergoing large-scale pre-training. As illustrated in the figure, our three sub-models, each tailored for a specific domain, achieved performance comparable to state-of-the-art methods within their respective domains. However, by integrating these models through a cross-domain self-supervised loss during pre-training, we attained significantly improved results compared to previous methods, despite the increased total number of parameters. Overall, our model consists of a total of 3.4M parameters. Specifically, the sub-model for the spatial domain contains 1.2M parameters, the sub-model for the tem-"}, {"title": "6. Conclusion", "content": "In our study, we developed a Multi-modal Cross-domain Self-supervised Pre-training model designed to integrate multimodal and multidomain data efficiently. We adapted the principles of pretraining to the realm of neuroimaging analysis, allowing for universal pretraining across diverse modalities and domains. Our approach incorporated domain-specific data augmentation to enhance feature representation within each domains. We then harnessed a cross-domain contrastive learning loss function to foster the en-"}]}