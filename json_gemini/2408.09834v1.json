{"title": "Minor DPO reject penalty to increase training robustness", "authors": ["Shiming Xie", "Hong Chen", "Fred Yu", "Zeye Sun", "Xiuyu Wu", "Yingfan Hu"], "abstract": "Learning from human preference is a paradigm used in large-scale language model (LLM) fine-tuning step to better align pretrained LLM to human preference for downstream task. In the past it uses reinforcement learning from human feedback (RLHF) algorithm to optimize the LLM policy to align with these preferences and not to draft too far from the original model. Recently, Direct Preference Optimization (DPO) has been proposed to solve the alignment problem with a simplified RL-free method. Using preference pairs of chosen and reject data, DPO models the relative log probability as implicit reward function and optimize LLM policy using a simple binary cross entropy objective directly. DPO is quite straight forward and easy to be understood. It perform efficiently and well in most cases. In this article, we analyze the working mechanism of \u03b2 in DPO, disclose its syntax difference between RL algorithm and DPO, and understand the potential shortage brought by the DPO simplification. With these insights, we propose MinorDPO, which is better aligned to the original RL algorithm, and increase the stability of preference optimization process.", "sections": [{"title": "Background", "content": "Recent LLM trained on very large datasets are extremely powerful in understanding human queries and providing surprisingly reasoning ability. Traditional industry like finance also begin to try using LLM in its business, but require a high demand of accuracy on number, entity, event and etc., which brings in much higher requirements on the preference alignment quality. In the past RLHF establish a paradigm for the alignment work, which use reward model and reinforcement learning like Schulman et al. (2017) to optimize the LLM model after a supervised fine-tuning. Rafailov et al. (2023) present a simplified algorithm DPO which directly optimizes the LLM with a simple cross entropy classification objective based on the preference pair data. DPO models the relative log probability as an implicit reward function and derived a closed objective form. Its concept is quite straight froward, easy to understand, and appropriate for quick start work. Meanwhile we have also found DPO is fragile in some data distribution, which need a careful tuning to avoid an optimization crash.\nDPO update the relative log probability of the preference pair data, using a dynamic sample-level importance weight to prevent the model degeneration. The key factor is the hyper-parameter \u03b2 used in DPO algorithm. Rafailov et al. (2023) claim \u03b2 accounting for the strength of the KL constraints. In this paper we analyze the internal working mechanism of DPO on how it optimize model, clarify the slightly different syntax of \u03b2 between RL Eq. 1 and DPO Eq. 2, to know the shortage DPO bring in.\nOur main contribution is that we give a comprehensive analysis of the \u03b2 mechanism and propose an improved loss function Minor DPO Eq. 6 based on the analysis. Minor DPO is better aligned to RL Eq. 1 without bringing new hyper-parameters, and it improve the stability and robustness of the preference alignment optimization."}, {"title": "Related Work", "content": "Reinforce Learning from human feedback(Ouyang et al. (2022), Ziegler et al. (2020)) present a paradigm to align LLM to human preference. It first creates a reward model from the preference data pair of chosen and reject. Then LLM is trained through RL algorithm like PPO (Schulman et al. (2017)) to maximize the learned reward on the LLM while maintaining the discrepancy between the optimized LLM and the original LLM to prevent model degeneration.\nDPO(Rafailov et al. (2023)) introduce a new parameterization of the reward model in RLHF and allow to solve the standard RLHF problem using a simplified solution with only a cross entropy classification loss based on the preference data pair directly. It's simpler to implement, requires less compute resource, and can fine-tune LLM to align with human as well as RL algorithm. Rafailov et al. (2024) derive that DPO is token-level MDP that satisfies the Bellman equation and works as a general inverse Q-learning algorithm in a theoretical way.\nThere are several other variants that optimize alignment objective based on the preference sample directly. Some variants contain a reference model while some variants are reference-free.\nIPO(Azar et al. (2023)) identify that DPO may be prone to over-fitting in situations where the preference probability of the chosen over the reject sample is close to 1 and propose Identity Preference Optimization based on preference data pair.\nKTO(Ethayarajh et al. (2024)) propose HALO that directly maximizes the utility of generation, instead of maximizing the log-likelihood of preference. It's able to train with only chosen samples, which is useful in some scenarios.\nDPOP(Pal et al. (2024)) identify that DPO may lead to degenerate when the preference data pair is close. It uses a theoretic analysis, prove both reward/chosen and reward/reject grows up to negative value with a high probability, and propose a fix by adding an additional non-linear reward on the chosen sample yw, somehow like adding an extra SFT loss conditional on the relative log probability distance of the chosen sample yw between the optimized LLM and the reference LLM.\nPCO(Adolphs et al. (2022)) argue that method such as unlikelihood which simply push down the probability of negative tokens may inadvertently push up the probability of low quality or rare tokens for that sequence position. It proposes pairwise cringe loss which use a sampled top-k token, and instead of push down the probability of the original negative token, it increases the probability of the sampled token and thus decrease the original negative token indirectly. The pairwise cringe loss doesn't use a ref model to constraint the discrepancy that the optimized model may draft from the original model.\nSimPO(Meng et al. (2024)) propose a simpler approach, it's reference-free and use the average log probability of the sequence as an implicit reward, and introduce a target reward margin to the Bradley-Terry objective.\nOrpo (Hong et al. (2024)) propose a variant of SFT by using odds ratio-based penalty to the conventional negative log-likelihood (NLL) loss for the reject samples. It's also reference-free.\nBeyond the RL and DPO variants, Nakano et al. (2022), Askell et al. (2021) Cobbe et al. (2021) explore best-of-n sampling to improve large language model generation by selecting the best response based on the human preference rewards among n sampled responses. RRHF(Yuan et al. (2023)) is targeted to learn the best response and comparisons based on the human preference rewards among n sampled responses to achieve alignment during optimization instead of inference."}, {"title": "Approach", "content": ""}, {"title": "DPO derivation", "content": "Ziegler et al. (2020) presents the RLHF pipeline paradigm. In the RL Fine-tuning phase, it uses the reward model learned in reward modeling phase to optimize the LLM model to retrieve maximum reward using a classical RL objective.\n$\\max _{X} E_{x \\sim D, y \\sim \\pi_{\\theta}(y | x)}[r(x, y)]-\\beta D_{K L}[\\pi_{\\theta}(Y | X) || \\pi_{r e f}(Y | X)]$\nrf(x, y) here is the reward function. $D_{k l}[\\pi_{\\theta}(y | x) || \\pi_{r e f}(y | x)]$ is a bound that constraint $\\pi_{\\theta}(y | x)$ not to deviate from $\\pi_{r e f}(y | x)$. And hyper-parameter \u03b2 presents the constraints strength. The KL constraints is important as it prevent the optimized model over-optimized on the training datasets, which may cause the model degenerate and lose the generalization ability learned during the pre-train phase. Due to the nature of discrete language generation, the objective is optimized with RL algorithm using PPO(Schulman et al. (2017)) or PPO with additional pre-train loss (ppo-ptx in Ouyang et al. (2022)).\nDue to the computation complexity of RL, Rafailov et al. (2023) propose a simplified implementation and derive DPO objective from RL in a closed form.\n$L_{D P O}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right)=-E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{w} | x\\right)}{\\pi_{r e f}\\left(y_{w} | x\\right)}-\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{l} | x\\right)}{\\pi_{r e f}\\left(y_{l} | x\\right)}\\right)\\right]$\nThe DPO objective optimize the model directly using a simple cross entropy loss based on the preference pair data. It avoids the reward model, critic model and sampling process used in the RL algorithm.\nlet's expand DPO loss equation and get DPO gradient equation.\n$\\nabla_{\\theta} L_{D P O}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right)=-\\beta E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\sigma\\left(-\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{w} | x\\right)}{\\pi_{r e f}\\left(y_{w} | x\\right)}+\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{l} | x\\right)}{\\pi_{r e f}\\left(y_{l} | x\\right)}\\right)\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(y_{w} | x\\right)-\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(y_{l} | x\\right)\\right]\\right]$\nIn this article we use rewards/chosen to represent $\\log \\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{r e f}(y_{w}|x)}$, use rewards/reject\u00b9 to represent $\\log \\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{r e f}(y_{l}|x)}$, and use margin to represent $\\log \\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\theta}(y_{l}|x)}$\nrewards/chosen is how much the optimized model prefer the chosen sample in preference data pair over the reference model. rewards/reject is how much the optimized model prefer the reject sample in preference data pair over the reference model. And margin is how much the optimized model prefer the chosen sample, and dis-prefer the reject sample in preference data pair over the reference model. In a summary, DPO objective is to enlarge the margin between the chosen samples and the reject samples.\nAnd thus Eq. 3, can be rewritten into\n$\\nabla_{\\theta} L_{D P O}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right)=-E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\beta * \\sigma(-\\beta * margin)[\\nabla_{\\theta} \\log \\pi\\left(y_{w} | x\\right)-\\nabla_{\\theta} \\log \\pi\\left(y_{l} | x\\right)\\right]\\right]$\nThe loss gradient can be divided into three parts,\n1.  A sample level dynamic coefficient $f(\\beta, margin) = \u03b2 * \u03c3(-\u03b2 * margin)$ related to \u03b2 and margin\n2.  A reward $\\nabla_{\\theta} \\log \\pi(y_{w}|x)$ to the positive sample Yw\n3.  A penalty - $\\nabla_{\\theta} \\log \\pi(y_{l}|x)$ to the reject sample y\u0131."}, {"title": "Different syntax between DPO and RL", "content": "In Rafailov et al. (2023) it claims 3 account for the strength of the KL constraints, something same as the \u03b2 hyper-parameter used in RL algorithm. Let's dig into the sample level dynamic coefficient and see how \u03b2 affect the training process.\nFigure 1 shows that \u03b2 control the shape of the coefficient $f(\\beta, margin)$. The bigger the \u03b2 is, the sooner the $f(\\beta, margin)$ decay, especially when the margin is positive(As DPO is to enlarge the margin, so the margin shall grow to positive). During the training process, when the margin grows up to some threshold value, bigger \u03b2 will return a smaller coefficient than smaller \u03b2, and affect the gradient magnitude in the back propagation process. In summary,"}, {"title": "DPO shortage", "content": "In Pal et al. (2024) the author has a theoretic analysis on why both rewards/chosen and rewards/reject grows up to negative when the preference samples are close, and it propose a fix by adding an additional non-linear reward on the yw sample.\n$L_{D P O P}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right)=-E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{w} | x\\right)}{\\pi_{r e f}\\left(y_{w} | x\\right)}-\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{l} | x\\right)}{\\pi_{r e f}\\left(y_{l} | x\\right)}\\right)-\\max \\left(0, \\log \\frac{\\pi_{\\theta}\\left(y_{w} | x\\right)}{\\pi_{r e f}\\left(y_{w} | x\\right)}\\right)\\right]$\nIt's same as adding an additional SFT loss on Yw, but somehow use a non-linear transformation with $\\max(0, \\log \\frac{\\pi_{r e f}(y_{w}|x)}{\\pi_{\\theta}(y_{w}|x)})$ format. It adds SFT loss only when $\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{r e f}(y_{w}|x)}$ is smaller than1, so that $\\log \\frac{\\pi_{r e f}(y_{w}|x)}{\\pi_{\\theta}(y_{w}|x)} \u2265 0$. It works like a compensation for the over penalty to the reject samples. The additional loss shall bring $\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{r e f}(y_{w}|x)} \\log(\\frac{\\pi_{r e f}(y_{w}|x)}{\\pi_{\\theta}(y_{w}|x)})\u2265 0$ for some data distribution. while it also brings an additional hyper parameter that needs to be tuned with the corresponding \u03b2 value.\nBesides rewards/chosen grows up to negative, in Adolphs et al. (2022), Jiang et al. (2022) they argue that method such as unlikelihood which simply push down the probability of reject tokens may inadvertently push up the probability of low quality or rare tokens for that sequence position, because there is no control over that effect. We have observed the same problem in our experiments, with a slightly larger learning rate like 5e \u2013 6 to 1e \u2013 5, the DPO optimized model may start to generate repeated token, or some unwanted symbol token in the answer, which we believe is caused by the over penalty. Although with an appropriate small learning rate, the DPO optimized model works, we still believe if they're enough training steps, DPO may still crash the model when the accumulated penalty on the reject samples surpasses a certain threshold.\nIn Adolphs et al. (2022) it uses a sampled token and instead of pushing down the probability of the original reject token, it increases the probability of the sampled token. Compared to the standard cringe loss, it uses the preference pair and get a dynamic margin between logp(yw|x) and logp(yl|x) to control the pair cringe loss. This method brings several hyper-parameters to tune, and in our experiment it may over-pull up the sampled token probability and cause the optimized model generate repeat token."}, {"title": "Minor DPO reject penalty to improve training robustness", "content": "They're several assumptions that we found really important and the our solution is based on those assumptions.\n1.  the Eq.1 suggest that the \u03c0\u03bf should not far from tref, for both yw and y\u0131, and it use an explicit KL loss to present the constraints, and use \u03b2 to control the constraints strength.\n2.  Tref in DPO is quite important and is the key factor to prevent over optimization. DPO use $\\log \\frac{\\pi_{\\theta}}{\\pi_{r e f}}$ to represent the dispersion between the optimized \u03c0\u03b8 and tref, and in theoretical we expect $\\log \\frac{\\pi_{\\theta}(Y_{w})}{\\pi_{r e f}(Y_{w})}\u2265 0$ and $\\log \\frac{\\pi_{\\theta}(Y_{l})}{\\pi_{r e f}(Y_{l})} < 0$ for the optimized model.\n3.  Although Rafailov et al. (2023) claims \u03b2 in DPO accounting for the strength of the KL constraints, with above analysis, \u03b2 is the constraints strength for margin and affect the training process by interleaving with the learning rate, instead of affecting \u03c0\u03bf and Tref directly as in Eq. 1\n4.  Pal et al. (2024) proves that DPO will lead both $\\pi_{\\theta}(Y_{w}|x) < \\pi_{r e f}(Y_{w}|x)$ and $\\pi_{\\theta}(y_{l}|x) \u2264 \\pi_{r e f}(y_{l}|x)$ when yw and y\u0131 is close, and Adolphs et al. (2022) declare that simply push down the probability of reject tokens may lead the optimized model to crash(actually in this case it's even worse as both $\\pi_{\\theta}(Y_{w}|x) \u2264 \\pi_{r e f}(Y_{w}|x)$ and $\\pi_{\\theta}(y_{l}|x) \u2264 \\pi_{r e f}(y_{l}|x)$)\nThus, we propose Minor DPO reject penalty equation\n$L_{M i n o r D P O}\\left(\\pi_{\\theta} ; \\pi_{r e f}\\right)=-E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_{\\theta}\\left(y_{w} | x\\right)}{\\pi_{r e f}\\left(y_{w} | x\\right)}-\\beta \\max \\left(0, \\log \\frac{\\pi_{\\theta}\\left(y_{l} | x\\right)}{\\pi_{r e f}\\left(y_{l} | x\\right)}\\right)\\right]\\right]$\nThe Eq.6 adds an additional constraints to reject samples, by replacing the original penalty $-\\log \\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{r e f}(y_{l}|x)}$ with $max(0, \\log \\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{r e f}(y_{l}|x)})$. It decreases penalty on reject samples, and hence we name it Minor DPO reject penalty and MinorDPO in short.\n1.  For those reject samples, \u03c0\u03b8 is more close to Tref as we stop the gradient descent on rejects samples that are $\\pi_{\\theta}(y_{l}|x) \u2264 \\pi_{r e f}(y_{w}|x)$, as we don't want to over-optimized on the rejected samples\n2.  For those positive samples, we could achieve $\\pi_{\\theta}(Y_{w}|x) \u2265 \\pi_{r e f}(Y_{w}|x)$\nComparing to DPO, MinorDPO decreases the reject penalty and slightly increases chosen reward for samples that are $\\pi_{\\theta}(y_{l}|x) \u2264 \\pi_{r e f}(y_{w}|x)$ . It ease the over-penalty problem and doesn't introduce new hyper-parameter. In most cases, it can use the same DPO hyper-parameter settings(or a bigger learning rate for training). We find that minor DPO is much more robust and can accept a much higher learning rate without crashing the model. (A note here is that with higher learning rate, bigger batch size is needed to maintain the training stability)"}, {"title": "Experiments", "content": "Since DPO, there exists several DPO-variants that propose some modifications for different purpose. Here we bring DPOP in the comparison because DPOP is the only methods we know so far that also notice DPO will degenerate when the preference training data is close. DPOP propose an addition SFT on the chosen sample, while we believe the degeneration is caused by the over penalty on the reject samples from DPO, and propose a solution that decrease the penalty, to make it more consistent with the original RL method.\nFor training settings, we use Qwen1.5-7B-Chat(Bai et al. (2023)) as the base model, use MetaMath(Yu et al. (2024)) as training data with the way mentioned in Pal et al. (2024)3, and use test set of GSM8K(Cobbe et al. (2021)) to compare.\nWe use LLaMa-Factory(Zheng et al. (2024)) as the training and inference framework with some customized code to implement the DPOP and MinorDPO algorithm. The experiments use batch size 128, warm-up ratio 0.1, linear decay learning rate, 1 epoch and run 3000+ steps.\nAs DPOP has an additional hyper-parameter A to control the compensation strength we use X = 50, which is proposed in Pal et al. (2024).\nFor inference settings, we use the prompt You are a helpful assistant. Please answer follow question step by step and give final answer in a separate line using the brief format So the answer is: . Below is the question: to concat the question content.\nFigure 6 show the experiment result. We also evaluate the base model and show it in both sub-figures for the comparison. It doesn't need training so its \u03b2 value is 0. Figure 6 shows several important points:\n1.  Due the closeness of the preference data pair, DPO lost in all settings, MinorDPO wins in most settings, and gets the top score for both learning rate of le - 5 and 1e - 6.\n2.  DPO performance decrease when \u03b2 become small from 0.2 to 0.02. when lr = le \u2013 5 and \u03b2=0.02, 0.04, the result model will generate repeated token in it answer.\n3.  DPOP and MinorDPO accept high learning rate, but when lr = 1e \u2013 5, \u03b2=0.02, DPOP will also generate repeat token occasionally, and in Figure 6b shows it decrease performance seriously, which indicate that the hyper-parameter A is also highly related to learning rate and \u03b2 in order to compensate the penalty."}, {"title": "Conclusion & Future work", "content": "In this article we analyze the different syntax of \u03b2 between DPO and RL, and find DPO shortage it brings in. For the preference sample pair, normally we just prefer the chosen samples over the reject samples, but it doesn't mean the reject samples are totally wrong. The symmetric form of increase on the chosen sample and decrease on the reject sample introduce too much penalty over the reject sample. DPO uses a sample level dynamic coefficient to ease the problem but doesn't solve it completely. We propose MinorDPO that break the symmetry by reducing the decrease on the reject samples. In this article we only focus on the over penalty problem thus only compare three methods: DPO, DPOP and MinorDPO using the MetaMath datasets. There may be more methods and more datasets to be tested in the future. We believe DPO is an elegant and efficient base to start with and investigate in for the preference alignment work. MinorDPO is an improvement on DPO and doesn't bring in additional hyper-parameter. With virtually no tuning of hyper-parameter, it performs same or better than DPO and other DPO variants.\nAs in our test, MinorDPO can accept higher learning rate. MinorDPO decrease the penalty over reject samples and thus may introduce under-fit problem for some data distributions, so an appropriate higher learning rate may help fixing it.\nAnother important work is how to prevent over-fit on the learning datasets. We know DPO and DPO variants are sensitive to the \u03b2 and learning rate, so tuning these two hyper-parameters may help. Inspired by B-free rewards and margin, We think there may exist some training metrics that can be used to quantify whether training is sufficient or not, and thus those metrics can be used inside DPO/MinorDPO in some way to solve the over-fit problem."}]}