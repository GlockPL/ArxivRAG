{"title": "Foundation Models and Adaptive Feature Selection: A Synergistic Approach to Video Question Answering", "authors": ["Sai Bhargav Rongali", "Mohamad Hassan N C", "Ankit Jha", "Neha Bhargava", "Biplab Banerjee", "Saurabh Prasad"], "abstract": "This paper tackles the intricate challenge of video question-answering (VideoQA). Despite notable progress, current methods fall short of effectively integrating questions with video frames and semantic object-level abstractions to create question-aware video representations. We introduce Local - Global Question Aware Video Embedding (LGQAVE), which incorporates three major innovations to integrate multi-modal knowledge better and emphasize semantic visual concepts relevant to specific questions. LGQAVE moves beyond traditional ad-hoc frame sampling by utilizing a cross-attention mechanism that precisely identifies the most relevant frames concerning the questions. It captures the dynamics of objects within these frames using distinct graphs, grounding them in question semantics with the miniGPT model. These graphs are processed by a question-aware dynamic graph transformer (Q-DGT), which refines the outputs to develop nuanced global and local video representations. An additional cross-attention module integrates these local and global embeddings to generate the final video embeddings, which a language model uses to generate answers. Extensive evaluations across multiple benchmarks demonstrate that LGQAVE significantly outperforms existing models in delivering accurate multi-choice and open-ended answers.", "sections": [{"title": "1. Introduction", "content": "Over the last decade, Video Question Answering (VideoQA) has evolved into a vital multidisciplinary field combining computer vision and natural language processing [50,55]. Despite advances, accurately interpreting video semantics relative to queries remains challenging, primarily due to the complex interplay between video content and questions, keeping VideoQA at the forefront of research demands. Current models focus on capturing spatiotemporal dynamics and aligning them with questions to derive answers [7,53], yet often require extensive dataset training and are prone to dataset biases, as frame selection is not guided by language. Recent advancements have moved beyond simple feature summarization, constructing scene and temporal graphs to depict object-level interactions [44]. However, while adept at handling broad context queries, these approaches often miss the finer details necessary to analyze specific object interactions at the frame level.\nRecently, the adoption of foundation models [31] (e.g. LlamaVid [22]) has significantly improved performance in several video comprehension tasks. While these models demonstrate superior performance, they face a specific challenge: they analyze all video frames indiscriminately, regardless of their relevance to the posed questions. Some studies have explored the paradigm of language-driven frame selection in contexts other than VideoQA [27, 49]. However, these approaches typically involve a complicated multi-stage pipeline, rely on secondary information sources such as image-based foundation models, or pose a multi-objective optimization framework, thus overburdening the entire process. Despite advancements in the frame selection stage, recent findings [41, 44] indicate that the outcomes of modern multi-modal foundation models for VideoQA are heavily biased towards the language cues, emphasizing the importance of the fundamental question: To what extent are VideoQA outcomes relevant to the video contents? Our research aims to precisely identify relevant video contents, both at the coarse and fine scales, guided by the given question semantics, for VideoQA.\nOur solution: We introduce a unified solution, LGQAVE, for VideoQA with three principal novelties.\nOur approach incorporates a learnable cross-attention module for question-aware video frame selection, which dynamically associates the question prompt with frame-level visual embeddings. This is achieved by applying a threshold to the cross-attention scores, enabling the precise isolation of video frames that are semantically aligned with the question. This method circumvents the complexities inherent in multi-stage frame selection pipelines and can be effortlessly integrated into any VideoQA system.\nLooking forward, we propose the construction of spatial graphs for frames identified as most pertinent to the questions, termed as question-aware local object selection and their interaction modeling. This task is approached as a question-guided visual grounding, avoiding traditional object detection frameworks. To this end, we employ the miniGPT4 model [56] to process the questions alongside the selected frames, generating bounding box coordinates for the relevant objects.\nSubsequently, frame-specific spatial graphs are constructed by considering the detected bounding boxes as the nodes and defining pairwise connections. When integrated with masked question embeddings, these graphs are fed into the Dynamic Graph Transformer (Q-DGT) model [44], which further refines the embeddings spatially and temporally, enhancing the semantic coherence between the visual content and the question context.\nIn our final step, we aim to derive both local and global video representations from the outputs of Q-DGT to effectively address both long-video level and fine-grained frame-level questions. This is achieved by refining the global video representation through a query-key-value-based cross-attention mechanism, utilizing localized frame-level graph embeddings for answer generation (Fig. 1). Our significant contributions are summarized as,\n[-] We introduce LGQAVE, an innovative model for Video QA that enhances the extraction of local and global video features, thoroughly guided by the question semantics.\n[-] Our approach begins with cross-attention for question-aware frame selection, followed by using miniGPT4 for visual grounding to establish object interaction graphs based on the posed question. We then intuitively obtain the video representations through the Q-DGT module.\n[-] We showcase the performance of LGQAVE on distinct VideoQA tasks and ablate the model rigorously. We observe steady improvements of 2-6% on average."}, {"title": "2. Related Works", "content": "Video question answering (VideoQA): Traditional VideoQA methods have primarily used video encoders on sparse frames [17, 36] or short segments [37], which struggle with spatiotemporal interactions and object compositionality [12], leading to suboptimal performance in reasoning tasks. Although cross-modal matching [3, 5, 6] and memory-based approaches [2,28] have improved video content extraction, they rely heavily on frame-level or clip-level representations, which are often inadequate for detailed object relation reasoning. Advances in graph-based methods have facilitated object-level rationale; however, these methods tend to use either unified graphs that do not effectively differentiate spatial from temporal relations or static graphs that ignore temporal dynamics [44].\nTransformers have significantly advanced the field of VideoQA. Models developed from datasets such as HowTo100M [24] employ proxy tasks like masked language modeling [13] and specific supervisions, such as future utterance prediction [46], to enhance performance. Despite outperforming traditional models [48, 57], transformer-based systems often focus on recognition or provide only shallow descriptions, struggling with visual relation reasoning due to noisy data and the limited scope of instructional videos [5]. Recent methods leveraging open-domain vision-text data face challenges with temporal relations and high operational costs. Despite their scalability, user-generated data can lead to overfitting. Large language models like BLIP-2 and MiniGPT-4 extend to video but encounter efficiency issues. Innovations such as MobileVLM and LLaMA-Vid have improved feature representation, and graphical models now effectively integrate both global and local features for enhanced dynamic reasoning [19, 25, 29, 51, 56]. Our LGQAVE model advances beyond existing approaches by integrating visual and linguistic synergies at multiple scales. This integration enables the extraction of both global and local perceptions of video content, effectively addressing various queries.\nGraphs in VideoQA: Early VideoQA models such as TGIF-QA [14] and MSVD-QA [11] targeted specific actions and objects in video clips, leveraging spatio-temporal features to generate responses. Subsequent advancements led to more sophisticated models like HME-VideoQA [7] and Co-Mem [9], which utilize hierarchical memory networks and co-attentional frameworks to capture dynamic interactions within videos more effectively. HME-VideoQA builds hierarchical graphs by representing different levels of video granularity to capture temporal relationships. CoMem creates graphs through collaborative memory, linking video frames and question embeddings. Additionally, graph-based methods have proven effective for detailed visual understanding by representing video objects as graph structures. For example, LLaVA [25] enhances VLM performance by identifying objects pertinent to specific questions and constructing corresponding graphs. Conversely, the Contrastive Video Graph Transformer (CoVGT) [44] excels in providing global representations by focusing on the overall video content. They take all the objects that are present in a frame and form a graph, yet it falls short in local representations and lacks question-specific conditioning.\nDespite these advancements, processing all video frames remains computationally expensive. Current methods focus on spatio-temporal dynamics and semantic alignment, yet they often manage vast amounts of data, leading to overlooking redundant content. Essential visual cues may be neglected, diminishing the accuracy of video interpretation.\nVision-language models (VLMs): Multimodal learning outperforms unimodal methods in tasks that require visual-semantic integration, such as image and text bridging. Recent developments have introduced foundation models like CLIP [35], FLORENCE [39], and ALIGN [15], which are particularly effective in these multimodal contexts. These models harness large-scale image-text pairs to tackle a variety of tasks in the CV/NLP domains, including zero-shot classification, object detection, image captioning, and VQA, to name a few. Despite their efficacy with still images, these models face challenges with long video sequences, primarily due to the extensive number of tokens required to represent each frame.\nModels such as CLIP and ALIGN have proven effective in video recognition [18, 24, 32, 34] and video-text retrieval [8, 28]. However, they encounter difficulties in accurately capturing interactions between video content and labels. Innovative models like Flamingo [1] and BLIP-2 [19] utilize web-scale image-text pairs, while Instruct-BLIP [52] and MiniGPT-4 [56] leverage high-quality instructional data sources. Methods such as Video-LLaMA [51] and VideoGPT [30] incorporate spatial and temporal pooling to overcome computational hurdles associated with long videos. LLaMA-VID [22] adopts a dual-token strategy to enhance the processing of long sequences. In contrast, LGQAVE is designed to systematically utilize question guidance for frame selection and the modeling of relevant objects within and across frames. This approach aims to minimize redundancy and irrelevance in video features, thereby enhancing the efficiency and accuracy of VideoQA."}, {"title": "3. Proposed Methodology", "content": "In this section, we define the problem and outline the objectives for the LGQAVE framework. We consider a dataset D that consists of video sequences V, questions Q, and corresponding labeled answers A. The primary objective is to learn a mapping function : (V, Q) \u2192 A that accurately predicts the correct answer A\u00bf for each given question Qi associated with a video Vi.\nTo accomplish this, LGQAVE is structured into four key components (Fig. 2): a. Question-driven frame selection module-identifies the most relevant video frames, thereby reducing redundancy at the frame level. b. Frame-centric object graph construction-emphasizes the important objects and their interactions within the selected frames through visual grounding, minimizing redundancy at a finer level. c. Question-aware dynamic graph transformer (Q-DGT)-facilitates effective selection and fusion of local and global video features. d. Answer prediction module-generates accurate answers based on the enriched video and question representations. These components, detailed below, collaboratively leverage the semantics of the questions to ensure a discriminative and contextually rich embedding space. The variables are summarized in the supplementary materials."}, {"title": "3.1. Question-aware video frame selection", "content": "In VideoQA, processing every video frame in a sequence is both computationally intensive and time-consuming, often leading to redundancy when dealing with frame splits. To tackle these issues, our LGQAVE framework incorporates a novel frame selection module designed to sample question-aware, key video frames from video-question pairs. This is achieved by utilizing a cross-attention mechanism to calculate relevance scores between the question tokens and video frames, ensuring that only the most pertinent frames are selected.\nMathematically, we denote the video frame at the tth time step for the ith instance as $V_{it} \\in R^{H \\times W \\times 3}$, where t\u2208 {1, ..., T} and Ti represents the total number of video frames for the ith instance. Here, H and W denote the height and width of the extracted frames, respectively. We use a frozen CLIP image encoder f to extract visual features $E \\in R^{N \\times C}$ from Vit, where N = H \u00d7 W, with p representing the patch size, and C being the embedding dimension. Additionally, We extract the text-guided query $Q_{i} \\in R^{M \\times C}$ using a pre-trained RoBERTa [26] model for the question Qi, where M denotes the number of queries. The visual features E and the text-guided query features Qi are then passed through learnable projection de and q layers to obtain the projected features \u0112 and Qi, respectively. These projected features are subsequently fed into the cross-attention module defined in Eq. 2, where a cross-attention score between the question and the t-th frame, st, is computed as follows,\n$\\overline{E} = \\varphi_e(E_t), Q_i = \\phi_q(Q_i)$ (1)\n$s_t = Mean \\left(Softmax \\left( \\overline{E}^T Q_i \\right) \\right)$ (2)\nFinally, we select the frame Vit based on the cross-attention score st, provided it surpasses a predefined threshold B. The subset of selected frames from Vi is denoted as Vi. These selected frames are then processed further to construct spatial object graphs for each selected frame."}, {"title": "3.2. Obtaining graph-based frame representation", "content": "We utilize the MiniGPT-4 architecture to construct question-aware object graphs from the selected frames in Vi, contrasting with traditional models that perform object detection across all frames without considering the question context. MiniGPT-4's efficiency lies in requiring only a linear layer to align visual features with the Vicuna model [54]. Additionally, we redefine the object detection task in LGQAVE as a visual grounding task, using the frames from Vi and the question Qi, where MiniGPT-4 excels.\nFor each selected frame $V_{it'} \\in V_i$, we also include the two preceding and two subsequent frames: $V_{t'-2}, V_{t'-1}$ through $V_{t'+1}, V_{t'+2}$ to ensure temporal continuity and minimize the risk of missing critical sequential information, which we fixed through empirical validation. These frames, along with the question prompt Qi from the frozen RoBERTa model, are fed into MiniGPT-4, which processes them to generate m bounding boxes B' around the objects pertinent to the question in Vit'. Four coordinates define each bounding box, and the total number of bounding boxes per frame is limited to m \u2264 10.\nWe enhance the graph representation methodology by utilizing detected objects, advancing beyond the approach in [44]. For each highlighted object instance in a video frame Vit', we extract Region of Interest (RoI)-aligned features as object appearance representations Ft which also contains spatial locations Ft of the respective objects. Additionally, we capture a frame-level feature Ff' to augment the graph representations derived from the local objects. We aim to construct a frame-specific spatial graph using $F = F_t \\cup F_f'$.\nWhile our methodology is inspired by [44], it differs significantly in its execution. Unlike [44], which assumes static object groups within a video clip and employs a fixed linking score based on appearance and spatial location F', our approach with MiniGPT-4 dynamically tracks objects across the video sequence. This dynamic tracking enhances the robustness and adaptability of our model, particularly improving its generalizability to longer video sequences.\nGraph construction for Vit: We propose to consider the bounding boxes from B' and the entire frame Vit' as constituting the m + 1 nodes in the frame-specific graph $G_{t'} \\equiv \\left(A_{t'}, R_{t'} \\right)$, with At' denoting the node-set, and we put up an edge between two bounding boxes, and the edge weights are defined as follows,\n$R_{t'} = Softmax \\left( \\phi_k(F_t)^T \\phi_q(F_{t'}) \\right)$ (3)\nHere, \u03a6k and 4 denote linear transformations and the transpose operation is denoted by (\u00b7)T. The obtained G\u00bf = {G,G,...} which contains the object representation and also the spatial representations Ft are passed to the Q-DGT module for video feature extraction."}, {"title": "3.3. Question-aware dynamic graph transformer", "content": "Following the methodologies proposed in [44], we utilize DGT to capture the complex dynamics of objects from the obtained graphs. However, different from [44], to enhance the relevance of the object dynamics to the specific questions posed, we condition the DGT on the question (Q-DGT), focusing the analysis only on the objects that are essential for answering the questions. This conditional approach ensures that our model's attention is selectively tuned to the pertinent elements of the video content. Furthermore, to enhance the contextual relevance of the visual information extracted by the Q-DGT module, our approach goes beyond the typical refinement processes described in [44], which focuses solely on global representations. We extend refinement to both global and local representations, thereby improving the accuracy and contextual depth of the answer prediction.\nIn Q-DGT, the question embedding Q\u00bf is intentionally masked to control the influence of the question representation on the model. This masking helps isolate specific features, mitigating the risk of overfitting by finely tuning the interaction between the question representation and the object dynamics captured by the DGT. Such an approach ensures that only the most relevant dynamics are emphasized, enhancing the model's accuracy and generalizability.\n$Q = M \\odot Q_i$\nHere, M is binary mask vector, denotes the element-wise (Hadamard) product.\nQ-DGT integrates both a temporal and a spatial graph transformer unit to process the input visual graphs within Gi. Ft is the input to the spatial unit that models the spatial relationships within each frame, and Fit' is the input to the temporal unit that captures relationships across different frames. These units are specifically designed to handle the different dimensions of data - temporal changes over time and spatial relationships within frames. The output of the Q-DGT 'is a local representation $F_{local}^{t'}$ corresponding to the t'-th frame, which is obtained by non-linearly transforming the embeddings from the frame-specific graph through a trainable projection layer with parameters local. This projection layer is crucial as it ensures that vital information pertinent to the video is preserved and not lost in transformation processes. The local representation formula is:\n$F_{local}^{t'} = \\phi_{local}(Q-DGT(G'_{t'}, \\hat{Q}))$ (4)\nAdditionally, a global representation Fglobal is derived by aggregating all the spatial and temporal representations through a global transformer, similar to the approach in [44]. This global transformer incorporates learnable sinusoidal temporal position embeddings to model the sequence of events within the video effectively. The outputs of this transformer are then mean-pooled to produce a comprehensive global representation Fglobal of the entire video, which encapsulates both the spatial and temporal dynamics across all processed frames. MHSA stands for Multihead Self Attention, and MPool represents the Maxpooling operation.\n$F_{global; i} = MPool(MHSA(Q-DGT(G_i, \\hat{Q}))) $ (5)\nFor further details regarding DGT, refer [44].\nInteraction of the question and graph features in Q-DGT: To integrate textual context effectively, we employ a RoBERTa language model to process the question Q and project the token outputs into a textual information space Z using a linear transformation:\n$Z_{\\hat{Q}} = \\phi(\\hat{Q}) = \\{z_h\\}_{h=1}^H,$\nwhere is a projection matrix in R768\u00d7d, H denotes the number of tokens in Q, and zh represents the embedded representation of the hth token. The encoded tokens include those representing the words of an open-ended question Q or QA pairs in a multiple-choice format.\nWithin the DGT framework, the cross-modal encoder Q-DGTcm processes the textual embeddings Z\u00f4 together with the visual embeddings \u0112t corresponding to the frame Vt for the ith instance. This integration facilitates a nuanced refinement of both local and global video representations:\n$F_{local} = Q-DGT_{cm}(F_{local}, Z_{\\hat{Q}}) = F_{local} + \\sum_{h=1}^H \\alpha_h^1 z_h $ (6)\n$F_{global} = Q-DGT_{cm}(F_{global}, Z_{\\hat{Q}}) = F_{global} + \\sum_{h=1}^H \\alpha_h^2 z_h $ (7)\nwhere a\u00b9 and a\u00b2 are attention weights. These weights are calculated by applying a sigmoid function o to the transpose"}, {"title": "3.4. Obtaining the final video features", "content": "Our method acknowledges the dynamic relevance of global image context and local properties based on the question posed. To adeptly handle this variability, we introduce an adaptive mechanism that updates the global embedding through a cross-attention process with the local embeddings obtained from the Q-DGT module.\nThe final representation of the answer leverages cross-attention between the local representations {Flocal} and the global representation Fglobal. Directly merging these representations often leads to redundancy due to overlapping information. To address this issue, our attention mechanism is designed such that Fglobal serves as the query, while {Flocal} function as both keys and values. This structure allows the model to dynamically emphasize the most relevant details from the local context when updating the global representation. This results in a more discriminative and contextually refined final representation.\n$F_{final} = (1 - \\gamma) F_{global} + \\gamma CrossAtt(F_{global}, \\{F_{local}\\}),$ (8)\ny is a weighting constant within the range [0,1]. Ffinal embodies a comprehensive, question-aware representation of the video. It seamlessly integrates the broad contextual overview provided by the global features with the detailed insights offered by the local features."}, {"title": "3.5. Answer generation", "content": "In our framework, we employ distinct strategies for answering objective and subjective questions, leveraging the synthesized representation Ffinal.\nFor objective questions, the answer prediction \u00c2 is determined by calculating the similarity scores between Ffinal and a set of pre-encoded answer representations FA. Here, A = {A}{4}, where |A| represents the number of answer options, and A\u2081 denotes the RoBERTa-encoded representation of each option l. The prediction is made by identifying the option associated with the highest similarity score:\n$\\hat{A} = arg max \\left( \\left(F_{final} \\right)^T A \\right)$ (9)\nWe adopt a methodology for subjective questions that enables a video-absent QA scenario, as delineated in prior works [44]. The answer is inferred by evaluating the similarities not only between Ffinal and A but also between the question representation Q and A. The final prediction \u00c2 is obtained by taking an element-wise product of these similarity matrices, thereby ensuring that the decision robustly integrates cues from both the video and the question:\n$\\hat{A} = arg max \\left( \\left(F_{final} \\right)^T A \\odot \\left(Q \\right)^T A \\right)$ (10)"}, {"title": "3.6. Loss objectives", "content": "Loss function for multi-choice QA: We employ a composite loss function in multi-choice question answering, where answers are selected from given options. The component Lvqa accounts for the interaction between the video, the question, and the multiple-choice options, while Lvq pertains solely to the video and the question:\n$L = L_{vqa} (F_{final}, Q \\& A^+, Q \\& A^- ) + \\lambda L_{vq}(F_{final}, Q^+, Q^-)$ (11)\nA+ and A represent the correct and incorrect answer options, respectively. Similarly, Q+ and Q denote the positive and negative questions associated with a video. The balancing parameter is represented by \u5165, and the symbol indicates a concatenation operation.\nLoss function for open-ended QA: For open-ended QA, where the answer a is not constrained to predefined options, the loss formulation needs to adapt to the broader scope of potential answers:\n$L = L_{vqa} (F_{final}, Q, A^+, A^- ) + \\lambda L_{vq}(F_{final}, Q^+, Q^-)$ (12)"}, {"title": "4. Experimental Evaluations", "content": "Datasets: : We conduct experiments across various datasets to evaluate different aspects of video understanding. The datasets include NEXT-QA [40], STAR-QA [38], and Causal-VidQA [20], which are designed to address complex temporal and causal relationships as well as commonsense reasoning within videos, with a particular focus on temporal dynamics. Additionally, we utilize TGIF FrameQA [14], MSRVTT-QA [45], and ActivityNetQA [4], which concentrate on the recognition of video objects, their attributes, actions, and activities, emphasizing static frame analysis."}, {"title": "4.1. Main results", "content": "We compare LGQAVE with several relevant and recent methods from the literature in Table 1 on all the datasets mentioned above. LGQAVE significantly surpasses the previous SOTAs on all tasks defined in previously mentioned datasets, improving the accuracy on an average by 9.29% vs. non-LLM methods like CoVGT [44] and 6.61% vs. LLM models like VideoLlava [23], respectively. Compared to other methods, we paid more attention to the video content related to the question instead of taking all the video"}, {"title": "5. Ablation analysis", "content": "To better understand the contribution of each component in the proposed model LGQAVE, we conducted ablation over the various components, shown in Table 3. The inclusion of a sampling strategy markedly enhances our model's performance. Without sampling (Configuration C-1), the model depends solely on global representations, which limits its focus on pertinent frames and leads to reduced accuracy, notably in Acc@All. Introducing sampling in Configuration C-2 improves focus on relevant frames, resulting in significant performance gains across all metrics, particularly in Acc@C (+3.36%) and Acc@T(+3.15%), by filtering out extraneous information.\nThe integration of miniGPT in C-3, combined with sampling but excluding local representations, significantly enhances accuracy, particularly in Acc@D(+6.16%), suggesting that miniGPT enriches the model's contextual understanding and response accuracy. In contrast, using graphs with all objects in a frame and employing both local and global representations leads to a severe drop in accuracy, as observed in C-4. In C-5, we leverage local and global representations by cross-attention to balance detailed object-level insights and broader scene context, resulting in the highest accuracy across all metrics. This approach outperforms models that rely solely on global features by +9.29% in Acc@All.\nIn Figure 3, we analyze the impact of varying the parameter \u03b2 and y across four datasets: NextQA, CasualQA, MSRVTT, and StarQA. The study suggests that the optimal \u03b2 and y are 0.4 and 0.9, respectively, where the highest performance is observed, with performance declining at higher or lower values. Also, we show that our cross-attention of Flocal and Fglobal gives better accuracy than using Fglobal or pooled Flocal or concatenating them.\nFig 4 highlights the impact of various model configurations on the task of answering video questions. The sampling module, made of graphs and global and local features, was individually assessed for their contribution to the model's overall performance. By isolating each module, the study reveals that using the question-aware object interaction graphs in combination with Local Features significantly improves the accuracy of the model's predictions. For instance, it enables the model to generate more specific and contextually appropriate answers, such as distinguishing between actions like \"Training the dog\" and \"Spectating the dog run.\" This suggests that incorporating both question-awareness and fine-grained local features plays a crucial role in understanding video content."}, {"title": "6. Takeaways", "content": "We present LGQAVE, a novel framework that addresses limitations in existing VideoQA approaches by enhancing multi-modal integration and focusing on semantic visual concepts relevant to the questions. Using cross-attention, LGQAVE identifies the most pertinent video frames for each query, surpassing traditional frame sampling techniques. Our approach generates precise video representations by capturing object dynamics through spatial graphs and grounding them in question semantics via the MiniGPT model. Q-DGT refines these representations, ensuring global and local video content is optimally encoded. An additional cross-attention module synthesizes final video embeddings conditioned on the questions, leading to more accurate answer generation by the language model. Extensive evaluations across benchmarks show that LGQAVE significantly improves accuracy in multi-choice and open-ended VideoQA tasks, suggesting future opportunities to leverage advanced graph-based and attention mechanisms for multi-modal integration."}]}