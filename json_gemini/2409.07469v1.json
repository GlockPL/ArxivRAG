{"title": "Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients", "authors": ["Mrs.Rashmi BN", "Dr. R.Guru", "Dr.Anusuya \u039c\u0391"], "abstract": "Advancements in object detection algorithms have opened new avenues for assistive technologies that cater to the needs of visually impaired individuals. This paper presents a novel approach for indoor assistance to the blind by addressing the challenge of small object detection. We propose a technique YOLO NAS Small architecture, a lightweight and efficient object detection model, optimized using the Super Gradients training framework. This combination enables real-time detection of small objects crucial for assisting the blind in navigating indoor environments, such as furniture, appliances, and household items. Proposed method emphasizes low latency and high accuracy, enabling timely and informative voice-based guidance to enhance the user's spatial awareness and interaction with their surroundings. The paper details the implementation, experimental results, and discusses the system's effectiveness in providing a practical solution for indoor assistance to the visually impaired.", "sections": [{"title": "1. Introduction", "content": "Assisting the visually impaired in navigating and interacting with indoor environments presents a significant challenge. Traditional approaches, such as canes and guide dogs, while helpful, have limitations in providing detailed information about the surroundings. Computer vision, particularly object detection, offers a promising avenue to enhance indoor navigation for the blind by providing real-time information about nearby objects. This paper introduces a novel system designed to aid the visually impaired in indoor environments by leveraging the power of small object detection. Our approach combines the efficiency of the YOLO NAS Small architecture with the advanced training capabilities of the Super Gradients framework. This combination allows us to achieve real-time performance crucial for delivering timely voice-based guidance to the user [1].We focus on detecting small objects commonly found in indoor settings, such as furniture, appliances, and household items, which are essential for safe and independent navigation. By accurately identifying and locating these objects, our system can provide blind individuals with a greater understanding of their surroundings, enabling them to make informed decisions about their movements. This paper details our proposed system's architecture, implementation, and experimental evaluation. We demonstrate its effectiveness in detecting small objects in indoor environments and discuss its potential to significantly improve the lives of visually impaired individuals by fostering greater independence and safety in their daily lives [2]."}, {"title": "1.1 Problem Statement", "content": "While object detection shows promise for assisting the visually impaired with indoor navigation, accurately detecting small objects in real-time, crucial for safe and independent movement, remains a significant challenge. Existing object detection models often struggle to balance accuracy and efficiency when dealing with small objects, limiting their practicality for real-time assistive technologies. This research addresses this problem by developing a system that leverages the YOLO NAS Small architecture and Super Gradients training framework to achieve accurate and efficient small object detection for providing real-time, voice-based guidance to the blind in indoor environments."}, {"title": "1.2 The Main Contributions made in the work", "content": "This research makes significant contributions by:\n\u2022\tImplemented a novel system specifically designed for small object detection in indoor environments to assist the visually impaired, leveraging the efficiency of the YOLO NAS Small architecture and the advanced training capabilities of the Super Gradients framework.\n\u2022\tDemonstrating the system's effectiveness in accurately detecting small objects crucial for indoor navigation, such as furniture, appliances, and household items, while maintaining real-time performance necessary for practical assistive technology.\n\u2022\tProviding a foundation for future research in developing robust and user-friendly assistive technologies for the visually impaired by showcasing the potential of combining state-of-the-art object detection models with tailored training frameworks.\nThis paper is framed as follows: section 1, provides the brief introduction with problem statement and contributions made in this work. Section 2, provides the related work and limitations of the existing work. Section 3 provides the detailed description of the methodology followed and algorithms proposed. Section 4 presents the results obtained and comparative analysis and finally conclusion is presented in section 5."}, {"title": "2. Related Work", "content": "Assisting the visually impaired in indoor environments is a crucial challenge that has garnered significant attention in recent years [3]. One promising approach to this problem is the use of computer vision techniques for detecting and recognizing small objects, which can then be conveyed to the user through voice assistance [4]. The YOLO (You Only Look Once) object detection model has emerged as a popular choice for this task due to its real-time performance and ability to handle a wide range of object types. However, the standard YOLO architecture may struggle with the detection of small objects, which are often crucial for assisting the blind in indoor settings. To address this limitation, researchers have proposed the use of the YOLO NAS Small architecture, which is designed to be more efficient and effective at detecting small objects [5]. This model utilizes a neural network-based regression approach to detect objects and a classification algorithm to identify them, streamlining the process into a single, fast pass through the network.\nThe field of small object detection has witnessed remarkable advancements in recent years, primarily driven by the advent of convolutional neural networks (CNNs). These developments have led to the emergence of two distinct methodologies: anchor-free and anchor-based approaches [18]. Within the anchor-based paradigm, researchers have explored both single-stage and two-stage algorithms. Two-stage algorithms, exemplified by the RCNN family, involve an initial region proposal generation followed by bounding box refinement. While these methods achieve high Mean Average Precision (mAP), their multi-stage architecture often results in slower inference speeds, limiting their applicability in real-time scenarios [19]. Despite efforts to enhance RCNN's efficiency through innovations like Fast-RCNN [20], Faster-RCNN [21], and Mask-RCNN [22], the fundamental two-stage structure continues to pose challenges for real-time performance.\nIn contrast, single-stage algorithms, notably the You Only Look Once (YOLO) family [23], have garnered significant attention for their speed and efficiency. YOLO's unified network architecture, which simultaneously predicts bounding box coordinates and class probabilities, enables remarkably fast inference. However, early YOLO iterations faced challenges such as relatively lower mAP and difficulties in detecting densely grouped objects. Subsequent YOLO versions have addressed these limitations, culminating in the recent release of YOLOv8. While YOLOv8 represents a significant leap forward in balancing real-time performance and accuracy, there remains potential for further optimization and enhancement in object detection algorithms.\nLin et al. [9] proposed a wearable assistive system utilizing deep learning for object detection and recognition to aid visually impaired individuals. While it doesn't explicitly focus on small object detection, it highlights the potential of deep learning in creating practical assistive technologies. Alagarsamy et al. [10], explored on real-time object detection using YOLO V3 and R-CNN for assisting the visually impaired. It emphasizes the importance of real-time performance and accurate object recognition for providing effective guidance and enhancing independence. Kuriakose et al. [1], presents DeepNAVI, a smartphone-based navigation assistant that utilizes deep learning for obstacle detection and scene recognition. It underscores the need for lightweight"}, {"title": "3. Methodology", "content": "The initial step in our methodology shown in figure 1, involves capturing images from indoor environments using a device, such as a camera or smartphone, carried by the blind individual. These images serve as the primary input for our system. To ensure the robustness of our detection model, the captured images undergo a comprehensive preprocessing stage. This stage includes image normalization to adjust brightness, contrast, and scale, thereby ensuring consistency in the input data. Additionally, data augmentation techniques, such as rotation, flipping, and scaling, are applied to increase the diversity of the training dataset. This augmentation helps in mitigating overfitting and enhances the model's ability to generalize across different indoor scenarios.The core of our system is the YOLO NAS Small model, specifically designed for efficient small object detection. The model comprises three main components: the backbone network for feature extraction, the neck network for aggregating features at various scales, and the head network for predicting bounding boxes, class probabilities, and confidence scores. To optimize the performance of YOLO NAS Small, we incorporate the Super Gradients optimization technique. This involves efficient gradient computation and the application of advanced training enhancements, such as learning rate scheduling, momentum, and weight decay. These techniques collectively ensure faster convergence and improved accuracy of the model. The training process is conducted on a diverse dataset of indoor images to fine-tune the model parameters for optimal detection performance.\nOnce the YOLO NAS Small model generates predictions, the outputs undergo a postprocessing stage to refine the detection results. Non-Maximum Suppression (NMS) is employed to eliminate redundant bounding boxes, retaining only the most accurate detections. Further refinement of the bounding boxes is performed to precisely match the detected objects. The final step involves converting the detection results into assistive feedback for the blind individual. This feedback can be delivered through auditory or haptic means, providing real-time information about the presence and location of small objects in the indoor environment. The seamless integration of detection and feedback mechanisms ensures an efficient and user-friendly system for indoor assistance to the blind.\nThe YOLO NAS Small model is particularly well-suited for deployment on resource-constrained devices, such as those used in assistive technologies for the blind [7]. To further enhance the performance of YOLO NAS Small on small objects, we integrate the Super Gradients training framework. Super Gradients is a state-of-the-art deep learning training library that offers advanced optimization techniques and architectural modifications to improve the accuracy and efficiency of neural networks. Our experiments demonstrate the effectiveness of this approach for small object detection in indoor environments, with a focus on objects that are crucial for assisting the blind, such as furniture, appliances, and other household items [8]."}, {"title": "3.1 Data Collection", "content": "To train our system for robust small object detection in indoor environments, we compiled a diverse dataset comprising images captured from various indoor settings, including homes, offices, and public spaces named roboflow. The dataset focused on capturing a wide range of small objects commonly encountered by visually impaired individuals, such as furniture (chairs, tables, shelves), appliances (refrigerators, microwaves, televisions), and household items (books, cups, remote controls). We ensured data diversity by varying lighting conditions, camera angles, and object occlusions to enhance the model's generalization capabilities. The Roboflow is a platform that provides tools and services for building and managing datasets, particularly for computer vision tasks. It offers a wide range of publicly available datasets that can be used for training and testing machine learning models. Roboflow datasets cover a wide range of applications, from object detection and classification to segmentation tasks, and they are used by researchers and developers to build and train computer vision models efficiently [12]. From roboflow platform,\nThe Yale-CMU-Berkeley (YCB) COCO dataset is a comprehensive and versatile benchmark for object detection and manipulation research [13]. YCB-3 Dataset is utilised for conducting the experiments. YCB-3 is a subset of the YCB (Yale-CMU-Berkeley) Object and Model Set, which is widely used for benchmarking in robotics, computer vision, and machine learning. his dataset combines the rich object variety of the YCB Object and Model Set with the annotation style and evaluation metrics of the popular COCO (Common Objects in Context) dataset. It features a diverse collection of everyday objects, carefully selected to represent a wide range of shapes, sizes, textures, and material properties. The dataset includes high-quality RGB images, depth maps, and precise 6D pose annotations for each object instance. Particularly relevant for our study on small object detection for indoor assistance to the blind, the YCB COCO dataset offers a subset focusing on small objects commonly found in indoor environments. This subset includes items such as food containers, fruits, household items, and sports equipment, making it ideal for training and evaluating models aimed at assisting visually impaired individuals in navigating and interacting with their immediate surroundings. The standardized COCO-style annotations facilitate easy integration with existing object detection frameworks, enabling fair comparisons across different models and methodologies."}, {"title": "3.2 Pre-Processing", "content": "Pre-processing involved resizing images to a consistent resolution suitable for the YOLO NAS Small architecture while preserving aspect ratios to avoid distortions. We applied data normalization techniques, specifically mean subtraction and standard deviation scaling, to standardize pixel values across the dataset, improving training stability and convergence speed. Data augmentation techniques, including random horizontal flipping, cropping, and minor rotations, were employed to artificially increase the dataset size and expose the model to a wider range of object appearances and orientations, further enhancing its robustness and ability to generalize to unseen scenarios."}, {"title": "3.3 YOLO NAS", "content": "Deci, a firm that builds production-grade models and tools to construct, optimise, and deploy deep learning models, launched YOLO-NAS [25] in May 2023 as shown in figure 2. With its ability to identify small objects, increase localization precision, and boost performance-per-compute ratio, YOLO-NAS is well suited for real-time edge-device apps. Furthermore, researchers can leverage its open-source architecture. Among YOLO-NAS's innovations are the following:\n\u2022\tQuantization-aware modules (QSP and QCI) [26], which integrate re-parameterization to reduce the accuracy loss during post-training quantization, opt for 8-bit quantization.\n\u2022\tDeci's exclusive NAS technology, AutoNAC, for automatic architecture design.\n\u2022\tUsing a hybrid quantization technique, you can quantize specific model components to achieve equilibrium.\n\u2022\tRather of using normal quantization, which affects all layers, consider latency and accuracy.\n\u2022\tPre-training consisting of self-distillation, automatically labelled data, and big datasets\nAfter pre-training the model using Objects365 [27], which has two million photos and 365 categories, pseudo-labels were produced using the COCO dataset. Lastly, the original 118k training photos from the COCO dataset are used to train the models. Three YOLO-NAS models with 16-bit precision and FP32, FP16, and INT8 precisions have been made available. They have an AP of 52.2% on MS COCO. YOLO-NAS was made possible by the adaptability of the AutoNAC system, which can handle any work, the details of the data, the establishment of performance targets, and the environment for drawing conclusions. It helps users choose the best structure that provides the ideal balance between accuracy and inference speed for their specific"}, {"title": "3.4 Super Gradients", "content": "While the YOLO NAS Small architecture provides an efficient base for our system, achieving optimal performance for small object detection in real-time requires a robust training framework. This is where Super Gradients plays a crucial role. Super Gradients is a powerful deep learning training library that offers advanced optimization algorithms, hyperparameter tuning strategies, and architectural modifications tailored for improved accuracy and efficiency. By integrating Super Gradients into our training pipeline, we leverage these advanced features to fine-tune the YOLO NAS Small model specifically for our task. This includes utilizing optimized learning rate schedules, data augmentation strategies, and loss functions tailored for small object detection. This targeted optimization process ensures that our model effectively learns to identify and localize small objects"}, {"title": "3.5 Non-Maximum Suppression", "content": "Object detection models, including YOLO NAS Small, often predict multiple bounding boxes for a single object, especially small objects, leading to redundant detections. Non-Maximum Suppression (NMS) is a crucial post-processing step that addresses this issue, ensuring that only the most accurate bounding box for each object is retained. In our methodology, NMS analyzes the predicted bounding boxes and their associated confidence scores. It starts by selecting the box with the highest confidence score. Then, it iteratively suppresses boxes that significantly overlap with the selected box and have lower confidence scores. This process continues until only the most confident and non-overlapping boxes, representing distinct object detections, remain. By applying NMS, we refine the model's output, reducing false positives and enhancing the accuracy of small object detection, which is essential for providing reliable guidance to visually impaired users."}, {"title": "3.5.1. Efficiency and Speed:", "content": "\u2022\tLightweight Architecture: YOLO NAS Small is designed for efficiency with a smaller model size and fewer computations compared to larger models like YOLOv7 and YOLOv8. This translates to faster inference speeds, crucial for real-time object detection and providing timely assistance to visually impaired users.\n\u2022\tSuper Gradients Optimization: Super Gradients further enhances efficiency by employing techniques like quantization and pruning, which reduce model size and complexity without significantly sacrificing accuracy. This optimization is crucial for deployment on devices with limited processing power, such as those used in assistive technologies."}, {"title": "3.5.2. Small Object Detection Focus:", "content": "\u2022\tArchitecture Tailored for Small Objects: YOLO NAS Small's architecture incorporates design choices that make it more sensitive to smaller objects. This could include using higher-resolution feature maps or specialized layers to capture fine-grained details often lost in larger models.\n\u2022\tSuper Gradients Data Augmentation: Super Gradients likely employs data augmentation techniques specifically designed to improve small object detection, such as mosaic augmentation or techniques that artificially increase the size of small objects during training."}, {"title": "3.5.3. Practicality for Assistive Technology:", "content": "\u2022\tResource Constraints: Assistive devices often have limited computational resources and battery life. YOLO NAS Small's efficiency and Super Gradients' optimization make it a practical choice for deployment on such devices.\n\u2022\tReal-time Performance: The faster inference speeds offered by YOLO NAS Small are essential for providing real-time feedback to users, enabling them to navigate their surroundings effectively and safely."}, {"title": "3.6 Object Localization and Assistive Feedback", "content": "Google Text-to-Speech (gTTS) is utilised to generate audio feedback for object localization and assistive feedback in an indoor assistance system for the blind. It iterates through a range of predictions (from 0 to 12) and extracts a specific part of the class name of each predicted object. The extracted part, p, is then converted to speech using gTTS, which saves the spoken text as a .wav file. The filename is dynamically generated based on the iteration index i. Finally, each saved audio file is played back automatically using IPython's Audio class, providing real-time audio feedback for each object detected in the scene, thus aiding visually impaired users in understanding their environment."}, {"title": "4. Implementation and Results", "content": "For our experimental setup, we utilized a robust GPU-accelerated system to expedite the training and evaluation processes. Our hardware configuration consisted of an NVIDIA GeForce RTX 3070 GPU with 8GB of GDDR6 memory, ensuring sufficient computational power and memory bandwidth to handle the demands of deep learning tasks. The system also included an Intel Core i7-11700K CPU and 32GB of DDR4 RAM to manage data loading, pre-processing, and other system-level operations efficiently. We implemented our proposed"}, {"title": "5. Conclusion", "content": "This study presents a novel approach to small object detection for indoor assistance to the blind, leveraging the YOLO NAS Small architecture optimized with Super Gradients. Our research demonstrates significant advancements in both the accuracy and efficiency of detecting small objects crucial for navigating indoor environments, addressing a critical need in assistive technology for visually impaired individuals.Key findings of our study include:\n\u2022\tSuperior Detection Performance: Our model achieved a mAP@0.50 of 0.96 and a recall of 0.98, outperforming several state-of-the-art object detection models in identifying small objects in indoor settings. This high recall is particularly crucial in ensuring that visually impaired users are made aware of all potential obstacles or items of interest in their environment\n\u2022\tInnovative Integration of YOLO NAS Small and Super Gradients: The synergy between the lightweight YOLO NAS Small architecture and the optimization capabilities of Super Gradients proved highly effective in addressing the unique challenges of small object detection for blind assistance.\n\u2022\tRobustness in Varied Indoor Environments: Through extensive testing on diverse indoor scenes from the coco dataset, our model demonstrated consistent performance across various lighting conditions and spatial arrangements, crucial for real-world applicability.\nThe implications of this research extend beyond technological advancement. By improving the accuracy and reliability of small object detection in indoor environments, our work has the potential to significantly enhance the independence and quality of life for visually impaired individuals. The ability to navigate indoor spaces with greater confidence and safety opens up new possibilities for social inclusion and autonomy.In conclusion, our work on small object detection using YOLO NAS Small and Super Gradients represents a significant step forward in creating more effective and reliable assistive technologies for the visually impaired. By pushing the boundaries of what's possible in real-time, efficient object detection, we contribute to the broader goal of creating more inclusive and accessible environments for all individuals, regardless of visual ability."}], "equations": ["I_{norm}=\\frac{I-\\mu}{\\sigma}", "L = \\sum_{i=1}^{n}(Loss_{box}(P_i, G_i) + Loss_{class}(P_i, G_i))", "\\triangledown_{\\theta}L = \\alpha \\triangledown L + \\beta \\nabla^2 L + \\gamma \\triangledown^3 L", "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}L", "Loss_{cls} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C}y_{i,c}\\log(\\hat{y}_{i,c})", "Loss_{iou} = 1 - \\frac{|B \\cap B_{gt}|}{|B \\cup B_{gt}|}", "Loss_{dfl} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{4}\\sum_{k=1}^{K}y_{i,j,k}\\log(\\hat{y}_{i,j,k})", "Loss = Loss_{cls} + \\lambda_{iou}Loss_{iou} + \\lambda_{dfl}Loss_{dfl}", "Precision @0.50 = \\frac{True Positives(TP)}{True Positives(TP) + False Positives(FP)}", "Recall @50 = \\frac{True Positives(TP)}{True Positives(TP) + False Negative(FN)}", "MAP @0.50 = \\frac{1}{N} \\sum_{i=1}^{N}AP", "F1@0/50 = 2.\\frac{Precision @0.50.Recall@0.50}{Precision@0.50.Recall@0.50}"]}