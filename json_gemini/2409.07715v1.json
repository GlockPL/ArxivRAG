{"title": "FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments", "authors": ["Devansh Dhrafani", "Yifei Liu", "Andrew Jong", "Ukcheol Shin", "Yao He", "Tyler Harp", "Yaoyu Hu", "Jean Oh", "Sebastian Scherer"], "abstract": "Robust depth perception in visually-degraded environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas. The dataset and source code are available at https://firestereo.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotics has great potential to help in environments characterized by one or more of the five \"D's\": dirty, dull, dangerous, difficult, and dear. Examples of such environ- ments include disaster response [1], construction [2], mining [3], and waste management [4], which often contain dense and cluttered obstacles. These environments may also suffer from visual degradations like smoke, dust, and darkness. To operate effectively, robots need a robust and accurate geometric understanding of the scene, allowing them to estimate obstacle distance and localize within a perceived map. However, these visual degradations interfere with com- mon sensors used for perception: particulates affect LiDAR (smoke, dust, rain, snow), while darkness hinders RGB cameras.\nOne type of sensor that is well suited for perception in smoke, dust, and darkness is the long-wave infrared (LWIR) thermal camera. Its infrared signal, in the 8 to 14 \u00b5m range, bypasses smoke and dust as shown in Fig. 2, and the heat signature of the environment still radiates in visual darkness.\nHowever, the development of thermal perception for cluttered environments is stalled by a scarcity of relevant"}, {"title": "II. RELATED WORK", "content": "A. Metric depth estimation\nReal-time inference of metric distance to obstacles is required for online unmanned aerial system (UAS) nav- igation. A representative method to estimate metric-scale depth maps is to convert a stereo image's disparity map into depth maps using the camera focal length f and baseline B (i.e. seperation between cameras). The disparity map encodes depth information of a scene by measuring the pixel displacement between corresponding points in stereo images. The conversion from disparity d to depth Z is given by:\n$Z = \\frac{f. B}{d}$                                                            (1)\nRecent stereo-matching methods achieve high accuracy and efficiency using neural architectures to estimate dis- parity, categorized into 3D [11-13] and 4D cost volume methods [14-16]. While 3D methods are more efficient by constructing single-channel cost volumes, 4D methods show more accurate disparity maps by constructing multiple- channel cost volumes and aggregating them via 3D convolu- tions. However, both approaches face a high computational burden, limiting their suitability for real-world applications and on-device inference. Therefore, another research area is designing a lightweight stereo-matching network for on- device inference [17], [18]. Models like MobileStereoNet [17] and FastACVNet [18] achieve efficiency by leveraging lightweight architectures and novel cost volume formula- tions, reducing computational complexity.\nWe train representative models from the discussed cate- gories on our dataset to benchmark their performance and analyze strengths and limitations for real-time metric depth estimation on UAS platforms.\nB. Visual datasets\nVisual sensors capture the visible spectrum of light and are commonly used in robotics applications. A diverse large- scale image dataset paired with ground truth depths are crucial for training deep learning models to predict distance. Prominent datasets like KITTI [20], Oxford [21], DDAD [22], Cityscape [23], and nuScenes [24] provide such pairs but are limited to driving scenes with constrained motion patterns, typically in the forward direction with small left or right turns [25]. Training on these datasets often lead to models that struggle to generalize to UAS tasks, which involve more aggressive, multi-axial motion and smaller stereo camera baselines.\nInteriorNet [26] and SceneNet [27] are synthetically gen- erated whereas TUM RGB-D [28] and ScanNet [29] are real world indoor datasets with aggressive motion patterns and small camera separation. However, the latter three lack stereo image pairs, critical for metric depth estimation. Although InteriorNet provides stereo pairs, it is limited to indoor envi- ronemnts, leading to poor generalization in outdoor settings. TartanAir [25] is a synthetic UAS dataset that provides stereo images with ground truth depth across various indoor and outdoor urban, rural, and nature scenes, resembling potential UAS diaster response scenarios. However, visible-spectrum cameras remain vulnerable to factors like illumination, dust, and smoke. Thermal cameras, being more robust to such conditions, have gained increasing attention in the robotics community.\nC. Thermal datasets\nThermal cameras, which capture infrared radiation from object surfaces, have gained popularity because of their robustness to adverse conditions, widespread availability and unique modality information (i.e., temperature). As thermal cameras became more accessible, many thermal datasets have been proposed. MultiSpectralMotion [6] is an indoor- outdoor handheld dataset with thermal image and ground truth depth map. OdomBeyondVision [7] is an indoor-only thermal dataset with handheld, unmanned ground vehicle (UGV) and UAS movements. ViViD++ [5] is an outdoor thermal dataset with vehicle and handheld movements. SubT- MRS [19] is an outdoor dataset with vehicle, UAS, legged and handheld motion in varying levels of visual degradation. SubT-MRS is the closest to our environmental needs but it does not contain wildland forest images. Moreover, all the above datasets include single thermal camera views and so cannot be used to train stereo depth estimation models."}, {"title": "III. FIRESTEREO DATASET", "content": "A. System overview\nHardware setup We designed a data collection platform comprising a pair of stereo thermal cameras, a LiDAR, and an inertial measurement unit (IMU), as illustrated in Fig. 3. These sensors are mounted on an unmanned aerial vehicle (UAV) platform, which facilitates data collection during handheld experiments and UAV flights. Sensor specifications are presented in Table II. The stereo thermal pair is oriented in a forward-facing direction with a 24.6 cm baseline, the LiDAR is mounted on top of the UAV, and an onboard NVIDIA\u00ae Jetson AGX Orint\u2122 computer is connected to the sensors.\nTime Synchronization Accurate time synchronization is a critical prerequisite for various tasks that involve multiple sensors. Our synchronization of the sensors is carried out us- ing the pulse-per-second (PPS) technique. The IMU, LiDAR, and thermal cameras are synchronized by electronic pulses from the CPU clock.\nB. Data collection\nSensor data was recorded through the Robot Operating System (ROS) framework running onboard the Orin. In addition to the processed dataset, we provide the corre- sponding dataloader, calibration files, and rosbags covering all data collection sessions. For Camera-IMU calibration, we use Kalibr [30] with a radial-tangential distortion model and heated checkerboard target [31]. For the FLIR Boson thermal camera, we capture the raw 16-bit data. The Flat Field Correction (FFC) of the thermal camera was set to manual and triggered at the start of each sequence, but not during collection. The data includes recordings from 4 distinct locations and 16 unique trajectories under various environmental conditions, including day, night, rain, cloud cover, and smoke. Smoke was emitted from training-grade smoke pots. All collected sensor data is time-synchronized with the CPU clock, making it also suitable to use for map- ping and localization. A closed-loop trajectory was followed with the same initial and final position, making the data set suitable for testing loop closure and accumulated drift for mapping and localization. Examples of such trajectories may be found in the 3D reconstruction maps in Fig. 4.\nC. Ground truth depth/disparity generation\nSupervised training of depth perception models requires Ground-Truth (GT) depth or disparity map outputs that correspond to each pair of input stereo images. Convention- ally, LiDAR-based datasets [8], [20] produce these maps by aggregating multiple LiDAR scans via the Iterative Closest Point (ICP) algorithm [32]. Although this methodology is"}, {"title": "D. Data description", "content": "The four collection sites differ significantly in tree density, vegetation, and proximity to urban structures such as parking lots and vehicles. We analyze the characteristics of our dataset quantitatively and qualitatively.\nQuantitative Data Description The processed FIReStereo dataset contains 204,594 stereo thermal images total across all environments. 29% are in urban environment, 15% are in mixed environment, 56% are in wilderness environment with dense trees. 84% of the images were collected in day- time and the rest were during night-time. Obstacles were measured at a median depth of 7.40 m with quartiles q1 = 5.17 m and q3 = 10.52 m, which falls within the typical range for UAS obstacle avoidance. 42% stereo thermal pairs are smokeless, while 58% contain smoke. Of the smokeless images, we label 35,706 with depth-map pairs annotated from LiDAR and Faster-LIO.\nFor the depth-annotated images, we train supervised mod- els representative of the depth-estimation literature on the data containing LiDAR ground truth. For model training and evaluation, we divide the depth-annotated data into train, validation, and test subsets, with approximately a 70%, 15%, 15% split, resulting in 25,653, 5,032, and 5,021 images, respectively. We ensured that each subset has a similar distribution of weather conditions and the type of scene. As some sequences were significantly longer than others, we split the sequences into two or more parts and then distributed the parts to one of the three subsets.\nQualitative Data Description Fig. 5 and Fig. 6 illustrate examples of the variety of data collected. The Hawkins se- quences feature scenes of dense forests and urban structures, recorded during cloudy, windy daytime conditions. In the first two sequences, the UAS navigates around dense trees with varying branch thickness, many of which are bare due to the spring season, alongside some evergreens. The next two sequences were captured in urban environments, with sequence 3 showing a parking lot with sparse trees and in- cludes views of a thin pole, trees, and buildings representing typical urban obstacles that a UAS might face in response to a wildfire disaster. Sequence 4 captures a car, pole and distant trees. Sequence 5 replicated a disaster scenario with an upside-down car engulfed in dense smoke. We later show inference on this data to demonstrate generalizability to unseen smoke-filled data.\nThe Frick experimental sequences were recorded during the night and under rainy conditions. The captured tem- perature range for these sequences is much lower and is"}, {"title": "IV. EVALUATION", "content": "A. Thermal image pre-processing\nThermal images captured in raw 16-bit format present challenges such as low texture, low contrast, and high noise patterns. To improve image quality and facilitate feature ex- traction for depth estimation, we apply several pre-processing steps to the stereo images:\n1. Intensity Binding [8]: Constrains the image histogram to the 1-99 percentile range to mitigate extreme outliers like the sun or smoke bombs, which can cause pixel saturation and detail loss.\n2. CLAHE (Contrast Limited Adaptive Histogram Equalization) [35]: Enhances local contrast and visibility without amplifying noise by adjusting tile histograms to a specified profile.\n3. Bilateral Filtering [36]: Preserves edges and reduces noise by averaging pixel intensities with spatial and radio-metric weights, maintaining edges in cluttered environments.\nThese preprocessing steps produce 8-bit images optimized for depth estimation models, characterized by enhanced contrast, preserved textures, and reduced noise as seen in Fig. 7. Lastly, we align the contrast between processed left and right stereo images IL, IR by equalizing their mean intensity \u00b5 as shown in Eq. (2).\n$\\begin{aligned} &J_{I R}=I_{R} \\frac{\\mu_{L}}{\\mu_{R}} \\text { if } \\mu_{R}<\\mu_{L} \\\\ &J_{I L}=I_{L} \\frac{\\mu_{R}}{\\mu_{L}} \\text { if } \\mu_{L}<\\mu_{R} \\end{aligned}$                                                            (2)\nB. Depth Model Selection\nWe implemented 5 representative stereo depth estimation models to evaluate the capabilities of our new dataset in facilitating robust depth estimation for UAS navigation in cluttered environment.\nLightweight networks: We selected 2 models optimized for rapid inference times, which is essential for deploy- ment in sUAS where computational resources and response times are limited. Fast-ACVNet [18] introduces Attention Concatenation Volume and Volume Attention Propagation for optimized cost volume construction and interpolation. MobileStereoNet [17] leveraged efficient point-wise and depth-wise convolutions to perform fast stereo matching on mobile platforms and its 2D version is used.\n3D networks: We selected 3 stereo depth model optimized for performance. AANet [13] proposed sparse points-based intra-scale cost aggregation to address edge-fattening issues. GWCNet [15] uses group-wise correlation to construct cost volume for improved feature similarities. PSMNet [14] uses spatial pyramid for global context and 3D CNN to regularize cost volume. These models benchmark our dataset's depth prediction capabilities within a well-established framework.\nC. Loss and Evaluation Metrics\nModels are trained with a smooth L1 loss as implemented in their official source code. To evaluate the models, we"}, {"title": "V. RESULTS", "content": "A. Implementation details\nAll models are trained until convergence using the archi- tecture provided in the official source code repository. We employ the Adam optimizer for gradient descent with a learn- ing rate of 1e-4. Training is conducted on 4x NVIDIA H100 80GB GPUs. The input image size is fixed at 640x512 for all models, with a batch size of 16. Networks are initialized with the provided backbone feature extractor model, following their original implementations [13], [15], [17], [18].\nB. Quantitative results\nWe report the quantitative metrics on the test set for the selected representative models analyzed separately for the day/cloudy and night/rainy conditions. For computational performance, we measure the Floating Point Operations Per Second (FLOPS) in billions and average inference time in milliseconds at a 640x512 resolution on an NVIDIA RTX 3060 GPU. Results are shown in Table IV.\nWe observe that all models perform better in day/cloudy conditions compared to night/rainy conditions. The best aver- age EPE score was 1.27px from AANet and Fast-ACVNet. GwcNet-gc performs best in D1 and the 1,2,3 px outlier errors, with a D1 score of 8.17%. However, GwcNet-gc has 6.91M parameters, while PSMNet, AANet, Fast-ACVNet,\nMobileStereoNet-2D have 5.23M, 4M, 3.2M, and 2.35M parameters respectively.\nWe observe that the lightweight model, Fast-ACVNet, with an inference speed of 42ms and a FLOPS of 39.5 G, delivers performance comparable to the representative GwcNet-gc and PSMNet model. Therfore, we select Fast- ACVNet for the qualitative results, as it is best suited for running on a low Size, Weight, Power, and Cost (SWaP-C) system while maintaining similar performance to the more resource-intensive models.\nC. Qualitative results\nWe report the qualitative inference results on the test set and compare the disparity prediction with ground truth. Additionally, we also run model inference on unseen smoke- filled images to show model generalization.\nFig. 8 shows the qualitative results on the test set. We evaluate the model in day and night conditions. We observe that the model can estimate the disparity for challenging objects such as thin tree branches and poles. We also note that the disparity is accurately estimated for nearby as well as far obstacles.\nWe further evaluate the trained model on unseen environ- ment with highly dense smoke conditions. The results are shown in Fig. 9. In Fig. 9(a) the smoke bomb is seen as a distinct hotspot and there is a trail of hot smoke near it. We also see a faint heat signature of the smoke bomb behind the turned car in Fig. 9(e). We observe that the thermal cameras are able to see through most of the smoke. The model is able to correctly estimate the obstacle disparity for these conditions. We can thus infer that the models trained with our training set generalize to smoke-filled environments."}, {"title": "VI. DISCUSSION", "content": "A critical aspect of ensuring accurate depth projection in our system is camera and LiDAR calibration. While we employed some manual corrections to address calibra- tion challenges, future work could benefit from automated LiDAR-Thermal cross-calibration tools designed to handle the specific properties of thermal cameras.\nFor future research, we suggest designing more lightweight stereo matching networks that reduce the"}, {"title": "VII. CONCLUSION", "content": "In this study, we introduced the FIReStereo dataset, specifically designed for enhancing geometric perception in visually degraded environments. This dataset leverages the capabilities of thermal long-wave infrared (LWIR) sensors, which are particularly effective in penetrating obscurities caused by darkness and smoke. Our evaluations demonstrate that models trained using the FIReStereo dataset not only closely adhere to established depth estimation benchmarks but also show substantial improvement in handling complex environments. Notably, our dataset enables depth prediction in scenarios where traditional ground truth collection is hindered by LiDAR interference from smoke.\nWe hope that our work will be used in advancing robotic perception capabilities for disaster response, enabling explo- ration and operation in areas previously deemed inaccessi- ble."}]}