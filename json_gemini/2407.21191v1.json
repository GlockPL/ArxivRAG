{"title": "GenRec: Generative Personalized Sequential Recommendation", "authors": ["Panfeng Cao", "Pietro Li\u00f2"], "abstract": "Sequential recommendation is a task to capture hidden user prefer-ences from historical user item interaction data. Significant progresshas been made in this domain by leveraging classification basedlearning methods. Inspired by the recent paradigm of \"pretrain,prompt and predict\" in NLP, we consider sequential recommen-dation as a sequence to sequence generation task and propose anovel model named Generative Recommendation (GenRec). Unlikeclassification based models that learn explicit user and item rep-resentations, GenRec utilizes the sequence modeling capability ofTransformer and adopts the masked item prediction objective toeffectively learn the hidden bidirectional sequential patterns. Dif-ferent from existing generative sequential recommendation models,GenRec does not rely on manually designed hard prompts. Theinput to GenRec is textual user item sequence and the output istop ranked next items. Moreover, GenRec is lightweight and re-quires only a few hours to train effectively in low-resource settings,making it highly applicable to real-world scenarios and helpingto democratize large language models in the sequential recom-mendation domain. Our extensive experiments have demonstratedthat GenRec generalizes on various public real-world datasets andachieves state-of-the-art results. Our experiments also validate theeffectiveness of the the proposed masked item prediction objectivethat improves the model performance by a large margin.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the great success of recommender systems in online platforms such as Amazon and Yelp, which help people make micro decisions and fulfil their demands in daily life. Within online platforms, historical sequential user interaction data is utilized to capture the evolved and dynamic user behaviors and make appropriate recommendations of next items for the user. Different from traditional recommender systems that treat each user behavior as a sample and directly model the user preference on a single item, sequential recommendation learns timestamp-aware sequential patterns to recommend the next item [7]. For example, one might purchase peripheral accessories after buying a desktop. Various methods have been proposed to accurately model sequential user behaviors. [6, 10, 11, 22, 23] employ Recurrent Neural Networks (RNN) to encode the left-to-right context. [20] utilizes Convolutional Neural Networks (CNN) to learn sequential patterns as local features. [2] leverages Graph Neural Networks (GNN) to construct the item-item interest graphs and user interest sequences. [27] designs contrastive loss to maximize the mutual information between different views of the sequential data. Contextual information such as item attributes is incorporated as self-supervised signals to learn user and item representations. The recent success of Transformer [21] based large language models (LLMs) in various NLP tasks has inspired the research of utilizing LLMs in recommender systems [9, 13, 19]. [13] utilizes the left-to-right self-attention mechanism to identify next relevant items from the sequential interaction history and [19] further improves the performance by employing the bidirectional self-attention mechanism. Although both methods are effective, they do not consider personalization and the model performance is potentially limited. Generative LLMs based recommender system is considered a promising approach [15]. [9] is the first model to unify different recommendation tasks in the same framework to facilitate the knowledge transfer and it is pretrained across task-specific datasets to capture the deep semantics for personalization and recommendation. However, different modality information is encoded in the textual format of natural language, which might cause suboptimal modality representation. Moreover, [9] relies on manually designed task-specific hard prompts to formulate the problem as question answering, which requires additional data processing and prompt search.\nTo address the mentioned limitations, we propose GenRec, a novel generative framework for personalized sequential recommendation. GenRec utilizes the encoder-decoder Transformer as the backbone and formulates sequential recommendation as a sequence-to-sequence generation task. We utilize the cloze task [5] as the training objective and pretrain the model on the corpus to learn the bidirectional sequential patterns. Inspired by [19], to make the downstream sequential recommendation task consistent with the pretraining cloze task, we append the [MASK] token at the end of"}, {"title": "3 TASK FORMULATION", "content": "The goal of sequential recommendation is to predict the next item that the user is most likely to interact with given the historical item interaction sequence of the user. Formally, ith user $u_i$ has item interaction sequence $t_i = {t_{i1}, ..., t_{in_i}}$ ordered chronologically. $t_{ij}$ and $n_i$ denotes the jth item in the sequence and the length of $t_i$ respectively. $t_{j:k}^i$ denotes the subsequence, i.e., $t_{j:k}^i = {t_{ij}, t_{ij+1}, ..., t_{ik}}$, where $1 \\leq j < k \\leq n_i$. Given the user item sequence ${u_i, t_{i;n_i}}$, the task of sequential recommendation is to predict the next item that $u_i$ is most likely to interact with at the $n_i + 1$ timestep."}, {"title": "4 METHODOLOGY", "content": "The overall architecture of GenRec is shown in Figure 1. Our GenRec is established upon a sequence-to-sequence Transformer encoder-decoder framework. The encoder of GenRec embeds cross modal user and item features from the input sequence and the decoder generates next items auto-regressively. In the following sections, we elaborate the feature embedding and learning objectives of pretraining and finetuning."}, {"title": "4.1 Model", "content": "The feature embedding consists of token embedding, positional embedding, user ID embedding and item ID embedding. The input sequence to the model ${u_i, t_{i;n_i}}$ is tokenized into a sequence of"}, {"title": "4.2 Pretraining", "content": "We pretrain GenRec with the masked sequence modeling task to deeply fuse the personalized sequential patterns. Given a user item sequence, we randomly sample an item, which is not necessarily the last item, from the sequence and replace it with the [MASK] token. The model is trained to generate the masked item in the decoder output. Cross-entropy loss is computed between the generated item and the masked item and minimized in pretraining. The masking process is also illustrated in Figure 2. Different from [19], where a number of items are masked, we only mask a single item in the sequence to unify the learning objectives of pretraining and finetuning. Our bidirectional masked sequence modeling task is based on the observation that the item sequence is not strictly ordered. For example, given similar items item_1 and item_2, user_1 might interact with item_1 and then item_2 while user_2 interacts with item_2 and then item_1. It is crucial to incorporate both left and right contexts to encode the user behavior [19]. To avoid the data leakage, we pretrain the model on the training split of datasets (See \u00a76 for details about the training split)."}, {"title": "4.3 Finetuning", "content": "In the finetuning stage, the learnt user and item sequential patterns in pretraining are utilized for the next item prediction task. As mentioned in section 4.2, finetuning has the same learning objective with pretraining for knowledge transfer. Following the masking mechanisms in Figure 2, we prepare the dataset in the same format as in pretraining by appending the [MASK] token to the end of the token sequence. The input to the model is the masked user item sequence and the output is the predicted next item."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "6 DATASETS", "content": "Amazon Sports and Amazon Beauty datasets are obtained from Amazon review datasets [18], which are collected from Amazon.com online platform. Yelp dataset is a large business recommendation dataset. Following [9, 27], we use transactions between January 1, 2019 and December 31, 2019. For all datasets, item sequences are extracted per user and sorted ascendingly by the interaction timestamps. Following the same data preprocessing in the baseline methods, unpopular items and inactive users with less than five interaction records are filtered. The statistics of the datasets after preprocessing are summarized in Table 2.\nTo generate the training, validation and test splits of datasets, we apply the leave-one-out strategy following [9, 13, 19, 27]. In each user item sequence, we use the last item as the test data, the item before the last item as the validation data, and the remaining item sequence as the training data."}, {"title": "7 BASELINES", "content": "We compare the performance of GenRec with several competitive sequential recommendation baselines.\nCaser [20] is a CNN based method, which utilizes both horizontal and vertical convolutional filters to capture high-order Markov chains for sequential recommendation.\nFDSA [25] models the feature sequence with the self attention module and captures the feature transition patterns.\nGRU4Rec [12] applies GRU to model the user click history se-quence for session based recommendation.\nHGN [17] adopts hierarchical gating networks to learn both long-term and short-term user behaviors."}, {"title": "7.1 Implementation Details", "content": "The model weights of GenRec is initialized from the pretrained BART [14] base model, which consists of a 6-layer Transformer encoder and a 6-layer Transformer decoder. There are 12 attention heads for both encoder and decoder and the model dimension is 768. The total number of model parameters is 184 million. We use the pretrained BART tokenizer for tokenization. GenRec is trained with an AdamW optimizer [16], in which 1e-5 is set as the learning rate and weight decay. The maximum length of the input tokens is set to 512. First 5% of iterations are used as the warmup stage. The beam size is set to 20 in inference. GenRec is pretrained for about an hour and finetuned for 25 epochs on one NVIDIA RTX 3090 GPU."}, {"title": "7.2 Evaluation Metrics", "content": "To evaluate the model performance, we employ top-k Hit Ratio (HR@k) and Normalized Discounted Cumulative Gain (NDCG@k) and report the the results of HR@{1, 5, 10} and NDCG@{5, 10} under the all-item setting, i.e. all items are possible candidates to be recommended as the next item. Our model is evaluated on Amazon Sports, Amazon Beauty and Yelp datasets. In the tables, bold numbers refer to the best scores, while underlined numbers refer to the second best scores."}, {"title": "7.3 Results and Analysis", "content": "This section provides experimental results and analyses of our model's effectiveness. The performance metrics of the baselines are obtained from the public results in [9]. See Appendix 7 for more"}, {"title": "8 ABLATION STUDY", "content": "To verify the effectiveness of the proposed masked sequence modeling task, we conduct the ablation study on Amazon Sports, Amazon Beauty and Yelp datasets. As is shown in Table 3, the model per-formance drops across all metrics without the masked sequence modeling task. It indicates the pretraining objective helps the model effectively capture the user behavior patterns and learn the user item representations. It is worth mentioning even without pretrain-ing, GenRec still achieves comparable performance with P5 and outperforms other baselines, which proves the effectiveness of our method."}, {"title": "9 CONCLUSION", "content": "In this work, we propose GenRec, a novel sequence to sequence framework that generates personalized sequential recommendation. Compared with existing generative models, GenRec is lightweight and efficient and does not rely on prompt engineering to find the best prompt. By leveraging the Transformer architecture as the backbone, GenRec effectively learns the bidirectional sequential patterns with the attention mechanism and achieves state-of-the-art performance on various public datasets. Besides, our proposed method is also flexible to integrate with other sequence to sequence language models and can be applied in other recommendation domains such as direct recommendation, which we leave for future work."}, {"title": "ID token cross embedding", "content": "$X_j = Emb(S_j) + PosEmb(j) + IDEmb(ID(j)), j \\in [0, n_i)$,\n$IDEmb \\in {UIDEmb, TIDEmb}$,\n$ID \\in {UID, TID}$"}]}