{"title": "GenRec: Generative Personalized Sequential Recommendation", "authors": ["Panfeng Cao", "Pietro Li\u00f2"], "abstract": "Sequential recommendation is a task to capture hidden user prefer-ences from historical user item interaction data. Significant progresshas been made in this domain by leveraging classification basedlearning methods. Inspired by the recent paradigm of \"pretrain,prompt and predict\" in NLP, we consider sequential recommen-dation as a sequence to sequence generation task and propose anovel model named Generative Recommendation (GenRec). Unlikeclassification based models that learn explicit user and item rep-resentations, GenRec utilizes the sequence modeling capability ofTransformer and adopts the masked item prediction objective toeffectively learn the hidden bidirectional sequential patterns. Dif-ferent from existing generative sequential recommendation models,GenRec does not rely on manually designed hard prompts. Theinput to GenRec is textual user item sequence and the output istop ranked next items. Moreover, GenRec is lightweight and re-quires only a few hours to train effectively in low-resource settings,making it highly applicable to real-world scenarios and helpingto democratize large language models in the sequential recom-mendation domain. Our extensive experiments have demonstratedthat GenRec generalizes on various public real-world datasets andachieves state-of-the-art results. Our experiments also validate theeffectiveness of the the proposed masked item prediction objectivethat improves the model performance by a large margin.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed the great success of recommender systems in online platforms such as Amazon and Yelp, which help people make micro decisions and fulfil their demands in daily life. Within online platforms, historical sequential user interaction data is utilized to capture the evolved and dynamic user behaviors and make appropriate recommendations of next items for the user. Different from traditional recommender systems that treat each user behavior as a sample and directly model the user preference on a single item, sequential recommendation learns timestamp-aware sequential patterns to recommend the next item [7]. For example, one might purchase peripheral accessories after buying a desktop. Various methods have been proposed to accurately model sequential user behaviors. [6, 10, 11, 22, 23] employ Recurrent Neural Networks (RNN) to encode the left-to-right context. [20] utilizes Convolutional Neural Networks (CNN) to learn sequential patterns as local features. [2] leverages Graph Neural Networks (GNN) to construct the item-item interest graphs and user interest sequences. [27] designs contrastive loss to maximize the mutual information between different views of the sequential data. Contextual information such as item attributes is incorporated as self-supervised signals to learn user and item representations. The recent success of Transformer [21] based large language models (LLMs) in various NLP tasks has inspired the research of utilizing LLMs in recommender systems [9, 13, 19]. [13] utilizes the left-to-right self-attention mechanism to identify next relevant items from the sequential interaction history and [19] further improves the performance by employing the bidirectional self-attention mechanism. Although both methods are effective, they do not consider personalization and the model performance is potentially limited. Generative LLMs based recommender system is considered a promising approach [15]. [9] is the first model to unify different recommendation tasks in the same framework to facilitate the knowledge transfer and it is pretrained across task-specific datasets to capture the deep semantics for personalization and recommendation. However, different modality information is encoded in the textual format of natural language, which might cause suboptimal modality representation. Moreover, [9] relies on manually designed task-specific hard prompts to formulate the problem as question answering, which requires additional data processing and prompt search.\nTo address the mentioned limitations, we propose GenRec, a novel generative framework for personalized sequential recommendation. GenRec utilizes the encoder-decoder Transformer as the backbone and formulates sequential recommendation as a sequence-to-sequence generation task. We utilize the cloze task [5] as the training objective and pretrain the model on the corpus to learn the bidirectional sequential patterns. Inspired by [19], to make the downstream sequential recommendation task consistent with the pretraining cloze task, we append the [MASK] token at the end of"}, {"title": "2 RELATED WORK", "content": "Transformer based LLMs have achieved remarkable success on various research fields such as text summarization, question answering, document understanding, etc. Typically, LLMs are trained on a vast amount of textual corpus from diverse sources including wikipedia, news, articles and books. LLMs can have emergent zero-shot learning capability as the model parameter size scales up with large training datasets [26], making LLMs better generalize to unseen domains. Recent efforts have been made to explore the potential of LLMs as recommender systems. [9] employs T5 as the model backbone and achieves competitive performance on a variety of recommendation tasks, demonstrating the generalization capability of LLMs based recommender systems. [4] develops a unified foundation model based on M6 and reduces the model size and training time by utilizing prompt tuning. Similar to [9], user interaction data is represented as plain texts and recommendation tasks are formulated as either natural language understanding or generation. [8] leverages ChatGPT to build a conversational recommender system to improve the interactivity and explainability of the recommendation process. [24] finetunes T5 with user-personalized instruction data, which is generated from manually designed templates, enabling users to communicate with the system with natural language instructions. [1] proposes an efficient framework to finetune LLMs for recommendation tasks and demonstrates significant improvements in the domains of movie and book recommendations. [3] employs a funneling approach where it first retrieves the candidates utilizing the user item interactions and then generates recommended items from candidates with a LLMs based framework."}, {"title": "3 TASK FORMULATION", "content": "The goal of sequential recommendation is to predict the next item that the user is most likely to interact with given the historical item interaction sequence of the user. Formally, ith user \\(u_i\\) has item interaction sequence \\(t_i = \\{t_1^i, ..., t_{n_i}^i\\}\\) ordered chronologically. \\(t_j^i\\) and \\(n_i\\) denotes the jth item in the sequence and the length of \\(t_i\\) respectively. \\(t_{j:k}^i\\) denotes the subsequence, i.e., \\(t_{j:k}^i = \\{t_j^i, t_{j+1}^i.... t_k^i\\}\\), where \\(1 \\leq j < k \\leq n_i\\). Given the user item sequence \\{u_i, t_{i;n_i}\\}, the task of sequential recommendation is to predict the next item that \\(u_i\\) is most likely to interact with at the \\(n_i + 1\\) timestep."}, {"title": "4 METHODOLOGY", "content": "The overall architecture of GenRec is shown in Figure 1. Our GenRec is established upon a sequence-to-sequence Transformer encoder-decoder framework. The encoder of GenRec embeds cross modal user and item features from the input sequence and the decoder generates next items auto-regressively. In the following sections, we elaborate the feature embedding and learning objectives of pretraining and finetuning.\n4.1 Model\nThe feature embedding consists of token embedding, positional embedding, user ID embedding and item ID embedding. The input sequence to the model \\{u_i, t_{i;n_i}\\) is tokenized into a sequence of"}, {"title": "Model", "content": "tokens, which is wrapped around with the start indicator token [BEG] and the end indicator token [END]. Extra [PAD] tokens are appended to the end to unify the sequence length in the batch. The token sequence S is represented as:\n\\(S = \\text{[BEG]}, Tok(u_i), Tok(t_1), ..., Tok(t_{n_i}), \\text{[END]}, ..., \\text{[PAD]},\\) (1)\nwhere Tok is the tokenizer function. For the jth token in S, we apply textual token embedding, positional embedding and user or item ID embedding. The textual token embedding is formulated as EMB(Sj), where EMB is the token embedding function. The positional information of tokens in the sequence is captured by the positional embedding \\(PosE_{MB}(j)\\), where \\(PosE_{MB}\\) is the 1D positional embedding function. And we utilize the user ID and item ID embedding to add personalized information of users and items to the sequence. Personalized information helps reveal the sequential patterns of user behaviors. For example, a user that previously viewed a Samsung tablet is more likely to continue viewing electronic products from Samsung or other brands. Different from P5 [9], which apply whole-word embeddings shared by both user and item tokens. Our ID embedding layers are not shared to better capture modality specific features. The user ID embedding is represented by \\(UID_{Emb}(UID(j))\\) and the item ID embedding is represented by \\(TID_{Emb}(TID(j))\\). \\(UID_{Emb}\\) and \\(TID_{Emb}\\) are the ID embedding functions for users and items respectively. Since a word in the sequence can be split into multiple tokens after tokenization, we utilize UID(j) or TID(j) to retrieve the user ID or item ID for the Jth token. For example, item_1234 is split into 4 tokens i.e. item, 12 and 34. Those tokens share the same item ID 1234 and embedding. Note only user tokens have user ID embeddings and only item tokens have item ID embeddings. All the aforementioned cross modal embeddings are added element-wisely to produce the final embedding X. Specifically the jth token cross modal embedding is formulated as:\n\\(X_j = Emb(S_j) + PosE_{MB}(j) + IDE_{mb}(ID(j)), j \\in [0, n_i),\\)\n\\(IDE_{mb} \\in \\{UID_{Emb}, TID_{Emb}\\},\\)\n\\(ID \\in \\{UID, TID\\}\\) (2)\nThe decoder input is the next item that serves as the learning target. The same tokenizer and textual embedding in the encoder are utilized. Following baseline methods, the model generates top 20 items with beam search during inference to calculate the evaluation metrics."}, {"title": "4.2 Pretraining", "content": "We pretrain GenRec with the masked sequence modeling task to deeply fuse the personalized sequential patterns. Given a user item sequence, we randomly sample an item, which is not necessarily the last item, from the sequence and replace it with the [MASK] token. The model is trained to generate the masked item in the decoder output. Cross-entropy loss is computed between the generated item and the masked item and minimized in pretraining. The masking process is also illustrated in Figure 2. Different from [19], where a number of items are masked, we only mask a single item in the sequence to unify the learning objectives of pretraining and finetuning. Our bidirectional masked sequence modeling task is based on the observation that the item sequence is not strictly ordered. For example, given similar items item_1 and item_2, user_1 might interact with item_1 and then item_2 while user_2 interacts with item_2 and then item_1. It is crucial to incorporate both left and right contexts to encode the user behavior [19]. To avoid the data leakage, we pretrain the model on the training split of datasets (See \u00a76 for details about the training split)."}, {"title": "4.3 Finetuning", "content": "In the finetuning stage, the learnt user and item sequential patterns in pretraining are utilized for the next item prediction task. As mentioned in section 4.2, finetuning has the same learning objective with pretraining for knowledge transfer. Following the masking mechanisms in Figure 2, we prepare the dataset in the same format as in pretraining by appending the [MASK] token to the end of the token sequence. The input to the model is the masked user item sequence and the output is the predicted next item."}, {"title": "5 EXPERIMENTS", "content": "Amazon Sports and Amazon Beauty datasets are obtained from Amazon review datasets [18], which are collected from Amazon.com online platform. Yelp dataset is a large business recommendation dataset. Following [9, 27], we use transactions between January 1, 2019 and December 31, 2019. For all datasets, item sequences are extracted per user and sorted ascendingly by the interaction timestamps. Following the same data preprocessing in the baseline methods, unpopular items and inactive users with less than five interaction records are filtered. The statistics of the datasets after preprocessing are summarized in Table 2.\nTo generate the training, validation and test splits of datasets, we apply the leave-one-out strategy following [9, 13, 19, 27]. In each user item sequence, we use the last item as the test data, the item before the last item as the validation data, and the remaining item sequence as the training data."}, {"title": "6 BASELINES", "content": "We compare the performance of GenRec with several competitive sequential recommendation baselines.\nCaser [20] is a CNN based method, which utilizes both horizontal and vertical convolutional filters to capture high-order Markov chains for sequential recommendation.\nFDSA [25] models the feature sequence with the self attention module and captures the feature transition patterns.\nGRU4Rec [12] applies GRU to model the user click history sequence for session based recommendation.\nHGN [17] adopts hierarchical gating networks to learn both long-term and short-term user behaviors."}, {"title": "7.1 Implementation Details", "content": "The model weights of GenRec is initialized from the pretrained BART [14] base model, which consists of a 6-layer Transformer encoder and a 6-layer Transformer decoder. There are 12 attention heads for both encoder and decoder and the model dimension is 768. The total number of model parameters is 184 million. We use the pretrained BART tokenizer for tokenization. GenRec is trained with an AdamW optimizer [16], in which 1e-5 is set as the learning rate and weight decay. The maximum length of the input tokens is set to 512. First 5% of iterations are used as the warmup stage. The beam size is set to 20 in inference. GenRec is pretrained for about an hour and finetuned for 25 epochs on one NVIDIA RTX 3090 GPU."}, {"title": "7.2 Evaluation Metrics", "content": "To evaluate the model performance, we employ top-k Hit Ratio (HR@k) and Normalized Discounted Cumulative Gain (NDCG@k) and report the the results of HR@\\{1, 5, 10\\} and NDCG@\\{5, 10\\} under the all-item setting, i.e. all items are possible candidates to be recommended as the next item. Our model is evaluated on Amazon Sports, Amazon Beauty and Yelp datasets. In the tables, bold numbers refer to the best scores, while underlined numbers refer to the second best scores."}, {"title": "7.3 Results and Analysis", "content": "This section provides experimental results and analyses of our model's effectiveness. The performance metrics of the baselines are obtained from the public results in [9]. See Appendix 7 for more details about baselines. As is presented in Table 1, GenRec outperforms other baselines on all metrics on Sports and Yelp datasets. Although it is outperformed by P5 on the HR@10 metrics of the Beauty dataset, GenRec still achieves comparable performance with other baselines.\nWe speculate due to the smaller size of Beauty dataset, the sequential patterns are not learnt thoroughly in pretraining. Increasing the number of pretraining epochs could potentially improve the performance. Since we focus on low-resource efficient training in this work, we leave the improvement for future work. Since P5 is a unified model and trained using different types of recommendation tasks, it learns better user item representation with the knowledge transfer between tasks on smaller datasets. The advantage of our method compared with P5 is that our model is lightweight and the training only takes a few hours to complete. We do not leverage prompt engineering and no prompt search is required to find the best prompt."}, {"title": "8 ABLATION STUDY", "content": "To verify the effectiveness of the proposed masked sequence modeling task, we conduct the ablation study on Amazon Sports, Amazon Beauty and Yelp datasets. As is shown in Table 3, the model performance drops across all metrics without the masked sequence modeling task. It indicates the pretraining objective helps the model effectively capture the user behavior patterns and learn the user item representations. It is worth mentioning even without pretraining, GenRec still achieves comparable performance with P5 and outperforms other baselines, which proves the effectiveness of our method."}, {"title": "9 CONCLUSION", "content": "In this work, we propose GenRec, a novel sequence to sequence framework that generates personalized sequential recommendation. Compared with existing generative models, GenRec is lightweight and efficient and does not rely on prompt engineering to find the best prompt. By leveraging the Transformer architecture as the backbone, GenRec effectively learns the bidirectional sequential patterns with the attention mechanism and achieves state-of-the-art performance on various public datasets. Besides, our proposed method is also flexible to integrate with other sequence to sequence language models and can be applied in other recommendation domains such as direct recommendation, which we leave for future work."}]}