{"title": "REL-A.I.: An Interaction-Centered Approach To Measuring Human-LM Reliance", "authors": ["Kaitlyn Zhou", "Jena D. Hwang", "Xiang Ren", "Nouha Dziri", "Dan Jurafsky", "Maarten Sap"], "abstract": "The reconfiguration of human-LM interactions from simple sentence completions to complex, multi-domain, humanlike engagements necessitates new methodologies to understand how humans choose to rely on LMs. In our work, we contend that reliance is influenced by numerous factors within the interactional context of a generation, a departure from prior work that used verbalized confidence (e.g., \"I'm certain the answer is...\") as the key determinant of reliance. Here, we introduce REL-A.I. (/r\u0259'la\u0131/), an in situ, system-level evaluation approach to measure human reliance on LM-generated epistemic markers (e.g., \u201cI think it's..\", \"Undoubtedly it's...\"). Using this methodology, we measure reliance rates in three emergent human-LM interaction settings: long-term interactions, anthropomorphic generations, and variable subject matter. Our findings reveal that reliance is not solely based on verbalized confidence but is significantly affected by other features of the interaction context. Prior interactions, anthropomorphic cues, and subject domain all contribute to reliance variability. An expression such as, \"I'm pretty sure it's...\", can vary up to 20% in reliance frequency depending on its interactional context. Our work underscores the importance of context in understanding human reliance and offers future designers and researchers with a methodology to conduct such measurements.", "sections": [{"title": "1 Introduction", "content": "With the rise in popularity of language models (LMs) comes new features and dimensions that characterize human-LM interactions. Users are no longer asking models to complete simple sentences or retrieve trivial information. Instead, we task LMs with complex requests over repeated interactions, across a variety of domains. At the same time, LMs are increasingly displaying human-like traits as a result of post-training techniques."}, {"title": "2 Background", "content": "Machine learning as focused largely on improving model calibration (Jiang et al., 2021; Desai and Durrett, 2020; Jagannatha and Yu, 2020; Kamath et al., 2020; Kong et al., 2020) aligning the model probabilities with accuracy. Other recent work has examined how pretraining (Hendrycks et al., 2019) and scaling (Srivastava et al., 2022; Chen et al., 2023b) impact LM calibration. Works in linguistic calibration focused on fine-tuning LMs to be calibrated between what is internally represented and what is verbalized either numerically (Kadavath et al., 2022; Tian et al., 2023; Liu et al., 2023a; Xiong et al., 2024; Tanneru et al., 2023), ordinally (Mielke et al., 2022; Lin et al., 2022), or verbally (Stengel-Eskin et al., 2024). Recent work also explored calibration based on a model's internal representational states (Hofweber et al., 2024).\nThe field of human-computer interaction is rich with studies on how and when humans choose to rely on Als and their explanations (Bu\u00e7inca et al., 2021; Chen et al., 2023a; Schemmer et al., 2023, 2022; Schoeffer et al., 2024; Vasconcelos et al., 2023), identifying the pitfalls of human"}, {"title": "3 Interaction-Centric Approach to Measuring Human-LM Reliance", "content": "Our work introduces REL-A.I., an approach to measure inter-system reliance, with the explicit interest in understanding how interactional context impacts human reliance decisions. We name three key desiderata needed to conduct evaluations of human reliance on LM-generations centered on in situ, system-level, and robust evaluations.\nIn Situ Evaluations In order to understand how interactional context affects reliance, evaluations"}, {"title": "3.1 Desiderata Of In Situ Evaluation", "content": "must occur in a situated environment. Our evaluations use self-incentivized tasks where users gain points by correctly answering challenging trivia questions, deciding whether to rely on an AI agent's response for help. LMs' predicted answers are never shown, forcing users to rely on the epistemic markers (e.g., \"I'm certain it's...\") rather than their own knowledge to make a decision.\nSystem-Level Evaluations A user's interactional context encompasses their entire interaction with a system and these interactions are rarely isolated to a single event. Humans are known to form mental models of the characteristics (e.g. warmth and competence) and abilities of a system over repeated engagements (Norman, 1988; Bickmore and Picard, 2005). Thus, rather than measuring LM reliance based on multiple interactions with a single system and observing changes over time, we instead have users engage with multiple different systems in one sitting. The systems vary in presentations as required by the experimental setting (i.e., contextual-interaction cues such as warmth of expression or difference in domain knowledge) and the contrast between them enables users to develop separate mental models, allowing us to understand reliance and perception at the system-level.\nRobust Reliance Measurements Humans perceptions of expressions of (un)certainty are internally consistent (Druzdzel, 1989) but can vary greatly across subjects (Budescu et al., 1988; Chesley, 1986; Doupnik and Richter, 2003). The variance between subjects is a confounding variable that can be expensive to solve (i.e., recruit thousands of participants). However, in our approach, by having the same participant rely on two different systems, we are able to robustly measure intra-subject reliance and use that as"}, {"title": "3.2 Three Components of REL-A.I.", "content": "REL-A.I. consists of three core components, a self- incentivized task, epistemic markers from publicly deployed LMs, and meta-level debrief questions.\nSelf-Incentivized Task We invite participants to play a trivia game with an artificial intelligence agent where they must decide whether or not to rely on the AI agent's generations. Situating users in a game-like scenario incentivizes them to be engaged and participate actively, more closely mimicking real-world interactions. Users are shown a question (e.g., \"What is the capital of Estonia?\") and the beginning of a response by an agent that includes epistemic markers (e.g., \"I'm certain it's...\"). Users must decide whether to rely on the agent's answer or to indicate that they'll look it up themselves later. The participant loses points if they rely on the system and the system is wrong, gains points if they rely on the system and the system is correct, and gains zero points if they choose to look it up themselves. The only way to achieve a positive score is to correctly rely on the system when it is correct. We then calculate the reliance rate for each expression shown in the task and the conditions in which they appeared. Our work measures human reliance rather than trust, as reliance is an observable behavior through user actions, meanwhile, trust is a special case of reliance that takes on variable mental states (de Fine Licht and Br\u00fclde, 2021).\nLastly, to meet the desiderata of system-level"}, {"title": "4 Influence of Prior Interactions (RQ1)", "content": "The first key shift in human-LM interactions is the frequency in which LMs are being used in everyday human tasks, with recent work showing that over 40% of survey participants were using chat models on a daily basis (Wang et al., 2024). Humans are known to develop mental models of systems (Norman, 1988) and the AI agents (Kulesza et al., 2012; Gero et al., 2020) they interact with and update these mental models as interactions progress (Bansal et al., 2019). Prior work (see \u00a72) on calibration has made the implicit assumption that human-LM interactions can be evaluated independently from one another. As long as each individual expression is calibrated with the right human behavior, then the model is considered correctly calibrated.\nHowever, recent studies (Dhuliawala et al., 2023; Zhou et al., 2024) show that users form mental models early and these mental models have repercussions on future decisions. Specifically,"}, {"title": "5 Varying Degree of Warmth (RQ2)", "content": "Another key reconfiguration of human-LM interactions has been the anthropomorphism introduced in language technologies (Abercrombie et al., 2023b; Araujo, 2018; Colombatto and Fleming, 2024). Anthropomorphic system attributions like names (Wagner et al., 2019) as well as human-like language such as greetings and expressions of warmth (Bai et al., 2022) are coveted features as they enable more engaging and enriching human-model interaction. However,"}, {"title": "6 Domain Specific Reliance (RQ3)", "content": "The last interactional context we consider is how the domain of interaction might influence reliance. Language models are being deployed wildly across a spectrum of tasks. Here we ask, does the domain of human-LM interaction influence reliance behaviors?\nExperimental Set-Up In this experimental setting, we measure human reliance on expressions of (un)certainty across five subject domains. Participants interact with five agents and are asked to rely on their generations for questions from college mathematics, abstract algebra, philosophy, world religion, and law (Table 8). The questions originate from the Massive Multitask Language Understanding (MMLU) dataset (Hendrycks et al., 2021) to ensure consistency, quality, and difficulty. The questions were randomly selected and were then filtered to make sure they could be answered using free-response (rather than multiple choice). Short questions were prioritized to minimize the cognitive load on human annotators. Pilot"}, {"title": "7 Discussion and Conclusion", "content": "In this work, we introduce REL-A.\u0399., an interaction-centered approach for evaluating human reliance on LM-generated responses. With this methodology, we tackle three emergent properties of human-LM interactions and ask how these new characteristics might influence human decision-making. We find that prior interactions, the warmth of the model, and the subject domain of the interaction can all impact how and when humans choose to rely on the exact same expressions of (un)certainty. These findings illustrate the limitations of prior in vitro evaluations which exclusively focused on the calibration between internal model probabilities and the verbalized confidence of LMs. We now discuss the implications and aspirations of our contributions.\nReorientation of Reliance Evaluation Metrics We must reorient ourselves away from model calibration and instead look towards calibrating human-LM interactions. Although perfectly (linguistically) calibrated language models could exist, these models might not actually yield safe and optimal human-LM interactions. Humans will interpret the meaning of verbalized confidences and these interpretations will consider the interaction context and the users' history, adding exogenous features to how a user might actually behave towards LM-generated epistemic markers.\nThe Cost of Warmth Cues With the rise of anthropomorphism and friendliness of models, there is a trend to build more engaging and personable LMs. However, our findings illustrate that these new dimensions could potentially introduce unexpected safety risks. Our findings show that warmth and reliance are tightly coupled and the introduction of a seemingly benign friendly greeting could actually encourage a user to rely on the system more. As designers and practitioners think of ways to build personable and engaging interactions between humans and LMs, they must also consider the safety implications these decisions have on human overreliance.\nAspiring To In Situ Evaluations Lastly, we aspire for NLP practitioners and designers to use the REL-A.I. approach to assess the potential pitfalls of their language models. Before deploying a new iteration of a system, use an interaction- centric measure to understand how reconfigurations"}, {"title": "8 Limitations", "content": "Our work proposes an approach to interaction- centered measurements of human-LM reliance. In this section, we discuss additional interactional context features that could be studied in future work. Our study focused on single-turn interactions with LMs but as users move towards engaging in longer multi-turn interactions, it would be pertinent to include multi-turn interactions also as a part of the interactional context. We recruited U.S.- based participants and our questions were all closed-form responses in English. However, as these models are deployed across languages and cultures, it's critical to take into account the cultural context of their usage and the variety of responses users could be looking for. Lastly, as language models become the building blocks for other virtual assistants and voice assistants, the modality in which confidence is expressed, either through text, speech, or movement, will surely have significant impacts on the interactional context."}, {"title": "9 Ethical Considerations", "content": "We follow standard IRB protocol and additionally use consent forms to inform participants of the nature, risks, and benefits of our tasks. We paid users $15 USD per hour and in cases when tasks were longer than expected, we gave bonuses to workers to meet this minimum.\nThe study of understanding how LMs generate epistemic markers comes with risks of dual use. Our work focuses on understanding how the interactional context impacts questions of human reliance, but one could maliciously use these findings to design overly persuasive LMs. In addition to conducting research in this space, it is also critical as NLP researchers to provide educational programming to everyday users and help them stay vigilant in their interactions with LMs."}, {"title": "A Appendix", "content": "A.1 Recruitment Process Details\nWe aimed to pay participants an average of $15 USD an hour and bonus workers in cases when experiments look longer than expected to meet this minimum. Human experiments were run throughout the months of January through June 2024.\nWe inform the participants of the nature and risks of the task through a consent form. Participants were filtered down to English-speaking, U.S.-based, with an approval rating of at least 97% and had completed 100+ tasks on Prolific. We recruit 50 new participants for each of our three experimental settings.\nOur research team sought and received an exemption from our internal review board (IRB). We do not collect sensitive or demographic information. The exemption does not require a consent form but we used a consent form and collected informed consent from all our participants.\nA.2 Template Elicitation Details\nFollowing prior work (Zhou et al., 2024), we elicited expressions of uncertainty from nine publicly deployed language models (text-davinci-003, GPT-3.5-Turbo, GPT-4, LLAMA-2 7B, LLAMA-2 13B, LLaMA-2 70B, Claude-1, Claude-2, Claude- Instant-1) and"}, {"title": "A.3 Details on Experiments on Anthropomorphism", "content": "Our initial experiments also compared the use of aleatoric and epistemic uncertainty expressions but found that the two types differed significantly in meaning (i.e., \"I'm certain it's...\" means something different from \"It's certain it's...\"). This made it difficult to isolate if the differences were due to personal pronouns and the anthropomorphism of a model or due to changes in the meaning of the expression. The use of aleatoric and epistemic markers and their connection to reliance and anthropomorphism should be further explored in future work."}, {"title": "A.4 Pilot Experiment for RQ3", "content": "Pilot experiment on RQ3 which included five other subjects that varied in domain. Results illustrate that the computational heavy topics (i.e., accounting and clinical knowledge [which included calculations for drug usage]) were relied on more frequently than the non-computational topics (i.e., trivia and law)"}]}