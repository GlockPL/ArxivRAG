{"title": "Automatic Input Rewriting Improves Translation with Large Language Models", "authors": ["Dayeon Ki", "Marine Carpuat"], "abstract": "Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.", "sections": [{"title": "1 Introduction", "content": "Machine translation (MT) users and developers have long exploited the idea that some texts are easier to translate than others. For instance, guiding people to edit their inputs so that they are well formed is a cornerstone of MT literacy courses (Bowker, 2021; Steigerwald et al., 2022), and adopting plain language has been shown to improve the readability of translated health content (Rossetti, 2019). In MT research, a wealth of studies have considered pre-processing strategies to rewrite inputs, particularly for statistical MT (Xia and McCord, 2004; Callison-Burch et al., 2006; \u0160tajner and Popovic, 2016). The growing use of Large Language Models (LLMs) for translation leads us to revisit the impact of rewriting inputs on MT. On the one hand, rewriting inputs for LLM translation aligns with the re-framing of MT as a multi-step process (Briakou et al., 2024a). LLMs have shown promise in rewriting MT outputs (Ki and Carpuat, 2024; Zeng et al., 2024; Xu et al., 2024), and can rewrite text according to various style specifications (Raheja et al., 2023; Hallinan et al., 2023; Shu et al., 2024; Krishna et al., 2024). On the other hand, current models might already be robust to input variability, since they are trained on vast amounts of heterogeneous data (Touvron et al., 2023), fine-tuned on diverse tasks (Raffel et al., 2020; Alves et al., 2024) and operate at a much higher quality level compared to the statistical MT systems used in previous pre-processing studies. How should inputs be rewritten for MT? The assumption that well-written texts are easier to translate drives recommendations for MT literacy, as well as the use of paraphrasing (Callison-Burch et al., 2006; Mirkin et al., 2009; Marton et al., 2009; Aziz et al., 2010) and simplification (\u0160tajner and Popovic, 2016; \u0160tajner and Popovi\u0107, 2019). However, can we more directly rewrite inputs so that they are easier to translate? Generic translatability has been defined as \"a measurement of the time and effort it takes to translate a text\" (Kumhyr et al., 1994). Uchimoto et al. (2005) introduced a metric to quantify MT translatability based on back-translation of MT hypotheses in the source language. Given recent progress in quality estimation (Fernandes et al., 2023; Naskar et al., 2023; Tomani et al., 2024), we propose instead to use reference-free quality estimation scores as a measure of translatability. We thus ask the following research questions: (1) Can we improve MT quality from LLMs by rewriting inputs for style? (2) Do quality estimation metrics provide useful translatability signals for input rewriting? We conduct an empirical study with 3 open-weight LLMs for a total of 21 input rewriting methods with varying levels of MT-awareness on translation from English into German, Russian and Chinese, and we further evaluate the generalizability of our best performing approach on translation from English into Czech, Hebrew and Japanese (\u00a74.4). Our results show that simple MT-Agnostic rewrites obtained by prompting LLMs to simplify, paraphrase, or change the style of the input, improve translatability, and that simplification most reliably improves translation quality. Interestingly, these MT-agnostic rewrites are more effective than Task-Aware rewrites, where LLMs are prompted to rewrite inputs for the purpose of MT (\u00a74.1). Finally, using quality estimation signals to assess translatability at the segment level and select when to use rewrites further improves MT quality, outperforming more expensive fine-tuning strategies (\u00a74.2). Human evaluation further confirms that simplified rewrites and their MT largely preserve the original meaning of the source and MT (\u00a75.3)."}, {"title": "2 Input Rewriting Methods", "content": "Within the process of source rewriting, the goal of a rewrite model is to rewrite the original source sentence s into another form that is easier to translate while preserving its intended meaning. For MT-Agnostic rewriting methods (\u00a72.1), which lacks translation-related knowledge, the rewrite model \\(M_\theta\\) can rewrite s into s':\n\\(s' = M_\theta(s)\\) \nOn the contrary, both Task-Aware (\u00a72.2) and Translatability-Aware (\u00a72.3) rewriting methods incorporate some translation signal. For Task-Aware, \\(M_\theta\\) rewrites s with the information of the end-task (MT):\n\\(s' = M_\theta(s, MT task)\\) \nFor Translatability-Aware method, it rewrites with the knowledge of segment level quality estimation scores between source and the output of a specific MT system MT(t):\n\\(s' = M_\theta(s, XCOMET(s, MT(t)))\\) \nTo find the most effective \\(M_\theta\\), we test a total of 21 input rewriting methods."}, {"title": "2.1 MT-Agnostic Rewriting", "content": "MT-agnostic rewriting methods reflect various a priori assumptions on what makes text easier to translate. They do not take as input any signal of translatability or knowledge about the end-task. We consider three prompting variants here, all inspired by prior works on source rewriting (Mirkin et al., 2009, 2013; \u0160tajner and Popovic, 2016). Simplification. Simplification includes replacing complex words with simpler ones, rephrasing complex syntactic structures, and shortening sentences (Chandrasekar and Bangalore, 1997; Feng, 2008). Prior works show that simplified inputs are more conducive to MT, and particularly improve the fluency of MT outputs (\u0160tajner and Popovi\u0107, 2019). Paraphrase. Paraphrases are alternative ways of expressing the same information within one language, which can help resolve unknown or complex words (Callison-Burch et al., 2006). Paraphrasing with LLMs might benefit MT by normalizing inputs using language patterns that are more frequent in LLM training data. Further, some LLMs, such as TOWER (Alves et al., 2024), are fine-tuned on both paraphrasing and MT tasks, and might thus produce paraphrases that are useful for MT. Stylistic. We employ an off-the-shelf text editing tool COEDIT-XL (Raheja et al., 2023) to rewrite inputs according to diverse style specifications: \u2022 Grammar: Fix the grammar. \u2022 Coherent: Make the text more coherent. \u2022 Understandable: Make it easier to understand. \u2022 Formal: Rewrite the text more formally. These operationalize the assumption that well-formed text is easier to translate."}, {"title": "2.2 Task-Aware Rewriting", "content": "For task-aware rewriting methods, we design prompts that account for the fact that rewrites are aimed at MT. Prior work has shown that LLMS can post-edit errors in MT outputs (Ki and Carpuat, 2024; Zeng et al., 2024; Treviso et al., 2024a; Xu et al., 2024; Briakou et al., 2024b), raising the question of whether this ability can be extended to rewriting inputs to enhance translatability. Additionally, TOWER-INSTRUCT has been jointly trained on paraphrasing, grammatical error correction (GEC), and translation tasks, suggesting it may be well-suited for performing translatability rewrites in a zero-shot fashion. We consider two prompting strategies (Refer to Appendix Table 6 for exact templates): Easy Translation. We prompt LLMs to rewrite inputs in a way that specifically facilitates translation into the target language. Chain of Thought Rewrite+Translate. We use a Chain of Thought (Wei et al. (2023), CoT) style prompt where LLMs are prompted to handle the entire rewriting and translation process in one sequence of CoT instructions within a single model."}, {"title": "2.3 Translatability-Aware Rewriting", "content": "We propose to use quality estimation scores for a given input and output pair to assess the translatability of inputs at the segment level. This makes it possible to inject translatability signals at inference or training time. We introduce a lightweight inference-time selection strategy, and contrast it against a more expensive fine-tuning approach. Inference-Time Selection. Input segments might not benefit from rewriting uniformly, since the quality of the original inputs and of their rewrites might vary. We thus propose to use translatability scores to decide whether or not to replace the original input with a rewrite at inference time. We use the state-of-the-art XCOMET quality estimation tool (Guerreiro et al., 2024) to assess how good the translation t' of a rewrite s' is: XCOMET(s',t'). We compare this score with the estimated quality of the translation t of the original source s, choosing to use the rewrite if XCOMET(s', t') > XCOMET(s, t), and keeping the original source otherwise. This straightforward approach allows us incorporate translatability signals at inference time, with little additional cost. Supervised Fine-tuning. The translatability-based selection process described above for inference could also be used to gather examples of good rewrites and enable instruction fine-tuning of models to rewrite text for improved translation. While designing an optimal approach for this task is out of scope for this work, we wish to compare our inference-time selection strategy with a straightforward training strategy. We construct a fine-tuning dataset of positive rewrite examples \\(D_{pos}\\), as follows: for a given input s, we generate rewrites using all MT-agnostic methods. We add to our training set the rewrites that improve translatability as measured by XCOMET(s', t') > xCOMET(s, t). The base LLM is then instruction fine-tuned based to rewrite input s so that it is better translated, using s' as supervision. Detailed prompt templates are shown in Appendix A.1."}, {"title": "3 Experimental Setup", "content": "3.1 Model & Data MT System. We use TOWER-INSTRUCT 7B as our MT system for all our experiments since it is specifically trained for translation-related tasks and has demonstrated superior MT performance compared to other LLMs (Alves et al., 2024). Rewriting Models. For prompting experiments, we use 7B variant of three open-weight LLMs in zero-shot setting: LLAMA-2 (Touvron et al., 2023) - the base model for TOWER-INSTRUCT, LLAMA-3 (Grattafiori et al., 2024) \u2013 more recent multilingual model compared to LLAMA-2, and TOWER-INSTRUCT (Alves et al., 2024) \u2013 the same LLM as used for our MT system. For supervised fine-tuning, we draw training samples from the English-German and English-Russian subset from WMT-20, 21, and 22 General MT task datasets (Freitag et al., 2021), and provide detailed parameter settings in Appendix A.2. Test Data. We use the WMT-23 General MT task from the TOWEREVAL dataset to guarantee that it was held out from the various training stages. We focus on translation from English into German (EN-DE), Russian (EN-RU) and Chinese (EN-ZH) for an extensive empirical comparison, and then test whether the most promising approaches generalize to translation from English into Czech (EN-CS), Hebrew (EN-HE) and Japanese (EN-JA). See Appendix Table 8 for data statistics."}, {"title": "3.2 Evaluation Metrics", "content": "We use XCOMET (Guerreiro et al., 2024) and METRICX (Juraska et al., 2023) to evaluate different aspects of rewrite quality. Specifically, we use XCOMET-XL and METRICX-23-XL. Higher scores indicate better performance for XCOMET, while lower scores are better with METRICX. Translatability. We quantify translatability with the quality estimation score for a specific input-output pair (XCOMET(s',t') or METRICX-QE(s', t')). A rewrite s' of the original input s is considered easier to translate if xCOMET(s', t') is higher than XCOMET(s, t). Meaning Preservation. We do not want rewrites that are easier to translate at the expense of changing the original meaning. Our meaning preservation metric evaluates how well the rewrite maintains the intended meaning of the translation as represented by the reference (Graham et al., 2015). We use a reference-based metric as opposed to using the semantic similarity between s and s' because it abstracts the meaning away from the specific formulation of s, reducing overfitting. We compute XCOMET scores between the rewrites and reference translations (XCOMET(s', r)). The desired behavior is to minimize the deterioration in xCOMET(s', r) compared to XCOMET(s,r). Translation Quality. We additionally report the combined evaluation metric, XCOMET(s', t', r) to take into account of the trade-off between the two above metrics, and METRICX(t', r) which also assesses translation quality of the rewrite but is not informed by the updated source s'."}, {"title": "4 Results", "content": "We first extensively compare rewrite strategies focusing on the overall translation quality achieved by MT-Agnostic rewrites (\u00a74.1) and Translatability-Aware rewrites (\u00a74.2). To understand how rewrites change translations, we then analyze the trade-offs between translatability and meaning preservation (\u00a74.3). Finally, we test whether the best-performing methods identified so far generalize to new language pairs (\u00a74.4)."}, {"title": "4.1 Simplifying Inputs Works Best", "content": "We first compare the MT Agnostic rewriting methods: simplification, paraphrasing, and stylistic edits. Due to space limits, we show the best and worst performing variations for each input rewriting method based on the overall translation quality metric XCOMET(s, t, r) for each language pair in Table 1. Full results are available in Appendix B.1. Results show that all rewriting strategies improve translatability, but only simplification also improves the overall translation quality. Even the lowest performing rewrites reach higher translatability than the original baseline. Each method surpasses the baseline by up to 0.056 and 0.027 XCOMET(s, t) average scores for EN-DE, up to 0.058 and 0.036 average scores for EN-RU, and up to 0.054 and 0.028 average scores for EN-ZH pair. Trends are consistent with METRICX(s,t). However, making inputs easier to translate often degrades quality when comparing against references r. Simplification with TOWER-INSTRUCT distinguishes itself by improving translation quality based on XCOMET(s, t, r) scores and maintaining it according to the METRICX(t, r) scores a harder metric to improve since the reference might be biased toward the original wording of the source. Among the three LLMs used for simplification, TOWER-INSTRUCT achieves the best translation quality, while LLAMA-3 excels in translatability at the expense of meaning preservation. Interestingly, there is no benefit to using a separate LLM, even one fine-tuned specifically on paraphrasing or style edits such as DIPPER or COEDIT. Overall, the best performing method for MT-agnostic rewrites is simplification with TOWER-INSTRUCT, the same model we use as our MT system. We attribute this to TOWER-INSTRUCT being instruction fine-tuned on translation related tasks (but not simplification) and having more domain knowledge of the WMT dataset used in our evaluation. As shown in Table 1, simplifying with TOWER-INSTRUCT still holds the top spot when compared to Task-Aware rewriting methods, as indicated by higher XCOMET(s,t,r) scores. This suggests that injecting knowledge about the end-task (MT) to LLMs is less effective than simplifying inputs to improve translation quality. Overall, these results confirm the intuition that simpler text is easier to translate, but establish that rewrites are not uniformly helpful for translation quality, motivating the need for more selective input rewriting strategies."}, {"title": "4.2 Selection via Translatability Improves MT", "content": "We evaluate the impact of inference-time selection based on translatability scores (Selection in Table 1), and compare it further with the more expensive supervised fine-tuning strategy (Fine-tune). All language pairs consistently benefit from selection. Translation quality improves significantly, with average XCOMET(s, t, r) gains of 0.024 for EN-DE, 0.031 for EN-RU, and 0.025 for EN-ZH, marking the best performance among all variants. METRICX(t, r) scores confirm this trend, showing average improvements of 0.073 for EN-DE, 0.198 for EN-RU, and 0.076 for EN-ZH. At the segment level, rewrites are preferred to original inputs in 1197/1557 cases for EN-DE, 1610/2074 cases for EN-RU, and 2163/3074 cases for EN-ZH. Fine-tuning shows smaller gains compared to MT-Agnostic or Task-Aware methods, both in terms of translatability and translation quality, despite being more resource-intensive. In summary, the results suggest that inference-time selection of inputs based on translatability scores is a promising strategy, outperforming MT-agnostic rewrites and rewrites obtained via a more expensive fine-tuning process."}, {"title": "4.3 Input Rewriting Trades Off Translatability and Meaning Preservation", "content": "We observe a moderate negative correlation between translatability and meaning preservation"}, {"title": "4.4 Best Input Rewriting Strategy Improves MT on Held-out Test sets", "content": "We evaluate whether the top methods that have emerged from the controlled empirical comparison conducted so far generalize to further test settings. As shown in Table 2, we test both simplification with TOWER-INSTRUCT (Simplification) and translatability-based input selection (Selection) on new test sets from the WMT-23 General MT task, English-Czech (EN-CS), English-Hebrew (EN-HE), and English-Japanese (EN-JA) to assess generalization to lower-resource target languages. Both simplification and translatability-based selection lead to progressive improvements in translation quality, as measured by XCOMET(s, t,r). Notably, the selection strategy tends to excel in language pairs with lower-resource target languages, showing translation quality gains of 0.064, 0.043, 0.051 scores for EN-CS, EN-HE, EN-JA, respectively, compared to increases of 0.017, 0.031, and 0.025 for EN-DE, EN-RU and EN-ZH. At the segment level, rewrites are also more preferred over original inputs, selected in 1395/2074 cases for EN-CS, 1309/2074 for EN-HE, and 1411/2074 for EN-JA. METRICX trends are consistent. In sum, our findings generalize well to held-out test sets, further validating the effectiveness of the translatability-based selection strategy. This approach offers a practical and scalable solution for input rewriting across a broader range of domains and language pairs, though there are many other dimensions that remain unexplored. We have conducted initial experiments with additional LLMs and source languages, shown in Appendix D.1 and D.2, which confirms our previous findings that simplification rewriting enhances translation quality. We leave a more comprehensive exploration of this direction for future work."}, {"title": "5 Analysis", "content": "5.1 Simplifying Inputs Improves MT Readability Simplification as an input rewriting strategy can balance translatability and meaning preservation, leading to overall improvements in translation quality. We also examine whether this enhances the readability of both inputs and, subsequently, translation outputs. In Table 3, we present the Flesch Reading Ease score and Gunning Fog index to measure input readability, and the Vienna formula (WSTF) (Zowalla et al., 2023) and the Russian version of Flesch Readability test (Solnyshkina et al., 2018) to assess output readability for EN-DE and EN-RU, respectively. As expected, input readability improves across all simplification methods, whether used in MT-Agnostic (LLAMA-2, LLAMA-3, and TOWER-INSTRUCT in Table 3) or Translatability-Aware (Selection in Table 3) manner. Interestingly, simplification not only leads to more readable input but also more readable outputs, with gains of up to 0.22 WSTF scores for EN-DE and 0.95 Flesch scores for EN-RU. We provide several qualitative examples in Appendix Tables 14 to 16 that illustrate how simplification rewrites can lead to varying degrees of readability improvements in both inputs and translation outputs."}, {"title": "5.2 Input Rewriting outperforms Post-Editing", "content": "The symmetric task to input rewriting is post-editing, which focuses on improving and correcting errors in translation outputs. Can post-editing alone achieve the same improvements, or are both strategies complementary? To explore this, we compare input rewriting to post-editing by prompting TOWER-INSTRUCT to simplify either inputs or outputs. As shown in Table 4, rewriting inputs (I) offers a notable advantage over post-editing outputs (Owo), even when post-editing is guided by the input sentence (Ow). Combining input rewriting and post-editing (I+O) yields the highest translation quality, though the difference compared to input rewriting alone is not statistically significant. This confirms that rewriting text for better translatability before translation plays a more decisive role than post-editing the output."}, {"title": "5.3 Human Evaluation", "content": "Original MT vs. Rewrite MT. We conduct a manual evaluation to determine whether bilingual human annotators rate translations generated using our winning rewrite method (simplification with TOWER-INSTRUCT) as superior to the original translations. For each language pairs (EN-DE, EN-RU, EN-ZH), we randomly select 20 pairs of instances, resulting in a total of 180 annotations from three annotators per pair. Inter-annotator agreement, measured by Fleiss' Kappa, is moderate, with values of 0.43, 0.39, and 0.51 for EN-DE, EN-RU, and EN-ZH, respectively. For each instance, annotators are first provided with two translations and asked to evaluate on three axes: 1) Fluency, 2) Understandability, and 3) Level of detail. Subsequently, we provide the reference translation, and annotators are asked to assess 4) Meaning preservation. Annotators are also given the option to provide free form comments. Further details on the annotation set-up are available in Appendix E.1. As illustrated in Figure 3, the human evaluation results confirm that translations from simplified inputs are rated as more fluent, understandable, and better at preserving the meaning of the reference translation. While this improvement is clear for the EN-DE and EN-ZH pair, for EN-RU pair, annotators rate original MT as more fluent and more faithful to the original meaning. Some EN-RU annotators who preferred the original MT noted that it often retained a more accurate sense of the words in the reference. In contrast, those who favored the simplified rewrite MT highlighted that translations are more contextually appropriate, easier to read, and more comprehensible than the original MT. Original vs. Rewrite. Our automatic meaning preservation metric evaluates the extent to which the original meaning is retained in the rewrite by comparing the rewritten source to the reference translation, rather than to the original source (Graham et al., 2015). Comparing to the original source is in the same language, but introduces a bias toward the original wording. On the other hand, comparing to the reference involves a cross-lingual comparison and is affected by unstable quality of references (Kocmi et al., 2022), but is less biased toward the original wording of the source. To complement our automatic metric, we conduct a manual evaluation to assess how well the rewrites from simplification with TOWER-INSTRUCT preserve the meaning of the original source. We randomly sample 30 pairs of instances and collect three annotations per pair, totaling 90 annotations. Annotators are presented with both the original and rewritten sources and asked to evaluate how well the rewrite captures the meaning of the original source using a 4-point Likert scale (1: Does not capture meaning, 2: Partially, 3: Mostly, 4: Fully). Inter-annotator agreement by Fleiss' Kappa is 0.45. Of the 90 annotations, 55 were rated as 4, 27 as 3, 7 as 2, and 1 as 1, resulting an average score of 3.51. These results indicate that simplified rewrites generated by TOWER-INSTRUCT, although compared against the original source, still largely preserve the original meaning. Further details are provided in Appendix E.2."}, {"title": "6 Related Work", "content": "Rewriting with LLMs. Recent advances in LLMs have demonstrated impressive zero-shot capabilities in rewriting textual input based on user requirements (Shu et al., 2024). Most LLM-assisted rewriting tasks focus on query rewriting (Efthimadis, 1996), which aims to reformulate text-based queries to enhance their representativeness and improve recall with retrieval-augmented LLMs (Mao et al., 2023; Zhu et al., 2024). Rewriting methods include prompting LLMs both as rewriters and rewrite editors (Ye et al., 2023; Kunilovskaya et al., 2024), and training LLMs as rewriters using feedback alignment learning (Ma et al., 2023; Mao et al., 2024). Another line of work focuses on style transfer, where the goal is to rewrite textual input into a specified style (Yuan et al., 2022; Hallinan et al., 2023). Our research aligns with efforts to rewrite texts with LLM assistance; however, unlike these works, we focus on rewriting source inputs to enhance MT quality. Quality Estimation Metrics. The discrepancy between lexical-based metrics (e.g., BLEU (Papineni et al., 2002), CHRF (Popovi\u0107, 2015)) and human judgments (Ma et al., 2019) has led to research in neural metrics. Particularly, quality estimation (QE) metrics, which compute a quality score for the translation conditioned only on the source sentence, have demonstrated benefits in improving MT quality. QE metrics are used for various purposes, including filtering out low-quality translations during training (Tomani et al., 2024), applying to post-editing workflows (B\u00e9chara et al., 2021), and providing feedback to users of MT systems (Mehandru et al., 2023). In our experiments, we use XCOMET as our main evaluation metric, as it shows the best correlation with human judgments (Agrawal et al., 2024). We primarily use XCOMET as a QE metric to compute translatability, further providing this information as knowledge to LLMs to improve MT quality. Rewriting MT Outputs. The symmetric task of post-editing MT outputs has received significantly more attention than rewriting MT inputs. Most recent work relies on LLMs to automatically detect and correct errors in MT outputs using their internal knowledge (Raunak et al., 2023; Zeng et al., 2024; Chen et al., 2024b), with the help of external feedback (Ki and Carpuat, 2024; Xu et al., 2024) or through fine-tuning (Treviso et al., 2024b). In contrast, the task of rewriting MT inputs to make them more suitable for translation has been relatively underexplored with LLMs. While there have been some efforts in query rewriting and style transfer to improve retrieval (Mao et al., 2023; Zhu et al., 2024) and stylistic coherence (Ye et al., 2023; Hallinan et al., 2023), the specific application of LLMs to rewrite inputs for the purpose of enhancing MT quality is still emerging. Our research addresses this gap by focusing on the potential of LLM-assisted input rewriting to improve the translatability and quality of the resulting translations."}, {"title": "7 Conclusion", "content": "In this work, we studied the effectiveness of automatic input rewriting with LLMs in improving the quality of machine translation outputs. We explored a range of rewriting strategies with varying levels of MT-awareness: 1) MT-Agnostic, 2) Task-Aware (knowledge of the end-task), and 3) Translatability-Aware rewrites (knowledge of translatability as measured with QE tools). Our findings show that simpler texts are more translatable. However, MT-Agnostic rewrites do not uniformly help translation quality (\u00a74.1), which motivates us to explore more selective strategies. Selecting inputs based on translatability scores during inference time further boosts translation quality (\u00a74.2), addressing the Pareto optimization challenge by striking a balance between translatability and meaning preservation (\u00a74.3). Analysis shows that simplifying inputs also results in more readable translation outputs (\u00a75.1), and that input rewriting complements post-editing strategies (\u00a75.2). Human evaluation complements our automatic metric by showing that both simplified rewrites and their corresponding MT largely preserve the original meaning of the source and MT (\u00a75.3). More broadly, this work suggests that LLM-assisted input rewriting is a promising direction for improving translations. The approaches introduced here represent a first step in this direction, and future work is needed to discover optimal rewriting strategies for a broader range of models. Furthermore, in line with growing research on LLM-based writing assistants (Lee et al., 2024), these results encourage future work on designing richer interactive approaches to translation with LLMs."}, {"title": "8 Limitations", "content": "We focus our investigation on TOWER-INSTRUCT 7B as our MT system, as it is an open-weight model. We exclude closed and larger models such as GPT-4 in the current experiments. The scope of our study is also limited to out-of-English language pairs, as rewriting with LLMs has been more extensively studied in English (Ma et al., 2023; Ye et al., 2023; Shu et al., 2024; Mao et al., 2024), and using English as the source language benefits performance from its prevalence in LLM training data. One critical area of future research lies in developing rewriting tools that support a wider range of languages beyond English."}, {"title": "9 Acknowledgement", "content": "We thank the anonymous reviewers and the members of the CLIP lab at University of Maryland for their constructive feedback. This work was supported in part by NSF Fairness in AI Grant 2147292, by the Institute for Trustworthy AI in Law and Society (TRAILS), which is supported by the National Science Foundation under Award No. 2229885, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200006, by NSF grant 2147292. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, NSF or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."}]}