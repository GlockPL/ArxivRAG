{"title": "MAINTAINING INFORMATIVE COHERENCE: MIGRATING HALLUCINATIONS IN LARGE LANGUAGE MODELS VIA ABSORBING MARKOV CHAINS", "authors": ["Jiemin Wu", "Songning Lai", "Ruiqiang Xiao", "Tianlang Xue", "Jiayu Yang", "Yutao Yue"], "abstract": "Large Language Models (LLMs) are powerful tools for text generation, translation, and summarization, but they often suffer from hallucinations-instances where they fail to maintain the fidelity and coherence of contextual information during decoding, sometimes overlooking critical details due to their sampling strategies and inherent biases from training data and fine-tuning discrepancies. These hallucinations can propagate through the web, affecting the trustworthiness of information disseminated online. To address this issue, we propose a novel decoding strategy that leverages absorbing Markov chains to quantify the significance of contextual information and measure the extent of information loss during generation. By considering all possible paths from the first to the last token, our approach enhances the reliability of model outputs without requiring additional training or external data. Evaluations on datasets including TruthfulQA, FACTOR, and HaluEval highlight the superior performance of our method in mitigating hallucinations, underscoring the necessity of ensuring accurate information flow in web-based applications.", "sections": [{"title": "Introduction", "content": "With the advent of Large Language Models (LLMs), a transformative era in natural language processing (NLP) has emerged, marking significant advancements in the capabilities of text generation [1], translation [2], summarization [3], and more [4; 5; 6]. Built upon deep learning architectures such as transformers [7], these models have demonstrated"}, {"title": "Related Work", "content": ""}, {"title": "Hallucinations in LLMs", "content": "Hallucinations in LLMs can arise from several sources, including massive training data that may introduce fabricated, outdated, or biased information [9]. The versatility of LLMs, designed to excel in cross-task, cross-lingual, and cross-domain settings, complicates the comprehensive evaluation and mitigation of hallucinations [10]. Various techniques have been proposed to mitigate hallucinations, such as Retrieval-Augmented Generation (RAG) [13], Inference-time intervention [14], Knowledge Retrieval [15], Self-Reflection [16], Exploiting Uncertainty [17], Chain-of-Thought Prompting [18], and System Prompts [19]. These methods aim to enhance the model's ability to ground its outputs in factual information and maintain consistency with the input context. Despite these advancements, the challenge of hallucinations in LLMs remains an active area of research, requiring further innovation and evaluation [9; 10]."}, {"title": "Constrained Decoding Strategies", "content": "Many studies focus on using modified decoding strategies during inference to mitigate hallucinations, as adjusting model parameters requires extensive computational resources. Notable techniques include Context-Aware Decoding (CAD), which uses a contrastive output distribution to amplify the difference between the output probabilities with and without context, effectively overriding the model's prior knowledge [20]. Inference-Time Intervention (ITI) shifts model activations during inference to enhance truthfulness by identifying and shifting activations along attention heads with high linear probing accuracy for truthfulness [14]. Decoding by Contrasting Layers (DOLA) contrasts logit differences between later and earlier layers to minimize the generation of incorrect facts [21]. Additionally, Activation Decoding manipulates the activation patterns of the model during inference to guide the generation process toward more factual and contextually appropriate outputs by measuring and optimizing the sharpness of in-context activations [22]. These strategies aim to improve the factuality and coherence of LLM outputs by guiding the generation process toward more reliable and contextually consistent results."}, {"title": "Information Flow in LLMs", "content": "Many studies have introduced the perspective of information flow to better understand the internal workings of large language models (LLMs). For instance, [23] proposed methods to quantify attention flow in Transformers, showing that raw attention weights can be unreliable for explaining model decisions. They introduced attention rollout and attention flow as post-hoc methods to approximate the relevance of input tokens, yielding higher correlations with importance scores obtained using ablation methods and input gradients. Similarly, [24] developed a method to automatically build"}, {"title": "Absorbing Markov Chain", "content": "AMCs are a special kind of Markov chain where some states are absorbing states that, once reached, cannot be left. Their mathematical foundation and applications have been extensively studied, providing valuable insights into various fields ranging from mathematics to engineering [28]. In particular, AMCs have been applied to model and analyze systems with terminal states, which has implications for reliability and queueing theory [29; 30]. Despite these advancements, the application of absorbing Markov chains (AMCs) to understand the flow of information during inference in large language models (LLMs) remains underexplored. Hallucinations in LLMs occur when the model generates nonsensical or unrelated content, often due to the lack of strong contextual cues or the influence of biased training data. By conceptualizing the last token as an absorbing state and considering all possible paths from prefix contexts to subsequent tokens, our approach can quantify the informational content of contextual tokens. This enables the model to focus more on overlooked tokens during decoding, ensuring the generation of factually accurate and coherent content."}, {"title": "Method", "content": "Our approach aims to optimize the inference process of large language models by treating it as an information flow from the beginning to the end of the context. Specifically, we view the entire inference process as information traveling along various paths from the first token to the last token in the context. To more effectively capture and utilize these information flows, we propose a framework based on an absorbing Markov chain model."}, {"title": "Language Model Architecture", "content": "An autoregressive language model operates by predicting the probability distribution over the vocabulary for the next token given the history of previously generated tokens. Mathematically, this can be expressed as:\n$P(X_{t+1} | X_1, X_2, ..., X_t)$"}, {"title": "Absorbing Markov Chain Formulation", "content": "In our approach, we conceptualize the process of predicting tokens through random walks at each time step as being equivalent to the states in a Markov chain. We first introduce our absorbing Markov chain. Our AMC is denoted by X, with the set of random variables $X = \\{X_n : n > 0\\}$. Within the probability space {\u03a9, F, P}, it uses a one-dimensional countable set as the index set, and this state space is denoted by s. The state space consists of the various states $s_i$, which are the predicted tokens in past t time steps. In our ACM, the random variables satisfies:\n$P\\{X_{t+1} | X_t, ..., X_1\\} = P\\{X_{t+1} | X_t\\}$"}, {"title": "Quantification of Information Flow", "content": "The core of our approach lies in quantifying token-i's flow and loss of information through the computation of the information score S(i) and information loss Linfo(i). These metrics help us understand how well the model retains and propagates key information during the generation process.\n1) Information Score S(i) of the i-th token: Define the information score as\n$S(i) = -log V_{1,i}$\nwhere V1,i represents the probabilities of visiting the transient states of the i-th token from the initial transient state before reaching an absorbing state. The information score S(i) provides a measure of the criticality of the current state. A high value of S(i) indicates that the current token is difficult to reach from its preceding states and contains more information.\n2) Information Loss Linfo(i): Define the information loss as\n$Linfo(i) = S(i) (1 \u2013 V_{i,t})$\nwhere V represents the transient visitation probabilities. The information loss Linfo(i) quantifies the degree to which the information from previous states is lost when transitioning to the current state. Specifically, it directly measures the transfer of information scores from all contextual tokens to the last token, reflecting the extent of information loss up to the last token. A high value of Linfo(i) indicates significant information loss, suggesting that the semantic up to the last token does not effectively preserve the key information from the previous context.\nBy using these metrics, we can identify tokens that are critical for maintaining the context and detecting where information loss occurs. This helps in mitigating hallucinations by ensuring that the model generates content that is more aligned with the key information in the input context."}, {"title": "Adjusting Token Probabilities", "content": "To mitigate hallucinations, we adjust the token probability distributions based on the information loss Linfo(t). The adjusted distribution D(t) for the t-th token is computed as:\n$D(t) = D(t) + \\lambda (Norm(Linfo (i). D(i))$\nwhere \u03bb is a tuning parameter that controls the adjustment extent."}, {"title": "Further elaboration of the methodology analysis", "content": "Why do we use the Markovian framework? By structuring token transitions as a Markov process, we inherently prioritize coherence and contextuality in text generation considering all possible information flow. The Markovian framework naturally lends itself to capturing dependencies and sequential relationships, which are critical for producing coherent text. The methodology introduces a quantitative measure of information flow and loss, enabling a more rigorous approach to diagnosing and mitigating hallucinations. This quantification is a step towards making the often qualitative assessments of text generation quality more objective and actionable.\nWhy this method could quantify information loss? The fundamental matrix N, derived from the submatrix Q of P, encapsulates the expected number of times the process visits each transient state before reaching an absorbing state. This matrix is pivotal for understanding how information propagates through the model. The information score S(t) and the information loss Linfo(t) are derived from N, providing a measure of how much information is retained or lost as tokens are generated.\nWhy are we adjusting token probabilities? The primary rationale for adjusting token probabilities based on information loss is to minimize the divergence from the expected information flow, thereby reducing the likelihood of hallucinatory content. By preferentially selecting tokens that contribute to a lower information loss, the model is guided towards more coherent and contextually relevant generations.\nThe adjustment process involves adding a weighted sum of the predicted next token distributions given all possible prefix context to the original distribution D(t) according to their information loss. This operation not only penalizes tokens associated with higher information loss but also normalizes the distribution to maintain probabilistic integrity. The parameter A allows for the tuning of this adjustment, striking a balance between adherence to the original model predictions and the corrective influence of the information loss metric."}, {"title": "Conclusion", "content": "In this paper, we tried to migrate the phenomenon of hallucinations in Large Language Models (LLMs), which is a critical challenge. Motivated by the need to enhance the reliability and trustworthiness of information generated by LLMs, we proposed a novel decoding strategy that leverages the theory of absorbing Markov chains to mitigate hallucinations. Our approach recognizes the importance of maintaining a coherent and accurate flow of information throughout the generation process, from the initial context to the final output.\nBy modeling the process of generating new content as an information flow problem, we were able to precisely quantify the significance of contextual information and measure the extent of information loss during the generation process. Our method considers all possible paths from the first to the last token, allowing for a comprehensive assessment of the importance of key information and its propagation. This innovative use of absorbing Markov chains provides a robust mechanism to ensure that the model remains focused on generating factually accurate and contextually coherent content.\nEvaluations on diverse datasets, including TruthfulQA, FACTOR, and HaluEval, demonstrated the superior performance of our method in reducing hallucinations and improving the quality of the generated text. These findings underscore the potential of our method to serve as a valuable tool in enhancing the reliability and trustworthiness of LLMs across various applications, from news writing to knowledge dissemination and beyond.\nIn conclusion, our work highlights the necessity of ensuring accurate information flow in LLMs and offers a promising direction for future research and development in the field of natural language processing. Through continuous innovation and exploration, we aim to further advance the capabilities of LLMs, making them more reliable and trustworthy sources of information for users around the world."}]}