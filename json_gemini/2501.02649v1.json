{"title": "Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features", "authors": ["Haixu Liu", "Penghao Jiang", "Zerui Tao", "Muyan Wan", "Qiuzhuang Sun"], "abstract": "Predicting plant species composition in specific spatiotemporal contexts plays an important role in biodiversity management and conservation, as well as in improving species identification tools. Our work utilizes 88,987 plant survey records conducted in specific spatiotemporal contexts across Europe. We also use the corresponding satellite images, time series data, climate time series, and other rasterized environmental data such as land cover, human footprint, bioclimatic, and soil variables as training data to train the model to predict the outcomes of 4,716 plant surveys. We propose a feature construction and result correction method based on the graph structure. Through comparative experiments, we select the best-performing backbone networks for feature extraction in both temporal and image modalities. In this process, we built a backbone network based on the Swin-Transformer Block for extracting temporal Cubes features. We then design a hierarchical cross-attention mechanism capable of robustly fusing features from multiple modalities. During training, we adopt a 10-fold cross-fusion method based on fine-tuning and use a Threshold Top-K method for post-processing. Ablation experiments demonstrate the improvements in model performance brought by our proposed solution pipeline. This work achieves a private leaderboard score of 0.36242 in the GeoLifeCLEF 2024 LifeCLEF & CVPR-FGVC Competition, securing third place in the rankings (Team name: Miss Qiu).", "sections": [{"title": "1. Introduction", "content": null}, {"title": "1.1. Background and related literature", "content": "Predicting the composition of plant species over segmented time and spatial scales plays an important role in managing and protecting ecosystem biodiversity and improving species identification tools. Therefore, the LifeCLEF lab of the CLEF conference and the FGVC11 workshop of CVPR jointly host the GeoLifeCLEF 2024 competition centered around this task [1, 2].\nWe review the strategies of past winners in this competition. The fourth-place [3] in 2022 mentions a method for constructing \"Patched\" approaches, where variables from eight different modalities provided that year are aligned into a single image format of (256,256,3). These modalities are then processed separately by ResNet50 [4], and the output features are concatenated and passed through a linear layer, with Top-K processing applied to the outputs. In contrast, the second-place entry from the same year [5] abandons data from other modalities and only uses remote sensing data, creating NDVI image format features using NIR and RGB data. They train multiple models based on ResNet50, DenseNet201, and Inception-v4, and fuse the logits.The champion's strategy in 2023 [6, 7] introduces three backbone networks based on ResNet. The first network solely extracts features from bioclimatic raster data, while"}, {"title": "1.2. Our method", "content": "Our study uses survey data of 11,255 species to train a machine-learning model for this goal. Each sample in the dataset comprises satellite imagery and time series [8, 9, 10] linked with geographical coordinates, climate time series [11, 12], and other rasterized environmental data such as land cover, human footprint, bioclimatic, and soil variables.\nTo effectively extract features across these modalities for accurate predictions, we propose the following solution pipeline for machine learning. First, we create a graph structure based on the available data, treating SurveyIDs as nodes. Nodes are connected based on whether they fall within the same ecological niche (less than 10 kilometers apart) and share geographical and annual similarities. Subsequently, we aggregate the labels of all adjacent nodes for each node, using these as new features for that node. In our model, we use a Swin-Transformer-based method for extracting temporal features, instead of the commonly used Resnet18 network. Additionally, we provide an optional replacement for the Swin-Transformer Tiny, used in the above baseline model for image feature extraction, with EfficientNet-B0. This substitution can significantly expedite model training with minimal impact on prediction accuracy. We also develop a hierarchical cross-attention mechanism (HCAM) to fuse features extracted from different modalities, which effectively uses information from multimodality data. Last, in the post-processing steps, we improve the traditional Top-K rule for multi-task classification. Specifically, we integrate a series of thresholds for the Top-K rule for model outputs (see Section 3.7). We then use the graph structure and the above model outputs for the final species prediction. The details of this novel solution pipeline is provided in Section 3."}, {"title": "1.3. Contributions", "content": "We summarize our contributions as follows:\n1. In preprocessing, we construct a graph structure for independent Survey IDs, using labels of nodes (Survey IDs) with adjacency relationships to generate new features. This graph structure improves the final prediction results.\n2. Our backbone network for visual features modifies the Swin-Transformer structure to extract temporal characteristics. This strategy is shown to enhance model performance.\n3. We design a hierarchical cross-attention mechanism to fuse features from different modalities. This strategically mitigates overfitting.\n4. Our proposed post-processing strategy combines the advantages of the threshold method and Top-K approach, also improving the final prediction accuracies.\nOur model, named Tighnari, is inspired by the Forest Ranger character in the popular open-world game \"Genshin Impact,\u201d who is adept at identifying a variety of species in rainforests and is committed to ecological conservation. Just as Tighnari ensures the health of the rainforest ecosystems, we aspire for our model to accurately predict the composition of plant species in given spatiotemporal contexts and make significant contributions to environmental protection. The model's acronym, TIGH, stands for T for Transformer, I for Image processing (computer vision) backbone, G for Graph feature extract, H for Hierarchical cross-attention fusion mechanism."}, {"title": "2. Exploratory Data Analysis", "content": "Exploratory Data Analysis (EDA) is essential for unveiling the motivations behind our choice of modeling techniques. We divide our analysis into several steps, starting with the visualization of unstructured data (shown in Figure 1 and Figure 2):"}, {"title": "3. Methodology", "content": "In this section, we provide a detailed description of each step in the modeling process and the motivations behind them."}, {"title": "3.1. Table Data Cleaning and Missing Value Imputation", "content": "Initially, we group the metadata for both PA and PO by Survey ID. We then replace outliers in the grouped metadata of PA and the test set with null values, and impute missing values with the mean. Categorical data were then one-hot encoded. Subsequently, for the label(speciesId), we encode each element according to its corresponding number using 0-1 encoding.\nNext, we access all tabular data for the training and test sets from the EnvironmentalRasters folder. We notice that the human footprint column contained -1 and extreme outliers. Based on the observed data distribution, we set all values greater than 255 to 255 and those less than 0 to 1. Subsequently, we merge all tables by Survey ID, replacing all infinite and infinitesimal values with nulls, and again impute all missing values with the mean. Finally, the training sets of the PA and test sets were merged again with the cleansed EnvironmentalRasters data by Survey ID to produce the cleaned tabular modality data.In the subsequent sections, we represent the features extracted from tables as F.\nFollowing this, we clean the time series data by folding it into cubes. In the baseline method, all null values in the Tensor are replaced with zeros. We change this replaced value to the mean of the Tensor. Additionally, as the Swin-Transformer cannot accept prime numbers as the dimensions of input image matrices, we trim the shapes of two sets of time series cubes from (4, 19, 12) and (6, 4, 21) to (4, 18, 12) and (6, 4, 20) respectively. This is justified because the last year in the series already had many missing values, so trimming directly does not result in significant information loss."}, {"title": "3.2. Graph Construction and Utilization", "content": "Graph construction and utilization are highlights of our work. To establish graph relationships among samples, we based our approach on two fundamental assumptions. First, we assumed a clustering tendency in the spatial distribution of individual species, meaning that if a species appears in nodes surrounding a particular Survey ID, the likelihood of its occurrence at that Survey ID increases. Second, we hypothesized that within the same ecological environment, there is a correlation between the spatiotemporal distributions of different species, implying that samples (nodes) close in geographical"}, {"title": "3.3. Temporal Feature Extraction", "content": "Another highlight of our work is the development of a temporal feature extraction method based on the Swin-Transformer Block. This approach is inspired by Haixu Wu et al., who used a CNN-based Inception backbone network to extract features from folded time series data in TimesNet[14]. Wu and colleagues argue that this method, compared to traditional time series neural networks, is not only better at capturing multi-scale sequential relationships but also has a stronger capability for spatio-temporal information fusion. It shows superior performance across various time series analysis tasks and offers more efficient training and inference. Our goal in processing time series is to obtain higher quality features that are more conducive to modality fusion, rather than making better predictions about future time points. Therefore, we believe that using a visual backbone network to process time series cubes should yield better feature extraction results than traditional time series models.\nIn the current research on deep learning technology, whether for image processing or time series prediction tasks, methods based on Transformers are considered superior to those based on CNNs. Consequently, we experiment with a backbone network specially designed for extracting temporal features using Swin-Transformer Blocks and Vision Transformer Blocks.\nTaking Swin-Transformer as an example, for time series cubes cropped to sizes (4, 18, 12) and (6, 4, 20), we set the Patch size to (3,3) and (2,5), and the Window size to (3,2) and (2,3), respectively. We stack a Swin-Transformer Stage with a depth of 2 and 12 attention heads and another with a depth of 6 and 24 attention heads to create the backbone network for handling our specific time series cubes. The attention function can be defined mathematically by the equation:\n$Attention(Q, K, V) = SoftMax(\\frac{QKT}{\\sqrt{d}} + B),$ \nwhere Q represents the query matrix, K represents the key matrix, V represents the value matrix, d is the dimensionality of the keys and queries, typically used for scaling, B represents the positional encoding for time sequences. Unlike the classic Swin-Transformer, where a two-dimensional vector indicates the absolute position of patches in an image, here we employ a one-dimensional vector to represent the position of each patch in a flattened sequence. This modification better adapices to temporal tasks. Apart from that, the meanings of other symbols and formulas are the same as those in the classic Swin-Transformer, and are not reiterated here [15].\nWe depart from the standard practice of stacking four stages in the Swin-Transformer backbone network because the dimensions of the time series cubes are too small. With each stacked stage, the dimensions for the subsequent stage are halved. Moreover, the Swin-Transformer Block does not accept odd dimensions for input feature maps, making two blocks the most logical configuration for our current time series cube dimensions.\nThe depths of the two stacked blocks are chosen to be 2 and 6, corresponding to the depths of the second and third stages in the classic Swin-Transformer backbone network. This configuration means that they extract shallow and deep features, respectively. This 1:3 depth ratio has also been adopted by subsequent backbone networks, such as ConvNeXt [16].\nWe select 12 and 24 as the attention head counts for the two stages, following the counts used in the third and fourth stages of the classic Swin-Transformer backbone network. Increasing the number of attention heads allows the model to independently capture features across more subspaces. Since the feature map sizes entering the third and fourth stages in the original model are already quite small, similar to the size of our time series cubes, having many attention heads does not overly increase the computational burden. Therefore, we use as many attention heads as possible to more comprehensively extract features from the time series cubes. The input and output representations of the final model are as follows:\n$T = Temporal-Swin-T(U),$ \nwhere $U \\in R^{H\\times W\\times C}$, H represents the number of seasons or months, W represents the number of years, C represents the number of channels, T are the features after Swin-Transformer processing, respectively."}, {"title": "3.4. Image Feature Extraction", "content": "Although this is a multi-task classification problem, the low resolution of satellite images clearly does not allow for accurate prediction of plant species in a given region. Therefore, the primary motivation for processing images is still to extract high-quality features that are conducive to modality fusion. In the baseline, the image feature extraction network employed is the tiny version of Swin-Transformer, which was presented in a Best Paper at ICCV 2021 [4]. We note that most models are trained with input sizes of (224,224) or (384,384). To better utilize the weights preserved in the pre-trained model and retain the original information carried by the satellite images, we resize the satellite images from (128,128) to (224,224) before inputting them into the pre-trained model for fine-tuning. Experimental evidence shows that this adjustment significantly enhances the model's performance.\nTo explore whether there are better alternatives, we conduct comparative experiments with other models such as EfficientNet-B0 [18], ConvNeXt-Base [16], and ViT-Base [20], all pre-trained with an input size of (224,224). The choice of ConvNeXt-Base and ViT-Base is based on their status along with Swin-Transformer as the current state-of-the-art (SOTA) solutions in computer vision backbone networks, and they are comparable to the tiny version of Swin-Transformer in terms of the number of parameters. EfficientNet-B0 was selected partly because it is one of the most powerful feature extraction networks among traditional CNN-based backbones, seen as a superior alternative to the ResNet scheme. Moreover, EfficientNet-B0 is exceptionally lightweight and converges quickly, both in terms of parameter count and training time, allowing for significant optimization of the overall model training and inference time without substantial performance loss.\nOur comparative experiments reveal that using the ResNet18 scheme for both temporal and image feature extraction achieved the highest accuracy. However, using Swin-Transformer for temporal feature extraction and EfficientNet-B0 for image feature extraction reduced training time by approximately 75% without a significant decrease in accuracy, thanks to faster per-epoch training durations and overall faster convergence. The input image is $I \\in R^{H\\times W\\times C}$.\n$S = Swin-Transformer-Tiny(I),$"}, {"title": "3.5. Hierarchical Cross-Attention Fusion Mechanism(HCAM)", "content": "Another significant highlight of our work is the introduction of a hierarchical cross-attention fusion mechanism to address the challenge of efficiently fusing feature vectors with varying information densities extracted from different modalities. In the aforementioned steps, we have extracted information (T, S, F', G') from four modalities, including time-series modal (U), satellite imagery modal (I), and tabular modal features (F), as well as graph modal features (G) we constructed and extracted ourselves. F' and G' represent F and G after being processed by fully connected layers.\nIn terms of information density, satellite imagery modal features are the most dense because the backbone network essentially compresses information carried by multiple channels of an image into a limited set of features for classification mapping via a fully connected layer. Time-series and tabular modal features are less dense, as in our model, we attempt to map them to feature vectors of the same or even higher dimensions. Graph modal features are the sparsest; although we aggregate features from different nodes, the majority of elements in a graph feature vector relative to all 11,255 dimensions are still marked as 0.\nInitially, we try to use concatenation for modality fusion similar to the baseline, but due to the high dimensionality and sparsity of the graph feature vectors, the model's loss reduction process is unstable."}, {"title": "3.6. Mix up +10 Fold Cross Fusion training strategy", "content": "We adopt the Mix up training strategy provided in the baseline, which is a very common method for data augmentation during training. Specifically, this method involves randomly shuffling the order of training data and labels in the current batch and then performing a weighted addition with the unshuffled training data and labels. The input matrices U, I, F, and G, as well as the label matrix L, are shuffled to create \u0164, \u0160, F, \u011e, and L:\n$T_{mix} = \\lambda T + (1 - \\lambda)\\tilde{T},$\n$S_{mix} = \\lambda S + (1 - \\lambda)\\tilde{S},$\n$F_{mix} = \\lambda F + (1 - \\lambda)\\tilde{F},$\n$G_{mix} = \\lambda G + (1 - \\lambda)\\tilde{G},$\n$L_{mix} = \\lambda L + (1 - \\lambda)\\tilde{L}.$\nThis approach enhances the robustness of the training process, improves the generalization perfor-mance of the model, smooths out the distribution of samples across different categories, and makes originally sparse labels relatively dense.\nMoreover, to fully utilize the training data and reduce model overfitting, we employ a ten-fold cross-fusion technique to train the model. The concept of ten-fold cross-fusion is an improvement over ten-fold cross-validation. The specific operation involves dividing the dataset into ten parts, with each part serving once as the validation set, while the other nine parts are used to train a brand new model. The logits output by these ten new models are then averaged. However, this approach also results in a training efficiency about ten times lower than before. To address this drawback, I reason that the training datasets used for each model are highly overlapping, and except for the first model, the training of the subsequent nine models can be considered as fine-tuning the first model using a slightly altered dataset. Motivated by this, we optimize our training strategy. For the first model, we initialize parameters and train it using an early stopping strategy. For each subsequent model, we clone the parameters of the first model and fine-tune it on a newly combined training set. This method reduces the number of training epochs for subsequent models, thereby enhancing training efficiency. The original dataset is D; we divide it into ten parts {$D_1, D_2, . . . , D_{10}$}. We train a model $M_1$ by setting each $D_i$ as the test dataset: $M_1$ = Train ($D\\backslash D_i$). Then we compute the average of logits from these models:\n$\\L = \\frac{1}{10}\\sum_{i=1}^{10}M(x),$ \nwhere x is the model input. The training process and results visualization for each model are as follows:"}, {"title": "3.7. Post-Processing: Threshold Top-K and Output Correction", "content": "Based on the official requirements, the final formula for calculating the score is as follows:\n$F_1 = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{TP_i}{TP_i + (FP_i + FN_i)/2},$\n$TP_i = Number-of-predicted-species-truly-present, i.e., \\hat{Y} \\cap Y_i,$\n$FP_i = Number-of-species predicted-but-absent, i.e.. \\hat{Y} \\backslash Y_i,$\n$FN_i = Number-of-species not-predicted-but-present, i.e., Y_i\\backslash \\hat{Y}.$\nIn the baseline, the classic multi-class task method of Top-K is used to filter the model's output. Typically, in conventional Top-K, a value for K is manually set or initially a range is predefined, within which K is enumerated to observe which K yields the best performance on the validation set inference results, and this K is then applied to the test set inference process.\nHowever, we observe drawbacks with this method. Some Survey IDs contain dozens or even hundreds of species. Although these species might have high probabilities in the model's output, they are truncated if they do not rank within the top K. Additionally, some test set Survey IDs, even with low probabilities for each species, are still forced to output the top K species by probability rank.\nTherefore, we improve the Top-K algorithm by setting a range of thresholds (0.1 to 0.5, in steps of 0.01) for each possible K to filter out species with predicted probabilities below these thresholds. By exhaustively combining K and threshold values and comparing the scores of the validation set outputs processed through them, we can identify the optimal pair of K and threshold.\nLet the probability of the model output be P = {$P_1,P_2,...,P_{11255}$}, where $p_i$ is the predicted probability of species i. We filter the output by setting the threshold $\u03b8$ and K values:\n$S = {i|p_i > \u03b8} \\cup Top-K(P),$\nwhere S is the set of filtered species.\nWe can see that the darker colored regions represent higher scores for this pair of parameters. In the end, we choose the optimal K of 44 and the optimal threshold of 0.23 (shown in Figure11)."}, {"title": "4. Experiments", "content": "In this chapter, we present the experiments conducted to validate the superiority of the Proposal model and analyze the experimental results. We omit the hyperparameter tuning process in the paper because each backbone network corresponds to different model hyperparameters and training hyperparameters. Given the limited time, it is challenging to prove the optimality of the selected hyperparameters through grid search. Instead, we judge the current parameters' potential to cause overfitting or underfitting by observing the gradient descent process, or whether the current parameter combination results in lower loss and higher scores compared to previous combinations.We perform exploratory tuning for each model involved in the comparative and ablation experiments to ensure that the current hyperparameter combination is the best among all attempts. We also employ an early stopping strategy, which minimizes the impact of hyperparameter changes on training when there is no significant overfitting or underfitting.\nAt the end of this chapter, we provide a table of the hyperparameters used for the final submission.We select the following metrics to analyze the model's performance, including public and private leaderboard scores on the official test set. Additionally, since we ultimately need to fuse the logits output by the models, we introduce the loss on the validation and training sets. Since the calculation of BCE essentially equals the sum of entropy and KL divergence, it describes the difference between the probability distribution of the model output and the distribution of the true labels. Therefore, the logits of models with lower validation and training set losses result in better fusion effects than models with lower validation scores but higher losses. Finally, we explore how to effectively lightweight the model, using the training time per epoch as a metric.\nOur experiments are all conducted on the Colab platform running on an A100 instance. The specific computational resources include 83.5GB of memory and 40GB of GPU memory.\nTo facilitate the customization of model parameters, we use the Timm library instead of the torchvision library to instantiate models. Surprisingly, the baseline reconstructed based on the Timm library achieved a better private leaderboard score compared to the official baseline (the official baseline's"}, {"title": "4.1. Comparative Experiments", "content": "For the satellite image feature extraction network, the comparative experiments are as follows:"}, {"title": "4.2. Ablation Studies", "content": "Based on the conclusions drawn from the comparative experiments, we initially replace the backbone networks for image and temporal feature extraction starting from the Baseline model. Subsequently, we attempt to use the Swin-T backbone network for temporal feature extraction and ViT-Base for satellite image feature extraction, but encounter difficulties with gradient descent. We analyze this issue and"}, {"title": "5. Conclusion and Discussion", "content": null}, {"title": "5.1. Conclusion", "content": "Our comprehensive comparative experiments demonstrate that we select the most suitable feature extraction backbone networks for each modality's data. Through rigorous ablation studies, we prove that each improvement we proposed incrementally enhance the model's performance, ultimately achieving a score of 0.36242 on the private leaderboard and in third place(The leaderboard showed 0.35292, but we continue to optimize some parameters while doing the experiment to finally get 0.36242). In addition to proposing high-scoring solutions, we also introduce a lightweight model and an efficient training strategy. Without significantly sacrificing accuracy, we significantly reduce the training time for the ten-fold cross-fusion model (by more than 50%) and the time for a single epoch and the total number of epochs (by approximately 75%). Thus, we manage to train ten models for logits fusion in roughly the same amount of time it previously takes to train one baseline model, achieving prediction results far exceeding the baseline score. It is worth mentioning that we average the outputs of all models that score higher than the baseline and apply post-processing, ultimately achieving a private leaderboard score of 0.36501.\nIn summary, from a theoretical perspective, our research provides new insights into the extraction of temporal information for modality fusion tasks, namely folding it into a 2D matrix and extracting features through a visual backbone network. We also design a robust fusion method for features extracted from multiple modalities with varying information densities, using a hierarchical cross-attention mechanism to dilute features from high to low density progressively. Additionally, we propose a graph-based feature construction method and output correction post-processing algorithm for the multi-task classification task of species prediction under specific spacetime, which often involves extremely unbalanced or sparse labels. From an application perspective, our research is of significant importance in fields such as ecology, agriculture, environmental protection, and climate change studies."}, {"title": "5.2. Future Work", "content": "Despite of our comprehensive comparative and ablation experiments, our work may be further explored in the following way:\n\u2022 We will further utilize the organized PO data, noting that there are some high-quality publishers within PO whose surveys of the same ID often contain many species. Additionally, the competition provides supplementary data about other modalities for Survey IDs in PO. We believe that these Survey IDs appearing in PO can be selected through certain logical criteria to serve as training samples for weakly supervised learning, incorporated into PA. This approach will better leverage crowdsourced data, reduce the workload of data collection, and enhance model accuracy.\n\u2022 Referring to past programs, highly ranked teams would extract the rasters around the Survey ID geographic location from the tiff files provided by EnvironmentalRasters as images to be processed. However, due to arithmetic and time constraints, we finally give up on implementing this scheme, and we plan to use this part of the data in our subsequent work to realize a leap in model performance.\n\u2022 Our current model establishes graph relationships solely for feature aggregation and result calibration. In future work, we plan to use graph neural networks to replace the current method of manually setting weights for adjacent nodes in feature aggregation. This will support the use of weakly supervised and semi-supervised learning training strategies to progressively correct labels of weakly supervised and semi-supervised nodes, thereby improving training outcomes.\n\u2022 We hope to introduce NAS technology to optimize the hyperparameters of our 2D time series feature extraction network based on Swin-Transformer and the hierarchical cross-attention mechanism, further enhancing the model's performance."}]}