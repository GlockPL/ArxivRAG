{"title": "From Natural Language to Extensive-Form Game Representations", "authors": ["Shilong Deng", "Yongzhao Wang", "Rahul Savani"], "abstract": "We introduce a framework for translating game descriptions in natural language into game-theoretic extensive-form representations, leveraging Large Language Models (LLMs) and in-context learning. We find that a naive application of in-context learning struggles on this problem, in particular with imperfect information. To address this, we introduce GameInterpreter, a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively. In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets and the corresponding partial tree structure. With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit. Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions. We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity. Our experimental results show that the framework significantly outperforms baseline approaches in generating accurate extensive-form games, with each module playing a critical role in its success.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, large language models (LLMs) have shown remarkable proficiency in handling complex tasks across various domains, including code generation [6, 7, 26, 42] and question answering [19, 27, 50]. Their success has sparked interest in exploring their potential across an even broader range of applications. Within the field of multi-agent systems, a primary research direction focuses on developing LLMs' capabilities for reasoning about games and making decisions directly from textual information. For instance, Fu et al. [14] applied LLMs to a bargaining game where LLMs serve as bargaining agents, engaging in price negotiations across several rounds. In this scenario, a successful bargaining agent must anticipate the behavior and private information of other agents, which requires strong game reasoning skills.\nAlthough there has been initial progress in this area, conducting game-theoretic analysis directly from textual descriptions (such as natural language game descriptions) remains challenging due to the varying degrees of strategic complexity in games, including imperfect information, chance events, and repeated interactions. Broadly, there are two technical approaches to tackle this task. The first approach involves training LLMs specifically to perform game-theoretic analysis. This method enables LLMs to conduct analysis directly but often requires extensive training data in games and their descriptions, as well as significant computational resources. The second approach utilizes LLMs to interpret game descriptions and generate structured representations that can then be analyzed using game-theoretic methods. Rather than equipping LLMs with full reasoning abilities, this approach integrates LLMs into the automated reasoning process, which can reduce computational demands.\nIn this work, we pursue the second approach, presenting GameInterpreter, a framework that translates natural language game descriptions into the \"extensive-form\". An extensive-form game (EFG) is a standard game-theoretic representation for sequential games [51]. It is a rooted tree with additional information structure called information sets, which group together decision nodes that are indistinguishable to a player. Our framework relies on LLMs and in-context learning, where the LLM uses context or examples in the input prompt to complete tasks without the need for fine-tuning or further training. However, due to the strategic complexities outlined above, directly describing the task in the prompts for in-context learning is insufficient. Among these complexities, we particularly emphasize the issue of imperfect information \u2013 that is, where at least one player does not have full knowledge about the current state of the game \u2013 which leads LLMs with naive in-context learning to produce incorrect game representations, as demonstrated in our experiments.\nTo address this, we take a divide-and-conquer approach using a two-stage process. In the first stage, we focus on any imperfect information in the game, by guiding LLMs through examples of dealing with imperfect information (e.g., identifying information sets) and the corresponding partial tree structures. With this foundation, the second stage leverages in-context learning to generate the complete EFG for the target game. The EFG is created using pygambit, the Python API for the widely used game-theoretic tool Gambit [43], which also enables automating tasks such as computing Nash equilibria from natural language descriptions. Additionally, we introduce a self-debugging module that returns pygambit error messages to the"}, {"title": "2 RELATED WORK", "content": "Many papers have explored the use of LLMs as agents to play games, ranging from simple matrix games [2, 29, 40, 47], to much more complex environments [32, 44, 53]. Akata et al. [2] revealed the different behavioral patterns of LLMs when playing in various types of games. Shi et al. [44] examined the ability LLM agents to cooperate in the Avalon game, and developed a memory-based module to facilitate the cooperation. Xu et al. [53] tested LLM agents in the incomplete information game Werewolf. They observed emergent strategic behaviors such as deception during gameplay. Fan et al. [13] analyzed the rationality of LLMs as agents, focusing on three specific aspects: building a clear desire, refining beliefs about uncertainty, and taking optimal actions. Silva [47] explored if LLMs can be used as an equilibrium solver for games, and highlighted the difficulty of this for games with only mixed-strategy equilibria, providing enhancements to address this.\nBesides using LLMs as agents, game-theoretic approaches could be utilized to improve the performance of LLMs. Gemp et al. [15] introduced a method that feeds the outputs from game-theoretic methods (e.g., an equilibrium distribution over instructions) to LLM agents in dialogues that can be formed as EFGs. They demonstrated that the integration with game-theoretic outputs could enhance the LLM generations compared to a baseline LLM that lacks access to game-theoretic supports. Ma et al. [30] studied the value alignment problem in LLMs. They gamified the attacks and counterattacks among LLMs and used equilibrium solutions to improve the level of value alignment of LLMs. Similarly, Jatova et al. [20] framed the generation of toxic content and defence against this as a strategic game between a language model and an adversarial prompt generator, and the equilibrium of the game demonstrated a reduction of harmful outputs."}, {"title": "2.1 LLMs with Game Theory"}, {"title": "2.2 Game Description Translation", "content": "We are aware of three works that directly addresses the task of game description translation [9, 34, 35]. The earliest one by Mensfelt et al. [34] is contemporaneous and independent work with ours. Rather than using EFGs, for representing games, Mensfelt et al. [34] employed logic representations as used by logic programming solvers. A further key difference between our work and theirs lies in the scope of games analyzed. Mensfelt et al. [34] focused on simultaneous-move games, with 110 bimatrix games of size 2x21, one bimatrix game of size 3x3, and one sequential game that corresponds to a bimatrix game of size 2x4. In their subsequent work, Mensfelt et al. [35] examined 55 simultaneous-move 2x2 bimatrix games. In contrast, our work explores more complex scenarios, with multiple sequential moves and complicated (imperfect) information structures. Both work leverage the ability of LLMs to generate code, which we discuss next. The same framework was also adopted in their follow-up work by Mensfelt et al. [35]. In this follow-up work, they presented a necessary condition for automatically verifying the correctness of the generated game. In addition to these two works, Daskalakis et al. [9] converted the sequential decision-making process described in the game derived from a story to an EFG. They achieved this by utilizing LLMs to introduce additional decision nodes, representing alternative choices players could have made in the story. Once the EFG is constructed, Gambit is used to compute the Nash equilibrium, providing a prediction of the players' behavior.\nMore broadly, several studies have used LLMs to translate general texts (i.e., not necessarily game descriptions) in natural language into formal specifications (i.e., required format of a software system or hardware component) [17, 23, 24, 54]. Hahn et al. [17] examined the ability of fine-tuned language models to convert natural language into formal specifications, which can be used in software verification, theorem proving, and industrial hardware. Zhai et al. [54] and Leong and Barbosa [24] focused on translating textual requirement descriptions into Java formal specifications. Leite et al. [23] employed LLMs to generate specifications for smart contracts."}, {"title": "2.3 LLMs for Code Generation", "content": "Code generation is the process of automatically creating source code using LLMs based on natural language task descriptions. Since the introduction of models like Codex [6], Alphacode [26], Pangu-Coder [7], and LLaMa Coder [42], general code generation has made significant advancements with the emergence of models such as LLaMa 3 [12] and GPT-4 [1]. Building on these models, many studies [22, 28, 46] further improved the performance of code generation through reinforcement learning or self-debugging prompts. In our work, we leverage the robustness of code generation (with self-debugging) for generating EFG representations in pygambit."}, {"title": "3 PRELIMINARIES", "content": "In-context learning refers to an LLM's ability to learn new information or skills by observing examples or instructions provided in its input, without any additional training or fine-tuning [11]. Suppose we have an LLM, represented by a function M. It has been trained to predict the next token or word, probabilistically mapping an input prompt to an output. An input prompt is a sequence of tokens z = (z1, z2, ..., zn). Given z, the probabilities for the next token y are then M(y|z) = P(y | z1, z2, ..., zn). For in-context learning, consider an input prompt that contains a sequence (Qi, Ai) of examples, which could be in the form of question-answer pairs, or these could be examples of how to solve examples of a specific task. We use these as context to predict an answer An+1 for a new question Qn+1, with our probabilistic answer being M(A|(Q1, A1), (Q2, A2), ..., (Qn, An), Qn+1). Importantly, with in-context learning, the model does not adjust its weights, but instead uses the examples in its the context window to to refine its conditional probability for the next token."}, {"title": "3.1 In-Context Learning"}, {"title": "3.2 Extensive-Form Game Representations", "content": "Extensive-form representations explicitly capture sequential decision making, in contrast to the strategic form, which condenses the game into a payoff matrix, interpreted as a simultaneous-move game. Therefore, EFGs are more expressive than strategic-form games, making them our preferred target format game translation. An extensive-form game consists of the following three elements:\nGame tree: The central element of an EFG is a rooted, directed tree. Each decision node in the tree is assigned to either one of the players or to a \"chance node,\" which represents nature2. Directed edges are known as moves or actions. A play of the game starts at the root and advances through the tree as the player that owns the current decision node chooses a move at the node (or a move is drawn from a discrete probability distribution at a chance node). The play ends when a terminal node (leaf) of the tree is reached.\nOutcomes and payoffs: Every terminal node corresponds to an outcome, with an associated payoff vector which prescribes the payoff for each player under this outcome.\nInformation sets: An information set groups together nodes of a given player, with the interpretation that a player knows they are at some node in an information set, but not which one3. Information sets allow us to model a player's lack of knowledge of past moves of other players including nature (or the same player in the case of imperfect recall). If all information sets are singletons the game is said to have perfect information, otherwise it is said to have imperfect information."}, {"title": "3.3 Code Generation with Gambit", "content": "Gambit is a software suite for game-theoretic analysis [43]. Gambit can compute game-theoretic solutions such as Nash equilibria and equilibrium refinements for strategic-form games and EFGs. It has a Python API known as pygambit. With pygambit, we create an internal representation of an EFG and export it to a standard file format, specifically an .efg file. This file can then be analyzed or visualized"}, {"title": "4 THE GAME INTERPRETER FRAMEWORK", "content": "In Figure 2, we present the full GameInterpreter framework for translating natural language game descriptions into EFG files. It involves two stages: imperfect information retrieval and complete EFG generation. In the first stage, we address the challenge of handling imperfect information by employing in-context learning to identify non-singleton information sets and their associated partial tree structures. The inputs for this in-context learning, detailed further below, include: general information about the task and the use of the pygambit API, a description of the target game, and instructions with examples for extracting imperfect information. At this stage, the expected output is a code block. For imperfect information games, the code block (should4) include a set of information sets defined by the function set_infoset(), which groups decision nodes that a player cannot distinguish between, accompanied by reasoning provided in the code comments. For perfect information games, the output contains only code comments, which includes a concluding statement such as \"there is no need to set any information sets in this game,\" along with reasoning for this conclusion.\nNotably, generating these information sets provides insights into the EFG tree structure. For example, Figure 3 shows an EFG with two players. Player 1 moves first by choosing one of three actions: L, C, or R. If either C or R is selected, then player 2 does not know which was selected (i.e., imperfect information). In contrast, if player 1 chooses action L, the resulting subgame has perfect information. After applying our method from stage one, the LLM can separate"}, {"title": "5 EXPERIMENTAL SETUP", "content": "In our experimental evaluation, we used two datasets: a custom dataset created specifically for this study, which focuses on sequential games with a single description provided for each underlying game, and a dataset from Mensfelt et al. [34], which emphasizes bimatrix (simultaneous-move) games and includes multiple descriptions for the same underlying game. The latter dataset is particularly useful for assessing the robustness of our method to variations in descriptions."}, {"title": "5.1 Datasets"}, {"title": "5.2 Baselines", "content": "We evaluate the effectiveness of each module in our framework by experimenting with various settings, both including and excluding the modules. In our most basic setting, we task the LLM with generating an EFG file directly based on a game description, without requiring it to utilize the pygambit API. We then investigate four settings that require the pygambit API:\nSetting A: The setting that utilizes minimal EFG generation prompts in Table 1, without incorporating additional information for imperfect information retrieval;\nSetting B: Setting A with self-debugging;\nSetting C: Setting A with imperfect information retrieval;\nSetting D (Full Pipeline): Setting A with both self-debugging and imperfect information retrieval."}, {"title": "5.3 Evaluation", "content": "When translating game descriptions into extensive-form representations, a key task is to ensure that the generated EFG is consistent with the description. Formally, a game description corresponds to a family of EFGs that are consistent with that description; this family can vary in size, and could possibly even by infinite, depending on the specificity of the game details provided. For instance, if a game description includes inequalities or relationships between payoffs but lacks precise values, many payoff assignments may be consistent with it, with different corresponding EFGs. Similarly, for descriptions of simultaneous-move games (e.g., bimatrix games), a consistent EFG could depict either of the two players acting first, with imperfect information modeling the simultaneity of their choices.\nFor checking the consistency, we manually assess whether the game tree, information sets, and payoffs in the generated EFG are consistent with the target game description. This requires firstly checking the generated tree structure, including which players move at which nodes, starting from the root, along with how the actions at the nodes lead to moves of other players; this is informed by the labels for moves that are provided in the generated EFG files. For information sets, we ensure that nodes indistinguishable to a given player are grouped within the same information set. Finally, for payoffs, if specific values are provided in the game description, we check that the generated payoffs at the appropriate terminal nodes (outcomes) match these values. If the game description only implies relative payoff strengths, we verify that the payoffs are consistent with the implied constraints.\nWe use pass@k (the solve rate given k \"samples\", that is, independently generated EFG files), as proposed by Chen et al. [6], to measure the success of our translations for a specific target game description. Specifically, we execute GameInterpreter k = 5 times in our experiments, and we record the total number of generated EFGs, s\u2208 [1,..., 5], that are consistent with the target game description. As described in more detail in Section 6, in our experimental analysis, we distinguish between the case where at least one sample was correct (i.e., s \u2265 1), so we \"pass@5\", and the case where all samples were correct (i.e., s = 5), referred to as \"pass all 5\", which is stricter than pass@5. While improvements under pass all 5 are more desirable, we also evaluate pass@5 to highlight any relative merits of different settings we investigate, particularly in cases where the strictness of pass all 5 might hide differences in performance."}, {"title": "5.4 Parameters", "content": "We use the OpenAI API to access various LLMs, in particular: gpt-4-0125-preview, gpt-4o, and gpt-3.5-turbo. All of these models have two key hyperparameters that relate to how the next tokens are chosen, namely the temperature and p threshold for top-p sampling; both take values in [0, 1]. In top-p sampling [18], also known as nucleus sampling, the threshold p is used to restrict sampling of the next token to only the smallest set of most-likely candidates whose cumulative probability exceeds p. We set p as 1 (i.e., we do not restrict the next tokens at all). We set the temperature of LLMs to 0, which minimizes the amount of randomness in the chosen tokens (a choice of 1 would maximize it); note that setting the temperature to 0 makes the output as deterministic as possible, but, even with a fixed prompt, the output of these LLMs still often varies in repeat trials with the temperature set to 0, which has been attributed to issues like multi-GPU inference with varying GPU clock times. When comparing our method to the work by Mensfelt et al. [34], we match their experimental settings and adjust the temperature to 1."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "Table 3 displays the performance of the baselines discussed in Section 5.2, across the 18 games of our custom dataset (see Table 2). A grey tick indicates that between 1 and 4 of the 5 generated samples were successfully solved, a red cross means none were solved, and a green tick indicates that all 5 samples were solved. Thus a grey tick indicates a pass@5 and a green tick indicates a pass all 5 (which is by definition also a pass@5).\nWe observed that for all LLMs, setting D, the full pipeline, outperformed its counterparts (the basic setting, and settings A, B, and C). Among the LLMs, GPT-4o achieved the highest performance, succeeding for pass@5 for all 18 games, while GPT-4 also performed well, succeeding on 15 of the games, and failing only Kuhn poker, Nim, and Extra Game Two, which we note are three of the largest games that we considered.\nIn the basic setting, as well as in settings A and B, we found that imperfect information games, like \"A Three-Player Game\", are challenging to solve. This motivates our approach using a first stage for imperfect information retrieval. Their poor performance shows that the LLMs had not effectively memorized the solutions for our custom dataset.\nAcross all settings, GPT-3.5 underperforms compared to GPT-4 and GPT-4o, and it does not benefit from the addition of imperfect"}, {"title": "6.1 Overview"}, {"title": "6.2 Performance of Self-Debugging", "content": "In Table 4, we present the pass@5 and stricter pass all 5 metrics (extracted from Table 3), after incorporating the self-debugging module for each LLM. From Figure 4, we observed that under pass@5, both GPT-3.5 and GPT-4o show improvements, while GPT-4 remains unchanged. Under pass all 5, both GPT-4 and GPT-4o improve, whereas GPT-3.5 fails in all games with or without self-debugging, with the performance constrained by the limitations of the LLM itself. These findings show that the self-debugging module contributes to an overall enhancement in the framework's performance."}, {"title": "6.3 Performance of Imperfect Info Retrieval", "content": "In Table 5, we examine the impact of the imperfect information retrieval module by comparing setting B with the full pipeline, under the pass@5 and stricter pass all 5 metrics (extracted from Table 3). We distinguish between imperfect information games and perfect information games to analyze the module's effect on each category. Table 5 shows that the imperfect information retrieval module significantly enhances the performance of GPT-4 and GPT-4o, increasing the number of imperfect information games passed under pass@5. Notably, GPT-4o benefits the most from the module. In contrast, for GPT-3.5, no performance improvement was observed,"}, {"title": "6.4 Experiments with Mensfelt et al. [34] Games", "content": "We further evaluate the performance of our framework using the 112 game descriptions from Mensfelt et al. [34], as discussed in Section 5.1. For a comparison with their experimental results, we adopt their approach to regeneration attempts: rather than always performing a fixed number of attempts, they stop on the first successful attempt, or give up after 5 failed attempts.\nMensfelt et al. [34] used a logic programming approach. Across the five underlying bimatrix games in their dataset, they achieved 100% accuracy for the Hawk-Dove and Stag Hunt games, but encountered errors for descriptions of the Prisoner's Dilemma, Battle of the Sexes, and Matching Pennies games. With the same setup, our approach correctly translated all 112 of their game descriptions. For 102 of these 112 cases, the first correct translation was on the first attempt, the remaining 10 were correctly translated on the second attempt. This demonstrates the robustness of our method to variations in game"}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "We introduced a two-stage framework for translating game descriptions in natural language into extensive-form representations in game theory, leveraging LLMs, in-context learning, and code generation using the pygambit library. In the first stage, we address imperfect information by designing a module that identifies information sets and the corresponding partial game tree structure. In the second stage, the output from stage one is used, along with a self-debugging module, to generate a complete EFG using pygambit. We evaluate the framework's overall performance, as well as its individual components, across three LLMs on 18+112=130 game descriptions spanning 21 different games7. Our experimental results show that the framework significantly outperforms baseline models in generating accurate EFGs, with each module playing a critical role in its success.\nOne potential direction for future work is to move beyond manual consistency checks and develop a robust, automated check. Specifically, automated validation may require a \"suite\" of checks covering various aspects of consistency. For instance, a-rank [37] could be applied to compare rankings of strategies between target and generated EFGs; more generally different types of strategic equivalence could used to design checks. Other checks could involve game feature extraction, such as identifying the number of players, possible"}, {"title": "A GAME DESCRIPTIONS", "content": "This section presents the descriptions for the nineteen games that are utilized in our evaluation."}, {"title": "A.1 A Three-Player Game [33]"}, {"title": "A.2 An Imperfect Recall Game [33]", "content": "Consider a three-stage game. In the first stage, a chance event randomly selects either \u201cL\u201d or \u201cR\u201d, each with a probability of 1/2. In the second stage, Player 1 observes this outcome and then selects either l or r. In the third stage, Player 1 must choose between \u201cA\u201d, and \"B\", but at this point, she has forgotten the outcome of the first stage and only remembers her decision from the second stage. The payoffs in all outcomes are set to 0."}, {"title": "A.3 Absent-Minded Driver [3]"}, {"title": "A.4 Bach or Stravinsky [39]", "content": "The \"Bach or Stravinsky?\" game involves two players, Alexis and Beverley, who want to attend a concert together. Alexis prefers Bach, while Beverley prefers Stravinsky. Both players make their choice between Bach and Stravinsky simultaneously and independently. If they both choose Bach, Alexis, the Bach enthusiast, receives a higher payoff, while Beverley receives a lower payoff. Conversely, if they both choose Stravinsky, Beverley, who favors Stravinsky, receives a higher payoff, and Alexis receives a lower payoff. If they choose different concerts, neither player receives any payoff."}, {"title": "A.5 Bagwell [4]"}, {"title": "A.6 Centipede [41]", "content": "Consider a game with two players, Alice and Bob, where Alice makes the first move. At the start, Alice has two piles of coins in front of her: one pile with 4 coins and another with 1 coin. Each player has two options on their turn: they can either take the larger pile, giving the smaller pile to the other player, or they can push both piles to the other player. Whenever the piles are pushed across the table, the number of coins in each pile doubles. For instance, if Alice chooses to push on her first turn, the piles of 1 and 4 coins are handed over to Bob, increasing to 2 and 8 coins. Bob can then decide either to take the pile of 8 coins and leave 2 for Alice or to push the piles back to Alice, further doubling them to 4 and 16 coins. If neither player takes the coins by the end of the game, Alice will receive the pile with the higher value, and Bob will get the one with the lower value. The game has four moves in total: Alice moves (take or push), Bob moves (take or push, where the final push also doubles the piles before the game ends), Alice moves again, and finally, Bob moves. All actions are visible to both players."}, {"title": "A.7 Colonial Control [16]"}, {"title": "A.8 Kuhn Poker [21]", "content": "This is a two-player card game between Alice and Bob, using a deck of only three cards: a King, Queen, and Jack. Each player contributes 1 to the pot at the start. Each player is dealt one of the three cards, and the third is put aside unseen by a chance node. There are six possible allocations of the cards, each with probability 1/6: JQ, JK, QJ, QK, KJ, and KQ. The game proceeds as follows for each of the six possible allocations:\nAlice can check or bet 1, without knowing the allocation of the chance node.\nIf Alice checks then Bob can check or bet 1, without knowing Alice's card."}, {"title": "A.9 Extra Game One", "content": "In the first stage, Player 1 can choose among three actions: A, B, or C. In the second stage, Player 2 knows when action A was taken, but otherwise cannot tell whether B or C was taken. At this point, Player 2 has three options: D, E, and F. Selecting F ends the game. If Player 2 chooses either D or E, the game advances to the third stage, where Player 1 has a choice between actions G and H. Selecting H ends the game and player 1 gets payoff 2 and player 2 gets payoff -1. If Player 1 instead chooses G, Player 2 then decides between actions Q and W, with both players getting 3 after Q, and both players getting 0 after W."}, {"title": "A.10 Extra Game Two", "content": "In the first stage, Player 1 has three options: A, B, and C. In the second stage, Player 2, without knowing Player 1's choice, can select either D or E. In the third stage, Player 1 has two actions to choose from: F or G. Finally, in the fourth stage, Player 3 can decide between actions Q and W. If Player 3 selects Q in the final stage, all players receive a payoff of 3. However, if Player 3 opts for W, Player 1 and Player 2 each receive a payoff of 2, while Player 3 gets 3."}, {"title": "A.11 Extra Game Three", "content": "A chance node will select one of four possible outcomes: A, B, C, or D. After observing the result of the chance node, Player 1 will choose from three available actions: E, F, or G. In the following stage, Player 2, having observed Player 1's choice, will choose between two actions: Q or W. Payoffs are as follows: under A, Q gives (1, -1) and W gives (2, -2); under B, Q gives (3, -3) and W gives (-3, 3); under C, Q gives (0, 0) and W gives (-1, 1); and under D, Q gives (4, -4) and W gives (-4, 4)."}, {"title": "A.12 Market Entry Model [25]", "content": "Firm 1 is a monopolist already established in the market. Firm 2, a potential competitor, can choose to enter the market or stay out (if it stays out, the game ends). If Firm 2 enters, Firm 1 must decide whether to compete aggressively (Fight) or allow some market share to Firm 2 (Accommodate). This game only lasts for one round. The payoffs are structured as follows: Firm 1 earns more if Firm 2 decides not to enter. If Firm 2 enters and Firm 1 accommodates, both firms"}, {"title": "A.13 Market Signalling Game [49]", "content": "A new manufacturer plans to enter the market, and its strength, determined by chance (not as a player decision), can be either strong (S) with probability 2/3 or weak (W) with probability 1/3. The new manufacturer will then send a signal, either strong (S) or weak (W). The current manufacturer does not know the new manufacturer's actual strength but observes the signal, which could indicate either strong (S) or weak (W). Based on this signal, the current manufacturer must decide to either compete aggressively (F) or accommodate the new competitor (A). The payoffs for each scenario are as follows:\nIf the new manufacturer is strong and sends a strong signal, and the existing manufacturer chooses to fight, the payoffs are (1, 0) for the new and existing manufacturers, respectively. If the existing manufacturer adapts, the payoffs are (3, 1).\nIf the new manufacturer is strong and sends a weak signal, and the existing manufacturer chooses to fight, the payoff is (0, 0). If the existing manufacturer adapts, the payoff is (2, 1).\nIf the new manufacturer is weak and sends a strong signal, and the existing manufacturer chooses to fight, the payoff is (0, 2). If the existing manufacturer adapts, the payoff is (2, 1).\nIf the new manufacturer is weak and sends a weak signal, and the existing manufacturer chooses to fight, the payoff is (1, 2). If the existing manufacturer adapts, the payoff is (3, 1)."}, {"title": "A.14 Nim (with five in one pile) [43]", "content": "The game involves a single pile of five stones, with two players, Alice and Bob, taking turns. On each turn, a player can remove either one or two stones (but only one stone if only one remains). The goal is to avoid taking the last stone. Here's how the moves unfold:\nAlice starts and can choose to remove one or two stones.\nIf Alice removes one stone, Bob can also remove one or two stones.\nIf Bob removes one stone, Alice can again choose to remove one or two stones.\nIf Alice removes one stone, Bob has the choice to remove one or two stones.\nIf Bob removes one stone, Alice can remove one stone, which would result in Alice losing the game.\nIf Bob removes two stones, Bob loses the game.\nIf Alice removes two stones, Bob can remove one stone, leading to Bob's loss.\nIf Bob removes two stones, Alice can remove one or two stones.\nIf Alice removes one stone, Bob can remove one stone, leading to Bob's loss.\nIf Alice removes two stones, Alice loses the game.\nIf Alice removes two stones, Bob has the option to remove one or two stones.\nIf Bob removes one stone, Alice can remove one or two stones.\nIf Alice removes one stone, Bob can remove one stone, leading to Bob's loss.\nIf Alice removes two stones, Alice loses the game.\nIf Bob removes two stones, Alice has only one stone left and loses."}, {"title": "A.15 Nuclear Crisis [31]", "content": "After Country B provokes Country A, a sequence of decisions unfolds. Country A must first decide to either escalate (E) the situation or ignore it (I) and end the game. If Country A ignores the provocation, both countries receive a small positive payoff. If Country A escalates (E), Country B then faces the choice to either back down (B) with a small payoff loss while Country A gains a small payoff or to escalate further (E). If both Country A and Country B choose to escalate (E), this brings about a nuclear crisis. Country A then chooses first between retreating (R) and losing credibility or detonating (D). Country B, unaware of Country A's decision, then decides to retreat (R) and lose credibility or detonate (D). If one country detonates while the other retreats, the detonating country gains a large positive payoff, while the retreating country incurs a large negative payoff. However, if neither country retreats, retaliation follows the first nuclear strike, resulting in a catastrophic nuclear disaster, yielding a very large negative payoff for both countries. If both countries retreat, they each suffer a small negative payoff."}, {"title": "A.16 Rock, Paper, Scissors [48]"}, {"title": "A.17 Simple Bargaining Game [36]", "content": "A and B are negotiating how to split 10", "rules": "First, A proposes a plan where A receives 5,500 pounds, and B receives 4,500 pounds. If B accepts, the negotiation concludes, with A getting 5,500"}]}