{"title": "Contrastive Factor Analysis", "authors": ["Zhibin Duan", "Tiansheng Wen", "Yifei Wang", "Chen Zhu", "Bo Chen", "Mingyuan Zhou"], "abstract": "Factor analysis, often regarded as a Bayesian variant of matrix factorization, offers\nsuperior capabilities in capturing uncertainty, modeling complex dependencies, and\nensuring robustness. As the deep learning era arrives, factor analysis is receiving\nless and less attention due to their limited expressive ability. On the contrary,\ncontrastive learning has emerged as a potent technique with demonstrated efficacy\nin unsupervised representational learning. While the two methods are different\nparadigms, recent theoretical analysis has revealed the mathematical equivalence\nbetween contrastive learning and matrix factorization, providing a potential pos-\nsibility for factor analysis combined with contrastive learning. Motivated by the\ninterconnectedness of contrastive learning, matrix factorization, and factor analysis,\nthis paper introduces a novel Contrastive Factor Analysis framework, aiming to\nleverage factor analysis's advantageous properties within the realm of contrastive\nlearning. To further leverage the interpretability properties of non-negative factor\nanalysis, which can learn disentangled representations, contrastive factor analysis\nis extended to a non-negative version. Finally, extensive experimental validation\nshowcases the efficacy of the proposed contrastive (non-negative) factor analysis\nmethodology across multiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.", "sections": [{"title": "1 Introduction", "content": "Before the era of deep learning, factor analysis (FA) [Chen et al., 2013] had received widespread\nattention and achieved impressive results. Generally, FA is a statistical method used to identify\nunderlying factors or latent variables that explain patterns of correlations among observed variables.\nWith its stochastic latent variables, FA often enjoys good properties for modeling complex dependence\nand enhancing robustness [Zhou et al., 2016]. Meanwhile, its non-negative extensions, such as Poisson\nfactor analysis [Zhou et al., 2012], can not only learn meaningful latent representation from high-\ndimension observation unsupervised but also can discover interpretable semantic concepts [Blei et al.,\n2003, Lee and Seung, 1999]. With these appealing properties, FA and non-negative FA have been\nwidely used in many applications, including text analysis [Zhou et al., 2012] image representation\nlearning [Chen et al., 2013].\nWith the deep learning era arriving [LeCun et al., 2015], FA is receiving less and less attention due\nto its limited expressiveness. Correspondingly, recent years have witnessed a surge of interest in\nself-supervised learning, such as contrastive learning (CL) [Oord et al., 2018, Chen et al., 2020],\nowing to its remarkable capacity to learn high-quality representations unsupervised, leading to notable\nadvancements in various fields [You et al., 2020, Gao et al., 2021]. Generally, CL methodologies\naim to extract discriminative features by training models to distinguish positive and negative samples.\nThe distinction between FA and CL lies primarily in the latter's ability to harness effective training\nobjectives [Ye et al., 2019] and expressive deep neural networks [He et al., 2015].\nGiven that FA and CL are regarded as two distinct research fields, integrating CL to enhance FA or vice\nversa might appear challenging. Fortunately, recent advancements in the theoretical underpinnings\nhave provided insights into this integration. Specifically, HaoChen et al. [2021] leverages spectral\ncontrastive loss to derive the mathematical equivalent of CL and matrix factorization (MF) [Eckart\nand Young, 1936]. Considering the relationship between MF and FA [Nakajima and Sugiyama,\n2011], the integration of FA with CL becomes feasible. Beyond feasibility, merging FA and CL\noffers numerous attractive potential benefits. In general, from the perspective of FA, enhancing\nFA with CL can alleviate its limitation in expressiveness, leading to richer and more informative\nrepresentations [Bengio et al., 2014]. From the perspective of CL, considering FA's capacity to\ncapture complex dependencies and its stochastic latent variables, incorporating CL with FA not only\nenhances performance but also facilitates improved uncertainty estimation and interpretability [Fan\net al., 2020, Higgins et al., 2017].\nDrawing inspiration from the mathematical equivalence established between MF, CL, and FA, as\nillustrated in Figure. 1, in this paper, we try to develop a Contrastive Factor Analysis (CFA) framework.\nUnlike FA, which directly models observation data, CFA follows CL methods to model the relation\nmatrix, which represents the relationship between positive and negative samples. More specifically,\nin CFA, the relation matrix is factorized into Gaussian latent variables. Considering the target matrix\nis non-negative, the CL can also be modified as a contrastive non-negative FA, which factorizes the\nrelation matrix into gamma latent variables. Compared with CFA, its non-negative extension can\nenjoy good properties for learning disentangled representation [Bengio et al., 2014, Wang et al., 2024].\nTo approximate the posterior distribution of the latent variables, we further develop a variational\ninference network. While it's straightforward to employ the Gaussian distribution to appropriate\nthe posterior of Gaussian latent variables, considering the non-reparameterizability of the gamma\ndistribution, we instead utilize the Weibull distribution to approximate the posterior of gamma latent\nvariables Zhang et al. [2018]. Finally, both models' parameters can be effectively trained within a\nvariational inference framework [Kingma and Welling, 2013] with reparameterize technique.\nTo assess the properties of contrastive factor analysis, we conducted a series of experiments targeting\nvarious aspects. These properties encompass the attainment of more expressive representations,\nrobustness against out-of-distribution data, interpretability, and uncertainty evaluation. The main\ncontributions of this paper can be summarized as follows:\n\u2022 Drawing inspiration from the interplay between contrastive learning, matrix factorization,\nand factor analysis, we introduce a novel contrastive (non-negative) factor analysis frame-\nwork.\n\u2022 To approximate the posterior distribution of latent variables, we propose two variational\ninference networks: Gaussian and Weibull variational inference network."}, {"title": "2 Preliminary on contrastive learning", "content": "In this section, we briefly review the standard contrastive learning modules and the relationship\nbetween contrastive learning and matrix factorization.\nContrastive learning. In representation learning, the aim is to learn an encoder function $f :$\n$\\mathbb{R}^D \\rightarrow \\mathbb{R}^d$ that maps high-dimensional input data $x \\in \\mathbb{R}^D$ to low-dimensional representations\n$z \\in \\mathbb{R}^d$. To perform contrastive learning, we start by building pairs of positive samples $(x,x^+)$\nby augmenting the same natural sample $x \\sim P(x)$, where $A (\\cdot | \\cdot)$ represents the augmentation\ndistribution. Additionally, negative samples are independently augmented samples, following the\nmarginal distribution $P (x) = E_{x'}A (x | x').$\nThe objective of contrastive learning is to encourage the model to map positive samples closer together\nin the feature space while pushing negative samples further apart. A well-known CL objective is the\nInfoNCE loss [Oord et al., 2018].\n$\\mathcal{L}_{NCE}(f) = -E_{x,x^+,\\{x^i\\}_{i=1}^M} log \\frac{exp (f(x)^T f (x^+))}{exp (f(x)^T f (x^+)) + \\sum_{i=1}^M exp (f(x)^T f(x^i))}$\n(1)\nwhere $\\{x^i\\}_{i=1}^M$ are $M$ negative samples independently drawn from $P(x)$. HaoChen et al. [2021]\nproposed the spectral contrastive loss that is more amenable for the theoretical analysis:\n$\\mathcal{L}_{sp}(f) = -2E_{x,x^+}f(x)^T f (x^+) + E_{x,x^+} (f(x)^T f (x^-))^2$\n(2)\nContrastive learning as matrix factorization. Notably, HaoChen et al. [2021] show that the\nspectral contrastive loss can be equivalently written as the following matrix factorization objective:\n$\\mathcal{L}_{MF} = ||\\bar{A} - FF^T||_F^2$, where $F_{x,:}=\\sqrt{P(x)}f(x)$\n(3)\nHere, $\\bar{A} = D^{-1/2}AD^{-1/2}$ denotes the normalized version of the co-occurrence matrix\n$A\\in \\mathbb{R}^{N\\times N}$ of all augmented samples $x \\in \\mathcal{X}$ (assuming $|\\mathcal{X}|\\ll N$ for ease of exposition):\n$\\forall x, x' \\in \\mathcal{X}, A_{x,x'} := P(x, x') = E_{x}[A(x|x)A(x'|x)]$."}, {"title": "3 Contrastive (non-negative) factor analysis", "content": "This section furnishes a comprehensive overview of the proposed contrastive (non-negative) factor\nanalysis, comprising the generative model (Sec.3.1) and the variational inference network (Sec.3.2).\nIt further elaborates on variational inference (Sec.3.3) and discusses techniques for uncertainty\nevaluation (Sec. 3.4)."}, {"title": "3.1 Bayesian generative model", "content": "In classical FA, an observation matrix $X$ is factorized into two latent variables $X \\sim Dis(\\Theta\\Phi)$, where\nboth $\\Theta$ and $\\Phi$ are drawn from distributions such as Gaussian. While FA is intuitive for feature learning,\nit may not be effective for learning meaningful representations with its reconstruction training objects\n[Tschannen et al., 2018]. Motivated by the connection between CL and MF, as indicated by Eq. 3,\nwe build contrastive factor analysis (CFA) by instead modeling the co-occurrence matrix $A$, which\nrepresents the relationship among positive and negative samples.\nUnlike classical FA, which focuses solely on modeling the generative process between the latent\nvariable $\\Theta$ and target data $X$, CFA requires modeling the generative process among observation data\nsamples $X$, the latent variable $\\Theta$, and the corresponding normalized co-occurrence matrix $\\bar{A}$, as\ndepicted in Figure. 2(b). To address this, we draw inspiration from conditional VAE [Sohn et al.,\n2015] to devise a modified FA. Specifically, given data samples and augmentations $X \\in \\mathbb{R}^{N\\times D}$\nand the corresponding normalized co-occurrence matrix $\\bar{A} \\in \\mathbb{R}^{N\\times N}_+$, CFA factorize $\\bar{A} \\in \\mathbb{R}^{N\\times N}_+$\nunder the Gaussian likelihood as follows:\n$A_{i,j} | X_i, X_j \\sim \\mathcal{N} (\\Theta_i \\Theta_j), \\Theta_i | x_i \\sim \\mathcal{N} (f(x_i), 1), \\Theta_j|x_j \\sim \\mathcal{N} (f(x_j), 1)$.\n(4)\nwhere $\\Theta_i, \\Theta_j \\in \\mathbb{R}$ is the factor score matrix, each column of which encodes the relative importance\nof each atom in a sample.\nFurther, recent studies suggest that modeling latent features using non-negative vectors can harness the\nadvantageous properties of non-negative matrix factorization (MF), which facilitates the acquisition\nof disentangled representations [Lee and Seung, 1999, Bengio et al., 2014, Wang et al., 2024]. Hence,\nwe draw inspiration from relevant research [Zhou et al., 2012] to develop a contrastive non-negative\nFA (CNFA) approach. Specifically, the CNFA factorizes $A$ into sparse and non-negative gamma\nl atent variables, which can be described as follows:\n$A_{i,j} | X_i, X_j \\sim \\mathcal{N} (\\Theta_i\\Theta_j), \\Theta_i | x_i \\sim Gamma (f(x_i), 1), \\Theta_j|x_j \\sim Gamma (f(x_j), 1) $.\n(5)\nIn our formulation, the ideal scenario involves modulating the prior of the latent variables $\\Theta_i$ by\nthe input $x_j$. However, we can readily relax this constraint to ensure that the latent variables are\nstatistically independent of the input variables [Sohn et al., 2015]. Thus, for contrastive factor analysis\n(CFA), we simply define the data-independent prior as $f (x_i) = 0$, while for contrastive non-negative\nfactor analysis (CNFA), we set $f(x_i) = 1$."}, {"title": "3.2 Reparameterizable latent variable distributions", "content": "Variational Gaussian posterior for CFA. The Gaussian distribution $\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$ has probability\ndensity function $f (\\theta/\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\sigma^2 \\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$, where $\\theta \\in \\mathbb{R}$. It is reparmeterizable as drawing\n$\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$ us equivalent to letting $\\theta = g(\\epsilon) := \\mu + \\epsilon\\sigma, \\epsilon \\sim \\mathcal{N}(0,1)$. The KL divergence\nis analysis as $KL (\\mathcal{N} (\\mu_1, \\sigma_1^2) || \\mathcal{N} (\\mu_2, \\sigma_2)) = log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$. As shown in Fig. 2(c), the\nvariational inference network utilize the obtained latent features $h_i$ from a deep feature extractor,\nsuch as ResNet[He et al., 2015], to construct the variational posterior:\n$q (\\Theta_i | x_i) = \\mathcal{N} (\\mu_i, \\sigma_i^2), \\mu_i = f_{\\mu}(h_i), \\sigma_i^2 = Softplus (f_{\\sigma}(h_i)),$\n(6)\nwhere $f_{(.)}$ denotes the neural network, and Softplus applies $log(1 + exp(-.))$ non-linearity to each\nelement to ensure positive Gaussian variance parameters. It's evident that the CFA will reduce to\nmatrix factorization as $\\sigma$ of the Gaussian distribution goes to zero. Therefore, the proposed CFA can\nbe viewed as a generalization of vanilla contrastive learning [HaoChen et al., 2021].\nVariational Weibull posterior for CNFA: While the gamma distribution is ideal for the pos-\nterior due to its encouragement of sparsity and satisfaction of non-negativity constraints, its non-\nreparameterizability and incompatibility with gradient descent optimization lead to the adoption of\nthe Weibull distribution as an approximation for the posterior of gamma latent variables.\nThe reason for choosing the Weibull distribution is threefold [Zhang et al., 2018]: i) the Weibull\ndistribution is similar to a gamma distribution, capable of modeling sparse, skewed and positive\ndistributions; ii) the Weibull distribution has a simple reparameterization so that it is easier to"}, {"title": "3.3 Variational inference", "content": "For the contrastive factor analysis, given the model parameters referred to as $\\mathcal{W}$, which consist of the\nparameters in the generative model and inference network, marginal likelihood is defined as:\n$p (A|\\mathcal{W}) = \\int \\prod_{i=1}^{N}\\prod_{j=1}^{N} P (A_{i,j} | \\Theta_i, \\Theta_j) \\prod_{j=1}^{N}p (\\Theta_j | x_j) d\\Theta_{1:N}$.\n(8)\nThe inference task is to learn the parameters of the generative model and the inference network. Simi-\nlar to VAEs, the optimization objective of contrastive factor analysis can be achieved by maximizing\nthe evidence lower bound (ELBO) of log-likelihood as\n$\\mathcal{L}(A) =  \\sum_{i=1}^{N}\\sum_{j=1}^{N} E_{q(\\Theta_i | x_i)} [lnp (A_{i,j} | \\Theta_i, \\Theta_j)] - \\sum_{j=1}^{N} E_{q(\\Theta_i | x_i)} [ln \\frac{q(\\Theta_i | x_i)}{p (\\Theta_i)}]$.\n(9)\nwhere the first term is the expected log-likelihood of the generative model, which ensures recon-\nstruction performance, and the second term is the Kullback-Leibler (KL) divergence that constrains\nthe variational distribution $q(\\Theta_i)$ to be close to its prior $p(\\Theta_j)$. Unfortunately, Equation. 9 becomes\nintractable with factor analysis algorithms as the input space $\\mathcal{X}$ grows exponentially large with\n$N\\rightarrow\\infty$. To address this challenge, we reformulate the factor analysis problem (Equation. 9)\nequivalently as a tractable contrastive learning (CL) objective that only necessitates sampling positive\nand negative samples from the joint and marginal distributions, respectively:\n$\\mathcal{L} =  - 2E_{x, x^+} [\\Theta^T \\Theta^+] + E_{x, x^+} [(\\Theta^T \\Theta^-)^2]  -E_{q(\\Theta^+ | x^+)} [ln \\frac{q (\\Theta^+ | x^+)}{p (\\Theta^+)}] - E_{q(\\Theta^- | x^-)} [ln \\frac{q (\\Theta^- | x^-)}{p (\\Theta^-)}].$\n(10)\nThe parameters in contrastive (non-negative) factor analysis can be directly optimized by advanced\ngradient algorithms, like Adam [Kingma and Ba, 2017].\nComplexity analysis Our framework is computationally and memory-efficient because only a few\nparameters need to be added to infer the distribution parameters. The extra memory cost comes from\nthe posterior inference network, which only adds $d \\times 1$ parameters due to $\\sigma \\in \\mathbb{R}^1$ and $k_i \\in \\mathbb{R}^1$,\nwhere $d$ is the dimension of feature $h_i$. Meanwhile, the additional computations involve the sampling\nprocess and computing the KL term, which is of scale $O(d)$. Both of above is inconsiderable\ncompared to the memory and computational scale of the deep backbone neural network [He et al.,\n2015]."}, {"title": "3.4 Uncertainty-aware representation", "content": "To quantify the inferred latent variable uncertainty, we adopt Shannon's entropy [Shannon, 1948] of\nthe variational posterior. Specifically, The Shannon's entropy of Gaussian variational posterior with\nparameters $(\\mu, \\sigma)$ is defined as:\n$H (q_{\\phi} (\\Theta_i |x_i)) =  \\sum_{m=1}^{d}  ln (\\sigma_m \\sqrt{2\\pi e})$\n(11)\n. And Shannon's entropy of the Weibull variational posterior with parameters $(\\lambda, k)$ is defined as:\n$H (q_{\\phi} (\\Theta_i |x_i)) =  \\sum_{m=1}^{d}  \\frac{(k_m - 1) \\gamma}{k_m} -  \\frac{\\lambda_m}{k_m} + ln  \\frac{\\lambda_m}{k_m} + 1$\n(12)\nwhere $\\gamma$ stands for Euler's constant. It should be noted that the higher the entropy, the higher the\nuncertainty."}, {"title": "4 Related work", "content": "Our work is mainly related to factor analysis and probabilistic representation learning. Here, we\nintroduce the related research.\nFactor analysis. Before the era of deep learning, factor analysis was widely used in representation\nlearning. And its non-negative extension [Blei et al., 2003] has achieved great progress in text\nanalysis. Further, there is increasing interest in hierarchical factor analysis, which is aimed at\nexploring hierarchical data representation. But all this work is different from ours, which mainly\nfocuses on improving factor analysis with contrastive learning. Besides, [Tosh et al., 2021] proves\ncontrastive learning is capable of revealing latent variable posterior information in FA. Nonetheless,\nour approach diverges from this earlier work as it places more emphasis on proposing a novel model\nthat can effectively learn representations.\nProbabilistic representation learning. Probabilistic representation learning has indeed made\nsignificant strides in recent years and has garnered considerable attention in the machine learning\ncommunity. Its ability to capture uncertainty and model complex data distributions has proven\nimmensely useful in various applications such as natural language processing [Zhou et al., 2016,\nVilnis and McCallum, 2014], computer vision [Shi and Jain, 2019, Oh et al., 2018, Chun et al.,\n2021, Wang et al., 2023], and graph processing [Elinas et al., 2020, Bojchevski and G\u00fcnnemann,\n2017]. In contrast to the previous efforts, this paper develops a general framework for (non-negative)\ncontrastive factor analysis that benefits from both (non-negative) factor analysis and contrastive\nlearning. Besides, there are efforts to improve contrastive learning with the Bayesian method, which\nis aimed at evaluating the uncertainty [Nakamura et al., 2023, Park et al., 2022, Kirchhof et al.,\n2023]. Unfortunately, most of these techniques rely on a decoder, which frequently degrades the\nperformance of downstream tasks [Aitchison and Ganev, 2021]. On the opposite hand, our research\nis an encoder-only approach that can enhance downstream tasks' performance."}, {"title": "5 Experiment", "content": "5.1 Feature learning\nBaselines & Datasets. To evaluate the expressiveness of the proposed model, we compare it\nwith strong baselines, including SimCLR(CL) [Chen et al., 2020] and non-negative contrastive\nlearning(NCL) [Wang et al., 2024]. More specifically, we compare CFA with CL and CNF with\nNCL. It should be noted that CFA is built on SimCLR codebase 3 with a minor modification to the\nhidden projector output. We evaluate linear probing and fine-tuning performance on three benchmark\ndatasets: CIFAR-10, CIFAR-100[Krizhevsky et al., 2009], and Imagenet-100[Deng et al., 2009].\nApart from in-distribution evaluation, we also compare models on three out-of-distribution datasets,\nincluding stylized ImageNet[Geirhos et al., 2018], ImageNet-Sketch[Wang et al., 2019], ImageNet-\nC[Hendrycks and Dietterich, 2019](restricted to ImageNet-100 classes). For all experiments, we\nrun 3 random trials and report their mean and standard deviation. All experiments are performed on\nworkstation equipped with a CPU i7-10700 and accelerated by four GPU NVIDIA RTX 4090 with\n24GB VRAM."}, {"title": "5.2 Uncertainty evaluation", "content": "In addition to their enhanced expressiveness and robustness on out-of-distribution (OOD) data,\nanother notable strength of CFA and CNFA is their ability to capture data uncertainty.\nEvaluation Metric. With the entropy of the testing result and a given entropy threshold, we can\ndetermine whether the model is certain or uncertain about one prediction. To evaluate the uncertainty\nestimates, we use Patch Accuracy vs Patch Uncertainty (PAvPU) [Mukhoti and Gal, 2019], which is\ndefined as $PAvPU = (n_{ac} + n_{iu})/(N_{ac} + N_{au} + n_{ic} + n_{iu})$, where $N_{ac}, N_{au}, N_{ic}, n_{iu}$ represent the\nnumbers of accurate and certain, accurate and uncertain, inaccurate and certain, and inaccurate and\nuncertain samples, respectively. Compared to Top-1 $Accuracy= (n_{ac}+n_{au})/(n_{ac}+N_{au}+N_{ic}+n_{iu})$,\nPAVPU metric would be higher if the model tends to generate accurate predictions with low uncertainty\nand inaccurate predictions with high uncertainty leading to a more trustworthy algorithm."}, {"title": "5.3 Disentanglement representation learning", "content": "Learning a robust representation involves extracting explanatory factors that are sparse, disentangled,\nand semantically meaningful [Bengio et al., 2014]. By incorporating non-negative constraints, NCL\ncan learn disentangled features [Wang et al., 2024]. On the other hand, the Gamma distribution\npromotes sparsity due to its probability density function, which helps filter out irrelevant features\nand focus on the most significant ones. Thus, we evaluate CFA's performance in disentangled\nrepresentation learning.\nEvaluation Metric. Many disentanglement metrics like MIG [Chen et al., 2019] are supervised\nand require ground-truth factors which are often unavailable in practice. Consequently, current\ndisentanglement evaluation is often limited to synthetic data[Carbonneau et al., 2022]. To val-\nidate the disentanglement on real-world data, we adopt an unsupervised disentanglement met-\nric SEPIN@k[Do and Tran, 2021]. SEPIN@k measures how each feature $f_i(x)$ is disentangled\nfrom others $f_{\\neq i}(x)$ by computing their conditional mutual information with the top k features, i.e.,\n$SEPIN@k=\\sum_{i=1}^{k} I(x, f_{r_i}(x)|f_{\\neq r_i}(x))$, which are estimated with InfoNCE lower bound [Oord\net al., 2018] implemented following [Wang et al., 2024].\nExperiment Result. Table 4 demonstrates that CNFA features show better disentanglement than\nCL and NCL in all top-k dimensions. The advantage is larger by considering the top features, since\nlearned features may contain noisy dimensions [Wang et al., 2024, Yu and Liu, 2003]. Incorporating\nproper prior distribution (e.g., Gamma distribution) will effectively mitigate this problem. Thanks\nto the inherent properties of the Gamma distribution, CNFA tends to produce disentangled features,\nmaking it more effective in distinguishing the underlying factors of variation within the data. In\nconclusion, CNFA achieves better feature disentanglement on real-world data and exhibits exceptional\nperformance in estimating uncertainty."}, {"title": "6 Conclusion", "content": "In conclusion, this paper introduces a novel framework called Contrastive Factor Analysis (CFA),\nwhich bridges the gap between traditional Factor Analysis (FA) and the powerful technique of\ncontrastive learning. By leveraging the mathematical equivalence between contrastive learning and\nmatrix factorization, CFA capitalizes on the strengths of both methods, offering enhanced expressive-ness, robustness, and interpretability. Further, by extending CFA to a non-negative version (CNFA),\nwe exploit the interpretability properties of non-negative factor analysis, enabling the learning of\ndisentangled representations. Through extensive experimental validation, our methodology demon-\nstrates efficacy across various dimensions, including expressiveness, robustness, interpretability, and\naccurate uncertainty estimation. In essence, CFA and CNFA represent promising approaches that\namalgamate the insights from factor analysis and contrastive learning, paving the way for more\neffective unsupervised representational learning in the deep learning era."}]}