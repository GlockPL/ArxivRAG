{"title": "Contrastive Factor Analysis", "authors": ["Zhibin Duan", "Tiansheng Wen", "Yifei Wang", "Chen Zhu", "Bo Chen", "Mingyuan Zhou"], "abstract": "Factor analysis, often regarded as a Bayesian variant of matrix factorization, offers\nsuperior capabilities in capturing uncertainty, modeling complex dependencies, and\nensuring robustness. As the deep learning era arrives, factor analysis is receiving\nless and less attention due to their limited expressive ability. On the contrary,\ncontrastive learning has emerged as a potent technique with demonstrated efficacy\nin unsupervised representational learning. While the two methods are different\nparadigms, recent theoretical analysis has revealed the mathematical equivalence\nbetween contrastive learning and matrix factorization, providing a potential pos-\nsibility for factor analysis combined with contrastive learning. Motivated by the\ninterconnectedness of contrastive learning, matrix factorization, and factor analysis,\nthis paper introduces a novel Contrastive Factor Analysis framework, aiming to\nleverage factor analysis's advantageous properties within the realm of contrastive\nlearning. To further leverage the interpretability properties of non-negative factor\nanalysis, which can learn disentangled representations, contrastive factor analysis\nis extended to a non-negative version. Finally, extensive experimental validation\nshowcases the efficacy of the proposed contrastive (non-negative) factor analysis\nmethodology across multiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.", "sections": [{"title": "1 Introduction", "content": "Before the era of deep learning, factor analysis (FA) [Chen et al., 2013] had received widespread\nattention and achieved impressive results. Generally, FA is a statistical method used to identify\nunderlying factors or latent variables that explain patterns of correlations among observed variables.\nWith its stochastic latent variables, FA often enjoys good properties for modeling complex dependence\nand enhancing robustness [Zhou et al., 2016]. Meanwhile, its non-negative extensions, such as Poisson\nfactor analysis [Zhou et al., 2012], can not only learn meaningful latent representation from high-\ndimension observation unsupervised but also can discover interpretable semantic concepts [Blei et al.,\n2003, Lee and Seung, 1999]. With these appealing properties, FA and non-negative FA have been"}, {"title": "2 Preliminary on contrastive learning", "content": "In this section, we briefly review the standard contrastive learning modules and the relationship\nbetween contrastive learning and matrix factorization.\nIn representation learning, the aim is to learn an encoder function $f :\\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{d}$ that maps high-dimensional input data $x \\in \\mathbb{R}^{D}$ to low-dimensional representations\n$z \\in \\mathbb{R}^{d}$. To perform contrastive learning, we start by building pairs of positive samples $(x,x+)$\nby augmenting the same natural sample $x \\sim P(x)$, where $A (\\cdot | \\cdot)$ represents the augmentation\ndistribution. Additionally, negative samples are independently augmented samples, following the\nmarginal distribution $P (x) = \\mathbb{E}_{x}A (x | x)$.\nThe objective of contrastive learning is to encourage the model to map positive samples closer together\nin the feature space while pushing negative samples further apart. A well-known CL objective is the\nInfoNCE loss [Oord et al., 2018].\n$\\mathcal{L}_{NCE}(f) = -\\mathbb{E}_{x,x^{+},\\{x_{i}\\}_{i=1}^{M}} \\log \\frac{\\exp \\left(f(x)^{T} f(x^{+})\\right)}{\\exp \\left(f(x)^{T} f(x^{+})\\right) + \\sum_{i=1}^{M} \\exp \\left(f(x)^{T} f(x_{i})\\right)}$\n(1)\nwhere $\\{x_{i}\\}_{i=1}^{M}$ are M negative samples independently drawn from $P(x)$. HaoChen et al. [2021]\nproposed the spectral contrastive loss that is more amenable for the theoretical analysis:\n$\\mathcal{L}_{S P}(f) = -2\\mathbb{E}_{x,x^{+}}f(x)^{T} f(x^{+}) + \\mathbb{E}_{x,x^{+}} \\left(f(x)^{T} f(x^{-})\\right)^{2}$\n(2)\nNotably, HaoChen et al. [2021] show that the\nspectral contrastive loss can be equivalently written as the following matrix factorization objective:\n$\\mathcal{L}_{M F}=\\left\\|\\bar{A}-F F^{T}\\right\\|_{F}^{2}, \\text { where } F_{x,:}=\\sqrt{P(x)} f(x)$\n(3)\nHere, $\\bar{A}=D^{-1 / 2} A D^{-1 / 2}$ denotes the normalized version of the co-occurrence matrix\n$A \\in \\mathbb{R}^{N \\times N}$ of all augmented samples $x \\in \\mathcal{X}$ (assuming $|\\mathcal{X}| \\leqslant N$ for ease of exposition):\n$\\forall x, x^{\\prime} \\in \\mathcal{X}, A_{x, x^{\\prime}}:=P\\left(x, x^{\\prime}\\right)=\\mathbb{E}_{z}\\left[A(x | z) A\\left(x^{\\prime} | z\\right)\\right]$."}, {"title": "3 Contrastive (non-negative) factor analysis", "content": "This section furnishes a comprehensive overview of the proposed contrastive (non-negative) factor\nanalysis, comprising the generative model (Sec.3.1) and the variational inference network (Sec.3.2).\nIt further elaborates on variational inference (Sec.3.3) and discusses techniques for uncertainty\nevaluation (Sec. 3.4)."}, {"title": "3.1 Bayesian generative model", "content": "In classical FA, an observation matrix $X$ is factorized into two latent variables $X \\sim Dis(\\Theta \\Phi)$, where\nboth $\\Theta$ and $\\Phi$ are drawn from distributions such as Gaussian. While FA is intuitive for feature learning,\nit may not be effective for learning meaningful representations with its reconstruction training objects\n[Tschannen et al., 2018]. Motivated by the connection between CL and MF, as indicated by Eq. 3,\nwe build contrastive factor analysis (CFA) by instead modeling the co-occurrence matrix $A$, which\nrepresents the relationship among positive and negative samples.\nUnlike classical FA, which focuses solely on modeling the generative process between the latent\nvariable $\\Theta$ and target data $X$, CFA requires modeling the generative process among observation data\nsamples $X$, the latent variable $\\theta$, and the corresponding normalized co-occurrence matrix $\\bar{A}$, as\ndepicted in Figure. 2(b). To address this, we draw inspiration from conditional VAE [Sohn et al.,\n2015] to devise a modified FA. Specifically, given data samples and augmentations $X \\in \\mathbb{R}^{N \\times D}$\nand the corresponding normalized co-occurrence matrix $\\bar{A} \\in \\mathbb{R}_{+}^{N \\times N}$, CFA factorize $\\bar{A} \\in \\mathbb{R}_{+}^{N \\times N}$\nunder the Gaussian likelihood as follows:\n$\\bar{A}_{i, j} | x_{i}, x_{j} \\sim \\mathcal{N} \\left(\\theta_{i} \\theta_{j}\\right), \\quad \\theta_{i} | x_{i} \\sim \\mathcal{N} \\left(f\\left(x_{i}\\right), 1\\right), \\quad \\theta_{j} | x_{j} \\sim \\mathcal{N} \\left(f\\left(x_{j}\\right), 1\\right)$.\n(4)\nwhere $\\theta_{i}, \\theta_{j} \\in \\mathbb{R}$ is the factor score matrix, each column of which encodes the relative importance\nof each atom in a sample.\nFurther, recent studies suggest that modeling latent features using non-negative vectors can harness the\nadvantageous properties of non-negative matrix factorization (MF), which facilitates the acquisition\nof disentangled representations [Lee and Seung, 1999, Bengio et al., 2014, Wang et al., 2024]. Hence,\nwe draw inspiration from relevant research [Zhou et al., 2012] to develop a contrastive non-negative\nFA (CNFA) approach. Specifically, the CNFA factorizes $A$ into sparse and non-negative gamma\nlatent variables, which can be described as follows:\n$\\bar{A}_{i, j} | x_{i}, x_{j} \\sim \\mathcal{N} \\left(\\theta_{i} \\theta_{j}\\right), \\quad \\theta_{i} | x_{i} \\sim \\operatorname{Gamma} \\left(f\\left(x_{i}\\right), 1\\right), \\quad \\theta_{j} | x_{j} \\sim \\operatorname{Gamma} \\left(f\\left(x_{j}\\right), 1\\right)$.\n(5)\nIn our formulation, the ideal scenario involves modulating the prior of the latent variables $\\theta_{i}$ by\nthe input $x_{j}$. However, we can readily relax this constraint to ensure that the latent variables are\nstatistically independent of the input variables [Sohn et al., 2015]. Thus, for contrastive factor analysis\n(CFA), we simply define the data-independent prior as $f\\left(x_{i}\\right) = 0$, while for contrastive non-negative\nfactor analysis (CNFA), we set $f(x_{i}) = 1$."}, {"title": "3.2 Reparameterizable latent variable distributions", "content": "The Gaussian distribution $\\theta \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)$ has probability\ndensity function $f\\left(\\theta / \\mu, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}}$, where $\\theta \\in \\mathbb{R}$. It is reparmeterizable as drawing\n$\\theta \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)$ us equivalent to letting $\\theta=g(\\epsilon):=\\mu+\\epsilon \\sigma, \\epsilon \\sim \\mathcal{N}(0,1)$. The KL divergence\nis analysis as $K L\\left(\\mathcal{N}\\left(\\mu_{1}, \\sigma_{1}^{2}\\right) \\|\\mathcal{N}\\left(\\mu_{2}, \\sigma_{2}^{2}\\right)\\right)=\\log \\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{\\sigma_{1}^{2}+\\left(\\mu_{1}-\\mu_{2}\\right)^{2}}{2 \\sigma_{2}^{2}}-\\frac{1}{2}$. As shown in Fig. 2(c), the\nvariational inference network utilize the obtained latent features $h_{i}$ from a deep feature extractor,\nsuch as ResNet [He et al., 2015], to construct the variational posterior:\n$q \\left(\\theta_{i} | x_{i}\\right)=\\mathcal{N}\\left(\\mu_{i}, \\sigma_{i}^{2}\\right), \\quad \\mu_{i}=f_{\\mu}\\left(h_{i}\\right), \\quad \\sigma_{i}^{2}=\\operatorname{Softplus} \\left(f_{\\sigma}\\left(h_{i}\\right)\\right)$,\n(6)\nwhere $f_{(\\cdot)} \\left(f_{t}(.)\\right)$ denotes the neural network, and Softplus applies $\\log (1+\\exp (\\cdot))$ non-linearity to each\nelement to ensure positive Gaussian variance parameters. It's evident that the CFA will reduce to\nmatrix factorization as $\\sigma$ of the Gaussian distribution goes to zero. Therefore, the proposed CFA can\nbe viewed as a generalization of vanilla contrastive learning [HaoChen et al., 2021].\nWhile the gamma distribution is ideal for the pos-\nterior due to its encouragement of sparsity and satisfaction of non-negativity constraints, its non-\nreparameterizability and incompatibility with gradient descent optimization lead to the adoption of\nthe Weibull distribution as an approximation for the posterior of gamma latent variables.\nThe reason for choosing the Weibull distribution is threefold [Zhang et al., 2018]: i) the Weibull\ndistribution is similar to a gamma distribution, capable of modeling sparse, skewed and positive\ndistributions; ii) the Weibull distribution has a simple reparameterization so that it is easier to"}, {"title": "3.3 Variational inference", "content": "For the contrastive factor analysis, given the model parameters referred to as $W$, which consist of the\nparameters in the generative model and inference network, marginal likelihood is defined as:\n$p (A|W) = \\int \\prod_{i=1}^{N} \\prod_{j=1}^{N} p \\left(A_{i, j} | \\theta_{i}, \\theta_{j}\\right) \\prod_{j=1}^{N} p \\left(\\theta_{j} | x_{j}\\right) d \\theta_{1: N}$.\n(8)\nThe inference task is to learn the parameters of the generative model and the inference network. Simi-\nlar to VAEs, the optimization objective of contrastive factor analysis can be achieved by maximizing\nthe evidence lower bound (ELBO) of log-likelihood as\n$\\mathcal{L}(A)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathbb{E}_{q(\\theta_{i} | x_{i})} [\\ln p (A_{i, j} | \\theta_{i}, \\theta_{j})] - \\sum_{j=1}^{N} \\mathbb{E}_{q(\\theta_{i} | x_{i})} [\\ln \\frac{q(\\theta_{i} | x_{i})}{p(\\theta_{i})}]$.\n(9)\nwhere the first term is the expected log-likelihood of the generative model, which ensures recon-\nstruction performance, and the second term is the Kullback-Leibler (KL) divergence that constrains\nthe variational distribution $q(\\theta_{i})$ to be close to its prior $p(\\theta_{i})$. Unfortunately, Equation. 9 becomes\nintractable with factor analysis algorithms as the input space $X$ grows exponentially large with\n$N \\rightarrow \\infty$. To address this challenge, we reformulate the factor analysis problem (Equation. 9)\nequivalently as a tractable contrastive learning (CL) objective that only necessitates sampling positive\nand negative samples from the joint and marginal distributions, respectively:\n$\\mathcal{L} = -2\\mathbb{E}_{x, x^{+}} f(x)^{T} f(x^{+}) + \\mathbb{E}_{x, x^{+}} \\left(f(x)^{T} f(x^{-})\\right)^{2}$$\n$\\quad - \\mathbb{E}_{q(\\theta^{+} | x^{+})} \\left[\\ln \\frac{q(\\theta^{+} | x^{+})}{p(\\theta^{+})}\\right]-\\mathbb{E}_{q(\\theta^{-} | x^{-})} \\left[\\ln \\frac{q(\\theta^{-} | x^{-})}{p(\\theta^{-})}\\right]$\n(10)\nThe parameters in contrastive (non-negative) factor analysis can be directly optimized by advanced\ngradient algorithms, like Adam [Kingma and Ba, 2017]."}, {"title": "3.4 Uncertainty-aware representation", "content": "To quantify the inferred latent variable uncertainty, we adopt Shannon's entropy [Shannon, 1948] of\nthe variational posterior. Specifically, The Shannon's entropy of Gaussian variational posterior with"}, {"title": "4 Related work", "content": "Our work is mainly related to factor analysis and probabilistic representation learning. Here, we\nintroduce the related research.\nBefore the era of deep learning, factor analysis was widely used in representation\nlearning. And its non-negative extension [Blei et al., 2003] has achieved great progress in text\nanalysis. Further, there is increasing interest in hierarchical factor analysis, which is aimed at\nexploring hierarchical data representation. But all this work is different from ours, which mainly\nfocuses on improving factor analysis with contrastive learning. Besides, [Tosh et al., 2021] proves\ncontrastive learning is capable of revealing latent variable posterior information in FA. Nonetheless,\nour approach diverges from this earlier work as it places more emphasis on proposing a novel model\nthat can effectively learn representations.\nProbabilistic representation learning has indeed made\nsignificant strides in recent years and has garnered considerable attention in the machine learning\ncommunity. Its ability to capture uncertainty and model complex data distributions has proven\nimmensely useful in various applications such as natural language processing [Zhou et al., 2016,\nVilnis and McCallum, 2014], computer vision [Shi and Jain, 2019, Oh et al., 2018, Chun et al.,\n2021, Wang et al., 2023], and graph processing [Elinas et al., 2020, Bojchevski and G\u00fcnnemann,\n2017]. In contrast to the previous efforts, this paper develops a general framework for (non-negative)\ncontrastive factor analysis that benefits from both (non-negative) factor analysis and contrastive\nlearning. Besides, there are efforts to improve contrastive learning with the Bayesian method, which\nis aimed at evaluating the uncertainty [Nakamura et al., 2023, Park et al., 2022, Kirchhof et al.,\n2023]. Unfortunately, most of these techniques rely on a decoder, which frequently degrades the\nperformance of downstream tasks [Aitchison and Ganev, 2021]. On the opposite hand, our research\nis an encoder-only approach that can enhance downstream tasks' performance."}, {"title": "5 Experiment", "content": "To evaluate the expressiveness of the proposed model, we compare it\nwith strong baselines, including SimCLR(CL) [Chen et al., 2020] and non-negative contrastive\nlearning(NCL) [Wang et al., 2024]. More specifically, we compare CFA with CL and CNF with\nNCL. It should be noted that CFA is built on SimCLR codebase 3 with a minor modification to the\nhidden projector output. We evaluate linear probing and fine-tuning performance on three benchmark\ndatasets: CIFAR-10, CIFAR-100[Krizhevsky et al., 2009], and Imagenet-100[Deng et al., 2009].\nApart from in-distribution evaluation, we also compare models on three out-of-distribution datasets,\nincluding stylized ImageNet[Geirhos et al., 2018], ImageNet-Sketch[Wang et al., 2019], ImageNet-\nC[Hendrycks and Dietterich, 2019](restricted to ImageNet-100 classes). For all experiments, we\nrun 3 random trials and report their mean and standard deviation. All experiments are performed on\nworkstation equipped with a CPU i7-10700 and accelerated by four GPU NVIDIA RTX 4090 with\n24GB VRAM."}, {"title": "5.2 Uncertainty evaluation", "content": "In addition to their enhanced expressiveness and robustness on out-of-distribution (OOD) data,\nanother notable strength of CFA and CNFA is their ability to capture data uncertainty.\nWith the entropy of the testing result and a given entropy threshold, we can\ndetermine whether the model is certain or uncertain about one prediction. To evaluate the uncertainty\nestimates, we use Patch Accuracy vs Patch Uncertainty (PAvPU) [Mukhoti and Gal, 2019], which is\ndefined as $PAvPU = \\left(n_{a c}+n_{i u}\\right) /\\left(n_{a c}+n_{a u}+n_{i c}+n_{i u}\\right)$, where $n_{a c}, n_{a u}, n_{i c}, n_{i u}$ represent the\nnumbers of accurate and certain, accurate and uncertain, inaccurate and certain, and inaccurate and\nuncertain samples, respectively. Compared to Top-1 Accuracy= $\\left(n_{a c}+n_{a u}\\right) /\\left(n_{a c}+n_{a u}+n_{i c}+n_{i u}\\right)$,\nPAVPU metric would be higher if the model tends to generate accurate predictions with low uncertainty\nand inaccurate predictions with high uncertainty leading to a more trustworthy algorithm."}, {"title": "5.3 Disentanglement representation learning", "content": "Learning a robust representation involves extracting explanatory factors that are sparse, disentangled,\nand semantically meaningful [Bengio et al., 2014]. By incorporating non-negative constraints, NCL\ncan learn disentangled features [Wang et al., 2024]. On the other hand, the Gamma distribution\npromotes sparsity due to its probability density function, which helps filter out irrelevant features\nand focus on the most significant ones. Thus, we evaluate CFA's performance in disentangled\nrepresentation learning.\nMany disentanglement metrics like MIG [Chen et al., 2019] are supervised\nand require ground-truth factors which are often unavailable in practice. Consequently, current\ndisentanglement evaluation is often limited to synthetic data[Carbonneau et al., 2022]. To val-\nidate the disentanglement on real-world data, we adopt an unsupervised disentanglement met-\nric SEPIN@k[Do and Tran, 2021]. SEPIN@k measures how each feature $f_{r_{i}}(x)$ is disentangled\nfrom others $f_{\\neq r_{i}}(x)$ by computing their conditional mutual information with the top k features, i.e.,\n$\\text{SEPIN@k} = \\sum_{i=1}^{k} I(x, f_{r_{i}}(x) | f_{\\neq r_{i}}(x))$, which are estimated with InfoNCE lower bound [Oord\net al., 2018] implemented following [Wang et al., 2024]."}, {"title": "6 Conclusion", "content": "In conclusion, this paper introduces a novel framework called Contrastive Factor Analysis (CFA),\nwhich bridges the gap between traditional Factor Analysis (FA) and the powerful technique of\ncontrastive learning. By leveraging the mathematical equivalence between contrastive learning and\nmatrix factorization, CFA capitalizes on the strengths of both methods, offering enhanced expressive-ness, robustness, and interpretability. Further, by extending CFA to a non-negative version (CNFA),\nwe exploit the interpretability properties of non-negative factor analysis, enabling the learning of\ndisentangled representations. Through extensive experimental validation, our methodology demon-strates efficacy across various dimensions, including expressiveness, robustness, interpretability, and\naccurate uncertainty estimation. In essence, CFA and CNFA represent promising approaches that\namalgamate the insights from factor analysis and contrastive learning, paving the way for more\neffective unsupervised representational learning in the deep learning era."}]}