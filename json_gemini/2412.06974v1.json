{"title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds", "authors": ["Zhenggang Tang", "Yuchen Fan", "Dilin Wang", "Hongyu Xu", "Rakesh Ranjan", "Alexander Schwing", "Zhicheng Yan"], "abstract": "Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.", "sections": [{"title": "1 Introduction", "content": "Multi-view scene reconstruction as shown in figure 1 has been a fundamental task in 3D computer vision for decades (Hartley and Zisserman, 2003). It is widely applicable in mixed reality (Avetisyan et al., 2024),"}, {"title": "2 Related Work", "content": "Structure-from-Motion (SfM). SfM methods reconstruct sparse scene geometry from a set of images and estimate individual camera poses. For this, SfM is often addressed in a few independent steps, including detecting/describing/matching local features across multiple views (e.g., SIFT (Lowe, 2004), ORB (Rublee et al., 2011), LIFT (Yi et al., 2016)), triangulating features to estimate sparse 3D geometry and camera poses (e.g., COLMAP (Schonberger and Frahm, 2016)), applying bundle adjustment over many views (see Triggs et al. (2000) for an overview), etc. Though steady progress has been made in the past decades (\u00d6zye\u015fil et al., 2017), and a large number of applications have been enabled (Westoby et al., 2012; Iglhaut et al., 2019; Carrivick et al., 2016), the classic SfM pipeline solves sub-tasks individually and sequentially, accumulating errors. More recent SfM methods improve traditional pipeline with learnable components (He et al., 2024; Wang et al., 2023a). MAST3R-SfM (Duisterhof et al., 2024) extends MAST3R (Leroy et al., 2024), which only produces local reconstructions for 2-view input, to perform global optimization for aligning local reconstructions via gradient descent to minimize 3D matching loss.\nMulti-View Stereo. MVS reconstructs dense 3D scene geometry from multiple views (Furukawa et al., 2015), often in the form of 3D points. In the classic PatchMatch-based framework (Zheng et al., 2014), per-pixel depth in the reference image is estimated from a set of unstructured source images via patch matching under a homography transform (Sch\u00f6nberger et al., 2016). Subsequent work has substantially improved feature matching (Wang et al., 2023c; Zhou et al., 2018a; Wang et al., 2021) and depth estimation (Galliani et al., 2015; Schmied et al., 2023; Xu and Tao, 2019). More recent learning-based approaches (Yao et al., 2018; Wang and Shen, 2018) often build an end-to-end pipeline, where deep models extract visual features, model"}, {"title": "3 Method", "content": "Our goal is to densely reconstruct a scene given a sparse set of rgb images with unknown camera intrinsics and poses. Following DUSt3R, our model predicts 3D pointmaps aligned with 2D pixels for each view. Different from DUST3R, our model jointly predicts 3D pointmaps for any number of input views in a single forward pass. Formally, given $N$ input image views of a scene $\\{I^v\\}_{v=1}^N$, where $I^v \\in \\mathbb{R}^{H\\times W \\times 3}$, from which we select one reference $r \\in \\{1, ..., N\\}$, our goal is to predict per-view 3D pointmaps $\\{X^{v,r}\\}_{v=1}^N$. Note, the 3D pointmap $X^{v,r} \\in \\mathbb{R}^{H\\times W \\times 3}$ denotes the coordinates of 3D points for image $I^v$ in the camera coordinate system of the reference view $r$.\nIn section 3.1, we introduce our Multi-View Dense Unconstrained Stereo 3D Reconstruction (MV-DUSt3R) network to efficiently processes all input views in one pass and without subsequent global optimization, while considering a single chosen reference view. In section 3.2, we present MV-DUSt3R+, which processes all input"}, {"title": "3.1 MV-DUSt3R", "content": "A Multi-View Model Architecture. As shown in figure 3, MV-DUSt3R consists of an encoder to transform images into visual tokens, decoder blocks to fuse tokens across views, and regression heads to predict per-view 3D pointmaps aligned with 2D pixels. Different from DUSt3R, our network uses decoder blocks to fuse tokens across all views rather than independently fusing only tokens for two views at a time. Concretely, a ViT (Dosovitskiy, 2020) encoder with shared weights, denoted as Enc, is first applied on input views $\\{I^v\\}_{v=1}^N$ to compute initial visual tokens $\\{F_d^v\\}_{v=1}^N$, i.e., $F_d^v = Enc(I^v)$. Note, the resolution of the encoder output features is 16\u00d7 smaller than the input image before being flattened into a sequence of tokens.\nTo fuse the tokens, two types of decoders are used, one for the chosen reference view and one for the remaining source views. They share the same architecture but their weights differ. Each decoder consists of D decoder blocks referred to as DecBlock^{ref}_d and DecBlock^{src}_d for $d\\in \\{1,...,D\\}$. Their difference is, DecBlock^{ref}_d is dedicated to update reference view tokens $F_d^r$, while DecBlock^{src}_d update tokens $\\{F_d^v\\}_{v\\neq r}$ from all other source views. Each decoder block takes as input a set of primary tokens from one view, and a set of secondary tokens from other views. In each block, a self-attention layer is applied to primary tokens only, and a cross-attention layer fuses primary tokens with secondary tokens before a final MLP is applied on the primary tokens. Layer norm is also applied before both attentions and the MLP. Using those, the decoder computes the final token representations $F_D^v$ via\n\n$F_D^v = \\begin{cases}\nDecBlock_d^{ref}(F_{d-1}^v, \\bar{F}_{d-1}^v) & \\text{if } v = r, \\\\\nDecBlock_d^{src}(F_{d-1}^v, \\bar{F}_{d-1}^v) & \\text{otherwise}.\n\\end{cases}$\n\nHere, the secondary tokens $\\bar{F}_d^v = \\{F_d^1,...,F_d^{v-1},F_d^{v+1},...,F_d^N\\}$ subsume tokens from all views other than the view of the primary tokens $F_d^v$.\nTo finally predict the per-view 3D pointmaps, we use two heads: $Head_{pcd}^{ref}$ for the reference view and $Head_{pcd}^{src}$ for all other views. They share the same architecture but use different weights. Each consists of a linear projection layer and a pixel shuffle layer with an upscale factor of 16 to restore the original input image resolution. As in DUSt3R, the head predicts 3D pointmaps $X^{v,r} \\in \\mathbb{R}^{H\\times W \\times 3}$ and confidence maps $C^{v,r} \\in \\mathbb{R}^{H\\times W}$ via\n\n$X^{v,r}, C^{v,r} = \\begin{cases}\nHead_{pcd}^{ref}(F_D^v) & \\text{if } v = r, \\\\\nHead_{pcd}^{src}(F_D^v) & \\text{otherwise}.\n\\end{cases}$\n\nNote that DUSt3R is a special case of MV-DUSt3R if the number of views $N = 2$. However, for multiple input views, MV-DUSt3R will update primary tokens using a much larger set of secondary tokens. Hence, it is able to benefit from many more views. Importantly, as our architecture components and structure only"}, {"title": "3.2 MV-DUSt3R+", "content": "As shown in figure 4, for different reference view choices, the quality of the scene reconstructed by MV-DUSt3R varies spatially. The predicted pointmap for an input source view tends to be better when the viewpoint change to the reference view is small, and deteriorates as the viewpoint change increases. However, to reconstruct a large scene with a sparse set of input views, a single reference view with only moderate viewpoint changes to all other source views is unlikely to exist. Therefore, it is difficult to reconstruct scene geometry equally well everywhere with a single selected reference view. To address this, we propose MV-DUSt3R+, which selects multiple views as the reference view, and jointly predicts pointmaps for all input views in the camera coordinate of each selected reference view. We hypothesize: while pointmaps of certain input views are difficult to predict for one reference view, they are easier to predict for a different reference view (e.g., smaller viewpoint change, more salient matching patterns). To holistically improve the pointmap prediction of all input views, we include a novel Cross-Reference-View block into MV-DUSt3R+.\nA Multi-Path Model Architecture. Let $R = \\{r_m\\}_{m=1}^M$ denote a set of $M$ reference views randomly chosen from an unordered set of input views. We adopt the same decoder blocks from MV-DUSt3R, deploy them in a multi-path model architecture (see figure 5), and use them to compute a reference-view dependent intermediate representation $G_d^{v,m}$ at decoder layer $d$ for input view $v$ and reference view $r_m$:\n\n$G_d^{v,m} = \\begin{cases}\nDecBlock_d^{ref}(F_{d-1}^{r_m}, \\bar{F}_{d-1}^{r_m}) & \\text{if } v = r_m, \\\\\nDecBlock_d^{src}(F_{d-1}^{v}, \\bar{F}_{d-1}^{r_m}) & \\text{otherwise},\n\\end{cases}$\n\n$\\bar{F}_d^{v,m} = CrossRefViewBlock_d(G_d^{v,m}, \\bar{G}_d^{v,-m}).$\n\nHere, $\\bar{F}_d^v = \\{F_d^1,...,F_d^{v-1},F_d^{v+1},...,F_d^N\\}$. As shown in figure 5, we fuse and update per-view tokens computed under different reference views by adding a Cross-Reference-View block after each decoder block (equation (6)), where $\\bar{G}_d^{v,-m} = \\{G_d^{v,1}, ..., G_d^{v,m-1}, G_d^{v,m+1}, ...,G_d^{v,M}\\}$. Following equation (2), we compute per-view pointmaps $X^{v,m}$ and confidence map $C^{v,m}$ under each reference view $r_m$."}, {"title": "3.3 MV-DUSt3R(+) for Novel View Synthesis", "content": "Next we extend our networks to support NVS with Gaussian primitives (Kerbl et al., 2023). For clarity, below we use MV-DUSt3R+ as an example. MV-DUST3R can be extended similarly.\nGaussian Head. We add a separate set of heads to predict per-pixel Gaussian parameters, including scaling factor $S^{v,m} \\in \\mathbb{R}^{H\\times W \\times 3}$, rotation quaternion $q^{v,m} \\in \\mathbb{R}^{H\\times W \\times 4}$, and opacity $a^{v,m} \\in \\mathbb{R}^{H\\times W}$. We add Gaussian heads $Head_{DGS}^{ref}$ and $Head_{DGS}^{src}$ for reference and other views:\n\n$S^{v,m}, q^{v,m}, a^{v,m} = \\begin{cases}\nHead_{DGS}^{ref}(F_D^m) & \\text{if } v = r_m, \\\\\nHead_{DGS}^{src}(F_D^m) & \\text{otherwise}.\n\\end{cases}$\n\nFor other Gaussian parameters, we use the predicted pointmap $X^{v,m}$ as the center, the pixel color $I^v$ as the color and fix the spherical harmonics degree to be 0."}, {"title": "4 Experiments", "content": "4.1 Datasets\nOur training data includes ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), HM3D (Ramakrishnan et al., 2021), and Gibson (Xia et al., 2018). Note, all of them are also used by DUSt3R. For evaluation, we use datasets MP3D (Chang et al., 2017), HM3D (Ramakrishnan et al., 2021), and ScanNet (Dai et al., 2017). While ScanNet scenes are often small single-room sized and with low diversity, scenes in MP3D and HM3D are often large multi-room sized and with high diversity. MP3D also contains outdoor scenes. See table 1 to compare evaluation datasets. We use the same train/test split as DUSt3R, and our training data is a subset of DUSt3R's training data (for details see appendix).\nTrajectory Generation. To generate a set of input views $\\{I^v\\}_{v=1}^N$ for $N > 2$, we first randomly select one frame and initialize the current scene point cloud using its data. Then we sequentially sample more candidate frames. We retain a candidate frame and add its corresponding point cloud to the current scene, if the overlap between the candidate frame's point cloud and the current scene point cloud is between a lower threshold $t_{min}$ and an upper bound $t_{max}$."}, {"title": "4.2 Implementation Details", "content": "We process input views at resolution 224 \u00d7 224. We utilize 64 Nvidia H100 GPUs for the model training. To initialize, DUSt3R model weights are used. We use the first $N = 8$ views of each trajectory as input views, and randomly select 1 view as the reference view for MV-DUSt3R and $M = 4$ views for MV-DUSt3R+. We train for 100 epochs using 150K trajectories per epoch, which takes 180 hours. For MVS reconstruction evaluation, to assess the performance of each method in reconstructing scenes of variable sizes, we report results with input views ranging from 4 to 24 views. For NVS evaluation, we use the remaining 6 views as novel views. Below we report results on all evaluation datasets for all choices of the number of input views using only one MV-DUSt3R model and one MV-DUSt3R+ model."}, {"title": "4.3 Multi-View Stereo Reconstruction", "content": "Metrics. We report Chamfer Distance (CD), as in prior work (Aan\u00e6s et al., 2016; Wang et al., 2024), as well as 2 additional metrics. NormalizedDistance (ND): $l_{regr}$ with zero-centering to make it scale and translation invariant; DistanceAccu@0.2 (DAc): proportion of pixels where the corresponding normalized distance between prediction and groundtruth in pointmap is $\\leq 0.2$.\nBaselines. We compare with baselines that reconstruct scenes from input rgb views without knowing camera intrinsics and poses. We evaluate the DUSt3R model trained at input resolution 224 \u00d7 224 with Global Optimization (GO), and also the Spann3R (Wang and Agapito, 2024) model on Github.\nResults are shown in table 2. In the supervised setting, we compare DUSt3R and our methods on HM3D and ScanNet. On HM3D (multi-room scenes), MV-DUSt3R consistently outperforms DUSt3R, as the scene size increases and more input views are sampled (from 4 to 24 views). For example, MV-DUSt3R reduces ND by 1.7x and increases DAc by 1.2\u00d7 for 4-view input. For 24-view input, MV-DUSt3R improves ND by 2x and DAc by 5.3x. This confirms: as more input views are available, single-stage MV-DUSt3R exploits multi-view cues to infer 3D scene geometry better than DUSt3R, which only exploits pairwise stereo cues"}, {"title": "4.4 Multi-View Pose Estimation", "content": "For both baseline and our methods, we estimate the relative camera pose for all pairs of input views from a given set of input views. We use the Weiszfeld algorithm (Plastria, 2011) to estimate camera intrinsics, and RANSAC (Fischler and Bolles, 1981) with PnP (Lepetit et al., 2009) to estimate camera pose (see appendix for more details).\nBaselines. We compare with other pose-free methods including DUSt3R and PoseDiffusion (Wang et al., 2023b), a recent diffusion based method for camera pose estimation.\nMetrics. Prior methods (Wang et al., 2024) report Relative Rotation Accuracy (RRA@15), Relative Translation"}, {"title": "4.5 Novel View Synthesis", "content": "Metrics. Following prior works (Smart et al., 2024; Charatan et al., 2024; Fan et al., 2024), we report Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) (Wang et al., 2004), and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018).\nBaseline. We compare with a DUSt3R-based baseline, which generates per-pixel Gaussian parameters as follows. We use the pointmap predicted by DUSt3R as the Gaussian center, use pixel RGB color $I^v$ as the color, a constant 0.001 for the scale factor $S^{v,m}$, an identity transform, 1.0 for opacity, and spherical harmonics with zero-degree. See appendix for more details on rendering.\nResults. As shown in table 4, MV-DUSt3R improves upon the DUSt3R baseline across all evaluation datasets under all choices of input views. The improvements are also confirmed qualitatively in figure 6: the novel views synthesized by MV-DUSt3R better infer 3D geometry of objects and background (e.g., walls, ceiling). MV-DUSt3R+ further improves in challenging situations, such as a scene with multiple close-by objects of similar appearance (e.g., chairs). As an example, consider the ScanNet scene with 20 views in figure 6. Using multiple reference views, and fusing features computed in different model paths help resolve ambiguity in inferring the spatial relations between input views."}, {"title": "4.6 Scene Reconstruction Time", "content": "We compare the time of MVS reconstruction in table 2. Our single-stage feed-forward networks entirely run on a GPU, without Global Optimization (GO). Compared with DUSt3R, our MV-DUSt3R runs 48\u00d7 to 78\u00d7"}, {"title": "4.7 Ablation Studies", "content": "Number of input views at training time. We compare 1-stage and 2-stage trained MV-DUST3R+ on the HM3D evaluation set. For 1-stage training, we choose the first 4 or 8 views of the trajectory. For 2-stage training, we finetune upon MV-DUSt3R+'s 1-stage 8-view training, by using a mixed set of inputs with the number of views uniformly sampled between 4 and 12. As shown in table 5, 1-stage training on 4 views doesn't generalize well to more views, 1-stage training on 8 views performs decent, and 2-stage training outperforms 1-stage training on almost all tasks on HM3D. See appendix for more 2-stage results.\nUpper bound performance of our networks. We study oracle performance if the best reference view is chosen based on groundtruth. For MV-DUSt3R, we consider all input views as reference view candidates, and manually select the one with best MVS reconstruction (MV-DUSt3Roracle). For MV-DUSt3R+, we choose the model path with best MVS reconstruction (MV-DUSt3R+oracle). As shown in tables 2 to 4, the performance gap between MV-DUSt3R+ and MV-DUSt3R+oracle is significantly smaller than that between MV-DUSt3R and MV-DUSt3Roracle. This validates our multi-path MV-DUSt3R+ architecture.\nImpact of adding Gaussian head on MVS reconstruction performance. In table 6, we compare MV-DUSt3R and"}]}