{"title": "When to Localize?\nA Risk-Constrained Reinforcement Learning Approach", "authors": ["Chak Lam Shek", "Kasra Torshizi", "Troi Williams", "Pratap Tokekar"], "abstract": "In a standard navigation pipeline, a robot localizes\nat every time step to lower navigational errors. However, in\nsome scenarios, a robot needs to selectively localize when it is\nexpensive to obtain observations. For example, an underwater\nrobot surfacing to localize too often hinders it from searching\nfor critical items underwater, such as black boxes from crashed\naircraft. On the other hand, if the robot never localizes,\npoor state estimates cause failure to find the items due to\ninadvertently leaving the search area or entering hazardous,\nrestricted areas. Motivated by these scenarios, we investigate\napproaches to help a robot determine \"when to localize?\" We\nformulate this as a bi-criteria optimization problem: minimize\nthe number of localization actions while ensuring the probabil-\nity of failure (due to collision or not reaching a desired goal)\nremains bounded. In recent work, we showed how to formulate\nthis active localization problem as a constrained Partially\nObservable Markov Decision Process (POMDP), which was\nsolved using an online POMDP solver. However, this approach\nis too slow and requires full knowledge of the robot transition\nand observation models. In this paper, we present RISKRL,\na constrained Reinforcement Learning (RL) framework that\novercomes these limitations. RISKRL uses particle filtering\nand recurrent Soft Actor-Critic network to learn a policy\nthat minimizes the number of localizations while ensuring\nthe probability of failure constraint is met. Our numerical\nexperiments show that RISKRL learns a robust policy that\noutperforms the baseline by at least 13% while also generalizing\nto unseen environments.", "sections": [{"title": "I. INTRODUCTION", "content": "In robotics, self-localization is crucial because it enhances\nnavigation accuracy, and situational awareness, and enables\ncomplex tasks. Typically, an autonomous robot perceives its\nenvironment and self-localizes, plans its subsequent actions,\nacts upon its plan phases, and repeats the cycle. However,\nsometimes, a robot may want to localize seldom when it\nis not advantageous. For example, underwater robots need\nto surface to localize in underwater rescue and recovery\nmissions. Surfacing to localize too often may hinder an\nunderwater robot from searching for critical underwater\nitems such as black boxes from crashed aircraft. On the other\nhand, if the robot never localizes, it will accumulate large\namounts of drift [1], which may prevent it from finding the\nitems due to inadvertently leaving the search area or entering\nhazardous, restricted areas. Therefore, in such scenarios,\nrobots must balance prolonged actions that achieve mission\nobjectives (such as searching for critical items underwater)\nwith localizing to improve navigation accuracy.\nWe explored such scenarios in our recent work [2]. Our\ncentral question was: how can a robot plan and act for long\nhorizons safely and only localize when necessary? (Figure 1\ndiscusses a general scenario to this question). We emphasize\nthat such a question is not trivial because we have two\ncompeting objectives. The first objective is localize often to\nmaximize mission safety and performance, where we can\nensure the vehicle remains within the search area and out\nof hazardous zones. On the other hand, the second objective\nis to localize infrequently to minimize the number of times\nthe vehicle must deviate from its mission, which in turn can\nreduce mission time. These two objectives are challenging\nto optimize via one objective, as shown with our POMCP\nbaseline in [2]. Therefore, we addressed the question by\nformulating it as a constrained Partially Observable Markov\nDecision-making Process (POMDP), where our objective\nwas to minimize the number of localization actions while\nnot exceeding a given probability of failure due to colli-\nsions. Then we employed CC-POMCP [3], a cost-constrained\nPOMDP solver, to find policies that determine when the\nrobot should move along a given path or localize."}, {"title": "II. RELATED WORK", "content": "This paper explores minimizing localization actions while\nnot exceeding pre-defined failure probabilities. In the follow-\ning subsections, we position our method within the active\nlocalization and constrained RL literature.\nActive perception [5], [6] encompasses various ap-\nproaches, including active localization [7], [8], active map-\nping [9]\u2013[11], and active SLAM [9]. These approaches focus\non finding optimal robot trajectories and observations to\nachieve mission goals. Of these approaches, our current\napproach falls under active localization. Typically, active\nlocalization methods address where a robot moves and looks\nto localize itself [12]\u2013[15] or a target [16], [17]. Thus,\nthese active localization approaches generally differ from\nour problem because we seek to determine when a robot\nlocalizes. However, one exception is our prior work [2],\nwhich proposes a preceding approach to the one in this paper.\nThis approach improves upon [2], where we now model the\nprobability of failure in terms of risk, reduce the inference\ntime significantly, and relax the need for well-defined noise\nmodels.\nModel-free reinforcement learning promises a more scal-\nable and general approach to solving the active localiza-\ntion problem since it requires less domain knowledge [18],\n[19]. However, many prior works applying RL to solving\nPOMDPs, even without constraints, have gotten poorer re-\nsults compared to more specialized methods [20]. A recent\narchitecture [21] utilizing a Soft-Actor Critic (SAC) with\ntwo separate recurrent networks for both the actor and value\nfunctions has shown promise to surpass more specialized\nmethods in select examples. Since recurrent networks also\nact on the history of observations, they can handle partial\nobservability. Our RISKRL approach is based on this twin\nrecurrent network SAC architecture [21]. However, unlike\n[21] we also seek to deal with risk constraints.\nThere is a separate line of work on constrained RL in\nthe fully observable setting [22]. Constrained RL extends\nmodel-free RL by incorporating real-world limitations, such\nas safety, budget, or resource constraints, into the learning\nprocess [22]. The primary challenge in constrained RL is bal-\nancing the trade-off between maximizing cumulative rewards\nand satisfying these secondary constraints. A commonly\nused approach to tackle this problem is the primal-dual\noptimization method [23], which iteratively adjusts both\nthe policy and constraint parameters. Various techniques are used\nto simplify and solve these problems, such as transforming\nthe constraints into convex functions [24] or employing\nstochastic approximation methods to handle probabilistic\nconstraints [4]. However, these prior works on constrained\nRL have only focused on the fully observable setting. In\nthis paper, we build on these two lines of work and present\nRISKRL, which handles both partial observability as well as\nchance constraints."}, {"title": "III. PROBLEM STATEMENT", "content": "This paper solves the same active localization problem as\nin our prior work [2]. That is, a robot aims to selectively\nlocalize while navigating along a path to a pre-defined goal.\nWhen the robot believes it is opportune to localize, it uses its\nsensors to obtain noisy observations of its pose to mitigate\nfailures such as collisions. Thus, we aim to generate a move-\nlocalize policy that 1) minimizes localization events and\n2) avoids exceeding a failure probability threshold \u0109. For"}, {"title": "IV. RISKRL ACTIVE LOCALIZATION ALGORITHM", "content": "In this section, we present our RISKRL algorithm for\nsolving the active localization problem given in Equation 1.\nWithin the context of RISKRL, the following subsections\ndiscuss our active localization framework (that is, the PF,\nlow-level planner, and high-level RL planner) in Section IV-\nA and provide detailed descriptions of the RL algorithm in\nthe remaining subsections.\nWe now discuss our RISKRL active localization frame-\nwork, which is depicted in Figure 2 and described in Algo-\nrithm 1. Our framework comprises three main components:\nan RL high-level planner, a PF, and a low-level planner. The\nhigh-level planner chooses which action (move or localize)\nthe agent performs given an input state $s_t$. Next, we use the\nPF to maintain a belief of the agent's state whenever the\nagent selects either action and computes a state vector for\nthe RL planner. Finally, a low-level planner determines how\nthe agent should move (that is, move forward, turn left, or\nturn right) if move is chosen or recomputes hazard-free paths\nif localize is chosen.\n\nThe output from the PF is fed as input to the high-level\npolicy. The goal of the high-level policy is to choose between\ntwo actions: $\\mathcal{A}_{RLP} = \\{move, localize\\}$. If the policy chooses\nto move, then we execute the next action given by the low-\nlevel planner without taking an observation. As described\nearlier, we use a constrained RL policy that minimizes\nthe number of localization actions while ensuring the risk\nconstraint is met.\nThe state input $s_t$ given to the RL policy is derived from\nthe output of the PF. Specifically, $s_t$ is a 2D vector containing\nthe collision probability and the Manhattan distance from\nthe robot to the goal. We compute the collision probability by\ncounting the number of particles in collision at the current\ntime step. We chose this observation representation so that\nour agent generalizes to unseen environments.\nThe reward $r_t = -1$ if the robot chooses the localize\naction and 0 otherwise. We can use standard RL to mini-\nmize the number of localization actions by maximizing this\nreward function. However, naive optimization will violate\nthe constraints in (1). The constraint probability is difficult\nto estimate because it requires interaction with the environ-\nment and varies based on the policy being used, making\nit challenging to establish a clear relationship between the\npolicy and the constraint. We follow the relaxation approach\noutlined in [4], converting the probabilistic constraint in a\nChance-Constrained POMDP into a cumulative constraint.\nSpecifically, we reformulate the optimization problem as\nfollows. Our goal is to maximize the expected cumulative\nreward V(0), defined by:\n$\\max V(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\sum_{t=0}^{T} r(s_t, a_t) \\ \\ | \\ \\ \\pi_{\\theta}$    (2)\nwhere \u03b8 denotes the parameters of the policy $\u03c0_\u03b8$, and V(0)\nrepresents the expected reward over time. We then impose a\ncumulative constraint:\n$\\mathbb{U}_{0}: \\sum_{t=0}^{T} \\gamma^{t} (1 - Pr(failure \\ | \\ X_{1:t}, \\ A_{1:t}, b(s_0))) \\geq c$   (3)\nHere, Ue is the accumulated discounted probability of failure,\nand c = $\\sum_{t=0}^{T} \\gamma^{t} (1 - \\epsilon)$ represents the risk-adjusted thresh-\nold. This formulation simplifies the original problem by\napproximating the probabilistic constraint, thus allowing the"}, {"title": "C. Handling Partial Observability", "content": "The previous section described how we can optimize\nthe policy using the primal-dual approach. This section\npresents the specific architecture we use for the actual policy.\nStemming from [21], we use a Soft Actor-Critic Model\nas it generally tends to have better sample efficiency. To\ndeal with partial observability, Recurrent Neural Networks\n(RNN) have been known to mitigate the effects of a noisy\nobservation by making decisions based on the past trajectory\ninstead of just the current observation (or fixed sequence of\nobservations) [25] [26]. We implement an LSTM module to\nhelp stabilize training [27]. Initial training results showed\nthat a deterministic actor would more often converge to a\nuniform policy (only moving or only localizing), therefore"}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "In this section, we report our findings from numerical\nexperiments comparing RISKRL with several baselines and\nevaluating the robustness and generalization capabilities of\nRISKRL. The training and testing environment are motivated\nby an underwater scenario introduced in our prior work [2].\nIn the scenarios, a robot is tasked with navigating through\ncomplex underwater environments, which include obstacles\nsuch as rocks and coral formations. The robot must perform\nlocalization and path planning while accounting for the\nunique challenges posed by water currents and variable\nvisibility. Our experimental code is located on GitHub2.\nBaselines. We compare RISKRL with four types of base-\nlines. The first type is two static policies (SP): (M2x,L),\nand (M3x,L), where for example, (M2x, L) is a fixed\nrepeating sequence of two move actions followed by a\nlocalize action. The second type is a threshold planner (TP)\nthat localizes the robot whenever the probability of failure\n(computed using the PF output) exceeds a threshold. The\nthird type is an implementation of CC-POMCP, a cost-aware,\nonline policy planner from our prior work [2]. Finally, the\nlast type is a standard, risk-unaware RL policy (BASERL). In\nthe latter, we penalize every time the robot chooses a localize\naction (-1) and when it collides (-256). This baseline is\nused to assess the advantage of employing risk-aware RL.\nEnvironments. Figure 4 shows our environmental setup.\nWe assume the robot knows the map, the start state, and the\ngoal region. We set the initial belief to the start state. In all\nof our experiments, we set the transition noise such that the\nrobot has an 80% chance of going in the direction it is facing,\na 10% chance of drifting to the left, and a 10% chance of\ndrifting to the right. Other than the results in Section V-E, we\nassume no observation noise when localizing to focus more\non the effect of transition noise. Figure 4 visually represents\nour environments.\nThe following subsections compare the performance of\nour baselines and the proposed algorithm. Our experiments\nran until the robot reached the goal or failure region. The\nresults were based on 100 runs for each algorithm in each\nenvironment (except CC-POMCP, which we only ran 75\ntimes in the train environment since it takes around 30X\nmore time to get through a trajectory compared to the rest\nof the planners). All bars depict the mean values in Figures\n7a and 7b.\n2https://github.com/raaslab/when-to-localize-riskrl"}, {"title": "VI. CONCLUSION", "content": "We developed a novel active localization approach termed\nRISKRL, which combines a chance-constrained planner with\na particle filter (PF). Our approach aims to minimize lo-\ncalization actions while not exceeding failure probabilities.\nThe chance-constrained planner determines when the robot\nmoves or localizes and was implemented using constrained\nreinforcement learning (RL). The PF maintains the robot's\nbelief by processing noisy 2D pose observations from the en-\nvironment and motion commands. We also use the PF's belief\nto compute the observation for our RL planner. The current\napproach succeeds our prior work [2], which employed an\nalgorithm that has a slower inference time and requires\nwell-defined transition and observation noise models, making\nit unusable in real-time, real-world scenarios. Our results\nrevealed three key findings. First, RISKRL consistently out-\nperformed the baseline by at least 13%, while maintaining\na similar number of localizations. This demonstrates the\nrobot's ability to optimize the timing of localization actions,\nachieving higher rewards. Second, the robot dynamically\nadjusts the frequency of localizations, showcasing its adapt-\nability to varying scenarios and environmental conditions.\nFinally, through bi-criteria optimization, RISKRL effectively\ncontrols risk levels while maximizing performance, ensuring\nthe robot operates safely. Notably, RISKRL demonstrates\nzero-shot transfer capabilities, handling new environments\nwithout retraining, underscoring its potential for real-world\ndeployment.\nWe believe that our models hold the potential for further\noptimization to achieve improved results. One promising\ndirection is the implementation of Evolving Rewards [28] to\nidentify a more effective set of reward values. Additionally,\nwe aim to extend the current formulation to encompass both\ncontinuous state and action spaces, with the expectation that\nthis will facilitate deployment in real-world scenarios."}]}