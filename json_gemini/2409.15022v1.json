{"title": "A Diagonal Structured State Space Model on Loihi 2\nfor Efficient Streaming Sequence Processing", "authors": ["Svea Marie Meyer", "Philipp Weidel", "Philipp Plank", "Leobardo Campos-Macias", "Sumit Bam Shrestha", "Philipp Stratmann", "Mathis Richter"], "abstract": "Deep State-Space Models (SSM) demonstrate state-of-the art performance on\nlong-range sequence modeling tasks. While the recurrent structure of SSMs can\nbe efficiently implemented as a convolution or as a parallel scan during training,\nrecurrent token-by-token processing cannot currently be implemented efficiently\non GPUs. Here, we demonstrate efficient token-by-token inference of the SSM\nS4D on Intel's Loihi 2 state-of-the-art neuromorphic processor. We compare this\nfirst ever neuromorphic-hardware implementation of an SSM on sMNIST, psM-\nNIST, and SCIFAR to a recurrent and a convolutional implementation of S4D on\nJetson Orin Nano (Jetson). While we find Jetson to perform better in an offline\nsample-by-sample based batched processing mode, Loihi 2 outperforms during\ntoken-by-token based processing, where it consumes 1000 times less energy with\na 75 times lower latency and a 75 times higher throughput compared to the re-\ncurrent implementation of S4D on Jetson. This opens up new avenues towards\nefficient real-time streaming applications of SSMs.", "sections": [{"title": "Introduction", "content": "The attention mechanism has been pervasive in enabling the incredible AI capabilities of today, such\nas intelligent chatbots [1], object detection [2], and multimodal video understanding [3]. While it\nexcels at associating context at different points in time and building a higher level understanding, the\ncomputational cost of the attention mechanism increases quadratically with the context length [4],\nmaking it infeasible for very long sequences. Different techniques realize more efficient computa-\ntion of attention [5, 6, 7] but the fundamental limitation of increased cost for higher context length\nremains.\nRecurrent networks, on the other hand, increase the compute cost only linearly with context length;\nthey are, however, difficult to train. Recently, a family of linear recurrent architectures, SSMs, based\non the memory property of state-space dynamics [8, 9, 10, 11, 12] have emerged as an alternative\nto the attention mechanism. With their recurrent formulation, they offer linearly increasing com-\nputational cost, while also being easily trainable using the convolution view of the same model [8].\nThey have shown superior performance compared to attention-based models on very long-range con-\ntext [9, 12, 13], while also maintaining competitive performance on language modeling tasks [12].\nThe recurrent formulation of SSMs with their local stateful computation aligns well with the ar-\nchitecture of neuromorphic processors, in which compute and memory are co-located [14]. This\nis in contrast to GPUs, where the separation of compute and memory makes them efficient only\nwhen processing highly structured compute and memory reads [15] (e.g., batching, convolution)\nand therefore inefficient when computing the recurrent formulation of SSMs. Previous simulated"}, {"title": "Background", "content": null}, {"title": "Deep State-Space Models", "content": "Deep SSMs have been increasingly used to overcome the challenges of transformers in modeling\nlong sequences [19], starting with the structured SSM, S4, developed by Gu et al. [8].\nS4 models can perform their computations using one of three representations, which can be trans-\nformed into each other and serve different functional purposes [8, 12]:\n$\\begin{aligned}\n\\dot{x}(t) &= A x(t) + B u(t),  \\\\\ny(t) &= C x(t)                                     (1)\\\\\\\\\nx_{k} &= \\bar{A} x_{k-1} + \\bar{B} u_{k}, \\\\\ny_{k} &= \\bar{C} x_{k}                                     (2)\\\\\\\\\nK &= (C \\bar{B}, C \\bar{A} \\bar{B}, \\ldots, C \\bar{A}^{L-1} \\bar{B}), \\\\\n(\\ldots, y_{k}, \\ldots, y_{L}) &= K * (..., u_{k}, ..., u_{L})                         (3)\n\\end{aligned}$\nThe continuous recurrent representation in Equation (1) processes continuous 1-D signals $u(t)$ to\noutput signals $y(t)$ via an N-dimensional latent space $x(t)$: $u(t) \\in \\mathbb{R} \\rightarrow x(t) \\in \\mathbb{R}^N \\rightarrow y(t) \\in \\mathbb{R}$.\nThe discrete recurrent representation in Equation (2) assumes constant step sizes $\\Delta$ to transform the\nmatrices $A$, $B$, and $C$ into discrete matrices $\\bar{A}$, $\\bar{B}$, and $\\bar{C}$ and enables fast autoregressive infer-\nence when inputs $u_k$ are presented sequentially. The convolutional representation in Equation (3)\ntransforms the linear time-invariant SSM in Equation (1) into a global convolution, which enables\nefficient, parallelized training when L data points are available in a batch.\nLong-range sequence modeling has seen algorithmic advancements and alternative recurrent formu-\nlations, including Liquid S4 [10], S5 [9], and the S6 layers in Mamba [12]. These models have\nachieved state-of-the-art accuracy in sequence modeling on tasks including speech commands [10],\nthe Path-X version of the Pathfinder challenge [9], autoregressive language modeling, audio wave-\nforms, and DNA sequences [12].\nSeveral hardware-aware adjustments have been applied to SSMs to make them more efficient on\nGPUs. Most notably, it has been shown that A can be diagonalized with little to no detrimental\neffect on the algorithmic performance [20], leading to the S4 variant S4D [11] used in the present\npaper. Furthermore, quantization has been applied to reduce the memory footprint and inference\ntime of SSMs [21]."}, {"title": "Intel's Neuromorphic Processor Loihi 2", "content": "Intel's Loihi 2 neuromorphic processor is a fully asynchronous, digital, scalable, event-driven archi-\ntecture, designed for efficient sparse signal processing.  Each Loihi 2 chip features an interconnect of compute and memory co-located computa-\ntional units, neurocores, that communicate using spiking events and support up to 8192 indepen-\ndent, programmable neurons. The neurons can be stateful, enabling an efficient implementation of"}, {"title": "Neuromorphic Diagonal Deep State-Space Model", "content": null}, {"title": "Model architecture on Loihi 2", "content": "shows the model architecture of S4D as implemented on the Loihi 2 neuromorphic pro-\ncessor. It consists of an encoder layer that expands the input to a higher dimensionality, four S4D\nblocks, and a decoder layer that reduces the dimensionality to the number of classes. At the top of the\nfigure, the dimensionality of each layer of the model is listed, where $I$ represents the input dimen-\nsionality, $H$ is the model dimension, and $N$ is the number of hidden states per model dimension; the\noutput dimensionality is 10. We evaluate two model sizes with 67k parameters ($H = 64, N = 32$)\nand 265k parameters ($H = 128, N = 64$) for different datasets (see Section 4 for details).\nEach S4D block consists of a simplified variant of the S4D model, computing the SSM dynamics\nas a recurrent neural network (Equation (2)). In contrast to the original S4D model, we only use\nReLU activations instead of GLUs and GeLUs to increase activation sparsity. To further simplify\nthe model, we also leave out normalization layers and residual connections. After each S4D layer,\nthe dimensions are mixed using a linear projection followed by another ReLU activation.\nAll S4D layers, ReLU activations, and biases (depicted in dark blue in ), are implemented\nas programmable neurons on Loihi 2. As the matrices $A$, $B$, and $C$ of the S4D dynamics are fully\ndiagonal, all hidden states in the S4D layers compute their dynamics fully independently without\ninteractions with other states. Therefore, the state-space dynamics can be computed within the\nprogrammable neurons, and without using self-recurrent connections or connections between them.\nAll projections (depicted in light blue in ), up-projection, expansion, reduction, mixing, and\ndown-projection, are implemented in Loihi 2 using linear synapses in the neurocore alongside their\nrespective neuron instances.\nWe optimize how each layer is distributed across neurocores to achieve uniform compute load. We\nplace subsequent layers onto neighboring neurocores to reduce message passing time, leading to a\nusage of 31 and 111 neurocores of a single Loihi 2 chip for the small and large model, respectively."}, {"title": "Post Training Quantization and Quantization Aware Fine Tuning", "content": "The S4D models are first trained in full precision in convolution mode. Since Loihi 2 only supports\nfixed precision computation, fake quantization is applied after the initial training. In this post-"}, {"title": "Reference evaluation on Jetson Orin Nano", "content": "As a reference, we implement both the convolution and the recurrent formulation of our S4D variant\non Jetson. Due to lack of complex tensor support in TensorRT, the evaluations are performed using a\nPyTorch just-in-time compiled model using fp32 precision. The power measurement reported by the\njtop API is used for characterization. For runtime, only the time spent to input the data and compute\nthe output was considered. The primary point of comparison for Loihi 2 vs Jetson implementation is\nthe streaming (batch=1) mode of inference. In addition to the streaming mode of inference, the peak-\nperforming batched mode of inference is also included for a representative optimum performance\non Jetson."}, {"title": "Results", "content": "We evaluate our S4D model running on Intel Loihi 2 and Nvidia's Jetson on the datasets sequential\nMNIST (sMNIST), permuted sequential MNIST (psMNIST), and sequential CIFAR10 (sCIFAR)."}, {"title": "Accuracy and parameter count", "content": "shows the accuracy on sMNIST, psMNIST, and sCIFAR of our S4D model in full precision,\nafter quantization, and on Loihi 2 in comparison to other models.\nAlthough we use a simplified version of the S4D model, by only using ReLU activations and no\nnormalization (see Section 3.1), the performance in full precision meets that of the original and\nmore complex S4 and S4D model; only on the sCIFAR dataset, the accuracy of our model is lower"}, {"title": "Computational cost", "content": "reports computational cost of inference on Loihi 2 and Jetson on the three datasets. In\naddition to energy, latency, and throughput, the energy-delay product (EDP) [28] is reported to\ncompare systems running at different speed. For these measurements on Loihi 2, we store a sequence\nof input values on a neurocore and inject them to the S4D network at peak capacity without IO\nconstraints to obtain stable power measurements; this is repeated for 10 representative samples. The\nenergy of the neurocore used to store input values is included in the results for Loihi 2, while the IO\npower of Jetson is excluded.\nSample-by-sample processing The right side of  shows results from sample-by-sample\nprocessing, which assumes that all tokens of a sample are available to the system at the beginning of\nprocessing and only a single classification is required for the whole sample. Recurrent formulations\nof the model have to process each token sequentially, while convolutional formulations can process\nthe entire sample with a single convolution.\nIn the online processing regime of batch=1, it is evident that Loihi 2 is very efficient compared\nto the recurrent mode on Jetson and shows better energy per sample of 1.8 mJ compared to 23 mJ\nfor Jetson in the convolutional mode that is favorable for GPU architectures. The throughput and\nlatency between Loihi 2 and Jetson in convolutional mode are also competitive. Jetson, however,\nachieves peak performance in higher batch mode with substantially reduced energy per sample of\n0.22 mJ and 0.96 mJ for sMNIST and sCIFAR along with orders of magnitude higher throughput,\nwhich is expected for GPU architectures."}, {"title": "Token-by-token processing", "content": "The middle columns of  show results of token-by-token pro-\ncessing. This assumes streaming input that requires a classification for every token. For these types\nof tasks, both Loihi 2 and the recurrent mode on Jetson process all tokens sequentially. The convo-\nlution mode on Jetson, however, has to perform a convolution over the entire sequence with every\nnew token. Its latency, energy, throughput, and EDP are therefore the same for processing one to-\nken in token-by-token processing as they are for processing one sample in the sample-by-sample\nprocessing mode when using the convolutional implementation.\nFor token-by-token processing, Loihi 2 outperforms Jetson in all metrics on all datasets. Latency and\nenergy are two to three orders of magnitude lower for Loihi 2. This is reflected in EDP: For MNIST\nworkloads, EDP for Loihi 2 is 0.0002 \u00b5Js and 0.001 \u00b5Js for sCIFAR. In contrast, the recurrent\nprocessing on Jetson incurs EDP of 70 \u00b5J s and 80 \u00b5J s on the respective datasets, demonstrating the\nefficiency of Loihi 2 in token-by-token processing."}, {"title": "Fall-through vs. pipelined processing", "content": "shows the tradeoff between latency and throughput\nwhen executing the model on Loihi 2 in fall-through or pipelined processing (see Section 2.2). With\nfall-through processing, we see a latency of 68 us per token on the sMNIST dataset, compared\nto 168 us in pipelined processing. This comes at the cost of throughput of only 14.705 token/s,\ncompared to 83.343 token/s in pipelined processing. The lower throughput per token in fall-through\nmode results in a higher latency per sample of 53.314 ms compared to the pipelined mode with\n9.57 ms. Results on the other datasets highlight the same tradeoff."}, {"title": "Discussion", "content": "The results in Section 4 show that the performance of S4D on Loihi 2 in comparison to edge GPU\nsystems such as the Jetson depends on the specific use-case.\nThe S4D model on Loihi 2 excels particularly in use-cases in which an incoming data stream must be\nprocessed on a token-by-token basis as quickly as possible. In those use-cases, Loihi 2 can leverage\nits compute and memory co-located architecture to outperform Jetson. On sCIFAR, the largest\nworkload we measured, Loihi 2 consumes 1000 times less energy with a 75 times lower latency and\na 75 times higher throughput compared to the recurrent implementation of S4D on Jetson.\nRunning S4D on Loihi 2 is not competitive with GPU systems for use-cases that involve the offline\nprocessing of large amounts of data in parallel. With offline processing, the data is readily available\nand multiple batches can be processed in parallel. In these cases, the Jetson can fully realize its\npotential by executing S4D in convolutional mode and outperform Loihi 2 in throughput, energy,\nand latency.\nIt should be noted that our implementation of S4D on Jetson is not completely optimized for speed\nand efficiency due to the lack of support in TensorRT for complex numbers (see Section 3.3). This\nwill be addressed in future work. We do not expect it to change the qualitative findings presented in\nthis paper.\nThe accuracy of S4D on Loihi 2 could be improved further by applying QAFT for more than one\nepoch. Direct extensions of S4 (e.g., Liquid-S4 [10], S5 [9]) have shown state-of-the-art perfor-\nmance on sequence modeling tasks; they are compatible with Loihi 2 and could be evaluated in\nfuture work.\nThe balance of latency, energy, and throughput can be further optimized on Loihi 2 for specific\nuse cases. There is a continuum between fall-through and pipelined processing, not explored here.\nIt enables balancing latency and throughput by introducing just enough pipelining such that the\nthroughput matches the sampling rate of the input sequence. It is furthermore possible to implement\nthe convolutional mode of S4D on Loihi 2. This would enable parallel processing of a configurable\nnumber of tokens per convolution\u2014another possible way to balance throughput against latency and\nenergy.\nUltimately, this work and possible optimizations should now be applied and tested in real-world\nstreaming use-cases, such as keyword-spotting, audio denoising, drone control, and other latency or\nenergy constrained domains."}]}