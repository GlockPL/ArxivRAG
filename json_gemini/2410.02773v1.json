{"title": "Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between Model Predictions and Human Responses in VQA", "authors": ["Jian Lan", "Diego Frassinelli", "Barbara Plank"], "abstract": "Large vision-language models frequently struggle to accurately predict responses provided by multiple human annotators, particularly when those responses exhibit human uncertainty. In this study, we focus on the Visual Question Answering (VQA) task, and we comprehensively evaluate how well the state-of-the-art vision-language models correlate with the distribution of human responses. To do so, we categorize our samples based on their levels (low, medium, high) of human uncertainty in disagreement (HUD) and employ not only accuracy but also three new human-correlated metrics in VQA, to investigate the impact of HUD. To better align models with humans, we also verify the effect of common calibration and human calibration (Baan et al. 2022). Our results show that even BEiT3, currently the best model for this task, struggles to capture the multi-label distribution inherent in diverse human responses. Additionally, we observe that the commonly used accuracy-oriented calibration technique adversely affects BEiT3's ability to capture HUD, further widening the gap between model predictions and human distributions. In contrast, we show the benefits of calibrating models towards human distributions for VQA, better aligning model confidence with human uncertainty. Our findings highlight that for VQA, the consistent alignment between human responses and model predictions is understudied and should become the next crucial target of future studies.", "sections": [{"title": "Introduction", "content": "Large vision-language models should exhibit reliable model confidence so that humans can trust their outputs. Regardless of high model accuracy, poor confidence would indicate that a model does not know when it does not know (Baan et al. 2022; Pavlick and Kwiatkowski 2019; Yang, Yoo, and Lee 2024). Addressing the importance of model confidence has drawn increasing attention in recent years (Naeini, Cooper, and Hauskrecht 2015; Guo et al. 2017), aiming to make models more reliable and capable of mirroring humans' uncertainty (Collins et al. 2023; Ilia and Aziz 2024; Peterson et al. 2019), especially for cases where human disagreement exists. We here refer to human uncertainty in human disagreement (HUD) as a two-part observation: a) the fact that for each sample, different individuals might have different knowledge, and thus it may lead to different (valid) responses (also known as human label variation (Plank 2022)); we here complement that with a second aspect, namely b) that the same humans may also have their own degrees of uncertainty in responding. HUD is a huge challenge that makes it difficult for models to align with humans (Peterson et al. 2019), as many samples may not have a single ground truth with complete human agreement and high confidence. This issue is particularly pronounced in the visual question answering (VQA) task, where humans are observed to frequently have different opinions, and are easy to be uncertain with their answers. Figure 1 shows an example of the task and the responses generated by the human annotators (R). It also provides the confidence labels (C) which express human uncertainty, where each annotator indicates their level of confidence in the given response. In the example, the two responses in (a) and (b) are the same (\"blue and gray\") but with different human confidence labels (1.0 vs. 0.5), while those in (c) and (d) have different response labels with the same human confidence labels. Traditionally, a model would predict the response with the highest model prediction probability (e.g., \"blue and gray\") but it would ignore the effect that human confidence plays on the prediction distribution. It will also miss the existence of multiple valid responses to the same question. For example, the answer \"blue\" is less probable in the model prediction, but in reality it has a high human confidence and is indeed a highly possible answer. Therefore, training a model to predict the most probable correct answer, while ignoring HUD, does not give models the ability to accurately reflect human behavior and distributions in real-world scenarios. It also makes models struggle to assign reliable probabilities to other valid answers (\u201cblue\u201d being 0.22). Most of the previous VQA studies have not explicitly addressed HUD, and have predominantly evaluated models based on accuracy of a single prediction of each sample alone.\nThis study examines how well state-of-the-art VQA models align with human confidence distributions when HUD exists. Unlike previous work on VQA, we propose to explicitly utilize HUD information to evaluate the discrepancies between model predictions and human responses. Specifically, we use VQA 2.0 (Goyal et al. 2017), a dataset where ten annotators answered each question and also indicated their confidence level in their answer. In this study we aim to answer the following research questions: RQ1: To what extent do different levels of HUD impact the accuracy of the model and its alignment with human confidence? RQ2: Does calibration improve the human-model alignment?\nTo answer the research questions, we start by adopting a splitting strategy, categorizing samples into three sets (low, medium, high) based on their levels of HUD. We then employ not only traditional accuracy but also propose to use three human-correlated metrics for VQA: Total Variation Distance (TVD), Kullback-Leibler divergence (KL), and Human Entropy Calibration Error (EntCE), to comprehensively analyze the discrepancies between models and human in the context of HUD. Furthermore, we apply Temperature Scaling (Guo et al. 2017) to verify the effectiveness of the traditional calibration method.\nOur results show that even state-of-the-art models such as BEiT3 (Wang et al. 2023) struggle to capture the multi-label distribution due to diverse human responses. Additionally, we observe that commonly used accuracy-oriented calibration techniques adversely affect BEiT3's confidence, further widening the gap between model prediction distributions and human confidence distributions. Instead, we propose to calibrate models towards human distributions, and thus better align models with humans under HUD. Our key contributions are: 1) To the best of our knowledge, we are the first to study human uncertainty in disagreement in VQA, investigating the gap between the SOTA model prediction distributions and human response distributions; 2) We are the first to implement a novel comprehensive evaluation of VQA models, evaluating on various human-centered metrics rather than solely relying on the VQA accuracy. We demonstrate that, when intrinsic HUD exists, the VQA accuracy can not adequately indicate a model's capacities; finally, 3) We demonstrate that the variations in HUD levels affect model performances differently. We therefore advocate for an increased future attention on improving the alignment between models and humans under HUD."}, {"title": "Related Work", "content": "Human Disagreement: Human disagreement, also known as human label variation (Plank 2022), has been studied in various natural language processing, computer vision, and human-computer interaction tasks, including natural language inference (Baan et al. 2022), machine translation (Popovi\u0107 2021), question answering (Kwiatkowski et al. 2019), image classification (Peterson et al. 2019), and social computing (Gordon et al. 2021). All these studies are based on datasets where multiple human annotators give different plausible answers for each sample (Nie, Zhou, and Bansal 2020). They then either aggregate human labels, using majority vote for training, or using human soft labels (Peterson et al. 2019), optimizing models towards a human distribution based on label frequency. Human uncertainty here means uncertainty reflected by annotators choosing different labels, and is different from annotators providing indicators of uncertainty in their response. Human uncertainty in responses so far is an understudied phenomenon in relation to human label variation.\nVisual Question Answering: The VQA task was first proposed by Antol et al. (2015), where they provide a benchmark including the VQA 1.0 dataset and the tradition evaluation metric VQA-Accuracy (VQA-Acc). VQA 2.0 (Goyal et al. 2017) is based on VQA 1.0, where commonsense questions are removed with the aim to force the model to uniquely rely on the image content for answering.\nFor this task, large pre-trained vision-language models have become mainstream (Tan and Bansal 2019; Li et al. 2022; Bao et al. 2022; Wang et al. 2023) and have been further improved by optimizing for VQA-Accuracy. Among them, LXMERT is a comparatively light model which provides the first baseline in the era of pre-trained models. It yields a VQA-Accuracy of 72.5 on VQA 2.0 test set. BEIT3 (Wang et al. 2023) is the latest and strongest SOTA model with a VQA-Accuracy of 84.03. In VQA, human disagreement is studied implicitly by Jolly, Pezzelle, and Nabi (2021), where they propose a diagnose tool to identify a sample's difficulty level by calculating the numbers of different response labels among human annotators and clustering the semantics of responses based on Word2Vec (Mikolov et al. 2018). However, to the best of our knowledge, human uncertainty in responses has not been yet studied in VQA, where most work only focuses on VQA-Accuracy as the sole evaluation metric.\nUncertainty in VQA: VQA is a complex vision-language task and, for this reason, it is normal that a human annotator might be uncertain in providing a label. The level of uncertainty in the responses indicates that the annotators are not completely sure about a given label. Unfortunately, only"}, {"title": "Human Uncertainty in Disagreement", "content": "We first introduce the VQA 2.0 samples used in our study, and then discuss how we calculate the HUD scores and group the samples in three HUD levels based on their scores."}, {"title": "Data", "content": "In this study we use VQA 2.0 (Goyal et al. 2017), one of the most commonly used VQA dataset. Given that both BEiT3 (Wang et al. 2023) and LXMERT (Tan and Bansal 2019) are pre-trained and fine-tuned only on VQA 2.0, this is the perfect dataset for a fair comparison between these two models.\nAs shown in Figure 1, given an image-question-answer triplet t = (i, q, A), where i is an image, q is a question, and A is an answer set, A consists of 10 independent humans' annotations. In each annotation $h_n$, n = 1,..., 10, every annotator gives their response $r_n$ and also a confidence level $c_n$. The level $c_n$ corresponds to one of three pre-defined categories < 'yes', 'no', \u2018maybe' >, indicating whether an annotator is confident in their answer."}, {"title": "The HUD Score", "content": "Since the confidence labels 'yes', 'no', and 'maybe' are expressed in natural language, they can not be directly utilized for comparisons with model prediction distributions. To quantify human uncertainty, we assign different human confidence scores to each response based on its confidence label, as shown in Figure 1. We quantify every 'yes' as 1.0, 'no' as 0.01, and 'maybe' as 0.5 respectively. We want the values of the three categories to be restricted between 0 and 1, as this range aligns with the distribution of the model's predicted probabilities. Additionally, in this work we assume that the value of 'maybe' is the average score of 'no' and 'yes', ensuring it does not bias towards either end. We assign a very small value to 'no' to avoid the situation of a distribution of all zeros for which entropy cannot be calculated.\nFor the same response label with multiple annotations, for example, in Figure 1 there are two people both answering 'blue and gray', we calculate their average confidence scores as the human confidence score for this answer, and thus we get the human uncertainty scores for each response label. We finally calculate the average across all human response labels as the HUD score for this sample."}, {"title": "Grouping samples based on HUD Levels", "content": "Our hypothesis is that a model would perform differently according to the different HUD level of the samples. For this reason, we divide the samples into three equal portions according to their HUD scores, namely low human uncertainty set (we use \u2018low' set in short for the rest of the paper), medium human uncertainty set (medium set), and high human uncertainty set (high set), where humans are more certain about the samples in low set, while more uncertain in the high set."}, {"title": "Model Confidence Scores and Calibration", "content": "Here we discuss how we obtain the model prediction distributions and how to better align them with humans."}, {"title": "Model Prediction Distribution", "content": "Given a sample x, the prediction distribution of a model M, denoted as M(x), corresponds to the probabilities $P_y(Y = y | X = x)$ the model assigns to each class y. The most standard and most commonly used method to gain $P_y$ is to extract the last hidden state of the neural network for each label, and use the softmax function, which converts a vector of raw scores (logits), into a probability distribution over multiple classes to get the final normalized probability distribution: $P_y = \\text{softmax}([l_1, l_2, ..., l_k])$, where K is the number of all labels."}, {"title": "Calibration Method", "content": "Calibration aims to adjust the predicted probabilities $P_y$ towards a more reliable distribution (Guo et al. 2017; Desai and Durrett 2020; Jiang et al. 2021). A well calibrated multi-class model accurately captures the true likelihood of predictions for all possible classes (Baan et al. 2022; Vaicenavicius et al. 2019; Kull et al. 2019). For instance, in VQA, if there are four possibly correct answers for a question and the humans' uncertainty scores for them are: [0.60, 0.30, 0.05, 0.05] respectively, a well calibrated model M should also have a prediction distribution approximately close to [0.60, 0.30, 0.05, 0.05] for same four labels.\nAmong different calibration techniques, Temperature Scaling (Guo et al. 2017) is one of the most widely used and most traditional methods. Temperature Scaling is a post-processing technique applied to the logits of a network by dividing them by a temperature parameter T > 0. Then we can use the modified logits and the softmax function to get a calibrated distribution: $P_y = \\frac{e^{(l_y/T)}}{\\sum_{j=1}^{K}e^{(l_j/T)}}$. In this work, we employ Temperature Scaling to test if this traditional calibration method is effective in our experiments, whether it is more effective for either of the two models, and indicate what a good calibration strategy is."}, {"title": "Experiments", "content": "Models: Our study focuses on the latest SOTA model BEIT3 (Wang et al. 2023) and the previous SOTA, also one of the most commonly used models LXMERT (Tan and Bansal 2019). We target on evaluating their differences on different HUD sets.\nFine-Tuning Set and Implementation Details: The open-sourced training set for VQA 2.0 includes 443,757 samples, and the validation set has 213,954 samples. The test set is not open-sourced and the human uncertainty labels are not available. Therefore, we exclude the test set from our experiments. We follow exactly the same implementation details provided by BEiT3\u00b9 and LXMERT\u00b2, using their checkpoints (for BEiT3, we use the BEiT3-base model) of the pre-trained models and fine-tune them following the original instructions. More implementation details are provided in the Appendix A. It is essential to point out that BEiT3 and LXMERT utilize slightly different sets for fine-tuning and validation. They partitioned the original validation set in a customized manner, where BEiT3 keeps 5,303 samples from the original validation set and add the remaining set together with the training set for fine-tuning, while LXMERT keeps 25,994 samples from it and do the same. Even though this setting is a bit different, in previous works they do not strictly restrict this when reporting and comparing model performances. Therefore, we also keep the original setting as initially proposed.\nValidation Set Details: We use BEiT3's 5,303 samples, and LXMERT's 25,994 samples for validation, since neither of models have been fine-tuned on these samples before. We also filter out all the samples with only one human response label (no disagreement). These samples do not have a human confidence distribution, and thus their entropy can not be computed.\nWe are left with a set of 3,248 samples for BEiT3, where there are 1,083, 1,083, 1,082 samples for the low, medium, and high set respectively. Similarly, we have a set of 15,408 samples for LXMERT, where there are 5,136 samples for each HUD set.\nFor BEiT3, the average number of distinct human responses per sample in low set is 3.06, with an average HUD score of 0.98. In the medium set, these values are 3.97 and 0.86, respectively. While in the high set, they are 4.82 and 0.64, respectively. Similarly, for LXMERT, for low, medium, and high set, the values are: 3.08 and 0.98, 4.04 and 0.86, 4.67 and 0.64, respectively. Figure 2 shows two very similar distributions of HUD scores for the two validation sets, where they have the same mean value and standard variation value (std). The two black lines in both figures show the split boundaries of the low, medium, and high sets. We point out that even though our HUD set split strategy causes seemingly high confidence scores on each set, we target on revealing the differences in model performances caused by three sets, where humans have comparatively high, medium, and low uncertainty. Also, since the data distributions are"}, {"title": "Evaluation metrics", "content": "To conduct our evaluation, we include the following metrics. The specific equations are in Appendix A.\nVQA-Accuracy: Given a model prediction, VQA-Accuracy (Antol et al. 2015) is defined as:\n$\\text{Acc(ans)} = \\min\\left\\{\\frac{\\text{#humans that said ans}}{3}, 1\\right\\}$.\nVQA-Accuracy takes into account human label frequency, assigning higher scores to answers annotated by a greater number of human annotators. It is maximized (1.0), if at least 3 raters gave the exact answer. The number 3 is a manually set parameter proposed in the original work (Antol et al. 2015)."}, {"title": "Results and Discussion", "content": "Before Calibration. Table 1 reports the performances of the two models (without calibration) across three different levels of human uncertainty, as well as for the overall evaluation set. Performance metrics include VQA-Accuracy, TVD, KL, and EntCE. Overall, we observe that BEiT3 consistently outperforms LXMERT in accuracy (0.67 vs. 0.84), thereby confirming that it is the better model in terms of accuracy. When zooming into the three HUD levels, both models show a similar trend: as human uncertainty level increases from low to high, their accuracy decreases. The models obtain the highest accuracy level for the stimuli in the low HUD set, as humans provide answers with high confidence to the stimuli in that category. The continuous decline in both models' VQA-Accuracy performance supports our hypothesis that the higher the HUD level is (more uncertainty), the more challenging it is for a model to predict the best answer.\nBesides accuracy, we look at model confidence and compare it against human label variation (which is now across raters). We provide models' performances on TVD, KL, and EntCE to reveal the extent to which the model prediction distributions approximate humans'. When we compare the two models, both on the overall set and on the three subsets, the results on all three metrics show that BEiT3 correlates better with humans than LXMERT. For both models, we surprisingly find they achieve their best performances on high set (high human uncertainty). We provide an in-depth analysis as follows.\nAs introduced in the validation set details, on average for both models, there are around 3.0, 4.0, and 4.8 different human response labels for a sample in low, medium, and high set, with 0.98, 0.86, 0.64 HUD scores respectively. This indicates that the more uncertain humans are, the higher is the human disagreement. We believe the reason both models have the highest scores on the low set is that the samples have comparatively fewer human response labels, and thus it is easier for a model to learn a 'seemingly most correct answer' but ignore the other possibilities. This is also why models are having higher accuracy on the low set; it learns to predict the most correct answer which is supported by the human majority vote, but it does not learn to align well with the overall human answer distribution, i.e., on all the other possible answers. On the contrary, for samples from the high human uncertainty set, there are comparatively more response labels with a smoother confidence distribution (less abrupt variations between the confidence scores on each label), where it helps the models to pay attention to possible labels rather than focusing on one single label. We further showcase and analyze this in the Case Study.\nCalibration towards VQA-Accuracy Table 2 shows the effectiveness of Temperature Scaling. The effect of TS is traditionally evaluated by Expected Calibration Error (ECE) (Guo et al. 2017), which measures the absolute difference between the accuracy of the predictions of a model and its confidence towards these predictions. This evaluation quantifies how well the model's predicted probabilities are calibrated towards its accuracy. In Table 2 we report the ECE results before and after TS. We empirically set the Temperature t by comparing the t from 0.1 to 2.0 with an interval of 0.05. We set t to 1.05 for LXMERT and 0.6 for BEIT3, where they each have the lowest ECE on the overall set after TS. We then fix t and report the results for TVD, KL, and EntCE. As expected, the ECE scores of all sets drop after calibration. However, using TS towards VQA-Accuracy undermines BEiT3's confidence (indicated by the minus symbol\u2018(-)'), where TVD, KL, and EntCE scores become higher e.g., from 1.701 to 3.232 on KL for BEiT3. On the contrary, calibrating towards VQA-Accuracy only helps LXMERT improve the correlation with the three human distributions. We conclude that the traditional calibration technique still works for slightly weaker models like LXMERT, but they do not help the latest strong model BEiT3. Besides, here we do not further compare the ECE results between different sets or between models, but only use ECE to test if using TS to calibrate towards VQA-Accuracy works and how strongly this technique influences the models' correlation with human. ECE does not consider human disagreement and distributions, and thus we do not further analyze ECE results."}, {"title": "Calibration towards human distributions", "content": "A crucial aspect of this work involves using TS as a calibration method to align with human distributions by reducing scores on TVD, KL, and EntCE. Similar to what we introduced in the previous paragraph, we empirically set the temperature t to be 2.0 for both models, as it helps model reach better results. Normally, t is not supposed to be a large value since a too high value makes the predicted probabilities become closer to a uniform distribution, reducing the differences between the classes, which is not desired. As shown in Table 2 on the right-hand side, on the overall set and also three subsets, calibrating towards human distributions rather than VQA-Accuracy helps both models reach lower scores (better results) on TVD (e.g. from 0.448 to 0.354 on LXMERT, from 0.436 to 0.340 on BEiT3), KL (e.g. from 1.886 to 0.731 on LXMERT, from 1.701 to 0.647 on BEiT3), and EntCE (e.g. from 0.555 to 0.353 on EntCE, from 0.541 to 0.333 on BEIT3) compared with the original results in Table 1. Notably, TVD and EntCE measure the average discrepancies between two distributions (on each element in the vector), but are not sensitive to element class rankings. KL is sensitive to class rankings, where small changes in probabilities can lead to large increases in the divergence. Therefore, we observe a drastic decrease in KL.\nOn each set split, BEiT3 still outperforms LXMERT on all three metrics, while both models reach the best performances on the high set, and the worst performances on low set. We conclude that, even though BEiT3 has a seemingly strong performance on all metrics, it is not yet correlated well with humans, especially on the low HUD set. Moreover, by calibrating towards human distributions, it even yields very similar human correlation performances when compared with LXMERT. This indicates that BEiT3 is simply better optimized towards human majority vote based on label frequency, but it is not much better at learning true human distribution. In other words, while BEiT3 is the better model in terms of accuracy compared to LXMERT, BEiT3 is less well-aligned to human preferences. This opens up for interesting future research directions on how to strike a good balance between training a high-accuracy model and one that is well aligned with humans."}, {"title": "Case Study", "content": "In Figure 3 we showcase two samples from the low (A(1)-A(3)) and high (B(1)-(B3)) sets and compare models' specific predictions against humans'. In line with what we discussed in the last section, on the low set (see A(1)), the model approximates only towards the most correct answer 'green', which has the highest VQA-Accuracy. However, besides 'green', in the human distribution, 'yellow' and 'blue' also have similar weights. This means the model should predict all three answers with similar distributions to mirror humans' confidence in the real world. However, both BEiT3 and LXMERT show an extremely high probability for 'green', and very low probabilities for 'yellow' and 'blue'. When calibrating towards VQA-Accuracy, in A(2) we see a very slight improvement in LXMERT. Once again, this traditional calibration method negatively affects the performance of BEiT3. On the contrary, when calibrating towards humans (see A(3)), there is a continuous improvement in both models, even though they are still far from human performance.On the right side (see B(1)), the sample from the high set contains more human response labels, each with lower but similar VQA-Accuracy scores. In this case, BEiT3 distributes the probability mass across most of the labels provided without the huge gap seen in A(1) (0.96 vs 0.01). Although LXMERT also assigns probability scores to multiple labels, it strongly favors one label ('opponent') with a probability much higher than the others. In B(2), calibrating towards VQA-Accuracy causes tiny changes on the labels 'girl' and 'opponent', while it again does harm BEiT3, causing much higher values for 'women', while decreases the probabilities for 'opponent' and 'man'. On the contrary, calibrating towards humans work well for both models, making them both correlate better with humans.\nFinally, by comparing the results in A(3) and in B(3) we see how even when calibrating towards humans, the models still predict one specific label (e.g., 'green') while mostly ignoring the other candidates (e.g., 'yellow' and 'blue'). This"}, {"title": "Conclusion", "content": "In this work, we study human uncertainty in Visual Question Answering. We find that when humans exhibit high levels of uncertainty in disagreement, the models have difficulty in predicting the most correct answer, but are actually better at correlating with human confidence distributions. We further find that the traditional calibration strategy towards accuracy does not work for BEiT3, the latest SOTA model. Instead, we then demonstrate that calibrating it towards human distribution is more effective. We conclude that evaluating VQA models uniquely on the VQA-Accuracy metric is not sufficient, and future studies should focus more on aligning models with human uncertainty and disagreement."}, {"title": "Limitation", "content": "Our study has three limitations. Firstly, while being highly reproducible, our study is restricted to two slightly different validation sets, even though we believe they are much comparable. Future study could extend our study to fine-tuning and validating models on more datasets. Secondly, it is not feasible to explore all the strategies to quantify human uncertainty labels. Although our study provides a very standard solution, this represents a good start point; we advocate for future research to explore more advanced quantification strategies. Thirdly, in our work we use Temperature Scaling; future studies should consider other calibration techniques to directly target human uncertainty in disagreement."}, {"title": "Ethics Statement", "content": "We anticipate no ethical concerns with this work. We utilized open-sourced data and models, which have been appropriately cited."}, {"title": "Appendix", "content": "A. Evaluation Metrics\nBesides the description in the main text, we provide the detailed equation of metrics used in our work.\nThe Total Variation Distance (Devroye and Lugosi 2001) between two probability distributions P and Q is defined as:\n$TVD(P,Q) = \\frac{1}{2} \\sum_{x \\in X} |P(x) - Q(x)|$. (2)\nThe TVD is robust to tiny changes in probabilities, and reflects the overall absolute differences between two distributions.\nThe Kullback-Leibler Divergence (Kullback and Leibler 1951) between two probability distributions Pand Qis defined as:\n$D_{KL}(P || Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$. (3)\nNotably, the $D_{KL}(P || Q)$ is not equal to $D_{KL}(Q || P)$. KL divergence is sensitive to small changes in the distribution, where the former distribution P in $D_{KL}(P || Q)$ is a given true distribution and the Q is an approximation."}, {"title": "The Human Entropy Calibration Error", "content": "(Baan et al. 2022) between two distributions is defined as:\n$EntCE(x) = H(f(x)) - H(\\bar{f}(x))$, (4)\nwhere f(x) is the model prediction distribution, and $\\bar{f}(x)$ is the human confidence distribution. H(\u00b7) is the entropy of a distribution. It measures the alignment between humans and a model's 'indecisiveness' (Baan et al. 2022).\nThe Expected Calibration Error (ECE) (Guo et al. 2017) is defined as:\n$ECE = \\sum_{m=1}^{M} \\frac{||B_m||}{N} |\\text{VQA-Accuracy}(B_m) - conf(B_m)|$, (5)\nwhere M is the number of bins, N is the total number of samples, and VQA-Accuracy(\u00b7) and conf(\u00b7) is model's accuracy and its probability on its prediction. The ECE divide the samples into different bins, and calculate the difference between model's accuracy and confidence according to these bins."}, {"title": "Reproducibility", "content": "Our experiments and results are highly reproducible. All experiments are implemented on four NVIDIA A100-SXM4-80G GPUs and an AMD EPYC 7763 64-Core CPU. We report the training hyper-parameters in Table 3. BEiT3 and LXMERT have different settings to reach the best performances. Our selected dataset VQA 2.0 is open-sourced with a standard training and validation set. Based on the dataset, the fine-tuning details and instructions are clearly stated in the original work (Tan and Bansal 2019; Wang et al. 2023), and model structures (#layers, #heads) are fixed. Our evaluation metrics are also standard and easy for implementation."}]}