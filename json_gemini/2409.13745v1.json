{"title": "Context-Aware Membership Inference Attacks against Pre-trained Large Language Models", "authors": ["Hongyan Chang", "Ali Shahin Shamsabadi", "Kleomenis Katevas", "Hamed Haddadi", "Reza Shokri"], "abstract": "Prior Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs), adapted from classification model attacks, fail due to ignoring the generative process of LLMs across token sequences. In this paper, we present a novel attack that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior loss-based approaches, revealing context-dependent memorization patterns in pre-trained LLMs.", "sections": [{"title": "Introduction", "content": "To assess memorization and information leakage in models, Membership Inference Attacks (MIAs) aim to determine if a data point was part of a model's training set [1]. However, MIAs designed for pre-trained Large Language Models (LLMs) have been largely ineffective [2, 3].\nThis is primarily because these MIAs, originally developed for classification models, fail to account for the sequential nature of LLMs. Unlike classification models, which produce a single prediction, LLMs generate text token-by-token, adjusting each prediction based on the context of preceding tokens (i.e., prefix). Prior MIAs overlook token-level loss dynamics and the influence of prefixes on next-token predictability, which contributes to memorization. This simplification misses critical LLM behaviors, notably context-dependent memorization, making these attacks ineffective at identifying training set members.\nAdditionally, state-of-the-art MIA tests [4-7] rely on reference models trained similarly to the target model but on distinct, similarly distributed data. Achieving this is extremely costly and often impractical for pre-trained LLMs. Also, arbitrarily using other available pre-trained models as reference models leads to inaccurate attacks due to significant differences in their training processes and model architectures (as reasoned analytically [8], and shown empirically in this paper and the prior work [2]).\nTo design a strong MIA against LLMs, we need to fully understand how and why memorization may occur during the training. Any piece of text is modeled as a sequence of tokens, and LLMs are trained to optimize parameters that maximize the conditional probabilities of generating each token based on the preceding context. This process is progressive, as the model adjusts its predictions with each new token, refining its understanding of the sequence.\nA key point we highlight in this paper is that memorization can occur at any stage of sub-sequence generation, particularly when the model encounters prefixes (contexts) that are"}, {"title": "Problem formulation", "content": ""}, {"title": "LLM pre-training", "content": "Let $\\mathcal{M}$ be an auto-regressive LLM. Let $\\text{PrivSet} = {X_i}_{i=1}^N$ be the private training set consisting of $N$ piece of texts $X_i$. $\\mathcal{M}$ first performs tokenization i.e., splitting the input text into $T$ individual units of text called tokens. This is done through a vocabulary $V = [v_1, ..., v_K]^T$ consisting of all ($K$) $d$-dimensional token embeddings $v_k \\in \\mathbb{R}^d$ that $\\mathcal{M}$ knows. For example, the Pythia model [9] splits the input text $X_1 = $ \"Membership inference attack\" into $T = 4$ tokens $\\mathbb{X} = [x_1 x_2 x_3 x_4]^T$ based on the defined token-number mapping in its vocabulary set: $x_1 =$ \"Members\", $x_2 =$ \"hip\", $x_3 =$ \" inference\", and $x_4 =$ \" attacks\", where $x_t \\in V$ for all $t\\in T$. The input texts have different lengths. Let $x_{<t} = {x_1,...,x_{t-1}}$ be a prefix containing all initial $t - 1$ tokens starting from the first token and end at the second to the last token. The auto-regressive $\\mathcal{M}$ is trained to minimize the loss of predicting the next token of each prefix when only seeing the prefix. In particular, for each $X_i$, $T$ (input, output) pairs are created as $(x_{<2},x_2), (x_{<3}, x_3), ..., (x_{<T},x_T)$. At each time step $t$, $\\mathcal{M}$ receives the prefix $x_{<t} = {x_1,...,x_{t-1}}$ and outputs a probability distribution over the vocabulary set $V$, denoted as $P(x|x_{<t};\\mathcal{M})$. The loss for predicting the next token $x_t$ given the prefix $x_{<t} = {x_1,...,x_{t-1}}$, $L_t$, is computed as the cross-entropy between $P(x|x_{<t}; \\mathcal{M})$ and the distribution whose mass is concentrated on the true next token $x_t$ (i.e., the Dirac delta function/unit impulse located on that token [12]):\n$$L_t(x_t) = - \\sum_{x\\in V} \\delta(x - x_t) \\log P(x|x_{<t}; \\mathcal{M}) = - \\log P(x_t|x_{<t}; \\mathcal{M}).$$\n$$\\delta(x - x_t)$ is the Dirac delta function, which is 1 when $x = x_t$ and 0 otherwise. Then, the total loss, $\\mathcal{L}$, for the given example $X_i$ is defined as the average loss over all $t\\in T$:\n$$\\mathcal{L}(X_i;\\mathcal{M}) = \\frac{1}{T} \\sum_{t=1}^T L_t(x_t) = -\\frac{1}{T} \\sum_{t=1}^T \\log P(x_t|x_{<t}; \\mathcal{M}).$$"}, {"title": "Threat model", "content": "We consider a service provider (e.g., a company or a hospital) that wants to (pre-)train the Large Language Model (LLM) $\\mathcal{M}$ on their unlabeled private data [2], PrivSet (as described above) and provides LLM-based services to their users. Providing such services may risk the privacy of PrivSet as $\\mathcal{M}$ unintentionally leaks information about the training data due to the data memorization [1]. Analyzing this privacy risk can be studied with various attacks [1, 13, 14, 6, 5, 15]. In our work, we focus on Membership Inference Attacks (MIAs). MIAs aim to determine whether a target"}, {"title": "MIAs against pre-trained LLMs", "content": "MIAs [13, 11, 18, 14, 14] define membership scoring function $f(X; \\mathcal{M})$ and compare its outputted score to a threshold $\\Gamma$ to determine the target input's membership. Loss-based attack [13] (LOSS) defines the membership scoring function based on the LLM's computed loss over all tokens, $f_{\\text{LOSS}}(X; \\mathcal{M}) = \\mathcal{L}(X; \\mathcal{M})$. Then, LOSS determines that $X$ is a member if the LLM's computed loss is below the threshold, $f_{\\text{Loss}}(X; \\mathcal{M}) < \\Gamma$. Recently, a few MIAs [11, 18, 19, 14] have been proposed in the literature to tailor the LOSS membership scoring function to pre-trained LLMs using calibration based on different techniques quantifying text complexities.\nMin-K% attack [11] uses only a subset of per-token loss $L_t$ based on the hypothesis that a non-member input text is most likely to have a few outlier tokens with high loss, while a member input text is less likely to have tokens with such high losses. Min-k% attack defines the membership scoring function based on the top k% of tokens with the lowest likelihoods as $f_{\\text{Min-K\\%}}(X; \\mathcal{M}) = \\frac{1}{\\vert \\text{min-k(X)}\\vert}\\sum_{x_t\\in \\text{min-k(X)}} L_t$. Min-K%++ attack [18] normalizes the loss for each token by the expectation $\\mu_{<t}$ and standard deviation $\\sigma_{<t}$ of the next token's $x_t$ log probability over the LLM's vocabulary $V$ given its prefix $x_{<t}$ as $\\tilde{L_t}(x_t) = \\frac{L_t(x_t)-\\mu_{<t}}{\\sigma_{<t}}$. Min-K%++ membership scoring function is $f_{\\text{Min-K\\%++}}(X; \\mathcal{M}) = \\frac{1}{\\vert \\text{min-K(X)}\\vert}\\sum_{x_t\\in \\text{min-k(X)}} \\tilde{L_t}(x_t)$.\nReference-based attack, Neighborhood attack and Zlib attack calibrate LOSS with respect to the intrinsic complexity of the input text using additional (reference) LLMs [14], additional queries to the target LLM [19] and entropy of the target input. Zlib attack [14] normalizes the LLM's computed loss with the zlib entropy of the target input [20], $f_{\\text{zlib}}(X; \\mathcal{M}) = \\mathcal{L}(X; \\mathcal{M})/zlib(X)$. Reference-based attack [14] defines the membership scoring function relative to a reference model"}, {"title": "Goal", "content": "MIMIR [2], which was presented at the 2024 Conference on Language Modeling, evaluated the performance of MIAs on pre-trained LLMs and demonstrated that \u201cthe performance across most MIAs and target domains is near-random\". This MIMIR benchmark highlights a need for designing better MIAs on pre-trained LLMs. Our goal is to design effective MIAs in this challenging setting by exploiting:\n\u2022 token-level complexities relative to its prefix. Prior MIAs compute input-level complexities failing to capture the fact that the complexity of predicting a specific token can vary a lot depending on its index (i.e., location within the input text) and the characteristics of its prefix.\n\u2022 per-token LLM's computed loss sequences. Prior MIAs use only LLM's computed loss in aggregation over all (or a subset) of tokens. This overlook of the loss dynamic misses the membership information encoded in the per-token loss sequences over time.\n\u2022 combined LLM's output and input text's semantics. Prior MIAs focus on each of these in isolation.\nTo achieve this goal, we design an MIA framework, called CAMIA. As shown in Figure 1, CAMIA 1) computes next-token loss sequences of the target input X based on black-box access to M; 2) extracts calibrated membership information; 3) compose the MIA tests based on calibrated membership information; and 4) decide whether X is a member or non-member based on the composed tests outcome.\nSection 3 introduces our proposed calibration technique, characterization of next-token loss dynamic followed by extracted membership information from the sequence of next-token losses. Section 4 describes ways to compose membership tests."}, {"title": "Our Calibration and Membership Information", "content": ""}, {"title": "Inherent text complexity", "content": "Prior MIAs for classification ML models exploit the model's tendency to overfit the training data. However, these loss-based approaches fail when applied to pre-trained LLMs due to a significant overlap in the LLM's computed loss histogram of members and non-members. A member sample may exhibit high loss if it is inherently complex (e.g., a piece of text containing multiple languages). This issue is more severe in pre-trained LLMs which are typically trained with only near-one epoch but on a massive amount of training data [2]. A non-member sample may have a very low loss simply because it is simple, containing repetitive and inherently predictable tokens.\nTherefore, it is important to calibrate LLM's computed loss based on input difficulties. Existing difficulty calibration techniques are not suitable for pre-trained LLMs. Calibration based on a reference model [14] requires i) access to data that is same-distribution but largely"}, {"title": "Next-token loss sequence", "content": "Recall from Section 2 that the input text $\\mathbb{X} = [x_1x_2....x_T]$ is a sequence of tokens and the model $\\mathcal{M}$ is trained to minimize each per-token loss given its prefix. To account for this loss dynamic, we define the next-token loss sequence of $X$ with respect to model $\\mathcal{M}$ as the sequence of loss on each token, i.e., $\\text{Seq}(X) = {L_1(x_1), L_2(x_2),......,L_T(x_T)}$. Then, we extract the membership information from the key characteristics of the loss sequence: the period of interest (which tokens should be considered?); the convergence speed (how fast the per-token loss $L_t(x_t)$ decreases as $t$ increases?); and the fluctuation rate (how regular is the per-token loss over time?)."}, {"title": "Filtering the loss sequence", "content": "Not all per-token losses are useful for MIAs. We aim to filter out the non-informative portion of the next-token loss sequence. Figure 3 (left) shows a separation between the members and non-members, based on the losses computed at the first 100 indices-members have smaller losses on average at each index whereas non-members have higher losses. Figure 3 (right) shows high overlaps between next-token losses of members and non-members at the last 100 indices (both members and non-members have low losses on average). Therefore, we propose to filter the loss sequence up to some T'.\n$$\\text{Filtered-Seq}(X) = {L_1(x_1), L_2(x_2),......, L_{T'}(x_{T'})}.$$\nWe refer to the loss averaged over the filtered loss sequence as cut-off loss, i.e., $f_{\\text{cut}}(X) = \\frac{1}{T'}\\sum_{t=1}^{T'} L_t(x_t)$, where $T'$ is the cut-off time."}, {"title": "Decreasing rate of the loss sequence", "content": "LLMs tend to have a lower loss for tokens at the end (i.e., $x_{t'}$s with larger t's), compared with tokens at the front (i.e., $x_{t'}$s with smaller t's) [21]. Figure 4 demonstrates that the decreasing rates for the loss sequence of members and non-members are different. In particular, the rate of a member is significantly higher than a non-member. The intuitive explanation is that as the model continues to predict the next tokens, the member (i.e., a sample from the training data) would seem more and more familiar to the model whereas for non-members, the model's uncertainty in next-token prediction remains. We quantify this decreasing rate by fitting a linear function on the loss sequence:\n$$L_t(x_t) \\approx at + b, \\text{ for } t = 1, 2, ...,T'.$$\nHere $a$ represents the decreasing rate of the linear function, which is the signal of our interest. We refer to this signal as the slope signal. Given the filtered sequence for the target input, i.e., $(L_1(x_1), L_2(x_2),..., L_{T'}(x_{T'}))$, the slope signal is computed as follows.\n$$f_{\\text{Slope}}(X) = \\frac{T'\\sum_{t=1}^{T'} tL_t(x_t) - \\sum_{t=1}^{T'} t \\sum_{t=1}^{T'} L_t(x_t)}{T'\\sum_{t=1}^{T'} t^2 - (\\sum_{t=1}^{T'} t)^2}.$$"}, {"title": "Number of low losses in the loss sequence", "content": "In general ML models, we expect members to have lower losses than non-members, as the model is trained to minimize the loss on members. When it comes to LLMs, however, simply computing the average loss over the loss sequence (see equation 2) is not a good option. For instance, if a member text contains multiple languages, the loss can spike significantly at the points where the language switches. As shown in Figure 5, the loss is extremely high when changing the language from English to Bulgarian. The loss at that point is 484.2, which dominates the sum of losses for all other tokens, resulting in an average loss of 9.33.\nTo mitigate this issue, we propose to count the number of losses in the loss sequence that are below a certain threshold, which can be seen as a more robust (quantized) measurement of average losses."}, {"title": "Fluctuations in the loss sequence", "content": "Figure 4 also demonstrates that the member and non-member have different levels of fluctuations in the next-token loss sequence. For tokens nearby each other, the loss can differ drastically for the non-member, compared to the member. Since the model is trained to minimize the loss on each token, it is expected to perform more consistently across all tokens for members. In contrast, for non-members, the model's performance may vary significantly across different tokens. Therefore, the fluctuations of the next-token loss sequence differ between members and non-members. To capture this difference, we propose to adopt two commonly applied measurements in time-series analysis as our membership signals: Approximate entropy [22] and Lempel-Ziv complexity [23]. At a high level, the approximate entropy quantifies the frequency of repeating patterns (i.e., segments in the sequence) in the next-token loss sequence. The Lempel-Ziv complexity measures the \"diversity\" of the sequence segments. We refer readers to Appendix A for detailed definitions."}, {"title": "Amplifying signals from the loss sequence", "content": "LLMs tend to perform better on repeated strings. Namely, when we create concatenated repetitions of the target X, the loss on the latter repetitions will be lower. Similar to the slope signal in Section 3.2.2, the loss decreasing rate on such concatenated copies may differ for members and non-members. To exploit such membership information, we propose to use the difference of signals computed at different repetitions as the membership signal, formalized as follows.\nWe use $f_{rep}(X)$ and $f_{rep'}(X)$ to denote the signal difference obtained from repeating once and twice, respectively.\n$$f_{rep}(X) = f(X; \\mathcal{M}) - f(X; \\mathcal{M}(X))$$\n$$f_{rep'}(X) = f(X; \\mathcal{M}) - f(X; \\mathcal{M}([X, \\text{`` ''}, X])),$$"}, {"title": "MIA Test Compositions", "content": "In this section, we combine our proposed membership signals, $\\mathcal{F}$ (see Section 3). We consider two common settings based on the adversary's knowledge of the underlying data distributions, following the MIA literature [24, 6, 4, 1, 25, 5]."}, {"title": "Compose multiple MIA tests", "content": "In the first setting, the adversary has access to a non-member dataset sampled from the same underlying distribution as the training dataset of the target model. The overall idea is as follows. First, based on each signal $f$ from the signal set $\\mathcal{F}$, we perform a hypothesis testing for MIA on X and compute the corresponding confidence that X is a member using this signal. Next, we combine the confidence of all hypothesis tests and get the predicted membership for X. Next, we go through our framework in more detail.\nHypothesis testing. We focus on one particular signal $f$ and formalize MIA under the hypothesis testing framework [26]. Under this framework, the null hypothesis corresponds to the target input text X being sampled from the underlying data distribution (i.e., not a member); the alternative hypothesis corresponds to the target data point being sampled from the training dataset (i.e., a member). Using the non-member dataset $\\mathcal{D}_{non-mem}$, we compute the signal $f(X_{non-mem})$ for every $X_{non-mem} \\in \\mathcal{D}_{non-mem}$. We assume, without loss of generality, that when the target data point X is a member, $f(X)$ should generally be smaller than most of $f(X_{non-mem})$; and vice versa (e.g., consider the loss signal where members have smaller losses than non-members), since otherwise the attacker can always flip the sign of the signal and obtains the same membership"}, {"title": "Prediction confidence", "content": "Based on the above hypothesis testing framework, we then compute the p-value associated with X. A smaller value indicates that X is more likely to be a member. Following prior work [27], we see observations of signal $f$ on the non-member dataset $\\mathcal{D}_{non-mem}$ as samples from some distribution $D_f$ (the distribution for the signals computed on the underlying data distribution). Based on the samples, we can then compute the p-value, which equals the cumulative distribution function of $D_f$ evaluated on the target data point's signal $f(X)$, written as $CDF_{D_f}(f(X))$. We denote this p-value $f$ as $p_f(X)$, which is a value between 0 and 1. For example, when $f(X)$ is smaller than or equal to 80 percent of the signal computed on samples from $D_f$ (i.e., the value of $p_f(X)$ is 0.2), then we are 80 percent confident that X is a member.\nCombining all p-values After we have computed the p-values corresponding to all signals in F, what is left to do is combining them to get an overall p-value that leads to membership prediction. This problem of combining p-values of multiple tests is well studied in the statistics literature, and there are various ways to do it, e.g., Edgington's method [28], Fisher's method [29], and George's method [30]. We adopt Edgington's method, which computes the sum of p-values corresponding to all signals in F, written as $P_{combined}(X) = \\sum_{f\\in F}p_f(X)$. Finally, based on $P_{combined}(X)$, we predict the target X as a member if this value is lower than a certain threshold. In our experiments, we demonstrate the attack performance using different combination methods."}, {"title": "Learning to compose", "content": "In the second setting, the adversary has access to a non-member dataset as well as a member dataset of the target model. We refer to the union of these two datasets as the attack dataset (denoted as $\\mathcal{D}_{attack}$) [24]. This dataset helps the adversary to learn how to combine all the membership signals, with the help of auxiliary machine learning models (unlike the above Edgington's method where the combination rule is fixed beforehand).\nFormalization. To formalize the combination of signals as a learning problem, we first need to specify the input and output of the model. Given the signal set $\\mathcal{F}$ and target data X and target model M, we construct the input features as $X_F = (f_1(X), f_2(X), ...)$, a |F|-dimensional vector where each dimension i corresponds to the signal $f_i$ computed on X and M. The output of this model is the probability that X is a member. The learning objective is to make the model correctly predict the membership of all samples from $\\mathcal{D}_{attack}$.\nChoice of model. We choose to use the logistic regression model as the attack model due to its simplicity and explainability. In particular, each input feature $f_i (X))$ is associated with a weight $w_i \\in \\mathbb{R}$. Based on the weighted combination of the features, the model outputs either O (i.e., non-member) or 1 (i.e., member) with probability as follows\n$$P(Y = 1 | X_F) = \\sigma((X_F, w)).$$\nHere $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function and $(\\cdot, \\cdot)$ denotes the inner product of two vectors. w is a ($|\\mathcal{F}| +1$)-dimensional vector written as $(w_1,...,w_{|\\mathcal{F}|})$. We optimize the weight vector w to minimize the following loss computed over $\\mathcal{D}_{attack}$\n$$\\sum_{(x,y) \\in \\mathcal{D}_{attack}} y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})$$\nwhere $\\hat{y} = \\sigma(\\cdot^T w)$ is the predicted membership label of x and y is the true label. Finally, on any new input target data point X, we first obtain the feature vector using the signal set F and then"}, {"title": "Dimensionality reduction", "content": "For each signal we introduced in Section 3, there are multiple variations of it. Take the slope signal as an example, we can vary the cut-off time and obtain many variations, each of which corresponds to one feature 3.2.2. We refer to all such variations of the same signal as a signal group. Since different variations may have different attack performances (and we do not know which one is the best beforehand), we may want to include all of them in our attack framework at first sight. However, as the number of features increases within a signal group, model redundancy may become an issue. To strike a balance between model redundancy and attack performance, we propose another component on top of the above logistic regression approach. In particular, we apply the Principal Component Analysis (PCA) [31] to compress the features computed from the same signal group, by computing the projection of the feature vector (composed of features from the same signal group) onto the principal subspace of the feature vector."}, {"title": "Experiments", "content": "Next, we evaluate the performance of CAMIA. Our evaluation is focused on the following two dimensions.\n1. How effective is CAMIA in inferring membership information? We show that CAMIA outperforms all prior solutions in terms of both AUC and TPR in low FPR regimes by a large margin.\n2. How effective is each individual signal in revealing the membership information? We find that (i): the effectiveness of different signals vary, and (ii): the effectiveness of the same signal also varies across different textual domains. Both findings in turn highlight the contribution of our proposed CAMIA, a holistic MIA framework that carefully combines all membership signals."}, {"title": "Setup", "content": "LLMs under MIA. We conduct membership inference attacks on three types of models, all of which are provided with ground-truth membership labels. (1) Pythia Suite [9] contains models trained on the Pile dataset [32] with parameter sizes of 70M, 160M, 1.4B, 2.8B, 6.9B, and 12B. (2) Pythia-deduped contains models trained on the Pile dataset with the same parameter configurations as in Pythia Suite, after the training data is deduplicated. (3) GPT-Neo Suite [10] contains models trained on the same dataset as Pythia Suite, with parameter sizes of 125M, 1.3B, and 2.7B.\nData. We consider seven data domains from the Pile dataset, Pile-CC (general web), Wikipedia (knowledge sources), PubMed Central (academic papers), Arxiv (academic papers), HackerNews (dialogues), DM Mathematics (specialized-domain math dataset), and Github (specialized-domain code dataset). For each domain, MIMIR benchmark [2] provides different levels of distinguishability between members and non-members. In most of our evaluations, we use the 7_gram_0.2 data split. Under this split, a non-member is selected for evaluation if less than 20 percent of its 7-gram substrings is a substring of some training data point of the target model. As highlighted in [2], due to the nature of the LM training, a high n-gram overlap may indicate that substrings of non-members may be used for training. Therefore, when there is a large overlap for a non-member, the definition of its membership is ambiguous."}, {"title": "Effectiveness of CAMIA", "content": "Table 2 showcases the performance of CAMIA compared to prior attacks across different data domains for Pythia-deduped models. Our approach consistently outperforms prior MIAs in all domains, even in the more challenging Pile-CC data set, and across all model sizes from 70M"}, {"title": "Effectiveness of individual signals", "content": "We evaluate the effectiveness of individual signals proposed in Section 3. For each signal, we compare it with a threshold to make the membership prediction, e.g., if the cut-off loss of the target text is lower than some threshold (i.e., $f_{Cut}(X) < \\tau$), then we predict it as a member and vice versa. Note that each choice of $\\tau$ corresponds to one TPR-FPR pair and varying $\\tau$ within a region gives a TPR-FPR curve (namely, ROC), on which we can compute the AUC.\nIn the following, we show results for five signals due to space constraints: the cut-off loss $f_{Cut}$ over the first 200 tokens, the cut-off loss over the first 200 tokens calibrated by the token diversity $f_{cal}$, the count below the constant value over the first 200 tokens with threshold 1 ($f_{CB}$), the slope signal over the first 800 tokens ($f_{Slope}$), and the cut-off loss difference obtained from repeating the text once ($f_{rep,Cut}$). The results for all signals are shown in Table 8 in the Appendix.\nPerformance of using each signal individually. Table 4 presents the performance of the attack when using different signals individually. We observe that the effectiveness of each signal varies significantly across different domains. For instance, the signal cut-off loss difference $f_{Cut}$ performs strongly in the Mathematics domain, with TPRs exceeding 59%, but is less effective in more challenging domains like HackerNews and Pile-CC. Similarly, the ($f_{cal}$) signal excels in GitHub, reaching up to 72.76% TPR, yet shows lower performance in other domains. Notably, the cut-off loss difference signal ($f_{rep,Cut}$) is particularly effective in the Arxiv domain, achieving"}, {"title": "Ablation study", "content": "Impact of model size. Table 5 compares our attacks and prior MIAs for Pythia-deduped models with different sizes. We find that, as increasing the size of the model, the attack performance changes differently for different data domains. For instance, on the GitHub domain, the TPR at 1% FPR increases by 10.88% when increasing the model size from 160M to 2.8B. In contrast, the TPR is reduced by 6.97% in the PubMed domain.\nWe further find the change in the attack performance is associated with the performance change. Specifically, as the size of the model increases, the loss for both training and test examples reduces. However, in different domains, the speed of loss reduction is different for the members and non-members. For instance, on Arxiv, the loss gap increases as the model size increases. As our attack and also loss attacks exploit the model performance disparity between members and non-members, the increase in the loss leads to a higher performance of MIA. On the other hand, for other domains, such as DM Mathematics, the loss gap reduces as we increase the model size. In this case, the attack performance is reduced. Those results indicate that using models with different sizes may have a disparity impact on the privacy risks for data in different domains.\nDifferent ways of p-values aggregation. Table 6 presents the performance of the attack when the adversary employs different methods to combine p-values. The table includes the most"}, {"title": "Conclusion", "content": "In this paper, we present CAMIA, a novel MIA framework specifically designed for pre-trained LLMs. Unlike traditional MIAs, our approach recognizes the unique, sequential nature of LLMs and the critical role that contextual factors play in their memorization tendencies. By accounting for token-level loss sequences, prefix complexity, and changes in next-token prediction loss across sequences, CAMIA is able to effectively capture context-dependent memorization patterns that previous methods overlook. Our comprehensive evaluation demonstrates that CAMIA significantly outperforms existing MIAs, particularly at low false-positive rates. These results underline the importance of adapting MIAs to the structure of LLMs which generate predictions based on token sequences rather than single outputs."}]}