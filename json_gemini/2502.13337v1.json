{"title": "Language Models are Few-Shot Graders", "authors": ["Chenyan Zhao", "Mariana Silva", "Seth Poulsen"], "abstract": "Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.", "sections": [{"title": "1 Introduction and Background", "content": "Providing grading and feedback has been shown to be highly effective in enhancing student learning outcomes [35]. However, manually grading assessments is one of the most time-consuming and tedious aspects of instruction, making it difficult to scale [1,12]. To alleviate this burden, educators often turn to closed-ended question formats, such as multiple-choice or numerical input questions, which can be automatically graded [27]. While these formats enable efficient grading and feedback, they also present limitations. For example, students may develop lower-level skills and rely on test-taking strategies rather than deeply engaging with the material [11]. Additionally, instructors face challenges in designing high-quality distractors and effective closed-ended questions [33].\nIn contrast, open-ended questions allow students to express their understanding in their own words, providing a more accurate measure of their knowledge [11]. However, grading such responses at scale remains a significant challenge. Automated Short Answer Grading (ASAG) systems leverage Natural Language Processing (NLP) techniques to assess student responses and have been widely studied in various research communities [1]. Recent advancements in Large Language Models (LLMs) have further facilitated the development of ASAG tools, making automatic grading more feasible and accessible.\nNatural Language Processing (NLP), a subfield of artificial intelligence, focuses on training and deploying models that interpret and generate human language. A major breakthrough in NLP came in 2017 with the introduction of the Transformer architecture, which relies entirely on attention mechanisms to capture dependencies in input and output sequences [37]. This architecture improved performance while reducing the computational resources needed for training and inference [37]. Further developments led to the introduction of various pretrained LLMs, such as BERT[3], Llama[36], and OpenAI's GPT family [2]. These models have demonstrated strong performance across a variety of NLP tasks, often requiring minimal prompt engineering to adapt to new datasets [9,18].\nMore recently, LLMs have inspired the development of new ASAG tools, which generally follow one of two main approaches. The first approach utilizes numerical representations of student responses, transforming them into vector embeddings using LLMs and then applying techniques such as logistic regression, linear regression, or reference-based comparison to generate grades [6,20,40]. While these methods have demonstrated high grading accuracy across a range of educational levels, they require extensive training data and struggle to provide detailed, interpretable feedback [40]. The second approach leverages generative models such as ChatGPT, where grades and feedback are produced through prompt engineering [14,16,22]. This method enables more flexible feedback generation and adaptive instruction, making it appealing for formative assessment. These approaches, which did not include graded examples in their prompts, were not able to consistently match the performance of top performing custom-built models on standard ASAG datasets [1,14,16,22]."}, {"title": "1.1 Few-Shot Training and Retrieval Augmented Generation", "content": "One major issue related to the use of LLMs is hallucination, where models generate responses that are inaccurate or inconsistent with the intended grading criteria [13]. Hallucination in ASAG can occur when the model lacks domain-specific knowledge, leading to incorrect grading and misleading feedback [10]. Strategies such as few-shot learning and retrieval-augmented generation (RAG) have been explored to mitigate this issue. Few-shot learning provides models with example responses to guide grading, while RAG improves contextual grounding by retrieving relevant information from external sources [2,19]. These techniques have shown promise in enhancing the model response accuracy and reducing hallucinations [19]. Few-shot learning has also been applied to ASAG. For example, Duong and Meng [4] incorporated graded examples and related course"}, {"title": "1.2 Rubrics and Human Grading", "content": "Another potentially critical factor in ASAG is the role of grading rubrics. Rubrics define evaluation criteria, quality benchmarks, and scoring strategies, making grading more transparent and interpretable [32]. In formative assessment settings, rubrics can help students regulate their learning and improve performance [26]. Instructors also use rubrics to maintain grading consistency across responses [32]. In the ASAG contexts, Senanayake and Asanka [34] integrated a grading rubric in the grading process, but they did not provide a complete analysis of the effect of the rubric.\nIn this work, we address the existing knowledge gap regarding the impact of graded example selection and grading rubrics on ASAG performance. To build on these insights, we propose an ASAG pipeline that integrates LLM-based grading strategies. This paper presents the following key contributions:\n1. Developing an ASAG pipeline leveraging existing Large Language Models.\n2. Comparing the grading performance of three OpenAI generative models.\n3. Examining the impact of integrating instructor-graded examples into the grading process, comparing three selection strategies: no examples, random selection, and RAG-based selection.\n4. Analyzing the effect of incorporating grading rubrics on grading accuracy."}, {"title": "2 Grading method", "content": ""}, {"title": "2.1 Data", "content": "We utilize a number of public benchmark ASAG datasets created in previous works. Each dataset consists of a set of question prompts, a set of student submissions to the questions, and instructor-provided grades to these submissions that we can use as ground truth labels for the models. Some of the datasets contain reference solutions to these questions as well. The Texas dataset [23] includes questions from a Data Structure courses, with over 80 questions and 2,500 graded submissions. SAF [7] covers a variety of college-level communication network topic, and has over 20 questions and 4,500 graded submissions. The SciEntsBank dataset [24] focuses on 15 different science domains such as Life Science and Space Science, containing almost 200 questions and over 10,000 submissions. Some other ASAG datasets such as Statistics [21], DigiKlausur [15], and Beetle [5] are not used in this study due to the lack of question prompts, which form a fundamental part of our prompting method. These datasets worked for earlier models using trained classifiers as the question prompt is not needed for these models. We also utilize the mathematical induction proof dataset [28]\nfor a comparison on the LLM's performance using rubrics. The Proof dataset contains over 3,000 submissions on 4 induction questions. These proofs were graded using a 7-item rubric, with each item corresponding to a step of the induction proof. Each step is also related to a misconception in induction proofs that has been identified before [40]."}, {"title": "2.2 Prompting", "content": "We use three LLM API endpoints throughout this study: gpt-4-turbo-2024-04-09, gpt-4o-2024-08-06, and o1-preview. gpt-4-turbo-2024-04-09 (we will refer to it as GPT-4) is an older version of a high-intelligence GPT model. It can be run at a lower cost and can used to compare against other models. gpt-4o-2024-08-06 (GPT-4o) is the latest GPT-4o model at the time of the study. o1-preview is the simplified and public version of o1, the latest OpenAI model. It utilizes reinforcement learning and is able to perform a long chain of thought before responding [25].\nWe design a prompt for LLMs aimed at achieving the best adaptability, as illustrated in Figure 1. We divide the prompts into four components. The first component includes general grading instructions, offering the LLMs context on how to generate grading results and construct feedback. This part of the prompt is generic and remains unchanged for any course or dataset. The second component provides the question prompt from the datasets, enabling the LLM to generate its own correct answer for comparison. The third component consists of some graded examples from the dataset: a subset of human-graded responses selected by the system. The number of examples included can vary depending on instructor preferences, but using a larger number increases computational costs. For this study, we utilize 5 examples per prompt, selected using strategies detailed in Section 2.3. Finally, the fourth component presents the student's response to be graded.\nNote that our grading prompt does not rely on sample problem solutions provided by the instructor. In some classroom settings, a written reference solution may not always be available, and full-score graded examples can serve as effective substitutes for reference solutions.\nTo enhance automation in the grading process, the LLM is instructed to return grading results in a structured JSON object. The JSON object contains 2"}, {"title": "2.3 Selecting Graded Examples", "content": "As each question in the datasets has multiple graded submissions, we can incorporate some submissions as examples for the LLM when grading another submission. We employ one of two selection strategies: Random or RAG. The Random strategy randomly selects a fixed-size subset from the pool of human-graded submissions for the question and is easy to implement. In contrast, the RAG strategy leverages a retrieval-based approach introduced in Section 1.1. It selects graded examples that are most similar to the current submission based on the Euclidean distance of their embeddings. While the Random approach is computationally simpler, the RAG method aims to maximize contextual relevance, potentially improving grading accuracy. As a baseline comparison, we add a None strategy that does not include any graded examples. The None, Random, and RAG strategies enable us to evaluate the effect of example selection on the performance of the grading pipeline."}, {"title": "2.4 Grading with rubrics", "content": "Our rubric-based prompt is adapted from the original grading prompt, enhanced with additional rubric-related information, as shown in Figure 2. Each rubric item is assigned a unique number for system reference, a description or name presented to students, and a detailed explanation clarifying its meaning and criteria for correctness. By incorporating rubrics, the LLM transitions to making a series of binary decisions on each rubric item based on the explanation of the item, as opposed to assessing the overall quality of a student submission without a standardized framework. Scores can be computed based on the weights assigned to each rubric item by the instructor. The grading accuracy metric can be evaluated either by analyzing the accuracy of individual rubric items or by examining the overall calculated scores derived from the rubric.\nTo mitigate the risk of LLM hallucination, such as inventing new rubric items or failing to return them correctly, we utilize structured output enforcement in the GPT-4 model. The output includes a string labeled \"feedback\" and a \"rubric_items\" object, which is a list of rubric items containing the name of each item and whether the item should be selected. This structured output ensures that the model consistently returns a list of results based on the provided rubric items, reducing the need for manual verification, particularly during large-scale deployment."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Comparison on models and effect of graded example selection method", "content": "The Texas, SAF, and SciEntsBank datasets provide scores that are not based on rubrics and are on different grading scales. To ensure comparability on the accuracy metrics, we normalize these scores to percentages and then evaluate the grading performance using Root Mean Squared Error (RMSE) and Pearson's r metrics.\nFor both GPT-4 and GPT-4o, the time to grade each submission is around 1-2 seconds, and the cost is less than $0.01. On the other hand, it takes o1-preview around 10 seconds to grade each submission, with the cost of around $0.1 per grading task. Due to this difference, we run GPT-4 and 4o using the entire\ndataset, but only use a random sample of 200 submissions for o1-preview. For all three datasets, the GPT-4o model without graded examples achieves comparable grading accuracy to the benchmark results presented in [22] using GPT-4o. Notably, GPT-4o outperforms GPT-4 likely due to its enhanced knowledge base. Based on the results in Figure 3, over 50% of the grading jobs performed by o1-preview achieve near-perfect results. This agrees with the promising performance of chain-of-thought methods observed in earlier studies [39]. However, there is also a larger variance in error for most experiment groups, agreeing with previous observations that reasoning paths can vary in quality [38,39].\nTo examine the effect of providing examples on absolute grading error, we combine all three datasets and conduct independent t-tests for each model. Within each model, incorporating few-shot learning through randomly selected graded examples significantly improves grading accuracy compared to not using graded examples ($t$ = 10.84, $p$ < 0.001 for GPT-4, $t$ = 11.98, $p$ < 0.001 for"}, {"title": "3.2 The Effect of Using a Rubric", "content": "We want to study the impact of using a validated ASAG rubric. We use the induction proof dataset for this comparison, which includes 4 different mathematical questions. The rubrics are described in Section 2.1. Since previous analysis in Section 3.1 suggests better accuracy with RAG-based example selection, we completed the grading tasks for the proof dataset by selecting 5 examples using retrieval. We use GPT-4o due to its lower cost compared to o1-preview and higher performance compared to GPT-4. For the grading with rubric, we use equal weights on the 7 rubric items and calculate the scores based on these weights. The RMSE and Pearson's r were completed using these weighted scores,\nIncluding rubric information results in a significantly higher grading accuracy for two of the questions questions, with $t$ = 2.20, $p$ = 0.03 for question 3 (P3), and $t$ = 4.64, $p$ < 0.001 for question 4 (P4). We observe negligible influence for the other two questions, with $t$ = 1.34, $p$ = 0.18 for question 1 (P1), and $t$ = 1.31, $p$ = 0.19 for question 2 (P2). Overall, providing a rubric generally aids the LLM in generating more accurate grades. Previous studies on ASAG and human grading [8,40] have shown that achieving 100% agreement among graders is nearly impossible due to the open-ended nature of the questions and the ambiguity of natural language. Incorporating a rubric can help LLMs achieve greater consistency and alignment with human grading standards. Interestingly, the by-rubric-item accuracy of GPT-4o is already comparable to that of GPT-3.5-based task-specific grading models as well as human graders recruited in earlier studies, highlighting the remarkable capability of GPT-4o."}, {"title": "4 Conclusions and Future Work", "content": "In this work, we develop an Automatic Short Answer Grading pipeline by prompting a Large Language Model to generate a score and evaluate its grading performance. Combining the use of GPT-4o, RAG-based example selection, and a grading rubric, the grading pipeline developed in this work achieves state of the art performance on all datasets with the exception of a few complex mathematical proof problems. The system also has similar accuracy human graders [40,8].\nAmong the OpenAI's GPT-4, GPT-4o, and o1-preview models, GPT-4o currently has the best performance on the grading tasks in terms of accuracy and cost. Although o1-preview produces a higher percentage of accurate grades, its high variance in error would make it unfit for real classroom settings. The cost for o1-preview is too high for scaled use as well. Further-developed chain-of-thought strategies could potentially make o1-preview a more suitable model for classroom use. For the grading tasks, providing graded examples in the prompt can effectively increase the grading accuracy, and choosing examples using RAG-based methods has a larger impact than randomly selecting examples. Providing a rubric in the grading pipeline can also increase grading accuracy by offering interpretability in the grading process.\nGiven the high accuracy, our grading pipeline can be used in real classroom settings to help decrease the workload on human graders. An appeal system could be used to mitigate the effects of grading mistakes, similar to how appeal systems are used to mitigate error in human TA grading. A future direction is to integrate the grading pipeline into an open question-answering platform with an interactive interface that allows instructors to actively assess the model's accuracy, determine its suitability for their use case, and provide additional graded examples as needed to refine performance. It can also be helpful for the"}]}