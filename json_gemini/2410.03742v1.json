{"title": "BEYOND SCALAR REWARD MODEL: LEARNING GENERATIVE JUDGE FROM PREFERENCE DATA", "authors": ["Ziyi Ye", "Xiangsheng Li", "Qiuchi Li", "Qingyao Ai", "Yujia Zhou", "Wei Shen", "Dong Yan", "Yiqun Liu"], "abstract": "Learning from preference feedback is a common practice for aligning large language models (LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference or reward. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging the generation capability of LLMs to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate positive and negative judgments, both supported with rationales in natural language form. The self-generated contrastive judgment pairs are used to train the generative judge with Direct Preference Optimization (DPO). This proposal of training the generative Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability due to the generated rationales together with the judgments, as well as high robustness against bias without the need for an additional reward head. Experimental results show that the performance of Con-J is comparable to the scalar reward model trained on the same collection of preference data, and demonstrate its superior interpretability and robustness in encoding human preferences.", "sections": [{"title": "1 INTRODUCTION", "content": "As Artificial Intelligence (AI) systems advance with the emergence of Large Language Models (LLMs), it is crucial to ensure they align with human instructions, values, and ethics. LLMs alignment is generally achieved by learning from preference data that compares pairs of responses to a question (Rafailov et al., 2024; Christiano et al., 2017; Liu et al., 2020). However, collecting high-quality human preference data is both time-consuming and costly. In practice, the construction of preference datasets often involves scaling with a combination of human and AI-generated feedback Lee et al. (2023); Hou et al. (2024). An additional advantage of AI feedback is its ability to be incorporated in real-time, which facilitates online algorithms such as iterative-DPO Xiong et al. (2024); Xu et al. (2023) and online-DPO Guo et al. (2024). Therefore, it is crucial to develop an efficient and accurate AI-based preference model that aligns with human values.\nTo obtain such preferences, industrial practices have used scalar models Hou et al. (2024) that concatenate the pre-trained LLM with a value head to generate scalar scores for the responses. However, the scalar model suffers from limitations, particularly in the following aspects: (i) Lack of interpretability: Beyond scalar scores, additional rationales are crucial to enhance the reliability of judgments and facilitate human involvement in the evaluation loop. (ii) Susceptibility to bias: It is prone to capturing the biases present in the preference dataset rather than human values. For example, when the majority of positive answers in preference datasets are longer sentences, the learned LLM will likely favor more verbose answers (Huang et al., 2024).\nTo address the above limitations, we propose Con-J, which trains a generative Judge using its self-generated Contrastive judgments (see Figure 1). Con-J leverages the LLM's pre-existing judg-"}, {"title": "2 PRELIMINARY", "content": "2.1 TASK DEFINITION\nGiven a question or prompt q and a pair of assistant responses $a^1$ and $a^2$, the task is to judge the preference between $a^1$ and $a^2$. To accomplish this, we train the model using an existing preference dataset $D = {(q, a^-, a^+)}_i=1^N$, where $a^+$ is a preferred answer compared to $a^-$. The model's performance is subsequently evaluated on a separate, non-overlapping preference dataset by measuring the accuracy of its preference judgments.\n2.2 SCALAR MODEL\nThe most common practice for getting the preference judgment is to use a scalar model (SM) similar to the reward model in the RLHF stage (Hou et al. (2024)). The SM predicts numerical scores $r(q, a)$ for $a \u2208 {a^1, a^2}$ and judges the preference by comparing $r(q, a^1)$ and $r(q, a^2)$. It is typically initialized by concatenating a pre-trained LLM with a randomly initialized shallow MLP head. The most widely used training objective for the SM follows the Bradley-Terry model, which maximizes the probability of $a^+$ being preferred:\n$P(a^+ > a^-|q) = \\frac{exp(r(q, a^+))}{exp(r(q, a^+)) + exp(r(q, a^-))} = \u03c3(r(q, a^+) \u2013 r(q, a^-))$ (1)\nwhere \u03c3 is the sigmoid function.\nThe above-mentioned SM utilizes the prompt q and a single answer a as input, which we denote as a pointwise SM. In addition, existing research has investigated a pairwise variant that uses a pair of candidates as input, i.e., $r(q, a^1, a^2)$ (Jiang et al., 2023). The pairwise vanilla reward formalizes the preference probability of $a^+$ as:\n$P(a^+ > a^-|q) = \u03c3(r(q,a^+,a^-)).$ (2)\nTo train the above-mentioned pointwise and pairwise SM r, we maximize the log-likelihood of the preferences by minimizing the following loss function:\n$l_r(r) = - \\sum_{(x,a^+,a^-)} logpr(a^+ \\succ a^-|x) = -\\sum_{(x,a^+,a^-)} logo (r(x, a^+) - r(x, a^-))$ (pointwise)\n$-\\sum_{(x,a^+,a^-)} logo (r(x,a^+,a^-))$ (pairwise) (3)"}, {"title": "3 IMPROVING GENERATIVE JUDGE BY TRAINING ON CONTRASTIVE JUDGMENTS", "content": "Instead of using a scalar model for preference judgment, we propose to leverage the LLM itself to make preference judgments (Guo et al. (2024); Lee et al. (2023)). Given the question q and a pair of answers $a^1$ and $a^2$, we construct a prompt p by concatenating a preamble with q, $a^1$, and $a^2$. The preamble is an instruction that describes the task and asks an LLM \u03c0 to act as a judge (see examples in Fig. 2 and Appendix 5). Then the LLM generates natural language judgments j = \u03c0(\u03c1), which contains the judgment as well as the rationale in a JSON style: a key named \"rationale\" includes a step-by-step explanation and verification of the answers, and another key named \"better_answer\" indicates the LLM's binary judgment.\nAs shown in Figure 2, the construction of Con-J consists of three steps: judgment sampling, judg-ment filtering, and Con-J training."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nDatasets. We train scalar models and Con-J on three datasets within different vertical domains: Creation, and Math, then evaluate their performance in terms of the accuracy of preference predictions. The datasets are self-built commercial datasets consisting of approximately 120,000, 50,000, and 50,000 preference samples for Creation, Math, and Code, respectively. The Creative dataset involves tasks on text creation such as writing poetry or crafting headlines, whereas Math and Code datasets concentrate on math problem-solving and code writing, respectively. The datasets cover"}, {"title": "6 DISCUSSIONS AND CONCLUSIONS", "content": "We introduced Con-J, a novel approach that trains a generative judge by self-bootstrapped learning from preference data. Con-J addresses the limitations of scalar reward models, including lack of interpretability and susceptibility to dataset bias. Our experiments on commercial datasets across Text Creation, Math, and Code domains, as well as publicly available benchmarks, demonstrate the effectiveness of Con-J. Moreover, we show that the correctness of the rationales generated by Con-J improves during learning from preference data. This enables Con-J not only to make accurate judgments but also to provide reasonable explanations, potentially facilitating human-in-the-loop supervision of LLM alignment. Finally, we found that Con-J is less susceptible to biases in datasets compared to its variants without rationales and the scalar models.\nAs AI systems become more powerful, many suggest that they will reach the point at which human are unable to easily and reliably assess the quality of their outputs (Casper et al., 2023). To address this issue, using another AI to supervise itself is a viable solution; however, researchers suggest that these methods may fail without human involvement (Shumailov et al., 2024). This paper contributes to addressing this issue in two ways. On the one hand, Con-J can be used to supervise LLMs by acting as a judge. At the same time, Con-J produces an explanation of its output that is legible to humans or another trusted system. This indicates that we can spot any possible errors made by Con-J. On the other hand, the training and construction of Con-J rely solely on preference data, which is easier to acquire from human annotators than high-quality instruction tunning data. Furthermore, in many cases humans often find it difficult to provide verbal reasons for their preference, the training"}, {"title": "A APPENDIX", "content": "Table 5: An example of a prompt fed to the LLM to generate preference reward judgments, con-sisting of a preamble (introduction and instructions describing the task), a question, and a pair of candidate answers. The preamble is neutral and does not indicate which answer is better."}]}