{"title": "TrIM: Triangular Input Movement Systolic Array\nfor Convolutional Neural Networks-Part II:\nArchitecture and Hardware Implementation", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "abstract": "Modern hardware architectures for Convolutional\nNeural Networks (CNNs), other than targeting high perfor-\nmance, aim at dissipating limited energy. Reducing the data\nmovement cost between the computing cores and the memory\nis a way to mitigate the energy consumption. Systolic arrays\nare suitable architectures to achieve this objective: they use\nmultiple processing elements that communicate each other to\nmaximize data utilization, based on proper dataflows like the\nweight stationary and row stationary. Motivated by this, we have\nproposed TrIM, an innovative dataflow based on a triangular\nmovement of inputs, and capable to reduce the number of\nmemory accesses by one order of magnitude when compared\nto state-of-the-art systolic arrays. In this paper, we present a\nTrIM-based hardware architecture for CNNs. As a showcase,\nthe accelerator is implemented onto a Field Programmable Gate\nArray (FPGA) to execute the VGG-16 CNN. The architecture\nachieves a peak throughput of 453.6 Giga Operations per Second,\noutperforming a state-of-the-art row stationary systolic array by\n~ 5.1\u00d7 in terms of memory accesses, and being up to ~ 12.2\u00d7\nmore energy-efficient than other FPGA accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolutional Neural Networks (CNNs) are leading\nAl models in various domains, ranging from computer\nvision [1], [2] to speech recognition [3], [4]. Their wide\napplicability is justified by the use of convolutions, which\ninfer locality between neurons, thus mimicking the behavior\nof human visual system to identify features from inputs [5].\nHowever, this translates in: (i) high memory requirements to\nmanage multi-dimensional feature maps (fmaps); (ii) billions\nof computations owing to the large fmaps to fulfill high\nstandards of image resolution and accuracy [6]. For instance,\nthe well-known VGG-16 CNN [7] requires up to ~22.7 MB of\nmemory to deal with 8-bit input fmaps (ifmaps) and weights,\nover a total of ~30.7 billions of operations on 224\u00d7224 RGB\nimages. Since former CLs process large fmaps, they require massive\nmemory for inputs, as well as computations. Conversely,\ndeeper CLs extract features requiring a dominant contribution\nof weights.\nIn order to follow the fast pace dictated by CNNs' ex-\nplosion, hardware designers are continuously struggling to\nguarantee energy-efficient architectures, capable to offer real-\ntime activity as well as low-energy footprint. While the\nformer can be met by means of high-parallelized engines,\nthe latter require to tackle the Von Neumann bottleneck, or\nmemory wall [8]. The major energy-draining source is the\ncommunication between the parallel computing cores and the\nmemory, either in the form of on-chip Static Random Access\nMemory (SRAM) or as an external Dynamic Random Access\nMemory (DRAM). For example, the relative energy cost for a\n32-bit SRAM read is estimated to be 5 pJ for a 45nm process\nat 0.9V, which in turn dramatically increases to 640 pJ for\na 32-bit DRAM read [9]. Overall, the DRAM contribution\nis ~200\u00d7 higher than a 32-bit multiplication performed by\nthe computing core. Among different strategies, on-chip data\nreuse allows to reduce the number of DRAM reads, by locally\nstoring fmaps and weights to maximize their utilization before\nbeing discarded [10].\nSystolic Arrays (SAs) [11], [12] are promising candidates"}, {"title": "II. RELATED WORKS", "content": "Given the way in which data flows between PEs, SAs are\nparticularly suitable to manage Matrix Multiplications (MMs)\nbetween inputs and weights, being compatible with Fully-\nConnected Neural Networks (FCNNs). On the contrary, CNNS\nmainly execute convolutions, where inputs must be arranged\nin sliding windows before being processed with the respective\nweights. To achieve this, Conv-to-GeMM has been introduced\nto arrange sliding windows in a matrix form [28].\nSeveral architectures based on SAs make use of Conv-to-\nGeMM [18]\u2013[21]. The Google Tensor Processing Unit (TPU)\n[18] hosts a SA consisting of 256\u00d7256 PEs. Based on the\nWS dataflow, inputs are supplied through First-In-First-Out\n(FIFO) buffers, and moved horizontally from left to right.\nThe produced psums move vertically from top to bottom. In\naddition, an accumulation logic consisting of synchronization\nFIFOs and adders further process the psums, coming from\nthe bottom side of the array, over multiple computational\nsteps. According to this process, such a TPU is able to\naccommodate several NN workloads, including FCLs, CNNs,\nand Recurrent NNs. However, this flexibility has a cost in\nterms of memory capacity and accesses to cope with Conv-to-\nGeMM, thus degrading the energy efficiency. On the contrary,\nthe accelerator proposed in [19] tackles the above issues by\nproposing an on-the-fly GeMM conversion. Specifically, a data\nfeeder unit is interfaced to a SA of 16x16 PEs and based on\nthe OS dataflow. The data feeder includes an addressing logic\nto fetch inputs from proper regions of the main memory, as\nwell as FIFOs to schedule inputs towards the array over time.\nIn [20], a Network-on-Chip based convolution is proposed to\ndeal with GeMM. Inputs and weights are split into packets\nand provided to nodes consisting of parallel and distributed\n3\u00d73 SAs. Despite this allows latency reduction, extra area is\nrequired to manage the NoC packets' scheduling. The work\npresented in [21] deals with sparse CNNs, using a SA of 8 \u00d7\n32 PEs. Ping-pong buffers assist local data reuse.\nOn another direction, some architectures [23]-[26] have\nproposed alternative ways to skip the disadvantages of GeMM"}, {"title": "III. THE TRIM ARCHITECTURE", "content": "The TrIM-based AI hardware architecture is designed as a\nhierarchy consisting of three levels:\nTrIM Slice. This module is responsible for performing\n2-D K \u00d7 K convolutions. Weights are kept station-\nary by each Processing Element (PE), while inputs are\nreused between PEs by exploiting the innovative trian-\ngular movement. Specifically, each slice processes an\nindependent ifmap and a specific kernel of one of the\nN 3-D filters.\nTrIM Core. This module accommodates multiple slices,\neach managing a specific ifmap and PM kernels be-\nlonging to one of the 3-D filters. PM is the number of\nkernels processed in parallel. Additional adders allow the\nprovisional outputs from each slice to be accumulated,\nthus generating one ofmap.\nTrIM Engine. This top-level module consists of PN cores,\neach dealing with an independent 3-D filter. PN is the\nnumber of filters processed in parallel and, in turn, the\nnumber of ofmaps generated in parallel. All the cores use\nthe same set of ifmaps."}, {"title": "A. TrIM Slice", "content": "The TrIM slice includes an array of K \u00d7 K PEs, an\nadder tree and K \u2013 1 Reconfigurable Shift Register Buffers\n(RSRBs), as illustrated in Fig. 3. The computing array is\norganized in K rows (Rowi, with 0 < i < K), each having\nK PEs. Preliminarily, weights are fetched from the memory\nand provided to the PEs of Rowo as groups of K elements\nper cycle, then shifted from top to down. This process lasts\nK cycles to ensure that the current kernel is completely\nstored at the slice level. Thus, inputs are first grabbed from\nthe periphery (Iext), moved in each row from right (IR) to\nleft (IL), and forwarded to the closest SRB to complete the\ntriangular movement. Each SRB dispatches K inputs (ID) to\nRowi-1. Psums are first accumulated vertically through PEs\nand, finally, through an adder tree that finalizes the K \u00d7 K\nconvolution.\nThe generic PE is equipped with two input registers to\nstore the external input Iext and the weight Wext (or W),\nrespectively. Two cascaded multiplexers assist the PE in sup-\nplying the multiplier with the correct input (Iext, ID or IR).\nA subsequent adder accumulates the current product with the\npsum coming from the PE placed in the same column in\nRowi-1. Finally, one output register delivers the current output\neither to the vertically-aligned PE in the Rowi+1 or to the\nAdder Tree, and another register provides the current input to\nthe left PE.\nIn order to make the slice agnostic to the ifmap size,\nthe RSRBs is equipped with run-time reconfigurability. To\nachieve this, each RSRB consists of WIM registers, with WIM\nbeing the width of the largest ifmaps that typically constitutes\nthe very first CL of a CNN. To manage the subsequent\nfmaps, having W\u2081 < WIM, the RSRB is split into multiple\nSub-Buffers (SBs), each consisting of Lsb registers (with sb\niterating over the SBs)"}, {"title": "B. TrIM Core", "content": "The TrIM core delivers 3-D convolutions by accommodating\nPM slices that operate in parallel, followed by an adder\ntree that further accumulates the PM provisional outputs, as\ndepicted in Fig. 5. With reference to a generic CL, the slices\nare fed by PM ifmaps (among the M available), as well as\nby PM kernels from one of the N filters. Taking into account\nthat each slice operates on different ifmaps and kernels, the\nparameter PM is constrained by the I/O bandwidth that the\narchitecture can sustain.\nThe subsequent adder tree produces a 2\u00d7 B + K +\n[log2(K)] + [log2(PM)]-bit result (core_out), followed by\nan output register. Although the adder tree complexity can\nconstraint the speed, pipelining can be added for shorter\ncritical paths and better timing closure."}, {"title": "C. TrIM Engine", "content": "The TrIM engine is the top-level of the hierarchy as shown\nin Fig. 6, where multiple cores work in parallel on the same\nbroadcast inputs. Specifically, PN cores are responsible for\ngenerating as many ofmaps, among the N ofmaps to be\nproduced. Considering that PN and PM are the parallelism\nparameters of the engine, up to [N/PN] \u00d7 [M/PM] compu-\ntational steps are required to finalize the computations. In the\ngeneric step, each core executes a 3-D convolution between\none filter of PM K \u00d7 K kernels and PM H1 \u00d7 W1 fmaps. As\na result, the core_out generated by the core is not final, but\nneeds to be accumulated over other [M/PM] - 1 iterations.\nTo achieve this, the engine associates an extra adder and a\nPsums Buffer to each core. This buffer needs to store up to\nHOM \u00d7 WOM output activations, where HOM and WOM\nrepresent the sizes of the largest ofmaps. Each activation is\n2 \u00d7 B + K + [log2(K)] + [log2(M)]-bit wide. For CLs\nhandling smaller ofmaps, the buffer stores Ho \u00d7 Wo output\nactivations, where Ho < Hom and Wo < W\u043em are the\nsizes of the current ofmap. Meanwhile, the remaining PN - 1"}, {"title": "IV. CASE STUDY: VGG-16", "content": "In order to showcase the capabilities offered by the TrIM-\nbased architecture, we consider the Visual Geometry Group\n(VGG) CNN [7] as a case-study. In particular, we refer to the\nVGG-16 architecture, where 13 CLs and 3 FCLs manage the\nimage classification task. The focus of this research activity is\noriented towards the hardware acceleration of the CLs only.\nWe introduce some metrics to enable a design space explo-\nration, in terms of achievable parallelism (i.e., the parameters\nPN and PM). Three metrics are taken under consideration:\nthroughput, psums buffers size, and I/O bandwidth. In order\nto compute the throughput, the total number of operations and\nthe execution time of the CNN under test are needed. Each\nCL performs 3-D convolutions between ifmaps and filters,\ndemanding the following number of operations:\nOPs = 2 \u00d7 K \u00d7 K \u00d7 Ho \u00d7 Wo \u00d7 M \u00d7 N\nWhile this expression is general and independent by the\narchitecture that processes the CLs' workload, the execution\ntime is strictly constrained on the underlying architecture. As\nillustrated in Section III, the TrIM engine requires [N/PN]\u00d7\n[M/PM] computational steps to finalize each CL. Each step\ncan be split into two phases: weights' loading and computation\nphase. During weights' loading, PN \u00d7 PM \u00d7 K \u00d7 K B-bit\nweights must be provided to the architecture. Assuming that\nenough I/O bandwidth is available, it is supposed to fill one\ncore with the needed weights every K clock cycles. Therefore,\nit is expected to complete the entire phase in PN \u00d7 K clock\ncycles. Thus, the computation phase may start: after an initial\nlatency dictated by the pipeline stages of the engine (L1),\nHox Wo cycles are needed to finalize the convolutions. The\nnumber of clock cycles (NC) is given by expression (2):\nNC = L1+[N/PN]\u00d7[M/PM]\u00d7(PN\u00d7K+Ho\u00d7Wo)\nThe execution time is retrieved by diving NC by the clock\nfrequency (fCLK).\nThe psums buffers size is considered because on-chip\nmemories contribute heavily to the energy footprint, other\nthan being limited resources on FPGAs. Psums buffers store\nthe provisional outputs from the TrIM cores, thus enabling\ntemporal accumulations. Given the considerations reported in\nSection III, each psums buffer stores at most a HOM \u00d7 WOM\nofmap, where each activation is 2 \u00d7 B + K + [log2(K)] +\n[log2(M)]-bit wide at least. Assuming 32-bit activations,\nenough to satisfy any on-chip accumulation, and taking into\naccount that PN buffers are accommodated into the TrIM\nengine, the total psums buffer size is given by equation (3):\nPsums Buffer Size = PN \u00d7 \u041d\u043e\u043c \u00d7 W\u043e\u043c \u0445 32\nThe I/O bandwidth is dictated by the transmission of\nweights, ifmaps and ofmaps. Weights are supplied before any"}, {"title": "V. HARDWARE IMPLEMENTATION", "content": "The TrIM Engine has been synthesized and implemented\nonto the FPGA of the AMD Zynq UltraScale+ XCZU7EV-\n2FFVC1156 MultiProcessor System-on-Chip (MPSoC). The\ndesign has been carried out through the AMD Vivado 2022.2\ntool, and using Verilog as Hardware Description Language.\nFirst, taking into account the design space reported in\nSection IV, the optimal parameters PM and PN have been\nretrieved. PN is constrained by the on-chip memory. Consid-\nering that the XCZU7EV part offers 11 Mb of Block RAMS\n(BRAMs), using equation (3), we got PN = 7, supposing that\neach psums buffer stores up to 224 \u00d7 224 output activations,\nwhich refer to the worst-case scenario (first two layers of\nVGG-16). The parameter PM strictly depends on the available\nI/O bandwidth. The XCZU7EV device is interfaced to a 64-\nbit DDR4 memory and exhibits a peak bandwidth of 19200\nMB/s. Supposing fCLK = 150 MHz, in line with state-of-the-\nart FPGA-based accelerators [21], [24], [25], BW1/0 = 1024,\nrounded to the closest power of 2. Therefore, considering\nequation (4) and PN\n7, we got P\u043c\n24. Hence, a\nTrIM Engine hosting PN = 7 cores, each having TM = 24\nslices, has been synthesized and implemented onto FPGA. The\nTrIM-based architecture achieves a peak throughput of 453.6\nGOPs/s, since 1512 PEs are responsible for performing 7 \u00d7 24"}, {"title": "VI. CONCLUSION", "content": "In this work, we present a hardware architecture to show-\ncase the TrIM dataflow. The architecture is organized in a\nhierarchical manner, to effectively cope with convolutional\nlayers. Reconfigurable shift register buffers are included to\nmake the engine agnostic to different ifmap sizes. As a case\nstudy, the VGG-16 CNN is considered and an architecture of\n1512 processing elements is implemented onto the XCZU7EV\nFPGA. The achieved peak throughput is 453.6 Giga Opera-\ntions per Second. When compared to Eyeriss, a state-of-the-art\naccelerator based on the row stationary dataflow, the TrIM-\nbased architecture guarantees ~ 5.1\u00d7 less memory accesses,\nconfirming the effectiveness of the triangular input movement\nto maximize data utilization. In addition, the proposed archi-\ntectures results up to ~ 12.2\u00d7 more energy-efficient than other\nFPGA counterparts.\nStimulated by these results, we plan the following future\nworks:\nReduction of the shift register buffers' footprint through\nresource sharing. Considering that different processing\nelements may work on the same set of ifmaps, it is\npossible to share the same shift register buffers to assist\nthe triangular movement of inputs.\nInvestigation about ifmap tiling to reduce the area re-\nquired by the reconfigurable shift register buffers. At the\nmoment, these local buffers are constrained on the largest\nifmap size. By enabling tiling, it will be possible to reduce\nthe number of registers per buffer, by further improving\nthe energy efficiency of the architecture.\nIn the direction of high energy efficiency, we aim to\ndesign and tape-out a TrIM-based architecture on ASIC,\nthus unlocking its applicability in large-scale systems."}]}