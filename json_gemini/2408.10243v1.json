{"title": "TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks-Part II: Architecture and Hardware Implementation", "authors": ["Cristian Sestito", "Shady Agwa", "Themis Prodromakis"], "abstract": "Modern hardware architectures for Convolutional Neural Networks (CNNs), other than targeting high perfor- mance, aim at dissipating limited energy. Reducing the data movement cost between the computing cores and the memory is a way to mitigate the energy consumption. Systolic arrays are suitable architectures to achieve this objective: they use multiple processing elements that communicate each other to maximize data utilization, based on proper dataflows like the weight stationary and row stationary. Motivated by this, we have proposed TrIM, an innovative dataflow based on a triangular movement of inputs, and capable to reduce the number of memory accesses by one order of magnitude when compared to state-of-the-art systolic arrays. In this paper, we present a TrIM-based hardware architecture for CNNs. As a showcase, the accelerator is implemented onto a Field Programmable Gate Array (FPGA) to execute the VGG-16 CNN. The architecture achieves a peak throughput of 453.6 Giga Operations per Second, outperforming a state-of-the-art row stationary systolic array by ~ 5.1\u00d7 in terms of memory accesses, and being up to ~ 12.2\u00d7 more energy-efficient than other FPGA accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolutional Neural Networks (CNNs) are leading Al models in various domains, ranging from computer vision [1], [2] to speech recognition [3], [4]. Their wide applicability is justified by the use of convolutions, which infer locality between neurons, thus mimicking the behavior of human visual system to identify features from inputs [5]. However, this translates in: (i) high memory requirements to manage multi-dimensional feature maps (fmaps); (ii) billions of computations owing to the large fmaps to fulfill high standards of image resolution and accuracy [6]. For instance, the well-known VGG-16 CNN [7] requires up to ~22.7 MB of memory to deal with 8-bit input fmaps (ifmaps) and weights, over a total of ~30.7 billions of operations on 224\u00d7224 RGB images. Since former CLs process large fmaps, they require massive memory for inputs, as well as computations. Conversely, deeper CLs extract features requiring a dominant contribution of weights. In order to follow the fast pace dictated by CNNs' ex- plosion, hardware designers are continuously struggling to guarantee energy-efficient architectures, capable to offer real- time activity as well as low-energy footprint. While the former can be met by means of high-parallelized engines, the latter require to tackle the Von Neumann bottleneck, or memory wall [8]. The major energy-draining source is the communication between the parallel computing cores and the memory, either in the form of on-chip Static Random Access Memory (SRAM) or as an external Dynamic Random Access Memory (DRAM). For example, the relative energy cost for a 32-bit SRAM read is estimated to be 5 pJ for a 45nm process at 0.9V, which in turn dramatically increases to 640 pJ for a 32-bit DRAM read [9]. Overall, the DRAM contribution is ~200\u00d7 higher than a 32-bit multiplication performed by the computing core. Among different strategies, on-chip data reuse allows to reduce the number of DRAM reads, by locally storing fmaps and weights to maximize their utilization before being discarded [10]. Systolic Arrays (SAs) [11], [12] are promising candidates to mitigate the Von Neumann bottleneck by maximizing data utilization. These architectures consist of an array of Processing Elements (PEs) that mainly perform Multiply- Accumulations (MACs) between inputs and weights. These computing resources are connected each other to ensure two levels of reuse: (i) reuse at the PE level, where one or more types of data (input, weight, partial sum (psum)) are kept stationary as long as required; (ii) reuse at the SA level, where one or more types of data are rhythmically moved in specific directions, thus being consumed by different PEs for a certain number of cycles. The reuse at the PE level dictates the specific dataflow of the SA: therefore, it is possible to have Input- Stationary (IS) [13], Weight-Stationary (WS) [14], or Output- Stationary (OS) SAs [15], other than mixed combinations of them [16], [17]. While SAs easily manage linear layers, where straightfor- ward MACs are performed between inputs and weights, they are not suitable to directly cope with the workflow exhibited by CLs. Hence, several architectures have converted convolutions into matrix multiplications (Conv-to-GeMM) [18]\u2013[22]. In such an approach, data redundancy is required, which in turn hurts the memory capacity and the main memory accesses. In another direction, GeMM-free SAs have been proposed [23]\u2013[26]. For instance, Eyeriss [23] has introduced a differ- ent dataflow named Row Stationary (RS), where inputs and weights are reused in rows at the PE level, by preserving the locality dictated by the convolutional workflow. However, such a dataflow requires many PEs as well as data redundancy at the SA level. To avoid Conv-to-GeMM and maximize local data utiliza- tion, we have proposed a novel dataflow named Triangular Input Movement (TrIM) [27] where inputs follows a triangular movement, while weights are kept fixed at the PE level. According to a design space exploration, TrIM has shown one order of magnitude saving in terms of memory accesses when compared to the GeMM-based WS dataflow, while achieving 81.8% throughput improvement when compared to the RS dataflow. In this work, we propose an AI hardware architecture based on TrIM, dealing with multi-dimensional CLs for CNNs. As a case study, the TrIM architecture is implemented onto a Field Programmable Gate Array (FPGA) to accelerate the VGG-16 CNN at 150 MHz clock frequency. The design, consisting of 1512 PEs, exhibits a peak throughput of 453.6 GOPs/s, by dissipating ~4.2 W of power. In addition, the TrIM architecture outperforms Eyeriss [23] by ~ 5.1\u00d7 in terms of memory accesses, and shows the best energy efficiency among state-of-the-art FPGA counterparts. The main contributions of this work can be summarized as follows: A high-level scalable TrIM-based AI hardware architec- ture is presented. It incorporates 7 cores, each consisting of 24 slices of 3 \u00d7 3 PEs. A reconfigurable shift-register buffer is presented. It handles different ifmap sizes for optimal SA utilization. This unlocks the TrIM-based architecture to cope with all the CLs of a generic CNN, being run-time reconfigured. We perform a design space exploration for the VGG- 16 CNN using a TrIM-based analytical model, where throughput, memory size and I/O bandwidth are consid- ered. An FPGA-based implementation is presented to charac- terize the TrIM architecture in terms of resource occupa- tion, performance, power and energy. Comparisons with Eyeriss and state-of-the-art FPGA architectures are also presented, in order to highlight the benefits offered by the TrIM-based architecture. The rest of the paper is organized as follows: in Section II state-of-the-art hardware implementation of SAs are reviewed; the TrIM-based hardware architecture is introduced in Section III; a design space analysis is presented in Section IV, con- sidering throughput, I/O bandwidth and on-chip memory size; Section V reports the hardware implementation results, as well as comparisons with previous works; finally, conclusions are drawn in Section VI, with insights for future work."}, {"title": "II. RELATED WORKS", "content": "Given the way in which data flows between PEs, SAs are particularly suitable to manage Matrix Multiplications (MMs) between inputs and weights, being compatible with Fully- Connected Neural Networks (FCNNs). On the contrary, CNNS mainly execute convolutions, where inputs must be arranged in sliding windows before being processed with the respective weights. To achieve this, Conv-to-GeMM has been introduced to arrange sliding windows in a matrix form [28]. Several architectures based on SAs make use of Conv-to- GeMM [18]\u2013[21]. The Google Tensor Processing Unit (TPU) [18] hosts a SA consisting of 256\u00d7256 PEs. Based on the WS dataflow, inputs are supplied through First-In-First-Out (FIFO) buffers, and moved horizontally from left to right. The produced psums move vertically from top to bottom. In addition, an accumulation logic consisting of synchronization FIFOs and adders further process the psums, coming from the bottom side of the array, over multiple computational steps. According to this process, such a TPU is able to accommodate several NN workloads, including FCLs, CNNs, and Recurrent NNs. However, this flexibility has a cost in terms of memory capacity and accesses to cope with Conv-to- GeMM, thus degrading the energy efficiency. On the contrary, the accelerator proposed in [19] tackles the above issues by proposing an on-the-fly GeMM conversion. Specifically, a data feeder unit is interfaced to a SA of 16x16 PEs and based on the OS dataflow. The data feeder includes an addressing logic to fetch inputs from proper regions of the main memory, as well as FIFOs to schedule inputs towards the array over time. In [20], a Network-on-Chip based convolution is proposed to deal with GeMM. Inputs and weights are split into packets and provided to nodes consisting of parallel and distributed 3\u00d73 SAs. Despite this allows latency reduction, extra area is required to manage the NoC packets' scheduling. The work presented in [21] deals with sparse CNNs, using a SA of 8 \u00d7 32 PEs. Ping-pong buffers assist local data reuse. On another direction, some architectures [23]\u2013[26] have proposed alternative ways to skip the disadvantages of GeMM conversion. Eyeriss [23] is the first hardware chip proposing the RS dataflow, where both inputs and weights are circulated at PE level, while broadcasting between PEs at the SA level. An array consisting of 12\u00d714 PEs is interfaced to the global buffer through FIFOs for proper synchronization. Each PE, accommodating a two-stage multiplier and the adder, is equipped with scratch pads to circulate inputs and weights over different cycles. Moreover, a scratch pad for psums guarantees reuse over time. Each PE is also equipped with control logic to supervise the process. However, significant area is required to host the local scratch pads, other than negatively impacting the energy footprint due to the continuous switching activity of these memories. Moreover, the way in which the RS dataflow works requires that the area of the SA depends on the fmaps' sizes. As a result, to avoid a very large number of PEs, ifmaps' tiling must be carefully implemented. The RS dataflow is also exploited in [24], where a 3-D 3\u00d73\u00d727 SAs is interfaced to a scheduling module to guarantee data reuse. thus avoiding inputs and weights circulation at the PE level. Differently from the RS dataflow, Sense [25] offers a 32\u00d732 SA where different dataflows can be exploited: while weights can be retained at the PE level, psums can be either accumulated locally or forwarded vertically between PEs, thus also offering the possibility to meet the OS dataflow. However, the PE exhibits higher hardware complexity since it accommodates an extra buffer for psums as well as a sparsity management facility to skip zeroed computations from convolutions. The USCA architecture [26] uses 64\u00d74 SAs, each having 4 PEs. Based on the OS dataflow, each PE is equipped with a selection logic to meet convolutions at different granularity, ranging from dense computations to dilated and transposed convolutions. PEs are equipped with sequential connections and skipping logic to properly place weights for sparse convolutions. However, other than requiring extra control logic, more connections must be placed at the boundaries of each PE, thus making routing challenging. In the spectrum of hardware-based SAs, the TrIM-based architecture sits at the GeMM-free side, by proposing an empowered WS dataflow where inputs are reused among different PEs through an innovative triangular dataflow. This allows a significant reduction of memory accesses. Multiple instances of K \u00d7 K PEs are used, with K being the kernel size of the 3-D convolutional filters that accommodate the weights. Few extra shift register buffers assist the inputs reuse."}, {"title": "III. THE TRIM ARCHITECTURE", "content": "The TrIM-based AI hardware architecture is designed as a hierarchy consisting of three levels: TrIM Slice. This module is responsible for performing 2-D K \u00d7 K convolutions. Weights are kept station- ary by each Processing Element (PE), while inputs are reused between PEs by exploiting the innovative trian- gular movement. Specifically, each slice processes an independent ifmap and a specific kernel of one of the N 3-D filters. TrIM Core. This module accommodates multiple slices, each managing a specific ifmap and PM kernels be- longing to one of the 3-D filters. PM is the number of kernels processed in parallel. Additional adders allow the provisional outputs from each slice to be accumulated, thus generating one ofmap. TrIM Engine. This top-level module consists of PN cores, each dealing with an independent 3-D filter. PN is the number of filters processed in parallel and, in turn, the number of ofmaps generated in parallel. All the cores use the same set of ifmaps. Fig. 2 depicts an example of CL executed by the TrIM- based architecture. M = 4 ifmaps and N = 2 filters, each consisting of M= 2 kernels, are considered. The TrIM Engine accommodates two TrIM Cores, each hosting two TrIM Slices. In the specific example, the first TrIM Core processes ifmaps 0 and 1, with kernels 00 and 01 (related to the first filter). The second TrIM Core processes ifmaps 0 and 1, with kernels 10 and 11 (related to the second filter). The TrIM Cores then produce provisional ofmaps, by summing up the psums coming from their respective slices. The TrIM Engine accumulates over time the provisional ofmaps coming from the Cores. In the following sub-sections, each part of the architecture is presented in detail."}, {"title": "A. TrIM Slice", "content": "The TrIM slice includes an array of K \u00d7 K PEs, an adder tree and K \u2013 1 Reconfigurable Shift Register Buffers (RSRBs), as illustrated in Fig. 3. The computing array is organized in K rows (Rowi, with 0 < i < K), each having K PEs. Preliminarily, weights are fetched from the memory and provided to the PEs of Rowo as groups of K elements per cycle, then shifted from top to down. This process lasts K cycles to ensure that the current kernel is completely stored at the slice level. Thus, inputs are first grabbed from the periphery (Iext), moved in each row from right (IR) to left (IL), and forwarded to the closest SRB to complete the triangular movement. Each SRB dispatches K inputs (ID) to Rowi-1. Psums are first accumulated vertically through PEs and, finally, through an adder tree that finalizes the K \u00d7 K convolution. The generic PE is equipped with two input registers to store the external input Iext and the weight Wext (or W), respectively. Two cascaded multiplexers assist the PE in sup- plying the multiplier with the correct input (Iext, ID or IR). A subsequent adder accumulates the current product with the psum coming from the PE placed in the same column in Rowi-1. Finally, one output register delivers the current output either to the vertically-aligned PE in the Rowi+1 or to the Adder Tree, and another register provides the current input to the left PE. In order to make the slice agnostic to the ifmap size, the RSRBs is equipped with run-time reconfigurability. To achieve this, each RSRB consists of WIM registers, with WIM being the width of the largest ifmaps that typically constitutes the very first CL of a CNN. To manage the subsequent fmaps, having W\u2081 < WIM, the RSRB is split into multiple Sub-Buffers (SBs), each consisting of Lsb registers (with sb iterating over the SBs), as shown in Fig. 4. While the majority of SBs simply forwards data from right to left, few SBs provide the data stored in the latest K registers to a selection logic. This consists of a multiplexer, which provides a group of K inputs to feed the Rowi\u22121, thus finalizing the triangular movement. It is worth underlining that the parameter Lsb and the number of SBs feeding the multiplexer can be generic or customized. The Adder Tree is supplied by K psums coming from the bottom row of the array for the final accumulation. It consists of [log2(K)] stages, with the last one followed by an output register. To conclude the description of the slice, some observations about the data representation and precision are needed. In the current version, the PEs support B-bit unsigned integer inputs and B-bit signed integer weights. As a result, signed integer psums are provided. In particular, the psums coming from the bottom row of the array have a bit-width equal to 2 \u00d7 B + K. Then, after the adder tree, the current output is a signed integer consisting of 2 \u00d7 B + K + [log2(K)] bits."}, {"title": "B. TrIM Core", "content": "The TrIM core delivers 3-D convolutions by accommodating PM slices that operate in parallel, followed by an adder tree that further accumulates the PM provisional outputs, as depicted in Fig. 5. With reference to a generic CL, the slices are fed by PM ifmaps (among the M available), as well as by PM kernels from one of the N filters. Taking into account that each slice operates on different ifmaps and kernels, the parameter PM is constrained by the I/O bandwidth that the architecture can sustain. The subsequent adder tree produces a 2\u00d7 B + K + [log2(K)] + [log2(PM)]-bit result (core_out), followed by an output register. Although the adder tree complexity can constraint the speed, pipelining can be added for shorter critical paths and better timing closure."}, {"title": "C. TrIM Engine", "content": "The TrIM engine is the top-level of the hierarchy as shown in Fig. 6, where multiple cores work in parallel on the same broadcast inputs. Specifically, PN cores are responsible for generating as many ofmaps, among the N ofmaps to be produced. Considering that PN and PM are the parallelism parameters of the engine, up to [N/PN] \u00d7 [M/PM] computational steps are required to finalize the computations. In the generic step, each core executes a 3-D convolution between one filter of PM K \u00d7 K kernels and PM H1 \u00d7 W1 fmaps. As a result, the core_out generated by the core is not final, but needs to be accumulated over other [M/PM] - 1 iterations. To achieve this, the engine associates an extra adder and a Psums Buffer to each core. This buffer needs to store up to HOM \u00d7 WOM output activations, where HOM and WOM represent the sizes of the largest ofmaps. Each activation is 2 \u00d7 B + K + [log2(K)] + [log2(M)]-bit wide. For CLs handling smaller ofmaps, the buffer stores Ho \u00d7 Wo output activations, where Ho < Hom and Wo < W\u043em are the sizes of the current ofmap. Meanwhile, the remaining PN - 1 cores processes different 3-D filters, but using the same set of PM ifmaps. In order to automatize the entire process over time, a Control Logic supervises the engine. Given that the scheduling of operations is the same for all the slices (thus, for all the cores), the cost of the controller is amortized by sharing it across the entire system."}, {"title": "IV. CASE STUDY: VGG-16", "content": "In order to showcase the capabilities offered by the TrIM- based architecture, we consider the Visual Geometry Group (VGG) CNN [7] as a case-study. In particular, we refer to the VGG-16 architecture, where 13 CLs and 3 FCLs manage the image classification task. The focus of this research activity is oriented towards the hardware acceleration of the CLs only. We introduce some metrics to enable a design space explo- ration, in terms of achievable parallelism (i.e., the parameters PN and PM). Three metrics are taken under consideration: throughput, psums buffers size, and I/O bandwidth. In order to compute the throughput, the total number of operations and the execution time of the CNN under test are needed. Each CL performs 3-D convolutions between ifmaps and filters, demanding the following number of operations:\n$OPs = 2 \\times K \\times K \\times H_o \\times W_o \\times M \\times N$ (1)\nWhile this expression is general and independent by the architecture that processes the CLs' workload, the execution time is strictly constrained on the underlying architecture. As illustrated in Section III, the TrIM engine requires [N/PN]\u00d7 [M/PM] computational steps to finalize each CL. Each step can be split into two phases: weights' loading and computation phase. During weights' loading, PN \u00d7 PM \u00d7 K \u00d7 K B-bit weights must be provided to the architecture. Assuming that enough I/O bandwidth is available, it is supposed to fill one core with the needed weights every K clock cycles. Therefore, it is expected to complete the entire phase in PN \u00d7 K clock cycles. Thus, the computation phase may start: after an initial latency dictated by the pipeline stages of the engine (L1), Hox Wo cycles are needed to finalize the convolutions. The number of clock cycles (NC) is given by expression (2):\n$NC = L_1+[N/P_N] \\times [M/P_M] \\times (P_N \\times K + H_o \\times W_o)$ (2)\nThe execution time is retrieved by diving NC by the clock frequency (fCLK). The psums buffers size is considered because on-chip memories contribute heavily to the energy footprint, other than being limited resources on FPGAs. Psums buffers store the provisional outputs from the TrIM cores, thus enabling temporal accumulations. Given the considerations reported in Section III, each psums buffer stores at most a HOM \u00d7 WOM ofmap, where each activation is 2 \u00d7 B + K + [log2(K)] + [log2(M)]-bit wide at least. Assuming 32-bit activations, enough to satisfy any on-chip accumulation, and taking into account that PN buffers are accommodated into the TrIM engine, the total psums buffer size is given by equation (3):\n$Psums \\ Buffer \\ Size = P_N \\times H_{OM} \\times W_{OM} \\times 32$ (3)\nThe I/O bandwidth is dictated by the transmission of weights, ifmaps and ofmaps. Weights are supplied before any processing: for each clock cycle, PM \u00d7 K \u00d7 B bits are moved. Conversely, the number of input activations vary over different cycles, to meet the triangular movement requirements at the slice level. Focusing on K = 3, up to PM \u00d7 5 \u00d7 B bits may be moved. Finally, the PN cores move as many B-bit quantized output activations, which can be transmitted to the periph- ery every [M/PM] steps. Considering that weights do not overlap with ifmaps/ofmaps transactions, the I/O bandwidth is constrained by the expression below:\n$BW_{I/O} = (P_M \\times 5 + P_N) \\times B$ (4)\nFig. 7 shows the design space analysis, considering the metrics defined above. We span the analysis ranging the number of slices and cores from 1 to 24 and based on a clock frequency of 150 MHz, as for the FPGA implementation. As expected, the higher the number of cores (PN) and the number of slices per core (PM), the higher the throughput. The best-case with PN = PM = 24 leads to a performance of 1243 Giga Operations per Second (GOPs/s). Furthermore, it is interesting to note how varying the number of cores and slices per core may bring to the same throughput. For example, an architecture consisting of 4 cores and 16 slices per core reaches the same throughput of an architecture managing 16 cores with 4 slices each. This because both the architectures use 576 PEs. However, the first solution with 4 cores is more efficient, since the psums buffer size is 4\u00d7 lower, resulting in less area and energy requirements. The I/O bandwidth increases significantly when varying the number of slices per core. This because the slices operate on different ifmaps, whereas multiple cores operate on the same set of ifmaps. Referring to the same example provided for the throughput, an architecture using 4 cores with 16 slices per core requires 2.3\u00d7 more bandwidth than an architecture based on 16 cores with 4 slices per core. We can conclude that a TrIM-based architecture constrained on the psums buffer size can reach a competitive throughput when using more slices per core than cores. On the contrary, when the I/O bandwidth constraints the architecture, the same throughput may be achieved using more cores than slices per core."}, {"title": "V. HARDWARE IMPLEMENTATION", "content": "The TrIM Engine has been synthesized and implemented onto the FPGA of the AMD Zynq UltraScale+ XCZU7EV- 2FFVC1156 MultiProcessor System-on-Chip (MPSoC). The design has been carried out through the AMD Vivado 2022.2 tool, and using Verilog as Hardware Description Language. First, taking into account the design space reported in Section IV, the optimal parameters PM and PN have been retrieved. PN is constrained by the on-chip memory. Consid- ering that the XCZU7EV part offers 11 Mb of Block RAMS (BRAMs), using equation (3), we got PN = 7, supposing that each psums buffer stores up to 224 \u00d7 224 output activations, which refer to the worst-case scenario (first two layers of VGG-16). The parameter PM strictly depends on the available I/O bandwidth. The XCZU7EV device is interfaced to a 64- bit DDR4 memory and exhibits a peak bandwidth of 19200 MB/s. Supposing fCLK = 150 MHz, in line with state-of-the- art FPGA-based accelerators [21], [24], [25], BW1/0 = 1024, rounded to the closest power of 2. Therefore, considering equation (4) and PN = 7, we got P\u043c = 24. Hence, a TrIM Engine hosting PN = 7 cores, each having TM = 24 slices, has been synthesized and implemented onto FPGA. The TrIM-based architecture achieves a peak throughput of 453.6 GOPs/s, since 1512 PEs are responsible for performing 7 \u00d7 24 parallel convolutions. With specific reference to the VGG-16 model, the engine guarantees a throughput of 391 GOPs/s, only 13.8% lower than the peak throughput. This is motivated by high PE utilization, which reaches the 93% on average. In order to highlight the advantages offered by TrIM in terms of performance and local data utilization, we present a comparative analysis with Eyeriss [23], state-of-the-art systolic array proposing the RS dataflow.  Eyeriss, de- spite working at 1.3\u00d7 higher frequency, reaches a throughput ~ 16\u00d7 lower, because using only 168 PEs. TrIM requires 358.71 millions of memory accesses to retrieve and send ifmaps, weights, psums, ofmaps. Specifically, ifmaps account for 259.26 millions of accesses, weights for 14.03 millions, psums for 72.5 millions and 12.92 millions for ofmaps. On average, TrIM outperforms Eyeriss by ~ 5.1\u00d7, thanks to the effectiveness of the triangular movement of inputs at the slice level. Indeed, the inputs overhead per slice (i.e., the number of extra accesses) is only the ~ 5.8% of the total memory ac- cesses. Furthermore, it worth to mention that Eyeriss exhibits an additional contribution related to the scratch pads, sitting at the PE level. Considering the consumption breakdown reported in [23], it has been estimated that scratch pads contribute from ~ 12.9\u00d7 to ~ 16.5\u00d7 more than the external memory. The TrIM-based architecture is also compared to some state-of-the-art accelerators implemented onto FPGA [21], [24], [25]. Table II summarizes the results, considering: (a) device type, data precision, number of processing elements and dataflow; (b) area occupation, in terms of Look-Up Tables (LUTs), Flip-Flops (FFs), Digital Signal Processing slices (DSPs), and Block RAMS (BRAMs); (c) clock frequency, peak throughput in GOPs/s; (d) power and energy efficiency. Sense [25] exploits a SA consisting of 32\u00d732 PEs. Con- sidering that each PE must ensure both the WS and OS dataflow, other than guaranteeing the management of sparse convolutions, this results in higher hardware complexity and, in turn, more area. Indeed, even though TrIM makes use of ~1.5\u00d7 more PEs, Sense requires ~79.9% and ~72.9% more LUTs and BRAMs, respectively. In addition, while TrIM completely manages multiply-accumulations through cheap logic offered by LUTs, Sense adopts DSPs. In turn, this affects the power consumption and, thus, the energy efficiency, being ~3\u00d7 lower than TrIM. The architecture presented in [21] proposes a SA using 8\u00d732 PEs and dealing with the WS dataflow. Among the counterparts, this hardware engine is the cheapest in terms of logic and on-chip memory, except for using 257 DSPs to implement the PEs. In turn, this results as the lowest power footprint. However, considering that [21] reaches a peak throughput 5.9\u00d7 lower than TrIM, this reflects in an energy efficiency ~ 2\u00d7 lower. The SA introduced in [24] makes use of 3\u00d73\u00d727 PEs using the RS dataflow. In particular, it proposes a scheduling module that arranges the rows of data beforehand, thus simplifying the hardware complexity of PEs with respect to Eyeriss. However, despite using ~6.2\u00d7 less PEs than TrIM, this scales only ~2\u00d7 in terms of LUTs, because of extra logic requirements to manage sparse convolutions. Overall, TrIM outperforms [24] by 12.2x in terms of energy efficiency."}, {"title": "VI. CONCLUSION", "content": "In this work, we present a hardware architecture to show- case the TrIM dataflow. The architecture is organized in a hierarchical manner, to effectively cope with convolutional layers. Reconfigurable shift register buffers are included to make the engine agnostic to different ifmap sizes. As a case study, the VGG-16 CNN is considered and an architecture of 1512 processing elements is implemented onto the XCZU7EV FPGA. The achieved peak throughput is 453.6 Giga Opera- tions per Second. When compared to Eyeriss, a state-of-the-art accelerator based on the row stationary dataflow, the TrIM- based architecture guarantees ~ 5.1\u00d7 less memory accesses, confirming the effectiveness of the triangular input movement to maximize data utilization. In addition, the proposed archi- tectures results up to ~ 12.2\u00d7 more energy-efficient than other FPGA counterparts. Stimulated by these results, we plan the following future works: Reduction of the shift register buffers' footprint through resource sharing. Considering that different processing elements may work on the same set of ifmaps, it is possible to share the same shift register buffers to assist the triangular movement of inputs. Investigation about ifmap tiling to reduce the area re- quired by the reconfigurable shift register buffers. At the moment, these local buffers are constrained on the largest ifmap size. By enabling tiling, it will be possible to reduce the number of registers per buffer, by further improving the energy efficiency of the architecture. In the direction of high energy efficiency, we aim to design and tape-out a TrIM-based architecture on ASIC, thus unlocking its applicability in large-scale systems."}]}