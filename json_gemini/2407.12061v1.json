{"title": "Situated Instruction Following", "authors": ["So Yeon Min", "Xavi Puig", "Devendra Singh Chaplot", "Tsung-Yen Yang", "Akshara Rai", "Priyam Parashar", "Ruslan Salakhutdinov", "Yonatan Bisk", "Roozbeh Mottaghi"], "abstract": "Language is never spoken in a vacuum. It is expressed, comprehended, and contextualized within the holistic backdrop of the speaker's history, actions, and environment. Since humans are used to communicating efficiently with situated language, the practicality of robotic assistants hinge on their ability to understand and act upon implicit and situated instructions. In traditional instruction following paradigms, the agent acts alone in an empty house, leading to language use that is both simplified and artificially \u201ccomplete.\u201d In contrast, we propose situated instruction following (SIF), which embraces the inherent underspecification and ambiguity of real-world communication with the physical presence of a human speaker. The meaning of situated instructions naturally unfold through the past actions and the expected future behaviors of the human involved. Specifically, within our settings we have instructions that (1) are ambiguously specified, (2) have temporally evolving intent, (3) can be interpreted more precisely with the agent's dynamic actions. Our experiments indicate that state-of-the-art Embodied Instruction Following (EIF) models lack holistic understanding of situated human intention.", "sections": [{"title": "1 Introduction", "content": "Humans naturally engage in communication that is contextually situated, providing just enough information as necessary. This is because our use of language is predicated on an assumed common ground [7], which encompasses our shared history, actions, and environment. For instance, the instruction \"Can you bring me a cup?\" can vary in meaning depending on the context. If spoken while the speaker is donning rubber gloves by the kitchen sink, it likely refers to a dirty cup located on the living room table. Conversely, the same request made in front of the bathroom sink typically implies a need for a clean cup. While it is possible to seek clarification, humans generally interpret and respond to such requests accurately without additional information. This capability demonstrates how humans skillfully use environmental and action cues to interpret ambiguous language, crafting meanings that are intricately nuanced and context-specific."}, {"title": "2 Related Work", "content": "SIF builds on instruction following, agent alignment, and situated reasoning.\nInstruction Following. We contextualize our baselines models and benchmark,"}, {"title": "3 Dataset", "content": "SIF extends previous work that primarily focus on abstract, decontextualized, common-sense reasoning [8,16,36] by evaluating reasoning scenarios constructed in situ. Below, we explain the dataset design choices and guiding philosophy."}, {"title": "3.1 Tasks", "content": "Overview. Our tasks are structured into two distinct phases: (1) the exploration phase and (2) the task phase. During the exploration phase, the agent is allotted N steps to navigate around a static house environment where object assets are positioned. The value of N is determined to ensure the agent has sufficient steps to thoroughly scan the environment; specifically, N = 1.5 x (the number of steps required to achieve a complete map using frontier-based exploration techniques). Following the exploration phase, some objects are repositioned without the agent's knowledge. As the task phase commences, the agent receives an instruction (e.g., \u201cBring me a cup,\" \"Put the cup in the sink", "I took a cup with me. I'll be getting ready for bed": "."}, {"title": "3.2 Dataset Construction", "content": "We explain how the tuple (H,I,C, Pe, Pt, Pg) is constructed. In a total of 10 houses, each with human-annotated room metadata (which includes details such as the top-down (x,y) coordinates corresponding to each room, the function of the room (e.g., bedroom), and grounding details (e.g., a bedroom with a yellow wall)), we place four to ten assets in [ObjectCat]. Assets are from YCB [6], Google Scanned Objects [13], and ABO [10] datasets. We sample Pe, Pt of assets so that they are initialized in a visible space and graspable. More details on filtering trivial and unsolvable tasks are in Appendix A.1.\nLanguage Directives. We explain the generation of I and C. The Instruction carries information about Pg, which is the desired location of an object necessary for task success. We first sample Pg, by sampling a room in the scene, a"}, {"title": "4 Baselines", "content": "As discussed in Sec. 2, many recent state-of-the-art EIF agents are modular models with an LLM planner, connected to learned/engineered episodic memory, perception, and execution tools. We present two baselines within this high-performing family. The first is REASONER, a closed-loop baseline that adapts FILM [32] and the prompts of llm-planner [45], and ReAct [59], and prompter [19], an open-loop SOTA agent built for ALFRED [43]."}, {"title": "4.1 Reasoner", "content": "We adopt the modular structure of FILM [32]. REASONER operates through three main components: (1) a semantic mapper that updates an allocentric map from egocentric RGB and depth inputs, (2) a prompt generator, and (3) the planner (GPT-40 [1]) that generates high-level actions (Fig. 2)."}, {"title": "4.2 PROMPTER", "content": "PROMPTER employs an open-loop planner. We utilize GPT 3.5 for planning and search in lieu of BERT [12] (as in the original work [19]), tailored to our dataset's requirements. Its operational logic is straightforward: if an object has been identified on the map, PROMPTER interacts with it; otherwise, it initiates a search based on object and receptacle relationships. The planning phase utilizes a template that is populated with specific details, executed once at the task's outset. For instance, in response to a command like \"Put a shoe on the couch,\" GPT 3.5 is prompted to formulate a high-level plan adhering to a structured format (e.g. \"[Pick up OBJ, Put on the RECEP]).\" This ensures GPT 3.5 primarily focuses on filling in the template's variables. For Shum tasks, we ask if there is enough evidence about the human goal location, based on the layout of the house and the human utterance; due to open-loop planning, we ask this only at the beginning of the task. If the planner answers \"Not enough evidence\", then we add \"Follow Human\" as the first high-level action (so the plan becomes e.g. [Follow Human, Pick up OBJ, Give OBJ to Human]).\nFor the search phase, PROMPTER, in the original work, determines where to look by sampling from the logit values from a text query akin to \"Something you find at [MASK] is apple.\" Instead of sampling directly, we query the LLM, by integrating the latest map's text representation into a search prompt (e.g., \"In which room is the shoe likely to be? Please answer in the format of Room X.\u201d). This caters to our dataset's complexity, featuring multiple smaller rooms. Since object search happens multiple times, this has the same effect as sampling."}, {"title": "5 Results and Analysis", "content": "We evaluate the baseline models against the validation and test splits of SIF.\nOracle: The oracle baseline replaces the learned planner of REASONER with ground-truth plans like \u201cGo to Room X", "Grab Obj X": ".", "Metrics": "Task Success is determined by whether the object is correctly placed in or on the intended receptacle or the right room within 600 timesteps. A clear task in Shum is deemed unsuccessful if it necessitates following the human for over 50 timesteps, since unconditional probing should not is a behavior that reduces the utility of the robot. The Success Rate is calculated as the average of individual task successes across the dataset. The SPL (Success weighted by Path Length) [3] is calculated using the formula:\n$SPL = E_{tasks}[S \\frac{L^*}{max(L, L^*)}]$, where s is task success, L is path-length outputted by the model for a task, and $L^*$ is the path given by the oracle baseline. SPL is the primary metric of evaluation, because it tests correct reasoning strategies."}, {"title": "5.1 Results", "content": "Results from our experiments are presented in Table 6. This table notably shows the following facts about our dataset and baselines. First, the gap of model performance (both REASONER and PROMPTER) across PNP versus Shum, Sobj shows that PNP can be solved with commonsense and mechanistic combination, and the rest two tasks cannot. PROMPTER shows SPL of ~20% (~60% with oracle perception) on PNP tasks, showing that these tasks are on par with existing tasks like ALFRED. Conversely, on Sobj and Shum tasks, it shows much lower performance (~ 30% with reasoning alone); this shows that these tasks have challenges beyond common sense and progress tracking. The reasoning challenges of Sobj and Shum are backed by the performance of REASONER with oracle perception/manipulation; it shows a stark contrast in PNP tasks (~ 75%) and Sobj, Shum tasks (~50%). Overall, REASONER shows a better performance than PROMPTER, with and without GT perception/manipulation.\nSecond, it shows that perception is still a bottleneck, as found in works on previous datasets [8,32,34]. Even the oracle baseline, which has perfect reasoning, suffers with learned semantic segmentation and manipulation. When combined with learned reasoning, the drop tends to be larger (oracle perception vs. learned perception of PROMPTER and REASONER), since the planner faces a further uncertainty from perception error."}, {"title": "5.2 Ablations and Analysis", "content": "To analyze the impact of non-reasoning components (visual perception, manipulation) on task performance, Table 7 shows the effect of using heuristic manipulation and learned segmentation across our three settings. Our findings align with previous studies [19,32,33], further emphasizing that segmentation continues to be a significant obstacle to progress.\nThe focal point of our study is reasoning. To this end, Table 8 analyzes the failure modes of REASONER and PROMPTER when they leverage accurate semantic segmentation and manipulation. This table enumerates the proportions of various reasoning error modes that we observed in unsuccessful tasks. It categorizes these errors as follows: parsing errors, where the format of the LLM's response deviates from expectations; planning errors, which include inadequate tracking of progress and incorrect actions; strategic errors in locating humans, objects, or rooms; and actuation errors, such as mismanaging object interactions. The table lists these errors chronologically, noting that early errors like parsing mistakes can preclude later ones. Notably, strategic errors often manifest as unnecessary repetitive movements\u2014more than five high-level navigational or exploratory actions or as overly confident yet inaccurate predictions about a human's location.\nThis table reveals several key insights. First, PNP tasks exhibit fewer strategy errors compared to the other two tasks, which happens primarily due to room grounding issues with less obvious rooms when placing objects for instance, identifying a \"study room.\" This implies that PNP tasks revolve around more common-sense reasoning, such as inferring room functions from objects and familiar human activities typically associated with those rooms. In contrast, Sobj and Shum tasks exhibit higher rates of strategy errors due to their need for a more situated understanding of human motion and activity. Second, PROMPTER'S open-loop planning leaves it more susceptible to parsing errors. Notably, PROMPTER often fails to generate a plan for instructions that, while atypical, are feasible (e.g., \u201cPut a shoe on the couch\u201d), responding with \u201cI'm sorry, but I can't comply with that request.\" In contrast, REASONER exhibits fewer fundamental errors (those that precede strategy errors) and records a higher count of strategy errors. However, this does not imply that REASONER commits more strategic mistakes than PROMPTER; from Table 6, it achieves a higher number of successful tasks with correct strategic execution.\""}, {"title": "6 Conclusion", "content": "We present Situated Instruction Following (SIF), a new dataset to evaluate situated and holistic understanding of language instructions. Our dataset reflects aspects of real-world instruction following: (1) ambiguous task specification, (2) evolving intent over time, and (3) dynamic interpretation influenced by agent action. SIF is carefully crafted to assess language comprehension and reasoning in situ. We show that current state-of-the-art models struggle with this level of understanding, further highlighting the complexity and uniqueness of our dataset."}, {"title": "A Task Details", "content": ""}, {"title": "A.1 Task Filtering", "content": "Tasks that are invalid or trivial are filtered, using the oracle agent (Section 5). First, a task is invalid if, for the goal ([Obj]) asset, its Pe is not findable or Pt is not reachable. To filter tasks with invalid Pe, we check whether [Obj] was detected in the oracle agent's semantic map during the exploration phase; to filter tasks with invalid Pt, we run the oracle agent for task phase, and filter tasks where \"Grab Obj\" was unsuccessful. Furthermore, a task is trivial if, for the goal ([Obj]) asset, one of < Pe, Pt > or < Pt, Pg > are very close or within the same receptacle. This makes the task trivial because there is not much change between exploration and task phase, or between the initial state at task phase and the goal state. We use simulator data to access the parent receptacle of [Obj] asset for Pe, Pt, Pg and filter the task if any of these belong to the same parent receptacle."}, {"title": "A.2 Details on Language Directives", "content": "The complete list of arguments (Sec. 3.1) for language directives are shown in Tab.A.1 The full list of [RoomFunctionActivity] is shown in Table A.2."}, {"title": "B Prompt Examples", "content": ""}, {"title": "B.1 REASONER prompt examples", "content": "We show an example prompt for a Sobj task (Prompt 1). Furthermore, we show an example ambiguity calibration prompt for a Shum task (Prompt 2)."}, {"title": "C Execution Details", "content": ""}, {"title": "C.1 Map Update", "content": "In the semantic map, position of objects and humans are algorithmically updated. There are channels for the latest position of the human and the entire human trajectory. The latest human position channel is updated to the projection from the current semantic segmentation mask, upon detecting a human. A dedicated human trajectory channel tracks all observed movements; the latest timestep is inputted to the projected area from the semantic segmentation mask of the human. For grabbable objects, updates occur upon Grab Obj/Put Obj actions. When a Grab Obj action is executed and the grasper is closed, the closest target object is removed from the map, reflecting the agent's changed perception. When Put Obj is executed and the object is detected in the agent's view, the projection of the object is put back on its corresponding channel."}, {"title": "C.2 Execution Tools", "content": "Execution tools for REASONER/PROMPTER and their working details/affordance are in Table C.3."}, {"title": "C.3 Following and Anticipating the Human", "content": "When the \"Follow Human\" skill is called (Table C.3), REASONER and PROMPTER uses an algorithmic execution tool to follow the human. First, the latest position of the human is set as the goal, and the FMM Planner navigates until agent is within 1m. of the goal. When the goal is reached, the agent rotates 40 times to get a full 360\u00b0view, to scan the last location of the human. If the human is visible in a new location, this location is set as the new goal for the FMM Planner. Otherwise, stop is declared; in the prompt, it is stated that \"the agent either lost track of the human or the human stopped.\"\nTo describe human motion in prompts (illustrated in the bottom row of Fig. 3), we use the following process. If the human has been visible for over 30 timesteps, we estimate their expected position by extending a vector from their position 30 timesteps prior to their last observed position, scaled to represent 10 future timesteps. The end point of this vector is the expected human position (the green star in the middle row of Fig. 3). We then create a plane orthogonal to this vector, selecting the side opposite to the most recent human position (the depicted white side in the middle row of Fig. 3). Rooms covered more than 50% by this white area are included in the \"walking towards\" section of the prompt (e.g. rooms 1 and 2 at T = 20 in Fig. 3). If no room meets this criterion, \"walking towards\" is omitted, and the prompt only mentions the current room with the human (e.g. human seen in room 1 at T = 60 in Fig. 3)."}, {"title": "D Detailed Results", "content": ""}, {"title": "D.1 Oracle Baseline", "content": "The oracle baseline is established by supplying the reasoner with the ground truth plan and adding exploration as required. For Sobj and PNP tasks, the protocol is to activate explore_room when the target object is not immediately found. For Shum tasks, we employ a trajectory that avoids following, as this approach is consistently more efficient than those involving following, regardless of whether the tasks are clear or ambiguous. These trajectories are considered the optimal paths for computing the SPL (Success weighted by Path Length). However, there are a small number of instances when the oracle trajectory might fail, as documented in Table 6. In these cases, we default to using the maximum permitted task duration of 600 timesteps as the optimal path length for SPL calculation."}, {"title": "D.2 Full Results with Success Rate", "content": "Results on success rate, that correspond to the SPL results of Table 6 are shown in Table D.4 below."}]}