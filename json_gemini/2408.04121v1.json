{"title": "Can Rule-Based Insights Enhance LLMs for Radiology Report\nClassification? Introducing the RadPrompt Methodology.", "authors": ["Panagiotis Fytas", "Anna Breger", "Simon Baker", "Ian Selby", "Shahab Shahipasand", "Anna Korhonen"], "abstract": "Developing imaging models capable of detect-\ning pathologies from chest X-rays can be cost\nand time-prohibitive for large datasets as it re-\nquires supervision to attain state-of-the-art per-\nformance. Instead, labels extracted from ra-\ndiology reports may serve as distant supervi-\nsion since these are routinely generated as part\nof clinical practice. Despite their widespread\nuse, current rule-based methods for label ex-\ntraction rely on extensive rule sets that are lim-\nited in their robustness to syntactic variabil-\nity. To alleviate these limitations, we introduce\nRadPert, a rule-based system that integrates\nan uncertainty-aware information schema with\na streamlined set of rules, enhancing perfor-\nmance. Additionally, we have developed Rad-\nPrompt, a multi-turn prompting strategy that\nleverages RadPert to bolster the zero-shot pre-\ndictive capabilities of large language models,\nachieving a statistically significant improve-\nment in weighted average F1 score over GPT-\n4 Turbo. Most notably, RadPrompt surpasses\nboth its underlying models, showcasing the syn-\nergistic potential of LLMs with rule-based mod-\nels. We have evaluated our methods on two En-\nglish Corpora: the MIMIC-CXR gold-standard\ntest set and a gold-standard dataset collected\nfrom the Cambridge University Hospitals.", "sections": [{"title": "1 Introduction", "content": "Supervised deep learning for medical imaging clas-\nsification has accomplished significant milestones.\nIn the chest X-ray (CXR) domain, such models\nhave exhibited predictive capabilities on par with\nexpert physicians (Rajpurkar et al., 2018; Tang\net al., 2020) and are being utilized in collaborative\nsettings to increase clinician accuracy (Rajpurkar\net al., 2020).\nAnnotating medical images, however, is expen-\nsive and arduous: it requires a committee of ex-\npert radiologists to resolve the inherently high de-\ngree of annotator variance and subjectivity (Razzak\net al., 2018). This issue is particularly problem-\natic considering the global shortage of radiologists\n(Jeganathan, 2023; Kalidindi and Gandhi, 2023;\nKonstantinidis, 2023). Instead, we often have ac-\ncess to a form of distant supervision: the radiol-\nogy report. Radiology reports are semi-structured\nfree-text interpretations of an X-ray image and are\ngenerated as a routine part of clinical practice to\ncommunicate findings.\nIn the past, rule-based models (Irvin et al., 2019;\nPeng et al., 2017) have been used to extract struc-\ntured labels from radiology reports in various imag-\ning datasets, including ChestX-ray14 (Wang et al.,\n2017), CheXpert (Irvin et al., 2019), MIMIC-CXR\n(Johnson et al., 2019) and BRAX (Reis et al., 2022).\nHowever, those rule-based methods are often based\non elementary techniques and, thus, exhibit lim-\nited robustness to syntactic variation. Naturally,\nsupervised deep learning models offer superior per-\nformance through their robustness to syntactic vari-\nability (Smit et al., 2020; Jain et al., 2021b). In con-\ntrast, Large Language Models (LLMs) represent a\nsignificant improvement over rule-based models in\nan unsupervised setting and have achieved impres-\nsive performance in the field of radiology (Infante\net al., 2024; Adams et al., 2023; Liu et al., 2023).\nIn this paper, we present RadPert, a rule-based\nmodel built on the RadGraph knowledge graph\n(Jain et al., 2021a). RadPert leverages entity-level\nuncertainty labels from RadGraph, reducing the"}, {"title": "2 Related Work", "content": "Numerous natural language processing methods\nhave been developed to derive structured predic-\ntions from radiology reports (Peng et al., 2017;\nHassanpour et al., 2017; Pons et al., 2016; Bozkurt\net al., 2019; Wang et al., 2018). Many of those\napproaches are designed for the multitask classi-\nfication of radiology reports, written in English,\ninto labels representing prevalent pathologies from\nCXRs. Each such label can exhibit one of four\noutput classes: Null, Positive, Negative and Uncer-\ntain. CheXpert (Irvin et al., 2019), the rule-based\nSOTA, follows an approach based on regular ex-\npression matching and the Universal Dependency\nGraph (UDG) of a radiology report. Due to the\nrudimentary regular expression matching, however,\nCheXpert is sensitive to syntactic variation. Thus,\nmultiple over-generalized rules are used in an at-\ntempt to alleviate these shortcomings. Furthermore,\nthe UDG is a type of information extraction that\ndoes not explicitly identify negation and uncer-\ntainty. Therefore, its ability to detect uncertainty\nin complex phrases is hampered despite the exten-\nsive rule set. Extensions of CheXpert have been\ndeveloped for Brazilian Portuguese (Reis et al.,\n2022) and German (Wollek et al., 2024). CheXbert\n(Smit et al., 2020) is a semi-supervised model pre-\ntrained on automatically extracted labels from the\nCheXpert model, fine-tuned on manually annotated\nreports, and evaluated on 687 MIMIC-CXR gold-\nstandard test set reports. However, the published\nmodel weights\u00b9 of CheXbert differ from the origi-\nnal model. This discrepancy complicates compar-\nisons on the MIMIC-CXR dataset as the published\nmodel is fine-tuned on unspecified MIMIC-CXR\nmanually annotated reports, which can potentially\noverlap with the MIMIC-CXR gold-standard test\nset.\nRecent work has also explored the adoption of\nLLMs for radiology report classification. Specifi-\ncally, Dorfner et al. (2024) examine the zero and\nfew-shot capabilities of LLMs. However, they\nmainly treat the task as a binary classification for\neach pathology. Namely, for multitask classifica-\ntion, they only report the few-shot results on an\nunpublished institutional dataset. CheX-GPT (Gu\net al., 2024) utilizes zero-shot GPT-4 labels as a dis-\ntant supervision to fine-tune a BERT-based model.\nNonetheless, they also simplify the task into binary\nclassification.\nAlternative approaches to the classification of\nchest X-rays (CXRs) explore moving away from\nthe distantly supervised paradigm of training uni-\nmodal vision models on classifying structured la-\nbels extracted from radiology reports. In lieu of\nstructured prediction, Vision-Language (VL) mod-\nels are trained to align the embedding representa-\ntions of CXRs with the representations of the cor-\nresponding radiology reports via self-supervised\ncontrastive learning objectives (Huang et al., 2021;\nBoecking et al., 2022; Tiu et al., 2022; Wang et al.,\n2022; Bannur et al., 2023). This alignment task is\ntransformed into CXR classification through the\ncosine similarity of CXR embeddings to the em-\nbeddings of textual prompts representing the ex-\nistence or absence of pathologies. However, vi-\nsion models trained with the structured prediction\nparadigm outperform VL models such as CheXzero\n(Tiu et al., 2022), even when the latter utilizes an\nexpert-annotated validation set for selecting opti-\nmal classification thresholds.\nIn this paper, we will focus on improving the\nunsupervised SOTA for the multitask classification\nof radiology reports."}, {"title": "3 Methods", "content": "Similar to CheXpert and CheXbert, we will focus\non the multitask classification of CXR radiology\nreports. Specifically, our models classify thirteen\nlabels that correspond to pathologies (Atelectasis,\nEdema, Cardiomegaly, Consolidation, Enlarged\nCardiomediastinum, Fracture, Lung Lesion, Lung\nOpacity, Pleural Effusion, Pleural Other, Pneumoth-"}, {"title": "3.2 RadPert", "content": "In order to overcome the limitations of existing\ntools, we have designed RadPert. RadPert incorpo-\nrates hand-crafted rules with the RadGraph (Jain\net al., 2021a) knowledge graph."}, {"title": "3.2.1 RadGraph Information Schema", "content": "RadGraph (Jain et al., 2021a) defines an infor-\nmation schema specifically designed for radiol-\nogy reports. It contains two top-level entity types:\nAnatomy (ANAT) and Observation (OBS). Anatomy\nentities describe bodily anatomical structures (e.g.\n\"lobe\") and their spatial characteristics (e.g. \"left\").\nObservation entities include pathological abnor-\nmalities (e.g. \"opacities\u201d), diagnosed diseases (e.g.\n\"pneumonia\") and various other characteristics (e.g.\n\"acute\"). It is important to note that Observation\nentities are further categorized into three second-\nlevel attributes: Definitely Present (DP), Definitely\nAbsent (DA) and Uncertain (U).\nAdditionally, RadGraph defines three types of\ndirected relations between entities. Firstly, the sug-\ngestive of relation indicates that some Observa-\ntion implies the existence of another Observation.\nSecondly, located at relations account for Observa-\ntions relating to specific Anatomies. Finally, modify\nrelations can exist only between the same type of\nentity and describe the characteristics relating to a\nspecific entity (e.g., modify(\u201cleft\u201d, \u201clung\")).\nThe RadGraph model is based on the Dy-\nGIE++ (Wadden et al., 2019) framework initial-\nized with PubMedBERT weights (Gu et al., 2021).\nThe model is fine-tuned on 500 expert-annotated\nMIMIC-CXR reports based on the RadGraph infor-\nmation schema.\""}, {"title": "3.2.2 RadPert Pipeline", "content": "RadPert employs the following four-stage pipeline:\nKnowledge graph extraction. We first extract\nthe RadGraph entities and relations from radiology\nreports. Utilizing RadGraph instead of the UDG\nallows uncertainty and negation classes to be ex-\ntracted at an entity level. Thus, the negation and\nthe uncertainty of various complex phrases can be\ndetermined based on those classes, reducing the\nneed for complex negation and uncertainty rules.\nMention extraction. In this stage, for each\npathology label, we have adapted and simplified\nthe CheXpert rules (Irvin et al., 2019) so they can\nbe applied to RadGraph entities and relations. Es-\nsentially, those rules can be represented as graphs"}, {"title": "3.3 RadPrompt", "content": "RadPert, through its rules, implicitly encodes ex-\npert knowledge vital to classifying radiology re-\nports. However, as a rule-based system, it is still\nsensitive to syntactic and lexical variability. To al-\nleviate this limitation, we propose RadPrompt, a\nzero-shot prompting technique that injects prompts\nwith insights derived from the application of Rad-\nPert. RadPrompt, as seen in Figure 1, employs a\ntwo-turn prompting strategy.\nIn the first turn, the zero-shot prompt contains\ninstructions, which define the task, and the radi-\nology report that needs to be classified. After a\nresponse is received from the LLM, the first-turn\nclassification outcome is compared with the output\nof RadPert.\nIn the second turn, a prompt is constructed by\nspecifying that a rule-based model is used to verify\nthe validity of the LLM's answer. Hints are then\nadded by specifying for each pathology either Rad-\nPert's agreement with the LLM or the radiology re-\nport sentence that leads RadPert to a disagreement.\nThis is possible since RadPert, as a rule-based sys-"}, {"title": "3.3.1 Base Model", "content": "As a base model for the RadPrompt strategy, we\nexplore various LLMs, including API-based mod-\nels such as Gemini-1.5 Pro (Reid et al., 2024),\nClaude-3 Sonnet, GPT-4 Turbo (OpenAI, 2023),\nand Llama-2 (Touvron et al., 2023). In the case of\nLlama-2, we are using the 70 billion parameter chat\nvariant, quantized with the Int 4 AWQ method (Lin\net al., 2024), which we run locally with a single\nNVIDIA RTX 6000 Ada GPU."}, {"title": "4 Results and Discussion", "content": "To allow comparison with previous work (Irvin\net al., 2019; Smit et al., 2020), for each pathol-\nogy, we evaluate our methodology based on the\nweighted average F1 score across three aspects\nof the task: negation detection, positive mention\ndetection, and uncertainty detection. We report\nthe F1 scores of the sub-tasks in the Appendix.\nEach of those sub-tasks amounts to binary classi-\nfication. For instance, Negative classes are trans-\nformed into positive in negation detection, while\nthe other classes are transformed into negative. Pos-\nitive mention detection and uncertainty detection\nare constructed with an analogous logic. The re-\nported scores correspond to the averages across\n1000 bootstrap replicates (Efron and Tibshirani,\n1986), reported along the 95% Confidence Inter-\nvals (CI)."}, {"title": "4.2 Data", "content": "For internal evaluation, we are evaluating the mod-\nels on the gold-standard test set of annotated ra-\ndiology reports used in the MIMIC-CXR paper\n(Johnson et al., 2019). MIMIC-CXR is considered\nan internal dataset for methods based on RadPert\nsince RadGraph is trained on MIMIC-CXR radi-\nology reports. The MIMIC-CXR gold-standard\ntest set contains 687 radiology reports that do not\noverlap with the training and validation set of Rad-\nGraph.\nFor external evaluation, we have collected a pri-\nvate dataset from the Cambridge University Hos-"}, {"title": "4.3 RadPert Evaluation", "content": "In Table 1, we report the weighted average F1\nscores across the sub-tasks of positive mention\ndetection, negation detection, and uncertainty De-\ntection for the MIMIC-CXR and CUH datasets.\nWe are also reporting the improvements over the\nCheXpert labeler alongside their confidence inter-\nvals. Radpert achieves a statistically significant\nimprovement both on average and on the majority\nof the pathologies. Namely, for MIMIC-CXR, Rad-\nPert is 8.0% (95% CI: 5.5%, 10.8%) better than\nCheXpert, yielding an average F1 score of 0.757\n(95% CI: 0.779, 0.800).\nIn Table 6 of the Appendix, we also report fine-\ngrained results in the distinct sub-tasks. In addition\nto the sub-tasks of negation, positive mention, and\nuncertainty detection, we also report the perfor-\nmance improvement in mention detection. Men-\ntion detection treats Null as the positive class, and\nNegative, Uncertain, and Positive as the negative\nclass."}, {"title": "4.3.1 Discussion on RadPert's Performance", "content": "We observe performance improvement in all sub-\ntasks. The strongest improvement is achieved in\nthe uncertainty detection task, showcasing the ef-\nfectiveness of utilizing the uncertainty labels of\nRadGraph. However, the improvement in mention\ndetection is marginal. A primary cause of mention\ndetection failure is the reliance on the RadGraph\nmodel, which occasionally fails to recall all entities\nand relations within a radiology report.\nFocusing on specific pathologies, RadPert fails\nto consistently outperform CheXpert for Atelecta-\nsis, Edema, and Pleural Effusion. In the case of\nAtelectasis and Edema, the rule sets are straight-\nforward, and their mentions often lack syntactic\nvariability in practice, offering limited benefit from\nthe uncertainty-aware entity representations of Rad-\nGraph. Regarding Pleural Effusion, RadPert is hin-"}, {"title": "4.4 RadPrompt Evaluation", "content": "In Table 2, we present the improvement in the\nweighted average F1 score of RadPrompt for vari-\nous base LLMs on the MIMIC-CXR gold-standard\ntest set. Specifically, we compare the revised classi-\nfication outcome of the second-turn prompt, which\nis infused with RadPert hints, to the first-turn clas-\nsification outcome. For all tested LLMs, we ob-\nserve that the RadPrompt strategy leads, on average\n(across pathologies), to a statistically significant im-\nprovement over the baseline zero-shot prompting.\nFor clarity, in Tables 7, 8, 9, 10 and 11 of the Ap-\npendix, we also report the task-specific F1 scores\nof the first and second turns of RadPrompt.\nFurthermore, we compare RadPrompt's second-"}, {"title": "4.4.1 Discussion on RadPrompt's\nPerformance", "content": "We can observe from Tables 2 and 3 that Rad-\nPrompt on Claude-3 Sonnet and on GPT-4 Turbo\nexceeds, on average, both RadPert and the initial\nLLM predictions. Namely, RadPrompt with GPT-4\nTurbo is 2.1% (CI 0.3%, 4.1%) better than baseline\nGPT-4 Turbo and 1.4% (CI -0.5%, 3.2%) better\nthan RadPert.\nFocusing on individual pathologies, we notice\nthat RadPrompt with a Gemini-1.5 Pro base man-\nages to outperform both of its underlying mod-\nels for Pleural Effusion, Pneumonia, and Support\nDevices. Additionally, RadPrompt with Claude-\n3 Sonnet surpasses its underlying models in the\ncase of Lung Lesion, Pleural Effusion, Pneumo-\nnia, Pneumothorax, and Support Devices. For a\nGPT-4 Turbo base, the same behavior is observed\nfor Consolidation, Pleural Effusion, Pleural Other,\nPneumonia, and Pneumothorax. The ability of Rad-\nPrompt to boost the performance of both its under-\nlying models demonstrates the potential of combin-\ning the language reasoning capabilities of LLMs\nwith the insights encoded in rule-based models.\nIn Table 4, we present a fine-grained comparison\nbetween the first and second turns of RadPrompt.\nWe observe that all models, with the exception\nof GPT-4 Turbo, initially struggled to understand\nthat we intended to classify only those patholo-\ngies explicitly mentioned in the report. This ef-\nfect disproportionately affects the Negative class\nsince Null is often conflated with Negative. The\ndistinction, however, between those two labels is\nnon-negligible. Inconsistencies often exist between\nthe gold-standard labels extracted directly from"}, {"title": "5 Limitations", "content": "While this study demonstrates promising improve-\nments in radiology report classification using the\nRadPrompt methodology, several limitations must\nbe considered.\nRadPert and RadPrompt are exclusively devel-\noped and tested for the English language. The\nstudy also centers around a list of pathologies typi-\ncal of chest X-rays. As such, the extension of our\nmethodologies to other languages, types of med-\nical imaging, and additional pathologies was not\nverified.\nFurthermore, previous studies have highlighted\ndiscrepancies between labels from radiology report\nannotations and those from the corresponding imag-\ning study annotations (Jain et al., 2021b; Wood\net al., 2020). The source of such inconsistencies\nincludes incomplete radiology report impressions,\nhierarchical relationships within labels, and the un-\ndeniable uncertainty of the task. In future work, we\naim to study this effect within the CUH test set.\nDue to ethical considerations, we are currently\nunable to perform inference for the CUH test set\nthrough third-party APIs. Thus, we have not evalu-\nated RadPrompt externally for SOTA LLMs. We\nexpect to overcome this limitation after the planned\nrelease of the CUH dataset.\nAdditionally, we cannot estimate the computa-\ntional cost and carbon footprint for GPT-4-based\nRadPrompt due to a lack of specific metrics. In\nthe Appendix, we provide carbon footprint esti-\nmates for the Llama-2-based RadPrompt, which\nis significantly higher than RadPert and CheXpert.\nNonetheless, RadPert delivers performance compa-\nrable to GPT-4 while operating on a commercial\nCPU with minimal carbon emissions, underscoring\nits benefits in resource-limited environments.\nFinally, there is an inherent degree of ambiguity\nin classifying radiology reports, especially as it per-\ntains to the Uncertainty labels. We aim to extend\ncurrent datasets with labels from multiple annota-"}, {"title": "6 Conclusions", "content": "This paper introduced RadPert, a rule-based system\nenhanced by the RadGraph information schema,\ndemonstrating significant improvements in the clas-\nsification of radiology reports. By leveraging entity-\nlevel uncertainty labels, RadPert reduces reliance\non comprehensive rule sets. Our evaluations show\nthat RadPert surpasses CheXpert, the previous rule-\nbased SOTA, by achieving an 8.0% (95% CI: 5.5%,\n10.8%) increase in F1 score, with confidence inter-\nvals strongly supporting this improvement.\nFurther extending the application of RadPert,\nwe developed RadPrompt, a multi-turn prompting\nstrategy that utilizes insights from RadPert to en-\nhance the zero-shot prediction capabilities of large\nlanguage models. RadPrompt demonstrated a 2.1%\n(95% CI: 0.3%, 4.1%) improvement in F1 score\nover GPT-4 Turbo, indicating its potential to re-\nfine predictions in clinical settings. These results\nhighlight the growing synergy between structured\nrule-based systems and large language models, of-\nfering a promising direction for future research in\nbiomedical Natural Language Processing.\nAs we continue to refine these tools, future\nwork will focus on expanding the existing datasets\nand addressing the discrepancies between gold-\nstandard image labels and those extracted from\nradiology reports."}, {"title": "Code and Data Availability", "content": "Code for RadPert and RadPrompt is available on\nGitHub. The CUH dataset is planned to be re-\nleased in the following months while managed and\nmade available through the hospital's clinical infor-\nmatics unit."}, {"title": "Ethical Considerations", "content": "For the MIMIC-CXR gold-standard test set, access\nto LLMs through APIs conforms to the PhysioNet\nresponsible use guidelines.\nThis ethical agreement with Cambridge Univer-\nsity Hospitals currently limits the use of third-party\nAPIs, but it is being revised prior to the dataset's\nrelease."}]}