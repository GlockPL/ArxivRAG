{"title": "IMMUNE: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Vaibhav Sigh", "Tianrui Guan", "Mengdi Wang", "Alvaro Velasquez", "Ahmad Beirami", "Furong Huang", "Dinesh Manocha", "Amrit Singh Bedi"], "abstract": "With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks-carefully crafted image-prompt pairs that compel the model to generate harmful content. In this work, we first highlight a critical safety gap, demonstrating that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model during decoding to defend against jailbreak attacks. Additionally, we provide a rigorous mathematical characterization of IMMUNE, offering provable guarantees against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that IMMUNE effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, IMMUNE reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.", "sections": [{"title": "1. Introduction", "content": "Multi-modal large language models (MLLMs) have made remarkable progress in vision-language reasoning tasks such as visual question-answering [1, 2, 40] and image caption-ing [1, 13]. However, ensuring their outputs are safe and free from discrimination, disinformation, and harmful content is critical for their adoption in social applications. Safety alignment through reinforcement learning from human feed-back (RLHF) has shown promise in aligning model behavior with social norms to prevent harmful outputs [5, 36, 45, 52]. Despite these significant alignment efforts, recent studies reveal that MLLMs remain vulnerable to jailbreaking attacks, where malicious image-text pairs bypass safety mechanisms, raising critical safety concerns [12, 16, 19, 26, 30, 33, 34, 37].\nWhy training-time safety alignment might not be enough? Fine-tuning-based safety alignment approaches rely on a static prompt distribution, leaving them inherently vulnerable to jailbreak attacks. These attacks exploit this limitation by solving a prompt (text or image) optimization problem to craft adversarial prompts that bypass the safety mecha-nisms of aligned models [30, 54]. In this work, we present an intriguing impossibility of safety against jailbreaks re-sult, demonstrating that for any given safety-aligned model, there always exists a jailbreak prompt distribution. This un-derscores the fundamental limitation of relying solely on training-time safety alignment procedures to ensure robust-ness. To establish this result, we reformulate the jailbreak problem as an inverse alignment problem (detailed in Section 3.2) and prove the existence of an adversarial prompter that can generate harmful responses from a safe model. Further-more, fine-tuning-based approaches are resource-intensive, requiring large annotated datasets of safe image-text pairs and incurring significant computational costs to retrain billions of parameters. These challenges lead to a critical question: \"Is it possible to develop a provable defense against jailbreaks in MLLMs?\"\nThe quick answer is yes, and this forms the focus of our work. To build provable defenses, we propose shifting from training-time alignment to inference-time safeguarding. Although initial efforts in this direction [10, 38] show promise, they ex-hibit weak empirical performance. Further, Weng et al. [39] note that adding a safety prompt to the input [38] can lead to overly cautious behavior, resulting in generic responses like, \u201cI am sorry...\" even for benign queries (see Figure 1),\""}, {"title": "2. Related Works", "content": "Jailbreak attacks on MLLMs. Jailbreaking MLLMs can be achieved by manipulating the visual input [11, 20, 30], the text prompt [23], or a combination of both [43]. Qi et al. [30] demonstrated that adding adversarial noise\u2014imperceptible to humans and guided by the model's input gradients-to"}, {"title": "3. Problem Formulation", "content": "3.1. Preliminaries\nMultimodal Large Language Models (MLLMs) are au-toregressive text generation models that can process multiple modalities, such as image [50], video [47], and audio [24], in addition to text. In this work, we consider input to MLLM in the form of an image and a text prompt. Mathematically, we denote a MLLM as a mapping $\\pi_{safe} : I \\times V^N \\rightarrow V^M$, where $I$ represents the image space, and $V$ denotes the vocabulary set. The MLLM $\\pi_{safe}$ input consists of an image $I \\in I$ and a sequence of tokens $x := \\{x_1,x_2,\\ldots, x_n\\}$ (referred as prompt), where each token $x_i \\in V$. Given $X_{input} := [I, x]$, the MLLM $\\pi_{safe} (\\cdot|X_{input})$ will generate the output sequence $y:= \\{y_1, y_2,\\ldots\\ldots, y_M\\}$, where $y_i \\in V$, in a token by token fashion.\nJailbreaking MLLMs. Despite safety tuning efforts aimed at preventing MLLMs from generating harmful content, jail-break attacks as formulated in Qi et al. [30], Zou et al. [54] can make the safety aligned model $\\pi_{safe}$ to generate harmful outputs with the help of an adversarial prompt $x_{adv}$ obtained by solving\n$\\displaystyle x_{adv} := \\underset{q \\in B}{\\text{arg min }} - \\sum_{i=1}^{m} \\log (\\pi_{safe} (Y_i |[X_{input}, q]))$, (1)\nwhere $B$ is a constraint on the adversarial prompts and $\\{Y_i\\}_{i=1}^{m}$ are few initial tokens of intended harmful response. For example, in Figure 1 (Left), we show that given an im-age of a house generated by stable diffusion [32] and then perturbed with adversarial noise, along with a malicious user query to explain \"How to break into and rob a house\", the model is forced to generate unsafe responses, effectively by-passing its safety alignment."}, {"title": "3.2. Safeguard Against Jailbreaks", "content": "From the definition of adversarial prompt in Equation (1), we observe that $x_{adv}$ can lead the MLLM model to generate a harmful response, $y_{unsafe} \\sim \\pi_{safe}(x_{adv})$. This raises the critical question: How can we prevent this from happening and ensure that the MLLM, $\\pi_{safe}$, always generates a safe response, regardless of the input prompt? Before addressing this central question, we must first consider two key prelimi-nary questions:\nQ1: Why is training-time safety alignment alone insufficient to protect against jailbreak attacks?\nQ2: What additional measures are required to provide better safety guarantees against jailbreak attacks?\nJailbreaking as an Inverse Alignment. To answer Q1, we first reformulate jailbreak attack as an Inverse Alignment problem and prove that there exists an adversarial prompt distribution $P_{adv}$ for any training time safety-aligned model $\\pi_{safe}$. This existence would establish the impossibility of defense against jailbreak with training time safety alignment alone. To explore this, we begin by considering the jailbreak problem in Equation (1) and defining the notion of an unsafe reward by using the objective in (1) as follows:\n$\\displaystyle R_{unsafe} (X_{input}, q) := \\sum_{i=1}^{m} \\log (\\pi_{safe} (Y_i |[X_{input}, q]))$. (2)\nWe note that the unsafe reward is defined for a given safe model $\\pi_{safe}$ and generated harmful response tokens $\\{y_i\\}_{i=1}^m$. The reward value in Equation (2) will be higher for $q$ which would result in high likelihood of generating a harmful se-quence of tokens $\\{y_i\\}_{i=1}^m$, and low otherwise. We emphasize"}, {"title": "4. Inference Time Safety Alignment", "content": "In this section, we propose improving the safety of response generation by using alignment through the decoding proce-dure proposed in [4, 25]. Our key insight is that well-trained safety rewards, denoted by $R_{safe}(x, y)$, are often accessible online, for instance, RewardBench [15]. This safety reward will give high scores for safe text and low scores for unsafe text. With this understanding, we propose to solve the fol-lowing decoding problem at inference time for each token t:\n$\\displaystyle \\pi_{safe-dec}(s_t) := \\underset{\\pi}{\\text{arg max}}\\mathbb{E}_{z \\sim \\pi(\\cdot | s_t)} [Q_{safe} (s_t, z)] \\\\ - \\alpha KL(\\pi(\\cdot|s_t)||\\pi_{safe}(\\cdot|s_t))$, (5)\nwhere $s_t := [x_{adv}, y_{<t}]$ and $y_{<t} = [y_0, y_1,\\ldots\\ldots, y_{t-1}]$ capture the tokens generated so far. In Equation (5), the term $Q_{safe} (s_t, z)$ is the action-value function defined as:\n$\\displaystyle Q_{safe} (s_t, z) = \\mathbb{E}_{\\tau \\sim P_{safe}(\\cdot |[s_t,z])} [R_{safe} ([s_t, z], \\tau)]$, (6)"}, {"title": "5. Theoretical Insights", "content": "In this section, we formally derive a bound on the sub-optimality of our proposed decoding-based approach under an adversarial prompt distribution. Given a safe prompt dis-tribution $p_0$ and an adversarial prompt distribution $p_{adv}$, the sub-optimality gap of our algorithm can be rigorously defined as\n$\\displaystyle \\Delta_{sub-gap} (X_{input}) \\\\ := \\mathbb{E}_{x \\sim p_0 (X_{input})\\atop y \\sim p^*(x)} [R_{safe} (x, y)] - \\mathbb{E}_{x \\sim P_{adv} (X_{input})\\atop y \\sim p_{safe-dec}(x)} [R_{safe} (x, y)]$. (8)\nThe suboptimality gap in (10) essentially captures how safe our proposed decoding-based approach is in generating re-sponses to adversarial prompts. We establish an upper bound in the following Theorem 1."}, {"title": "6. Experiments", "content": "6.1. Experimental Details\nJail-break Datasets. We conduct a comprehensive evalu-ation by considering both image-based and text-based jail-"}, {"title": "6.3. Capability Evaluations Results", "content": "IMMUNE preserves the model's original capabilities. An ef-fective jailbreak defense strategy should minimize the attack success rate while retaining the model's original capabilities. To assess this, we compare the visual comprehension abilities of various MLLMs employing different defense strategies on the MM-Vet dataset [44]. This multimodal benchmark evaluates MLLM responses across six categories: Recog-nition, Knowledge, Optical Character Recognition, Spatial Awareness, Language Generation, and Math. We report the average performance across all categories in Figure 3. Our results indicate that, compared to other defense strategies, IMMUNE achieves the highest score on MM-Vet, demonstrat-ing that it not only enhances model safety but also preserves the model's original capabilities."}, {"title": "6.4. Inference Speed Evaluations results", "content": "Inference Time of Immune. In Table 5, we compare the inference time of various jailbreak defense strategies across different MLLMs. Specifically, we report the average re-sponse generation time, in seconds, over 100 prompts to ac-count for variability in prompt lengths. All defense strategies were evaluated using the same hardware and software con-figuration as detailed in Appendix 8. Among the baselines, COCA [10] exhibits the longest inference time-nearly dou-ble that of the original decoding process as it requires two forward passes. While our method, IMMUNE, has a slightly higher inference time than original and AdaShield [38], this tradeoff is well-justified by the substantial reduction in ASR.\nAblations on hyper-parameters. In Section 6, we demon-strated the superior efficacy of Immune compared to the baseline defense strategies through a comprehensive evalua-tion. In this section, we present an ablation study on different hyperparameters, such as the number of tokens sampled (k), and the alignment parameter (a) as defined in Algorithm 1. We report the ablation results on k and a in Figure 4. Specif-ically, we measure the attack success rate and model utility (measured by MM-Vet score [44]) of the generated responses based on different combinations of the hyperparameters k and a. Our observations indicate that using k = 10 and a = 1.0 leads to optimal ASR and model utility."}, {"title": "7. Conclusion", "content": "In this work, we introduce IMMUNE, a provable inference-time defense framework designed to protect MLLMs against jail-break attacks. IMMUNE employs a safety-aware reward model to align the MLLM during inference, effectively mitigating jailbreak vulnerabilities. We present a precise mathemati-cal formulation of our defense framework, framing jailbreak mitigation as an alignment problem. Through extensive ex-periments, we demonstrate that IMMUNE consistently and significantly outperforms competitive baselines, enhancing model safety while preserving the model's original function-ality."}]}