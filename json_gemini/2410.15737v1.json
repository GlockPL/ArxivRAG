{"title": "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "authors": ["Quang Hieu Pham", "Hoang Ngo", "Anh Tuan Luu", "Dat Quoc Nguyen"], "abstract": "Retrieval-augmented generation (RAG) methods are viable solutions for addressing the static memory limits of pre-trained language models. Nevertheless, encountering conflicting sources of information within the retrieval context is an inevitable practical challenge. In such situations, the language models are recommended to transparently inform users about the conflicts rather than autonomously deciding what to present based on their inherent biases. To analyze how current large language models (LLMs) align with our recommendation, we introduce WhoQA, a public benchmark dataset to examine model's behavior in knowledge conflict situations. We induce conflicts by asking about a common property among entities having the same name, resulting in questions with up to 8 distinctive answers. WhoQA evaluation set includes 5K questions across 13 Wikidata property types and 150K Wikipedia entities. Our experiments show that despite the simplicity of WhoQA questions, knowledge conflicts significantly degrades LLMs' performance in RAG settings.", "sections": [{"title": "1 Introduction", "content": "Recent large-scale pretrained language models (LLMs) excel in tasks requiring natural language understanding (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2023; OpenAI et al., 2023). However, they often \"hallucinate\" plausible but incorrect content due to outdated or incorrect pre-training information (Parikh et al., 2020; Wang, 2019; Dhingra et al., 2022; Luu et al., 2022). Retrieval augmented generation (RAG) methods provide contextual knowledge to address this, but conflicts within retrieval contexts are still inevitable (Chen et al., 2024; Ge et al., 2024).\nAccording to Xie et al. (2024), under certain knowledge conflict circumstances, large lan-"}, {"title": "2 Our WhoQA Dataset", "content": "From the set of named entities in the English Wikipedia and Wikidata dump, we group all entities that have the same name together, collect question-answer pairs as well as supporting contexts, and manually revise the evaluation set.\nTake an example from Figure 1 with a set S containing three people whose names are\nFormally, WhoQA is a multi-answer question answering dataset in which each question can be described as a quadruplet (q, A, S, C), where q is a question asking about a property p of a named-entity s; A = {A_i}_{i=1}^m is a set of m possible distinctive answer sets to q; S = {s_i}_{i=1}^n is a set of n named entities that share the same name mentioned in q; and C = {c_i}_{i=1}^n where c_i is the corresponding supporting context for entity s_i. In a real-world RAG setup, C is obtained from a retrieval system. In this work, we focus on studying knowledge conflicts, hence each c_i is a textual context extracted from the corresponding Wikipedia document of s_i.\nQuestion-Answer pairs collection: Starting with a set S containing named entities s_i that share the same name, we induce knowledge conflicts by creating questions about all shared properties (e.g., occupation, date of birth, and the like) among the entities. We only consider the top 60 most popular properties reported by Wikipedia. For each property, we manually construct 5 question templates and substitute the shared name among entities in S to create specific questions. An example of a template for the property p = \"occupation\" is \"What is <shared_entity_name>'s occupation?\". See details of these question templates in Appendix A.\nFor each entity s_i in the set S, the answer to a question q, created for the property p, is the corresponding set A_i in the Wikidata property triplet (s_i, p, A_i) of s_i. We only consider properties p in which the corresponding answer set A_i contains Wikidata entities, plain texts or a date-time string. If A_i is a Wikipedia entity, we take all the alternative names annotated by Wikidata. For A_i that are date-time strings, we follow the Wikipedia Manual of Style instructions to cover all of their recommended written forms. We left A_i which are plain texts in their original form. This step generates about 293K questions, not counting different"}, {"title": "3 Experiments", "content": "We study LLMs' behavior when dealing with knowledge conflicts in three different scenarios:\n1. The first setting is a simple QA scenario where only a single context is presented to a model, hence no conflict.\n2. In the second setting, we provide the model with conflicting contexts without informing it about the presence of conflicts.\n3. Finally, in the third setting, we explicitly specify the presence of conflicts in the model's input."}, {"title": "3.1 Experimental setup", "content": "We evaluate 10 open source models: Gemma 7B (Team et al., 2024), Mistral 7B (Jiang et al., 2023) , Mixtral 8x7B (Jiang et al., 2024), Qwen1.5 Chat (7B, 14B, 32B, 72B) (Bai et al., 2023), Command"}, {"title": "3.2 Simple QA", "content": "We divide each question into n turns, each corresponding to one of the n conflicting contexts. As shown in Table 1, when conflicts are removed, most LLMs can accurately answer single-hop questions. This result highlights the ease of finding answers within the individual contexts of WhoQA, allowing our subsequent experiments to focus more on the issue of knowledge conflicts."}, {"title": "3.3 Knowledge conflict: without specification", "content": "We hypothesize that an ideal LLM should inherently recognize knowledge conflicts. To test this, we provide all conflicting contexts for each question and ask the LLM to answer without indicating the presence of conflicts.\nSimple knowledge conflicts substantially impairs LLMs. As shown in Table 1, LLMs in our experiments exhibit significant performance drops when knowledge conflicts occur. Since finding answers from each single context in WhoQA is straightforward, this decline is attributable to knowledge conflicts. Therefore, the varying levels of performance drop indicate LLMs' robustness against knowledge conflicts.\nWithout being noticed, LLMs are not sensitive to subtle knowledge conflicts. We examine how LLM performance changes as the number of conflicting answers increases. It is logical to assume that more conflicting information in the input context leads to higher information entropy. Table 2 shows that LLMs generally perform better on questions with more conflicting answers. This suggests that LLMs are less sensitive to subtle conflicts within their input contexts. Therefore, LLM practitioners should address knowledge conflicts in tasks where fine-grained answers are expected.\nThere is a tradeoff between helpfulness and accuracy when LLMs meet knowledge conflicts. We consider questions where LLMs miss all answers from all contexts as hard questions and manually review at most 100 hard question responses for each LLM. Table 3 shows the rate at which each model gives no answer to questions. Specifically, gpt-3.5-turbo often states that there are multiple individuals with the same name in the input contexts"}, {"title": "3.4 Knowledge conflict: with specification", "content": "We modify the prompt template to include few-shot examples, notifying LLMs of knowledge conflicts within their input context. Results in Table 1 show significant performance improvement for most models. The exceptions are the two LLama3 models, which show minimal benefit. A review of their responses reveals that these models are aware of conflicts and attempt to provide all available answers, regardless of whether conflicts are stated. This behavior supports our view that LLMs should be transparent and serve as tools to aid users' decisions. Results from Table 2 suggest that questions with more conflicting answers are indeed more challenging for LLMs."}, {"title": "4 Conclusions", "content": "We introduce WhoQA, a high-quality benchmark dataset designed to examine language model behaviors in knowledge conflict situations. WhoQA bridges the gap with previous work by inducing knowledge conflicts without generating counter-factuals, demonstrating that conflicts can arise not only from misinformation but also naturally due to ambiguity in retrieved contexts, user questions, or similarities among entities. Through our ex-"}, {"title": "Limitations", "content": "The simplicity of our matching process to collect supporting contexts results in the fact that questions in WhoQA are mostly single-hop. We think that it could be noteworthy to propose a way to induce knowledge conflicts for multi-hop questions so as to challenge future LLMs. We still leave the question on how to make LLMs' responses more informative in case on dealing with knowledge conflict. Possibly, looking at fine-tuning methods which control how LLMs deal with unfamiliar examples (Kang et al., 2024; Ge et al., 2024) is a promising direction."}, {"title": "A Question templates", "content": "We include in Table 4 all the 65 question templates for the final 13 Wikidata properties in our final dataset."}, {"title": "B Prompt templates", "content": "We report the prompt templates used in our experiments in Table 5."}, {"title": "C Models rejection rate", "content": "We consider questions where LLMs miss all answers from all contexts as hard questions and manually review 100 hard question responses for each LLM. Table 6 shows the rate at which each model gives no answer to questions."}]}