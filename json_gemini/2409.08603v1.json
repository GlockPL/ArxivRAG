{"title": "Using Convolutional Neural Networks for Denoising and Deblending of Marine Seismic Data", "authors": ["S. Slang", "J. Sun", "T. Elboth", "S. McDonald", "L. Gelius"], "abstract": "Processing marine seismic data is computationally demanding and consists of multiple time-\nconsuming steps. Neural network based processing can, in theory, significantly reduce processing\ntime and has the potential to change the way seismic processing is done. In this paper we are using\ndeep convolutional neural networks (CNNs) to remove seismic interference noise and to deblend\nseismic data. To train such networks, a significant amount of computational memory is needed since a\nsingle shot gather consists of more than 10\u00ba data samples. Preliminary results are promising both for\ndenoising and deblending. However, we also observed that the results are affected by the signal-to-\nnoise ratio (SnR). Moving to common channel domain is a way of breaking the coherency of the noise\nwhile also reducing the input volume size. This makes it easier for the network to distinguish between\nsignal and noise. It also increases the efficiency of the GPU memory usage by enabling better\nutilization of multi core processing. Deblending in common channel domain with the use of a CNN\nyields relatively good results and is an improvement compared to shot domain.", "sections": [{"title": "Introduction", "content": "The recent availability of powerful GPUs and open source software have enabled artificial neural\nnetworks (ANNs) to be applied to a number of practical and industrial scale problems. The level of\nadoption of this technology within the field of O&G exploration is well illustrated by the number of\nabstracts related to ANNs that are submitted to the annual EAGE and SEG conferences. Since 2001,\nthere have typically been one or two papers per year discussing ANNs. In 2018 the level rose\nsignificantly to between 50 and 100 papers.\nIn seismic processing, ANNs have the potential to be applied to many of the key processing steps\n(swell noise attenuation, seismic interference attenuation, deblending, deghosting etc.) which today\ninvolve significant testing time and computational power. Once trained, ANNs are computationally\nvery light and potentially adaptable to varied datasets. Their use could therefore significantly save\nprocessing times and, in the long term, impact the whole business sector.\nA natural first step in this direction is to look at various forms of seismic data denoising. This is not an\nentirely new concept, and is clearly inspired by work done on natural picture denoising, where we\nrefer to Zhang et al. (2017) for a recent overview. However, this field is still immature and, as\nindicated by Xie et al. (2018), a lot of work is needed before ANNs can be effectively applied to\nseismic data denoising.\nA common limitation in recent papers using ANNs for seismic data denoising (see e.g. Ma (2018),\nBaardman (2018), Si and Yang (2018), Li et al. (2018), Jin et al. (2018), and Zhang et al. (2018)) is\nthat they only use synthetic data or noise on datasets with limited dynamic range and/or frequency\ncontent. As proof of concept, this has value. However, we have not yet seen convincing examples that\ncompare against existing state-of-the-art denoising results.\nIn conventional processing, it is a common practice to sort and/or transform the seismic data into\ndomains wherein it is easier to separate the noise from the desired signal. We have not yet seen this\napproach utilized in ANN denoising, and we believe that this could potentially improve results\nsignificantly.\nThis paper is structured as follows: in the theory section, we will introduce our network architecture\nand outline the design approach. We will then present two examples of denoising done on real marine\nseismic data, before pointing towards how we believe this work could be taken further."}, {"title": "Theory - CNNs for seismic denoising", "content": "A common type of layer in ANNs is the Fully Connected (FC) layer. They tend to give good results,\nbut they are computationally heavy since they have one parameter for each sample in the input data.\nSeismic datasets tend to be large (~10\u00ba samples per shot gather), making the use of FC layers a\nchallenging undertaking given the large memory requirements and computational demand. This leads\nus to another common ANN type, which is the Convolutional Neural Networks (CNNs).\nCNNs are neural networks consisting of at least one convolutional layer. Convolutional layers differ\nfrom other types of layers in that they employ convolutions over subsets of the data, rather than a\ngeneral matrix multiplication. According to Goodfellow et al. (2016), this makes CNNs well suited\nfor 2D images where neighboring pixels are connected in a larger pattern. It should therefore be\npossible to denoise seismic data with localized and \u2018random' noise either in the shot domain or when\nsorted to, for example, the channel domain. We assume that CNNs will be able to handle seismic data,\ngiven its continuous nature and 2D structure. This will greatly reduce the computational cost and\nmemory requirements compared to FC layers. Although it is much more efficient to use CNNs with\nrespect to computational power, seismic images are large and still push the limits of hardware\navailable today.\nSeismic noise varies a lot in amplitude and might be orders of magnitude larger than the underlying\nsignal, making it hard for the network to recreate the underlying signal structure. Given the difficulties\nraised by the large input volumes and the sometimes low SnR that can occur in recorded seismic data,\nit is understandable that previous attempts have been made with synthetic data or data subsets with\nlimited size and dynamic range. When using synthetic data, the user has full control over the dataset.\nIn this work, we apply CNNs to real life, full-scale marine seismic gathers and investigate how well\nthis works for two types of commonly encountered seismic noise attenuation problems."}, {"title": "Example 1: Seismic interference noise attenuation", "content": "The first example investigated is the attenuation of seismic interference (SI) noise. This is dispersive\ncoherent acoustic energy originating from other seismic crews operating nearby. The energy is mostly\npropagating in the water column, and is typically recorded with amplitudes similar to or larger than\nthat of the seismic reflection signal. As such, it is common practice to try to attenuate this noise early\non in the processing flow.\nCNNs require both noisy images and clean images, which are regarded as ground truth, in image\ndenoising. The network calculates the error between the denoised image and ground truth to update\nthe weights. Our marine seismic training data consists of 800 records containing almost pure SI-noise\nrecorded from different directions, and 482 (nearly) noise-free seismic shot gathers from the North\nSea. It is a straightforward task to blend clean shots with varying SI-noise creating a dataset for the\nnetwork to train on where ground truth is known.\nThe network architecture is based on common models used in image analysis with CNNs. It consists\nof convolutional layers with batch normalization and Rectified Linear Unit (ReLU) activation\nfunction. To handle the full seismic range, residual learning is applied, making sure all layers learn\nfrom the full frequency range in the original input image. The main difference compared to other\nattempts is that no downscaling is applied. The image is full-size throughout the network to reduce\npotential blurring and precision loss. This results in a large model requiring significant computational\npower and about 12GB GPU memory to train on a single image. The overall training process to\nachieve this level of denoising involved around 104 shot gathers, and took nearly two weeks on a\nmodern GPU (6 GB x2). However, we note that once the network is trained, the actual denoising of a\nsingle shot gather is done in less than a second.\nFigure 1 illustrates that the network is removing SI rather well for cases where the SnR is relatively\nhigh. This is visible in the spectra showing that the network has a good recovery for all frequencies."}, {"title": "Example 2: Seismic data deblending", "content": "The second example is the deblending of N+1 shot data to extend the useful record length in the\nseismic data. Deblending (separation) of overlapping seismic records is important since it is a key\ntechnology enabling improved sampling and/or more efficient acquisition. The basic idea was\nprobably first introduced almost 50 years ago by Barbier (1971), but wide scale industrial adoption\nhas only been achieved in the last few years.\nOur dataset consists of 1300 towed unblended marine split spread gathers. The shot gathers are\nartificially blended with the N+1 shot with a fixed delay of 1.2 second and a specially designed dither.\nBased on our study, we have found that it is difficult for the neural network to learn how to deblend\nseismic data in the shot domain. The events of two different shots in the blended data share similar\ncharacteristics in both amplitudes and dip, thus there is no clear characteristic features for the network\nto learn. Sorting the data into common channel domain, as mentioned in Example 1, gives better\nresults. The N+1 shot in common channel exhibits randomness, while shot N has a continuous nature.\nThus, deblending of overlapping seismic records in common channel domain is similar to removing\nrandom noise in image processing. The characteristics of CNNs, as mentioned in the theory section, is\nsuitable for this case. The unblended data serves as ground truth for the network and is used to update\nthe weights of the artificial neurons.\nFigure 2 illustrates that the network has the ability to learn deblending, and works well when we only\nintroduce 80% of shot N+1, thus artificially enhancing the SnR. The recreated plot visible in C) has a\nquality that is close to current state-of-the-art commercial processing, but would compare less\nfavorably without artificial SnR enhancement. The training, validation, and test sets consist of 14400,\n2700, 900 images respectively with 60 channels per image to reduce memory usage. This particular\ntest took approximately 2.5 days to run, however, once the network was trained, the computational\ncost of deblending a single image was approximately the same as Example 1 \u2013 less than a second. All\nthe processing is done in common channel domain. The records with no overlapping noise are almost\nperfectly recreated.\nThe network architecture is similar to Example 1, with a network consisting of convolutional layers\nwith applied batch normalization. The largest difference is the usage of Leaky ReLU activation\nfunction to reduce the risk of \"dead\" neurons. Compared to common CNNs, both our mentioned\nnetwork models differ in terms of static image size and thus no max pooling. This reduces the risk of\nblurring and loss of geological data. Even with a robust network, deblending in common channel\ndomain remains a challenging task. This is because the SnR tends to always be low over a large area\ncompared to other types of noise removal. Although this issue may cause some residual noise in the\noutput image, the network is removing a significant amount of the N+1 shot and keeping detailed\nunderlying signal intact. As mentioned in Example 1, there are multiple ways to potentially enhance\nthe result. One way could be preprocessing the data to improve the SnR or moving the processing to a\nsparser domain. These novel approaches will be the focus of our future work."}, {"title": "Conclusions", "content": "Convolutional neural networks show promising results for interference noise attenuation and N+1\ndeblending of marine seismic data. When applied alone, the results are below the level achieved by\nstate-of-the-art commercial processing. However, we have shown that applying machine learning to\nseismic data processing can still produce encouraging results and is an approach worth exploring\nfurther. For the problems shown, sorting the data into common channel domain seems to give better\nresults than shot domain, due to the random nature of the noise in said domain. The higher the signal\nto noise ratio, the better the results become. We finally mention the importance of having high quality\ntraining datasets. The ground truth should ideally be without any noise. This is difficult to achieve\nwhen we work with real data, which inevitably will have some noise contamination."}]}