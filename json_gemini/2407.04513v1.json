{"title": "LAYERSHUFFLE: ENHANCING ROBUSTNESS IN VISION\nTRANSFORMERS BY RANDOMIZING LAYER\nEXECUTION ORDER", "authors": ["Matthias Freiberger", "Peter Kun", "Anders Sundnes L\u00f8vlie", "Sebastian Risi"], "abstract": "Due to their architecture and how they are trained, artificial neural networks are\ntypically not robust toward pruning, replacing, or shuffling layers at test time.\nHowever, such properties would be desirable for different applications, such as\ndistributed neural network architectures where the order of execution cannot be\nguaranteed or parts of the network can fail during inference. In this work, we\naddress these issues through a number of proposed training approaches for vision\ntransformers whose most important component is randomizing the execution order\nof attention modules at training time. We show that with our proposed approaches,\nvision transformers are indeed capable to adapt to arbitrary layer execution orders\nat test time assuming one tolerates a reduction (about 20%) in accuracy at the\nsame model size. We also find that our trained models can be randomly merged\nwith each other resulting in functional (\"Frankenstein\u201d) models without loss of\nperformance compared to the source models. Finally, we layer-prune our models\nat test time and find that their performance declines gracefully.", "sections": [{"title": "INTRODUCTION", "content": "While demonstrating impressive performance in many domains (e.g. see Krizhevsky et al. (2012);\nVaswani et al. (2017); Radford et al. (2021); Rombach et al. (2022)), deep learning systems demand\nboth extensive computational resources and tight integration of their parts. For applications at scale,\nthey therefore increasingly require the construction of large datacenters with thousands of dedicated\nhardware accelerators. A paradigm shift from central to decentral model inference, where loosely\ncoupled neural networks are distributed over a number of edge devices that share the computational\nload (Gacoin et al., 2019) of the model therefore seems ultimately desirable. Unfortunately, current\ndeep learning models lack the robustness necessary for such a paradigm shift.\nIn general, artificial neural networks (ANNs) (Schmidhuber, 2022) are not robust toward pruning or\nreplacing network layers during deployment. Similarly, changing the order of execution in-between\nlayers without further training usually results in catastrophic losses in accuracy. Nevertheless, these\nproperties would be desirable e.g. in distributed setups as described above, where a model is executed\non a number of shared nodes in a network. In such a setting, overloaded or malfunctioning nodes\ncould simply be skipped in favor of other available nodes. Furthermore, malfunctioning nodes or\nabsent nodes could simply be replaced by a similar (not the same) node, allowing for simple logistics\nwhen deploying models in practice.\nAugmenting models with these properties has historically been challenging. Due to the structure of\nthe most common types of ANNs and how they are trained through backpropagation (Linnainmaa,\n1970; Werbos, 1982; Rumelhart et al., 1986), each neuron can only function by adapting to both its\nconnected input and output neurons as well as the overall desired output of the network at training\ntime. Furthermore, the hierarchical organization of explanatory factors is usually considered a nec-\nessary prior in deep learning, i.e. one assumes that subsequent layers extract increasingly high-level\nfeatures (Bengio et al., 2013). Therefore, switching the execution orders of layers implies that layers\nwould need to adapt and extract either low-level or high-level features depending on their position in\nthe network. Unfortunately, network layers adapting in such a way to a changed order of execution"}, {"title": "RELATED WORK", "content": "Zhu et al. (2020) find that for particular subsets of inputs, transformers perform better when changing\nthe execution order of layers to an input-specific sequence. They optimize the execution order per\nsample in order to maximize the performance of the model for natural language processing tasks.\nWhile the goal in their work is to find a layer sequence of a pre-trained model that is optimal for a\ngiven input, our approach aims to make the model robust to any sequence of execution, where layers\nmight even be missing.\nIn parallel to our work on vision transformers and within the wider framework of mechanistic in-\nterpretability for large language models (LLMs), Lad et al. (2024) found that LLMs are very robust\nto changing the positions of adjacent layers or ablating single layers from the model. The main\ndifference to our work is that the authors do not perform any refinement on the models and switch\nand ablate layers locally with the aim of better understanding the inner workings of LLMs. On\nthe other hand, we focus on methods and training approaches to increase this innate robustness of\nthe transformer architecture to a point where models at test time function regardless of their layer\nexecution order, and respond gracefully to the ablation of several layers in any stage of the network.\nAnother related work is LayerDrop (Fan et al., 2019), where the authors focus on robust scalability\nfor models on edge devices. They propose dropping whole transformer layers during training and\nshow that this training approach allows models to still deliver acceptable (if somewhat reduced)\nperformance upon pruning layers at test time (e.g. for balancing computational load). The main\ndifference to our approach is that we randomly change the execution order during training, and,\ncontrary to LayerDrop, do not remove any layers. Also, LayerDrop focuses on entirely on load bal-\nancing in compute-limited production systems while our main focus is on arbitrary execution order\nand the possibility to replace defective nodes by others on top of these issues in case of overloaded\nor malfunctioning nodes in distributed systems.\nRecent work improves the performance of LLMs on predefined tasks, by merging them using evolu-\ntionary strategies (Akiba et al., 2024). Similar to Zhu et al. (2020), the authors' overall aim is to in-\ncrease performance rather than robustness in distributed environments, so contrary to our approach,\nlayer execution order and scaling for reduced numbers of layers are in general not considered.\nWork on introducing permutation invariance into neural networks has been conducted by Lee et al.\n(2019), Tang & Ha (2021) as well as Pedersen & Risi (2022). The corresponding former two ap-\nproaches exploit the permutation equivariance of attention, i.e. the fact that the order in which a\nsequence of vectors gets presented to the attention module does not change its result, but merely\nshuffles the sequence of output vectors. This equivariance is achieved by using a fixed-seed query\nvector in order to obtain an permutation invariant latent code. This latent code stays the same no\nmatter in which order input tokens/patches are presented to the module. The main contrast to our\nwork here is that we exploit permutation invariance in the order of layer executions rather than the\norder of tokens and patch embeddings and can therefore not make use of permutation equivariance\nof the attention operation, as it does not apply to switching inputs and outputs. Pedersen & Risi\n(2022) also consider permutation invariance with respect to the input, although the emphasis of their\napproach lies on a number of smaller recurrent neural networks Werbos (1988); Hochreiter et al.\n(2001) with shared weights, which extract a permutation-invariant representation of the input vector\nin a self-organized way without utilizing the permutation-equivariance of self-attention modules.\nFinally, the work of Gacoin et al. (2019), not unlike our own, is motivated by the observation that\na paradigm of distributed model inference over a number of loosely coupled compute nodes, edge\ndevices or swarm agents promises a positive impact on the ecological and economical footprint\nof deep learning solutions. The authors propose a graph-theory-based framework to optimize the\ndistribution of model parts to individual devices and optimize the overall energy consumption of the\nnetwork. While our work sets out from the same motivation, it complements the approach of Gacoin\net al. (2019) as the the authors do not address robustness to adverse conditions in such distributed\nsetups while it is the entire focus of this paper. The exact distribution of our models on the other\nhand, is beyond the scope of our work but combining our models with the approaches in (Gacoin\net al., 2019) seems a promising direction of future research."}, {"title": "METHODS", "content": "After giving a brief overview on vision transformers (Section 3.1), we investigate three approaches to\nthem for arbitrary layer execution order: First, we simply permute the order of layers randomly dur-\ning training, such that every training batch is presented to the network's layers in a different random\norder (Section 3.2). Second, while randomly permuting the layer order as in the previous approach,\nwe use an layer-depth encoding inspired by learned word embedding approaches (Section 3.3) to see\nif this additional information would further improve performance. Third, while randomly permuting\nlayer order as in the previous approaches, we try to predict from the output of every layer at which\nposition the layer is currently located in the network using a small layer position prediction network\nfor every layer (Section 3.4). We now describe these approaches in more detail."}, {"title": "VISION TRANSFORMERS", "content": "Dosovitskiy et al. (2020) have successfully adapted the transformer architecture to computer vision\nby introducing a preprocessing step that converts images to suitable sequences. They do so by\nsplitting an image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of N flattened patches \\(x_p \\in \\mathbb{R}^{N \\times (P^2 C)}\\), and then\npass each patch through a linear embedding layer \\(E \\in \\mathbb{R}^{(P^2 C) \\times D}\\). H,W and C are here the height,\nwidth and number of channels of the image respectively and P is the patch size. D is the internal\nlatent dimension of the transformer which remains constant throughout the network and can be set\nas a hyperparameter.\nAfter converting the image into a sequence that can be processed by a transformer encoder, inspired\nby BERT (Devlin et al., 2018), the authors prepend a class token to \\(x_p\\) in which the class infor-\nmation of the input image can be aggregated by the transformer. To encode position information\ninto the embedding, a positional embedding tensor \\(E_{pos} \\in \\mathbb{R}^{(N+1) \\times D}\\) is added. Both the class\ntoken as well as the positional embeddings are learnable embeddings, which are trained jointly with\nthe rest of the network. The resulting input sequence presented to the transformer network can be\nexpressed as\n\\[\nZ_0 = [x_{class}; x_p E; x_p E; ...; x_p E] + E_{pos}.\n\\]\nThis sequence is presented to a standard transformer architecture of stacked attention modules. Each\nattention module consists of a multi-head self-attention (MSA) layer and a feedforward layer or mul-\ntilayer perceptron (MLP) layer. MSA layers utilize self-attention (SA) (Vaswani et al., 2017), a pow-\nerful concept that allows transformers to relate and combine its feature embeddings with each other.\nSelf-attention extracts features from the input sequence z, which in turn preforms a transformation\nof the input vector sequence.\nSpecifically, self-attention extracts query, key and value sequences q, k and v from the input se-\nquence using a linear projection \\(U_{qkv} \\in \\mathbb{R}^{D \\times 3D_h}: [q,k, v] = zU_{qkv}\\). The q and k sequences are\nthen used to compute a Softmax-normalized transformation matrix A indicating how to incorpo-\nrate information of the whole sequence (i.e. in our case all image patches) for every single vector\nof the sequence: \\(A = \\text{Softmax}(\\frac{qk^T}{\\sqrt{D_h}})\\). Scaling the dot-product product by \\(\\sqrt{D_h}\\) here ensures a\nbalanced distribution of the Softmax output. After obtaining A, the output of SA is computed as\nSA(z) = Av.\nA multi-head self-attention (MSA) layer (Vaswani et al., 2017) performs several attention operations\nin parallel, concatenates the result and projects it back to the internally used latent dimension of the\ntransformer:\nMSA(z) = [SA\u2081(z); SA2(z); ...; SAk(z)]Umsa\nIn an attention module the multi-head self-attention layer is followed by a multi-layer-perceptron\n(MLP) layer transforming the recently combined embeddings to extract new feature representations.\nBefore presenting z to each layer in the module, the embeddings are normalized using LayerNorm\n(Ba et al., 2016). To ensure consistent gradient flow during training, residual connections (He et al.,\n2016) are behind both the MSA and the MLP layers (Wang et al., 2019). Furthermore, as a regu-\nlarization measure, Dropout (Srivastava et al., 2014) is applied after every MSA and MLP layer. In\nsummary, given the sequence \\(z_{t-1}\\) from a previous attention module as input, we first compute the\nintermediate representation\n\\[\nz' = \\text{MSA}(\\text{LN}(z_{t-1})) + z_{t-1},\n\\]\nwhich is the presented to the MLP layer to compute the final output of the module\n\\[\nz_t = \\text{MLP}(\\text{LN}(z')) + z'.\n\\]\nFinally, after N attention modules, the first vector of the sequence (corresponding to the class-\ntoken in the preprocessed input) is handed to a linear layer \\(W_{out} \\in \\mathbb{R}^{D \\times C}\\) to predict the final class\nof the image: \\(y = \\text{argmax}(z_t W_{out})\\). C denotes the number of classes."}, {"title": "RANDOMLY PERMUTING LAYER ORDER DURING FORWARD PASS", "content": "During each forward pass, i.e. for each batch presented to the vision transformer, we randomly\npermute the execution order of layers during training. The intention here is to teach the layers to not"}, {"title": "LAYER POSITION ENCODING", "content": "In the second approach, LayerShuffle-position, we provide each layer\nwith its current position in the network. Through this variation we aim\nto test if each layer can already adapt sufficiently by itself to information\ncoming from different layers during test time or if giving it the current\nposition can help further. In more detail, jointly with permuting the layer\nexecution order, each layer learns a vector embedding \\(e_{layer}^p \\in \\mathbb{R}^F\\) for\neach possible index position \\(p \\in [1, L]\\) of the layer during training, where\nL is the number of layers and F = 32 is our chosen embedding dimen-\nsion. The layer's current index p in the network is presented together\nwith the input to the layer \\(Z_{t-1}\\) (Figure 2). The layer fetches the embed-\nding vector \\(e_{layer}^p\\) associated with the passed index p and concatenates it\nto the input vector \\(z_{t-1}\\): \\(h_t = \\text{concat}(z_{t-1}, \\text{repeat}(e_{layer}^p, N + 1))\\). N\nis the number of patches extracted form the input image, the functions\nconcat and repeat respectively concatenate and repeat tensors along their\nlast (most varying) dimension. A projection network, which consists of\na LayerNorm (LN) (Ba et al., 2016) module, a single linear layer \\(W_{proj}\\),\na GELU (Hendrycks & Gimpel, 2016) activation function as well as a\nDropout (Srivastava et al., 2014) module, is then used to combine input\nand embedding and reduce it again to the used latent dimension D of the\ntransformer. To ensure gradient flow during training, a residual connec-\ntion is added as well:\n\\[\nz' = \\text{Dropout}(\\text{GELU}(\\text{LN}(h_t)W_{proj})) + Z_{t-1}.\n\\]\nThe resulting output \\(z'\\) is passed on to a regular multi-head-attention-and-feed-forward structure as\ndescribed in Equations 1 and 2."}, {"title": "PREDICTING CURRENT LAYER POSITION", "content": "To determine if the incoming information to each attention layer is\nindeed sufficient for it to figure out its role, we specifically test for\nthis ability with the LayerShuffle-predict variant. We equip each\nlayer of the network with a simple position prediction module that\ntakes the current layer output as an input and seeks to predict the\ncurrent position of the layer in the network (Figure 3). The module\nconsists of a single linear layer \\(W_{pred} \\in \\mathbb{R}^{D \\times L}\\) receiving layer-\nnormalized (LN)input. \\(u = \\text{LN}(z_t)W_{pred}\\).\nEach of these layer order prediction modules optimizes a cross-\nentropy loss where then the overall network optimizes the loss\n\\(L_{out} + \\sum L_i\\). Here, \\(L_{out}\\) is the regular cross-entropy loss of the\noutput layer, and \\(L_i\\) is the layer position prediction loss of layer i,\nwhich is also a cross-entropy loss:\n\\[\nL_i = - \\log \\frac{\\exp(u_p)}{\\sum_{v \\in L} \\exp(u_v)},\n\\]\nwhere L is the number of layers in the network, u is the L-dimensional output of the position\nprediction network of layer i, and \\(u_l\\) denotes the l-th dimension of the vector. \\(u_p\\) is the output logit\ndenoting the network's predicted confidence that the layer currently is deployed at its actual position\nwith index p."}, {"title": "EXPERIMENTS", "content": "We conduct our experiments on the ILSVRC2012 dataset (Russakovsky et al., 2015), more com-\nmonly termed ImageNet2012. We use the original ViT-B/16 (Dosovitskiy et al., 2020) vision trans-\nformer, with the publicly available pre-trained weights provided on huggingface (vit). This model\nhas been pre-trained on ImageNet21k (Ridnik et al., 2021) at an 224\u00d7224 input image resolution\nand refined on ImageNet2012 at the same resolution. We refine the model again on ImageNet2012\nat the same resolution, but using the training processes as described in Section 3. That is, we ran-\ndomly permute layer execution order while refining the model. To establish a baseline, we refine the\noriginal ViT-B/16 for one more epoch without changing the layer order. We have limited ourselves\nto a single epoch here as we found any longer training to be unlikely to bring additional improve-\nment in preliminary experiments. For each approach, including the baseline, we train 5 networks\nand compare their average validation accuracy.\nAll models are refined using Adam (Kingma & Ba, 2014) (\\(\\beta_1\\) = 0.9, \\(\\beta_2\\) = 0.999, \\(\\epsilon\\) = \\(10^{-6}\\)),\nwhere an initial learning rate of \\(10^{-4}\\) was empirically found to work best. In terms of batch size, we\nevaluate training batch sizes of 640 images, which is the maximum multiple of 8 that can fit in the\nvideo memory of our used GPU, as well as 128 images for models that benefit from a smaller batch\nsize. We find that even smaller batch sizes do not yield any improvement in performance for our\nmodels. Inspecting training curves shows that the performance of our models plateaus at 20 epochs\nthe latest, which we therefore set as the maximum number of training epochs. We use a form of\nearly stopping by evaluating the model achieving lowest crossentropy loss on the validation set after\n20 epochs. All models have been trained on a single NVIDIA H100 Tensor Core GPU with 80GB\nof memory, where the training of a single model for 20 epochs takes about 7 hours."}, {"title": "SEQUENTIAL VS. ARBITRARY EXECUTION ORDER", "content": "We report the average accuracy for the baseline and LayerShuffle approaches in Table 1. Our base-\nline refined from a pre-trained ViT-B/16 model performs very much as expected: For a classic se-\nquential execution order of the model layers, the average validation accuracy performs very close\nto the original model (Dosovitskiy et al., 2020) at 82.61% at a low standard deviation of 0.08. Not\nsurprisingly, for an arbitrary layer execution order, the average model accuracy declines catastroph-\nically to 0.13%. Our original assertion that in general, neural networks are not robust to changing\nthe execution order of their layers, is in line with these results.\nOur LayerShuffle approach shows slightly lower performance than the baseline when executing\nlayers in their original order (average accuracies of 75.22, 75.28, and 74.41 for our LayerShuf-\nfle, LayerShuffle-position and LayerShuffle-predict respectively), but improves dramatically over\nthe baseline model in an arbitrary execution order setting. Surprisingly, the simplest LayerShuffle\napproach receives an accuracy of 62.77%, only slightly lower than the accuracy 63.61% of the\nLayerShuffle-position, which receives information about the layer position. While all three ap-\nproaches seem to be overall on par, we observe the position prediction approach, LayerShuffle-\npredict, to be the least well-performing of the proposed approaches with an average accuracy of\n61.18%. A possible explanation might be that due to optimization of multiple objectives (fitting\nboth the output labels as well as predicting the current position of the layer) this approach requires\nmore careful hyperparameter tuning. This hypothesis is also supported by the somewhat higher\nstandard deviation of 1.06 percentage points in accuracy, compared to the 0.41 and 0.23 percentage\npoints of LayerShuffle and LayerShuffle-position respectively. A further interesting observation is\nto be made when comparing the performance for sequential and arbitrary execution order for each\napproach respectively. For all approaches, using the original layer order for sequential execution\nstill performs better than an arbitrary order. This is most likely a consequence of fine-tuning from a\nsequentially trained model.\nFor the layer position prediction approach, we measure the average accuracy of layer position pre-\ndictions over all five trained LayerShuffle-predict models, and find that the layer position is predicted\ncorrectly in 99.99% of all cases. These results demonstrate that each layer has enough information\ncoming from its inputs alone to predict where it is in the network, providing the basis to adapt to its\ncurrent position. We investigate this further when analyzing intermediate network representations\nin Section 4.3.\nIn conclusion, refining a pre-trained model while randomly permuting the execution order of the\nnetwork layers can make a model more robust towards such arbitrary execution orders at test time.\nOn the other hand, Dropout and LayerNorm by themselves do not have the same effect and fail to\nproduce networks robust against layer shuffling."}, {"title": "REMOVING LAYERS DURING TEST TIME", "content": "To determine how neural networks trained with LayerShuffle would perform when several devices in\na (distributed) model become unavailable, we further investigate the effect of pruning an increasing\namount of layers during test time. We evaluate its average validation accuracy over 5 models when\nonly using 3, 6, or 9 layers. In addition, we refine the original ViT-B/16 transformer using LayerDrop\n(Fan et al., 2019) with a drop probability of 0.2 (as recommended by the authors) and compare it as\na baseline to our approach under identical conditions. Note that whenever we evaluate the accuracy\nof our proposed approaches as well as the baseline, we do so two times: Once, for the original\n\"sequential\" layer order as originally intended and trained for the ViT-B/16 transformer, and once\nwith arbitrary layer execution order where we change the order randomly for every forward path.\nFor sequential execution (Figures 1b), LayerDrop with a drop rate of 0.2 behaves similarly to Lay-\nerShuffle, with the exception that our approach performs better for a small number (3) of layers with\nan average accuracy of approximately 18% vs. close to 0% for LayerDrop. While for 6 layers, both\napproaches are roughly on par, for 9 layers LayerShuffle is slightly outperformed by LayerDrop as\nboth approaches show an average accuracy in the 70 \u2013 80% range. At the full amount of 12 layers,"}, {"title": "ANALYSIS OF INTERMEDIATE NETWORK REPRESENTATIONS", "content": "To gain a deeper insight into how information is encoded in our trained models, we compute Uni-\nform Manfold Approximation and Projection (UMAP) (McInnes et al., 2018) embeddings of the\noutputs of a particular attention module, where we color-code all output vectors based on the po-\nsition the module held in the network when producing this output. For the UMAP analysis, we\npresent 1,000 randomly sampled images from the ImageNet2012 validation set to a LayerShuffle-\ntrained model. While we use an evaluation batch size of 1 image and record all outputs of a single,\npreviously selected layer, we randomly permute the execution order of layers such that the selected\nlayer changes position in the network during every forward path. After the layer output vectors for\nall 1,000 images have been recorded, a UMAP reduction of the output space to 2D is performed."}, {"title": "MERGING MODELS WITH ARBITRARY EXECUTION ORDER", "content": "Being robust against permuting the layer execution order, opens interesting other possibilities such\nas model merging, i.e. creating a new model from the layers of several identically trained models.\nThe underlying rationale is that such merged models could also occur in a distributed setting, where\ncompute nodes, whose layers have been trained as part of distinct models, but with the same training\nprocess, could form ad-hoc models together.\nTo construct merged models, where each layer stems from a different model, we require 12 models\nfor the 12 layers of the ViT-B/16. We therefore train 7 more networks for our LayerShuffle approach\nand the baseline. Subsequently, we create 100 merged models (out of 12! possible combinations) by\nrandomly sampling from these models for our proposed approach as well as the baseline respectively\n(models are not mixed between approaches). As mentioned previously, layers are sampled in such\na way that no two layers in a merged model stem from the same model. We then evaluate the\nvalidation accuracy of all 100 models for both approaches.\nTable 5 summarises the results. The merged baseline model ViT-B/16 deteriorates from 82.61%\naverage accuracy to 1.87% (despite sequential layer execution as required by the model) making\nthe resulting merged model effectively unusable. The merged LayerShuffle models, on the other\nhand, perform slightly below the original model with an average accuracy of 59.68% as opposed\nto the 62.77% of the latter. Less surprisingly, merged models show a higher standard deviation at\n1.15 percentage points for the merged models vs. 0.41 percentage points for the original ones as\nmerged models do not contain any two layers that have been trained together, which makes their\nperformance vary more. We can further improve performance by ensembling the 12 models trained\nwith LayerShuffle, using the average of their output logit vectors. Such neural network ensembles\noften reach a better performance (Hansen & Salamon, 1990), which is also the case here with a\nsignificant improvement and an accuracy of 69.20% for LayerShuffle Ensemble.\nVisualizing the accuracies of all achieved models in a swarm plot paints a similar picture (Figure 5).\nWhile most merged LayerShuffle achieve an accuracy close to the average accuracy of the original\nmodels, for the baseline, all merged models exhibit very low accuracies. In conclusion, we find that\npermuting layer order during training enables the construction of merged (or \"Frankenstein\u201d) vision\ntransformers, where each layer of the transformer can be taken from a different model, as long as all\nmodels have been refined from the same base model on the same data."}, {"title": "DISCUSSION AND FUTURE WORK", "content": "This paper presented a new approach called LayerShuffle, which enabled vision transformers to be\nrobust to arbitrary order execution, pruning at test time, as well as adhoc-construction of merged\nmodels. For sequential execution, LayerShuffle performs on average only slightly worse than the\nLayerDrop approach but is the only method that works when the layer execution is arbitrary. A\nlatent space analysis confirmed that layers of models trained with LayerShuffle adjust their output\ndepending on which position they hold in the network. Finally, we investigated whether it is possible\nto build merged models from the models trained with LayerShuffle and found the performance of\nthe built merged models to be only slightly less than the performance of our trained models, contrary\nto the baseline, where virtually all merged models delivered very poor performance.\nThat LayerShuffle works well is surprising and here we have only scratched the surface of its effect\non the trained neural networks. To gain a deeper understanding, further analysis will include an\ninspection of the norms of the outputs of multi-head-attention and multi-layer-perceptron layers.\nThis type of investigation could reveal whether layers learn to switch off their output for inputs\nthey cannot process such that further relevant processing can occur in a suitable downstream layer\nto which the data is relayed through the attention module's residual connections. Inspecting the\nmodel's attention maps and embedding the intermediate latent vectors of all layers at all positions in\na single two-dimensional embedding might yield further insights as well.\nIn the future, these properties could make LayerShuffle-trained models ideal candidates to be dis-\ntributed over a number of very loosely coupled compute nodes to share the computational load of\nmodel inference. Given the enormous engineering, financial and logistical effort as well as the envi-\nronmental impact (Strubell et al., 2020) of building and maintaining datacenters for state-of-the-art\ndeep learning approaches on the one hand, as well as the large amount of available, but scattered\ncompute through existing smartphones, laptop computers, smart appliances and other edge devices\non the other hand, approaches that allow ad-hoc neural networks performing inference together could\nbe of great impact. We therefore consider the deployment and orchestration of our trained models\nonto an actual set of edge devices and the practical implementation of the inference process on a\nnetwork of such devices, likely by combining our approach with previously proposed frameworks\nto address this issue (Gacoin et al., 2019), a very promising direction of future research."}]}