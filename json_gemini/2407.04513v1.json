{"title": "LAYERSHUFFLE: ENHANCING ROBUSTNESS IN VISION TRANSFORMERS BY RANDOMIZING LAYER EXECUTION ORDER", "authors": ["Matthias Freiberger", "Peter Kun", "Anders Sundnes L\u00f8vlie", "Sebastian Risi"], "abstract": "Due to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning, replacing, or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of proposed training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. We show that with our proposed approaches, vision transformers are indeed capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20%) in accuracy at the same model size. We also find that our trained models can be randomly merged with each other resulting in functional (\"Frankenstein\u201d) models without loss of performance compared to the source models. Finally, we layer-prune our models at test time and find that their performance declines gracefully.", "sections": [{"title": "1 INTRODUCTION", "content": "While demonstrating impressive performance in many domains (e.g. see Krizhevsky et al. (2012); Vaswani et al. (2017); Radford et al. (2021); Rombach et al. (2022)), deep learning systems demand both extensive computational resources and tight integration of their parts. For applications at scale, they therefore increasingly require the construction of large datacenters with thousands of dedicated hardware accelerators. A paradigm shift from central to decentral model inference, where loosely coupled neural networks are distributed over a number of edge devices that share the computational load (Gacoin et al., 2019) of the model therefore seems ultimately desirable. Unfortunately, current deep learning models lack the robustness necessary for such a paradigm shift.\nIn general, artificial neural networks (ANNs) (Schmidhuber, 2022) are not robust toward pruning or replacing network layers during deployment. Similarly, changing the order of execution in-between layers without further training usually results in catastrophic losses in accuracy. Nevertheless, these properties would be desirable e.g. in distributed setups as described above, where a model is executed on a number of shared nodes in a network. In such a setting, overloaded or malfunctioning nodes could simply be skipped in favor of other available nodes. Furthermore, malfunctioning nodes or absent nodes could simply be replaced by a similar (not the same) node, allowing for simple logistics when deploying models in practice.\nAugmenting models with these properties has historically been challenging. Due to the structure of the most common types of ANNs and how they are trained through backpropagation (Linnainmaa, 1970; Werbos, 1982; Rumelhart et al., 1986), each neuron can only function by adapting to both its connected input and output neurons as well as the overall desired output of the network at training time. Furthermore, the hierarchical organization of explanatory factors is usually considered a nec- essary prior in deep learning, i.e. one assumes that subsequent layers extract increasingly high-level features (Bengio et al., 2013). Therefore, switching the execution orders of layers implies that layers would need to adapt and extract either low-level or high-level features depending on their position in the network. Unfortunately, network layers adapting in such a way to a changed order of execution"}, {"title": "2 RELATED WORK", "content": "Zhu et al. (2020) find that for particular subsets of inputs, transformers perform better when changing the execution order of layers to an input-specific sequence. They optimize the execution order per sample in order to maximize the performance of the model for natural language processing tasks. While the goal in their work is to find a layer sequence of a pre-trained model that is optimal for a given input, our approach aims to make the model robust to any sequence of execution, where layers might even be missing."}, {"title": "3 METHODS", "content": "After giving a brief overview on vision transformers (Section 3.1), we investigate three approaches to them for arbitrary layer execution order: First, we simply permute the order of layers randomly dur- ing training, such that every training batch is presented to the network's layers in a different random order (Section 3.2). Second, while randomly permuting the layer order as in the previous approach, we use an layer-depth encoding inspired by learned word embedding approaches (Section 3.3) to see if this additional information would further improve performance. Third, while randomly permuting layer order as in the previous approaches, we try to predict from the output of every layer at which position the layer is currently located in the network using a small layer position prediction network for every layer (Section 3.4). We now describe these approaches in more detail."}, {"title": "3.1 VISION TRANSFORMERS", "content": "Dosovitskiy et al. (2020) have successfully adapted the transformer architecture to computer vision by introducing a preprocessing step that converts images to suitable sequences. They do so by splitting an image $x \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of N flattened patches $x_p \\in \\mathbb{R}^{N \\times (P^2C)}$, and then pass each patch through a linear embedding layer $E \\in \\mathbb{R}^{(P^2C) \\times D}$. H,W and C are here the height, width and number of channels of the image respectively and P is the patch size. D is the internal latent dimension of the transformer which remains constant throughout the network and can be set as a hyperparameter.\nAfter converting the image into a sequence that can be processed by a transformer encoder, inspired by BERT (Devlin et al., 2018), the authors prepend a class token to $x_p$ in which the class infor- mation of the input image can be aggregated by the transformer. To encode position information into the embedding, a positional embedding tensor $E_{pos} \\in \\mathbb{R}^{(N+1) \\times D}$ is added. Both the class token as well as the positional embeddings are learnable embeddings, which are trained jointly with the rest of the network. The resulting input sequence presented to the transformer network can be expressed as\n$Z_0 = [x_{class}; x_1E; x_2E; ...; x_NE] + E_{pos}$.\nThis sequence is presented to a standard transformer architecture of stacked attention modules. Each attention module consists of a multi-head self-attention (MSA) layer and a feedforward layer or mul- tilayer perceptron (MLP) layer. MSA layers utilize self-attention (SA) (Vaswani et al., 2017), a pow- erful concept that allows transformers to relate and combine its feature embeddings with each other. Self-attention extracts features from the input sequence z, which in turn preforms a transformation of the input vector sequence.\nSpecifically, self-attention extracts query, key and value sequences q, k and v from the input se- quence using a linear projection $U_{qkv} \\in \\mathbb{R}^{D \\times 3D_h}: [q,k, v] = zU_{qkv}$. The q and k sequences are then used to compute a Softmax-normalized transformation matrix A indicating how to incorpo- rate information of the whole sequence (i.e. in our case all image patches) for every single vector of the sequence: $A = Softmax(\\frac{qk^T}{\\sqrt{D_h}})$. Scaling the dot-product product by $\\sqrt{D_h}$ here ensures a balanced distribution of the Softmax output. After obtaining A, the output of SA is computed as $SA(z) = Av$.\nA multi-head self-attention (MSA) layer (Vaswani et al., 2017) performs several attention operations in parallel, concatenates the result and projects it back to the internally used latent dimension of the transformer:\n$MSA(z) = [SA_1(z); SA_2(z); ...; SA_k(z)]U_{msa}$\nIn an attention module the multi-head self-attention layer is followed by a multi-layer-perceptron (MLP) layer transforming the recently combined embeddings to extract new feature representations. Before presenting z to each layer in the module, the embeddings are normalized using LayerNorm (Ba et al., 2016). To ensure consistent gradient flow during training, residual connections (He et al., 2016) are behind both the MSA and the MLP layers (Wang et al., 2019). Furthermore, as a regu- larization measure, Dropout (Srivastava et al., 2014) is applied after every MSA and MLP layer. In summary, given the sequence $z_{t-1}$ from a previous attention module as input, we first compute the intermediate representation\n$z'_t = MSA(LN(Z_{t-1})) + Z_{t-1}$, (1)\nwhich is the presented to the MLP layer to compute the final output of the module\n$z_t = MLP(LN(z'_t)) + z'_t$. (2)\nFinally, after N attention modules, the first vector of the sequence (corresponding to the class- token in the preprocessed input) is handed to a linear layer $W_{out} \\in \\mathbb{R}^{D \\times C}$ to predict the final class of the image: $y = argmax(z_1 W_{out})$. C denotes the number of classes."}, {"title": "3.2 RANDOMLY PERMUTING LAYER ORDER DURING FORWARD PASS", "content": "During each forward pass, i.e. for each batch presented to the vision transformer, we randomly permute the execution order of layers during training. The intention here is to teach the layers to not"}, {"title": "3.3 LAYER POSITION ENCODING", "content": "In the second approach, LayerShuffle-position, we provide each layer with its current position in the network. Through this variation we aim to test if each layer can already adapt sufficiently by itself to information coming from different layers during test time or if giving it the current position can help further. In more detail, jointly with permuting the layer execution order, each layer learns a vector embedding $e_{layer}^p \\in \\mathbb{R}^{F}$ for each possible index position $p \\in [1, L]$ of the layer during training, where L is the number of layers and F = 32 is our chosen embedding dimen- sion. The layer's current index p in the network is presented together with the input to the layer $Z_{t-1}$. The layer fetches the embed- ding vector $e_{layer}^p$ associated with the passed index p and concatenates it to the input vector $Z_{t-1}: h_t = concat(Z_{t-1}, repeat(e_{layer}^p, N + 1))$. N is the number of patches extracted form the input image, the functions concat and repeat respectively concatenate and repeat tensors along their last (most varying) dimension. A projection network, which consists of a LayerNorm (LN) (Ba et al., 2016) module, a single linear layer $W_{proj}$, a GELU (Hendrycks & Gimpel, 2016) activation function as well as a Dropout (Srivastava et al., 2014) module, is then used to combine input and embedding and reduce it again to the used latent dimension D of the transformer. To ensure gradient flow during training, a residual connec- tion is added as well:\n$z'_t = Dropout(GELU(LN(h_t)W_{proj})) + Z_{t-1}$\nThe resulting output $z'_t$ is passed on to a regular multi-head-attention-and-feed-forward structure as described in Equations 1 and 2."}, {"title": "3.4 PREDICTING CURRENT LAYER POSITION", "content": "To determine if the incoming information to each attention layer is indeed sufficient for it to figure out its role, we specifically test for this ability with the LayerShuffle-predict variant. We equip each layer of the network with a simple position prediction module that takes the current layer output as an input and seeks to predict the current position of the layer in the network. The module consists of a single linear layer $W_{pred} \\in \\mathbb{R}^{D \\times L}$ receiving layer- normalized (LN)input. $u = LN(z_t)W_{pred}$.\nEach of these layer order prediction modules optimizes a cross- entropy loss where then the overall network optimizes the loss\n$L_{out} + \\sum_i L_i$. Here, $L_{out}$ is the regular cross-entropy loss of the output layer, and $L_i$ is the layer position prediction loss of layer i, which is also a cross-entropy loss:\n$L_i = - log( \\frac{exp(u_p)}{\\sum_{l \\in L} exp(u_l)})$,\nwhere L is the number of layers in the network, u is the L-dimensional output of the position prediction network of layer i, and $u_l$ denotes the l-th dimension of the vector. $u_p$ is the output logit denoting the network's predicted confidence that the layer currently is deployed at its actual position with index p."}, {"title": "4 EXPERIMENTS", "content": "We conduct our experiments on the ILSVRC2012 dataset (Russakovsky et al., 2015), more com- monly termed ImageNet2012. We use the original ViT-B/16 (Dosovitskiy et al., 2020) vision trans- former, with the publicly available pre-trained weights provided on huggingface (vit). This model has been pre-trained on ImageNet21k (Ridnik et al., 2021) at an 224\u00d7224 input image resolution and refined on ImageNet2012 at the same resolution. We refine the model again on ImageNet2012 at the same resolution, but using the training processes as described in Section 3. That is, we ran- domly permute layer execution order while refining the model. To establish a baseline, we refine the original ViT-B/16 for one more epoch without changing the layer order. We have limited ourselves to a single epoch here as we found any longer training to be unlikely to bring additional improve- ment in preliminary experiments. For each approach, including the baseline, we train 5 networks and compare their average validation accuracy.\nAll models are refined using Adam (Kingma & Ba, 2014) ($\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-6}$),\nwhere an initial learning rate of $10^{-4}$ was empirically found to work best. In terms of batch size, we evaluate training batch sizes of 640 images, which is the maximum multiple of 8 that can fit in the video memory of our used GPU, as well as 128 images for models that benefit from a smaller batch size. We find that even smaller batch sizes do not yield any improvement in performance for our models. Inspecting training curves shows that the performance of our models plateaus at 20 epochs the latest, which we therefore set as the maximum number of training epochs. We use a form of early stopping by evaluating the model achieving lowest crossentropy loss on the validation set after 20 epochs. All models have been trained on a single NVIDIA H100 Tensor Core GPU with 80GB of memory, where the training of a single model for 20 epochs takes about 7 hours."}, {"title": "4.1 SEQUENTIAL VS. ARBITRARY EXECUTION ORDER", "content": "We report the average accuracy for the baseline and LayerShuffle approaches in Table 1. Our base- line refined from a pre-trained ViT-B/16 model performs very much as expected: For a classic se- quential execution order of the model layers, the average validation accuracy performs very close to the original model (Dosovitskiy et al., 2020) at 82.61% at a low standard deviation of 0.08. Not surprisingly, for an arbitrary layer execution order, the average model accuracy declines catastroph- ically to 0.13%. Our original assertion that in general, neural networks are not robust to changing the execution order of their layers, is in line with these results."}, {"title": "4.2 REMOVING LAYERS DURING TEST TIME", "content": "To determine how neural networks trained with LayerShuffle would perform when several devices in a (distributed) model become unavailable, we further investigate the effect of pruning an increasing amount of layers during test time. We evaluate its average validation accuracy over 5 models when only using 3, 6, or 9 layers. In addition, we refine the original ViT-B/16 transformer using LayerDrop (Fan et al., 2019) with a drop probability of 0.2 (as recommended by the authors) and compare it as a baseline to our approach under identical conditions. Note that whenever we evaluate the accuracy of our proposed approaches as well as the baseline, we do so two times: Once, for the original \"sequential\" layer order as originally intended and trained for the ViT-B/16 transformer, and once with arbitrary layer execution order where we change the order randomly for every forward path.\nFor sequential execution LayerDrop with a drop rate of 0.2 behaves similarly to Lay- erShuffle, with the exception that our approach performs better for a small number (3) of layers with an average accuracy of approximately 18% vs. close to 0% for LayerDrop. While for 6 layers, both approaches are roughly on par, for 9 layers LayerShuffle is slightly outperformed by LayerDrop as both approaches show an average accuracy in the 70 \u2013 80% range. At the full amount of 12 layers,"}, {"title": "4.3 ANALYSIS OF INTERMEDIATE NETWORK REPRESENTATIONS", "content": "To gain a deeper insight into how information is encoded in our trained models, we compute Uni- form Manfold Approximation and Projection (UMAP) (McInnes et al., 2018) embeddings of the outputs of a particular attention module, where we color-code all output vectors based on the po- sition the module held in the network when producing this output. For the UMAP analysis, we present 1,000 randomly sampled images from the ImageNet2012 validation set to a LayerShuffle- trained model. While we use an evaluation batch size of 1 image and record all outputs of a single, previously selected layer, we randomly permute the execution order of layers such that the selected layer changes position in the network during every forward path. After the layer output vectors for all 1,000 images have been recorded, a UMAP reduction of the output space to 2D is performed."}, {"title": "4.4 MERGING MODELS WITH ARBITRARY EXECUTION ORDER", "content": "Being robust against permuting the layer execution order, opens interesting other possibilities such as model merging, i.e. creating a new model from the layers of several identically trained models. The underlying rationale is that such merged models could also occur in a distributed setting, where compute nodes, whose layers have been trained as part of distinct models, but with the same training process, could form ad-hoc models together.\nTo construct merged models, where each layer stems from a different model, we require 12 models for the 12 layers of the ViT-B/16. We therefore train 7 more networks for our LayerShuffle approach and the baseline. Subsequently, we create 100 merged models (out of 12! possible combinations) by randomly sampling from these models for our proposed approach as well as the baseline respectively (models are not mixed between approaches). As mentioned previously, layers are sampled in such a way that no two layers in a merged model stem from the same model. We then evaluate the validation accuracy of all 100 models for both approaches.\nTable 5 summarises the results. The merged baseline model ViT-B/16 deteriorates from 82.61% average accuracy to 1.87% (despite sequential layer execution as required by the model) making the resulting merged model effectively unusable. The merged LayerShuffle models, on the other hand, perform slightly below the original model with an average accuracy of 59.68% as opposed to the 62.77% of the latter. Less surprisingly, merged models show a higher standard deviation at 1.15 percentage points for the merged models vs. 0.41 percentage points for the original ones as merged models do not contain any two layers that have been trained together, which makes their performance vary more. We can further improve performance by ensembling the 12 models trained with LayerShuffle, using the average of their output logit vectors. Such neural network ensembles often reach a better performance (Hansen & Salamon, 1990), which is also the case here with a significant improvement and an accuracy of 69.20% for LayerShuffle Ensemble.\nVisualizing the accuracies of all achieved models in a swarm plot paints a similar picture. While most merged LayerShuffle achieve an accuracy close to the average accuracy of the original models, for the baseline, all merged models exhibit very low accuracies. In conclusion, we find that permuting layer order during training enables the construction of merged (or \"Frankenstein\u201d) vision transformers, where each layer of the transformer can be taken from a different model, as long as all models have been refined from the same base model on the same data."}, {"title": "5 DISCUSSION AND FUTURE WORK", "content": "This paper presented a new approach called LayerShuffle, which enabled vision transformers to be robust to arbitrary order execution, pruning at test time, as well as adhoc-construction of merged models. For sequential execution, LayerShuffle performs on average only slightly worse than the LayerDrop approach but is the only method that works when the layer execution is arbitrary. A latent space analysis confirmed that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network. Finally, we investigated whether it is possible to build merged models from the models trained with LayerShuffle and found the performance of the built merged models to be only slightly less than the performance of our trained models, contrary to the baseline, where virtually all merged models delivered very poor performance.\nThat LayerShuffle works well is surprising and here we have only scratched the surface of its effect on the trained neural networks. To gain a deeper understanding, further analysis will include an inspection of the norms of the outputs of multi-head-attention and multi-layer-perceptron layers. This type of investigation could reveal whether layers learn to switch off their output for inputs they cannot process such that further relevant processing can occur in a suitable downstream layer to which the data is relayed through the attention module's residual connections. Inspecting the model's attention maps and embedding the intermediate latent vectors of all layers at all positions in a single two-dimensional embedding might yield further insights as well.\nIn the future, these properties could make LayerShuffle-trained models ideal candidates to be dis- tributed over a number of very loosely coupled compute nodes to share the computational load of model inference. Given the enormous engineering, financial and logistical effort as well as the envi- ronmental impact of building and maintaining datacenters for state-of-the-art deep learning approaches on the one hand, as well as the large amount of available, but scattered compute through existing smartphones, laptop computers, smart appliances and other edge devices on the other hand, approaches that allow ad-hoc neural networks performing inference together could be of great impact. We therefore consider the deployment and orchestration of our trained models onto an actual set of edge devices and the practical implementation of the inference process on a network of such devices, likely by combining our approach with previously proposed frameworks to address this issue (Gacoin et al., 2019), a very promising direction of future research."}]}