{"title": "Open Problems in Machine Unlearning for AI Safety", "authors": ["Fazl Barez", "Tingchen Fu", "Ameya Prabhu", "Stephen Casper", "Amartya Sanyal", "Adel Bibi", "Aidan O'Gara", "Robert Kirk", "Ben Bucknall", "Tim Fist", "Luke Ong", "Philip Torr", "Kwok-Yan Lam", "Robert Trager", "David Krueger", "S\u00f6ren Mindermann", "Jose Hernandez-Orallo", "Mor Geva", "Yarin Gal"], "abstract": "As AI systems become more capable, widely deployed, and increasingly au- tonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount. Machine unlearning - the ability to selectively forget or suppress specific types of knowl- edge - has shown promise for privacy and data removal tasks, which has been the primary focus of existing research. More recently, its potential application to AI safety has gained attention. In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety. In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes - unlearning this information could strongly affect beneficial uses. We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms. Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning. By mapping these lim- itations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required.", "sections": [{"title": "Introduction", "content": "For much of the history of machine learning, the primary challenge was enabling models to acquire broad knowledge effectively. However, as models have grown increasingly capable, their ability to access and process vast amounts of information \u2013 particularly in sensitive domains such as biology, chemistry, and cybersecurity \u2013 has heightened concerns about their potential to cause significant harm. Improving the safety, controllability, and alignment of AI systems increasingly requires preventing them from exhibiting harmful behaviors."}, {"title": "Applications of Unlearning for AI Safety: An Overview", "content": "To fully explore the interaction between unlearning and AI safety, it is essential to first clarify the objective of machine unlearning and how machine unlearning modifies system behavior for the broader landscape of AI safety."}, {"title": "Goal: Machine Unlearning for AI Safety", "content": "Machine unlearning aims to modify an AI system so it forgets specific knowledge or behaviors, examples of which are provided in a \u201cunlearning corpus\". Forgetting means the updated system should no longer exhibit or retain any knowledge or behaviors demonstrated in the forget corpus. Simultaneously, the system's performance on tasks unrelated to the forget corpus must remain unaffected, ensuring its overall utility is preserved.\nWhile privacy-focused indistinguishability objectives seek to prevent models from learning private information with minimal impact on capabilities, unlearning aims to undo the effect of learning from certain data points entirely. In other words, unlearning for AI safety requires the deliberate suppression or removal of specific knowledge or behaviors, ensuring the system cannot express or leverage the forgotten information. In this section, we shall provide a more in-depth exploration of the application areas of unlearning for AI safety: (i) Safety-Critical Knowledge Management, (ii) Correcting Value Alignment and Improving Corrigibility, (iii) Verification and Governance Aspects, and (iv) Privacy and Legal Compliance."}, {"title": "Unlearning for Safety-Critical Knowledge Management", "content": "Dual-Use Hazardous Knowledge: Increasingly advanced models may pose serious real-world risks from hazardous knowledge such as assisting in synthesis pathways for CBRN materials or cybersecurity vulnerabilities. Current approaches address this through multiple strategies: knowledge editing and targeted pruning to identify and remove specific parameters encoding undesired knowledge; analysis of model attention weights and activation patterns to identify neurons strongly associated with targeted knowledge."}, {"title": "Mitigating Adversarial Attacks & Jailbreaks", "content": "Models face vulnerabilities from carefully crafted inputs that can bypass safety measures. For instance, adversarial prompts might cause language models to output inappropriate violent, sexual, or biased content . Protection against these threats could involve selective forgetting of features associated with adversarial examples, training with weighted adversarial examples to reduce attack pattern sensitivity , and rigorous verification through adversarial robustness testing ."}, {"title": "Correcting Value Alignment and Improving Corrigibility via Unlearning", "content": "Reward Hacking: In reinforcement learning contexts, policy models can discover unintended shortcuts or loopholes in reward models. A common example is how language models may generate unnecessarily verbose outputs after preference learning, exploiting the tendency for longer responses to be preferred in pair-wise preference datasets. Popular approaches that aim to address this include: early stopping, information-theoretic reward modeling , spurious factor disentangling , and implementing constrained optimization with heuristic human knowledge about the shortcut .\nValue Alignment: Models may develop behaviors that are misaligned with specific human prefer- ences. Potential directions to achieve better value alignment include: reward modeling with iterative refinement based on human feedback, applying gradient-based modification of parameters associated with misaligned behaviors, and integrating interpretability methods for verification .\nSituational Awareness: Recent LLMs have demonstrated some awareness of the situations of their own development, including knowledge about their creators, capabilities, and the broader LLM development and deployment lifecycle. With greater situational awareness, some researchers have raised concerns that AI systems could leverage this knowledge harmfully if their goals are misaligned with the goals of their developers. For example, AI systems could manipulate the known biases of human reviewers to earn higher reward scores, or insert security vulnerabilities into users' codebases to enable self-exfiltration. Recent work shows that in certain circumstances, large language models can leverage situational knowledge about their training process to \u201cfake alignment\" in an effort to prevent a developer from changing the model's objectives . New unlearning methods could help address these risks by providing a mechanism to selectively modify or remove an AI system's knowledge of its own situation. This controlled modification serves as both a diagnostic tool, revealing potential misbehavior patterns, and a preventive measure, allowing misbehavior to be corrected before deployment."}, {"title": "Verification and Governance Aspects of Unlearning", "content": "As AI systems become increasingly pervasive, ensuring compliance with governance frameworks becomes critical. Strategies for verification and compliance include: explicit compliance objectives in system design, utilizing interpretability methods to identify non-compliant behaviors, integrating formal verification methods where applicable , and ensuring adherence to industry standards and regulatory requirements. Designing and implementing robust evaluation standards , algorithms for formal verification , and audit protocols with varying levels of model access are essential components of effective machine unlearning. For example, Hong et al.  identity parametric concept vectors that are strongly correlated with specific dual-use knowledge and assesses the effectiveness of the unlearning approach by the alternation of concept vectors."}, {"title": "Unlearning for Privacy, Data Protection and Legal Compliance", "content": "In today's digital landscape, apps, websites, and home automation devices continuously collect user behavior patterns, including personally identifiable information (PII). When users become aware of potential PII leakage, they may request deletion as mandated by regulations like GDPR . However, simply deleting PII from databases proves insufficient, as models may have memorized this information during training, potentially allowing for data reconstruction by model developers or even other users of a model. Existing verification techniques can only provide probabilistic guarantees about knowledge removal. This mismatch creates practical challenges where legal frameworks assume binary deletion while technical reality operates on a continuous spectrum.\nTo fully address these privacy concerns, several key capabilities are necessary. First, unlearning algorithms must be able to identify and remove specific factual knowledge about individuals. Second, they need mechanisms to modify model behavior to avoid generating content that reveals private information. Finally, they can adjust internal representations to reduce membership inference risks. While this application represents an essential aspect of AI safety, it will not be the focus of our study, as it has been the primary focus of existing unlearning surveys (see Yao et al. )."}, {"title": "Unlearning Practices and Evaluation Techniques for AI safety", "content": "In pursuit of a safe and reliable AI system, various techniques have been developed for machine un- learning. In this section, we provide an overview of machine unlearning including current techniques for unlearning and evaluation dimensions to assess unlearning algorithms ."}, {"title": "Evaluation Methods for Unlearning: What are the promising directions for AI Safety?", "content": "Establishing evaluation methods and metrics is crucial to assess and compare the effectiveness of unlearning techniques. Drawing inspiration from recent literature on machine unlearning and interpretability, we can identify several key evaluation metrics and methods.\nGeneralizability: Downstream performance is an important dimension to evaluate the success of unlearning. If the targeted objective of unlearning is a specific down-stream task , the down-stream performance on the specific task is the most direct way for evaluation. For example, ZRF score measures the similarity between the unlearned model performance and a randomly initialized model. On the other hand, if the unlearning objective is a specific type of harmful knowledge, evaluation on corresponding harmfulness benchmarks is indispendisble .\nLocality: Locality is another important dimension for unlearning algorithms. During the unlearning process, the benign capacities of the AI system is supposed to remain unaffected. For unlearning on large language models, language modeling benchmarks are good choices to measure the maintenance of basic language modeling ability. Meanwhile, instruction-following benchmarks and world knowledge benchmarks are widely adopted to measure the side effects of unlearning on the instruction-following and commonsense reasoning abilities of LLMs.\nEfficiency and Resource: Aside from downstream accuracy and behavioral change, practical considerations are essential for real-world applications of unlearning. Time cost and computational resources required are two important factors when comparing and assessing different unlearning algorithms, since some approaches to unlearning involve the computation or approximation of the Hessian matrix , which is time-consuming. In addition, early work on the unlearning of the vision model tends to track the time or the difficulty involved in relearning the unlearned task which provides insights into the depth of unlearning."}, {"title": "What Metrics Would be Most Suitable for AI Safety Applications?", "content": "These simple metrics related to unlearning success are useful, standardizable measures for unlearning progress. However, they are not suitable for real-world safety circumstances in which unlearning may be relied on. We expand here on on what we feel are the most promising directions for metrics.\nThere are many ways to elicit knowledge from models , so placing too great a focus on some measures can cause other practical failure modes to be neglected. This highlights the need for adversarial evaluations for machine unlearning to more thoroughly evaluate the practical success of unlearning methods. Prior work has shown that input-space knowledge elicitation techniques can be effective at extracting unlearned knowledge from LLMs . Other threat models may also be applicable for cases in which models might be deployed open-source or with a fine-tuning API. have all demonstrated that \u201cunlearned\" knowledge can be extracted from analysis of the internal mechanisms of LLMs.\nHowever, a variety of works have shown that unlearned knowledge can be very sample-efficiently re-learned through few-shot fine-tuning. This poses interesting open challenges to suitable evaluation metrics for unlearning in safety-specific applications."}, {"title": "Current Unlearning Methods", "content": "To achieve effective unlearning for AI systems, various approaches have been developed in recent years. In this subsection, we provide an extensive review of recent approaches and elaborate on their pros and cons.\nGradient Ascent: To unlearn or offset the effect of the unlearning corpus, an intuitive approach is to reverse the direction of the parameter gradient and maximize the loss function on the unlearning corpus. Mathematically, the learning objective of gradient ascent to maximize is\n $L = -E_{(x,y)\\sim D_f} log M(y | x)$(1)\nHowever, first-order gradient ascent on the full set of model parameters tends to suffer from perfor- mance degradation, as the reversed gradient disrupts not only knowledge and abilities related to the unlearning corpus but also the irrelevant knowledge. To deal with the deficits of first-order gradient ascent, a common approach is to balance the gradient ascent with KL divergence or language modeling loss on the remaining corpus . For instance, Wang et al. Chen and Yang minimizes the KL divergence between the unlearned model $M'$ and the original model $M$ on the remaining corpus while enlarging their KL divergence on the unlearning corpus. Apart from constraining the model parameter updates with the KL divergence, performing gradient ascent only on the model parameters that are most related to the unlearning target is another plausible approach. As an example, Yu et al.  identify bias-related neurons with integrated gradient  and conduct gradient ascent only on the selected neurons. Recently, second-order optimization with Sophia or inverse empirical Fisher approximation seems to be another promising approach to implement gradient ascent without sacrificing the model utility.\nRepresentation Misdirection: Different from gradient ascent that optimizes model parameters towards a reversed optimization function, representation misdirection aims to remove the unlearning target by misdirecting their intermediate hidden representation towards random noise. For example, Li et al.  minimize the Euclidian distance between the representation of CBRN knowledge after the eighth transformer layer and a fixed random noise. Similarly, Rosati et al. and Zou et al. optimize the representation of potentially harmful knowledge towards an uninformative noise. However, a recent work suggests that the techniques might fail when the norm of the harmful representation is larger than that of the random noise. They further propose to use an adaptive noise for scaling the random noise to avoid this case. Meanwhile, another recent work finds that a significant proportion of the effect of representation misdirection is attributable to the random noise injected into the residual norm when the harmful context is detected, rather than removing the potentially harmful knowledge directly.\nTask Vectors: Recently, task vectors have emerged as a lightweight technique to achieve unlearning. Originally proposed by Ilharco et al. task vectors refer to the difference between a fine-tuned model and its base pre-trained model in the model parameter space. To be more specific, to achieve machine unlearning, first a reinforced model $M_f$ is obtained by tuning the original language model $M$ on the unlearning corpus. Then we can find the forgetting task vector by subtracting M from $M_f$ in an element-wise manner. Subsequently, the task vector is subtracted from the original model to produce an unlearned model, which is verified to forget the learning corpus without harming other abilities . Intuitively, the task vector serves as the"}, {"title": "Open Problems in Machine Unlearning for AI Safety", "content": "The application of machine unlearning to AI safety introduces unique challenges that extends beyond traditional machine unlearning. Resolving these open problems will be crucial for developing unlearning as a reliable tool for AI safety."}, {"title": "Evaluation of Unlearning", "content": "Simple metrics that check whether models can reproduce specific training examples fail to capture the deeper challenges of unlearning in safety-critical contexts. When models undergo modifications, face adversarial attacks, or encounter unusual inputs, unlearned capabilities can unexpectedly resurface, particularly in cases where the unlearning relied on fine-tuning or basic parameter adjustments . This happens because these methods typically mask rather than eliminate capabilities, leaving the fundamental neural patterns that enable them largely untouched . More rigorous standards are helpful in addressing these limitations. This includes ensuring that forgotten knowledge cannot be recovered, does not reappear during extended interactions, and remains inaccessible even in new contexts or under adversarial pressure. Such verification must consider both specific knowledge removal and broader capability prevention , examining:\n1. How models might rebuild unlearned capabilities through indirect means, like reconstructing security exploits by combining basic programming concepts\n2. Ways that remaining knowledge could combine to recreate harmful capabilities\n3. Whether the unlearning remains robust against deliberate attempts to recover removed capabilities\nThese challenges highlight a fundamental issue: preventing a model from reproducing specific content doesn't guarantee that it can't reconstruct the underlying capabilities through other means. Developing reliable verification methods that can detect and prevent such reconstruction remains an active challenge in the field.\nEvaluation Challenges: As discussed earlier in Section 3.1, existing metrics and assessments appear successful on specific tasks, but they might fail to capture the broader impact on model capabilities - a model might pass targeted tests while retaining problematic behaviors that appear in subtler ways. Long-term effects remain particularly challenging to assess. Current metrics typically focus on immediate post-unlearning evaluation but provide little insight into how unlearning impacts model behavior over extended periods or under adversarial attack . This limitation becomes critical when considering how models might gradually reconstruct supposedly removed capabilities through continued operation. The field needs standardized benchmarks that the AI safety community can rely on. While newer approaches show promise-such as tracking changes in model activations and using interpretability tools to understand how unlearning affects learned features-integrating these into practical, computationally feasible frameworks remains an open challenge."}, {"title": "Robustness to Relearning", "content": "Even when effective, unlearning can be surprisingly vulnerable to fine-tuning and could quickly relearn the hazardous knowledge, even if fine-tuned on small amount of benign, unrelated data. This suggests that existing techniques have a limited ability to thoroughly remove hazarous knowledge from LLMs. It also poses a significant challenge to the safety of open-source models or proprietary models that can be fine-tuned . Some works have aimed to perform unlearning in a way that is more robust to post-deployment tampering ."}, {"title": "Dual-use Capabilities Can Emerge From Beneficial Elements", "content": "The distinction between knowledge and capabilities presents a fundamental challenge in safety-critical knowledge removal domains . Traditional unlearning scenarios target specific data points or patterns, but safety-critical applications must focus on preventing dual-use capabilities while preserving beneficial ones. While knowledge typically represents localized information (like specific facts or patterns), capabilities emerge from the complex interaction and integration of multiple pieces of knowledge potentially across different domains, making them inherently more distributed throughout a model's parameters and allowing potentially harmful capabilities to emerge even from combinations of seemingly harmless knowledge. It remains a critical challenge to prevent harmful capabilities from emerging from combinations of knowledge."}, {"title": "Context-Dependent Challenges", "content": "Knowledge removal in AI systems presents distinct challenges depending on how the knowledge is represented and used. At the simplest level, removing the effect of specific training data points is relatively straightforward. More complex cases arise when removing general knowledge that may be distributed across multiple training instances. The most challenging scenarios involve knowledge that is either context-dependent (as in dual-use cases) or knowledge that combines with other model capabilities to enable complex behaviors not directly tied to specific training examples \u2013 for example, when basic language understanding combines with reasoning skills to enable sophisticated problem- solving abilities.\nThis progression reflects an increasing difficulty in attribution - from clearly identifiable training data points, to distributed but traceable knowledge, to knowledge whose safety depends on the context, and finally to capability-related knowledge where attribution becomes highly complex due to how capabilities arise from the synthesis of multiple basic competencies rather than from discrete, identifiable training examples.\nThe management of dual-use knowledge in domains like CBRN and cybersecurity necessitates sophisticated access control mechanisms. Knowledge appropriate for trusted contexts may prove dangerous in others\u2014for instance, detailed vulnerability information crucial for cybersecurity profes- sionals requires careful containment. The challenge extends beyond simple knowledge removal to implementing context-dependent access controls through unlearning techniques .\nIn addition, removing dual-use knowledge can create blind spots in the model's safety mechanisms. For instance, removing a detailed understanding of security vulnerabilities might prevent a model from effectively identifying and avoiding similar vulnerabilities in new contexts. This creates a practical dilemma: maintaining robust safety guardrails may require preserving some of the very knowledge we aim to remove ."}, {"title": "Neural and Representational Level Interventions", "content": "Attempts to manipulate knowledge in AI systems must grapple with the complex relationship between low-level neural parameters (weights and activations) and higher-level patterns that emerge from them. While we can intervene at the level of individual weights or try to target specific semantic concepts , the relationship between these different aspects of the system is not fully understood. For example, removing a model's ability to generate harmful content might inadvertently affect its understanding of context and nuance in related but benign domains.\nThese challenges directly connect to our earlier discussion of knowledge and capabilities in Section 4.4. While knowledge might be modified at the representational level (like removing understand- ing of specific harmful concepts), the distributed nature of capabilities means that the corresponding neural-level changes could affect multiple capabilities simultaneously."}, {"title": "Continual and Iterative Unlearning", "content": "Current unlearning methods mostly have a fixed unlearning corpus and struggle to fully unlearn un- wanted behaviorsanderelated knowledge at the same time. As a possible remedy, iterative unlearning has the potential for attaining a better trade-off among multiple unlearning objectives. By dynamically adjusting the weights of multiple optimization objectives in the loss function of unlearning , iterative learning enables a balance between effectiveness and locality.\nSequential unlearning is another common challenge in real-world scenarios where users make sequential unlearning requests across time . Current unlearning methods show trade-offs between the degree of unlearning achieved and preservation of model utility. These methods typically achieve partial unlearning while experiencing some degradation in model performance. When unlearning requests are processed sequentially, each operation begins with a model that is already degraded, leading to a cumulative loss in utility over time. As performance deteriorates over multiple rounds, the model can be destabilized and most valuable knowledge can be unintentionally erased. One key direction is developing solutions that can scale across large number of unlearning requests, maintaining the broad knowledge of foundation models measured by performance, while accommodating numerous unlearning requests across time.\nThe specification of the unlearning targets themselves requires an iterative approach. Similar to how reward specification in reinforcement learning often requires iterative refinement when agents find unexpected ways to exploit the initial reward function , precisely defining what knowledge to unlearn often requires multiple rounds of adjustment as we discover new ways that harmful capabilities can manifest. This necessitates developing frameworks for progressive refinement of unlearning targets, allowing more nuanced and precise interventions over time, aimed at reducing the risk of missing critical information or removing valuable data. Additional key questions include whether unlearning operations should be amortized by combining multiple unlearning requests before execution, and how to effectively intersperse unlearning with ongoing learning processes."}, {"title": "Capability Interactions and Dependencies", "content": "The interactions between safety measures reveal several challenges and opportunities that emerge when implementing unlearning in practice. These relationships go beyond simple dependencies, exposing fundamental tensions and trade-offs to be carefully managed."}, {"title": "Conclusion", "content": "In this paper we have explored the usefulness of unlearning techniques for AI Safety. Despite its popularity as a potential solution for AI Safety challenges, a series of fundamental constraints limit unlearning's effectiveness, particularly for capability control. While unlearning shows promise for data removal tasks, our analysis reveals inherent limitations in controlling AI capabilities. The emergent nature of dual-use capabilities from combining seemingly benign knowledge presents an inherent limitation that unlearning cannot fully address. The context-dependent nature of knowledge representation, especially in dual-use scenarios, further prevents reliable selective capability removal. Several constraints compound these core limitations. Current approaches to neural-level interventions often produce unintended effects on broader model capabilities, adding practical challenges to selective capability control, while the difficulty of verifying unlearning success and robustness against relearning raises additional concerns. Furthermore, unlearning interventions can create tensions with existing safety mechanisms, potentially affecting their reliability. These inherent constraints demonstrate that unlearning must be viewed as one component in a broader safety framework, not a complete solution. By mapping these fundamental limitations, we aim to guide research toward developing realistic approaches that acknowledge unlearning's bounded role in AI safety. Future work should proceed along two paths: (1) identifying specific use cases where unlearning can be effectively applied for data removal, and (2) developing alternative approaches for capability control given the fundamental limitations of unlearning in this domain."}]}