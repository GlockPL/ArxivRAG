{"title": "Bias Detection via Maximum Subgroup Discrepancy", "authors": ["Ji\u0159\u00ed N\u011bme\u010dek", "Mark Kozdoba", "Illia Kryvoviaz", "Tom\u00e1\u0161 Pevn\u00fd", "Jakub Mare\u010dek"], "abstract": "Bias evaluation is fundamental to trustworthy AI, both in terms of checking data quality and in terms of checking the outputs of AI systems. In testing data quality, for example, one may study a distance of a given dataset, viewed as a distribution, to a given ground-truth reference dataset. However, classical metrics, such as the Total Variation and the Wasserstein distances, are known to have high sample complexities and, therefore, may fail to provide meaningful distinction in many practical scenarios.\nIn this paper, we propose a new notion of distance, the Maximum Subgroup Discrepancy (MSD). In this metric, two distributions are close if, roughly, discrepancies are low for all feature subgroups. While the number of subgroups may be exponential, we show that the sample complexity is linear in the number of features, thus making it feasible for practical applications. Moreover, we provide a practical algorithm for the evaluation of the distance, based on Mixed-integer optimization (MIO). We also note that the proposed distance is easily interpretable, thus providing clearer paths to fixing the biases once they have been identified. It also provides guarantees for all subgroups. Finally, we empirically evaluate, compare with other metrics, and demonstrate the above properties of MSD on real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Regulatory frameworks, such as the AI Act [41] in Europe, suggest that one needs to measure data quality, including bias detection in training data, as well as to detect bias in the output of the AI system, but provide no suggestions as to what bias measures to use. This is the case of the very recent IEEE Standard for Algorithmic Bias Considerations [24] and earlier NIST white papers [42] too, where the latter stops at the \"majority of fairness metrics are observational as they can be expressed using probability statements involving the available random variables\".\nAt the most basic level, one could imagine bias detection as a two-sample problem in statistics, where, given two sets of samples, one asks whether they"}, {"title": "2 Background and Related work", "content": "There are numerous distances on measure spaces used in applied probability, including (in the approximately chronological order) Total Variation (TV, [25]), Hellinger distance [22], Kullback-Leibler (KL) divergence [31] and its variants, Wasserstein-2 [50, 13] and its variants such as Wasserstein-1 [50], and Maximum"}, {"title": "2.1 Distances on Measure Spaces", "content": "Mean Discrepancy (MMD, [19]). As we outline below, most of these methods have exponential sample complexity in terms of the number of dimensions. As can be seen in Table 1, most of these distances come with either undecidability results [33] or exponential lower bounds on their sample complexity in the worst case. For MMD, while [19] claimed polynomial sample complexity, [47] explained the lower bounds on sample complexity under strong assumptions. Yet for other methods, such as Hellinger and Jeffreys distances, high sample complexity follows from their relationship to TV distance. We refer to [40] for a thorough survey.\nObviously, one can consider additional assumptions, such as having cardinality of the support (which scales with exp(d) in general), bounded by a constant. Testing TV closeness in time sublinear in the support was derived in [4]. Entropy estimation bounds were obtained in [49]. [14, 2] give an algorithm for estimating TV between just product measures, polynomial in product dimension. Likewise, one can consider smoothness of the measures and certain invariance properties [5, 46], or focus on Gaussian distributions [23] only. Ising type models testing was explored in [26]. While these assumptions are of considerable interest, it is not easy to test that those assumptions hold in real-world data sets."}, {"title": "2.2 Subgroup and Intersectional Fairness", "content": "The notion of subgroups gave rise to the work on subgroup fairness [27], and underlies the work on intersectional fairness [16, 18]. In particular, in the legal scholarship, Intersectional Fairness ideas go back to the work of Crenshaw [8], but remain a subject of lively debate [7] to the present day.\nIn the algorithmic fairness literature, the sample complexity of certain fairness estimates (statistical parity, false positive fairness) was considered in [27]. The notion of distance, which we consider here, is conceptually different, as it concerns data quality (see Section 1) rather than a fairness test of a particular given classifier. Moreover, the algorithms of [27] were developed with linear subgroups in mind, and consequently evaluated on linear subgroups [28]. In particular, these algorithms require certain specific heuristics (oracles), that are mainly developed for the linear case. We note that such linear subgroups are considerably less interpretable and less suitable for real-world applications compared to the"}, {"title": "2.3 Learning DNF", "content": "A (protected) subgroup is naturally defined as a conjunction of a few feature-value pairs, e.g., sex = \"F\" AND race = \"black\". A disjunction of multiple conjunctions (i.e., a union of subgroups) is a logical formula in Disjunctive Normal Form (DNF).\nThe study of learning DNF formulas is a fundamental part of theoretical computer science. Despite its polynomial sample complexity, finding a general algorithm with polynomial-time complexity has proven elusive in many settings. Since the breakthrough reduction to polynomial threshold functions of [29], which matched the 1968 lower bound of [36], [9] helped understand the complexity-theoretic barriers. See also [43] for an overview."}, {"title": "2.4 Mixed-integer optimization", "content": "Mixed-integer optimization (MIO, [55]) is a powerful framework for modeling and solving mathematical optimization problems, where some decision variables take values from a discrete set while others are continuously valued. Despite MIO being a general framework for solving NP-hard problems, the MIO solvers are speeding up by approximately 22% every year, excluding hardware speedups [30]. We use the abbreviation MIO, though we consider only mixed-integer linear formulations.\nMIO is used in machine learning, especially when one optimizes over discrete measures or decisions. This includes learning logical rules, including DNFs. Malioutov and Varshney [35] learn DNFs through a sequential generation of terms using an LP relaxation of an MIO formulation. Later, Wang and Rudin [51] used MIO to optimize full DNFs, and Su et al. [45] introduced a formulation with Hamming distance as an alternative objective function. A crucial improvement to the scalability of exact learning of DNFs was the BRCG [10] utilizing column generation to generate candidate terms. Recently, MIO was utilized to learn a DNF classifier with fairness constraints [32]. Importantly, we optimize only a single term (conjunction), representing the maximally discrepant subgroup, to evaluate bias. We do not perform predictions.\nAs an aside, note that [38] (see also [21]) use DNFs to compare two models. This is done by building two DNFs as proxy models and comparing them."}, {"title": "3 Maximum Subgroup Discrepancy", "content": "Let X C Rd be an input space with d features and let PC [d] be a subset of features that are protected attributes. Throughout the rest of the paper, we assume that the protected attributes are binary, which can always be achieved by quantization and one-hot encodings. For a vector x \u2208 Rd and p \u2208 P let Xp \u2208 {0,1} denote its p-th coordinate, and let Ip := 1 - Xp denote its logical negation. The set of functions L = {Xp, Ip}p\u2208p is called literals. A term is a conjunction of literals. That is, for a subset SCL, the term Xs is defined as a function Rd \u2192 {0,1} given by\n$$X_s(x) = \\prod_{s \\in S} s(x).$$\nThe subsets SCL are called subgroups, and with some abuse of notations we also refer to the corresponding parts of the data defined by S,\n$$X_s = \\{x \\in R^d | x_s(x) = 1\\},$$\nas subgroups. The set of all possible subgroups SC Lis denoted by Sp.\nFor a distribution \u03bc on Rd and a function f : Rd \u2192 R denote by \u00b5(f) the integral \u00b5(f) = f f(x)d\u00b5(x).\nWith this notation, the MSD diff distance on feature set P between two distributions, \u03bc,\u03bd, is defined as\n$$MSD_{\\text{diff}}(\\mu, \\nu; P) = \\sup_{S \\in S_P} |\\mu(x_S) - \\nu(x_S)|.$$\nIn words, as discussed earlier, two distributions are similar w.r.t MSD if all subgroups induced by P have similar weights in both measures.\nLet us now discuss the relation between MSD and the two standard distances the lo and l1, or equivalently, the TV distance. Define the base terms as\n$$B_P = \\{\\prod_{p \\in P} l_p | \\text{where } l_p = x_p \\text{ or } l_p = \\bar{x_p} \\} \\subset S_P.$$\nThat is, the terms in Bp correspond to all possible different values the projection of x onto features P might have. Equivalently, base terms correspond to subgroups where all protected attributes have a specified value (rather than the more general specification of only part of the values). Clearly, there are precisely 2|P| base terms.\nThe lo norm may then be defined as\n$$|| \\mu - \\nu ||_\\infty = \\sup_{S \\in B} |\\mu(x_S) - \\nu(x_S)|.$$\nThat is, we consider the maximal weight difference over the atoms in P.\nLet Fp, be the set of functions f : Rd \u2192 R that depend only on the coordinates in P, and are bounded by 1. Then, the total variation\u00b2 (TV), may be written as\n$$|| \\mu - \\nu ||_{TV} = \\frac{1}{2} \\sup_{f \\in F_P} |\\mu(f) - \\nu(f)|$$\n$$\\frac{1}{2} \\sum_{S \\in B} | \\mu(x_S) - \\nu(x_S)|,$$ where (8) and (9) are the dual and the standard definitions of the l\u2081 (and hence the TV) norms.\nBy comparing the definition (5) with (7) and (8), we see that\n$$|| \\mu - \\nu ||_\\infty \\le MSD_{\\text{diff}}(\\mu, \\nu; P) \\le || \\mu - \\nu ||_{TV},$$ where the second inequality follows since all terms in Sp are clearly functions bounded by 1.\nWe thus observe that MSDdiff (\u03bc, \u03bd; P) is a stronger distance than l\u221e, which allows us to consider subgroups with partially specified values. At the same time, it is not as strong as the total variation, which requires the sum over all base term differences to be small (rather than each of the differences being small individually). On the other hand, as discussed in Section 1, TV is, in fact, too strong to be practically computable due to its high sample complexity, while MSDdiff has a sample complexity linear in the number of attributes."}, {"title": "3.1 Sample Complexity", "content": "In this section, we prove Theorem 3.1, which allows us to quantify the error of estimating MSDdiff (\u03bc, \u03bd; P) from finite samples of these distributions.\nLet {xi}i<N\u2081 be N\u2081 independent samples from \u00b5, and {x{}i<N\u2082 be N2 independent samples from v. Define the corresponding empirical distributions, \u00fb, \u00fb by\n$$\\hat{\\mu} = \\frac{1}{N_1} \\sum_{i} \\delta_{x_i} \\text{ and } \\hat{\\nu} = \\frac{1}{N_2} \\sum_{i} \\delta_{x'_i}$$\nwhere dx is an atomic distribution at point x, with weight 1.\nTheorem 3.1. Fix \u03b4 > 0 and set N = min(N1, N2). Then with probability at least 1 - 2\u03b4 over the samples,\n$$MSD_{\\text{diff}}(\\mu, \\nu; P) \\le MSD_{\\text{diff}}(\\hat{\\mu}, \\hat{\\nu}; P) + 4 \\sqrt{\\frac{2^{|P|}+\\log \\frac{1}{\\delta}}{2N}}$$\nIn words, if the number of samples N is of the order of the number of protected attributes, |P|, or larger, we can estimate MSDdiff (\u03bc, \u03bd; P) by computing the MSDdiff on the empirical distributions, MSD diff (\u00fb, \u00fb; P)."}, {"title": "3.2 MSD Estimation As Classification", "content": "To estimate the MSD for empirical distributions \u00fb, \u00fb, it is more convenient to rephrase the maximization problem (5) as minimization of a classification loss, where the classifiers are taken from the family of terms, Sp. As we detail in Section 4, this point of view allows us to incorporate various useful ideas from the field of DNF classification into our MIO-based minimization algorithm.\nTo recast the problem as classification, denote by {i}i<N\u2081 and the {x;}j<N2 datasets sampled from u and v respectively. Assign a label y = 1 to all samples from \u00b5 and y = 0 to all samples from v. Let c(a, b) be the binary classification loss (0-1 loss), c(a, b) = 0 if a = b and 1 otherwise.\nThen, for a binary classifier f of a label y as above, the binary classification loss would be\n$$E_{x \\sim \\mu} c(f(x), 1) + E_{x \\sim \\nu} c(f(x), 0)$$\n$$= E_{x \\sim \\mu} (1 - f(x)) + E_{x \\sim \\nu} f(x)$$\n$$= 1 - E_{x \\sim \\mu} f(x) + E_{x \\sim \\nu} f(x)$$\n$$= 1 - (\\mu(f) - \\nu(f))$$"}, {"title": "4 Practical Estimation", "content": "In Section 3, we have described a measure with practically feasible sample complexity requirements. However, there is still a question of how to estimate it in practice. The challenge lies in the cardinality of Sp, which is exponential in the number of protected attributes. See Figure 1 for an illustration.\nThus, there is a question of how to estimate the full MSDdiff distance effectively. To this end, recall that subgroups S\u2208 Sp constitute terms, i.e., conjunctions of literals formed on a subset of features P. And we also showed that one can formulate the MSD diff distance maximization using two classification problems.\nBy using the methods of [51, 1, 53], we could learn a DNF that distinguishes between \u00b5 and v. Note that by definition, every term in such a DNF would"}, {"title": "4.1 MIO formulation", "content": "To find the globally optimal subgroup (i.e., conjunction or 1-term DNF), one can utilize Mixed-Integer Optimization (MIO). Let D+ and D be sets of indices of samples from \u00b5 and v, respectively, and let D = D+ UD_ be the set of indices of all samples. Our formulation is related to the 0-1 error DNF formulation of [45], but we search for a single term only and use a different objective, which leads to us also having to use further constraints.\nSince we maximize the non-linear absolute difference, we must reformulate it using an auxiliary variable. We define variable o as the absolute value objective we maximize. We bound it from above by the two potential values of the absolute value, such that it takes the higher of them. The entire formulation is\n$$\\max o$$\ns.t. o < \\frac{1}{|D^+|} \\sum_{i \\in D^+} y_i - \\frac{1}{|D^-|} \\sum_{i \\in D^-} y_i + 2b\n0 < \\frac{1}{|D^-|} \\sum_{i \\in D^-} y_i - \\frac{1}{|D^+|} \\sum_{i \\in D^+} y_i + 2(1-b)\nyi \\le 1 - (u_j - x_{i,j} u_j) i \\in D, j \\in P\nyi \\ge 1 - \\sum_{j \\in P} (u_j - x_{i,j} u_j) i \\in D\n\\sum_{i \\in D} \\hat{y}_i \\ge N_{min}\n0 \\le \\hat{y}_i \\le 1 i \\in D\nu_j, b \\in \\{0,1\\} j \\in P,\nwhere uj is equal to 1 if and only if feature j is present in the conjunction and \u0177 \u2208 {0,1} are variables representing whether each sample belongs to the subgroup (i.e., is classified as 1). Indeed, \u0177i takes only binary values, due to the constraints on its value, together with the fact that xi,j and uj are binary."}, {"title": "5 Experiments", "content": "Our main goal is to measure the distance between distributions, focusing on protected subgroups. We thus wish to compare MSDdiff to other measures of distances on distributions, as per Table 1. Since the data we consider is binary or categorical, we utilize the overlap kernel function k(Xi1, Xi\u2082) = \u03a3=1 [Xi1,j = Xiz,j] /d for the MMD evaluation. We use our own implementation of TV and MMD, and for W\u2081 and W2, we utilize POT [15], an open-source Python library for the computation of optimal transport.\nAdditionally, we would like to compare the evaluation of MSDdiff utilizing the proposed MIO formulation to other DNF optimization algorithms. We choose the classical algorithm Ripper [6] and BRCG [10] as a modern DNF learner. Since there is no public implementation of the original BRCG, we take the non-MIO re-implementation of BRCG, called BRCG-light [1] and the Ripper algorithm available in the AIX360 library [1]. We modify both implementations to return a single-term DNF, to ensure comparability to our method. As addressed earlier, we run each method two times, once to minimize error and the second time to maximize it (minimize the error with flipped labels). In case one of the distributions has more samples, we subsample it to have an equal number of samples for each distribution. This makes minimizing the 0-1 loss equivalent to finding the MSDdiff.\nTo solve the proposed MIO formulation (15), we model it using the Pyomo library [3] allowing easy use of a variety of solvers. We use the Gurobi solver [20]. We set the Nmin parameter to 10, allowing only subgroups with at least ten samples, although one could also use (13) and a fixed probability level \u03b4."}, {"title": "5.1 Datasests", "content": "To showcase the intended use of our method, we use the US Census data via the Folktables library [12]. Specifically, we use two families of five datasets.\nThe first family consists of the five pre-defined prediction tasks: ACSIncome, ACSPublic Coverage, ACSMobility, ACSEmployment, and ACSTravelTime. We compare the distributions of samples with y\u2081 = 1 and y\u2081 = 0 on the data of residents of California. For example, in ACSIncome, this means comparing the distributions between people earning more than $50,000 a year and people earning less, similar to the well-known Adult dataset. For more details see the original paper [12]. To evaluate the distance comparably between methods, we only consider protected attributes, the list of which is in Appendix A.1.1.\nThe second family of datasets aims at comparing distributions of population between different US states. We take attributes that could be considered protected (e.g., sex, race, disability, age) and take data for five distinct pairs of states. The state pairs are as follows: Hawaii v. Maine, California v. Wyoming, Mississippi v. New Hampshire, Maryland v. Mississippi, and Louisiana v. Utah. The state selection aims to compare demographically, socioeconomically, and culturally different states, allowing for a comparison of diverse populations across the United States. The complete list of used attributes is in Appendix A.1.1.\nAll used data is 1-year horizon data from the survey year 2018. We work with binary data, where we binarize continuous values to 10 equally spaced bins and semantically reduce the cardinalities of certain subgroups. The sample sizes span from around 10,000 to 380,000 and the number of protected features spans from 3 (with around 1,200 possible subgroups) to 14 (with more than 5.109 subgroups). Details are in Appendix A.1."}, {"title": "5.2 Setup", "content": "We aim to see how the number of samples influences the value of the distance. Ideally, one would need only a few samples to compute the true distance of the distributions. For that, each dataset is subsampled to form 5 (smaller) datasets, with sizes forming a geometric sequence from 1000 samples to the maximum number of samples for the given dataset. Each method is evaluated on each dataset. We ran each experiment configuration five times with different random seeds.\nEach experiment was run on a cluster. Each node had 32 GB of RAM and either AMD EPYC 7543 or Intel Xeon Scalable Gold 6146 processor. Each distance computation had a time limit of 10 minutes. All code is openly available on GitHub\u00b3."}, {"title": "5.3 Results", "content": "Since most evaluated methods have different notions of distance, it is difficult to compare them directly. Thus, we present relative distances w.r.t. the best"}, {"title": "6 Conclusion", "content": "We pointed out the difficulties of evaluating the distance of probability distributions by current methods, especially in the context of evaluating bias for intersectional subgroups. We proposed a family of distance measures MSD\u25b3 as the maximal distance A over all subgroups.\nWe further defined a specific MSDdiff and proved its linear sample complexity, in contrast to the exponential sample complexities of existing methods. We validated this on datasets of real US Census data with varying numbers of protected attributes and sample sizes. Only MMD showed empirically comparable performance to our MIO-based MSDdiff, but it does not provide as good of an interpretation, that could be useful in bias evaluation.\nThe MIO formulation also outperforms more general DNF learners, which also lack guarantees due to not being global optimizers. In addition, the global optimality of MIO does not necessarily come at a disadvantage as is otherwise common. That is because the linear sample complexity means that we do not need exponentially many samples, thus the formulation remains smaller and solves faster.\nFuture work As we have shown, evaluating bias in subgroups is an intrinsically difficult task. There are many challenges that require more work, one of which is the training of ML models under intersectional fairness constraints. We believe MSDdiff could be a good measure for learning with fairness guarantees. Additionally, there may be other A functions that could be efficiently used for MSD measures, e.g., the SPSF [27]."}, {"title": "Impact Statement", "content": "By its nature, the positive potential societal impact of Algorithmic Fairness is well understood, and consequently, the field has been intensively developed in the past decade. The choice of a bias measure has a substantial impact on the evaluation of any AI system. Our bias measure facilitates the study of intersectional fairness of [8], which would be intractable with many other means of evaluation of bias."}, {"title": "A Appendix", "content": "We utilize 2 types of datasets, both constructed from the US Census data American Community Survey (ACS). The first 5 (with names starting with ACS) represent tasks constructed by the creators of the folktables library [12]. We simply access it, taking 1-year survey data of Californians for the year 2018.\nThe datasets of the other type are constructed by us using the folktables API from the same data source. We select five pairs of distinctive US states and compare their data distributions. We simply take a random half of the available data for each state and combine it to form our dataset. Each dataset has the same attributes, listed in Table 3.\nThe dataset statistics are in Table 2. Note the Hawaii X Maine dataset for an illustration of the difficulty of assessing the subgroup bias. It is quite difficult to evaluate the bias of more than 400,000x more groups than there are samples."}, {"title": "A.1 Datasets description", "content": "We utilize 2 types of datasets, both constructed from the US Census data American Community Survey (ACS). The first 5 (with names starting with ACS) represent tasks constructed by the creators of the folktables library [12]. We simply access it, taking 1-year survey data of Californians for the year 2018.\nThe datasets of the other type are constructed by us using the folktables API from the same data source. We select five pairs of distinctive US states and compare their data distributions. We simply take a random half of the available data for each state and combine it to form our dataset. Each dataset has the same attributes, listed in Table 3.\nThe dataset statistics are in Table 2. Note the Hawaii X Maine dataset for an illustration of the difficulty of assessing the subgroup bias. It is quite difficult to evaluate the bias of more than 400,000x more groups than there are samples."}, {"title": "A.1.1 Protected attributes", "content": "Below, we list the enumerate the attributes for each dataset. For more discussion and statistics of the attributes, see Table 3."}, {"title": "A.2 More results", "content": ""}, {"title": "A.2.1 Original distances", "content": "In addition to the results in the main body of the paper, we present the results on the true distances, before \"normalization\" by the best-estimated distance, in Figure 3. It is indeed difficult to compare the methods together, one might just comment on the distances separately. For example, note the range of estimates of TV, oftentimes, the value changes by 0.3 from the first to last estimate, which is essentially a third of the range of feasible values TV can take.\nAlso, one can notice that Wasserstein metrics and the MMD are not well-suitable for the evaluation of intersectional bias by comparing results on Hawaii"}, {"title": "A.2.2 RSE", "content": "Finally, we present the results by evaluating and plotting the Relative Standard Error (RSE) in Figure 4. It shows the stability of the estimate over various seeds. The methods evaluating MSDdiff seem to generally struggle more, which might be a feature of the measure, taking the supremum rather than some mean value. This could likely make the measure more \"volatile\" using different seeds. Note, however, that MMD has comparable RSE, despite being a measure computed as a mean value."}]}