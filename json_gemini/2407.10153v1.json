{"title": "Look Within, Why LLMs Hallucinate: A Causal Perspective", "authors": ["He Li", "Haoang Chi", "Mingyu Liu", "Wengjing Yang"], "abstract": "The emergence of large language models (LLMs) is a mile-\nstone in generative artificial intelligence, achieving significant success in\ntext comprehension and generation tasks. Despite the tremendous suc-\ncess of LLMs in many downstream tasks, they suffer from severe hallu-\ncination problems, posing significant challenges to the practical applica-\ntions of LLMs. Most of the works about LLMs' hallucinations focus on\ndata quality. Self-attention is a core module in transformer-based LLMs,\nwhile its potential relationship with LLMs' hallucination has been hardly\ninvestigated. To fill this gap, we study this problem from a causal per-\nspective. We propose a method to intervene in LLMs' self-attention lay-\ners and maintain their structures and sizes intact. Specifically, we disable\ndifferent self-attention layers in several popular open-source LLMs and\nthen compare their degrees of hallucination with the original ones. We\nevaluate the intervened LLMs on hallucination assessment benchmarks\nand conclude that disabling some specific self-attention layers in the front\nor tail of the LLMs can alleviate hallucination issues. The study paves a\nnew way for understanding and mitigating LLMs' hallucinations.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) marks a milestone in generative\nartificial intelligence and natural language processing (NLP) [13,36]. LLMs are\ntypically pre-trained on extensive unlabeled corpora and fine-tuned on specific\ntask datasets. LLMs, such as LLAMA [29], LLaMA-2 [30], Gemma [28], Mistral\n[15], GPT-3 [4], PaLM [6], BLOOM [34], GPT-4 [25], have achieved tremendous\nsuccess in tasks such as text generation, text comprehension [5], logical reason-\ning [11,12], and have demonstrated excellent in-context learning abilities [7,21].\nThis powerful capability gives LLMs tremendous practical value in real-world\nscenarios, such as assisting in healthcare and coding.\nAlthough LLMs have achieved remarkable success in many downstream tasks,\nthey exhibit the phenomenon of hallucination, generating contents that do not\nalign with user requirements or conflict with objective facts [13,36]. Fact-conflicting"}, {"title": "2.1 Hallucination of LLMs", "content": "In psychology, hallucination refers to a sense of perception that individuals ex-\nperience in response to appropriate stimuli from the external world, creating a\nsensation that seems real but may be deceptive [2,10,22]. In the field of LLMs,\nsimilar to the hallucination in human psychology, the hallucination of LLMs\nrefers to generating deceptive or meaningless content by these models [14]. Sev-\neral studies [13,36,20,23,14] indicate that LLMs suffer from hallucination issues.\nThere are diverse hallucinations among LLMs, in this work, we consider fact-\nconflicting hallucinations. Fact-conflicting hallucinations refer to conflicts be-\ntween the content generated by LLMs and objective knowledge from the real\nworld, which are difficult for humans to perceive and may pose harm [36]."}, {"title": "2.2 Self-Attention Mechanism of LLMS", "content": "The self-attention mechanism discussed in this section is the Multi-Head At-\ntention module within the Transformer architecture [32]. The core of the self-\nattention mechanism involves weighting and combining input vectors according\nto different self-attention weights. By performing a linear transformation on the\ninput to obtain three matrices, Q, K, and V, let dk represent the second dimen-\nsion of matrix K, the calculation formula is as follows:\nAttention(Q, K, V) = softmax(\\frac{Q K^{T}}{\\sqrt{d_{k}}})V, (1)\nwhere $Q K^{T}$ denotes the attention weights.\nThe multi-head attention mechanism processes the input through multiple\nattention operations, concatenates each attention operation's results, and finally\nperforms a linear transformation to obtain the output. The calculation formula\nis as follows:\nhead\u2081 = Attention(Q$W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}$), (2)\nMultiHead(Q, K, V) = Concat(head1,...,headn)$W^{O}$, (3)\nwhere $W^{Q}, W^{K}, W^{V}$ and $W^{O}$ are the matrices of linear transformations.\nThe sefl-attention module inside LLMs is mainly the stack of attention layers,\neach layer contains a multi-head attention module. A simplified schema of the\nself-attention mechanism inside LLMs is shown in Figure 3."}, {"title": "2.3 Causal Methods", "content": "Structural causal model (SCM) [26] is one causal method used in this paper.\nFormally, let X = {$X_{1}$, $X_{2}$, ...,Xn} denote the random variables, G denote a\ncausal graph among X, and F represent the collection of structure equations.\nThen Structural Causal Model (SCM) SCM := (G, F) consists of a causal graph\nrepresenting the causal relationships between random variables and a set of struc-\nture equations determining the values of each random variable. Specifically, the"}, {"title": "2.4 Front-Door Criterion", "content": "The front-door criterion is an important concept in causal inference, which pro-\nvides a framework for identifying causal effects in certain scenarios [26]. In the\nfront-door criterion, we seek a set of mediating variables M such that X can only\naffect Y through M, and X and Y are independent given M. In other words, M\nfully explains the association between X and Y. This set of mediating variables\nM helps identify the causal effect of X on Y, even in the presence of unobserved\nconfounders [26].\nTo explain the front-door criterion intuitively, we provide an example: sup-\npose we want to study the relationship between coffee consumption (X) and\nheart disease (Y). We find a mediating variable: caffeine intake (M). Drinking\ncoffee can only affect heart disease through caffeine intake, and conditioned on\ncaffeine intake, whether one drinks coffee or not is unrelated to heart disease.\nThus, caffeine intake (M) fully explains the association between coffee consump-\ntion X and heart disease Y. In this case, we can use the front-door criterion to\nmodel and estimate the causal effect of drinking coffee on heart disease."}, {"title": "3 Methodology", "content": "In this section, we introduce our method for analyzing the effects of the self-\nattention mechanism on LLMs' hallucinations. The proposed method encom-\npasses the following steps: First, we establish a structural causal model (SCM)\nto depict the text generation process of LLMs with hallucinated contents. Sec-\nond, motivated by the front-door criterion, we identify the self-attention layers\nthat contain hallucinated contents that can be viewed as mediating variables. In\nthe end, we employ front-door adjustment to calculate the effect of self-attention\nlayers on hallucinations. Specifically, we develop a novel method to intervene in\nthe self-attention layers without affecting the size and architecture of LLMs."}, {"title": "3.1 Front-Door Adjustment of Hallucinations", "content": "SCM of the Text Generation Process with Hallucinated Contents In\nthis section, we build a SCM of the text generation process with hallucinated\ncontents. Figure 2 illustrates the text generation procedure of LLMs, which can\nbe divided into three stages. In the first stage, the input text X is inputted\ninto an LLM, which is transformed into latent factors $Z_{1}$,..., $Z_{n}$ inside the\nmodel. The training data that contains subjectively biased contents will produce\nthe hallucinative contents in latent factors $Z_{1}$,..., $Z_{n}$ [8]. Subsequently, limited\nby the capabilities of LLMs, these latent factors constitute truthful contents T\nand hallucinated contents H. In the final phase, the truthful contents T and\nhallucinated contents H are transformed into readable text Y as the output."}, {"title": "A Causal Intuition for Removing Hallucinations", "content": "In this section, we\ndemonstrate how the idea of the front-door criterion motivates our method.\nAs shown in Figure 2, according to the front-door criterion introduced in section\n2.4, the hallucinated contents H can be viewed as a mediating variable between\n$Z_{1}$,..., $Z_{n}$ and the generated texts Y. Specifically, the latent factors $Z_{1}$,..., $Z_{n}$\ncan only affect the hallucinated contents in Y through H. Therefore, the hallu-\ncinated contents H is the mediating variable that we can employ to analyze the\ncausal effect of the latent factors $Z_{1}$,..., $Z_{n}$ on the hallucinated contents in Y.\nIntervening on the variable H can partially mitigate LLM hallucinations."}, {"title": "3.2 Method for Disabling the Self-Attention Layers", "content": "We consider the hallucinated contents H to be the self-attention layers that\ncontain hallucinated contents, and now we introduce the method to disable the\nself-attention layers. Specifically, we modify the attention output tensor of all\nself-attention heads in a self-attention layer to be zero tensors during the forward\nprocess, therefore the output of this self-attention layer is zero and will not\nfunction during the LLM's inference stage. The equations of the method are\nshown below:\nhead\u2081 = Attention(Q$W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}$) \u2190 0, (6)"}, {"title": "4 Related Work", "content": "In this section, we will review the related works on the LLMs' hallucinations and\nmodel editing."}, {"title": "4.1 Source of the Hallucinations of LLMS", "content": "[3] suggests that training LLMs by maximizing the conditional probability of\nthe output sequence given the input sequence does not allow the models to\nlearn semantic knowledge from the training corpus. Therefore, the conditional"}, {"title": "4.2 Detection of the Hallucinations of LLMs", "content": "Currently, the benchmarks for detecting hallucinations in LLMs are mainly\nquestion-answering datasets. TruthfulQA [20] is a benchmark designed for as-\nsessing the truthfulness of outputs from LLMs, which can be used to evaluate\nthe extent of hallucinations in these models. This benchmark is a question-\nanswering dataset that includes human-designed questions and corresponding\nreference correct and incorrect answers. Besides, [20] finds larger-scale models\nmore prone to generating false answers. HaluEval [17] is another benchmark for\ndetecting fact-conflicting hallucinations in LLMs. [17] utilizes LLMs to gener-\nate the questions and corresponding answers and employs human annotation\nto mark the hallucination answers. The LLMs are required to discriminate the\nhallucination response of a question from the correct one. This paper evaluates\nLLMs on these two benchmarks to assess their hallucination issue."}, {"title": "4.3 Model Editing", "content": "Model editing refers to editing a base model to correct its behavior in a certain\nfield while maintaining its performance in other areas [35,24]. [19] study the\ngender bias in LLMs, they perform causal tracing to find out which part of\nthe model contains the gender-biased representation. Their results show that\ncertain mid-upper feed-forward layers tend to store gender-biased information.\n[9] argue that not all layers are essential during LLMs' inference stage. They\nanalyze the activated layers during model inference among different downstream\ntasks and propose a criterion to decide which part of the model is important\nand when to terminate the inference in advance. [16] conduct a qualitative and\nquantitative analysis of the information encoded in the self-attention heads of\nBERT. They find that some attention heads encode redundant information, and\nafter disabling them, BERT shows performance improvement on certain GLUE\n[33] tasks. Compared with the above works, the difference in our work is that\nwe focus on the hallucination issue of LLMs."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments on multiple open-source LLMs across\ntwo hallucination detection benchmarks."}, {"title": "5.1 Open-Source LLMS", "content": "The chosen open-source LLMs are LLaMA2-7B-chat [30], Gemma series [28],\nand mistral-7B v0.1 [15], and the details of these models are shown in Table 1.\nLLaMA2-7B-chat and Gemma-instruct are the supervised fine-tuned versions of\nthe LLaMA2-7B and Gemma series. The parameters of these models range from\ntwo billion to seven billion, and number of layers varies from 18 to 32."}, {"title": "5.2 Benchmarks", "content": "We employ the TruthfulQA dataset [20] and the Halueval dataset [17] as bench-\nmarks to evaluate the hallucination issues."}, {"title": "TruthfulQA Dataset", "content": "TruthfulQA [20] is a question-answering dataset that\nincludes 817 questions designed manually covering 38 categories, along with\ncorresponding reference correct and incorrect answers. TruthfulQA is widely\nused, and some open-source LLMs such as Gemma [28], and LLaMA2 [30] have\nadopted it upon release, giving it authority in the field."}, {"title": "Halueval Dataset", "content": "HaluEval dataset [17] is another benchmark to evaluate\nthe hallucination issue of LLMs. Unlike the TruthfulQA dataset, HaluEval is a\nlarge dataset that consists of 35000 samples generated by LLMs with human\nannotation. [17] utilize ChatGPT to generate questions with the correspond-\ning answers, and then they ask humans to annotate the hallucination answers\nwithin the generated content. The HaluEval dataset contains questions with cor-\nresponding right answers and hallucinative answers. We find this benchmark is\nsimilar to the TruthfulQA dataset. Therefore, we ask LLMs to answer questions\nin HaluEval and judge the correctness of the answers according to the right\nanswers and hallucinative answers."}, {"title": "5.3 Automated Evaluation", "content": "We evaluate the original LLMs and the intervened LLMs on the above bench-\nmarks. Due to the enormous number of questions in these benchmarks, manually\nevaluating the correctness of LLMs' output would be costly. Therefore, we con-\nsider using other SOTA LLMs for automatic answer evaluation [20]."}, {"title": "5.4 Experimental Results", "content": "Notations Now we introduce the notations we used in the experiments. We\nuse the symbol zo to represent the original, unaltered LLMs, and symbol zi to\nrepresent the LLMs where we disable the i-th self-attention layer.\nResults on TruthfulQA Dataset We evaluate open-source LLMs on the\nTruthfulQA dataset, and the intervention setting is shown in Table 2. To mitigate\nrandomness's influence, we evaluate LLMs on the TruthfulQA dataset five times\nin every intervention setting and take the average accuracy as the final accuracy.\nThe results are shown in Figure 5.\nResults on HaluEval Dataset Due to the enormous number of questions in\nthe HaluEval dataset, it is hard for us to evaluate LLMs on all the questions.\nTherefore, we randomly select 500 questions from it for evaluation. To reduce"}, {"title": "6 Results Analysis", "content": "Attention Layers inside LLMs affect hallucinations differently. Figures\n5 and 6 illustrate how disabling different layers impacts LLM performance (ACC)\non two benchmarks. For instance, Figure 6d shows that disabling the 13th layer\n(213) increases ACC compared to the original LLM (zo). This suggests that\ndifferent attention layers have varying effects on hallucinations.\nFront or tail layers are most prone to convey hallucinations. Figure\n5b, 5c, 5a, 6d, 6c, and 6b illustrate that the hallucinations are reduced when\nwe disable certain front or tail layers. To be specific, as seen in Figure 5a, the\nACC of Gemma-7B-instruct increases when we disable the 3-th layer (23) and\n23-th layer (223). Although disabling attention layers does not improve the ACC\nof the Mistral model, disabling the front or tail attention layers still largely\npreserved the model's performance on TruthfulQA, as shown in Figure 5d. This\ndemonstrates that front or tail layers may represent hallucinative content.\nMiddle layers may contain factual knowledge. Figure 5b, 5a, 5d, 6d,\n6c and 6b demonstrate that disabling middle attention layers largely undermine\nthe LLMs' performances on these benchmarks. Specifically, as shown in Figure"}, {"title": "7 Conclusions", "content": "In this paper, we study the effects of the self-attention mechanism of LLMs\non hallucinations through the lens of causality. We propose a novel method to\ndisable the self-attention layers inside LLMs while maintaining LLMs' size and\narchitecture. We evaluate multiple open-source LLMs on hallucination detection\nbenchmarks. Our results show that the front or tail layers are most prone to\nconvey hallucinations, and the middle layers may contain factual knowledge."}]}