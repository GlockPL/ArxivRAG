{"title": "Look Within, Why LLMs Hallucinate: A Causal Perspective", "authors": ["He Li", "Haoang Chi", "Mingyu Liu", "Wengjing Yang"], "abstract": "The emergence of large language models (LLMs) is a milestone in generative artificial intelligence, achieving significant success in text comprehension and generation tasks. Despite the tremendous success of LLMs in many downstream tasks, they suffer from severe hallucination problems, posing significant challenges to the practical applications of LLMs. Most of the works about LLMs' hallucinations focus on data quality. Self-attention is a core module in transformer-based LLMs, while its potential relationship with LLMs' hallucination has been hardly investigated. To fill this gap, we study this problem from a causal perspective. We propose a method to intervene in LLMs' self-attention layers and maintain their structures and sizes intact. Specifically, we disable different self-attention layers in several popular open-source LLMs and then compare their degrees of hallucination with the original ones. We evaluate the intervened LLMs on hallucination assessment benchmarks and conclude that disabling some specific self-attention layers in the front or tail of the LLMs can alleviate hallucination issues. The study paves a new way for understanding and mitigating LLMs' hallucinations.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) marks a milestone in generative artificial intelligence and natural language processing (NLP) [13,36]. LLMs are typically pre-trained on extensive unlabeled corpora and fine-tuned on specific task datasets. LLMs, such as LLAMA [29], LLaMA-2 [30], Gemma [28], Mistral [15], GPT-3 [4], PaLM [6], BLOOM [34], GPT-4 [25], have achieved tremendous success in tasks such as text generation, text comprehension [5], logical reasoning [11,12], and have demonstrated excellent in-context learning abilities [7,21]. This powerful capability gives LLMs tremendous practical value in real-world scenarios, such as assisting in healthcare and coding.\nAlthough LLMs have achieved remarkable success in many downstream tasks, they exhibit the phenomenon of hallucination, generating contents that do not align with user requirements or conflict with objective facts [13,36]. Fact-conflicting"}, {"title": "2 Background", "content": "This paper investigates the effect of the self-attention mechanism on hallucinations through the lens of causality. This section will provide a brief introduction to the main concepts involved in this paper."}, {"title": "2.1 Hallucination of LLMs", "content": "In psychology, hallucination refers to a sense of perception that individuals experience in response to appropriate stimuli from the external world, creating a sensation that seems real but may be deceptive [2,10,22]. In the field of LLMs, similar to the hallucination in human psychology, the hallucination of LLMs refers to generating deceptive or meaningless content by these models [14]. Several studies [13,36,20,23,14] indicate that LLMs suffer from hallucination issues. There are diverse hallucinations among LLMs, in this work, we consider fact-conflicting hallucinations. Fact-conflicting hallucinations refer to conflicts between the content generated by LLMs and objective knowledge from the real world, which are difficult for humans to perceive and may pose harm [36]."}, {"title": "2.2 Self-Attention Mechanism of LLMS", "content": "The self-attention mechanism discussed in this section is the Multi-Head Attention module within the Transformer architecture [32]. The core of the self-attention mechanism involves weighting and combining input vectors according to different self-attention weights. By performing a linear transformation on the input to obtain three matrices, Q, K, and V, let dk represent the second dimension of matrix K, the calculation formula is as follows:\n$$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V,$$ (1)\nwhere $$QK^T$$ denotes the attention weights.\nThe multi-head attention mechanism processes the input through multiple attention operations, concatenates each attention operation's results, and finally performs a linear transformation to obtain the output. The calculation formula is as follows:\n$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V),$$ (2)\n$$MultiHead(Q, K, V) = Concat(head_1,...,head_n)W^O,$$ (3)\nwhere $$W_i^Q$$, $$W_i^K$$, $$W_i^V$$ and $$W^O$$ are the matrices of linear transformations.\nThe sefl-attention module inside LLMs is mainly the stack of attention layers, each layer contains a multi-head attention module."}, {"title": "2.3 Causal Methods", "content": "Structural causal model (SCM) [26] is one causal method used in this paper. Formally, let $$X = \\{X_1, X_2, ...,X_n\\}$$ denote the random variables, G denote a causal graph among X, and F represent the collection of structure equations. Then Structural Causal Model (SCM) SCM := (G, F) consists of a causal graph representing the causal relationships between random variables and a set of structure equations determining the values of each random variable. Specifically, the"}, {"title": "2.4 Front-Door Criterion", "content": "The front-door criterion is an important concept in causal inference, which provides a framework for identifying causal effects in certain scenarios [26]. In the front-door criterion, we seek a set of mediating variables M such that X can only affect Y through M, and X and Y are independent given M. In other words, M fully explains the association between X and Y. This set of mediating variables M helps identify the causal effect of X on Y, even in the presence of unobserved confounders [26].\nTo explain the front-door criterion intuitively, we provide an example: suppose we want to study the relationship between coffee consumption (X) and heart disease (Y). We find a mediating variable: caffeine intake (M). Drinking coffee can only affect heart disease through caffeine intake, and conditioned on caffeine intake, whether one drinks coffee or not is unrelated to heart disease. Thus, caffeine intake (M) fully explains the association between coffee consumption X and heart disease Y. In this case, we can use the front-door criterion to model and estimate the causal effect of drinking coffee on heart disease."}, {"title": "3 Methodology", "content": "In this section, we introduce our method for analyzing the effects of the self-attention mechanism on LLMs' hallucinations. The proposed method encompasses the following steps: First, we establish a structural causal model (SCM) to depict the text generation process of LLMs with hallucinated contents. Second, motivated by the front-door criterion, we identify the self-attention layers that contain hallucinated contents that can be viewed as mediating variables. In the end, we employ front-door adjustment to calculate the effect of self-attention layers on hallucinations. Specifically, we develop a novel method to intervene in the self-attention layers without affecting the size and architecture of LLMs."}, {"title": "3.1 Front-Door Adjustment of Hallucinations", "content": "SCM of the Text Generation Process with Hallucinated Contents In this section, we build a SCM of the text generation process with hallucinated contents. Figure 2 illustrates the text generation procedure of LLMs, which can be divided into three stages. In the first stage, the input text X is inputted into an LLM, which is transformed into latent factors $$Z_1,..., Z_n$$ inside the model. The training data that contains subjectively biased contents will produce the hallucinative contents in latent factors $$Z_1,..., Z_n$$ [8]. Subsequently, limited by the capabilities of LLMs, these latent factors constitute truthful contents T and hallucinated contents H. In the final phase, the truthful contents T and hallucinated contents H are transformed into readable text Y as the output.\nA Causal Intuition for Removing Hallucinations In this section, we demonstrate how the idea of the front-door criterion motivates our method. As shown in Figure 2, according to the front-door criterion introduced in section 2.4, the hallucinated contents H can be viewed as a mediating variable between $$Z_1,..., Z_n$$ and the generated texts Y. Specifically, the latent factors $$Z_1,..., Z_n$$ can only affect the hallucinated contents in Y through H. Therefore, the hallucinated contents H is the mediating variable that we can employ to analyze the causal effect of the latent factors $$Z_1,..., Z_n$$ on the hallucinated contents in Y. Intervening on the variable H can partially mitigate LLM hallucinations."}, {"title": "3.2 Method for Disabling the Self-Attention Layers", "content": "We consider the hallucinated contents H to be the self-attention layers that contain hallucinated contents, and now we introduce the method to disable the self-attention layers. Specifically, we modify the attention output tensor of all self-attention heads in a self-attention layer to be zero tensors during the forward process, therefore the output of this self-attention layer is zero and will not function during the LLM's inference stage. The equations of the method are shown below:\n$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) \\leftarrow 0,$$ (6)\n$$MultiHead(Q, K, V) = Concat(head_1, ..., head_n)W^O \\leftarrow 0.$$ (7)\nIn Eq. (7), n is the number of self-attention heads within a self-attention layer. Since the timing of modification is during the forward pass, the size and architecture of LLMs remain intact. Figure 3 illustrates our method.\nAlthough we can disable any self-attention heads in LLMs using this method, we choose to disable the self-attention layer rather than a single attention head because of the enormous amount of self-attention heads in LLMs."}, {"title": "4 Related Work", "content": "In this section, we will review the related works on the LLMs' hallucinations and model editing."}, {"title": "4.1 Source of the Hallucinations of LLMS", "content": "[3] suggests that training LLMs by maximizing the conditional probability of the output sequence given the input sequence does not allow the models to learn semantic knowledge from the training corpus. Therefore, the conditional"}, {"title": "4.2 Detection of the Hallucinations of LLMs", "content": "Currently, the benchmarks for detecting hallucinations in LLMs are mainly question-answering datasets. TruthfulQA [20] is a benchmark designed for assessing the truthfulness of outputs from LLMs, which can be used to evaluate the extent of hallucinations in these models. This benchmark is a question-answering dataset that includes human-designed questions and corresponding reference correct and incorrect answers. Besides, [20] finds larger-scale models more prone to generating false answers. HaluEval [17] is another benchmark for detecting fact-conflicting hallucinations in LLMs. [17] utilizes LLMs to generate the questions and corresponding answers and employs human annotation to mark the hallucination answers. The LLMs are required to discriminate the hallucination response of a question from the correct one. This paper evaluates LLMs on these two benchmarks to assess their hallucination issue."}, {"title": "4.3 Model Editing", "content": "Model editing refers to editing a base model to correct its behavior in a certain field while maintaining its performance in other areas [35,24]. [19] study the gender bias in LLMs, they perform causal tracing to find out which part of the model contains the gender-biased representation. Their results show that certain mid-upper feed-forward layers tend to store gender-biased information. [9] argue that not all layers are essential during LLMs' inference stage. They analyze the activated layers during model inference among different downstream tasks and propose a criterion to decide which part of the model is important and when to terminate the inference in advance. [16] conduct a qualitative and quantitative analysis of the information encoded in the self-attention heads of BERT. They find that some attention heads encode redundant information, and after disabling them, BERT shows performance improvement on certain GLUE [33] tasks. Compared with the above works, the difference in our work is that we focus on the hallucination issue of LLMs."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments on multiple open-source LLMs across two hallucination detection benchmarks."}, {"title": "5.1 Open-Source LLMS", "content": "The chosen open-source LLMs are LLaMA2-7B-chat [30], Gemma series [28], and mistral-7B v0.1 [15], and the details of these models are shown in Table 1. LLaMA2-7B-chat and Gemma-instruct are the supervised fine-tuned versions of the LLaMA2-7B and Gemma series. The parameters of these models range from two billion to seven billion, and number of layers varies from 18 to 32."}, {"title": "5.2 Benchmarks", "content": "We employ the TruthfulQA dataset [20] and the Halueval dataset [17] as benchmarks to evaluate the hallucination issues."}, {"title": "TruthfulQA Dataset", "content": "TruthfulQA [20] is a question-answering dataset that includes 817 questions designed manually covering 38 categories, along with corresponding reference correct and incorrect answers. TruthfulQA is widely used, and some open-source LLMs such as Gemma [28], and LLaMA2 [30] have adopted it upon release, giving it authority in the field."}, {"title": "Halueval Dataset", "content": "HaluEval dataset [17] is another benchmark to evaluate the hallucination issue of LLMs. Unlike the TruthfulQA dataset, HaluEval is a large dataset that consists of 35000 samples generated by LLMs with human annotation. [17] utilize ChatGPT to generate questions with the corresponding answers, and then they ask humans to annotate the hallucination answers within the generated content. The HaluEval dataset contains questions with corresponding right answers and hallucinative answers. We find this benchmark is similar to the TruthfulQA dataset. Therefore, we ask LLMs to answer questions in HaluEval and judge the correctness of the answers according to the right answers and hallucinative answers."}, {"title": "5.3 Automated Evaluation", "content": "We evaluate the original LLMs and the intervened LLMs on the above benchmarks. Due to the enormous number of questions in these benchmarks, manually evaluating the correctness of LLMs' output would be costly. Therefore, we consider using other SOTA LLMs for automatic answer evaluation [20]."}, {"title": "5.4 Experimental Results", "content": "Notations Now we introduce the notations we used in the experiments. We use the symbol zo to represent the original, unaltered LLMs, and symbol zi to represent the LLMs where we disable the i-th self-attention layer.\nResults on TruthfulQA Dataset We evaluate open-source LLMs on the TruthfulQA dataset, and the intervention setting is shown in Table 2. To mitigate randomness's influence, we evaluate LLMs on the TruthfulQA dataset five times in every intervention setting and take the average accuracy as the final accuracy. The results are shown in Figure 5.\nResults on HaluEval Dataset Due to the enormous number of questions in the HaluEval dataset, it is hard for us to evaluate LLMs on all the questions. Therefore, we randomly select 500 questions from it for evaluation. To reduce"}, {"title": "6 Results Analysis", "content": "Attention Layers inside LLMs affect hallucinations differently. Figures 5 and 6 illustrate how disabling different layers impacts LLM performance (ACC) on two benchmarks. For instance, Figure 6d shows that disabling the 13th layer (z13) increases ACC compared to the original LLM (z0). This suggests that different attention layers have varying effects on hallucinations.\nFront or tail layers are most prone to convey hallucinations. Figure 5b, 5c, 5a, 6d, 6c, and 6b illustrate that the hallucinations are reduced when we disable certain front or tail layers. To be specific, as seen in Figure 5a, the ACC of Gemma-7B-instruct increases when we disable the 3-th layer (z3) and 23-th layer (z23). Although disabling attention layers does not improve the ACC of the Mistral model, disabling the front or tail attention layers still largely preserved the model's performance on TruthfulQA, as shown in Figure 5d. This demonstrates that front or tail layers may represent hallucinative content.\nMiddle layers may contain factual knowledge. Figure 5b, 5a, 5d, 6d, 6c and 6b demonstrate that disabling middle attention layers largely undermine the LLMs' performances on these benchmarks. Specifically, as shown in Figure 5b, ACC of LLaMA 2-7B-Chat largely declines after we disable the 12-th layer (z12) and 16-layer (z16). This indicates that middle layers may contain factual knowledge, so performance drops when these layers don't function properly.\nThe answers change with attention layers disabled. Despite the LLMs' overall performances on these benchmarks, we also focus on the effect of disabling attention layers on answers to questions. As shown in Figure 7, LLAMA 2-7B-Chat produces the true answer after the 8-th layer (z8) is disabled and Gemma-2B-instruct generates the true answer when we disable the 8-th layer (z8)."}, {"title": "7 Conclusions", "content": "In this paper, we study the effects of the self-attention mechanism of LLMs on hallucinations through the lens of causality. We propose a novel method to disable the self-attention layers inside LLMs while maintaining LLMs' size and architecture. We evaluate multiple open-source LLMs on hallucination detection benchmarks. Our results show that the front or tail layers are most prone to convey hallucinations, and the middle layers may contain factual knowledge."}]}