{"title": "Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models", "authors": ["Mazda Moayeri", "Samyadeep Basu", "Sriram Balasubramanian", "Priyatham Kattakinda", "Atoosa Chengini", "Robert Brauneis", "Soheil Feizi"], "abstract": "Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy\"artistic style\" is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of \u201cartistic copyright infringement\u201d to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, including TagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today's popular text-to-image generative models.", "sections": [{"title": "Introduction", "content": "In the recent years diffusion-based text-to-image generative models such as Stable Diffusion, Imagen, Mid-Journey, and DeepFloyd [1, 16, 20, 21] have captured widespread attention due to their impressive image generation capabilities. Notably, these models demonstrate exceptional performance with very low FID scores on various conditional image generation benchmarks, showcasing their advanced capabilities. These models are pre-trained on a large data corpus such as LAION [23] containing up to 5B image-text pairs, which mirror a vast range of internet content, including potentially copyrighted material. This raises an important question - to what extent do image generative models learn from these copyrighted images? While previous studies [4, 25, 26] have shown that direct copying in diffusion models on the level of individual images is generally rare and mostly occurs due to duplications in the training data, the degree to which image generative models replicate art styles as opposed to art works remains unclear. This issue is increasingly critical as artists express concerns about generative models mimicking their unique styles, potentially saturating the market with imitations and undermining the value of original art. Furthermore, there are no laws currently to identify and protect an artist's style - mainly due to challenges in definition and a previous lack of necessity.\nArtistic styles are complex, broadly defined over a set of artworks created over their lifetime, making it challenging to determine a style by inspecting individual works of art (a la previous image-wise copy studies). We frame artistic style as characterized by a set of elements that co-occur frequently across works by that artist. For e.g., Vincent Van Gogh had a characteristic art style associated with Post-Impressionism - comprising expressive wavy lines, bright unblended coloring and his signature choppy textured brushwork. \nTo empirically study style copying in generative models and to build a corpus of artistic styles, we first collect an art dataset consisting of artworks from WikiArt from 372 artists, along with the artist labels. We then proceed to develop ArtSavant a practical tool which can effectively detect and attribute an image to its original artist. The design of this tool is strongly motivated by (i) the notions of \u2018holistic' and 'analytic' comparisons from the copyright legal literature [11,14] and (ii) shedding insight on the question: Is there a unique set of elements co-occurring across a given artist's works, and if so, can we extract this 'signature'?\nFor a style to be considered unique, it must be distinguishable from the styles of other artists. However, describing an art style is challenging and making a case"}, {"title": "Related Works", "content": "As image generative models have rapidly improved in scale and sophistication, the possibility of them mimicking artists' personal styles has been an important topic of discussion in the literature [18]. Many previous works describe ways to either detect potential direct image copying in generated images, or to foil any future copying attempts by imperceptibly altering the artists' works to prevent effective training by the generative models. These include techniques like adding imperceptible watermarks to copyrighted artworks [7,8,28], and crafting \"un-learnable\" examples on which models struggle to learn the style-relevant information [24, 29, 30]. These methods are typically computationally expensive and incur a loss in image quality, which may render these techniques impractical for many artists. Also, they do not protect artworks which have been previously uploaded to the internet without any safeguards. Others have suggested methods to mitigate this issue from the model owner's perspective - to either de-duplicate the dataset before training [4, 25, 26], or to remove concepts from the model after training (\"unlearning\") [3,9,13]. These are also technically challenging, and require the model owner to invest significant resources which may again inhibit their practicality. Methods like [4, 25, 26] are also more focused on analyzing direct image copying from the training data, and thus may not be applicable to preventing style copying.\nNone of these works tackle the problem of detecting potentially copied art styles in generated art, especially in a manner which may be relevant to legal standards of copyright infringement. According to current US legal standards [2], an artwork has to meet the \u201csubstantial similarity\" test for it to be infringing on copyright. This similarity has to be established on analytic and holistic terms"}, {"title": "Motivation", "content": "Recent works have investigated copying on an image-wise level, showing that state-of-the-art diffusion models can generate exact replicas of a small fraction of training set images [4, 25, 26]. Typically, these works involve representing images in a deep embedding space via models like SSCD [15] or DINO [5], and computing image-to-image similarities across generated and real images. These results, as well as anecdotal instances, have raised concerns amongst artists, since generative models may pose a risk of saturating the market with replications, thus jeopardizing the artists' livelihoods, as well as cheapening creative work they likely feel personal ownership and attachment to. Inspired by these valid concerns and existing results on image replication, we first explore if generative"}, {"title": "Towards Practical Artistic Style Copy Detection", "content": "To argue an artist's style is copied, one must first demonstrate the existence of a unique style for the artist. An analytic approach is to articulate the frequently co-occurring elements that comprise the artist's style. Alternatively, a holistic argument is to show that the artist's work can consistently be distinguished from that of other artists, than there must exist something unique that is present across the artist's portfolio. In the latter case, we have reduced style copy detection to a classification problem over sets of images (i.e. artist portfolios), something neural networks are well suited to do. We now propose DeepMatch and TagMatch, two complementary methods (w.r.t. accuracy and interpretability) that detect artistic styles in holistic and analytic manners, respectively."}, {"title": "WikiArt Dataset", "content": "To distinguish on artist's style from that of others, we need a corpus of artistic styles (consisting of portfolios from many artists) to compare against. To this end, we curate a dataset D consisting of artworks from WikiArt 3 to serve as (i) a reference set of artistic styles, (ii) a validation set of real art to show our method's can recognize an artist's style when shown a held-out set of their works, and (iii) a test-bed to explore if text-to-image models replicate the styles of the artists in our dataset in their generated images. Previous work [27] uses images from WikiArt to for a different purpose (GAN training). Since the content of WikiArt has been updated since then, we re-scrape WikiArt, perform a filtering step and subsequently curate a repository of ~91k painting images encompassing 372 unique artists (denoted by the set A). The filtering step ensures that each artist a \u2208 A has at least 100 images. We also denote the set of images (or portfolio) for each artist a \u2208 A as Da. Each of the images in our dataset is annotated with its corresponding genre (e.g., landscape) and style (e.g., Impressionism) which provides useful information about the paintings beyond their given titles. We provide an easy-to-execute script with all the necessary filtering step, so to generate newer versions of WikiArt if desired (see Appendix). We now detail our two complementary methods that compare a test set of images to our reference corpus so to detect if any of the reference styles reappear."}, {"title": "DeepMatch: Black-Box Detector", "content": "DeepMatch consists of a light-weight artist classifier (on images) and a majority voting aggregation scheme to obatin one prediction for a set of images. Majority voting requires that at least half the images in a test set Da are predicted to a for DeepMatch to predict a, allowing for abstention in case no specific style is recognized with sufficient confidence. For our classifier, we train a two layer MLP on top of embeddings from a frozen CLIP ViT-B\\16 vision encoder [17] to classify artwork to their respective artist, using a train split containing 80% of our dataset. We employ weighted sampling to account for class imbalance. Because we utilize frozen embeddings, training is very fast, taking only a few minutes. Thus, a new artist could easily retrain a detector to include their works (and thus encode their artistic style)."}, {"title": "Interpretable Artistic Signatures", "content": "Now we provide an interpretable alternative to matching via neural signatures. We draw inspiration from the interpretable failure mode extraction of [19]. Namely, we first tag images with atomic tags drawn from a vocabulary of stylistic elements. Then, we compose tags efficiently to go from atomic tags that are common across artists to longer tag compositions that are unique to each artist (i.e. tag signatures). Lastly, we detect artist styles in an attributable way via match tag signatures in a test portfolio to those for artists in our reference corpus. We detail each step of TagMatch, as well as validation results, below."}, {"title": "TagMatch: Interpretable and Attributable Style Detection", "content": "In Section 4.2, we outlined a holistic approach to accurately detect artistic styles. While DeepMatch obtains high accuracy (recognizing styles for 89.3% of artists), the neural signatures it relies upon lack interpretability. For a copyright detection tool to be useful in practice (e.g., to be used as assistive technologies), providing explanations of the classification decisions can tremendously benefit the end-user. To this end, we leverage our efficient tag composition algorithm as defined in Section 4.3 to develop TagMatch an interpretable classification and attribution method which can effectively classify a set of artworks to an artist, as well provide reasoning behind the classification and example images from both sets that present the matched tag signature. TagMatch follows the intuition of matching a test portfolio to a reference artist who's portfolio shares the most unique tag signatures. Given a set of N test images T = {x_i}_{i=1}^N , we first obtain a number of tag compositions for them using our iterative algorithm in Section 4.3. These tag compositions are then compared with the tag compositions of the artists in the reference corpus in order of uniqueness (i.e. we first consider tag signatures present in the test portfolio that occur for the fewest number of reference artists). We can then rank reference artists by how unique the shared tags are with the test portfolio. Altogether, TagMatch is remarkably fast, taking only about a minute, after caching embeddings of all images."}, {"title": "Analysis on Generated Art from Text-to-Image Models", "content": "We now turn to generated images, towards two ends. First, we seek to demonstrate the tools we validated on real art can be similarly effective in recognizing and articulating artistic styles in generated art. Secondly, by conducting a systematic empirical study, we aim to shed quantitative insight into the phenomena of style infringement by generative models. While enough instances of style mimicry have been observed to raise concern [18,24], the prevalence and nature of such instances remains nebulous. We hope our analysis can provide a more complete picture of the current state of style copying by generative models.\nSpecifically, we employ TagMatch and DeepMatch to generated versions of the art in our WikiArt dataset, so to quantify the degree to which generative models reproduce the stylistic signatures of the 372 artists in our dataset. These artists are somewhat representatitve in the sense that they touch a wide spectrum of broader styles, and they are each somewhat popular and prolific (with respect to having at least 100 works on WikiArt), making them good candidates to potentially have their styles infringed by generative models.\nSetup. We extract the titles from the paintings in our dataset from WikiArt and augment them with the name of the artist. Using these prompts (e.g.\" the starry night by Vincent Van Gogh\u201d or \u201cthe water lillies by Claude Monet\"), we generate images from 3 text-to-image generative models: (i) Stable-Diffusion-v1.4; (ii) Stable-Diffusion-v2.0; and (iii) OpenJourney from PromptHero. We note that (i) and (ii) are pre-trained on a subset of the LAION dataset [22], while (iii) is pre-trained on LAION and then fine-tuned on Mid-Journey generated images. We also note that (ii) uses a stronger CLIP text-encoder which can help generating images with better fidelity to the text-prompt. These characteristics make these generative models unique to one another, thereby providing a diverse range of artistic interpretations and styles in our image generation experiments.\nThus, for each artist a\u017c in our dataset, in addition to a set of his or her real artworks Da\u2081, we obtain a corresponding set of generated images Da, per generative model. We then compare each set of generated art to the entire corpus of existing art. Namely, we seek to quantify the frequency with which generated art prompted to be in the style of a specific artist is matched to that artist; we call this the match rate. Match rate is a percentage over 372 artists, as each artist is either matched correctly or not (i.e. to the wrong artist or no artist at all). We also consider top-5 and top-10 match rates, where a top-k match refers occurs when artist a\u017c is amongst the top k predictions for the set Da generated using prompts of the form \"{title of a work by ai} by {az}\".\""}, {"title": "DeepMatch Recognizes Artistic Styles for Half of the Artists", "content": "We first employ DeepMatch, the more accurate but less interpretable of our two style recognition systems, to quantify the degree to which the unique styles of artists from our dataset are reproduced by generative models prompted to recreate works from these artists. Recall that DeepMatch predicts an artist from a set of test images by first inferring the artist for each image, and then aggregating"}, {"title": "Articulating Style Infringements with TagMatch", "content": "We now utilize TagMatch because of its enhanced interpretability. Recall that in addition to predicting an artistic style, TagMatch also names the specific signature shared between the test set of images and the reference set of images for the predicted style. Thus, we can inspect the shared signature, as well as instances from both sets where the signature is present.  Thus, TagMatch can serve as an effective tool for describing the way in which stylistic elements are copied via language, as well as providing direct evidence of the potential infringement.\nTagMatch yields match rates of 12.95%, 37.31%, and 52.3% for top-1, top-5, and top-10 matches respectively. Low top-1 match rates using TagMatch alone cannot be used to argue that generative models do not reproduce artistic styles, but convincing arguments can be made combining the two methods. TagMatch also allows for understanding image distributions from the perspective of interpretable tags."}, {"title": "Conclusion", "content": "In our paper, we rethink the problem of copyright infringement in the context of artistic styles. We first argue that image-similarity approaches to copy detection may not fully capture the nuance of artistic style copying. After reformulating the task to a classification problem over image sets, we develop a novel tool ArtSavant, consisting of a dataset and two complementary methods that can effectively recognize artistic styles, via neural and tag based signatures. The success of our method offer strong evidence to the existence of unique artistic signatures, a necessary pre-requisite for styles to be protected. We highlight Tag-Match, which scaffolds a black-box AI component with interpretable intermediate outputs and a transparent way in which intermediate outputs are combined to arrive at a final prediction, resulting in a white(r)-box AI system. TagMatch, which can classify a set of images to an artist with reasonable accuracy as well as provide succinct text explanations and image attributions. Using these two detectors, we analyze generated images from different text-to-image generative models highlighting that amongst all the artists (including many famous ones), only 20% of the artists are recognized as having their style copied."}, {"title": "Limitations", "content": "Our work tackles a novel problem of artistic style infringements. Style, however, is qualitative. We merely put forward one definition for artistic style, along with two implementations for demonstrating the existence of a style given example works from an artist and recognizing the identified style in other works.\nImportantly, we argue that an artist's style is unique if we can consistently distinguish their work from that of other artists. However, we can only proxy the entire space of artists. We construct a dataset consisting of works from 372 artists spanning diverse schools of art and time periods in attempt to represent the space of existing artists, though of course we will always fall short in capturing all kinds of art. We provide tools to allow for this dataset to grow with time, and we caution that if only one artist for some broader artistic style is not present in our reference set, the uniqueness of that artist's style may be overestimated, and as such, generated images may be matched to this artist with an overestimated confidence. However, if only one out of 372 artists exhibits some style, than one could argue that that alone reflects a notable uniqueness of that artist. To employ a stricter criterion for alleging style copying, we'd recommend augmenting the reference set to include more artists with very similar styles to the artist in question. Nonetheless, we believe our reference dataset does well in representing all art, to where analysis based on this reference set is still informative.\nWe also note that our atomic tagging leverages an existing foundation model (CLIP) with no additional training. While we verify the precision of our tags, CLIP is known to have issues with complex concepts. Further, we do not claim our tags achieve perfect recall (most image taggers do not). We advise users to interpret the assignment of a tag to indicate a strong presence of that concept, relative to similar concepts (i.e. from the same aspect of artistic style). While our tagger is not perfect, it is objective and automatic, enabling interpretable style articulation and detection. Also, we note that the field of image tagging in general has seen rapid improvement in the past year [12], and an improved tagger could easily be swapped into our pipeline.\nLastly, we only analyze generated images using off-the-shelf text-to-image models. It is possible that particularly determined and AI-adept style thiefs fine-tune a model to more closely replicate specific artistic styles. This is a much more threatening scenario, though requires greater effort and ability by the style thief. We elect to demonstrate the feasability of our approach in the more broadly accessible setting of using models off-the-shelf, and note that our method can flexibly accept generated images produced in a different way (or perhaps discovered on the internet); notice generated images are an optional input in  We look forward to explorations of more threatening scenarios in future work, and hope both our formulation and methods for measuring style copying prove to be of use."}, {"title": "A nuance in artistic style infringements: Existing Artists can have very similar styles", "content": "A crucial step in arguing that an artist's style has been infringed is to first demonstrate the existence of the given artist's unique style. We note that doing so objectively is non-trivial, as a style may not have a clear definition, and thus, it can be challenging to systematically compare to all other artistic styles, so to show uniqueness. In our work, we utilized classification, claiming that if an artist's works can consistently be mapped (i.e. at least half the time) to that artist (over a large set of other artists), than that artist must have some underlying unique style (parameterized by a neural signature).\nIn doing so, we found that 89.3% of artists could be recognized based of a set of (at least 20 of) their works (held-out in training the classifier). What about the remaining 10.7% of artists? We now take a closer look at these artists, and also introduce a second, stricter style copying criterion. Namely, we consider the notion that it may be unfair to claim a generative model is copying the style of an artist, if another existing artist seems to also be copying that artist. That is, we propose a way to verify that the generative model not only shows a substantial similarity to the copied artist, but also an unprecedented similarity."}, {"title": "Artists who's styles were not recognized", "content": "First, we inspect more examples from artists who were not recognized using our majority voting threshold in DeepMatch. That is, less than half of their held-out works were predicted to them. First, the styles of artists who operate in the same broader genre (e.g. portraiture, landscapes, narrative scenes in renaissance styles, etc) can be extremely similar. We even see an instance where an artist's son's style is indistinguishable from his father's (Jamie and Andrew Wyeth). Lastly, we note that in most cases, the artists only marginally fall short of our recognition threshold (i.e. accuracy for their held-out works is only a bit below 50%). We utilize majority voting because (i) it is intuitive, (ii) it requires consistent appearance of the neural signature across works, and (iii) it allows for abstention when no particular style is strongly present. However, the exact threshold of 50% can be altered as desired. In summary, as in , we see artistic styles can be very similar, making the existence of unique artistic styles for the vast majority of artists a non-trivial observation.\nIf an artist's style cannot be recognized over their own held-out works, arguing that a generative model copies that style is strenuous, as the style itself is ill-defined. Notably, in these cases, the classifier had an option to predict the correct artist. However, in applying DeepMatch to generated images, there is no direct option for the classifier to abstain from predicting anyone, under that generated art comes from a \"new artist\", which takes inspiration from existing artists. Note that abstention is still possible (due to the majority voting in DeepMatch), and occurs when a match confidence falls below 50%. To make comparisons fairer to generative models, we now discuss a stricter criterion of unprecedented similarity."}, {"title": "Unprecedented Similarity: Do generative models copy styles more than existing artists already do?", "content": "A nuance that requires consideration when studying artistic style copying is that it is possible for two artists to have very similar styles. Thus, it may be unfair to allege that a generative model is copying an artist a if there exists another artist b who's style is just as or in fact even more similar to artist a. Towards this end, we introduce unprecedented similarity, which requires that the similarity between works of a generative model A' and works of the artist inteded to be copied A is higher than the similarity of any existing artist with A. That is, \n$\n  sim(A, A') \\geq sim(A, B) \n$\nfor works B from all other existing artists b.\nNote that this is a stricter criterion than our previous threshold. In Deep-Match, we required that at least half of the works in a given set of test images were predicted to a single artist in order for us to flag the test images as a potential style infringmenet. In other words, that threshold required that \n$\n  sim(A, A') \\geq 0.5 \n$\n, which in turn implies that \n$\n  sim(A, A') \\geq sim(A', B) \n$\nfor all B (with room to spare; here we use match confidence to denote similarity)."}, {"title": "Details on TagMatch", "content": "We now provide greater details regarding the implementation of TagMatch, a central technical contribution of our work. TagMatch is a method to classify a set of images to a class; specifically, we map a set of artworks to one artist, selected over 372 choices. TagMatch is not as accurate as DeepMatch, as it maps held-out works of each artist in our WikiArt dataset to the correct artist about 61% of the time (compared to 89% top-1 accuracy for DeepMatch). However, top-5 accuracy is more reasonabe, achieving above 80%. Most notably, TagMatch is inherently interpretable and attributable. It consists of three steps: (i) assigning atomic tags to images, (ii) efficiently composing tags to obtain more unique tag signatures, and (iii) matching a test set of images to a reference artist based on the uniqueness of the tags shared between the test set and works from the predicted reference artist.\nOur method is fast and flexible: after caching image embeddings, the whole thing only takes minutes, and it is easy to modify the concept vocabulary as desired, as the tagging is done in a zero-shot manner. Through MTurk studies, we verify that the atomic tags we assign our mostly precise, though we recognize that these descriptors can be subjective. Thus, while we do not claim perfect tagging, we stress that our method is easy to understand, and crucially, is deterministic per image. Therefore, ideally our tagging may be more reliable biased than human judgements, particularly when the humans involved may be biased (e.g. an artist alleging copying and a lawyer defending a generative model would have strong and opposing stakes).\nBelow, we provide details for image tagging (\u00a7D.1), artist tagging (\u00a7D.2), artistic style inference via tag matching (\u00a7D.3), effect of hyperparameters (\u00a7D.4), details on efficiency (\u00a7D.5), and a review of validation (\u00a7D.6)."}, {"title": "Image Tagging", "content": "As explained in \u00a74.3, we utilize CLIP to attain a diverse set of atomic tags per image in a zero-shot manner. Specifically, we first define a vocabulary of descriptors along various aspects of artistic style. Then, given an image, we do selective multi-label zero-shot classification for each aspect. Performing zero-shot classification per aspect proves to be critical in order to achieve a diversity of tags and"}, {"title": "From Image Tags to unique Artist Tags", "content": "Recall that we define styles not per-image, but over a set of images. Namely, we seek to surface tags that occur frequently. The best way to do so is to simply count the occurrences of each tag, and discard the ones that rarely appear. However, each atomic tag is not particularly unique with respect to artists. We utilized efficient composition of atomic tags to arrive at more unique tag signatures, as shown in figure 5 and detailed in algorithm 1. Importantly, we utilize a threshold here to differentiate what a common tag is; we require a tag to appear in at least three works for an artist in order for the tag to count as a frequently used tag by the artist. We note that tag composition can be done efficiently because we have a relatively low number of tags per image: on average, there are 6.2 atomic tags per image. Moreover, because the number of occurrences for a composed tag is bound belo by the number of occurrences of each atomic tag in the composition, we can ignore all non-frequent atomic tags. Thus, we can iterate over the powerset of common atomic tags per image without it taking exorbitantly long. We include one fail safe, which is that in the rare instance where an image has a very high number of common atomic tags, we truncate the tag list to include only 25 tags. Over the 91k images that we encounter, this happens only once. We highlight that our tag composition takes inspiration from [19]."}, {"title": "Predicting Artistic Styles based on Matched Tags", "content": "Once we have converted tags per image to tags per artist, we can then utilize these artist tags to perform inference over a set of images. Namely, given a test set of images, we extract common tags (including tag compositions) for the test set and compare them to tags extracted for each artist in our reference corpus. Then, we predict the reference artist who shares the most unique tags with the test set. Figure 12 best explains our method, as it shows the documented code. We note that all code will be released upon acceptance. We'll now explain it step by step. First, for each artist and for the test set of images, we find common tags via (i) assigning atomic tags to each image, (ii) finding the commonly occurring atomic tags, (iii) counting compositions of the commonly occurring atomic tags, and (iv) discarding tags (including compositions) that do not occur frequently enough. The code shows this done for the test set of images; we perform this per reference artist when the TagMatcher object (for which tag_match is function) is initialized; notice fields like self.ref_tags_w_counts_by_artist, which contain useful information about the reference artists, computed once and re-used for each inference."}, {"title": "Choosing Hyperparameters", "content": "Overall, there are three hyperparameters to our method: the z-score threshold, the tag count threshold, and the number of matches to consider per artist. Here is quick refresher on what they each do:\nThe z-score threshold determines how much more similar a descriptor needs to be to an image compared to other descriptors for the same aspect in order for the descriptor to be assigned as an atomic tag of the image. The value we use is 1.5.\nThe tag count threshold is the minimum number of an artist's works that a tag needs to be present in order for a the tag to be deemed common for the artist. The value we use is 3.\nThe number of matches to consider per artist pertains to how many matched tags are considered when computing the final score per artist in tag match. That is, the final score for an artist is the average of the top-k most unique"}, {"title": "Efficiency of TagMatch: Runs in ~ 1 minute", "content": "TagMatch is surprisingly fast. The longest step by far is computing CLIP embeddings for the reference artworks. This takes us about 5 minutes using one rtx2080 GPU with four CPU cores to embed the 73k training split images using a CLIP ViT-B\\16 model. Importantly, this step is done only once, and in practice, is done offline. The other steps and approximate time needed for each are as follows: embedding concepts (5 seconds), extracting common atomic tags and composing them (45 seconds), reorganizing tags and removing non-common tags (3 seconds). Then, inference for a test set of 100 \u2013 200 works takes about 10 to 15 seconds. Again, we will release all code upon acceptance, as we truly hope our tool can be of use to artists who are concerned by generative models potential infringing upon their unique styles."}, {"title": "Validation", "content": "Because tag match has multiple steps, we perform multiple validations. First, for image tagging, we utilize an MTurk study. We collect 3000 separate human judgements on instances of assigned atomic tags. Namely, we show 1000 randomly selected (tag, image) pairs to three annotators each.  MTurkers provide consent and are awarded $0.15 per task, resulting in an estimated hourly pay of $12 \u2013 $18. For each task, they answer 'yes', 'no', or 'unsure' to the question \u2018does the term {atomic tag} match the artwork below?' They are also shown example artworks for each term which were manually verified to be correct. Response rates were as follows: 69.89% yes, 8.99% unsure, 21.12% no. In investigating inter-annotator agreement, we find that at least 2 annotators agree 92.1% of the time, but all 3 agree only 51.52% of the time. This reflects the subjectivity associated with assigning artistic tags, and partially motivates the need for a deterministic automated alternative, in order to objectively tag images at scale. All three annotators said no only 5.16% of the time, and at least two said no 17.11% of the time, suggesting that our zero-shot tagging mechanism achieves reasonable precision.\nTo validate the value of tag composition, we refer to figure 5, which shows how tags become more unique as they get longer (i.e. consist of more atomic tags). Moreover, our time analyses show that the added benefit of composing tags to find unique tag signatures does not come at the cost of the efficiency of our method. Finally, the non-trivial top-1 matching accuracy and strong top-5 matching accuracy shows that the extracted tag signatures do indeed capture"}, {"title": "Patch Match: Generating Additional Visual Evidence of Copying", "content": "Detecting artistic style copying in a given art requires analyzing local stylistic elements that manifest across an artist's body of work. To address this, we employ a patch-based approach that compares small image regions between a given art and original artworks, enabling a fine-grained analysis of stylistic and semantic (e.g. objects) similarities at a local level. We consider three patch matching methods: CLIP-based, DINO-based, and Gram matrix-based."}, {"title": "Gram Matrix-based Patch Matching [10]", "content": "The Gram matrix is a measure of style similarity introduced in the context of neural style transfer. It captures the correlations between the activations of different feature maps in a convolutional neural network, representing the style of an image. For patch matching, the Gram matrices of patches from the given art and original arts can be computed and compared using a suitable distance metric (e.g., Frobenius norm). The Gram matrix is specifically designed to capture stylistic elements, making it well-suited for detecting style copying."}, {"title": "CLIP-based Patch Matching [17]", "content": "CLIP (Contrastive Language-Image Pre-training) is a powerful model that can effectively capture the semantic similarity between text and images. In the context of patch matching, CLIP embeddings can be used to measure the similarity between a patch from a given art and patches from original artworks. The patches can be encoded using the CLIP image encoder, and the cosine similarity between their embeddings can be computed to find the closest matches. CLIP may not be as sensitive to low-level stylistic elements, such as brushstrokes, textures, and color palettes, however it focuses more on higher-level semantic concepts, which can be useful to find if the given art pictured the same objects as the selected original patch."}, {"title": "DINO-based Patch Matching [6]", "content": "DINO is a self-supervised vision transformer that learns robust visual representations by solving a self-distillation task. DINO embeddings can be used for patch matching by computing the cosine similarity between the embeddings of patches from the given art and original artworks. We use DINO to capture higher semantical similarities, and check whether the given art pictured similar subjects of interest and high-level visual features as selected original artworks."}, {"title": "Experimental setting", "content": "For our experiments, we aim to identify the most similar artwork from a pool of 10"}]}