{"title": "InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance", "authors": ["Rui Xu", "Mengya (Mia) Hu", "Deren Lei", "Yaxi Li", "David Lowe", "Alex Gorevski", "Mingyu Wang", "Emily Ching", "Alex Deng"], "abstract": "The proliferation of Al-generated images has intensified the need for robust content authentication methods. We present InvisMark, a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR~51, SSIM ~ 0.998) while maintaining over 97% bit accuracy across various image manipulations. Notably, we demonstrate the successful encoding of 256-bit watermarks, significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes, achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility, extended payload capacity, and resilience to manipulations, InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content. Source code of this paper is available at: https://github.com/microsoft/InvisMark.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of generative AI (GenAI) technologies has revolutionized the creation of digital images, enabling the production of hyper-realistic deepfakes with unprecedented ease. While this technological leap offers exciting possibilities for creative expression, it simultaneously poses significant challenges to information integrity and public trust. The potential for these AI-generated images to be used in manipulating elections, damaging reputations, and undermining societal foundations underscores the urgent need for robust solutions to verify the origin and authenticity of digital content [18].\nIn response to these challenges, the Coalition for Content Provenance and Authenticity (C2PA) has emerged as a collaborative effort aimed at combating misinformation within the digital content ecosystem [1,31]. The C2PA suggests adding signed provenance information directly into its metadata. However, this approach is vulnerable to metadata stripping by malicious actors or during content sharing on social media platforms [7, 8, 38]. In this case, the stripped provenance can potentially be recovered through soft bindings, such as fingerprinting or watermarking, from trusted repository. Fingerprinting techniques leverage near-duplicate search in trusted databases for recovering content provenance [7,8,38]. However, these matches often lack precision, necessitating human intervention for verification. Image watermarking offers an alternative solution by inserting an imperceptible identifier within the content itself [14]. This allows for exact matching and retrieval of associated provenance from databases.\nTraditional watermarking techniques have focused on embedding imperceptible patterns within images, either directly in pixel values or in transformed frequency domains. Pixel-based methods, such as least significant bit (LSB) embedding, are simple to implement but highly susceptible to removal [34]. Frequency-domain techniques, utilizing transforms like Discrete Wavelet Transform (DWT) or Discrete Cosine Transform (DCT), offer improved robustness to certain transformations and have seen industrial adoption, such as the commerical implementation of Stable Diffusion [4,30]. Despite the improvements, frequency domain methods still suffer from vulnerability to relatively minor alterations to the image, limiting their robustness in real-world scenarios [20,21,29].\nThe advent of GenAI has spurred the development of innovative watermarking algorithms that integrate seamlessly into the image generation process. These approaches include watermarking training images with pre-trained en-"}, {"title": "2. Methodology", "content": "The goal of image watermarking is to embed a secret message $w \\in {0,1}^l$ (a binary string of length $l$) within a cover image $x$. Similar to previous approaches, InvisMark utilizes an encoder module $\\mathcal{E}$ to embed the message and create the watermarked image $\\tilde{x} = \\mathcal{E}(x, w)$ and a decoder module $\\mathcal{D}$ to extract it $\\tilde{w} = \\mathcal{D}(\\tilde{x})$. The primary goal is to minimize the perceptual distortions introduced to the cover image (quantified by image quality loss $\\mathcal{L}_q(x, \\tilde{x})$) while ensuring the watermark remains detectable even after the image undergoes common transformations (measured by watermark recovery loss $\\mathcal{L}_r(w, \\tilde{w})$)."}, {"title": "2.1. Watermark Encoder", "content": "The watermark, initially a bit array, is transformed via a linear layer into a 3D tensor, We then upsample the tensor and add zero-padding to match the downscaled image dimension. This preprocessing step is designed to enhance the watermark's resilience to geometric attacks by controlling its spatial distribution. Additionally, it facilitates the co-embedding of multiple watermarks within a single image, allowing for tailored traceability and security features.\nWe utilize a MUNIT-based encoder architecture with skip connections to preserve fine image details during watermark encoding [22]. Inspired by TrustMark, we also incorporate multiple 1\u00d71 convolution layers as post-processing to maintain high fidelity in the encoded image [10]. The encoder outputs the watermarked image's residuals at the low resolution, which are then upscaled to match the original image resolution using a TrustMark-like resolution scaling approach [10], ultimately yielding the final watermarked image."}, {"title": "2.2. Watermark Decoder", "content": "The decoder $\\mathcal{D}$ is a network trained to recover the hidden message from the encoded image $\\tilde{w} = \\mathcal{D}(\\tilde{x}) \\in {0,1}^l$. We use ConvNeXT-base with pre-trained weights as the default decoder. ConvNeXT is a \"modernized\" version of the standard ResNet [19] toward the design of a vision Transformer and has shown to reach state-of-the-art performance on many computer vision tasks [24]. We replace the last classifier layer with a $l$-dimension sigmoid activated fully connected layer to predict the $l$-bit secret message. We also explored using ResNet-based networks as decoder. However, we observed lower validation robustness due to unstable training arising from Batch Normalization layers, which is deprecated in the ConvNeXT architecture."}, {"title": "2.3. Noiser", "content": "We introduce a noise module after the encoded image. Instead of using randomly sampled noises to enhance robustness, we approach watermark resilience as a robust optimization problem, focusing on worst-case scenarios. However, unlike TrustMark, we introduce variability in geometric transformations to better reflect real-world scenarios. Specifically, for RandomResizedCrop, we randomly crop up to 25% of the original image area with an aspect ratio ranging from 3/4 to 4/3, then resize it back to the original resolution. For rotation, we apply random rotations of up to 10 degrees."}, {"title": "2.4. Model Training", "content": ""}, {"title": "2.4.1 Dataset", "content": "Our model is trained on 100k DALL\u00b7E 3 images generated from randomly sampled user prompts. While this training dataset is not available to the public, we evaluate our results against open-source DALL\u00b7E 3 dataset [2]. Additionally, we perform the evaluation on DIV2K, a well-established super-resolution benchmark containing 900 high-quality, 2K resolution images [3]. By benchmarking on both DALL-E 3 and DIV2K, we aim to target a more realistic product environment, demonstrating the model's ability to watermark images for both AI-generated and non AI-generated high-resolution images."}, {"title": "2.4.2 Loss", "content": "The overall training loss is the weighted sum of encoded image quality loss $\\mathcal{L}_q$ and watermark recovery loss $\\mathcal{L}_r$\n$\\mathcal{L} = \\alpha_q\\mathcal{L}_q(x, \\tilde{x}) + \\alpha_r\\mathcal{L}_r(w,\\tilde{w})$\nwhere $\\alpha_{q,r}$ are the relative weights for each loss term.\nImage Quality Loss: The image quality loss $\\mathcal{L}_q$ comprises four components: $\\mathcal{L}_{Yuv}$ measures the pixel-level mean squared error in the YUV color space, $\\mathcal{L}_{LPIPS}$ evaluates the perceptual similarity between the reconstructed and original images [37], $\\mathcal{L}_{FFL}$ focus on frequency loss targets to reduce artifacts in the frequency domain [23] and $\\mathcal{L}_{GAN}$ is Wasserstein GAN loss, used to train a discriminator by encouraging the encoded images to be indistinguishable from real images [6]. The $\\beta$-coefficients, representing the relative weights of each loss term, are maintained constant 1.0 during training. Image quality loss is computed at the downscaled resolution rather than the original image resolution.\n$\\mathcal{L}_q(x, \\tilde{x}) =\\beta_{Yuv}\\mathcal{L}_{Yuv} + \\beta_{LPIPS}\\mathcal{L}_{LPIPS} + \\beta_{FFL}\\mathcal{L}_{FFL} + \\beta_{GAN} \\mathcal{L}_{GAN}$"}, {"title": "Watermark Recovery Loss:", "content": "In addition to the standard binary cross-entropy loss applied to the encoded image, we also incorporate a watermark recovery loss specifically tailored to images that have undergone a pre-defined list of $n$ image transformations (defined as $\\Phi_i(x)$). We treat watermark robustness under various image distortions as a robust optimization problem, focusing on performance across worst-case scenarios. However, to reduce computational complexity, we avoid solving the minimax problem at every iteration. Instead, each 200 steps, we reassess the watermark recovery loss across all noises and select the top-k (with $k = 2$) noises that result in the highest loss, as detailed in Equation 3. The noises employed in training remain constant until the next reevaluation. The parameter $\\gamma_i$, controlling the relative weight of the watermark recovery loss after transformation $i$ compared to the original, is held constant at 0.5.\n$\\mathcal{L}_r(w, \\tilde{w}) = \\mathcal{L}_{BCE}(w, \\mathcal{D}(\\tilde{x})) + \\max_{I \\subset [0,n], |I|=k} \\sum_{i \\in I} \\gamma_i \\mathcal{L}_{BCE}(w, \\mathcal{D}(\\Phi_i(\\tilde{x})))$"}, {"title": "2.4.3 Training Strategy", "content": "The training process is structured into three distinct stages:\nWatermark Extraction: In the initial stage, we focus on optimizing the watermark decoder by setting the weight for image quality loss $\\alpha_q$ to a low value. This approach stabilizes training, as the watermark signal is inherently weaker compared to the image content itself [10].\nImage Reconstruction: Once a high watermark recovery rate is achieved, we progressively increase $\\alpha_q$ to a predetermined maximum value ($\\alpha_{q,max} = 10.0$ in our experiments). This encourages the model to enhance the quality of image reconstruction while maintaining watermark detectability. During the first two stages, we avoid introducing image distortions to enable the model to learn optimal encoding and decoding under ideal conditions. However, due to the end-to-end training with resolution scaling, the model inherently demonstrates robustness against certain distortions like resizing and compression.\nRobustness Enhancement: Robust optimization is activated once $\\alpha_q$, the weight for image reconstruction loss, reaches its maximum value. We've observed that certain image transformations, such as random cropping and rotation, pose significantly greater challenges for watermark recovery compared to others. Introducing these complex distortions prematurely during training can hinder model convergence. Therefore, we strategically incorporate robust optimization, represented by the second loss term in Equation 3, only in the final training stage. This targeted approach allows us to fine-tune the model specifically for resilience against these challenging distortions without compromising performance for other distortions or sacrificing the quality of the encoded image."}, {"title": "3. Experiments", "content": "We benchmark our approach against recent watermarking and steganography methods, including TrustMark [10], SSL [17], StegaStamp [33], and dwtDctSvd [28]. For fair comparison, we use all 900 images from DIV2K dataset and 900 randomly sampled images from open sourced DALL\u00b7E 3 dataset [2]. Each image is encoded with randomly generated 100 bit watermark. We utilize TrustMark-Q and SSL models with default pretrained weights. As no pretrained weights are available for StegaStamp, we retrain the model on our dataset using the default hyperparameters. To evaluate image quality, we employ the standard peak signal-to-noise ratio (PSNR) and Structural Similarity Index Measure (SSIM) for imperceptibility, and decoded bit accuracy for watermark recovery (where 50% signifies random guessing). All metrics are computed at the original image resolution. We apply resolution scaling if an algorithm lacks native resolution support."}, {"title": "3.1. Watermark Quality", "content": "Figure 2 presents watermarked images along with their magnified residuals (20\u00d7) for both DALLE 3 and DIV2K datasets. TrustMark and StegaStamp generate residuals with a more uniform color distribution, whereas InvisMark exhibits color stripes that could become more noticeable if residual amplitudes were comparable. However, InvisMark's residuals are significantly weaker than other methods, resulting in better image quality and imperceptibility. The watermarked images achieve PSNR values around 51 (refer to Table 2 and Figure 3), significantly outperforming prior works that reached approximately 40."}, {"title": "3.2. Watermark Robustness", "content": "A key contribution of this work is the development of several components specifically designed to enhance watermark robustness against common image manipulations. Firstly, by incorporating resolution scaling for watermark residuals directly into the training process, the watermark model becomes inherently resilient to a wide array of transformations, even without additional noise injection. Secondly, we employ a robust optimization technique that prioritizes the most challenging scenarios, ensuring optimal performance against all image transformations encountered during training. Finally, we depart from the commonly used ResNet architecture and employ a more sophisticated decoder model, enabling it to better capture the subtle patterns within encoded watermarks, thereby further boosting their robustness.\nTable 3 shows the superior robustness of our watermarking method compared to previous works. To emphasize performance differences, we employed relatively high noise levels, similar to the \"medium\" level noises in TrustMark. While all methods perform well on clean images, their effectiveness varies significantly under noisy conditions.\nTraditional watermarking techniques like dwtDctSvd typically struggle in the presence of noise, with a few exceptions such as RandomErasing and GaussianBlur. While StegaStamp delivers promising results for high-resolution images when resolution scaling is employed, the encoded image quality often falls short (PSNR~37), limiting its creative applications. Both SSL and TrustMark show reasonable watermark recoverability under noise, but each has specific vulnerabilities: SSL struggles with JPEG compression and ColorShift, while TrustMark is susceptible to Gaussian Noise and Rotation. We were unable to replicate TrustMark's reported results for SSL and StegaStamp, even on clean images. Our analysis indicates that these two methods actually achieve better performance than reported in the TrustMark paper.\nIn contrast, InvisMark consistently outperforms the closest baseline across almost all tested image distortions, achieving near-perfect decode bit accuracy for many common noises like Gaussian blur. Even when faced with more challenging distortions, InvisMark maintains remarkable bit accuracy, consistently exceeding 97%."}, {"title": "3.3. Watermark Attacks", "content": "Recently, more advanced and sophisticated watermark attacks have emerged. Adversarial attacks, for instance, generate adversarial examples with similar latent space features to deceive watermark detection systems [25, 32]. Regeneration attacks, on the other hand, use diffusion models or VAEs to remove watermarks by introducing noise and subsequently denoising the image [39]. Even more concerning are forgery attacks, which aim to replicate and apply legitimate watermarks to unauthorized images [41]. In this section, we will discuss the robustness of our watermarking solution against these attacks and explore potential mitigation strategies.\nAdverserial Attack: We evaluated our watermark solution against embedding attack, a common form of adversarial attack where the goal is to subtly alter the latent representation to remove or disrupt the watermark. To simulate this, we used the off-the-shelf encoder AdvEmbG-KLVAE8, which leverages the KL-VAE encoder (F8) [5]. This grey-box setting mirrors the use of public VAEs in pro-"}, {"title": "Forgery Attack and Mitigation:", "content": "While regeneration and adversarial attacks can successfully remove our watermarks, they do so at the cost of significantly degrading image quality (PSNR~25). However, in the context of content provenance, watermark removal merely indicates that watermarking can no longer be used to authenticate images; it does not necessarily imply the image is inauthentic. A more significant threat lies in forgery attacks that could deceive watermark decoders into classifying attacked images as authentic.\nWe present a simple forgery attack on our watermarks, assuming public access to the watermark encoder. An attacker could extract watermark residuals from pre- and post-watermarked images, then resize and add these residuals to other images to potentially trick the decoder into authenticating the forged content.\nTo mitigate forgery attacks, we propose binding the image watermark to its content using fingerprinting techniques [13]. Content Credentials enable storing image fingerprints within the C2PA manifest referenced by the watermark identifier. During database lookup, the queried image's fingerprint is compared to the stored one, rejecting any mismatches and thereby enhancing security."}, {"title": "3.4. UUID encoding", "content": "While longer watermarks can negatively impact image quality and watermark recovery, our work demonstrates the successful encoding of significantly longer watermarks while still maintaining exceptional image quality and watermark recoverability. The image quality is further supported by an average PSNR of 48 and SSIM of 0.997 (see Table 5), surpassing prior work with shorter watermarks. InvisMark also exhibit remarkable robustness, achieving an average bit accuracy of 99% across tested transformations, with a worst-case ~97% accuracy under resized crop or JPEG compression.\nBesides bit accuracy, we introduce watermark decoding success rate as another key metric, representing the probability of complete watermark recovery with error corrections. This is particularly vital for content provenance applications, where accurate decoding of all bits is essential for database retrieval. Employing Bose-Chaudhuri\u2013Hocquenghem (BCH) error correction code [9] and 128-bit UUIDs as data bits, we achieve a perfect 100% success rate across various transformations (refer to Table 6), and maintain over 90% success rate even under most challenging image distortions.\nLarger payloads also enable further system improvements, such as embedding image fingerprints as part of the watermark secret for client-side verification. This approach could potentially eliminate the need for datastore lookups in the case of fingerprint mismatches, thereby boosting overall system efficiency."}, {"title": "3.5. Limitations", "content": "While InvisMark typically preserves extremely high image quality, perfect imperceptibility cannot be guaranteed. In rare instances, subtle artifacts may become visible in areas with large, uniform backgrounds, like clear blue skies (see Figure 6 for examples). Future research focusing on enhancing residual color uniformity could significantly address this issue."}, {"title": "4. Conclusion", "content": "In this work, we presented InvisMark, a novel high-capacity and robust watermarking approach dedicated for high-resolution images. We leverage resolution scaling and robust optimization techniques and significantly improved the decoder's resilience against common image transformations, while minimizing visual artifacts in the encoded image. Through extensive experiments, we demonstrated that InvisMark achieves superior performance in terms of imperceptibility and robustness across diverse datasets, including both AI-generated and traditional images, significantly surpassing existing approaches. We push the boundaries of watermark payload capacity by successfully embedding 256 bits of watermark while maintaining high levels of imperceptibility and resilience. We also examined the vulnerabilities of our watermarks, acknowledging that sophisticated attacks can potentially remove our watermarks at the cost of degrading image quality. To address more severe threats such as forgery attacks, we propose integrating InvisMark with complementary fingerprinting techniques. This combined approach effectively mitigates such attack vectors, making InvisMark a trustful tool for ensuring digital content traceability and authenticity."}]}