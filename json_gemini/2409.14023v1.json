{"title": "FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs", "authors": ["Ehsan Kabir", "Md. Arafat Kabir", "Austin R.J. Downey", "Jason D. Bakos", "David Andrews", "Miaoqing Huang"], "abstract": "Transformer neural networks (TNNs) are being applied across a widening range of application domains, including natural language processing (NLP), machine translation, and computer vision (CV). Their popularity is largely attributed to the exceptional performance of their multi-head self-attention blocks when analyzing sequential data and extracting features. To date, there are limited hardware accelerators tailored for this mechanism, which is the first step before designing an accelerator for a complete model. This paper proposes FAMOUS, a flexible hardware accelerator for dense multi-head attention (MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is optimized for high utilization of processing elements and on-chip memories to improve parallelism and reduce latency. An efficient tiling of large matrices has been employed to distribute memory and computing resources across different modules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C and U200 data center cards containing Ultrascale+ FPGAs. Experimental results are presented that show that it can attain a maximum throughput, number of parallel attention heads, embedding dimension and tile size of 328 (giga operations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore, it is 3.28\u00d7 and 2.6x faster than the Intel Xeon Gold 5220R CPU and NVIDIA V100 GPU respectively. It is also 1.3\u00d7 faster than the fastest state-of-the-art FPGA-based accelerator.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer neural networks have demonstrated significant advancements in natural language processing (NLP) [1], [2], machine translation [3], computer vision [4], and other domains in recent years. Numerous transformer-based models have surfaced, including full transformers containing an encoder and decoder [2], BERT [5], [6], Transformer-XL [7], ALBERT [8], T5 [9], Routing Transformers [10], structBERT [11], and more. These models contain a remarkable feature named multi-headed attention (MHA) mechanism which is different from the traditional convolutional neural network (CNN), recurrent neural network (RNN), and long short term memory (LSTM) model. It is even replacing RNNs and LSTMs for NLP tasks, as well as convolutional layers in CV tasks, because it enables a high level of computational parallelism for both training and inference phase. This makes it well-suited for acceleration on hardware such as GPUs and FPGAs.\nNevertheless, the attention mechanism incurs high computational costs due to intensive matrix computations and intricate data flows [12]. It consumes a significant amount of runtime in many existing TNNs, ranging from about 38% to 64% when the sequence length (number of tokens in the input sequence) varies from 64 to 256 [13]. Unfortunately, general-purpose platforms such as GPUs and CPUs are inefficient for processing TNNs due to low computational efficiency, underutilized memory bandwidth, and significant compilation overheads [14]. In contrast, FPGAs have gained widespread use for accelerating DNNs due to their high level of parallelism and low latency [15], [16]. Many works focus on parallelizing computations to accelerate CNN, LSTM, Graph Convolutional Network (GCN) [17]-[20] on FPGAs. Recently, some works have successfully built FPGA or application-specific integrated circuit (ASIC) hardware accelerators for transformers [21]\u2013[23]. Most of these works compress the model by using different weight pruning strategies to accelerate attention, and they reduce latency by incorporating sparse matrices. Thus, they have specialized sparse architecture for a specific application. However, different applications require different sparsity patterns, necessitating the redesign of hardware architectures for optimal results, which is a time-consuming and challenging task. ASICs are designed for a specific model and configuration, so, they perform poorly on different models or even the same model with varying configurations [24]. Custom FPGA accelerators also lack the flexibility to be reconfigured for a different model during runtime.\nThus, a versatile accelerator is needed that can efficiently handle dense matrix computations across various TNN applications. The study in [21] utilizes logic resources to implement a systolic array (SA) for parallelism, leading to a waste of digital signal processing (DSP) resources that are capable of high-speed computation at higher frequencies. DSP consumption also depends on the implementation method. For example, most accelerators [23], [25]\u2013[27] used high-level synthesis (HLS) tools, while some used hardware description language (HDL) [28]\u2013[30] for design. While HLS takes less implementation time compared to HDL, it is challenging to write efficient HLS code that can effectively manage certain FPGA resources like DSPs on an FPGA for optimal performance [18]. Analysis done in [22], [31]\u2013[34] showed"}, {"title": "II. BACKGROUND", "content": "There are several building blocks in transformers as shown in Fig. 1. An input sequence of tokens is converted into embeddings. The positional encoder enables the model to consider the order of tokens in a sequence by adding positional information to the embeddings. It generates vectors that give context according to the word's position in a sentence. Then the vectors are linearly transformed into three tensors: Q (queries), K (keys), and V (values) by multiplying the embedding matrix with three weight matrices. The encoder block handles these tensors, transforming them into a higher-level representation that encapsulates crucial information. This process ensures the proper capture of features and contextual relationships within the input sequence. The encoder architecture comprises two main sub-layers: (1) the self-attention mechanism, and (2) the position-wise feed-forward network. The self-attention mechanism enables the model to assess different segments of an input sequence simultaneously. It captures long-range relationships by measuring attention scores and utilizing multi-head projections for various input representations. Thus, it can learn complex patterns, dependencies, and relationships effectively. The position-wise feed-forward network (FFN), which is equivalent to a multilayer perceptron (MLP), applies linear transformations to every position independently in the input sequence. In this network, two linear transformations are executed. They mainly contain matrix-vector multiplication. The first linear transformation has activation functions such as the Rectified Linear Unit (ReLU) or Gaussian Error Linear Unit (GeLU) but the second one does not have these.\nFurthermore, each sub-layer includes a residual connection combined with layer normalization (LN). This solves the vanishing gradient problem during training. Residual addition and LN layers are inserted after each MHA and FFN. It mainly includes the addition of matrix elements and nonlinear functions. The decoder block illustrated in Fig. 1 is responsible for generating the output sequence based on the encoded representations supplied by the encoder. Like the encoder, the decoder also consists of a stack of N identical layers. Each layer within the decoder contains three sub-layers. They are: (1) the Masked Attention Mechanism, resembling the encoder's self-attention, and it includes a masking feature that restricts the output's dependency on known preceding outputs; and (2) an attention layer that directs its focus to the encoder's output, enabling the decoder to emphasize relevant sections of the input sequence for each output element. and (3) a position-wise feed-forward network.\nAs illustrated in Fig 2, the scaled dot product attention in each head is a crucial part of the multihead attention layer. The attention weights are computed by performing the dot product of the Q and K matrices and subsequently scaling them down by the square root of the 2nd dimension of the K matrix. This scaling is essential to prevent the dot products from becoming excessively large, contributing to the stabilization of gradients during the training process. Subsequently, the scaled dot products undergo the softmax function, resulting in the computation of attention weights. These weights are then utilized to perform a weighted sum of the value vectors. The"}, {"title": "III. RELATED WORK", "content": "Several FPGA and ASIC accelerators exist for accelerating attention mechanisms. The ASIC design in [22] exploited parallelism and datapath specialization to significantly improve performance and energy efficiency. Another ASIC called ELSA [13] used specialized approximation algorithms to reduce computation. SpAtten [33] ASIC utilized sparsity and quantization to reduce computations and memory access. A hardware-software co-design framework called Sanger [12] enabled dynamic sparsity by a reconfigurable architecture on ASIC. The FPGA accelerator proposed by Lu et al. [21] is the first hardware architecture to accelerate both the multi-head attention (MHA) layer and the feedforward network (FFN) of the transformer. However, their implementation was done using HDL for a single attention head. Ye et al. [35] proposed an FPGA accelerator for MHA with reconfigurable architecture, efficient systolic arrays, and hardware-friendly radix-2 softmax, but they did not consider maximization of BRAM and DSP usage to maximize parallelism. Fujimaki et al. [36] also proposed a systolic array-based accelerator for attention mechanism within CNN where MHA consumed 63% of the computation time, but they used MHA within CNN, not TNN. A shared computing architecture is implemented in [37], where a parallel computing array is shared between M\u041d\u0410 and FFNs for a CNN application. A novel structural pruning method was proposed by [38] and the associated accelerator on FPGA was designed to reduce memory footprint. All of the existing hardware architectures are designed for a specific TNN and a specific sparsity pattern. They lack the flexibility to reconfigure the computing structure for different applications during runtime. Furthermore, they have not explored which tile size and what utilization of BRAMs and DSPs could achieve optimum parallelism."}, {"title": "IV. ACCELERATOR ARCHITECTURE", "content": "The core of the accelerator was designed in C language on Vitis high level synthesis (HLS) 2022.2.1 tool. Functional verification was performed through its C simulation and C/RTL co-simulation features. This section describes the HLS design technique that generates an optimized architecture utilizing most of the BRAMs and DSPs in the processing elements, ensuring high parallelism.\nA. Overall Structure\nThe overall structure of the accelerator is shown in Fig. 3. There are three main processing modules in it. They are denoted as QKVPM, QKPM and SVPM according to the output they produce. The number of instances for these modules depends on the number of attention heads (h). Each module contains an array of processing elements (PE). A PE is comprised of a DSP48 performing multiplication and accumulation (MAC) operations. The number of PEs (t) depends on the unrolling factor of the inner loop and the initiation interval of the pipelined outer loop. The PE array's data access pattern and computational requirements differ across modules. Therefore, they are defined separately with distinct sets of PE arrays. This approach enables optimization of each module separately. Input data and weights are stored in multiple BRAMs to enable parallel access.\nIn our architecture, each PE is independent, with its own local memory, control and computing unit. The weights (Wq, Wk, Wu) for generating query (Q), key (K) and value (V) matrix are declared as separate two-dimensional arrays of size (dmodel \u00d7 TS) in HLS. TS is tile_size which represents the dimension of the sub-matrices into which the larger weight matrices are divided. The number of heads and tiles, and the array partition directive on HLS determine how the arrays will be partitioned to generate multiple two-port BRAMs. Due to the limited ports of BRAMs, array partitioning and data loading are efficiently managed to ensure that data required simultaneously by a DSP are stored in separate BRAMs. The Q, K, and V matrices of size (SL \u00d7 dmodel) are stored in intermediate buffers, which are also implemented as BRAMs. SL is sequence_length.\n1) QKVPM module: QKVPM module generates the query, key, and value matrices. This module contains the Wq, Wk, WBRAMs, and input (Xi) BRAMs from which data is"}, {"title": "V. OVERALL SYSTEM", "content": "Fig. 5 shows the complete system design for running the multi-head attention layer on different FPGA platforms such as U200 (UltraScale+XCU200-FSGD2104-2-E) and U55C (UltraScale+XCU55C-FSVH2892-2L-E) for our experiments. Each design parameter can be programmed during runtime up to a maximum value by MicroBlaze (\u00b5B) softcore processor.\nThe overall system was designed on Vivado 2022.1.2 design suite. It contains a custom IP block for the MHA accelerator, which is exported from HLS. The inputs and weights are fetched from off-chip high-bandwidth memory (HBM) using AXI4 master interfaces [39] when the load instruction from the accelerator controller is received according to demand. The accelerator receives control signals from the processor through an AXI-lite slave interface. \u00b5B can access the HBMs which is connected to the MHA accelerator. It is used to transfer data to the BRAMS from HBMs. It also sends control signals to the accelerator. The boards are connected to a host PC with USB-JTAG interface and PCIe 3.0\u00d74 interface. This host can communicate with other IPs except \u00b5B using the DMA/Bridge Subsystem for PCI Express IP [40] in the system, but PCI communication was not needed in this work. \u00b5B uses AXI-TIMER [41] to measure the latency, which includes the time between the start and stop signal from the custom IP module. The host connected to JTAG cable [42] displays the results on the terminal using the UARTlite interface [43]."}, {"title": "VI. EVALUATION AND RESULTS", "content": "Table I illustrates the runtime programmable capability, resource utilization, and performance of FAMOUS. Synthesis was performed once for a constant tile size. The design parameters such as embedding dimension ($d_{model}$), number of heads (h), and sequence length (SL) of the accelerator were configured before synthesis with the fixed values of 768, 8, and 64, respectively, according to a variant of BERT [6] and the available FPGA resources. Then they were dynamically adjusted during runtime using \u00b5B. Hence, FAMOUS can be synthesized for a fixed number of resources, but it will remain flexible enough to accommodate smaller architectures as needed. The tile size can be adjusted only before synthesis. The data was quantized into 8-bit fixed-point numbers. Quantization for various applications may lead to accuracy loss, although it wasn't our primary focus. If a larger bit width is necessary, the design can be easily adjusted by modifying certain parameters in HLS code during design time, which will affect resource utilization and latency. Tests no. 1, 2 & 3 show how the number of heads can be varied within the same accelerator dynamically affecting the latency and throughput where throughput is defined as number of giga operations per second (GOPS). On Alveo U55C, the lowest latency of 0.94 ms and the highest GOPS of 328 were achieved for 8 parallel heads. Tests no. 4 & 5 show the effect of varying embedding dimensions on performance where latency increased and GOPS decreased for a larger dimension. Sequence length was dynamically varied for tests no. 6, 7 & 8 and performance deteriorated as the length increased. It can be observed that resource utilization remained unchanged from tests 1 to 8 because the accelerator was synthesized only once when tile size was constant, while other parameters could be reconfigured at runtime from the software.\nTests no. 9 & 10 had different tile sizes, necessitating resynthesis of the accelerator, which resulted in different resource utilization. Resource utilization decreased with a reduction in tile size, leading to increased latency and decreased GOPS. This is because a smaller tile size requires more frequent loading of each tile from external memory to on-chip memory. Tests no. 11 and 12 demonstrated the performance and resource utilization of FAMOUS on Alveo U200, highlighting its portability. We ensured high resource utilization levels, with 46% DSPs, 78% BRAMS, and 98% LUTs. Further DSP utilization was not feasible, as it would have exceeded the capacity of LUTs. The optimal number of attention heads operating in parallel was determined to be 8 and 6 when the tile size is 64 on Alveo U55C and U200, respectively. Otherwise, the LUT becomes overutilized by the QKVPM module. Reducing the tile size helped to decrease resource consumption, although at the expense of speed. Six parallel attention heads were feasible on U200, and this decrease in parallelism led to an increase in latency.\nTable II shows a comparison of FAMOUS with some GPUs and CPUs running approximately at 1.5GHz frequency. Topologies include sequence length, embedding dimension, and number of heads. Assuming that attention heads operate in parallel, their number should not impact the latency. Therefore, we did not alter the number of attention heads, even though other works used different numbers. However, we varied the embedding dimensions in line with other studies to ensure a fair comparison. We achieved 3.28\u00d7, 2.6\u00d7, 1.17\u00d7 speed up and increase in throughput compared to Intel Xeon Gold 5220R CPU, NVIDIA V100 GPU, and Intel E5 2698 v4 CPU respectively because of higher parallelism."}, {"title": "VII. ANALYTICAL MODEL FOR LATENCY", "content": "The parameters that affect the resource utilization and performance of FAMOUS are the tile size or number of tiles in the attention module, the number of attention heads, the sequence length, and the embedding dimension when the bit width is fixed. An analytical model was developed to establish the relationship between these parameters and latency. This model aids in estimating both latency and the value of the parameters before synthesis.\nThe design is modular, and each module is implemented as a function with loops. Thus, the latency of the modules depends on the loop iteration latency, which in turn depends on the loop pipeline and unrolling pragmas. For the nested loops in the modules, the second loop from the last is pipelined resulting in a complete unroll of the innermost loop. The outermost loop had no pragmas to avoid complicated pipeline depth and resource requirements. Pipelined loop latency (PLL) can be calculated by equation 3. If it is enclosed by another loop, then the total latency (TL) will be given by equation 4. Here, Trip_count (TC) is the number of iterations of a loop, and the initiation interval (II) is the latency between the initiation of two consecutive iterations. Pipeline_Depth is the latency to finish one iteration. It depends on the sequential and parallel operations executed in one iteration. Different modules of FAMOUS can have different Pipeline_Depth (PD). The latency is measured in clock cycles (cc).\n$PLL =(TC \u2212 1) \u00d7 II + Pipeline_Depth [46]$ (3)\n$TL = PLL \u00d7 Outer_Loop_TC [46]$ (4)\nEquation 3 & 4 are generalized equations, the variables of which differ for different modules of FAMOUS as shown in the following equations.\n$LI =[(dmodel \u2212 1) \u00d7 1 + PD_L] \u00d7 SL$ (5)\n$LB = (\\frac{dmodel}{h} \u2212 1) \u00d7 1 + PD_L$ (6)\n$LIA =[(TS \u2212 1) \u00d7 1 + PD_L] \u00d7 SL$ (7)\n$LWA = [(\\frac{dmodel}{h} \u2212 1) \u00d7 1 + PD_L] \u00d7 SL$ (8)\n$SA = [(\\frac{dmodel}{h} \u2212 1) \u00d7 1 + PD_MHA] \u00d7 SL$ (9)\n$BA = [(\\frac{dmodel}{h} \u2212 1) \u00d7 1 + PD_BA] \u00d7 SL$ (10)\n$S = [(SL-1) \u00d7 1 + PD_S] \u00d7 SL$ (11)\n$SV = [(\\frac{dmodel}{h} \u2212 1) \u00d7 1 + PD_SV] \u00d7 SL$ (12)\nwhere, PD_L includes the time required to establish communication with HBM using AXI master interface (7 cc), read address location (1 cc), load (1 cc), and store (1 cc) data from and to that address, and convert floating point data to fixed point (3 cc) for tasks such as loading all inputs (LI) and all biases (LB), as well as loading inputs (LIA) and weights (LWA) for each attention head. PD_MHA equals ($\\frac{dmodel}{TS}$) plus the time required to load (1 cc), multiply (2 cc), add (1 cc), and store (1 cc) for computing self-attention (SA) in QKVPM module. PD_BA includes latency associated with loading, adding, and storing operations in bias addition (BA) tasks. PD_S equals (SL) , the time required to compute the score (S) in QKPM module. PD_SV equals SL in the computation of SV within the SVPM module. Experimental results showed that\nEquation 13 represents the total latency ($LAT_{total}$) in clock cycles (cc), which was calculated by summing equations 5 through 12. Equation 14 converts clock cycles into milliseconds (ms).\n$LAT_{total(cc)} =LI + LB + LIA + LWA+ SA+BA+ S + SV$ (13)\n$LAT_{total(ms)} = \\frac{LAT_{total (cc)} \u00d7 10^{3}}{Frequency(Hz)}$ (14)\nFor instance, the analytical model predicts a latency of 0.98 ms at 400 MHz for the configuration of test 1 in Table I, closely matching the experimental result of 0.94 ms. Likewise, for test 6, the analytical model estimates a latency of 1.9 ms, which is very close to the 2 ms observed experimentally. Other data from the same table will also comply with the analytical model."}, {"title": "VIII. CONCLUSION & FUTURE WORKS", "content": "In this research, a flexible FPGA-based accelerator was designed for the multi-head attention (MHA) layer of a transformer neural network (TNN) using a high-level synthesis (HLS) tool. The accelerator architecture leverages the parallelism of the FPGA and the inherent parallelism of the MHA layer. Resources including BRAMS, DSPs, and LUTs were utilized in an optimized way on Alveo U55C and U200 FPGA platforms to maximize parallelism and minimize the latency. The accelerator is runtime programmable to support different topologies without going through synthesis steps. An efficient tiling and loading of weight arrays were implemented to accommodate large models in on-chip memory without exceeding the capacity of computation units. Experimental results demonstrate that our design achieves a maximum throughput of 328 GOPS, surpassing some CPUs and GPUs. It achieves comparable throughput with state-of-the-art ASIC accelerators, which operate at higher frequencies and leverage sparsity to reduce computation and resources. Moreover, it achieved a latency that is 1.3\u00d7 lower than the fastest state-of-the-art FPGA-based accelerator. In this paper, the architecture supports only the attention module, but it will be expanded to support the full encoder and eventually both the encoder and decoder of the transformer in future work using the same architectural concept."}]}