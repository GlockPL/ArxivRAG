{"title": "Towards a Unified Paradigm: Integrating Recommendation Systems as a New Language in Large Models", "authors": ["Kai Zheng", "Qingfeng Sun", "Can Xu", "Peng Yu", "Qingwei Guo"], "abstract": "This paper explores the use of Large Language Models (LLMs) for sequential recommendation, which predicts users' future interactions based on their past behavior. We introduce a new concept, \"Integrating Recommendation Systems as a New Language in Large Models\" (RSLLM), which combines the strengths of traditional recommenders and LLMs. RSLLM uses a unique prompting method that combines ID-based item embeddings from conventional recommendation models with textual item features. It treats users' sequential behaviors as a distinct language and aligns the ID embeddings with the LLM's input space using a projector. We also propose a two-stage LLM fine-tuning framework that refines a pretrained LLM using a combination of two contrastive losses and a language modeling loss. The LLM is first fine-tuned using text-only prompts, followed by target domain fine-tuning with unified prompts. This trains the model to incorporate behavioral knowledge from the traditional sequential recommender into the LLM. Our empirical results validate the effectiveness of our proposed framework.", "sections": [{"title": "1 Introduction", "content": "The field of sequential recommendation (Fang et al., 2019; Wang et al., 2020) has long been focused on predicting users' future interactions with items based on their historical engagement sequences (Hidasi et al., 2015; Kang and McAuley, 2018; Tang and Wang, 2018). This task is crucial for enhancing user experience and satisfaction in various online platforms, such as e-commerce, streaming services, and social media. The ability to accurately predict what a user will interact with next can significantly improve the relevance of recommendations, thereby increasing user engagement and retention.\nRecently, the advent of Large Language Models (LLMs) (Zheng et al., 2023b; Touvron et al., 2023) has opened new avenues for sequential recommendation by conceptualizing it as a form of language modeling. This innovative approach leverages the powerful capabilities of LLMs to understand and generate human-like text, thereby offering a novel perspective on recommendation systems (Bao et al., 2023; Cui et al., 2022; Dai et al., 2023; Geng et al., 2023). LLMs, such as GPT-3 (Brown et al., 2020) and BERT, have demonstrated remarkable proficiency in capturing complex patterns and relationships within textual data, making them well-suited for the task of sequential recommendation.\nTraditional methods in sequential recommendation have typically represented items within LLMs' input prompts as either ID indices (Geng et al., 2023; Hua et al., 2023) or textual metadata (Bao et al., 2023; Cui et al., 2022; Hou et al., 2023; Song et al., 2023; Li et al., 2023b). While these approaches have shown promise, they often fall short in encapsulating comprehensive world knowledge or demonstrating a deep understanding of user behavior. ID-based representations can be limited in their ability to convey rich semantic information about items, while textual metadata may not fully capture the nuances of user interactions and preferences.\nTo address these limitations, we propose a paradigm-shifting framework that integrates recommendation systems as a new language within large models, termed \"Integrating Recommendation Systems as a New Language in Large Models\" (RSLLM). The novelty of RSLLM lies in its unified prompting method, which combines ID-based item embeddings, learned by conventional recommendation models, with textual item features. By treating the \"sequential behaviors of users\" as a distinct language beyond text, RSLLM introduces a projector to align the traditional recommender's ID embeddings with the LLM's input space. This innovative approach allows the model to seamlessly incorporate behavioral knowledge from traditional sequential recommenders into the LLM, thereby enhancing its ability to predict user interactions more accurately.\nFurthermore, we propose a two-stage fine-tuning framework for LLMs that introduces the alignment of user and item representations through a two-tower contrastive training approach. This framework refines a pretrained LLM using a combination of two contrastive losses and a language modeling loss. The initial stage of fine-tuning employs text-only prompts, aligning with the LLM's inherent language modeling capabilities. Subsequently, the framework undergoes target domain fine-tuning with unified prompts, effectively integrating behavioral knowledge from traditional recommenders. This two-stage process ensures the LLM's ability to leverage both textual and behavioral information, resulting in a robust and accurate recommendation system. At the ID level, we align the traditional recommender's ID embeddings with the LLM's input space using a projector, effectively integrating user behavioral patterns. At the token level, the LLM processes textual item features, utilizing its extensive world knowledge to enhance the recommendation process. At the user/item level, we employ a two-tower contrastive learning method to seamlessly incorporate behavioral knowledge from the traditional sequential recommender into the LLM, enabling effective understanding and prediction of user behaviors based on their historical engagement sequences.\nEmpirical results (Harper et al., 2016; Kang and McAuley, 2018; Cantador et al., 2011) substantiate the efficacy of our proposed framework, demonstrating significant improvements in prediction accuracy and user satisfaction. By integrating recommendation systems as a new language within large models, RSLLM represents a significant step towards a unified paradigm in sequential recommendation, offering a novel and effective approach to capturing user behavioral patterns and world knowledge. This paradigm shift has the potential to revolutionize the field of recommendation systems, paving the way for more intelligent and context-aware recommendations that better serve users' needs and preferences.\nContributions of this work are three-fold: (1) To the best of our knowledge, it is the first work to investigate the multi-granularity (ID, token, user/item) alignment of pre-trained large language"}, {"title": "2 Related Work", "content": "Large Language Models LLMs like GPT-4 (OpenAI et al., 2024) and Llama (Touvron et al., 2023) have transformed fields such as natural language processing, machine learning, and information retrieval (Ouyang et al., 2022; Zhao et al., 2023; He et al., 2023; Xu et al., 2023; Guo et al., 2023; Zheng et al., 2023a; Li et al., 2023a). Pretrained on extensive text and fine-tuned for specific tasks, LLMs excel at capturing complex patterns and relationships in textual data. They can generate human-like text and understand intricate semantic relationships (Luo et al., 2023; Ma et al., 2023; Hu et al., 2023; Zhu et al., 2023; Wu et al., 2023). We utilize these capabilities to treat sequential recommendation as a form of language modeling.\nLLMs for Sequential Recommendation LLMs are a new direction in sequential recommendation systems (Hou et al., 2024; Li et al., 2023b; Liu et al., 2023; Zhang et al., 2024; Zhao et al., 2024). They address the limitations of traditional methods like collaborative filtering and content-based methods (Wu et al., 2022; Hidasi et al., 2015) by treating sequential recommendation as language modeling. Several studies represent items in prompts as either ID indices or textual metadata. For example, Geng et al. (2023) use an ID number for each item, while Hua et al. (2023) use a randomly-initialized ID token. Other models use textual metadata such as titles (Bao et al., 2023; Cui et al., 2022) and descriptions (Hou et al., 2023; Li et al., 2023b). These models have improved recommendation accuracy. However, existing approaches often fall"}, {"title": "3 Problem Formalization", "content": "Task Formulation. Given a user u \u2208 U who has an interaction sequence that consists of a sequence of n items U = {I1, I2, ..., In} in chronological order, predict the next item In+1, where n is the length of U and I \u2208 I. Each item I has its corresponding ID and text information (e.g. title, etc.)."}, {"title": "4 Methodology", "content": "This section introduce the two important components in proposed RSLLM, including i) Unified Prompting Method; ii) Two-Stage Fine-Tuning Framework. Figure 1 shows the overall RSLLM."}, {"title": "4.1 Unified Prompting Method", "content": "4.1.1 Prompt Construction\nThe proposed method utilizes titles to describe items and uses item titles from historical interactions to describe users. Uniquely, to integrate collaborative information, we introduce additional user and item ID-related fields.\nText-ID Prompting. For the ID numeric representation of the RS, we introduce the Text-ID Prompting approach for Large Language Model (LLM) instruction tuning. Items are represented via their textual metadata and ID numeric data within the prompt, as illustrated as follows (Example 4.1):"}, {"title": "4.1.2 Hybrid Encoding", "content": "The Hybrid Encoding component is used to convert the input prompt into latent vectors, i.e., embeddings suitable for LLM processing. We employ a hybrid encoding approach, where for all textual content, we use the LLM's built-in tokenization and embedding mechanism to convert it into tokens and subsequent token embeddings. In contrast, when dealing with the item ID fields, we leverage an Adapter module, as illustrated in Figure 1(c), built with a conventional collaborative recommender, to extract collaborative information for the LLM to utilize.\nFormally, for a prompt corresponding to the sample(u, i) \u2208 D, we first use the LLM Tokenizer to tokenize its textual content. The tokenization result is denoted as P = {t1, t2,...,tn,i,...,tk} where te represents a text token, and i signifies the item (ID) placed within the respective fields. We then further encode the prompt into a sequence of embeddings: E = {et\u2081,...,Ctp,Ci, ..., @tk }, where et \u2208 R1xd denotes the token embedding for te in the LLM with dimension d, obtained via embedding lookup, and the embeddings for the item IDs, denoted as e\u00a1 \u2208 R1\u00d7d, are obtained via the Adapter module.\nTo facilitate alignment, we project the ID-based item representation e into the LLM space using a trainable projector, Proj (i.e., two-layer perceptions). This results in a behavioral token representation, (en) = Proj(e;0p), where \u03b8p are the parameters of the projector."}, {"title": "Hybrid Token Representation", "content": "Upon obtaining the textual tokens (e) and the behavioral token (es) for item i, we integrate these two components. This integration provides a comprehensive representation of item i, effectively combining the distinct yet complementary aspects captured by each token. A specific concat (ef) is used in this process (where the c token indicates that the following subsequence is a representation of the recommendation ID):\n(en) = Concat((ex); (ef); (en)) (1)"}, {"title": "Algorithm 1 Two-Stage Optimization Algorithm", "content": "Require: s: number of training iterations\n1: D: dataset\n2: M: model\n3: N-1\n4: H1 \u2190 GEN(D, I) \u25b7 Text-ID Prompting\n5: Ho \u2190 GEN(H1, O) \u25b7 Hybrid Prompting\n6: for i = 1 to s do\n7: Update M by minimizing the loss function in Eq.5 on H1\n8: end for\n9: for i = 1 to s do\n10: Update M by minimizing the loss function in Eq.5 on Ho\n11: end for\n12: return M"}, {"title": "4.2 Two-Stage Fine-Tuning Framework", "content": "Our Two-Stage Fine-Tuning Framework refines a pretrained LLM using two contrastive losses and a language modeling loss. It consists of two stages:\nText-Only Fine-Tuning The LLM is fine-tuned using text-only prompts, aligning with its inherent"}, {"title": "4.2.1 Contrastive Alignment", "content": "We now delve into the methodology for optimizing the model's parameters. The primary objective we employ for fine-tuning Large Language Models (LLMs) is the Next Item Prediction (NIP) Objective. The NIP objective is designed to predict the textual description of the subsequent item based on the historical sequence of items described in text. Let us represent the tokenized user sequence as as(u\u2081, u2, ..., Un)}, where n denotes the length of the sequence. The first m tokens correspond to all items except the last one, with the remaining m tokens dedicated to the target item. Our proposed objective for adapting LLMs to sequential recommendation is formulated as follows:\nLN = -\\sum_{j=m+1}^{n} [logmo(P(uj|u1:j\u22121;0))] (2)\nwhere @ encompasses all trainable parameters within the LLM.\nTo address the limitation of NIP, which operates on a token level rather than an item/user level, we introduce Contrastive Alignment. This involves an auxiliary contrastive objective that functions at the item/user level. We employ a two-tower training framework: one tower processes the target item, yielding a mean-pooled feature g\u00b9, while the other incorporates the entire user sequence, resulting in features gu for the user history and glU for the target item conditioned on the user history. We experiment with two contrastive losses for user-and item-level alignments, both inspired by the InfoNCE loss, a robust choice in contrastive learning (van den Oord et al., 2018; Li et al., 2022, 2023c; Gao et al., 2021; Chen et al., 2020):\nL1 = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{e^{(cos(g_i^lg_i^1)/T_c)}}{\\sum_{j=1}^{N}e^{(cos(g_i^lg_j^1)/T_c)}} (3)\nLu = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{e^{(cos(g_i^ug_i^U)/T_c)}}{\\sum_{j=1}^{N}e^{(cos(g_i^ug_j^U)/T_c)}} (4)\nwhere in-batch negative samples are used, N represents the batch size, cos(\u00b7, \u00b7) denotes the cosine similarity, and Tc is the temperature parameter for contrastive alignments. Our final training objective combines LN with the contrastive losses as depicted in Figure1c:\nL = LN + \u03b3LI + \u03b2LU (5)"}, {"title": "5 Experiments", "content": "This section first introduces the experimental settings in Section 5.1, and then presents the main experimental results in Section 5.2. Ablation studies were conducted in Section 5.3. In Section 5.4, we compare RSLLM with different representation and recommendation models."}, {"title": "5.1 Experimental Setup", "content": "Following the experiment setting in LLaRA (Liao et al., 2024), we conduct experiments on three real-world datasets:MovieLens (Harper et al., 2016), Steam (Kang and McAuley, 2018), and LastFM (Cantador et al., 2011), detailed statistics of the datasets are provided in Table 3. For each benchmark, we conduct experiment following (Liao et al., 2024): For each sequence, we randomly select 20 non-interacted items to construct the candidate set, ensuring the inclusion of the correct subsequent item. RSLLM and other baseline models aim to identify the correct item from this candidate set, and their performance is evaluated using the HitRatio@1 metric. And LLM-based metric: valid ratio. It quantifies the proportion of valid responses (i.e., items in the candidate set) across all sequences, serving as a measure of the models' capability of instruction following. We repeated the experiment 5 times and averaged the results according to the previous works (Kang and McAuley, 2018; Harper"}, {"title": "5.2 Main Results", "content": "As shown in Table 2, our proposed RSLLM significantly outperforms all baseline models across the three datasets in terms of HitRatio@1 and ValidRatio metrics. Specifically, on the MovieLens dataset, RSLLM surpasses the best baseline by 4.3% in HitRatio@1. Similar trends are observed on the other two datasets, with RSLLM exceeding the next best method by 3.0% on Steam and 4.8% on LastFM in"}, {"title": "5.3 Ablation Study", "content": "Item Representation As shown in Table 4, without Textual Feature Representation, we directly fine-tune the model on behavioral data without using textual features such as titles and descriptions. Without Item ID Representation, we remove the item ID representations, which are crucial for capturing the unique identity and sequential relationships of items. Without IID Tokens, we limit the model by not using the <IID> tokens, which typically represent the IDs of the items in the dataset. Without Pre-loading Item Embeddings, we disregard the preloaded embeddings that are typically used to inject prior knowledge about items into the model. As shown in the results, the fully fine-tuned PLMs without textual feature representation perform worse than our proposed RSLLM method (16.8% HitRatio@1 lower average, especifically drop 27.5% HitRatio@1 and 9.7% ValidRatio in LastFM ), showing the positive contribution of textual features for accurate recommendations. Further, removing item ID representation or IID tokens also delegate the performance by 7.4% and 6.4% average, showing the importance of using these components to learn a reasonable item representation. Similarly, without pre-loading item embeddings, the model achieves similar performance as when the embeddings are included. It is recommended to directly train the item representation parameters.\nContrastive Alignment Next, we show the effect of different user-item alignment strategies in our RSLLM framework. The RSLLM (U-I Only) and RSLLM (I-I Only) configurations only retain the user-tower to target-tower alignment and the target-tower to target-tower alignment, respectively. As shown in Table 4, these two aligments successfully boost up the model performance(2.3% HitRatio@1 gain average compare with Basline, especifically in MovieLens and LastFM ). However, their corresponding models perform worse than the ones supported by the full RSLLM. This shows that aligment from different views provide meaningful and different training signals to the models. Interestingly, models trained on the Target-Tower to User-Tower Alignment (I-I Only) perform better than the ones trained on the User-Tower to Target-Tower Alignment (U-I Only), indicating that the reciprocal alignment from the user-tower to the target-tower might be more instrumental in capturing the bidirectional relationship between user history and item preferences. Finally, the trained model removing Contrastive Alignmen perform worse than RSLLM (3.1% HitRatio@1 lower average), showing the importance of Contrastive Alignment.\nImpact of Two-stage Training Finally, we examine the effect of the two-stage training framework in our RSLLM model. In Table 4, we show the model performance with only the first stage (Stage 1 Only), only the second stage (Stage 2 Only), and with both stages (RSLLM Two-stage). The two-stage training has an important effect on the model performance. Without integrating behavioral knowledge, the performance gap almost disappears. The sequential fine-tuning also has a positive effect on the model performance. In particular, in the MovieLens dataset, the model performance increases significantly after the second stage. This shows that the initial text-only fine-tuning provides a necessary foundation for the subsequent target domain fine-tuning to be most effective. The results validate the hypothesis that a phased approach, which first establishes a strong textual understanding and then refines this with behavioral knowledge, leads to the most accurate and robust sequential recommendation model."}, {"title": "5.4 Discussions", "content": "Evaluation of item representation methods We conduct a comprehensive empirical evaluation of prevalent item representation methods in sequential recommendation tasks, including numerical indexing, behavior tokens, text feature representation, and the unified representation utilized in our RSLLM framework. As shown in Figure 3, the results demonstrate that the item representation method employed by RSLLM outperforms the other approaches in terms of HitRatio@1 across all three datasets. This not only validates the effectiveness of RSLLM's innovative item representation technique, but also highlights the advantages of its more comprehensive alignment between the LLM and the recommendation system, compared to conventional single-faceted item representation methods.\nRegarding the limitations of the individual methods: Numerical Indexing: For numerical indexing, LLMs do not initially store any inherent information. These indices are processed as plain text by the LLMs, causing the tokenizer to divide them into multiple tokens, which may limit the model's understanding. Behavior Token: When using behavior tokens, LLMs primarily exploit the distribution of the input behavioral embeddings, without effectively extracting the knowledge encapsulated within the LLMs. Text Feature: In the case of text features, the absence of user behavior patterns allows the LLM to solely infer the correlations among items in a user's historical interactions, guided solely by the background knowledge of these items preserved in the LLM. In contrast, RSLLM's unified item representation integrates both world knowledge and sequential information, thereby enhancing performance in sequential recommendation. By fusing item ID, behavior tokens, and text tokens, RSLLM is able to capture a more comprehensive representation of items, which leads to superior recommendation capabilities compared to the other approaches.\nDiscussion for Different recommendation model We evaluate our proposed RSLLM framework using item embeddings derived from three traditional sequential recommendation backbones: GRU4Rec, Caser, and SASRec. These models represent the three main categories of recommendation models : RNN-based, CNN-based and self-attention-based. The empirical results in Figure 2 and Appendix A.1 show that RSLLM outperforms all baseline models(RS, GPT-4, LLM-based model) and using SASRec as the backbone achieves the best performance, outperforming the other backbones. This validates that self-attention is better able to capture both local and global dependencies in user-item interactions compared to RNNs and CNNs. However,"}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel and effectiveness framework: RSLLM. Experiments on various benchmarks show the effectiveness of RSLLM. In the future, we plan to expand RSLLM to other recommendation tasks, like conversational recommendation and multi-modal recommendation."}, {"title": "Limitations", "content": "The RSLLM fine-tuning process and the integration of ID-based item embeddings with textual item features can be computationally intensive, which requires large GPU memory."}, {"title": "A Appendix", "content": "A.1 GRU4Rec and SASRec result in Figure 3\nFigure 4 and Figure 5 presents the results of recommendation model efficiency analysis with GRU4Rec and SASRec in the Figure 3."}]}