{"title": "Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks", "authors": ["Ibrahim Abdelaziz", "Kinjal Basu", "Mayank Agarwal", "Sadhana Kumaravel", "Matthew Stallone", "Rameswar Panda", "Yara Rizk", "GP Bhargav", "Maxwell Crouse", "Chulaka Gunasekara", "Shajith Ikbal", "Sachin Joshi", "Hima Karanam", "Vineet Kumar", "Asim Munawar", "Sumit Neelam", "Dinesh Raghu", "Udit Sharma", "Adriana Meza Soria", "Dheeraj Sreedhar", "Praveen Venkateswaran", "Merve Unuvar", "David Cox", "Salim Roukos", "Luis Lastras", "Pavan Kapanipathi"], "abstract": "Large language models (LLMs) have recently shown tremendous promise in serving as the backbone to agentic systems, as demonstrated by their performance in multi-faceted, challenging benchmarks like SWE-Bench and Agent-Bench. However, to realize the true potential of LLMs as autonomous agents, they must learn to identify, call, and interact with external tools and application program interfaces (APIs) to complete complex tasks. These tasks together are termed function calling. Endowing LLMs with function calling abilities leads to a myriad of advantages, such as access to current and domain-specific information in databases and knowledge sources, and the ability to outsource tasks that can be reliably performed by tools, e.g., a Python interpreter or calculator. While there has been significant progress in function calling with LLMs, there is still a dearth of open models that perform on par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this work, we introduce the GRANITE-20B-FUNCTIONCALLING1 model under an Apache 2.0 license. The model is trained using a multi-task training approach on seven fundamental tasks encompassed in function calling, those being Nested Function Calling, Function Chaining, Parallel Functions, Function Name Detection, Parameter-Value Pair Detection, Next-Best Function, and Response Generation. We present a comprehensive evaluation on multiple out-of-domain datasets comparing GRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open models. GRANITE-20B-FUNCTIONCALLING provides the best performance among all open models on the Berkeley Function Calling Leaderboard and fourth overall. As a result of the diverse tasks and datasets used for training our model, we show that GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks in seven different evaluation datasets.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have garnered significant attention due to their broad applicability to an important set of challenging domains, e.g., programming (Mishra et al., 2024; Roziere et al., 2023), reasoning (Reid et al., 2024; Jiang et al., 2023), and multi-modal interaction (Reid et al., 2024). Increasingly, applying these models to solve real-world problems requires them to act as autonomous agents powering intelligent decision-making"}, {"title": "2 Related Work", "content": "2.1 Instruction Tuning\nOur work is an instantiation of instruction tuning (Wei et al., 2021), a fine-tuning method that improves an LLM's ability to solve natural language tasks (Mishra et al., 2022; Wang et al., 2023). It involves taking a large collection of NLP datasets, reformulating those datasets into a set of instruction-following tasks, and then fine-tuning an LLM on the modified data. While the earliest versions of instruction tuning straightforwardly combined large datasets together, the most recent iterations use more sophisticated mixtures of tasks to achieve the best results (Li et al., 2024; Sudalairaj et al., 2024). Our work draws largely upon API-Blend (Basu et al., 2024) and API Pack (Guo et al., 2024), two recently introduced instruction-tuning datasets specifically focused on tasks related to APIs, e.g., slot filling and API intent detection.\n2.2 Function Calling by LLMs\nFunction calling augmentation has broadened the scope of problems addressable by LLMs to include those that cannot be solved with internal knowledge alone. For instance, prior work has demonstrated the use of API-enhanced LLMs to solve problems requiring up-to-date information retrieval (Schick et al., 2023), intricate mathematical calculations (He-Yueya et al., 2023; Patel et al., 2021), internet use (Komeili et al., 2022; Gur et al., 2023), task orchestration (Jain et al., 2024), and even programming abilities (Gao et al., 2023).\nMultiple strategies have been proposed for how best to enable LLM function calling. One line of prior research has investigated the design of elaborate prompting approaches, best exemplified by the popular ReACT prompting framework (Yao et al., 2022). Such prompting methods can vary in their design, with some works optimizing for cost (Xu et al., 2023a), raw performance (Shinn et al., 2023; Yang et al., 2023), or a blend of both (Crouse et al., 2023). More relevant to our approach are methods that train models to directly output function calls (Tang et al., 2023; Qin et al., 2023). Typically, these works will use some form of self-supervision to enable scaling to the breadth of domains required for general-purpose function use (Schick et al., 2023; Parisi et al., 2022; Yang et al., 2024b).\nRecently, many language models with function-calling capabilities have been introduced. They broadly fall into two categories: pre-trained models which are capable of function-calling (Reid et al., 2024; CodeGemma Team et al., 2024; CohereForAI, 2024; AI@Meta, 2024; Jiang et al., 2023), and models fine-tuned specifically for function-calling (Qin et al., 2023; Tang et al., 2023; MeetKai, 2024; Patil et al., 2023; Nous-Research, 2023; Nexusflow.ai, 2023). While the pre-trained models enable function-calling using a combination of supervised and preference fine-tuning, details of the datasets used to train models for these tasks are not generally available. On the other hand, specialized function-calling models mostly rely on synthetic data generated from proprietary state-of-the-art models. Models like Gorilla (Patil et al., 2023), ToolLlama (Qin et al., 2023), ToolAlpaca (Tang et al., 2023), and the NousResearch Hermes series of models (Nous-Research, 2023) utilize GPT-4 or ChatGPT to generate synthetic instruction tuning datasets and fine-tune a base model such as the Llama or Mistral model for function-calling tasks. The NexusRaven models (Nexusflow.ai, 2023) are one of the few open-source models that focus on building function-calling models for commercial purposes by avoiding using proprietary models for synthetic data generation.\nIn section 5, we compare our model to the above models and show that GRANITE-20B-FUNCTIONCALLING provides the best or comparable performance amongst all open models across multiple tasks."}, {"title": "3 Multi-Task Training Data", "content": "In this section, we describe our detailed approach to fine-tune the GRANITE-20B-CODE-INSTRUCT model with multi-task data related to functions to build GRANITE-20B-FUNCTIONCALLING, a robust model designed for function-calling. We use API-BLEND (Basu et al., 2024), a diverse corpora of multiple API datasets for training LLMs. It consists of five datasets with a total of about 160K training examples: SeqSGD, SeqSNIPS, Seq TopV2, SeqATIS, and SeqMultiWOZ.\n3.1 Data Unification\nIn addition to the datasets in API-BLEND (Basu et al., 2024) (i.e., SeqSGD, SeqSNIPS, Seq TopV2, SeqATIS, SeqMultiWOZ), we also use Glaive-V29 to prepare the training data mixture for GRANITE-20B-FUNCTIONCALLING, where each dataset is multi-purposed for different function calling related tasks with different instructions. These datasets come from different sources and have various function formats that require unification to an identical representation for better usability.\nIn this unification process, we convert all the APIs, tools, and functions from the data into a JSON format representation. We choose this format because (a) JSON is a language-independent, human-readable, and widely used data format for code-related tasks; (b) it is easily parsable to insert/extract information; and (c) many web services, APIs, and tools accept JSON objects and generate responses in JSON format.\nIn GRANITE-20B-FUNCTIONCALLING, we unify the model output representation of function calls to the following format:\n3.2 High-Level Function Calling Tasks\nIn general, these tasks are challenging for LLMs to accomplish since they require the LLM to generate multiple function calls with the parameters and their values. In the following sections, we describe different types of high-level function-calling tasks with examples.\nNested Function Calling The main characteristic of this task is in the output function sequence, where the current function's output becomes an input to the next function. So, the answer to a user query is a sequence of nested function calls selected from the function library. Furthermore, the parameters of these function calls need to be filled by extracting the values from the user query. The example below lists two functions in the output under <function_call> tag 10 and the output of the first function will populate the destination parameter in the second function.\nFunction Chaining In this task, a model needs to call multiple functions in a sequence to answer a user query. However, unlike Nested Function Calling, these functions do not have to be nested. Also, for each function, the parameters whose values are present in the user query must be passed as arguments. The following example demonstrates how three functions must be called in sequence to answer the query.\nParallel Functions Similar to the Function Chaining task, here, the answer to a user query requires the same function to be called multiple times (in parallel). Also, the parameters should be mapped with the values extracted from the user query. In the below example, the get_weather function has been called twice with different parameter-value pairs to answer the user query.\n3.3 Low-Level Function Calling Tasks\nCompared to High-Level tasks, these tasks are simpler because they either request function names (without parameters) or parameter-value pairs for a function. To excel in High-Level function calling tasks, it is crucial for any LLM to master these foundational sub-tasks. In what follows, we demonstrate three types of low-level function-calling tasks.\nNext-Best Function In this task, given the function library along with the user query and the partial function sequence, the models are supposed to select the next most suitable function from the function library. It only requires the model to choose one function name without any parameters. The following example demonstrates how the partial function sequence (provided under <|partial_function_call_list|>) can be passed with the query and the model is supposed to predict the next best function.\nFunction Name Detection This task expects the model to produce only the sequence of function names (without parameters) from the function library that are required to answer the user query. This task closely resembles Function Chaining (a High-Level task), with the sole distinction being it does not necessitate the model to populate the function's arguments. Below is an example of this task.\nParameter-Value Pair Detection In this task, when provided with a user query or a user-agent conversation along with a list of parameters and their descriptions, the model must identify all the parameters for which the values are present in the query or conversation. See an example of this task below. To keep the data format consistent and JSON parsable, we use the function name - \u201cdummy\u201d for all the data under this task."}, {"title": "3.4 Response Generation", "content": "Natural language response generation is a crucial feature of any LLM. In this task, the model must comprehend an ongoing conversation between a user and an AI assistant. Then, it generates a natural language response, answering the most recent user utterance. Such responses are needed to chit-chat with the user, ask clarifying questions, or synthesize a function call's output into a natural language response."}, {"title": "4 Instruct Tuning", "content": "4.1 Training Data Mixture Creation\nAfter generating the data for various tasks, the next step is to create a training data mixture including all the data. We programmatically generate the mixture of data by following a weighted configuration for datasets and tasks. Following is an example of the weighted configuration, where the total mixture samples will be divided between Function Chaining and Next-Best Function in a 3:5 ratio. Within the Function Chaining portion, the allocation is split between SeqSGD and Glaive-V2 in a 2:3 ratio. Similarly, the Next-Best Function chunk will be divided in a 2:1 ratio between SeqTopV2 and SeqSNIPS.\n4.2 Training\nGRANITE-20B-FUNCTIONCALLING is instruct-tuned version of GRANITE-20B-CODE-INSTRUCT (Mishra et al., 2024)11. For training data, we created a mixture of 142K examples spanning all the tasks' datasets discussed above. We then trained our model using QLoRA fine-tuning (Dettmers et al., 2023) based on our multi-task training mixture discussed above. In particular, we trained GRANITE-20B-FUNCTIONCALLING a QLoRA rank of 8, alpha of 32 and a dropout of 0.1. We also used a learning rate of 5e-5 and ApexFusedAdam as our optimizer with a linear learning rate scheduler. Training was done using a single node of 8 A100_80GB GPUs with 800GB of RAM for a total of 3 epochs."}, {"title": "5 Experimental Setup and Evaluation", "content": "In the section below, we detail our extensive evaluation on various evaluation datasets and public leaderboard. We provide a comprehensive comparison of our GRANITE-20B-"}, {"title": "5.1 Datasets", "content": "The evaluation datasets and leaderboards for function calling are gaining a lot of traction in the recent past. In particular, to evaluate the models' generalizability, we evaluated GRANITE-20B-FUNCTIONCALLING on a variety of function calling benchmarks, all of which are out-of-domain evaluation for our model. It is worth noting that some of these datasets; e.g. ToolAlpaca and ToolLLM, have training data releases. However, we did not use any of these benchmarks to train GRANITE-20B-FUNCTIONCALLING and we only used the datasets in 1. 12 Table 3 depicts the details of the evaluation datasets we used. We list the details of each of these evaluation datasets below.\n(1) Berkeley Function-Calling Leaderboard (BFCL) 13 is a comprehensive function calling leaderboard that includes a dataset of over 1,700 instances. The leaderboard evaluates tasks that include (a) Simple Function, Multiple Function, Parallel Function, and Parallel Multiple Function for Python Language; and (b) for non-Python, they evaluate function relevance detection, REST API, JavaScript, and Java.\n(2) ToolBench (Xu et al., 2023b) is a subset of the data in ToolBench (as released by the authors) focused on HomeSearch and Booking domains.\n(3) ToolLLM (Qin et al., 2023)14 is synthetically generated using ChatGPT. The approach uses an initial collection of 16,000 APIs from RapidAPI15 for synthetic data generation. The evaluation is done on the three test sets categorized based on complexity; G1 \u2013 single-tool, G2 \u2013 intra-category multi-tool, G3 \u2013 intra-collection multi-tool.\n(4) RestGPT Song et al. (2023) is a function calling dataset that has 157 test examples with 85 APIs from Spotify and TMDB. This dataset focuses only on testing model's ability to detect function names.\n(5) API-Bank (Li et al., 2023b) has 314 tool-use dialogues with 753 API calls to assess LLMs' capabilities in planning, retrieving, and calling APIs.\n(6) ToolAlpaca (Tang et al., 2023) is a synthetic data generation approach that has both training and evaluation benchmarks. It contains 271 tool-use instances spanning 50 distinct categories. Similar to Nexusflow.ai (2023), we used the simulated part of ToolAlpaca which has a total of 100 test examples.\n(7) NexusRaven API Evaluation16 is another function calling dataset with 318 test examples covering a total of 65 different APIs."}, {"title": "5.2 Evaluation Metrics", "content": "Below, we define the metrics we adopted for specific tasks in function calling.\nBFCL Metrics17: BFCL evaluates multiple tasks using the following four metrics.\n(1) AST summary compares the abstract syntax tree of the function output to the ground truth and the function definition. It captures the correctness of the functions called, their parameters (required or not), and the parameter types.\n(2) Execution Summary compares the execution output from generated and ground-truth function calls. This metric is used to evaluate REST APIs and non-REST data samples.\n(3) Relevance evaluates the model's ability to detect no function calls when the given list of functions is irrelevant to the user query. This inversely captures the hallucination rate of models.\n(4) Overall Accuracy is the weighted average of all individual data splits in BFCL.\nThe same metrics described above cannot be used for our out-of-domain datasets because of missing information, varied formats, and response generation task. For example, ToolLLM datasets has missing arguments, ToolAlpaca has missing argument types, and API-Bank has response generation task. Therefore, we use the following metrics to evaluate the models on other datasets:\nF1 measure: Based on Basu et al. (2024), we opted for standard metrics like precision, recall, and F1 scores which focus on exactly matching API and parameters' names. The reason behind this is that APIs are very specific and unless everything (e.g., name, parameters, input/output format, etc.) matches the API specifications, executing such APIs will not be possible. We report F1 for matching function names as well as parameter names and values.\nLongest Common Subsequence (LCS) and Exact match: We also used LCS from (Basu et al., 2024) to capture the overlap between the gold and predicted sequences of APIs. This allows us to compute models' ability to predict APIs in the correct sequence as required by the user. Similarly, exact match score (Basu et al., 2024) checks if all APIs are predicted by the model and are in the same order.\nBERTScore, ROUGE-L and BLEU: We follow the evaluation in API-Bank (Li et al., 2023b), a dialog dataset that also evaluates model responses based on language generation metrics such as Rouge-L (Lin, 2004), BertScore (Zhang et al., 2019), and BLEU (Papineni et al., 2002).\nHallucination Rate: We compute the hallucination rate as the number of samples where the model predicted an API not provided in the function library."}, {"title": "5.3 Evaluation Results", "content": "Tables 4, 5, 6, 7, and Figure 3 depicts an extensive evaluation of GRANITE-20B-FUNCTIONCALLING model in comparison to other state of the art function calling models. In order to detail this evaluation and analyses, below we categorize the results into (a) Berkeley Function Calling Leaderboard Evaluation, and (b) Function calling academic benchmarks.\n5.3.1 BFCL Leaderboard Evaluation Results\nTable 4 shows that GRANITE-20B-FUNCTIONCALLING is ranked fourth on the overall accuracy metric among the top 15 models on BFCL and is highest among models with open licenses18. While it is tied with the Gorilla (Patil et al., 2023) model, it is important to note that the latter was finetuned on data that are (a) generated from ChatGPT, and (b) similar data to the test set and hasn't generalized well to other datasets as shown in Table 5 and Figure 3. In the context of model sizes, GRANITE-20B-FUNCTIONCALLING is one of the smallest\n5.3.2 Function Calling Academic Benchmarks\nTables 5 and 6 focus on evaluating the models' performance on Function Matching using F1-measure, LCS, and Exact Match. In this experiment, we reuse the model handlers from the BFCL code base, including the optimized prompts for each model. However, since the Cohere Command-R-v01 and Mistral-Instruct-v0.3 handlers available in BFCL use the REST API interface for inference, we reimplement handlers for these models, utilizing local models using prompts suggested by the respective model developers for function calling.\nFunction Name Detection: On ToolLLM datasets (G1, G2, and G3) and RestGPT, GRANITE-20B-FUNCTIONCALLING performs the best on detecting function names given a natural language utterance with 8% better F1 score than the next best function calling model, as shown in Table 5. Since these datasets have multiple functions in sequence, we also compute sequencing metrics; exact score and LCS. On this front, GRANITE-20B-FUNCTIONCALLING model also outperforms other function calling models by 7% on LCS and 11% on Exact Match scores.\nFull Function Calling: Table 6 reports on the models' performance on the API-Bank, ToolBench, and ToolAlpaca datasets that are out-of-domain and evaluated in a zero-shot manner. No single model outperforms all other models across datasets. Note that datasets like ToolAlpaca and API-Bank come with training data split which we never used for\n5.3.3 Response Generation\nIn Table 7, we show models' performance on response generation task. We use API-Bank dataset and follow their response generation task evaluation with BertScore, Rouge-L, and BLUE. Meta-Llama-3-70B-Instruct has the best performance across the three metrics with GRANITE-20B-FUNCTIONCALLING coming in close second (difference in performance ranged between 1-5%). Both models significantly outperform all other evaluated models. The gap is larger when we compare GRANITE-20B-FUNCTIONCALLING to the ones specifically trained for function calling such as Functionary-small-v2.5 and Gorilla-openfunctions-v2.\n5.3.4 Further Improvements\nWe have instruct-tuned the GRANITE-20B-FUNCTIONCALLING in such a way that it develops implicit function searching capability from a long list of functions. For example, in out-of-domain evaluation tasks, for ToolAlpaca the model needs to find the Function from"}, {"title": "6 Conclusion", "content": "In this paper, we introduced GRANITE-20B-FUNCTIONCALLING, a capable function calling open model with Apache 2 license. GRANITE-20B-FUNCTIONCALLING is trained using a suite of datasets transformed from semantic parsing, task-oriented dialog, personal assistants and conversational domains. The training setup is a multi-task learning approach where granular tasks in function calling such as function detection, parameter detection, sequencing, and next best function are used for instruction tuning the model. We performed an extensive evaluation of GRANITE-20B-FUNCTIONCALLING in comparison to other state-of-the-art function calling models. On multiple out-of-domain datasets, including Berkeley Function Calling Leaderboard, GRANITE-20B-FUNCTIONCALLING provides the best performance among the models that have open licenses. Even compared to multiple proprietary models with much larger sizes, GRANITE-20B-FUNCTIONCALLING showed on-par and in some cases better performance on multiple datasets and tasks."}]}