{"title": "GradINN: Gradient Informed Neural Network", "authors": ["Filippo Aglietti", "Francesco Della Santa", "Andrea Piano", "Virginia Aglietti"], "abstract": "We propose Gradient Informed Neural Networks (GradINNs), a methodology in-spired by Physics Informed Neural Networks (PINNS) that can be used to efficiently approximate a wide range of physical systems for which the underlying governing equations are completely unknown or cannot be defined, a condition that is often met in complex engineering problems. GradINNS leverage prior beliefs about a system's gradient to constrain the predicted function's gradient across all input dimensions. This is achieved using two neural networks: one modeling the target function and an auxiliary network expressing prior beliefs, e.g., smoothness. A customized loss function enables training the first network while enforcing gradient constraints derived from the auxiliary network. We demonstrate the advantages of GradINNS, particularly in low-data regimes, on diverse problems spanning non-time-dependent systems (Friedman function, Stokes Flow) and time-dependent systems (Lotka-Volterra, Burger's equation). Experimental results showcase strong performance compared to standard neural networks and PINN-like approaches across all tested scenarios.", "sections": [{"title": "Introduction", "content": "In the field of computational physics, Neural Networks (NNs) have become an increasingly important tool for modeling complex physical systems that cannot be derived in closed form or for which the traditional empirical models fail to achieve the desired accuracy [19, 6]. Several studies have shown how NNs are powerful function approximators, able to model a wide variety of large, complex, and highly non-linear systems with unprecedented computational efficiency when a large training dataset is available [11, 21, 26, 13]. However, in settings where data is limited or widely dispersed, NNs face considerable difficulties. For instance, in the physical sciences, data is often obtained experimentally and is thus expensive and/or challenging to collect. In such scenarios, NNs show a decreasing prediction performance and a higher probability to overfit to the training data compared to physics white/gray models. On the other hand, the latter suffer from lack of flexibility and expressivity (for instance 0/1-dimensional models) or can require high computational effort as Finite Element Method models [25]. In order to address these difficulties, Physics-Informed Neural Networks (PINNS) emerged in recent years [23]. These models leverage prior knowledge, often in the form of known differential equations (DE), by embedding it directly into the training process. Specifically, they introduce an additional term in the loss function which represents the residual of the underlying DE evaluated on a set of so-called collocation points. This formulation increases robustness against flawed data, e.g., missing or noisy values, and offers physically consistent predictions, particularly in"}, {"title": "Preliminaries", "content": "Notation We denote by $u(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ a function we aim to model with inputs given by $(t, \\mathbf{\\xi})$, where $t \\in \\mathbb{R}$ is time and $\\mathbf{\\xi} := [\\xi_1,..., \\xi_{d-1}] \\in \\mathbb{R}^{d-1}$ is the vector of spatial variables. To avoid cluttering notation we denote $u(\\cdot)$ by $u$ hereinafter. Let $u_t := \\partial u/\\partial t$ be the partial derivative of $u$ with respect to $t$ and $u_{\\xi_i} := \\partial u/\\partial \\xi_i$ be the partial derivative of $u$ with respect to the i-th spatial variable $\\xi_i$. Moreover, we denote by $\\nabla_{\\xi}$ the differential operator that returns the vector of partial derivatives of a function with respect to $\\mathbf{\\xi}$ and with $\\nabla_{\\xi\\xi}$ the operator for computing the matrix of second-order partial derivatives with respect to $\\mathbf{\\xi}$. Therefore, $\\nabla_{\\xi}u := [u_{\\xi_1},..., u_{\\xi_{d-1}}] \\in \\mathbb{R}^{d-1}$ and $\\nabla_{\\xi\\xi} u := (u_{\\xi_i\\xi_j}) = (\\partial^2 u/\\partial \\xi_i\\partial \\xi_j) \\in \\mathbb{R}^{(d-1)\\times(d-1)}$.\nAs originally formulated in Raissi [24], PINNS study physical systems governed by a known PDE of the general form:\n$u_t = N(t, \\mathbf{\\xi}, u, \\nabla_{\\xi} u, \\nabla_{\\xi\\xi} u, ...),$\nwhere $N$ represents a known potentially nonlinear differential operator that depends on $t$, on the spatial variables $\\mathbf{\\xi}$, on $u$ and on its derivatives with respect to $\\mathbf{\\xi}$. PINNS approximate the solution $u$ via a NN denoted by $U(\\cdot; \\Theta_U) : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ where $\\Theta_U$ are the network parameters. From Eq. (1) it is possible to define the function $f := u_t - N(t, \\mathbf{\\xi}, u, \\nabla_{\\xi} u, \\nabla_{\\xi\\xi} u, ...)$ and, by plugging in the network $U$, write:\n$f := U_t(t, \\mathbf{\\xi}; \\Theta_U) - N(t,\\mathbf{\\xi},U,\\nabla_{\\xi}U,\\nabla_{\\xi\\xi}U, ...).$\nwhere the derivatives $U_t, \\nabla_{\\xi} U, \\nabla_{\\xi\\xi} U, ...$ of the network $U$ are computed by automatic differentiation [2]. Given a training dataset $D = \\{(\\mathbf{t}_n, \\mathbf{\\xi}_n), u_n\\}_{n=1}^{N_u}$, including the initial and boundary conditions on $u$, and a set of collocation points $B = \\{(\\mathbf{t}_m,\\mathbf{\\xi}_m)\\}_{m=1}^{N_f}$, the parameters $\\Theta_U$ can be learned by minimizing the loss function $L$ defined as:\n$L(\\Theta_U) = L_U(\\Theta_U) + L_f(\\Theta_U)$\n$L_U(\\Theta_U) = \\frac{1}{N_u} \\sum_{n=1}^{N_u} (U(\\mathbf{t}_n, \\mathbf{\\xi}_n; \\Theta_U) - u_n)^2 + \\frac{1}{N_f} \\sum_{m=1}^{N_f} (f(\\mathbf{t}_m, \\mathbf{\\xi}_m; \\Theta_U))^2.$"}, {"title": "GradINN", "content": "The approach we propose in this paper, which we name Gradient Informed Neural Network (GradINN), targets systems of the form $u(x) \\in \\mathbb{R}^{d_o}$, where $x \\in \\mathbb{R}^{d}$ represents all the dimensions of the problem making no distinction between temporal and spatial variables. As done in Section 2, we denote by $\\nabla_x$ and $\\nabla_{xx}$ the differential operators for the first and second order derivatives with respect to all the dimensions of vector $x$ respectively. We first introduce the methodology for uni-dimensional outputs ($d_o = 1$) and then discuss the extension to multi-output systems ($d_o > 1$).\nGradINN deals with systems where the governing equation, i.e., Eq. (1), is unknown or cannot be defined, and only general behaviour for $\\nabla u$ can be assumed. Similar to Eq. (4), GradINN consists of two paired NNS, $U(\\cdot; \\Theta_U) : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and $F(\\cdot; \\Theta_F) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$. $U$ is used to approximate $u$ while $F$, taking $x$ as input and returning the beliefs about $\\nabla_x u$ as output, is used to apply implicit constraints on $\\nabla_x U$. In particular, the initialization of $F$ encodes general prior beliefs on the behavior of $\\nabla_x u$, e.g., smoothness, and, therefore, of $u$ itself. For instance, $F$ can be initialized to give constant outputs to encourage smoothness in the predicted solution given by $U$ (see below and Fig. 1 for a toy example). Given a training dataset $D_{TRAIN} = \\{(\\mathbf{x}_n, u_n)\\}_{n=1}^{N_u}$ and a set of collocation points $C = \\{(\\mathbf{x}_m)\\}_{m=1}^{M}$, we train the paired NNs simultaneously by considering the following loss function:\n$L(\\Theta_U, \\Theta_F) = L_U(\\Theta_U) + L_F(\\Theta_F; \\Theta_U) =$\n$L_U(\\Theta_U) + \\frac{1}{M} \\sum_{m=1}^{M} ||F(\\mathbf{x}_m; \\Theta_F) - \\nabla_x U(\\mathbf{x}_m; \\Theta_U)||^2 =$\n$L_U(\\Theta_U) + \\frac{1}{M} \\sum_{m=1}^{M} \\sum_{i=1}^{d} (F_i(\\mathbf{x}_m; \\Theta_F) - U_{x_i}(\\mathbf{x}_m; \\Theta_U))^2,$\nwhere the first component $L_U(\\Theta_U)$ is defined, similarly to Eq. (3), as $L_U(\\Theta_U) = \\frac{1}{N_u} \\sum_{n=1}^{N_u} (U(\\mathbf{x}_n; \\Theta_U) - u_n)^2$. Note how $L_U$ optimizes $\\Theta_U$ to have $U(\\mathbf{x}; \\Theta_U) \\approx u(\\mathbf{x})$ over $D_{TRAIN}$. At the same time, $L_F$ allows optimizing both $\\Theta_U$ and $\\Theta_F$ such that $F(\\mathbf{x}; \\Theta_F) - \\nabla_x U(\\mathbf{x}; \\Theta_U) \\approx 0$ for $x \\in C$. In particular, by updating $\\Theta_U$, this loss term encourages $U$ to have a gradient behavior similar to $F$. Simultaneously, it modifies $\\Theta_F$ to bring $F$ closer to $\\nabla_x U$, thereby relaxing the prior beliefs on $C$ using the gradient information derived from $D_{TRAIN}$ via $L_U$. As $C$ does not require access to the output values, it can be increased without the need to collect expensive experimental data but only accounting for the available computation resources or model efficiency considerations. Finally,"}, {"title": "Experiments", "content": "We test GradINNS on a well known synthetic function, that is the Friedman function (FRIEDMAN, $d = 5, d_o = 1$), and three physical systems featuring different characteristics in terms of time dependency, input and output dimension and smoothness of the gradients:\n\\begin{itemize}\n    \\item the Stokes Flow (STOKES), which can be used to describe the motion of fluids around a sphere, in\n    conditions of low Reynolds number ($Re < 1$). In this experiment $d_o = 1$, there is no dependency\n    of $u$ on time and $d = 2$.\n    \\item The Lotka-Volterra system (LV), an ODE model of predator-prey dynamics in ecological systems.\n    This system's $u$ depends on time, which is the only input ($d = 1$), and $d_o = 2$.\n    \\item The Burger's equation (BURGER), a challenging PDE studied in fluid mechanics to represents shock\n    waves and turbulence. In this case $u$ depends on time and a spatial variable ($d = 2$) and $d_o = 1$.\n    This equation is particularly challenging due to steep partial derivatives within the solution's\n    domain.\n\\end{itemize}"}, {"title": "FRIEDMAN", "content": "In this section we assess GradINN's performance on the five dimensional Friedman function [8]:\n$u(x) = 10 sin(\\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10x_4 + 5x_5 + \\epsilon$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma^2$ taking different values across experiments. We construct $D_{TRAIN}$ and $C$ by sampling input points (via Latin hypercube sampling) in the 5-dimensional unit hypercube and obtaining the corresponding output values via Eq. (9). The test dataset includes $N_{TEST} = 10^4$ points uniformly sampled in the 5-dimensional unit hypercube. We first demonstrate our approach on noise-free data ($\\sigma^2 = 0$) and then explore settings with increasing level of $\\sigma^2$.\nWe compare GradINN with varying number $M$ of collocation points against s-NN, s-NN with $l_1$ and $l_2$ regularization and ST, despite the different training dataset considered by the latter."}, {"title": "STOKES", "content": "Next, we test our proposed methodology on the Stokes flow [28], also known as creeping flow or viscous flow. This regime describes fluid motion at very low Reynolds numbers, where inertial forces are negligible compared to viscous forces. In the case of a sphere of radius $R$ (white circle in Fig. 4) moving through a fluid with relative far-field velocity $\\hat{\\mathbf{u}}_{\\infty}$, the Stokes flow permits a closed-form solution $\\mathbf{u}(\\mathbf{x}) \\in \\mathbb{R}^3$ at any point $\\mathbf{x} = [x_1, x_2, x_3]^T$. Denote by $\\mathbf{x} \\otimes \\mathbf{x}$ the tensor product of $\\mathbf{x}$ with itself, by $|\\|\\mathbf{x}\\||$ the Euclidean norm of $\\mathbf{x}$ and by $I$ the identity matrix, the Stoke flow is defined as:\n$\\mathbf{u}(\\mathbf{x}) = \\bigg( \\frac{3R}{4 |\\|\\mathbf{x}\\||} + \\frac{R^3}{4 |\\|\\mathbf{x}\\||^3} I - \\frac{3 R^3 \\mathbf{x} \\otimes \\mathbf{x}}{4 |\\|\\mathbf{x}\\||^5} \\bigg) \\hat{\\mathbf{u}}_{\\infty}.$\nWe reduce the input dimension for $\\mathbf{x}$ to $d=2$ by fixing $x_3 = 0$ and considering input values, for both $x_1$ and $x_2$, in [-5,5]. Similarly, we reduce the output dimension to one by taking the Euclidean norm $|\\|u\\||$ of each output vector and training both GradINN and s-NN on those. We fix $\\hat{\\mathbf{u}}_{\\infty} = [5,0,0]^T$. We generate $D_{TRAIN}$ by considering a regular grid of input values of size $N_u \\in [300, 500, 700]$ (see dotted white lines in Fig. 4). As the homogeneous Dirichlet boundary conditions are assumed to be known, we add 50 points along the sphere's perimeter section with $|\\|u\\|| = 0$ to this training dataset. $C$ is constructed by taking $M = 10^4$ points in the [-5, 5]^2 domain. Similarly, $D_{TEST}$ includes a regular grid of points $N_{TEST} = 4 \\times 10^4$ in the same domain. GradINN outperforms s-NN across all $N_u$ values, both in terms of output and gradients predictions"}, {"title": "LV", "content": "Moving to time-dependent systems, we test GradINN on the Lotka-Volterra (LV) system [3], described by the following ODEs:\n$\\begin{cases}x_t = \\alpha x - \\beta xy \\\\ y_t = \\delta xy - \\gamma y,\\end{cases}$\nwhere $x$ and $y$ represent the populations of two species, typically prey and predators respectively. The parameters $\\alpha$ and $\\beta$ characterize the prey dynamics while $\\gamma$ and $\\delta$ characterize the predator dynamics. As in Podina et al. [22], we numerically solve the system for $[\\u03b1, \\beta, \\gamma, \\delta] = [1.3, 0.9, 0.8, 1.4]$ with initial condition $[x_0, y_0] = [0.44249296, 4.6280594]$ and exploit the solution to generate $D_{TRAIN}$ with $N_u = 5$ and uniformly distributed input values in $t \\in [0,3]$. For GradINN we set $M = 1000$. GradINN gives more accurate predictions compared to s-NN, both in terms of the predicted output and gradients."}, {"title": "BURGER", "content": "Finally, we test GradINN's capability to solve PDES by considering the Burgers' equation [1]. For a given field $u(t, x)$ and kinematic viscosity $\\nu$ this is defined as $u_t + u u_x = \\nu u_{xx}$. We consider the initial conditions $u(0, x) = -sin(\\pi x/8)$ with $x \\in [-8, 8], t \\in [0, 10], \\nu = 0.1$ and known Dirichlet boundary conditions. We take the solutions of the PDE from [24]. This gives a set of $201 \\times 256$"}, {"title": "Conclusions and discussion", "content": "We introduced GradINN, a general methodology for training NN that (i) allows regularizing gradient of different orders using prior beliefs, (ii) does not require prior knowledge of the system and (iii) can be used across setting where input and output have different dimensions. Our extensive experimental comparison showed how GradINN can be used to accurately predict the behaviour of various physical systems in sparse data conditions. In particular, GradINN outperforms s-NN and performs similarly to PINN-like approaches by only using data and prior beliefs.\nLimitations In this work we focused on physical systems and consider smooth networks' ini-tializations. However, this might not be the optimal initialization for problems that exhibit local discontinuities or regions with very steep gradients. For example, as shown for BURGER, a smooth initialization can lead oversmooth predictions in areas where the solution is very steep. This highlights the need to define more complex priors to handle such conditions, as well as an effective way to embed such priors in the network $F$ (e.g., dedicated pretraining of the auxiliary network $F$). Additionally, GradINN incurs a higher computational cost compared to S-NN. Indeed, in addition to the standard forward and backward propagation required to update the weights of the networks, GradINN needs to differentiate the network to evaluate the gradient $U$ which translates into a higher computational cost.\nFuture Work This work opens up several promising avenues for future research. First, different variations of the proposed loss $L(\\Theta_U, \\Theta_F)$ could be considered. For instance, one could use a weighted sum of $L_U$ and $L_F$ with hyperparameters multiplying the two losses thus prioritizing training data vs prior beliefs (or viceversa). The loss could also be futher modified to incorporate a consistency loss as discussed in Appendix B. Finally, more work is needed to understand the impact"}, {"title": "Different network initializations", "content": "We repeat the experiment shown in Fig. 1 using a different initialization for both $U$ and $F$ to clarify the impact of a non-smooth initializations. We first consider a non-smooth initialization of $U$ but a smooth intialization of $F$ (top row of Fig. 7). In this setting, s-NN converges to a very wiggly solution (blue curve) that improves within the interval [-1,2] when considering the l2 norm. When GradINN is used, similarly to Fig. 1, the smoothness of $F$ allows recovering a smoother solution with fewer oscillations (red line). When instead the $F$ initialization is less smooth (dotted grey line in the bottom row of Fig. 7), GradINN recover a less smooth prediction. This is in line with our interpretation of the auxiliary network as expressing prior beliefs."}, {"title": "Additional loss term", "content": "Considering the formulation given in Section 3 for the higher order derivatives, we can embed the second order derivatives with further information adding a consistency loss among the networks $F$ and $G$. In particular, given that $F$ represents the prior belief on the behaviour of $\\nabla_x u$ and $G$ represent the prior belief on the behaviour of $\\nabla^2 u$, we can consider the following additional loss term that allows minimizing the residual between $G$ and the first-order derivative of $F$:\n$L_{GF}(\\Theta_G, \\Theta_F) = \\frac{1}{M} \\sum_{m=1}^{M} \\sum_{i,j=1}^{d} \\bigg(G_{ij}(\\mathbf{x}_m; \\Theta_G) - \\frac{\\partial}{\\partial x_j} F_i(\\mathbf{x}_m; \\Theta_F)\\bigg)^2.$\nThe investigation of the effect of this additional loss term will be part of future work."}]}