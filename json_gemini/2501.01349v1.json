{"title": "Rethinking Relation Extraction: Beyond Shortcuts to Generalization with a Debiased Benchmark", "authors": ["Liang He", "Yougang Chu", "Zhen Wu", "Jianbing Zhang", "Xinyu Dai", "Jiajun Chen"], "abstract": "Benchmarks are crucial for evaluating machine learning algorithm performance, facilitating comparison and identifying superior solutions. However, biases within datasets can lead models to learn shortcut patterns, resulting in inaccurate assessments and hindering real-world applicability. This paper addresses the issue of entity bias in relation extraction tasks, where models tend to rely on entity mentions rather than context. We propose a debiased relation extraction benchmark DREB that breaks the pseudo-correlation between entity mentions and relation types through entity replacement. DREB utilizes Bias Evaluator and PPL Evaluator to ensure low bias and high naturalness, providing a reliable and accurate assessment of model generalization in entity bias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a debiasing method combining data-level and model training-level techniques. MixDebias effectively improves model performance on DREB while maintaining performance on the original dataset. Extensive experiments demonstrate the effectiveness and robustness of MixDebias compared to existing methods, highlighting its potential for improving the generalization ability of relation extraction models. We will release DREB and MixDebias publicly.", "sections": [{"title": "Introduction", "content": "Benchmarks are crucial for evaluating machine learning algorithms, providing standardized datasets to compare methods and identify top performers. However, reliance on specific datasets can introduce biases, causing models to learn shortcut patterns instead of true semantic understanding, which hinders their real-world applicability. Studies show that improved performance often stems from exploiting dataset biases rather than enhanced comprehension. For example, in natural language inference, models tend to predict based on lexical overlap ratios or the presence of negation words on SNLI (Bowman et al. 2015) and MNLI (Williams, Nangia, and Bowman 2018) datasets (Gururangan et al. 2018; McCoy, Pavlick, and Linzen 2019), and in fact verification tasks, they often rely on specific phrases rather than the contextual relationship between claims and evidence (Schuster et al. 2019).\nIn relation extraction tasks, widely-used datasets like SemEval 2010 Task 8 (Hendrickx et al. 2010), TACRED (Zhang et al. 2017), TACREV (Alt, Gabryszak, and Hennig 2020), and Re-TACRED (Stoica, Platanios, and P\u00f3czos 2021) exhibit entity bias, where entity mentions can provide superficial cues for relation types (Figure 1). This pseudo-correlation between entity mentions and relation types means models can often predict accurately without textual context (Zhang, Qi, and Manning 2018; Peng et al. 2020). For instance, over half of TACRED instances can be correctly predicted using only entity mentions (Wang et al. 2022). After entity replacement, state-of-the-art models like LUKE (Yamada et al. 2020) and IRE (Zhou and Chen 2022) experience significant drops in performance (30% - 50% F1 score) (Wang et al. 2023b). Large language models exacerbate this bias by disregarding contradictory or underrepresented contextual information, overly relying on biased parametric knowledge (Longpre et al. 2021) for predictions (Wang et al. 2023a). These findings highlight a critical over-reliance on entity mentions, severely impacting model performance when entity mentions are absent or debiased.\nTo address the entity bias issue, various approaches have been explored at both the data and model levels. However, existing works still face challenges: At the data level, debiasing methods may inadvertently introduce new biases, compromising evaluation reliability. For instance, (Wang et al."}, {"title": "DREB: A Debiased Relation Extraction\nBenchmark", "content": "We introduce DREB, a debiased relation extraction benchmark designed to dismantle pseudo-correlations between entity mentions and relation types, preventing models from solely inferring relations based on entity mentions. As illustrated in Figure 2, DREB construction involves substituting entities in the test set with entities of the same type from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014) to generate pseudo samples. Our method uniquely incorporates a Bias Evaluator to select replacements with minimal bias and a PPL Evaluator to ensure the naturalness and quality of the pseudo samples.\nBias Evaluator. Bias is fundamentally a pseudo-correlation between biased dataset features and their corresponding labels. To counter this, we employ a neural network to model these correlations directly. Given a sample denoted by x and its corresponding label y, the process of extracting bias features from x is represented by $\\phi(x)$. By training the network $F : \\phi(x) \\rightarrow y$, the output $F(\\phi(x))$ reflects the bias inherent in x. For entity bias specifically, the feature extraction process $\\phi$ is defined such that it captures the essence of the entity bias present in relation extraction samples. For instance, $\\phi($\u201cSteve Jobs founded Apple in a garage.\u201d$)$ would yield \u201cSteve Jobs\u201d and \u201cApple.\u201d We preprocess the relation extraction training set D with $\\phi(x)$ to construct a synthetic dataset $D_{entityBias}$, which allows us to model the entity bias directly. The resulting model, once trained, serves as a bias evaluator to measure the degree of entity bias in pseudo samples.\nPPL Evaluator. Entity replacement schemes generate synthetic text data, which may result in some degree of unnaturalness. To improve the quality of the challenge set, we generate multiple synthetic samples in batches and use GPT-2 (Radford et al. 2019) as a language model to calculate the perplexity of these samples. Given a sequence $W = (W_1, W_2, ..., W_n)$, where $w_i$ is the i-th word and n is the number of words in the sequence, the perplexity can be calculated using the following formula:\n$\\log PPL(W)=\\log \\sqrt[n]{\\frac{1}{P(W_1,W_2,...,W_n)}}$   (1)\n$\\qquad=\\frac{1}{n} \\sum_{i=1}^{n} \\log P(w_i/W_1,..., W_{i-1})$"}, {"title": "Benchmark Analysis", "content": "Does DREB introduce distribution biases? Figure 3 compares the relation distributions between DREB, the original datasets, and the method by (Wang et al. 2022). It shows that the datasets constructed by (Wang et al. 2022)'s method exhibit significant shifts in relation distribution compared to the original datasets, particularly with a notable reduction in the proportion of no_relation. This suggests that models could simply lower their classification thresholds to boost recall, artificially inflating evaluation metrics. In contrast, DREB maintains identical relation distributions to the original datasets, avoiding the introduction of new distribution biases and ensuring the accurate assessment of debiasing methods.\nDoes DREB introduce semantic biases? We compared the semantic distribution differences between DREB test set samples generated with and without the PPL Evaluator (w/ and w/o PPL Evaluator, respectively) and the original test set"}, {"title": "MixDebias: A New Baseline on DREB", "content": "Based on the DREB, we also introduce a method called MixDebias as a new baseline, which debiases from both the"}, {"title": "Data-level debiasing (RDA, Regularized Debias Ap-proach):", "content": "Entity mentions, despite their potential to cause bias, are valuable as they can prevent ambiguity, particularly in sentences with multiple entities of the same type. Instead of simplistically substituting entities with their corresponding entity types, we propose an approach that generates multiple data-augmented samples from an original training sample through entity replacement. This process is guided by a Kullback-Leibler Divergence (KL Divergence) constraint that encourages the model to produce probability distributions P and Paug that are as similar as possible when presented with the original and augmented samples, respectively. We term this KL divergence constraint LRDA, and its incorporation effectively reduces the model's reliance on the entities present in the input, thereby enhancing the model's generalization capabilities.\nSpecifically, we construct an entity dictionary (EntityDict) by extracting entities from the training set, facilitating the dynamic creation of data-augmented samples during training through entity replacement. We deliberately avoid sourcing entities from external resources like Wikidata for augmentation to prevent the introduction of lexical bias during the training phase. Throughout training, for an original sample, we dynamically retrieve entities of the same type from the EntityDict and generate a new data-augmented sample via entity replacement. Both the original and augmented samples are then fed into the relation extraction model, yielding two probability distributions, P and Paug. We calculate the KL divergence between these distributions. Due to the asymmetry of KL divergence, we calculate $D_{KL}(P||P_{aug})$ and $D_{KL}(P_{aug}||P)$ and average them to get $L_{RDA}$."}, {"title": "Model-level debiasing (CDA, Casual Debias Approach):", "content": "The CDA method identifies and quantifies entity bias through causal effect estimation and uses this estimation to guide model training, reducing the model's dependence on input features that may lead to bias. In causal effect estimation, we try to understand how different factors affect the results, especially how other variables affect the results when some variables are controlled. For relation extraction models, causal effects can be used to identify and reduce the model's dependence on input features that may have pseudo-correlation with the target output, rather than real causal relationships. The CDA method uses causal effect estimation to build a bias model (Bias Model), which assesses the degree of entity bias in each sample. Specifically, by providing only the context input to the model to obtain the probability distribution $P_{context}$, and the original sample input to the model to obtain the probability distribution P, then calculate $P \u2013 \\lambda P_{context}$ to obtain the bias probability distribution $P_{bias}$, where $\\lambda$ is a hyperparameter. This bias probability distribution reflects the degree of entity bias in the sample. The CDA method uses Debiased Focal Loss (Mahabadi, Belinkov, and Henderson 2020) for model training, which adjusts the model's predictions using the bias probability, thereby reducing the model's dependence on entity mentions.\n$L_{CDA} = - (1 - P_{bias})\\log P_{i}$   (2)\nwhere j is the correct relation type label. When $\\lambda$ is 0, $L_{CDA}$ degenerates into $\u2212(1\u2212 P_i) log P_i$, which is the Focal Loss. As a common form of model regularization loss, we modify it with $P_{context}$ to achieve a debiasing effect. In this way, the CDA method reduces the entity bias learned by the model during the training process, improving the model's generalization ability when facing different entities.\nFinally, we introduce a hyperparameter $\\beta$ to combine $L_{RDA}$ and $L_{CDA}$ in a weighted manner to obtain the final loss function $L_{MixDebias}$:\n$L_{MixDebias} = L_{CDA} + \\beta L_{RDA}$ \n$= -(1 \u2013 P_{bias}) log P_{i} + \\frac{\\beta}{2} (D_{KL}(P||P_{aug}) + D_{KL}(P_{aug}||P))$   (3)\n$= \u2212(1\u2212 (P \u2013 \\lambda P_{context})) log P_{i} + \\frac{\\beta}{2} (D_{KL}(P||P_{aug}) + D_{KL}(P_{aug}||P))$"}, {"title": "Evaluation", "content": "Evaluation metric. Consistent with previous work, we adopt the F1-score, which is the harmonic mean of pre-"}, {"title": "Baselines.", "content": "To focus on analyzing the debiasing effects of the model, models that retain entity mentions in the input during the preprocessing stage better meet our needs. We selected LUKE (Yamada et al. 2020) and IRE (Zhou and Chen 2022) for this purpose. LUKE is a transformer-based model that introduces a novel pretraining task for learning contextualized representations of both words and entities. IRE introduces typed entity markers that include both the entity spans and their types into the input text, allowing for a more comprehensive representation of entity mentions. In terms of debiasing methods, we primarily chose the following as baseline methods for comparison: Focal (Lin et al. 2018) reduces the model's reliance on entities by attenuating the influence of easily classified samples and amplifying the significance of challenging ones, thereby recalibrating the training focus towards hard-to-classify instances. R-Drop (Liang et al. 2021) enhances model generalization by enforcing consistency between output distributions of sub-models created through dropout, processing each mini-batch data sample twice to generate distinct outputs, and minimizing the bidirectional Kullback-Leibler divergence, thereby reducing reliance on entity mentions and improving the model's robustness. DFL (Mahabadi, Belinkov, and Henderson 2020) adjusts the loss function using a focusing parameter based on the bias-only model's predictions, effectively reducing the model's dependency on entities by downweighting samples with high entity bias, which enhances the model's robustness and generalization without altering its original architecture. PoE (Hinton 2002) employs a unique"}, {"title": "Main results.", "content": "Table 1 demonstrates the performance comparison of various relation extraction models and different debiasing methods on different datasets, where $F1_{origin}$ represents the performance on the original test set, and $F1_{DREB}$ represents the performance on DREB benchmark proposed in this paper.\nThe experimental outcomes yield these insights: LUKE and IRE experienced a notable decline in performance on the DREB, suggesting their initial high results were partially due to reliance on entity mentions that were either removed or disguised in the DREB context, thereby affecting their efficacy. Focal and R-Drop, though not originally intended to tackle entity bias, have still been found to alleviate it. These techniques, primarily targeting overfitting, incidentally lessen the models' dependency on entity cues, indicating that generalization-focused strategies can also indirectly benefit bias reduction. DFL and PoE, as targeted debiasing approaches, markedly bolstered model performance on DREB through the incorporation of bias evaluation and adjustment within the training regime. However, this enhancement seems to have compromised the models' performance on the original data. CoRE, tailored to counteract entity bias, successfully improved DREB performance without sacrificing the original dataset's results, reflecting a balanced and potent debiasing approach. In sum, our proposed MixDebias method has impressively uplifted performance on DREB while also maintaining or even enhancing the original dataset's performance, showcasing its robust adaptability and debiasing capabilities."}, {"title": "Ablation study.", "content": "As shown in Table 2, we conducted an ablation study on the two components of MixDebias, RDA and CDA. From the experimental results, we can draw the following conclusions: Both RDA and CDA are effective methods for removing entity bias. Overall, RDA is more effective than CDA. However, in most scenarios, these two methods are complementary and can enhance performance on DREB while minimizing the impact on the performance of the original dataset.\nAt the same time, we conducted a more detailed ablation analysis on the hyperparameters $\\beta$ and $\\lambda$ in MixDebias. Here, $\\beta$ represents the weight of the KL divergence, with a value range of [0.0, 1.0]; and $\\lambda$ represents the hyperparameter for estimating the biased probability distribution of samples using causal effects, with a value range of [-0.6, 0.6]. From Figure 6, we can draw the following conclusions: When $\\beta$ = 0, it is equivalent to the model not considering RDA. However, when $\\beta$ \u2260 0, introducing RDA leads to significant performance improvements, and as $\\beta$ increases, the debiasing effect becomes stronger. Particularly on noisy datasets such as TACRED and TACREV, the model also shows a slight performance improvement on the original dataset. Compared to $\\beta$, the $\\lambda$ parameter has a smaller impact on model performance. When $\\lambda$ = 0.2, the model performs optimally. This suggests that after applying the RDA method, the level of entity bias in the samples is already significantly reduced. In this case, CDA mainly addresses the bias that is difficult to correct at the data level, serving as a complementary effect to RDA, thereby further reducing the model's reliance on entities."}, {"title": "Model generalization analysis.", "content": "As shown in Figure 7, we plotted the label probability distribution of the model under the setting of entity-only input in the original test set before and after debiasing on the TACRED, TACREV, and Re-TACRED datasets. The experimental results show that for the baseline relation extraction model, under the entity-only input setting, the label probabilities are primarily concentrated around values close to 1, indicating that entity mentions significantly influences the model's prediction outcomes. After applying MixDebias debiasing method, the output probabilities of the model become notably more uniform. At this point, the pseudo-correlation between en-"}, {"title": "Conclusion", "content": "This paper introduces DREB, a debiased relation extraction benchmark, and MixDebias, a novel debiasing method that addresses entity bias in relation extraction models. DREB's strength lies in its ability to sever spurious links between entity mentions and relation types through strategic entity re-placement, fostering a benchmark with diminished bias and elevated naturalness. This is achieved with Bias Evaluator and PPL Evaluator, which ensure the benchmark maintains a high standard of impartiality and linguistic authenticity. MixDebias enhances model performance on DREB while maintaining robustness on the original dataset through a combination of data-level augmentation and model-level debiasing strategies. Comprehensive experiments demonstrate MixDebias's effectiveness in improving model generalization and reducing reliance on entity mentions, setting a new standard for debiasing in relation extraction tasks."}, {"title": "Related Work", "content": "For debiasing in relation extraction, efforts have focused on both data and model levels. Data Level: (Wang et al. 2022) introduces a filtered evaluation setting based on the TACRED dataset, retaining only samples where the relation cannot be accurately predicted using just the entity pairs. ENTRED (Wang et al. 2023b) employs type-constrained and random entity replacements to assess model robustness. Type-constrained replacement maintains entity class consistency, while random replacement introduces diversity. Model Level: DFL (Mahabadi, Belinkov, and Henderson 2020) adjusts the loss function based on bias-only model predictions, enabling the model to focus more on hard examples and less on biased ones. R-Drop (Liang et al. 2021) enforces consistency among output distributions of sub-models generated by dropout, improving generalization. CORE (Wang et al. 2022) constructs a causal graph to identify and mitigate biases caused by reliance on entity mentions, focusing predictions more on textual context."}]}