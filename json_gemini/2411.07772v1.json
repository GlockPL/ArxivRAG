{"title": "AUTOMATIC ALBUM SEQUENCING", "authors": ["Vincent Herrmann", "Dylan R. Ashley", "J\u00fcrgen Schmidhuber"], "abstract": "Album sequencing is a critical part of the album production process. Recently, a data-driven approach was proposed that sequences general collections of independent media by extracting the narrative essence of the items in the collections. While this approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of previous work, we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this-alongside a full copy of our implementation is publicly available at https://github.com/dylanashley/automatic-album-sequencing", "sections": [{"title": "1. INTRODUCTION", "content": "Album sequencing is the process of taking a music album and ordering it so that listening to it in that order produces a desired emotional response in the listener. Despite its importance in producing an impactful music album, album sequencing has received comparatively little attention from the artificial intelligence community.\nOur previous research [1] introduced a way to compress different kinds of media down into an ultra-low dimensional representation. This representation captures their relevancy to the overarching story induced by the ordering of the collection they belonged to, i.e., their narrative essence. This is accomplished by using neural networks and contrastive learning [2, 3]. Then, evolutionary algorithms are used to learn a set of template curves and a novel curve-fitting algorithm to fit the narrative essence of new media collections to these template curves. The above was principally done with music albums from the FMA dataset [4], though it is shown that this applies to other forms of media as well.\nThere are two key issues with our previous work. First, our previous method requires knowledge of advanced machine learning techniques, making it inaccessible to many people who perform album sequencing. Second, it requires a complex pipeline with (1) a neural network to extract the narrative essence followed by (2) a separate evolutionary algorithm to learn a set of templates and then (3) a fitting algorithm to produce a final ordering. This is a highly complex and particularly problematic setup that does not allow information like the genre of an album to flow between the narrative essence and the final ordering, resulting in genre-agnostic templates.\nHere, we address both of the aforementioned issues. To address the latter issue, we introduce a new approach that replaces the full pipeline with a single Transformer [5-7]. While this does not outperform the more complicated pipeline proposed in our previous work, the new simpler pipeline still outperforms a random baseline, making it useful for automatic album sequencing. Next, to address the former issue, we implement and release a dedicated user-friendly web-based interface that allows a less technically inclined user to run both the narrative essence-based and the new simplified album sequencing approaches on the user's own music. We release this interface alongside a complete implementation of our approach publicly at https://github.com/dylanashley/automatic-album-sequencing\nIn summary, our contributions are as follows: (1) We introduce a new direct method to perform automatic album sequencing. (2) We show that, despite the simpler pipelinethod outperforms a random baseline. (3) We release a web-based user interface tool that makes automatic album sequencing accessible to a less technical audience."}, {"title": "2. USER INTERFACE", "content": "Our user interface is shown in Figure 1. This interface is built in Python using the streamlit library and thus requires comparatively minimal effort to use. The interface"}, {"title": "3. DIRECT ALBUM SEQUENCING", "content": null}, {"title": "3.1 Method", "content": "Let $x_i$, $i \\in 1... M$, represent the M songs of an album A, in their original order $o(A)$. An encoder network $f_o$ transforms each song $x_i$ into a low-dimensional representation $z_i$. The core of our method is the ordering predictor $h_o$, whose task is to reconstruct the original album order from the unordered set ${z_i|1 \\leq i \\leq M}$.\nWe use a sequence-to-sequence model for $h_o$ (see Figure 3). For each album, the input to $h_o$ is a sequence of $z_i$ values ordered according to a random permutation $\\sigma$. This permutation $\\sigma$ changes with every album and is not known to the model. The decoder's task is to predict the inverse permutation $\\sigma^{-1}$, represented by the indices that reorders the permuted songs into their original album order $o(A)$.\nAlternatively, we can frame this task probabilistically. The model $h_o$ estimates the probability $p(o(A)|{f_o(x)|x \\in A})$. We jointly train the encoder $f_o$ and the ordering predictor $h_o$ using the standard cross-entropy loss for sequence modelling, where the target is the inverse permutation of the song indices.\nIn our experiments, $f_o$ is a simple two-layer fully connected neural network, and $h_o$ is a two-layer encoder-decoder transformer model. The training data comes from the FMA dataset [4], from which we only take albums of between 3 and 20 songs (inclusive). Each song is represented by a 525-dimensional feature vector composed of precomputed commonly used attributes provided by the FMA dataset. The encoder $f_o$ reduces every song into a 1-dimensional feature, similar to the narrative essence of our earlier work [1]. This allows the visualization of the narrative arc (see, e.g., [8-10]) proposed by the model. To generate $n$ orders for a single album, we sample $m > n$ orders from the distribution modelled by $h_o$ and take the top $n$ most likely orders."}, {"title": "3.2 Results and Discussion", "content": "Figure 2 shows the string edit score of our method as compared to Ashley and Herrmann et al.'s narrative essence approach. The string edit score measures how closely the set of proposed orders matches the original album order. It is defined as $f(T, o) = \\max_{\\hat{o} \\in T} \\frac{1}{k} g(\\hat{o}, o)$, where T is the set of k proposed orders, o is the ground-truth order, and g is the string edit distance (Levenshtein distance [11]). Despite the much simpler design of ours, for one proposed order, both approaches perform comparably. For multiple proposed orders, however, our method suffers from not being specifically designed to generate diverse sets of orders.\nThe direct method is able to capture 1.235 \u00b1 0.022 bits of mutual information between album songs and their order, in comparison 1.924 \u00b10.030 for the narrative essence approach [1]. Also, in the direct method, increasing the dimensionality of the learned representations $z_i$ does not significantly benefit the performance."}, {"title": "4. CONCLUSION AND FUTURE WORK", "content": "We introduced a direct transformer-based method for automatic album sequencing and developed a user-friendly web interface to make this technology accessible to a wider audience. Future work will focus on improving the diversity of generated orderings and incorporating user feedback."}]}