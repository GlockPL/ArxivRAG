{"title": "Coarse-to-Fine Lightweight Meta-Embedding for ID-Based Recommendation", "authors": ["Yang WANG", "Haipeng LIU", "Zeqian YI", "Biao QIAN", "Meng WANG"], "abstract": "The state-of-the-art recommendation systems have shifted the attention to efficient recommendation, e.g., on-device recommendation, under memory constraints. To this end, the existing methods either focused on the lightweight embeddings for both users and items, or involved on-device systems enjoying the compact embeddings to enhance reusability and reduces space complexity. However, they focus solely on the coarse granularity of embedding, while overlook the fine-grained semantic nuances, to adversarially downgrade the efficacy of meta-embeddings in capturing the intricate relationship over both user and item, consequently resulting into the suboptimal recommendations. In this paper, we aim to study how the meta-embedding can efficiently learn varied grained semantics, together with how the fine-grained meta-embedding can strengthen the representation of coarse-grained meta-embedding. To answer these questions, we develop a novel graph neural networks (GNNs) based recommender where each user and item serves as the node, linked directly to coarse-grained virtual nodes and indirectly to fine-grained virtual nodes, ensuring different grained semantic learning, while disclosing: 1) In contrast to coarse-grained semantics, fine-grained semantics are well captured through sparse meta-embeddings, which adaptively 2) balance the embedding uniqueness and memory constraint. Additionally, the initialization method come up upon SparsePCA, along with a soft thresholding activation function to render the sparseness of the meta-embeddings. We propose a weight bridging update strategy that focuses on matching each coarse-grained meta-embedding with several fine-grained meta-embeddings based on the users/items' semantics. Extensive experiments substantiate our method's superiority over existing baselines.", "sections": [{"title": "1 Introduction", "content": "Recommender systems (RSs) [1-3] have revolutionized the way we navigate the overwhelming volume of available information and products by tailoring recommendations to individual user preferences. Benefiting from the collaborative filtering, RSs map users and items into high-dimensional latent spaces [4-6] for personalized recommendations via the strategies such as the dot product [7], multi-layer perceptrons [8], and graph neural networks [9]. However, with the data scale growing up, complexity and resource demands of these systems heavily grows up. This issue is particularly highlighted in ID-based recommendation systems, where entities are used to uniformly represent all users and items, with each requiring a unique embedding vector. Consequently, traditional RSs suffer from substantial memory overhead, making them difficult to scale up. To address the challenge of optimizing memory usage without compromising recommendation accuracy, there has been a shift towards developing lightweight recommender systems [10-14] via deep neural network [15, 16].\nOne strategy is to construct the compact on-device session-based recommender systems [17-20, 43], trained in the cloud and deployed on devices like smartphones. The primary challenge is to reduce the model size to fit resource-constrained devices. To tackle this, [21] proposed to learn the elastic item embeddings, while adapting to device-specific memory constraints without retraining. Additionally, [22] proposed a personalized elastic embedding learning framework to on-device recommendations, by considering user and device heterogeneity. Moreover, [23] suggested semi-tensor decomposition and"}, {"title": "2 Preliminaries", "content": "Before shedding light on our technique, we elaborate the lightweight embedding for ID-based recommendation and the GNN-based meta-embedding learning."}, {"title": "2.1 Efficient Meta-Embedding for ID-Based Recommendations", "content": "In ID-based recommendation, with the user set U and the item set I, the total number of entities is given by N = |U| + |I|. The user-item interactions are denoted by $R\\in {0,1}^{|U|\\times|I|}$, where 0 signifies no interaction, and 1 indicates the presence of an interaction between a user-item pair. To enhance the memory efficiency of the embedding layer, a common approach involves utilizing a compact meta-embedding codebook, i.e., $E_{meta} \\in R^{m \\times d}$, with d representing the dimensionality of the embeddings. This codebook serves as a replacement for the more extensive full embedding matrix $E \\in R^{N \\times d}$, where the count of meta-embeddings m is significantly smaller than N. To generate an embedding for each entity, a common approach involves using an assignment matrix $S \\in R^{N\\times m}$. Each row of this matrix corresponds to a user or item entity, where the non-zero elements specify which meta-embeddings are selected from the m available options, along with their respective weights in forming the entity's embedding. Consequently, the compositional embeddings for all entities, denoted as $\\bar{E}\\in R^{N \\times d}$, can be calculated as follows:\n$\\bar{E} = S E_{meta}$.\nBased on the above, recent developments have demonstrated that Graph Neural Networks (GNNs) can effectively propagate collaborative signals among interconnected entities. Leveraging this, GNN-based recommenders construct a user-item interaction graph to encapsulate the semantic correlations present among entities, as evidenced in their interactions."}, {"title": "2.2 Graph-Propagated Meta-Embeddings", "content": "The GNN-based recommenders typically adopt an adjacency matrix $A \\in R^{N\\times N}$ to represent those connections in the user-item interaction graph, facilitating the propagation of collaborative signals among"}, {"title": "2.3 Learning Assignment Weights", "content": "As the meta-embedding codebook $E_{meta}$ is learned, it is essential to continually update the weights in S to achieve a more accurate mapping between entities and their meta-embedding assignments. For instance, two closely related entities should have their embeddings composed of a similar set of meta-embeddings, reflecting the core principle of collaborative filtering. However, jointly updating the S and $E_{meta}$ will introduce difficulties in finding a stable solution. To solve this challenges, [31] proposes to preserved the mapping from each entity to its corresponding meta-embeddings after L layers of propagation in the expanded interaction graph A' as same as the in the input layer of the GNN. Thus:\n$H_{full} = S H_{meta}$,\nwhere $H_{full} \\in R^{N \\times d}$ and $H_{meta} \\in R^{m\\times d}$ are the graph-propagated embeddings as in Eq.7. Based on that, calculating the pseudo-inverse (i.e., $H_{meta}^\\dagger$) of $H_{meta}$ via the Moore-Penrose inverse [33], then the assignment matrix S is computed as:\n$S = H_{full}H_{meta}^\\dagger = H_{full}V\\Sigma^{-1}U^*$,\nwhere $V \\in R^{d \\times m}$ is a unitary matrix, $\\Sigma^{-1} \\in R^{m\\times m}$ is the inverse matrix of the square matrix $\\Sigma$ which contains singular values along the diagonal, $U^* \\in R^{m\\times m}$ is the conjugate transpose of unitary matrix $U \\in R^{m\\times m}$. Updating S via a gradient-free learning strategy enhances computational efficiency. Additionally, this method preserves semantic associations among entities in their meta-embedding assignments. Specifically, similar rows in $H_{full}$ yield comparable outcomes after the multiplication process in Eq. 11."}, {"title": "3 Overall Framework", "content": "Central to our method lies in three aspects: 1) learning coarse meta-embedding for coarse-grained semantics (Sec.3.1); 2) learning sparse fine meta-embedding for fine-grained semantics based on the coarse meta-embedding (Sec.3.2); 3) how the fine meta-embedding can strength the representation of coarse meta-embedding (Sec.3.3). The overall algorithm is in Sec.3.4. Additionally, we provide Table.1, which explains the descriptions of the various symbols used in the paper."}, {"title": "3.1 Coarse Graph-Propagated Meta-Embeddings", "content": "Based on Sec.2.2, we now outline the construction of our coarse graph-propagated meta-embeddings. Initially, we use the coarse meta-embedding codebook $E_{meta}^c \\in R^{m^c\\times d}$ and the coarse assignment matrix $S^c$ to compute the coarse compositional embeddings $\\bar{E}^c$ as follows:\n$\\bar{E}^c = S^c E_{meta}^c$.\nWe then incorporate $S^c$ into the user-item interaction graph A, resulting in the coarse interaction graph $A^c\\in R^{(N+m^c)\\times(N+m^c)}$.\nBy stacking $\\bar{E}^c$ and $E_{meta}^c$ as the input embeddings of $A^c$, we obtain the graph-propagated coarse representations for the complete entity set $H_{full}^c \\in R^{N\\times d}$ and the coarse meta-embedding codebook $H_{meta}^c \\in R^{m^c\\times d}$. We then extract the graph-propagated embeddings of entities from $H_{full}^c$ and compute the similarity score between each user-item pair as defined in Eq.8. With a fixed assignment $S^c$ in each iteration, the predicted scores of a training batch are fed into Eq.9 to compute the BPR loss $L_{BPR}$, facilitating back-propagation to update the coarse meta-embedding codebook $H_{meta}^c$. Besides, we will update the $S^c$ as described in Sec.2.3."}, {"title": "3.2 Fine Graph-Propagated Meta-Embeddings", "content": "Based on the coarse meta-embedding, it is crucial to capture more fine representations to improve upon coarse meta-embeddings. To learn more fine-grained semantic information, rather than directly connecting coarse meta-embeddings to a set of entity nodes for representation learning, we propose connecting these fine meta-embeddings as auxiliary nodes to the coarse meta-embedding nodes. This approach prevents learning redundant information from the coarse stage. Consequently, we can construct the fine interaction graph $A^r \\in R^{(N+m^c+m^r)\\times(N+m^c+m^r)}$ as:\nwhere $S^r \\in R^{m^c\\times m^r}$ is the indirect assignment matrix between the two kinds of meta-embeddings.\nBefore discussing how to refine the coarse meta-embedding codebook, we need to learn the fine-grained representation $E_{meta}^r \\in R^{m^r \\times d}$.\nAs shown in Fig.2, we propose a method to improve the initialization process for the coarse meta-embedding codebook $E_{meta}^c$. Instead of randomly assigning values, we suggest selecting r coarse meta-embeddings from $E_{meta}^c$ to form a new matrix $E_{meta}^r$. This selection process allows us to initialize $E_{meta}^r$ based on these chosen coarse meta-embeddings, thus enhancing the semantic relationships between entities from the start. Furthermore, when refining the coarse meta-embeddings, we aim to avoid learning dense vectors identical to the coarse ones, which would result in a dense matrix $E_{meta}^r$. Instead, our objective is to make $E_{meta}^r$ sparse. This approach serves two purposes. Firstly, it ensures that the coarse stage focuses on capturing coarse-grained semantic information, while the fine stage deals with representing fine-grained details as auxiliary information. Secondly, it aligns with the goal of compressing the total number of meta-embeddings across both stages. To achieve more flexible initialization of sparse $E_{meta}^r$, we propose utilizing SparsePCA (Principal Component Analysis) for initialization. SparsePCA inherits PCA's benefits in effectively capturing the main patterns of variability while reducing computational complexity and feature redundancy. Moreover, it introduces the capability to control sparsity within the extracted components. This control over sparsity is crucial as it allows us to regulate the degree"}, {"title": "3.3 Learning Coarse-to-Fine Assignment Weights", "content": "Unlike the coarse stage, which can directly calculate the value of $S^c$ using Eq.11, the fine interaction graph is constructed hierarchically. Therefore, we need to calculate the weight matrix $S^r$ between two types of meta-embedding codebooks. We aim for each coarse meta-embedding to efficiently select the corresponding fine meta-embedding as a semantic refinement. However, directly calculating the similarity score between coarse and fine meta-embedding would ignore the entities' semantics. As mentioned earlier, Eq.10 shows that the mapping relationship from each entity to its corresponding meta-embeddings, after L layers of propagation, remains the same as in the input layer of the GNN. In this spirit, entities can serve as a bridge [38], linking each coarse meta-embedding with several refined meta-embeddings according to their semantics. Thus, we propose the following bridging updating strategy for the coarse-to-fine assignment matrix (as illustrated in Fig.3):\nwhere $(H_{meta}^c)^\\dagger$ is the pseudo-inverse of $H_{meta}^c$ and $(H_{meta}^r)^\\dagger$ is the pseudo-inverse of $H_{meta}^r$, both calculated using the Moore-Penrose inverse. Since Eq.23 yields a dense matrix, to maintain the sparsity of $S^r$, we perform sparsification by retaining the largest $t^r$ non-zero weights in each row of $S^r$. This ensures that each coarse meta-embedding uses exactly $t^r$ fine meta-embeddings at all times:\n$S^r[p, q] =\\begin{cases}S^r[p, q] & \\text{if } q \\in idxtop-(t^r)(S^r[p, :])\\\\0 & \\text{otherwise}\\end{cases}$\nwhere $idxtop-(t^r)(\\cdot)$ returns the indices of the top-($t^r$) entries of a given vector. Essentially, we retain the relatively larger weights in $S^r$ only for those fine meta-embeddings considered critical for a coarse meta-embedding. Last but not least, given the fine-grained meta-embedding, a well-initialized coarse-to-fine assignment matrix $S^r$ can significantly contribute to improving the recommendation accuracy of the coarse-grained meta-embedding and accelerating training. The goal is to link each coarse meta-"}, {"title": "3.4 The Overall Algorithm", "content": "The provided pseudocode outlines the two stages of our algorithm: the Coarse Stage (Algorithm 1) and the Refined Stage (Algorithm 2). In the coarse stage, we initialize the coarse meta-embedding matrix $E_{meta}^c$ and the assignment matrix $S^c$. The update frequency for this stage, denoted as $f^c$, is set to control the frequency of assignment updates. Additionally, we compute the adjacency matrix $A^c$ based on a specific equation. During iterative optimization of the Bayesian Personalized Ranking ($L_{BPR}$) loss, we periodically update the assignment matrix and regenerate $A^c$ to reflect any changes in entity-meta-embedding associations. Transitioning to the fine stage, we initialize the fine meta-embedding matrix $E_{meta}^r$ and the coarse-to-fine assignment matrix $S^r$ via our proposed strategy. Here, we freeze the coarse meta-embedding matrix $E_{meta}^c$. Similar to the coarse stage, we determine the update frequency $f^r$ for this stage. Following initialization, we compute the adjacency matrix $A^r$ to capture fine relationships among entities and meta-embeddings. During iterative optimization of the $L_{BPR}$ loss, we update the coarse-to-fine assignment matrix and regenerate $A^r$ at intervals dictated by $f^r$, ensuring consistent refinement and adaptation to evolving data dynamics."}, {"title": "4 Experiments", "content": "In this section, we conduct a series of experiments to validate the effectiveness of our work. We organize our experiments around the following research questions (RQs):\n\u2022 Q1: How is the recommendation accuracy of our work compared to other baselines?"}, {"title": "4.1 Experimental Settings", "content": "In our evaluation, we engage three distinct benchmark data corpus: Gowalla [35], Yelp2020 [41], and Amazon-book [36]. They are notable for their open accessibility and the diversity they encompass in scale, sectors, and user-item interaction density. We split the train/test/validation dataset according to [33]. The details of these datasets are inventoried in Table 2."}, {"title": "4.1.2 Baseline Algorithms", "content": "To validate the superiority of our method, we compare it with the typical recommender models, including: PEP [10], QR [27] and LEGCF [31] allows the generation of embedding layers to satisfy various memory targets; AutoEmb [11], ESAPN [12], OptEmbed [13], CIESS [14], DHE [15], NimbleTT [16] ignore the final embedding structure meets the memory budget, because these methods do not formalize the memory target as an optimization objective; we also provide the results of the non-compressed versions of LightGCN with relatively large dimension sizes, i.e., UD - dim 128 and UD - dim 64, to compare the performance compromise of various lightweight embedding methods. Due to those on-device recommenders are designed for session-based recommendation, so we can not provide their results of id-based recommendation."}, {"title": "4.1.3 Evaluation Metrics.", "content": "Following metrics are used to evaluate compared methods, we set N to values of {5, 10, 20} to observe recommendations' performance.\n(1) NDCG@N (Normalized Discounted Cumulative Gain): This metric appraises the ranking quality by gauging the graded relevance of the recommended items. Unlike precision-based metrics, NDCG@N accounts for the decay of relevance as users are less likely to consider items lower in their feed. It assigns maximum gain to the top-ranked relevant recommendations, attenuating the value logarithmically as the rank diminishes.\n(2) Recall@N: Embodies the proportion of a user's preferred items that appear in the top-N slice of the recommendation list. Recall@N is insightful for discerning the coverage breadth of a recommender system, with an ideal system uncovering all items of interest in the user's top-tier selections."}, {"title": "4.1.4 Implementation Details.", "content": "In our work, we set the embedding size d for meta-embedding to 128. To create the training set, we randomly draw 5 negative items for each user-positive item interaction. Meta-embeddings and recommender weights are initialized using Xavier Initialization [39], and the ADAM optimizer [40] is employed for training. In the coarse training stage, to ensure that the model learns a better assignment matrix, we update the coarse assignment matrix every epoch and set the early stop threshold to 10 for both the fixed assignment training stage and the assignment update stage. The learning rate is set to $10^{-3}$, and the L2 penalty factor $\\lambda$ is $5 \\times 10^{-4}$. In contrast, for the fine-grained training stage, we maintain the same update frequency for the coarse-to-fine assignment matrix as in the coarse assignment matrix. However, to reduce complexity, we set the early stop threshold to 5 for both the fixed assignment training stage and the assignment update stage. The learning rate for this stage is $3 \\times 10^{-3}$, with the L2 penalty factor $\\lambda$ also set to $5\\times10^{-4}$. We utilize a 3-layer GNN for the Gowalla dataset and a 4-layer GNN for the Yelp2020 and Amazon-book datasets. The codebook is divided into two parts: 300 buckets for coarse meta-embeddings and 100 buckets for fine meta-embeddings. The overall performance evaluated with this configuration is discussed in RQ1. For variable-size baselines, we first calculate the total number of parameters used in our 500-bucket configuration. This memory budget is then applied to set the appropriate sparsity target or number of hashing buckets for methods like PEP and QR, ensuring that their embedding layers do not exceed our specified memory constraints. For baselines that lack control over final parameter sizes, we establish suitable search spaces to avoid excessive memory usage while providing flexibility for optimal setting selection. All the experiments are implemented with the pytorch framework and run on NVIDIA A6000."}, {"title": "4.2 Overall Performance (Q1)", "content": "We set the coarse meta-embedding bucket size $m^c$ = 300 and the fine meta-embedding bucket size $m^r$ = 100 for our method. The overall performance of our method compared to baseline methods is summarized in Table 3. Our method achieves higher NDCG@{10,20} and Recall@{10,20} scores,"}, {"title": "4.3 Model Component Analysis (Q2)", "content": "To validate the performance gain from each key component, we conduct the ablation studies on the several innovative components including the fine meta-embedding initialization strategy, soft thresholding sparsity strategy and weight bridging updating strategy. The performance comparison is depicted in Fig.4\n(1) Fine Meta-Embedding Initialization: We enhance the initialization strategy for the fine meta-embedding $E_{meta}^r$ in two ways. First, instead of randomly initializing the embedding values, we select a portion of the coarse meta-embedding to form $E_{meta}^r$. Second, we use SparsePCA to preserve the main patterns and control sparsity. Our experiments show that the absence of these two components (Case A and B) leads to significantly worse performance. This indicates that initializing $E_{meta}^r$ with semantic and sparse properties is crucial for training an accurate meta-embedding codebook, resulting in high-quality entity embeddings. Such fact further discloses our core contribution \u2013 refining the coarse meta-embedding via the fine-gained semantic information (Sec.3.2)\n(2) Soft Thresholding Sparsity Strategy: To evaluate the effect of soft thresholding, we removed that and directly used $E_{meta}^r$ to refine $E_{meta}^c$ (Case C). The resulting performance decline without soft thresholding demonstrates that soft thresholding technique is integrated to dynamically adjust the sparsity levels by smoothly driving small coefficients to zero while retaining the significant ones. This method ensures that the resulting embeddings remain sparse and computationally efficient, facilitating better convergence in training processes, as claimed in Sec.3.2.\n(3) Weight Bridging Updating Strategy: We leverage the Eq.23 to calculate $S^r$, we abandon that and match these two kinds of meta-embedding without considering the users/items' full embedding to validate the importance of our bridging strategy (Case D). Our proposed strategy aims at well matching each coarse meta-embedding with several fine meta-embeddings according to users/items' semantics (Sec.3.3). Fig.4 proves that.."}, {"title": "4.4 Hyperparameter Analysis (Q3)", "content": "To study the sensitiveness of our work on hyperparameters, we conduct hyperparameter analysis on the desired number of new features d' in SparsePCA, the threshold parameter $\\lambda$ in soft thresholding, the weight of the fine meta-embedding $w_{cr}$ and the number of fine meta-embedding assigned to each coarse meta-embedding $t^r$.\n(1) Number of New Features d': The embedding size d for both $E_{meta}^c$ and $E_{meta}^r$ is set to 128. For testing, we set the dr values to {40; 60; 80; 100; 120}. The performance results on both datasets are shown in Fig. 5(a-1) and (b-1). It was found that increasing the number of non-zero values does not guarantee performance improvement. This is primarily because the fine stage focuses on learning fine-grained semantic information. A densely fine meta-embedding may capture redundant semantics similar to those in the coarse stage, thereby neglecting finer details. Conversely, embeddings that are too sparse (e.g., d' = 40) can impair the ability to express semantic nuances effectively. By experimenting with different levels of sparsity for initialization, we aim to balance capturing detailed semantics and avoiding redundancy.\n(2) Threshold of Soft Thresholding \u03bb: \u03a4o investigate the impact of the soft thresholding parameter $\\lambda$ on embedding sparsity and representation quality, we conducted experiments with $\\lambda$ values set to {0.5; 1; 3; 5; 10}. These values determine different ratios of non-zero values in the fine meta-embedding. As shown in Fig. 5(a-2) and (b-2), our method achieves optimal performance on both datasets when the fine meta-embedding remains sparse. Specifically, for Gowalla and Yelp2020, the best performance is observed with $\\lambda$ = 3, which results in approximately half of the values being non-zero. This outcome aligns with our previous findings regarding the hyperparameter dr, further validating that maintaining an optimal level of sparsity is crucial for capturing fine-grained semantic information without redundancy. Our results can also indicate that the refined stage is designed to work under tight memory budget.\n(3) Weight of the Fine Meta-Embedding $w_{cr}$: The chart showing the balance between the coarse meta-embedding and the fine meta-embedding is depicted in Fig.5(a-3) and (b-3). The set of $w_{cr}$ values tested includes {0.1, 0.3, 0.5, 0.7, 0.9}. It is observed that for the Gowalla dataset, the best result is achieved with a weight of 0.5, while for Yelp2020, the optimal $w_{cr}$ is 0.3. As the weight decreases, the performance becomes less competitive. This phenomenon indicates that the fine stage is essential for enhancing the coarse meta-embedding, and that the sparse fine meta-embedding performs well even under tight memory constraints."}, {"title": "4.5 Space Complexity (Q4)", "content": "We evaluate the memory consumption of our framework by adjusting the ratios between the coarse meta-embeddings and the fine ones. In LEGCF, the space complexity is O(tN + md) for its lightweight embeddings, where t is the number of meta-embeddings per entity and m is the number of meta-embedding codebooks. Compared to the space complexity of a full embedding table, where t < m, we have O(tN +\nmd) < O(Nd). In our method, the space complexity of the coarse stage is O($t^cN + m^cd$), similar to LEGCF. However, in the fine stage, as discussed in Section 4.4, setting half of the fine meta-embedding values to zero achieves the best performance. Therefore, the space complexity for this stage can be calculated as O($tm^r + m^{ra}d$). The overall complexity is O($t^cN + tm^r + m^cd+m^{ra}d$) <O(tN +md), given that $m^c+ m^r \\leq m$, $m^r \\leq m^c$, and t << d in our setting. Under a fixed memory constraint, a lower\nm results in lower space complexity. The fine stage helps to balance the trade-off between embedding uniqueness and embedding fidelity. Fig.6 further demonstrates that.\nWe conducted two types of ablation studies to explore how to set the number of coarse-grained and fine-grained meta-embeddings to reduce space complexity while achieving better performance. As shown in Fig.6(a), based on a fixed number of coarse-grained meta-embeddings, we adjusted the ratios of fine-grained meta-embeddings. The results show that because the coarse-grained training stage effectively establishes strong correlations between entities and coarse-grained meta-embeddings, increasing the num-ber of fine-grained meta-embeddings significantly improves performance. However, when space capacity is strictly limited, fewer coarse-grained meta-embeddings cannot adequately balance embedding unique-ness and fidelity. In such cases, a smaller ratio of fine-grained meta-embeddings may perform better.\nAs illustrated in Fig.6(b), we conducted experiments with a fixed total number of coarse-grained and fine-grained meta-embeddings to validate this observation."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel GNN-based model for multi-grained semantic learning of lightweight meta-embeddings for ID-based recommendation systems. This hierarchical and selective connectivity ensures our model to learn wide-ranging semantic information at the coarse level, while focusing on"}]}