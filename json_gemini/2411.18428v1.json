{"title": "MM-Path: Multi-modal, Multi-granularity Path Representation Learning-Extended Version", "authors": ["Ronghui Xu", "Hanyin Cheng", "Chenjuan Guo", "Hongfan Gao", "Jilin Hu", "Sean Bin Yang", "Bin Yang"], "abstract": "Developing effective path representations has become increasingly essential across various fields within intelligent transportation. Although pre-trained path representation learning models have shown improved performance, they predominantly focus on the topological structures from single modality data, i.e., road networks, overlooking the geometric and contextual features associated with path-related images, e.g., remote sensing images. Similar to human understanding, integrating information from multiple modalities can provide a more comprehensive view, enhancing both representation accuracy and generalization. However, variations in information granularity impede the semantic alignment of road network-based paths (road paths) and image-based paths (image paths), while the heterogeneity of multi-modal data poses substantial challenges for effective fusion and utilization. In this paper, we propose a novel Multi-modal, Multi-granularity Path Representation Learning Framework (MM-Path), which can learn a generic path representation by integrating modalities from both road paths and image paths. To enhance the alignment of multi-modal data, we develop a multi-granularity alignment strategy that systematically associates nodes, road sub-paths, and road paths with their corresponding image patches, ensuring the synchronization of both detailed local information and broader global contexts. To address the heterogeneity of multi-modal data effectively, we introduce a graph-based cross-modal residual fusion component designed to comprehensively fuse information across different modalities and granularities. Finally, we conduct extensive experiments on two large-scale real-world datasets under two downstream tasks, validating the effectiveness of the proposed MM-Path. This is an extended version of the paper accepted by KDD 2025. The code is available at: https://github.com/decisionintelligence/MM-Path.", "sections": [{"title": "1 Introduction", "content": "Understanding paths and developing effective path representations are increasingly essential, offering invaluable insights for diverse fields such as intelligent navigation [18, 19, 34, 35, 43], route recommendation [7, 12, 49], urban planning [11, 17, 42], and urban emergency management [14]. Recent studies focus on developing pre-trained path representation learning models, which have demonstrated outstanding generalization capabilities [5, 21, 45]. These models efficiently produce generic path representations in an unsupervised manner. With simple fine-tuning and little labeled data, they are adaptable to diverse downstream tasks such as travel time estimation and path ranking score estimation. Consequently, they significantly improve computational efficiency by reducing both labeled data and runtime.\nPaths have different modalities that provide richer, more diverse information. For example, while paths derived from road networks (road paths for short) elucidate topological relationships among road segments in paths, remote sensing images of paths (image paths for short) provide insights into geometric features and broader environmental contexts (see Figure 1). Integrating these modalities enriches path representations with varied perspectives, thereby improving accuracy and enhancing generalization capabilities. However, current path representation learning models primarily rely on single-modality data from road networks, which fails to capture the deep, comprehensive context essential for a complete understanding of paths. This calls for developing a multi-modal pre-trained path representation learning model. Nonetheless, constructing such a model faces several challenges:\nInformation granularity discrepancies between road paths and image paths significantly hinder cross-modal semantic alignment. Effective cross-modal alignment, which ensures semantic consistency and complementarity among various modalities, is crucial for constructing multi-modal models [26]. However, the discrepancies in information granularity between road paths and image paths are substantial. As depicted in Figure 1, road paths typically focus on detailed topological structures and delineate road connectivity, while the image paths capture global environmental contexts on a large scale, reflecting the functional attributes of corresponding regions. It is worth noting that images may include extensive regions that show low relevance to the road paths, such as the dark regions in Figure 1 (c). Current image-text multi-modal models [2, 30, 37, 40] typically align individual images with textual sequences. However, such single-granularity and coarse alignment methods introduce noise, which are not suitable for the precise alignment required for paths. Additionally, as shown in Figure 1 (a), roads have different granularities in nature, including intersections, road segments, and sub-roads. Fully understanding paths at different granularities can provide insights from micro to macro levels, mitigating the negative effects caused by the differences in information granularity across modalities. Although some studies [9, 32] have explored multi-granularity in single-modal data, they have not adequately addressed the requirements for multi-granularity analysis in multi-modal contexts. Thus, it is crucial to refine multi-granularity data processing and develop multi-granularity methods for cross-modal alignment.\nThe inherent heterogeneity of road paths and image paths poses a significant challenge during feature fusion. The differences in data structure and information granularity between road paths and image paths extend to their learning methods. Road path representation learning typically focuses on connectivity and reachability between roads and intersections, as well as analyzing graph structures [3, 21, 44, 45]. Conversely, image learning methods that are able to learn image paths prioritize object recognition and feature extraction, aiming for a broad understanding of image content [1, 20]. These disparate learning methods lead to road paths and image paths mapped to different embedding spaces, resulting in feature dimensions with similar semantics containing entirely different information. Simple fusion methods like early fusion (i.e., integrating multiple modalities before or during the feature extraction stage) and late fusion (i.e., keeping each modality independently processed until the final fusion stage) may result in information loss and increased bias, and fail to capture subtle correlations between road paths and image paths [26, 47]. Therefore, a multi-modal fusion method that can capture the relationships among entities in different modalities and ensuring effective data fusion, is critically needed.\nTo address these challenges, we propose a Multi-modal, Multi-granularity Path Representation Learning Framework, namely MM-Path, for learning generic path representations.\nTo address the first challenge, we propose a multi-granularity alignment component. This component systematically associates intersections, road sub-paths, and entire road paths with their corresponding image information to capture details accurately at a finer granularity as well as maintaining global correspondence at a coarser granularity. Specifically, we divide the image of the entire interested region into small fixed-size images, collect the small fixed-size images along each path, and arrange the collected images into an image path (i.e., image sequence). We employ modal-specific tokenizers to generate the initial embeddings for road paths and image paths, respectively. Subsequently, these initial embeddings are fed into the powerful Transformer architecture to learn complex encoded embeddings for each modality at three granularities. Finally, a multi-granularity alignment loss function is employed to ensure the alignment of road and image encoded embeddings across different granularities.\nTo address the second challenge, we introduce a graph-based cross-modal residual fusion component, which is designed to effectively fuse cross-modal features while incorporating spatial contextual information. Specifically, we link the encoded embeddings of each modality with the initial embeddings of the other modality to create road and image residual embeddings, respectively, with the purpose of fusing cross-modal features from different stages. We then build a cross-modal adjacency matrix for each path based on spatial correspondences and contextual information. This matrix guides the GCN to iteratively fuse the residual embeddings of each modality separately, thus obtaining road and image fused embeddings. Finally, we apply contrastive loss to ensure the consistency of the fused embeddings across the two modalities. As the final representation effectively integrates cross-stage features of the two modalities with spatial context information, this component not only achieves deep multi-modal fusion but also enhances the comprehensive utilization of information.\nThe contributions of this work are delineated as follows:\n\u2022 We propose a Multi-modal, Multi-granularity Path Representation Learning Framework that learns generic path representations applicable to various downstream tasks. To the best of our knowledge, MM-Path is the first model that leverages road network data and remote sensing images to learn generic path representations.\n\u2022 We model and align the multi-modal path information using a fine-to-coarse multi-granularity alignment strategy. This strategy effectively captures both intricate local details and the broader global context of the path.\n\u2022 We introduce a graph-based cross-modal residual fusion component. This component utilizes a cross-modal GCN to fully integrate information from different modalities while maintaining the consistency of dual modalities.\n\u2022 We conduct extensive experiments on diverse tasks using two real-world datasets to demonstrate the adaptability and superiority of our model."}, {"title": "2 Preliminaries", "content": "Basic Conception\nPath. A path p is a sequence of continuous junctions, which can be observed from the road network view and the image view.\nRoad network. A road network is denoted as G = (V, &), where V and & represent a set of nodes and edges, respectively. Node"}, {"title": "2.1", "content": "v \u2208 V is a road intersection or a road end. Edge e \u2208 & denotes a road segment connecting two nodes.\nRoad paths. We define the sequence of nodes on a road network for a path p as a road path R(p) = (\u03c51, \u03c52, ..., \u03c5Nroad), where each element represents a node, and Nroad represents the length of the road path R(p). It is noted that there must be an edge e \u2208 & connecting any adjacent nodes in the road path.\nImage paths. Given an interested region, we partition the region into fixed-size segments to generate a set of images, Mrs, consisting of Nimage disjoint, fixed-size remote sensing images. Each image within this set is denoted as m\u2208 Rrxrxc, where c represents the number of channels, and (r, r) denotes the resolution. Subsequently, given road path R(p), the image path (i.e., image sequence of the path) M(p) is formed by selecting a series of image mi that correspond to specific latitudes and longitudes along the nodes in the road path.\nFor example, as shown in the upper part of Figure 2, consider the road path R(p) = (\u03c51, ..., \u03c58), where nodes \u03c51, \u03c52, and \u03c53 are located in image m1, nodes \u03c54, \u03c55 and \u03c56 in image m2, and nodes \u03c57 and \u03c58 in image m3. This results in the image path M(p) = (m1, m2, m3).\nRoad Sub-paths. Given a road path R(p) and an image path M(p), the nodes of R(p) located in the same image belong to a road sub-path.\nTaken Figure 2 as an example, the road path R(p) = (\u03c51, ..., \u03c58) has three road sub-paths: s1 = (\u03c51, \u03c52, \u03c53), s2 = (\u03c54, \u03c55, \u03c56) and s3 = (\u03c57, \u03c58)."}, {"title": "2.2 Problem Statement", "content": "Given the road path R(p) and image path M(p) of a path p, the goal is to learn an embedding function f that returns a generic representation of path p. This function can be formalized as follows:\nx = f(R(p), M(p)), (1)\nwhere x \u2208 Rd represents the generic embedding of path p, and d denotes the dimension of the embedding x.\nThese learned path embeddings are supposed to be generic, which should support a variety of downstream tasks, e.g., path travel time estimation and path ranking score estimation."}, {"title": "3 Methodology", "content": "Figure 3 illustrates the framework of MM-Path. This section introduces the framework of MM-Path, describes its two main components, and details the final training objective."}, {"title": "3.1 Overall Framework", "content": "Different from existing methods that are limited to data from a single modality, MM-Path leverages data from both road networks and images for pre-training, providing a more comprehensive perspective. MM-Path comprises two main components: the multi-granularity alignment component and the graph-based cross-modal residual fusion component.\nThe multi-granularity alignment component is designed to concentrate on path-related information while capturing fine-grained details and coarse-grained global context. Initially, we convert the image of interested region into fixed-sized image sequences to obtain image paths. Subsequently, We establish road path and image path encoding branches to process the two modalities. In each branch, a modal-specific tokenizer generates initial embeddings for each modality at three granularities: node/patch, road sub-path/image, and road path/image path. These initial embeddings are then processed by road and image transformers to produce road and image encoded embeddings, respectively, which are also generated at the same three granularities. A multi-granularity loss function is utilized to synchronize the semantic information of road encoded embeddings and image encoded embeddings, and to capture their interrelations at different granularities, from fine to coarse.\nThe graph-based cross-modal residual fusion component is designed to effectively fuse cross-modal heterogeneous data. A cross-modal residual connection merges the encoded embedding from each branch with the initial embedding from the other branch, generating road and image residual embeddings. This connection considers cross-modal features at different stages, promoting deep cross-modal feature fusion. Subsequently, we construct a cross-modal adjacency matrix for each path based on spatial correspondences and contextual information. This matrix, embedded within a GCN, guides the fusion of the two modalities for each branch. Consequently, a fused embedding is obtained for each branch. We introduce a contrastive loss to ensure the consistency between the road fused embedding and image fused embedding. Finally, we concatenate these two fused embeddings to obtain a generic path representation."}, {"title": "3.2 Multi-granularity Alignment", "content": "We model the road paths and image paths using Transformer architecture, respectively. We then construct a multi-granularity loss function to ensure alignment between these two modalities."}, {"title": "3.2.1 Input Representations", "content": "Due to information granularity discrepancies between road paths and image paths, direct alignment is often disturbed by irrelevant information. To solve this problem, we use a sequence of fixed-size images, instead of a single image commonly used in traditional image-text multi-modal methods [2, 30, 37, 40], to model image paths. This procedure preserves the scale and shape features of the images by avoiding distortions caused by inconsistencies in image sizes. Furthermore, as these fixed-size images can be utilized across different paths, the storage of images is reduced. Then, we utilize specialized tokenizers to separately encode the data of each modality into a unified format.\nThe patch tokenizer segments each image within an image path into a series of patches to extract fine-grained semantic information. Specifically, as shown in Figure 2, an image mi \u2208 Rrxrxc is reshaped into a sequence of r\u00b2/o\u00b2 (e.g., 16) patches, where c represents the number of channels, (r, r) denotes the resolution of fixed-size image, and (o, o) defines the resolution per patch. After patching, we concatenate the patch sequences from all images within a image path to form a unified patch sequence. Then, we place a special [cls] token at the beginning of the patch sequence. As the [cls] token captures the global information of the entire sequence [1], it can be regarded as a representation of the entire image path. Special [sep] tokens are placed at the end of each image to delineate local information of each image. For example, the token sequence of image path M(p) = (m1, m2, m3) is [cls, m(1)1,..., m(1)16, sep, ..., ..., sep, m(j)i,..., m(j)16, sep], where m(j)i\u2208 Roxoxc denotes the j-th patch of the i-th image.\nEach patch m(j)i is then projected into a patch embedding m(j)i \u2208 Rd, which can be initialized using pre-trained ResNet50 [20]. The image initial embeddings are computed by summing the patch embeddings with the image position embeddings Timage \u2208 Rn1\u00d7d, resulting in H(0) = [mcls, m(1), ..., m(r2/o2), msep|M(p)] + Timage. Here, mcls and msep are the image initial embeddings of the [cls] and [sep] tokens, respectively. n1 denotes the length of the patch token sequence, and d represents the dimension of the embeddings.\nThe modeling for a road path is similar, starting with a [cls] token at the beginning and placing [sep] tokens at the end of each road sub-path. For instance, a road path R(p) comprising three road sub-paths-s1 = (\u03c51, \u03c52, \u03c53), s2 = (\u03c54, \u03c55, \u03c56), and s3 = (\u03c57, \u03c58)- generates the node token sequence [cls, \u03c51, \u03c52, \u03c53, sep, \u03c54, \u03c55, \u03c56, sep, \u03c57, \u03c58, sep]. Then, the node tokenizer linearly projects each node vi from"}, {"title": "3.2.2 Image Path Encoding", "content": "To model the image path, the image initial embeddings H(0) are fed into the l layers Image-Transformer, which can be formulated as,\nH(j) = Image-Transformer(H(j-1)), (2)\nwhere j = 1, ..., l, and H(l) \u2208 Rn1\u00d7d is the final output of the Image-Transformer. For a simplify, we denote the image encoded embeddings H(l) as H = [hcls, h(j)1, ..., h(j)r2/o2, hsep|M(p)]. h(j)i \u2208 Rd denotes the encoded embedding of the j-th patch of the i-th image. hsep and hcls \u2208 Rd represent the encoded embeddings of the i-th image and the whole image path M(p), respectively."}, {"title": "3.2.3 Road Path Encoding", "content": "To facilitate alignment between road paths and image paths, we utilize a similar Transformer architecture for road path modeling. The encoder is comprised of l layers of Transformer blocks, defined as:\nP(j) = Road-Transformer(P(j-1)), (3)\nwhere j = 1, ..., l, P(l) \u2208 Rn2\u00d7d is the output of the j-th layer. In a brief, the road encoded embeddings P(l) are represented as P = [pcls, p1,..., p|R(p)|, psep|M(p)]. Here, pi, psep, pcls \u2208 Rd denote the encoded embeddings of the i-th node, the sub-path si, and the entire road path R(p), respectively. |R(p)| represents the number of nodes in road path R(p), and |M(p)| indicates the number of images in image path M(p), which also corresponds to the number of road sub-paths."}, {"title": "3.2.4 Modalities Aligning", "content": "The encoded embeddings from each branch capture the hidden semantic information within their respective modality, including fine-grained node/patch embeddings, medium-grained road sub-path/image embeddings, and coarse-grained entire road path/image path embeddings. We aim for embeddings with similar semantics across modalities to be proximate within the embedding space. Additionally, we seek detailed alignment between the two modalities while maintaining global correspondence. Accordingly, we design a loss function that operates at three distinct levels of granularity-fine, medium and coarse-corresponding to node/patch, road sub-path/image, and entire road path/image path, respectively.\nFine granularity. Since each patch may contain more than one node, the encoded embeddings of a node and the corresponding patch (Ref. as to the dark yellow triangle the dark green rectangle with yellow borders in Figure 3) should maintain directional consistency. To precisely capture the semantic information of fine-grained paths, we minimize the cosine distance between the encoded embeddings of nodes and their corresponding patches. Consequently, we construct the fine-grained loss function as follows:\nLfine = \u2211p\u2208P\u2211vi\u2208R(p),L(vi)=m(k)(1-  pihk||pi||||hk|| ), (5)\nwhere P represents the training set of paths, L(vi) is a function that returns the patch corresponding to the node vi, pi and h(k)j are the encoded embeddings of vi and m(k)j, respectively.\nMedium granularity. Similarly, to align road sub-paths with images, we construct the following medium-grained loss function:\nLmedium = \u2211p\u2208P\u2211si\u2208R(p)(1-  psepihsep||psepi||||hsep|| ), (6)\nwhere psepi and hsepi (Ref. as to the dark yellow rectangle and the dark green triangle with blue borders in Figure 3) are the encoded embeddings of road sub-path si and the corresponding image mi, respectively.\nCoarse granularity. Due to the unique correspondence between road path and the corresponding image path, a clearer distinction is necessary. Therefore, We construct a contrastive loss function for coarse-grained data. Considering a batch of road path-image path pairs B, the objective of this contrastive learning loss is to accurately identify the matched pairs among the |B|\u00d7 |B| possible combinations. Within a training batch, there are |B|2 \u2212 |B| negative pairs. The contrastive learning loss function can be formulated as:\nLcoarse = - \u2211p\u2208P(log(exp(sim(pcl, hcls)/\u03c3)\u2211Eneg\u2208Bexp(sim(pcl, hcls)/\u03c3)) + exp(sim(pcl, hcls)/\u03c3)\u2211Eneg\u2208Bexp(sim(pcl, hcls)/\u03c3)), (7)\nwhere pcls and hcls (Ref. as to the dark yellow rectangle and the dark green triangle with dark blue borders in Figure 3) correspond to the encoded embeddings of the entire road path and image path in a road path-image path pairs. pNeg and mNeg are the negative road path and image path in the batch set B, respectively. \u03c3 is a learned temperature parameter. sim(pcls, hcls) returns the Euclidean distance between pels and hels.\nFinally, the multi-granularity loss can be formulated as Lmulti = Lfine + Lmedium + Lcoarse."}, {"title": "3.3 Graph-based Cross-modal Residual Fusion", "content": "In this framework, each path is represented through two distinct modalities, providing complementary perspectives. To effectively leverage these modalities, we propose a graph-based cross-modal residual fusion component."}, {"title": "3.3.1 Cross-modal Residual Connection", "content": "To facilitate comprehensive information exchange between modalities, we introduce cross-modal residual connections that effectively concatenate embeddings across different stages and modalities. These connections enable direct propagation of gradients to earlier layers, thereby enhancing stability and improving training efficiency. Specifically, we concatenate the road initial embeddings P(0) with the image encoded embeddings H, and the image initial embeddings H(0) with the road encoded embeddings P. The resulting image residual embeddings and road residual embeddings are defined as U = P(0) ||H and Q = P||H(0), respectively. Here, U, Q \u2208 R(n1+n2)\u00d7d, and || denotes the concatenation operation."}, {"title": "3.3.2 Graph-based Fusion", "content": "Although traditional attention mechanisms proficiently identify correlations among entities, they often fail to incorporate contextual information concurrently. To address this limitation, we utilize graph neural networks [4], which incorporate contextual information into the learning process by representing it as graph structures. Leveraging this capability, we introduce a graph-based fusion method to enhance the accuracy of information understanding across different modalities.\nInitially, we construct a specialized cross-modal directed graph for each path. This graph treats all tokens, including [cls] and [sep] tokens from both modalities, as entities. These entities are connected via three types of relationships: intra-modal context, cross-modal correspondence, and cross-modal context. The intra-modal context focuses on interactions within a single modality, facilitating a deep understanding of its specific information. Cross-modal correspondence aids in comprehending and learning the spatial correspondence between different modalities. Cross-modal context addresses indirect relationships between different modal entities, which enhances the model's ability to interpret complex scenes. Collectively, leveraging these relationships significantly boosts the model's capacity to handle multi-modal data effectively.\nFigure 4 demonstrates the construction of the graph. Taking node \u03c54 as an example, node \u03c54 is connected by directed edges from five entities: adjacent context nodes \u03c53 and \u03c55, its corresponding patch L(\u03c54), and their respective patches L(\u03c53) and L(\u03c55). The patch L(\u03c54) (i.e., m(j)i) is connected by directed edges from nine entities, including \u03c53, \u03c54, \u03c55, L(\u03c53), L(\u03c55), and four geographically adjacent image patches m(j-1)i , m(j+1)i, m(j)i-1,and m(j)i+1, where j denotes the number of patches per row of an image. Additionally, the [sep] tokens are connected by their context [sep] tokens, corresponding [sep] tokens from another modality, and cross-modal context tokens. The [cls] token, encapsulating more global information, is connected by all [sep] tokens and corresponding [cls] token from another modality.\nThen, we construct an adjacency matrix A \u2208 R(n1+n2)\u00d7(n1+n2) for each path p to capture the comprehensive relation within the multi-modal data. Given the effectiveness of GCNs in transferring and fusing information across entities within a graph structure, we employ a GCN to derive the updated embeddings for both branches. The updated embeddings are computed as follows:\n\u00db = Relu(D\u00c2D Relu(D\u00c2DUW1)W2), (8)\nQ = Relu(D\u00c2D Relu(D\u00c2DQW3)W4), (9)\nwhere W1, W2, W3, W4 \u2208 Rd\u00d7d are weight matrices, and D is the degree matrix of \u00c2. The augmented adjacency matrix \u00c2 = A + I', where I' is a modified identity matrix with all diagonal elements set to 1, except for those corresponding to patches without relationship to any nodes (Ref. as to the dark green rectangle with a white border in Figure 3). This modification aims to exclude patches that are relatively unrelated to the path, thereby preventing the introduction of noise into the model.\nAfter iterative graph convolution operations, the embeddings of each entity within the graph are updated. We perform average pooling on \u00db and Q to aggregate the updated embeddings, respectively. The fused embedding for each branch is then obtained by:\ny = AvgPooling(\u00db), (10)\nz = AvgPooling(Q), (11)\nwhere y, z \u2208 Rd denote the image fused embedding and the road fused embedding, respectively."}, {"title": "3.3.3 Cross-modal Constrastive Loss", "content": "The image fused embedding y and the road fused embedding z encapsulate features across multiple modalities of the same path, reflecting an inherent similarity. Therefore, we implement a quadruplet loss function to ensure that the difference between y and z is smaller than the differences with the fused embeddings of other paths. For negative samples, we randomly sample the image fused embedding yn and the road fused embedding zn from the batch. The loss function is defined as:\nLfuse = \u2211p\u2208P([||y - z|| - ||y - zn||+ + \u03b2]+ + [||y - z|| - ||z - yn||+ + \u03b2]+), (12)\nwhere \u03b2 is a hyperparameter that controls the margin of the distance between pairs of positive and negative samples, and [\u00b7]+ is a shorthand for max(0,.)."}, {"title": "3.4 Training objective", "content": "The final training objective of our model integrates all previously proposed loss functions, formulated as follows:\nL = \u03bbmaskLmask + \u03bbmultiLmulti + \u03bbfuseLfuse, (13)\nwhere \u03bbmask, \u03bbmulti and \u03bbfuse are the weights assigned to Lmask, Lmulti, and Lfuse.\nAfter pre-training, we combine the image fused embedding y with the road fused embedding z into a generic path embedding x = y||z, achieving a more robust and generalized representation. The generic path embedding is then fine-tuned using interchangeable linear layer task heads, enabling the model to adapt to a variety of downstream tasks effectively."}, {"title": "4 Experiments", "content": "Experimental Setups"}, {"title": "4.1", "content": "Datasets. We utilize the road networks, GPS datasets, and remote sensing image datasets of two cities: Aalborg, Denmark, and Xi'an, China. The road networks are sourced from OpenStreetMap\u00b9, while the remote sensing image datasets are acquired from Google Earth Engine[15]. Employing an existing tool [31], we map-match all GPS records to road networks to generate the path datasets and historical trajectory datasets. The details of the datasets are shown in Table 1."}, {"title": "4.1.1", "content": "Implementation Details. All experiments are conducted using PyTorch [33] on Python 3.8 and executed on an NVIDIA Tesla-A800 GPU. Each fixed-size image is 500 \u00d7 500 pixels, with each pixel corresponding to 2 meters on the earth. In other words, an image covers a 1km \u00d7 1km region. We segment each image into 16 patches and set the embedding dimension d to 64. Both the Image-Transformer and Road-Transformer comprise five layers. To enhance the pre-training efficiency, we initialize our Road-Transformer with the pre-trained LightPath [45]. The mask ratio is set at 15%. The weights \u03bbmask, \u03bbfuse, and \u03bbmulti are uniformly set to 1. Training proceeds for up to 60 epochs with a learning rate of 0.02. The linear layer task head includes two fully connected layers, with dimensions of 32 and 1, respectively. Training MM-Path on the Aalborg and"}, {"title": "4.1.2", "content": "Downstream Tasks and Metrics. Path Travel Time Estimation: We calculate the average travel time (in seconds) for each path based on historical trajectories. The accuracy of travel time estimations is evaluated using three metrics: Mean Absolute Error (MAE), Mean Absolute Relative Error (MARE), and Mean Absolute Percentage Error (MAPE). Path Ranking Score Estimation (Path Ranking): Each path is assigned a ranking score ranging from 0 to 1, derived from historical trajectories by following existing studies [44\u201346]. We evaluate the effectiveness of path ranking using MAE, the Kendall rank correlation coefficient (\u03c4), and Spearman's rank correlation coefficient (\u03c1)."}, {"title": "4.1.3", "content": "Baselines. We compare the proposed model with 5 unsupervised single-modal path pre-trained methods and 5 unsupervised multi-modal methods. The single-modal path pre-trained methods are:\n\u2022 Node2vec [16]: This is an unsupervised model that learn node representation based on a graph.\n\u2022 PIM [3]: This is an unsupervised path representation learning approach based on mutual information maximization.\n\u2022 LightPath [45]: This is a lightweight and scalable path representation learning method.\n\u2022 TrajCL [5]: This is a contrastive learning-based trajectory modeling method.\n\u2022 START [21]: This is a self-supervised trajectory representation learning framework with temporal regularities and travel semantics.\nThe multi-modal methods are:\n\u2022 CLIP [37]: This is a classic pre-trained multi-modal model. For each path, we use a single rectangular image for the image modality and replace the original text sequence with a node sequence. After pre-training, we concatenate the representations of the two modalities and use them as input to the linear layer task head.\n\u2022 USPM [8] utilizes both images and road network to profile individual streets, but not paths (i.e., sequences of streets). We adapt USPM to support path representation learning.\n\u2022 JGRM [29]: This is a representation learning framework that combines GPS data and road network-based data. In this study, we replace the GPS data with the image path.\n\u2022 LightPath+image: This is a multi-modal variant of Light-Path. We concatenate the patch embedding with the node embedding to replace the original node embedding for training.\n\u2022 START+image: This is a multi-modal variant of START, processed similarly to LightPath+image.\nFor all methods, we standardize the embedding dimensionality (d) to 50. All parameters are set according to the specifications in the original papers. All baselines are fine-tuned using a linear layer task head. The output of this task head serves as the prediction result. For all methods, we initially pre-train using unlabeled training data (e.g., 30K unlabeled Aalborg dataset and 160K unlabeled Xi'an dataset). Subsequently, we use a smaller volume of labeled data (e.g., 10K labeled Aalborg dataset and 40K labeled Xi'an dataset) for task-specific fine-tuning. Validation and evaluation processes are conducted on separate validation dataset (e.g., 5K Aalborg dataset and 20K Xi'an dataset) and test dataset (e.g., 10K Aalborg dataset and 40K Xi'an dataset), respectively."}, {"title": "4.1.4", "content": "Overall Performance. Table 2 presents the overall performance on both tasks. We use '\u2191' (and '\u2193') to indicate that larger (and smaller) values are better. For each task, we highlight the best and second-best performance in bold and underline. \"Improvement\" and \"Improvement*\" quantify the enhancements achieved by MM-Path over the best single-modal and multi-modal baselines, respectively.\nOverall, MM-Path outperforms all baselines on these tasks across both datasets, demonstrating its superiority. Specifically, we can make the following observations: The graph representation learning method Node2vec significantly underperforms compared to MM-Path, primarily due to its focus solely on the topological information of nodes while overlooking the sequential information of paths. Single-modal models like PIM, LightPath, and TracjCL show improved performance over Node2vec, indicating the importance of capturing sequential correlations within paths. Among the single-modal models, START achieves the best performance. It adeptly integrate sequential path information with spatio-temporal transition relationships derived from historical trajectory data. However, as a single-modal model, its capabilities are inherently constrained. As a multi-modal model, CLIP exhibits the weakest performance. Designed primarily for general corpora, it focuses on single, coarse-grained image representations, which often introduce noise into path modeling. Consequently, CLIP struggles to effectively capture complex spatial information and correspondences, making it unsuitable for modeling paths. USPM performs poorly because it analyzes individual streets using images and road networks, rather than paths (i.e., street sequences). As a result, it fails to effectively mine the sequential relationships present in the two modalities. The variants LightPath+image and START+image perform comparably to their single-modal models (i.e., LightPath and START), suggesting that"}, {"title": "4.2", "content": "Ablation Study. We design eight variants of MM-Path to verify the necessity of the components of our model: (1) MM-Path-z: This variant leverages the road fused embedding z as a generic representation of the path. (2) MM-Path-y: This model utilizes the image fused embedding y as a generic representation of the path. (3) w/o alignment: This version excludes the multi-granularity loss. (4) w/o fusion: This variant substitutes the graph-based residual fusion component with average pooling of the encoded embeddings from both modalities. (5) w/o GCN: This model replaces the GCN in the graph-based cross-modal residual fusion component with a cross-attention mechanism. (6) w/o fine", "coarse": "These variants omit the fine-grained, medium-grained, and coarse-grained loss, respectively.\nThe results are summarized in Tables 3 and 4. We can observe that MM-Path w/o alignment shows poor performance, which is attributed to its reliance solely on multi-modal data fusion without considering multi-granularity alignment. The variants, w/o fine, w/o medium, and w/o coarse, outperform w/o alignment but still worse than the full MM-Path, demonstrating the"}]}