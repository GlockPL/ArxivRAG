{"title": "Sparsity-based Safety Conservatism for Constrained Offline Reinforcement Learning", "authors": ["Minjae Cho", "Chuangchuang Sun"], "abstract": "Reinforcement Learning (RL) has made notable success in decision-making fields like healthcare, autonomous driving, and robotic manipulation. Yet, its reliance on real-time feedback from the environment poses challenges in costly or hazardous settings. Furthermore, RL's training approach, centered on \"on-policy\" sampling assumptions, doesn't fully capitalize on previously gathered data. Hence, Offline RL has emerged as a compelling alternative, particularly in scenarios where conducting additional experiments is impractical and abundant datasets are available for leverage. However, the well-documented challenge of distributional shift (extrapolation), indicating the disparity between data distributions and learning policies, also poses a risk in offline RL, potentially leading to significant safety breaches due to estimation errors (interpolation). This concern is particularly pronounced in safety-critical domains, where real-world problems are prevalent. To address both extrapolation and interpolation errors, numerous studies have introduced additional constraints to confine policy behavior, steering it towards a more cautious decision-making regime. While many studies have delved into addressing extrapolation errors [1]-[3], fewer have focused on providing effective solutions for tackling interpolation errors. For example, [4] and [5], tackle this issue by incorporating potential cost-maximizing optimization by perturbing the original dataset. However, this, involving a bi-level optimization structure, may introduce significant instability or complicate problem-solving in high-dimensional tasks, calling for an effective method to induce conservatism. This motivates us to pinpoint areas where hazards may be more prevalent than initially estimated based on the sparsity of available data by providing significant insight into constrained offline RL. In this paper, we present conservative metrics based on data sparsity that demonstrate the high generalizability to any methods and efficacy compared to using bi-level cost-ub-maximization.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) has demonstrated remarkable success in decision-making across diverse domains, including healthcare, autonomous driving, and robotic manipulation. The development of efficient learning algorithms has empowered RL systems to extract valuable insights and learn optimal strategies through interactions with their environments, yielding substantial success. However, the current RL paradigm, reliant on an impractical number of experiments, encounters difficulties in environments where real-time feedback is either costly or hazardous. This limitation impedes the practical deployment of RL in many real-world scenarios.\nA significant constraint of RL stems from its dependence on \"on-policy\" sampling assumptions during training, limiting its efficacy in actively sampling domains, despite the abundance of pre-collected data in most settings. To mitigate this constraint, Offline RL has garnered attention as an alternative approach, especially in situations where conducting further experiments is impractical, yet substantial datasets are accessible for learning. While the transition towards an offline learning paradigm presents promising prospects, it also introduces fresh challenges, prominently the concern of estimation errors.\nEstimation errors present a substantial risk in offline RL, as they result in inaccurate predictions for unseen state-action pairs, often leading to overestimation of less favorable areas. This phenomenon, known as distributional shift, highlights disparities between the distributions of data and learning policies. Distributional shift encompasses both extrapolation and interpolation errors, both of which fall under the category of estimation errors. While previous methods have effectively addressed the avoidance of extrapolation While [1]\u2013[3], interpolation errors persist as a critical concern, especially in safety-critical domains, where the ramifications of such errors can be severe.\nResearchers have pursued avenues to address distributional shifts, employing constraint methods that impose additional restrictions to steer policies towards more conservative behavior. While considerable attention has been directed toward mitigating extrapolation errors, fewer studies have focused on effectively combating interpolation errors. Some efforts have been made to address interpolation errors in safety-critical domains by integrating potential cost-maximizing optimization techniques, such as perturbing the original dataset [4], [5]. However, these methods frequently entail intricate bi-level optimization structures, which may introduce instability or complicate problem-solving, particularly in high-dimensional tasks. Furthermore, these methods do not demonstrate compliance with the specified safety thresholds, merely increasing cost estimates for potentially hazardous regions based on estimations. As a result, there is a pressing need for more efficient approaches to incorporate conservatism into offline RL systems.\nMotivated by these challenges, this paper aims to identify areas where hazards may be more prevalent than initially estimated based on the sparsity of available data. We present extensive empirical findings that demonstrate the effectiveness of our approach compared to traditional bi-level cost upper-bound maximization methods. Through this research, we aim to achieve: 1) an efficient safe-conservatism method, and 2) generalizability that is applicable to any algorithmic setting."}, {"title": "II. PRELIMINARIES", "content": "Constrained Markov Decision Process (CMDP) [7] considers additional Cost function $C(s, a)$ into an MDP representation encapsulated by $M=(S, A, T, R, C, p_0, \\gamma)$. Here, $S$ denotes the state set, $A$ denotes the action set, $R : S \\times A \\rightarrow R$ and $C: S \\times A \\rightarrow R$ are the reward function and cost function respectively with $T : S \\times A \\rightarrow S'$ represents the transition probabilities. Additionally, $p_0 \\subset S_0$ characterizes the initial state distribution, while $\\gamma \\subset [0,1]$ stands as the discount factor. The policy $\\pi: S \\rightarrow A$ plays a central role, mapping states to action distributions. Our objective here is to maximize this mapping, thereby enhancing the cumulative reward while meeting specified safety constraints.\nConstrained online RL, as exemplified by [8] and [9], rely on \"on-policy\" samples, updating the policy with samples collected under the currently updating policy. However, Constrained Offline Reinforcement Learning (CORL), as demonstrated in the work by [5], entails the agent's learning of an optimal policy using a fixed static dataset (\u201coff-policy samples\u201d) $D = \\{(s_i, a_i, r_i, c_i, s'_i)\\}_{i=1}^{N}$ without any knowledge of how the dataset was initially generated. This transition from an online to offline paradigm introduces several challenges that necessitate: 1) intricate hyperparameter adjustments to mitigate overfitting or underfitting, 2) effective generalization to capture the broader environmental dynamics, and 3) robust techniques for Off-Policy Evaluation (OPE) for policy updates with minimal violations of estimations."}, {"title": "B. Off-Policy Evaluation", "content": "OPE involves estimating the expected performance of a learning policy based on historical experiences stored in dataset $D$ to maximize estimated cumulative returns. Therefore, OPE fulfills several crucial roles, including furnishing high-confidence guarantees prior to deployment and aiding in hyperparameter tuning to evade suboptimal solutions. The sensitivity to factors like learning rates and the level of conservatism regarding Out-of-Distribution (OOD) or in-distribution underscores the importance of effective OPE methods for policy validation. While multiple OPE approaches are available, our focus will be on Importance Sampling (IS) due to its conciseness and relevance to both our theoretical and experimental framework."}, {"title": "1) Importance Sampling:", "content": "IS is an OPE method that gauges the effectiveness of a learning policy as a value function by approximating the probability ratio between the behavior policy $\\pi_\\beta$ and our training policy $\\pi$. The value estimation of $\\pi$ is articulated as follows:\n$V_\\pi = E_{(s,a)\\sim d_s} W_{0:H} \\sum_{t=0}^{H} r(s_t, a_t)$  (1)\nHere, $W_{0:H} = (\\prod_{t=i}^{H} \\pi(a_t|s_t))/({\\prod_{t=i}^{H} \\pi_\\beta(a_t,s_t)})$ represents the product of importance weights and accounts for the ratio of probabilities under policies $\\pi$ and $\\pi_\\beta$ for a given trajectory. Intuitively, this estimation involves computing the cumulative rewards multiplied by the disparity in visitation frequencies for each state-action pair in the dataset. However, this product's inclusion over the entire horizon (0 to H) can introduce high variance in the correction term $w$, resulting in ratios that are so close to zero that they are practically indistinguishable. To address this, variance reduction techniques like doubly robust and marginalized IS, embraced by the DICE-family (DIstribution Correction Estimation) algorithms, have emerged. These methods have excelled in various domains due to their low variance and solid performance by estimating the stationary distribution $d^\\pi$ through marginalized probability ratios, where $d^\\pi$ is defined as:\n$d^{\\pi}(s, a) = \\begin{cases}\n    \\frac{1}{T + 1}\\sum_{t=0}^{T}\\Pr(s_t, a_t), & \\text{if $\\gamma = 1$}.\n    (1 - \\gamma)\\sum_{t=0}^{\\infty} \\gamma^t\\Pr(s_t, a_t),              & \\text{if $\\gamma < 1$}.\n\\end{cases}$   (2)\nThe DICE algorithms with additional cost consideration aim to maximize the expected return of policy $\\pi$ through the following optimization shape:\n$\\max\\limits_{\\pi}\\quad E_{(s,a)\\sim d_D} [R(s, a)w(s, a)]$\n$\\\\quad s.t.\\quad E_{(s,a)\\sim d_D} [C_k(s,a)w(s,a)] \\leq \\hat{c}_k$\n$\\sum_{a'}d(s', a') = (1 - \\gamma)p_0(s') + \\gamma\\sum_{s,a} dP(s, a)T(s'|s,a)$ \nHere, $w(s, a) = \\frac{d^{\\pi}(s,a)}{d_D(s,a)}$ (marginalized probability ratios), and the term $E_{(s,a)\\sim d_D}[\\hat{R}(s, a)w(s, a)]$ is therefore the estimated return of policy $\\pi: [R(s,a)d^\\pi]$. This entire process relies exclusively on the state-action distribution within the provided dataset D and the estimation of $w(s, a)."}, {"title": "2) COptiDICE: Constrained Offline Policy Optimization via DICE:", "content": "COptiDICE is a constrained offline RL algorithm that directly estimates the stationary distribution corrections of the optimal policy by preventing both OOD and in-distributional actions:\n$\\max\\limits_{\\pi}\\quad E_{(s,a)\\sim d_D} [R(s, a) - \\alpha D_f(d^{\\pi}||d_D)]$\n$\\quad s.t.\\quad E_{(s,a)\\sim d_D} [C_k(s,a)w(s,a)w^*(s_0, s, a, s')] \\leq \\hat{c}_k$\n$\\sum_{a'}d(s', a') = (1 - \\gamma)p_0(s') + \\gamma\\sum_{s,a} dP(s, a)T(s'|s, a)$  (3)\nwhere $\\alpha D_f(d^{\\pi}||d_D)]$ is restricts OOD actions scaled by $\\alpha$ and $w^*(s_0, s, a, s')$ is normalized cost upper bound estimation to further bring conservatism within in-distributional actions. It has demonstrated competitive performance in Tabular MDP and robotic control tasks in safety-constrained settings when compared to other state-of-the-art methods."}, {"title": "C. K-mean Clusterings", "content": "K-means clustering is a popular unsupervised machine learning technique used in data science. The goal of K-Means is to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean (centroid). The algorithm aims to minimize the variance within each cluster. Mathematically, it can be expressed as follows:\n$\\arg\\min_G \\sum_{i=1}^K \\sum_{x \\in G_i} ||x - \\mu_i||^2$ (4)\nwhere $G$ is the clustering solution, $\\mu_i$ is the centroid of cluster i, and $||x - \\mu_i||$ represents the Euclidean distance between data point x and centroid $\\mu_i$. K-means offers simplicity and efficiency, making it a valuable tool for data analysis, image processing, and more. We utilize this clustering technique to efficiently discretize the continuous state space S into K number of clusters for additional cost penalization by measuring their sparsity within the cluster."}, {"title": "III. SIMPLE SPARSITY-BASED COST PENALTY", "content": "As outlined in Section II, DICE algorithms represent a model-free policy gradient method that estimates marginalized visitation ratios between the distributions of data and the learning policy. Because this requires an estimation with a finite set of data, estimation error is inevitable. This observation highlights a significant trend: while numerous methods concentrate on mitigating extrapolation using f-divergence metrics between the distributions of the learning policy and the data, there's a noticeable absence of studies addressing interpolation errors stemming from discontinuities in the inner distributions of the data.\nHence, relying on previous naive OPE methods for learning can result in safety breaches primarily due to interpolation errors, as illustrated in [5]. Given the considerable challenges of ensuring uncertainty in offline and nonlinear settings, a straightforward yet effective approach is to introduce additional conservatism. Specifically, one may alleviate safety violations in naive OPE via extra conservatism for estimation errors. However, one must take a holistic and sophisticated approach to measure such conservatism by avoiding over-conservatism but effectively addressing estimation errors. Thus, our motivation for applying a stronger penalty in sparser regions is twofold: 1) estimation errors by interpolation are more prone to occur in sparse data regions, and 2) the need to avoid over-conservatism to preserve the potential for the highest return. In essence, our aim is to introduce a relative degree of conservatism by categorizing the given data distributions into regions of confidence and regions of lesser confidence. This effectiveness is demonstrated in Section IV through a comparison with naive penalty methods (i.e., simply multiplying an integer for penalization)."}, {"title": "A. Tabular settings", "content": "Through the empirical studies, we found that the number of visits was effective in tabular settings. Emerged as a measure of the 'exploration', it was initially incorporated into Reinforcement Learning as a term bonus in [10]. Bonus, $b(s, a)$, is a function of state-action, and it is defined as $b(s,a) \\propto 1/\\sqrt{n(s,a)}$, where $n(s,a)$ representing the ac- cumulated visits to state-action pairs. By incorporating this measure, the policy update involves subtracting a 'bonus' from the expected return: $\\pi_{k+1} = \\arg\\max_{\\pi} (Q_\\pi - b(s, a))$.\nThis reduction in expected values for less-explored (uncertain) state-action pairs helps restrict out-of-distribution actions. In constrained offline RL, we demonstrate that this 'bonus' is also valuable for identifying potentially more hazardous state-action pairs in tabular settings. In essence, we repurpose the bonus for cost overestimation, introducing a hyperparameter $\\alpha > 0$. To connect the notations to that of COptiDICE, we use $w_{tabular}(s, a)$ instead of bonus, and formulation becomes:\n$E_{(s,a)\\sim d_D} [C_k(s, a)w(s, a)w_{tabular}(s,a)] \\leq \\hat{c}_k$. (5)\nwhere $w_{tabular}(s,a) := [\\alpha/\\sqrt{n(s,a) + 1}] \\geq 1$. Prior to imposing this constraint on Equation (3), we illustrate the estimation error of naive COptiDICE (without $w_{tabular}(s, a)$) in Figure 1 which is Random MDP with composition of $(S \\times A C R^{50 \\times 4})$. Each small dot, denoting the top 10 estimation errors, corresponds to relatively high conservatism (indicated by red-tone colors, with specific values shown in the adjacent bar). This implies that less-explored regions are more susceptible to errors. For experiment results in a tabular setting, please see Figure 3."}, {"title": "B. Continuous State Space", "content": "While the bonus, $b(s, a)$, proved valuable in simple discrete tabular settings, its applicability wanes in the case of con- tinuous state-action spaces. Counting state-action visitation, $n(s, a)$, in continuous contexts becomes pointless due to the infinite number of possible choices, rendering it an ineffectual measure. Additionally, the challenge of scalability arises when discretizing state-action spaces to continue utilizing bonus metrics. This involves the need to partition spaces effectively, which can become infeasible for tasks with high-dimensional state and action spaces. To overcome this challenge, [10]"}, {"title": "CartPole Data Sparsity Visualization", "content": "advocated the use of counting by density models [11], [12] of the $k^{th}$ clusters:\nor counting after hash function [13] to gauge the extent of the exploration. Nonetheless, both of these methods entail the\ndevelopment of an additional model, introducing an added $w_k(s) = Normalize \\frac{\\sum_{i=1}^{N_m} e^{\\frac{-(C_k-d_{ie})^2}{2}}}{(N_m)}  (6)$\nlayer of complexity. However, our approach offers a novel solution as a simple pre-processing stage before training\nbegins. In particular, by measuring sparsity exclusively from In this formula, $N$ represents the number of data points in each\nthe provided static dataset, it circumvents the necessity for an cluster, $C_k$ is the centroid of the cluster, $d_{ie}$ denotes the i-th\nextra model or optimization problem, as mandated by prior element within the $k^{th}$ cluster, and m is the state dimension.\nmethodologies. This calculation provides a normalized measure of sparsity for\neach given data. Building on the methodology outlined in [5],\nwe utilized the following conservatism metric for each batch\n1) Sparsity Measure: As part of the pre-training process, update:\nwe can measure point-wise nearest numerical deviations to\nquantify sparsity. While this approach intuitively gauges how $w^*(s) := softmax \\frac{[w_k(s)]}{w_k(s) * batch\\_size}, w^*(s) > 1  (7)$\nclosely a data point is positioned to its neighbors, it de-\nmands significant computational resources to handle high- The above definition allows for an exponential penalty,\ndimensional datasets with large volumes. This computational applied to sparse regions using the normalized conservatism\nchallenge renders the use of such a sparsity metric less score in Equation (6) for the data in the k-th cluster. In contrast,\nappealing compared to previous methods, primarily due to its approaches like [4] and [5] have tackled this issue by em-\nimpracticality. ploying an additional bi-level cost upper bound optimization\nHowever, in this paper, we address this issue by grouping through perturbation. Meanwhile, [11], [12], and [13] require\ndata into K number of clusters using K-means clustering, the use of an additional model for cost overestimation, which\ndenoted as $S \\subset D^{n \\times m} \\Rightarrow S_k \\subset D^{n_k \\times m}$, where D is the are also susceptible to weaknesses in hyperparameter tuning.\ngiven dataset. By discretizing the continuous state space into In the following section, we evaluate the effectiveness of the\nK clusters, we relax the computational burden and measure sparsity-based penalty through empirical studies. We provide\nnumerical deviations of surrounding data points with respect our pseudo-code in Algorithm 1.\nto their respective centroids. In particular, we refrain from\ndividing the data dimensions into uniform sectors, opting\ninstead to discretize them into clusters. This allows us to\nconsider the respective sparsity of data instead of discretizing\nthe whole data regime into uniform sectors."}, {"title": "IV. EXPERIMENTS", "content": "In this experiment, we performed a comparative analysis between our sparsity-based penalization approach and the perturbation method employed by [5], using their respective algorithms. Given the limited existing research in the field of constrained offline Reinforcement Learning, we selected Constraints Penalized Q-learning (CPQ) [14] and Constrained Offline Policy Optimization via Stationary DIstribution Correction Estimation (COptiDICE) [5] as our comparative methods. For our experimental setup, we opted for the Random CMDP environment as utilized by [5] and [15] for discrete state and action spaces. Additionally, we selected four cost-violating environments from benchmarks provided by Datasets for Safe Reinforcement Learning (DSRL) [16], using RWRL suite [6] for continuous state-action spaces."}, {"title": "A. Discrete: Random CMDP", "content": "Following [5] and [15], we created a random Constrained Markov Decision Process (CMDP) with the following parameters for our experiments:\n\u2022 Random CMDP ($S \\subset R^{50}, A \\subset R^{4}$): A goal was placed in rarely-visited states by assigning a reward of 1, while all other states received a reward of 0. The cost function, C(s,a), was generated randomly with a cost threshold $\\hat{c}$ = 0.1.\nThe transition probabilities were randomly generated with a connectivity of 4, and $|A| = 4$.\n1) Evaluation: With both optimal and behavior policies, we compare our SP-cdice (SParsity-based conservative COptiDICE) algorithm, described in Algorithm 1, with its naive and conservative versions. For comprehensive evaluation, we also include the policy iteration algorithm for this discrete environment setting. Running 10 seeds, our SP-cdice demonstrates performance close to the conservative COptiDICE while remaining less computationally expensive. This suggests that SP-cdice can be applied to other settings, such as continuous environments, as discussed below."}, {"title": "B. Continuous: CartPole", "content": "Here, we showcase the performance of our approach in a continuous setting. Specifically, we test it in the Cartpole environment from RealWorldRL (RWRL) [6], as in [5]:\n\u2022 Cartpole ($S \\subset R^{4}, A \\subset R^{1}$): The cart aims to keep the pole upright without falling, while being constrained to stay within specified regions. The action is the force applied to the body of the cart.\nOur primary focus is on validating the efficacy of sparsity-based conservatism. To achieve this, we apply a multiplier to the corresponding costs, penalizing the cost in sparse regions at a higher scale compared to dense regions. This approach suggests a relative degree of conservatism between sparse and dense data regions to effectively address interpolation errors. In contrast, the naive penalty approach involves multiplying a constant across all costs in the data for overestimation.\n1) Evaluation: The value of $\\alpha$ was selected to ensure the curve stays below the cost limit through trial and error, demonstrating our approach's relative degree of conservatism. This is validated by our return results, showing that our method achieves higher returns while remaining under the cost limit."}, {"title": "V. RELATED WORKS", "content": "Constrained reinforcement learning (RL) has been an active research area, with significant contributions from various studies. [8] introduced the trust-region approach for constrained RL, ensuring monotonic improvement with safety guarantees. Building on this, [17] extended its theoretical guarantees to the constrained meta-learning setting. [4] employed Conditional Value-at-Risk (CVaR) as a constraining metric, while [18] implemented a shielding mechanism. [19] conducted a theoretical study on regret in constrained settings, and [20] utilized Gaussian Processes (GP) for robust probabilistic uncertainty analysis.\nIn the context of offline RL, several notable works have emerged. [3] presented a model-based approach without constraints, incorporating an uncertainty penalty to remain within the data regime. [21] enabled robust out-of-distribution exploration by leveraging causal knowledge and novel network architectures to detect erroneous predictions. [22] proposed a value-based approach that also constrains out-of-distribution actions. Finally, model-free approaches such as [5], [23] estimated stationary distributional correction to bridge offline gradient estimators to online settings."}, {"title": "VI. CONCLUSION", "content": "We presented a novel approach to sparsity-based safe reinforcement learning, leveraging the power of clustering to introduce an additional layer of conservatism in offline RL settings. By testing our SP-cdice (SParsity-based conservative COptiDICE) algorithm in both discrete and continuous environments, we have demonstrated its effectiveness and computational efficiency. Our SP-cdice algorithm performs comparably to conservative COptiDICE, achieving substantial returns while being highly generalizable as a preprocessing step for data, indicating its potential for broader applications."}]}