{"title": "Efficiently serving large multimedia models using EPD Disaggregation", "authors": ["Gursimran Singh", "Xinglu Wang", "Ivan Hu", "Timothy Yu", "Linzi Xing", "Wei Jiang", "Zhefeng Wang", "Xiaolong Bai", "Yi Li", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "abstract": "Large Multimodal Models (LMMs) extend Large Language\nModels (LLMs) by handling diverse inputs such as images,\naudio, and video, but at the cost of adding a multimodal en-\ncoding stage that increases both computational and mem-\nory overhead. This step helps convert raw inputs into to-\nkenized representations that inflate the token sequence for\nthe prefill phase, negatively impacting key Service Level\nObjectives (SLOs) like time to first token (TTFT) and end-\nto-end throughput. We introduce Encode-Prefill-Decode\n(EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated re-\nsources. Unlike current systems, which bundle encoding\nand prefill together, our disaggregation approach alleviates\nmemory bottlenecks, mitigates synchronization delays, and\nsupports flexible batching. Specifically, we employ a new\ncaching mechanism for multimodal tokens, enabling asyn-\nchronous transfer of multimodal tokens and introduce an in-\ntegrated module to find optimal config for EPD system and\nminimize resource usage while maximizing SLO-based per-\nformance metric. Experimental evaluations with popular\nLMMs show substantial gains in memory efficiency (up to\n15\u00d7 lesser for encoding-stage GPUs), that supports upto\n22x higher batch sizes, 10\u00d7 more number of images/ re-\nquest, 2.2\u00d7 higher kv cache size. Further, it leads to sig-\nnificant improvements in end-to-end throughput (up to 57%\nbetter), and latency metrics (TTFT up to 71% lower), com-\npared to systems that do not disaggregate. Our findings\nunderscore the potential of EPD disaggregation to enable\nresource-efficient and high-performance multimodal infer-\nence at scale.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized lan-\nguage understanding and reasoning, achieving superhuman\nperformance across a variety of tasks [1, 4]. Recently,\nthe scope of these models has expanded to include multi-\nple modalities, such as images, audio, and videos, result-\ning in the advent of Large Multimodal Models (LMMs)\n[5, 11, 21]. LMMs enable users to interact with diverse\ndata types, such as posing questions about visual scenes or\nanalyzing audio clips, thereby unlocking novel applications\nacross fields like healthcare, autonomous systems, and cre-\native industries.\nHowever, serving LMMs in an efficient manner presents\nunique challenges. Meeting strict Service Level Objectives\n(SLOs) such as end-to-end throughput (E2ETP), time to\nfirst token (TTFT), and time per output token (TPOT) be-\ncomes increasingly difficult given the additional computa-\ntional and memory demands introduced by processing mul-\ntimodal data [12]. Unlike LLMs, where inference involves\nprefill and decoding stages, LMMs require an additional en-\ncoding stage to process raw multimodal inputs (e.g., im-\nages or videos) into tokenized representations. This stage\nis computationally intensive, especially for high-resolution\nor complex multimodal inputs, and often produces a sub-\nstantial number of additional tokens [20]. The resulting\nincrease in tokens inflates resource consumption and leads\nto quadratic growth in prefill-stage compute demands, ad-\nversely impacting SLO attainment.\nDisaggregating the prefill stage from the decode stage\nhas emerged as a well-studied solution in the literature to\nimprove inference efficiency for LLMs [7, 9, 14, 15, 24]. By\nassigning separate resources to each stage, prefill-decode\ndisaggregation enables independent optimization of batch-\ning, scheduling, and resource allocation strategies, signif-\nicantly enhancing system throughput and memory utiliza-\ntion. However, these techniques fail to address the chal-\nlenges of LMM deployment, where the addition of an en-\ncoding stage fundamentally alters the resource dynamics.\nThe encoding stage, with its high computational and mem-\nory overhead, introduces token inflation and dependency\nbottlenecks that ripple across subsequent stages, necessitat-\ning a rethinking of disaggregation strategies to accommo-\ndate multimodal workloads.\nIn current systems, the encoding and prefill stages are\ntypically aggregated into a single monolithic and syn-\nchronous step executed on the same set of GPUs. This"}, {"title": "2. Related Work", "content": "Serving LLMs and LMMs, which often consist of billions\nof parameters, poses significant challenges due to slow in-\nference speeds [12, 25]. Efficiently serving these models to\nbillions of users while adhering to stringent Quality of Ser-\nvice (QoS) criteria has therefore become an active area of\nresearch [6]. This has led to the development of numerous\nproduction-grade systems, such as vLLM [10], TorchServe\n[17], NVIDIA Triton [13], SGLang [23], and HuggingFace\nText Generation Inference (TGI) [18], that integrate ideas\nemerging from recent advancements in this field.\nOne\nprominent direction of research focuses on improving\nthroughput, a critical system-level metric, by optimizing\nGPU utilization. For instance, Orca [22] proposed continu-\nous batching, a technique that dynamically increases batch\nsize without interrupting ongoing decoding, thereby en-\nhancing GPU efficiency. vLLM [10] introduced paged KV\ncache management, which reduces memory fragmentation\nand frees up GPU memory, enabling larger batch sizes and\nultimately boosting throughput. Similarly, SARATHI [2]\nintroduced chunked prefill, which splits prefill requests into\nsmaller chunks and piggybacks decoding requests along-\nside them to improve GPU utilization. However, these\nsystems are designed for LLMs and do not account for\nthe resource-intensive multimodal encoding step unique to\nLMMs, which precedes the prefill phase.\nAnother line\nof work focuses on improving user-level metrics, such\nas request latency, for individual queries. For example,\nFastServe [19] employs iteration-level preemptive schedul-\ning to reduce latency caused by head-of-line blocking in\nthroughput-optimized systems. To gain finer control over\nTTFT and TPOT, approaches like SplitWise [14], DistServe\n[24], and D\u00e9j\u00e0 Vu [16] disaggregate the prefill and decode\nstages, mitigating interference between them. However,\nthese methods are designed for LLMs and overlook the\nresource-intensive encoding step present in LMMs. Since\nencoding is coupled with prefill, these systems suffer from"}, {"title": "3. Method", "content": "At a high level, the LMM system transforms multimodal\ndata and a text prompt into a coherent textual output. The\nsystem receives three types of input: (1) a text prompt $i_p$,\nwhich provides the context for the task (e.g., \"What is the\ncontent of the image?\"), (2) multimodal data $i_m$, which in-\ncludes images, videos, or other sensory inputs (e.g., one or\nmore images of a cat), and (3) sampling data $i_s$, which con-\nsists of parameters such as the desired number of output to-\nkens and temperature settings. The system processes these\ninputs to generate a text output $o$, which is the model's re-\nsponse (e.g., \"The image contains a cat.\"). For simplicity,\nwe focus on the visual modality in this paper, but the frame-\nwork is extensible to other types of multimodal data as well."}, {"title": "3.2. EPD Disaggregation", "content": "Disaggregating an LMM system involves breaking the in-\nference process into distinct stages, each with specific in-\nputs and outputs. As shown in Figure 1, the system follows\na multi-stage inference pipeline consisting of three primary\ncomponents: encoding, prefill, and decode. Additionally,\nthe transitions between these stages are managed through\nEP-migration and PD-migration, which handle the transfer\nof data between encoding-prefill and prefill-decode stages,\nrespectively. In the following, we describe each stage and\nthe transformations involved.\nEncoding (E): The multimodal input $i_m$ is processed by\nthe multimodal encoder (MME) $E$, which converts the input\ndata into multimodal tokens $v^e$ on encoding stage GPUs.\nThese tokens represent a high-dimensional embedding of\nthe input and serve as the foundation for the next stage of\nthe pipeline.\n$v^e = E(i_m)$"}, {"title": "3.3. System Design and Optimization", "content": "Figure 2 illustrates the architecture of our EPD Dis-\naggregated Inference system. Each stage of the\npipeline\u2014Encoding, Prefill, and Decoding\u2014features fully\nindependent instances capable of running the corresponding\nstage. These instances are designed to operate in DP mode,\nmeaning multiple instances per stage can process different\nrequests concurrently, ensuring scalability and efficiency.\nEach instance comprises a scheduler, responsible for\nscheduling requests, block managers (responsible for man-\naging cache(s)), and multiple workers. The workers operate\nin tensor-parallel (TP) and/or pipeline-parallel (PP) mode,\nwhere each worker holds only a subset of the model weights\nand the corresponding caches required for the stage."}, {"title": "3.3.1. Asynchronous Token Transfer", "content": "To minimize latency during token transfers between stages,\nour system supports direct, asynchronous token transfers\nvia high-bandwidth NVLink/ InfiniBand / NVSwitch chan-\nnels. The transfer occurs asynchronously, allowing the sys-\ntem to continue processing new requests while the token\ntransfer is in progress. Specifically, we maintain a mul-"}, {"title": "3.3.2. Intra-Request Parallel (IRP)", "content": "Often, multimodal requests include multiple high-\nresolution images, each of which is partitioned into patches\nfor high-resolution encoding. For modern LMMs, the"}, {"title": "3.3.3. Optimized Resource Allocation", "content": "The system dynamically determines configurations such as\nbatch sizes, scheduling strategies, and parallelization ap-\nproaches for each stage of the pipeline. This ensures that\ncritical performance metrics are kept near optimal, based on\nthe workload samples collected during deployment. These\nworkload samples are gathered in real time from a recent\ntime window. If the optimal performance metrics vary sig-\nnificantly in subsequent time windows, the system may ad-\njust the configuration of the EPD pipeline accordingly.\nWe leverage a black-box optimization algorithm to se-\nlect configurations, which minimizes GPU usage and max-\nimizes a critical performance metric. For online serving\nscenarios, we define goodput as the key performance met-\nric. Goodput represents the maximum request rate at which\n90% of requests meet the TTFT and TPOT requirements.\nIn offline scenarios, where TTFT and TPOT constraints are\nless critical, we use E2ETP as the performance metric. By\nthe nature of performance metrics, the optimized config will\nminimize pipeline inefficiencies (e.g., idle time) and ensure\nallocated resources are utilized efficiently.\nThe optimization problem is formulated as:\n$\\max_{(p,b,s) \\in X} f(p, b, s) \u2013 \\beta cost(p)$"}, {"title": "4. Implementation", "content": "EPD is a fully capable distributed serving system for\nLMMs, comprising several key components: a load estima-\ntion module, a resource allocation module, a RESTful API\nfrontend, and a multimodal-aware orchestration layer. The\nentire framework is implemented with a mix of Python and\nC++/ CUDA implementations, ensuring superior scalabil-\nity and performance. To facilitate integration, we repurpose\nthe distributed execution engine from vLLM, which sup-\nports numerous popular LLMs and LMMs, allowing easy\nadaptation of new models into our disaggregated framework\nwith minimal effort.\nThe API interface adheres to OpenAI's multimodal spec-\nifications, enabling users to specify parameters such as out-\nput length, temperature, and multimodal data inputs.\nThe scheduler is specifically designed for the disaggre-\ngated EPD framework, dynamically managing batch sizes\nand enabling asynchronous execution of the encoding, pre-\nfill, and decoding phases. The load estimation module en-\nsures efficient GPU allocation across these phases, adapting\nto changing workload demands in real time."}, {"title": "5. Experiments", "content": "In this section, we analyze and compare the performance of\nthe proposed EPD disaggregation method against the base-\nlines.\nWe compared our proposed method\nEPD against two popular baselines: DistServe [24] and\nVLLM [10]. The DistServe baseline implements the prefill-\ndecode (PD) disaggregation approach, where the prefill and\nencoding are executed on one set of GPUs, while the decode\nphase is disaggregated on a separate GPUs. In contrast, the\nvLLM baseline adopts a monolithic architecture, where all\nthree stages runs on the same set of GPUs. In addition, we\nalso consider an ablation of our method EPD without the\nIRP feature, named as EPD-IRP.\nSince DistServe was originally designed for LLMs, we\nextended its framework to support LMMs. This required ex-\ntending the request handling infrastructure to process mul-\ntimodal data, modifying the block manager to accommo-\ndate multimodal tokens, and changing the execution engine\nfrom SwiftTransformer to vLLM, which natively supports\nmultiple LLMs. The change to vLLM also ensured consis-\ntency in execution engine across all baselines, eliminating\npotential confounding performance differences arising from\nvariations due to the execution engine.\nModels: We utilized three LMMs in our analysis:\nMiniCPM-V 2.6 [21], InternVL 2.0 8B, and InternVL 2.0\n26B [5]. These models are renowned for their advanced ca-\npabilities in processing and understanding multimodal data.\nMiniCPM-V 2.6 integrates a SigLip-400M vision en-\ncoder, comprising 400 million parameters, with a Qwen2-\n7B language model, containing 7.6 billion parameters, cul-"}, {"title": "5.1. SLO Attainment for End-to-End Generation", "content": "In this experiment, we evaluate the end-to-end SLO per-\nformance of the proposed method (EPD), an ablation of"}, {"title": "5.2. First Token Generation Latency", "content": "In this experiment, we analyze the TTFT performance of the\nproposed method (EPD), an ablation of our method with-\nout IRP (EPD-IRP) and an existing state-of-the-art baseline,\nDistServe. Unlike prior analyses that focused on end-to-end\ninference, this experiment isolates the prefill phase of the\ncomputation, as the generation of the first token represents\na critical latency bottleneck due to large number of prefill\ntokens in multimodel workloads. Note that in this setup the\nDistServe baseline is equivalent to vLLM since the decod-\ning phase is not involved.\nFig. 5 shows the results for the three models: MiniCPMv\n2.6, InternVL2-8B, and InternVL2-26B, shown in the left,\nmiddle, and right plots, respectively. Each subplot presents\nbox plots of TTFT distributions, with the x-axis represent-\ning the number of images per request and the y-axis show-\ning the TTFT. The results are generated using 100 requests\nissued at a request rate of 0.25 for the MiniCPMv 2.6 and\n0.08 for the InternVL2-8B and InternVL2-26B models. We"}, {"title": "5.3. Memory Savings through Stage Disaggregation", "content": "This section demonstrates the significant memory savings\nachieved by disaggregating the encoding and prefill stages\nin the EPD system. In existing systems, such as DistServe\nand vLLM, the encoding and prefill stages are co-located\non the same GPUs, requiring each GPU to load both the\nMME and the LLM. This co-location results in higher mem-\nory consumption, as both models must be loaded simultane-\nously on the same GPUs.\nIn contrast, the proposed disaggregated system optimizes\nmemory usage by assigning the encoding stage to GPUs that\nload only the MME, while the prefill stage is handled by\nseparate GPUs that load only the LLM. This separation sig-\nnificantly reduces memory requirements for both stages.\nThe memory savings achieved through this disaggre-\ngation are substantial. Specifically, the encoding-stage\nGPUs experience memory reductions of approximately\n95%, 96.2%, and 78.3% for the MiniCPMv 2.6, InternVL2-\n8B, and InternVL2-26B models, respectively. On the other\nhand, the prefill-stage GPUs achieve memory savings of\nabout 5%, 3.7%, and 21.6% for the same models. In prac-\ntice, the memory savings could be as high as 15x lower as\nencoding stage do not need to load the KV cache which is\nonly required for prefill. These reductions in memory us-\nage enable the EPD system to allocate resources more ef-\nficiently, allowing for the support of additional tasks and\nmodels, as demonstrated in the subsequent experiments."}, {"title": "5.4. Throughput in Offline Settings", "content": "In this section, we demonstrate the benefits of the EPD\nmethod in terms of throughput, specifically focusing on its\nmemory efficiency and the ability to support higher batch\nsizes for both the encoding (E) and prefill (P) stages.\nConsider an offline scenario where a batch of requests is\nsubmitted, allowing the system to process them overnight\nwith a focus on maximizing end-to-end (E2E) throughput.\nIn the traditional DistServe method, memory is allocated\nfor both the encoding and prefill stages on the same worker,\nwhich is constrained by the total memory capacity of the\nGPU.\nThis memory limitation becomes especially significant"}, {"title": "6. Conclusion", "content": "In this paper, we present a novel approach to optimizing\nLarge Multimodal Model (LMM) systems through the dis-\naggregation of key processing stages. By separating the\nencoding, prefill, and decoding tasks into distinct stages,\nour system offers enhanced flexibility in resource alloca-\ntion, enabling more efficient management of computational\nand memory resources. This disaggregation, combined with\ndynamic resource allocation, asynchronous token transfer,\nand advanced parallelization strategies, directly addresses\nseveral critical challenges in LMM deployment, including\nlatency reduction, memory optimization, and efficient com-\nputational resource usage.\nOur results demonstrate significant improvements in\nboth throughput and memory efficiency, particularly in het-\nerogeneous environments where resources are limited. By\nefficiently parallelizing the encoding and prefill stages, we\nare able to mitigate common bottlenecks and improve the\nscalability of multimodal inference tasks. Furthermore, the\noptimized memory management strategies ensure that the\nsystem can scale to handle larger models and more com-\nplex multimodal inputs, which are increasingly common in\nreal-world applications.\nThe approach presented in this work represents a signif-\nicant step forward in the development of scalable and effi-"}]}