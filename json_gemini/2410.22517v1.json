{"title": "ATTENTION SPEAKS VOLUMES: LOCALIZING AND MITIGATING BIAS IN LANGUAGE MODELS", "authors": ["Rishabh Adiga", "Besmira Nushi", "Varun Chandrasekaran"], "abstract": "We explore the internal mechanisms of how bias emerges in large language models (LLMs) when provided with ambiguous comparative prompts: inputs that compare or enforce choosing between two or more entities without providing clear context for preference. Most approaches for bias mitigation focus on either post-hoc analysis or data augmentation. However, these are transient solutions, without addressing the root cause: the model itself. Numerous prior works show the influence of the attention module towards steering generations. We believe that analyzing attention is also crucial for understanding bias, as it provides insight into how the LLM distributes its focus across different entities and how this contributes to biased decisions. To this end, we first introduce a metric to quantify the LLM's preference for one entity over another. We then propose ATLAS (Attention-based Targeted Layer Analysis and Scaling), a technique to localize bias to specific layers of the LLM by analyzing attention scores and then reduce bias by scaling attention in these biased layers. To evaluate our method, we conduct experiments across 3 datasets (BBQ, Crows-Pairs, and WinoGender) using GPT-2 XL (1.5B), GPT-J (6B), LLAMA-2 (7B) and LLAMA-3 (8B). Our experiments demonstrate that bias is concentrated in the later layers, typically around the last third. We also show how ATLAS effectively mitigates bias through targeted interventions without compromising downstream performance and an average increase of only 0.82% in perplexity when the intervention is applied. We see an average improvement of 0.28 points in the bias score across all the datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) has enabled AI to perform increasingly complex tasks (Brown et al., 2020). Despite these advancements, LLMs often generate biased content, particularly when confronted with ambiguous prompts that require nuanced decision-making (Gallegos et al., 2024). Bias in models can manifest in various forms which do not always involve harmful language: reinforcing societal stereotypes (Caliskan et al., 2017), displaying gender bias (Bolukbasi et al., 2016), or demonstrating preferential treatment towards specific demographic groups (Gupta et al., 2023). This has led to growing concerns about the ethical implications of deploying such LLMs, especially when their outputs affect sensitive domains like hiring, legal advice, or healthcare (An et al., 2024). These manifestations of bias, where explicit harmful language is not part of the picture, are arguably also most difficult to mitigate because commonly used mitigations such as post-inference content filters and guards (Inan et al., 2023) are not applicable.\nTo enable more reliable deployment, one must localize and minimize bias in these LLMs. However, this is non-trivial. First, if one is to believe that data is the \"only\" cause, naively sanitizing data may not only be difficult to execute, but could also inadvertently degrade downstream model performance. Second, bias in LLMs is highly context-dependent (Sclar et al., 2024); it varies based on the input prompt, which makes the mitigation process more complex, necessitating a prompt-dependent approach for mitigation. Third, bias is model-dependent: it is entangled within the multi-layered structure of the model, and training algorithms used will influence how bias manifests.\nThe attention module (Vaswani et al., 2017) governs how most modern LLMs assign importance to different parts of the input. We conjecture that attention can also shed light on how bias is embedded in LLMs, in the way models internally distributes attention between competing"}, {"title": "2 BACKGROUND ON LLMS AND ATTENTION", "content": "We borrow some notation from the works of Elhage et al. (2021) and Meng et al. (2024) to delve into the details of the attention mechanism within transformers (Vaswani et al., 2017), concentrating on autoregressive, decoder-only LLMs. To streamline our explanation, we will bypass the inclusion of bias terms and layer normalization. Given an input sequence of tokens $t_1, ..., t_e$ from a vocabulary V, each token $t_i$ is initially mapped to a d-dimensional vector $x \\in \\mathbb{R}^d$ using an embedding matrix $E \\in \\mathbb{R}^{|V|\\times d}$. The LLM processes these embeddings through L layers, where each layer comprises a multi-head self-attention (MHSA) sublayer followed by a multi-layer perceptron (MLP) sublayer."}, {"title": "3 BIAS IN A COMPARATIVE PROMPT FRAMEWORK", "content": "What is the bias we are referring to? Bias in LLMs manifests when they demonstrate preferential implicit treatment or assumptions towards certain groups or entities, often reinforcing societal stereotypes or exhibiting disparate performance across different demographic sub-groups (Faisal & Anastasopoulos, 2022; Gupta et al., 2023).\nHow have we minimized/mitigated bias thus far? Some methods often focus on classifying outputs as either biased or unbiased, but such a binary view overlooks the complexity and subtleties in LLM decision-making and typically requires a post-hoc classifier (which requires additional overheads to train) (Ruggeri et al., 2023). To capture nuances associated with bias, it is necessary to go beyond this. Although one could attempt to probe the LLM's outputs to evaluate bias, such probing fails to faithfully represent the internal decision-making mechanisms at play (Turpin et al., 2024).\nTo better understand and address bias, we need to investigate the internal mechanisms and processes of the LLM. The attention weights are particularly important (Yuksekgonul et al., 2023), as they serve as measurable signals for how much importance the model assigns to different entities, which can play a critical role in bias formation during generation.\nIn what setting are we going to focus on? We focus on comparative prompts (Parrish et al., 2022; Nangia et al., 2020; Rudinger et al., 2018) where models are required to make a choice or express preference towards a decision that may favor or otherwise stereotype specific groups. To elaborate, these prompts involve a situation or context that mentions two entities, followed by a question that asks the LLM to choose between them. We believe this setting is both interesting and natural to"}, {"title": "4 ATTENTION-BASED TARGETED LAYER ANALYSIS AND SCALING (ATLAS)", "content": "We now outline how our two-step approach, ATLAS (Attention-based Targeted Layer Analysis and Scaling), is used to localize and mitigate bias in LLMs when responding to ambiguous comparative prompts. As its name suggests, ATLAS involves first localizing the layers in the model where bias is most prominent (\u00a7 4.1) and then applying targeted interventions to reduce this effect (\u00a7 4.2). Figure 2 demonstrates this process and its end goal."}, {"title": "4.1 LOCALIZING BIAS USING ATTENTION ON ENTITIES", "content": "We examine the attention scores assigned to the candidate entities (mentioned in the context) when the model is about to generate the answer i.e., at the last token T, where the (T + 1)-th token will be generated. By focusing on the attention scores from the entities across different layers, we can identify \"which\u201d layers of the model are contributing most to biased outcomes. We use attention scores rather than the MLP layers because attention mechanisms explicitly dictate how information is distributed across tokens, allowing us to directly observe the model's focus on specific entities"}, {"title": "4.2 INTERVENTIONS ON THE BIASED LAYERS", "content": "Once the biased layers have been localized, the next step is to intervene at the attention module to minimize the bias manifestation.\nScaling Attention: Let $A^{(l,h)}$ be the attention matrix at layer l for head h. To adjust the attention contributions, we scale the attention scores for all token indices corresponding to the higher probability candidate using a scaling factor $\\lambda\\in [0,1]$. Maintaining the same convention, let $C_{i^*}$ be the candidate entity with the higher probability, and let $TI(C_{i^*}) = \\{i_1, i_2, ..., i_m\\}$ be the set of token indices corresponding to $C_{i^*}$ in the prompt (see \u00a7 4.1). The scaling factor $\\lambda$ is applied as follows:\n$A_{T_i}^{(l,h)} = \\lambda \\cdot A_{T_i}^{(l,h)}$ for all $i \\in TI(C_{i^*})$ and $l \\in L_k$\nwhere $\\tilde{A}^{(l,h)}$ would represent the adjusted/scaled attention matrix and T is the last token in the prompt, after which the model generation starts.\nThe new attention score for entity $C_{i^*}$ after scaling is:\n$\\bar{a}^{(l,h)}(C_{i^*}) = \\sum_{T_i \\in TI(C_{i^*})} A_{T_i}^{(l,h)}$"}, {"title": "4.3 EVIDENCE FOR LOCALIZATION EFFICACY", "content": "To validate the effectiveness of ATLAS, we apply the scaling intervention described in \u00a7 4.2 for different layer categories: top-k, top-1, random-k, middle-k, and bottom-k (for k = 3). For each prompt in the BBQ dataset and using the GPT-J model (details in \u00a7 5 and Appendix B), we find these set of layers using Equation 4. We obtain $L_l$ using this equation, where $L = |L|$ is the total number of layers in the model (refer \u00a7 2). This provides an \u201cordered ranking\u201d of layers based on their contribution to bias, allowing us to easily extract the top-k, top-1, middle-k and bottom-k most biased layers. For random-k, we select k random layers from the model for each prompt."}, {"title": "5 EXPERIMENTAL SETUP", "content": "Datasets: For our evaluations, we utilize the BBQ (Bias Benchmark for Question Answering) dataset (Parrish et al., 2022), CrowS-Pairs dataset (Nangia et al., 2020), and WinoGender"}, {"title": "6 RESULTS", "content": "In our evaluation, we aim to answer the following questions: (1) Does ATLAS effectively mitigate bias in LLMs when responding to ambiguous comparative prompts? (c.f. \u00a7 6.1); (2) How do alternate methods such as activation steering or rank reduction perform compared to ATLAS? (c.f. \u00a7 6.2.1 and \u00a7 6.2.2); and (3) Does ATLAS affect the model's response quality? (c.f. \u00a7 6.3)\nOur results show that: (1) ATLAS demonstrates its effectiveness in reducing bias across all tested models and datasets, showing a consistent improvement in the EBS; (2) Although steering and other approaches provide some bias reduction, ATLAS consistently achieves better results than cotemporary approaches due to its targeted approach, which enables more precise bias localization and adjustment; and (3) ATLAS has minimal effect on response fluency and retains model quality, as measured by perplexity, while effectively shifting model preferences to address bias."}, {"title": "6.1 DOES ATLAS REDUCE BIAS?", "content": "We analyze the effect of the model intervention across multiple datasets and models in Table 1. We see large improvements in the EBS across all models and all datasets.\nImprovement Across Models: Our results demonstrate consistent improvements across all models. GPT-J exhibits the most dramatic enhancements, with EBS increasing by an average of 0.313 points"}, {"title": "6.2 BASELINE COMPARISONS", "content": ""}, {"title": "6.2.1 ATTENTION STEERING WITH PASTA", "content": "Activation steering techniques (Arditi et al., 2024;\nResults: We observe that while PASTA2 results in improvements, ATLAS still achieves better performance. This is likely because of PASTA's reliance on pre-determined attention heads which do not fully account for prompt-specific nuances in the attention distribution. In contrast, ATLAS's targeted approach to bias localization across layers allows for more refined interventions, specifically addressing the layers most responsible for biased behavior for each prompt. On average, ATLAS performs 0.10 points better than PASTA across categories."}, {"title": "6.2.2 PROMPTING & OTHER BASELINES", "content": "Prompting the model to be less biased is a natural comparison point. We included a fairness persona in the prompts which has been shown to improve scores on various tasks (Tseng et al., 2024); more details are presented in Appendix E.1. Our results on the BBQ dataset using the GPT-J model, as shown in Table 4, demonstrate that using this persona results in marginal improvements over the default setting, indicating that prompting in itself is insufficient.\nFurther, we compare our methodology against LASER (Sharma et al., 2023) in Appendix E.2 which perform matrix rank reductions in selective layers. However, LASER shows no significant improvements in the IEBS scores."}, {"title": "6.3 DOES THE INTERVENTION DEGRADE RESPONSE QUALITY?", "content": "An essential consideration in bias mitigation is ensuring that interventions aimed at reducing bias do not significantly degrade the overall response quality of the model. To assess this, we analyze the perplexity of the model's generated outputs pre- and post-ATLAS. Perplexity serves as a measure of fluency, with lower values indicating more fluent text (Kann et al., 2018). We also measure how often our scaling intervention changes the model's preferred output candidate when we use greedy decoding. Specifically, we report the percentage of prompts where, after applying our method, the model now generates the candidate that it had previously not selected. This helps us quantify how effectively our intervention alters the model's biased preferences."}, {"title": "7 RELATED WORK", "content": "Localization: Causal methods have been used to analyze model internals and address biases by intervening directly on model processing components. Techniques such as neuron ablations (Lakretz et al., 2019; Mohebbi et al., 2023) and replacing activations with baseline or alternative activations (Vaswani et al., 2017; Geiger et al., 2024) offer insights into the causal mechanisms behind model behavior. However, Meng et al. (2024) and Hase et al. (2024) show that localization methods should be carefully validated, as causal interventions may not always lead to predictive success.\nMitigation Strategies via Representation Editing: While hard-debias techniques (Bolukbasi et al., 2016; Ravfogel et al., 2020) aimed to remove biases by modifying embedding spaces, more recent approaches such as LEACE (Belrose et al., 2024) and DiffMask (De Cao et al., 2020) focus on run-time activation changes. These methods effectively reduce only gender bias by making alterations to the model's internal representations. Mitigations in word embeddings has also been a major focus, given their prevalence in NLP tasks (Caliskan et al., 2017; Manzini et al., 2019). In contrast, our work addresses biases in transformer models, specifically targeting attention layers that contribute to biased decision-making rather than modifying static embeddings.\nActivation Steering: Recent work on activation steering aims to dynamically influence model behavior during runtime by steering the activation space of LLMs. For instance, Turner et al. (2024) introduced the concept of \u201cactivation addition\u201d, which steers model outputs by adding specific activation vectors. Arditi et al. (2024) demonstrated that specific directions in the activation space mediate refusal behaviors in LLMs, providing a potential avenue for bias mitigation. Similarly, Pan-ickssery et al. (2024) uses contrastive activation addition to steer models like Llama 2 by adjusting internal activations post-hoc.\nSparse Autoencoders: Cunningham et al. (2023) has demonstrated that sparse autoencoders can capture interpretable features in LLMs, providing a pathway for targeting specific biases. Work on principled evaluation of these sparse autoencoders for interpretability (Makelov et al., 2024)"}, {"title": "8 CONCLUSIONS", "content": "In this paper, we provide a two-step approach, ATLAS, for identifying and mitigating bias in LLMs when responding to ambiguous comparative prompts. To capture bias in this framework, we first define the bias ratio (and the exponential bias score) metric. By analyzing attention distributions, ATLAS can localize biased entity information to specific layers of the model. ATLAS systematically reduces bias by scaling attention scores in these layers without degrading model performance. Experimental results highlight the efficacy of this approach. However, it is not without limitations. ATLAS is designed for the comparative prompting framework with two entities. Determining the scaling factor requires many inference calls, proportional to the number of layers being edited. Given the computational costs associated with the experiments, we are unable to perform every experiment discussed with all models."}, {"title": "C.1 MORE DETAILS ABOUT ATTENTION LOCALIZATION", "content": "Cost of the Approach: This method of localizing bias by analyzing attention scores involves one inference pass. During this pass, the generation is used to identify the higher probability candidate $C_{i^*}$ while also collecting the attention scores at every layer. This allows us to calculate $\\bar{a}^{(l)}(C_{i^*})$, and identify the top-k bias-contributing layers without requiring any additional forward passes.\nAnother popular method to localize information in LLMs is using causal-tracing (Meng et al., 2024). This approach involves several runs with corrupted and restored activations across each node in each layer of the model (one inference pass is needed per node during restoration phase) to first localize information. In contrast, our methodology only requires access to the attention scores at the last token, thus making the computational costs substantially lower compared to causal-tracing methods."}, {"title": "C.2 MORE DETAILS ABOUT THE APPLIED INTERVENTION", "content": "Why Scaling? We chose to scale attention scores rather than introducing random perturbations or other (drastic) modifications to preserve the model's internal decision-making integrity (evaluated in \u00a7 6.3). This intervention is straightforward, and works by reducing the representation of the candidate that is over-represented or assigned a higher probability by the model and doing so directly reduces the bias ratio. Another key advantage of scaling is that it does not require access to the model's weights, specifically the Q, K, and V matrices. Instead, we only need access to the atten-"}, {"title": "C.3 ABSENCE OF MONOTONIC BEHAVIORS WITH SCALING", "content": ""}, {"title": "D PASTA IMPLEMENTATION DETAILS", "content": "Setup: In our setup, we use task-agnostic and task specific attention heads directly to redistribute the model's focus towards the token with the lower bias probability, aiming to balance the attention across entities in a manner that improves the bias score. The scaling coefficient a controls the extent of attention re-weighting for the identified attention heads. It determines the strength of influence exerted by these heads on the target tokens, allowing fine-grained adjustments to the model's focus during generation. While the authors state that PASTA is not sensitive to the scaling coefficient a, we observed that performance can indeed depend on it, likely due to applying too much or too little emphasis on the lower probability token. To address this, we performed a search for the best IEBS score, testing different values of a in {0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, and report the best scores in Table 2."}, {"title": "E ADDITIONAL EXPERIMENTAL RESULTS", "content": ""}, {"title": "E.1 PROMPTING BASELINE", "content": "We also explored prompting the model with a fairness persona (Tseng et al., 2024) which has been shown to improve scores on various tasks. Specifically, we add the following to the prompt: \"You are a judge who embodies fairness, impartiality, and is not biased.\". Our results, as shown in Table 4 demonstrate that using this persona results in marginal improvements, but usng it along with ATLAS produced significant gains!"}, {"title": "E.2 LASER BASELINE", "content": "We experimented with LASER (Sharma et al., 2023),\nwhich involves the rank reduction of weight matrices.\nThe core idea behind LASER is to reduce higher-order\ncomponents of the weight matrices in specific layers\nof the transformer, which can lead to improvements\nin the model's performance on tasks without introduc-\ning new parameters or requiring further training. We\nconsider this approach as a baseline as Sharma et al.\n(2023) demonstrate that LASER reduces biases in the\nmodel's output, but for different datasets. Addition-\nally, this method is computationally efficient, making\nit a feasible option for large scale models without ex-\ntensive retraining."}, {"title": "E.3 VARYING INFERENCE TIME PARAMETERS", "content": "Motivation: To assess the effect of our intervention on the generated output, we varied inference time parameters including temperature, top-p, and top-k3. These parameters control the diversity and randomness of the generated text, which in turn influence model behavior. By evaluating these parameters, we aim to understand the effect of ATLAS across different inference settings, as models can exhibit more or less bias depending on how they sample from the output probability distribution."}]}