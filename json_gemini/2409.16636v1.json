{"title": "TRAINING LANGUAGE MODELS TO WIN DEBATES\nWITH SELF-PLAY IMPROVES JUDGE ACCURACY", "authors": ["Samuel Arnesen", "David Rein", "Julian Michael"], "abstract": "We test the robustness of debate as a method of scalable oversight by training mod-\nels to debate with data generated via self-play. In a long-context reading compre-\nhension task, we find that language model based evaluators answer questions more\naccurately when judging models optimized to win debates. By contrast, we find\nno such relationship for consultancy models trained to persuade a judge without\nan opposing debater present. In quantitative and qualitative comparisons between\nour debate models and novel consultancy baselines, we find evidence that debate\ntraining encourages stronger and more informative arguments, showing promise\nthat it can help provide high-quality supervision for tasks that are difficult to di-\nrectly evaluate.", "sections": [{"title": "1 INTRODUCTION", "content": "As AI systems tackle increasingly challenging problems, it will become correspondingly more dif-\nficult for humans to verify their answers as safe, useful, and accurate. For example, confirming the\nsolution to a graduate-level physics problem requires domain expertise, evaluating a literature re-\nview requires considerable time, and identifying a race condition in code requires careful reasoning,\nall of which a human may struggle with under practical time and resource constraints. As existing\nAI alignment and oversight approaches depend on reliable human supervision, we will need new\ninteraction mechanisms and training protocols for scalable oversight (Amodei et al., 2016; Bowman\net al., 2022), i.e., ones which scale with the increased complexity of the tasks being performed by\nstate-of-the-art AI models.\nDebate, proposed as a scalable oversight method by Irving et al. (2018), works by having two copies\nof a model argue against each other in defense of alternative responses to a question. A judge,\nwho can be either a human or a weaker, trusted model, tries to discern which debater is defending\nthe correct answer. In principle, debate should simplify evaluation by incentivizing the competing\nmodels to discover and explain the subtle flaws that a human or weaker model may not notice due\nto a lack of expertise, care, or time. As long as the refutational abilities of models scale alongside\ntheir general argumentation skills, we would expect that debates between more proficient models\nwill yield more accurate judgments.\nValidating debate as an oversight paradigm requires showing this empirically. Existing work has pro-\nduced promising results for debate in human experiments (Michael et al., 2023) and with inference-\ntime optimization of frontier models (Khan et al., 2024; Kenton et al., 2024), but prior work training\nmodels to debate has failed to show significant increases in evaluator accuracy (Radhakrishnan,\n2023).\nIn this work, we show for the first time that training language models to win debates can pro-\nduce more accurate evaluator judgments, taking another crucial step in implementing and validat-\ning debate as a practical scalable oversight method.\u00b9 To do so, we train a calibrated judge model\nand develop a variant of Direct Preference Optimization (DPO; Rafailov et al., 2023) for multi-\nturn debate training (Section 3). For our experiments, following Michael et al. (2023), we study\ninformation-asymmetric debates on reading comprehension questions from the QUALITY dataset\n(Pang et al., 2022), where the judge cannot see the underlying short story except through quotes"}, {"title": "2 EXPERIMENTAL SETUP", "content": "Following Parrish et al. (2022b), our debates center on questions from the QuALITY dataset (Pang\net al., 2022) of multiple choice reading comprehension questions over short stories. We exclusively\nsample questions from the QuALITY-HARD subset, with one debater defending the correct answer\nand the other defending the answer marked as the best distractor by annotators.\nTo ensure that the judge needs the debate transcript to answer each question, we follow Michael\net al. (2023) in using a hidden information setup where the debaters have access to the short story\ntext while the judge can only read quotes from the story selected by the debaters. This setup models\na scalable oversight setting by artificially elevating the capabilities of the debaters relative to that of\nthe judge: in this case, the debater's expertise comes from superior access to information."}, {"title": "2.2 DEBATE PROTOCOL", "content": "Our debates follow a two-turn, simultaneous debate structure similar to the one used by Khan et al.\n(2024). Both debaters begin by presenting their opening claims without view of the other's ar-\nguments. Then they each generate follow-up arguments conditioned on both debaters' opening"}, {"title": "2.3 BASELINES", "content": "For our baselines, we extend the consultancy setup introduced by Michael et al. (2023) into three\nvariants.\n\u2022 Under single consultancy, an individual debater (the \u201cconsultant\") introduces arguments\nin favor of their assigned side with no opposing debater present, with the consultant being\nassigned the correct side half of the time. For their second speech in the round, the con-\nsultant generates additional arguments to support the claims they advanced in their initial\nspeech. This setup represents a simplified version of the oversight process in Reinforcement\nLearning from Human Feedback (Christiano et al., 2017), testing whether a supervisor can\nproperly evaluate arguments without the aid of AI critiques or competing claims. It also\nmatches the consultancy setup of Michael et al. (2023), with the exception that we include\nno judge interaction.\n\u2022 Ensembled consultancy averages together the results of two separate single consultancies\nin which the consultant argues for each of the two possible answers to a question. This\nkind of pairwise comparison should, in theory, improve the judge's accuracy by correcting\nfor calibration errors in which the judge systematically gives scores that are either too\nhigh or too low (Zheng et al., 2023; Liusie et al., 2024). These errors can arise from\nsycophancy bias, where the judge is overly inclined to agree with its interlocutor(Perez\net al., 2022), or from the model learning to misleadingly convince its evaluator (Wen et al.,\n2024). Ensembled consultancy may also increase accuracy if the judge is often uncertain\nexcept for the occasional case where the correct answer has ironclad arguments in its favor.\n\u2022 Double consultancy is similar to ensembled consultancy except that both sets of speeches\nare presented to the judge in one context, allowing the judge to explicitly compare the\narguments to produce a single judgment. It differs from debate in that the debaters never get\nto see the claims advanced by their opponent. The difference between the debate and double\nconsultancy results lets us measure the strength of the debaters' capacity for refutation and\nthe importance of refutation in the judge's decision-making process.\""}, {"title": "2.4 EVALUATION", "content": "For each debater and consultant model, we compute its win rate compared to other models, which\ntracks how well it optimizes its training objective, and the judge accuracy when evaluating tran-\nscripts, which tracks how well the training and evaluation protocol produce truth-seeking behavior.\nFollowing Khan et al. (2024), we also investigate how judge accuracy changes with optimization-\nif the trend is positive, this provides evidence that the oversight protocol will continue to produce\ntruth-seeking behavior with more advanced Al systems.\n\u2022 Judge accuracy is measured using self-play where each model is pit against a copy of\nitself. The judge is considered to be correct if it assigns greater than 50% probability to the\ncorrect answer. For single consultancy, where there is no opposing model, judge accuracy\nis averaged equally between cases where the consultant is advocating for the correct and\nincorrect answer.\n\u2022 Debater win rate is measured using a round-robin tournament where each model debates\nevery other model. Since some positions are easier to defend than others, each question\ngets debated twice, with the debaters flipping sides between rounds, and a debater wins if\nit receives an average judge confidence over 50% across both rounds. The results of the\nround-robin tournament are then used to construct Elo scores for each model. These Elo\nscores yield an implied probability that a given model will defeat an average debater, which\nwe report as the final reported win rate.\n\u2022 Consultant win rate is the frequency with which the judge assigns a greater than 50%\nprobability to the position being defended by the consultant in single consultancy. Although\nensembled and double consultancy allow for head-to-head matchups, we use the single\nconsultancy win rate when tracking the relationship between consultant skill and judge\naccuracy because the models are trained to win at single consultancy,"}, {"title": "3 TRAINING METHODS", "content": "We use a finetuned version of GPT-4-Turbo (GPT-4T) as our judge. Although Khan et al. (2024)\nfound that the publicly vended version could judge rounds with high accuracy, two weaknesses limit\nits utility for our purposes:\n1. Calibration: We are interested not only in choosing a winner, but also in assigning a\nprobability to each answer in order to construct a high-quality reward signal. We found it\ndifficult to extract calibrated probabilities from GPT-4T, as asking directly for probabilities\nas tokens produced overconfident and uncalibrated results (Figure 3).\n2. Sycophancy: Many RLHF-trained models default to agreeing with their user (Perez et al.,\n2022). Although this is not necessarily a problem for debate, where the judge sees argu-\nments for both sides, it makes the judge more exploitable by a one-sided consultant. In our\nexperiments, an untrained GPT-4-Turbo judge agreed with the consultant 72% of the time,\neven before consultancy training. For a tougher baseline, we want a judge that is calibrated\nto the 50/50 prior probability that the consultant is correct.\nTo circumvent these issues, we finetune GPT-4T using the OpenAI finetuning API. As training data,\nwe use the human judgments on debate and consultancy transcripts from Michael et al. (2023) and\nKhan et al. (2024). Although we trained the model to output its confidence as tokens, we found that\nwe obtained the most calibrated results by using the token-level probabilities associated with each\ndebater's name, which were no longer clustered at the boundaries as they were prior to finetuning\n(Figure 3)."}, {"title": "3.2 DEBATERS AND CONSULTANTS", "content": "We train our debate and consultancy models using a combination of supervised finetuning on exist-\ning debate transcripts (Section 3.2.1) and Direct Preference Optimization training on self-generated\ndata to maximize the probability of winning under our judge model (Section 3.2.2)."}, {"title": "3.2.1 SUPERVISED TRAINING", "content": "We start with a version of Llama3-8B-Instruct that was finetuned by GradientAI to extend the context\nlength to from 8k to 262k tokens (AI@Meta, 2024; GradientAI, 2024). This context length extension\nis necessary to accommodate the full text of the QUALITY stories, which run to over 10k tokens\n(Pang et al., 2022).\nWe further finetune the model on transcripts of human debaters collected by Michael et al. (2023)\nand GPT-4 debaters collected by Khan et al. (2024). All of the debate transcripts are reformatted to\nmatch our prompt templates (Appendix H) which are based on prompts by Khan et al. (2024). \u03a4\u03bf\nprevent the model from losing its instruction-following abilities, we intermix instruction-following\nexamples from the Alpaca dataset (Taori et al., 2023) at a ratio of 1 instruction-following example\nfor every 2 regular samples."}, {"title": "3.2.2 SELF-PLAY DPO TRAINING", "content": "After supervised finetuning, we further finetune our models with multiple iterations of a novel,\nmodified version of Direct Preference Optimization (DPO; Rafailov et al., 2023). We choose DPO\nover standard RL methods like PPO (Schulman et al., 2017) because of ease of implementation\nand tuning. However, the standard formulation of DPO assumes access only to discrete preference\njudgments. Since we have access to the AI judge's output probabilities, this means throwing away\ninformation about the exact reward. We modify DPO to take advantage of this information.\nTraining Objective Standard DPO optimizes the following objective:\narg max $E_{x \\sim X} log \\sigma(\\beta(log \\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{ref}(y_{w}|x)} - log \\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{ref}(y_{l}|x)}))$\nwhere $\\pi_{\\theta}$ represents the language model policy parameterized by $\\theta$, $\\pi_{ref}$ is the pre-trained policy, $\\beta$\nis a KL penalty (regularization) coefficient, x is a prompt sampled from the dataset of prompts X,\nand yw and y\u0131 represent potential completions to the prompt x, with some external labeler marking\nyw as being preferable to yr. In our case, yw and y\u0131 are two speeches defending the same side of the\nsame debate topic. The idea is that the learned policy should generally prefer the winning responses\nover the rejected responses, while not drifting too far from the initial pretrained (reference) policy.\nThe latter stipulation reduces the risk of a degenerate solution and is governed by $\\beta$, the KL penalty\ncoefficient.\nDPO assumes that the preference judgments are drawn from a binary preference distribution related\nto the scalar reward by the Bradley-Terry model (Bradley & Terry, 1952), where $P(y_{0} > Y_{1} | x) =$"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "To measure how longer training and higher win rates affect the accuracy of the judge, we test dif-\nferent training checkpoints on the QuALITY-HARD validation set. Since some of Michael et al.\n(2023)'s debates drew from the validation set, we exclude some questions that were either present in\nthe training transcripts or shared a short story with a question in the training transcripts. After this\nfiltering, we end up testing on 433 distinct questions."}, {"title": "4.1 TRAINING IMPROVES MODEL SKILL", "content": "First, we verify that the models are optimizing the intended objective. We find that, as expected,\ntraining for longer increases win rate (Figure 4), with the debater Elo scores implying that the initial\nmodel, trained only via supervised finetuning (SFT), wins 31% of rounds against other checkpoints\nwhile the fully-trained DPO model wins 67% of the time.\nConsultancy results are similar. The judge votes for the SFT model only 36% of the time but votes for\nthe fully-trained DPO model 50% of the time. We suspect that this initially low winning percentage\nis due to the judge being trained on transcripts where the consultant was either a human or GPT-\n4-Turbo. Whether due to differences in skill or distribution shift, the judge treats the initial SFT\nmodel's outputs with skepticism, but is increasingly convinced by the more trained models."}, {"title": "4.2 BETTER DEBATERS (BUT NOT CONSULTANTS) YIELD MORE ACCURATE JUDGMENTS", "content": "Similarly, judges are better able to identify the correct answers when judging stronger debaters, with\nthe fully-DPO-trained model being associated with a 4 percentage point higher judge accuracy than\nthe SFT model (p < 10\u22126). At no point is the judge trained on transcripts from our debater models\nnor are the debaters trained to maximize judge accuracy. That there is still a positive relationship\nbetween debater skill and judge accuracy suggests that, in learning to become stronger debaters,\nthe models are learning features that are useful to the judge. By contrast, there is no apparent\nrelationship between consultant skill and judge accuracy, with the judge being no more accurate\nwhen evaluating the full DPO models than when evaluating the SFT models."}, {"title": "4.3 EXPOSURE TO INFORMATION INFLUENCES JUDGE ACCURACY MORE THAN\nREFUTATION", "content": "Results for double and ensembled consultancy provide further insight into why debate can help\njudges answer questions with greater accuracy than single-consultancy-like alternatives. The\nstrength of ensembled consultancy (72% accuracy across all checkpoints) relative to its single coun-\nterpart (68% accuracy) suggests that a sizeable portion of the gap between debate and consultancy\nis due to asymmetric strength of evidence, where considering only the strength of the evidence for\neach side allows for a better decision.\nSimilarly, double consultancy (75% accuracy) is more likely to yield correct answers than ensembled\nconsultancy, implying that the judge benefits from being able to see speeches from both sides in\ncontext at the same time. That judge accuracy in double consultancy is close to that of debate\n(77% accuracy) despite the former precluding each side from seeing the other's arguments suggests\nthat either (a) the debaters are failing to engage in meaningful refutation, or (b) the judge does not\nbenefit from reading the models' refutations. We also run experiments with single-turn debate and\nconsultancy which yield a similar conclusion, as the one-turn debates where no explicit refutation\ncould occur are judged just as accurately as two-turn debates (see Appendix G)."}, {"title": "4.4 DEBATERS LEARN MORE HELPFUL POLICIES THAN CONSULTANTS", "content": "We track two different metrics to explain how the models are able to become more convincing over\nthe course of training.\n1. Use of evidence (Figure 6, left): The speeches of the fully-trained debate model contain\n96% more quoted words than the initial SFT model. The story is the reverse for consul-\ntancy: the final model uses 70% fewer quotes than the initial SFT model, although this trend\nis barely significant due to high variance. The consultant also becomes more repetitive over\ntime, with 98% of quoted material in the final model's second speech being repeated from\nits initial speech.\n2. Transfer to other judges (Figure 6, right): We also evaluate the debaters and consultants\nusing an untrained version of GPT-40 (OpenAI, 2024). We find a Pearson correlation of\n0.98 between the win rates of the debaters according to the trained GPT-4T judge and\nuntrained GPT-40 judge but only a 0.51 correlation for consultancy. This suggests that\nthe debater may be learning more generally useful or informative argumentation strategies\nwhile the consultant model is learning features that are idiosyncratically convincing to the\nparticular judge it was optimized against."}, {"title": "5 DISCUSSION", "content": "DPO training can help current open-source 8B-sized models be more convincing to a GPT-4-\nlevel judge. The fully trained debate model wins 67% of the time against the average debater in\nthe sample and 82.5% against its SFT-trained base model. Similarly, the fully-trained consultant\nconvinces the judge of its position 52% of the time, up from 36% with just supervised finetuning.\nThese results suggest it may be feasible to train much stronger persuasive models with larger LMs\nand more compute.\nModels trained to debate are more likely to learn helpful policies than models trained for\nconsultancy. As the debate models grow stronger during training, they use more evidence from\nthe underlying text. Not only does this behavior fail to arise in the consultant models, we instead\nobserve an increase in repetition and the adoption of argumentative strategies that convince our judge\nmodel but do not convince other models.\nIt seems plausible that the presence of competing arguments at training time as is true in debate\nshould help prevent this behavior. For example, it might be more obvious that a debater is making\nassertions without supporting evidence if the speech is juxtaposed against another, better evidenced\none. In general, if the persuasiveness of a cheap argumentative strategy, like repetition or baseless\nclaims, is independent of the truth value of the claim being defended, then incentivizing the adoption\nof such a strategy should fail to improve judge accuracy.\nExplicit refutation does not yet seem to play a role in judge decision making in our setting.\nWhen first proposing debate as a means of scalable oversight, Irving et al. (2018) cited refutation\nas a key mechanism behind why debate might succeed. The idea is that each of the debaters could\nidentify flaws in their opponent's facts and reasoning, which would be easier for the judge to evaluate\nthan if they had to personally originate the various counter-considerations. Although a surface-level\nreading of the transcripts does find cases of apparent refutation (Figure 2), we find little evidence\nthat this refutation materially affects the judge's decision making.\nInstead, our results support the idea that debate outperforms consultancy due to a combination of\nseveral factors:\n1. The presentation of two different sides gives the judge more opportunities to settle the ques-\ntion on the basis of strong arguments, taking advantage of cases with asymmetric strength\nof evidence for either side. This would explain why the judge is more accurate when eval-\nuating ensembled consultancies than single consultancies."}, {"title": "5.1 ANALYSIS", "content": "First, we do not foreclose the possibility that even stronger models might find strategies\nthat perplex the judge and draw out debates, like Barnes (2020)'s obfuscated arguments. Second,\nour judge-debater expertise gap, relying on asymmetric access to textual information, may not be\nthe best proxy for expertise gaps in, e.g., reasoning abilities (Kirchner et al., 2024), that we will need\nto supervise across in the future. Third, we focus only on reading comprehension questions. In their\nexperiments, Kenton et al. (2024) find that debate is more helpful for these kinds of questions than\nfor other reasoning-related tasks. However, more recently, George et al. (2024) document affirmative\nevidence that GPT-3.5 can supervise GPT-4\u2013level debaters on knowledge-based multiple-choice\nquestions, providing preliminary evidence that the debate procedure can succeed in other domains."}, {"title": "5.2 RELATED WORK AND LIMITATIONS", "content": "Previous literature has mixed results on the question of whether debate helps evaluators discern truth,\nwith several negative results using humans as debaters and judges (Barnes & Christiano, 2020; Par-\nrish et al., 2022b;a). On the other hand, Michael et al. (2023) find a positive result for human debate\nrelative to consultancy, citing the length, flexibility, and interactivity of their debates as reasons for\nthe difference from prior findings. Research on debate between language models has shown more\noptimistic results, with some caveats. Khan et al. (2024) find that debate outperforms a baseline sim-\nilar to our single consultancy, and that this effect grows alongside the abilities of the debaters. How-\never, the parity of this skill-accuracy relationship is ambiguous for stronger, GPT-4\u2013level models,\nand their consultancy results optimize against a GPT-4T judge which is overly sycophantic (agree-\ning with their strongest consultants over 90% of the time). Similarly, while Kenton et al. (2024)\nrecord similar findings for debates on reading comprehension tasks, they are unable to replicate the\nfindings on other kinds of tasks.\nIn this work, we show that the positive judge accuracy trend observed for inference-time opti-\nmization of debate (Khan et al., 2024; Kenton et al., 2024) persists with debate training, a result\nwhich Radhakrishnan (2023)\u2014the only prior work to train models to debate in a scalable oversight\ncontext failed to observe. On top of this, we show that this effect persists even after mitigating\nsycophancy bias with a trained judge, and our two novel consultancy baselines help explain debate's\nstronger performance.\nNonetheless, debate's mixed record in the literature suggests that our results should be interpreted\nwith caution."}, {"title": "6 CONCLUSION", "content": "We explore whether training models to win debates can also help judges determine the correct an-\nswer to reading comprehension questions where the judge does not have access to the text of the\nstory being discussed. We find that there is indeed a small but significant positive relationship be-\ntween the ability of the model to win a debate and the usefulness of that model's debate transcripts\nin discovering true answers.\nNon-adversarial alternatives, in which a single model argues for an assigned answer, are compar-\natively less productive. We trace this weakness to three sources: one-sided information (the judge\nis unaware of the strength of the alternative answer), lack of explicit comparison (the judge cannot\nsee arguments side-by-side), and the rewarding of non-truth-seeking strategies (where the lack of an\nadversary makes the judge easier to exploit).\nAlthough our conclusions are limited to one particular domain and set of model capabilities, these\nresults nonetheless suggest that debate training has unique properties that make it well suited for\nsupervising more sophisticated models."}, {"title": "A RELATED WORK", "content": "Debate fits within the broader paradigm of scalable oversight, which attempts to empower a less\ncapable evaluator to oversee a more capable model (Amodei et al., 2016; Bowman et al., 2022).\nOur approach is a variant of sandwiching, where the outputs of the oversight protocol are compared\nagainst experts more capable than the supervisor (the weakest participant) and models that are not\nrobustly aligned (Cotra, 2021). In our case, we engineer the capability gap using information asym-\nmetry (where the story the question is fully visible only to the debaters), and engineer the models'\nmisalignment by forcing models to argue for the right answer exactly 50% of the time).\nIrving et al. (2018) introduced the concept of AI safety via debate, arguing via analogy to compu-\ntational complexity theory that debate should should simplify the supervisor's job, allowing a poly-\nnomial judge to correctly answer questions in PSPACE under the assumption of optimal debaters.\nRecent work by Brown-Cohen et al. (2023) develops this theory further.\nHowever, other work has also identified problems with certain debate protocols. Barnes (2020) and\nBarnes & Christiano (2020) identify an obfuscated arguments problem, where the debater advocat-\ning for the incorrect position are able to make lengthy, complicated argument chains against which\nthe correct debater was unable to mount a simple and concise rebuttal.\nSubsequent work with humans taking the place of models has also reached mixed conclusions.\n\u2022 Parrish et al. (2022b) and Parrish et al. (2022a) found that debate did not improve the\naccuracy of judges in practice. Like us, they used the QuALITY dataset from Pang et al.\n(2022) and experimented on one- to two-round debates. Unlike us, they limited the judge's\naccess to the underlying short story to a narrow time window, rather than obfuscating it\nentirely.\n\u2022 In contrast, Michael et al. (2023) found that debate improves judge accuracy, evaluating\non the same QuALITY questions. They attribute their divergent conclusion to the length\nof their debates (the round only ended when the judge chose to end it), the capability gap\nbetween the debaters and judge (unlike Parrish et al. (2022a), the judge could not read the\nstory at all), and interactivity (the judge was allowed to ask questions of the debaters).\nMore recently, there has also been work that has tested how well debate has performed with language\nmodels as the debaters.\n\u2022 Also looking at questions from the QuALITY dataset, Khan et al. (2024) tested different\nAPI-based models and found that the accuracy of the judges (both human and model-based)\nimproved as the debaters got stronger. They varied the model type and used Best-of-N\ndecoding and critique-and-refinement to generate models of varying strength.\n\u2022 Concurrently with our work, Kenton et al. (2024) evaluated debate across a suite of different\ntasks, also using Best-of-N and varying model size to generate debaters of various skill\nlevels. They found positive results for reading comprehension, but more muted results in\nother settings.\nThe most similar work to ours is Radhakrishnan (2023), who used reinforcement learning to train\nClaude to participate in single-turn debates. We differ from their work by using open-source models,\npublic training details, and validating against a baseline. We also use multi-turn debates, affording\nthe debaters the opportunity to respond to their opponents."}, {"title": "A.1 DEBATE FOR SCALABLE OVERSIGHT", "content": "Debate fits within the broader paradigm of scalable oversight, which attempts to empower a less\ncapable evaluator to oversee a more capable model (Amodei et al., 2016; Bowman et al., 2022).\nOur approach is a variant of sandwiching, where the outputs of the oversight protocol are compared\nagainst experts more capable than the supervisor (the weakest participant) and models that are not\nrobustly aligned (Cotra, 2021). In our case, we engineer the capability gap using information asym-\nmetry (where the story the question is fully visible only to the debaters), and engineer the models'\nmisalignment by forcing models to argue for the right answer exactly 50% of the time).\nIrving et al. (2018) introduced the concept of AI safety via debate, arguing via analogy to compu-\ntational complexity theory that debate should should simplify the supervisor's job, allowing a poly-\nnomial judge to correctly answer questions in PSPACE under the assumption of optimal debaters.\nRecent work by Brown-Cohen et al. (2023) develops this theory further.\nHowever, other work has also identified problems with certain debate protocols. Barnes (2020) and\nBarnes & Christiano (2020) identify an obfuscated arguments problem, where the debater advocat-\ning for the incorrect position are able to make lengthy, complicated argument chains against which\nthe correct debater was unable to mount a simple and concise rebuttal.\nSubsequent work with humans taking the place of models has also reached mixed conclusions.\n\u2022 Parrish et al. (2022b) and Parrish et al. (2022a) found that debate did not improve the\naccuracy of judges in practice. Like us, they used the QuALITY dataset from Pang et al.\n(2022) and experimented on one- to two-round debates. Unlike us, they limited the judge's\naccess to the underlying short story to a narrow time window, rather than obfuscating it\nentirely.\n\u2022 In contrast, Michael et al. (2023) found that debate improves judge accuracy, evaluating\non the same QuALITY questions. They attribute their divergent conclusion to the length\nof their debates (the round only ended when the judge chose to end it), the capability gap\nbetween the debaters and judge (unlike Parrish et al. (2022a), the judge could not read the\nstory at all), and interactivity (the judge was allowed to ask questions of the debaters)."}, {"title": "A.2 DEBATE AS CAPABILITY ELICITATION", "content": "Outside of the scalable oversight literature, debate and multi-agent discussion have been explored\nas methods to unlock new capabilities from language models at decoding time. Work in this area\ngenerally falls into one of two categories:\n\u2022 Viewpoint Diversity: Many works prompt models to mimic the behavior of different kinds\nof people, in order to produce a final output that represents a wider variety of perspectives"}, {"title": "A.3 LANGUAGE MODELS AS EVALUATORS", "content": "Although not the core of our contribution, our work is related to the literature on language models\nas evaluators. Most works in this specialty focus on devising techniques to enable language models\nto score the quality of other language model completions. Some of these take the form of prompting\n(Liu et al., 2023) while others take the form of specially-trained models (Kim et al., 2024b; Vu\net al., 2024). Automated judges have also been used as scorers on widely-cited benchmarks (Li\net al., 2023; Zheng et al., 2023; Lin et al., 2024). These works serve a similar purpose as reward\nmodeling (Christiano et al., 2017), with the distinction being that the latter uses a classification rather\nthan language modeling head in their final layer.\nA few works have also specifically designed language models to judge debates, including Rescala\net al. (2024) and Liang et al. (2024).\nAlthough many of these works attempt to address known biases such as self-preference (Panickssery\net al., 2024; Koo et al., 2023), length (Dubois et al., 2024), position order (Koo et al., 2023), and\nsycophancy biases (Perez et al., 2022), we have the additional constraint in that our judge needs to\nbe robust to adversarial optimization pressure."}, {"title": "B SUPERVISED TRAINING DETAILS", "content": "For our debater models, we began with a supervised finetuning step on a total of 1,716 instruction\ntuning examples from the Alpaca (Taori et al., 2023) dataset and 2,574 debate speeches. Of the\ndebate speeches, 564 of them come from 97 debates collected by Michael et al. during their exper-\niments with human debaters and judges. The 97 debates were random selections from their full set\nof transcripts, with 20% held out for validation and testing. The remaining 2,010 speeches came\nfrom 335 randomly-selected debates collected by Khan et al. during their experiments with LLM-\nbased debaters. We specifically selected only those speeches generated by Khan et al. (2024)'s best\nperforming model configuration, which was GPT-4T with Best-of-32 selection.\nFor our consultancy models, we trained on a sample of 2,530 consultant speeches and 1,686\ninstruction-tuning examples. 458 of the speeches came from 98 distinct rounds collected by Michael\net al. (2023) with the remainder coming from Khan et al. (2024)."}, {"title": "B.1"}]}