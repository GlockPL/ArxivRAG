{"title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts", "authors": ["Nikolas Gritsch", "Qizhen Zhang", "Acyr Locatelli", "Sara Hooker", "Ahmet \u00dcst\u00fcn"], "abstract": "Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on \"upcycling\" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.", "sections": [{"title": "Introduction", "content": "In an era of bigger and bigger models , there are several key objectives driving state-of-art progress. Doing more with less by improving efficiency  remains paramount, but in addition to efficiency the deployment of these models in the wild means that the ability to adapt to new data , and specialization of compute  have gained renewed focus. While all these properties are desirable, a formidable challenge is designing architectures that can fulfill all of these requirements.\n\nThe Mixture-of-Expert (MoE) approach gained prominence because of its efficiency properties. In contrast to dense models which require significant compute to deploy, MoE approaches only activate a subset of the parameters for every single token. Intuitively, not all parameters are necessary for each request, as some parameters will specialize on certain tasks, and those unrelated to the current request can be ignored. However, while MoEs greatly improved efficiency, the ability to induce meaningful specialization has been more limited with observations that experts don't appear to exhibit dedicated expertise . Furthermore, MoEs tend to suffer from severe training instabilities .\n\nRecent work has attempted to address both the training instabilities and the lack of specialization. These techniques often train completely separate experts and \u201cupcycle\u201d (combine) them into a single unified MoE model after dense training . This reduces the memory and communication cost, and improves efficiency during training as computations are more local and cross-device communication is reduced . Notably, the other major advantage of these approaches is the increase in specialization with separate experts that are trained on specific domains, making them clearly responsible for their human-interpretable subset of the data. On the other hand, MoEs with a standard router, which needs to be trained on a mix of all training data, are not designed to maintain domain specialization .\n\nHowever, efficiently integrating new experts into upcycled MoE models a setting that is of great interest for adaptability objectives is far less studied. For most practitioners, given the scale of modern LLMs  training MoEs repeatedly is an infeasible computational cost. Furthermore, most model development fails to take into account distribution drift in use cases, with limited flexibility and applicability across different tasks and domains . However, human language is shaped by a cumulative culture, constantly building upon itself and evolving over time . Also, specialized use cases such as multilingual, code and math often require tailored additional training.\n\nIn this work, we attempt to reconcile all three desirable properties: efficiency, specialization, and adaptability. We ask \u201chow can we adaptively combine separately trained specialized experts?\u201d \u03a4\u03bf address this, we introduce Nexus, a novel MoE architecture that parameterizes the router based on domain-specific data by learning to project the embedding of each data domain to an expert embedding. This learnable projection for the router allows for the easy extension of the MoE model with new experts that are trained independently on new datasets of interest. This also avoids the difficulties of MoE training, as our learned router scales with the number of experts without needing to be trained from scratch, which enables adding or removing experts as desired.\n\nOur experiments show that Nexus outperforms previous work when upscaling an MoE from separately trained specialized domain experts. Going beyond the single upscaling phase, Nexus can be efficiently extended with a new expert trained on a new domain, by finetuning it with much fewer tokens, compared to the finetuning after the initial upcycling.\n\nIn summary, our contributions are as follows:\n(i) We present Nexus, a novel MoE framework designed to enhance sparse upcycling of specialized trained dense experts, while reducing the training cost of MoEs by facilitating easy adaptation to unseen data distributions. In Nexus, the traditional linear router from vanilla MoE models is replaced with routing based on the similarity of layer inputs to an expert embedding vector, derived from the average embedding of the corresponding expert training dataset.\n(ii) Our method outperforms the existing approach for upcycling specialized models into MoE, leading to 2.1% and 1.6% relative increase over the upcycled MoE (linear router) in 470M and 2.8B scales respectively. This enables performance increase in general tasks with 5.8% and 7.4% relative gains over the dense seed model at 470M and 2.8B respectively.\n(iii) Our method enables efficient adaptation to new domains by extending upcycled MoE with the new experts trained on unseen dataset. In this setting, Nexus outperforms the baseline MoE"}, {"title": "Background", "content": "Sparse Mixture of Experts architectures  replace the feed-forward network (FFN) with an MoE layer in the Transformer block . An MoE layer consists of a router network $R$ and a set of $n$ experts, $E_1, ..., E_n$, where each expert $E_i$ corresponds to an independent dense feed-forward network. The router network $R$ is commonly parameterized by trainable weights $W_r \\in \\mathbb{R}^{h \\times n}$ where $h$ is the model hidden dimension, and followed by a softmax function which takes an intermediate token representation $x$ as input and combines the output of each expert based on the gating scores $s_1, ..., s_n$. Sparse MoEs only use the top-k experts $E_k$ based on experts gating scores $s_i$.\n\n$s_i = R(x) = softmax(W_r^T x$  (Router)\n$S_k = TopK(s_i)$ (Top-K Routing)\n$y = \\sum_{i=1}^{k} S_k \\cdot E_k(x)$ (MOE)\nRecent work has also shown that using a shared expert $E_0$ that is always activated is beneficial to remove parameter redundancy among other experts :\n\n$y = E_0(x) + \\sum_{i=1}^{k} s_k \\cdot E_k(x)$ (MoE + shared expert)\nSparse Upcycling  initializes an MoE model from a dense Transformer model. The dense model's FFN layers are copied $n$ times to initialize each of the $n$ experts, and the router layer is trained from scratch. BTX  generalize this approach to initialize each expert from the FFN layer of a different expert model, and all other parameters as the average over all of these models. The experts models are finetuned versions of the original dense model, which allows weight merging without major losses.\n\nNexus leverages upcycling specialized expert models similar to BTX, however, it diverges in terms of MoE training, in particular with its novel MoE router, which enables to efficiently extend the MoE in multiple rounds after the sparse upcycling. We describe our method in the next section."}, {"title": "Adaptive Router for Upcycling Specialized Experts as MoE", "content": "The core component of an MoE model is the router, as it determines which experts to activate for any given input. In vanilla MoEs, the router is a learned linear layer that takes the token intermediate representations as input and computes the expert probabilities. However, this router does not necessarily learn specialization as MoEs are commonly trained using an auxiliary load balancing loss to improve training stability . In Nexus, we propose a novel MoE router where per MoE block we learn a projection layer from given pre-computed domain embeddings to expert embeddings. We parametrize this projection layer $P_r$ as a two-layer MLP with a SwiGLU activation function :\n$e_i = P_r(d_i)$  (Domain to Expert Embeddings)\n$= W_2 \\cdot SwiGLU(W_1\\cdot d_i)$\nwhere $d_i \\in \\mathbb{R}^m$, and $e_i \\in \\mathbb{R}^h$ are the domain and expert embeddings for the $i$th domain respectively., where $m$ and $h$ are the domain embedding and the model dimensions. $W_1 \\in \\mathbb{R}^{2h \\times d}$, $W_2 \\in \\mathbb{R}^{l\\times l}$ are linear layers, and SwiGLU is defined as $\\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^n$. Given the expert embeddings $e_i$ and layer inputs $x \\in \\mathbb{R}^{s \\times h}$, we then compute routing probabilities $s_i$ as:\n\n$s_i = softmax(x e_i)$ (Routing Scores)\nUnlike the standard router, Nexus's router includes a stronger inductive bias through pre-computed domain embeddings\u00b9 that enables expert embedding to specialize. Thus, $xe_i$ gives a high value"}, {"title": "Experiments", "content": "Our experimental setup includes 3 phases. Figure 1 shows the architecture of Nexus and the corresponding experimental setting:\n\n1. Training specialized expert LMs. For training the dense specialized experts, we use the sub-datasets from the SlimPajama dataset , a 627B token English-language corpus assembled from web data of various sources. We initialize four dense experts from the weights of the seed model and train them on the ARXIV, BOOKS, C4, GITHUB, STACKEXCHANGE,"}, {"title": "Baselines", "content": "We compare our experiments against two baselines:\n\n1. Dense Merging: We compare MoE variants against merging all separately pre-trained experts and the seed model into a dense Transformer via equal weight averaging similar to BTM [Li et al., 2022]. This allows us to ask What are the benefits of routing MoE over simple averaging?\n\n2. MoE (Linear Router): To evaluate Nexus's novel router for upcycling, we compare it against an MoE with a standard linear router that is upcycled from dense experts. Here, we ask how does our specialized routing compare to conventional learned linear routing? For a fair comparison, we also train this MoE model on the same datasets and for the same number of tokens as our method, and use the same architectural modifications such as shared experts."}, {"title": "Evaluation", "content": "For the downstream evaluation, we measure the performance of each model on 15 tasks\u00b3 from five evaluation categories that reflect different capabilities based on the tasks and the datasets used in the benchmarks:\n\n\u2022 Knowledge: To measure question-answering capabilities based on world knowledge and web documents such as Wikipedia, we report the performance on OpenBookQA , Natural Questions , TriviaQA , QUAC  (all 0-shot) and SQUAD (4-shot) .\n\n\u2022 Science: For measuring knowledge in science-oriented academic benchmarks, we use ARC-Easy, ARC-Challenge , SciQ  (all 0-shot).\n\n\u2022 Reasoning: For reasoning abilities, we use CommonSenseQA , SIQA , PIQA , WinoGrande , and HellaSwag  (all 0-shot).\n\n\u2022 General Language Understanding: We use MMLU (5-shot)  to test general language understanding.\n\n\u2022 Code: For code generation, we evaluate models on MBPP , LBPP  and HumanEval-Pack  that includes Cpp, Javascript, Java, Go, Python, and Rust (all 0-shot)."}, {"title": "Results and Discussion", "content": "We first compare Nexus to the upcycled baselines MoE with linear router and dense merging.\nHere, we ask \u201cHow does our MoE upcycling recipe with adaptive routing compare against baseline upcycling approaches?\u201d"}, {"title": "Adaptive Router for Upcycling Specialized Experts as MoE", "content": "The core component of an MoE model is the router, as it determines which experts to activate for any given input. In vanilla MoEs, the router is a learned linear layer that takes the token intermediate representations as input and computes the expert probabilities. However, this router does not necessarily learn specialization as MoEs are commonly trained using an auxiliary load balancing loss to improve training stability . In Nexus, we propose a novel MoE router where per MoE block we learn a projection layer from given pre-computed domain embeddings to expert embeddings. We parametrize this projection layer $P_r$ as a two-layer MLP with a SwiGLU activation function :\n$e_i = P_r(d_i)$  (Domain to Expert Embeddings)\n$= W_2 \\cdot SwiGLU(W_1\\cdot d_i)$\nwhere $d_i \\in \\mathbb{R}^m$, and $e_i \\in \\mathbb{R}^h$ are the domain and expert embeddings for the $i$th domain respectively., where $m$ and $h$ are the domain embedding and the model dimensions. $W_1 \\in \\mathbb{R}^{2h \\times d}$, $W_2 \\in \\mathbb{R}^{l\\times l}$ are linear layers, and SwiGLU is defined as $\\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^n$. Given the expert embeddings $e_i$ and layer inputs $x \\in \\mathbb{R}^{s \\times h}$, we then compute routing probabilities $s_i$ as:\n\n$s_i = softmax(x e_i)$ (Routing Scores)\nUnlike the standard router, Nexus's router includes a stronger inductive bias through pre-computed domain embeddings\u00b9 that enables expert embedding to specialize. Thus, $xe_i$ gives a high value"}, {"title": "Experiments", "content": "Our experimental setup includes 3 phases. Figure 1 shows the architecture of Nexus and the corresponding experimental setting:\n\n1. Training specialized expert LMs. For training the dense specialized experts, we use the sub-datasets from the SlimPajama dataset , a 627B token English-language corpus assembled from web data of various sources. We initialize four dense experts from the weights of the seed model and train them on the ARXIV, BOOKS, C4, GITHUB, STACKEXCHANGE,"}, {"title": "Extending the Upcycled MoE model with a New Expert", "content": "To support fully modular and efficient training of MoEs, besides upcycling the existing expert models, it is crucial for an adaptive method to have the ability to continuously extend the upcycled MoE with new experts trained using previously unseen data domains. To evaluate this, we train a dense CODE expert and extend the upcycled MoEs (both Nexus and MoE (linear router)) as described in Section 3. We perform a small-scale finetuning of up to 1B tokens after extending the models.\nFigure 4 shows both the general performance and the target code performance at 200M, 500M, and 1B finetuning tokens. Here, we ask \u201cCan we continuously upcycle dense models into an MoE without requiring large-scale MoE training each time?\u201d\nPerformance on the new domain. As shown in Figure 4 (right), Nexus outperforms the MoE (linear router) for 200M, 500M and 1B finetuning tokens with 18.4%, 6.2% and 18.8% relative gains respectively. Unlike MoE (linear router), where the router weights are reset after extending the MoE layers, Nexus uses the information that is available about the new domain by mapping the domain embedding to a new expert embedding for the router, and therefore finetunes the router weights without a restart.\nComparison with the dense models. Nexus reaches the code performance of the seed model while retaining superior performance on general tasks. In comparison to the seed model and the dense code expert (trained for 8B code-only tokens on top of the seed model), although the dense code expert still performs higher than both upcycled MoEs with a score of 14.3, its performance on general tasks is far inferior (42.1). Our method also achieves up to 18.8% relative gains over the MoE (linear router). These results show that with a fraction of the original upcycling budget (1B vs 40B tokens for initial upcycling, and 1B vs 8B tokens for code expert training), Nexus can acquire a new capability.\nPerformance on general tasks. As a proxy for the knowledge for previously learned domains, Figure 4 (left) shows the average performance of Nexus and MoE (linear router) in general tasks. Although there is a slight drop on the general tasks for Nexus compared to initial upcycling (a relative decrease of 1.9%), the competitive performance is maintained across different numbers of finetuning tokens. We relate this to the composition of the finetuning mix where we use a high percentage of the code data (50% of the code and 50% of the previous domains)."}, {"title": "Expert Specialization", "content": "To measure the specialization in our MoE, we take a closer look at how the MoE experts are activated for samples of separate domains. We compute average routing frequencies across all Transformer layers in Figure 5, where the labels on the x-axis represent which domain the tokens are coming from, and the colored bars show the routing frequencies for each of the experts trained on one of the domains. Since we select only one routed expert per token in each MoE layer, and expert FFN layers are inherited from dense experts, average routing frequencies present a good proxy for specialization of each of the experts. Here, we ask \u201ccan Nexus retain a high degree of specialization after upcycling?\u201d\nRouting for the upcycled experts. As shown in Figure 5, we find that the expert trained on the corresponding domain always receives the highest share of the tokens from that domain, confirming that Nexus retains the specialization from the specialized dense models. Concretely, this specialization is higher for ArXiv, Books, and Wikipedia with 63.0%, 64.7%, and 69.8% respectively. Interestingly, tokens from C4 are routed only 40.9% of the time to the C4 expert and distributed to the other experts approximately 20% for each one. We relate this to the broad coverage of the C4 dataset, which potentially includes samples closer to other domains and also a large percentage of the C4 used in the MoE training phase (proportional to its size in the SlimPjama dataset). Especially the latter factor pushes tokens from C4 to be distributed to the other experts due to the load balancing factor.\nSpecialized routing for the new expert. Next, we measure expert specialization for the newly added expert on the new code domain. Figure 6 shows the average routing probability per expert for sampled code tokens. We compute routing probabilities on the Nexus model with the code expert after 1B finetuning tokens (See Section 5.2 for details). Here, we see clearly that code tokens are routed to the code expert 69.1% of the time on average. This shows that Nexus not only retains the specialization for the initial upcycling but also exhibits a high degree of specialization for a newly added expert for its own domain."}, {"title": "Ablations", "content": "Mixture-of-expert models are known to be sensitive to the choice of load balancing loss factor  and sampling weights for each data domains during training. As additional ablations, we run two new sets of experiments at 470M scale, one with a lower load balancing factor and the other one with equal weighting of each domain during training (whereas originally the weights were proportional to the share of tokens of that domain in SlimPajama). Figure 7 compares Nexus and MoE (linear router) in terms of their downstream performances for these ablations. Finally, in this section, we also visualize domain and projected expert embeddings to see if the relationship between embeddings is preserved after the learned projection.\nLowering the load balancing loss factor. In Figure 7 (baseline vs low load-bal.), we compare two Nexus models with the corresponding MoE (linear router) baselines where we use load balancing loss factor of 0.05 and 0.0005 for each set of experiments. We find that using a significantly lower factor for the load balancing loss hurts MoE (linear router) performance by approximately 2% relative drop while Nexus shows a robust performance across both load balancing factors. We hypothesize that because the expert embeddings in our router are always based on the domain representations, we achieve more stable distribution of tokens even if the load balancing loss is weighted extremely low.\nChanging the training data composition. Next, we compare our default of sampling specialized domain data proportional to the size of the domain (total amount of tokens in SlimPajama), with a uniform sampling over all domains. Figure 7 (baseline vs equal data) shows the downstream performances for both Nexus and MoE (linear). Although sampling domains differently does not significantly impact the downstream performance for both models, we find that it helps Nexus to improve specialization for all the domains in terms of expert routing probabilities (Figure 9, Appendix A). In particular, compared to the size proportional sampling, tokens from the C4 domain are routed more accurately (27.6% vs 71.1%) when data is equally sampled, which potentially impacts the model's behavior for particular input sequences.\nDomain embeddings before and after projection. Finally, in Figure 8, we visualize cosine similarities between domains and the projected expert embeddings from the last Transformer block, in our main upcycling experiments at the 470M scale. Comparing the embeddings before and after mapping, we find that the router's learned projection preserves the main relationship between domains. For instance, relatively high cosine similarity between Books & C4, and StackExchange & GitHub exist both between their domain embeddings and the projected expert embeddings. Interestingly, while preserving the main relationships, we also find that the learned projection pushes expert embeddings further away from each other, potentially due to our choice of only activating a single expert per token besides the shared expert."}, {"title": "Related Work", "content": "Routing Variants of MoEs. The most common MoE architecture  employs a linear router with a top-k routing scheme, where k typically equals 1 or 2. In this standard routing schema, only the k experts with the highest router gate values are activated. The MoE layer's output is computed as the weighted linear combination of these activated experts, with the weights corresponding to the router gate values. There is substantial research proposing alternatives to top-k expert assignments . For example, DeepSeek-MoE  introduces a routing variant where a number of experts are permanently active, always assigned to all tokens. Our work also adopts this \"shared expert\" approach for our general base expert. Another notable work is BASE Layers , where authors formulate the token-to-expert assignment as a linear assignment problem. However, these efforts primarily focus on improving the general performance and/or training stability of MoEs. In contrast, our work puts emphasis adaptability and extensibility.\nEfficient MoE Training by Re-Using Existing Dense Models. Training MoEs from scratch, i.e. from a random weight initialization, is computationally expensive  and often challenging due to training instabilities . Alternatively, recent works have explored re-using existing dense models to initialize MoEs, thereby enhancing training efficiency. Sparse Upcycling  re-uses a single dense model to initialize the MoE by by replicating dense model's FFN weights N times into N FFN experts in the MoE. The router is initialized randomly, and all other parameters are copied directly from the dense model. BTX  extends this approach by upcycling not from a single dense model, but from multiple specialized dense expert models to encourage diversity in the MoE initialization. Furthermore, BAM  expands BTX to upcycle not just FFN experts but also attention experts, further enhancing performance. Our work also leverages this approach by reusing existing specialized dense experts for MoE initialization, while extending it further to facilitate on-the-fly adaptations for new experts specialized in unseen data domains.\nEfficient MoE Architectures.  proposes replacing traditional MoE's computation-heavy feed-forward network (FFN) experts with more efficient experts comprised of smaller vectors and adapters, which are activated in parallel to a single dense FFN. This lightweight architecture necessitates only a limited number of parameter updates when finetuning, offering efficiency advantages. However, unlike our approach, it does not leverage existing specialized dense models and lacks a notion of specialized experts, which are central to our method. Similar to our work,  study combining separately trained experts into a unified model. However, they focus on parameter-efficient adapters such as LoRA  and supervised finetuning. In this work, we focus on efficiently pre-training fully-fledged MoE models via upcycling.\nAdaptive MoEs and Ensemble Models. ModuleFormer  also aims to produce adaptable MoEs. The authors achieve adaptability by freezing existing MoE parameters while only training newly added modules with optimization constraints to the router. Unlike our work, ModuleFormer does not leverage existing expert dense seed models for efficiency gains, nor does it have a notion of specialization which is central to our work. Similar to our work, DEMix  independently trains different FFN experts on specialized data domains, with each expert functioning as a domain-specific module. Modules can be added on-the-fly for adaptability. Followup works BTM and C-BTM  extend DEMix to create adaptive ensemble models. However, all three works use a router requiring a forward pass for every expert at inference instead of sparsely activating them, which significantly increases inference costs, especially with a large number of experts. Unlike these approaches, our router cost is approximately the same as standard top-k routing during both training and inference, offering a more scalable solution for adaptability."}, {"title": "Conclusion", "content": "We propose Nexus, a new LLM framework that enables efficient upcycling of specialized dense experts into a sparsely activated MoE model. We show that individual experts in our method retain their specialization after upcycling, and that our router based on expert embeddings outperforms previous approaches for combining the dense experts. Furthermore, the model can be extended efficiently with new dense experts after the initial training phase, saving much compute compared to re-training the upcycled model or training from scratch."}, {"title": "Limitations", "content": "The MoE architecture is often employed for larger models in the multi-billion parameter range, where efficiency is paramount. However, to facilitate a broader set of experiments, we limit our setup to using 2.8B parameter seed models for the main results and 470M parameter seed models for ablations. Furthermore, our dense experts are based on existing data sources in the SlimPajama dataset which is pre-defined. Future work could extend our method by discovering specialized data domains through unsupervised clustering similar to ."}]}