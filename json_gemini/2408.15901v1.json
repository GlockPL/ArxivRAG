{"title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts", "authors": ["Nikolas Gritsch", "Qizhen Zhang", "Acyr Locatelli", "Sara Hooker", "Ahmet \u00dcst\u00fcn"], "abstract": "Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on \"upcycling\" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.", "sections": [{"title": "Introduction", "content": "In an era of bigger and bigger models [Canziani et al., 2016; Strubell et al., 2019; Rae et al., 2021; Raffel et al., 2020; Bommasani et al., 2022; Hooker, 2024], there are several key objectives driving state-of-art progress. Doing more with less by improving efficiency [Treviso et al., 2023] remains paramount, but in addition to efficiency the deployment of these models in the wild means that the ability to adapt to new data [Pozzobon et al., 2023b; Gururangan et al., 2020a; Jang et al., 2022; Jin et al., 2022], and specialization of compute [Zadouri et al., 2024; Shazeer et al., 2018; Riquelme et al., 2021; Du et al., 2022; Fedus et al., 2022] have gained renewed focus. While all these properties are desirable, a formidable challenge is designing architectures that can fulfill all of these requirements.\n The Mixture-of-Expert (MoE) approach gained prominence because of its efficiency properties. In contrast to dense models which require significant compute to deploy, MoE approaches only activate\na subset of the parameters for every single token. Intuitively, not all parameters are necessary for each request, as some parameters will specialize on certain tasks, and those unrelated to the current request can be ignored. However, while MoEs greatly improved efficiency, the ability to induce meaningful specialization has been more limited with observations that experts don't appear to exhibit dedicated expertise [Jiang et al., 2024; Zoph et al., 2022; Zadouri et al., 2023]. Furthermore, MoEs tend to suffer from severe training instabilities [Zoph et al., 2022].\nRecent work has attempted to address both the training instabilities and the lack of specialization. These techniques often train completely separate experts and \u201cupcycle\u201d (combine) them into a single unified MoE model after dense training [Sukhbaatar et al., 2024]. This reduces the memory and communication cost, and improves efficiency during training as computations are more local and cross-device communication is reduced [Li et al., 2022; Gururangan et al., 2023]. Notably, the other major advantage of these approaches is the increase in specialization with separate experts that are trained on specific domains, making them clearly responsible for their human-interpretable subset of the data. On the other hand, MoEs with a standard router, which needs to be trained on a mix of all training data, are not designed to maintain domain specialization [Jiang et al., 2024].\nHowever, efficiently integrating new experts into upcycled MoE models a setting that is of great interest for adaptability objectives is far less studied. For most practitioners, given the scale of modern LLMs [Brown et al., 2020; Touvron et al., 2023; Kaplan et al., 2020; Anil et al., 2023] training MoEs repeatedly is an infeasible computational cost. Furthermore, most model development fails to take into account distribution drift in use cases, with limited flexibility and applicability across\ndifferent tasks and domains [Pozzobon et al., 2023a; Gururangan et al., 2020b]. However, human language is shaped by a cumulative culture, constantly building upon itself and evolving over time [Silvey, 2016]. Also, specialized use cases such as multilingual, code and math often require tailored additional training.\nIn this work, we attempt to reconcile all three desirable properties: efficiency, specialization, and adaptability. We ask \u201chow can we adaptively combine separately trained specialized experts?\u201d \u03a4\u03bf address this, we introduce Nexus, a novel MoE architecture that parameterizes the router based on domain-specific data by learning to project the embedding of each data domain to an expert embedding. This learnable projection for the router allows for the easy extension of the MoE model with new experts that are trained independently on new datasets of interest. This also avoids the difficulties of MoE training, as our learned router scales with the number of experts without needing to be trained from scratch, which enables adding or removing experts as desired.\nOur experiments show that Nexus outperforms previous work when upscaling an MoE from separately trained specialized domain experts. Going beyond the single upscaling phase, Nexus can be efficiently extended with a new expert trained on a new domain, by finetuning it with much fewer tokens, compared to the finetuning after the initial upcycling.\nIn summary, our contributions are as follows:\n(i) We present Nexus, a novel MoE framework designed to enhance sparse upcycling of specialized trained dense experts, while reducing the training cost of MoEs by facilitating easy adaptation to unseen data distributions. In Nexus, the traditional linear router from vanilla MoE models is replaced with routing based on the similarity of layer inputs to an expert embedding vector, derived from the average embedding of the corresponding expert training dataset.\n(ii) Our method outperforms the existing approach for upcycling specialized models into MoE, leading to 2.1% and 1.6% relative increase over the upcycled MoE (linear router) in 470M and 2.8B scales respectively. This enables performance increase in general tasks with 5.8% and 7.4% relative gains over the dense seed model at 470M and 2.8B respectively.\n(iii) Our method enables efficient adaptation to new domains by extending upcycled MoE with the new experts trained on unseen dataset. In this setting, Nexus outperforms the baseline MoE\n(linear router) when finetuning on the limited amount of data, leading 18.8% relative gain on the new domain with 1B finetuning tokens upon MoE extension.\n(iv) Finally, we show that our method is robust across different load balancing and data mixtures, and consistently outperforms the MoE with a linear router for specialized upcycling, confirming the benefits of the adaptive routing based on domain projections used in Nexus."}, {"title": "Background", "content": "Sparse Mixture of Experts architectures [Shazeer et al., 2017; Fedus et al., 2022] replace the feed-forward network (FFN) with an MoE layer in the Transformer block [Vaswani et al., 2017]. An MoE layer consists of a router network R and a set of n experts, $E_1, ..., E_n$, where each expert $E_i$ corresponds to an independent dense feed-forward network. The router network R is commonly parameterized by trainable weights $W_r \\in \\mathbb{R}^{h \\times n}$ where h is the model hidden dimension, and followed by a softmax function which takes an intermediate token representation x as input and combines the output of each expert based on the gating scores $s_1, ..., s_n$. Sparse MoEs only use the top-k experts $E_k$ based on experts gating scores $s_i$.\n\n$s_i = R(x) = softmax(W_r^T x)$ (Router)\n$s_k = TopK(s_i)$ (Top-K Routing)\n$y = \\sum_{i=1}^k s_k \\cdot E_k (x)$ (MOE)\nRecent work has also shown that using a shared expert $E_0$ that is always activated is beneficial to remove parameter redundancy among other experts [Rajbhandari et al., 2022; Dai et al., 2024]:\n$y = E_0(x) + \\sum_{i=1}^k s_k \\cdot E_k (x)$ (MoE + shared expert)\nSparse Upcycling [Komatsuzaki et al., 2023] initializes an MoE model from a dense Transformer model. The dense model's FFN layers are copied n times to initialize each of the n experts, and the router layer is trained from scratch. BTX [Sukhbaatar et al., 2024] generalize this approach to initialize each expert from the FFN layer of a different expert model, and all other parameters as the average over all of these models. The experts models are finetuned versions of the original dense model, which allows weight merging without major losses.\nNexus leverages upcycling specialized expert models similar to BTX, however, it diverges in terms of MoE training, in particular with its novel MoE router, which enables to efficiently extend the MoE in multiple rounds after the sparse upcycling. We describe our method in the next section."}, {"title": "Adaptive Router for Upcycling Specialized Experts as MoE", "content": "The core component of an MoE model is the router, as it determines which experts to activate for any given input. In vanilla MoEs, the router is a learned linear layer that takes the token intermediate representations as input and computes the expert probabilities. However, this router does not\nnecessarily learn specialization as MoEs are commonly trained using an auxiliary load balancing loss to improve training stability [Fedus et al., 2022; Jiang et al., 2024]. In Nexus, we propose a novel MoE router where per MoE block we learn a projection layer from given pre-computed domain embeddings to expert embeddings. We parametrize this projection layer $P_r$ as a two-layer MLP with a SwiGLU activation function [Shazeer, 2020]:\n$e_i = P_r(d_i)$ (Domain to Expert Embeddings)\n$= W_2 \\cdot SwiGLU(W_1 \\cdot d_i)$\nwhere $d_i \\in \\mathbb{R}^m$, and $e_i \\in \\mathbb{R}^h$ are the domain and expert embeddings for the ith domain respectively., where m and h are the domain embedding and the model dimensions. $W_1 \\in \\mathbb{R}^{2h \\times d}$, $W_2 \\in \\mathbb{R}^{l \\times l}$ are linear layers, and SwiGLU is defined as $\\mathbb{R}^{2n} \\rightarrow \\mathbb{R}^n$. Given the expert embeddings $e_i$ and layer inputs $x \\in \\mathbb{R}^{s \\times h}$, we then compute routing probabilities $s_i$ as:\n$s_i = softmax(x e_i)$ (Routing Scores)\nUnlike the standard router, Nexus's router includes a stronger inductive bias through pre-computed domain embeddings\u00b9 that enables expert embedding to specialize. Thus, $x e_i$ gives a high value\nfor input tokens that are closer to the domain of the corresponding expert. Notably, this router is particularly suited for the sparse upcycling setting where the dense experts are separately trained on different domains.\nConnection to hypernetworks. Our router parametrization is closely related to hypernetworks [Ha et al., 2016] as the projection layer $P_r$ generates parameters for the router during runtime for a given input. We use domain embeddings as the input to the projection layer, enabling efficient adaptation and also a better cross-domain transfer based on the similarity between domain embeddings as shown in previous work [Mahabadi et al., 2021; \u00dcst\u00fcn et al., 2022].\nUpcycling dense experts as an MoE. After training dense expert models, we merge the individual experts into a unified MoE by appending their FFNs along a new dimension to create an MoE layer per Transformer block. Unlike Sukhbaatar et al. [2024], instead of using the original FFN of the seed model as one of the routed experts in an MoE layer, we use it as the shared expert (FFNs) to better preserve the previous capabilities in the MoE model. For all non-FFN parameters including the attention weights, we merge expert parameters using simple weight averaging:\n$FFN_{moe} = FFN_s + [FFN_{e1}, FFN_{e2}, ..., FFN_{en}]$ (MoE Layer FFNs)\n$\\Phi_{moe} = \\frac{\\sum_{i=1}^{n} \\Phi_i}{n}$ (Merge Non-FFN params.)\nEfficient adaptation to new domains. An important advantage of method is that when a new data domain is present after MoE training, we use the learned projection $P_r$ to compute expert embedding of the new domain as $e_{new} = P_r(d_{new})$. This enables to enhance the trained MoE model with additional dense experts, which are trained in the same way as the initial experts. The FFN parameters of the new expert are simply appended to the array of existing experts.\nTo adequately preserve the non-FFN parameters of existing experts, we perform a weighted average $\\Phi_f = (1 - \\lambda) \\cdot \\Phi_{moe} + \\lambda \\cdot \\Phi_{new}$ where $\\Phi_f$, $\\Phi_e$, and $\\Phi_{moe}$ are parameters of the final MoE, dense expert, and initial MoE model and $\\lambda = 1/(n+1)$. This enables efficient adaptation Nexus to new domain by extending it with the new dense expert trained independently. After extending the MoE with a new expert, we perform a lightweight finetuning with a limited number of tokens for quick adaptation."}, {"title": "Experiments", "content": "4.1 Experimental setting\nOur experimental setup includes 3 phases. Figure 1 shows the architecture of Nexus and the corresponding experimental setting:\n1. Training specialized expert LMs. For training the dense specialized experts, we use the sub-datasets from the SlimPajama dataset [Soboleva et al., 2023], a 627B token English-language corpus assembled from web data of various sources. We initialize four dense experts from the weights of the seed model and train them on the ARXIV, BOOKS, C4, GITHUB, STACKEXCHANGE,"}, {"title": "Results and Discussion", "content": "5.1 Main Results for Upcycled Models\nWe first compare Nexus to the upcycled baselines MoE with linear router and dense merging. Here, we ask \u201cHow does our MoE upcycling recipe with adaptive routing compare against baseline upcycling approaches?\u201d\n470M parameter seed model. Table 2 shows performances of upcycled models including Nexus where a 470M seed model is used to train dense experts. Both Nexus and the upcycled MoE (linear router)) consist of 1 shared and 6 routed experts, corresponding to a total number of 1.3B parameters where 605M parameters are activated per input for top-2 routing (1 expert always activated, 1 chosen by the router). The dense merging baseline is created by averaging the weights of all dense experts and the seed model, and therefore has the same number of parameters as the seed model.\nCompared to the seed model, Nexus performs better in all evaluation categories with a 5.8% relative gain on average (38.5 vs 36.4). Compared to upcycled models, Nexus outperforms MoE (linear router) in 3 out of 4 categories with 3.2% relative gain (38.5 vs 37.3) on average, and beats dense merging by 8.5% overall relative increase (38.5 vs 35.5). Notably, while both upcycled MoEs outperform the seed model, dense merging underperforms on average, showing the benefits of MoE upcycling over parameter averaging.\n2.8B parameter seed model. Next, we experiment by upcycling dense models with 2.7B parameters to validate if the results from the 470M seed model hold at a larger scale. Table 3 compares Nexus with MoE (linear router) and dense merging. Both Nexus and MoE (linear router) use 1 shared expert and 4 routed experts in these experiments, corresponding to 4.3B active\n5.2 Extending the Upcycled MoE model with a New Expert\nTo support fully modular and efficient training of MoEs, besides upcycling the existing expert models, it is crucial for an adaptive method to have the ability to continuously extend the upcycled MoE with new experts trained using previously unseen data domains. To evaluate this, we train a dense CODE expert and extend the upcycled MoEs (both Nexus and MoE (linear router)) as described in Section 3. We perform a small-scale finetuning of up to 1B tokens after extending the models. Figure 4 shows both the general performance and the target code performance at 200M, 500M, and\n1B finetuning tokens. Here, we ask \u201cCan we continuously upcycle dense models into an MoE without requiring large-scale MoE training each time?\u201d\nPerformance on the new domain. As shown in Figure 4 (right), Nexus outperforms the MoE (linear router) for 200M, 500M and 1B finetuning tokens with 18.4%, 6.2% and 18.8% relative gains respectively. Unlike MoE (linear router), where the router weights are reset after extending the MoE layers, Nexus uses the information that is available about the new domain by mapping the domain embedding to a new expert embedding for the router, and therefore finetunes the router weights without a restart.\nComparison with the dense models. Nexus reaches the code performance of the seed model while retaining superior performance on general tasks. In comparison to the seed model and the dense code expert (trained for 8B code-only tokens on top of the seed model), although the dense code expert still performs higher than both upcycled MoEs with a score of 14.3, its performance on general tasks is far inferior (42.1). Our method also achieves up to 18.8% relative gains over the MoE (linear router). These results show that with a fraction of the original upcycling budget (1B vs 40B tokens for initial upcycling, and 1B vs 8B tokens for code expert training), Nexus can acquire a new capability.\nPerformance on general tasks. As a proxy for the knowledge for previously learned domains, Figure 4 (left) shows the average performance of Nexus and MoE (linear router) in general tasks. Although there is a slight drop on the general tasks for Nexus compared to initial upcycling (a relative decrease of 1.9%), the competitive performance is maintained across different numbers of finetuning tokens. We relate this to the composition of the finetuning mix where we use a high percentage of the code data (50% of the code and 50% of the previous domains)."}, {"title": "Expert Specialization", "content": "To measure the specialization in our MoE, we take a closer look at how the MoE experts are activated for samples of separate domains. We compute average routing frequencies across all Transformer layers in Figure 5, where the labels on the x-axis represent which domain the tokens are coming from, and the colored bars show the routing frequencies for each of the experts trained on one of the domains. Since we select only one routed expert per token in each MoE layer, and expert FFN layers are inherited from dense experts, average routing frequencies present a good proxy for specialization of each of the experts. Here, we ask \u201ccan Nexus retain a high degree of specialization after upcycling?\u201d\nRouting for the upcycled experts. As shown in Figure 5, we find that the expert trained on the corresponding domain always receives the highest share of the tokens from that domain, confirming that Nexus retains the specialization from the specialized dense models. Concretely, this specialization is higher for ArXiv, Books, and Wikipedia with 63.0%, 64.7%, and 69.8% respectively. Interestingly, tokens from C4 are routed only 40.9% of the time to the C4 expert and distributed to the other experts approximately 20% for each one. We relate this to the broad coverage of the C4 dataset, which potentially includes samples closer to other domains and also a large percentage of the C4 used in the MoE training phase (proportional to its size in the SlimPjama dataset). Especially the latter factor pushes tokens from C4 to be distributed to the other experts due to the load balancing factor.\nSpecialized routing for the new expert. Next, we measure expert specialization for the newly"}, {"title": "Ablations", "content": "Mixture-of-expert models are known to be sensitive to the choice of load balancing loss factor [Fedus et al., 2022; Zoph et al., 2022] and sampling weights for each data domains during training. As additional ablations, we run two new sets of experiments at 470M scale, one with a lower load balancing factor and the other one with equal weighting of each domain during training (whereas originally the weights were proportional to the share of tokens of that domain in SlimPajama). Figure 7 compares Nexus and MoE (linear router) in terms of their downstream performances for these ablations. Finally, in this section, we also visualize domain and projected expert embeddings to see if the relationship between embeddings is preserved after the learned projection.\nLowering the load balancing loss factor. In Figure 7 (baseline vs low load-bal.), we compare two Nexus models with the corresponding MoE (linear router) baselines where we use load balancing loss factor of 0.05 and 0.0005 for each set of experiments. We find that using a significantly lower factor for the load balancing loss hurts MoE (linear router) performance by approximately 2% relative drop while Nexus shows a robust performance across both load balancing factors. We hypothesize that because the expert embeddings in our router are always based on the domain representations, we achieve more stable distribution of tokens even if the load balancing loss is weighted extremely low.\nChanging the training data composition. Next, we compare our default of sampling specialized domain data proportional to the size of the domain (total amount of tokens in SlimPajama), with a uniform sampling over all domains. Figure 7 (baseline vs equal data) shows the downstream performances for both Nexus and MoE (linear). Although sampling domains differently does not significantly impact the downstream performance for both models, we find that it helps Nexus to improve specialization for all the domains in terms of expert routing probabilities (Figure 9, Appendix A). In particular, compared to the size proportional sampling, tokens from the C4 domain are routed more accurately (27.6% vs 71.1%) when data is equally sampled, which potentially impacts the model's behavior for particular input sequences.\nDomain embeddings before and after projection. Finally, in Figure 8, we visualize cosine similarities between domains and the projected expert embeddings from the last Transformer block, in our main upcycling experiments at the 470M scale. Comparing the embeddings before and after mapping, we find that the router's learned projection preserves the main relationship between domains. For instance, relatively high cosine similarity between Books & C4, and StackExchange & GitHub exist both between their domain embeddings and the projected expert embeddings. Interestingly, while preserving the main relationships, we also find that the learned projection pushes expert embeddings further away from each other, potentially due to our choice of only activating a single expert per token besides the shared expert."}, {"title": "Related Work", "content": "Routing Variants of MoEs. The most common MoE architecture [Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022] employs a linear router with a top-k routing scheme, where k typically equals 1 or 2. In this standard routing schema, only the k experts with the highest router gate values are activated. The MoE layer's output is computed as the weighted linear combination of these activated experts, with the weights corresponding to the router gate values. There is substantial research proposing alternatives to top-k expert assignments [Hazimeh et al., 2021; Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022; Zuo et al., 2022]. For example, DeepSeek-MoE [Dai et al., 2024] introduces a routing variant where a number of experts are permanently active, always assigned to all tokens. Our work also adopts this \"shared expert\" approach for our general base expert. Another notable work is BASE Layers [Lewis et al., 2021], where authors formulate the token-to-expert assignment as a linear assignment problem. However, these efforts primarily focus on improving the general performance and/or training stability of MoEs. In contrast, our work puts emphasis adaptability and extensibility.\nEfficient MoE Training by Re-Using Existing Dense Models. Training MoEs from scratch, i.e. from a random weight initialization, is computationally expensive [Gale et al., 2023; Fedus et al., 2022] and often challenging due to training instabilities [Zoph et al., 2022]. Alternatively, recent works have explored re-using existing dense models to initialize MoEs, thereby enhancing training efficiency. Sparse Upcycling [Komatsuzaki et al., 2023] re-uses a single dense model to initialize the MoE by by replicating dense model's FFN weights N times into N FFN experts in the MoE. The router is initialized randomly, and all other parameters are copied directly from the dense model. BTX [Sukhbaatar et al., 2024] extends this approach by upcycling not from a single dense model, but from multiple specialized dense expert models to encourage diversity in the MoE initialization. Furthermore, BAM [Zhang et al., 2024] expands BTX to upcycle not just FFN experts but also attention experts, further enhancing performance. Our work also leverages this approach by reusing\nexisting specialized dense experts for MoE initialization, while extending it further to facilitate on-the-fly adaptations for new experts specialized in unseen data domains.\nEfficient MoE Architectures. Zadouri et al. [2024] proposes replacing traditional MoE's computation-heavy feed-forward network (FFN) experts with more efficient experts comprised of smaller vectors and adapters, which are activated in parallel to a single dense FFN. This lightweight architecture necessitates only a limited number of parameter updates when finetuning, offering efficiency advantages. However, unlike our approach, it does not leverage existing specialized dense models and lacks a notion of specialized experts, which are central to our method. Similar to our work, Muqeeth et al. [2024] and Ostapenko et al. [2024] study combining separately trained experts into a unified model. However, they focus on parameter-efficient adapters such as LoRA [Hu et al., 2021] and supervised finetuning. In this work, we focus on efficiently pre-training fully-fledged MoE models via upcycling.\nAdaptive MoEs and Ensemble Models. ModuleFormer [Shen et al., 2023] also aims to produce adaptable MoEs. The authors achieve adaptability by freezing existing MoE parameters while only training newly added modules with optimization constraints to the router. Unlike our work, ModuleFormer does not leverage existing expert dense seed models for efficiency gains, nor does it have a notion of specialization which is central to our work. Similar to our work, DEMix [Gururangan et al., 2021] independently trains different FFN experts on specialized data domains, with each expert functioning as a domain-specific module. Modules can be added on-the-fly for adaptability. Followup works BTM and C-BTM [Li et al., 2022; Gururangan et al., 2023] extend DEMix to create adaptive ensemble models. However, all three works use a router requiring a forward pass for every expert at inference instead of sparsely activating them, which significantly increases inference costs, especially with a large number of experts. Unlike these approaches, our router cost is approximately the same as standard top-k routing during both training and inference, offering a more scalable solution for adaptability."}, {"title": "Conclusion", "content": "We propose Nexus, a new LLM framework that enables efficient upcycling of specialized dense experts into a sparsely activated MoE model. We show that individual experts in our method retain their specialization after upcycling, and that our router based on expert embeddings outperforms previous approaches for combining the dense experts. Furthermore, the model can be extended efficiently with new dense experts after the initial training phase, saving much compute compared to re-training the upcycled model or training from scratch."}, {"title": "Limitations", "content": "The MoE architecture is often employed for larger models in the multi-billion parameter range, where efficiency is paramount. However, to facilitate a broader set of experiments, we limit our setup to using 2.8B parameter seed models for the main results and 470M parameter seed models for ablations. Furthermore, our dense experts are based on existing data sources in the SlimPajama dataset which is pre-defined. Future work could extend our method by discovering specialized data domains through unsupervised clustering similar to Gururangan et al. [2023]."}]}