{"title": "A framework for measuring the training efficiency\nof a neural architecture", "authors": ["Eduardo Cueto-Mendoza", "John Kelleher"], "abstract": "Measuring Efficiency in neural network system development is an open research\nproblem. This paper presents an experimental framework to measure the training\nefficiency of a neural architecture. To demonstrate our approach, we analyze the\ntraining efficiency of Convolutional Neural Networks and Bayesian equivalents on\nthe MNIST and CIFAR-10 tasks. Our results show that training efficiency decays\nas training progresses and varies across different stopping criteria for a given\nneural model and learning task. We also find a non-linear relationship between\ntraining stopping criteria, training Efficiency, model size, and training Efficiency.\nFurthermore, we illustrate the potential confounding effects of overtraining on\nmeasuring the training efficiency of a neural architecture. Regarding relative\ntraining efficiency across different architectures, our results indicate that CNNS\nare more efficient than BCNNs on both datasets. More generally, as a learning\ntask becomes more complex, the relative difference in training efficiency between\ndifferent architectures becomes more pronounced.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence is predicted to be a critical enabling technology for many of\nthe 17 Sustainable Development Goals (SDGs). However, its current dependency on\nmassive datasets and computer power means that it will also inhibit the attainment of\nsome SDGs, particularly SDG7 (Affordable and Clean Energy) and SDG 13 (Climate\naction) [1]. Modern Artificial Intelligence (AI) uses data-driven methods like deep\nlearning. It is primarily driven by trends of ever larger datasets, larger models, and\nmore powerful computers with the sole concern of improving model accuracy [2]. This\ndynamic resulted in a 300,000x increase between 2012 and 2018 in the computation\nrequired to train a competitive DL model [3] (this trend far exceeds Moore's Law).\nIndeed, it has recently been estimated that training one AI model generated the CO2\nemissions equivalent to driving 700,000 km [4].\nThe environmental challenge posed by AI's growing energy needs and associated car-\nbon emissions has been recognized in recent years. For example, researchers in AI\nEthics have highlighted this challenge [5] and have called for more research on \u201csus-\ntainable methods of AI\" [6]. In response to these calls, there is a growing trend within\nAI research to move beyond systems evaluations solely based on accuracy. Recent\nresearch tends to report hardware details and training time alongside accuracy, and\nsome papers report FLOPS. However, time and FLOPS are not sufficient to charac-\nterize Efficiency. There is a growing body of work (e.g., [7], [8], [9], [10], [11]) that\nshows that more data is required to understand the energy and resource trade-off of\ndeep neural networks. Consequently, a critical step in developing sustainable AI is the\ndevelopment of measures for Efficiency that can be integrated into the development\nprocess of an AI system.\nThis paper directly addresses the need for a measure to characterize the Efficiency of\na neural network architecture on a specific hardware and learning task. A natural effi-\nciency ratio of interest for a neural architecture is the ratio between the accuracy of a\nneural model and the energy consumed to achieve this accuracy. Accuracy is usually\nmeasured using an appropriate measure for the task and dataset distribution (e.g., F1,\nAUC-ROC, etc.). However, several recent results highlight a non-linear relationship\nbetween the accuracy of a neural model and the size of the model [12]. This suggests\nthat there is likely a non-linear relationship between the training efficiency of an archi-\ntecture and the size of the model instantiating the architecture. At the same time,\nthere is a gap in the research literature in terms of how the training efficiency of a\nneural architecture varies across training. Understanding the dynamics of training effi-\nciency is crucial as it informs decisions relating to the stopping criterion for training.\nConsequently, in this work, we set out an experimental methodology for comparing the\nrelative Efficiency of different neural architectures in terms of their efficiency dynamics\nas training progresses and the changes in Efficiency as the size of the models instanti-\nating the architectures vary. This experimental methodology includes both a measure\nof Efficiency and an experimental framework for capturing the necessary data for the\nefficiency measure.\nIn order to test and demonstrate the usefulness of our efficiency measure, we use our\nexperimental framework to analyze the relative Efficiency of two different neural archi-\ntectures, a CNN network (LeNet) and a Bayesian Convolutional Network (BCNN),"}, {"title": "2 Related Work", "content": "Research on Efficiency in AI can broadly be categorized into four research streams:\narchitectures, compression, training regimes, and metrics. The first of these streams\nfocuses on developing more computationally efficient neural architectures. For exam-\nple, improving the Efficiency of the attention mechanism in transformer models [14]\nhas frequently been a target for this type of research. This is due to the popularity of\ntransformer models and the high complexity in time and space O(n\u00b2)\u2014of the standard\nattention mechanism. Within this category of work, the Reformer [15] proposes an\nefficiency improvement (in terms of computation and memory) to the standard trans-\nformer that replaces the regular dot-product attention mechanism with one that uses\nlocality-sensitive hashing, and the Linformer [16] replaces the transformer attention\nmechanism and approximates it by a low-rank matrix which reduces the complexity of\nthe attention layer to O(n). A recent survey of work on improving Efficiency in trans-fers is presented in [17]. Also, although research on neural architecture search has\ntraditionally focused on optimizing for a single objective (such as accuracy), recently,\nthere has been a growing interest in multi-objective neural architecture search which\nconsiders Efficiency (frequently hardware efficiency to enable edge deployment) as part\nof the optimization problem (see e.g., [18-21]).\nA second stream of research has focused on improving Efficiency by reducing model\nsize. Some of this work trades extra computation during initial model training for\nsmaller, more efficient models at inference. For example, the EfficientNet [22] and Effi-\ncientNet v2 [23] papers propose model scaling methods that seek to maximize model"}, {"title": "3 Defining an efficiency measure for Deep Neural\nNetworks", "content": "The concept of Efficiency is fundamental to this work:\nDefinition 1 Efficiency measures a system's capacity to achieve a goal (measured by\na metric) with a given amount of resources.\nWhen considering the training efficiency of a neural network on a learning task, it is\nnatural to consider how the accuracy of the network architecture varies as the energy\nconsumed for training changes. This is the efficiency ratio that equation 1 defines and\nthat Figure 1 illustrates (in this figure, the arrow represents an efficiency calculation-\nin the form of Equation 1-where the arrow points from the denominator to the\nnumerator).\nAccuracy\nEnergy Efficiency \u221d \n(1)\nEnergy --> Accuracy"}, {"title": "3.1 Metrics for Energy and Accuracy", "content": "Deciding what system components to report energy consumption over is not trivial.\nFor example, although the CPU, GPU, and memory are natural system components\nto consider when tracking energy consumption during the training of a network, other\nparts of the system, such as fans, buses, and transistors, also consume energy related to\ntraining [11, 56]. However, due to the difficulties in measuring the energy consumption\nof these secondary or satellite components, we have decided to focus our analysis on\nthe energy consumed during our experiments from the GPU, CPU, and RAM."}, {"title": "3.2 Allowing for hyperparameter variations: Model Size", "content": "To experimentally control for the effect of model size we propose to run each experi-\nment multiple times for each neural architecture using a different size model in each\nrun, and for each model size, record both the total energy consumed during training\nEsamples [W] and the accuracy obtained by the model. We then calculate the Effi-\nciency for each model on an experimental task as the ratio of accuracy to the total\nenergy consumed to train it. Finally, we calculate the Efficiency of a neural architec-\nture on an experimental task as the mean Efficiency of the models implementing that\narchitecture on the task. Figure 3 illustrates how model size is included in the experi-\nmental design, and Equation 3 defines how we integrate model size into the calculation\nof the training efficiency of a network architecture.\nModel Size: j={1, ...,N}\nEnergy --> Accuracy\nEff (arch, j = size) = \u2211 jEff (Acc, W, i = epoch);\n(3)"}, {"title": "3.3 Training Regime Variations: Convergence Criteria", "content": "The training efficiency of a network (accuracy/energy) is likely to vary as training\nprogresses; in other words, the gain in model accuracy per unit of energy expended is\nlikely to change between the early epochs of training and the later epochs of training.\nAt the same time, the amount of time a network is trained for will vary depending\non the convergence criteria used to stop training. To control for this, we define four"}, {"title": "4 Case Study: Convolutional and Bayesian\nConvolutional Architectures", "content": "In this case study, we demonstrate the use of our efficiency framework by comparing the\nEfficiency of a CNN network (LeNet) with that of a Bayesian Convolutional Network\n(BCNN). The BCNN network is trained using approximate variational inference, which"}, {"title": "4.1 Results from the case study", "content": "This section presents the results for the 50 epoch, early-stopping, energy-bound, and\naccuracy-bound experiments. For each experiment, dataset, and neural architecture,\nwe present a table showing the efficiency calculation across model size for each archi-\ntecture under the convergence criteria specified in that experiment (using Equation\n3). Note that in supplementary material we present, for each experiment, plots of the\ntraining and test accuracy by training epoch for each model."}, {"title": "4.1.1 50 epoch experiment", "content": "In this first experiment, the stopping criterion for training was set at 50 epochs. For\neach architecture (LeNet and BCNN), the experiment is run a total of 10 times per\narchitecture: once for each of the 5 model sizes (LeNet-1 to LeNet-5, and BCNN-\n1 to BCNN-5) on both the datasets (MNIST and CIFAR). During each run of the\nexperiment, we repeatedly recorded the energy being consumed and the amount of\nmemory (GPU and RAM) being used (recorded as model size (MiB) size in RAM and\nGPU memory)."}, {"title": "4.1.2 Early-Stopping experiment", "content": "This experiment has the same design as the 50 epoch experiment presented above,\nwith a single change in the convergence criteria used for training; in this experiment,\nwe use early-stopping criteria for accuracy.\nFor the MNIST dataset, Table 5 lists the efficiency calculation using Equation 3. For\nCIFAR Table 6 presents the efficiency calculation using Equation 3."}, {"title": "4.1.3 Energy bound experiment", "content": "In this experiment, the convergence criterion used to stop training was when the\nenergy samples recorded for a training run on an architecture cumulatively summed\nup to 100,000 W. Apart from this, the design of the experiment is the same as those\nreported in the previous two sections.\nMirroring the results from the previous experiments, for the MNIST dataset, Table 7\nlists the efficiency calculation using Equation 3. Similarly, for CIFAR, Table 8 presents\nthe efficiency calculation using Equation 3. Note that some of the values for total\nenergy listed in the results for this experiment are above the training convergence\ncriterion of 100,000W. These values are correct values from the experiment. The reason\nfor these values is that although we sample throughout the training process (the\naverage sampling rate for energy was 973 per second for the NVIDIA system and 1052\nsamples per second for the AMD system), we perform the check of the cumulative\namount of energy consumed during training at the end of each epoch. Consequently,\nthe energy consumed during a training run exceeds the stropping threshold if the\nprocess crosses that threshold during an epoch."}, {"title": "4.1.4 Accuracy bound experiment", "content": "The convergence criteria used in these experiments was to stop training when a model\nobtained a specified accuracy threshold. For the MNIST dataset this accuracy thresh-\nold was set at 99% on the training set, and on the CIFAR dataset (where we used\na six-fold cross-validation methodology) for each fold the training was stopped when\nthe model had obtained an accuracy threshold of 50% on the training data for that\nfold. Our reason for using a lower accuracy threshold for CIFAR was that an accuracy\nthreshold > 50% required training to proceed for more time than our Collab account\nallowed, and if this time threshold was exceeded, then the training was interrupted,\nand results were lost.\nFor MNIST Table 9 lists the efficiency calculation using Equation 3. Similarly, for\nCIFAR, Table 10 presents the efficiency calculation using Equation 3."}, {"title": "5 Analysis of experimental data", "content": "This section presents the analysis of the data obtained from our experiments regarding\nhow Efficiency behaves as training progresses, the relationship between model size and\nEfficiency, and the relative overall Efficiency of the LeNet and BCNN architectures."}, {"title": "5.1 Efficiency as training progresses", "content": "Figure 5 and Figure 6 plot for each of the models trained (LeNet sizes 1-5, and BCNN\nsizes 1-5) how the Efficiency of the model changes across epochs as training progresses.\nWe base this analysis solely on the results from the 50 epoch experiment because, in\nthis experiment, we have collected the same number of epochs for all sizes and both\narchitectures. As a result, the x-axis, which records the training epochs, goes from 0\nto 50 in both figures. The y-axis in the graph plots the Efficiency of a model at a\ngiven epoch as defined by Equation 2. This definition of Efficiency is the ratio of a\nmodel's performance on a validation set after epoch i of training to the cumulative\nenergy expended in training the model up to that point in training."}, {"title": "5.2 Relationship between stopping criteria and Efficiency, and\nmodel size and Efficiency", "content": "The results presented in Tables 3-10 reveal significant variation in architecture effi-\nciency across different stopping criteria. Note that this analysis considers the variation\nin Efficiency by model size. This variation is particularly noticeable in the MNIST\ndataset. Table 11 summarises (from Tables 3, 5, 7 and 9) the efficiency results for both\narchitectures across the four stopping criteria on the MNIST dataset. Examining the\nresults for LeNet, the maximum Efficiency (0.00002610) is obtained using an accu-\nracy bound stopping criterion, and the minimum Efficiency (0.00000809) is recorded\nusing the 50 epoch criterion. This means that LeNet is, averaging across model sizes,\napproximately 3.22 times more efficient on MNIST when the accuracy bound criterion\nis applied compared to the 50 epoch criterion. A similar variation in Efficiency across\nstopping criteria is observable for the BCNN architecture. However, the criteria that\nresult in the maximum and minimum values differ. For the BCNN architecture on\nMNIST, using an energy bound stopping criterion gives the maximum Efficiency of\n0.00000618 compared to the minimum Efficiency of 0.00000100 using early stopping,\na variation in Efficiency of 6.18 times. More generally, we observe a complex non-\nlinear interaction across architectures and convergence criteria, as shown in Figure 7,\nwhich plots the LeNet versus BCNN efficiency scores by convergence criteria. The\nwithin-architecture efficiency variation across stopping criteria and the complex inter-\nactions across architectures and stopping criteria highlight the need to include multiple\nstopping criteria within the efficiency framework."}, {"title": "5.3 Efficiency of the LeNet architecture against BCNN\narchitecture", "content": "Table 12 presents the overall efficiency calculations for the LeNet and BCNN architec-\ntures on the MNIST and CIFAR datasets. These efficiencies are the mean Efficiency of\narchitecture on a dataset across the multiple model sizes and convergence criteria (see\nEquation 4). On both datasets, LeNet is more efficient than the BCNN architecture."}, {"title": "5.4 On the risks of over-training (over-fitting)", "content": "As discussed in Section 5.1, the Efficiency of a neural architecture tends to decay as\ntraining progresses; this trend is evident in Figure 5 and Figure 6 where for both\narchitectures on both datasets efficiency consistently reduces as training progresses.\nThis trend reflects that as training progresses, model performance saturates after a\ncertain point, and further training expends more energy with no gain in performance.\nAn implication of this is that if a neural model is trained for an extreme number of\nepochs, then the training efficiency of that architecture will tend to zero, and further-\nmore, in such a scenario, comparing the Efficiency of different neural architectures is\nno longer sensible because all architectures will have an efficiency of zero. Put another\nway, the measurement of the training efficiency for a neural architecture only makes\nsense when models are not overtrained.\nThe most direct definition of overtraining is epochs of training that do not improve\nmodel performance. Another complementary way of identifying when overtraining has\noccurred is through the concept of over-fitting. Overfitting occurs when a model learns\nto perform well on the training data but fails to generalize to unseen data, compromis-\ning its Efficiency. Overfitting can be checked for by comparing the divergence between\na model's performance on training data versus non-training data. To illustrate both\noverfitting and the impact of overtraining training efficiency, we extend our 50-epoch\nexperiment to 100 epochs. We then perform two levels of analysis. First, we check\nwhether the models trained for 100 epochs exhibit overtraining (compared to those\ntrained for 50 epochs). Then, we calculate the Efficiency of both architectures using\nthe results from the 100-epoch experiment in order to understand how overtraining\ncan affect training efficiency.\nWe examine two measures to check whether extending training from 50 to 100 results\nin overtraining a model. First, we check whether the extra training resulted in an\nappreciable increase in model performance on the test set; if there is no increase in test\nset performance between the 50th and 100th epoch, then we deem the 100 epoch model\nto be overtrained. Second, suppose a model exhibits an increase in test set performance\nbetween the 50th and 100th epochs. We check for overfitting by comparing the model's\nperformance on the training data and the test set. The intuition behind this analysis is\nthat the more significant the drop in the performance between the training data and a\ntest set, the more likely the model will be overfitted (and hence overtrained). In more\ndetail, we calculate the difference between a model's training and test performance\nafter 50 epochs of training and after 100 epochs of training and then calculate the\ndelta between these differences. This delta in the differences reveals the extent of\ndivergence between training and test performance caused by the extra 50 epochs of\ntraining. Using this delta metric, we deem a model to be overtrained if the delta is of\na comparable scale to the increase in the test set performance of the model between\nthe 50th and 100th epochs."}, {"title": "6 Conclusions", "content": "We present a framework for measuring the training efficiency of a neural architecture\non a learning task. This framework involves running multiple experiments but does not\nrequire hardware profiling. Moreover, the framework enables a multifaceted analysis\nof the training efficiency of a neural architecture, including the analysis of how the\nEfficiency of a model varies across training epochs (Equation 2), how the Efficiency of\na neural architecture varies with model size (Equation 3) and the overall Efficiency of\na neural architecture on a learning task taking into account variations in model size\nand stopping criteria (Equation 4). Furthermore, the ability to calculate an overall\nefficiency for a neural architecture on a learning task enables the analysis of the relative\nEfficiency of different neural architectures on a learning task and how the relative\nEfficiency of neural architectures varies across learning tasks.\nApplying the framework to the case study comparing CNNs with BCNNs on MNIST\nand CIFAR, we find that the Efficiency of both architectures on both learning tasks\nchanges substantially as training progresses (see Section 5.1), with all models exhibit-\ning a drop in Efficiency across epochs. The analysis in Section 5.2 reveals a non-linear\nrelationship between stopping criteria and training Efficiency and model size and\ntraining Efficiency. We observed significant variation in training efficiency across dif-\nferent stopping criteria for both architectures. This variation across stopping criteria\nillustrates the need for multiple stopping criteria within the efficiency framework.\nMoreover, including multiple convergence criteria within the framework mitigates the\nrisk of overtraining affecting the analysis of the training efficiency of neural archi-\ntectures (see Section 5.4). More generally, we believe that the potential confounding\neffect of overtraining on neural training efficiency research is not given sufficient atten-\ntion in the literature. To take a recent example, [46] report, as a key finding, that the\nefficiency improvements obtained by several training regime modifications vanished\nwhen the compute budget allowed for training increases. However, in their analysis,\nthe authors did not consider that this finding may result from overtraining occur-\nring at different points under different training regimes. Indeed, the more efficient a\ntraining regime is, the earlier in the training process overtraining will begin, in which\ncase, using a fixed compute budget as a convergence criterion is likely to result in\nmore efficient training regimes overtraining for longer. So, the extra overtraining will"}, {"title": "Appendix A Hardware comparison", "content": "We replicated our experiments on a second hardware setup to demonstrate our frame-\nwork's generalizability and findings. Table Al shows the characteristics of this second\n(AMD) hardware platform. Due to the smaller capabilities of this hardware platform,\nthe training regime was modified for the CIFAR dataset; instead of using six-fold val-\nidation, we used a single 70-30 split on the data. This modification allows the training\nto be completed on this AMD hardware without any memory overflow. Apart from\nthis modification, the same training regimen, architectures, and hyperparameters as\ndescribed in Section 4 were used in these experiments.\nThe experimental data was processed in the same manner as in Section 4.1, obtaining\nthe following results:"}]}