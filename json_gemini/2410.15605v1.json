{"title": "DEEP ACTIVE LEARNING WITH MANIFOLD-PRESERVING TRAJECTORY SAMPLING", "authors": ["Yingrui Ji", "Vijaya Sindhoori Kaza", "Nishanth Artham", "Tianyang Wang"], "abstract": "Active learning (AL) is for optimizing the selection of unlabeled data for annotation (labeling), aiming to enhance model performance while minimizing labeling effort. The key question in AL is which unlabeled data should be selected for annotation. Existing deep AL methods arguably suffer from bias incurred by clabeled data, which takes a much lower percentage than unlabeled data in AL context. We observe that such an issue is severe in different types of data, such as vision and non-vision data. To address this issue, we propose a novel method, namely Manifold-Preserving Trajectory Sampling (MPTS), aiming to enforce the feature space learned from labeled data to represent a more accurate manifold. By doing so, we expect to effectively correct the bias incurred by labeled data, which can cause a biased selection of unlabeled data. Despite its focus on manifold, the proposed method can be conveniently implemented by performing distribution mapping with MMD (Maximum Mean Discrepancies). Extensive experiments on various vision and non-vision benchmark datasets demonstrate the superiority of our method. Our source code can be found here.", "sections": [{"title": "1. INTRODUCTION", "content": "Active learning (AL) has emerged as an effective strategy to enhance model performance while minimizing the need for extensive labeled data. The fundamental concept of active learning [1] lies in allowing the model to selectively choose the data it learns from, thereby achieving significant performance improvements with fewer labeled samples.\nTypical uncertainty-based AL solutions include techniques such as Monte Carlo (MC) dropout [2], which estimates uncertainty by performing multiple stochastic forward passes with dropout layers enabled. The assumption behind this is to improve uncertainty estimation by sampling parameters from their posterior distribution. Another approach Bayesian Neural Network (BNN) [3] models the posterior distribution over the parameters directly, often assuming a Gaussian form. Other solutions involve model ensemble [4, 5], which trains multiple independent instances of a neural network with different initialization. While these methods provide improved estimates of uncertainty, they often suffer from two fundamental limitations as follows.\nFirst, to accurately estimate the posterior distribution of model parameters $P(\\theta|D)$, it is crucial to have a representative distribution of the data. However, current active learning methods often face the risk of biased data sampling due to the limited number of examples used for model training. This issue becomes more pronounced in a multi-cycle active annotation process, where the repeated selection of the most uncertain examples can lead to a progressively biased data distribution. Second, many existing approaches rely on explicit assumptions, such as assuming a Gaussian form for the posterior distribution, or require modifications to the model architecture, such as incorporating dropout layers, to enable parameter sampling. These assumptions and architectural changes may not always be reliable, potentially limiting the applicability of the methods to different models.\nTo address these challenges, we introduce a novel active learning algorithm named Manifold-Preserving Trajectory Sampling (MPTS). A core idea in our approach is to ensure the model is trained to be consistent with the true data manifold, thereby mitigating the risk of bias that arises from only using the most uncertain examples across multiple active learning cycles. When training the discriminative model for uncertainty estimation, we utilize the sufficient unlabeled examples to regularize the feature distribution from the labeled examples. During the regularized training, we sample parameters from the optimization trajectory near local minima by periodically averaging model parameters encountered during the latter stages of training. This technique provides an effective sampling by considering multiple points along the trajectory that are close to a local minimum, thereby capturing a diverse set of model parameters that represent different modes of the posterior distribution.\nTo our best knowledge, our paper is the first to propose data bias correction in estimating the posterior distribution $P(\\theta|D)$ for active learning. Besides, while parameter sampling from optimization trajectories has been explored in the literature, these methods often rely on explicit distribution modeling, which can increase complexity and introduce the risk of making unreliable assumptions. Our approach ensures an effective and unbiased parameter sampling from the optimization trajectory fir uncertainty estimation. Through extensive experiments, we demonstrate that our method con-"}, {"title": "2. RELATED WORK", "content": "Active learning is a pivot research area in machine learning, focused on optimizing data annotations to enhance model performance with fewer labelled samples. Most AL methods mainly consider uncertainty as a crucial criterion to intelligently sample data that improves model's generalization. Such methods prioritize data points with high prediction variance or near the decision boundary, employing techniques like MC-Dropout [2], Query-by-Committee (QBC) [6], and adversarial training [7] to address overconfident deep neural networks [8, 9]. Influence-based AL approaches select data points based on their estimated impact on model performance, using schemes like Learning Loss [10], and the Influence Function [11] that leverages gradient to estimate changes in prediction accuracy [12, 13]. Besides, BADGE [14] also aims to select uncertain data by evaluating gradient. Many deep AL methods resort to auxiliary models to estimate data uncertainty. Typical works include VAAL [9] that uses an auxiliary auto-encoder, and GCNAL [15] that employs a graph network as the auxiliary model. Unlike these methods, the Coreset [16] is free of any auxiliary models, but suffering from a slow optimization process (e.g., solving a classical K-center or 0-1 Knapsack problem) during data selection. Several other works, such as [17], rely on complicated training fashion (e.g., adversarial), and it will be challenging if using such methods on a different data format (e.g., 3D medical images of voxels) other than 2D natural images."}, {"title": "2.2. Posterior Approximation for Bayesian Neural Networks", "content": "Bayesian Neural Networks (BNNs) are designed to provide robust uncertainty estimates by treating the network's parameters as probabilistic distributions rather than fixed values. This approach is essential for capturing uncertainty in tasks like active learning. Several works [18, 19, 20] propose to estimate posterior distributions by averaging the training checkpoints. To this end, they use Stochastic Weight Averaging (SWA) to perform the averaging operation, improving the uncertainty estimation. These methods offer practical solutions for reliable uncertainty estimation in deep networks. Notably, our method has a very low level of similarity with these methods, as we propose a brand new solution to estimate posterior considering both labeled and unlabeled data simultaneously."}, {"title": "3. METHOD", "content": "We first introduce the multi-cycle deep active learning problem setting. Starting with a set of unlabeled samples, U, and a labeled set, L, the objective is to select a subset from U according to a predefined annotation budget. This chosen subset, $X_N$, is annotated by experts or equivalent sources, resulting in labeled data: $L \\leftarrow L\\cup\\{X_N, Y_N \\}$, where $Y_N$ are the labels. The remaining unlabeled data is updated: $U \\leftarrow U\\backslash X_N$. In the process of subset selection, we typically need to train a model parameterized with $\\theta$ as $f(\\cdot;\\theta)$. Considering a classification task with C classes, Entropy can be used to estimate the prediction uncertainty for a given example x as\n$H(x) = - \\sum_{c \\in C} p(y = c|x, \\theta) \\log p(y = c|x, \\theta)$.\nwhere $p(y = c|x, \\theta)$ corresponds to the softmax probability of the c-th class from the prediction $f(x; \\theta)$.\nFor better estimation of the probability, BNN approaches incorporate the posterior distribution of $\\theta$ by\n$p(y = c|x, D) = \\int p(y = c|x, \\theta) p(\\theta|D) d\\theta.$\nwhere D represents the data distribution. While $p(y = c|x, \\theta)$ can be easily calculated from the network, the key problems turn into (1) estimating the posterior distribution $p(\\theta|D)$ and (2) sampling from the parameter distribution to approximate the integration. Next, we introduce our solutions for them."}, {"title": "3.2. Posterior Estimation with Manifold-Preserving", "content": "Traditional MC-dropout [2] or other BNN based approaches suggest to estimate the posterior distribution $p(\\theta|D)$ by training a model with the available labeled set L. Here we analyze its potential risk and propose our method by the following analysis.\nSince a direct calculation of $p(\\theta|D)$ is infeasible, we apply the Bayes rule as\n$p(\\theta|D) = \\frac{p(\\theta)p(D|\\theta)}{p(D)} \\propto p(\\theta)p(D|\\theta)$.\nRegarding p(D) as data sampling, we further decompose the posterior distribution into the multiplication of the prior term $p(\\theta)$ and the likelihood $p(D|\\theta)$. Since $p(\\theta)$ can be realized by common regularization techniques such as weight decay, the flexible task is to estimate $p(D|\\theta)$. Given the data distribution D, we convert the problem of sampling the most possible parameters into finding those $\\theta$ that maximize the likelihood term.\nDenoting the input data by X, labels by Y and the deep features by Z with the dependency chain as $X \\rightarrow Z \\rightarrow Y$ in a discriminative model, we further decompose the likelihood as $p(D|\\theta) = p(X, Z, Y|\\theta)$. Therefore we have\n$p(D|\\theta) = p(X|\\theta) p(Z|X, \\theta) P(Y|X, Z, \\theta)$.\nGiven the potentially biased labeled set L as the training set, existing approaches only have the effect of maximizing the conditional distribution term $p(Y|X, Z, \\theta)$, which is sub-optimal in maximizing $p(D|\\theta)$."}, {"title": "3.3. Sampling from Optimization Trajectory", "content": "We use the optimization trajectory to sample a diverse set of checkpoints by leveraging the path of minimizing Eq. (7) with stochastic gradient descent (SGD). This approach intends to explore different regions of the parameter space that are likely to have a high posterior probability as in Eq. (3). Specifically, we adopt the technique proposed by Stochastic Weight Averaging (SWA) for parameter sampling. By using a cyclic learning rate after the half convergence, a set of checkpoints $\\{\\theta_\\tau\\}_{t=1}^n$ are sampled and saved.\nFinally, we calculate the prediction probability by averaging the prediction of each sampled parameter as\n$p(y = c|x, D) = \\frac{1}{n} \\sum_{\\tau=1}^n p(y = c|x, \\theta_\\tau),$\nwhere each $\\theta_\\tau$ is sampled in the trajectory of minimizing Eq. (7). We substitute Eq. (8) into Eq. (1) to calculate the uncertainty for each unlabeled example from U.\nNote that the manifold preserving and parameter averaging approaches are designed only to select uncertain examples in our active learning framework, despite that SWA is more popular as a strategy to improve model accuracy."}, {"title": "4. EXPERIMENTS", "content": "Here we introduce a series of experiments conducted to validate the proposed method. To make the evaluation more comprehensive, we consider multiple datasets, backbone models, as well as different AL settings. We refer readers to Table 1 for details."}, {"title": "4.1. Datasets and Baselines", "content": "Datasets. As most deep AL methods have been evaluated in computer vision challenges, we also adopt four widely used benchmark vision datasets to evaluate our method, including MNIST [22], CIFAR10 [23], SVHN [24], and Mini-ImageNet [25]. In addition, we also incorporate two typical non-vision datasets for the evaluation, namely OpenML-6 [26] and OpenML-155 [26], which are tabular datasets from the OpenML repository, including structured data with mixed types of features.\nBaselines. The baseline methods that we consider in this paper can be categorized into three groups. The first group resorts to estimate data uncertainty based on posterior, including Entropy [27], BALD [3], BADGE [28]. The second group designs customized methods to evaluate data uncertainty, including Coreset [29], CDAL [30], and Feature Mixing [31]. The third group relies on auxiliary models and/or special training fashion (e.g., adversarial), including Adversarial Deep Fool [7] and GCNAL [15]. In addition, Random selection is also included as it is a straightforward yet effective method in several scenarios.\nModels. We use three types of deep models as the backbones. Specifically, we use MLP [28] for MNIST, ResNet-18 [32] as a typical CNN for CIFAR10 and SVHN, and vision transformer (ViT) [33] as a typical foundation model for Mini-ImageNet. We also use MLP for the two non-vision datasets."}, {"title": "4.2. Experimental Settings", "content": "For each dataset, following the common practice in AL literature, we randomly select a small portion of data as initial samples and annotate them. The number of such such samples is 100 for all the datasets, except the Mini-ImageNet in which we use 1000 initial samples. Then in each AL round (covering both model training and data selection phases), we select 100 unlabeled samples (labeling budget) for all the datasets, except the Mini-ImageNet where we select 1000 unlabeled samples for initial annotation. When MLP or CNN is used as the backbone, within each AL round, we train the model for 100 epochs. When pre-trained ViT is used, we fine-tune it for 1000 epochs within each AL round. We adopt a learning rate of $1e-3$ for vision datasets and $1e-4$ for non-vision datasets. The batchsize is set to 64 for all the experiments. Notably, we train the MLP and CNN from scratch, whereas we fine-tune the pre-trained ViT following the practice in [31] for a fair comparison. To reduce randomness, we repeat each experiment for 5 times and average the results as the final one."}, {"title": "4.3. Results and Analysis", "content": "As shown in Figure 1, across the multiple datasets widely used in computer vision scenarios, the proposed method outperforms the others in terms of the classification accuracy. In addition to this overall evaluation, we have the following observations. First, our method yields solid results when different backbone models are employed, i.e., MLP for MNIST, CNN for CIFAR10 and SVHN, and ViT for Mini-ImageNet. This observation demonstrates the strong adaptability to the mainstream deep learning models. Second, when labeling budget is limited, such that only 100 samples can be annotated at a time (i.e., MNIST, CIFAR10, and SVHN), our method gives consistent better performance than the others, demonstrating its potential in real-world scenarios where labeling cost could be extremely high (e.g., in medical domain). Third, compared to the improvement in MNIST, our method outperforms the others by a more significant margin in CIFAR10 and SVHN, indicating the capability of our method on handling data of complex scene (i.e., CIFAR10 and SVHN have more complex scene than MNIST). Fourth, when labeling budget is relatively high (i.e., 1000 in Mini-ImageNet whereas 100 in the others), our method still shows consistent superiority over the others. This further demonstrates the strong capability of our method since most AL peers fail to show their superiority when labeling budget is high. Last, compared to the AL method that is based on auxiliary models, i.e., GCNAL [15] that uses an auxiliary graph network for data selection in addition to the task model, our method yields more appealing results, suggesting that developing deep AL methods without leveraging auxiliary models could be a promising effort.\nIn addition, as illustrated in Figure 2 we surprisingly find that our method significantly outperforms the others in non-vision datasets, such as OpenML-6 and OpenML-155. As MLP is used as the backbone model rather than CNN for such structured tabular data, the inferior performance yielded by the other methods indicate that MLP is more impacted by data bias, while our method can effectively handle the bias issue. Notably, in MNIST where MLP is also used (see Fig. 1), the performance gaps between our method and the others are not that big since MNIST is a fairly simple dataset that cannot make the methods show their upper bounds."}, {"title": "5. CONCLUSION", "content": "In this paper, we theoretically find that it is risky to estimate data uncertainty based on estimating posterior distribution only with labeled data in deep active learning. Motivated by this, we propose a novel substitution leveraging manifold-preserving to avoid the risk. We then design a simple and feasible solution to integrate the proposed scheme into the training of deep models within active learning context. Experimental results demonstrate that the proposed method is superior in various scenarios. Moreover, the simplicity of the realization may lead to its potential of being widely used in active learning tasks and beyond."}]}