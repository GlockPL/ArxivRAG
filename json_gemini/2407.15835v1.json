{"title": "dMel: Speech Tokenization made Simple", "authors": ["He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Zakaria Aldeneh", "Ruixiang Zhang", "Navdeep Jaitly"], "abstract": "Large language models have revolutionized natural language processing by lever-\naging self-supervised pretraining on vast textual data. Inspired by this success,\nresearchers have investigated complicated speech tokenization methods to dis-\ncretize continuous speech signals so that language modeling techniques can be\napplied to speech data. However, existing approaches either model semantic to-\nkens, potentially losing acoustic information, or model acoustic tokens, risking the\nloss of semantic information. Having multiple token types also complicates the\narchitecture and requires additional pretraining. Here we show that discretizing\nmel-filterbank channels into discrete intensity bins produces a simple representa-\ntion (dMel), that performs better than other existing speech tokenization methods.\nUsing a transformer decoder-only architecture for speech-text modeling, we com-\nprehensively evaluate different speech tokenization methods on speech recognition\n(ASR) and speech synthesis (TTS). Our results demonstrate the effectiveness of\ndMel in achieving high performance on both tasks within a unified framework,\npaving the way for efficient and effective joint modeling of speech and text. Our\ncode will be published soon.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have achieved remarkable success in various natural language process-\ning tasks by leveraging self-supervised pretraining on massive amounts of textual data [8]. Inspired by\nthis success, numerous works [7, 27, 39, 31] have sought to extend the language modeling approach\nto speech processing, aiming to build unified models capable of both speech understanding and gen-\neration tasks. However, a key challenge lies in the continuous nature of speech signals, necessitating\neffective tokenization methods to discretize the input for language model-based processing.\n\nCurrent speech tokenization approaches can be broadly categorized into two types: semantic tokens\nand acoustic tokens. Semantic tokens, extracted from self-supervised (SSL) pretrained speech\nmodels [2, 14], where the speech signal is first encoded into speech representations and then clustered\ninto semantic tokens with k-means method. However, such SSL pretrained models are not useful\nfor high fidelity speech synthesis as speaker identity and other details of raw speech are lost in\ntraining [7]. Conversely, acoustic tokens can be obtained from audio compression models that are\ntrained to compress the speech signal into codebook indices with residual vector quantization (RVQ)\nand reconstruction objectives [37, 11]. These tokens prioritize acoustic reconstruction but lose\nsemantic information which can lead to poorer results in generating audio [31].\n\nTo combine the advantages of both semantic and acoustic tokens, AudioLM [7] proposed to model\nboth semantic tokens and acoustic tokens with 3 stages: semantic modeling, coarse acoustic modeling,"}, {"title": null, "content": "and fine acoustic modeling. The coarse-to-fine modeling strategy is designed to match the residual\nstructure of RVQ based acoustic tokens. This solution addresses both content and speech quality,\nbut its multi-stage hierarchical structure complicates the model and can lead to slower training and\ninference. Another solution is to combine the semantic and acoustic features together. Zhang et al.\n[40] proposed to distill the semantic tokens into the acoustic token's first residual channel during the\ntraining of the RVQ model in a teacher-student manner. In this way, they showed that the new feature\ncan preserve the semantic information better and also reconstruct high quality speech signals.\n\nIn this paper, we raise the following fundamental question \u2013 do we really need to separate speech\ninto semantic and acoustic tokens first, and process them with idiosyncratic architectures ?\nWe propose a simple alternative called dMel (see Figure 1) that discretizes mel-filterbanks en-\nergies directly into ordinal bins. Intriguingly, we find that discretizing Mel spectrograms has\nlittle impact on the ability of off-the-shelf mel-filterbank vocoders to reconstruct waveforms."}, {"title": "Method", "content": "In this section, we first introduce our proposed dMel speech tokenization method, which discretizes\nmel-filterbanks energies directly into bins. We then describe our unified transformer decoder-only\nmodel for ASR and TTS tasks, which leverages dMel for speech tokenization. The model architecture\nis illustrated in Figure 2."}, {"title": "dMel Speech Tokenizer", "content": "Different from existing VQ-VAE [7, 40, 17, 37] based speech tokenizers, we propose a discretized\nmel-filterbanks based speech tokenizer. The outline of the discretization method is shown in Figure 1.\nLater in the paper, we show that this tokenizer allows the model to process the input speech signal\nefficiently and capture the relevant acoustic features for both ASR and TTS tasks."}, {"title": null, "content": "We denote tensors as $X$ while $X_{i,...}$ denote the $(i, ...)$-th component of tensor $X$. First, the speech\ntokenizer takes the input speech signal x and computes the mel-filterbanks representation M:\n\n$M = Mel(x)$,\n\nwhere $Mel(\\cdot)$ represents the function that computes the mel-filterbanks, $M \\in \\mathbb{R}^{T \\times N}$, N is the\nnumber of filterbanks and T is the number of frames in the spectrogram."}, {"title": "Tokenization", "content": "To discretize the mel-filterbanks representation M into speech tokens, we adopt a\ncodebook C. In this paper, we apply a simple linear discretization, so that the codebook $C\\in \\mathbb{R}^{2K}$\nand its values are evenly spaced in the range of the mel-filterbanks values:\n\n$m = \\min(M_{t,i}), \\qquad M = \\max(M_{t,i}) \\qquad \\delta = \\frac{M - m}{2K},$\n\n$C = [m, m + \\delta, m + 2\\delta, .., m + (2K - 1)\\delta]$.\n\nIn practice, we compute the minimum m and maximum M values of mel-filterbanks across the\nentire dataset to define the codebook C. Then we map a magnitude $M_{t,i}$ of every frequency channel\n$i = 1 ... N$ for the time frame $t = 1 . . . T$ into a bin index of the codebook C in the following way:\n\n$S_{t,i} = Discretize(M_{t,i}) = \\underset{j}{\\text{argmin}}|M_{t,i} \u2013 C_j|$\n\nwhere $S \\in B^{T \\times N}$ represents the discretized mel-filterbanks (dMel) with $B = {j|j = 1, 2, 3, . . .2^K }$\nand $S_t \\in B^N$ being the t-th speech token. As the codebook C has $2^K$ distinct values and thus\nnumber of bins $|B| = 2^K$, each speech token is represented by $N \\cdot K$ bits where every K bits are\nused to represent one of N frequency channels."}, {"title": "Detokenization", "content": "To reconstruct the speech signal x from the speech tokens S, we first transform\nbin indices back to the mel-filterbanks representation via the codebook C:\n\n$M_{t,i} = C_{S_{t,i}}$\n\nThen, we apply a vocoder [36] to transform reconstructed mel-filterbanks $M_{t,i}$ back into the time\ndomain signal x. The vocoder is trained independently and is not part of the transformer decoder-\nbased model."}, {"title": "Unified Speech-Text Transformer Decoder", "content": "Modeling speech and text sequences jointly is essential for a model to understand and generate both\nmodalities. However, it is challenging to design a unified model that can handle both speech-to-text\nand text-to-speech effectively. In this work, we apply a unified transformer decoder-only model that\ntakes speech and text tokens as input and generates the output tokens in the target sequence. The\nmodel is trained in end-to-end on a combined dataset of speech and text pairs, enabling it to learn the\njoint representations for ASR and TTS tasks. As we show in the rest of the paper, the crucial part for\nthe joint model training is the proper speech tokenization which dMel provides."}, {"title": "Token Representation", "content": "For text data, we apply a character-level tokenizer to convert the input text\ninto a sequence of text tokens. The text tokens are passed through an embedding layer, Embed() :\n{j|j = 1,2,3... L} \u2192 $\\mathbb{R}^{D}$, where D is the embedding dimension and L is the vocabulary size. The\ndimension of the speech token embedding is set to be the same as the text token embedding D and\nno further mapping is required. The motivation for using a character-level tokenizer is to reduce the\nvocabulary size L and improve the model's generalization ability. Also, character tokens can capture\nthe fine-grained linguistic features that are essential for both ASR and TTS tasks.\n\nFor speech signal, we apply the dMel speech tokenizer to convert the input speech signal into a\nsequence of speech tokens. Then, the speech tokens $S \\in B^{T \\times N}$ are passed through a learnable\nembedding layer, Embed(\u00b7) : $B \u2192 \\mathbb{R}^{d}$, and a learnable linear layer, Linear(\u00b7) : $\\mathbb{R}^{N \\times d} \u2192 \\mathbb{R}^{D}$, to\nobtain the speech token representation $E \\in \\mathbb{R}^{T \\times D}$:\n\n$E'_t = Concatenate([Embed(S_{t,1}), Embed(S_{t,2}), . . ., Embed(S_{t,N})])$,\n\n$E_t = Linear(E'_t)$,\n\nwhere $E_t \\in \\mathbb{R}^{D}$ is the speech token representation. Here, for every time frame t a speech token $S_t$ is\nprocessed in parallel and independently for every frequency channel i by Embed($S_{t,i}$) mapping, and\nthen embeddings of all frequency channels are stacked together to form one vector representation $E'_t$\nfor the frame t. Finally, the speech token embeddings $E_t$ are fed into the decoder-only transformer\nmodels for further processing.\n\nWe also implemented other popular speech tokenizers including HuBERT-KM [19] and SpeechTok-\nenizer [40] for comparison. The main difference among these speech tokenizers is the codebook size\nand codes dimension, which are shown in Table 2. For both HuBERT-KM and SpeechTokenizer the\nspeech tokens are mapped via a learnable linear layer from their dimension to the text embedding\ndimension D before feeding into the transformer decoder-only model."}, {"title": "Speaker Representation", "content": "To properly model multi-speaker data, we also include speaker embed-\ndings as input to the transformer decoder. The speaker embeddings are extracted from an independent\ndvector [30] model. We use a learnable linear layer to map the speaker embeddings to the same\ndimension as the speech and text token embeddings D. The speaker representation is optional for\nASR task, but required for TTS task. Hence, during the training, it is applied for text-to-speech and\nignored for speech-to-text."}, {"title": "Transformer Decoder", "content": "The transformer decoder is trained end-to-end on a combined dataset of\nspeech and text pairs. For TTS training, the input sequence is constructed by concatenating the\nspeaker embedding (extracted from a random audio for the same speaker of the current sample), text\ntokens, and speech tokens. For ASR training, the input sequence is constructed by concatenating the\nspeech tokens and text tokens. Both tasks are trained with causal masking, where the model is trained\nto predict the next token based on the previous tokens. The loss is calculated using the cross-entropy\nloss between the predicted tokens and the ground-truth tokens. Loss calculation is skipped on the\nspeech tokens for ASR task and on the text tokens for TTS task. Note, that all frequency channels at\ntime frame t for dMel tokenizer are predicted independently and in parallel.\n\nTo capture the relative distances between tokens in the input sequence we apply multiplicative relative\npositional embedding RoPE [29]. This allows the model to learn the positional relationships between\nspeech tokens, text tokens, and speaker embeddings, enhancing its ability to generate coherent output\nsequences. For positional embeddings we do not distinguish between text, speech and speaker\nembeddings having global positions notation across all kind of tokens."}, {"title": "Robust Training", "content": "Compared to language modeling, audio frames are highly redundant with strong local correlations.\nThis makes long form generation difficult for models due to exposure bias [6]. To mitigate exposure\nbias during training, we apply span-masking [25] to the speech token context, masking out multiple\nrandom spans of speech frames. The model is trained to predict the next token based on the masked\ncontext. This context-masking strategy helps the model learn to generate accurate speech tokens in\nthe presence of missing information, improving its robustness and generalization. It forces the model\nto attend to the text rather than copying previously inferred speech tokens due to learnt correlations.\nWe also find that span-masking text tokens improves the ASR task."}, {"title": "Experiments", "content": "In this section, we conduct ASR and TTS experiments using dMel tokens with a unified transformer\ndecoder-only model. We evaluate the performance of our model mainly on the LibriSpeech dataset\nand compare it with state-of-the-art speech tokenization methods, ASR and TTS models."}, {"title": "Training Data", "content": "We use several open-sourced datasets with paired speech and text transcription to conduct experi-\nments: i) LibriSpeech [22] dataset (CC BY 4.0) consists of English speech recordings (960h, 16kHz)\nfrom various speakers (~2k) and conditions; ii) LibriTTS [38] (CC BY 4.0) dataset derived from Lib-\nriSpeech improves over it with the proper sentence split, text normalization and keeping 24kHz. It has\naround 500h; iii) VCTK [34] contains 44 hours of English speech (108 speakers); iv) LJSpeech [15]\ndataset (public domain in USA) is a single speaker English audio recordings of 16kHz with read"}, {"title": "Training Configuration", "content": "We train decoder-only transformers in three different sizes: Small, Base, and Large (see Appendix\nTable 9). By default, the Base model is used in all experiments if not stated otherwise. All models use\npre-LayerNorm with dropout set to 0.1 for residual, attention and embedding and 0.3 for positional\nembedding. dMel uses 16 discrete bins for each channel while text is tokenized with a character\nvocabulary; the speaker embedding dvector has 512 dimensions(see Appendx D for details)."}, {"title": "Main Results", "content": "In this section, we first evaluate different tokenization methods on speech reconstruction. Then we\nreport the TTS and ASR results using an LM-style (decoder-only) model with different speech tokens."}, {"title": "Speech Reconstruction", "content": "Following [40], we randomly sample 300 speech utterances and their ground truth transcriptions\nfrom the LibriSpeech test-clean dataset. We use the speech2unit and unit2speech modules to convert\nthe speech signal to speech tokens and then reconstruct the speech signal from the speech tokens. We\ncompute the WER between the ASR outputs from HuBERT-Large [14] on the audio samples and\ntheir ground truth transcripts. We also report MOS-LQO (Mean Opinion Score - Listening Quality\nObjective) score to measure the reconstruction quality using ViSQOL [13]. Finally, use human\nevaluation to measure the naturalness of the reconstructed speech using a MOS score. We instruct the\nhuman evaluators to rate the naturalness of the reconstructed speech on a scale of 1 to 5, where 1 is\nthe worst and 5 is the best. The results are shown in Table 3."}, {"title": "LM-Style Text-to-Speech", "content": "Here we compare the accuracy and naturalness of speech synthesized by LM-style text-to-\nspeech (TTS) models trained on different tokenization methods. For TTS evaluation, we utilize\nWhisperX [4] (\u201cbase.en\u201d from [24]) to transcribe our generated speech into text and calculate the\nWER and the character error rate (CER). We report both WER and CER to facilitate comparisons to\nprior works which have reported only one or the other.\n\nWe trained the TTS model using the same architecture but with three different tokenization methods:\nHuBERT+KM (with 200 clusters), SpeechTokenizer, and dMel. Additionally, we present the results\nfrom VOXTLM [21] and USLM [40] for comparison. VOXTLM is a larger model trained on more\ndata that is initialized from a pretrained LLM (OPT) using HuBERT-KM as the speech tokenizer.\nUSLM comprises an autoregressive (AR) model and a non-autoregressive (NAR) model, both trained\nwith the SpeechTokenizer.\n\nAs shown in Table 4 for training on LibriSpeech dataset, our decoder-only model with dMel tokeniza-\ntion achieves a WER of 4.3 and a CER of 1.8, significantly outperforming the baseline methods. This\nindicates that our model can generate more accurate speech with less hallucination and distortion.\nFurthermore, we observed that the AR model trained on SpeechTokenizer-trained model exhibits a\nmuch higher WER compared to the idiosyncratic coarse to fine models (labeled AR+NAR) developed\nfor these residual tokenizers \u2013 indicating that dMel lies on a simpler data manifold.\n\nGiven the success of our decoder-only dMel TTS model, dubbed RichTTS, we further evaluate it\non various datasets, including LJSpeech [10], VCTK [35], and LibriTTS [38], and compare it with\npopular TTS models, including Tacotron2 [28], FastSpeech2 [26], and VITS [16]. We conduct human\nevaluation to measure the naturalness of 50 randomly sampled synthesized speech from VCTK test\nset. The results are shown in Table 5. Our model achieves competitive performance on the TTS\ntask in terms of both MOS and WER, demonstrating its effectiveness in generating high-quality\nsynthesized speech. Interestingly, we find that VITS performs poorly on the VCTK WER. We suspect\nthat this is because VITS tends to make more mistakes at the beginning of each sequence, and since\nVCTK comprises short sequences, even one or two word errors can lead to a high WER."}, {"title": "LM-Style Speech-to-Text", "content": "Training an LM-style speech-to-text (ASR) model can test if the speech tokens can preserve the\nsemantic information in the speech signal and support the speech understanding task. Table 7 shows\nresults of our model RichASR trained with different tokenizations including dMel for the ASR task.\nOur decoder-only model with dMel speech tokenization achieves 4.2% WER on the test-clean and\n10.4% WER on the test-other sets outperforming both HuBERT-KM and SpeechTokenizer. We also\nobserve that our model with HuBERT-KM [19] outperforms the SpeechTokenizer [40] for ASR,\nwhich is reasonable as semantic tokens are more suitable for the ASR task. Additionally, our method\noutperforms VOXTLM [21], which is a larger model trained with more data and initialized from a\npretrained LLM (OPT) using HuBERT-KM (we use exactly the same tokenization with 200 clusters)\nas the speech tokenizer. The ASR results clearly demonstrate the benefit of using our speech tokenizer\nfor the speech understanding task, as it better preserves the semantic information in the speech signal.\nFurther details and ablations on data size and model size can be found in Appendix D and Appendix E\nTable 11."}, {"title": "Joint Speech-Text Modeling", "content": "Table 8: Results of ASR and TTS jointly trained model on LibriSpeech dataset.\n\nOur model design allows us to train a single model for both ASR and TTS tasks leading to a\nsimpler setup. We train a single model with the same architecture and tokenization as RichTTS, by\nconstructing the training data with <text, speech> and <speech, text> pairs for ASR and TTS tasks,\nrespectively. By mixing these two types of data, we can train a single model for both tasks.\n\nTable 8 shows that the joint model is worse on both tasks, but ASR is affected more than TTS.\nComparing our results to VOXTLM [21], which initializes its model from pretrained LLM (OPT)\nand finetunes it with multiple tasks and datasets, we speculate that our joint model needs text-only\ntraining to learn a good LM for better ASR performance. Our model structure trivially allows for this\ntext-only training, but we leave those experiments for future work."}, {"title": "Related Work", "content": "Speech Tokenization Speech tokenization is a critical step in speech processing tasks, enabling the\ndiscretization of continuous speech signals into tokens for language model-based processing. Existing\nspeech tokenization methods can be broadly categorized into two types: semantic tokens and acoustic\ntokens. Semantic tokens are extracted from self-supervised pretrained speech models, where the\nspeech signal is first encoded into speech representations and then clustered into semantic tokens with\nk-means clustering [2, 14]. Acoustic tokens are obtained from pretrained audio compression models,\nwhich compress the speech signal into codebook indices with residual vector quantization (RVQ)\nand reconstruction objectives [37, 11]. To combine the advantage of both semantic and acoustic\ntokens, AudioLM [7] proposed to modeling both semantic tokens and acoustic tokens with 3 stages:\nsemantic modeling, coarse acoustic modeling, and fine acoustic modeling. This solution covers\nboth the content and speech quality, but its multi-stage hierarchical structure complicates the model\nand slows the training and inference. AudioPalm [27] follows a similar approach, and show that\nthe scale of speech tokenizer's training data and model parameters are critical for this multi-stage\nmodeling. This observation indicates the speech compression model is far from lossless, and not\ngeneralizable to low-resource scenarios. Another solution is combining the semantic and acoustic\nfeature together, and formulate a new token that capture both the semantic and acoustic information.\nZhang et al. [40] proposed to distill the semantic tokens into the acoustic token's first residual channel\nduring the training of the RVQ model in a teacher-student manner. However, such combined methods\nare complex and require additional pretraining, complicating the model architecture and increasing\ncomputational overhead. Also, the model architecture is still not a single stage model, where only\nthe semantic modeling is skipped. In comparison, dMel is a train-free speech tokenization method\nthat discretizes mel-filterbanks directly into bins, inherently preserving both semantic and acoustic\ninformation in a unified representation."}, {"title": null, "content": "Speech-Text Modeling Modeling speech and text jointly is a challenging task, as speech signals\nare continuous and while text is discrete. Existing works have explored various approaches to\naddress this challenge, including usage of separate encoders for different modalities [1, 5]. Bai\net al. [3] proposed an encoder only model A3T for speech-text modeling, by introducing alignment\nembedding to encourage cross-modal transfer between text and speech. Although A3T achieved good\nperformance on speech synthesis and editing tasks, it cannot generate text and cannot generalize\nto longform generation because of its encoder-only architecture and mask-reconstruction training\nstrategy. VioLA [32] also targets a unified speech-text model which can generate speech and text with\na single model, but it is specifically designed for the Encodec [11] style feature, and compelled to\nmodel speech tokens in a multi-stage hierarchical manner. Maiti et al. [21] proposed a decoder-only\nlanguage model VOXTLM, to model speech and text jointly. However, VOXTLM is only models\nthe HuBERT semantic tokens, and relies on an external generation model to transform semantic\ntokens into waveform, but the speaker and acoustic information are lost. In comparison, the model\narchitecture in this paper is a simple, single stage decoder-only transformer language model, and can\nhandle both the speech generation and text generation tasks."}, {"title": "Conclusion", "content": "In this work, we proposed dMel, a novel train-free speech tokenization method that discretizes\nmel-filterbank energies directly into bins. By operating on the authentic mel-filterbank representation,\ndMel inherently preserves both semantic and acoustic information in a unified tokenized represen-\ntation. Our key contribution is the evaluation of dMel within a unified transformer decoder-only"}, {"title": null, "content": "architecture for speech recognition (ASR) and speech synthesis (TTS) tasks. Our dMel-based ASR\nmodel, RichASR, achieved the lowest word error rate among tokenization methods, robustly preserv-\ning semantic content. For TTS, dMel's generation yielded the lowest WER, accurately reconstructing\nspeech waveforms. Our dMel-based TTS model, RichTTS, achieved competitive naturalness, lowest\nerror rates, and long audio generation capabilities.\n\ndMel's simplicity circumvents separate tokenizers or multi-stage modeling, reducing computational\noverhead and dependence on pretrained models. By unifying semantic and acoustic modeling, dMel\nenables efficient speech-text modeling frameworks. While initial joint TTS-ASR training showed\npromise, further work is needed. Our primary contribution demonstrates dMel's effectiveness for\nhigh-performing separate TTS and ASR models within a unified decoder architecture."}, {"title": "Limitations", "content": "Because TTS work is tremendously fragmented and clear protocols are not often available for training\nand evaluation, we reimplemented other tokenizers within our code base using publicly available,\nofficial implementations where available. We trained our models on those tokenizations. While we\nmade the best effort to tune the tokenization methods and the models, there is always a possibility we\nmissed some details. However, our results seem to tell a consistent story when viewed from multiple\nangles, and when viewed on multiple datasets. For the joint model, we did not do extensive multi-task\ntraining using text only data and we intend to do that in the future work. We also did not train on\nlarger model sizes (>1B parameters), larger datasets (>1k hours), or using pretrained models."}, {"title": "Acknowledgements", "content": "We thank Dan Busbridge, Ronan Collobert and Jason Ramapuram for their helpful feedback and\ncritical discussions at all stages of the research; Rick Chang, David Grangier, Barry Theobald for\ntheir helpful feedback on the initial paper draft. Names are in alphabetical order by last name within\ngroup."}, {"title": "Ethics Statement", "content": "The development and deployment of speech technologies carry important ethical considerations.\nWhile our proposed dMel method aims to advance the state-of-the-art in speech-text modeling, it\nis crucial to highlight potential ethical risks and raise the awareness so that new methods may be\ndeveloped to mitigate these risks.\n\nOur first main concern is the potential dual-use of speech synthesis technologies for nefarious\npurposes such as impersonation, misleading audio-visual content generation, or voice spoofing\nattacks. Proactive measures, including watermarking techniques and robust speaker verification\nmethods, should be explored to counter such risks. The former attempts to build markers into the\ngenerated speech that make it easy to detect, while the latter focusses on distinguishing synthetic\nfrom real data. Prior work [20] has shown that neural networks can be trained to distinguish speech\nsynthesized from their model from real speech, probably because of artifacts from the use of mel\nspectral vocoders. While we did not train a network to do so in our work yet (we will create one before\ncode release), the vocoders we use are similar to their work \u2013 going from mel spectrogram to raw\nwaveforms. Our model also does not use prosody, phoneme duration and other predictions that more\nsophisticated TTS systems use to allow the model to perform very well on imitating speaker styles in\nzero-shot settings. However our model can probably mimic the styles of training speakers very well.\nIt is our hope that releasing our methods will facilitate more research on fake speech verification and\nwatermarking techniques \u2013 even if current classifiers are able to perform this detection, the quality\nof the generative models is improving. It is also our hope that future works will attempt to perform\nmore credit assignment \u2013 by providing metrics that show which real data samples a synthetic speech\nexample copies its style and substance from.\n\nAnother concern is the perpetuation of societal biases encoded within training data. Speech datasets\nmay exhibit biases along dimensions such as gender, race, age, or socioeconomic status, which could\nbe propagated or amplified by trained models. Rigorous debiasing techniques and careful curation\nof representative training data are essential to mitigate these risks. On the mitigating side of this\nequation, we also hope that with better, more controllable TTS systems, ASR systems can improve\nbecause more data can be generated for underrepresented segments of the distribution from the TTS\nmodels.\n\nFurthermore, the development and deployment of speech technologies should prioritize accessibility\nand inclusivity. Models should be evaluated for performance across diverse demographics, accents,\nand language varieties to ensure equitable access and quality of service.\n\nFinally, it is important to foster transparency and accountability in the research and development\nprocess. Clear documentation of model capabilities, limitations, and potential failure modes should\nbe provided to enable informed decision-making and responsible usage.\n\nAddressing these ethical considerations requires a multistakeholder approach involving researchers,\ndevelopers, policymakers, and end-users. By prioritizing ethical principles such as fairness, privacy,\nand accountability, we can work towards realizing the benefits of speech technologies while mitigating\npotential risks and adverse societal impacts."}, {"title": "Data, Code, Reproducibility", "content": "We made the best effort to use publicly available data and official implementations of prior works\nwhere it is possible. All data we used are under permissive license for research. We provided as\nmuch as detail as is possible without code such as details on our model training and hyperparameters\nthroughout the paper and in the Appendix. We plan to open-source our code upon paper acceptance.\n\nWe do not plan to open-source any pre-trained models for sake of privacy, safety and misuse."}, {"title": "Subjective Evaluation for TTS", "content": "We use crowd-sourcing to collect subjective ratings to compare the naturalness of the reconstructed\nspeech from the different tokenizers. We evaluate the quality of the same (randomly sampled) 50\nutterances for each model by collecting around seven ratings per sample. Overall, we collect 3500\nratings from 65 raters. The raters were English-speaking and were paid at least the minimum wage."}, {"title": "Training Details", "content": "For our decoder-only model we stack together speaker embedding, speech tokens and text tokens.\nBoth speech and text tokens have prepended begin of sentence token () and appended end of\nsentence token ().\n\nWe train all models using the Adam optimizer with a learning rate of 1e-3, learning rate warmup of 4k\nsteps for ASR and 5k for TTS, cosine learning rate schedule and gradient clipping of 1.0 for TTS and\n0.1 for ASR and joint models. We use dynamic batching to optimize the data packing with total batch\nsize of 1.4h/1.4h/0.7h for ASR training and 1h/2h/2h for TTS training for Small/Base/Large models.\nWe train TTS models for 600k steps and ASR models 80k steps with mixed precision training and\nBF16 on A100 and H100 GPUs with 80GB. Both ASR models and TTS models are trained with\n8GPUs for less than a day and for 2-4 days for ASR and TTS respectively."}, {"title": "LM-Style Speech-to-Text", "content": "For ASR training as an augmentation we apply SpecAugment [23] with 2 frequency masks with\nmax width 30 and 10 time masks with max width 50 and ratio 0.1. With ablations we found that\nSpecAugment masking with average value instead of zero is slightly better. Without applying\nSpecAugment performance of ASR is 7.3% WER on dev-clean and 20.3% WER on dev-other, which\nis further can be improved with usage of frequency masking only to 6.4% WER on dev-clean and\n16.6% WER on dev-other. Usage of both frequency masking and time masking results in the best\nperformance of Table 7.\n\nDuring experiments with ASR decoder-only models and dMel tokenization we observed some model\ntraining instabilities resulting in the spikes in the gradient norm and thus sometimes spikes in training\nloss. To stabilize training i) we reduce the gradient clipping from 1.0 used for TTS training to 0.1; ii)\nwe add queries and keys normalizations via LayerNorm [12] on the head dimension before computing\nattention matrix.\n\nWe found that span masking is key part of model training to enforce slef-attention to attend to speech\npart as well as to reduce exposure bias. The masking strategy is similar to the one used for TTS\ntraining: for every training step with probability p the sample in the minibatch is masked with the\nmean span of 3 tokens with masking ration of 0.5. We found that the mean span of 1 token or 5\ntokens gives the same results; while the mask probability p is the most important hyper-parameter.\nThe optimimal value for ASR is found to be 0.8, which is used in all final models."}, {"title": "LM-Style Text-to-Speech", "content": "Scaling results for RichTTS are shown in Table 10."}, {"title": "LM-Style Speech-to-Text", "content": "Table 11: Our ASR models trained on different subsets (train-clean-100 LS-100, train-clean-360\nLS-360, full LibriSpeech LS-960) of LibriSpeech, with different model size and different speech\ntokenization (greedy decoding is reported). Results are shown across 2 runs with mean WER and\nstandard deviation."}]}