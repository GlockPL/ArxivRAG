{"title": "dMel: Speech Tokenization made Simple", "authors": ["He Bai", "Tatiana Likhomanenko", "Ruixiang Zhang", "Zijin Gu", "Zakaria Aldeneh", "Navdeep Jaitly"], "abstract": "Large language models have revolutionized natural language processing by lever- aging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated complicated speech tokenization methods to dis- cretize continuous speech signals so that language modeling techniques can be applied to speech data. However, existing approaches either model semantic to- kens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic information. Having multiple token types also complicates the architecture and requires additional pretraining. Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representa- tion (dMel), that performs better than other existing speech tokenization methods. Using a transformer decoder-only architecture for speech-text modeling, we com- prehensively evaluate different speech tokenization methods on speech recognition (ASR) and speech synthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text. Our code will be published soon.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have achieved remarkable success in various natural language process- ing tasks by leveraging self-supervised pretraining on massive amounts of textual data [8]. Inspired by this success, numerous works [7, 27, 39, 31] have sought to extend the language modeling approach to speech processing, aiming to build unified models capable of both speech understanding and gen- eration tasks. However, a key challenge lies in the continuous nature of speech signals, necessitating effective tokenization methods to discretize the input for language model-based processing.\nCurrent speech tokenization approaches can be broadly categorized into two types: semantic tokens and acoustic tokens. Semantic tokens, extracted from self-supervised (SSL) pretrained speech models [2, 14], where the speech signal is first encoded into speech representations and then clustered into semantic tokens with k-means method. However, such SSL pretrained models are not useful for high fidelity speech synthesis as speaker identity and other details of raw speech are lost in training [7]. Conversely, acoustic tokens can be obtained from audio compression models that are trained to compress the speech signal into codebook indices with residual vector quantization (RVQ) and reconstruction objectives [37, 11]. These tokens prioritize acoustic reconstruction but lose semantic information which can lead to poorer results in generating audio [31].\nTo combine the advantages of both semantic and acoustic tokens, AudioLM [7] proposed to model both semantic tokens and acoustic tokens with 3 stages: semantic modeling, coarse acoustic modeling,"}, {"title": "Method", "content": "In this section, we first introduce our proposed dMel speech tokenization method, which discretizes mel-filterbanks energies directly into bins. We then describe our unified transformer decoder-only model for ASR and TTS tasks, which leverages dMel for speech tokenization. The model architecture is illustrated in Figure 2."}, {"title": "dMel Speech Tokenizer", "content": "Different from existing VQ-VAE [7, 40, 17, 37] based speech tokenizers, we propose a discretized mel-filterbanks based speech tokenizer. The outline of the discretization method is shown in Figure 1. Later in the paper, we show that this tokenizer allows the model to process the input speech signal efficiently and capture the relevant acoustic features for both ASR and TTS tasks.\nWe denote tensors as X while $X_{i,...}$ denote the (i, ...)-th component of tensor X. First, the speech tokenizer takes the input speech signal x and computes the mel-filterbanks representation M:\n$M = Mel(x),$ (1)\nwhere Mel(\u00b7) represents the function that computes the mel-filterbanks, $M \\in R^{T \\times N}$, N is the number of filterbanks and T is the number of frames in the spectrogram.\nTokenization To discretize the mel-filterbanks representation M into speech tokens, we adopt a codebook C. In this paper, we apply a simple linear discretization, so that the codebook $C \\in R^{2K}$ and its values are evenly spaced in the range of the mel-filterbanks values:\n$m = min(M_{t,i}),  M = max(M_{t,i})  \u03b4 = \\frac{M - m}{2K},$ (2)\n$C = [m, m + \u03b4, m + 2\u03b4, .., m + (2^K \u2212 1)\u03b4].$ (3)\nIn practice, we compute the minimum m and maximum M values of mel-filterbanks across the entire dataset to define the codebook C. Then we map a magnitude $M_{t,i}$ of every frequency channel i = 1 ... N for the time frame t = 1 . . . T into a bin index of the codebook C in the following way:\n$S_{t,i} = Discretize(M_{t,i}) = argmin_j|M_{t,i} \u2013 C_j|$ (4)\nwhere $S \\in B^{T \\times N}$ represents the discretized mel-filterbanks (dMel) with $B = {j|j = 1, 2, 3, . . .2^K }$ and $S_t \\in B^N$ being the t-th speech token. As the codebook C has $2^K$ distinct values and thus number of bins |B| = $2^K$, each speech token is represented by N \u00b7 K bits where every K bits are used to represent one of N frequency channels.\nDetokenization To reconstruct the speech signal x from the speech tokens S, we first transform bin indices back to the mel-filterbanks representation via the codebook C:\n$M_{t,i} = C_{st,i}$ (5)\nThen, we apply a vocoder [36] to transform reconstructed mel-filterbanks $M_{t,i}$ back into the time domain signal x. The vocoder is trained independently and is not part of the transformer decoder- based model."}, {"title": "Unified Speech-Text Transformer Decoder", "content": "Modeling speech and text sequences jointly is essential for a model to understand and generate both modalities. However, it is challenging to design a unified model that can handle both speech-to-text and text-to-speech effectively. In this work, we apply a unified transformer decoder-only model that takes speech and text tokens as input and generates the output tokens in the target sequence. The model is trained in end-to-end on a combined dataset of speech and text pairs, enabling it to learn the joint representations for ASR and TTS tasks. As we show in the rest of the paper, the crucial part for the joint model training is the proper speech tokenization which dMel provides.\nToken Representation For text data, we apply a character-level tokenizer to convert the input text into a sequence of text tokens. The text tokens are passed through an embedding layer, Embed() :\n{j|j = 1,2,3... L} \u2192 $R^D$, where D is the embedding dimension and L is the vocabulary size. The dimension of the speech token embedding is set to be the same as the text token embedding D and no further mapping is required. The motivation for using a character-level tokenizer is to reduce the vocabulary size L and improve the model's generalization ability. Also, character tokens can capture the fine-grained linguistic features that are essential for both ASR and TTS tasks.\nFor speech signal, we apply the dMel speech tokenizer to convert the input speech signal into a sequence of speech tokens. Then, the speech tokens $S \\in B^{T \\times N}$ are passed through a learnable embedding layer, Embed(\u00b7) : B \u2192 $R^d$, and a learnable linear layer, Linear(\u00b7) : $R^{N \\times d}$ \u2192 $R^D$, to obtain the speech token representation $E \\in R^{T \\times D}$:\n$E'_t = Concatenate([Embed(S_{t,1}), Embed(S_{t,2}), . . ., Embed(S_{t,N})])$ (6)\n$E_t = Linear(E'_t),$ (7)\nwhere $E_t \\in R^D$ is the speech token representation. Here, for every time frame t a speech token $S_t$ is processed in parallel and independently for every frequency channel i by Embed($S_{t,i}$) mapping, and then embeddings of all frequency channels are stacked together to form one vector representation $E'_t$ for the frame t. Finally, the speech token embeddings $E_t$ are fed into the decoder-only transformer models for further processing.\nWe also implemented other popular speech tokenizers including HuBERT-KM [19] and SpeechTok- enizer [40] for comparison. The main difference among these speech tokenizers is the codebook size and codes dimension, which are shown in Table 2. For both HuBERT-KM and SpeechTokenizer the speech tokens are mapped via a learnable linear layer from their dimension to the text embedding dimension D before feeding into the transformer decoder-only model."}, {"title": "Robust Training", "content": "Compared to language modeling, audio frames are highly redundant with strong local correlations. This makes long form generation difficult for models due to exposure bias [6]. To mitigate exposure bias during training, we apply span-masking [25] to the speech token context, masking out multiple random spans of speech frames. The model is trained to predict the next token based on the masked context. This context-masking strategy helps the model learn to generate accurate speech tokens in the presence of missing information, improving its robustness and generalization. It forces the model to attend to the text rather than copying previously inferred speech tokens due to learnt correlations. We also find that span-masking text tokens improves the ASR task."}, {"title": "Experiments", "content": "In this section, we conduct ASR and TTS experiments using dMel tokens with a unified transformer decoder-only model. We evaluate the performance of our model mainly on the LibriSpeech dataset and compare it with state-of-the-art speech tokenization methods, ASR and TTS models."}, {"title": "Training Data", "content": "We use several open-sourced datasets with paired speech and text transcription to conduct experi- ments: i) LibriSpeech [22] dataset (CC BY 4.0) consists of English speech recordings (960h, 16kHz) from various speakers (~2k) and conditions; ii) LibriTTS [38] (CC BY 4.0) dataset derived from Lib- riSpeech improves over it with the proper sentence split, text normalization and keeping 24kHz. It has around 500h; iii) VCTK [34] contains 44 hours of English speech (108 speakers); iv) LJSpeech [15] dataset (public domain in USA) is a single speaker English audio recordings of 16kHz with read"}, {"title": "Training Configuration", "content": "We train decoder-only transformers in three different sizes: Small, Base, and Large (see Appendix Table 9). By default, the Base model is used in all experiments if not stated otherwise. All models use pre-LayerNorm with dropout set to 0.1 for residual, attention and embedding and 0.3 for positional embedding. dMel uses 16 discrete bins for each channel while text is tokenized with a character vocabulary; the speaker embedding dvector has 512 dimensions(see Appendx D for details)."}, {"title": "Main Results", "content": "In this section, we first evaluate different tokenization methods on speech reconstruction. Then we report the TTS and ASR results using an LM-style (decoder-only) model with different speech tokens."}, {"title": "Speech Reconstruction", "content": "Following [40], we randomly sample 300 speech utterances and their ground truth transcriptions from the LibriSpeech test-clean dataset. We use the speech2unit and unit2speech modules to convert the speech signal to speech tokens and then reconstruct the speech signal from the speech tokens. We compute the WER between the ASR outputs from HuBERT-Large [14] on the audio samples and their ground truth transcripts. We also report MOS-LQO (Mean Opinion Score - Listening Quality Objective) score to measure the reconstruction quality using ViSQOL [13]. Finally, use human evaluation to measure the naturalness of the reconstructed speech using a MOS score. We instruct the human evaluators to rate the naturalness of the reconstructed speech on a scale of 1 to 5, where 1 is the worst and 5 is the best. The results are shown in Table 3.\nFrom Table 3, we can see that semantic tokenization (HuBERT-KM) is not good for speech re- construction. Meanwhile, acoustic tokenizers that are optimized to reconstruct the signal directly (EnCodec and SpeechTokenizer) do well.\nWe apply different vocoders to reconstruct the speech signal from mel-filterbanks, and find that the WER of the reconstructed speech signal is comparable to the acoustic tokenization methods with a fraction of the parameters. Also, mel-filterbanks achieve a better MOS-LQO score, which indicates that the reconstructed audio is more similar to the original audio. By comparing Mel and dMel, we can see that discretization has little impact on WER and MOS-LQO scores. We also find that the exact vocoder matters much less than the frame rate of tokenization: the WER goes from 2.08 to 2.13 when switching from HifiGAN to ParallelWaveGAN, but it falls from 2.13 to 2.36 when the frame"}, {"title": "LM-Style Text-to-Speech", "content": "Here we compare the accuracy and naturalness of speech synthesized by LM-style text-to- speech (TTS) models trained on different tokenization methods. For TTS evaluation, we utilize WhisperX [4] (\u201cbase.en\u201d from [24]) to transcribe our generated speech into text and calculate the WER and the character error rate (CER). We report both WER and CER to facilitate comparisons to prior works which have reported only one or the other.\nWe trained the TTS model using the same architecture but with three different tokenization methods: HuBERT+KM (with 200 clusters), SpeechTokenizer, and dMel. Additionally, we present the results from VOXTLM [21] and USLM [40] for comparison. VOXTLM is a larger model trained on more data that is initialized from a pretrained LLM (OPT) using HuBERT-KM as the speech tokenizer. USLM comprises an autoregressive (AR) model and a non-autoregressive (NAR) model, both trained with the SpeechTokenizer.\nAs shown in Table 4 for training on LibriSpeech dataset, our decoder-only model with dMel tokeniza- tion achieves a WER of 4.3 and a CER of 1.8, significantly outperforming the baseline methods. This indicates that our model can generate more accurate speech with less hallucination and distortion. Furthermore, we observed that the AR model trained on SpeechTokenizer-trained model exhibits a much higher WER compared to the idiosyncratic coarse to fine models (labeled AR+NAR) developed for these residual tokenizers \u2013 indicating that dMel lies on a simpler data manifold.\nGiven the success of our decoder-only dMel TTS model, dubbed RichTTS, we further evaluate it on various datasets, including LJSpeech [10], VCTK [35], and LibriTTS [38], and compare it with popular TTS models, including Tacotron2 [28], FastSpeech2 [26], and VITS [16]. We conduct human evaluation to measure the naturalness of 50 randomly sampled synthesized speech from VCTK test set. The results are shown in Table 5. Our model achieves competitive performance on the TTS task in terms of both MOS and WER, demonstrating its effectiveness in generating high-quality synthesized speech. Interestingly, we find that VITS performs poorly on the VCTK WER. We suspect that this is because VITS tends to make more mistakes at the beginning of each sequence, and since VCTK comprises short sequences, even one or two word errors can lead to a high WER."}, {"title": "LM-Style Speech-to-Text", "content": "Training an LM-style speech-to-text (ASR) model can test if the speech tokens can preserve the semantic information in the speech signal and support the speech understanding task. Table 7 shows results of our model RichASR trained with different tokenizations including dMel for the ASR task. Our decoder-only model with dMel speech tokenization achieves 4.2% WER on the test-clean and 10.4% WER on the test-other sets outperforming both HuBERT-KM and SpeechTokenizer. We also observe that our model with HuBERT-KM [19] outperforms the SpeechTokenizer [40] for ASR, which is reasonable as semantic tokens are more suitable for the ASR task. Additionally, our method outperforms VOXTLM [21], which is a larger model trained with more data and initialized from a pretrained LLM (OPT) using HuBERT-KM (we use exactly the same tokenization with 200 clusters) as the speech tokenizer. The ASR results clearly demonstrate the benefit of using our speech tokenizer for the speech understanding task, as it better preserves the semantic information in the speech signal."}, {"title": "Joint Speech-Text Modeling", "content": "Our model design allows us to train a single model for both ASR and TTS tasks leading to a simpler setup. We train a single model with the same architecture and tokenization as RichTTS, by constructing the training data with <text, speech> and <speech, text> pairs for ASR and TTS tasks, respectively. By mixing these two types of data, we can train a single model for both tasks.\nTable 8 shows that the joint model is worse on both tasks, but ASR is affected more than TTS. Comparing our results to VOXTLM [21], which initializes its model from pretrained LLM (OPT) and finetunes it with multiple tasks and datasets, we speculate that our joint model needs text-only training to learn a good LM for better ASR performance. Our model structure trivially allows for this text-only training, but we leave those experiments for future work."}, {"title": "Related Work", "content": "Speech Tokenization Speech tokenization is a critical step in speech processing tasks, enabling the discretization of continuous speech signals into tokens for language model-based processing. Existing speech tokenization methods can be broadly categorized into two types: semantic tokens and acoustic tokens. Semantic tokens are extracted from self-supervised pretrained speech models, where the speech signal is first encoded into speech representations and then clustered into semantic tokens with k-means clustering [2, 14]. Acoustic tokens are obtained from pretrained audio compression models, which compress the speech signal into codebook indices with residual vector quantization (RVQ) and reconstruction objectives [37, 11]. To combine the advantage of both semantic and acoustic tokens, AudioLM [7] proposed to modeling both semantic tokens and acoustic tokens with 3 stages: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. This solution covers both the content and speech quality, but its multi-stage hierarchical structure complicates the model and slows the training and inference. AudioPalm [27] follows a similar approach, and show that the scale of speech tokenizer's training data and model parameters are critical for this multi-stage modeling. This observation indicates the speech compression model is far from lossless, and not generalizable to low-resource scenarios. Another solution is combining the semantic and acoustic feature together, and formulate a new token that capture both the semantic and acoustic information. Zhang et al. [40] proposed to distill the semantic tokens into the acoustic token's first residual channel during the training of the RVQ model in a teacher-student manner. However, such combined methods are complex and require additional pretraining, complicating the model architecture and increasing computational overhead. Also, the model architecture is still not a single stage model, where only the semantic modeling is skipped. In comparison, dMel is a train-free speech tokenization method that discretizes mel-filterbanks directly into bins, inherently preserving both semantic and acoustic information in a unified representation.\nSpeech-Text Modeling Modeling speech and text jointly is a challenging task, as speech signals are continuous and while text is discrete. Existing works have explored various approaches to address this challenge, including usage of separate encoders for different modalities [1, 5]. Bai et al. [3] proposed an encoder only model A3T for speech-text modeling, by introducing alignment embedding to encourage cross-modal transfer between text and speech. Although A3T achieved good performance on speech synthesis and editing tasks, it cannot generate text and cannot generalize to longform generation because of its encoder-only architecture and mask-reconstruction training strategy. VioLA [32] also targets a unified speech-text model which can generate speech and text with a single model, but it is specifically designed for the Encodec [11] style feature, and compelled to model speech tokens in a multi-stage hierarchical manner. Maiti et al. [21] proposed a decoder-only language model VOXTLM, to model speech and text jointly. However, VOXTLM is only models the HuBERT semantic tokens, and relies on an external generation model to transform semantic tokens into waveform, but the speaker and acoustic information are lost. In comparison, the model architecture in this paper is a simple, single stage decoder-only transformer language model, and can handle both the speech generation and text generation tasks."}, {"title": "Conclusion", "content": "In this work, we proposed dMel, a novel train-free speech tokenization method that discretizes mel-filterbank energies directly into bins. By operating on the authentic mel-filterbank representation, dMel inherently preserves both semantic and acoustic information in a unified tokenized represen- tation. Our key contribution is the evaluation of dMel within a unified transformer decoder-only"}, {"title": "Limitations", "content": "Because TTS work is tremendously fragmented and clear protocols are not often available for training and evaluation, we reimplemented other tokenizers within our code base using publicly available, official implementations where available. We trained our models on those tokenizations. While we made the best effort to tune the tokenization methods and the models, there is always a possibility we missed some details. However, our results seem to tell a consistent story when viewed from multiple angles, and when viewed on multiple datasets. For the joint model, we did not do extensive multi-task training using text only data and we intend to do that in the future work. We also did not train on larger model sizes (>1B parameters), larger datasets (>1k hours), or using pretrained models."}, {"title": "Ethics Statement", "content": "The development and deployment of speech technologies carry important ethical considerations. While our proposed dMel method aims to advance the state-of-the-art in speech-text modeling, it is crucial to highlight potential ethical risks and raise the awareness so that new methods may be developed to mitigate these risks.\nOur first main concern is the potential dual-use of speech synthesis technologies for nefarious purposes such as impersonation, misleading audio-visual content generation, or voice spoofing attacks. Proactive measures, including watermarking techniques and robust speaker verification methods, should be explored to counter such risks. The former attempts to build markers into the generated speech that make it easy to detect, while the latter focusses on distinguishing synthetic from real data. Prior work [20] has shown that neural networks can be trained to distinguish speech synthesized from their model from real speech, probably because of artifacts from the use of mel spectral vocoders. While we did not train a network to do so in our work yet (we will create one before code release), the vocoders we use are similar to their work \u2013 going from mel spectrogram to raw waveforms. Our model also does not use prosody, phoneme duration and other predictions that more sophisticated TTS systems use to allow the model to perform very well on imitating speaker styles in zero-shot settings. However our model can probably mimic the styles of training speakers very well. It is our hope that releasing our methods will facilitate more research on fake speech verification and watermarking techniques \u2013 even if current classifiers are able to perform this detection, the quality of the generative models is improving. It is also our hope that future works will attempt to perform more credit assignment \u2013 by providing metrics that show which real data samples a synthetic speech example copies its style and substance from.\nAnother concern is the perpetuation of societal biases encoded within training data. Speech datasets may exhibit biases along dimensions such as gender, race, age, or socioeconomic status, which could be propagated or amplified by trained models. Rigorous debiasing techniques and careful curation of representative training data are essential to mitigate these risks. On the mitigating side of this equation, we also hope that with better, more controllable TTS systems, ASR systems can improve because more data can be generated for underrepresented segments of the distribution from the TTS models.\nFurthermore, the development and deployment of speech technologies should prioritize accessibility and inclusivity. Models should be evaluated for performance across diverse demographics, accents, and language varieties to ensure equitable access and quality of service.\nFinally, it is important to foster transparency and accountability in the research and development process. Clear documentation of model capabilities, limitations, and potential failure modes should be provided to enable informed decision-making and responsible usage.\nAddressing these ethical considerations requires a multistakeholder approach involving researchers, developers, policymakers, and end-users. By prioritizing ethical principles such as fairness, privacy, and accountability, we can work towards realizing the benefits of speech technologies while mitigating potential risks and adverse societal impacts."}, {"title": "Data, Code, Reproducibility", "content": "We made the best effort to use publicly available data and official implementations of prior works where it is possible. All data we used are under permissive license for research. We provided as much as detail as is possible without code such as details on our model training and hyperparameters throughout the paper and in the Appendix. We plan to open-source our code upon paper acceptance.\nWe do not plan to open-source any pre-trained models for sake of privacy, safety and misuse."}, {"title": "Subjective Evaluation for TTS", "content": "We use crowd-sourcing to collect subjective ratings to compare the naturalness of the reconstructed speech from the different tokenizers. We evaluate the quality of the same (randomly sampled) 50 utterances for each model by collecting around seven ratings per sample. Overall, we collect 3500 ratings from 65 raters. The raters were English-speaking and were paid at least the minimum wage."}, {"title": "Training Details", "content": "For our decoder-only model we stack together speaker embedding, speech tokens and text tokens. Both speech and text tokens have prepended begin of sentence token () and appended end of sentence token ().\nWe train all models using the Adam optimizer with a learning rate of 1e-3, learning rate warmup of 4k steps for ASR and 5k for TTS, cosine learning rate schedule and gradient clipping of 1.0 for TTS and 0.1 for ASR and joint models. We use dynamic batching to optimize the data packing with total batch size of 1.4h/1.4h/0.7h for ASR training and 1h/2h/2h for TTS training for Small/Base/Large models. We train TTS models for 600k steps and ASR models 80k steps with mixed precision training and BF16 on A100 and H100 GPUs with 80GB. Both ASR models and TTS models are trained with 8GPUs for less than a day and for 2-4 days for ASR and TTS respectively."}, {"title": "LM-Style Speech-to-Text", "content": "For ASR training as an augmentation we apply SpecAugment [23] with 2 frequency masks with max width 30 and 10 time masks with max width 50 and ratio 0.1. With ablations we found that SpecAugment masking with average value instead of zero is slightly better. Without applying SpecAugment performance of ASR is 7.3% WER on dev-clean and 20.3% WER on dev-other, which is further can be improved with usage of frequency masking only to 6.4% WER on dev-clean and 16.6% WER on dev-other. Usage of both frequency masking and time masking results in the best performance of Table 7.\nDuring experiments with ASR decoder-only models and dMel tokenization we observed some model training instabilities resulting in the spikes in the gradient norm and thus sometimes spikes in training loss. To stabilize training i) we reduce the gradient clipping from 1.0 used for TTS training to 0.1; ii) we add queries and keys normalizations via LayerNorm [12] on the head dimension before computing attention matrix.\nWe found that span masking is key part of model training to enforce slef-attention to attend to speech part as well as to reduce exposure bias. The masking strategy is similar to the one used for TTS training: for every training step with probability p the sample in the minibatch is masked with the mean span of 3 tokens with masking ration of 0.5. We found that the mean span of 1 token or 5 tokens gives the same results; while the mask probability p is the most important hyper-parameter. The optimimal value for ASR is found to be 0.8, which is used in all final models."}, {"title": "LM-Style Text-to-Speech", "content": "Scaling results for RichTTS are shown in Table 10."}, {"title": "LM-Style Speech-to-Text", "content": ""}]}