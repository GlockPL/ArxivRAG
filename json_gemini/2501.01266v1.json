{"title": "PIMAEX: Multi-Agent Exploration through Peer Incentivization", "authors": ["Michael K\u00f6lle", "Johannes Tochtermann", "Julian Sch\u00f6nberger", "Gerhard Stenzel", "Philipp Altmann", "Claudia Linnhoff-Popien"], "abstract": "While exploration in single-agent reinforcement learning has been studied extensively in recent years, considerably less work has focused on its counterpart in multi-agent reinforcement learning. To address this issue, this work proposes a peer-incentivized reward function inspired by previous research on intrinsic curiosity and influence-based rewards. The PIMAEX reward, short for Peer-Incentivized Multi-Agent Exploration, aims to improve exploration in the multi-agent setting by encouraging agents to exert influence over each other to increase the likelihood of encountering novel states. We evaluate the PIMAEX reward in conjunction with PIMAEX-Communication, a multi-agent training algorithm that employs a communication channel for agents to influence one another. The evaluation is conducted in the Consume/Explore environment, a partially observable environment with deceptive rewards, specifically designed to challenge the exploration vs. exploitation dilemma and the credit-assignment problem. The results empirically demonstrate that agents using the PIMAEX reward with PIMAEX-Communication outperform those that do not.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the main challenges in Reinforcement Learning (RL) is the exploration vs. exploitation dilemma; that is, an RL agent must find a suitable trade-off between exploratory and exploitative behavior to avoid getting stuck in local optima. This is especially important for hard exploration problems, which often exhibit sparse or deceptive rewards; that is, rewards may occur rarely or be misleading. This issue is also related to another important problem in RL, the credit-assignment problem: if a long sequence of actions without any direct reward must be taken to eventually obtain a reward, RL agents might fail to assign credit to those non-rewarding actions that are temporally distant from the eventual reward.\nIn such problems, naive approaches based purely on random exploration, such as e-greedy policies, often fail to learn successful policies. Consequently, many approaches have been proposed to tackle these challenges. While much work in single-agent RL has focused on intrinsic curiosity rewards and novelty of encountered states to aid exploration, there is considerably less literature aimed specifically at multi-agent exploration. This is likely because the state space of multi-agent RL (MARL) systems grows exponentially with the number of agents, making exploration in this setting a much harder problem than in single-agent RL. Contrary to the relative lack of work addressing the multi-agent exploration problem, however, there is an abundance of research dedicated to improving coordination and cooperation in MARL. One specific direction of this work is the field of influence-based rewards, wherein agents are rewarded for the influence they have over other agents.\nInspired by previous work in intrinsic curiosity and influence-based rewards, this work proposes a peer-incentivization scheme, in which an agent rewards its peers for influencing it to discover novel states. Accordingly, the main contribution of this work is the formulation of a multi-agent social influence peer reward function, the PIMAEX reward, aimed at improving exploration in multi-agent settings with sparse or deceptive rewards. Additionally, a multi-agent Reinforcement Learning algorithm employing this reward, PIMAEX-Communication, is introduced. The PIMAEX reward is a specific instance of a generalized multi-agent social influence peer reward function, also introduced in this work, comprising three terms \u03b1, \u03b2, and y. The a term is essentially the influence reward introduced by (Jaques et al., 2018), while the y term is comparable to that in (Wang et al., 2019) and was part of the proposed future work in (Jaques et al., 2018). Therefore, the"}, {"title": "2 RELATED WORK", "content": "The PIMAEX reward function is inspired by two main areas in reinforcement learning: intrinsic curiosity rewards, which encourage exploration, and influence-based rewards, where agents receive rewards based on the impact of their actions on peers. This section explores these areas, with intrinsic curiosity rewards in Section 2.1 and influence-based rewards in Section 2.3. Influence-based rewards naturally apply to multi-agent reinforcement learning, where they assist with coordination and cooperation. An overview of the related cooperative MARL approaches is given in Section 2.2, focusing on methods most relevant to the mechanisms proposed in this work."}, {"title": "2.1 Intrinsic Motivation: Curiosity", "content": "Intrinsic curiosity rewards are widely used to drive exploration, particularly in challenging environments. These rewards supplement or sometimes replace the environment's reward by incentivizing agents to seek novelty. Two common methods are count-based exploration and prediction-error exploration. Count-based methods compute novelty by counting state visits, e.g., by giving a reward proportional to $\\frac{1}{\\sqrt{N(s)}}$ .\nHowever, this approach is feasible only in small state spaces and relies on approximations, such as density models or hash functions, in larger spaces (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2016).\nPrediction-error methods, introduced by Schmidhuber (Schmidhuber, 1991), reward agents based on the error of a learned model predicting future states. High prediction errors signify novel states, making this method effective for exploration. Variants of this approach use forward dynamics models to predict next states (Oudeyer et al., 2007; Stadie et al., 2015) or inverse dynamics models to avoid uncontrollable environmental factors (Pathak et al., 2017). To overcome issues like the \"noisy TV problem\" (Burda et al., 2018), where agents get attracted to random, high-error stimuli, Burda et al. propose Random Network Distillation (RND) (Burda et al., 2018). In RND, a randomly initialized neural network serves as the target for a second network to predict, with prediction errors used as curiosity rewards. This method is computationally light, but requires observation and reward normalization to avoid inconsistencies (Burda et al., 2018)."}, {"title": "2.2 Multi-Agent Cooperation and Coordination", "content": "Influence-based rewards are part of broader MARL approaches that promote agent cooperation. Many methods leverage centralized training and decentralized execution (CTDE), sharing Q-networks across agents and decomposing centralized Q-functions (Fu et al., 2022; Foerster et al., 2017; Rashid et al., 2018). Communication between agents is also common for improved coordination (Peng et al., 2017; Sukhbaatar et al., 2016). Another approach involves counterfactual reasoning to determine individual agent contributions in the absence of explicit individual rewards, as in (Foerster et al., 2017). Peer incentivization, where agents can reward or penalize others, is a relevant direction (Yang et al., 2020; Schmid et al., 2021)."}, {"title": "2.3 Social Influence", "content": "In settings where agents maximize their own rewards, social influence can encourage collaboration without a central reward. Jaques et al. (Jaques et al., 2018) propose rewarding agents based on the influence they exert on other agents' policies, measured via counterfactual reasoning. They evaluate influence by conditioning one agent's policy on another's actions and comparing it with a counterfactual scenario where the influence is removed. This discrepancy quantifies influence and encourages coordination by maximizing mutual information.\nIn Jaques et al.'s experiments, agents either influence others through discrete message communication or use models to predict others' actions. In the latter, agents employ a Model of Other Agents (MOA) to relax the need for centralized training. They note that social influence reduces policy gradient variance, which can increase with the number of agents (Lowe et al., 2017)."}, {"title": "Influence-Based Multi-Agent Exploration", "content": "Wang et al. (Wang et al., 2019) address limitations in single-agent curiosity by proposing two approaches: exploration via information-theoretic in-"}, {"title": "3 PEER-INCENTIVIZED\nMULTI-AGENT EXPLORATION", "content": "Similar to other aspects that distinguish single-agent from multi-agent reinforcement learning, exploration in the multi-agent setting is significantly more challenging than in the single-agent case. This increased difficulty arises because the joint state space of a multi-agent system grows exponentially with the number of agents. Additionally, since state transition probabilities depend on the joint actions of all agents, it is highly unlikely that a single agent can explore a significant portion of the state space alone. Therefore, in many settings, especially cooperative ones, meaningful multi-agent exploration usually requires coordination among agents. While substantial previous work exists on improving exploration in the single-agent setting and enhancing coordination in multi-agent settings, relatively little work specifically addresses multi-agent exploration. An overview of this and related prior work is provided in Section 2.\nBuilding upon prior work, particularly in intrinsic curiosity and influence-based rewards, we introduce the Peer Incentivized Multi-Agent Exploration (PIMAEX) reward function in Section 3.1 as a proposed method to improve multi-agent exploration. The PIMAEX reward function rewards an agent for influencing other agents so that they are more likely to transition to novel or rarely visited states. Moreover, it is a specific instance of a generalized multi-agent social influence reward function, also introduced in Section 3.1, which is based on the influence agents exert over each other. This social influence reward function is formulated generally enough to encompass similar influence-based reward functions from prior work, such as (Jaques et al., 2018) and (Wang et al., 2019), to incorporate various influence measures, and to allow different mechanisms through which agents can influence each other, including communication channels.\nTo demonstrate practical application of these reward functions, Section 4 introduces PIMAEX-Communication, a multi-agent training algorithm inspired by (Jaques et al., 2018). PIMAEX-Communication employs a communication channel to allow agents to send messages to each other and thus to exercise influence over each other. Additionally, counterfactual reasoning is employed to marginalize the influence of an agent's message on another agent's behavior, enabling the measurement of this influence and the use of the PIMAEX reward. PIMAEX-Communication can, in principle, be used with any actor-critic algorithm. Therefore, the algorithm presented in Section 4 and the accompanying discussion focus solely on the adjustments necessary for incorporating the communication channel, counterfactual reasoning, and the PIMAEX reward, rather than on the specific policy and value function updates. These adjustments impact the neural network inference functions of agents, as well as the acting and learning components of a typical RL training loop, and thus will be the focus of Section 4."}, {"title": "3.1 Multi-Agent Social Influence Reward Functions", "content": "The generalized social influence reward function presented here builds on prior work in (Jaques et al., 2018) and (Wang et al., 2019) and unifies these concepts to form a comprehensive framework. Using this formulation, a specific reward, the PIMAEX reward, incentivizes agents to explore novel states by combining influence measures with intrinsic curiosity rewards. Two types of influence are included: policy influence, akin to causal influence in (Jaques et al., 2018), and value influence, similar to the Value of Interaction (VoI) from (Wang et al., 2019). Definitions for these measures are covered in Section 3.1.1."}, {"title": "3.1.1 Policy and Value Influence", "content": "Let $info_{j \\to i}$ denote information from agent j available to agent i at time t. This could include past actions, observations, or messages. An informed policy $\\pi_{i}^{info}$ and value function $V_{i}^{info}$ for agent i use both $o_i$ (the agent's observation at t) and $info_{j \\to i}$. The marginal policy $\\pi_{i}^{marginal}$ and value $V_{i}^{marginal}$ of agent i are derived by excluding $info_{j \\to i}$, representing how i would act if uninfluenced by j.\nThe marginal policies and values can be computed by replacing $info_{j \\to i}$ with various counterfactuals $info_{N_{ji}^{cf}}$ and averaging to remove its effect. This counterfactual approach is computationally efficient and avoids assumptions needed in alternative methods. Policy influence (PI) is then measured as the divergence between $\\pi_{i}^{info}$ and $\\pi_{i}^{marginal}$, while value"}, {"title": "4 PIMAEX-COMMUNICATION", "content": "PIMAEX-Communication is a MARL training algorithm inspired by (Jaques et al., 2018), where agents use a discrete communication policy and value function. At each timestep, agents emit discrete messages, which are concatenated and provided as input to all agents at the next timestep, forming a communication channel that enables mutual influence. This setup represents a decentralized networked agent setting, assuming peer rewards are viewed as additional communication actions.\nFollowing (Jaques et al., 2018) and (Wang et al., 2019), we employ counterfactual reasoning to isolate the influence of one agent's message on another's behavior, facilitating the calculation of influence and use of the PIMAEX reward. PIMAEX-Communication is compatible with any actor-critic algorithm, so specifics of policy and value updates are left to the underlying actor-critic method. Additionally, PIMAEX-Communication is agnostic to the intrinsic reward computation method; thus, we present the algorithms in a general form. In the following sections, we discuss the required modifications to the agents' network architecture and the actor and learner loops, with technical implementation details provided in Section 4.3."}, {"title": "4.1 Network Architecture", "content": "To implement PIMAEX-Communication, two main modifications are necessary in the neural networks of agents. First, each agent requires a communication policy and value head alongside their environment policy and value function to enable message exchange. Second, agents need an additional value head for intrinsic returns, resulting in three value functions-one each for extrinsic, intrinsic, and communication rewards and two policies.\nAdditional details include deciding where to incorporate the communication observation (i.e., message vector) in the network. Following (Jaques et al., 2018), we input the communication observation, concatenated with latent features from the environment observations, to the last shared hidden layer of all policy and value heads. An embedding layer is then added between this shared layer and the communication policy and value head.\nLastly, a key consideration is the computational overhead in calculating marginal policies and values. For each agent i, marginal values are computed by averaging over counterfactual messages for each agent j\u2260i. Let N be the number of agents, M the counterfactual messages per agent, and B the"}, {"title": "4.2 Actor and Learner", "content": "At each timestep, a PIMAEX-Communication agent calculates value estimates for three reward streams and samples actions for both communication and environment policies. Additionally, M counterfactual actions are sampled for each agent from a uniform distribution. Here, M = |Acomm| \u2013 1 (i.e., all possible actions except the one taken). For small action spaces, this approach is feasible; larger spaces may require fewer samples.\nCounterfactual message vectors can be constructed either centrally in the actor or individually by each agent. We chose a central construction for computational efficiency, avoiding redundant calculations. For message embedding, our implementation uses one-hot embeddings to represent discrete communication actions, which are concatenated to form the joint message vector. Influence is calculated on the actor side, while intrinsic and PIMAEX rewards are computed on the learner side, which leverages Random Network Distillation (RND)(Burda et al., 2018) to normalize intrinsic rewards efficiently. Deferring these computations to the learner side enables batched operations and speeds up processing. In off-policy algorithms, computing influence on the actor side is advisable to avoid drifts between actor and learner policies, ensuring correct reward attribution.\nIn our asynchronous PPO(Schulman et al., 2017) setup, where parameters are asynchronously synchronized, actor-side influence computation with learner-side intrinsic rewards was preferred, as reflected in the algorithms. For policy-reward association, the communication policy is rewarded by a weighted combination of environment, intrinsic curiosity, and PIMAEX rewards. Meanwhile, the environment policy follows (Burda et al., 2018) with a combination of en-"}, {"title": "5 EXPERIMENTAL SETUP", "content": "In this Section, we describe details about our experimental setup we use to evaluate our approach. First, we introduce the Consume/Explore environment (Section 5.1), a challenging task designed to test credit assignment and the exploration-exploitation dilemma. Then, we provide details about the agents used, including hyperparameters, network architectures, and other relevant information. We discuss the evaluation methodology in Section 5.2, followed by the results, including performance measures collected"}, {"title": "5.1 Consume/Explore Environment", "content": "The Consume/Explore environment is a partially observable multi-agent task designed to challenge agents with the exploration-exploitation dilemma and credit-assignment problem. It offers parameters to adjust difficulty in terms of credit assignment, stochasticity, and resource abundance. It features a deceptive reward and is a sequential social dilemma(Leibo et al., 2017), where agents can choose to cooperate or defect. We describe the environment's dynamics, parameters, and the observation and action spaces.\nIn this environment, each of the N agents has a private production line producing C consumable items every M steps, where C\u2208 [Cinit, Cmax]. Produced items are stored in the agent's supply depot with maximum capacity Smax. If the depot is full, production halts until space is available. Agents start with Sinit items. Parameters M, Cinit, Cmax, Sinit, and Smax control resource abundance.\nAgents choose between three actions: do nothing, consume, or explore. Consuming an item yields a reward R if the agent has items in its depot; otherwise, the action has no effect. The explore action aims to increase the production yield C for all agents but offers no immediate reward. The benefit of exploring is delayed and not directly signaled to the agents.\nReaching the next production yield level requires Cmax successful exploration actions. The success of an explore action depends on the threshold parameter E: for 0 \u2264 E \u2264, success is guaranteed; for E >, success depends on the number of agents simultaneously exploring and random chance. Thus, E controls the required degree of coordination. For each successful explore action, the counter c increments by 1. When c reaches Cmax and C < Cmax, C increases by 1, and c resets to 0. Larger Cmax values increase the difficulty of credit assignment, as more exploration is needed to increase C. Unsuccessful explore actions may incur a penalty P, introducing risk to exploration.\nThe parameters used in our experiments are reported in Table 1. Importantly, Cmax is set so that increasing C by more than two levels requires team effort. The exploration success threshold E is set to 0.5, requiring at least two agents to explore simultaneously for guaranteed success. All experiments involve four agents per environment.\nEach agent's observation is a five-element vector, consisting of three private elements and two global elements. The private elements are:\n1. The agent's current supply of consumable units.\n2. A boolean indicating whether the supply depot has enough space for the next production yield.\n3. The time remaining until the next production yield is completed.\nThe global elements are:\n1. The current production yield level C.\n2. The number of successful exploration actions c toward the next yield level C +1.\nAll quantities are normalized to the range [0, 1]. For the remainder of this work, we refer to the private observations as the \"local agent state space\" and the global observations as the \"exploration state space\"."}, {"title": "5.2 Methodology", "content": "To evaluate the performance of PIMAEX-Communication, we compared several agents against two baseline methods: 'vanilla' PPO agents without intrinsic curiosity or PIMAEX reward, and PPO+RND agents with intrinsic curiosity via Random Network Distillation (RND). We first conducted exploratory training runs with 'vanilla' PPO agents to identify an environment parameterization that posed sufficient difficulty. The hyperparameters from these experiments (Table 2) were then used as a basis for subsequent experiments. Next, we performed a hyperparameter search over RND-specific settings for PPO+RND agents to find configurations that outperformed 'vanilla' PPO agents (Table 3). The best-performing hyperparameters were then used for all PIMAEX-Communication agents. To assess the impact of the individual terms \u03b1, \u03b2, \u03b3 in the PIMAEX reward, agents were trained using only one term at a time, referred to as 'single-term' PIMAEX agents.\nEach model was trained with three different random seeds, and results were averaged. Inference performance was evaluated over 600 episodes per model (200 per seed), and results were averaged.\nPerformance was evaluated using various measures focused on exploration behavior. The joint episode return indicates team performance, while differences in individual returns suggest division of labor-agents with lower returns may have focused on exploration, while those with higher returns exploited resources. This can be corroborated by action statistics, such as the percentage of consume/explore actions per agent and the number of simultaneous actions per episode, indicating coordination levels. Additionally, state space coverage was used as a direct measure of exploration. Finally, the final production yield level and the number of steps taken to reach each yield level served as indicators of exploration and co-"}, {"title": "6 RESULTS", "content": "We compare the best-performing models of each agent class: 'vanilla' PPO, PPO with RND intrinsic curiosity rewards (abbreviated as RND), and 'single-term' PIMAEX agents (PIMAEX \u03b1, PIMAEX \u1e9e, and \u03a1\u0399\u039c\u0391\u0395\u03a7 \u03b3). Hyperparameter settings for these models are given in Table 5. As mentioned in the previous section, agent performance is assessed using various measures, focusing on exploration behavior.\nFig. 1 displays the total episode return (joint return of all agents) over training. The left figure shows return for actor processes (agents act stochastically), and the right shows the evaluator process (agents act greedily). 'Vanilla' PPO agents fail to learn a successful policy, performing worse as training progresses. In contrast, curious agents (PPO+RND and PIMAEX agents) initially prioritize maximizing intrinsic return, resulting in low extrinsic return early on, likely due to higher prediction error in the RND model at the start of training. However, this does not hinder long-term performance: PPO+RND outperforms 'vanilla' PPO, and is itself outperformed by 'single-term' PIMAEX agents, with PIMAEX \u1e9e being the best-performing method.\nmethods, a consistent pattern across all performance metrics.\nof the agent classes coordinate explore actions in teams larger than two, though they do so for consumption. When focusing on pairs of simultaneous explore actions, PIMAEX agents explore in teams of two for about one-third of the episode, closely followed by RND. Again, PIMAEX \u1e9e agents display considerably less standard deviation than others.\nDifferences in state space coverage are less pronounced than those observed in returns. While final exploration state space coverage varies slightly among methods (except for PPO), differences are more evident when examining coverage within an episode. Here, PIMAEX a agents are the best explorers, despite not participating in other agents' intrinsic returns like the \u1e9e and y agents. This suggests that influence rewards combined with individual curiosity may suffice to enhance multi-agent exploration. Notably, PIMAEX \u1e9e agents exhibit significantly reduced standard deviation in both local agent state space coverage within an episode and final agent state space coverage after training.\nAn interesting observation is that PIMAEX B agents appear to specialize in teams of two: agents 1 and 3 are the best explorers, while agents 2 and 4 explore less, especially within an episode.\nFigure Fig. 6 corroborates these findings. It shows that PIMAEX \u1e9e agents 2 and 4 predominantly consume and rarely explore. Again, PIMAEX & agents are the most active explorers, followed by RND and PIMAEX \u03b3. This supports the hypothesis that participating in others' intrinsic returns does not necessarily promote more exploration, and that individual intrinsic returns, possibly combined with influence rewards, can drive multi-agent exploration.\nExamining the number of simultaneous consume (left) or explore (right) actions in Figure Fig. 6, none"}, {"title": "7 CONCLUSION", "content": "This work introduced two reward functions: the PIMAEX reward, a peer incentivization mechanism based on intrinsic curiosity and social influence, and its more generalized version, which is also applicable to agents without intrinsic curiosity, although this was not evaluated in this work. The RL training algorithm used with the PIMAEX reward, PIMAEX-Communication, can, in principle, be used with any actor-critic algorithm. Its communication mechanism, adopted from (Jaques et al., 2018), is comparatively easy to implement on top of existing actor-critic algorithms. Additionally, due to its highly flexible customization options, the Consume/Explore environment introduced in this work is a promising tool for future research in multi-agent reinforcement learning, especially for creating sequential social dilemmas and other challenging tasks.\nThe empirical results presented in this work show that using the PIMAEX reward in conjunction with PIMAEX-Communication improves the overall return of a multi-agent system in the Consume/Explore task when compared to baseline methods without social influence. PIMAEX \u1e9e, the main novel contribution of this work, achieves the highest overall return among these methods. However, as these results also show, participation in other agents' intrinsic return does not necessarily lead to more exploration compared to methods where this is not possible. Notably, PIMAEX a agents, which rely only on social influence and individual intrinsic curiosity, exhibited the strongest exploratory behavior, which is an interesting and somewhat unexpected insight. Furthermore, PIMAEX \u1e9e agents' policies are more stable at inference time compared to all other agents, exhibiting far less standard deviation.\nDespite these promising results, this work has several limitations that suggest directions for future research. First, the neural networks used were relatively small feed-forward architectures. Evaluating PIMAEX with larger and more sophisticated neural networks, including recurrent models, might improve performance and provide a more robust comparison with baseline methods. Additionally, PIMAEX was only evaluated using the PPO algorithm; exploring its performance with other actor-critic methods like IMPALA(Espeholt et al., 2018) would be valuable. Second, the agents were trained for a relatively short duration, and extending training times could yield different insights, especially with more capable models. Moreover, the evaluation was limited to a single task with a small state and action space. Future work should test PIMAEX on more complex and realistic tasks with larger state and action spaces, and assess its scalability with a greater number of agents. Addressing these limitations will help to draw more general conclusions about the effectiveness of PIMAEX in multi-agent reinforcement learning."}]}