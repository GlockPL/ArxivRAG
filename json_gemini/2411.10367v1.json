{"title": "Continual Adversarial Reinforcement Learning\n(CARL) of False Data Injection detection:\nforgetting and explainability", "authors": ["Pooja Aslami", "Kejun Chen", "Timothy M. Hansen", "Malik Hassanaly"], "abstract": "False data injection attacks (FDIAs) on smart in-\nverters are a growing concern linked to increased renewable\nenergy production. While data-based FDIA detection methods\nare also actively developed, we show that they remain vulnerable\nto impactful and stealthy adversarial examples that can be crafted\nusing Reinforcement Learning (RL). We propose to include such\nadversarial examples in data-based detection training procedure\nvia a continual adversarial RL (CARL) approach. This way, one\ncan pinpoint the deficiencies of data-based detection, thereby\noffering explainability during their incremental improvement.\nWe show that a continual learning implementation is subject\nto catastrophic forgetting, and additionally show that forgetting\ncan be addressed by employing a joint training strategy on all\ngenerated FDIA scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of power electronic-based distributed energy\nresources (DER) negatively affects the power grid inertia,\nincreasing frequency stability issues [1]. Smart inverters (SI)\nplaced at the interface between DERs and the power grid\nare a promising solution to mitigate the effects of inertia\nloss [2]. However, SIs depend on data exchanges, making their\nactuation policy vulnerable to False Data Injection Attacks\n(FDIAs), that may tamper with the exchanged data or with\nthe control logic of the SI [3]. Critically, the effects of FDIAs\ncan be long-lived if specifically crafted to be stealthy [4].\nIn view of these risks, several FDIA detection methods\nhave been introduced and increasingly make use of data-\nbased approaches [5]. Such methods rely on the definition\nof a decision boundary that typically requires supervised\ntraining for at least some components of the FDIA detector"}, {"title": "II. METHOD", "content": "This section presents the proposed continual adversarial\nRL (CARL) framework. The RL environment, including the\nfrequency dynamics model, is discussed in Sec. II-A. The\ntraining procedure is described in Sec. II-B."}, {"title": "A. RL Environment", "content": "1) Frequency Dynamics Model: The frequency dynamics\nof the power system with a primary frequency control can be\nmodeled with the swing equation [13]:\n\\theta_i = \\omega_i, \\\\\nM_i \\dot{\\omega}_i = P_i - P_{e,i} - D_i \\omega_i - p,\\\\\nwhere $P_{e,i} = \\sum_{j\\in(N\\{i\\})} B_{ij}sin(\\theta_i - \\theta_j)$, $i \\in N = \\{1, ..., n\\}$ \nis the bus index, $\\theta_i$ and $\\omega_i$ are the voltage phase angle and\nfrequency deviation of bus $i$, respectively, $M_i$, $D_i$, $P_i$ are\nthe inertia, damping coefficient, and net power injection of\nbus $i$, respectively, $P_{e,i}$ is the electric power, and $B_{ij}$ is\nthe susceptance of the line between bus $i$ and $j$. A linear\ndroop controller for primary frequency control is used, i.e.,\n$p = k_i \\omega_i$, where $k_i$ is the droop coefficient of the respective\nSI at bus $i$.\n2) False Data Injection Attack Threat Model: It is assumed\nthat an adversary has gained remote access to all SIs, and\ncan alter their control logic to induce frequency instabilities.\nConsistently with [8], the FDIA is conducted by modifying\nthe droop coefficient of the SI, leading to $p = k_i \\omega_i$ where\n$k_{i,t} \\in \\{-1,0,1\\}$ is the discrete altered droop coefficient for\nbus $i$ at time $t$. For simplification, at most one droop coefficient\ncan be modified at a given time $t$, i.e., $||k-k||_0 \\leq 1$,\nwhich provides a baseline for performance analysis. The FDIA\nattempts to maximize the frequency deviation, i.e., maximize\n$\\sum_{t\\in\\tau} \\sum_{i\\in N} |\\omega_i - \\omega_{ief}|$, where $\\omega_{ief}$ is the frequency devi-\nation obtained with unaltered droop coefficients [13]. Every $d$\ntimesteps, the adversary either selects a bus $g^A_t \\in N$ to attack\nor chooses to not attack. If it decides to attack, it can either\nmodify the droop coefficient or temporarily mute itself. This\nperiodic choice simplifies the definition of the detection which\nis described below.\n3) FDIA Detector: The FDIA detection uses a state pre-\ndictor and a classifier as described in [6]. The state predictor\nis a long-short-term memory (LSTM) network that predicts\nthe theoretical next state based on the past $d - 1$ states if no\nFDIA occurs. Every $d$ timesteps, a neural network multi-class\nclassifier reads the difference between the predicted and the\nobserved states, and outputs $g^D_t \\in N$, the bus index thought\nto be under attack. While the state predictor can be trained in\nan unsupervised way, the classifier cannot and is vulnerable to\nadversarial examples. The overarching objective is to improve\nthe classifier. The detector is successful if $g^D_t = g^A_t$, and $g^A_t$\nis attacked at any point in the $d$-timestep window. The $d$-\nperiodicity of the FDIA (Sec. II-A2) avoids any ambiguity\nabout the bus under attack."}, {"title": "B. Proposed Continual Adversarial RL (CARL) framework", "content": "This section describes the proposed CARL for FDIA detec-\ntion (illustrated in Fig. 1 (i)), and includes the sequential train-\ning of an adversary RL agent and a defender RL agent. The\nadversary is trained to use the FDIA threat model (Sec. II-A2)\nto induce large frequency deviation while minimizing the\nnumber of successful detections from the defender agent. The\ndetector agent is trained to maximize the number of successful\ndetections. Below, the RL agent definitions are provided.\n1) Adversary:\na) Observation: The observations $s_t^A$ of the adversary\nincludes the frequency deviation and voltage phase angle from\nall buses at time $t$ and, a normalized simulation step $t'$, i.e.,\n$s_t^A = [\\omega_t, \\theta_t, t']^T$. Given that the detector is not active at\nevery timestep, the normalized step allows the adversary to\nadapt its policy over time accordingly.\nb) Action: The action $a_t^A$ includes the bus under attack,\ntampered droop coefficient, and an attack decision variable\n$m_t \\in \\{0,1\\}$, i.e., $[g^A_t, k'_{i,t}, m_t]$. Here, $m_t = 1$ means that no\nattack will occur at that specific timestep.\nc) Reward: The reward $r^A_t$ is defined as follows:\n$r_t^A = \\begin{cases}\n-p, & \\text{if } g^A_t = g^D_t \\neq -1, \\forall t \\in \\{d, 2d, ...\\}, \\\\\nr_t, & \\text{otherwise},\n\\end{cases}$\nwhere,\n$r_t = c \\sum_{i\\in N} |\\omega_i - \\omega_{ief}|$,\nwhere $p > 0$ is a detection penalizer and $c$ is a scaling factor\nthat makes the penalty commensurate with the reward.\n2) Defender: The LSTM state predictor is not updated and\nonly the multiclass classifier is trained. The detection is only\nperformed at $t \\in \\{d, 2d, ...\\}$.\na) Observation: The observation $s^D_t$ for the defender\nincludes the frequency deviation and voltage phase angle from\nall buses over the $d$-timestep window, i.e., $[\\omega_{d:t}, \\theta_{d:t}]^T$.\nb) Action: The action $a^D_t$ for the defender is the pre-\ndicted bus index $g^D_t$.\nc) Reward: The reward $r^D_t$ encourages accurate detec-\ntion and is defined as:\n$r^D_t = \\begin{cases}\n+p, & \\text{if } g^A_t = g^D_t, \\\\\n-p, & \\text{otherwise}.\n\\end{cases}$"}, {"title": "3) CARL Training procedure:", "content": "The training process of $A$\nand $D$ agents in the proposed CARL framework is shown\nin Algorithm 1. Here, policy $\\pi^A_{\\eta, \\theta}$ and $\\pi^D_{\\eta, \\theta}$ from $A$ and $D$\nare trained with separate environments $FDIA$ and $FDID$,\nrespectively. The starting point of the CARL framework is\nan offline FDIA detector built as described in Sec. II-B,\nand denoted as $D_0$ trained following Ref. [9]. To train the\nclassifier, throughout the episode steps, FDIAs are syntheti-\ncally constructed by modifying the droop coefficient of the\nbus $i$ within randomly selected $\\tau_a$ fraction of steps, where\n$\\tau_a \\in \\{0.16, 0.2, 0.4, 0.6, 0.8\\}$. This synthetic attack is referred\nto as $A_0$ hereafter. Therefore, during an entire episode, $A_0$\ntargets the same bus which is not the case for the other\nattackers. After the first CARL iteration, and to retain memory,\nthe agents are warm-started with the weights of the previous\nCARL iteration."}, {"title": "C. Rehearsal Continual Adversarial RL (R-CARL)", "content": "To address the issue of CF in continual learning [12], several\nmethods have been proposed such as loss regularization [14],\n[15] and model expansion strategies [16]. When possible,\nmethods that use examples of old tasks when training for a new\ntask were particularly successful [15], [17]. Here, a rehearsal\ncontinual adversarial RL (R-CARL) approach that trains an\nultimate detector $D$ against an ensemble of adversaries is used\nto address the problem of CF. The same procedure can be\nused to train an ultimate adversary $A$ against an ensemble of\ndetectors.\nR-CARL follows Algorithm 1 with $n = 1$ and modification\nto steps 4 and 21: instead of uploading a single $D$ and $A$\nagent in the FDID and FDIA environments, this approach\nincorporates the entire list of $D$ and $A$ agents obtained\nfrom CARL training, including $D_0$ in the defender list. The\nagent chosen for each environment step is sampled from the\nprobability distribution $P$, defined in Eq. 4.\n$A \\leftarrow \\text{Train against } D \\sim U\\{D_0, D_1,..., D_N\\},$\n$D \\leftarrow \\text{Train against } A \\sim \\begin{cases}\nA_0, & P(A_0) = 0.8 \\\\\nA_n, & P(A_i) = \\frac{0.2}{N}, n \\in [N],\n\\end{cases}$\nwhere $N$ is the number of trained $D$ and $A$ agents from CARL\ntraining."}, {"title": "III. RESULTS", "content": "The system investigated is a 10-bus Kron reduced IEEE\nNew England 39-bus [18]. Each episode is run for a time\ninterval $T = [0,5s]$ and is integrated with a timestep of 0.01s\nusing the implementation of [13]. Therefore, the output of the\nclassifier defender contains 11 classes (one for each bus and\none for a label of no attack). The RL agents were trained as\nproximal policy optimization stochastic agents, trained with\na constant learning rate of $10^{-4}$, using the Ray library [19].\nThroughout the paper, each classifier and adversary is a neural\nnetwork with 2 hidden layers and 256 neurons per layer. The\ndetection period is $d = 6$. The detection penalizer is set to\n$p = 0.1$ the scaling factor is set to $c = 0.1$."}, {"title": "A. Adversarial example for an offline detector", "content": "Table I shows the detection accuracy attained for each pair\nof $(A_i, D_j)$. Because each adversary is stochastic, the accuracy\nis tested on a randomly sampled realization of the adversary.\nFor $A_0$, the detection is evaluated on a synthetic FDIA unseen\nat training time. In Tab. I, the unscaled frequency reward ($r_t/c$\nin Eq. 2) is also reported. The baseline accuracy is that of the\noffline defender ($D_0$) achieved against synthetic attacks ($A_0$).\nAt the second CARL iteration, an adversary $A_1$ is trained\nto defeat $D_0$ while inducing large frequency deviations (Fig. 2\nright). As shown in Tab. I, $A_1$ can reduce the detection rate"}, {"title": "B. Continual learning and forgetting", "content": "Following the continual learning training sequence\n(Algo. 1), three additional detectors and adversaries are\ntrained and the objective here is to understand how fast CF\noccurs.\nTab. I shows that the detection accuracy of $D_1$ on $A_0$\ndropped from 71.6% at the beginning of the $D_1$ training\nprocedure to 23.6%. Therefore, the CARL framework is un-\nsurprisingly subject to CF even after one CARL iteration, and\nwarm-starting is not enough to promote knowledge retention.\nThree additional detectors ($D_2, D_3, D_4$) and three addi-\ntional adversaries ($A_2, A_3, A_4$) are similarly trained, and\nthe detection accuracies are reported in Tab. I. The table\ndiagonal (highlighted in bold), represents pairs of adversary-\ndefenders, where the adversary action was observed by the\ndefender. This setup leads to the highest detection accuracies.\nStarting from the diagonal and reading towards the left gives\nthe backward transfer rate as defined in Ref. [17], i.e., the rate\nat which a defender forgets older attacks. After one iteration,\nthe detection accuracy is reduced on average by a factor 1.7,\nafter two iterations by a factor 2.3, and after three iterations\nby a factor 3.1.\nLikewise, starting from the diagonal and reading down-\nwards, the detection accuracy for each attack policy steadily\ndecreases. After about four CARL iterations, an attack policy\nonce addressed is almost entirely forgotten: the detection rate"}, {"title": "C. Addressing catastrophic forgetting with R-CARL", "content": "Sec. III-B showed that the CARL iteration procedure was\nnot immune to CF. To address this issue, we employ a rehearsal\nstrategy as described in Sec. II-C that exposes a detector to\nall the available adversaries. The detection accuracy attained\nfrom R-CARL-trained detector $\\bar{D}$ is shown in Tab. I.\nWhile not specialized in one attack, $\\bar{D}$ outperforms all\nthe other detectors, including the offline detector that was\nspecialized to detect $A_0$. This type of accuracy improvement\non a single task obtained with multi-task training [20] sug-\ngests that adversarial examples are beneficial for improving\ndetection rates in general. The detection accuracy on the RL\nadversaries is high primarily because the adversary exhibited\na lack of diversity in the attack generated: therefore, a large\npart of the testing data was seen during training. We leave\nthe improvement of the adversary diversity as future work.\nNevertheless, the same diversity issues affected the detectors\ntrained via continual learning ($D_1, D_2, D_3$), and the warm-\nstarted defenders could not achieve as high detection accuracy.\nTherefore, the rehearsal strategy outperforms the continual\nlearning strategy, while retaining the detection capabilities on\na synthetically generated set of FDIAS ($A_0$).\nA similar procedure can be conducted for the adversaries\nby exposing one ($\\bar{A}$) to a library of detectors. There again,\nthe detection accuracies are almost always lower than all the\nCARL-trained adversaries (except for $D_0$ and $D_2$)."}, {"title": "D. Explainability of adversaries", "content": "The benefit of the CARL and R-CARL approaches is that\none can access the individual adversaries that are incrementally\nadded to the knowledge pool of the detector. Here, the objec-\ntive is to leverage this property to understand how adversaries\nchange throughout the CARL iterations.\nIn a threat model (Sec. II-A2), every $d = 6$ timesteps an\nadversary chooses a single bus and can either attack or not.\nThe choice of the attacked bus for adversary $A_k$ is represented\nby a transition matrix $T_k$ where the entry of row $i$ and column\n$j$ is filled with the probability $P_{i\\rightarrow j}$. This probability denotes\nthe probability that if bus $i$ is targeted during one 6-timestep\nwindow, bus $j$ will be targeted during the next window. The\ndifference $\\epsilon_{kl}$ between adversaries $A_k$ and $A_l$ can be measured\nas\n$\\epsilon_{kl} = \\frac{||T_k -T_l||}{\\sqrt{||T_k||||T_l||}},$"}, {"title": "IV. CONCLUSIONS", "content": "This work demonstrated that data-based detection of FDIA\nis vulnerable to impactful and stealthy adversarial attacks\nthat can be crafted with RL. Through a continual learning\napproach, one can build a library of adversarial examples\nthat can be used to create an explainable knowledge pool for\nthe detection. Currently, we simplified the training process by\nfocusing on a single initial condition. In the future, it would be"}]}