{"title": "DEEP ADVERSARIAL DEFENSE AGAINST MULTILEVEL-lp ATTACKS", "authors": ["Ren Wang", "Yuxuan Li", "Alfred Hero"], "abstract": "Deep learning models have shown considerable vulnerability to adversarial attacks, particularly as attacker strategies become more sophisticated. While traditional adversarial training (AT) techniques offer some resilience, they often focus on defending against a single type of attack, e.g., the lo-norm attack, which can fail for other types. This paper introduces a computationally efficient multilevel lp defense, called the Efficient Robust Mode Connectivity (EMRC) method, which aims to enhance a deep learning model's resilience against multiple lp-norm attacks. Similar to analytical continuation approaches used in continuous optimization, the method blends two p-specific adversarially optimal models, the l\u2081- and l-norm AT solutions, to provide good adversarial robustness for a range of p. We present experiments demonstrating that our approach performs better on various attacks as compared to AT-l\u221e, E-AT, and MSD, for datasets/architectures including: CIFAR-10, CIFAR-100 / PreResNet110, WideResNet, ViT-Base.", "sections": [{"title": "1. INTRODUCTION", "content": "Deep learning models have revolutionized numerous fields, offering innovative solutions to complex problems [1, 2]. However, their vulnerability to adversarial attacks remains a significant concern [3, 4, 5], undermining their practical utility and reliability. Specifically, these models are sensitive to slight, yet strategic, perturbations in their input data, which can mislead them into making incorrect predictions. While several methods aim to defend against such adversarial ma- nipulations, most focus on enhancing the model's resilience against attacks based on a single type of perturbation met- ric, often measured by a lp norm [6, 7, 8, 9] for specific p\u2208 [1,\u221e]. This focus creates a defensive blind spot, leaving models vulnerable to other types of adversarial perturbations. On the other hand, recent studies that aim to achieve universal robustness across multiple lp norms either suffer from high\ncomputational costs or do not entirely solve the universal robustness problem: maintaining robustness against different types of perturbations concurrently [10, 11, 12].\nThis paper addresses the shortcomings of current meth- ods by proposing universally robust models capable of coun- tering diverse types of lp-norm adversarial attacks. Previ- ous studies have identified the mode connectivity property, which suggests that a path of high accuracy and low loss ex- ists between two well-trained models in the parameter space [13, 14, 15]. Building on this concept and the theoretical ev- idence that affine classifiers can withstand multiple types of lp attacks if they are already resistant to l\u2081 and lo perturba- tions [10], our work presents a novel approach: Efficient Ro- bust Mode Connectivity (ERMC) combined with Model En- semble which waeves l\u2081 and l\u221e robustness into the fabric of mode connectivity to derive a new training methodology. This amalgamation enables the identification of parameter paths that remain highly resistant to both l\u2081 and lo perturbations, and therefore, multiple types of lp norm perturbations. We introduce an optimized fine-tuning technique with reduced computational complexity. Lastly, we employ a model en- semble strategy to select and aggregate models from this ro- bust path, further improving robustness. Specifically, the al- gorithm works as follows. We first train one endpoint model optimized for l\u221e-norm adversarial training then retrain the model to be optimal relative to the l\u2081-norm. Using these two endpoint models, and leveraging the mode connectivity prop- erty of deep neural networks (DNN), we identify a low-loss, high-robustness path connecting these endpoints. Finally, we deploy ensemble model aggregation to select models along this path that exhibit collective robustness against all types of lp-norm attacks, 1 < p < \u221e.\nContributions. We summarize our contributions below.\n1. We improve upon traditional mode connectivity ap- proaches to the design of DNN by integrating adver- sarial robustness, thereby uncovering a path that links an lo and an l\u2081 adversarially trained model. This path demonstrates high resistance to other lp-norm attacks for p \u2208 [1,\u221e].\n2. We propose an Efficient Robust Mode Connectivity (ERMC) method, supplemented with model ensem- ble aggregation, that results in an efficient adversarial training algorithm with enhanced robustness."}, {"title": "2. BACKGROUND AND RELATED WORK", "content": "Recent studies have revealed that conventional machine learn- ing models are susceptible to adversarially modified datasets. For a model 0 an adversary can target each feature x \u2208 Rd in a database D of feature-label pairs D = {x, y}, by solving the following attacker's optimization problem:\n$\\arg \\max L(\\theta; x',y), \\text{s.t.} d_p(x',x) \\leq \\epsilon_p.$\n(1)\nHere L represents the training loss, e.g., the cross-entropy loss. Ep is the attack-strength parameter of the type-p attacker, and dp is a distance metric of type-p over the model param- eter space. As in many other studies, we restrict attention to the case that dp is the lp norm with p \u2208 [1,\u221e]. The solu- tion to (1) is commonly known as the lp adversarial attack [6]. This problem is often iteratively solved using the fast gradient sign method [3] or projected gradient descent (PGD) [6], which computes the gradient \u2207x\u00b4L(0; x', y) combined with a projection that constrains the perturbation x' x to the lp-ball of radius ep. The projection for the lp adversarial attack is denoted by Pep. These methods may result in sub- optimal solutions to (1) due to incorrect hyper-parameter tun- ing and gradient masking. To address these issues, methods such as the Auto Attack (AA) [16] and Multi Steepest De- scent (MSD) [12] were introduced. Adversarial attacks can operate in a black-box manner, meaning the attacker does not have access to the model's parameters [17, 18]. However, this paper focuses only on scenarios where the attacker is aware of the model's parameters. To counter the attackers strategy (1), adversarial training (AT) methods are effective defense mech- anisms [6, 7, 8, 9]. However, these methods often focus on a single type of lp disturbance, leading to decreased robustness against different types of perturbations [19]."}, {"title": "2.2. Robustness Towards Multiple lp Norm Perturba- tions", "content": "In [11] the authors propose training on l\u221e-generated ad- versarial examples while selectively discarding inputs having low confidence scores, showing empirically that this results in a degree of robustness to lp-attacks for p = 0,1,2,000. The authors of [19] propose calculating the worst case at- tack by either picking the attack type that leads to the max- imum loss or averaging the loss across all attack types. The Multi Steepest Descent (MSD) Defense [12] integrates mul- tiple perturbation schemes to yield a more comprehensive lp robustness. The work in [10] offers a theoretically guaranteed defense mechanism but it only applies to affine classifiers. The Extreme Norm Adversarial Training (E-AT) method [20] employs a form of fine-tuning to practically implement the pathway from [10] and to reduce AT computational load. In contrast, in this paper we exploit the mode connectivity prop- erty of deep neural networks [13, 14, 15] to define the ERMC method that improves on the performance reported in [20]."}, {"title": "3. MULTILEVEL lp-DEFENSE", "content": "Adversarial training (AT). Complementing the attacker's optimization (1), the defender aims to solve the defender's optimization problem:\n$\\min_{\\theta} E_{(x,y) \\in D} \\left[ \\max_{x':d_p(x',x)<\\epsilon_p} L(\\theta; x', y) \\right],$\n(2)\nusing training data from D to empirically estimate the statis- tical expectation in (2), resulting in a solution we call Ad- versarial Training (AT)-lp. The main issue addressed in this section is that the solution AT-lp for a given p does not en- sure robustness to other values of p in [1,\u221e]. Furthermore, while in principle one could compute a dense set of solutions {AT-lp}p\u2208[1,0], it is not clear how such solutions could be computed and combined in a computationally tractable man- ner to provide robustness over a range of p [19].\nOptimizing robustness against multilevel lp perturba- tions. As argued in [10], affine and piecewise affine clas- sifiers (like CNN with ReLU) can resist multiple lp norm attacks if they are already robust to l\u2081 and l\u221e perturbations. Specifically, Theorem 3.1 in [10] states that the convex hull of the union ball of the l\u2081 and lo provides satisfactory robustness to lp perturbations, 1 < p < x:\nTheorem 1 [10] Suppose that the classifier is piecewise affine. Let C be the convex hull of the union ball of the l\u2081 and"}, {"title": "4. PROPOSED METHODS", "content": "We aim to improve the joint robustness to both lo\u221e and l\u2081 perturbations by leveraging two adversarially trained models.\nFor neural networks, mode connectivity is the property that pairs of local minima (modes) discovered by gradient-based optimization techniques are connected through simple paths over which the model's loss does not change appreciably [13, 14]. In [14] mode connectivity is established for a wide range of DNNs and training datasets. The path between a pair of modes 01, 02 is constructed over the parameter space of the neural network by minimizing the averaged loss function, L, over all possible simple paths. The path is represented as \u03c6\u03b8 = {\u0444\u04e9(t), t \u2208 [0, 1]}, where e is a free parameter (control point), which satisfies the endpoint conditions \u03c6\u03b8(0) = 01 and o(1) = 02. Specifically, to find a desired low-loss path between the modes 01 and 02, one minimizes the following statistical expectation\n$\\min_{\\theta} E_{t\\sim U (0,1)} E_{(x,y)\\sim D}L(\\phi_{\\theta}(t); (x, y)),$\n(4)\nwhere U(0,1) represents the uniform distribution over the interval [0, 1]. The curve \u0444\u04e9 is fixed as a Quadratic Bezier Curve (QBC) [21] across this paper:\n$\\phi_{\\theta}(t) = (1 - t)^2\\theta_1 + 2t(1 - t)\\theta + t^2\\theta_2.$\n(5)\nThe main assumption behind this paper is that the no- tion of mode connectivity can be extended to adversarial loss\nfunctions associated with different lp-types, resulting in paths that maintain a high level of robustness against both l\u221e and l1 attacks, in addition to improving robustness to other lp attacks. The proposed extension consists of two additional steps: (Step 1) The endpoint parameters 01 and 02 are trained via AT-l\u221e and AT-l1; (Step 2) We solve the following mod- ification of (4) to preserve adversarial robustness for p\u0404 {1,0}:\n$\\min_{\\theta} E_{t\\sim U(0,1)}E_{(x,y)\\sim D} \\sum_{p \\in \\{1,\\infty\\}} \\max_{d_p(x',x)<\\epsilon_p} L(\\phi_{\\theta}(t); (x', y)),$\n(6)\nwhere (0) and $\u04e9(1) are the two AT models, AT-lx and AT-l1, respectively. In the inner optimization loop dp corre- sponds to the lo and l\u2081 distances for p = 0 and p = 1. We use a Multi Steepest Descent (MSD) technique to solve the maximization in the inner loop that encompasses both l\u221e and l1 perturbations within each step of PGD, similarly to [12]. In each epoch, for every data batch, we randomly choose a value for t. The subsequent training closely resembles Adversar- ial Training (AT), with the key difference being that we pick the worst-case perturbation from two types of perturbations in each inner loop iteration. Consequently, the algorithmic complexity remains similar to that of standard AT."}, {"title": "4.2. ERMC with model ensemble", "content": "We reduce the computation burden of solving two indepen- dent AT-lp problems, for p = \u221e and p = 1, by introducing a more efficient approach: the efficient robust model connectiv- ity algorithm. In ERMC, initially a model with high robust- ness to either l\u221e or l\u2081 perturbation is trained, after which a copy is created and retrained for another few epochs under the other perturbation type using the same training set. Similarly to its use in E-AT [20], the fine-tuning step provides more effi- cient computation of the AT-l and AT-l1 adversarial models in EMRC. In the experiments described below, the number of fine-tuning epochs is set to 10 yielding a computationally less burdensome determination of the second endpoint model, while retaining the first one, facilitating the identification of a high-robustness path as provided by (6). The full algorithm of ERMC is presented in Algorithm 1. The second endpoint (1) is trained from the first endpoint $\u04e9(0) using a dif- ferent perturbation type. In each epoch, we sample a t uni- formly from the uniform distribution. Then, in each iteration of generating perturbations, we consider two types: l\u221e and l1. Subsequently, we select the perturbations that cause the highest losses and use them to update the model parameters. The number of iterations, denoted by J, is set at 10 for our experiments.\nWe have observed in experiments that certain regions along the path contain models that exhibit high levels of ro- bustness for both types of perturbations. The optimal model along the path can be identified by assessing the trajectory with lower robust accuracy under l\u221e and l\u2081 attacks, and then selecting the point that performs best in this worst-case scenario. This single-model approach offers the advantage of circumventing the limitations inherent to E-AT [20] while capitalizing on robustness against both types of perturbations. However, given the existence of many models along the path that exhibit high degrees of robustness to l\u221e and l\u2081 attacks, it's natural to consider a model ensemble strategy to further bolster performance. This leads to a model that is collectively more robust to both l\u221e and l\u2081 perturbations. The ensemble selection proceeds as follows. We find a segment [a, b] along the path e satisfying the criterion: each point on the segment has robust accuracies surpassing two prefixed model selec- tion thresholds a\u221e, a1 under l\u221e and l\u2081 attacks, respectively. We then choose n > 1 models at path locations given by t = a + b-ai, i, where i ranges from 0 to n \u2013 1. If multi- ple non-continuous intervals meet the above criterion, the n points can be distributed among them proportionately to their respective lengths. We denote ERMC with n selected models as ERMC-n and average the outputs of these n models' final layers to form our class probability prediction."}, {"title": "5. EXPERIMENTS", "content": "Dataset selection and model architectures. We test our proposed techniques on CIFAR-10 (as the default dataset) and CIFAR-100 [23] datasets, utilizing PreResNet110 (as the default architecture), WideResNet-28-10, and Vision Transformer-base (ViT-base).\nEvaluation methods and metrics. We set the attack strength parameters constraining the l\u221e, 12, and l\u2081 norms to the com- monly used values \u20ac = 8/255,1, and 12, respectively. In our evaluation, we implemented basic PGD adversarial at- tacks as well as Auto-Attack (AA) [16] under l\u221e, 12, 11 norm perturbations, in addition to implementing the MSD attack. Metrics for assessment include: Standard accuracy (SA) on clean test data; \u2461 Robust accuracies under various per- turbation types including l\u221e/12/l1-PGD, MSD attack, and lx/12/l1 AA; and \u2462 Sample-wise worst-case scenario accu- racy (Union) calculated from all three basic PGD adversarial methods. A sample is considered correct only if it is accu- rately predicted under each of the three basic PGD adversar- ial attacks. These experiments were run on two NVIDIA RTX A100 GPUs.\nExperimental results. As a baseline, endpoint models are trained for 150 epochs, with paths derived through an ex- tra 50 epochs. The models at the left (right) endpoints are trained with AT-l\u221e (AT-lox and fine-tuned with AT-l1). The results are displayed in Fig. 1. The upper panels show the clean test accuracy and accuracies under l\u221e/l2/l1-PGD at- tacks. The lower panels show the corresponding loss values. t varies from 0 to 1. Moving from left to right in Fig. 1, panels (a) and (b) depict results obtained from the CIFAR- 10 and CIFAR-100 datasets, respectively, using the PreRes- Net110 model architecture. Conversely, panels (c) and (d) present results from the CIFAR-10 dataset, but utilizing the WideResNet-28-10 and ViT-base model architectures. No- tably, we find: \u25cf The existence of robust paths, which shows that the ERMC application enhances resilience to multiple attack types, although they don't form straight lines like in mode connectivity; ERMC performs well on all considered datasets and architectures; The robust paths also function as effective mode connectivity paths, where both the clean accu- racy and loss (indicated by red lines) maintain consistent lev-"}, {"title": "6. CONCLUSION", "content": "This paper introduces the Efficient Robust Mode Connectiv- ity (ERMC) method, a novel approach for enhancing the re- silience of deep learning models against various adversarial lp-norm attacks. By combining the robustness benefits of l1 and loo adversarial training within a single framework, ERMC transcends the limitations of traditional methods that focus on single-type perturbations. Leveraging mode connec- tivity theory with efficient tuning and ensemble strategies, the method achieves a robust defense. Experimental results show that ERMC outperforms established defenses like AT-l\u221e, E- AT, and MSD Defense, particularly against loo and l\u2081 pertur- bations and other lp-norm attacks. Its integration of multiple adversarial training types enhances defense capabilities while preserving efficiency, marking a significant step forward in adversarial robustness and suggesting new directions for fur- ther research in the security of deep learning."}]}