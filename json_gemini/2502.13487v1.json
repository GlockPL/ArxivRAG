{"title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging", "authors": ["Chen-An Li", "Tzu-Han Lin", "Yun-Nung Chen", "Hung-yi Lee"], "abstract": "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.", "sections": [{"title": "1 Introduction", "content": "Large vision-language models (LVLMs) have shown exceptional performance across a wide range of multimodal tasks (Hurst et al., 2024; Team et al., 2024; Anthropic, 2024), primarily due to the implementation of reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), which utilizes preference data (Sun et al., 2024; Li et al., 2024b). This process often requires the use of reward models (RMs). However, LVLMs still struggle to assess generated content effectively (Chen et al., 2024a; Li et al., 2024a), and training an RM with preference data is resource-intensive.\nIn this work, we investigate an alternative approach: Can knowledge derived from text-only preference data be transferred to LVLMs without additional training? Several state-of-the-art LVLMs are built upon pre-trained language models with vision encoders and adapters (Dubey et al., 2024; Team, 2025; Lu et al., 2024). This architectural design suggests that textual preferences learned by text-based RMs may potentially integrate into LVLMs through parameter merging.\nBuilding on this idea, we propose merging LVLMs with text-based RMs to create vision-language reward models (VLRMs), as illustrated"}, {"title": "2 Related Work", "content": "Preference Dataset A common approach to train a reward model is to use the Bradley-Terry model (Bradley and Terry, 1952), which relies on paired data for learning. In NLP, many high-quality preference datasets are already available (Stiennon et al., 2020; Bai et al., 2022; Ethayarajh et al., 2022; K\u00f6pf et al., 2023; Cui et al., 2024; Zhu et al., 2024; Wang et al., 2024). Similarly, in the vision-language domain, several preference datasets have been introduced (Yu et al., 2024b,c; Chen et al., 2024b; Wijaya et al., 2024; Li et al., 2024c; Zhou et al., 2024; Xiao et al., 2024). In this work, we explore the potential of transferring textual preferences to LVLMs in a training-free manner, specifically through model merging.\nLVLM-as-a-Judge & Evaluation LVLM-as-a-Judge refers to utilizing strong large vision-language models for evaluation and judgment. These LVLMs can be either closed-source (OpenAI, 2023; Hurst et al., 2024; Team et al., 2024; Anthropic, 2024) or open-source (Lee et al., 2024; Dubey et al., 2024; Deitke et al., 2024; Team, 2025). To assess LVLMs as generative reward models, Chen et al. (2024a) established benchmarks and found that LVLMs exhibit high agreement with humans in pairwise comparison judgments, but perform poorly in scoring evaluation and batch ranking tasks. Recently, VL-RewardBench (Li et al., 2024a) introduced challenging cases and complex multimodal reasoning tasks, revealing that most off-the-shelf LVLMs struggle with such evaluations.\nModel Merging Model merging is a common, training-free method for combining skills from multiple models within the parameter space. A basic approach involves simple weighted averaging (Wortsman et al., 2022), while more advanced techniques have been developed (Yadav et al., 2024; Yu et al., 2024a; Yang et al., 2024). These techniques have already proven effective in reward modeling (Rame et al., 2024; Lin et al., 2024) and LLM-as-a-judge (Kim et al., 2024) in NLP. Recently, REMEDY (Zhu et al., 2025) introduced strategies for merging LVLMs. In contrast, our work focuses on merging textual reward models into the language modeling components of LVLMs."}, {"title": "3 Methodology", "content": "We propose a training-free method to transfer textual preferences from a text-based RM )RM to a LVLM LVLM through model merging.\nSince both models originate from the same pre-trained language model PRE, we merge modules that appear in both models and preserve the LVLM's vision capabilities and text-based RM reward function, resulting in a VLRM that can assess textual and visual content without additional training. Below, we outline the components and merging strategies involved."}, {"title": "3.1 Model Components", "content": "The pre-trained language model consists of:\n$\\Theta^{PRE} = \\{\\Theta_{emb}^{PRE}, \\Theta_{trans}^{PRE}, \\Theta_{lm}^{PRE}\\}$,\nwhere $\\Theta_{emb}^{PRE}$ is the embedding layer, $\\Theta_{trans}^{PRE}$ is the transformer, and $\\Theta_{lm}^{PRE}$ is the language modeling head, which maps the final hidden state of the transformer to the vocabulary.\nThe LVLM expands upon this with:\n$\\Theta^{LVLM} = \\{\\Theta_{venc}^{LVLM}, \\Theta_{adapt}^{LVLM}, \\Theta_{emb}^{LVLM}, \\Theta_{trans}^{LVLM}, \\Theta_{lm}^{LVLM}\\}$,\nwhere $\\Theta_{venc}^{LVLM}$ is the vision encoder, and $\\Theta_{adapt}^{LVLM}$ is the adapter that integrates the vision encoder outputs into the language model.\nSimilarly, the text-based RM is defined as:\n$\\Theta^{RM} = \\{\\Theta_{emb}^{RM}, \\Theta_{trans}^{RM}, \\Theta_{rm}^{RM}\\}$,\nwhere $\\Theta_{rm}^{RM}$ is the reward modeling head, which projects the transformer's final hidden state to a scalar value as the reward for a given input."}, {"title": "3.2 Merging Strategies", "content": "We explore four merging strategies.\nWeighted Averaging The weighted averaging strategy is defined as:\n$\\Theta_{trans}^{MERGE} = \\lambda \\cdot \\Theta_{trans}^{LVLM} + (1 - \\lambda) \\cdot \\Theta_{trans}^{RM}$,\nwhere \\lamba is a hyperparameter that controls the weight distribution between the two terms.\nTask Arithmetic Task arithmetic strategy is defined as:\n$\\Delta\\Theta_{trans}^{LVLM} = \\Theta_{trans}^{LVLM} - \\Theta_{trans}^{PRE}$\n$\\Delta\\Theta_{trans}^{RM} = \\Theta_{trans}^{RM} - \\Theta_{trans}^{PRE}$,\n$\\Theta_{trans}^{MERGE} = \\lambda \\Delta \\Theta_{trans}^{LVLM} + \\lambda \\cdot \\Delta \\Theta_{trans}^{RM}$,\nwhere $\\Delta \\Theta_{trans}^{LVLM}$ represents the task vector derived from instruction tuning, and $\\Delta \\Theta_{trans}^{RM}$ is the task vector obtained from reward modeling. The hyperparameter \\lamba controls the contribution of the task vectors.\nTIES & DARE For the TIES and DARE strategies, we simplify the expression to:\n$\\Theta_{trans}^{MERGE} = f(\\Theta_{trans}^{LVLM}, d) + \\lambda \\cdot f(\\Theta_{trans}^{RM}, d)$,\nwhere f() denotes the function for trimming, selecting, and rescaling the task vector, and d is the density determining how many parameters are retained. The two strategies apply different methods for trimming, selecting, and rescaling. See Appendix A for more details on TIES and DARE."}, {"title": "3.3 Merged VLRM", "content": "The merged embedding parameters, $\\Theta_{emb}^{MERGE}$ are obtained following standard embedding merging techniques outlined in MergeKit (Goddard et al., 2024), as detailed in Appendix A.\nFinally, the merged VLRM $\\Theta^{MERGE}$ is obtained by combining several components:\n$\\Theta^{MERGE} = \\{\\Theta_{venc}^{LVLM}, \\Theta_{adapt}^{LVLM}, \\Theta_{emb}^{MERGE}, \\Theta_{trans}^{MERGE}, \\Theta_{rm}^{RM}\\}$,\nAs a result, the merged VLRM can be used to provide rewards for both text and image content."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Models", "content": "In this paper, we employ Llama-3.2-11B-Vision-Instruct (Dubey et al., 2024) as our LVLM, referred to as Llama-3.2-Vision. For text-based RMs, we use Llama-3.1-Tulu-2-8B-uf-mean-rm (Ivison et al., 2024) and Llama-3.1-Tulu-3-8B-RM (Lambert et al., 2024), which we denote as Tulu-2.5-RM and Tulu-3-RM, respectively. All models derive from the same pre-trained language model Llama-3.1-8B. Our main results focus on Tulu-2.5-RM since it outperforms Tulu-3-RM on several VQA tasks with text-based input. Please refer to Appendix E for the model details."}, {"title": "4.1.2 Model Merging", "content": "We use MergeKit for model merging and apply several techniques: weighted averaging, task arithmetic, TIES, and DARE-labeled as Linear, Task Vec., TIES, and DARE, respectively. Additionally, we explore combining DARE with task arithmetic and TIES for a more thorough analysis. To determine the optimal merging hyperparameters, we conduct a hyperparameter search and sample 400 instances from the RLAIF-V (Yu et al., 2024c) training set as our validation set. More details are provided in Appendix A."}, {"title": "4.2 Reward Model Evaluation", "content": ""}, {"title": "4.2.1 VL-RewardBench", "content": "We assess the merged VLRMs using VL-RewardBench (Li et al., 2024a), a benchmark that includes three domains: general multimodal instructions, hallucination-related tasks, and multimodal reasoning tasks. Each instance includes a multimodal query that consists of an image and a user prompt, along with a chosen response and a rejected response."}, {"title": "4.2.2 Best-of-N Sampling", "content": "We assess our reward model's effectiveness in enhancing performance through reranking using Best-of-N sampling, where N = 8 in our work. This method scores and ranks responses to check if the highest-scoring one matches the correct answer. Specifically, we use Llama-3.2-11B-Vision-Instruct to generate eight candidates for the TextVQA (Singh et al., 2019) and MMMU-Pro (Yue et al., 2024b) datasets. See Appendix B for dataset details."}, {"title": "4.3 Main Results", "content": "Table 1 demonstrates the effectiveness of merging methods for combining an LVLM with a text-based RM. The baseline approaches include Llama-3.2-Vision, which utilizes the LVLM for direct scoring\u2014pairwise scoring in VL-RewardBench and verbalized scoring in Best-of-N sampling tasks. Another baseline method, Tulu-2.5-RM, utilizes the text-based RM that focuses solely on evaluating the textual elements of questions and responses. We also incorporate a Random baseline that randomly selects responses. Furthermore, we implement a Cascade approach that employs a two-stage process: it first uses the LVLM to generate text descriptions of images based on the given question, then passes these de-"}, {"title": "4.4 Analysis", "content": ""}, {"title": "Without Image Input", "content": "To further investigate whether the merged VLRMs effectively use the vision encoder, we conduct an ablation study by evaluating the models without image input. As shown in Table 2, most models with image input outperform those without it across various merging techniques. This result suggests that the vision encoder plays an active role after merging, with performance gains not solely attributed to the text-based RM. These findings highlight how merging methods effectively combine textual and visual information. However, image input does not improve performance in the MMMU-Pro Standard"}, {"title": "Effect of Merging Hyperparameters", "content": "We also investigate how merging hyperparameters impacts performance. Figure 2 presents the results of searching for d within the range [0.2, 0.4, 0.6, 0.8] and \\lamba within [0.5, 0.7, 1.0] for DARE + Task Vec.. Our findings indicate that optimal hyperparameter values vary across benchmarks. For example, in VL-RewardBench, \\lamba values do not have a significant effect, but in the MMMU-Pro standard set, we observe that \\lamba = 1.0 performs best. This variation indicates that the choice of hyperparameters affects the performance of the final merged VLRM differently across tasks. Consequently, it highlights the importance of a well-curated validation set when selecting the optimal hyperparameters, which could be further explored in future research.\nFurthermore, our results for d align with previous studies on TIES and DARE: even when task vectors are trimmed to lower rates (e.g., 0.4, 0.2), the merged VLRMs maintain strong performance, consistent with the findings on LLM merging. For further hyperparameter search results across other methods and benchmarks, refer to Appendix G.3."}, {"title": "5 Conclusion", "content": "This work presents a training-free approach for integrating text-based RMs into LVLMs through model merging. Our method enables the efficient transfer of textual preferences without the expensive multimodal preference data collection or additional training. Experimental results show that our approach outperforms LVLM scoring and text-based RMs in multimodal reward assessment tasks."}, {"title": "Limitations", "content": "Our study has several limitations. First, we focused on a specific 11B vision-language model paired with an 8B text-based reward model, primarily due to limitations in computational resources. Additionally, we focused solely on the LLaMA architecture and did not explore alternatives like Qwen (Bai et al., 2023a,b) due to the absence of a suitable Qwen-based reward model for our experiments. Furthermore, we did not perform extensive ablation studies on the validation set. Our experimental results highlight the importance of a well-curated validation set in selecting optimal hyperparameters, which could be explored further in future research. Finally, due to the sensitivity of RLHF to hyperparameter tuning and our computational constraints, we did not implement algorithms like PPO (Schulman et al., 2017). Future work could explore integrating RLHF with merged VLRMs to assess its potential impact."}, {"title": "Ethics Statement", "content": "Our approach leverages pre-trained language and reward models, which may inherit biases from the training data. While merging models can enhance efficiency, it does not inherently mitigate existing biases. We encourage further research to evaluate and address potential biases in merged models to ensure fairness across diverse user groups."}, {"title": "A Merging Details", "content": "Weighted Averaging Wortsman et al. (2022) showed that combining the weights of multiple models fine-tuned with varying hyperparameter settings often leads to improved accuracy and robustness. In this work, we employ a weighted averaging strategy as a straightforward method to merge a large vision-language model with a text-based reward model. The weighted averaging strategy is formally defined as:\n$\\Theta_{trans}^{MERGE} = \\lambda \\cdot \\Theta_{trans}^{LVLM} + (1 - \\lambda) \\cdot \\Theta_{trans}^{RM}$,\nwhere \\lamba is a hyperparameter that determines the weight distribution between the two models. We explore \\lamba values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0].\nTask Arithmetic Ilharco et al. (2023) demonstrated that the task vector, obtained by subtracting the weights of a pre-trained model from those of the same model after fine-tuning for a specific task, defines the task direction. Utilizing this task vector can improve task performance. We also apply the task arithmetic approach to develop a vision-language reward model. The task arithmetic strategy is formally defined as:\n$\\Delta\\Theta_{trans}^{LVLM} = \\Theta_{trans}^{LVLM} - \\Theta_{trans}^{PRE}$\n$\\Delta\\Theta_{trans}^{RM} = \\Theta_{trans}^{RM} - \\Theta_{trans}^{PRE}$,\n$\\Theta_{trans}^{MERGE} = \\lambda \\Delta \\Theta_{trans}^{LVLM} + \\lambda \\cdot \\Delta \\Theta_{trans}^{RM}$,\nwhere $\\Delta \\Theta_{trans}^{LVLM}$ denotes the task vector derived from instruction tuning, and $\\Delta \\Theta_{trans}^{RM}$ refers to the task vector obtained from reward modeling. The hyperparameter \\lamba controls the relative contribution of task vectors. We explore \\lamba values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0].\nTIES Yadav et al. (2024) consider the interference between parameters from different models during the model merging process. Their approach consists of three main steps. First, they prune task vector values based on magnitude, retaining only a proportion d of the task vector. Second, they resolve sign conflicts by calculating the total magnitude of parameter values in positive and negative directions and selecting the direction with the larger total magnitude. Only values that match the chosen sign are retained. Finally, they compute the mean of the retained values to determine the final parameter value. The TIES method can be simply expressed as:\n$\\Theta_{trans}^{MERGE} = f(\\Theta_{trans}^{LVLM}, d) + \\lambda \\cdot f(\\Theta_{trans}^{RM}, d)$,\nwhere f() denotes the function for trimming, selecting, and rescaling the task vector, and d is the density determining how many parameters are retained. We search for optimal values of \\lamba within the range [0.5, 0.7, 1.0] and d within the range [0.2, 0.4, 0.6, 0.8].\nDARE Yu et al. (2024a) also addresses the interference between parameters from different models during the model merging process. They randomly drop delta parameters with a probability of p and rescale the remaining ones by 1/(1-p). The DARE method can be combined with both the Task Arithmetic and TIES approaches. When combined with Task Arithmetic, a proportion p of task vectors is randomly dropped, and the remaining ones are rescaled by 1/(1 \u2013 p). When DARE is combined with TIES, a proportion p of task vectors is randomly dropped, and the sign of each parameter is determined by comparing the total magnitude in the positive and negative directions. The sign corresponding to the larger total magnitude is selected, and only values matching this sign are retained. Their mean is then computed as the final parameter value, and the result is rescaled by 1/(1 \u2013 p). The DARE method can also be expressed as:\n$\\Theta_{trans}^{MERGE} = f(\\Theta_{trans}^{LVLM}, d) + \\lambda \\cdot f(\\Theta_{trans}^{RM}, d)$,\nwhere d represents the density, determining the proportion of retained parameters, with d = 1 \u2013 p. We search for optimal values of \\lamba within the range [0.5, 0.7, 1.0] and d within the range [0.2, 0.4, 0.6, 0.8].\nMerging Embeddings We follow the embedding merging procedure from MergeKit (Goddard et al., 2024). The process is as follows:\n1. If a token exists in the pre-trained model, we use its embedding from that model.\n2. If a token appears in only one model (either the LVLM or the text-based RM), we use its embedding from that model.\n3. If a token appears in multiple models, we compute the average of its embeddings.\nNotably, the pre-trained model is not required for the weighted averaging method. Therefore, we omit the first step when applying this merging approach."}, {"title": "Merging Hyperparameter Selection", "content": "We select the merging hyperparameter by using a sampled set of 400 instances from the RLAIF-V (Yu et al., 2024c) training set as our validation set. In case of a tie in scores, an additional 100 sampled instances will be used for evaluation. Results are discussed in Appendix G.3."}, {"title": "B Dataset Details", "content": "VL-RewardBench VL-RewardBench (Li et al., 2024a) is a benchmark comprising 1,250 high-quality examples spanning three domains: general multimodal instructions, hallucination-related tasks, and multimodal reasoning tasks. Each example includes a multimodal query\u2014consisting of an image and a user prompt\u2014along with a selected response and a rejected response.\nTextVQA TextVQA (Singh et al., 2019) is a dataset designed to evaluate the ability of visual question-answering (VQA) models to read and reason about text within images. We use its validation set, which contains 5,000 instances, to assess our merged VLRMs.\nMMMU-Pro MMMU-Pro (Yue et al., 2024b) is an advanced benchmark designed to assess the understanding and reasoning abilities of multimodal models. It is derived from the original MMMU (Yue et al., 2024a) dataset and consists of two subsets: a standard set, which includes image and text queries with 10 answer options, and a vision set, which features a vision-only input scenario. In the vision set, the questions are embedded within screenshots or photos, with no explicit text provided.\nRLAIF-V RLAIF-V (Yu et al., 2024c) preference dataset is created by generating multiple candidate responses for a given prompt and image using various random seeds. Each response is divided into individual claims, which are then assessed using an open-source large vision-language model. This model assigns confidence scores to each claim, which are combined to form an overall response score. Preference pairs are generated by comparing the response scores for the same prompt, selecting the preferred response and the less favorable one based on the score differences. Pairs with significant length disparities are excluded to avoid bias. We select 400 instances from this preference dataset to serve as our validation set for selecting the hyperparameters of merging methods."}, {"title": "C Best-of-N Sampling Details", "content": "We use Imms-eval (Zhang et al., 2024) for response generation with the Best-of-N sampling technique. For the TextVQA dataset, we set both the temperature and top-p to 1.0, sampling 8 responses. To encourage concise answers, we append \u201cAnswer the question using a single word or phrase.\u201d after the generation prompt. For the MMMU-Pro dataset, we also set the temperature and top p to 1.0, with a maximum token limit of 4096, to sample 8 responses. Additionally, we apply chain-of-thought (CoT) for generating both answers and their reasoning."}, {"title": "D Prompt Template", "content": "For Best-of-N sampling using LLaMA-3.2-Vision as the generative reward model, the prompt template is provided in Table 3. For image captioning with LLAMA-3.2-Vision and reward modeling using Tulu-3-RM and Tulu-2.5-RM, the detailed prompt template can also be found in Table 3."}, {"title": "E Open-Source Model Details", "content": "Llama-3.2-11B-Vision-Instruct Llama-3.2-11B-Vision-Instruct (Dubey et al., 2024) is an 11B-parameter LVLM consisting of three main components: a vision encoder, an adapter, and a pre-trained language model. The language model is based on Llama-3.1-8B-Instruct. The adapter incorporates cross-attention layers to integrate image representations into the language model. During adapter training, the language model remains frozen, enabling seamless drop-in replacement for Llama-3.1 series models without requiring retraining.\nTulu-2.5-RM Tulu-2.5-RM (Ivison et al., 2024) is a reward model initialized from Llama-3.1-8B and fine-tuned using the Tulu 2 recipe (Ivison et al., 2023). It is adapted for reward modeling by replacing the language modeling head with a linear layer and fine-tuning it on preference data from diverse sources, including Ultrafeedback (Cui et al., 2024), Nectar (Zhu et al., 2024), HH-RLHF (Bai et al., 2022), and AlpacaFarm (Dubois et al., 2023), among others.\nTulu-3-RM Tulu-3-RM (Lambert et al., 2024) is another reward model initialized from Llama-3.1-8B and fine-tuned following the Tulu 3 recipe (Lambert et al., 2024). Like Tulu-2.5-RM, it is adapted for reward modeling by replacing"}, {"title": "F Qualitative Results", "content": "We investigate reward model behavior before and after merging, and we evaluate qualitatively on VL-RewardBench. Tables 4 and 5 present results for Tulu-2.5-RM, while Tables 6 and 7 show Tulu-3-RM. Red text indicates misalignment with the image. Before merging, the text-based reward model made incorrect predictions. After merging, the vision-language reward models correctly identified the better response. In most cases, more advanced merging methods\u2014such as task arithmetic, TIES, and DARE\u2014produce larger reward differences between chosen and rejected responses than simple weighted averaging."}, {"title": "G Full Results", "content": ""}, {"title": "G.1 Main Results", "content": "The main results of merging with Tulu-2.5-RM are discussed in Section 4.3 of the main text. As shown in Table 1, merged VLRMs consistently outperform Llama-3.2-Vision and Tulu-2.5-RM across nearly all merging methods and benchmarks. Notably, in VL-RewardBench, they show the greatest improvement in the Hallucination domain. In Best-of-N evaluation, they perform well in both TextVQA and MMMU-Pro. Additionally, merged VLRMs match or surpass the strong Cascade baseline, suggesting that merging captures more information than simply cascading two models.\nA similar trend is observed when merging with Tulu-3-RM. As shown in Table 8, merged VLRMs outperform Llama-3.2-Vision and Tulu-3-RM across most methods and benchmarks. In VL-RewardBench, they improve mainly in the General and Hallucination domains. For Best-of-N evaluation, they perform well in MMMU-Pro, but only a few achieve results comparable to Llama-3.2-Vision in TextVQA, likely due to Tulu-3-RM's weaker performance in this task. While merging with Llama-3.2-Vision enhances"}, {"title": "G.2 Without Image Input", "content": "We conduct an ablation study by evaluating models without image input. Full results with Tulu-2.5-RM are shown in Table 9. Models with image input consistently outperform those without it across various merging techniques, suggesting that the vision encoder actively contributes after merging rather than performance gains being solely due to the text-based RM. This indicates that merged VLRMs effectively utilize the vision encoder in most cases. Notably, in VL-RewardBench, merged VLRMs match or surpass those without image input, especially in the hallucination domain, where image input significantly improves performance. In Best-of-N evaluation, models with image input perform better in the TextVQA and MMMU-Pro Vision sets. However, in the MMMU-Pro Standard set, image input does not provide an advantage, likely because this set emphasizes text reasoning, where reward assessments depend more on textual coherence than visual information.\nFull results with Tulu-3-RM are shown in Table 10, following a similar trend. In VL-RewardBench, merged VLRMs outperform those without image input in the hallucination domain and are comparable to or surpass them in general and reasoning domains. Image input also enhances Best-of-N evaluation, particularly in TextVQA and MMMU-Pro Vision. However, in the MMMU-Pro Standard, image input does not provide a clear advantage, reaffirming that this set prioritizes text reasoning over visual input."}, {"title": "G.3 Effect of Merging Hyperparameters", "content": "In this study, we optimize hyperparameter merging using sampled instances from RLAIF-V. The results, based on 400 sampled RLAIF-V instances used as a validation set, are presented in Tables 12"}]}