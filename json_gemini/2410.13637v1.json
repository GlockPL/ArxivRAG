{"title": "Normalizing self-supervised learning for provably reliable Change Point Detection", "authors": ["Alexandra Bazarova", "Evgenia Romanenkova", "Alexey Zaytsev"], "abstract": "Change point detection (CPD) methods aim to identify abrupt shifts in the distribution of input data streams. Accurate estimators for this task are crucial across various real-world scenarios. Yet, traditional unsupervised CPD techniques face significant limitations, often relying on strong assumptions or suffering from low expressive power due to inherent model simplicity. In contrast, representation learning methods overcome these drawbacks by offering flexibility and the ability to capture the full complexity of the data without imposing restrictive assumptions. However, these approaches are still emerging in the CPD field and lack robust theoretical foundations to ensure their reliability. Our work addresses this gap by integrating the expressive power of representation learning with the groundedness of traditional CPD techniques. We adopt spectral normalization (SN) for deep representation learning in CPD tasks and prove that the embeddings after SN are highly informative for CPD. Our method significantly outperforms current state-of-the-art methods during the comprehensive evaluation via three standard CPD datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Change point detection (CPD) is designed to identify unexpected changes in data streams as quickly and accurately as possible. Such a problem statement has been vital for decades for various real-world problems, spanning detailed theoretical analysis and the development of diverse applied methods. The examples include industrial quality control [1], medicine [2], video surveillance [3], finance [4], climate change [5], human activity recognition [6], and more. Most of these problems fall under the unsupervised setting due to the frequent unavailability of annotated data [3], [7]. This common scenario necessitates robust techniques that can operate effectively without labeled examples.\nTraditionally, CPD problems in the unsupervised setting are solved via different theoretically-justified parametric and non-parametric approaches [8], [9]. Although these methods have a solid theoretical base, they suffer from strong assumptions about the underlying distribution and type of changes [10], [11]. For example, they look for the change in the mean or the variance in the underlying distribution [12], [13].\nSuch limitations can be handled via deep learning meth-ods: with no assumptions for input data distribution, a well-designed neural network captures the key aspects of time series structure, achieving better detection [14], [15]. Moreover, they provide the only way to work with complex high-dimensional streams [3]. For the unsupervised setting, self-supervised learning (SSL) methods provide a consistent approach for, e.g., anomaly detection, among other problems. These methods have been widely used and led to advances for specific problems and modalities [7], [16]. However, only a few papers on this topic have been published in the context of change point detection [15], [17], [18]. The problem is the complexity of the representation space in an unsupervised regime and no explanation of their behavior even for anomaly detection [19], [20]. Thus, there is no theoretically rigorous understanding of how to construct the most suitable embeddings for successful CPD.\nWe take the best of both worlds: our method improves state-of-the-art representation learning models and aligns them with classic theory from the CPD area to produce reasonable embedding space. Within our approach, the SN forces the representation learning model to keep distributional shifts in embedding space, leading to superior empirical performance. The results are supported by theoretical justification and various experiments for different datasets and models.\nOur main contributions are:\n\u2022 A proof that the usage of Spectral Normalization (SN) for neural networks ensures that changes in the raw data are preserved in the representation space, maintaining the test power for standard change point detection methods.\n\u2022 A framework based on self-supervised representation learning augmented with SN for change point detection. As base SSL methods, we consider two models: contrastive TS2Vec [7] and non-contrastive TS-BYOL our adaptation of BYOL [16] for time series data. The pipeline is presented in Figure 1.\n\u2022 Empirical evidence on the effectiveness of the proposed framework for three standard CPD datasets."}, {"title": "II. RELATED WORKS", "content": "We focus on the development of CPD methods in the unsupervised setting. It is of particular interest since it is often challenging to obtain annotated data due to the complexity and high cost of the annotation process. For example, in the oil&gas domain, well-log data segmentation [21] requires"}, {"title": "III. METHODOLOGY", "content": "Let us begin with the formal statement of change point detection problem [31]. As input, we have a sequence of independent D-dimensional observations $X_1, X_2, ...$ that possibly has a change in distribution at an unknown moment $v$. So, observations $x_1, X_2, ..., X_v$ come from one distribution, while $X_{v+1}, X_{v+2},...$ from another distribution. Without loss of generality we can assume that $x_i$ belongs to the D-dimensional sphere $S^{D-1}$. The moment $v$ of the abrupt change is called the change point, with $v = \\infty$ denoting the absence of a change. The goal is to detect the change point as precisely as possible, avoiding false alarms and long delays.\nFollowing the state-of-the-art CPD methods [15], we propose to solve this problem in the representations space instead of the raw data space. The goal of the transition into the representation space is to capture the underlying structure of the time series, discard measurement artifacts and noise, and reduce data dimensionality. The paper [32] shows that InfoNCE loss facilitates this by maximizing mutual information between the representations.\nWhile getting rid of the noise, we need to make sure that nothing important is lost we hope that the representations, if not more informative than the raw data itself, are at least as informative as the raw data in the context of the change point detection task.\nTo ensure this, we propose applying the spectral normalization (SN) technique [29] on the weight matrices $\\{W_l\\}_{l=1}^{L}$ for an L-layered representation learning network. At every training step (before the optimization step), the SN method evaluates the spectral norm of the matrix $\\lambda \\approx ||W_l||_2$ via the power iteration method [33] and then enforces it to be less than a predefined hyperparameter $c$:\n\n$W_l = \\begin{cases}\n    \\frac{c}{\\lambda}W_l, & \\text{if } c < \\lambda, \\\\\n    W_l, & \\text{otherwise}.\n  \\end{cases}$ (1)"}, {"title": "A. Spectral Normalization preserves the CPD quality", "content": "Consider the following problem setting. Let $X_1,..., X_t$ be a sequence of independent observations. Before the CP, observations come from the distribution $p_\\infty$, after from $p_0$. Both distributions and the change point $v$ are considered unknown. The change point detection problem is a problem of testing a hypothesis $H_0$ versus an alternative $H_1$:\n\n$H_0 : v = t, \\quad i.e. x_i \\sim p_\\infty \\quad \\forall 1 \\leq i \\leq t,$ (2)\n\n$H_1 : v \\in \\{1, ..., t - 1\\}, \\quad i.e. x_{<v} \\sim p_\\infty, X_{>v} \\sim p_0.$\n\nThis problem can be boiled down to a standard two-sample hypothesis testing, which is a focus of this study. We will consider the two most common approaches in this setting: the two-sample tests built on kernel-based statistics and the two-sample likelihood ratio-based tests. Below, it is shown that spectral normalization ensures the transition into the space of representations:\n\n1) in the case of kernel-based tests does not change the type II error rate of convergence to zero (for no assumptions about the data distribution at all);\n2) in the case of likelihood ratio-based tests does not decrease the power of such tests (for reasonable assumptions about the data distribution).\nBut first, let us outline the main property that the spectral normalization equips the neural networks with.\n1) Bi-Lipschitz neural networks: The paper [19] showed that the SN technique ensures the bi-Lipschitz property for the neural networks G of the form:\n\n$G(X) = h \\circ g(X),$ (3)\n\nwhere $g(X) = AX + B$, and $h(X)$ is a composition of L residual blocks:\n\n$h(X) = h_L \\circ ... \\circ h_1 (X)$,\n\n$h_l(X) = X + g_l(X), l = 1, ..., L,$ (4)\n\nwhere $g_l(X) = \\sigma(W_lX + B)$. Here, we can consider the case of consecutive mappings of $x_i$ s.t. $y_i = G(X_{i-w:i})$ and $Y = \\{y_i\\}_{i=1}^t$ or the case when $Y = G(X)$ is a single vector; i.e., the mapping $G$ may (depending on the specific CPD procedure) have an image in either $R^d$ or $R^{t\\times d}$. Inside $g_l(X)$, $\\sigma$ is a sigmoid function, but other non-linearity functions are possible. Neural networks of such form are common among the modern methods [34], [35].\nMore precisely, the authors of [19] proved the following lemma:"}, {"title": "Lemma III.1.", "content": "[19] Consider a hidden mapping $h : X \\rightarrow H$ of a form (4). If for 0 < a < 1 all $g_l$'s are a-Lipschitz, i.e., $||g_l(X) - g_l(X')||_H \\leq a||X - X'||_X \\quad \\forall (X, X') \\in X^2$. Then:\n\n$L_1||X - X'||_X \\leq ||g_l(X) - g_l(X')||_H \\leq L_2||X - X'||_X,$ (5)\n\nwhere $L_1 = (1 - a)L, L_2 = (1 + a)L$.\nLet $X = H = R^n$, and $||.||_2$ denote the Euclidean distance. Since the SN technique enforces $||W_l||_2 \\leq c$, and the Lipschitz constant of the residual block (4) is bounded by $||W_l||_2$:\n\n$||g_l(X) - g_l(X')||_2 \\leq ||W_lX - W_lX'||_2 \\leq ||W_l||_2||X - X'||_2$.\nWith $c \\leq 1$, the spectral normalized hidden mapping h ensures that the Euclidean norm is preserved; the same is true for the aforementioned mapping $G(X) = h \\circ g(X)$. Note that in finite dimensional vector spaces, all norms are equivalent; hence, the preservation of the Euclidean distance implies the preservation of any vector norm distance. We will say that the mappings for which lemma III.1 holds are distance-preserving."}, {"title": "2) Preservation of kernel distances:", "content": "In change point detection, the test statistic $S(X)$ is often a function of kernel distances between the observations $k(x, x')$ [28], [36]. Most popular kernels, such as RBF $k(x,x') = exp(-\\frac{||x-x'||^2}{\\sigma^2})$, rely on the vector norm of the difference $||x - x'||$. In the previous section, we proved the bi-Lipschitz property of the SN networks, i.e., the preservation of such distances. Then, the following statement holds."}, {"title": "Lemma III.2.", "content": "Consider two observations $x_i, x_j \\in S^{D-1}$ and their representations $y_i, y_j$. Consider the RBF kernel $k(x,x')$. SN networks preserve RBF kernel distance, i.e. $ \\exists \\underline{C}, \\overline{C} > 0$:\n\n$\\underline{C} k(x_i, x_j) \\leq k(y_i, y_j) \\leq \\overline{C} k(x_i, x_j)$.\n\nWe provide the proof in the Appendix A-C. Note that this statement holds for both $G : X \\rightarrow R^d$ and $G : X \\rightarrow R^{t \\times d}$"}, {"title": "3) Kernel-based test:", "content": "In our experiments, we consider the MMD (maximum mean discrepancy) [36] as a kernel-based statistic. This is a nonparametric probabilistic distance commonly used in two-sample tests. Given a kernel $k$ of the RKHS $H_k$, the MMD distance between two distributions $P$ and $Q$ is defined as\n\n$MMD(P, Q) = ||\\mu_P - \\mu_Q||_{H_k}^2 = $ (6)\n\n$= E_P[k(\\zeta, \\zeta')] - 2E_{P, Q}[k(\\zeta, \\xi)] + E_Q[k(\\xi, \\xi')]$,\n\nwhere $\\mu_P = E_{\\zeta \\sim P}[k(\\zeta, .)], \\mu_Q = E_{\\xi \\sim Q}[k(\\xi, .)]$ are the kernel mean embeddings of distributions $P$ and $Q$, accordingly. We use the biased empirical estimate of the MMD distance: given observations $Z = (\\zeta_1, ..., \\zeta_m) \\sim P, \\Xi = (\\xi_1,... \\xi_m) \\sim Q$\n\n$MMD(Z, \\Xi) = \\frac{1}{m^2} \\sum_{i, j} k(\\zeta_i, \\zeta_j) - $ (7)\n\n$-\\frac{2}{m^2} \\sum_{i, j} k(\\zeta_i, \\xi_j) + \\frac{1}{m^2} \\sum_{i, j} k(\\xi_i, \\xi_j).$"}, {"title": "Theorem 1.", "content": "Consider two sequences of observations $X = [x_1,..., x_n]$ and $\\hat{X} = [\\hat{x_1},..., \\hat{x_n}], X_i, \\hat{X_i} \\in S^{D-1}$. Denote by $Y, \\hat{Y}$ their images under the distance-preserving mapping G. Consider a bounded kernel $0 < k(x,x') < K$ that is preserved under G and the corresponding sample MMD statistic. The MMD-based two-sample test of level $ \\alpha$ has the same type II error rate $O(m^{-\\frac{1}{2}})$ of convergence to zero in the space of embeddings $y$ as in the space of raw observations X.\nFor proof, see Appendix A-D. Note that the MMD statistic based on RBF kernels is a particular case of Theorem 1; we use this statistic in our experiments.\nTo sum up, spectral normalization ensures that SN mappings preserve kernel distances, which, in turn, leads to the preservation of MMDb-based two-sample test properties in the latent space."}, {"title": "4) Likelihood ratio tests:", "content": "Suppose that $p_0, p_\\infty$ in the setting (2) are known. Consider the likelihood ratio of an interval $X = [x_1,..., x_t] \\in R^{t\\times D}$ in the setting (2) as its test statistic:\n\n$S(X) = \\frac{p_0^{[1:t]}(x_1,...,x_t)}{p_\\infty^{[1:t]}(x_1,...,x_t)}$ (8)\n\nThe following proposition gives sufficient conditions for the neural network $G : R^{t\\times D} \\rightarrow R^{t\\times d}$ of the form (3) to preserve likelihood ratio for elliptical distributions."}, {"title": "Proposition III.1.", "content": "Consider a sequence of independent variables $X = [x_1,..., x_t]$ distributed according to an elliptical distribution with a possible change in mean at an unknown location $v$. Consider $G(X)$ of the form (3), with $h(X)$ being an invertible function. Denote by $Y = [y_1,..., y_t]$ the transformation of the original sequence, i.e. $Y = G(X)$.\nLet $p_0$ and $p_\\infty$ be the joint PDFs tested for the original sequence X, $\\tilde{p}_0$ and $\\tilde{p}_\\infty$ be the corresponding joint PDFs for the transformed sequence Y. Then\n\n$\\frac{p_0(X)}{p_\\infty(X)} = \\frac{\\tilde{p}_0(Y)}{\\tilde{p}_\\infty(Y)}$ (9)\n\nThe proof is presented in the Appendix.\nTherefore, for the neural network $G(X)$ to preserve like-lihood ratio, it is sufficient for its component $h(X)$ to be invertible. Thankfully, the SN technique provides this property. As mentioned, the SN enforces the Lipschitz constant of $g_l$ to be not greater than $c$ for any predefined $c > 0$. The authors of [30] show that for $c < 1$, such restraint ensures the invertibility of the residual block $h_l$ (4). Hence, applying SN to all layers $h_1,...,h_L$ enforces the invertibility of $h(X)$.\nThis likelihood ratio preservation property entails the following theorem."}, {"title": "Theorem 2.", "content": "Consider a statistical CPD test based on the likelihood ratio. The proposed strategy ensures that for any elliptical distribution, the power of such test of level a preserves, i.e.,\n\n$P_{H_1}(reject H_0 | raw data space) =$ (10)\n$= P_{H_1}(reject H_0 | representation space).$\n\nThe proof is given in Appendix A-B.\nSummary. The SN technique ensures the test power-preserving property in neural networks. This property is valuable since it encourages the results of any considered statistical test of considered form not to deviate too much when carried out in the latent space instead of the observation space. Test power preservation is in some way similar to information preservation but through the lens of the change point detection task."}, {"title": "B. General pipeline", "content": "Now that we have outlined the theoretical properties of the proposed approach let us describe the general CPD pipeline depicted in Figure 1 that we follow. The pipeline consists of three stages: (1) representation construction, (2) test statistic calculation, and (3) change point detection.\nIn the first stage, following the standard sliding window technique, the raw time series data is cropped into a sequence of overlapping time intervals X of length 2w; for each interval, we consider separately its first $X_{past} \\in R^{w \\times D}$ and second part $X_{future} \\in R^{w \\times D}$. Then, each $X_{past}$ and $X_{future}$ is transformed into its representation. Denote the embeddings by ($Y_{past}, Y_{future}$).\nIn the second stage, for each pair ($Y_{past}, Y_{future}$), the test statistic is calculated. We consider two test statistics: the cosine distance and the MMD-score.\nIn the last stage, the change point detection, we estimate the change points from the obtained statistic values. We report a change at every pair of intervals where the statistic is greater than some empirically chosen threshold $\\delta$: an example is shown in Figure 2."}, {"title": "C. Models", "content": "As base models, we consider two self-supervised models: TS2Vec [7] and our adaptation of BYOL [16] for time series data. During the CPD procedure, the embeddings of TS2Vec are obtained in two modes: either an interval is mapped to its embedding, i.e., the model performs the mapping G:"}, {"title": "1) TS2Vec:", "content": "[7] leverages hierarchical contrastive loss for learning informative time series representations. The structure of the model is depicted in Figure 3.\nIn the TS2Vec encoder learning pipeline, the input time series is cropped into two overlapping subseries, creating augmented views of the original data. These samples are then encoded into timestamp-wise sequences of embeddings. The resulting embeddings are contrasted in two dimensions: temporal and instance-wise. The loss for the t-th timestamp of the i-th sample in a batch includes two corresponding terms:\n\n$L_{i,t}^{(inst)} = - log \\frac{exp(h_{i,t} \\cdot h_{i,t'})}{S^{(inst)}}$;\n\n$S^{(inst)} = \\sum_{j=1}^{B}(exp(h_{i,t} \\cdot h_{j,t}) + 1(i \\neq j) exp(h_{i,t} \\cdot h_{j,t}))$;\n\n$L_{i,t}^{(temp)} = - log \\frac{exp(h_{i,t} \\cdot h_{i,t'})}{S^{(temp)}}$;\n\n$S^{(temp)} = \\sum_{\\tau' \\in \\Omega}(exp(h_{i,t} \\cdot h_{i,t'}) + 1(t \\neq t') exp(h_{i,t} \\cdot h_{i,t'})).$\n\nHere, $h_{i,t}$ and $h_{i,t'}$ refer to the projections of representations of the same timestamp but from two different augmentations of the original sample; $S^{inst}$ is the sum over the batch; $\\Omega$ refers to the overlapping part of the two subseries.\nThe contrasting is performed hierarchically: after each iteration of loss calculation, the max pooling operation is executed along the time axis until the dimension of the time axis reduces to one. The final loss comprises of multiple terms corresponding to different levels of representations' granularity."}, {"title": "2) TS-BYOL:", "content": "We adapt the self-supervised BYOL [16] model, originally designed for images, as an alternative source of representations. The model (Figure 4) consists of two asymmetrical parts: the \"target\" and \"online\" networks. The online network has an additional prediction head and is trained via backpropagation, while the target network's weights are updated slowly using exponential moving average (EMA):\n\n$\\xi := \\beta \\xi + (1 - \\beta)\\theta,$ (11)\n\nwhere $\\xi$ and $\\theta$ denote the weights of the target and the online network, accordingly, and the hyperparameter $\\beta$ is usually close to 1. In our experiments, we set $\\beta = 0.996$.\nOnline and target networks build embeddings $h', h''$ from augmented views $X', X''$ of the same sample X. The online network minimizes the $L_2$-norm difference between normalized embeddings $\\tilde{h}'$ to match the target network's embeddings:\n\n$L(h', h'') = ||\\tilde{h}' - \\tilde{h}''||_2 = 2 - 2\\frac{\\langle h', h'' \\rangle}{||h'||_2||h''||_2}.$ (12)"}, {"title": "IV. RESULTS", "content": "A. Compared methods\n\u2022 TS-CP2 [15] is a self-supervised model intended for change point detection. It was the first method to leverage contrastive learning to obtain informative time series data representations.\n\u2022 KL-CPD [14] is a principled CPD framework that employs deep kernel learning for two-sample hypothesis testing, being another option for NN-based CPD.\n\u2022 ESPRESSO [27] is a hybrid approach that exploits both statistical and temporal shape properties in the CPD process. This method does not involve deep learning while showing decent performance.\n\u2022 A self-supervised TS2Vec [7] employs hierarchical contrastive loss for representation learning, showing state-of-the-art results in time series classification, forecasting and anomaly detection. SN-TS2Vec is our modification of the TS2Vec model: we performed spectral normalization for each of its convolutional layers.\n\u2022 An SSL approach BYOL [16] provides an additional way to obtain a representation learning model via a non-contrastive loss function. We employ SN-BYOL in some experiments to compare SN-equipped version to a vanilla one.\nSuffixes cos and MMD denote the type of test statistic that is used during the change point detection procedure: either the cosine distance or the MMD score, accordingly.\nB. Datasets\nWe provide the analysis of our method on the datasets below. These datasets are considered, following TS-CP2 [15], since we aimed to compare this SOTA model to our method. All datasets are publicly available.\n\u2022 Yahoo [38]. It is a widely used anomaly detection benchmark consisting of time series that contain metrics of the various Yahoo services with manually labeled anomalies. Following [15], we used the fourth benchmark since it includes annotations of change points.\n\u2022 HASC [39]. This dataset contains human activity data collected by three-axis accelerometers. We used the same subset as [14] and [15]. Change points in this time series indicate alternations in the type of activities (stay, walk, jog, skip, stair up, and stair down).\n\u2022 USC-HAD [40]. The dataset also contains human activity data monitored by wearable sensors. The observed data represents basic daily life activities such as walking, sitting, sleeping, etc. We followed the pipeline from [15] and combined 30 random activities from six participants, using only the data from the accelerometer.\nC. $F_1$-score for change point detection\nSince we aim to compare our model to the current SOTA in this field, TS-CP2 [15], we used their equivalent of $F_1$-score for the change point detection task as the primary metric of detection quality This procedure's inputs are a sequence of binary ground truth labels (for each interval, whether it contains a change point or not) and a sequence of predicted labels, and the output is a value between 0 and 1. The metric suggests that in order to correctly detect a change point, it is sufficient for the detection algorithm to indicate the presence of a change point in at least one of the consecutive intervals containing it. For extra details, see the TS-CP2 paper [15]."}, {"title": "D. Main results", "content": "Table II compares SN-TS2Vec to three other state-of-the-art CPD methods. For both Yahoo! A4Benchmark and USC-HAD datasets, the SN-TS2Vec approach with the cosine distance used as a test statistic achieves two of the three best results. For the HASC dataset, all TS2Vec variants show results close to KL-CPD and ESPRESSO, outperforming the TS-CP2 approach. So, SN-TS2Vec either dominates the existing methods or performs almost as well as they do.\nAs for the TS-BYOL model, even though it does not achieve SOTA performance, it shows solid results on the Yahoo!A4Benchmark dataset. Comparison of the SN-BYOL to its vanilla version confirms that the SN technique enhances model properties in the context of CPD task."}, {"title": "1) TS2Vec with and without SN:", "content": "The results of the comparison of the model modifications are presented in Table III. The experiments were conducted for three different window sizes for each dataset; the hidden representations size was set to 16 for the Yahoo and USC-HAD datasets and to 32 for the HASC dataset. On the Yahoo!A4Benchmark dataset, the SN-TS2Vec strongly outperforms its vanilla version, with the cosine distance version being slightly dominant over the MMD-score one. The results on USC-HAD also show the superiority of the SN version: two out of three best results for each dataset belong to it. On the HASC dataset, all models demonstrate comparable results.\nWe also provide the extended version of this comparison in Figure 5, considering multiple code sizes (i.e., hidden dimensions). The results on the Yahoo!A4Benchmark con-"}, {"title": "2) TS-BYOL with and without spectral normalization:", "content": "To enrich our comparison, we conducted additional experiments with an alternative self-supervised approach, TS-BYOL, on all three datasets. The basic TS-BYOL was compared to the TS-BYOL with SN. The results can be seen in Figure 7. It is clear that TS-BYOL with SN either outperforms or performs on par with its vanilla version on two of three datasets Yahoo!A4Benchmark and HASC. The results obtained on the USC-HAD dataset are different; however, it should be noted that the total number of change points in the test set of USC-HAD is the smallest among all datasets, and the standard deviation of the obtained metrics is relatively high (up to std = 0.4). Therefore, the results on USC-HAD are not as representative as those of the other two datasets."}, {"title": "E. The dynamics of the representations", "content": "We compared the dynamics of the representations for Vanilla TS2Vec and SN-TS2Vec. The experiment was performed as follows:\n1) For each change point, we sampled a subsequence of length 300 right before the CP; let us call it X.\n2) We cloned each X and replaced the last half with the subsequence from right after the CP, denoting the sample obtained by X. Thus, for each CP, we have X and X: their first halves are identical, while their last halves belong to different distributions.\n3) After that, we transformed X and X into a sequence of representations via the sliding window procedure.\n4) Finally, we calculated cosine similarities between the corresponding representations in X and X, obtaining a sequence of similarities for each CP. Supposedly, they should begin to diverge right after a CP appears.\nWe averaged the obtained similarity arrays over all the change points. The results are presented in Figure 6. For two out of three considered datasets, the embeddings obtained with SN-TS2Vec diverge faster, achieving greater dissimilarity values. This visualization confirms that SN-TS2Vec is more suitable for CPD tasks than its vanilla version."}, {"title": "VI. CONCLUSION", "content": "Our findings provide a step forward in the CPD field, bridging the gap between the expressive ability of representation learning models and the theoretical grounds of CPD problems. We showed how to design self-supervised learning models with respect to CPD task specificity. The main contribution is the usage of Spectral Normalization, which, as we theoretically and empirically demonstrate, preserves test power for various two-sample tests in latent space. Thus, the resulting representations appear to be more robust for further change detection.\nThrough a series of experiments, we confirm the effectiveness of the self-supervised approach, showing that the modern TS2Vec model outperforms current state-of-the-art approaches for CPD. Moreover, Spectral Normalization enhances the performance of TS2Vec, leading to a 5% improvement in the target F1 metrics."}, {"title": "APPENDIX A", "content": "PROOFS FROM THE SECTION III\nA. Proof of proposition III.1\nThe proof is provided for the case of Gaussian random variables, but it is clear that the similar calculations may be performed for any distribution from the elliptical family.\nX can be viewed as a sample from the matrix normal distribution [41], and the joint PDFs $p_0$ and $p_\\infty$ have the densities of the following form (with the means $M_0$ and $M_\\infty$, accordingly):\n\n$p(X|M, U, V) = $\n\n$= C. exp(-\\frac{1}{2}tr [V^{-1}(X-M)U^{-1}(X - M)]).$ (13)"}, {"title": "First, consider g(X) = AX + b.", "content": "It is a linear transformation, so the distribution of g(X) can be written out explicitly:\n\n$p(AX A|M, AUAT, V) = $\n\n$= \\frac{1}{C} exp(-\\frac{1}{2}tr [V^{-1} (X-M)^T A^T U^{-1} A^{-1} A V (X - M)]) = \\frac{1}{C} \\cdot p(X|M, U, V).$ (14)\n\nHere we let b = 0 to simplify the calculations. This implies that the linear transformation preserves the likelihood-ratio of the intervals. Now let us consider the invertible component h(X). We know that for invertible transformations the PDF of the transformed distribution is calculated as\n\n$p(y) = p(h^{-1}(y))|J_{h^{-1}}(y)|,$ (15)\n\nwhere $J_{h^{-1}}$ is the Jacobian matrix of the inverse transformation $h^{-1}$. Then the likelihood-ratio after applying h becomes\n\n$\\frac{p_0(Y)}{p_{\\infty}(Y)} = \\frac{p_0(X)|J_{h^{-1}}(Y)|}{p_{\\infty}(X)|J_{h^{-1}}(Y)|} = \\frac{p_0(X)}{p_{\\infty}(X)},$ (16)\ni.e. it remains the same."}, {"title": "B. Proof of lem\u0442\u0430 2", "content": "As was shown in Proposition III.1, spectral normalized neural networks of a form (3) preserve the likelihood ratio, i.e., $A(X) = \\frac{p_0(X)}{p_{\\infty}(X)} = \\frac{\\tilde{p}_0(Y)}{\\tilde{p}_\\infty(Y)} = A(Y)$, where A(X) and A(Y) denote the likelihood ratio in the raw data space and the representation space, accordingly. Therefore, it is true that for any function of the likelihood ratio S(A) and a predefined threshold h\n\n$S(A(X)) > h \\Leftrightarrow S(\\hat{A}(Y)) > h.$ (17)\n\nThe rejection of the null hypothesis is usually equivalent to the statistic value exceeding the threshold; hence, due to (17), the rejection of the null hypothesis in the raw data space coincides with its rejection in the representation space."}, {"title": "C. Proof of the RBF kernel distance preservation", "content": "The proof is given for $G : R^{w \\times D"}, "rightarrow R^d$; the same calculations for a two-dimensional codomain of G can also easily be conducted. Given:\n\n$\\underline{L_1} ||[x_{i-w}, ..., x_i"]}