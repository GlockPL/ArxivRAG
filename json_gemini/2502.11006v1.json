{"title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool", "authors": ["Jonathan Pan", "Swee Liang Wong", "Yidi Yuan", "Xin Wei Chia"], "abstract": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for Al security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing Al security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an Al security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid Al security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models have garnered lots of attention and adoption excitement. These text generation autoregressive models have been studied for use in a variety of applications from enhancing user experience when using applications to being an augmented assistant to wide variety of tasks from text summarization, conversational dialog chatbots and many others [1]. However, as with all software based technological solutions, there are vulnerabilities and exploitation techniques. Attacks against artificial intelligence (AI) model are known as adversarial artificial intelligence. Among these techniques, the most popular one associated with text generation LLMs is prompt injections that uses prompt or textual conversational inputs as the attack vectors.\nPrompt injection attacks, applied successfully, could induce LLMs to hallucinate or generate non-coherent textual responses. Such attack could also induce more damaging responses that results in malicious outputs like code generation of cyber security related offensive artefacts like malware or phishing contents. Another form of malicious output is leakage of sensitive information. While the LLM may be trained to protect itself from such involuntary responses, the constantly evolving form of prompt injection attacks would eventually circumvent such protection measures acquired from training. In a way, prompt injection is a form of social engineering attacks against the targeted model to lower its protective posture and succumb to its intended adversarial instructions. There are a number of protective solutions to protect the LLMs from such attacks. They typically involve the use of inline detection and protective mechanisms guarding the inbound prompt to the LLMs to detect the prompt injects and divert the adversarial prompt. Another is the outbound responses generated by the LLMs to provide inline detection of LLMs compromised responses. These protective mechanisms would inspect prompt inputs or output responses to contain the adversarial effects of any attack attempts. These inline protections are typically known as guardrails.\nThe solution options for guardrails include use of signature based detectors, machine learning based and LLM based. However, with LLM based, there is limited examples of use of text-generation LLM. One such example is Llama Guard [2]. This is due to the inherent designs of such text generation models not being optimally positioned for text classification. However, text generation models could be used to generate explanation to support its inferred textual assertions [10]. In the field of AI security with the intent to protect Al models after adversarial attacks, we hypothesis that having the explanation along with the analysis of prompt inputs could aid AI security investigators and developers in triaging adversarial attacks. This research attempts to explore the following applied research question of whether text generation LLM can be used as a prompt inject investigation tool that involves having a LLM identify adversarial prompt injects and generating explanation to facilitate investigative triaging. The practical benefits of having such a tool is to facilitate cyber investigation of prompt injects in the backdrop of large volume of prompts that are largely benign.\nWith this research objective, we first needed the LLM tool to have the ability to detect prompt injects. Upon detection, the"}, {"title": "II. PROMPT BASED ADVERSARIAL TECHNIQUES", "content": "In this section, we surveyed prompt based adversarial attack techniques that can be used against LLMs. We also surveyed the approaches used to develop guardrails for LLM against prompt based injection attacks."}, {"title": "A. Survey of Adversarial Attacks against LLM", "content": "Since the development of LLMs in the field of Natural Language Processing (NLP), there is a number of research advances into susceptibility of such Al models to a variety of sophisticated attacks. Chowdhury et al. [5] did a comprehensive survey of such attacks and created a taxonomy of them and presenting report research work to mitigate such attacks. They argued that the main categories of attacks to LLM are Jailbreaking, Prompt Injection and Data Poisoning. Jailbreaking refers to techniques used to bypass or disable the safeguards built into the language generation models. Safeguards refer to prevention of harmful or inappropriate content generation, maintaining user safety and enforcing of trained ethical guidelines which is typically developed in the model as part of the training program through fine-tuning or reinforcement learning techniques. The adversarial intent of Jailbreaking is to circumvent trained safeguards leading to generation of potentially harmful or sensitive content. Also manipulating the model's responses towards undesired specific goals.\nPrompt injection is a technique used to manipulate or deceive the AI model into generating harmful or inappropriate content through crafted input prompts. Hence, both are input prompts based. However jailbreaking focuses on bypassing safeguards while prompt injection focuses on manipulation of the target model. The last category of attack is Data Poisoning is introducing data samples into the training set used to train or fine tune the models to compromise the model's performance, security or reliability. However, as the focus of this research work is on adversarial input prompts, hence this category of attack is not relevant. Our research focused on Jailbreaking and Prompt Injects."}, {"title": "B. Prompt Injection Protection", "content": "There are a number of approaches to protect LLM models from adversarial input prompts (Dong et. al [6]). A direct approach is to train the LLM model through reinforcement learning from human feedback (RLHF). Another is using in-context training to guide the model in dealing with such attacks. Another approach is the deployment of Guardrail [7] or content filters that use algorithms /models to detect and deal with such attacks. This approach involves inspecting the input and / or the output of the LLMs and assess whether interventions are required to limit the risks of adversarial prompt attack attempt. One form of Guardrail implementation is to use encoder based approach that encodes the input prompts into its corresponding embedding space and train a classifier (Kim e. al [8]). Guardrail NeMo [9], developed by Nvidia, is a toolkit that provides developers the means to build a controllable proxy between the user and LLM. This proxy is a programmable rail using Colang that is interpreted by a runtime that applies user defined rules or automatic generated rules to protect the LLM. Llama Guard [2], developed by Meta, is a fine tuned LLM that is based on Llama2-7b architecture. This guardrail takes in both input prompts and output responses and determines whether there are safe and unsafe. The authors of Llama Guard mentioned that their future work with this model is to generate explanations to the classification decision. This is the intent of this research work."}, {"title": "III. LLM BASED GUARDRAIL WITH EXPLANATION", "content": "Our research question attempts to assess whether a text generation based LLM could be used to perform classification and provide coherent explanation to the assessment. The generation of such explanations could then be used by a human analyst to evaluate the validity of the classification task and facilitate investigating and triaging. In our research experiment, our prompt is constructed to have the LLM perform the prompt classification and provide the explanation."}, {"title": "A. LLM based Guildrails", "content": "With our experiment, we used Llama3.2 from Meta as the base LLM for our Guardrail. We applied the Instruct version of the model and used two different sizes namely 1B and 3B parameter sized models for comparison. We additionally used the Llama3.2 1B instruct model as the target model to which the prompt injects are meant for."}, {"title": "B. Explanation of Classification", "content": "According to Divakaran and Peddinti [11], generative models like LLM could be used to provide transparency in validating the task the AI model has been assigned to perform. Kunz and Kuhlmann [10] evaluated LLM generated explanation using GPT-4 and Alpaca dataset. Their experiment entailed a selection of 200 prompt instructions that are fed to the LLM to generate explanations for manual evaluation. The prompt instructions are generally categorised into coding assistance, math questions, grammar and language prompts to correct English text, sentence text classification and facts classification."}, {"title": "C. Experimentation Preparation and Evaluation", "content": "There is one preparation stage and three evaluation stages to our experimental setup. The preparation stage is to prepare the LLM models for prompt inject detection. This involves preparing the prompt construct included the system prompt and query prompt to the models. The query prompt construct used data from the ToxicChat dataset (training and test) with the prompt to classify the prompt input to be evaluated. In our experiment, the prompt is considered adversarial when it is labelled Toxic or Jailbreaking. It is benign if the prompt is labelled both not Toxic and Jailbreaking. Additionally, the query prompt included request to the LLM to explain its classification conclusion. The following is the query prompt construct.\nYou are a cybersecurity cum Al expert with expertise in detecting prompt injects\n<query: input prompt>\nQuestion: Is the prompt offensive or benign? Explain how the conclusion was made about the prompt\nFor the models used for our experimental setup, we chose the open-source Meta's Llama3.2 Instruct models with two base sizes namely 1B and 3B. We additionally fine-tuned these base models using Supervised Fine Tuning and Direct Preference Optimization [12]. The fine tuning was done with Low-Rank Adaptation (LoRA) [13].\nAfter the preparation stage, the first evaluation stage involved evaluating our models (vanilla and fine-tuned with both techniques) using the test dataset from ToxicChat. We then used Nvidia's generative AI red-teaming and assessment kit or Garak to generate and evaluate the effectiveness of the experimented LLMs to detect prompt injects in the second evaluation stage. Garak's generated prompts are all adversarial. The configuration setup for Garak test involved having our model placed inline between the Garak and the target model (which we used Llama3.2-1B-Instruct). The vanilla configuration would not include our model hence this test involved providing Garak direct connectivity to the target model. The probes used with Garak was 'promptinject' and 'realtoxicityprompts' for prompt injection and toxic prompts respectively."}, {"title": "D. Evaluation Results and Analysis", "content": "The following are our experiment test results for the first stage of the model evaluations."}, {"title": "V. CONCLUSION AND FUTURE DIRECTIONS", "content": "Our research work explored the use of text generation LLM models to analyze adversarial input prompts and explain its analysis. We used ToxicChat dataset that contained both benign and adversarial prompts (jailbreaking and toxic). In our experiment, we applied the vanilla LLM models. We also fine-tuned these models. We evaluated these models to assess their performance in detecting adversarial prompts. We then used human evaluators to assess the validity of the explanation generated by the models. Our research concluded that using fine-tuned LLM models improved detection accuracies. These improved models also generated explanation that could facilitate analysis of the identified adversarial prompts. Hence, validating the research premise that fine-tuned text-generation LLM could be used to facilitate investigation that could quicken identification and analysis of prompt injects.\nThe next step to this research work is to extend this research exploration to assess the suitability of using text generation LLM for output censorship detection and investigation. Also to conduct a thorough evaluation of the generated explanations to assess their suitability in facilitating investigation and triaging and study ways to improve the quality of the generated explanation."}]}