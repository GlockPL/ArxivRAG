{"title": "ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis", "authors": ["Xu Wang", "Jiaju Kang", "Puyu Han"], "abstract": "We present ECG-Expert-QA, a comprehensive multimodal dataset designed for evaluating diagnostic capabilities in ECG interpretation, integrating real clinical data with systematically generated synthetic cases. The dataset encompasses six fundamental diagnostic tasks, comprising 47,211 meticulously curated question-answer pairs that span a spectrum of clinical scenarios, from basic rhythm analysis to complex case interpretation. By simulating challenging clinical cases through a rigorous medical knowledge-guided process, ECG-Expert-QA not only enhances the availability of annotated diagnostic data but also significantly increases the complexity and diversity of clinical presentations, including rare cardiac conditions and temporal progression patterns. This design enables comprehensive evaluation of medical language models across multiple dimensions, including diagnostic accuracy, clinical reasoning, and knowledge integration. To facilitate global research collaboration, ECG-Expert-QA is available in both Chinese and English versions, with rigorous quality control ensuring linguistic and clinical consistency. The dataset's challenging diagnostic tasks, which include interpretation of complex arrhythmias, identification of subtle ischemic changes, and integration of clinical context, establish it as an effective benchmark for advancing AI-assisted ECG interpretation and pushing the boundaries of current diagnostic models. Our dataset is open-source and available at https://github.com/Zaozzz/ECG-Expert-QA.", "sections": [{"title": "INTRODUCTION", "content": "This study systematically investigates the technological advancements and evaluation challenges of multimodal large language models (LLMs) in the field of intelligent electrocardiogram (ECG) diagnosis, proposing innovative solutions. Current research faces three systemic challenges: first, traditional manual evaluation methods suffer from efficiency bottlenecks, with their resource-intensive nature, subjective dependency, and lack of scalability severely constraining large-scale model validation; second, existing datasets exhibit significant deficiencies in case complexity, clinical scenario coverage, and multimodal reasoning capability assessment; third, linguistic homogeneity hinders the validation of applicability in cross-cultural medical settings, particularly lacking in-depth characterization of rare pathological conditions and ethical risks."}, {"title": "RELATED WORK", "content": "The MIT-BIH [1] Arrhythmia Database is an early representative dataset, containing 48 half-hour dual-channel dynamic ECG recordings from 47 patients, covering various types of arrhythmias. The MIT-BIH database is not only the first publicly released standardized ECG test dataset, but also specifically used for evaluating the performance of arrhythmia detection systems. All records were independently annotated by multiple cardiologists. Although the MIT-BIH database has only 48 records and 47 patients, there are certain limitations in data scale, time span, testing equipment, and sample diversity. However, the MIT-BIH database laid an important foundation for the development of subsequent ECG datasets, promoting the release of ECG datasets for various application scenarios and expanding needs.\nThe MIMIC-IV [2], [3] dataset compensates for the limitations of MIT-BIH in terms of data scale, time span, testing equipment, and sample diversity. This dataset includes a large amount of physiological data and diagnostic information from critically ill patients, although it lacks normal samples, it holds extremely high application value in clinical research. The PTB-XL [4], [5] dataset was constructed by Patrick Wagner and others, containing 21,837 12-lead ECG records, covering"}, {"title": "METHODS", "content": "The text descriptions in the dataset come from the MIMIC-IV-ECG [26] dataset and have been filtered based on the dataset's CSV annotations. The specific steps are as follows:\n\u2022\n\u2022 Similarity Calculation: Cosine similarity from sklearn.metrics.pairwise [27], [28] is used to compute text similarity, and data with a similarity greater than 0.8 are excluded to enhance the dataset's diversity.\n\u2022 Report Quantity Screening: Reports with fewer than four records are excluded to ensure the completeness of the information, thereby enhancing the model's judgment and training capabilities.\n\u2022 Patient Case Summary: The dataset is organized by patient, with all cases for each patient being aggregated. Only patients with more than four cases are selected, creating comprehensive patient profiles. This method allows the model to analyze data more thoroughly, considering ECG variations at different time points, thus improving its analytical capability [29].\nThis study proposes three main dataset generation methods: expert knowledge-guided professional knowledge assessment, cross-modal diagnosis in complex medical environments, and medical risk assessment. Specifically, the expert knowledge-guided method aims to generate professional question-answer pairs by guiding large models with specialized knowledge; cross-modal diagnosis transforms semi-structured data such as electrocardiograms (ECG) into textual descriptions to build diagnostic-related question-answer pairs; medical risk assessment utilizes medical diagnostic results to generate question-answer pairs involving medical counterfactuals, ethical harms, and patient informed rights. Each method employs different input forms and interaction strategies with the model to ensure that the generated datasets effectively support tasks such as knowledge learning in the medical field, pathological reasoning, and multi-turn dialogues. These methods not only significantly enhance the application capabilities of large models in specialized fields but also improve the system's adaptability and accuracy in complex medical environments.\nExpert Knowledge-Guided Professional Knowledge\nThis method builds a corpus of medical question-answer pairs by conducting continuous dialogues with multiple existing high-quality large language models (LLMs), such as GPT-4. The specific process includes deploying local large models"}, {"title": "EXPERIMENTS", "content": "Evaluation Metrics: Evaluation metrics provide objective means to assess the quality of generated text by comparing it to reference texts. They evaluate aspects such as fluency, semantic accuracy, and content coverage. Below is a concise description of key metrics like BLEU, METEOR, NIST, and ROUGE, highlighting their unique strengths in text evaluation.\n\u2022\nBLEU (Bilingual Evaluation Understudy): BLEU is\nan automated metric for evaluating machine translation\noutput by comparing the generated translation with a\nset of reference translations. BLEU focuses on precision\n(how many n-grams in the generated text match n-grams\nin the reference texts) and applies a brevity penalty to\ndiscourage excessively short translations. The idea is that\na good translation should contain most of the n-grams\nfrom the reference translations without overly repeating\nwords.\nFormula:\n$$BLEU = BP \\cdot exp \\left ( \\sum_{n=1}^{N} w_n \\cdot log p_n \\right )$$\nwhere:\nBP is the Brevity Penalty, which adjusts for transla-\ntions that are too short. It is computed as:\n$$BP = \\begin{cases} 1 & \\text{if } c > r\\\\ exp \\left ( 1 - \\frac{r}{c} \\right ) & \\text{if } c \\leq r \\end{cases}$$\nwhere c is the length of the candidate (generated)\ntranslation, and r is the effective reference length\n\u2022\nVariants of BLEU:\nBLEU@1 (1-gram): Measures precision for single\nwords (1-grams). This metric is useful for evaluat-\ning the accuracy of word choices in the generated\ntranslation. The precision is calculated as:\n$$P_1 = \\frac{\\text{Number of matching 1-grams}}{\\text{Number of 1-grams in the generated translation}}$$\nBLEU@5 (5-gram): Measures precision for 5-\ngrams, which are sequences of 5 consecutive words.\nBLEU@5 places more emphasis on evaluating the\nfluency and grammatical structure of the generated\ntranslation. It ensures that not only individual words\nbut also longer word sequences match the reference\ntranslations.\n$$P_5 = \\frac{\\text{Number of matching 5-grams}}{\\text{Number of 5-grams in the generated translation}}$$\n\u2022\nMETEOR (Metric for Evaluation of Translation with\nExplicit ORdering): METEOR was developed to address\nsome of the limitations of BLEU, such as its inability to\nhandle synonyms and word order variations. METEOR\nevaluates translation quality by considering word overlap,\nsynonym matching, stemming (reduction to base forms\nof words), and word order. METEOR has been shown\nto correlate better with human judgment than BLEU,\nespecially when it comes to evaluating readability and\nsemantic correctness.\n$$METEOR = F \\cdot (1 - \\alpha \\cdot penalty)$$\nwhere:\nF is the harmonic mean of precision and recall,\nsimilar to the F1 score in classification tasks, which\ncombines how many words from the generated trans-\nlation are correct (precision) and how many relevant\nwords are retrieved (recall).\n$$F = \\frac{10 \\cdot \\text{precision} \\cdot \\text{recall}}{9 \\cdot \\text{precision} + \\text{recall}}$$\n\u2022 ROUGE (Recall-Oriented Understudy for Gisting\nEvaluation): ROUGE is a set of metrics used pri-\nmarily for evaluating automatic summarization systems,\nthough it is also used for machine translation and other\nNLP tasks. Unlike BLEU, which emphasizes precision,\nROUGE focuses on recall, evaluating how much of the\nimportant content in the reference summary is captured\nby the generated summary.\nROUGE-1: Measures the recall of individual words\n(1-grams). This metric evaluates how much of the\nvocabulary from the reference summary is captured\nin the generated summary.\n$$ROUGE-1 = \\frac{\\sum_{n=1}^N \\text{Recall of 1-grams}}{n}$$\nROUGE-2: Measures the recall of 2-grams, which\nare pairs of consecutive words. ROUGE-2 is more\nsensitive to the coverage of content and is useful\nfor assessing how well the summary covers the\nimportant content of the reference.\n$$ROUGE-2 = \\frac{\\sum_{n=1}^N \\text{Recall of 2-grams}}{n}$$\nThrough these metrics, we can comprehensively evaluate\nthe quality of generated text, ranging from lexical accuracy to\nsemantic coherence and content coverage. The combined use\nof these metrics helps to identify the strengths and weaknesses\nof models, providing valuable information for performance\noptimization.\nModel Selection: Given the computational and financial\nlimitations of this study, we focused our practical tests on a\ncarefully chosen subset of the dataset, specifically targeting\nprofessional knowledge related to ECG analysis. This ap-\nproach allowed us to evaluate the performance of various mod-\nels under realistic constraints while ensuring relevance to criti-\ncal medical applications. We selected a diverse range of state-\nof-the-art market models for evaluation, encompassing both\nwidely recognized and emerging systems, including ChatGPT-4,\nGLM-4-Air, ChatGPT-3.5-turbo, Qwen-plus, Gemini-Pro,\nClaude 3, Llama 3.1 8B, NVIDIA Nemotron, 360GPT, and\nDeepSeek-Chat. This selection reflects a balance of established\nbenchmarks and innovative solutions, providing a comprehen-\nsive perspective on their capabilities and limitations within the\ndomain of ECG analysis.\nExperiments: We implemented a consistent prompt design\nacross all large models, ensuring uniformity in the input\nstructure. This approach allows for a fair and systematic\nevaluation by minimizing the variability introduced by prompt\ndifferences. The outputs generated by the models were then\nassessed against our established ground truth data, allowing\na precise and objective comparison. This method not only\nstandardizes the evaluation process, but also ensures that\nperformance differences among models can be attributed to\ntheir inherent capabilities rather than inconsistencies in input\ndesign. By maintaining a consistent input format, we ensure re-\nlible and reproducible results, providing meaningful insights\ninto the strengths and weaknesses of each model."}, {"title": "RESULT", "content": "In this study, we constructed the ECG-Expert-QA dataset and evaluated it using various modern medical language models. The experimental results indicate significant differences in the performance of different models on ECG diagnostic tasks. In terms of evaluation metrics such as BLEU@1, METEOR, NIST, and ROUGE-1, ChatGPT-4 performed exceptionally well, particularly in generating medically relevant content, with noticeably higher semantic accuracy and content coverage compared to other models. Additionally, after multiple rounds"}, {"title": "CONCLUSION", "content": "The ECG-Expert-QA dataset presented in this study not only provides a diversified and challenging evaluation platform but also holds significant importance in testing the diagnostic capabilities, clinical reasoning, and cross-modal information integration of medical language models. When compared with existing medical datasets, this dataset demonstrates unique value through multidimensional testing in pathology analysis, case complexity, and ethical decision-making. Nevertheless, as AI technology advances, future research should focus on enhancing dynamic evaluation capabilities in clinical workflows, optimizing multilingual support, and verifying its applicability in real-world clinical environments. In conclusion, the ECG-Expert-QA dataset provides a crucial reference for building medical AI evaluation systems and contributes to the ongoing development of intelligent medical diagnostic technologies."}]}