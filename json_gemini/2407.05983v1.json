{"title": "Towards A Comprehensive Visual Saliency Explanation Framework for Al-based Face Recognition Systems", "authors": ["YUHANG LU", "ZEWEI XU", "TOURADJ EBRAHIMI"], "abstract": "Over recent years, deep convolutional neural networks have significantly advanced the field of face recognition techniques for both verification and identification purposes. Despite the impressive accuracy, these neural networks are often criticized for lacking explainability. There is a growing demand for understanding the decision-making process of AI-based face recognition systems. Some studies have investigated the use of visual saliency maps as explanations, but they have predominantly focused on the specific face verification case. The discussion on more general face recognition scenarios and the corresponding evaluation methodology for these explanations have long been absent in current research. Therefore, this manuscript conceives a comprehensive explanation framework for face recognition tasks. Firstly, an exhaustive definition of visual saliency map-based explanations for AI-based face recognition systems is provided, taking into account the two most common recognition situations individually, i.e., face verification and identification. Secondly, a new model-agnostic explanation method named CorrRISE is proposed to produce saliency maps, which reveal both the similar and dissimilar regions between any given face images. Subsequently, the explanation framework conceives a new evaluation methodology that offers quantitative measurement and comparison of the performance of general visual saliency explanation methods in face recognition. Consequently, extensive experiments are carried out on multiple verification and identification scenarios. The results showcase that CorrRISE generates insightful saliency maps and demonstrates superior performance, particularly in similarity maps in comparison with the state-of-the-art explanation approaches.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed great advances in face recognition due to the rapid development of deep learning techniques. Current deep learning-based face recognition systems achieve exceptional performance on established benchmarks and have been widely deployed in several applications, including but not limited to access control and surveillance. However, the predictions made by these systems tend to be challenging to interpret. The deployment of such biometric systems poses a potential threat to privacy and data protection, resulting in serious public concern. To address these issues, it is essential to comprehend and explain the behavior of face recognition systems, thereby improving their performance and making them more widely accepted in society.\nEarly on, some studies [20, 38, 47] have exposed the bias problem of specific deep learning-based face recognition models concerning ethnicity, gender, and age. Lu et al. [23] further enhanced their transparency by investigating the performance in the presence of various realistic influencing factors. However, these studies primarily focus on revealing the weaknesses of face recognition systems, while overlooking the explanation of the decision-making process. Existing deep learning-based face recognition systems often rely on a complicated and unintuitive process to reach a final decision, often referred to as a \"black box\". The lack of interpretation of these decisions can undermine user trust and hinder the governance of face recognition technology.\nIn recent years, visual saliency algorithms have emerged as the prevailing approach to explain AI-based decision models in vision tasks, highlighting internal neural network layers [2, 30, 43] or crucial pixels in the input image that"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Visual Explanation via Saliency Maps", "content": "In the field of explainable artificial intelligence (XAI), visual saliency algorithms have been widely used to explain decision systems in vision tasks that rely on deep learning techniques. A saliency map is essentially an image where each pixel value represents the importance of the corresponding pixel. This map helps identify the significant areas of an input image that contribute to the final output of a \"black-box\" model. In general, there are two types of methods to create such saliency maps.\nThe first category of methods involves backpropagating an importance score from the model's output to the input pixels through the neural network layers. Notable examples includes Gradient Backpropagation [36], Layer-wise Relevance Propagation [3], Class Activation Maps (CAM) [46], and Excitation Backpropagation [44]. Many of these methods require access to the intrinsic architecture or gradient information of the model. Grad-CAM [35] and Grad-CAM++ [5] generalized CAM to be applied to arbitrary convolutional neural networks (CNNs) by weighing the feature activation values with the class-specific gradient information that flows into the final convolution layer. Considering their fame and flexibility in CNNs-based models, this manuscript adapts them to the explainable face recognition problem to compare with our proposed method.\nThe second category of methods performs random perturbations on the input image, such as adding noise or occlusion, and produces saliency maps by observing the impact on the model's output [6, 10, 31, 33, 43]. For example, Zeiler et al. [43] masked square parts of an image with a sliding window and determined the importance by observing the drop in classification accuracy. Ribeiro et al. [33] proposed an interpretable approximate linear decision model (LIME) in the vicinity of a particular input, which analyzes the relation between the input data and the prediction through a perturbation-based forward propagation. The RISE [31] algorithm generates random masks, applies them to the input image, and utilizes the output class probabilities as weights to compute a weighted sum of the masks as a saliency map."}, {"title": "2.2 Explainable Face Recognition", "content": "While most XAI techniques involving saliency are developed for image classification, there is a growing demand for explanation methods in other image understanding tasks, such as object detection [32] and image similarity search and retrieval [8, 14, 37]. In face recognition, earlier endeavors [4, 40] primarily adapted saliency-based explanation algorithms [31, 34, 35, 44] from classification tasks. Alternative research directions focus on face verification models that are explainable by themselves, often referred to as intrinsic explanation methods. For example, Yin et al. [42] designed a feature activation diverse loss to encourage learning more interpretable face representations. Lin et al. [22] proposed a learnable module that can be integrated into face recognition models and generate meaningful attention maps. Xu et al. [41] leveraged a face reconstruction module to localize discriminative face regions. However, these self-explained models need to be trained exclusively and thus are impractical for third-party deployed recognition systems. Instead, recent studies offer more flexible solutions by leveraging gradient backpropagation [17, 24], which do not need to access or retrain the face recognition model. Another category of methods provides purely \"black-box\" explanations for arbitrary face recognition models. Mery et al [28, 29] introduced several perturbation-based methods to create explainable saliency maps without altering or retraining the model, yielding visually promising results. xFace [19] further improved them by applying more systematic occlusions to inputs and measuring the feature distance deviations."}, {"title": "3 Proposed Method", "content": ""}, {"title": "3.1 Definition of Explanation for Face Recognition", "content": "A face recognition system gives different types of decisions depending on whether it is employed in verification and identification scenarios. In face verification, the system computes the cosine similarity score between two input"}, {"title": "3.1.1 Definition for Explainable Face Verification", "content": "An earlier study [4] approached the explainability of a face verification model similarly to that of an image classification model, aiming to visualize the discriminative information within each individual face image. However, the verification process typically involves two images. The global critical regions of each image identified by the model may not inherently be similar or dissimilar areas between two images. Williford et al. [40] utilized a face triplet, i.e., probe, matching gallery, and non-matching gallery, to explain the relative importance of facial regions. They defined explainable face verification as a way to highlight specific regions of the probe image which simultaneously maximize the similarity with the matching image and minimize the similarity with the non-matching. However, the face verification process operates independently for a pair of inputs instead of a triplet, making this definition less practical. Mery [28] improved the definition by directly exploring the relevant parts between two images when a match is established. Nevertheless, their definition overlooks the irrelevant parts between inputs, which particularly dominate the decision-making process for non-matching pairs of images.\nAs shown in Fig. 1a, this manuscript defines the problem of explainable face verification as follows. Given a pair of images feeding into a face verification system, the explanation method should generate corresponding saliency maps for both input images, which should clearly interpret the prediction results by answering the following questions:\n\u2022 If the face verification system believes the input pair is matching, which regions are similar to the model?\n\u2022 If the face verification system believes the input pair is non-matching, which regions are dissimilar to the model?"}, {"title": "3.1.2 Definition for Explainable Face Identification", "content": "Previous research has proactively explored the problem of ex-plainable face verification, whereas, to the best of our knowledge, there is no existing work that interprets the face identification process using saliency maps. Fig. 1b shows a typical process of face identification, where a face identifica-tion model computes the 1:n similarity and then determines the subject of the given probe image based on the most similar face from the gallery database. In practice, the correct subject for the probe image often appears in the top-K most similar gallery images, depending on the efficacy of the model or the difficulty of identifying the specific subject. Therefore, an explanation method should help understand why the identification model ranks a subject over others.\nAs illustrated in Fig. 1b, this work outlines the definition of explainable face identification as follows: Given a probe face image and numerous gallery images, the explanation method should generate saliency maps for the top-K gallery images, highlighting the pixels that are similar to the probe image respectively. In principle, K can be selected to any number larger than one based on the need for explainability. For demonstration purposes, K = 5 is selected in this manuscript."}, {"title": "3.2 Proposed explanation method", "content": "This section presents a new model-agnostic explanation method called CorrRISE to address the new definition of explainable face recognition, see Fig. 2. In principle, CorrRISE generates saliency maps by injecting perturbation and observing the impact on output. Thus, it provides \"black-box\u201d explanations and can be applied to any face recognition"}, {"title": "3.2.1 Mask Generation", "content": "Mask generation is an essential step that injects random perturbations into the input. The random mask generator in Fig. 2 randomly samples multiple small square patches in various locations on a plain image. As illustrated in the figure, the values of patches are set to 0 and all the patches constitute a binary mask, which occludes the corresponding pixels in a face image. We additionally test a variety of approaches to generate the patch values, such as purely random initiation, bilinear interpolation between [0, 1], and Gaussian distribution, while the binary mask obtains the best performance. In summary, the mask generation steps are as follows.\n(1) Initialize the parameters of the mask generator, i.e., the total number of masks N, and the number and size of square patches in each mask.\n(2) Sample multiple square patches with zero values in random locations of each mask Mi and finally get the mask set {Mi, i = 1, ..., N}.\n(3) Inject perturbation by multiplying the mask and input images."}, {"title": "3.2.2 Correlation-based Saliency Map Generation", "content": "Fig. 2 illustrates an overview of the proposed CorrRISE method and Algorithm 1 presents detailed steps for saliency map generation. In general, given a pair of images {IA, IB} and a face recognition model fx, the objective is to produce saliency maps highlighting both the similar and dissimilar regions between two faces. First, as shown in the previous step, CorrRISE leverages a mask generator to randomly produce N masks M = {Mi, i = 1, ..., N}. Each mask Mi is then multiplied with the corresponding input image, e.g., IA. The masked IA \u00a9 Mi and unmasked IB are fed into the face recognition model fx to capture the deep face representation {xm, XB}. The cosine similarity score SC\u012f between the deep features is then calculated. After iterating all the N masks,\nthe list of scores SC = {SC\u012f, i = 1, ..., N} corresponding to the mask list is recorded. Subsequently, Pearson correlation is performed between SC and M on a pixel-wise basis to obtain the final saliency map SA for IA. The location of positive correlation coefficients represents the regions on IA that are similar to IB, while the locations of negative coefficients represent the dissimilar regions. The same procedure is replicated for IB to obtain the saliency map SB.\nThe generation of two saliency maps is conducted separately for each image, as depicted in Fig. 2. Because a face recognition system can mistakenly match two irrelevant but both masked face images, interfering with the computation of similarity scores during generation. Furthermore, while the saliency map generation of CorrRISE is grounded in the face verification process, it can be easily adapted to the identification scenario by replicating the generation process for the same probe image and various gallery images."}, {"title": "Algorithm 1", "content": "```\nInput: number of iterations N, face recognition model fx, similarity function Score(), face images IA and IB\nOutput: saliency maps SA, SA, SB, SB\nSize (IA) \u2192 H, W\nfor k \u2190 1: N do\nMk \u2190 RandomMaskGenerator(H, W)\nXA, XB \u2190 fx (IA), fx (IB)\nxm, xm \u2190 fx (IA\u00a9 Mk), fx (IB \u00a9 Mk)\nSCA[k] \u2190 Score(xm, XB)\nSCB[k] \u2190 Score(xm, XA)\nM[k, :,:] \u2190 Mk\nend for\nfor i=1: H do\nfor j = 1: W do\nSA [i, j] \u2190 PearsonCorr (SCA, M[:, i, j])\nSB [i, j] \u2190 PearsonCorr(SCB, M[:, i, j])\nend for\nend for\nSS \u2190 SA[SA \u2265 0], SA[SA < 0]\nSBSB \u2190 SB [SB \u2265 0], SB [SB < 0]\n```"}, {"title": "4 Evaluation Methodology", "content": "Despite the promising development of explainability methods in vision tasks, the importance of rigorous objective evaluation methodologies has long been overlooked. In particular, only a few objective metrics or protocols have been designed for visual saliency explanation tools. The evaluation of previous explainable face recognition methods has been mostly based on visualization, making it difficult to compare to others.\nThis manuscript contributes new \"Deletion\" and \"Insertion\" metrics to better assess explanation methods for the general face recognition task, including both verification and identification scenarios. In principle, these metrics measure the change in the recognition performance after modifying the input image according to the importance map generated by the explanation method. The intuition behind the proposed metrics is that an effective saliency map is expected to precisely highlight the most important regions of two faces with the smallest number of pixels, based on which the face recognition model makes final decisions. The faster the overall recognition performance drops/rises after removing/adding salient pixels, the more accurate the produced saliency map. Changes in recognition performance"}, {"title": "Algorithm 2 Deletion and Insertion Metric for Face Verification Scenario", "content": "```\nInput: face recognition model fx, evaluator eval(), testing dataset D, saliency maps S, number of steps n\nOutput: deletion score d\u2081, insertion score d2\nS \u2190 GaussianBlur(S)\nS \u2190 Sorting(S)\nfor i = 1:n do\np \u2190 i/n \u00d7 100\nMask the first p% pixels of D to get D'\u2081\nInsert the first p% pixels to plain images to get D'\u2082\nacc1i \u2190 eval(fx, D\u2081)\nacc2i \u2190 eval(fx, D2)\nend for\nd\u2081 \u2190 AreaUnderCurve (accii vs. i/n, i = 1 : n)\nd2 \u2190 AreaUnderCurve (acc2i vs. i/n, i = 1 : n)\n```"}, {"title": "5 Experimental Results", "content": ""}, {"title": "5.1 Implementation Details", "content": ""}, {"title": "5.1.1 Explanation Method Setup", "content": "The proposed CorrRISE explanation method does not require any training or access to the internal architecture of the face recognition model. During the explanation process, the default number of generated masks, i.e., the number of iterations, is set to 1000. For each mask, there are 10 patches and the size of each patch is 30\u00d730 pixels."}, {"title": "5.1.2 Face Recognition Model Setup", "content": "Extensive experiments are conducted using the popular ArcFace [7] model with ResNet-50 [12] backbone. To demonstrate the generalization capability of our proposed explanation method across various face recognition models, its explainability performance is additionally tested on two face recognition models employing distinct loss functions, i.e., AdaFace [18] and MagFace [27]. All the face recognition models are trained on the same dataset by running their official publicly available codes."}, {"title": "5.1.3 Dataset", "content": "The face recognition models are trained with the MS1M [11] dataset cleaned by Deng et al. [7]. For evaluation, this manuscript first selects samples from various datasets, namely LFW [16], CPLFW [45], LFR [9], Webface-Occ [15], and IJB-C [26], for visual demonstration in a variety of recognition scenarios. In the proposed objective evaluation methodology, LFW, CPLFW, and CALFW datasets are employed for quantitative evaluation in the verification scenario, while the IJB-C dataset is utilized for the identification scenario. All the images are cropped and resized to 112x112 pixels."}, {"title": "5.1.4 State-of-the-art Explanation Methods Setup", "content": "For comparison, several state-of-the-art explainable face recognition methods, namely MinPlus [28], xFace [19], xSSAB [17], and FGGB [24], have been tested based on their official open-source code. MinPlus and xFace methods offer several variations in their publications and the best-performing ones (\"AVG\" for MinPlus and \"Method-1\" for xFace) are selected. Notably, although all these methods were originally designed for the face verification task, we adapt their code to the identification scenario following the definition in Section 3.1 and further accelerate them by performing the computation in batches on GPU. Additinally, several XAI methods [5, 31, 33, 34] have been adapted and tested. For Grad-CAM [34] and Grad-CAM++ [5], instead of backpropagating the gradients of class-wise posterior probability to activation layers, we adapt them by performing backpropagation for gradients of similarity scores between two input images. Moreover, the third-party adaptation from authors of [28] is utilized for LIME [33] and RISE [31]."}, {"title": "5.2 Sanity Check", "content": "A recent study [1] has raised doubts about the reliability of visual saliency methods, suggesting that the produced explanation heatmaps can be independent of the deep model or the input data. To address this concern, they introduced a model parameter randomization test for a sanity check. In the context of face recognition, an explanation method may provide visually compelling heatmaps by directly emphasizing the center of the faces without interacting with the face recognition model. Therefore, this manuscript employs a similar sanity check to validate the effectiveness of the proposed method. Specifically, the parameters of the ResNet-50 backbone network are randomly initialized and then the CorrRISE algorithm is applied to the randomized model. The first row of Fig. 4 shows that the CorrRISE algorithm will generate nonsensical saliency maps when attempting to explain a face recognition model with fake parameters. This result indicates that the proposed explanation method relies on a valid recognition model and is capable of producing meaningful interpretations."}, {"title": "5.3 Visual Explanation Results", "content": "This section presents the visual results of the saliency maps generated by our proposed CorrRISE algorithm. For clarity and fair comparison, all the experiments here are conducted on the ArcFace model. First, the explanation ability of CorrRISE is tested on the standard face verification scenario with face images sampled from LFW and WebFace-Occ datasets. Then, the behavior of the face recognition model in several challenging verification scenarios is analyzed and explained through CorrRISE-produced saliency maps, including some failure cases due to very similar subjects or significant head pose variations. Subsequently, CorrRISE is employed in a standard face identification scenario with two illustrative examples presented. Finally, a visual comparison with other explanation approaches is provided."}, {"title": "5.3.1 Standard Verification Scenario", "content": "Fig. 5 illustrates the visual explanations for the model's decision regarding four matching and four non-matching pairs of images taken from the LFW dataset. Remarkably, here the deep model makes correct predictions on all eight pairs.\nAs a result, the saliency maps produced by CorrRISE properly highlight the similar regions between the matching pairs. Generally, the salient region focuses on eyes, noses, and mouths, while there are variations from person to person. For example, the face recognition model relies more on the cheek when comparing the matching pair in the first row, while it emphasizes the open mouth for the pair in the third row. It is also noteworthy that the dissimilar regions often concentrate on irrelevant backgrounds and unexpected occlusions, such as hats. For the non-matching pairs, CorrRISE attempts to localize the similar areas between the non-matching faces but with relatively low salient values. In contrast, it produces saliency maps that clearly highlight the most dissimilar regions in their faces, indicating very low similarity between them."}, {"title": "5.3.2 Challenging Verification Scenario", "content": "Face verification systems can encounter various challenging situations in daily usage, such as head pose changes or very similar identities. It is important to provide reliable explanations for the system's behavior in specific scenarios.\nHead pose variation is a well-known challenge for face recognition systems. Fig. 7 shows four examples, where the model correctly recognizes the first two but fails at the last two. The saliency maps in Fig. 7a indicate that the model manages to make correct predictions by localizing similar regions, such as cheeks and noses, with high saliency values even on the profile faces. In contrast, it fails to find enough similarities in the examples in Fig. 7b and makes false predictions due to lacking sufficient information. For instance, the dissimilar saliency map spotlights the left eye of the front face in the third example, which corresponds to the missing parts in the profile face.\nFig. 8 further presents two examples of similar identity scenarios in triplet format, where probe images are very similar to both the matching and non-matching gallery images. In both cases, the face recognition model has mistakenly verified the non-matching pair (right) as faces belonging to the same subject. The saliency maps generated by CorrRISE"}, {"title": "5.3.3 Standard Identification Scenario", "content": "Fig. 9 presents two examples from the IJB-C dataset, each including a probe image and the top five most similar gallery images based on the prediction of the ArcFace face recognition model, see the first row. The similarity score between the probe image and each gallery image ranks from left to right.\nIn the first example, the saliency map explanation produced by CorrRISE, see the last row, interprets that the model perceives a high similarity between the probe and gallery image. It also shows that the central face region of the second image, the nose of the third image, and the glasses of the fourth and fifth images are similar to the corresponding parts of the probe image, but to a lesser extent when compared to the first gallery image. In the second example, CorrRISE reveals that the nose and beard areas of the first gallery image exhibit the highest similarity to the probe image. Overall, this visual explanation helps users understand why the face recognition model ranks the gallery images as it does, highlighting the specific regions that contribute to its decision."}, {"title": "5.3.4 Comparison with Other Explanation Approaches", "content": "A visual comparison is made between the proposed method and eight explanation approaches, including adaptations of four classical XAI techniques, i.e., Grad-CAM [34], Grad-CAM++ [5], LIME [33], and RISE [31], and four state-of-the-art explanation methods in face recognition, namely MinPlus [28], xFace [19], xSSAB [17], and FGGB [24]. Implementation details of all the explanation methods refer to Section 5.1.1.\nFig. 10 and 9 illustrate examples of saliency maps created by various explanation approaches under verification and identification scenarios respectively. In Fig. 10, only similar regions between two images are visualized and compared, due to the limitation of several earlier developed methods. The results show that CorrRISE consistently yields stable"}, {"title": "5.4 Quantitative Evaluation Results", "content": "This section reports the \"Deletion\" and \"Insertion\" metrics for the quantitative evaluation of various explanation approaches. As described in Section 4, these metrics measure the changes in recognition performance after modifying the input images according to the saliency map.\nTable 1 first demonstrates the evaluation results of similarity maps generated by the state-of-the-art explanation methods in three typical face verification scenarios. Generally, the reported metrics are consistent with the visual observations in Fig. 10. For example, the adapted Grad-CAM and Grad-CAM++ show poorer performance than other state-of-the-art explanation methods. Notably, CorrRISE achieves much better quantitative results than a straightforward adaptation of RISE. The latter can only obtain comparable results with other adapted XAI methods such as LIME, demonstrating the advantage and value of our proposed CorrRISE method. Although the recent explainable face verification methods present some visually compelling results, quantitative metrics show that CorrRISE is capable of providing more precise saliency maps on all three testing datasets. It is also interesting to observe that perturbation-based methods, such as xFace and CorrRISE, generally obtain more precise similarity maps than gradient-based methods, i.e., xSSAB and FGGB. On the other hand, Table 2 reports the evaluation results on dissimilarity maps in non-matching cases. It involves only four explanation methods as other approaches cannot deal with non-matching cases. The results show that the gradient backpropagation mechanism leads to better performance in generating dissimilarity maps.\nIn the face identification scenario, we leverage rank-1 and rank-5 identification on the IJB-C dataset as an auxiliary task and perform similar \"Deletion\" and \"Insertion\" processes to evaluate the similarity maps created by different explanation methods. As shown in Table 3, CorrRISE consistently achieves superior results in both metrics, which"}, {"title": "5.5 Generalization across Different Face Recognition Models", "content": "CorrRISE is additionally applied to different face recognition models to showcase the generalization ability of our proposed method. The verification accuracy of these models and the explainability performance of CorrRISE are reported in Table 5. In principle, the explainability metrics are used to compare explanation methods tested on the same face recognition model and the scores are not directly comparable across different models. However, the table also shows that when the models achieve similar verification accuracy on the LFW dataset, the generated saliency maps also report similar scores, which validates that CorrRISE is model-agnostic."}, {"title": "5.6 Limitation and Improvement", "content": "As shown in Section 5.4, although CorrRISE achieves superior performance in generating similarity maps, it shows relatively poor results on dissimilarity maps when compared to gradient-based methods xSSAB and FGGB. The other perturbation-based method, xFace, falls into a similar situation. This is a potential limitation of perturbation-based explanation methods when they are applied to vision tasks involving comparisons between images, such as face recognition, image retrieval, etc. In principle, perturbation-based methods first inject occlusions into an input image and observe the impact on output. In the case of face recognition, the output is a cosine similarity score, and the impact is typically measured as the change in the similarity score. For matching image pairs, the original similarity score is already high and the subsequent change in it after injecting perturbations is obvious to detect. Nevertheless, for non-matching pairs with naturally low similarity scores, the perturbations added to them bring very limited impact to the output, thus resulting in less accurate dissimilarity maps.\nThis section proposes a solution to alleviate this problem by introducing an additional regularization term to a specific step in the proposed CorrRISE algorithm. The idea is to reformulate the cosine similarity score when calculating dissimilarity maps for non-matching images. Specifically, we first replace the unmasked area of IA with corresponding pixels from IB to obtain Ieg, namely:\nIeg = IA \u00a9 (1 \u2013 Mi) + IB \u00a9 Mi. (1)\nregA reg reg A\nIt is fed to the feature extractor to get deep feature representation xreg\nA and xB. Then, the cosine similarity between xreg\nA and\nXB is calculated. Ideally, when the random mask occludes the most dissimilar regions of {IA, IB}, the similarity score"}, {"title": "6 Conclusion", "content": "This manuscript presented a significant step forward in the field of explainable face recognition by contributing a comprehensive explanation framework. It broadened the application of visual saliency map-based explanations to the most common face verification and identification tasks and provided fair definitions for this problem. The framework provided a model-agnostic explanation algorithm, called CorrRISE. It addressed the explainability problem in face recognition systems by generating saliency maps that highlight both similar and dissimilar regions between given face images. Extensive visualization results in multiple scenarios demonstrated the advantage of our proposed method and showcased its capability to analyze potential failure cases in challenging recognition scenarios and, thereby provided insights into improving the current face recognition system. Furthermore, a new evaluation methodology was designed, constituting a key component in the explanation framework. This methodology offered a quantitative assessment and fair comparison for general saliency map-based explainable face recognition approaches and would benefit future research in this area. The quantitative assessment results using this new evaluation methodology demonstrated the state-of-the-art performance of CorrRISE in generating similarity maps between face images. Lastly, this manuscript exploited a limitation of general perturbation-based methods in identifying dissimilar regions between images and provided a tentative solution to improve them."}]}