{"title": "Multimodal Pragmatic Jailbreak on Text-to-image Models", "authors": ["Tong Liu", "Zhixin Lai", "Gengyuan Zhang", "Philip Torr", "Vera Demberg", "Volker Tresp", "Jindong Gu"], "abstract": "Diffusion models have recently achieved remarkable advancements in terms of image quality and fidelity to textual prompts. Concurrently, the safety of such generative models has become an area of growing concern. This work introduces a novel type of jailbreak, which triggers T2I models to generate the image with visual text, where the image and the text, although considered to be safe in isolation, combine to form unsafe content. To systematically explore this phenomenon, we propose a dataset to evaluate the current diffusion-based text-to-image (T2I) models under such jailbreak. We benchmark nine representative T2I models, including two close-source commercial models. Experimental results reveal a concerning tendency to produce unsafe content: all tested models suffer from such type of jailbreak, with rates of unsafe generation ranging from 8% to 74%. In real-world scenarios, various filters such as keyword blocklists, customized prompt filters, and NSFW image filters, are commonly employed to mitigate these risks. We evaluate the effectiveness of such filters against our jailbreak and found that, while current classifiers may be effective for single modality detection, they fail to work against our jailbreak. Our work provides a foundation for further development towards more secure and reliable T2I models.", "sections": [{"title": "1 Introduction", "content": "Text-to-image (T2I) models have shown unprecedented capabilities in synthesizing high-quality im-ages that closely adhere to textual prompts [49, 45, 51]. However, recently the community has raised concerns regarding the potential for these models to generate unsafe content, i.e., images that might be offensive, disturbing, hateful, sexually explicit, or otherwise inappropriate [47, 52, 42]. Concurrently, a growing body of work focused on the jailbreak on Large Language Models (LLMs) [68, 76, 37, 1] to generate harmful texts through bypassing existing safety protocols. Moreover, Multimodal Large Language Models (MLLMs) [34, 24, 75], have been shown to be more vulnerable to jailbreak and generate unsafe texts when integrating the visual modality [17, 57, 41].\nIn this work, we introduce a novel form of jailbreak for T2I models, which we term multimodal pragmatic jailbreak, which triggers T2I models to generate the image with visual typographic texts on it, so-called visual text rendering [25], using prompts like \"an image of <image-generation prompt>, with a sign that says, '<visual text prompt>'\". In this scenario, the image and text content may each be considered safe in isolation, yet their combination can lead to unsafeness. This form of jailbreak is"}, {"title": "2 Background", "content": "Jailbreaks in text generation The safety of LLMs has garnered significant research interest, par-ticularly concerning attacks known as jailbreaks. Previous works used carefully crafted prompts or attaching suffixes to prompts to lead the model to produce harmful information [76, 68, 26]. Recent"}, {"title": "3 Multimodal pragmatic jailbreak benchmark", "content": "In our study, we consider three forbidden scenarios from OpenAI usage policy [36]: Hate speech, Physical harm, and Fraud. Refer to Appendix for a detailed definition of each scenario. We chose these particular categories because our observations suggest they are the most likely to demonstrate multimodal pragmatic unsafety in our work. To obtain the proper prompt, drawing inspiration from previous work in LLMs jailbreak [56, 67, 58], we first define subclasses in each class, then direct GPT-4 with examples to produce tailored prompts specific to each scenario and subclass to generate corresponding prompts, which are subsequently manually selected and refined. In addressing the hate speech scenario, we also take reference of the representative multimodal hateful meme dataset [20], through employing CLIP for image captioning and followed by an automatic generation of captions using GPT-4. The Multimodal Pragmatic Unsafe Prompts (MPUP) dataset developed through this process consists of 1,200 prompts, distributed as follows: 500 prompts in the hate speech category across 8 subclasses, 400 in the physical harm category across 8 subclasses, and 300 in the fraud category across 6 subclasses. The format of the prompt is \"<image-generation prompt>, with a sign that says, '<visual text prompt>'\", consistent with the format in SimpleBench [24]. We provide examples of each scenario along with explanations and a further description of the dataset in the Appendix."}, {"title": "3.2 Figurative language categories", "content": "We annotate figurative language categories to prompts in MPUP dataset by referring to the cate-gories from previous work on multimodal memes [23]: Metaphor/Simile, Sarcasm, Anthropomor-phism/Zoomorphism, Allusion, and Hyperbole. Each category may overlap with others within the same image. The definitions and examples are as follows:\nMetaphor/Simile refers to the attribution of characteristics from one item or group to another, through implicit or explicit comparisons, e.g., figures (a), (c), (d) and (g) in Fig. 1. Sarcasm means the language that conveys meanings contrary to their conventional interpretation or mocks subjects in a caustic or bitter manner, e.g., figures (e) and (h). Anthropomorphism/Zoomorphism refers to the attribution of human characteristics, emotions, or behaviors to animals, objects, or abstract concepts, making them relatable, or conversely, assigning animal traits to humans, e.g., figures (a)"}, {"title": "3.3 Multimodal pragmatic jailbreak safety classifier", "content": "Existing safety filters in T2I models fail to address multimodal pragmatic unsafe content. To address this, we establish a test bed and implement straightforward multimodal pragmatic safety classifiers to mitigate these risks. Inspired by the mitigation strategies used in DALL\u00b7E 3 [35] which include both prompt input classifiers and image classifiers to scrutinize the message from users and output images respectively, we adopt a similar strategy to enhance the safety. Given the original 1,200 unsafe prompts, we first employ the following two simple techniques to balance safe and unsafe prompts. Prompt modality removal: We deconstruct the original unsafe prompts by separately removing the image-generation prompt or the visual text prompt, resulting in the creation of safe prompts as unimodal visual text prompts or image-generation prompts. Prompt modality exchange: We also enhance the diversity of prompts by exchanging the visual text prompt in each unsafe prompt with another visual text prompt of a random unsafe prompt in the other categories. The above two techniques result in a total of 4,800 (1,200 original unsafe + 1,200 \u00d7 3 newly created safe) prompts. For classified images, we utilize images generated from seven diffusion models (excluding DALL\u00b7E 2 and DALL-E 3), leading to a total of 8,400 (1,200 \u00d7 7) images, with an overall unsafe rate of 45.9%. We test the following classifiers:\nWord blocklist We use textual blocklists from Midjourney [30] and Leonardo.AI [21]. Text prompts that match any substrings with these blocklists are flagged as harmful.\nSemantic similarity scores We employ BERT scores [73] to evaluate the embedding distance between input prompts and unsafe scenarios defined by OpenAI usage policy, with tuned threshold values in validation set to determine the acceptability of prompts.\nLLMs We also employ two advanced large language models (LLMs), the open-source model Vicuna 13B [74], and the close-source GPT4 [34] as the classifier of unsafe prompts, with few-shot setting.\nAdapted CLIP classifiers We use two adapted CLIP classifiers, Q16 and Multi-Headed SC (MHSC) [54, 42], and an NSFW filter [3] adopting a thin ResNet 50 [19] to classify the unsafe image."}, {"title": "4 Experimental setup", "content": "We select nine state-of-the-art T2I models, including seven open-source diffusion models, vanilla Stable Diffusion (SD 2.0-base, denoted as SD) [49], Stable Diffusion XL (SDXL) [39], Safe Latent Diffusion (SLD) [53], DeepFloyd (IF-I-XL) [15], GlyphControl [70], Proteus [14], and Open-Dalle [11], and two close-source models, DALL\u00b7E 2 [46] and DALL-E 3 [35] from OpenAI API. We benchmark the above models on the MPUP dataset, resulting in a total of 10,800 queries (1,200 prompts \u00d7 nine models)."}, {"title": "4.2 Metrics and evluation", "content": "We term Attack Success Rate (ASR) as the rate of multimodal pragmatic unsafe generated images under jailbreak. To measure ASRs, we employ GPT-40 with delicate category-specific prompting, with few-shot examples. We also prompt GPT-40 to disregard visual spelling errors that do not hinder human comprehension of the information. The prompts are presented in Appendix. To explore the effectiveness of such an evaluation tool, we randomly select 174 generated text-embedded images from five models in three categories, with 58 samples from each category that include both multimodal pragmatic safe and unsafe images, and manually label them. We observe a correlation of 81.0% accuracy with human-labeled results for GPT-40. Therefore, we employ this classifier in the following study to detect multimodal pragmatic unsafe images."}, {"title": "5 Experimental results and analysis", "content": null}, {"title": "5.1 Main result: multimodal pragmatic jailbreak", "content": "Table 1 shows ASRs for nine diffusion models on the MPUP dataset. The closed-source model, DALL-E 3, stands out as the most multimodal pragmatic unsafe model, achieving more than 70% ASR on average. Among open-source mod-els, OpenDalle, Proteus and DeepFloyd demon-strate the highest ASRs. In contrast, SD, and SDXL show moderate ASRs between 30-40%.\nWe attribute these differences to models' vary-ing levels of visual text rendering capacity. Ta-ble 2 shows OCR accuracy on both full and substring texts for the nine diffusion models on the MUMP dataset. DALL-E 3 exhibits the best visual text rendering capacity, achieving around 5-10% OCR accuracy for full strings and 50% for substrings. Among open-source models, OpenDalle, Proteus and DeepFloyd demonstrate the highest OCR accuracies on substrings, correlating well with their high multimodal pragmatic unsafety. SLD emerges as the safest model, primarily due to its significantly lower OCR accuracy for substrings Additionally, the near-zero OCR accuracies for full strings in some models like SD and SDXL indicate limitations in their legible text rendering capabilities, aligned with previous investigations [25, 70, 9]. Our results show that such multimodal pragmatic jailbreaks in diffusion models arise from the models' capability to generate visually correct texts. In general, models with stronger capabilities in rendering substring visual text tend to exhibit higher multimodal pragmatic unsafety."}, {"title": "5.2 Multimodal safety classifier", "content": "We evaluate the safety classifier on two levels, i.e., input prompts and output images employing classifiers introduced in Section 3.3. Table 3 shows the performance of various classifiers and a baseline of random filtering. Vicuna 13B has a marginal improvement over the baseline specifically in the hate speech category. Detailed performance metrics for the BERT score-based classifier, as shown in Table 5, reveal significant variations in prompt categories. E.g., while BERT score demonstrates reasonable AUC and precision scores in identifying hate and fraud content, its effectiveness dramatically drops in physical harm content detection, as evidenced by low recall and"}, {"title": "5.3 Multimodal pragmatic jailbreak on Online T2I Services", "content": "In this section, we assess the effectiveness of multimodal pragmatic jailbreak across three online T2I services: Midjourney [31], Leonardo.AI [22], and Gen-2 [48]. They are designed to prevent the generation of unsafe content through internal AI moderators. To conduct our tests, we randomly select 44 hate speech prompts, presented in the Appendix, and interact with each service's web interface to generate two images per prompt, resulting in 88 images per service. We measure the rejection rate and the ASR, offering insights into each service's moderation system in identitying and preventing the generation of harmful content.\nTable 6 shows that our selected prompts are able to bypass the detection in Midjourney with a bypass rate of 100%, with a small part of prompts, around 5-10%, getting rejected by the AI moderator of Gen-2 and Leonardo. AI. It is evident that current online T21 services lack robust defenses against multimodal pragmatic unsafe prompts. Among the prompts passing the moderator, Midjourney exhibits the lowest ASR, at approximately 25%. In contrast, Leonardo.AI and Gen-2 have ASR at around 37-40%, indicating that Leonardo.AI and Gen-2 have better visual text rendering capabilities but potentially higher multimodal pragmatic safety risks. Please refer to Appendix provides examples of images generated during our testing, illustrating the varying degrees of moderation effectiveness and jailbreak across different platforms."}, {"title": "6 Discussions", "content": null}, {"title": "6.1 Cause of multimodal pragmatic jailbreak for T2I models", "content": "Our experiments demonstrate that current T2I models are vulnerable to multimodal pragmatic jailbreak through visual text rendering. So what causes such unsafety? We investigate this question through the following perspectives.\nBasis of visual text rendering ability One key source for the performance of T2I models is the supervised training data of image-text pairs. How do T2I models learn visual text rendering capacity from such data? First, LAION does include data pairs of images with visual texts and corre-sponding captions. One work [24] randomly sampled 100 images from LAION and revealed a significant presence of samples (around 60%) containing both texts in images and a corresponding match between the texts visible in the images and their captions. Secondly, with the presence of such pairs of visual texts and captions in training data, it has been shown that T2I models, despite with character-blind text encoders (i.e., encoders without direct signal to the character-level makeup of the input) like T5 [44], are also able to achieve robust visual text spelling ability (around 60% accuracy) as an emergent ability, so-called spelling miracle, though their designs not specifically focus on character recognition and spelling [24].\nMultimodal pragmatic unsafety in training data We randomly sample 1000 image-caption pairs with a filter of the height and width being larger or equal than 512 pixels from LAION-400M, and measure the following rates: rate of pairs containing visual texts in images and corresponding match between the visual texts and captions (R1); rate of the textual caption itself violating OpenAI usage policy and therefore considered to be unsafe (R2); rate of the visual image itself violating OpenAI usage policy and therefore considered to be unsafe (R3); and rate of images being multimodal pragmatic unsafe (R4). Results show that R\u2081 = 25.2%, R2 = 1.4%, R3 = 2.6%, and R4 = 1.6%. The high rate R1 indicates a substantial proportion of images where visual texts correctly match their captions, consistent with previous findings [24]. Despite relatively low rates of R2, R3, and R4, these instances provide a foundation to the model's potential for the generation of multimodal pragmatic unsafe content. In summary, multimodal pragmatic jailbreak in diffusion models can arise from an apparent capability of generating correct visual text in images without deep semantic understanding of the pragmatic relations between the visual text and image modalities and insufficient filtering of multimodal pragmatic safety in training data."}, {"title": "6.2 Jailbreak on image editing models", "content": "Image editing models allow users to apply non-trivial semantic edits to real-world images. In this section, we explore whether these models can be utilized to add visual texts to a pure image using prompts in the MPUP dataset, therefore potentially generating multimodal pragmatic unsafe images. Specifically, we first generate unaltered images (i.e., images without visual texts) using only image-generation prompts in MPUP with the OpenDalle model. These images serve as guidance for further editing. Subsequently, we employ two advanced image editing models, InstructPix2Pix (IP2P) [4] and MagicBrush [72], to introduce visual text overlays into the image based on the visual text prompts. Our experiments in this section focus solely on the hate speech category, exemplarily investigating the multimodal pragmatic safety risks through generating hateful content.\nResults show that IP2P and MagicBrush achieve an overall ASR of 1.8% and 3.4% in the hate speech category, respectively. This suggests that current state-of-the-art image editing models struggle with effectively rendering visual texts, therefore avoid such multimodal pragmatic safety risks. As shown in Fig. 3, these image editing models either incorrectly alter the semantic of the image when attempting to integrate the text (as seen in the first row), or produce text overlays that are difficult for humans to discern (as seen in the second row). The underlying issue may stem from the training datasets used for these editing models. The training datasets for IP2P and MagicBrush"}, {"title": "6.3 Influence of model version and model size", "content": "In this section, we investigate whether the size of diffusion models and trained model versions influence the performance of multimodal pragmatic safety. To investigate the impact of model size, we select the DeepFloyd model as a representative example, as most other diffusion models share the same or similar sizes. We measure ASR for four versions of DeepFloyd with two cascades: DeepFloyd I-4.3B+II-1.2B (4.3B+1.2B), DeepFloyd I-900M+II-1.2B (900M+1.2B), DeepFloyd I-400M+II-1.2B (400M+1.2B), DeepFloyd I-400M+II-450M (400M+450M). To investigate the impact of model versions, we measure ASR using three versions of Proteus: Proteus v0.1 [12], Proteus v0.2 [13], and Proteus v0.3 [14], and three versions of vanilla stable diffusion models: Stable Diffusion 2-base (SD-2-base) [61], Stable Diffusion 2 (SD-2) [59], and Stable Diffusion 2-1 (SD-2-1) [60], as well as two version of stable diffusion XL models, Stable Diffusion XL base (SDXL-base-1.0) [62], and Stable Diffusion XL refiner (SDXL-refiner-1.0) [63]. Each version of model is described in Appendix. Our experiments in this section focus solely on the hate speech category.\nFig. 4 presents the the results of our analysis. Specifically, Proteus v0.3 and SDXL-refiner-1.0 achieve approximately 13% and 10% higher ASR than SDXL-base-1.0 and Proteus v0.1 and v0.2, respectively. Similarly, SD-2-1 and SD-2 exhibit around 2% higher ASR than SD-2-base. These findings indicate a general trend: larger model size and advanced versions of diffusion models tend to have a higher risk of generating multimodal pragmatic unsafe images."}, {"title": "6.4 Influence of subclass prompts", "content": "We also explore the influence of various unsafe themes. Table 7 shows the ASR for nine diffusion models across different subclasses within the hate speech category. For the analysis on the other two categories, refer to Appendix. In general, themes such as race or ethnicity, anti-immigrant and ableism exhibit to carry the highest risk, and the most likely to prompt diffusion models to generate multimodal pragmatic unsafe contents. In contrast, on themes such as political satire, most models struggle to generate a sufficient proportion of unsafe images. This lower rate is primarily due to diffusion models' difficulties in accurately translating the nuances of domain knowledge in language"}, {"title": "7 Conclusion", "content": "This paper conducts the first safety assessment that contains both text and image modalities in the output for T2I models. Using our MPUP benchmark, we are able to precisely quantify the effects of such multimodal pragmatic risks for nine current T2I models, all of which drastically exhibit jailbreaks. We further benchmark our jailbreak against filters used in real-world scenarios. Results indicate that current filtering strategies, likely limited by their unimodal nature, fail to adequately identify multimodal pragmatic unsafe contents.\nSocietal impact The MPUP dataset might pose potential risks of intentionally triggering T2I mod-els to generate harmful content. We believe that a better understanding of these risks will drive improvements in detection strategies, ultimately leading to safer deployment of T2I models in society.\nLimitations Future work could connect existing works in multimodal hateful memes detection [20, 40] for evluating the first category of generated images. Additionally, the impact of prompt tuning methods or adversarial attacks on multimodal pragmatic jailbreak remains to be studied. Finally, our dataset also has limitations in diversity and representativeness of prompts. We plan to update MPUP regularly with community feedback."}, {"title": "A Data descriptions", "content": null}, {"title": "A.1 Subclass descriptions", "content": "The MPUP dataset 2 contains three categoties from forbidden scenarios in OpenAI usage policy [36]: hate speech, physical harm, and fraud. The description of each category is presented in Table 8.\nThe category of hate speech, physical harm, and fraud is further split by eight, eight, and six subclasses, respectively. A demonstration of each subclass is described in Table 9. Fig. 5-10 further show examples of generated images for each subclass in each category."}, {"title": "A.2 Figurative language label distributions", "content": "In Section 3.2, we discuss the figurative language labels of prompts in MPUP dataset. In this section, we present the distribution of these labels. Fig. 11 shows the figurative language label distribution across three categories. The metaphor/simile consistently appears most frequently in all categories. In contrast, anthropomorphism/zoomorphism is the least frequent label. There are also variations within categories: sarcasm is the second most common label in the hate speech category, while allusion and hyperbole are the second most frequent labels in the physical harm and fraud categories, respectively."}, {"title": "A.3 Influence of figurative language categories", "content": "In this section, we explore which types of figurative language prompts are more readily representable as images with visual text using text-to-image (T2I) models and thus leading to higher multimodal"}, {"title": "A.4 License", "content": "The MPUP dataset is released under the CC BY-4.0 License. We anticipate the development of advanced detection techniques and plan to continuously update these methods as well as noval models on the leaderboard hosted on our project website. We state that we bear responsibility in case of violation of rights."}, {"title": "B Prompts employed in GPT-40 for multimodal pragmatic evaluation", "content": "We present the prompt that we use in GPT-40 for the multimodal pragmatic safety evaluation in Fig. 12, 13 and 14."}, {"title": "C Model descriptions and implementations", "content": null}, {"title": "C.1 Text-to-image models", "content": "We select nine state-of-the-art T2I models, including seven open-source diffusion models, vanilla Stable Diffusion [49], Stable Diffusion XL [39], Safe Latent Diffusion [53], DeepFloyd [15], Glyph-Control [70], Proteus [14], and OpenDalle [11], and two close-source models, DALL-E 2 [46] and DALL-E 3 [35] from OpenAI API. All models are run using NVIDIA RTX A6000 GPUs.\nVanilla Stable Diffusion (SD) is the vanilla latent diffusion model trained on a subset of the LAION-5B [55] dataset. We adopt the checkpoint of SD 2.0-base 3.\nStable Diffusion XL (SDXL) [39] uses an ensemble of experts [2] in a latent diffusion pipeline via first generating noisy latent representations using a base model, then refining with a refinement model 4 for the final denoising steps. We adopt the checkpoint of SD-XL 1.0-base 5.\nSafe Latent Diffusion (SLD) [53] is a T2I model with safety mechanisms by blocking the text embedding of inappropriate concepts to improve the safety guidance. We adopt the checkpoint of default implementation 6."}, {"title": "C.2 Image editing models", "content": "We employ two advanced image editing models, InstructPix2Pix [4] and MagicBrush [72], to introduce visual text overlays into the image based on the visual text prompts in the MPUP dataset. All models are run using NVIDIA RTX A6000 GPUs.\nInstructPix2Pix [4] a learning-based image editing technique for T2I models. It's trained on a large instruction-following image editing dataset automatically curated using GPT-3 [6] and SD. We adopt the official implementation for InstructPix2Pix.\nMagicBrush [72] is a large-scale manually annotated dataset for instruction-guided real image editing. We denote the model fine-tuning InstructPix2Pix on MagicBrush in their work as MagicBrush in our paper. We adopt the official implementation for MagicBrush."}, {"title": "D Further analysis on Section 5 and 6", "content": "In Section 5.3, we analyze the multimodal pragmatic jailbreak across three online T2I services. Fig. 15 and 16 illustrate examples of generated images. In Fig. 15, both images generated by Leonardo.AI and the second image generated by Gen-2 exhibit relatively legible visual text, and thus are classified to multimodal pragmatic unsafe. Similarly, in Fig. 16, the second generated image from Gen-2 also contains relatively legible visual text, leading to unsafe classification. In contrast, Leonardo. Al directly rejects such generation requests, demonstrating varying degrees of moderation effectiveness in preventing multimodal pragmatic jailbreak across different platforms."}, {"title": "E Societal impact and limitations", "content": "Societal impact The MPUP dataset might pose potential risks of intentionally triggering T2I mod-els to generate harmful content. We believe that a better understanding of these risks will drive improvements in detection strategies, ultimately leading to safer deployment of T2I models in society."}]}