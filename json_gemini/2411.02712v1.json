{"title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "authors": ["Yuxi Xie", "Guanzhen Li", "Xiao Xu", "Min-Yen Kan"], "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs.\nWe tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; Chiang et al., 2023; OpenAI, 2023) have catalyzed the evolution of Large Vision-Language Models (LVLMs) (Liu et al., 2023c,b; Dai et al., 2023; Anil et al., 2023) in understanding and reasoning across visual and textual modalities. Despite their impressive performance on various vision-language tasks, existing LVLMs still struggle with the issue of hallucination, where the model outputs are not factually grounded in the input visual contents (Rohrbach et al., 2018; Li et al., 2023b; Gunjal et al., 2024; Liu et al., 2024). Hallucination in LVLMs refers to non-existing or erroneous descriptions of visual contents, such as objects, attributes, and relationships, which is especially challenging to understanding unconventional images, as shown in Figure 1a.\nThe phenomenon of hallucination in LVLMs can be attributed to the integration of pre-trained LLMs in the architecture. Recent works reveal that this issue is closely tied to insufficient context attention, where the model prioritizes language patterns and focuses on partial tokens rather than fully grounding the generated content in both visual and textual context (Lee et al., 2023; Wang et al., 2024). To mitigate the over-reliance on language priors, many efforts have been devoted to decoding optimization with penalties on over-trust candidates (Huang et al., 2023) or a focus on visual uncertainty (Chen et al., 2024). However, these methods require increased inference time and specific infrastructure designs (Lee et al., 2023), obstructing their generalizability and scalability across diverse data domains and sizes. In contrast, our study explores training strategies to alleviate the over-reliance on language priors via preference learning, enhancing visual understanding to mitigate hallucination in LVLMs.\nGiven the difference in the likelihoods between accurate and hallucinatory samples on vision-conditioned $p(response \\vert image, query)$ and textual-only $p(response \\vert query)$ distributions, Figure 1b illustrates the shifts of this difference after aligning the model with hallucination-free data via preference learning. Before alignment, the textual-only distributions dominate the model decision on determining accurate samples as preferred compared to hallucinatory ones, reflected by the distributions (in green) of the same shape for both probabilities. This dominance in pairwise preference illustrates the over-reliance on language priors in LVLMs, which is especially crucial for unseen images in training (e.g., Figure 1a), limiting the model generalizability across different data. Motivated by this challenge, we propose Vision-guided Direct Preference Optimization (V-DPO), a vision-specific variant of Direct Preference Optimization (DPO) (Rafailov et al., 2023), to employ visual guidance during preference learning for hallucination mitigation in LVLMs. We adapt Classifier-Free Guidance (CFG) (Ho and Salimans, 2022) to integrate the visual guidance into the optimization target, inspired by its effectiveness in improving the specificity of model generations tailored for specific contents (Sanchez et al., 2023; Kornblith et al., 2023). To assess the generalizability of V-DPO, especially on unconventional contents, we construct a synthetic dataset containing both response-contrast and image-contrast preference pairs, compared against existing human-annotated preferences such as RLHF-V (Yu et al., 2023). Our approach exhibits significant and stable performance improvements through extensive experiments on various hallucination benchmarks. Further analysis of the distribution shifts from training demonstrates the effectiveness of V-DPO in mitigating the over-reliance on language priors on both image- and response-contrast data."}, {"title": "2 Related Work", "content": "Hallucination has emerged as a significant challenge to model reliability and generalizability in LVLM development. To alleviate hallucinated content, existing works can be divided as following two directions. The first focuses on post-processing approaches, including post-hoc corrections (Zhou et al., 2023; Yin et al., 2023; Lee et al., 2023) and specialized decoding (Huang et al., 2023; Chen et al., 2024). However, these methods often require increased inference time, obstructing their generalizability and scalability across diverse data domains and sizes (Bai et al., 2024).\nThe second line of work attempts to collect hallucination-aware data to mitigate hallucination in LVLMs through preference optimization leaning toward hallucination-free outputs. For example, Sun et al. (2023) and Yu et al. (2023) adapt the Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) paradigms in LLMs, respectively, to align LVLMs with hallucination-aware human preferences. Zhao et al. (2023) and Sarkar et al. (2024) propose data augmentation pipelines to construct (accurate, hallucinatory) preference pairs for contrastive tuning. Our work mitigates hallucination in the context of preference optimization with not only augmented data including both response- and image-contrast preference pairs, but also a vision-specific optimization target to enhance visual understanding."}, {"title": "3 Background and Motivations", "content": "We explore strategies to enhance visual understanding in LVLM preference optimization. Our framework starts from a supervised fine-tuned (SFT) model, obtained by jointly training a visual encoder and a pre-trained LLM via visual instruction tuning (Liu et al., 2023c). Specifically, we incorporate visual guidance by integrating Classifier-Free Guidance (CFG) into vanilla DPO."}, {"title": "3.1 Preference Optimization for LVLMs", "content": "We consider a policy LVLM $\\pi_\\theta$ parameterized by $\\theta$. For a vision-conditioned text generation task, given an input image $v \\sim \\mathcal{I}$ and a textual query $x \\sim \\mathcal{P}$, we optimize for the KL-constrained reward maximization objective:\n$$\\underset{\\pi}{\\text{max}} \\mathbb{E}_{(v,x) \\sim \\mathcal{I} \\times \\mathcal{P},y \\sim \\pi} \\left[r(v, x, y)-\\beta D_{K L}\\left[\\pi(y \\mid v, x) \\| \\pi_{\\text {ref }}(y \\mid v, x)\\right]\\right]$$\nunder reward function $r(v,x,y)$ and reference model $\\pi_{\\text{ref}}$. DPO solves the optimal policy as:\n$$\\pi_{\\gamma}(y \\mid v, x)=\\frac{\\pi_{\\text{sft }}(y \\mid v, x) \\exp (\\alpha r(v, x, y))}{Z(v, x)}$$"}, {"title": "3.2 Classifier-Free Guidance in LLMS", "content": "CFG was originally proposed in the context of conditioned diffusion models (Dhariwal and Nichol, 2021). Given a noisy image $y$ and a class condition $c$, the model predicts probability likelihood $p$ for the conditioned step-wise sample $\\pi_\\theta(y \\vert c) \\propto \\pi_\\theta(y) \\cdot \\pi_\\phi(c \\vert y)^{\\gamma}$, where $\\gamma > 0$ controls the guidance strength from the classifier $\\pi_\\phi$. Ho and Salimans (2022) observe that the guidance can be offered without a classifier:\n$$\\pi_\\theta(y \\mid c) \\propto \\pi_\\theta(y) \\cdot \\pi_\\phi(c \\vert y)^{\\gamma} \\propto \\frac{\\pi_\\theta(y \\mid c)}{\\pi_\\theta(y)^{\\gamma-1}}$$\nGiven a textual completion $y = \\{y_i\\}_{i=1}^{N}$ and a conditional prompt or image $c$, we can extend CFG to autoregressive models as $\\pi_\\theta(y \\vert c) \\propto \\frac{\\pi_\\theta(y \\mid c)}{\\pi_\\theta(y)^{\\gamma-1}} = \\prod_{i=1}^{N} \\frac{\\pi_{\\theta}\\left(y_{i} \\mid y_{<i}, c\\right)}{\\pi_{\\theta}\\left(y_{i} \\mid y_{<i}\\right)^{\\gamma-1}}$. Previous works show that CFG increases the specificity of the generation to be more pertinent toward the prompt (Sanchez et al., 2023) or image (Kornblith et al., 2023). Enlightened by this insight, we apply CFG in LVLM preference optimization to enhance the importance of visual context. This employment is non-trivial considering the dynamics in the training process, which we will detail next."}, {"title": "4 Vision-Guided Preference Learning", "content": "In this work, we focus on mitigating hallucinations in LVLMs caused by insufficient context attention to visual information. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual understanding on both response- and image-contrast preference data."}, {"title": "4.1 Vision-Guided DPO", "content": "Our V-DPO approach builds on the insight that CFG-modified distribution produces more condition-specific generation than vanilla decoding. As we will detail next, our core contribution originates from a vision-specific term in the reward maximization objective of DPO.\nV-DPO Objective. We start with the definition of visual guidance in the context of LVLMs. Following Eq. 5, we apply CFG to vision-conditioned text generation:\n$$\\pi_{\\theta}(y \\mid v, x)=\\frac{\\pi_{\\theta}(y \\mid v, x)}{\\pi_{\\theta}(y \\mid x)^{\\gamma-1}}$$\nwhere $\\pi_{\\theta}(y \\mid v, x)$ is the guidance from the visual context $v$ to increase the specificity of the response $y$ toward the image, given the input query $x$. We integrate this term as an additional target to optimize in Eq. 1. Our result vision-enhanced reward maximization objective is then:\n$$\\underset{\\pi}{\\text{max}} \\left[\\mathbb{E}_{(v, x) \\sim \\mathcal{I} \\times \\mathcal{P}, y \\sim \\pi}\\left[r(v, x, y)-\\beta D_{K L}\\left[\\pi(y \\mid v, x) \\| \\pi_{\\text {ref }}(y \\mid v, x)\\right]+\\alpha D_{K L}\\left[\\pi(y \\mid v, x) \\| \\pi(y \\mid x)\\right]\\right]\\right]$$\nwhere $\\alpha > 0$ controls the weight of the visual guidance to optimize. Solving the optimal solution $\\pi$ to the above objective, we have:\n$$\\frac{\\pi_{\\gamma}(y \\mid v, x)}{\\pi_{\\gamma}(y \\mid x)^{\\gamma-1}}=\\frac{\\pi_{\\gamma}(y \\mid v, x)}{Z(v, x)} \\exp (\\alpha r(v, x, y))$$\n$$\\pi_{\\gamma}(y \\mid v, x)=\\frac{1}{Z(v, x)}\\left(\\frac{\\pi_{\\gamma}(y \\mid v, x)}{\\pi_{\\gamma}(y \\mid x)}\\right)^{\\gamma-1} \\pi_{\\text{sft }}(y \\mid v, x) \\exp (r(v, x, y))$$\nwhere $\\gamma = 1 - \\frac{\\alpha}{\\beta}$. Unlike inference-time CFG, we decrease $\\gamma < 1$; i.e., increasing $\\alpha > 0$, to strengthen the guidance of visual context during training. We detail the complete derivations in Appendix A. Although only a proportional relationship holds here (as $\\pi_{\\gamma}(y \\mid v, x) / \\pi_{\\gamma}(y \\mid x)^{\\gamma-1}$ is an unnormalized probability distribution), we can still obtain the reward difference of a preference pair using the Bradley-Terry model. Similar to Eqs. 3 and 4, we derive our policy objective as:\n$$\\mathcal{L}_{V D P O}\\left(\\pi_{\\theta} ; \\pi_{\\text {ref }}\\right)=-\\mathbb{E}_{(v, x, y_w, y_l) \\sim D} \\log \\sigma\\left(\\beta u_{w, l}\\right)$$"}, {"title": "Implementation of Visual Guidance", "content": "In Eq. 9, we disable gradient backpropagation on $\\varphi_\\theta(v, x, y)$ to maintain a stable textual-only distribution $\\pi_\\theta(\\cdot | x)$ during training. This aims to provide a reliable reference value to calculate the visual guidance. We further discuss the choice of $\\pi(\\cdot | x)$ in Section 5.3. Following the implementation of Liu et al. (2023c), we pass zeroes in place of the conditioning visual context to get the textual-only distribution:\n$$\\pi_{\\theta}(\\cdot | x)=\\pi_{\\theta}(\\cdot | 0, x)$$\nWith the integration of visual guidance, we modify $\\pi_{\\theta}(y | v,x)$ in vanilla DPO to be a non-normalized probability distribution, $\\pi_{\\theta}(y | v,x)\\varphi_\\theta(v,x,y)$. Empirically, this can progressively decrease the effect of visual guidance as the visual-conditioned and unconditioned distributions diverge from each other through training. To mitigate this problem, we follow Kornblith et al. (2023) to normalize it as:\n$$\\pi_{\\theta}(\\cdot | v, x) \\varphi_{\\theta}(v, x, \\cdot) \\propto \\varphi\\left(\\frac{\\gamma}{\\gamma-1}\\left(h_{\\theta}(v, x)+(\\gamma-1)\\left(\\hat{h}_{v}(v, x)-\\hat{h}_{o}(0, x)\\right)\\right)\\right)$$"}, {"title": "4.2 Constructing Contrast Images", "content": "As discussed in Section 3.1, we augment the preference data with image-contrast pairs to enhance visual understanding via preference learning. The construction of contrastive image pairs aims to bolster the visual understanding ability to discern nuanced visual differences between similar images. Specifically, we manipulate images by replacing conventional items with unconventional ones, considering the limited capability of LVLMs to understand weird images (Guetta et al., 2023). This section details the automatic construction process we use to collect image-contrast preference data.\nProposing Replacement Elements. Given an image from an existing dataset, we extract object-level information using LVLMs and generate detailed captions with objects grounded in respective positions in the image. Based on the layout-grounded descriptions, we employ LLMs to propose replacements for visual elements, thereby"}, {"title": "5 Experiments", "content": "We now assess V-DPO across various multimodal hallucination benchmarks. To interpret how V-DPO improves visual understanding, we compare performance using various preference data. Specifically, unlike previous studies focusing on performance improvement using specific data, this work aims to demonstrate the effectiveness and generalizability of V-DPO across different training datasets and benchmarks for fair comparison."}, {"title": "5.1 Setup", "content": "We choose LLAVA-v1.5-7B (Liu et al., 2023b) as our initial SFT model and conduct preference learning with full fine-tuning. Our synthetic augmented data contains 5K response- and image-contrast preference pairs, compared against the human-annotated response-contrast data RLHF-V (5K) (Yu et al., 2023) of equal size. In Appendix D, we further conduct extended experiments on LLAVA-V1.6-7B to demonstrate the generalizability of V-DPO.\nBenchmarks. We evaluate our approach on four hallucination benchmarks: (1) POPE (Li et al., 2023b) on object hallucination with discriminative tasks; (2) AMBER (Wang et al., 2023) containing both generative and discriminative tasks on object, attribute, and relation hallucination; (3) HallusionBench (Liu et al., 2023a) assessing visual illusion and knowledge hallucination with systematically structured discriminative tasks; and (4) MMHalBench (Sun et al., 2023) covering different question types and object topics. We also conduct general-purpose evaluation on MMBench (Xu et al., 2023) across various multimodal tasks in Appendix E.\nBaselines. We compare our method against the initial SFT model and vanilla DPO as the fundamental and strengthened baselines, respectively. We also consider Hallucination-Aware Direct Preference Optimization (HA-DPO) (Zhao et al., 2023) as a variant of DPO baseline trained on 16K style-consistent hallucination sample pairs."}, {"title": "5.2 Main Results", "content": "We compare V-DPO with vanilla DPO methods across various hallucination benchmarks to show the effectiveness and stability of our approach.\nPOPE. Table 1 compares model performance (F1 score) and tendency to answer \u201cyes\u201d (Yes Ratio) on POPE. V-DPO outperforms the SFT and vanilla DPO baselines on random sets and more challenging tasks such as the adversarial scenario. Furthermore, V-DPO significantly increases the F1 scores from 85.98 to 86.92 and 87.22 trained on synthetic and human-annotated data, respectively, with mitigated bias in yes ratios 47.43% and 48.66%, compared to 44.22% and 47.88% of vanilla DPO. This suggests that V-DPO achieves better hallucination performance while mitigating the over-reliance on language priors with visual guidance.\nAMBER. In Table 3, our approach achieves significant improvements on both AMBER's generative and discriminative tasks. For CHAIR scores, we observe an absolute improvement of"}, {"title": "5.3 Ablation Study", "content": "We conduct analyses to investigate the effect of visual guidance in V-DPO. We consider ablations on the $\\gamma$-controlled strength of visual guidance, the calculation of vision-unconditioned distribution, and guidance inflation from normalization.\nStrength of Visual Guidance. Figure 4 illustrates the performance changes on AMBER with"}, {"title": "5.4 Further Analysis", "content": "We now investigate the distribution shifts in V-DPO and analyze the qualitative results on MMHalBench. Finally, we use the non-hallucination benchmark MMBench to assess the stability of our approach in general tasks in Appendix E.\nShifts of Distribution Gaps in V-DPO. Our ablation study (\u00a7 5.3) shows that preference learning can also shift the distribution gaps between accurate and hallucinatory samples in the vision-unconditioned case. In Figure 5, we show how V-DPO shifts the distributions across different preference data. Our V-DPO approach is more effective than vanilla DPO in enhancing the ability to determine image-contrast hallucination samples, with a shift of 11.01, compared with 9.37 in DPO, as measured by the log-likelihood pairwise preference data differences. For the response-contrast scenario, V-DPO also increases the discriminability with a shift of 19.17. Furthermore, we observe a smaller shift of 6.35 in V-DPO in the textual-only distributions compared with that of 7.58 in DPO, indicating the effectiveness of our approach to mitigate the over-reliance on language priors with visual guidance.\nQualitative Analysis on MMHal-Bench. We conduct qualitative analysis to investigate how V-DPO eliminates hallucination in the generated responses. Figure 6 compares the different generations of V-DPO and the baseline on three examples from MMHal-Bench. The first example, from the adversarial split, shows the significant efficacy of our approach in mitigating the language priors, which may provide a plausible but incorrect answer to the question; i.e. \u201cfour people eating\". In the third example, the model learns to justify its answer \u201cMacbook\" according to the specific visual clue of the \"Apple logo\" in the image. This indicates that our approach enhances visual understanding to elicit related details in the images, improving the"}, {"title": "6 Conclusion", "content": "We propose V-DPO, utilizing Classifier-Free Guidance (CFG) to integrate visual guidance in LVLM preference learning. Integrating visual guidance into the training process enhances visual context understanding via preference optimization, improving the accuracy and specificity of model generations. Extensive experiments on various preference data demonstrate the generalizability of V-DPO. We hope our work sheds light on visual guidance for more general tasks in LVLM alignment."}, {"title": "Limitations", "content": "The main limitations of our work come from two parts. The first one, regarding the V-DPO approach, is the unexplored domains where the language priors are important to guide LVLMs to provide correct answers. For example, preference pairs that prioritize the fluency of the generated text are not considered in our data construction. As this study mainly focuses on the over-reliance on language priors, we leave it to future work to explore more general scenarios where both visual and textual modalities are important to elicit the preferred responses. The second one, related to constructing our synthetic dataset, is the noise and bias introduced by the automatic generation pipeline, which may cause performance degradation during preference optimization. For future work, we may consider a more reliable and scalable way to conduct data filtering and reweighting to refine the quality of synthetic augmented data."}, {"title": "Ethics Statement", "content": "This work mainly focuses on enhancing visual understanding via preference optimization to mitigate hallucination in LVMs. One potential ethical concern may come from the data collection process for our synthetic preference pair construction. As the image manipulation process is conducted collaboratively among LVLMs, LLMs, and Stable Diffusion models, systematic bias may be introduced into the generated data. In this case, usage of our synthetic augmented data should be constrained within research-only targets. We leave it to future work to mitigate the bias in model-generated data to further improve the quality of our preference data."}, {"title": "B Implementation Details", "content": "We tune the initial SFT model, LLaVA-v1.5-7B, using our V-DPO and the vanilla DPO approaches with the highest learning rate 1e-6 through 4 epochs on both synthetic and human-annotated data scenarios. We adopt a batch size of 64 and set $\\beta$ = 0.1, following the DPO paper (Rafailov et al., 2023). We employ different weights of visual guidance on the synthetic ($\\gamma$ = 0.75) and human-annotated ($\\gamma$ = 0.0) data according to their sensitivity to the control strength. All experiments are conducted with a maximum of 4 \u00d7 40GB GPUs (NVIDIA A100)."}, {"title": "C More Details in Preference Data Construction", "content": "We choose the images from COCO (Lin et al., 2014), Visual-Genome (Krishna et al., 2017), Visual Commonsense Reaosning (VCR) (Zellers et al., 2018) as the seed set for our synthetic data augmentation pipeline, covering various types of visual content including daily-life scenes and drama-event or human-involved scenarios. Our result synthetic augmented data contains preference pairs, including image-contrast and response-contrast samples on visual instruction following, visual question answering, and region description tasks."}, {"title": "Implementation of Visual Guidance.", "content": "In Eq. 9, we disable gradient backpropagation on $\\varphi_\\theta(v, x, y)$ to maintain a stable textual-only distribution $\\pi_\\theta(\\cdot | x)$ during training. This aims to provide a reliable reference value to calculate the visual guidance. We further discuss the choice of $\\pi(\\cdot | x)$ in Section 5.3. Following the implementation of Liu et al. (2023c), we pass zeroes in place of the conditioning visual context to get the textual-only distribution:\n$$\\pi_{\\theta}(\\cdot | x)=\\pi_{\\theta}(\\cdot | 0, x)$$\nWith the integration of visual guidance, we modify $\\pi_{\\theta}(y | v,x)$ in vanilla DPO to be a non-normalized probability distribution, $\\pi_{\\theta}(y | v,x)\\varphi_\\theta(v,x,y)$. Empirically, this can progressively decrease the effect of visual guidance as the visual-conditioned and unconditioned distributions diverge from each other through training. To mitigate this problem, we follow Kornblith et al. (2023) to normalize it as:\n$$\\pi_{\\theta}(\\cdot | v, x) \\varphi_{\\theta}(v, x, \\cdot) \\propto \\varphi\\left(\\frac{\\gamma}{\\gamma-1}\\left(h_{\\theta}(v, x)+(\\gamma-1)\\left(\\hat{h}_{v}(v, x)-\\hat{h}_{o}(0, x)\\right)\\right)\\right)$$"}, {"title": "Prompt Templates to utilize LLMs to guide the image manipulation process.", "content": "<<<< Element Replacement >>>>\nSystem: You are a good assistant to help me do academic research.\nUser: I have an image with the caption: \u201cA train is passing by a church.\u201d. Substitute each of the following objects with something unexpected to create a sense of discordance: train, church in the format: [what] -> [what]. Provide a brief sentence explaining each substitution.\nAssistant:\n<<<< Captioning for Manipulated Images \u00bb\nSystem: You are a good assistant to generate new captions.\nUser: I have an original caption and a substitution operation. Return the new caption after conducting the substitution. The original caption is: A train is passing by a church. The substitution involves changing the train to an elephant. Return the updated caption.\nAssistant:\n<<<< Question Generation >>>>\nSystem: You are a good assistant to generate questions.\nUser: I have a pair of descriptions. Could you help me generate a question that will lead to different answers based on the two descriptions? Ensure that the question is suitable for both descriptions.\nThe first description is: A woman is cleaning her dining room.\nThe second description is: A robot is cleaning her dining room.\nReturn a question and the corresponding answers according to the two descriptions.\nAssistant:\n<<<< Distractor (Answer Candidate) Generation >>>>\nSystem: You are a good assistant to generate possible answers.\nUser: Given a question, please help me to generate some reasonable answers that are common in the real life.\nThe question is: Where is the bear sitting?\nA reasonable answer can be: In a grassy area.\nAn unreasonable answer can be: In a floating jelly beans. Please help me to generate several reasonable answers, and seperate each answer with \u201c|\u201d.\nAssistant:"}, {"title": "C.1 Prompts for Image Manipulation", "content": "We show the designed prompts to elicit element replacement ideas from LLMs such as ChatGPT (OpenAI, 2023) in and examples of generated preference pairs in Figures 7a to 7c."}, {"title": "C.2 Filtering via CLIPScore", "content": "shows the distributions regarding the difference in CLIPScore between positive and negative samples before filtering. We set a threshold $t=\\frac{CLIPScore_{p}}{CLIPScore_{n}} \\geq t = 1.5$ to approve the synthetic samples as a valid preference pair."}]}