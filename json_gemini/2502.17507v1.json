{"title": "C-3DPO: Constrained Controlled Classification for Direct Preference Optimization", "authors": ["Kavosh Asadi", "Julien Han", "Xingzi Xu", "Dominique Perrault-Joncas", "Shoham Sabach", "Karim Bouyarmane", "Mohammad Ghavamzadeh"], "abstract": "Direct preference optimization (DPO)-style algorithms have emerged as a promising approach for solving the alignment problem in AI. We present a novel perspective that formulates these algorithms as implicit classification algorithms. This classification framework enables us to recover many variants of DPO-style algorithms by choosing appropriate classification labels and loss functions. We then leverage this classification framework to demonstrate that the underlying problem solved in these algorithms is underspecified, making them susceptible to probability collapse of the winner-loser responses. We address this by proposing a set of constraints designed to control the movement of probability mass between the winner and loser in the reference and target policies. Our resulting algorithm, which we call Constrained Controlled Classification DPO (C-3DPO), has a meaningful RLHF interpretation. By hedging against probability collapse, C-3DPO provides practical improvements over vanilla DPO when aligning several large language models using standard preference datasets.", "sections": [{"title": "1. Introduction", "content": "The problem of ensuring that AI systems act in accordance with human preferences, also known as the alignment problem, has become a critical focus in machine learning. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising approach towards solving this problem (Christiano et al., 2017). RLHF proceeds by first learning a reward model (RM) from a winner-loser preference dataset followed by employing standard RL algorithms to maximize the learned RM while keeping the model close to a reference model.\nRecent years have witnessed the emergence of algorithms that solve the two RLHF sub-problems as a single optimization problem. Chief among these methods is the Direct Preference Optimization (DPO) algorithm (Rafailov et al., 2023). It proceeds by leveraging the closed-form solution of the RLHF objective and using the preference dataset to align the model, thus obviating the explicit reward-learning phase of RLHF. Since then, numerous extensions and successors of DPO have been proposed, highlighting the need for a deeper examination of this new class of algorithms in order to unify and connect their underlying principles, and to highlight and fix their potential pitfalls.\nIn this paper, we first present a classification framework that reformulates the family of DPO-style algorithms. The primary benefit of this framework is that it unifies and extends a variety of existing DPO-style algorithms. Most notably, with specific choices of classification labels and loss function, we recover the popular DPO (Rafailov et al., 2023) and IPO (Azar et al., 2024) algorithms, revealing them as special cases of our classification formulation. Moreover, unlike traditional RLHF approaches that rely solely on binary preference pairs, our classification framework can naturally incorporate richer information, such as ranked lists of preferences and auxiliary score information (e.g., ratings) about the responses.\nWe next leverage this classification perspective to investigate a peculiar behavior observed in several DPO-style algorithms: the decrease, or even collapse to zero, of the winner-loser probabilities. Note that prior work has documented this phenomenon (Adler et al., 2024; Xu et al., 2024; Pal et al., 2024; Fisch et al., 2024; Xiao et al., 2024; Shen et al., 2024; Wu et al., 2024; D'Oosterlinck et al., 2024). Using our proposed classification framework, we pinpoint the root cause of this behavior: minimizing the loss in the DPO-style of algorithms only provides a single constraint for learning two probabilities, leaving the problem under-constrained. Leaning on this insight, we then propose a new set of constraints that can provably fix the under-specified nature of the DPO-style algorithms, thus effectively addressing the probability-collapse issue. These constraints are designed to control the movement of"}, {"title": "2. Preliminaries", "content": "In this section, we present the key ingredients of preference optimization, which we will build upon in the subsequent sections. In this setting, we are given a dataset D of triplets (x, yw, y\u0131), where x is a prompt, while yw and y\u0131 reflect our preference in choosing yw over y\u0131 conditioned on x. We are also given a reference Tref serving as a guardrail.\nIn RLHF, we first employ D to train a parameterized RM, ro, and then use it to solve the following:\n$\\max_{\\pi_\\theta} E_{x} [E_{y \\sim \\pi_\\theta} [r_\\theta(x, y)] \u2013 \\beta K L(\\pi_\\theta(\\cdot|x)||\\pi_{ref}(x))],$ (1)\nwhere \u03b2 > 0 is a hyper-parameter denoting the relative importance of reward maximization against ensuring a low deviation from \u03c0ref. The RM is typically learned assuming that preferences follow the Bradley-Terry (BT) model:\n$p(y_w > y_l | x) = \\sigma(r(x, y_w) \u2013 r(x, y_l)),$ (2)\nwhere \u03c3(x) = 1/(1 + exp(-x)) is the sigmoid function and r is the latent reward of the annotator. Therefore, finetuning \u03c0\u03b8 in the RLHF approach is split into two stages: reward-learning using the BT model, followed by a policy optimization using (1).\nMore recently, a family of algorithms have emerged that solve the above two problems in a single stage, which is referred to as Direct Preference Optimization (DPO)-style algorithms. The key insight in DPO-style algorithms is that problem (1) admits the following closed-form solution:\n$\\pi^*(y|x) = \\frac{\\pi_{ref}(y|x) \\exp (r(x, y)/\\beta)}{Z(x)},$ (3)\nwhere Z(x) is the partition function and is generally intractable. We can rewrite (3) as\n$r(x,y) = \\beta \\log \\frac{Z(x)\\pi^*(y|x)}{\\pi_{ref}(y|x)}.$ (4)\nSubstituting r(x, y) from (4), Z(x) cancels out, leading to the optimization problem solved by DPO:\n$\\min_\\theta \u2013 \\sum_{(x,y_w,y_l) \\in D} \\log \\sigma( \\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} \u2013 \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{ref}(y_l | x)} ).$ (5)"}, {"title": "3. A Classification View of DPO-style Algorithms", "content": "In this section, we show that DPO-style algorithms can be viewed as classification algorithms. The standard classification setting has three main ingredients.\nFirst, we typically construct a classifier hypothesis space by defining a parametric form for probabilities assigned to each class, and then train the classifier to adjust these probabilities. We show that in DPO-style algorithms these probabilities are implicitly defined as follows:\n$P_\\theta(x, y_w, y_l) := softmax(r_\\theta(x,y_w), r_\\theta(x,y_l)),$ (6)\nwhere r\u03b8 is the reward defined in (4) having substituted \u03c0* with \u03c0\u03b8. It is straightforward to see that under distribution p\u03b8, the probability assigned to the winner (preferred) response yw, denoted by pw, does not depend on the partition function Z(x) and can be written as\n$P_\\theta (x, y_\\omega, y_\\iota) := \\frac{\\frac{(\\pi_\\theta (y_\\omega|x))^\\beta}{\\pi_{ref} (y_\\omega|x)}}{\\frac{(\\pi_\\theta (y_\\omega x))^\\beta}{\\pi_{ref} (y_\\omega|x)} + \\frac{(\\pi_\\theta (y_\\iota|x))^\\beta}{\\pi_{ref} (y_\\iota|x)}}.$ (7)\nThe probability assigned to the loser (dispreferred) response yl, denoted by p\u00e5, is defined similarly. Note that the distribution p\u03b8 in (6) is a function of \u03c0\u03b8, \u03b2, and \u03c0ref, and can be thought of as a generalization of the conditional probability of a response y given that y \u2208 {yw, y\u0131}.\nSecond, in the standard classification setting, the dataset gives us access to labels, which we use to extract the probabilities associated with each class. To obtain these target probabilities, we simply use any distribution p = (pw, pl) from the simplex A2, which is defined as the set of all vectors p in R2 satisfying pw, p\u00b2 > 0 and pw + p\u00b2 = 1. In the most basic case we just use the one-hot vector (pw,p') = (+1,0) akin to using hard labels. This indicates that we desire to always prefer the answer yw to yi when presented with a choice between the two. More generally, we can use soft labels, meaning that we put some non-zero weight behind each class. Soft labels have shown to be quite useful in classification (M\u00fcller et al., 2019), and by extension have been useful in preference optimization where we are less certain about the given preference Yw > Y\u03b9.\nThird, to solve a classification problem, we often define a differentiable loss function measuring the discrepancy between a distribution p\u03b8 in the hypothesis space and the target distribution p. We can use any standard loss L for classification leading to the optimization problem: $\\min_\\theta \\sum_D L(p_\\theta, p)$. The loss function should be non-negative L(p1, p2) \u2265 0, and satisfy L(p1, p2) = 0 if and only if p1 = p2. A good example is the CE loss.\nWe now show that a large number of DPO-style algorithms can be viewed as specific instances of this classification"}, {"title": "3.1. List of Preferences", "content": "Our classification framework can be extended to work with lists, rather than pairs, of preferences. In particular, assume that we have N responses for each prompt x, giving"}, {"title": "3.2. Auxiliary Information", "content": "A second important extension pertains to the definition of soft labels in our classification setting. So far we have only worked with soft labels that are fixed across the entire dataset, for instance, p := (pw,p') = (\u03c3(1/2), \u03c3(\u22121/2)) for all (x, yw, y\u0131) in IPO. These fixed labels are agnostic about any extra information we may have about our data triplets (x, yw, Y\u0131). However, in some applications we may have access to some auxiliary scores, sw, sl, (e.g., ratings) associated with each response, which can then be used to enrich our soft labels.\nMore formally, suppose now that our dataset is comprised of D = (x, yw, Sw, Yl, S\u0131). To obtain the soft labels we can employ, for instance, p = softmax(sw, s\u0131). Combining this with the IPO loss (8), we recover Distilled DPO (Eq. 7 in Fisch et al. 2024). Using the same soft labels, but with the CE loss recovers RPO (see Sec. 3.3.2 in Adler et al. 2024). We provide more details on both algorithms in Appendix D. These natural extensions further demonstrate that our classification framework is fairly general as well as sufficiently flexible to capture a large number of existing DPO-style algorithms."}, {"title": "4. A Pitfall of DPO-style Algorithms", "content": "We now leverage the classification framework developed in Section 3 to explain a peculiar behavior of several DPO-style algorithms, namely the decrease, or even the col-"}, {"title": "5. Constrained Controlled Classification for DPO (C-3DPO)", "content": "We now present a general family of constraints to fix the under-specified nature of the DPO-style algorithms described in Section 4. These constraints are on the probability mass of the winner-loser pair and we use them to control how much this mass changes from the reference policy, ref, to target policy \u03c0\u03b8. We then show how these constraints can be incorporated into any DPO-style loss function and propose our algorithms which we refer to as Constrained Controlled Classification for Direct Preference Optimization (C-3DPO).\nThe constraint, with respect to an arbitrary function 6 : R\u2192R, takes the general form:\n$\\varphi(\\pi_\\theta(y_w|x)) + \\varphi(\\pi_\\theta(y_l|x)) = \\varphi(\\pi_{ref}(y_w|x)) + (\\pi_{ref}(y_l|x)).$ (10)\nNote that the right-hand-side is fixed during training, so the two probabilities on the left-hand-side cannot move in the same direction.\nWe now generalize this intuition by showing that when the constraint function is monotonic and is added to the solution characterization of DPO-style algorithms (9), then we can control the direction of the movement of probability mass for winner-loser pairs.\nProposition 5.1. Let \u03c6 : R \u2192 R be a monotonic function and assume that (10) holds. Then, \u03c0\u03c1*(Yw|x) > Tref(Yw|x) and \u03c0\u03bf* (yi|x) < \u03c0ref(yl|x)."}, {"title": "5.1. Logarithmic Constraint \u03c6(x) := log x", "content": "Our first choice is to employ the logarithmic constraint:\n$\\log (\\pi_\\theta(y_w|x)) + \\log (\\pi_\\theta(y_l|x)) = \\log (\\pi_{ref}(y_w|x)) + \\log (\\pi_{ref}(y_l|x)),$ (12)\nwhich is nice to work with empirically in light of the fact that all terms are in the log-space. Moreover, these log probabilities are already computed in DPO, which would help with efficient implementation of the corresponding C-3DPO algorithm.\nRather than using hard constraints, it is easier to compute the deviation from the constraint using either l\u2081 or l2 norm, and then add it as a regularizer to the original DPO-style loss with a regularization parameter, \u03bb, that trades-off the relative importance of the two terms. Equipping the DPO"}, {"title": "5.2. Identity Constraint f(x) = x", "content": "A second interesting choice would be to simply use the identity constraint:\n$\\pi_\\theta(y_w|x) + \\pi_\\theta(y_l|x) = \\pi_{ref}(y_w|x) + \\pi_{ref}(y_l|x).$ (15)\nWhile (15) is also a plausible constraint, at first glance it is unclear how to implement it since the constraint is no longer in the log-space and is specified in terms of raw probabilities. Working with raw probabilities is prone to numerical underflow issues, thus, we would like to derive a constraint which is equivalent to (15) and operates in the log-space. To do so, we make use of the following lemma whose proof is reported in Appendix E."}, {"title": "Lemma 5.1.", "content": "For any two numbers a and b, we have\nlog(a + b) = log a \u2013 logo (loga \u2013 log b).\nApplying log to both sides of (15), we obtain\nlog (\u03c0\u03b8(Yw|x)+\u03c0\u03bf(y\u0131|x)) = log (\u03c0ref(Yw|x)+\u3160ref(yl|x)),\nwhich can be rewritten using Lemma 5.1 as\n$( \\begin{aligned} & \\log (\\pi_\\theta(y_w|x)) \u2013 log_\\sigma (\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)}) = \\\\ & \\log (\\pi_{ref}(y_w|x)) \u2013 log_\\sigma (\\log \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}) \\end{aligned} )$ (16)\nMoving from (15) to (16), we have rewritten the constraint entirely in the log-space, thus, avoiding numerical issues, and similar to the logarithmic constraint in Section 5.1, allowing a straightforward implementation of the corresponding C-3DPO algorithm. Equipping the DPO loss (5) with the logarithmic constraint (16), we obtain the following loss for C-3DPO:\n$( \\begin{aligned} \\min_\\theta & \u2013 \\sum_D log_\\sigma (\u03b2log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} \u2013 \u03b2log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}) + \\\\ & A  log (\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)}) + log_\\sigma (\\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)}) )^2 \\end{aligned} )$ (17)\nWe conclude the section by noting that unlike \u03c6(x) = log x, deriving an RLHF interpretation under f(x) = x is subtle. That said, an interesting property under f(x) = x is that the winner probability can increase only by an amount equal to \u03c0ref(ylx). This means that we will not put the entire probability mass behind yw. Thus, this constraint can serve as an additional guardrail to maintain a stochastic \u03c0\u03b8."}, {"title": "6. Experiments", "content": "In this section, we empirically evaluate C-3DPO across 2 standard datasets and 3 initial checkpoints."}, {"title": "6.1. Ultrafeedback Binarized", "content": "The original Ultrafeedback dataset comprises 64k prompts, each paired with four completions from various models. To construct Ultrafeedback-Binarized, GPT-4 assigns scores based on criteria such as helpfulness. UltraFeedback_binarized was then derived by selecting the highestscoring completion as yw and a randomly selected as y\u0131.\nFollowing Rasul et al. (2024), we align Zephyr-7B-SFT by performing various preference optimization algorithms on"}, {"title": "6.2. Reddit TL;DR", "content": "We then evaluate our proposed method on a summarization task with human-assigned scores for pairs of summaries. For this purpose, we employ the Reddit TL;DR dataset from Stiennon et al. (2022), specifically focusing on the posts under relationships and relationship_advice subreddits. We follow Amini et al. (2024) in creating the dataset and include pairs of summaries where one received a higher quality score than the other. The training and validation"}, {"title": "7. Related Work", "content": "A few recent papers have presented a unifying perspective on DPO-style algorithms. Notably, Tang et al. (2024) presented a generalization of DPO where different supervised learning losses are applied to the difference of implicit rewards, (f(x, y) \u2212 f(x,y1))/\u03b2. In contrast, we make a clear connection between DPO-style algorithms and classification, and also extend our results to lists and auxiliary information. A second notable example was to generalize from KL to any f-divergence when measuring the discrepancy between the target and reference model (Han et al., 2024). We also note that the first ingredient of our classification framework, namely \u03c0\u03bf, was used by Sharifnassab et al. (2024) to propose a soft version of DPO.\nAs mentioned in Section 4, earlier works studied the undesirable decrease of the winner probability during training. These works explained this phenomenon from different angles, and thus, proposed different losses to address it. Here we provide a brief overview of a number of these results, especially those that we experimentally compare against. Pal et al. (2024) proposed DPOP which addresses the phenomenon by adding the penalty term max (0, -r(x,yw)/\u03b2) within the log-sigmoid of the DPO loss (5). DPOP can also be viewed as DPO with a modified BT model. In this sense, it has similarities with the a-DPO loss (Shen et al., 2024).\nXiao et al. (2024) attributed the undesirable behavior to the contrastive loss of DPO not being scale-calibrated, i.e., ignoring the absolute values of implicit rewards, f(x, yw) and f(x, y). They address this by constraining the implicit rewards to a scale that matches the ground-truth rewards r(x, y). Thus, in their proposed loss, Cal-DPO, they add"}, {"title": "8. Conclusion & Future Work", "content": "In this work, we made two contributions to the preference optimization literature. First, we showed that many DPOstyle algorithms can be viewed as classification algorithms. In this classification framework, one may choose a specific classification label (soft or hard, using auxiliary information or not) and a specific loss to obtain a DPO-style algorithm. We believe that revealing this connection promotes a mutual transfer of ideas between the classification and preference optimization.\nThe classification framework also revealed that the underlying problem solved in DPO-style algorithms is underspecified and therefore susceptible to the collapse of both winner-loser probabilities. By endowing the DPO-style algorithms with our newly proposed constraints, we showed that the problem becomes specified, and thus, probabilitycollapse can be avoided.\nA curious observation we made is that while we do want to avoid probability collapse, it is also not true that increasing the probability of the winner will always lead to the best final model. This means that the best performance is usually achieved by increasing the probability of some of the unseen answers. Thus, in future, it is interesting to do a more systematic study of the kinds of unseen responses for which DPO-style algorithms increase the probability during training."}, {"title": "A. Derivations of IPO as a Classification Algorithm", "content": "We recall that IPO can be formulated as a classification problem with the soft labels p := (pw,p') = (\u03c3(1/2), \u03c3(-1/2)) and the loss given by\n$L(p_\\theta, p) = (log_\\frac{p^w}{p_\\theta} \u2013 log_\\frac{p^l}{p_\\theta} )^2$ (18)\nTo show that we indeed recover the IPO, we first note that\n$p^w = \\sigma(1/2) = \\frac{1}{1 + exp(-1/2)} = exp(1/2),$\n$p^l = \\sigma(-1/2) = \\frac{1}{1 + exp(1/2)}$\nwhere the second equality follows from the definition of the sigmoid \u03c3(x) = 1/(1 + exp(-x)). Moreover, using the definitions of pu (see (7)) and p\u00e5, we obtain that\n$\\frac{p^w}{p^l_\\theta} = \\frac{\\frac{(\\pi_\\theta (y_\\omega|x))^\\beta}{\\pi_{ref} (y_\\omega|x)}}{\\frac{(\\pi_\\theta (y_\\iota|x))^\\beta}{\\pi_{ref} (y_\\iota|x)}} = (\\frac{\\pi_\\theta(y_\\omega|x)\\pi_{ref}(y_\\iota|x)}{\\pi_\\theta(y_\\iota|x)\\pi_{ref}(y_\\omega|x)})^\\beta$\nPlugging these two developments to the loss in (18) yields\n$L(p_\\theta, p) = (log_\\frac{p^w}{p_\\theta} \u2013 log_\\frac{p^l}{p_\\theta} )^2 = (log (\\frac{\\pi_\\theta(y_\\omega|x)\\pi_{ref}(y_\\iota|x)}{\\pi_\\theta(y_\\iota|x)\\pi_{ref}(y_\\omega|x)})^\\beta \\frac{1}{exp(1/2)} )^2 = (\u03b2log(\\frac{\\pi_\\theta(y_\\omega|x)\\pi_{ref}(y_\\iota|x)}{\\pi_\\theta(y_\\iota|x)\\pi_{ref}(y_\\omega|x)}) \u2013 2 )^2$\nwhich is exactly IPO (see Eq. (17) of Azar et al. 2024)."}, {"title": "B. CDPO", "content": "Recall the CDPO (Mitchell, 2024) loss is given for any triplet (x, Yw, Y\u0131) by\n$- (1 \u2013 \u03b5) log \u03c3(\u03b2log (\\frac{\\pi_\\theta(y_\\omega x)}{\\pi_{ref}(y_\\omega|x)} \u2013 \u03b2log_\\frac{\\pi_\\theta(y_\\iota|x)}{\\pi_{ref}(y_\\iota|x)} ))  \u2013 \u03b5log \u03c3(\u03b2log (\\frac{\\pi_\\theta(y_\\iota|x)}{\\pi_{ref}(y_\\iota|x)} \u2013 \u03b2log_\\frac{\\pi_\\theta(y_\\omega x)}{\\pi_{ref}(y_\\omega|x)} ))$\nand then summed over all the triplets in the dataset D. In order to see CDPO as a classification with the soft labels p := (pw,p\u00b2) = (1 \u2212 \u03b5, \u03b5) and the CE loss, we will use the following technical fact\n$\\sigma(\u03b2log(a/b)) = \\frac{1}{1 + exp(-\u03b2log(a/b))} = \\frac{1}{1 + exp log \\frac{b^\\beta}{a^\\beta}} = \\frac{1}{1 + \\frac{b^\\beta}{a^\\beta}} = \\frac{1}{a^\\beta + b^\\beta}$\nTherefore, with a = $\\frac{\\pi_\\theta(y_\\omega x)}{\\pi_{ref}(y_\\omega|x)}$ and b = $\\frac{\\pi_\\theta(y_\\iota|x)}{\\pi_{ref}(y_\\iota|x)}$, we get from (7) that \u03c3(\u03b2loga/b) = pw. Similarly, we get that \u03c3(\u03b2log(b/a)) = p\u00e5. Therefore, the CDPO loss can be written as follows\n$- (1 \u2013 \u03b5) log p^w \u2013 \u03b5log p^l = \u2212p^w log p^w \u2013 p^l log p^l,$\nwhich is exactly the CE loss on the vectors p\u03b8 = (pw, po) and p = (pw,p')."}, {"title": "C. DPO (PL)", "content": "In this setting, we are given a dataset of the form D = {(x, Y1, Y2,...,YN)}. In order to recover, the DPO with Plackett-Luce (Rafailov et al., 2023), we need to generalize the definition of the probability vector pe from pairs as in (7) to the following N - 1 subsets of the list, namely the first N, then the first N \u2013 1, then first N - 2 and so on. More precisely, for any 1 < n < N we define\nP\u03b8(X, Yn, Yn+1,..., yv) = softmax((re(x, Yn), ro(X,Yn+1),...,re(x,yN))) ."}, {"title": "D. RPO and Distilled DPO", "content": "As we discussed in Section 3.2, RPO can be reformulated as a classification with soft labels, which are defined by p = softmax(sw, si). Therefore, we immediately see that\n$\\frac{p^w}{p^l} = \\frac{exp(s_w)}{exp(s_w) + exp(s_l)} = \\frac{1}{1 + exp(-(s_w - s_\\iota))},$\nand thus pw = \u03c3(sw \u2212 s\u03b9). Similarly, we get that p\u00b2 = \u03c3(\u2212(sw \u2212 s\u0131)). Thus, RPO can be seen as a generalization of CDPO where the soft labels are given by a certain score and not fixed. To recover the RPO loss we denote a =\n$\\frac{\\beta log \\pi_\\theta(y_\\omega x)}{\\pi_{ref}(y_\\omega|x)}$ and b = sw \u2212 s\u03b9. Then, we get that\n$\\frac{\\sigma(b) log \\frac{\\sigma(b)}{\\sigma(a)} + (1 \u2013 \u03c3(b)) log \\frac{1 - \u03c3(b)}{1 - \u03c3(a)}}{\u03c3(b)} = p^w log \\frac{p^w}{p_\\theta} + p^l log \\frac{p^l}{p_\\theta}$\nBy eliminating the constant terms (with respect to \u03b8) pw log p\u2122 + p' log p\u00b9, we indeed get the CE loss.\nTo recover the Distilled DPO (Equation (7) of Fisch et al. (2024)), we consider the soft labels p = softmax(sw, S1) with the loss of (8)\n$L(p_\\theta, p) = (log_\\frac{p^w}{p_\\theta} \u2013 log_\\frac{p^l}{p_\\theta} )^2$\nIndeed, if this case we have that\n$\\frac{p^w}{p^l} = \\frac{exp(s_w)}{exp(s_w) + exp(s_l)} \u2013 \\frac{exp(s_l)}{exp(s_w) + exp(s_l)} = \\frac{exp(s_w)}{exp(s_l)}$\nand therefore log(pw/p') = Sw \u2212 St. Combining this with (19) yields the desired result."}, {"title": "E. Proof of Lemma 5.1", "content": "First, we recall the lemma.\nLemma 5.1. For any two numbers a and b, we have\nlog(a + b) = log a \u2013 logo (loga \u2013 log b).\nProof. First, we write\n$a + b = a \\frac{a + b}{a} = a(1 + \\frac{b}{a})^-1$,\nwhich tanks to classical logarithmic rules yields that\n$log(a + b) = log a \u2013 logo (loga \u2013 log b).$"}, {"title": "F. C-3DPO implementation details", "content": "We show implementation details of C-3DPO-Log-l1, C-3DPO-Log-l2, C-3DPO-I-l\u2081, and C-3DPO-I-l2 below."}, {"title": "G. Ultrafeedback Binarized Claude 3.5 Sonnet v2 win rate prompt and hyperparameters", "content": "In this section we include the prompt used to generate win rates for the Ultrafeedback Binarized experiments. We use Claude Sonnet 3.5 v2 (AWS Bedrock model ID anthropic.claude-3-5-sonnet-20241022-v2:0) to generate win rates. We set the max_tokens to 1024, temperature to 0, and used default value 0.999 for top-p and top_k disabled.\nFor the following query to a chatbot, which response is more helpful?\nQuery: <prompt>\nResponse A:\n<one of the responses>\nResponse B:\n<the other response>\nFIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful. SECOND, on a new line, state only \"A\" or \"B\" to indicate which response is more helpful. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nMore helpful: <\"A\" or \"B\">"}, {"title": "H. Reddit TL;DR Claude 3.5 Sonnet v2 win rate prompt and hyperparameters", "content": "In this section we include the prompt used to generate win rates for the Reddit TL;DR experiments. We use Claude Sonnet 3.5 v2 (AWS Bedrock model ID anthropic.claude-3-5-sonnet-20241022-v2:0) to generate win rates. We set the max_tokens to 1024, temperature to 0, and used default value 0.999 for top_p and top_k disabled.\nWhich of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? A good summary is both precise and concise.\nPost: {prompt}\nSummary A:\n{baseline_response}\nSummary B:\n{generated_response}\nFIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your choice. Your response should use the format:\nComparison: <one-sentence comparison and explanation>\nPreferred: <\"A\" or \"B\">"}, {"title": "I. Additional win rates analysis of C-3DPO with Zephyr-7B-SFT aligned on Ultrafeedback Binarized", "content": "Figure 7 shows win rates comparison between Zephyr-7B-SFT aligned with DPO and C-3DPO. We align Zephyr-7B-SFT following Rasul et al. (2024) using DPO, C-3DPO-Log-l1, C-3DPO-Log-l2, C-3DPO-I-l1, and C-3DPO-I-l2 for one epoch, all C-3DPO use hyper-parameter X = 2 \u00d7 10\u22124. With each checkpoint, we generate responses using test split of Ultrafeedback Binarized using hyperparameters max_tokens=1000, temperature=1.0, top_p=0.9, top_k=50. Different from the head to head setting, we ask Claude to compare the generated response directly with the preferred response in the dataset. The win rates and standard errors are calculated based on 10 different inference runs."}]}