{"title": "SRTFD: Scalable Real-Time Fault Diagnosis through Online Continual Learning", "authors": ["Dandan Zhao", "Karthick Sharma", "Hongpeng Yin", "Yuxin Qi", "Shuhao Zhang"], "abstract": "Fault diagnosis (FD) is essential for maintaining operational safety and minimizing economic losses by detecting system abnormalities. Recently, deep learning (DL)-driven FD methods have gained prominence, offering significant improvements in precision and adaptability through the utilization of extensive datasets and advanced DL models. Modern industrial environments, however, demand FD methods that can handle new fault types, dynamic conditions, large-scale data, and provide real-time responses with minimal prior information. Although online continual learning (OCL) demonstrates potential in addressing these requirements by enabling DL models to continuously learn from streaming data, it faces challenges such as data redundancy, imbalance, and limited labeled data. To overcome these limitations, we propose SRTFD, a scalable real-time fault diagnosis framework that enhances OCL with three critical methods: Retrospect Coreset Selection (RCS), which selects the most relevant data to reduce redundant training and improve efficiency; Global Balance Technique (GBT), which ensures balanced coreset selection and robust model performance; and Confidence and Uncertainty-driven Pseudo-label Learning (CUPL), which updates the model using unlabeled data for continuous adaptation. Extensive experiments on a real-world dataset and two public simulated datasets demonstrate SRTFD's effectiveness and potential for providing advanced, scalable, and precise fault diagnosis in modern industrial systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Faults in engineering systems pose substantial risks to both performance and safety [29]. For instance, faults in steel hot rolling processes can lead to defective steel slabs, resulting in significant scrap losses and compromising worker safety. Similarly, faults in autonomous vehicles can cause navigation errors or system malfunctions, potentially leading to accidents and endangering passengers and pedestrians. Consequently, fault diagnosis (FD) \u2013 the process of detecting and identifying potential faults within these systems - is crucial for maintaining reliability, safety, and economic efficiency [1]. While recent research in computer vision has focused on industrial anomaly detection (IAD) [4, 10, 14, 15, 35], primarily addressing quality faults in end products using image data to identify statistical features and pattern changes, FD targets process faults. FD is essential for immediate fault detection and intervention, focusing on specific functional problems using sensor data to prevent system failures.\nFD techniques have evolved significantly over time. In the 19th century, FD began with basic limit checking and progressed to model-based FD, which involves physically modeling the entire industrial system's working process and incorporating statistical methods such as trend analysis and parameter estimation [16]. The evolution continued with the development of knowledge-based FD, which utilizes signal models and spectral analysis [27]. The advent of artificial intelligence and neural networks marked a new era of data-driven FD [8, 37]. Today, the explosion of data has further propelled the development of deep learning (DL)-driven FD methods. By leveraging large datasets and sophisticated DL models, these methods have significantly enhanced the precision and adaptability of FD, making them crucial for modern industrial environments [9]. Our work builds on this progression, aiming to address the challenges of real-time fault diagnosis in dynamic and large-scale systems with DL-driven FD methods."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 Traditional Data-driven-based FD", "content": "Traditional data-driven FD involves three steps: data collection, model training, and fault prediction. Let $X^{tr} = \\{x_1^{tr}, x_2^{tr},...,x_{n_{tr}}^{tr}\\}$ and $X^{te} = \\{x_1^{te}, x_2^{te}, ..., x_{n_{te}}^{te} \\}$ be the training and testing samples, respectively. Corresponding labels are $Y^{tr} = \\{y_1^{tr}, y_2^{tr},...,y_{n_{tr}}^{tr}\\}$ and $y^{te} = \\{y_1^{te}, y_2^{te},..., y_{n_{te}}^{te} \\}$, with $n_{tr}$ and $n_{te}$ being the number of samples. Labels range from 0 to c, where 0 indicates normal samples and 1 to c denote different fault categories, with c being the total number of fault categories.\nFeatures of collected samples are extracted by $\\phi(.)$ before model training. The FD model is then trained using the following loss function:\n$\\min \\sum_{i=1}^{n_{tr}} L(f(\\phi(x_i); \\theta), y_i),$ (1)\nwhere, $\\theta$ denotes the model parameters, $x_i^{tr}$ is the i-th training sample (i = 1, 2, ..., $n_{tr}$). For a new test sample $x_j^{te}$ (j = 1, 2, ..., $n_{te}$), the corresponding label $y_j^{te}$ can be predicted by the trained FD model as follows:\n$y^{te} = f(\\phi(x_j); \\theta).$ (2)\nEquations (1) and (2) show that fault diagnosis performance depends on the dataset quality. As monitoring data evolves, the model's performance degrades, necessitating data recollection and model retraining. This highlights a key limitation of traditional fault diagnosis methods in meeting real-world demands."}, {"title": "2.2 Online Continuous Learning", "content": "OCL [17] may address several limitations of traditional FD by enabling models to learn incrementally from streaming data. In OCL, monitoring data is collected over time as $X_t \\in \\mathbb{R}^{d \\times n} = \\{x_1, x_2,...,x_n\\}$ and $Y_t \\in \\mathbb{R}^{1 \\times n} = \\{y_1, y_2, \u2026\u2026\u2026, y_n \\}$, where d is the sample dimension, n is the number of samples, and t is the collection time. Thus, the loss function equation (1) becomes:\n$\\min \\sum_{i=1}^{t} L(f(\\phi(x_i^t); \\theta), y_i^t),$ (3)\nwhere, $x_i^t \\in X^t$, $y_i^t \\in Y^t$, and $\\theta_t$ are updated continuously as new data arrives. To update the model at time t without losing previous information, $\\theta_t$ is adjusted based on $\\theta_{t-1}$ and the loss function gradient. For models optimized by stochastic gradient descent (SGD) [31], the update rule is:\n$\\theta_t \\leftarrow \\theta_{t-1} - \\eta \\frac{1}{n} \\sum_{i=1}^{n} w_i \\nabla L(f(\\phi(x_i); \\theta_{t-1}), y_i),$ (4)\nwhere, $\\eta$ is the learning rate and $w_i$ is the weight of data $(x_i, y_i)$. Typically, $w_i = 1$ for all $i \\in [n]$, assuming all samples are equally important. This continuous learning process allows the model to adapt to new fault classes and variable working conditions without retraining from scratch.\nAlthough OCL addresses evolving monitoring data, it has a critical drawback: high model update costs when the data volume at time t is large. Coreset selection [22] mitigates this by selecting a smaller representative subset from the current batch, reducing data volume and computational requirements. This approach lowers training complexity, allowing fault diagnosis systems to efficiently manage large datasets and enhancing the scalability and performance of OCL models."}, {"title": "2.3 Motivation", "content": "Developing a robust FD framework using OCL and coreset selection is promising but faces challenges due to the unique characteristics of industrial monitoring data.\n* Redundancy: Monitoring data is highly redundant, making OCL methods inefficient for model updates. Coreset selection within each batch overlooks global information, leading to unnecessary updates and extra training costs when consecutive batches are similar.\n* Data Imbalance: FD data is imbalanced, with more normal operating data than fault data, and varying frequencies of different faults. This imbalance results in an unbalanced coreset, reducing FD system performance and reliability.\n* Labeled Data Scarcity: Collecting labeled monitoring data is challenging due to the time-consuming and labor-intensive nature of manual labeling. The lack of labeled data makes model updating difficult.\nTo address these challenges, we propose SRTFD, a scalable real-time fault diagnosis method through OCL. The details of this approach will be introduced in the next section."}, {"title": "3 PROBLEM STATEMENT", "content": "Achieving a realistic fault diagnosis framework in complex systems is challenging due to several key issues mentioned in the previous section. These problems are formulated in this section.\nLet $D^0 = (X, Y^0)$ represent the data used for model pre-training, and $X^u$ denote the arriving unlabeled data. The pseudo-labels $Y^u$ for $X^u$ can be predicted by well-trained model $f_{\\theta_0}(.)$. To handle the large-scale data that accumulates over time t, a small subset $S_t = (Z_t, V_t)$ is selected from the pseudo-labeled samples $U_t = (X^u, Y^u)$. The semi-supervised learning strategy is employed by incorporating a small amount of labeled data $D_t = (X_t, Y_t)$ to update the model along with the pseudo-labeled samples. Thus, the loss function at time t becomes:\n$\\min \\sum_{i=1}^{n} L(f(\\phi(x_i); \\theta_t), y_i) + \\sum_{j=1}^{s} L(f(\\phi(z_j); \\theta_t), v_j),$ (5)\nwhere $z_j \\in Z_t$, $v_j \\in V_t$, and s denotes the total number of samples in the subset $S_t$. The model parameter update rule becomes:\n$\\theta_t \\leftarrow \\theta_{t-1} - \\eta (A + B),$\n$A = \\frac{1}{n} \\sum_{i=1}^{n} w_i \\nabla L(f(\\phi(x_i); \\theta_{t-1}), y_i),$\n$B = \\frac{1}{s} \\sum_{j=1}^{s} w_j' \\nabla L(f(\\phi(z_j); \\theta_{t-1}), v_j),$\n(6)\nwhere, $w_i$ and $w_j'$ are the weights of the labeled and pseudo-labeled samples, respectively, and $\\eta$ is the learning rate. The buffer $B_t$ is crucial in OCL to avoid catastrophic forgetting. It contains data from the pseudo-labeled dataset $S_t$ and the labeled dataset $D_t$, formulated as $B_t \\coloneqq (S_t \\cup D_t)$. This allows the model to review and learn from past data while updating with new information.\nThe goal of SRTFD is to select an effective coreset from the current batch of data while considering historical data. First, the coreset must effectively represent the current batch of data. Thus, we have:\n$\\min \\sum_{j=1}^{s} L(f(\\phi(z_j); \\theta_t), v_j) - \\sum_{k=1}^{n} L(f(\\phi(x_k); \\theta_t), y_k)$ (7)"}, {"title": "4 METHODOLOGY", "content": "This section introduces the SRTFD framework, illustrated in Figure 1. The prediction and training processes are synchronized, with most data for model updating coming from previously unlabeled data and only a small portion being labeled. This approach is realistic for modern complex systems. The framework consists of three main components: Retrospect Coreset Selection (RCS), Global Balance Technique (GBT), and Confidence and Uncertainty-driven Pseudo-label Learning (CUPL). Each component will be detailed in the following subsections."}, {"title": "4.1 Retrospect Coreset Selection (RCS)", "content": "To optimize fault diagnosis (FD) tasks, we introduce Retrospect Coreset Selection (RCS). RCS selects a representative coreset by considering all historical data, and addressing memory constraints through efficient implementation. Specifically, we use the buffer in the system to store samples that encapsulate all historical data, enabling effective RCS. By assessing the similarity between incoming data and buffer data, we eliminate redundancy. When new data arrives, the buffer filters out redundant information, ensuring that coreset selection focuses on novel, non-redundant data. This approach also reduces computational load by bypassing updates when new batches closely resemble existing data.\nGiven the large-scale nature of the data, directly computing Euclidean distances between new samples and buffer entries is computationally intensive. To mitigate this, we employ batch-wise metrics and cluster the buffer data. Let $B_t = \\{(x_{b1},y_{b1}),..., (x_{bn}, Y_{bn})\\}$ represent the buffer at time t, where $x_i \\in (X_t \\cup X^u)$ and $y_i \\in (Y_t \\cup Y^u)$, with $i = 1, 2, ..., b_n$, and $b_n$ being the total number of buffer samples at time t. Labels $y_i$ fall within $\\{0, 1, ..., b_{ct}\\}$, where $b_{ct}$ is the number of classes at time t. We partition buffer data into $b_{ct}$ clusters based on these labels, denoted as $\\{X_1^b, X_2^b, ..., X_{b_{ct}}^b \\}$. By clustering the buffer data, we reduce the computational burden, enabling batch-wise comparison instead of individual distance calculations.\nFor the newly arrived data $X^u$, we employ the MiniBatchKMeans algorithm [33], a highly efficient clustering method designed for large-scale datasets, to partition the data into different clusters. This algorithm accelerates computation and reduces memory usage by processing data in small batches. By clustering $X^u$ into $u_c$ clusters, denoted as $\\{X_1^u, X_2^u, ..., X_{u_c}^u \\}$, where $X_i^u$ represents the $u_i$-th cluster and $u_i = 0, 1, ..., u_c$. The number of clusters $u_c$ must be less than or equal to the number of samples in the arriving batch $u_n$, and there is no clustering performed if $u_c$ equals $u_n$. Then, the similarity between $X_i^u$ and $B_t$ can be calculated by Kullback-Leibler (KL) divergence [21], which is denoted as:\n$d_{b_i,u_i} = KL(X_i^u||X_{u_i}^b)$. (10)\nIf $d_{b_i,u_i}$ is less than a threshold $\\tau$, the cluster $X_i^u$ is considered non-redundant. Then, we have\n$S_t = \\{ X_i^u| d_{b_i,u_i} \\geq \\tau\\}$ (11)\nThis buffer retrospective process ensures that only the most informative and non-redundant data points are retained in the coreset. Thus, the goal of $S_t \\cap S_T = \\emptyset$, for $T = 1, 2, . . ., t \u2212 1, is achieved."}, {"title": "4.2 Global Balance Technique (GBT)", "content": "As highlighted in the introduction, the imbalance problem significantly impacts the performance of fault diagnosis systems. Current online continual learning (OCL) methods rarely address this issue adequately. They typically attempt to mitigate it by selecting a coreset that appears balanced from the current batch data, prioritizing samples from less-represented classes. However, this often results in a pseudo-balanced coreset that does not reflect a truly balanced distribution. For instance, if a class has many samples in the buffer but few in the current batch, existing OCL methods will select more samples from this class, neglecting genuinely underrepresented classes. Furthermore, in cases of extreme imbalance, selecting a balanced coreset alone cannot resolve the problem, as the selected coreset often remains unbalanced. Therefore, we propose GBT, which addresses this issue based on two critical factors affecting fault diagnosis performance: the quality of training data and the frequency of model updates.\nFrom the perspective of training data, feeding a balanced dataset into the model will undoubtedly enhance its performance. However, the FD model is trained on both the current batch of data and the data stored in the buffer. Focusing solely on the current batch data makes it easy to obtain a pseudo-balanced coreset. Thus, it is essential to consider both the buffer samples and the categories, and the corresponding objective function (8) becomes:\n$\\min \\(\\sum_{l=1}^{b_{ct}} \\frac{1}{b_{ct}} - p_s^l\\)^2 + \\lambda \\(\\sum_{l=1}^{b_{ct}} \\frac{1}{b_{ct}} - p_b^l\\)^2 $ (13)\nwhere $p_s^l$ and $p_b^l$ represent the true probabilities of the $l$-th classes in the current coreset and buffer, respectively. $b_n$ represents the total sample size in the buffer, and $b_{ct}$ denotes the number of classes in the buffer at t time. This approach ensures a genuinely balanced dataset by selecting samples from the least represented classes in both the buffer and the current coreset.\nFrom the perspective of model updates, considering the scenario where certain classes have a significantly low number of samples, with $p_s^l << \\frac{1}{b_{ct}}$ and $p_b^l < \\frac{1}{b_{ct}}$, it becomes evident that selecting a balanced coreset is unfeasible. Therefore, we introduce the following loss function:\n$L = -\\sum_{i=1}^{n} (1-p_i) \\log(p_i) - \\alpha \\sum_{j=1}^{n} (1-p_j) \\log(p_j)$ (14)"}, {"title": "4.3 Confidence and Uncertainty-driven Pseudo-label Learning (CUPL)", "content": "The proposed CUPL module enhances the SRTFD training process by utilizing both labeled and unlabeled data. The critical components of this method include generating pseudo-labels and employing a selection strategy to incorporate these labels into the training set. Here, the pseudo-labels $v_j$ are determined directly by the maximum probability, expressed as $v_j = [\\max(p_j)]$. For the selection strategy, the conventional approach for selecting pseudo-labeled samples is to choose those with high confidence. However, since most newly arriving data are normal data, the model shows high confidence in predicting these normal instances but low confidence in identifying rare faults. Thus, relying solely on high-confidence selections can degrade overall performance in FD tasks.\nInspired by the work of Mamshad et al. [30], we introduce both positive and negative label selection. Specifically, samples are selected as positive pseudo-labels when the model's predicted probability is high, and as negative pseudo-labels when the probability is extremely low, which can be denoted as follows:\n$g_j = 1[p_j \\geq \\tau_p] + 1[p_j \\leq \\tau_\\eta],$ (15)\nwhere $g_j$ is an indicator that determines whether the sample j is selected. The indicator function $1[.]$ returns 1 if the condition inside the brackets is true and 0 otherwise. The thresholds $\\tau_p$ and $\\tau_\\eta$ are used to select positive and negative pseudo-labels, respectively.\nAdditionally, it has been demonstrated that predictions with low uncertainty are more likely to result in correct pseudo-labels, as shown by Mamshad's research. Therefore, we incorporate an uncertainty constraint into the positive and negative pseudo-label selection process. The uncertainty of model predictions can be estimated using Monte Carlo dropout (MCdropout) [25]. MCdropout estimates uncertainty by incorporating dropout during the inference phase. The process involves performing multiple stochastic forward passes through the network, each time applying dropout with a certain probability p. For a layer with output h, the output with dropout is $h' = hd$, where $d \\sim Bernoulli(p)$. Repeating this process T times results in a set of predictions $\\{\\hat{y}^{(1)}, \\hat{y}^{(2)},  .....,\\hat{y}^{(T)}\\}$. The mean prediction is calculated as $\\hat{y}_{mean} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{y}^{(t)}$, and the uncertainty is estimated using the variance $\\sigma^2 = \\frac{1}{T} \\sum_{t=1}^{T}(\\hat{y}^{(t)} - \\hat{y}_{mean})^2$. This approach is simple to implement, requiring no complex modifications to the model architecture, and provides valuable insights into the model's confidence in its predictions, especially with uncertain or complex input data.\nBy utilizing both the confidence and uncertainty of network predictions, a more accurate subset of pseudo-labels can be selected. The selection function becomes:\n$g_i = 1 [\\sigma^2 \\leq \\gamma \\text{ and } p_j \\geq \\tau_p] + 1 [\\sigma^2 \\leq \\gamma \\text{ and } p_j \\leq \\tau_\\eta],$ (16)\nwhere $\\gamma$ is the uncertainty thresholds."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Experiments were conducted on an Intel(R) Xeon(R) w7-3455 system with an NVIDIA RTX 6000 Ada Generation GPU (48GB GDDR6) and 512GB of RAM. The software used includes Python 3.12.4 and PyTorch 2.3.0 with CUDA 12.1.\nDatasets: (1) Hot Roll of Steel (HRS): The hot rolling process in steel manufacturing requires monitoring conveyor rollers to prevent billet deformation, surface defects, downtime, and increased costs. This dataset, from an actual hot rolling steel industry, includes data from 294 rollers recorded via motor current signals and converted to digital data. It encompasses five fault categories: roller swing, roller stuck, overcurrent, squeaking, and base deformation. Due to the random occurrence of these faults, the dataset is highly unbalanced and contains limited samples. (2) Tennessee-Eastman process (TEP): It is widely used to validate fault diagnosis methods. It includes 21 distinct fault categories with training and testing sets. In this study, 4320 normal samples and 800 samples for each fault are used. (3) CAR Learning to Act (CARLA): Yan et al. [39] collected this dataset using Dosovitskiy's [12] autonomous driving simulator. It features single-sensor and multi-sensor faults across three maps: rainy, cloudy, and sunny conditions. There are 9 categories of single-sensor faults and 4 categories of multi-sensor faults. Each map had a 30-minute simulation with sensor data sampled at 60Hz.\nTesting Scenarios: We conduct experiments on three datasets under two scenarios: class-incremental and variable working conditions. The proposed method supports online continuous learning, eliminating the need to divide data into training and testing sets. We first initialize the network using 1000 normal samples. As monitoring data arrives, labeled samples and reliable samples with predicted labels from previous tasks are used for model training while predicting the new data. For class-incremental scenarios, normal samples are randomly divided equally into each task, and each fault sample appears in each task in turn. For varying working conditions, only the CARLA dataset includes three conditions, while the HRS and TEP datasets each have one, as shown in Table 2. We gradually introduce noise into the HRS and TEP samples to simulate different working conditions. In the CARLA dataset, monitoring data evolve randomly from condition 1 to condition 2, and then to condition 3.\nCompeting Methods: We selected five methods, including state-of-the-art FD and four advanced OCL approaches, as benchmark algorithms for performance comparison: 1) Baseline: A basic model trained using experience replay (ER) [7], achieving online continuous learning by replaying examples from previous tasks. 2) Adversarial Shapley Value Experience Replay (ASER) [34]: Maintains learning stability and optimizes new class boundaries in the online class-incremental setting. 3) Camel [22]: An advanced OCL method that accelerates model training and improves data efficiency through coreset selection. 4) Averaged Gradient Episodic Memory (AGEM) [6]: Evaluates OCL efficiency in terms of sample complexity, computational cost, and memory usage. 5) MPOS-RVFL [18]: An advanced ML-based FD method focused on real-time fault diagnosis with imbalanced data.\nImplementations: The transformer network structure is used for all DL-based benchmarks. The basic neural network consists of an encoder, decoder, transformer encoder as the feature extractor, and a fully connected layer as the predictor. The encoder's hidden layer dimensions are 500, 500, and 2000, respectively, and the decoder's dimensions are 2000, 500, and 500. Model training used a learning rate of 0.0001 with the SGD optimizer, a maximum of 200 epochs, and a batch size of 100."}, {"title": "5.2 Performance Comparison", "content": "Given the imbalanced nature of the experimental datasets, accuracy is not reliable. Therefore, we used metrics specifically designed for imbalanced datasets: Recall, Precision, G-mean, and F1 score. Recall measures the proportion of actual positives correctly identified, while Precision measures the proportion of predicted positives that are correct. G-mean balances sensitivity and specificity, and the F1 score is the harmonic mean of Precision and Recall. We also compared model training time. Our results are averaged across all tasks after the final update, denoted as Avg-End-Rel for Recall, Avg-End-Pre for Precision, Avg-End-Gmean for G-mean, and Avg-End-F1 for F1 score.\nClass-incremental: The comparison results for class-incremental methods are shown in the left part of Table 4. Our method requires the least training time on all datasets and outperforms other DL-based methods. Although the ML-based method MPOS-RVFL requires less training time, its performance is significantly inferior to other DL methods. The ER method is competitive, outperforming our method by 2.3% in the four metrics on the CARLA-S dataset. However, ER's training time is 4.56 times longer, at 222.44 seconds compared to our 48.40 seconds. These results highlight the effectiveness of SRTFD in balancing high performance with reasonable training times across various datasets and conditions.\nVariable working conditions: The right part of Table 4 shows the performance comparison under variable working conditions across three datasets. SRTFD consistently outperforms other approaches, achieving the highest scores in most performance metrics. While the ER method outperforms our method on the CARLS-S dataset, it requires much more training time (ER: 456.87 seconds vs. SRTFD: 95.99 seconds). The CAMEL method maintains the second shortest training time across all datasets, leading on the TEP dataset with 105.18 seconds, 20.26 seconds shorter than SRTFD (125.44 seconds). However, CAMEL's overall performance is 9.7% lower than SRTFD in other metrics. These results highlight the effectiveness of SRTFD in balancing high performance with reasonable training times across various datasets and conditions."}, {"title": "5.3 Ablation Study", "content": "To evaluate the contribution of each component in SRTFD, we separately removed the RSC, GBT, and CUFL components and conducted experiments on all datasets. We compared the performance before and after removing each component under class-incremental and variable working conditions. From Table 4, the following three conclusions can be drawn."}, {"title": "5.4 Analysis and Discussion", "content": ""}, {"title": "5.4.1 Effects of coreset ratio and cluster number uc.", "content": "The impact of these two parameters is illustrated in Figure 3 and Figure 5 in the appendix. Panels (a-d) in Figure 5 show the performance across all datasets in the class-incremental scenario for coreset ratios of [0.5, 0.6, 0.7, 0.8, 0.9]. From this figure, it can be observed that as the coreset ratio increases, the number of samples in the coreset grows, leading to longer training times for the model. Additionally, when the coreset ratio is 0.6, the performance of the HRS, CARLS-S, and CARLS-M models is relatively better, and the training time is shorter. Additionally, Figures 5 (e-h) demonstrate the model performance with different numbers of clusters, uc, set to [3, 6, 9, 12, 15]. From the figures, it can be observed that although the training performance of the models is relatively stable across different uc values, the choice of uc is related to the number of categories in the dataset. For instance, the HRS dataset has 5 categories, while the TEP dataset has 22 categories. Therefore, on the HRS dataset, a uc value of 3 achieves a trade-off in performance across the four metrics and training time, whereas on the TEP dataset, a uc value of 12 achieves a balance."}, {"title": "5.4.2 Effects of parameters for pseudo-label samples selection.", "content": "For the selection of pseudo-label samples, there are three thresholds, positive label threshold $\\tau_p$, negative label threshold $\\tau_\\eta$, and uncertainty threshold of model prediction $\\gamma$. Figure 4 (a, b, c) and Figure 6 in the appendix demonstrate the performance of these three thresholds on different datasets. Regarding the positive label threshold, as shown in Figures 6 (a-d), higher values lead to better model performance due to using pseudo-labels with higher confidence. For the negative label threshold, as shown in Figure 6 (e-h), a value of 0.45 results in the best performance across all datasets. When the threshold exceeds 0.45, the model performance tends to decline, as illustrated in Figure 6 (f) for the TEP dataset. The impact of the uncertainty threshold on model performance is shown in Figures 6 (i-1). When the value is set to 2, the model performs better across all datasets."}, {"title": "6 RELATED WORK", "content": "Fault Diagnosis. Current state-of-the-art fault diagnosis (FD) techniques, summarized in Table 1, fail to meet real-world demands. Modern FD methods must adapt to new fault classes and conditions, handle scalable data, provide real-time responses, and function with minimal prior information, such as limited labeled samples. Despite advances, existing FD methods face significant challenges. For instance, OSELM [19], D-CART [11], and TCNN [38] struggle with data efficiency and scalability. OSSBLS [28] and ODDFD [23] manage limited labeled data but fail to address data imbalance effectively. AMPNet [15] handles large-scale data but lacks adaptability to new fault types and changing environments. Additionally, 1D-CNN [13] faces significant challenges with scalability. OLFA [39] manages limited labeled data but does not effectively address real-time response and high accuracy. Real-time response capabilities are crucial, yet only TCNN [38] shows low training costs conducive to quick responses, unlike AMPNet [15] and OLFA [39]. Effective diagnosis with minimal labeled samples remains a challenge for many methods. High performance and low training costs are essential but not prioritized by several methods, making them impractical in real-world applications. Thus, there is a critical need for advanced, scalable, and precise FD techniques that address these challenges effectively. In parallel, recent advancements in computer vision have focused on industrial anomaly detection (IAD) [4, 10, 14, 35]. However, existing IAD approaches are unsuitable for FD due to fundamental differences. FD targets specific functional problems using sensor data [15], while IAD identifies statistical features and pattern changes indicating abnormalities using image data [3, 36]. Thus, the clear distinction of use cases drives also a clear separation of concerns.\nOnline Continual Learning. OCL aims to develop models that can continuously learn from a stream of data without forgetting previously acquired knowledge [2]. Key methods include replay-based techniques like experience replay (ER) [7] and adversarial shapley value experience Replay (ASER) [34], which store and replay subsets of past data. Regularization approaches such as gradient episodic memory (GEM) [24] and averaged GEM (AGEM) [6] add constraints to protect important weights. Parameter isolation methods like progressive neural networks [32] and PackNet [26], allocate different model parameters to different tasks. Coreset selection methods like GoodCore [5] and Camel [22], further address scalability and memory efficiency issues of OCL by selecting representative data subsets. While OCL provides a robust framework, it requires significant adaptation to effectively meet the unique demands of real-time fault diagnosis in modern industrial environments."}, {"title": "7 CONCLUSION", "content": "This paper addresses high training costs, limited labeled samples, and imbalance issues in scalable real-time fault diagnosis using online continual learning with coreset selection. The proposed SRTFD framework comprises three key components: RCS, GBT, and CUPL. These innovations enhance fault diagnosis by improving model performance and efficiency in industrial settings. Extensive validation on real-world and simulated datasets shows significant improvements in four metrics while reducing training time. Our approach tackles data redundancy and imbalance, providing cost-efficient and robust solutions for industrial fault diagnosis. Future work will optimize the SRTFD framework and explore its applicability to other industrial processes and environments."}, {"title": "A REPRODUCIBILITY", "content": "1. Environment Requirements\nEnsure you have all the necessary Python packages by installing them from the provided 'requirements.txt file.\n2. Data Sources\n- HRS Dataset: This dataset is private and specific to the requirements of cooperative factories.\nTEP and CARLS Datasets: These two datasets are included in our publicly available code.\n3. Setting Up and Running SRTFD\ni. Install Required Packages\nMake sure you have Python installed. Then, navigate to the project directory and install the required packages using the following command: pip install -r requirements.txt\nii. Run SRTFD\nExecute the main script to start the SRTFD process:\npython3 general_main.py -data TEP -num_tasks 22 -cl_type nc -agent SRTFD -num_runs 1 -N 1000\npython3 general_main.py -data TEP -num_tasks 22 -cl_type vc -agent SRTFD -num_runs 1 -N 1000\npython3 general_main.py-data CARLS_S-num_tasks 10-cl_type nc -agent SRTFD -num_runs 1 -N 1000\npython3 general_main.py -data CARLS_S -num_tasks 10 -cl_type vc -agent SRTFD -num_runs 1 -N 1000\npython3 general_main.py -data CARLS_M-num_tasks 5 -cl_type nc -agent SRTFD -num_runs 1 -N 1000\npython3 general_main.py -data CARLS_M-num_tasks 5 -cl_type vc -agent SRTFD -num_runs 1 -N 1000\nAdditional Resources For more detailed instructions and documentation, please refer to the project's test.bash file or the official documentation provided with the project."}, {"title": "B MORE EXPERIMENTAL RESULTS", "content": "The"}]}