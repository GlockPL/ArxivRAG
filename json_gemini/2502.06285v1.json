{"title": "End-to-End Multi-Microphone Speaker Extraction Using Relative Transfer Functions", "authors": ["Aviad Eisenberg", "Sharon Gannot", "Shlomo E. Chazan"], "abstract": "This paper introduces a multi-microphone method for extracting a desired speaker from a mixture involving multiple speakers and directional noise in a reverberant environment. In this work, we propose leveraging the instantaneous relative transfer function (RTF), estimated from a reference utterance recorded in the same position as the desired source. The effectiveness of the RTF-based spatial cue is compared with direction of arrival (DOA)-based spatial cue and the conventional spectral embedding. Experimental results in challenging acoustic scenarios demonstrate that using spatial cues yields better performance than the spectral-based cue and that the instantaneous RTF outperforms the DOA-based spatial cue.", "sections": [{"title": "I. INTRODUCTION", "content": "The extraction of a desired speaker from multi-microphone mixtures containing multiple simultaneous speakers is essential in numerous modern applications and devices, including virtual assistants, hearing aids, and smartphones.\nIn recent years, multi-microphone algorithms based on deep neural networks (DNNs) have emerged. In [1]\u2013[3], the network directly processes multi-microphone signals to separate speakers in the scene. However, the absence of a beamformer structure complicates the explainability of the spatial properties of these approaches.\nIn another category of multi-microphone audio processing, known beamforming criteria or the beamformer weights are estimated directly [4]\u2013[12]. Studies, such as [10], [11], demonstrate that the spatial properties of the filter-and-sum operation can be preserved.\nWhen side information about the desired speaker is available, the speaker separation task is referred to as target speaker extraction (TSE). This side information can take various forms, such as a \u201cvoice signature\" obtained during an enrollment stage, spatial information like the DOA of the desired speaker, or visual cues, such as lip movements. A comprehensive survey on speech extraction methods is available in [13]. In [14]\u2013[18], voice signatures are used to extract the desired source from a mixture recorded by a single microphone. This is typically achieved by inferring a speaker embedding from an enrollment stage. In addition to the speaker embedding, speaker extraction can also leverage spatial information, usually inferred from multi-microphone measurements. The spatial information can be incorporated through DOA estimation and/or by steering a beamformer [19]\u2013[29]. The works in [28], [29] specifically analyze the spatial properties of the multi-microphone extraction method. TSE algorithms that employ \"voice signature\" for enrollment do not fully exploit spatial information, albeit processing multichannel data. Integrating the target speaker's DOA as a spatial cue may enhance performance. However, it is well established that RTF-based beamformers typically outperform DOA-based beamformers in reverberant environments [30].\nTo address these limitations, we propose utilizing the instantaneous RTF features of the desired speaker. Furthermore, we conduct an extensive comparison between the proposed RTF-based method and alternative approaches that use spectral and DOA enrollment features, along with a comparison to the minimum variance distortion beamformer (MVDR) beamformer. Our results demonstrate that the proposed method consistently outperforms all other approaches."}, {"title": "II. PROBLEM FORMULATION", "content": "A scenario comprising Q concurrently active speakers, captured by J microphones in a reverberant and noisy environment, is addressed. The problem is formulated in the short-time Fourier transform (STFT) domain, with $k \\in \\{0, . . ., K - 1\\}$ and $t \\in \\{0,...,T - 1\\}$ representing the frequency index and time-frame index, respectively, with T and K the total number of time-frames and frequency bands, respectively.\nThe observed signal, as received by the microphone array, can be modeled as:\n$x(t, k) = \\sum_{q=1}^{Q} h_q(k) \\cdot s_q(t, k) + n(t, k) \\cdot h_n(k) + v(t, k),$ (1)\nwhere $s_q(t, k)$ denotes the clean, anechoic speech signal of the q-th speaker, $h_q(k)$ is a J \u00d7 1 vector comprising the time-invariant acoustic transfer functions (ATFs) relating the q-th source and the microphone array, $n(t, k)$ is a directional noise source, $h_n(k)$ is a J \u00d7 1 vector comprising the ATFs relating the noise and the microphone array, and $v(t, k)$ represents the sensor noise.\nWe focus on the scenario where only two concurrent speakers are present, namely Q = 2, referred to as the desired speaker $s_d(t, k)$ and the interference speaker $s_i(t, k)$. Define the reverberant speech sources as received by the microphones as $\\check{s}_q(t, k) = s_q(t, k)h_q(k)$.\nDenote $\\check{s}_{ref}(t, k)$, the enrollment signal for $s_d(t, k)$ and the respective reverberant signal as received by the microphone array as $s_{ref}(t, k)$. Given the mixed signal $x(t, k)$ and the"}, {"title": "III. PROPOSED MODEL", "content": "In this section, we present the proposed TSE architecture, utilizing various types of enrollment features.", "subsections": [{"title": "A. Architecture", "content": "The proposed framework consists of a multi-channel encoder, a decoder, and an additional encoder for each enrollment feature. The multi-channel encoder processes the mixture signal, while the enrollment encoder extracts feature-specific embeddings. The decoder then utilizes both encoders to separate the target speaker. Unlike [18], Siamese encoders are not implemented in this study, as the input to both encoders has a different nature, as will be elaborated in the sequel.\nThe mixture encoder design employs multiple convolution layers, a two-dimensional batch normalization layer, and a 'ReLU' activation function. Subsequently, the channel and frequency dimensions are merged, and a fully connected (FC) layer is used to reduce dimensionality. A single self-attention layer is then applied. Finally, one of the alternative reference embedding vectors, described in the following paragraphs, is multiplied with each vector along the frame dimension of the mixture embedding on a frame-by-frame basis.\nThe decoder architecture consists of six self-attention layers and a FC layer to restore the original dimension. To enable the use of skip connections as needed, transpose-convolution layers are employed to adapt to the convolution layers in the encoder. Finally, a self-attention layer is applied."}, {"title": "B. Alternative Enrollment Information", "content": "In this study, we employed the Real-Imaginary (RI) components of the STFT as the features of the mixture signal. As for the reference features, we explored various alternatives. Two primary approaches emerged: utilizing prior spatial information or leveraging the speech attributes of our target speaker. We utilized either the instantaneous RTF or the oracle DOA in the former option. In the latter approach, we utilized the embedding of the single channel (SC) model, which does not involve spatial information."}, {"title": "1) RTF Features:", "content": "While ATF estimation is a blind problem, the RTF estimation is a non-blind problem. The RTF, proposed in [31], is widely used for beamforming and localization of sound sources. The RTF of the q-th source w.r.t. microphone $m\\in \\{1, ..., J\\}$ is defined as:\n$r_q(k) = \\frac{1}{h_{mq}(k)} h_q(k)$. (2)\nThe RTF encodes the spatial information and will be utilized as an auxiliary feature for the proposed model. In this study, assuming $s_{ref}(t, k)$ is noiseless, we employ the instantaneous estimate of the RTF [32] as the ratio of the STFT of the microphone signals:\n$\\tilde{r}_{d}(t, k) = \\frac{s_{ref}(t, k)}{s_{ref,m}(t, k)},$ (3)\nwhere $s_{ref,m}(t, k)$ represents the enrollment signal captured by the m-th microphone, an arbitrarily selected reference microphone. In our simulation study, we use the enrollment of the desired speaker, but in practice, any signal from the desired location can be utilized. The encoder architecture for the instantaneous RTF features parallels that of the mixture encoder described earlier. The encoder's output, which is an embedding of the input features, is averaged across the frame dimension to yield a single representation vector, guiding the model toward the desired speaker."}, {"title": "2) DOA Features:", "content": "As an alternative to the instantaneous RTF, we provided the TSE framework with the oracle DOA of the desired speaker. It is important to note that, in real-world scenarios, the DOA must be estimated. In environments with high reverberation levels, accurate estimation becomes more challenging, which could impact the performance of the extraction model. Unlike the RI features of the STFT and RTF, which share the same dimensions, the DOA is an integer. Consequently, we modified the encoder architecture to allow it to learn the DOA representation.\nUsing a lookup table, we introduce an embedding vector that is learned for each DOA. Each DOA is used to select a corresponding row in the table. This vector is then passed through a self-attention layer. Finally, the embedding vector is used in the bottleneck, as described in III-A."}, {"title": "3) Spectral Features:", "content": "In addition to the previously discussed design options, we explored a scenario where spatial information about the desired speaker is unavailable, and the speaker's voice signature is provided instead. To achieve this, we employed the initial stage of the SC model in [18] with the m-th microphone signal. We then use the embedding vector derived from the enrollment signal as it was used with the DOA and RTF features. This approach shifted the model's attention from the spatial information of the desired speaker to their speech characteristics."}]}, {"title": "C. Objective function", "content": "To train the proposed model for the extraction task, we used the time-domain scale-invariant signal-to-distortion ratio (SI-SDR) loss function, which is known to be effective in blind source separation (BSS) tasks.\nTo further enhance the training process, we employed a technique where, for each training sample, we used the same mixture and swapped the desired and interference signals. The corresponding enrollment signal was then used for each of the extracted sources. The two losses were subsequently averaged, resulting in the following expression:\n$L_{SI-SDR} = \\frac{1}{Q} \\sum_{q=1}^{Q} SI-SDR (\\hat{s}_q, s_q)$ (4)"}, {"title": "IV. EXPERIMENTAL STUDY", "content": "We evaluate the proposed approach on a simulated dataset. It consists of 40000 utterances for the training set, 5000 for the validation set, and 300 for the test set. Each example is created by randomly choosing two utterances from the LibriSpeech database and convolving the signals with four channel room impulse response (RIR) simulated by the Image Method [33]. The simulations were conducted in a room with dimensions uniformly distributed in U[3,10] m and a reverberation time of U[0.2, 0.8] sec. Microphones were spaced 8 cm apart and placed at least 0.7 m away from the walls. Source positions were uniformly distributed in U[0\u00b0, 180\u00b0], with distances from the microphones in the range U[1,4] m.\nFurthermore, to enhance the model's spatial resolution, we introduced directional noise into the mixture with SNR drawn in the range U[-5,20] dB. The noise source was extracted from the audiolabs' dataset\u00b9 and convolved with 4-channels RIR originating from the same room as the two primary speakers. The model's task becomes more challenging when the noise exhibits directional characteristics because relying on spatial references to identify the desired speaker is complicated by the presence of another directive source. To simulate sensor noise, we introduced pink noise at an signal-to-noise ratio (SNR) of 20 dB into the mixture, in addition to the directional noise.\nFor the mixture simulation, we randomly selected two signals from each speaker, designating one as the desired signal and the other as the enrollment. Both signals from each speaker underwent convolution with the same RIR. This choice is based on the assumption that there are brief intervals where only the noiseless desired speaker is present, making it suitable for use as an enrollment signal. Both signals must bear the same spatial information to ensure spatial consistency between the desired source and the corresponding enrollment. The utterances of the desired and interference sources are summed together with the reverberant noise and sensor noise to form the mixture signal."}, {"title": "B. Algorithm Settings", "content": "The speech and noise signals were drawn from the database and downsampled to 8 [KHz]. The frame size of the STFT is set to 256 samples with a 50% overlap. Only the first 129 frequency bins are processed due to the symmetry of the discrete Fourier transform (DFT).\nFor the training procedure, we employed the Adam optimizer [34] with a learning rate of 0.001 and a training batch size of 14. The weights are initialized randomly, and the signal lengths are varied randomly for each batch."}, {"title": "C. Evaluation Measures", "content": "To assess the effectiveness of the proposed algorithm, we employ two evaluation metrics: SI-SDR and short-time objective intelligibility (STOI) [35]. The former indicates the efficiency of speaker separation, while the latter indicates audio intelligibility."}, {"title": "D. Compared Methods", "content": "We compared our proposed methods with our prior work [18], using only a single-channel input. Additionally, we compared the proposed methods with two variants of the MVDR beamformer. The MVDR beamformer aims to estimate the desired signal with minimal distortion while simultaneously minimizing noise. Denoting $g_d(k)$ as the MVDR beamformer at frequency k, the estimated signal is given by:\n$\\hat{s}_d(k,t) = g(k) x(t, k),$ (5)\nwhere H stands for the Hermitian operator. The MVDR weights are given by:\n$g_d(k) = \\frac{Q^{-1}(k)r_d(k)}{r_d(k)Q^{-1}(k)r_d(k)},$ (6)\nwhere Q(k) is the spatial covariance matrix of the noise and the interference. The RTF-based MVDR aims at estimating the reverberant desired signal, as captured by a reference microphone. Given a directional noise and an additional (directional) speaker, we expect Q(k) to exhibit two main eigenvalues. Unlike the linear constraint minimum variance (LCMV) beamformer, the MVDR design does not include a constraint to place a null toward the interfering source. Instead, interference suppression is handled implicitly as part of the noise minimization.\nWe implemented two variants of the MVDR beamformer. In the first variant, we assume the availability of a two-second interval containing only the desired speaker and noise signals, as well as an additional two-second segment containing only the noise signals (i.e., the interference is inactive). The RTF of the desired speaker is then estimated using the covariance-whitening procedure [36]. To estimate the spatial covariance matrix Q(k), we assume access to a segment containing interference plus the noises. The estimate is calculated as:\n$\\hat{Q}(k) = \\frac{1}{T} \\sum_{t=0}^{T-1} a(k,t) a^H(k,t)$ (7)\nwhere $a(k,t) = \\check{s}_i(t,k) + n(t, k) \\cdot h_n(k) + v(t, k)$. This procedure constitutes the estimated MVDR beamformer.\nIn the second variant, we assume that noiseless enrollments are available. Hence, the spatial covariance matrix of the noise can be substituted with the identity matrix. This procedure constitutes the Oracle MVDR beamformer."}, {"title": "E. Results", "content": "The SI-SDR and STOI results for all methods evaluated on the dataset described above are shown in the middle part of Table I. All three proposed variants outperform the single-channel model and the estimated MVDR. Among these variants, the algorithm using the instantaneous RTF as a feature vector achieves the best performance. \nTo further evaluate our model, we simulated a scenario where both speakers have the same DOA but differ in their distances from the array. Specifically, we generated data where two speakers are aligned with the array's center, while directional noise originates from a different direction. In this setup, a reference-based DOA model would fail to distinguish the speakers due to their shared DOA. Conversely, an RTF-based approach can differentiate the sources, as it captures spatial information derived from multiple arrivals caused by room reverberation and signal propagation. We therefore assess the performance of both the Proposed-RTF variant and the RTF-based MVDR.\nThe numerical results are in the bottom of Table I. The results show that the oracle MVDR outperforms the proposed method in the intelligibility score, but our model excels when considering separation capabilities. The separation results are on par with those obtained for the case of different DOAs, which is quite remarkable."}, {"title": "V. CONCLUSIONS", "content": "This study introduces a model for target speaker extraction (TSE) using a noiseless enrollment signal by utilizing the RTF associated with the desired speaker. We evaluate the effectiveness of different features, specifically the known DOA and spectral characteristics. Additionally, we present a comparison with the MVDR beamformer. Our results demonstrate the benefits of leveraging the RTF, even when both speakers originate from the same direction."}]}