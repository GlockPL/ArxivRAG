{"title": "Personalized News Recommendation System via LLM Embedding and Co-Occurrence Patterns", "authors": ["ZhengLi", "KaiZhang"], "abstract": "In the past two years, large language models (LLMs) have achieved rapid development and demonstrated remarkable emerging capabilities. Concurrently, with powerful semantic understanding and reasoning capabilities, LLMs have significantly empowered the rapid advancement of the recommendation system field. Specifically, in news recommendation (NR), systems must comprehend and process a vast amount of clicked news text to infer the probability of candidate news clicks. This requirement exceeds the capabilities of traditional NR models but aligns well with the strengths of LLMs. In this paper, we propose a novel NR algorithm to reshape the news model via LLM Embedding and Co-Occurrence Pattern (LECOP). On one hand, we fintuned LLM by contrastive learning using large-scale datasets to encode news, which can fully explore the semantic information of news to thoroughly identify user preferences. On the other hand, we explored multiple co-occurrence patterns to mine collaborative information. Those patterns include news ID co-occurrence, Item-Item keywords co-occurrence and Intra-Item keywords co-occurrence. The keywords mentioned above are all generated by LLM. As far as we know, this is the first time that constructing such detailed Co-Occurrence Patterns via LLM to capture collaboration. Extensive experiments demonstrate the superior performance of our proposed novel method.", "sections": [{"title": "1 INTRODUCTION", "content": "Excellent recommendation algorithms play a huge role in solving information overload and improving user experience. In news platforms, NR algorithms provide users with personalized news. Those NR algorithms are ones that mines users' potential interests and preferences based on his/her historical click sequences and predicts the click probability of candidate news sets.\nHowever, news is timely, which means news reports highlight recent events in order to quickly convey information to the public. That is why there are serious cold start problems in NR systems. Most of the news items that NR systems will face in the future will be ones that have never been seen before. Therefore, In NR field, our use of ID is not as flexible as in other sequential recommendation fields (such as movie recommendations, product recommendations).\nAt the same time, in order to achieve accurate recommendations, news semantic mining is important. How to process news collaborative information and deeply mine news semantic information are two key issues for accurate prediction.\nThe Glove method, as a text encoding method, is widely used in NR. And it utilizes co-occurrence statistical information of words to learn vector representations of words. Inspired"}, {"title": "2 RELATED WORK", "content": "2.1 NR\nThere are some classic NR algorithms. NPA[7] Uses the attention mechanism to establish item-level representation and user-level representation respectively. And CNN is used to learn the"}, {"title": "2.2 LLM for NR", "content": "LKPNR[12] uses LLM embedding to mine news text semantic information and uses KG information to represent the collaborative relationship between items. However, LLM is not fine-tuned with news-related corpus and the KG entities are sparse so it performs poorly. Most importantly, it does not get rid of the traditional embedding method, but is only an additional. GNR leverages LLM to generate news theme and user interest. Then this method adds theme level representation to the news model, as well as add interest level representation to the user model. But as far as we know, it has not escaped the traditional embedding method either."}, {"title": "3 BACKGROUND", "content": "Background is presented in this section. We define_$\\mathcal{N}_{click} = \\{n_1, n_2, n_3, \u2026, n_m\\}$_ as the clicked news sequence, where m is the number of click news items. Each news item contains the following information: newsid\u3001title\u3001abstract\u3001category\u3001subcategory and can be denoted as $n_i$ =\n_(newsid_i, t_i, a_i, c_i, sc_i)_ , where newsid_i, t_i, a_i, c_i, sc_i representing separately newsid\u3001title\u3001abstract\u3001category and subcategory. We define $\\mathcal{N}_{candidate} = \\{n_1, n_2, n_3, \u2026, n_l\\}$ as the candidate news, where 1 is the number of candidate news items. Our task target is to predict the click propability of candidate news items."}, {"title": "4 PROPOSED MODEL", "content": "4.1 LLM2vec\nWe use LLM2vec[1] to get news embedding. We choose LLM2vec with supervised contrastive learning.\nFirst, we fintune LLM2vec model using public datasets (E5 dataset) and the news dataset (MIND). To get training materials related with news, we reconstract the MIND news dataset to the new format, One example is shown in Fig.2"}, {"title": "4.2 Keywords Co-occurrence", "content": "After generating keywords, in order to explore collaboration, we construct co-occurrence relationships using the historical click sequences of news items and keywords generated by LLM."}, {"title": "5 EXPERIMENTS", "content": "5.1 Dataset and Experimental Settings\nIn our experiment, we use MIND[5] dataset, which is shown in table 2. The MIND dataset is divided into two sizes, where the smaller one is obtained through random sampling from the larger one. Due to limited resources, in this paper, we mainly conduct experiments on the smaller one. The number of heads and the output dimension of each head are set to be consistent."}, {"title": "5.2 Performance Evaluation", "content": "From the table3, it can be seen that our method(LECOP) achieved the best performance. From the comparative experiment of PPSR, it can be seen that when only LE (LLM embedding) is used, the optimal effect is not achieved, but with the addition of COP (Co-Occurrence Patterns), the effect reaches its optimum. Therefore, co-occurrence patterns are necessary."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce LLM embedding and Co-occurrence patterns into the NR system. First of all, to enhance semantic comprehension ability, we finetune LLM via contrast learning on a large scale datasets including MIND datasets and several public datasets. Then, to provide richer collaborative information, we propose three types of occurrence patterns including news ID co-occurrence, Item-Item keywords co-occurrence and Intra-Item keywords co-occurrence. Correspondingly, we can obtain three types of graphs separately. Using graph embedding, we obtain co-occurrence embedding. Finally, we combine LLM embedding and co-occurrence embedding in news model to replace traditional Glove semantic encoding in NR. Extensive experiments demonstrate the superiority of our novel method."}]}