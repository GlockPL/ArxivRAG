{"title": "Roles of LLMs in the Overall Mental Architecture", "authors": ["Ron Sun"], "abstract": "To better understand existing LLMs, we may examine the human mental (cognitive/psychological) architecture, and its components and structures. Based on psychological, philosophical, and cognitive science literatures, it is argued that, within the human mental architecture, existing LLMs correspond well with implicit mental processes (intuition, instinct, and so on). However, beyond such implicit processes, explicit processes (with better symbolic capabilities) are also present within the human mental architecture, judging from psychological, philosophical, and cognitive science literatures. Various theoretical and empirical issues and questions in this regard are explored. Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs. The results are synergistic combinations (in several different senses simultaneously).", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), which are usually based on the Transformer neural network model (Vaswani et al., 2017), having an enormous number of adjustable parameters, and trained on a massive amount of data, have achieved spectacular successes. These models have found their way into large-scale applications, dealing with not only texts but also images, voices, and other modalities. However, at the same time, unlike humans, existing LLMs suffer from some fundamental shortcomings, including limited abstract reasoning and planning capabilities, lack of human-like generalization, limited reliability and trustworthiness, lack of explainability, and so on. To improve such LLMs, it is reasonable that we look into, and draw inspirations from, the study of the human mind, which seems to be able to overcome these problems (at least to some extent).\nIn the opposite direction and complementarily, understanding and modelling of the human mind can benefit from better understanding and utilizing LLMs. Understanding of the human mind has always relied on latest technological advances: over time, from simple"}, {"title": "2. Propositions 1 and 2: Natural Division of Implicit and Explicit Processes", "content": "To address Proposition 1, a quicky review of some background regarding intuition and instinct is in order."}, {"title": "2.1. Intuition", "content": "First, what is intuition that LLMs are supposed to capture? According to New Oxford American Dictionary, intuition is \u201cthe ability to understand something immediately, without the need for conscious reasoning\u201d. According to Merriam-Webster Dictionary, it is \u201cthe power or faculty of attaining direct knowledge without evident rational thought and inference\u201d. That is, intuition denotes thoughts without conscious reasoning or thinking --- quick, automatic, based presumably on contexts, patterns, and experiences (Sun & Wilson, 2014).\nHowever, one may view intuition as a form of reasoning nevertheless: Reasoning encompasses explicit processes on the one hand and implicit processes (intuition) on the other (Sun, 1994). Intuition, as well as insight resulting from intuition, are arguably indispensable elements of reasoning, supplementing and guiding explicit reasoning (Helie & Sun, 2010). Both intuition and explicit reasoning are guided in turn by one's motives, needs, goals, and so on (Sun & Wilson, 2014)\nTo make this notion more concrete to get a better sense of it, we may examine some specific characteristics of human intuition, as gathered from empirical research in psychology and cognitive science. In fact, there are sizable literatures out there. For instance,\n\u2022 Implicit Learning: First, intuition may develop on the basis of (repeated) experiential learning, which often occurs without conscious awareness (especially of details). For"}, {"title": "2.2. Instinct", "content": "The next question is: What is instinct that LLMs are also supposed to capture?\nAccording to Merriam-Webster Dictionary, it is \u201cbehavior that is mediated by reactions below the conscious level\u201d. According to New Oxford American Dictionary, it is \u201can innate, typically fixed pattern of behavior in animals in response to certain stimuli\u201d. However, in the second definition, we should cross out \u201cinnate\u201d and \u201cin animal\u201d, in order for it to be consistent with the first definition, notwithstanding the fact that instincts are indeed largely rooted in our evolutionary history, guiding automatic responses to stimuli, in humans and animals alike.\nAs with intuition, to better understand instinct, we may examine specific characteristics of human instinct. Some characteristics can be garnered from empirical research in psychology,"}, {"title": "2.3. Beyond Intuition and Instinct", "content": "Beyond intuition and instinct, what else is there in the human mind that is important?\nIntuition and instinct may be lumped into what one may term implicit processes (which, of course, may also include, e.g., low-level sensory and motor processes, and other processes of similar nature; see Reber, 1989; Sun, 2016). Now, beyond implicit processes, what else is needed to understand the full extent of the human mind? What is still missing?\nQuite a few things. Here is a partial list to begin with. First, a symbolic processing capability with productivity, compositionality, and systematicity (i.e., more than just occasional cases of apparent symbol manipulation) is characteristic of human thoughts and languages, but not adequately captured by intuition and instinct (recall the debates in the 1980s; Fodor, 1975; Fodor & Pylyshyn, 1988). Also missing is rigorous logical reasoning (more than just some simple fragments of logic), which (educated) humans are clearly capable of (Bringsjord et al., 2023). More generally speaking, explicit human thinking, which is controlled (as opposed to being automatic), deliberate (as opposed to being associative or reactive), effortful, usually working memory intensive, and often rule-governed, are needed beyond intuition and instinct (as argued by, e.g., Evans, 2003; Kahneman, 2011; Sun, 1994). And so on and so forth.\nTherefore, what this discussion seems to point to is the presence of dual processes in the human mind, as argued by researchers from psychology, philosophy, and other disciplines. Dual-"}, {"title": "2.4. A Few Crucial Questions Regarding Intuition", "content": "Given such a DPT, some questions immediately arise concerning its implications for the Propositions 1 and 2 that were introduced earlier regarding LLMs. These questions include:\n\u2022 Can LLMs adequately capture human intuition (as well as instinct and other implicit processes) as posited in the DPT?\n\u2022 Is there a difference between behavior (by LLMs) exhibiting intuition and intuition itself?\n\u2022 Why can LLMs capture human intuition, computationally speaking (beyond just the hypothesis of the DPT)?\n\u2022 Can LLMs capture more than intuition (or generally, more than implicit processes), computationally (that is, in contrast to the DPT)?\n\u2022 Are symbolic processes needed beyond LLMs, computationally? (That is, is the DPT correct in positing such dual processes?)\nThe first question above, which concerns adequacy of LLMs, can be addressed in an empirical way (as what will follow). However, even if LLMs can capture intuition, an objection frequently brought up is that one may capture intuition-related behavior, but not intuition itself. So addressing the second question is naturally of importance. Then we need to also look into computational characteristics of LLMs, as well as various philosophical speculations concerning them, to see why and how LLMs can capture intuition computationally. The last two questions"}, {"title": "2.4.1. Empirically, Can LLMs Capture Intuition?", "content": "Although focus will be on intuition, our discussion is applicable to other implicit processes (especially instinct) as well. Below we examine some examples of empirical confirmations of capturing various aspects of intuition by LLMs.\nFor instance, Han et al. (2023) showed that an LLM, when performing property induction tasks, captured various effects found in human performance of this task, including effects of premise typicality, premise diversity, premise-conclusion similarity, and so on. That is, humans and (at least some) LLMs showed similar inductive biases, which were notably not based on logic or any other formal systems.\nSimilarly, Dasgupta et al. (2022) discovered that an LLM showed content-sensitivity in reasoning in ways similar to humans: It was more accurate when the logically correct hypothesis was also believable content-wise. The work indicated that humans and LLMs similarly relied on semantic content in reasoning, rather than on logic alone. See also Saparov & He (2022) for other effects in reasoning by LLMs."}, {"title": "2.4.2. Is There a Difference Between Behavior Exhibiting Intuition and Intuition Itself?", "content": "However, we must also address the second question asked earlier: Is there a difference between behavior exhibiting intuition and intuition itself? Some claim that LLMs can capture behavior exhibiting intuition but not intuition itself. Is there such a difference?\nTo address this question, we may look at an analogous question: Is there any difference between behavior exhibiting understanding and understanding per se? John Searle's Chinese room argument seems to suggest that there is (Searle, 1980, 1990). For the sake of lengths, we cannot possibly get into this messy debate that has been occurring for long in any detail. But it is worth pointing out that Searle's Chinese room cannot possibly, in reality, produce realistic behavior exhibiting understanding by any objective measures or tests. For example, apply the Turing test as originally specified by Turing (1950) It will, without question, fail the test (due to complexity, speed, and other difficulties). One also cannot easily explain this failure away by relying on the questionable distinction between competence and performance. So, the lesson seems to be: No understanding means no behavior exhibiting understanding --- There is no dissociation there at all. Likewise, there is likely no dissociation between intuition and behavior exhibiting intuition: If there is no intuition, there is likely no (reasonably broad range of)"}, {"title": "2.4.3. Computationally, Why Can LLMs Capture Intuition?", "content": "Then, why are LLMs capable of capturing intuition (and behavior exhibiting intuition), computationally speaking? Merely through text-based training for next-token prediction on a large Transformer-based neural network model? To address this question, meta-theoretically, in terms of the relationship between words (text) and the world, it is useful to quote Mollo & Milli\u00e8re (2023): \u201cIt is important to distinguish between the proximate and ultimate functions performed by LLMs during training and inference\u201d. A proximate function may give rise to an ultimate function, where the proximate function may be next-token prediction, but the ultimate function is capturing intuition. \u201cThere are compelling reasons to believe that even LLMs trained solely on text can extract information about the world by means of the causal interactions of the agents whose linguistic outputs are reflected in the training data\u201d (Mollo & Milli\u00e8re, 2023).\nSomeone else put it even more bluntly: \"Text is generated by the real world, and therefore one"}, {"title": "2.4.4. Computationally, Can LLMs Capture More Than Intuition?", "content": "Assuming LLMs can capture intuition, now the question is whether LLMs can capture, computationally, more than intuition, for example, rigorous logical reasoning or, more generally, systematic symbolic processing. Currently, the answer seems to be no (see, e.g., Bubeck et al., 2023; Chang & Bergen, 2024; Lenci, 2024). But eventually, the answer is maybe. This is because, as they get bigger, LLMs are getting better: better reasoning, fewer errors, less hallucination, and so on (as demonstrated by researchers, e.g., Chang & Bergen, 2024; Hagendorff et al., 2023; Han et al., 2024; Sartori & Orru, 2023; etc.). So, maybe eventually quantity might make up for quality: A larger number of parameters perhaps could lead to a qualitatively different outcome. This is therefore an open question. As an analogy and a cautionary tale, for example, Hubert Dreyfus predicted that computers would never defeat best human chess players, because computers would not have the power of human intuition (Dreyfus & Dreyfus, 1986). DeepBlue showed that he was wrong: Brute force raw computing power (with some knowledge engineering) may substitute for human intuition."}, {"title": "2.4.5. Why Are Symbolic Processes Needed Beyond LLMs, Computationally?", "content": "Proposition 2 claims that, in addition to_LLMs, symbolic processes are also needed. Now we come to a crucial question: Why should LLMs not be the only thing that is needed to fully capture the human mind?\nWithin the context of identifying the limitations of LLMs, Lenci (2024) proposed the distinction between simple associations and formal reasoning and argued that LLMs were good at the former but not the latter. Lenci's proposal immediately brings to mind William James' (1890) distinction between \u201cempirical thinking\u201d and \u201ctrue reasoning\u201d, which was a construct well argued for and time tested. Another idea that also comes to mind is the distinction between"}, {"title": "3. Propositions 3 and 4: Synergistic Combination of LLMs and CCAS", "content": "Having addressed the first two propositions, we now turn to the last two propositions. To recap, on the basis of the first two propositions, we need to further show that:\n1. Dual-process CCAs, as an overarching framework that incorporates both implicit and explicit processes, can help LLMs, as they can readily enhance LLMs with symbolic capabilities and deal with the interaction between LLMs and symbolic components.\n2. LLMs can in turn help CCAs, by providing CCAs with the capability of capturing human intuition (and other implicit processes) better and more completely.\nThe overall goal is to harness the strengths of both LLMs and CCAs, while mitigating their weaknesses. All of these objectives above can be accomplished in a cognitively/psychologically realistic or plausible way. Given the extensive discussion of Propositions 1 and 2 earlier, to argue for these two points above, it suffices to examine an example of CCAs as proof of possibility.\nRecall that there have been many DPTs out there (as cited before). Likewise, there have been many CCAs, including Soar (Rosenbloom et al., 1993), ACT-R (Anderson & Lebiere,1998), and Clarion (Sun, 2002, 2016). But the intersection of the above two categories seems very small. Among dual-process CCAs, Clarion (Sun, 2016) stands out, as it is well developed in terms of the co-existence and interaction of the two types of processes, and it is based on the DPT described in Section 2.3. (For other possibilities, see, e.g., Booch et al., 2021.)\nNote also that, although CCAs have been in development since the 1970s and meant to capture human performance in a wide range of activities, their actual capabilities have been limited, not always in keeping with technological advances. For instance, some CCAs were initially conceived in the 1970s and relied on technology available then. Though some new"}, {"title": "3.1. The Original Clarion Framework", "content": "A quick introduction to Clarion is in order. Clarion consists of four major subsystems: ACS, NACS, MS, and MCS (Sun, 2016). See Figure 1. ACS stands for the action-centered subsystem, which is for dealing with actions, involving procedural (i.e., action-centered) knowledge. NACS stands for the non-action-centered subsystem, which is for reasoning and memory, involving declarative (i.e., factual) knowledge. In addition, there are the motivational subsystem (MS) for dealing with motivation, and the metacognitive subsystem (MCS) for regulating other subsystems.\nClarion centers on dual processes: Each subsystem involves both implicit and explicit processes, at the bottom and the top \u201clevel\u201d (i.e., part) of each subsystem, respectively. Computationally, one \u201clevel\u201d is symbolic and the other neural-network-based. The two are inter-connected: The symbolic representations at the top level, which are in the forms of \u201cchunk\u201d nodes (each representing a concept, defined, however, via a set of microfeatures at the bottom level) and rules connecting these nodes, are linked to corresponding neural representations at the bottom level (i.e., microfeatures). The two levels interact in this way to generate combined outcomes. Thus, in Clarion, implicit and explicit processes are separate but interacting, leading to synergistic outcomes as mentioned before (Sun et al., 2001, 2005). Among different kinds of"}, {"title": "3.2. The Enhanced Clarion LArchitecture", "content": "Enhancing Clarion with LLMs is straightforward; only a few relatively simple steps are needed. Essentially, at the bottom level of Clarion, LLMs are used to capture implicit processes, while symbolic explicit processes, largely unchanged, remain at the top level of Clarion.\nSpecifically, different implicit processes are captured separately: Intuition is captured by an LLM at the bottom level of NACS, while instinct is captured by another LLM at the bottom level of ACS. That is, two separate LLMs reside at the bottom level of these two subsystems, respectively, capturing two different kinds of implicit processes. On the other hand, MS and MCS serve as the basis of intuition and instinct (Sun & Wilson, 2014), which themselves involve implicit processes captured by LLMs. So, the overall structure of Clarion remains the same: LLMs at the bottom level for capturing implicit processes, and symbolic structures and representations at the top level for capturing explicit thinking.\nThis enhanced cognitive architecture is named Clarion_L, where L indicates the role of LLMs. It is Illustrated by Figure 2, where LLMs capture implicit processes."}, {"title": "4. Concluding Remarks", "content": "It is obviously important (and expedient), at the present time, to produce larger and larger LLMs, by curating and using more and more training data and running them with enormous computing power, in order to show increasingly better performance of LLMs. However, it is also crucial to explore alternative approaches and/or perspectives We should not put all eggs in one basket. It is equally or even more important that we continue to look into cognitive science, psychology, neuroscience, and so on. They were inspirations in the past resulting in the advances that we currently see, and they should continue to be inspirations for future advances. New, better ideas and methods may emerge that way from \"reverse engineering\u201d of the human mind or brain, concerning cognitive architectures, dual processes, intuition, reasoning, and so on. Moreover, and maybe more importantly, combinations of LLMs and CCAs can also help us to\nbetter understand, and to better capture in computational models, the human mind, likely leading to the next generation of theoretical and practical tools.\nWhat has been emphasized in the present article is the importance of dual-process computational cognitive architectures (and consequent hybrid neuro-symbolic approaches) in addressing the limitations of currently existing models and systems. In particular, Clarion provides a theoretical foundation for the integration of CCAs and LLMs, contributing to synergistic integration of the two subdisciplines for more robust, more intelligent, and more human-like systems."}]}