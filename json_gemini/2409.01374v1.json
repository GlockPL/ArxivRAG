{"title": "H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark", "authors": ["Solim LeGris", "Wai Keen Vong", "Brenden M. Lake", "Todd M. Gureckis"], "abstract": "The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis benchmark designed to test challenging out-of-distribution generalization in humans and machines. Since 2019, limited progress has been observed on the challenge using existing artificial intelligence methods. Comparing human and machine performance is important for the validity of the benchmark. While previous work explored how well humans can solve tasks from the ARC benchmark, they either did so using only a subset of tasks from the original dataset, or from variants of ARC, and therefore only provided a tentative estimate of human performance. In this work, we obtain a more robust estimate of human performance by evaluating 1729 humans on the full set of 400 training and 400 evaluation tasks from the original ARC problem set. We estimate that average human performance lies between 73.3% and 77.2% correct with a reported empirical average of 76.2% on the training set, and between 55.9% and 68.9% correct with a reported empirical average of 64.2% on the public evaluation set. However, we also find that 790 out of the 800 tasks were solvable by at least one person in three attempts, suggesting that the vast majority of the publicly available ARC tasks are in principle solvable by typical crowd-workers recruited over the internet. Notably, while these numbers are slightly lower than earlier estimates, human performance still greatly exceeds current state-of-the-art approaches for solving ARC. To facilitate research on ARC, we publicly release our dataset, called H-ARC (human-ARC), which includes all of the submissions and action traces from human participants.", "sections": [{"title": "1 Introduction", "content": "In the last several years, large language models (LLMs) have reached impressive performance on a wide variety of benchmarks, demonstrating competency in natural language understanding, coding and mathematics [1, 15]. With larger and more powerful LLMs, many benchmarks have had a limited shelf life, with performance rapidly increasing to human or superhuman levels [10]. In contrast, The Abstraction and Reasoning Corpus (ARC, [5]) has proven to be a persistent and formidable challenge for state-of-the-art AI systems.\n\nThe ARC benchmark [5] was designed to evaluate broad generalization, measuring how algorithms handle a broad category of novel tasks given just a few examples each. Each task requires inferring an underlying transformation rule or program from a series of training input-output pairs which consist of abstract visual grids (see Figure 1), and to use this rule to correctly generate an output grid given a novel test input. Although visually simple, the tasks are conceptually rich and challenging, requiring the identification of compositional rules involving objects and relations, geometry, counting, visual instructions, and logical operations.\n\nPrevious attempts at benchmarking human performance on ARC found human accuracy to be 83.8% correct, which was estimated using a semi-randomly selected subset of 40 tasks"}, {"title": "2 Methods", "content": "We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC in two separate phases (extending the subset of 40 training tasks previously collected and described in [8]). Each task has 1\u201310 training input-output pairs, and 1-3 test input-output pairs. While only a small proportion of tasks have multiple test input-output pairs (16 and 19 pairs in the training and evaluation set respectively), we opted to evaluate humans using only the first test example for all tasks. On average, 11.8 participants completed each of the 400 training tasks while 10.3 participants completed each of the 400 evaluation tasks."}, {"title": "2.1 Design", "content": "We recruited 784 participants (60.2% male, 37.3% female, 2.5% other) on the training set tasks and 948 participants (51.1% male, 46.1% female, 2.8% other) on the evaluation set tasks from Amazon Mechanical Turk using the CloudResearch2 platform to ensure high quality data [7]. Participants were between 18 and 77 years old (M=39.8, SD=10.4). They were compensated $10 and were also given a bonus of $1 if they succeeded at a randomly selected task and its written solution description was judged adequate by the experimenters.3"}, {"title": "2.2 Participants", "content": "We evaluated humans using the same evaluation procedure proposed in the original paper describing the ARC benchmark [5]. In particular, human participants were allowed three attempts per task to generate a correct solution, and were only given minimal feedback on whether each submission attempt was correct or not.\n\nUser Interface. Participants were first given instructions about the experiment and explanations about the different aspects of the ARC user interface. As in previous experiments [8], the user interface closely mirrored the original interface proposed by [5] (see Figure 2). The interface allowed participants to select different colors and either edit one cell at a time or multiple selected cells. More sophisticated tools allowed the participant to copy and paste a selection from the test input to the test output grid or use the flood fill tool to change the color of all neighbouring cells of the same color to a new color. Participants could resize the grid height and width as well as copy the full test input grid to the test output grid. A reset button allowed participants to revert the output grid back to the initial state, a 3 \u00d7 3 black grid. Finally, unlike in previous iterations of the interface, we added an additional tool allowing participants to undo actions and revert the state of the output grid to the previous state before the last action was taken. At any point in time, the participant could click the help button to display the full set of instructions.\n\nTutorial: At the beginning of the experiment, participants were provided with animated instructions outlining the user interface with an example task, and then asked to solve the same task to familiarize themselves with the interface. A relatively simple task\u2074 was given to participants for the tutorial and they were required to generate the correct test output to proceed (see Figure 2 for an example). After the tutorial, participants were asked to answer several comprehension questions to make sure they understood the instructions. The"}, {"title": "2.3 Experiment", "content": "For both ARC datasets, we report a range of performance values that reflect different ways of thinking about our estimate of human performance on ARC tasks and its inherent uncertainty (see Figure 5). In general, we find that human performance is higher on the training set compared to the evaluation set of ARC, validating previous intuitions about the relative difficulty between the two splits. Yet, despite the drop in performance on the evaluation set, humans still greatly outperform current state-of-the-art approaches to ARC, and we report and compare performance with two recent LLM-based solutions from the ARC prize public leaderboard (see Table 1).\n\nThe first LLM-based solution [9] we report involves minimal prompting of Claude 3.5 Sonnet [3] using text-only representations of ARC problems. Input and output training examples as well as test inputs are labelled and represented as lists of lists. The model is instructed to generate a JSON formatted list of lists response representing the output grid after applying the"}, {"title": "3 Results", "content": "We computed accuracy by calculating the proportion of successful submissions on each task after three attempts or less and averaged across all tasks. We also estimate pessimistic and optimistic performance values. In the pessimistic case, for each participant that completed $k < 5$ tasks, we sample $5 - k$ random tasks and assume failure. Conversely, in the optimistic case, we repeat the same procedure but assume success on every sampled task.\u2079 We run 1000 simulations for each case, imputing missing data using the sampled tasks and outcomes. We then take the resulting average mean task success rate to compute pessimistic and optimistic estimates which we report in brackets. Our results suggest an estimated average task accuracy of 76.2% (SD=21.5%, [73.3%, 77.2%]) on the training set of ARC. We also report average task accuracy based on participants' first and second attempts, resulting in average task accuracy of 59.9% (SD=24.8%, [57.6%, 61.5%]) and 72.6% (SD=22.9%, [69.7%, 73.7%]) respectively.\n\nParticipants solve ARC training tasks in 1.3 attempts on average, with the modal and median number of attempts being one. Of the 400 training tasks, we find 74 tasks (18.5% of the training set) for which all participant who attempted the task generated the correct solution after three submissions or less. Conversely, we also find 5 tasks (1.3% of the training set) which no participants were able to solve correctly after three attempts (see Figure 3).\u00b9\u2070 Finally, we find that 40.1% of participants solved all training set tasks they were presented and that 8.6% of participants solved none (see Figure 6).\n\nIndependent samples t-tests suggest that evaluation tasks are significantly harder for people than training tasks, t(798) = 7.67, p < .001. We estimate that the average task accuracy after three attempts on the evaluation set is 64.2% (SD=22.8%, [55.9%, 68.9%]). In addition to this result, we report a first and second attempt average task accuracy of 47.8% (SD=23.2%, [41.6%, 54.6%] and 60.2% (SD=23.3%, [52.4%, 65.4%]) respectively."}, {"title": "3.1 Performance", "content": "Although we don't find any conclusive indicators of why the evaluation set is harder than the training set, we report our attempt to understand what drives the increased difficulty. In particular, we examined two aspects of each dataset which we hypothesized to be contributors to difficulty: test output grid size and time spent on problems.\n\nOutput grid size. An independent samples t-test confirmed that output grid size (number of grid cells) is significantly larger on average in the evaluation set (M=235.1, SD=246.7) than in the training set (M=136.2, SD=164.9), t(798) = 6.81, p < .001. Although we find that grid size is mildly correlated with difficulty on the training set, r(398) = \u22120.16, p = .001, we did not find a similar relationship between output grid size and performance in the evaluation set r(398) = -0.02, p = .649.\n\nTime spent on problems. Reaction time or time spent thinking about a problem, has been shown to reflect value of computation in chess games [13], relates effort allocation and resulting scores in IQ tests [4] and has a strong history in psychology as a window into underlying cognitive processes. We performed a number of temporal analyses based on various timing-related aspects of the human behavioral data.\n\nFirst, we computed the time taken from seeing a new task to submitting a final solution for each participant and task. We then normalized these values by the number of attempts taken by each participant on each task. An independent samples t-test showed that participants spend significantly less time solving tasks from the training set (M=4m 19s, SD=4m 4s) than tasks from the evaluation set (M=6m 39s, SD=5m 59s), t(8874) = \u221221.63,p < .001. Surprisingly, average performance on the evaluation set was lower despite participants spending more time and effort on average per task compared to the training tasks.\n\nSecond, how much time do people spend thinking instead of acting? As a first approximation of how much time is spent thinking about ARC puzzles, for each attempt per participant, we computed the sequence of inter-action times: the time spent between each action (i.e., clicks) taken to modify the state of the output grid or otherwise interact with the user interface. We then filtered out any inter-action time interval smaller than 5 seconds, corrected for time spent"}, {"title": "3.2 Cross-dataset comparisons", "content": "writing and only considered series of submissions that led to a correct solution. Finally, we sum the remaining inter-action time intervals and normalized by the number of attempts taken to solve each task. An independent samples t-test confirmed that participants spend significantly less time thinking about training set tasks (M=1m 6s, SD=1m 10s) than they do about evaluation set tasks (M=1m 37s, SD=1m 39s), t(1505) = \u22126.90, p < .001. This result provides evidence that more thinking is required to infer the underlying rule or program of evaluation tasks as opposed to training tasks, and suggests that the reasoning process underlying finding a solution for evaluation tasks is more computationally demanding than for training tasks."}, {"title": "3.3 Errors", "content": "Another aspect of the behavioral data that is potentially revealing of the underlying cognitive processes involved in solving ARC problems is looking at the types and patterns of errors that participants make (see Figure 4). We explore a number of preliminary analyses looking at errors along a variety of dimensions: errors on the dimensions of the test output grid, edit distance from the true test output, error divergence and copying, and compare them to the machine outputs reported in this document (see Table 1).\n\nGrid dimension errors. One of the most common first actions that people take on both training and evaluation set tasks (33.2%) is to resize the output grid to the intended size by selecting the height or width drop-down menu. Although people make a non-negligible amount of height and width errors, within the set of incorrect submissions, we find that 68.2% and 73.5% of submission attempts have both the correct height and width in training and evaluation set tasks respectively. Conversely, we find that both current top solutions to the public ARC leaderboard reported in this document (see Table 1) make fewer mistakes in selecting the correct height and width of the test output compared to humans, with Claude-3.5-N making grid dimension errors on 10.3% of incorrect submissions and GPT-40-NS making grid dimension errors 8.3% of the time.\n\nEdit distance. As a proxy for how close to the true test output incorrect submissions tend to be for ARC tasks, we computed the distribution of edit distances for all errors on each task.11 To facilitate comparison, we computed edit distance on grids with correct dimensions and normalized by grid size. We find that edit distance to ground truth distributions for GPT-40-NS (M=0.19, SD=0.18), Claude-3.5-N (M=0.19, SD=0.16) and humans (M=0.19, SD=0.15) are strikingly similar. Nonetheless, we find an average normalized pairwise distance between human errors and machine errors across tasks of 0.25 for both reported models ($SD_{GPT-40}$=0.19,\n$SD_{Claude3.5}$=0.20). Despite people and machines having similar edit distance to ground truth distributions, this result clearly suggests that they are making substantially different errors.\n\nError divergence. We computed error divergence as the number of unique incorrect grids over the total number of submitted grids for each problem. Task success rate and error divergence were found to be strongly negatively correlated on training tasks, r(378) = -0.81, p < .001, and on evaluation tasks r(395) = -0.88, p < .001. Since we do not have repeated samples data from models, we omit comparative analyses here.\n\nCopy errors. We also find that 10.0% and 9.2% of errors on training and evaluation set tasks respectively were copy errors. Copy errors refer to any incorrect submission that was a copy of either the example inputs (0.6% and 0.3% for training and evaluation sets respectively), example outputs (2.4% and 3.6% for training and evaluation sets respectively) or the test input (7.0% and 5.4% for training and evaluation sets respectively). In contrast, only 3.5% and 1.6% of incorrect outputs from GPT-40-NS and Claude-3.5-N were copy errors, respectively."}, {"title": "4 Discussion", "content": "The present study aimed to provide a comprehensive evaluation of human performance on the ARC benchmark. To estimate performance on ARC, we collected data from 1729 Amazon Mechanical Turk workers who were each assigned 5 randomly selected tasks from either the training or evaluation set. We estimate that 3-shot human performance is between 73.3% and 77.2% on the training set with an observed empirical average of 76.2%, and between 55.9% and 68.9% on the evaluation set with an observed empirical average of 64.2%. We also report that 98.8% of both the training and evaluation sets are solved by at least one person. Overall, we find that the evaluation set is more difficult for people than the training set. Although it remains unclear why evaluation set ARC puzzles are harder, we find that people spend significantly more time thinking about evaluation set tasks than they do about training tasks. Finally, we analyzed error patterns related to grid dimension, edit distance, error divergence and copying for both people and machines. Although people solve more tasks than state-of-the-art approaches, we find that machines outperform people on most error metrics we analyzed. These results along with edit distance results suggest that the errors people and machines make on ARC tasks are of a different nature, further emphasizing that these approaches do not capture how people solve ARC problems. We discuss these issues further in the following sections."}, {"title": "4.1 Competence versus performance: contextualizing average performance", "content": "It is important to bear in mind that average performance on ARC tasks as reported here can be affected by many contextual factors and does not reflect some absolute measure of"}, {"title": "4.2 Human competence", "content": "For almost every task (98.8%) in the combined ARC training and evaluation sets, there is at least one person that solved it and over 90% of tasks were solved by at least three randomly assigned online participants. This is interesting because effectively this means that if you contact 10 random people on the Internet (via CloudResearch), at least one will be able to solve any given ARC problem. This is in contrast to biased sampling from a selective group like AI researchers (including the inventor of the ARC problem set) or highly educated academics. And this estimate is clearly biased to be low because only 10 people completed each problem with three attempts (if we had 1000 people complete each problem, or gave each person unlimited attempts, we would expect the odds of a human solving every problem would go up). The point is we didn't have to do this exhaustive sampling to find almost universal solvability of the ARC problems. We believe this critically highlights that human intelligence is in principle capable of carrying out the required computations to solve any ARC task which is unlike any AI model reported so far. This result would appear to put aside doubts about the solvability of ARC tasks by humans in both the training and evaluation sets. Furthermore, similarly to how expertise at games like chess and Go had been pitted as the threshold for successful AI models, our human best performance score suggests a challenging goal for AI."}, {"title": "4.3 Self-correction through minimal feedback", "content": "Although people make errors, our analyses as well as qualitative judgements suggest that people are better at learning from minimal feedback, and correcting for those errors than machines. In fact, most correct answers from either top solution reported here are obtained on a first attempt, with only +7% and +9.1% proportional increase in accuracy with a second attempt for Claude-3.5-N and GPT-40-NS respectively. For humans, we see substantial improvements in accuracy after 2 attempts (+21.2% on the training set and +25.9% on the evaluation set) and still more proportional increase in accuracy when given 3 attempts (+5.0% on the training set and +10.0% on the evaluation set). People will often make initially wrong guesses but they are capable of self-correction and can flexibly consider alternative solutions. Understanding how people achieve this is likely to be useful for improving machine intelligence in ARC tasks, and more generally for problem-solving."}, {"title": "4.4 Why is the evaluation set more difficult?", "content": "The evaluation set is found to be significantly more difficult than the training set for people. Although it is still unclear why that is the case, our results suggest that factors other than output grid size are contributing to difficulty on the evaluation set and that for equally sized output grids, evaluation tasks are still often more difficult than training tasks. We suspect that the primitive operations underlying the transformations for evaluation tasks are more difficult to infer and/or execute than in the training set. For instance, we have found that logic-based operations that require superimposing grid sections tend to be difficult for people (see Figure 4).\n\nARC problems present an interesting challenge: how can the outputs of a program given its inputs be decomposed into subroutines and primitive operations that recover the underlying program? Previous work has explored a hypothesis-generation model using an LLM to solve a subset of ARC tasks [14]. Results from this research demonstrated that an important bottleneck on LLM performance in this setting was generating accurate and useful natural language hypotheses about the underlying program or rule of each task. Another line of research has found that nameability or codability can modulate concept acquisition [16], making it harder to learn concepts that have features that are more difficult to name. Similarly, we believe that certain primitive operations underlying ARC tasks in the evaluation set might be more difficult to identify or retrieve for people, in turn making it more difficult to generate plausible"}, {"title": "5 Conclusion", "content": "Although we provide a comprehensive estimate of human performance on ARC tasks and preliminary analyses, more in-depth analyses of essential elements of people's problem solving strategies such as state space trajectories, action traces and natural language descriptions are needed. H-ARC affords answering questions about these aspects of behavior which are likely to be informative for understanding the underlying mental representations that support abstract reasoning and problem-solving in people. As a result, we hope that the dataset we release along with this document will allow developments on two fronts: our understanding of how people solve novel abstract problems and how machines can be improved to reason about such problems in a more human-like and intelligent way."}]}