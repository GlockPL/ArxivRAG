{"title": "LEARNING IN COMPLEX ACTION SPACES\nWITHOUT POLICY GRADIENTS", "authors": ["Arash Tavakoli", "Sina Ghiassian", "Nemanja Raki\u0107evi\u0107"], "abstract": "Conventional wisdom suggests that policy gradient methods are better suited to\ncomplex action spaces than action-value methods. However, foundational studies\nhave shown equivalences between these paradigms in small and finite action spaces\n(O'Donoghue et al., 2017; Schulman et al., 2017a). This raises the question of why\ntheir computational applicability and performance diverge as the complexity of\nthe action space increases. We hypothesize that the apparent superiority of policy\ngradients in such settings stems not from intrinsic qualities of the paradigm, but\nfrom universal principles that can also be applied to action-value methods to serve\nsimilar functionality. We identify three such principles and provide a framework\nfor incorporating them into action-value methods. To support our hypothesis, we\ninstantiate this framework in what we term QMLE, for Q-learning with maximum\nlikelihood estimation. Our results show that QMLE can be applied to complex\naction spaces with a controllable computational cost that is comparable to that of\npolicy gradient methods, all without using policy gradients. Furthermore, QMLE\ndemonstrates strong performance on the DeepMind Control Suite, even when\ncompared to the state-of-the-art methods such as DMPO and D4PG.", "sections": [{"title": "1 INTRODUCTION", "content": "In reinforcement learning, policy gradients have become the backbone of solutions for environments\nwith complex action spaces, including those involving large, continuous, combinatorial, or structured\nsubaction spaces (Dulac-Arnold et al., 2015; OpenAI et al., 2019; Vinyals et al., 2019; Hubert et al.,\n2021; Ouyang et al., 2022). In contrast, action-value methods have traditionally been confined to\ntabular-action models for small and finite action spaces. However, where applicable, such as on the\nAtari Suite (Bellemare et al., 2013; Machado et al., 2018), action-value methods are frequently the\npreferred approach over policy gradient methods (Kapturowski et al., 2023; Schwarzer et al., 2023).\nOver the past years, foundational research has shown that the distinction between action-value and\npolicy gradient methods is narrower than previously understood, particularly in the basic case of\ntabular-action models in small and finite action spaces (see, e.g., Schulman et al., 2017a). Notably,\nO'Donoghue et al. (2017) established an equivalency between these paradigms, revealing a direct\nconnection between the fixed-points of the action-preferences of policies optimized by regularized\npolicy gradients and the action-values learned by action-value methods. These insights invite further\nexploration of the discrepancies that emerge as the complexity of action spaces increases.\nWhat are the core principles that underpin the greater computational applicability and performance\nof policy gradient methods in such settings? In this paper, we identify three such principles. First,\npolicy gradient methods leverage Monte Carlo (MC) approximations for summation or integration\nover the action space, enabling computational feasibility even in environments with complex action\nspaces. Second, they employ amortized maximization through a special form of maximum likelihood\nestimation (namely, the policy gradient itself), iteratively refining the policy to increase the likelihood\nof selecting high-value actions without requiring brute-force arg max over the action space. Third,\nscalable policy gradient methods employ action-in architectures for action-value approximation,\nwhich covertly enable representation learning and generalization across the joint state-action space."}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 THE REINFORCEMENT LEARNING PROBLEM", "content": "The reinforcement learning (RL) problem (Sutton & Barto, 2018) is generally described as a Markov\ndecision process (MDP) (Puterman, 1994), defined by the tuple $(S, A, P, R)$, where S is a state space,\nA is an action space, $P : S \\times A \\rightarrow \\triangle(S)$ is a state-transition function, and $R : S \\times A \\times S \\rightarrow \\triangle(\\mathbb{R})$\nis a reward function. The behavior of an agent in an RL problem can be formalized by a policy\n$\\pi : S \\rightarrow \\Delta(A)$, which maps a state to a distribution over actions. The value of state s under policy $\\pi$\nmay be defined as the expected discounted sum of rewards: $V^{\\pi}(s) = \\mathbb{E}_{\\pi, P, R}[\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} | S_0 = s]$,\nwhere $\\gamma\\in (0, 1)$ is a discount factor used to exponentially decay the present value of future rewards.\nThe goal of an RL agent is defined as finding an optimal policy $\\pi^*$ that maximizes this quantity across\nthe state space: $V^{\\pi^*} > V^{\\pi}$ for all $\\pi$. While there may be more than one optimal policy, they all share\nthe same state-value function: $V^* = V^{\\pi^*}$. Similarly, we can define the value of state s and action\na under policy $\\pi$: $Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi, P, R}[\\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} | S_0 = s, a_1 = a]$. Notice that the goal can be\nequivalently phrased as finding an optimal policy $\\pi^*$ that maximizes this alternative quantity across\nthe joint state-action space: $Q^* > Q^{\\pi}$ for all $\\pi$. Same as before, optimal policies share the same\naction-value function: $Q^* = Q^{\\pi^*}$.\nThe state and action value functions are related to each other via: $V^{\\pi}(s) = \\sum_a Q^{\\pi}(s,a)\\pi(a|s)$,\nwhere we use $\\sum$ to signify both summation and integration over discrete or continuous actions.\nFor all MDPs there is always at least one deterministic optimal policy, which can be deduced by\nmaximizing the optimal action-value function: $argmax_a Q^*(s, a)$ in any given state s. It is worth\nnoting that there may be cases where multiple actions yield the same maximum value, resulting in ties.\nBy breaking such ties at random, considering all conceivable distributions, we can construct the set\nof all optimal policies, including both deterministic and stochastic policies. Regardless of the optimal\npolicy, the optimal state-value and action-value functions are related to each other in the following\nway: $V^*(s) = max_a Q^*(s, a)$. Similarly, the optimal state-value function can be used to extract"}, {"title": "2.2 ACTION-VALUE LEARNING", "content": "Optimizing the action-value function and deducing an optimal policy from it seems to be the most\ndirect approach to solving the RL problem in a model-free manner. To this end, we first consider the\nBellman recurrence for action-values (Bellman, 1957):\n$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi, P, R}[r_{t+1} + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1}) | s_t = s, a_t = a]$,\nwhere $\\pi$ is in general a stochastic policy and $a_{t+1} \\sim \\pi(.|s_{t+1})$. By substituting policy $\\pi$ with an\noptimal policy $\\pi^*$ and invoking $Q^*(s, argmax_a Q^*(s, a)) = max_a Q^*(s, a)$, we can rewrite Eq. 1:\n$Q^*(s, a) = \\mathbb{E}_{P, R}[r_{t+1} + \\gamma max_{a'} Q^*(s_{t+1}, a') | s_t = s, a_t = a]$.\nThe method of temporal differences (TD) (Sutton, 1988) leverages equations (1) and (2) to contrive\ntwo foundational algorithms for model-free RL: Sarsa (Rummery & Niranjan, 1994) and Q-learning\n(Watkins, 1989). Sarsa updates its action-value estimates, $Q(s_t, a_t)$, by minimizing the TD residual:\n$(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})) - Q(s_t, a_t)$,\nwhereas Q-learning does so by minimizing the TD residual:\n$(r_{t+1} + \\gamma max_a Q(s_{t+1}, a)) - Q(s_t, a_t)$.\nBoth algorithms have been shown to converge to the unique fixed-point $Q^*$ of Eq. 2 under similar\nconditions, with one additional and crucial condition for Sarsa (Watkins & Dayan, 1992; Jaakkola\net al., 1994; Singh et al., 2000). Namely, because Sarsa uses the action-value of the action chosen\nby its policy in the successor state, the action-values can converge to optimality in the limit only if\nit chooses actions greedily in the limit: $lim_{k \\rightarrow \\infty} \\pi_k(a|s) = 1_{a=argmax_{a'} Q(s, a')}$. This is in contrast\nwith Q-learning which uses its maximum action-value in the successor state regardless of its policy,\nthus liberating its learning updates from how it chooses to act. This key distinction makes Sarsa\nan on-policy and Q-learning an off-policy algorithm. As a final point, the action-value function can\nbe approximated by a parameterized function $Q$, such as a neural network, with parameters w and\ntrained by minimizing the squared form of the TD residual (3) or (4)."}, {"title": "2.3 POLICY GRADIENT METHODS", "content": "Unlike action-value methods (\u00a72.2), policy gradient methods do not require an action-value function\nfor action selection. Instead they work by explicitly representing the policy using a parameterized\nfunction $\\pi$, such as a neural network, with parameters $\\theta$ and only utilizing action-value estimates to\nlearn the policy parameters. To demonstrate the main idea underpinning policy gradient methods, we\nstart from the following formulation of the RL problem (cf. \u00a72.1):\n$\\pi^* = argmax_{\\pi} \\mathbb{E}_{P}[V^{\\pi}(s_t)]$.\nThe objective function in this formulation is the expected state-value function, where the expectation\nis taken over the state distribution induced by policy $\\pi$ and state-transition function P. This problem\ncan be solved approximately via gradient-based optimization. In fact, this forms the basis of policy\ngradient methods. Accordingly, the policy gradient theorem (Sutton et al., 1999) proves that the\ngradient of the expected state-value function with respect to policy parameters $\\theta$ is governed by:\n$\\nabla_{\\mathbb{E}_{\\pi, P}}[V^*(S_t)] = \\nabla_{\\mathbb{E}_{\\pi, P}}[\\sum_a Q^*(S_t, a)\\pi(a|S_t)] \\propto \\mathbb{E}_{P} [\\sum_a Q^*(s_t, a)\\nabla \\pi(a|S_t)]$.\nBy using an estimator of the above expression, denoted $\\nabla J(\\theta)$, policy parameters can be updated via\nstochastic gradient ascent: $\\theta \\leftarrow \\theta + \\alpha \\nabla J(\\theta)$, where $\\alpha$ is a positive step-size. It is important to note\nthat, like Sarsa (\u00a72.2), policy gradients are on-policy learners: applying one step of policy gradient"}, {"title": "2.4 MAXIMUM LIKELIHOOD ESTIMATION", "content": "Suppose we have a data set $\\{(x_i, y_i)\\}$ drawn from an unknown joint distribution $p(x, y)$, where\nrandom variables $x_i$ and $y_i$ respectively represent inputs and targets. Frequently, problem scenarios\ninvolve determining the parameters of an assumed probability distribution that best describe the data.\nThe method of maximum likelihood estimation (MLE) addresses this by posing the question: \"Under\nwhich parameter values is the observed data most likely?\". In this context, we typically start by\nrepresenting our assumed distribution using a parameterized function $f$, such as a neural network,\nwith parameters $\\theta$. Hence, $\\phi = f(x)$ serves as our estimator for the distributional parameters in x.\nFor example, $\\phi$ contains K values in the case of a categorical distribution with K categories, and\ncontains means $\\mu$ and variances $\\sigma$ in the case of a multivariate heteroscedastic Gaussian distribution.\nWe will denote the probability distribution that is specified by parameters $\\phi = f(x)$ as $f(y|x)$. The\nproblem of finding the optimal parameters can then be formulated as:\n$argmax_{\\phi} \\mathbb{E}_{p(x,y)} [log f (y_i|x_i)]$.\nThis problem can be solved approximately via gradient-based optimization by leveraging the log-\nlikelihood gradient with respect to parameters $\\theta$:\n$\\nabla log f(y_i|x_i)]$.\nBy using estimates of the above expression, denoted $\\nabla J(\\theta)$, we can iteratively refine our distributional\nparameters $\\phi$ via stochastic gradient ascent on $\\theta$: $\\theta \\leftarrow \\theta + \\alpha \\nabla J(\\theta)$, where $\\alpha$ is a positive step-size."}, {"title": "3 THE PRINCIPLES UNDERPINNING SCALABILITY IN POLICY GRADIENTS", "content": "As we discussed in Section 2.2, both Sarsa and Q-learning require maximization of the action-\nvalue function: Sarsa relies on greedy action-selection in the limit for optimal convergence and\nQ-learning needs maximizing the action-value function in the successor state to compute its TD\ntarget. Additionally, both Sarsa and Q-learning need action-value maximization in the current state\nfor exploitation or, more generally, for constructing their policies (e.g. an $\\epsilon$-greedy policy relies\non choosing greedy actions with probability $1 - \\epsilon$ and uniformly at random otherwise). However,\nperforming exact maximization in complex action spaces is computationally prohibitive. This has in\nturn limited the applicability of Sarsa and Q-learning to small and finite action spaces. On the other\nhand, policy gradient methods are widely believed to be suitable for dealing with complex action\nspaces. In this section, we identify the core principles underlying the scalability of policy gradient\nmethods and describe each such principle in isolation."}, {"title": "3.1 APPROXIMATE SUMMATION OR INTEGRATION USING MONTE CARLO METHODS", "content": "The scalability of policy gradients in their general stochastic form relies heavily on the identity:\n$\\mathbb{E}_{\\pi, P} [\\sum_a Q^{\\pi}(s_t, a)\\nabla \\pi(a|S_t)] = \\mathbb{E}_{\\pi, P} [Q^*(S_t, \\frac{\\pi(a_t|S_t)}{\\pi(a_t|S_t)})\n= \\mathbb{E}_{\\pi, P} [Q^*(S_t, a_t) \\frac{\\pi(a_t|S_t)}{\\pi(a_t|S_t)}]\n= \\mathbb{E}_{\\pi, P} [Q^*(s_t, a_t) \\nabla log \\pi(a_t|s_t)]$,\nwhere the middle expression is derived from our original policy gradient expression (6) by substituting\nan importance sampling estimator in place of the exact summation or integration over the action\nspace. The rightmost expression is then derived simply by invoking the logarithm differentiation rule,\nwhere log denotes the natural logarithm. Consequently, using an experience batch of the usual form\n$\\{(S_t, a_t, r_{t+1}, S_{t+1})\\}$ with size n, we can construct an estimator of the policy gradient as follows:\n$\\frac{1}{n}\\sum_t Q(s_t, a_t)log \\pi(a_t|s_t)$,\nwhere $Q^{\\pi}$ is the true action-value function under policy $\\pi$ which itself needs to be estimated from\nexperience, e.g. via $Q^{\\pi}(s_t, a_t) \\approx r_{t+1} + \\gamma V(S_{t+1})$ with V serving as a learned approximator of $V^{\\pi}$.\nConsidering the fact that the policy gradient estimator (13) is founded upon replacing the exact\nsummation or integration over the action space with an on-trajectory (single-action) MC estimator,\nwe can construct a more general class of policy gradient estimators by enabling off-trajectory action\nsamples to also contribute to this numerical computation (Petit et al., 2019):\n$\\frac{1}{n}\\sum_t \\frac{1}{m+1} (Q^{\\pi}(s_t, a_t)log(\\pi(a_t)S_t) + \\sum_{i=0}^{m+1} Q^*(s_t, a_i) \\nabla log \\pi(a_i|S_t))$,\nwhere m is the number of off-trajectory action samples $a_i \\sim \\pi(.|s_t)$ per state $s_t$. When m = 0, this\nreduces to the original on-trajectory policy gradient estimator (13). It is important to note that using\nthe policy gradient estimator (14) with m > 0 requires direct approximation of the action-values $Q^{\\pi}$\nby a function Q, e.g. a neural network trained by minimizing the squared form of the TD residual (3)."}, {"title": "3.2 AMORTIZED MAXIMIZATION USING MAXIMUM LIKELIHOOD ESTIMATION", "content": "In RL and dynamic programming, generalized policy iteration (GPI) (Bertsekas, 2017) represents a\nclass of solution methods for optimizing a policy by alternating between estimating the value function"}, {"title": "3.3 REPRESENTATION LEARNING VIA ACTION-IN ARCHITECTURES", "content": "There are two functional forms for constructing an approximate action-value predictor Q: action-in\nand action-out architectures. An action-in architecture predicts Q-values for a given state-action pair\nat input. An action-out architecture outputs Q predictions for all possible actions in an input state.\nAction-out architectures have the computational advantage that a single forward pass through the\npredictor collects all actions' values in a given state, versus requiring as many forward passes as there\nare actions in a state by an action-in architecture. Of course, such an advantage is only pertinent\nwhen evaluating all possible actions, or a considerable subset of them, in a given state\u2014a necessity\nthat varies depending on the algorithm. On the other hand, one notable limitation of action-out\narchitectures is their incapacity to predict Q-values in continuous action domains without imposing\nstrict modeling constraints on the functional form of the estimated Q-function (Gu et al., 2016).\nAction-value methods are commonly employed with action-out architectures, including DQN (Mnih\net al., 2015) and Rainbow (Hessel et al., 2018). Conversely, policy gradient algorithms that involve Q\napproximations resort to action-in architectures for tackling complex action spaces, such as DDPG\n(Lillicrap et al., 2016) and MPO (Abdolmaleki et al., 2018). Considering the specific requisites of the\ntwo families of methods in their standard forms, these are reasonable choices. In particular, standard\naction-value methods require evaluation of all possible actions in a given state in order to perform the\nmaximization operation, thereby an action-out architecture is more efficient from a computational\nperspective. In contrast, policy gradient methods that rely on Q approximation require evaluation of\nonly one or a fixed number of actions in any given state (\u00a73.1). Hence, using action-in architectures\nin the context of policy gradient methods is more computationally efficient in finite action spaces and\none that functionally supports Q evaluation in complex action spaces.\nSo far, we have compared action-in and action-out architectures from computational and functional\nstandpoints. Now, we turn to a fundamental but often overlooked advantage of action-in architectures:\ntheir capacity for representation learning and generalization with respect to actions. Specifically,\nby treating both states and actions as inputs, action-in architectures unify the process of learning\nrepresentations for both. For example, when training an action-in Q approximator with deep learning,\nbackpropagation enables learning representations over the joint state-action space. In contrast, action-\nout architectures are limited in their capacity for generalizing across actions (Zhou et al., 2022). This\nlimitation arises because, although many layers may serve to learn deep representations of input\nstates, action conditioning is introduced only at the output layer in a tabular-like form. While some\naction-out architectures introduce structural inductive biases that support combinatorial generalization\nacross multi-dimensional actions (see, e.g., Tavakoli et al., 2018; 2021), they do not capacitate action\nrepresentation learning and generalization in the general form. Moreover, such architectures remain\nlimited to discrete action spaces and are, generally, subject to statistical biases."}, {"title": "4 INCORPORATING THE PRINCIPLES INTO ACTION-VALUE LEARNING", "content": "In Section 3, we identified three core principles that we argued underpin the effectiveness of popular\npolicy gradient algorithms in complex action spaces. In this section, we challenge the conventional\nwisdom that policy gradient methods are inherently more suitable in tackling complex action spaces\nby showing that the same principles can be integrated into action-value methods, thus enabling them\nto exhibit similar scaling properties to policy gradient methods without the need for policy gradients.\nPrinciple 1 In the same spirit as using an MC estimator in place of exact summation or integration\nover the action space in policy gradient methods (\u00a73.1), the first principle that we consolidate into\naction-value learning is substituting exact maximization over the action space with a sampling-based"}, {"title": "5 EXPERIMENTS", "content": "To evaluate our framework, we instantiate Q-learning with maximum likelihood estimation (QMLE)\nas an example of integrating the adapted core principles (\u00a74) into approximate Q-learning with deep\nneural networks (Mnih et al., 2015). Appendix A presents the QMLE algorithm in a general form.\nOur illustrative study (\u00a75.1) employs a simplified implementation of this algorithm. Appendix B\nprovides the details of the QMLE agent used in our benchmarking experiments (\u00a75.2)."}, {"title": "5.1 ILLUSTRATIVE EXAMPLE", "content": "We compare QMLE to the deterministic policy gradient (DPG) algorithm in a continuous 2D bandit\nproblem with deterministic and bimodal rewards (similar to that presented by Metz et al., 2019).\nThis problem setting minimizes confounding factors by reducing action-value learning to supervised\nlearning of rewards and eliminating contributions from differing bootstrapping mechanisms in the\ntwo methods. For an apples-to-apples comparison, we constrain QMLE to only a single parametric\narg max predictor based on a delta distribution, mirroring the strict limitation of DPG to delta policies.\nWe further simplify QMLE by aligning its computation of greedy actions with that of DPG. This\nensures the only remaining difference between QMLE and DPG is in how their delta parameters\nare updated, not in how their greedy actions are computed for constructing behavior policies. Both\nmethods use the same hyper-parameters, model architecture, and initialization across all experiments.\nWe examine two simplified variants of QMLE. The first one uses local sampling around the delta\nparameters for arg max approximations that are used as targets for MLE training. Precisely, we only\nallow samples $A_m$ drawn from $\\delta_{\\rho} (s) + \\xi$, where $\\delta_{\\theta}$ denotes the delta-based arg max predictor and $\\xi$ is\na zero-mean Gaussian noise with a standard deviation of 0.001 (cf. Eq. 21). This is akin to computing\nan MC approximation of $\\nabla_aQ^{\\pi} (s_t, a=\\pi(s_t))$ in DPG (7c). The second variant incorporates global\nsampling alongside local sampling, by additionally sampling from the uniform distribution of Eq. 21.\nThis study illustrates key properties of QMLE with respect to DPG: subsumption, where QMLE\nwith local sampling approximates DPG updates, and transcendence, where global sampling allows\nQMLE to overcome the local tendencies of policy gradients and surpass DPG."}, {"title": "5.2 BENCHMARKING RESULTS", "content": "In this section, we evaluate QMLE on 18 continuous control tasks from the DeepMind Control Suite\n(Tassa et al., 2018). Figure 3 shows learning curves of QMLE alongside the learning curves or final\nperformances of several baselines, including state-of-the-art methods DMPO (Hoffman et al., 2022)\nand D4PG (Barth-Maron et al., 2018), as well as the canonical (on-policy) A3C (Mnih et al., 2016)\nand (off-policy) DDPG (Lillicrap et al., 2016). Results for DMPO (12 tasks) are from Seyde et al.\n(2023), while those for A3C, DDPG, and D4PG (16 tasks) are from Tassa et al. (2018).\nWith the exception of the Finger Turn Hard task, QMLE consistently performs between DDPG and\nD4PG. Notably, it matches or outperforms DDPG on 14 out of 16 tasks, with DDPG being the closest\ncounterpart from the policy gradient paradigm to QMLE. Moreover, QMLE substantially exceeds the\nperformance of A3C across all tasks. This is despite QMLE being trained on 10 to 100\u00d7 fewer steps\ncompared to A3C, DDPG, and D4PG. While QMLE competes well with DMPO in low-dimensional\naction spaces, it trails in higher-dimensional ones. Nonetheless, the strong performance of QMLE in\ncontinuous control tasks with up to 38 action dimensions, all without policy gradients, in and of itself\ntestifies to the core nature of our identified principles and their adaptability to action-value methods."}, {"title": "6 CONCLUSION", "content": "In this paper, we distilled the success of policy gradient methods in complex action spaces into three\ncore principles: MC approximation of sums or integrals, amortized maximization using a special\nform of MLE, and action-in architectures for representation learning and generalization over actions.\nWe then argued that these principles are not exclusive to the policy gradient paradigm and can be\nadapted to action-value methods. In turn, we presented a framework for incorporating adaptations\nof these principles into action-value methods. To examine our arguments, we instantiated QMLE\nby implementing our adapted principles into approximate Q-learning with deep neural networks.\nOur results showed that QMLE performs strongly in continuous control problems with up to 38\naction dimensions, largely outperforming its closest policy gradient counterpart DDPG. These results\nprovided empirical support for the core nature of our identified principles and demonstrated that\naction-value methods could adopt them to achieve similar qualities, all without policy gradients. In a\ncomparative study using DPG and two simplified QMLE variants, we highlighted a key limitation of\npolicy gradients and showed how QMLE could overcome it. This study serves as a motivator for a\nshift from policy gradients toward action-value methods with our adapted principles. It also offers a\npotential explanation for the improvements observed over DDPG in our benchmarking experiments."}, {"title": "A Q-LEARNING WITH MAXIMUM LIKELIHOOD ESTIMATION", "content": "In this section, we present the Q-learning with maximum likelihood estimation (QMLE) algorithm.\nSpecifically, our presentation is based on integrating our framework (\u00a74) into the deep Q-learning\nalgorithm by Mnih et al. (2015). In line with this, we make use of experience replay and a target\nnetwork that is only periodically updated with the parameters of the online network. Importantly, we\nextend the scope of the target network to encompass the arg max predictors in QMLE. Although the\nalgorithm does not mandate the use of action-in Q approximators per se, such architectures become\nnecessary for addressing problems with arbitrarily complex action spaces (\u00a73.3).\nAlgorithm 1 details the training procedures for QMLE. Notably, the algorithm is flexible regarding\nthe composition of the ensemble of arg max predictors. For instance, the ensemble can consist of\na combination of continuous and discrete distributions for problems with continuous action spaces.\nQMLE introduces several hyper-parameters related to its action-sampling processes. These include\nthe sampling budgets for target maximization, $m_{target}$, and greedy action selection in the environment,\n$m_{greedy}$. Additionally, QMLE uses sample allocation ratios $\\{p_0, p_1, ..., p_k \\}$, where $p_0$ corresponds\nto the proportion of the budget allocated to uniform sampling from the action space, and $p_1$ through\n$p_k$ correspond to the proportions assigned to the ensemble of k parametric arg max predictors.\nTo effectively manage training inference costs in QMLE, we recommend allocating a larger budget to\n$m_{greedy}$ than to $m_{target}$. Since $m_{greedy}$ is used at most once per interaction step, increasing it incurs\nrelatively little computational burden. In addition, more accurate arg max approximations during\ntraining interactions can lead to higher quality data for learning, making this increase particularly\nbeneficial. In contrast, each training update requires $m_{target} \\times N_b$ inferences on the target Q-network,\nwhere $N_b$ is the batch size. This makes increasing $m_{target}$ much more costly in terms of training\ninference costs. On that account, choosing a moderate $m_{target}$ allows for computational tractability\nwith larger batch sizes. Remarkably, a moderate $m_{target}$ could also help reduce the overestimation\nof action-values (Hasselt, 2010; van Hasselt et al., 2016). Also, assigning a smaller $m_{target}$ relative"}, {"title": "B EXPERIMENTAL DETAILS", "content": "This section details the specific QMLE instance that we evaluated in our benchmarking experiments.\nWe adopted prioritized experience replay (Schaul et al., 2016), in place of the uniform variant that was\ndescribed in Algorithm 1. Furthermore, we deployed QMLE with two arg max predictors: one based\non a delta distribution over the continuous action space, and another based on a factored categorical\ndistribution defined over a finite subset of the original action space (Tang & Agrawal, 2020).\nTo build the discrete action support, we applied the bang-off-bang (3 bins) discretization scheme to\nthe action space (Seyde et al., 2021). For sampling from the delta-based arg max predictor, we always\nincluded the parameter of the delta distribution as the initial sample. Any additional samples were\ngenerated through Gaussian perturbations around this parameter using a small standard deviation.\nSections B.1, B.2, and B.3 provide details around the model architecture, hyper-parameters, and\nimplementation of QMLE in our benchmarking experiments, respectively. Section B.4 details the\nnumber of seeds per agent and the computation of our learning curves."}, {"title": "C SUPPLEMENTARY RESULTS", "content": "Figures 5 and 6 provide comparisons of QMLE with a range of mainstream policy gradient methods.\nThe baseline results are due to Pardo (2020).\n\u2022 Figure 5 presents a comparison between QMLE and policy gradient methods that rely on\naction-value approximation: MPO (Abdolmaleki et al., 2018), SAC (Haarnoja et al., 2018),\nand TD3 (Fujimoto et al., 2018).\n\u2022 Figure 6 compares QMLE with policy gradient methods that use state-value approximation:\nPPO (Schulman et al., 2017b), TRPO (Schulman et al., 2015), and A2C (Mnih et al., 2016)."}, {"title": "D FUTURE WORK", "content": null}, {"title": "D.1 COMBINING WITH OTHER IMPROVEMENTS", "content": "In this paper, we integrated our framework into the deep Q-learning algorithm of Mnih et al. (2015),\nin a proof-of-concept agent that we termed QMLE (Algorithm 1). In our benchmarking experiments,\nwe further combined QMLE with prioritized experience replay (Schaul et al., 2016; see details in\nSection B). While this setup is relatively basic compared to the advancements in deep Q-learning, it\nserved our purpose of demonstrating the general competency of action-value methods in complex\naction spaces without involving policy gradients. We anticipate that a purposeful integration with\nadvancements in deep Q-learning could significantly improve the performance of our QMLE agent.\nFor instance, fundamental methods that can be trivially combined with QMLE include N-step returns\nand distributional learning, similarly to the critics in DMPO and D4PG. Certain methods, including\ndouble Q-learning (Hasselt, 2010; van Hasselt et al., 2016) and dueling networks (Wang et al.,\n2016) may not be directly applicable or relevant to QMLE, underscoring the importance of careful\nintegration. We are particularly excited about using a cross-entropy classification loss in place of\nregression for training Q approximators (Farebrother et al., 2024), as well as combining with ideas\nintroduced by Li et al. (2023); Schwarzer et al. (2023). Moreover, formal explorations into the space\nof value mappings (van Seijen et al., 2019; Fatemi & Tavakoli, 2022), particularly those that benefit\nQ-function approximation with action-in architectures, offer an intriguing direction for future work.\nSince our approach employs maximum likelihood estimation (MLE) in a disentangled manner (see\ndiscussions in Section 3.2), it makes it trivial to incorporate advances from supervised learning for\ntraining the parametric arg max predictors. To provide an example, advancements in heteroscedastic\nuncertainty estimation, such that introduced by Seitzer et al. (2022), can be readily applied to model\nstate-conditional variances for Gaussian arg max predictors."}, {"title": "D.2 MULTIAGENT REINFORCEMENT LEARNING VIA CTDE", "content": ""}]}