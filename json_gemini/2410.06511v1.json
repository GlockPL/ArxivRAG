{"title": "Torch Titan: One-stop PyTorch native solution for production ready LLM pre-training", "authors": ["Wanchao Liang", "Tianyu Liu", "Less Wright", "Will Constable", "Andrew Gu", "Chien-Chin Huang", "Iris Zhang", "Wei Feng", "Howard Huang", "Junjie Wang", "Sanket Purandare", "Gokul Nadathur", "Stratos Idreos"], "abstract": "The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens require sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort.\nThis paper introduces TORCHTITAN, an open-source, PyTorch-native distributed training system that unifies and advances state-of-the-art techniques, streamlining integration and reducing engineering overhead. TORCHTITAN enables seamless application of 3D parallelism in a modular and composable manner, while featuring elastic scaling to adapt to changing computational requirements. The system provides comprehensive logging, efficient checkpointing, and debugging tools, ensuring production-ready training. Moreover, TORCHTITAN incorporates innovative hardware-software co-designed solutions, leveraging cutting-edge features like Float8 training and SymmetricMemory to maximize hardware utilization. As a flexible experimental test bed, TORCHTITAN facilitates the curation and comparison of custom recipes for diverse training contexts. By leveraging TORCHTITAN, we developed optimized training recipes for the Llama 3.1 family and provide actionable guidance on selecting and combining distributed training techniques to maximize training efficiency, based on our hands-on experiences.\nWe thoroughly assess TORCHTITAN on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized baselines.", "sections": [{"title": "Introduction", "content": "LLMs are at the forefront of NLP advancement. Large Language Models (LLMs) (Devlin, 2018; Liu et al., 2019; Radford et al., 2019; Chowdhery et al., 2023; Anil et al., 2023; Achiam et al., 2023; Dubey et al., 2024; Jiang et al., 2024; Abdin et al., 2024) have been driving force behind the advancement of natural language processing (NLP) applications spanning language translation, content/code generation, conversational AI, text data analysis, creative writing and art, education and research etc.\nLLMs require billions of parameters and training over trillion tokens to achieve state-of-the-art performance. Achieving state-of-the-art LLM performance requires massive scale, exemplified by top-performing models like"}, {"title": "Elasticity through Composability", "content": "TORCHTITAN incorporates various parallelism techniques in a modular manner to enable easy, user-selectable combinations of multi-dimensional parallelisms. This composability enables the tackling of difficult scaling challenges by enhancing the ease of frontier exploration for optimizing training efficiencies at scale.\nThe codebase of TORCHTITAN is organized purposefully to enable composability and extensibility. We intentionally keep three main components separate and as orthogonal as possible: (1) the model definition, which is parallelism-agnostic and designed for readability, (2) parallelism helpers, which apply Data Parallel, Tensor Parallel, and Pipeline Parallel to a particular model, and (3) a generalized training loop. All these components are configurable via TOML files with command-line overrides, and it is easy to add new models and parallelism techniques on top of the existing codebase."}, {"title": "Composable N-D parallelism training", "content": "In this section, we will walk through the entire regime of scaling model training on large clusters, including meta device initialization and the core composable multi-dimensional parallelisms, to showcase how these techniques can be composed to train LLMs efficiently at increasing scale in TORCHTITAN. The corresponding actual code snippets in TORCHTITAN can be found in Appendix A."}, {"title": "Large-scale model initialization using meta device", "content": "Given the exponential increase in model sizes for LLMs, the first scaling issue appears even before the actual training starts. This is the need to instantiate a large model for sharding across the cluster, yet without overflowing CPU or GPU memory.\nTo tackle this, we enabled meta device initialization for models in TORCHTITAN, where the model is first initialized on a \"meta\" device type. The meta device tensor only holds the metadata information, not the actual data, making initialization ultra-fast. After that, we perform model sharding and transforming the model parameters into Distributed Tensors (DTensors) where each parameter holds a local shard that lives on the meta device. Finally, we perform parameter initialization based on the user-defined initialization functions. We leverage Distributed Tensor to properly sync Random Number Generator (RNG) seeds, and initialize the parameters according to their sharding layouts. This ensures the parameters start with the same values as if the whole model were initialized on one device before sharding, and thus facilitating convergence comparisons between different parallelism configurations."}, {"title": "Fully Sharded Data Parallel", "content": "The original Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) is an effective implementation of ZeRO that offers large model training capability in PyTorch. However, the original implementation (FSDP1) in PyTorch suffers from various limitations due to its FlatParameter implementation (see details in Appendix B.1).\nGiven these limitations, TORCHTITAN integrates a new version of Fully Sharded Data Parallel (FSDP2), which uses the per-parameter Distributed Tensor sharding representation and thus provides better composability with model parallelism techniques and other features that require the manipulation of individual parameters,\nTORCHTITAN integrates and leverages FSDP2 as it's default 1D parallelism, benefiting from the improved memory management (often 7 percent lower per GPU memory requirement vs FSDP1) and the slight performance gains (average of 1.5 percent gain vs FSDP1). More details on FSDP2 and usage example are shown in Appendix B.1. TORCHTITAN makes it simple to run with FSDP2 by embedding appropriate defaults, including auto-sharding with your world size automatically.\nFor scaling to even larger world sizes, TORCHTITAN also integrates Hybrid Sharded Data Parallel (HSDP) which extends FSDP2 by creating sharding groups. Details are shown in Appendix B.2"}, {"title": "Tensor Parallel", "content": "Tensor Parallel (TP) (Narayanan et al., 2021), together with Sequence Parallel (SP) (Korthikanti et al., 2023), is a key model parallelism technique to enable large model training at scale."}, {"title": "Pipeline Parallel", "content": "For pre-training at the largest scales, TORCHTITAN offers Pipeline Parallelism, which becomes essential due to having the lightest amount of communication overhead and leveraging P2P communications.\nPipeline Parallel (PP) views the model as a sequence of operations, chunking the operations (and the parameters used by them) into S stages which run on separate groups of devices. In the typical case, one stage represents a single model layer or a group of N adjacent model layers, but in theory it could be even be a partial layer. For the forward pass, a stage receives input activations (except stage 0), performs local computation, and sends output activations (except stage S1). The last stage performs a loss computation, and begins the backward pass, sending gradients in the reverse order through the pipeline. To improve efficiency, the input batch is broken into microbatches and a pipeline schedule overlaps computation on one microbatch with communication for others. TORCHTITAN enables a variety of pipeline schedules, with their schedules previously described in other works (Narayanan et al., 2019; Huang et al., 2019; Narayanan et al., 2021; Tang et al., 2024).\nThe training loop must also account for creation of pipeline stages, and executing a pipeline schedule rather than invoking model.forward() directly. Because the schedule computes loss per microbatch, the loss computation and any logging code must be updated for PP. In TORCHTITAN, we propose to define a shared loss_fn to be used by both pipeline and non-pipeline code paths, and thus minimise divergence in the training loop.\nInteractions with data parallelism, such as ensuring that data-parallel reduction happens only after the last microbatch in the schedule, and scheduling of shard and unshard operations when using zero-3, are also handled transparently inside the pipeline schedule executor, simplifying the trainer implementation in TORCHTITAN. For it's usage in TORCHTITAN, please see Appendix \u0412.4."}, {"title": "Optimizing training efficiencies", "content": ""}, {"title": "Navigating compute-memory trade-offs using activation checkpointing", "content": "Activation checkpointing (AC) (Chen et al., 2016) and selective activation checkpointing (SAC) (Korthikanti et al., 2023) are standard training techniques to reduce peak GPU memory usage, by trading activation recomputation during the backward pass for memory savings. It is often needed even after applying multi-dimensional parallelisms."}, {"title": "Regional compilation to exploit torch.compile optimizations", "content": "torch.compile was released in PyTorch 2 (Ansel et al., 2024) with TorchDynamo as the frontend to extract PyTorch operations into an FX graph, and TorchInductor as the backend to compile the FX graph into fused Triton code to improve the performance.\nIn TORCHTITAN, we use regional compilation, which applies torch.compile to each individual TransformerBlock in the Transformer model. This has two main benefits: (1) we get a full graph (without graph breaks) for each region, compatible with FSDP2 and TP (and more generally torch.Tensor subclasses such as DTensor) and other PyTorch distributed training techniques; (2) since the Llama model stacks identical TransformerBlock layers one after another, torch.compile can identify the same structure being repeatedly compiled and only compile once, thus greatly reducing compilation time.\ntorch.compile brings efficiency in both throughput and memory (see Section 3.2) via computation fusions and computation-communication reordering, in a model-agnostic way with a simple user interface. Below we further elaborate how torch.compile composability helps TORCHTITAN unlock hardware-optimized performance gain with simple user interface, with the integration of advanced features such as Asynchronous TP and Float8."}, {"title": "Asynchronous Tensor Parallel to maximally overlap communication", "content": "By default TP incurs blocking communications before/after the sharded computations, causing computation resources to not be effectively utilized. Asynchronous TP (AsyncTP) (Wang et al., 2022) achieves computation-communication overlap by fractionalizing the TP matrix multiplications within the attention and feed-forward modules into smaller chunks, and overlapping communication collectives in between each section. The overlap is achieved by a micro-pipelining optimization, where results are being communicated at the same time that the other chunks of the matmul are being computed.\nPyTorch AsyncTP is based on a SymmetricMemory abstraction, which creates intra-node buffers to write faster communication collectives (Wang et al., 2024). This is done by allocating an shared memory buffer on each GPU in order to provide direct P2P access.\nWith TORCHTITAN's integration of torch.compile, AsyncTP can be easily configured in TORCHTITAN to achieve meaningful end-to-end speedups (see Section 3.2 for details) on newer hardware (H100 or newer GPUs with NVSwitch within a node). Usage details are in Appendix B.6"}, {"title": "Boosting throughput with mixed precision training and Float8 support", "content": "Mixed precision training (Micikevicius et al., 2018) provides both memory and computational savings while ensuring training stability. FSDP2 has built-in support for mixed precision training with basic torch.dtype. This covers the popular usage of performing FSDP all-gather and computation in a low precision (e.g. torch.bfloat16), and perform lossless FSDP reduce-scatter (gradient) in high precision (e.g. torch.float32) for better numerical results. See Appendix B.7 for usage details.\nTORCHTITAN also supports more advanced mixed precision training with Float8 (a derived data type) on newer hardware like H100, with substantial performance gains (reported in Section 3.2). The Float8 feature from torchao.float8 supports multiple per-tensor scaling strategies, including dynamic, delayed, and static (see Micikevicius et al. (2022); Vasiliy Kuznetsov (2024), Section 4.3 for details), while being composable with other key PyTorch-native systems such as autograd, torch.compile, FSDP2 and TP (with Float8 all-gather capability (Feng et al., 2024))."}, {"title": "Production ready training", "content": "To enable production-grade training, TORCHTITAN offers seamless integration with key features out of the box. These include (1) efficient checkpointing using PyTorch Distributed Checkpointing (DCP), and (2) debugging stuck or crashed jobs through integration with Flight Recorder."}, {"title": "Scalable and efficient Distributed Checkpointing", "content": "Checkpoints are crucial in training large language models for two reasons: they facilitate model reuse in applications like inference and evaluation, and they provide a recovery mechanism in case of failures. An optimal checkpointing workflow should ensure ease of reuse across different parallelisms and maintain high performance without slowing down training. There are two typical checkpointing methods. The first aggregates the state (model parameters and optimizer states) into an unsharded version that is parallelism-agnostic, facilitating easy reuse but requiring expensive communication. The second method has each trainer save its local sharded state, which speeds up the process but complicates reuse due to embedded parallelism information.\nDCP addresses these challenges using DTensor, which encapsulates both global and local tensor information independently of parallelism. DCP converts this information into an internal format for storage. During loading, DCP matches the stored shards with the current DTensor-based model parameters and optimizer states, fetching the necessary shard from storage. TORCHTITAN, which utilizes all native PyTorch parallelisms, effectively uses DCP to balance efficiency and usability. Furthermore, DCP enhances efficiency through asynchronous checkpointing by processing storage persistence in a separate thread, allowing this operation to overlap with subsequent training iterations. TORCHTITAN utilizes DCP's asynchronous checkpointing to reduce the checkpointing overhead by 5-15x compared to synchronous distributed checkpointing for the Llama 3.1 8B model (Zhang et al., 2024; Huang et al., 2024)."}, {"title": "Flight Recorder to debug job crashes", "content": "A common failure mode when developing parallelism code, or when running at large scale, is to observe a NCCL collective timeout and then the need to figure out the root cause. Since communication kernels are typically asynchronous from the perspective of the CPU, by the time something times out, it can be very hard to pinpoint which operation failed and why. PyTorch provides a Flight Recorder for NCCL collectives to help resolve this dilemma. It records the start and end time (on GPU) as well as the enqueue time (on CPU) for every collective or p2p operation. Additionally, it logs metadata such as which process group was used, who the source rank is (and destination, for p2p), tensor sizes, and stack traces.\nWe find the data contained in the Flight Recorder helps debug collective hangs and p2p hangs caused by bugs in parallelism code. For PP, there may be schedule bugs that lead to hangs, due to a missing or improperly ordered send or recv operation. Analysis based on the Flight Recorder data can pinpoint the latest send or recv that has completed on the GPU. For FSDP or TP, it is possible to determine whether one or more ranks has not called into a collective, perhaps due to a bug in PP scheduling or faulty logic in TP."}, {"title": "Experimentation", "content": "In this section, we demonstrate the effectiveness of elastic distributed training using TORCHTITAN, via experiments on Llama 3.1 8B, 70B, and 405B, from 1D parallelism to 3D parallelism (respectively), at the scale from 8 GPUs to 512 GPUs. We also share the knowledge and experience gained through TORCHTITAN experimentation. A walkthrough of the codebase on how we apply (up to) 3D parallelism can be found in Appendix A."}, {"title": "Experimental setup", "content": "The experiments are conducted on NVIDIA H100 GPUs\u00b9 with 95 GiB memory, where each host is equipped with 8 GPUs and NVSwitch. Two hosts form a rack connected to a TOR switch. A backend RDMA network connects the TOR switches. In TORCHTITAN we integrate a checkpointable data loader and provide built-in support for the C4 dataset (en variant), a colossal, cleaned version of Common Crawl's web crawl corpus (Raffel et al., 2020). We use the same dataset for all experiments in this section. For the tokenizer, we use the official one (tiktoken) released together with Llama 3.1."}, {"title": "Performance", "content": "To showcase the elasticity and scalability of TORCHTITAN, we experiment on a wide range of GPU scales (from 8 to 512), as the underlying model size increases (8B, 70B, and 405B) with a varying number of parallelism dimensions (1D, 2D, and 3D, respectively). To demonstrate the effectiveness of the optimization techniques introduced in Section 2.2, we show how training throughput improves when adding each individual technique on appropriate baselines. In particular, when training on a higher dimensional parallelism with new features, the baseline is always updated to include all previous techniques.\nWe note that, throughout our experimentation, memory readings are stable across the whole training process2, whereas throughput numbers (token per second, per GPU) are calculated and logged every 10 iterations, and always read at the (arbitrarily determined) 90th iteration. We do not report Model FLOPS Utilization (MFU) (Chowdhery et al., 2023) because when Float8 is enabled in TORCHTITAN, both BFLOAT16 Tensor Core and FP8 Tensor Core are involved in model training, but they have different peak FLOPS and the definition of MFU under such scenario is not well-defined. We note that the 1D Llama 3.1 8B model training on 8 or 128 H100 GPUs without Float8 achieves 33% to 42% MFU."}, {"title": "Scaling with TorchTitan 3D parallelism", "content": "The LLM scaling law imposes challenges due to increasingly larger model size and huge amount of data, which requires applying parallelism strategies on massive number of GPUs. TORCHTITAN provides the ability to compose different parallelisms to efficiently scale model training to thousands of GPUs. This section discusses the observations and motivations to apply TORCHTITAN 3D parallelism when training LLMs at large scale. Please note that there could be many 3D parallelism combinations, but we choose to only discuss one combination in this paper, which could be summarized as the following diagram:"}, {"title": "Scaling with FSDP", "content": "FSDP (ZERO) is a generalized technique which can be applied to any model architecture, making it a good choice for the first or the only degree of parallelism. As long as the FSDP communication is faster than the corresponding computation (which is the case for LLMs trained on up to hundreds of, say 512, GPUs), and there is no need to reduce (effective) per-GPU batch size to be below 1 (for reasons mentioned below in the TP section), 1D FSDP should be sufficient.\nExisting ring-based implementations of NCCL collectives (all-gather, reduce-scatter) will incur a latency overhead which becomes severe at large scale (e.g. 512 GPUs). FSDP alone will become less efficient due to the collective latency increasing linearly with the world size, resulting in FSDP collectives that cannot be hidden by the computation any more. To further scale out, one needs to consider combining model parallelism solutions such as TP and PP."}, {"title": "2D parallelism: Apply TP with FSDP", "content": "Model Parallelism (TP and PP) can help avoid the increased collective latency faced by scaling FSDP alone. TP can further lower the effective local batch size (to a minimum of $\\frac{1}{TP degree}$ when the local batch size is set to 1), as TP sharded models on multiple GPUs work jointly to process the same batch. This can be vital for reducing peak memory usage so that training could fit in GPU memory (e.g. due to large model size or sequence length), or for strong scaling with fixed desired global batch size (e.g. due to training efficiency considerations).\nIn addition, TP performs feature dimension sharding. This can bring more optimized matrix multiplication shapes for better FLOP utilization.\nAs TP introduces extra blocking collectives, in practice TP is only applied within a node that have fast interconnect (NVLink). AsyncTP could improve the performance by fully overlapping the communication,"}, {"title": "3D Parallelism: Apply PP with 2D Parallelism", "content": "Compared to other model parallelisms, PP requires less communication bandwidth by virtue of only transmit-ting activations and gradients between stages in a P2P manner. It is especially useful (1) to further reduce FSDP communication latency when the FSDP world size becomes large again that FSDP+TP still exposes FSDP collectives; or (2) to train with bandwidth-limited cluster.\nWe note that, the performance of PP, and in particular the \"bubble\" size, could vary by pipeline schedules being used and the microbatch size, assuming fixed global batch size and the world size."}, {"title": "Demonstrating adaptability and extensibility", "content": "In this section, we demonstrate the adaptability and extensibility of TORCHTITAN by highlighting ongoing work and external contributions that showcase its ability to seamlessly integrate and compare new techniques, optimizations, and models."}, {"title": "Ongoing work: 4D parallelism and zero-bubble pipeline schedules", "content": "TORCHTITAN'S modular and extensible architecture enables the seamless integration of new techniques and optimizations. For instance, ongoing work includes incorporating Context Parallel (Liu et al., 2023; Liu and Abbeel, 2024; NVIDIA, 2023) to enable 4D parallelism, and leveraging the torch.distributed.pipelining package to support zero-bubble schedules (Tang et al., 2024). This demonstrates TORCHTITAN's ability to adapt to evolving machine learning landscapes."}, {"title": "External contributions: Building and evaluating custom innovations", "content": "TORCHTITAN's flexible architecture also empowers users to easily integrate and compare new innovations. By providing a modular and efficient test bed, TORCHTITAN enables users to rapidly benchmark new techniques, optimizations, and hardware on their training performance. This has already led to the refinement of a new production-grade dataloader, improvements in a new ZeRO implementation, advancements in an Adam-based optimizer, and the training of a top-tier diffusion model."}, {"title": "Related works", "content": "With the rapidly growing significance of LLMs (Dubey et al., 2024; Achiam et al., 2023), there is substantial research and industry focus on improving infrastructure for training LLMs of various sizes. Since the very nature of these models are large, distributed training support becomes inevitable. Libraries like Megatron (Narayanan et al., 2021), DeepSpeed (Rasley et al., 2020), and PyTorch distributed (Pytorch native) (Paszke et al., 2019; Meta Platforms, Inc., 2024a) offer APIs to build distributed training workflows. NVIDIA NeMo (NVIDIA Corporation, 2024), built on Megatron-LM, offers a packaged solution for handling complex end-to-end model life-cycle from data curation to model deployment. Pytorch-native solutions like torchtune (Meta Platforms, Inc., 2024b) focus on fine-tuning LLMs in a simplified workflow. TORCHTITAN differs from these solutions by focusing on production grade pre-training using PyTorch-native APIs. The library is designed with elastic composability to accomodate the scale required to pre-train LLMs with minimal external dependencies. This lowers the bar to understand and extend pre-training, while offering features like async distributed checkpointing for building an end-to-end production workflow."}, {"title": "Conclusion", "content": "TORCHTITAN is a powerful and flexible framework for training LLMs. It offers composability, allowing users to combine various parallelism techniques (FSDP, TP, and PP), memory optimization methods (Float8 and"}, {"title": "Composable 3D parallelism walkthrough", "content": "We have discussed the scaling with TORCHTITAN 3D parallelism and the motivations to apply different parallelisms to scale training to thousands of GPUs. In this section we will walk through the 3D parallelism code in TORCHTITAN.\nThe first step is to create an instance of the model (e.g. the Transformer for Llama models) on the meta device. We then apply PP by splitting the model into multiple PP stages according to the pipeline_parallel_split_points config. Note that for PP with looped schedules, we may obtain multiple model_parts from PP splitting, where each item in model_parts is one stage-model-chunk. Next we apply SPMD-style distributed training techniques including TP, activation checkpointing, torch.compile, FSDP, and mixed precision training for each model part, before actually initializing the sharded model on GPU.\n```python\n# meta init\nwith torch.device(\"meta\"):\n model = model_cls.from_model_args(model_config)\n\n# apply PP\npp_schedule, model_parts = models_pipelining_fns[model_name](\n model, pp_mesh, parallel_dims, job_config, device, model_config, loss_fn\n)\n\nfor m in model_parts:\n # apply SPMD-style distributed training techniques\n models_parallelize_fns[model_name](m, world_mesh, parallel_dims, job_config)\n # move sharded model to GPU and initialize weights via DTensor\n m.to_empty(device=\"cuda\")\n m.init_weights()\n```\nTo apply PP to the model, we run the following code at the high level. pipeline_llama_manual_split splits the model into multiple stages according to the manually given pipeline_parallel_split_points config, by removing the unused model components from a complete model (on the meta device). Then build_pipeline_schedule make the pipeline schedule with various options from torch.distributed.pipelining, including 1F1B (Narayanan et al., 2019), GPipe (Huang et al., 2019), interleaved 1F1B (Narayanan et al., 2021), etc. instructed by the pipeline_parallel_schedule config.\n```python\nstages, models = pipeline_llama_manual_split(\n model, pp_mesh, parallel_dims, job_config, device, model_config\n)\npp_schedule = build_pipeline_schedule(job_config, stages, loss_fn)\nreturn pp_schedule, models\n```\nTP and FSDP are applied in the SPMD-style models_parallelize_fns function. To apply TP, we utilize the DTensor parallelize_module API, by providing a TP \"plan\" as the instruction of how model parameters should be sharded. In the example below, we showcase the (incomplete) code for sharding the repeated Transformer Block.\n```python\nfor layer_id, transformer_block in model.layers.items():\n layer_tp_plan = {\n \"attention_norm\": SequenceParallel(),\n \"attention\": PrepareModuleInput(\n input_layouts=(Shard(1), None),\n desired_input_layouts=(Replicate(), None),\n ),\n \"attention.wq\": ColwiseParallel(),\n }\n parallelize_module(\n module=transformer_block,\n device_mesh-tp_mesh,\n parallelize_plan=layer_tp_plan,\n )\n```\nFinally, we apply the FSDP by wrapping each individual TransformerBlock and then the whole model. Note that the FSDP2 implementation in PyTorch comes with mixed precision training support. By default, we use torch.bfloat16 on parameters all-gather and activation computations, and use torch.float32 on gradient reduce-scatter communication and optimizer updates.\n```python\nmp_policy = MixedPrecisionPolicy(param_dtype, reduce_dtype)\nfsdp_config = {\"mesh\": dp_mesh, \"mp_policy\": mp_policy}\n\nfor layer_id, transformer_block in model.layers.items():\n # As an optimization, do not reshard_after_forward for the last\n # TransformerBlock since FSDP would prefetch it immediately\n reshard_after_forward = int(layer_id) < len(model.layers) - 1\n fully_shard(\n transformer_block,\n **fsdp_config,\n reshard_after_forward=reshard_after_forward,\n )\n\nfully_shard(model, **fsdp_config)\n```"}, {"title": "Supplementary Materials", "content": ""}, {"title": "Fully Sharded Data Parallel", "content": "FSDP2 makes improvements over the original FSDP1 FlatParameter grouping. Specifically, parameters are now represented as DTensors sharded on the tensor dimension 0. This provides better composability with model parallelism techniques and other features that requires the manipulation of individual parameters, allowing sharded state dict to be represented by DTensor without any communication, and provides for a simpler meta-device initialization flow via DTensor. For example, FSDP2 unlocks finer grained tensor level quantization, especially Float8 tensor quantization, which we will showcase in the results section.\nAs part of the rewrite from FSDP1 to FSDP2, FSDP2 implements an improved memory management system by avoiding the use of record stream. This enables deterministic memory release, and as a result provides lower memory requirements per GPU relative to FSDP1. For example on Llama 2 7B, FSDP2 records an average of 7% lower GPU memory versus FSDP1.\nIn addition, by writing efficient kernels to perform multi-tensor allgather and reduce scatter, FSDP2 shows on-par performance compare to FSDP1, an there are slight performance gains from FSDP2 - using the Llama 2 7B, FSDP2 shows an average gain of 1.5% faster throughput.\nThe performance gains are the result of employing two small performance improvements. First, only a single division kernel is run for the FP32 reduce scatter (pre-dividing the local FP32 reduce-scatter gradient by world size, instead of a two step pre and post divide by square root of world size). Secondly, in TORCHTITAN, FSDP2 is integrated with a default of not sharding the final block in a transformer layer during the forward pass, since it will be immediately re-gathered at the start of the backward pass. Thus we can skip a round of communications delay."}, {"title": "Hybrid Sharded Data Parallel", "content": "Hybrid Sharded Data Parallel (HSDP) is an extension of FSDP (Zhang et al., 2022), which enables a larger total world size to be used. In FSDP, all devices are part of a single global group across which all communications are enabled. However, at some point, adding more computation is offset by the increasing communication overhead due to adding more participants which require equal communication participation. This is due to the fact that the latency of collective communications have a direct correlation with the total number of participants. At this saturation point, FSDP throughput will effectively flat-line even as more computation is added. HSDP obviates this to some degree by creating smaller sharding groups (islands) within the original global group (ocean), where each sharding group runs FSDP amongst itself, and gradients are synced across sharding groups at set frequency during the backward pass to ensure a global gradient is maintained. This ensures speedy communications as the total participant communication size is now a fraction of the original world size, and the only global communication is for the gradient all-reduce between the sharding groups. By using sharding groups, we have seen that HSDP can extend the total world size by 3-6x relative to FSDP's communication saturation point (this will vary, depending on the speed of network interconnects).\nTORCHTITAN makes it easy to run HSDP with two user configurable settings for sharding group size and replication group size, from the command line or TOML file."}, {"title": "Tensor Parallel", "content": "TP partitions the attention and feed forward network (MLP) modules of a transformer layer across multiple devices, where the number of devices used is the TP degree. This allows for multiple GPU's to cooperatively process a transformer layer that would otherwise exceed a single GPU's ability, at the cost of adding all-reduce/all-gather/reduce-scatter operations to synchronize intermediates.\nDue to the additional collectives introduced by TP, it needs to happen on a fast network (i.e NVLink). When training LLMS, TP is usually combined with FSDP, where TP shards within nodes and FSDP shards across nodes to create the 2D hierarchical sharding on different DeviceMesh dimensions."}, {"title": "Pipeline Parallel", "content": "We expose several parameters to configure PP. pipeline_parallel_degree controls the number of ranks participating in PP. pipeline_parallel_split_points accepts a list of strings, representing layer fully-qualified-names before which a split will be performed. Thus, the total number of pipeline stages V will be determined by the length of this list. pipeline_parallel_schedule accepts the name of the schedule to be used. If the"}, {"title": "Activation checkpointing", "content": "TORCHTITAN offers two types of Selective Activation Checkpointing which allow for a more nuanced tradeoff between memory and recomputation. Specifically, we offer the option to selectively checkpoint \"per layer\" or"}, {"title": "AsyncTP", "content": "The Symmetric Memory collectives used in AsyncTP are faster than standard NCCL collectives and operate by having each GPU allocate an identical memory buffer in order to provide direct P2P access. SymmetricMemory relies on having NVSwitch within the node, and is thus generally only available for H100 or newer GPUs."}, {"title": "Customizing F"}]}