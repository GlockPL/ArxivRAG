{"title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders", "authors": ["Cheng Li", "May Fung", "Qingyun Wang", "Chi Han", "Manling Li", "Jindong Wang", "Heng Ji"], "abstract": "Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-40. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main", "sections": [{"title": "1 Introduction", "content": "Mental health disorders include a variety of conditions such as anxiety, depression, and schizophrenia, which affect people's thinking, emotions, behavior, or mood [Prince et al., 2007]. In 2019, approximately 970 million people worldwide lived with a mental health disorder, with anxiety and depression being most prevalent [WHO, 2022]. The number increased by 28% in 2020 and continues to increase. Despite the availability of effective treatments, many individuals lack access to adequate care due to under-resourced health systems. For example, only 29% of people with psychosis and one third of people with depression receive formal mental healthcare [WHO, 2022]. It is indispensable to develop machine learning models for the automatic diagnosis and treatment of such diseases. However, existing AI therapist systems use templates and decision trees, which are not flexible to meet the large demands on personalized care [Devarakonda et al., 2019, D'Alfonso, 2020, Fiske et al., 2019, Grodniewicz and Hohol, 2023].\nThe key to training powerful models is to collect sufficient training data. However, due to privacy concerns in the medical domain, data collection, especially personalized data for mental health disorders, is inherently challenging. A growing body of work has focused on enhancing mental health language models by sourcing additional domain-specific data from social media [Hu et al., 2024a, Xu et al., 2024, Yang et al., 2024a]. However, social media data are inherently biased and under-representative, failing to capture the full spectrum of people's mental health needs. Moreover, as LLMs continue to scale, the availability of training data in the real world becomes increasingly limited, further exacerbating this challenge. Existing methods are likely to soon reach their performance limit.\nRecently, several works have focused on self-play [Hu et al., 2024b, Liang et al., 2024, Wang et al., 2024d, Wu et al., 2024, Yang et al., 2024b], where models play different roles and self-evolve or co-evolve during interaction with other models. A model synthesizes training data on its own and then use the generated data to train itself. However, there are two challenges that prevent us from adopting self-play training for mental health disorders: (1) Scarcity of high-quality"}, {"title": "2 Related Work", "content": "2.1 Large Language Models for healthcare\nResearchers have explored the potential of large language models (LLMs) in healthcare [Jiang et al., 2023, Li et al., 2023a,b, Liu et al., 2023, Lupetti et al., 2023, Nori et al., 2023a, Singhal et al., 2023, Wang et al., 2024c, Wu et al., 2023]. For example, Singhal et al. [2023] fine-tuned PaLM-2 for medical applications, achieving 86.5% accuracy on the MedQA dataset. Similarly, Wu et al. [2023] fine-tuned LLaMA on medical literature, showing strong performance in biomedical QA tasks.\nIn the mental health domain, research has taken two main approaches. The first involves fine-tuning domain-specific LLMs on existing datasets or social media data, such as Mental-LLaMA [Yang et al., 2024a] and Mental-LLM, fine-tuned on Reddit data [Xu et al., 2024]. The second approach enhances mental health performance through prompt engineering. Yang et al. [2023] proposed emotion-enhanced prompting strategies to guide LLMs in explainable mental health analyses.\nUnlike previous methods, MentalArena fine-tunes mental health models through self-play training, in which the base model assumes both patient and therapist. Training data is generated dynamically during the interactions between these two roles, allowing for more effective model refinement.\n2.2 Self-play frameworks in Large Language Models\nSelf-play involves a model evolving through interactions with copies of itself, creating a feedback loop that refines performance without external input. It is particularly effective in environments where the model simulates multiple roles, such as multiplayer games [Silver et al., 2016, 2017]. Compared to interactive methods, self-play provides a more efficient strategy for obtaining feedback without relying on an external environment.\nTaubenfeld et al. [2024] examine biases in LLM-generated debate simulations, while Ulmer et al. [2024] focus on principle-guided conversations. Role-playing approaches, like Lu et al. [2024]'s self-simulated dialogues with character profiles and Askari et al. [2024]'s SOLID framework for intent-aware role-play, leverage LLMs to generate information-rich exchanges.\nDue to the lack of sufficient data in the training corpus, LLMs are unable to accurately simulate real patients, presenting a significant challenge for self-play training. To overcome this, MentalArena introduces Symptom Encoder, a component designed to effectively model real mental health patients."}, {"title": "3 MentalArena", "content": "3.1 Preliminaries\nWe first go over the process of the diagnosis and treatment of mental health disorder and explain key concepts. Mental health diagnosis begins with assessing an individual's health state, encompassing mental and emotional well-being. Symptoms are key indicators of possible problems, including emotional (e.g., anxiety, depression), cognitive (e.g., memory problems) and behavioral changes (e.g., social withdrawal). These symptoms lead to a formal diagnosis made through clinical interviews identifying specific disorders such as depression, anxiety, or schizophrenia. Once diagnosed, the treatment process begins, often involving a combination of psychotherapy (e.g. cognitive-behavioral therapy), lifestyle changes, and sometimes medication. Medications, such as antidepressants and mood stabilizers, are used to regulate brain chemicals and alleviate symptoms. [Prince et al., 2007]\n3.2 Overview of the Framework\nAlthough it is trivial to adopt the self-play training paradigm in fine-tuning general language models [Askari et al., 2024, Lu et al., 2024, Taubenfeld et al., 2024, Ulmer et al., 2024, Wang et al., 2024b,d], it remains unexplored and challenging to exploit such a framework in the medical domain due to the data deficiency in medical and intent bias problem between patients and therapists. The first challenge makes it difficult to play a patient role [Schmidgall et al., 2024, Wang et al., 2024a] due to the data deficiency of the patient in the training corpus, while the latter undermines the effective diagnosis and treatment of explicit symptoms."}, {"title": "3.3 Patient: Symptom Encoder", "content": "The module Symptom Encoder aims to model mental health patient from both cognitive and behavioral perspectives, which learns meaningful symptoms $S_o$ from the original patient health data $x$. Specifically, the module learns symptoms from the aspects of cognition and behavior. The cognitive model is designed based on cognitive behavior therapy (CBT) principles [Beck, 2020], a popular paradigm in psychotherapy. Cognitive models address maladaptive cognitive structures that are embedded in various contexts, including familial conflicts, relationship challenges, workplace challenges, and other areas. The models consist of eight key components: relevant history, core beliefs, intermediate beliefs, coping strategies, situational factors, automatic thoughts, emotions, and behaviors. [Beck, 2020] The explanation of each component of the cognitive model can be found in Appendix F.2. Appendix F.1 shows the example of cognitive models. We obtain 106 patient cognitive models from previous work [Wang et al., 2024a], which are created by clinical psychologists. To simulate the cognitive activity of the mental health patient, we encode those cognitive models into patient via prompt. Our prompts are shown in Appendix A.\nFor patient behavior modeling, we use behavior principles collected by Louie et al. [2024] as a behavior library, created by 25 mental health experts. Examples of behavior patterns are shown in Appendix F.1. To find the proper behavior pattern for each cognitive model, we first semantically match the coping strategies of cognitive model with each behavior pattern. We obtain the embeddings for each coping strategy and behavior principle via Bert-base [Devlin et al., 2018], considering on effectiveness and cost. Then we compute the semantic similarity between coping strategies and behavior pattern. The max similarity score of all behavior principles in one behavior pattern is selected to represent the score of the pattern. The five behavior patterns with the highest scores are kept. To further ensure the most appropriate pattern, we prompt GPT-4-turbo [OpenAI, 2023b] to pick one from the five patterns. The final behavior pattern is also integrated into patient via prompt, which is shown in Appendix A."}, {"title": "3.4 Therapist: Symptom Decoder", "content": "During interactions between a real therapist and a real patient, the patient may try to express one opinion while the therapist misunderstands the intent due to prior knowledge and deficiency of experience [Britten et al., 2000, West, 1984]. Intention bias can similarly arise in conversations between patients and therapists played by AI models, resulting in inaccurate diagnosis and treatment. Symptom Decoder is designed to mitigate the intent bias. After several conversations, the therapist reviews the patient's health information from previous interactions and conducts a detailed analysis of the patient's cognitive and behavioral patterns, resulting in the diagnosed symptom $S_a$. We then semantically match the encoded symptom $S_0$ with the diagnosed symptom $S_a$ and guide subsequent conversations based on the differences between $S_0$ and $S_a$.\nAs shown in Figure 2(left), the therapist decodes cognitive and behavior principles according to the conversation history. For example, the decoded cognitive principle is: \"The patient stay away from family interactions to minimize emotional distress and feelings of abandonment\u201d. The decoded behavior principle is: \"When sharing personal struggles, express feelings of confusion, doubt, and emotional turmoil to convey a sense of vulnerability and authenticity\u201d. Then we compute the semantic similarity score of the decoded symptom $S_a$ and the encoded symptom $S_0$. If the score is greater than 0.9, the conversation will end, indicating that the therapist has fully understood the health state of the patient. Otherwise, it indicates the existence of intent bias. To help the therapist better know more about the health state of the patient, we summarize the differences between the decoded symptom $S_a$ and encoded symptom $S_0$ and generate some feedback for further inquiries via the GPT-4-turbo [OpenAI, 2023b], which can remind the therapist of missing or confusing information about the patient. For instance, the feedback is like \u201cThe therapist can focus on what is going on that has been making the patient feel tense.\u201d And the conversation will not end until the similarity score between $S_a$ and $S_0$ is greater than 0.9.\nAfter the conversation ends, the therapist analyzes the patient's symptom, $S_a$, and formulates several diagnostic plans $(\\delta_1, \\delta_2, ..., \\delta_n)$. To ensure diagnostic accuracy, the patient reviews each plan and selects the most appropriate one based on their health condition. Subsequently, the therapist proposes a series of treatment and medication plans $(\\{\\gamma_1, \\beta_1\\}, ..., \\{\\gamma_k, \\beta_k\\})$ in accordance with the selected diagnosis $(\\delta_{best})$. To identify the optimal treatment and medication plans, we apply each plan to the patient (initially represented by the encoded symptom $S_0$) and monitor the progression"}, {"title": "3.5 Model Optimizer", "content": "After obtaining treatment, diagnosis, and medication through Symptom Decoder, we train $M$ in a self-play manner to get a better model capable of making a personalized diagnosis and treatment (as a therapist) and presenting information (as a patient). An example of such a supervised fine-tuning process is illustrated in Figure 8.\nDuring each iteration, the patient and the therapist are powered by the same model $M$ and both get improved when $M$ is updated. While our framework is flexible to allow for different base models for the two roles, we adopt the same one due to the following reasons. First, it is intuitive that training one base model is more efficient compared to training different models. Second, and more importantly, training one base model can help reduce the knowledge gap between two roles. Two different base models can certainly exhibit knowledge gaps, and iterative training will enlarge them due to different architectures and pre-training data of the models. Appendix H shows the detailed training settings."}, {"title": "4 Experiment", "content": "4.1 Setup\nDatasets: As summarized in Table 1, we adopt 6 datasets: MedQA [Jin et al., 2021], MedMCQA [Pal et al., 2022], PubMedQA [Jin et al., 2019], CASM [Garg et al., 2022], Dreaddit [Turcan and McKeown, 2019] and Irf [Garg et al., 2023]. Our evaluation spans biomedical QA and mental health detection, covering knowledge on diagnosis, treatment, and medication. These datasets include general mental health tasks, such as depression/suicide, stress, and interpersonal risk factors detection, as well as real-world mental health cases. Details on the benchmarks are provided in Appendix C\nBaselines: We compare our models with other mental health models with different prompt engineering methods. For baseline models, we compare with the state-of-the-art LLMs: GPT-3.5-turbo [OpenAI, 2023a], GPT-4o [OpenAI, 2024] and Llama-3-8b [Dubey et al., 2024]. We also compare with recent specific models on mental health: MentaLLaMa-13b [Yang et al., 2024a], Mental-LLM-alpaca [Xu et al., 2024] and Mental-LLM-t5 [Xu et al., 2024]. For prompt engineering, we compare with MedPrompt [Nori et al., 2023b], and Zero-shot CoT [Kojima et al., 2022], which are proved to be effective in the biomedical domain. The prompt templates are shown in Appendix B. Those strategies are implemented on GPT-3.5-turbo, GPT-4o and Llama-3-8b for fair comparison. We used a zero-shot setting in all experiments to assess LLMs' domain knowledge, except for baseline experiments on MedPrompt and Zero-shot CoT.\nAll results are reported based on accuracy."}, {"title": "4.2 Main Results and Ablation Study", "content": "We report the main results in Table 2, highlighting two key findings: 1) First, our fine-tuned model perform the best in each group. Our model fine-tuned on GPT-3.5-turbo is the strongest model among all open-source and closed-source models. Our fine-tuned models all surpass GPT-4o, whose baseline models (GPT-3.5-turbo and Llama-3-8b) are much weaker than GPT-4o. 2) Second, our method brings a great improvement to the baseline models. Our model fine-tuned on GPT-3.5-turbo surpasses GPT-3.5-turbo 20.74% on average. Our model fine-tuned on Llama-3-8b surpasses Llama-3-8b 6.64% on average.\nWe perform an ablation study on models based on GPT-3.5-turbo and Llama-3-8b. There are seven different settings. \"Baseline+c\" means training baseline model on cognitive seed data. We convert each seed sample (Cognitive Model) into two QA pairs and fine-tune baseline models. The examples are shown in Appendix E. \"Baseline+d\" means training with only diagnosis data. \"Baseline+d+t\" means training with diagnosis and treatment data. \u201cBaseline+d+t+m\" means training with diagnosis, treatment and medicine data. Training examples are shown in Figure 8. For \"Baseline+d+t+m"}, {"title": "4.3 Effectiveness Analysis", "content": "Why self-play training improves the performance? Table 4 presents detailed results for each iteration. Initially, the models improve iteratively until performance peaks, after which it declines. For GPT-3.5-turbo, performance improves over the first two iterations, then declines. For Llama-3-8b, performance increases over the first four iterations before weakening after iter_4.\nWhich iteration gives the best model? To answer this question, we compute perplexity score [Marion et al., 2023, Wang et al., 2023] and diversity gain [Bilmes, 2022] for training data at each iteration. The details on those metrics can be found in Appendix D. Specifically, we sample 500 generated data at each iteration to compute the perplexity score. We compute the diversity gain for the data in the current iteration comparing with that in the last iteration. Figure 4 shows the results\u00b3. 1) The trend of perplexity score and that of model performance are highly similar, indicating their high relevance. 2) For diversity gain, a borderline is related to model performance. The model performance will increase if diversity gain surpasses the borderline. And it will decline if diversity gain is below the borderline. For example, as shown in Figure 4, diversity gain at the first four iterations all surpass the borderline and the performance also get improved continuously. And diversity gain for the last two iterations are below the borderline and the performance also decline."}, {"title": "5 Discussion", "content": "5.1 Can Symptom Encoder mimic real mental health patient?\nTo explore the problem, we generate 50 four-turn conversations between an AI-patient and an AI-therapist, where the AI-patient is powered by either baseline models or our models, and the AI-therapist is powered by GPT-4o [OpenAI, 2024]. After each conversation, the AI-therapist assesses whether the patient is human or AI-generated. We analyze the results provided by GPT-40 and present them in Table 3. The findings indicate that our models more accurately simulate mental health patients compared to the baseline models.\n5.2 The validity of generated data\nTo verify the validity of our generated data, we random select 1500 samples from the data for fine-tune our GPT and Llama version model, respectively. The validity check is conducted by prompting GPT-4o with the query:\nQuestion: ()Answer: () Is the answer reasonable? Please respond with Yes or No. We then com-\npute the validity rate of these QA pairs. The results, presented in Table 3, demonstrate that the data generated by MentalArena is both valid and reasonable.\n5.3 Generalization\nWe generate data for training domain model via simulating cognitive and behavior patterns of real mental health patient. According to Medicine [2024], an estimated 26% of Americans ages 18 and older-about 1 in 4 adults-suffers from a diagnosable mental disorder in a given year. Therefore, a large scale of patients may exhibit similar cognitive and behavioral patterns as those with mental health conditions. In this part, we explore whether MentalArena can generalize to other illnesses.\nWe select MedMCQA [Pal et al., 2022] and MMLU [Hendrycks et al., 2020] as benchmarks. Appendix C.2 shows details on benchmarks. We evaluate on 6 medically relevant subset of MMLU tasks: medical genetics test, college biology test, college medicine test, professional medicine test, clinical knowledge test, high school biology test. Figure 5 shows the results on above tasks. Our models surpass corresponding baseline models for a large margin on all tasks, covering several different diseases. It proves the generalization ability of our method in medical domain.\n5.4 Fine-tuning vs. forgetting\nIt is a potential dilemma that fine-tuning an LLM on specific tasks might face catas-trophic forgetting of its original capabilities. In this section, we explore the forgetting possibility of MentalArena on BIG-Bench-Hard (BBH) [Suzgun et al., 2022]. BBH"}, {"title": "5.5 Qualitative analysis", "content": "We conduct a qualitative analysis of our models in comparison to the corresponding baseline models. Figure 7 illustrates an example of the outputs from GPT-3.5-turbo and our fine-tuned model. Our model accurately answers the medical question, while GPT-3.5-turbo provides an incorrect response. This discrepancy arises because the data generated during the patient-therapist interactions contains valuable medical knowledge, which aids in the analysis and formulation of the answer. Additional cases for comparison are presented in Appendix I."}, {"title": "6 Conclusion, Societal Impact and Limitations", "content": "In this paper, we introduce MentalArena, a self-play framework designed to train language models by generating domain-specific personalized data. This approach enables the creation of models capable of making personalized diagnosis and treatment (as a therapist) and presenting information (as a patient). We evaluated MentalArena against six benchmarks, including biomedicalQA and mental health tasks, in comparison to six advanced models. Our models, fine-tuned on both GPT-3.5-turbo and Llama-3-8b, significantly outperform their counterparts, including GPT-4o.\nMentalArena offers promising solutions for personalized care, enhancing accessibility to tailored treatments while safeguarding patient privacy. Such innovations can help bridge the gap between mental health needs and the availability of effective, individualized care, ultimately fostering a more supportive and informed society.\nOur work has the following limitations. 1) The experiments on data authenticity and validity (Sections 5.1 and 5.2) were evaluated using GPT-40, which may introduce deviations in the results due to potential limitations in GPT-40's performance. 2) Our model based on Llama-3-8b may not represent the optimal model of MentalArena, as large-scale training was constrained by computational resources. 3) Further implementation on additional open-source models could provide stronger evidence supporting the effectiveness of MentalArena."}, {"title": "Ethics Statement", "content": "In this study, ethical considerations focus on ensuring privacy and safeguarding personal data, particularly in the sensitive domain of mental health. The use of AI-generated data must be transparent, with clear guidelines on its role in augmenting human judgment without replacing healthcare professionals. Additionally, measures to prevent bias and ensure fairness in diagnosis and treatment are essential to avoid exacerbating existing disparities in mental healthcare."}]}