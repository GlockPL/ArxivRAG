{"title": "A Survey on Benchmarks of Multimodal Large Language Models", "authors": ["Jian Li", "Weiheng Lu"], "abstract": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity\nin both academia and industry due to their remarkable performance in various ap-\nplications such as visual question answering, visual perception, understanding, and\nreasoning. Over the past few years, significant efforts have been made to examine\nMLLMs from multiple perspectives. This paper presents a comprehensive review\nof 180 benchmarks and evaluation for MLLMs, focusing on (1)perception and\nunderstanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities,\nand (5)other modalities. Finally, we discuss the limitations of the current evaluation\nmethods for MLLMs and explore promising future directions. Our key argument\nis that evaluation should be regarded as a crucial discipline to better support the\ndevelopment of MLLMs. For more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLM) have recently garnered substantial interest across academic and\nindustrial domains. The impressive performance of LLMs such as GPT [1] has fueled optimism that\nthey could represent a step towards Artificial General Intelligence (AGI) in this era. The remarkable\nabilities of LLM have inspired efforts to integrate them with other modality-based models to enhance\nmultimodal competencies. Consequently, Multimodal Large Language Models (MLLMs) [2] have\nemerged, This concept is further supported by the extraordinary success of proprietary models like\nOpenAI's GPT-4V [3] and Google's Gemini[4]. In contrast to earlier models that were limited\nto solving specific tasks, Multimodal Large Language Models (MLLMs) demonstrate exceptional\nperformance across a variety of applications, including both general Visual Question Answering\n(VQA) tasks and domain-specific challenges.\nA comprehensive and objective benchmark for evaluating MLLMs is essential for comparing and\ninvestigating the performance of various models, and it plays a crucial role in the success of MLLMs.\nFirst, evaluating LLMs helps us better understand the strengths and weaknesses of MLLMs. For\nexample, the SEED-Bench [5] illustrates that current MLLMs show weaker abilities in understanding\nspatial relationships between objects while achieving relatively high performance on global image\ncomprehension. Second, evaluations across various scenarios can offer valuable guidance for MLLM\napplications in fields such as medicine, industry, and autonomous driving. This, in turn, can inspire\nfuture designs and expand the scope of their capabilities. Third, the broad applicability of MLLMs\nunderscores the importance of ensuring their robustness, safety, and reliability, especially in safety-\nsensitive sectors. Finally, it is significant to evaluate other user-friendly features of MLLMs including\nthe ability to handle long contexts and accurately follow instructions. Therefore, we aim to raise\nawareness in the community of the importance of MLLM evaluations by reviewing the current\nevaluation protocols."}, {"title": "2 Preliminaries", "content": "Figure. 1 compares several common MLLMs including GPT4[3], Gemini[4], LLaVA[185], Qwen-\nVL[186], Claude[187], InstructBLIP[188], mPLUG-Owl2[189], SPHINX[190], Intern-VL[191],\nYi-VL[192], VideoChat2[193], Video-LLaMA[194], Cambrian-1[195], PLLaVA[196], Blip2[197],\nMiniGPT4-Video[198]. The standard MLLM framework can be divided into three main modules: a\nvisual encoder g tasked with receiving and processing visual inputs, a pre-trained language model that\nmanages the received multimodal signals and performs reasoning, and a visual-language projector P\nwhich functions as a bridge to align the two modalities. A diagram of the architecture and training\nprocess is illustrated in Figure. 3. This figure outlines the base LLM, the vision encoder, the projector,\nas well as the pertaining and instruction tuning."}, {"title": "2.1 MLLM Architecture", "content": "Vision Encoder Taking the input image $X_v$ as input, the vision encoder compresses the original\nimage into more compact patch features $Z_v$, as represented by the following formula:\n$Z_v = g(X_v)$.\n(1)\nThe encoder $g$ can also be an audio encoder for audio feature extraction or an encoder for other\nmodalities. The common vision encoders are CLIP [199], SigLIP [200] and DINO [201, 202].\nVision-Language Projector The task of the vision-language projector is to map the visual patch\nembeddings $Z_v$ into the text feature space:\n$H_v = P(Z_v)$,\n(2)\nwhere $H_v$ denotes the projected visual embeddings. The aligned visual features are used as prompts\nand inputted into the language model along with the text embeddings. Several works, such as Qformer\nin BLIP-2 [197], design new projectors to reduce the number of visual tokens for efficiency.\nLarge Language Model The pre-trained Large language model serves as the core component of\nMLLMS, endowing it with many outstanding capabilities, such as zero-shot generalization, instruction\nfollowing, and in-context learning. The LLM accepts input sequences containing multiple modalities\nand outputs corresponding text sequences. A text tokenizer is typically bundled with the LLM,\nmapping text prompts $X_q$ to the text tokens $H_q$. The text tokens $H_q$ and the visual tokens $H_v$ are\nconcatenated as the input of the language model, which outputs the final response sequence $Y_a$ in an\nautoregressive manner:\n$p(Y_a|H_v,H_q) = \\prod_{i=1}^{L} P(Y_i|H_v,H_q,Y_{<i}),$\n(3)\nwhere $L$ denotes the length of $Y_a$. The parameter sizes of large language models (LLMs) range from\n3 billion to tens of billions. Commonly used open-source LLMs include the Llama series[203, 204,\n205, 206], Phi [207, 208], Gemma [209],Qwen [210]."}, {"title": "2.2 MLLM Training", "content": "The standard training process of MLLMs is a crucial factor that determines their performance on\ndownstream tasks and their ability to handle diverse tasks. In this section, we provide an overview of\nvarious training methodologies, including pre-training, and instruction-tuning."}, {"title": "3 Perception and Understanding", "content": "When evaluating the perception and understanding capabilities of MLLMs, we focus on benchmarks\nthat assess the model's fundamental abilities in visual information processing. This includes evalu-\nating the MLLMs' accuracy in object identification and detection, understanding of scene context\nand object relationships, and ability to respond to questions about image content. Perception and\nunderstanding abilities are the cornerstone of MLLMs, enabling them to perform a wide range of tasks\nand applications. This section first introduces comprehensive evaluation benchmarks for MLLMs,\nand then separately discusses coarse-grained and fine-grained benchmarks for visual perception."}, {"title": "3.1 Comprehensive Evaluation", "content": "MLLMs rely on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities\nin various studies. In order to fully match the flourish of MLLMs, Many comprehensive evaluation\nbenchmarks are proposed."}, {"title": "3.2 Fine-grained Perception", "content": "One indispensable cornerstone of MLLMs is the ability to perceive visible objects within scenes\nprecisely. This includes evaluating MLLMs' capabilities in object detection and recognition, un-\nderstanding details within local regions, and achieving accurate vision-language alignment. Such\nfine-grained perception is crucial for effective multimodal understanding and interaction.\nVisual Grounding and Object Detection: Object-level grounding and detection are critical steps\nin better perceiving images and solving image-related questions, providing a stronger link between\nQA pairs and images. Flickr30k Entities [19] and Visual7W [20] were early benchmarks for entity\nlocalization, focusing on the detailed grounding of specific phrases in image regions and QA tasks\nrelated to local areas. These benchmarks served as the foundation for subsequent developments in\nthe field. However, they did not provide enough detailed contextual information about objects within\nthe scene. Zang et al. [18] investigated contextual object detection, which involved understanding\nvisible objects within human-AI interactive contexts, and presented the CODE benchmark to facilitate\nresearch in this area. To evaluate MLLMs' ability in challenging scenarios where images contained\nabundant and complex information, Wu et al. [21] introduced V*Bench, a benchmark focused on\ndetailed visual grounding in high-resolution images.\nFine-grained Identification and Recognition:Fine-grained identification and recognition requires\nMLLMs to identify and analyze detailed visual features. In order to determine what makes for a good\nvisual tokenizer, GVT-bench[22] was designed to evaluate MLLMs' fine-grained visual perception\nthrough object counting and multi-class identification. However, the data used in GVT-bench were\nconstrained to relatively small resolutions. To address this limitation, Li et al. [27] extended the input\nresolution to a much larger range and introduced MagnifierBench, which tested MLLMs' ability to\ndiscern the details of small objects in high-resolution images. Considering that most open-source\nMLLMS adopted the Pretrained Contrastive Language-Image PreTraining (CLIP) model[199] as the\nvisual encoder, Tong et al. [23] proposed the MMVP benchmark consisting of CLIP-blind pairs.\nThey found that all tested models struggled with simple questions about visual details. Given that\nexisting benchmarks were insufficient in size and did not cover crucial visual elements such as depth\nand spatial awareness, CV-Bench [24] was created to evaluate the fundamental 2D and 3D visual\nunderstanding of MLLMs, using a significantly larger number of examples. Additionally, P2GB [25]\nwas designed to assess fine-grained visual capabilities, especially in text-rich scenarios, requiring"}, {"title": "3.3 Image Understanding", "content": "The image understanding task involves analyzing visual content to extract meaningful information,\nwhich includes grasping the context of scenes and integrating visual details with textual information\nto generate coherent descriptions and insights.\nMulti-image Understanding: Multi-image understanding requires MLLMs to compare, analyze,\nand comprehend the relationships or variations among multiple images, thereby enabling more\ncomprehensive insights into visual content. Mementos[34] was designed to assess MLLMs' sequential\nimage understanding abilities, but it primarily focused on state changes among images, while\nneglecting other aspects. To address this issue, MileBench [35] and MuirBench [36] were constructed\nto assess multi-image cognition through a variety of task types. However, these benchmarks fell short\nin terms of task depth and breadth. Furthermore, MMIU [38] provided a comprehensive evaluation\nby including a large number of test samples that spanned a wide range of multi-image tasks and\nrelationships. Besides, Kil et al.[37] focused on tasks involving relativity and comparison between\nmultiple visual input, then introduced COMPBENCH to evaluate the comparative understanding\ncapabilities of MLLMs. COMPBENCH required MLLMs to answer questions based on a pair of\nvisually or semantically related images.\nImplication Understanding: Understanding the meaning of images requires not only intuitive\nobservation but also an exploration of the human emotions and cultural contexts they reflect. II-\nBench[39] and ImplicitAVE[40] aimed to assess MLLMs' higher-order perceptual abilities with\nvisual implications. To evaluate the emotional perception capabilities of MLLMs, FABA-Bench [41]\nwas designed for facial affective behavior analysis. FABA-Bench required MLLM to recognize facial\nexpressions and movements, which are critical to understanding an individual's emotional states and\nintentions.\nImage Quality and Aesthetics Perception:Image quality and aesthetics perception involves assessing\nimage quality, perceiving visual distortions, and understanding low-level attributes such as color,\nlighting, composition, and style. It also relates to the aesthetics and design sense of photographs.\nQ-Bench[45] explored the potential of MLLMs in low-level perception abilities. To highlight subtle\ndifferences or similarities that might not be evident when images were viewed in isolation, Q-\nBench+[46] extended the evaluation of low-level perception from single images to image pairs. To\nbetter align with human aesthetics, comprehensive aesthetic evaluation benchmarks AesBench[42]\nand UNIAA[43] were constructed to systematically evaluate the aesthetic abilities of MLLMs.\nBesides, Lin et al.[211] proposed DesignProbe to comprehensively assess design capabilities of\nMLLMs from both the element level and the overall design level."}, {"title": "4 Cognition and Reasoning", "content": "MLLMs' cognitive and reasoning abilities encompass the model's capacity for advanced processing\nand complex inference beyond basic perception and understanding. Cognitive abilities involve\nintegrating and manipulating extracted information to form coherent representations, while reasoning\nabilities focus on drawing logical conclusions and solving problems. Strong cognitive and reasoning\nabilities enable MLLMs to perform effective logical inference in complex tasks."}, {"title": "4.1 General Reasoning", "content": "The reasoning ability of MLLMs involves extracting and inferring relevant information from visual\nand textual inputs to draw logical conclusions and answer questions. This section introduces bench-\nmarks for evaluating MLLMs' general reasoning capabilities, focusing on three key areas: visual\nrelation reasoning, vision-indispensable reasoning, and context-related reasoning.\nVisual Relation: Evaluating MLLMs' reasoning of visual spatial relationships involves assessing their\nabilities to understand the spatial arrangement, relative positions, and interactions of objects. VSR[51]\nand What's Up[48] were developed to test MLLMs' ability to reason about spatial relations in natural\nimage-text pairs. However, these benchmarks primarily focused on classification tasks rather than\ncomprehending relations within scenes. To assess more complex and general relational reasoning of\nMLLMs, Wang et al.[50] introduced the Circular-based Relation Probing Evaluation (CRPE), the first\nbenchmark to encompass all elements of relation triplets (subject, predicate, object). Recognizing that\ninteractions and associations between distinct objects remained a significant challenge for MLLMs,\nNie et al.[47] developed Multi-Modal Relation Understanding (MMRel), a large-scale benchmark\nconsisting of data with well-defined inter-object relations. Additionally, GSR-BENCH[49] extended\nthe What's Up benchmark by incorporating bounding box annotations and depth information to\nbetter evaluate grounded spatial relation understanding. For the evaluation of 3D spatial cognition in\nMLLMs, Chen et al.[52] proposed SpatialRGBT-Bench, a benchmark that incorporated ground-truth\n3D annotations and flexibly integrated depth information.\nContext-related reasoning: Context-related visual comprehension requires MLLMs to effectively\nleverage contextual knowledge in solving visual problems. To assess the ability of MLLMs to answer\ncontext-dependent questions, CODIS [53] was created, requiring MLLMs to use context from free-\nform text to enhance visual comprehension. Recognizing that existing MLLMs often trusted what\nthey saw but struggled to understand presuppositions in sentences, Li et al.[54] introduced CFMM, a\ncounterfactual reasoning benchmark designed to assess MLLMs' ability to make presuppositions\nbased on established facts. Zong et al. [55] developed VL-ICL Bench, a benchmark suite designed to\nspecifically evaluate VLLMs' in-context learning, which involved utilizing contextual information to\ncomplete new tasks.\nVision-Indispensable reasoning: Recognizing that MLLMs may rely on language priors rather\nthan visual information when answering questions, some works have aimed to compel MLLMs to\nprioritize visual data. Goyal et al.[57] introduced VQAv2, which consisted of pairs of similar images\nthat led to different answers. However, this approach did not effectively handle open-ended questions.\nIn response, CLEVR[56] was designed with open-ended question answering. It also ensured that\nexternal information sources, such as commonsense knowledge, did not influence answer accuracy.\nNonetheless, CLEVR's reliance on synthetic images overlooked the realism and diversity found in\nnatural photographs. To address these limitations, GQA[58] was developed, offering well-defined\nsemantic representations along with the rich semantic and visual complexity of real-world images.\nAdditionally, Chen et al.[59] introduced MMStar, a vision-indispensable benchmark that covered a\nwide range of tasks and difficulty levels."}, {"title": "4.2 Knowledge-based Reasoning", "content": "Evaluating MLLMs' ability to utilize knowledge is crucial for ensuring their effectiveness in complex\ntasks and enhancing their real-world performance. These benchmarks mainly focus on two key\naspects. One aspect is knowledge-based question answering, which tests MLLMs' ability to handle\nquestions that require structured or extensive external knowledge. The other aspect is knowledge\nediting, which assesses MLLMs' accuracy and consistency in updating and maintaining knowledge\ncontent."}, {"title": "4.3 Intelligence&Cognition:", "content": "Inspired by the development of human intelligence, some benchmarks leverage cognitive and ed-\nucational theories to assess the intelligence of MLLMs. For instance, intelligence tests featuring\nabstraction visual reasoning and various levels of mathematical problems are used to evaluate MLLMs'\nlogical reasoning capabilities. Additionally, multidisciplinary questions from various educational\nperiods are employed to assess MLLMs' ability to integrate diverse knowledge and apply complex\nreasoning skills to solve intricate problems. These approaches are crucial for understanding and\nenhancing the cognitive and problem-solving capabilities of MLLMs.\nIntelligent Question Answering: Intelligent question answering aims to explore the intelligence\nof MLLMs through cognitive science perspectives. One key aspect is abstract visual reasoning\n(AVR)-the ability to discern relationships among patterns in images and predict subsequent patterns.\nRAVEN[69] tested abstract visual reasoning primarily with mathematical patterns over predefined\ngeometric shapes, but its evaluation scope was limited as it addressed only a single task type.\nTherefore, MARVEL[70] and VCog-Bench[71] were introduced to evaluate MLLMs across multi-\ndimensional AVR tasks, but remained confined to AVR, neglecting other dimensions of cognition. To\nintegrate cognitive science principles for a comprehensive understanding of MLLMs' intelligence,\nSong[72] identified five key cognitive factors based on the well-recognized Cattell-Horn-Carroll\n(CHC) model and introduced M3GIA, the first comprehensive cognitive-driven benchmark designed\nto evaluate the general intelligence of MLLMs.\nMathematical Question Answering: Using mathematics problems to evaluate MLLMs is essential\nfor assessing their logical reasoning capabilities, as these problems require complex reasoning,\npattern recognition, and abstract thinking. Such tasks help determine if MLLMs can apply rules,\ndiscover patterns, and perform sophisticated reasoning. Geometry3K[78] was introduced to evaluate\ncapabilities in solving geometry problems, but it had a narrow focus on specific aspects of plane\ngeometry. To address this issue, Lu et al.[73] collected multiple datasets to construct an integrated\nbenchmark, MathVista, which covered a range of mathematical tasks, such as functions and solid\ngeometry. However, MathVista lacked a detailed classification of mathematical subdomains and\nemphasized visual abilities more than pure mathematical reasoning. Consequently, Math-V[76] and\nMathVerse[74] were developed, confining data to specific mathematical subjects and focusing on\nmathematical reasoning abilities. Additionally, Fan et al.[75] proposed NPHardEval4V, a benchmark\nthat used algorithmic problems and converted their textual descriptions into visual representations,\naimed at evaluating the pure reasoning capabilities of MLLMs. Motivated by the idea that \"if a model\ntruly understands a problem, it should perform robustly across various tasks related to that problem,\"\nZhou et al.[77] introduced MATHCHECK-GEO, a benchmark focused on the universality of tasks"}, {"title": "5 Specific Domains", "content": "This section focuses on MLLMs' capabilities in specific tasks and applications, such as their ability\nto integrate complex visual and textual information, adapt to decision-making roles in dynamic\nenvironments, and effectively process diverse cultural and linguistic data. It then extends to discuss\nthe practical applications of MLLMs, highlighting their impact on various sectors such as medicine,\nindustry, and autonomous driving. By providing an overview of these benchmarks, this section aims\nto underscore the advancements in evaluating MLLMs' performance and their potential to address\nreal-world challenges across different domains."}, {"title": "5.1 Text-rich VQA", "content": "Evaluating MLLMs in text-rich visual question answering is crucial for understanding how well\nmodels interpret and integrate textual and visual information within images. This evaluation covers\nseveral aspects, including the accuracy of text recognition, contextual understanding, and the ability\nto synthesize information from both modalities. In addition to text comprehension, it also requires\nan understanding of layout and structure to effectively analyze multimodal documents, charts, and\nHTML.\nText-oriented Question Answering: Some works evaluated MLLMs' effectiveness in text-related\nvisual tasks, such as text recognition and scene text-centric visual question answering. Singh et al. [87]\nintroduced TextVQA, which contained questions requiring the model to read and reason about the text\nin the image to provide answers. However, the short answers provided in TextVQA were insufficient\nfor a comprehensive description of the image. To address this issue, TextCaps[88] extended the\nlength of sentences in the answers and involved many switches between OCR and vocabulary tokens.\nDespite these efforts, existing benchmarks could be time-consuming, and inaccurate annotations in\nsome datasets made accuracy-based evaluation less precise. In response, Liu et al.[86] developed\nOCRBench to facilitate the accurate and convenient evaluation of MLLMs' OCR capabilities. To\nquantitatively assess visual reasoning capabilities in text-rich and high-resolution scenarios, Chen et\nal.[25] constructed a challenging benchmark, P2GB. This benchmark included comprehensive image\nunderstanding, fine-grained recognition, and image text content understanding. In order to cover a\nbroader spectrum of text-rich scenarios, SEED-Bench-2-Plus[89] was developed to evaluate MLLMs'\nperformance in comprehending text-rich visual data across a wide range of real-world scenarios.\nDocument-oriented Question Answering: Document-oriented question answering requires MLLMs\nnot only to read text but also to interpret it within the layout and structure of the document. Mathew et\nal.[92] introduced InfographicVQA, which included questions requiring the combination of multiple\ncues. Although InfographicVQA showcased significant diversity in topics and designs, it still\npreferred using visual aids over long text passages. To address this limitation, Single Page DocVQA\n(SPDocVQA)[90] was introduced as a more diverse benchmark, featuring documents of various\ntypes and origins created over several decades. However, it was built exclusively on single-page"}, {"title": "5.2 Decision-making Agents", "content": "Decision-making agents expect MLLMs to possess human-level planning and scheduling abilities,\nwhich are fundamental for making informed decisions and taking appropriate actions in complex\nenvironments. This capability holds significant potential for addressing real-world problems.\nEmbodied Decision-making: Embodied Decision-making requires MLLMs to be able to integrate\nsensory inputs and interact with the environment in a way that mimics human physical experi-\nences. OpenEQA[108] was the first benchmark for embodied question answering, supporting both\nepisodic memory and active exploration use cases. However, it focused solely on the answers\nprovided by MLLMs and did not consider the intermediate reasoning processes. Chen et al.[107]\nargued that it was essential to enable multi-dimensional evaluation of the decision-making process,\nencompassing perception, reasoning, and action perspectives, rather than relying solely on final\nrewards or success rates. They proposed PCA-EVAL for evaluating the embodied decision-making\nability of MLLMs from different perspectives, including three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. However, these benchmarks were limited by\nthe small number of handcrafted questions and reliance on single-image visual observation. To\nstudy the embodied planning and decision-making capabilities of MLLMs more systematically, Chen\net al.[106] used large-scale egocentric videos reflecting daily human activities from a first-person\nperspective to construct EgoPlan-Bench. EgoPlan-Bench aimed to assess MLLMs' human-level\nplanning capabilities in real-world scenarios, featuring realistic tasks, diverse actions, and complex\nvisual observations. Despite that, these benchmarks failed to sufficiently challenge or showcase the\nfull potential of MLLMs in complex environments. To address this gap, Liu et al.[105] introduced\nVisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train"}, {"title": "5.3 Diverse Cultures and Languages", "content": "Most benchmarks primarily use English, leading to the neglect of other languages and cultures. To\naddress this limitation, some benchmarks have been introduced to supplement data in a broader\nrange of languages. CMMU[83], CMMMU[80], and MULTI[85] were presented for multi-modal\nand multi-type questions in Chinese, featuring a wider variety of question types. Besides, the\nHenna benchmark[112] was proposed to test MLLMs in Arabic culture, while the LaVy-Bench\nbenchmark[113] was designed for evaluating MLLMs' understanding of Vietnamese visual language\ntasks. To further enrich language diversity, MTVQA[114] was proposed as the first benchmark\nfeaturing high-quality human expert annotations across 9 diverse languages. However, although\nMTVQA extended its linguistic range, it kept images the same, resulting in a narrow cultural\nrepresentation. To address this limitation, Romero[115] constructed CVQA, a culturally diverse\nmultilingual visual question answering benchmark designed to cover a rich set of languages and\ncultures."}, {"title": "5.4 Other Applications", "content": "Some works focused on assessing MLLMs' abilities to handle highly professional and domain-\nspecific data, such as medicine, transportation, engineering, remote sensing, and autonomous driving.\nThese evaluations provided insights into how well MLLMs could adapt to and process specialized\ninformation in various complex fields, highlighting their potential for applications in areas requiring\ndeep expertise and precise knowledge.\nGeography and Remote Sensing:MLLMs can help enhance geographic information extraction\nand environmental monitoring by analyzing multimodal data in the fields of geography and remote\nsensing. Roberts et al.[117] constructed a benchmark to probe the key geographic and geospatial\nknowledge of a suite MLLMs. However, the diverse geographical landscapes and varied objects\nin remote sensing imagery were not adequately considered. To bridge this gap, Muhtar et al.[116]\nintroduced LHRS-Bench, a benchmark for thoroughly evaluating MLLMs' abilities in remote sensing\nimage understanding.\nMedicine:Medical-based benchmarks primarily assess MLLMs' ability to integrate medical knowl-\nedge with visual modalities to perform accurate medical diagnoses and recommendations. The\nAsclepius benchmark[120] covered a range of medical specialties, aiming to comprehensively evalu-\nate MLLMs' capabilities across various medical fields. However, it primarily focused on 2D medical\nimages, leaving 3D images less explored. To address this, Bai et al. [119] introduced M3D-Bench, a\n3D multimodal medical benchmark that enabled automatic evaluation across eight tasks. Further-\nmore, Chen et al.[118] developed GMAI-MMBench, the most comprehensive general medical AI\nbenchmark to date, featuring a well-categorized data structure and multi-perceptual granularity.\nIndustry: Some benchmarks have been designed to assess MLLMs' capabilities in industrial design\nand manufacturing applications. Doris et al. [44] proposed a benchmark called DesignQA to explore\nMLLMs' understanding of design based on engineering requirement documents. DesignQA integrated\ninformation from both visual and long-text inputs, emphasizing the complexity and multimodal nature\nof real-world engineering tasks. To evaluate MLLMs' abilities in robotic applications, the MMRO"}, {"title": "6 Key Capabilities", "content": "These benchmarks evaluated dialogue capabilities, including handling extended dialogues and accu-\nrately following instructions, as well as assessing the model's level of hallucination and trustworthi-\nness. Such capabilities are crucial for ensuring that MLLMs perform effectively across a range of\nreal-world applications and can adapt to various practical scenarios."}, {"title": "6.1 Conversation Abilities:", "content": "Some benchmarks focus on evaluating MLLMs' performance in conversations, specifically assessing\nhow well these models handle long contexts and follow complex instructions accurately. Such\nevaluations are crucial for ensuring that MLLMs can engage effectively in diverse dialogues and\ndeliver reliable performance in real-world applications.\nLong-context Capabilities: Due to the window length limitations inherent in MLLMs' architectures,\nevaluating their ability to handle long contexts is challenging. This involves assessing whether\nMLLMs can maintain accurate recall and effective understanding as the amount of contextual\ninformation increases. MileBench [35] and MMNeedle [127] explored the long-context recall\nabilities of MLLMs using needle-in-a-haystack (NIAH) method and image retrieval tasks. However,\nevaluating the long-context understanding of MLLMs in videos remained a significant challenge.\nTherefore, Zhou et al. [128] proposed MLVU, which was developed using long videos of diversified\nlengths.\nInstruction Adherence: Instruction adherence requires that MLLMs execute complicated instruc-\ntions. This involved not only recognizing the content of the instructions but also meticulously\nexecuting the detailed demands without deviation. Wang et al.[131] built a benchmark called Demon\nfor demonstrative instruction understanding. However, it focused only on demonstrative instruction\nfollowing and ignored other flexible instruction scenarios. To explore how MLLMs performed\non broader, open-ended prompts, Bitton et al.[132] created VisIT-Bench to cover a wide array of\n'instruction families' that resembled real-world user behavior. Although these benchmarks evaluated\nthe basic instruction-following capabilities of MLLMs, the ability of MLLMs to adapt to new in-\nstructions while incorporating both old and new ones remained unclear. Therefore, Chen et al.[129]\npresented a benchmark named Continual Instruction Tuning (CoIN) to assess MLLMs in a sequential\ninstruction tuning paradigm. To better measure MLLM adherence to instructions, MIA-Bench[130]\nwas introduced to test how well MLLMs follow layered instructions and generate accurate responses\nmatching specific patterns."}, {"title": "6.2 Hallucination", "content": "Hallucination refers to information in LVLMs' responses that does not accurately reflect the visual\ninput, which poses potential risks of substantial consequences.\nTo measure object hallucination, Rohrbach et al.[141] proposed CHAIR (Caption Hallucination\nAssessment with Image Relevance), which assessed captioned objects that were actually present\nin an image. However, CHAIR was unstable and required complex human-crafted parsing rules\nfor exact matching. Alternatively, POPE[133] converted hallucination into a binary classification\nproblem, but it required the input questions to follow specific templates, such as 'Is there a/an <object>\nin the image?'. In comparison, GAVIE[134] can evaluate model hallucination in an open-ended\nmanner without requiring ground-truth answers or pre-designed instruction formats. However, it\nstill focused on evaluating object hallucinations, neglecting other types of hallucinations in MLLMs.\nM-HalDetect[136] and MMHAL-BENCH [144] extended the scope of previous works by not only\nconsidering hallucinations related to objects but also addressing other categories such as object\nattributes and spatial relations. Moreover, Chen et al. [142] presented MHaluBench, a meta-evaluation\nbenchmark that encompassed various hallucination categories and multimodal tasks. Additionally,\nZhang et al.[212] introduced MRHalBench to evaluate hallucinations in multi-round dialogues. To\nassess MLLMs' hallucination in video understanding, Wang et al.[143] introduced VideoHallucer,\nthe first comprehensive benchmark for hallucination detection in videos.\nSome benchmarks aimed to explore more cost-effective and feasible methods for evaluating hal-\nlucinations. Wang et al. [135] argued that hallucinations measured using object-based evaluations\nlike POPE merely exploited the judgment bias present in MLLMs, rather than reflecting their actual\nhallucinations. They proposed Hallucination Evaluation based on Large Language Models (HaELM),\nthe first to utilize LLMs for hallucination evaluation within MLLMs. Considering that reliance\non LLMs resulted in significant costs, Wang et al.[145] proposed an LLM-free multi-dimensional\nbenchmark, AMBER. AMBER provided comprehensive coverage of evaluations for various types of\nhallucinations and offered detailed annotations to support an LLM-free evaluation pipeline. Despite\nthat, many of the visual hallucination (VH) instances in these benchmarks came from existing datasets,\nwhich resulted in a biased understanding of MLLMs' performance due to the limited diversity of\nsuch VH instances. Therefore, Huang et al.[139] proposed generating diverse VH instances using a\ntext-to-image generative model. Based on this method, they collected a benchmark dataset called\nVHTest. Additionally, to eliminate the need for costly data annotation and minimize the risk of\ntraining data contamination, Cao et al. [146] proposed an annotation-free evaluation method that\nrequired only unimodal data to measure inter-modality semantic coherence and inversely assessed\nMLLMs' tendency to hallucinate.\nThere are also some works that explored the causes and mechanisms of hallucination. Observing that\nMLLMs' strong language bias often overshadowed visual information, leading to an overreliance on\nlanguage priors rather than visual context, HallusionBench[138] was proposed to focus on diagnosing\nboth the visual illusion and knowledge hallucination of MLLMs. Cui et al. [137] constructed a\nbenchmark called Bingo, which systematically categorized and analyzed the reasons behind the\noccurrence of hallucinations. Moreover, Han et al.[140] identified a typical class of inputs that baffled\nMLLMs: images that were highly relevant but inconsistent with answers, causing MLLMs to suffer\nfrom hallucination. To quantify this effect, they proposed CorrelationQA, the first benchmark that\nassessed the hallucination level given spurious images."}, {"title": "6.3 Trustworthiness", "content": "Evaluating the trustworthiness of multimodal large language models (MLLMs) encompasses various\naspects, including accuracy, consistency across different scenarios, and safety in handling sensitive\ncontent. This section focuses on benchmarks that assess MLLMs specifically in terms of robust-\nness and safety. Robustness examines how well the model performs with diverse or unexpected\ninputs, ensuring reliable outputs across various conditions. Safety evaluates the model's capacity\nto avoid generating harmful or inappropriate content, thereby protecting users"}]}