{"title": "A Survey on Benchmarks of Multimodal Large Language Models", "authors": ["Jian Li", "Weiheng Lu"], "abstract": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity\nin both academia and industry due to their remarkable performance in various ap-\nplications such as visual question answering, visual perception, understanding, and\nreasoning. Over the past few years, significant efforts have been made to examine\nMLLMs from multiple perspectives. This paper presents a comprehensive review\nof 180 benchmarks and evaluation for MLLMs, focusing on (1)perception and\nunderstanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities,\nand (5)other modalities. Finally, we discuss the limitations of the current evaluation\nmethods for MLLMs and explore promising future directions. Our key argument\nis that evaluation should be regarded as a crucial discipline to better support the\ndevelopment of MLLMs. For more details, please visit our GitHub repository:\nhttps://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLM) have recently garnered substantial interest across academic and\nindustrial domains. The impressive performance of LLMs such as GPT [1] has fueled optimism that\nthey could represent a step towards Artificial General Intelligence (AGI) in this era. The remarkable\nabilities of LLM have inspired efforts to integrate them with other modality-based models to enhance\nmultimodal competencies. Consequently, Multimodal Large Language Models (MLLMs) [2] have\nemerged, This concept is further supported by the extraordinary success of proprietary models like\nOpenAI's GPT-4V [3] and Google's Gemini[4]. In contrast to earlier models that were limited\nto solving specific tasks, Multimodal Large Language Models (MLLMs) demonstrate exceptional\nperformance across a variety of applications, including both general Visual Question Answering\n(VQA) tasks and domain-specific challenges.\nA comprehensive and objective benchmark for evaluating MLLMs is essential for comparing and\ninvestigating the performance of various models, and it plays a crucial role in the success of MLLMs.\nFirst, evaluating LLMs helps us better understand the strengths and weaknesses of MLLMs. For\nexample, the SEED-Bench [5] illustrates that current MLLMs show weaker abilities in understanding\nspatial relationships between objects while achieving relatively high performance on global image\ncomprehension. Second, evaluations across various scenarios can offer valuable guidance for MLLM\napplications in fields such as medicine, industry, and autonomous driving. This, in turn, can inspire\nfuture designs and expand the scope of their capabilities. Third, the broad applicability of MLLMs\nunderscores the importance of ensuring their robustness, safety, and reliability, especially in safety-\nsensitive sectors. Finally, it is significant to evaluate other user-friendly features of MLLMs including\nthe ability to handle long contexts and accurately follow instructions. Therefore, we aim to raise\nawareness in the community of the importance of MLLM evaluations by reviewing the current\nevaluation protocols."}, {"title": "2 Preliminaries", "content": "Figure. 1 compares several common MLLMs including GPT4[3], Gemini[4], LLaVA[185], Qwen-\nVL[186], Claude[187], InstructBLIP[188], mPLUG-Owl2[189], SPHINX[190], Intern-VL[191],\nYi-VL[192], VideoChat2[193], Video-LLaMA[194], Cambrian-1[195], PLLaVA[196], Blip2[197],\nMiniGPT4-Video[198]. The standard MLLM framework can be divided into three main modules: a\nvisual encoder g tasked with receiving and processing visual inputs, a pre-trained language model that\nmanages the received multimodal signals and performs reasoning, and a visual-language projector P\nwhich functions as a bridge to align the two modalities. A diagram of the architecture and training\nprocess is illustrated in Figure. 3. This figure outlines the base LLM, the vision encoder, the projector,\nas well as the pertaining and instruction tuning."}, {"title": "2.1 MLLM Architecture", "content": "Vision Encoder Taking the input image X as input, the vision encoder compresses the original\nimage into more compact patch features Z, as represented by the following formula:\n\u0396\u03c5 = g(\u03a7\u03c5).\n(1)\nThe encoder g can also be an audio encoder for audio feature extraction or an encoder for other\nmodalities. The common vision encoders are CLIP [199], SigLIP [200] and DINO [201, 202].\nVision-Language Projector The task of the vision-language projector is to map the visual patch\nembeddings Z into the text feature space:\nH\u2082 = P(Z),  (2)\nwhere H denotes the projected visual embeddings. The aligned visual features are used as prompts\nand inputted into the language model along with the text embeddings. Several works, such as Qformer\nin BLIP-2 [197], design new projectors to reduce the number of visual tokens for efficiency.\nLarge Language Model The pre-trained Large language model serves as the core component of\nMLLMS, endowing it with many outstanding capabilities, such as zero-shot generalization, instruction\nfollowing, and in-context learning. The LLM accepts input sequences containing multiple modalities\nand outputs corresponding text sequences. A text tokenizer is typically bundled with the LLM,\nmapping text prompts X\u2081 to the text tokens Hq. The text tokens Hq and the visual tokens H\u2082 are\nconcatenated as the input of the language model, which outputs the final response sequence Ya in an\nautoregressive manner:\np(Ya|Hv, Hq) = \\prod_{i=1}^{L} P(Yi|Hv, Hq, Y<i), (3)\nwhere L denotes the length of Ya. The parameter sizes of large language models (LLMs) range from\n3 billion to tens of billions. Commonly used open-source LLMs include the Llama series[203, 204,\n205, 206], Phi [207, 208], Gemma [209],Qwen [210]."}, {"title": "2.2 MLLM Training", "content": "The standard training process of MLLMs is a crucial factor that determines their performance on\ndownstream tasks and their ability to handle diverse tasks. In this section, we provide an overview of\nvarious training methodologies, including pre-training, and instruction-tuning."}, {"title": "Pre-training", "content": "The pre-training(PT) stage focuses on aligning different modalities within the em-\nbedding space, enabling the language model to accept inputs from various modalities. This phase\nprimarily involves large-scale text-paired data, predominantly in the form of image-caption pairs.\nAn image-caption pair (X, Y) is typically expanded into a single-turn conversation (Xinstruct, Xa),\nwhere Xinstruct contains an image X and a randomly sampled question Xq from a set of instruc-\ntions asking the assistant to briefly describe the image, and Xa is the original image description.\nGiven such a conversation, the model is trained to autoregressively predict the image description.\nConsequently, following the \"next token prediction\" paradigm, we can compute the probability of\npredicting Xa conditioned by X and optimize it using a standard cross-entropy loss function:\nmax_{\u03b8} log p\u03b8(xi|Xu, Xinstruct, Xa,<i),  (4)\nwhere L is the length of Xa and \u03b8 denotes the trainable parameters. In order to better align different\nmodalities of knowledge and avoid catastrophic forgetting during the PT stage, \u03b8 typically includes\nonly a learnable modality interface, i.e., a vision-language projector."}, {"title": "Instruction-tuning", "content": "Instruction-tuning(IT) aims to fine-tune the models on specific tasks by lever-\naging task-specific instructions. IT is typically conducted within the paradigm of Supervised Fine-\nTuning (SFT). The IT datasets are transformed into an instruction-based format, presented in the form\nof single-turn or multi-turn dialogue structures. Given an image X and its caption, a conversation\ndata (X, X,..., X, XT) can be generated, where T is the total number of turns. Typically, we\ncan organize the data into a sequence of instructions and responses, as outlined in [185]. With this\nmultimodal instruction-following sequence, IT can be performed using the same auto-regressive\ntraining objective as in the PT stage. A common strategy involves maintaining the visual encoder\nweights fixed while continuing to update the PT weights of both the projector and the LLM during\nthe IT process."}, {"title": "3 Perception and Understanding", "content": "When evaluating the perception and understanding capabilities of MLLMs, we focus on benchmarks\nthat assess the model's fundamental abilities in visual information processing. This includes evalu-\nating the MLLMs' accuracy in object identification and detection, understanding of scene context\nand object relationships, and ability to respond to questions about image content. Perception and\nunderstanding abilities are the cornerstone of MLLMs, enabling them to perform a wide range of tasks\nand applications. This section first introduces comprehensive evaluation benchmarks for MLLMs,\nand then separately discusses coarse-grained and fine-grained benchmarks for visual perception."}, {"title": "3.1 Comprehensive Evaluation", "content": "MLLMs rely on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities\nin various studies. In order to fully match the flourish of MLLMs, Many comprehensive evaluation\nbenchmarks are proposed."}, {"title": "3.2 Fine-grained Perception", "content": "One indispensable cornerstone of MLLMs is the ability to perceive visible objects within scenes\nprecisely. This includes evaluating MLLMs' capabilities in object detection and recognition, un-\nderstanding details within local regions, and achieving accurate vision-language alignment. Such\nfine-grained perception is crucial for effective multimodal understanding and interaction."}, {"title": "Visual Grounding and Object Detection:", "content": "Object-level grounding and detection are critical steps\nin better perceiving images and solving image-related questions, providing a stronger link between\nQA pairs and images. Flickr30k Entities [19] and Visual7W [20] were early benchmarks for entity\nlocalization, focusing on the detailed grounding of specific phrases in image regions and QA tasks\nrelated to local areas. These benchmarks served as the foundation for subsequent developments in\nthe field. However, they did not provide enough detailed contextual information about objects within\nthe scene. Zang et al. [18] investigated contextual object detection, which involved understanding\nvisible objects within human-AI interactive contexts, and presented the CODE benchmark to facilitate\nresearch in this area. To evaluate MLLMs' ability in challenging scenarios where images contained\nabundant and complex information, Wu et al. [21] introduced V*Bench, a benchmark focused on\ndetailed visual grounding in high-resolution images."}, {"title": "Fine-grained Identification and Recognition:", "content": "Fine-grained identification and recognition requires\nMLLMs to identify and analyze detailed visual features. In order to determine what makes for a good\nvisual tokenizer, GVT-bench[22] was designed to evaluate MLLMs' fine-grained visual perception\nthrough object counting and multi-class identification. However, the data used in GVT-bench were\nconstrained to relatively small resolutions. To address this limitation, Li et al. [27] extended the input\nresolution to a much larger range and introduced MagnifierBench, which tested MLLMs' ability to\ndiscern the details of small objects in high-resolution images. Considering that most open-source\nMLLMS adopted the Pretrained Contrastive Language-Image PreTraining (CLIP) model[199] as the\nvisual encoder, Tong et al. [23] proposed the MMVP benchmark consisting of CLIP-blind pairs.\nThey found that all tested models struggled with simple questions about visual details. Given that\nexisting benchmarks were insufficient in size and did not cover crucial visual elements such as depth\nand spatial awareness, CV-Bench [24] was created to evaluate the fundamental 2D and 3D visual\nunderstanding of MLLMs, using a significantly larger number of examples. Additionally, P2GB [25]\nwas designed to assess fine-grained visual capabilities, especially in text-rich scenarios, requiring"}, {"title": "Nuanced Vision-language Alignment:", "content": "Nuanced vision-language alignment involves interpreting\ncomplex interactions between visual and textual information, grasping subtle meanings, and aligning\nsemantics between images and text. Winoground [32] was designed to require models to match\ntwo images with two captions that contained the same set of words in different orders. However,\nthe scale of Winoground was restricted by its costly curation, and it lacked a focus on linguistic\nphenomena. To address these limitations, VALSE [30] and VLChecklist [31] examined how MLLMs\nunderstood visual-linguistic concepts by converting real captions into confusing alternatives. By\nmodifying textual representations related to relationships, attributes, and order, ARO [33] assessed\nwhether MLLMs could achieve fine-grained visual-language alignment on key concepts. Moreover,\nEqben[28] assessed whether MLLMs were sensitive to visual semantic changes by making minimal\nsemantic changes in images, but the image diversity was limited by virtual engines. To produce\ndiverse images that met the requirements, Peng et al.[29] developed a benchmark called SPEC,\nutilizing a progressive pipeline to synthesize images that varied in a specific attribute while ensuring\nconsistency in other aspects."}, {"title": "3.3 Image Understanding", "content": "The image understanding task involves analyzing visual content to extract meaningful information,\nwhich includes grasping the context of scenes and integrating visual details with textual information\nto generate coherent descriptions and insights."}, {"title": "Multi-image Understanding:", "content": "Multi-image understanding requires MLLMs to compare, analyze,\nand comprehend the relationships or variations among multiple images, thereby enabling more\ncomprehensive insights into visual content. Mementos[34] was designed to assess MLLMs' sequential\nimage understanding abilities, but it primarily focused on state changes among images, while\nneglecting other aspects. To address this issue, MileBench [35] and MuirBench [36] were constructed\nto assess multi-image cognition through a variety of task types. However, these benchmarks fell short\nin terms of task depth and breadth. Furthermore, MMIU [38] provided a comprehensive evaluation\nby including a large number of test samples that spanned a wide range of multi-image tasks and\nrelationships. Besides, Kil et al.[37] focused on tasks involving relativity and comparison between\nmultiple visual input, then introduced COMPBENCH to evaluate the comparative understanding\ncapabilities of MLLMs. COMPBENCH required MLLMs to answer questions based on a pair of\nvisually or semantically related images."}, {"title": "Implication Understanding:", "content": "Understanding the meaning of images requires not only intuitive\nobservation but also an exploration of the human emotions and cultural contexts they reflect. II-\nBench[39] and ImplicitAVE[40] aimed to assess MLLMs' higher-order perceptual abilities with\nvisual implications. To evaluate the emotional perception capabilities of MLLMs, FABA-Bench [41]\nwas designed for facial affective behavior analysis. FABA-Bench required MLLM to recognize facial\nexpressions and movements, which are critical to understanding an individual's emotional states and\nintentions."}, {"title": "Image Quality and Aesthetics Perception:", "content": "Image quality and aesthetics perception involves assessing\nimage quality, perceiving visual distortions, and understanding low-level attributes such as color,\nlighting, composition, and style. It also relates to the aesthetics and design sense of photographs.\nQ-Bench[45] explored the potential of MLLMs in low-level perception abilities. To highlight subtle\ndifferences or similarities that might not be evident when images were viewed in isolation, Q-\nBench+[46] extended the evaluation of low-level perception from single images to image pairs. To\nbetter align with human aesthetics, comprehensive aesthetic evaluation benchmarks AesBench[42]\nand UNIAA[43] were constructed to systematically evaluate the aesthetic abilities of MLLMs.\nBesides, Lin et al.[211] proposed DesignProbe to comprehensively assess design capabilities of\nMLLMs from both the element level and the overall design level."}, {"title": "4 Cognition and Reasoning", "content": "MLLMs' cognitive and reasoning abilities encompass the model's capacity for advanced processing\nand complex inference beyond basic perception and understanding. Cognitive abilities involve\nintegrating and manipulating extracted information to form coherent representations, while reasoning\nabilities focus on drawing logical conclusions and solving problems. Strong cognitive and reasoning\nabilities enable MLLMs to perform effective logical inference in complex tasks."}, {"title": "4.1 General Reasoning", "content": "The reasoning ability of MLLMs involves extracting and inferring relevant information from visual\nand textual inputs to draw logical conclusions and answer questions. This section introduces bench-\nmarks for evaluating MLLMs' general reasoning capabilities, focusing on three key areas: visual\nrelation reasoning, vision-indispensable reasoning, and context-related reasoning."}, {"title": "Visual Relation:", "content": "Evaluating MLLMs' reasoning of visual spatial relationships involves assessing their\nabilities to understand the spatial arrangement, relative positions, and interactions of objects. VSR[51]\nand What's Up[48] were developed to test MLLMs' ability to reason about spatial relations in natural\nimage-text pairs. However, these benchmarks primarily focused on classification tasks rather than\ncomprehending relations within scenes. To assess more complex and general relational reasoning of\nMLLMs, Wang et al.[50] introduced the Circular-based Relation Probing Evaluation (CRPE), the first\nbenchmark to encompass all elements of relation triplets (subject, predicate, object). Recognizing that\ninteractions and associations between distinct objects remained a significant challenge for MLLMs,\nNie et al.[47] developed Multi-Modal Relation Understanding (MMRel), a large-scale benchmark\nconsisting of data with well-defined inter-object relations. Additionally, GSR-BENCH[49] extended\nthe What's Up benchmark by incorporating bounding box annotations and depth information to\nbetter evaluate grounded spatial relation understanding. For the evaluation of 3D spatial cognition in\nMLLMs, Chen et al.[52] proposed SpatialRGBT-Bench, a benchmark that incorporated ground-truth\n3D annotations and flexibly integrated depth information."}, {"title": "Context-related reasoning:", "content": "Context-related visual comprehension requires MLLMs to effectively\nleverage contextual knowledge in solving visual problems. To assess the ability of MLLMs to answer\ncontext-dependent questions, CODIS [53] was created, requiring MLLMs to use context from free-\nform text to enhance visual comprehension. Recognizing that existing MLLMs often trusted what\nthey saw but struggled to understand presuppositions in sentences, Li et al.[54] introduced CFMM, a\ncounterfactual reasoning benchmark designed to assess MLLMs' ability to make presuppositions\nbased on established facts. Zong et al. [55] developed VL-ICL Bench, a benchmark suite designed to\nspecifically evaluate VLLMs' in-context learning, which involved utilizing contextual information to\ncomplete new tasks."}, {"title": "Vision-Indispensable reasoning:", "content": "Recognizing that MLLMs may rely on language priors rather\nthan visual information when answering questions, some works have aimed to compel MLLMs to\nprioritize visual data. Goyal et al.[57] introduced VQAv2, which consisted of pairs of similar images\nthat led to different answers. However, this approach did not effectively handle open-ended questions.\nIn response, CLEVR[56] was designed with open-ended question answering. It also ensured that\nexternal information sources, such as commonsense knowledge, did not influence answer accuracy.\nNonetheless, CLEVR's reliance on synthetic images overlooked the realism and diversity found in\nnatural photographs. To address these limitations, GQA[58] was developed, offering well-defined\nsemantic representations along with the rich semantic and visual complexity of real-world images.\nAdditionally, Chen et al.[59] introduced MMStar, a vision-indispensable benchmark that covered a\nwide range of tasks and difficulty levels."}, {"title": "4.2 Knowledge-based Reasoning", "content": "Evaluating MLLMs' ability to utilize knowledge is crucial for ensuring their effectiveness in complex\ntasks and enhancing their real-world performance. These benchmarks mainly focus on two key\naspects. One aspect is knowledge-based question answering, which tests MLLMs' ability to handle\nquestions that require structured or extensive external knowledge. The other aspect is knowledge\nediting, which assesses MLLMs' accuracy and consistency in updating and maintaining knowledge\ncontent."}, {"title": "Knowledge-based Visual Question Answering:", "content": "Knowledge-based visual question answering\nrequires various types of knowledge, including explicit fact-based information from knowledge\nbases, commonsense knowledge about human behavior, and general external knowledge. The earliest\nbenchmarks in this area were KB-VQA[60] and FVQA[61], but they were limited by their use of\n\"closed\" knowledge sources. OK-VQA[62] advanced these benchmarks by providing a larger scale\nand higher quality of questions and images, utilizing \"open domain\" knowledge rather than a fixed\nsource. However, OK-VQA still relied on simple lookup knowledge and required minimal reasoning.\nTo overcome this limitation, A-OKVQA[63] was introduced, incorporating more commonsense\nknowledge and demanding greater reasoning. Furthermore, Wang et al.[64] developed SOK-Bench\nto evaluate MLLMs' situated and open-world commonsense reasoning in videos."}, {"title": "Knowledge Editing:", "content": "Knowledge editing refers to the ability to update outdated, unknown, or\nincorrect information within MLLMs. The benchmark MMEdit, proposed by Cheng et al.[65],\nprovided a platform for testing the editability of MLLMs. However, it primarily focused on coarse-\ngrained knowledge, which often failed to accurately represent fine-grained entities and scenarios\nin the real world. To address this limitation, Cheng[65] introduced MIKE, a comprehensive and\nchallenging benchmark for fine-grained multimodal entity knowledge editing. Meanwhile, VLKEB\n[67] expanded the evaluation of knowledge editing portability, demonstrating MLLMs' ability to\neffectively apply edited knowledge in related contexts. Despite that, these benchmarks overlooked\nthe organization of multimodal knowledge and lacked a precise definition of multimodal knowledge\nediting. To fill this gap, MC-MKE[68] was developed as a benchmark to evaluate the reliability,\nlocality, generality, and consistency of MLLMs across different editing formats."}, {"title": "4.3 Intelligence&Cognition:", "content": "Inspired by the development of human intelligence, some benchmarks leverage cognitive and ed-\nucational theories to assess the intelligence of MLLMs. For instance, intelligence tests featuring\nabstraction visual reasoning and various levels of mathematical problems are used to evaluate MLLMs'\nlogical reasoning capabilities. Additionally, multidisciplinary questions from various educational\nperiods are employed to assess MLLMs' ability to integrate diverse knowledge and apply complex\nreasoning skills to solve intricate problems. These approaches are crucial for understanding and\nenhancing the cognitive and problem-solving capabilities of MLLMs."}, {"title": "Intelligent Question Answering:", "content": "Intelligent question answering aims to explore the intelligence\nof MLLMs through cognitive science perspectives. One key aspect is abstract visual reasoning\n(AVR)-the ability to discern relationships among patterns in images and predict subsequent patterns.\nRAVEN[69] tested abstract visual reasoning primarily with mathematical patterns over predefined\ngeometric shapes, but its evaluation scope was limited as it addressed only a single task type.\nTherefore, MARVEL[70] and VCog-Bench[71] were introduced to evaluate MLLMs across multi-\ndimensional AVR tasks, but remained confined to AVR, neglecting other dimensions of cognition. To\nintegrate cognitive science principles for a comprehensive understanding of MLLMs' intelligence,\nSong[72] identified five key cognitive factors based on the well-recognized Cattell-Horn-Carroll\n(CHC) model and introduced M3GIA, the first comprehensive cognitive-driven benchmark designed\nto evaluate the general intelligence of MLLMs."}, {"title": "Mathematical Question Answering:", "content": "Using mathematics problems to evaluate MLLMs is essential\nfor assessing their logical reasoning capabilities, as these problems require complex reasoning,\npattern recognition, and abstract thinking. Such tasks help determine if MLLMs can apply rules,\ndiscover patterns, and perform sophisticated reasoning. Geometry3K[78] was introduced to evaluate\ncapabilities in solving geometry problems, but it had a narrow focus on specific aspects of plane\ngeometry. To address this issue, Lu et al.[73] collected multiple datasets to construct an integrated\nbenchmark, MathVista, which covered a range of mathematical tasks, such as functions and solid\ngeometry. However, MathVista lacked a detailed classification of mathematical subdomains and\nemphasized visual abilities more than pure mathematical reasoning. Consequently, Math-V[76] and\nMathVerse[74] were developed, confining data to specific mathematical subjects and focusing on\nmathematical reasoning abilities. Additionally, Fan et al.[75] proposed NPHardEval4V, a benchmark\nthat used algorithmic problems and converted their textual descriptions into visual representations,\naimed at evaluating the pure reasoning capabilities of MLLMs. Motivated by the idea that \"if a model\ntruly understands a problem, it should perform robustly across various tasks related to that problem,\"\nZhou et al.[77] introduced MATHCHECK-GEO, a benchmark focused on the universality of tasks"}, {"title": "Multidisciplinary Question Answering:", "content": "Evaluating MLLMs using multidisciplinary questions from\nvarious educational stages assesses their ability to integrate and apply knowledge across different\ndomains. This approach tests MLLMs' reasoning and problem-solving skills in diverse contexts,\nproviding a comprehensive measure of general intelligence and cognition. ScienceQA[81] was a\nbenchmark containing multimodal science questions with rich domain diversity. While it covered a\nrange of disciplines, most of the questions were at the elementary to middle school level, which limited\nits depth. To address this issue, M3Exam[79] was proposed with a multilevel structure, featuring\nexams from three critical educational stages to comprehensively assess MLLMs' proficiency at\ndifferent levels. Additionally, SceMQA[84] focused on college entrance-level problems. Due to\nthe importance of this stage, SceMQA comprised questions with answers accompanied by more\ndetailed explanations. Intended to evaluate expert-level understanding, Yue et al.[82] introduced\nMMMU, which included problems from college exams, quizzes, and textbooks across six common\ndisciplines. However, these benchmarks were primarily available in English, restricting the evaluation\nto a single language. Therefore, CMMMU[80], CMMU[83], and MULTI[85] were created to evaluate\nmulti-discipline and multi-type question understanding and reasoning in Chinese."}, {"title": "5 Specific Domains", "content": "This section focuses on MLLMs' capabilities in specific tasks and applications, such as their ability\nto integrate complex visual and textual information, adapt to decision-making roles in dynamic\nenvironments, and effectively process diverse cultural and linguistic data. It then extends to discuss\nthe practical applications of MLLMs, highlighting their impact on various sectors such as medicine,\nindustry, and autonomous driving. By providing an overview of these benchmarks, this section aims\nto underscore the advancements in evaluating MLLMs' performance and their potential to address\nreal-world challenges across different domains."}, {"title": "5.1 Text-rich VQA", "content": "Evaluating MLLMs in text-rich visual question answering is crucial for understanding how well\nmodels interpret and integrate textual and visual information within images. This evaluation covers\nseveral aspects, including the accuracy of text recognition, contextual understanding, and the ability\nto synthesize information from both modalities. In addition to text comprehension, it also requires\nan understanding of layout and structure to effectively analyze multimodal documents, charts, and\nHTML."}, {"title": "Text-oriented Question Answering:", "content": "Some works evaluated MLLMs' effectiveness in text-related\nvisual tasks, such as text recognition and scene text-centric visual question answering. Singh et al. [87]\nintroduced TextVQA, which contained questions requiring the model to read and reason about the text\nin the image to provide answers. However, the short answers provided in TextVQA were insufficient\nfor a comprehensive description of the image. To address this issue, TextCaps[88] extended the\nlength of sentences in the answers and involved many switches between OCR and vocabulary tokens.\nDespite these efforts, existing benchmarks could be time-consuming, and inaccurate annotations in\nsome datasets made accuracy-based evaluation less precise. In response, Liu et al.[86] developed\nOCRBench to facilitate the accurate and convenient evaluation of MLLMs' OCR capabilities. To\nquantitatively assess visual reasoning capabilities in text-rich and high-resolution scenarios, Chen et\nal.[25] constructed a challenging benchmark, P2GB. This benchmark included comprehensive image\nunderstanding, fine-grained recognition, and image text content understanding. In order to cover a\nbroader spectrum of text-rich scenarios, SEED-Bench-2-Plus[89] was developed to evaluate MLLMs'\nperformance in comprehending text-rich visual data across a wide range of real-world scenarios."}, {"title": "Document-oriented Question Answering:", "content": "Document-oriented question answering requires MLLMs\nnot only to read text but also to interpret it within the layout and structure of the document. Mathew et\nal.[92] introduced InfographicVQA, which included questions requiring the combination of multiple\ncues. Although InfographicVQA showcased significant diversity in topics and designs, it still\npreferred using visual aids over long text passages. To address this limitation, Single Page DocVQA\n(SPDocVQA)[90] was introduced as a more diverse benchmark, featuring documents of various\ntypes and origins created over several decades. However, it was built exclusively on single-page"}, {"title": "Chart-oriented Question Answering:", "content": "Chart understanding plays a pivotal role when applying\nMLLMs to real-world tasks such as analyzing scientific papers or financial reports. ChartQA[95]\nwas a large-scale benchmark involving visual and logical reasoning over charts, but it had limited\nchart types. To scale up, Li et al.[98] utilized Arxiv papers and constructed SciGraphQA with\nmulti-turn question-answer conversations on scientific graphs. MMC[99] and ChartBench[97] further\nexpanded the types and tasks of chart data. However, the question design in these benchmarks\nremained relatively simplistic. To further validate MLLMs' capabilities in more complex reasoning\ntasks involving chart data, Xia et al. [96] proposed the benchmark ChartX for comprehensive chart\nunderstanding and reasoning. However, ChartX yielded artificial charts generated by GPT-4[3],\nleading to a narrow distribution. Therefore, CharXiv[100] was proposed for evaluating MLLMs'\nunderstanding of real-world scientific charts, including complex compositions with multiple subplots\ncollected from arXiv. Moreover, in order to assess MLLMs' understanding of charts from various\nperspectives, Fan et al.[101] proposed CHOPINLLM, including three different levels of questions\n(literal, inferential, and reasoning QAs)."}, {"title": "Html-oriented Question Answering:", "content": "Web pages present a complex interplay of visual and textual\ninformation, along with interactive elements, requiring MLLMs to possess rigorous understanding\nabilities over hierarchical structures and contextual relationships. Web2Code[102] was a benchmark\nwith web pages based on instruction-response pairs. The responses included structured questions\nand answers about the webpage. However, web elements are often small, numerous, and scattered\nacross the page, demanding fine-grained recognition and accurate spatial reasoning. To address\nthese limitations, Liu et al. [103] introduced VisualWebBench, which assessed MLLMs at three\nlevels: website-level, element-level, and action-level. Additionally, following the principle \"What\nI cannot create, I do not understand,\" Plot2Code[104] evaluated MLLMs' ability to generate code\nthat effectively rendered a provided image from HTML files, further showcasing their multimodal\nunderstanding and reasoning capabilities."}, {"title": "5.2 Decision-making Agents", "content": "Decision-making agents expect MLLMs to possess human-level planning and scheduling abilities,\nwhich are fundamental for making informed decisions and taking appropriate actions in complex\nenvironments. This capability holds significant potential for addressing real-world problems."}, {"title": "Embodied Decision-making:", "content": "Embodied Decision-making requires MLLMs to be able to integrate\nsensory inputs and interact with the environment in a way that mimics human physical experi-\nences. OpenEQA[108] was the first benchmark for embodied question answering, supporting both\nepisodic memory and active exploration use cases. However, it focused solely on the answers\nprovided by MLLMs and did not consider the intermediate reasoning processes. Chen et al.[107]\nargued that it was essential to enable multi-dimensional evaluation of the decision-making process,\nencompassing perception, reasoning, and action perspectives, rather than relying solely on final\nrewards or success rates. They proposed PCA-EVAL for evaluating the embodied decision-making\nability of MLLMs from different perspectives, including three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. However, these benchmarks were limited by\nthe small number of handcrafted questions and reliance on single-image visual observation. To\nstudy the embodied planning and decision-making capabilities of MLLMs more systematically, Chen\net al.[106] used large-scale egocentric videos reflecting daily human activities from a first-person\nperspective to construct EgoPlan-Bench. EgoPlan-Bench aimed to assess MLLMs' human-level\nplanning capabilities in real-world scenarios, featuring realistic tasks, diverse actions, and complex\nvisual observations. Despite that, these benchmarks failed to sufficiently challenge or showcase the\nfull potential of MLLMs in complex environments. To address this gap, Liu et al.[105] introduced\nVisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train"}, {"title": "Mobile Agency:", "content": "Mobile-Agent leverages visual perception tools to accurately identify and locate both\nvisual and textual elements within the app's front-end interface. Drawing on the visual information,\nit autonomously plans and decomposes complex tasks"}]}