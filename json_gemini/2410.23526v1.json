{"title": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models", "authors": ["Hieu Tran", "Junda Wang", "Yujan Ting", "Weijing Huang", "Terrence Chen"], "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language processing tasks, yet they often struggle with maintaining factual accuracy, particularly in knowledge-intensive domains like healthcare. This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking, a novel approach designed to enhance the factual reliability of LLMs, with a focus on medical question answering (QA). LEAF utilizes a dual strategy to enhance the factual accuracy of responses from models such as Llama 3 70B Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG, improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking results to guide the retrieval process without updating model parameters. The second strategy, Learning from Fact-Checks via Self-Training, involves supervised fine-tuning (SFT) on fact-checked responses or applying Simple Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both updating LLM parameters from supervision. Experimental results demonstrate that LEAF not only effectively detects inaccurate responses but also significantly enhances the model's accuracy. These findings suggest that integrating fact-checked responses-whether through RAG enhancement or self-training-enhances the reliability and factual correctness of LLM outputs, offering a promising solution for applications where information accuracy is crucial.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities in generating coherent and contextually relevant text. However, a significant challenge persists across various applications: LLMs sometimes generate plausible yet factually incorrect or unverified content (Ji et al. 2023; Bang et al. 2023). This issue is particularly concerning in domains such as healthcare, where the accuracy and reliability of information are critical.\nThe issue of factual inconsistency in LLMs occurs due to various factors, including insufficient training data, biases in the training corpus, and the model's inherent limitations in understanding complex real-world knowledge. The tendency to produce content that seems plausible but may not be factually grounded can potentially cause harm if used in sensitive contexts like medical diagnosis or treatment recommendations, underscoring the need for effective mitigation strategies (Zellers et al. 2019; Marcus and Davis 2020).\nIn order to address this challenge, fact-checking has emerged as a promising approach. Fact-checking mechanisms involve verifying the factual accuracy of generated content against reliable data sources, thus providing a filter to identify and rectify misinformation. Prior work has investigated a variety of methods for integrating fact-checking into LLM workflows, including retrieval-augmented generation (RAG) and other verification techniques such as Factcheck-GPT (Lewis et al. 2020; Petroni et al. 2021; Wang et al. 2023). However, existing approaches have notable limitations. Proprietary models like Factcheck-GPT cannot be deployed on private datasets, restricting their use in sensitive domains such as healthcare. Moreover, these models are often designed for general-purpose use rather than specialized for specific areas like medical expertise. Their inability to be fine-tuned further limits their effectiveness in specialized domains that require nuanced understanding.\nIn this study, we introduce LEAF: Learning and Evaluation Augmented by Fact-Checking, a novel approach featuring two parallel strategies for leveraging LLMs. The first strategy enhances LLM applications without updating model parameters, which is particularly suitable for proprietary models like ChatGPT that can't be fine-tuned. The second strategy employs self-training to update LLM parameters, enabling the model to exploit extensive unlabeled data by iteratively refining its performance using its own predictions combined with fact-checking results as pseudo-labels. These strategies represent our two main contributions:\n\u2022 Fact-Check-Then-RAG: We propose a novel approach to Retrieval-Augmented Generation (RAG) where the retrieval process is guided by the results of fact-checking. This method ensures that the retrieved information specifically enhances the factual accuracy in the model's initial output, leading to more contextually relevant and accurate responses without updating the underlying LLM parameters.\n\u2022 Learning from Fact-Check via Self-Training: We explore self-training mechanisms using fact-checked responses to improve factualness, which encompasses two"}, {"title": "Methodology", "content": "In this section, we describe our proposed methodology to enhance the factual accuracy and reliability of large language models in generating responses. Our approach, LEAF, integrates fact-checking, retrieval-augmented generation, and self-training mechanisms to systematically improve factuality in LLM outputs. The workflow of our proposed method is illustrated in Figure 1.\nThe proposed workflow is developed to enhance the factual accuracy of LLM-generated responses by integrating a rigorous fact-checking process. In the conventional LLM workflow (Figure 1(a)), the model generates responses to prompts, providing reasoning or explanations, and directly delivers the final answers. However, this approach does not inherently ensure the factual correctness of the generated content. To address this limitation, we introduce LEAF Mechanism I: Fact-Check-Then-RAG. As depicted in our enhanced workflow (Figure 1(c)), after the LLM generates a response, it undergoes evaluation by a fact-checking system. If the response is deemed factually accurate, it is retained as the final output. Conversely, if the response is identified as incorrect, the workflow triggers a Retrieval-Augmented Generation (RAG) approach, as illustrated in Figure 1(b). In this phase, relevant documents retrieved dur-"}, {"title": "Fact-Checking for LLM Responses", "content": "We leverage the Search-Augmented Factuality Evaluator (SAFE) (Wei et al. 2024), adapting it specifically for the medical domain to evaluate the factual accuracy of LLM-generated responses. SAFE overcomes the limitations of traditional evaluation methods that rely on preset reference answers, which are often insufficient for complex, long-form responses. SAFE's design includes breaking down responses into individual facts and dynamically verifying these facts through iterative Google Search queries, ensuring precise and timely evaluations.\nFor our adaptation, we focused on the following enhancements to make the system zero-cost, controllable, and effective, especially in the medical domain:\n\u2022 Incorporation of Question Context: In medical QA tasks, it is crucial to consider the context provided by the question, as these often involve specific scenarios that require accurate and nuanced responses. Our adaptation ensures that the fact-checking process integrates this context, leading to more accurate assessments of the LLM responses' relevance and correctness. More detailed information on the prompts used in this process can be found in the Appendix table 6.\n\u2022 Deployment of Qwen2-72B-Instruct as the Rater: To enhance the reliability of the factuality ratings, we replaced GPT-3.5 with the Qwen2-72B-Instruct (Yang et al. 2024) large language model. This model offers advanced capabilities in evaluating complex medical facts and ensuring that the responses are both accurate and relevant.\n\u2022 Use of MedRAG Corpus with ColBERT Retrieval: Instead of using Google search results, which may not always be relevant or accessible for medical queries, we employ the MedRAG corpus (Xiong et al. 2024). This corpus includes authoritative sources such as Wikipedia, PubMed, textbooks, and StatPearls. By using the Col-BERT (Khattab and Zaharia 2020) retrieval model, we can efficiently extract relevant documents from these sources. This approach ensures that our fact-checking system remains zero-cost, fully controllable, and tailored to the specific needs of the medical domain.\nOur comparative analysis shows that this adapted system outperforms Factcheck-GPT (Wang et al. 2023)\u2014a similar system that uses ChatGPT-3.5 and Google search\u2014in terms of filtered accuracy in medical QA tasks. This indicates that our approach is more adept at ensuring factual correctness in the responses generated by LLMs, providing a robust and scalable solution for verifying the accuracy of information in critical domains like healthcare."}, {"title": "Calculation of Fact-Check Scores", "content": "To evaluate the factual accuracy of generated responses, we employed a sentence-level fact-checking approach. Each response is decomposed into individual sentences, and each sentence is independently verified against retrieved external knowledge sources. The fact-checking system assesses whether each sentence is supported by the retrieved knowledge. Specifically, for each sentence in the response, the system attempts to retrieve relevant documents or facts that confirm or refute the content of the sentence. A sentence is considered supported if the retrieved knowledge substantiates its factual accuracy.\nThe fact-check score for a response is calculated as the ratio of supported sentences to the total number of sentences in the response. Formally, the fact-check score LEAF for a response is given by:\nLEAF = $\\frac{\\text{Number of Supported Sentences}}{\\text{Total Number of Sentences in the Response}}$"}, {"title": "LEAF Mechanism I: Fact-Check-Then-RAG", "content": "Our first innovative mechanism, Fact-Check-Then-RAG, seamlessly integrates the fact-checking stage with Retrieval-Augmented Generation (RAG). This approach leverages the documents retrieved during the fact-checking process to enhance the generation of responses. The key idea is to utilize the knowledge retrieved from the fact-checking stage, specifically for individual facts that did not pass the fact-check test. This strategy ensures that when a fact is not supported by the retrieved knowledge sources, the relevant documents are included in the RAG prompt to help the LLM refine its reasoning or answer, potentially improving performance. As illustrated in Figure 2, the methodology involves several steps:\nFirst, during the fact-checking stage, each individual fact in a response is evaluated for factual correctness using SAFE. If a fact is not supported by the knowledge retrieved (i.e., it fails the fact-check test), it indicates a gap between the LLM and the knowledge base. For these unsupported facts, relevant documents are retrieved from a comprehensive medical corpus (MedRAG), which includes authoritative sources like Wikipedia, PubMed, textbooks, and StatPearls. The ColBERT retrieval model is used to extract these documents.\nNext, the retrieved documents are included in the RAG prompt. This additional context provides the LLM with the necessary information to adjust its reasoning or answer, addressing the knowledge gap identified during the fact-checking stage. The LLM then generates new responses using the RAG framework, which is now enhanced with the relevant knowledge retrieved earlier. This iterative process ensures that the LLM's output is more informed and accurate.\nBy integrating fact-checking with RAG, our approach effectively addresses the knowledge gaps identified during the"}, {"title": "LEAF Mechanism II: Learning from Fact-Check via Self-Training", "content": "We explore self-training mechanisms using fact-checked responses to enhance the performance of LLMs. This approach consists of two main parts: supervised fine-tuning on factually correct responses and optimization with Simple Preference Optimization.\nSupervised Fine-Tuning on Factually Correct Responses\nThe first part involves fine-tuning the model using responses that have passed the fact-check test. This ensures the model is trained on verified, accurate information, thereby improving its overall performance. The process is as follows:\n\u2022 Response Generation and Fact-Checking: The LLM generates multiple responses to a given prompt, which are then evaluated using the fact-checking system.\n\u2022 Selection of Factual Responses: Only those responses that pass the fact-checking process are selected for fine-tuning. And \"pass\" is defined as the LEAF score of the response is 1.\n\u2022 Fine-Tuning: The model is fine-tuned on these factually correct responses, reinforcing its ability to produce accurate and reliable outputs.\nOptimization with SimPO The second part of our self-training approach utilizes Simple Preference Optimization (Meng, Xia, and Chen 2024) to rank and optimize responses based on their factual accuracy. SimPO aligns the reward formulation directly with the generation metric, eliminating the need for a reference model. This process involves Fact-Checking as a Ranking Model: The fact-checking system assigns scores to generated responses based on their factual accuracy. The highest-scoring responses are selected as \"chosen\" and the lowest-scoring ones as \u201crejected.\" By using the fact-checking system as a ranking model, SimPO effectively guides the model to prefer factually accurate responses."}, {"title": "Experiments", "content": "We detail two main experimental settings across different model configurations. For the Llama 3 70B Instruct model, we implemented some techniques to improve the performance of the model without updating the model parameters. In contrast, with the Llama 3 8B Instruct model, we explored self-training techniques where the parameters were updated based on fact-checking rather than labeled data. The self-training was performed using either supervised fine-tuning or SimPO, with the training data curated through a rigorous"}, {"title": "Filtered Accuracy via Fact-Checking", "content": "The experiments conducted to evaluate the accuracy of responses generated by the LLaMA 3 70B Instruct model, when filtered through LEAF's fact-checking, are described across five medical datasets. Specifically, we calculate the filtered accuracy, which measures the model's accuracy on responses that pass the fact-check test (i.e., those with a fact-check score of 1.0). We compare the original accuracy of the model on these datasets with the accuracy filtered through our fact-check system (LEAF) and a baseline fact-check system Factcheck-GPT (Wang et al. 2023), which uses GPT-3.5 as the rating model with Google Search results as the knowledge source.\n demonstrates the effectiveness of our fact-checking approach in enhancing the accuracy of the LLaMA 3 70B Instruct model's outputs. As shown, filtering responses using our LEAF fact-check system significantly improves accuracy on all datasets compared to the original model accuracy and the baseline Factcheck-GPT. The highest gains in accuracy are observed when using LEAF, indicating its superior performance in validating factually correct answers. This highlights the robustness of our approach in leveraging fact-checking for validating and improving large language model outputs."}, {"title": "Fact-Check-Then-RAG", "content": "To evaluate the effectiveness of our Fact-Check-Then-RAG (FC-RAG) approach, we present the experiments conducted comparing it to the original performance of the LLaMA 3 70B Instruct model and the standard RAG setting in MedRAG (Xiong et al. 2024). In MedRAG, the question is used as a query to retrieve relevant documents, which are then included in the prompt. In our FC-RAG approach, we use information obtained in the fact-checking stage to include in the prompt.\ncompares the performance of the Llama 3 70B Instruct model across five medical QA datasets: USMLE, MMLU-Medical, PubMedQA, BioASQ, and MedMCQA. The table lists the original accuracy of the Llama 3 70B Instruct model on each dataset, the accuracy when using MedRAG, and the accuracy using our FC-RAG approach. The LLaMA 3 70B (MedRAG) shows the model's performance when applying the standard RAG method. While MedRAG (Xiong et al. 2024) is designed to improve the model's contextual grounding by providing additional information, the results reveal that it actually harms performance on the USMLE and MMLU-Medical datasets-consistent with findings in MedRAG original paper (Xiong et al. 2024). This suggests that while MedRAG can be beneficial in certain contexts, it may introduce noise or irrelevant information in others, leading to decreased accuracy. In contrast, the FC-RAG approach consistently improves accuracy across all datasets. By incorporating fact-checking results into the RAG process, FC-RAG ensures that the model's outputs are more reliable and factually correct. This method leverages verified information during generation, leading to significant performance gains: a 4.99% improvement on USMLE, 1.66% on MMLU-Medical, 13.0% on PubMedQA, 7.28% on BioASQ, and 1.56% on MedMCQA compared to the original model performance. These results demonstrate the robustness and efficiency of FC-RAG in enhancing the outputs of large language models, particularly in domains where factual accuracy is critical."}, {"title": "Supervised Fine-Tuning on Factually Correct Responses", "content": "In order to assess the effectiveness of a model fine-tuned on fact-checked generated responses, we initiated a series of experiments. The LLaMA 3 8B Instruct model was tested on prompts drawn from five datasets, generating responses that were subsequently fact-checked. We perform supervised fine-tuning on the responses that pass the fact-check test(the response with fact-check score is 1.0). We compare the performance of the SFT model with the original model and also conduct the same experiments on the Factcheck-GPT (Wang et al. 2023).\npresents the performance of the Llama 3 8B Instruct model across five medical QA datasets: USMLE, MMLU-Medical, PubMedQA, BioASQ, and MedMCQA. The table lists the original accuracy of the Llama 3 8B Instruct model on each dataset, the accuracy after supervised fine-tuning using Factcheck-GPT, and the accuracy after supervised fine-tuning using our system (LEAF). As evident from the table, SFT on fact-checked responses significantly improves the accuracy of the model on all datasets compared to the original accuracy. Specifically, the SFT approach using LEAF shows notable improvements: an increase of 4.71% on USMLE, 4.87% on MMLU-Medical, 6.60% on PubMedQA, 4.53% on BioASQ, and 2.97% on MedMCQA compared to the original model performance. Furthermore, our fact-check system outperforms the baseline, indicating its robustness and efficiency in ensuring that the generated answers are factually correct and contextually relevant. This demonstrates the potential of combining fact-checking with fine-tuning to enhance the outputs of large language models."}, {"title": "Fact-Checking as a Ranking Model", "content": "We conducted a series of experiments to assess the effectiveness of our fact-checking system as a ranking model for responses generated by large language models. Five responses were generated using the LLaMA 3 8B Instruct model with a temperature setting of 0.8. Each response was then scored by our fact-checking system, and the performance of the highest and lowest-scored responses was analyzed. For comparison, we also ran similar experiments using ArmoRM. (Wang et al. 2024), a reward model designed to align LLMs with human preferences. ArmoRM is trained using human preference data, employing a Mixture-of-Experts (MoE) strategy to select suitable reward objectives based on context. presents the performance of the highest and lowest-scored responses using both our fact-checking system and"}, {"title": "Related Work", "content": "Evaluating factuality in Model Responses Evaluating the factuality of model responses is crucial for ensuring the reliability of large language models. Recent studies have demonstrated that LLMs can serve as effective tools for fact verification (Guan et al. 2024; Tian et al. 2023). Improvements in human evaluation techniques have further enhanced factuality assessment (Cheng et al. 2024). Factcheck-GPT (Wang et al. 2023) presents an end-to-end solution for annotating factuality in LLM outputs, offering fine-grained labels for verifiability and factual inconsistencies. Inspired by methods that break down responses for evaluation (Chern et al. 2023), SAFE (Wei et al. 2024) applies a similar approach in the long-form factuality setting, leveraging search-augmented models. Our work adapts SAFE for the medical domain, incorporating question context, deploying Qwen2-72B-Instruct for reliable factuality ratings, and using the MedRAG (Xiong et al. 2024) corpus with ColBERT (Khattab and Zaharia 2020) retrieval.\nRetrieval-Augmented Generation Retrieval-Augmented Generation, proposed by (Yih 2020), integrates relevant retrieved information into the generation process of LLMs, enhancing their performance on knowledge-intensive tasks. This approach helps improve factualness by grounding the LLMs on provided contexts and supplying up-to-date knowledge that might not be encoded in the models. Many studies have built upon the original RAG framework to further improve its effectiveness, including works by (Borgeaud et al. 2022; Ram et al. 2023; Gao et al. 2023; Jiang et al. 2023). In the biomedical field, RAG has been explored for literature information-seeking and clinical decision-making (Frisoni et al. 2022; Naik et al. 2022; Jin, Leaman, and Lu 2023). However, LLMs often struggle to capture fine-grained knowledge and frequently produce inaccurate or fabricated information, commonly referred to as hallucination. Current RAG methods remain under-explored in the context of fact verification, particularly in terms of accurate evidence retrieval and fine-grained classification. As such, our study introduces the Fact-Check-Then-RAG approach, which integrates a fact-checking stage to further enhance the reliability and accuracy of LLM-generated responses.\nLearning from Fact-Check via Self-Training Inspired by the Med-Gemini model's self-training with web search integration to enhance clinical reasoning (Saab et al. 2024), we developed a self-training approach with fact-checking to improve the accuracy and reliability of large language models. Self-training with search involves generating reasoning paths with and without web search, refining the model iteratively by integrating search results and expert demonstrations, which allows the model to be deployed offline on private servers while also improving the efficiency of inference. In contrast, our self-training with fact-checking generates multiple responses to prompts, evaluates them for factual accuracy using a fact-checking system, and fine-tunes the model on validated responses. This method ensures learning from accurate information and reduces hallucinations. While self-training with search relies on real-time web data, our fact-checking approach validates against established knowledge bases, offering a controlled and reliable framework for model enhancement, particularly in low-resource domains."}, {"title": "Conclusion", "content": "In this study, we investigated the potential of fact-checking mechanisms to improve factuality ability in LLM within the context of medical question-answering tasks. We validated the effectiveness of our approach through the original model, trained model, Retrieval-Augmented Generation (RAG), and fine-tuning. Through a series of experiments with models such as Llama 3, we demonstrated significant improvements in performance, particularly in enhancing the correctness of generated responses across all datasets compared to traditional RAG, underscoring the effectiveness of leveraging fact-checked information to provide more contextually relevant responses. In our second strategy, the use of fact-checking as a ranking model in conjunction with SimPO further refined the model's output, illustrating a clear path toward higher accuracy and robustness in LLM-generated content. Furthermore, we demonstrate that the system architecture can replace closed-source LLMs integrated with Google Search by using self-deployed open-source LLMs with specialized corpus retrieval. This approach is more controllable and cost-effective, allowing precise tuning for specific datasets and domains while reducing dependency on external APIs."}]}