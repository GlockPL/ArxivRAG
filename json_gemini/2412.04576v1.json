{"title": "Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs", "authors": ["Brandon Jaipersaud", "Zining Zhu", "Frank Rudzicz", "Elliot Creager"], "abstract": "Tools for analyzing character portrayal in fiction are valuable for writers and literary scholars in developing and interpreting compelling stories. Existing tools, such as visualization tools for analyzing fictional characters, primarily rely on explicit textual indicators of character attributes. However, portrayal is often implicit, revealed through actions and behaviors rather than explicit statements. We address this gap by leveraging large language models (LLMs) to uncover implicit character portrayals. We start by generating a dataset for this task with greater cross-topic similarity, lexical diversity, and narrative lengths than existing narrative text corpora such as TinyStories and WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis), a framework for prompting LLMs to uncover character portrayals. LIIPA can be configured to use various types of intermediate computation (character attribute word lists, chain-of-thought) to infer how fictional characters are portrayed in the source text. We find that LIIPA outperforms existing approaches, and is more robust to increasing character counts (number of unique persons depicted) due to its ability to utilize full narrative context. Lastly, we investigate the sensitivity of portrayal estimates to character demographics, identifying a fairness-accuracy tradeoff among methods in our LIIPA framework a phenomenon familiar within the algorithmic fairness literature. Despite this tradeoff, all LIIPA variants consistently outperform non-LLM baselines in both fairness and accuracy. Our work demonstrates the potential benefits of using LLMs to analyze complex characters and to better understand how implicit portrayal biases may manifest in narrative texts.", "sections": [{"title": "Introduction", "content": "Computational tools for analyzing character portrayal in narratives facilitate bias detection in literary fiction [8, 17] and AI-generated narratives [14]. They also assist writers and literary scholars in refining their story drafts and character analyses [13].\nMost of these existing tools rely on using explicit indicators in the text to uncover how a character is portrayed. However, portrayal is usually implicit, where a character's traits should be clear from their actions and behaviours rather than explicitly stated in the text [4]. For instance, \"She was stranded on an island and built a boat to escape\", which implicitly suggests high intelligence and resourcefulness. Uncovering implicit portrayal is more challenging than explicit portrayal, as it requires using commonsense knowledge to make inferences about how a character is portrayed. It becomes even more difficult to uncover from longer narratives depicting multiple distinct characters. Moreover, implicit portrayal is a key principle of literary design [24] so it is concerning that most existing tools focus on visualizing characters using explicit indicators of portrayal. Furthermore, the evaluation of new methods for implicit portrayal is difficult due to the reliance of existing benchmarks on explicit character behavior to derive target labels [19].\nPrior approaches to implicit character portrayal use the \u201cCommonsense Transformer\u201d (COMET) [2], a generative model for knowledge bases over text, to infer the mental state and motivations of protagonists [14, 15]. These methods are constrained by COMET's limitations in that it can only process simple event structures and cannot utilize long context lengths.\nIn this paper, we use LLMs to build expressive models for uncovering implicit character portrayal. We start by generating the first benchmark dataset designed specifically for this task. Compared with existing narrative text corpora such as TinyStories and WritingPrompts, our dataset offers greater cross-topic similarity, greater lexical diversity, and a broader representation of character roles. We then introduce a family of LLM prompting techniques that outperform the COMET-based approach for implicit character portrayal. We explore different prompting design choices, including the use of an intermediate character attribute list (as in COMET) to describe characters. We find that LLMs are more performant than the previous approach, although we also identify a fairness-accuracy tradeoff within LLM-based approaches. This suggests that care (beyond picking the \"optimal\" prompt) is required when designing socially beneficial tools for literary analysis.\nOur contributions can be summarized as follows:"}, {"title": "Curating a Narrative Text Dataset using Synthetic Data Generation", "content": "We formulate the task of uncovering character portrayal from text as a multi-label classification problem. Our objective is to develop a function that maps an input narrative text and a specific character\u00b9 from that text to a set of labels across three dimensions: intellect, appearance, and power (Figure 2). Each dimension is classified as either low, neutral, or high, with the \"neutral\" label reserved for cases where the text provides insufficient information to make a definitive inference about the character's portrayal. Formally, we aim to learn a function $f : X \\times C \\rightarrow Y$ that takes a narrative text $x^{(i)} \\in X$ and character $c^{(i)} \\in C$ from that text, and produces a set of labels $y^{(i)}$ representing the character's portrayal. The label space $y$ is defined as the Cartesian product of the label spaces for each dimension: $y = \\mathcal{Y}_{intellect} \\times \\mathcal{Y}_{appearance} \\times \\mathcal{Y}_{power}$, where each dimension's label space consists of the values {low, neutral, high}.\nWe select intellect, appearance, and power as our dimensions for character portrayal, as these have been previously studied in relation to social biases when analysing AI-generated narratives"}, {"title": "Dataset Curation Methodology", "content": "To curate a dataset for the implicit character portrayal classification task, we use LLMs to generate narrative texts under a set of controlled conditions such as character count (number of unique persons depicted) and narrative length. This approach allows us to increase the diversity of synthetically generated narratives while reducing representational biases that pertain to the controlled conditions [28]. We formulate this process as generating a narrative text $x^{(i)}$ subject to a set of natural language constraints $C_i$. Below, we describe our constraints which are added as natural language instructions to the LLM prompt. A full, detailed list can be found in \u00a7A.3. We refer to this dataset as ImPortPrompts (Implicit Portrayal Prompts).\nNatural Language Constraints: The narrative must contain exactly $N^{(i)}$ characters and have a length of $L^{(i)}$ sentences. Each character should be assigned a role from {protagonist, antagonist, victim} and be given a character portrayal label set $y \\in \\mathcal{Y}$. To ensure implicit portrayal, each character must be portrayed implicitly through their actions, decisions, and interactions, rather than through explicit words and statements. For each of the three portrayal categories, the narrative should avoid using words that directly describe a character's intellect (e.g., intelligent, stupid, clever), appearance (e.g., beautiful, ugly, attractive), or power (e.g., powerful, weak, strong). The socio-demographic background of characters should not be stated or implied. This includes using gender-neutral names, avoiding mentions of racial characteristics, religious affiliations, and socioeconomic status. References to age, physical attributes, or cultural backgrounds that might reveal demographic information should also be omitted. The narrative genre and topic are selected from Table 4 (\u00a7A.4).\nOur choice of character roles are motivated by prior works that develop automated methods for character role detection [10] and extraction [25] in narratives. Our definitions for these roles can be found in Table 3 (\u00a7A.1) which allow for multiple protagonists and antagonists. Our socio-demographic constraint facilitates the measurement of fairness by eliminating socio-demographic information from the generated narratives as will be discussed in Section 3.2. The socio-demographic groups we use come from [11] which we repeat verbatim in Table 18 (\u00a7C.2).\nExperimental Setup: We follow a similar setup to Perez et al. [23] and use LLMs as narrative"}, {"title": "Exploratory Data Analysis", "content": "Next we explore the diversity and distributional properties of ImPortPrompts, while comparing it with existing narrative text corpora.\nMetrics: We measure narrative quality using lexical and semantic diversity. For lexical diversity, we use the following indices: HD-D (Hypergeometric Distribution Diversity), Maas, and MTLD (Measure of Textual Lexical Diversity) [18], which are more robust to varying text lengths compared to standard TTR (token-type ratio). Lower Maas scores and higher HD-D and MTLD scores indicate greater lexical diversity. We measure semantic diversity using inter- and intra-topic APS (average pairwise similarity) and INGF (inter-sample N-gram Frequency). For both APS and INGF, lower values signify higher diversity. For APS, we use cosine similarity to compute pairwise similarity.\nComparison with existing datasets: We compare ImPortPrompts to ROCStories [19], WritingPrompts [7], and TinyStories [16]. ImPortPrompts consists of 2000 samples\u00b2 (n = 2000). To ensure consistent comparison, we randomly select an equal number of narratives from each other"}, {"title": "Uncovering Character Portrayal from Narrative Texts", "content": "Given a narrative text $x^{(i)}$ with constraints $C_i$, our primary goal is to classify the intellect, appearance, and power of each of the $N_c$ characters into {low, neutral, high}. This can be defined by a function $f : X \\times C \\rightarrow Y$ where $y^{(i)}_j = f(x^{(i)}, c_j)$. i.e. $y^{(i)}_j$ is the label set for character $c_j$ in narrative i."}, {"title": "LIIPA: LLMs for Inferring Implicit Portrayal for Character Analysis", "content": "We start by comparing the efficacy of LLMs against the method of Huang et al. [14], which used COMET [2] as an auxiliary model to infer implicit character portrayal. We refer to this baseline as COMET-Implicit Character Portrayal (COMET-ICP). We hypothesize that LLMs offer superior performance in this task because of their comprehensive world knowledge acquired through extensive pretraining and their capacity to effectively leverage long-range context for making predictions. COMET is limited to processing sentences with a simple event structure and generating a set of attributes describing the subject of the sentence. For instance, given the sentence: \"Alice gave Bob a cup of coffee.\", it may output the character attribute word list $z_w$ = {generous, kind, thoughtful}.\nEvaluation Methodology and Experimental Setup: We compare three LLM-based approaches against COMET-ICP (Figure 4). With LIIPA-STORY and LIIPA-SENTENCE, we prompt the LLM to generate character attribute word lists ($z_w$) from complete stories and individual sentences respectively, mirroring COMET's output format. With LIIPA-DIRECT, we prompt the LLM to directly classify character portrayal based on the entire narrative, bypassing the wordlist generation. We refer to this entire framework as LIIPA.\nOur choice of classification LLM is Google's Gemini [9]. We choose a different LLM family from those used to generate narratives (GPT/Claude) to avoid self-preference bias, a phenomenon where LLM evaluators recognize and favor their own generations [21]. All prompts for this section can be found in \u00a7B.1. To ensure reproducible results, we set the LLM temperature parameter to 0 for all experiments in this section.\nTo assess the quality of generated character attribute wordlists for uncovering a character's IAP, we use LLMs-as-a-judge [29] which involves prompting a separate evaluation LLM to infer a character's IAP solely from the generated wordlist. The underlying premise is that a high-quality wordlist should contain enough relevant information to enable accurate IAP inference. By applying LLMs-as-a-judge to wordlists generated by LLMs and COMET-ICP, we can quantitatively compare their effectiveness. If LLMs-as-a-judge achieves higher performance on LLM-generated wordlists compared to COMET-ICP-generated ones, we can conclude that the LLM wordlists are superior indicators of a character's IAP. Our choice of LLM-judge is GPT-4. Note that although we used the GPT LLM family for narrative generation, we still avoid self-preference bias since this model is mapping Gemini-generated wordlists to labels without access to the underlying GPT-generated narratives. For a discussion on self-preference bias, see \u00a7C.1.\nAccuracy Comparison: Figure 4 presents our accuracy results across all three portrayal dimensions. LIIPA-DIRECT consistently outperforms the other methods across all dimensions and labels, indicating that directly prompting the model to generate character portrayal labels is more effective than the predominant approach of mapping character attribute wordlists to labels.\nWe observe that LIIPA-SENTENCE is more accurate than COMET-ICP, suggesting that LLMs can generate more informative character attribute wordlists when used as a direct substitute for COMET. Furthermore, LIIPA-STORY is more accurate than LIIPA-SENTENCE, indicating that the additional context gained from using the entire narrative as input (rather than individual sentences) helps in generating more informative wordlists. This may be because when analyzing a full story, the contextual methods can identify character arcs, interactions between characters, and how traits are revealed over time, whereas sentence-level analysis might miss these broader narrative elements. We also observe that accuracy varies significantly depending on the dimension"}, {"title": "Fairness Implications of LLM-based Character Portrayal Inference", "content": "Metrics and Experimental Setup: We aim to ensure consistent performance of LLMs in character portrayal analysis across diverse demographic backgrounds, such as a black female antagonist or a disabled protagonist. We investigate this by prompting an LLM to insert character socio-demographic information (from Table 18) into our anonymized dataset [26]. We then measure disparate model performance when using LIIPA to classify implicit character portrayal. To get an overall estimate for disparity across demographic groups (e.g., gender), we compute the variance in accuracy between group members (e.g., man, woman) and then average these variances across groups.\nAccuracy Disparities across Demographic Groups: Figure 6 reveals significant accuracy disparities between group members when demographic information is inserted. We focus on the power portrayal dimension but find similar disparity patterns across appearance and intelligence (\u00a7B.2). For instance, assigning a character as a woman results in a ~6% accuracy drop, while no such drop occurs for men. Rare instances of \"positive discrimination\" emerge: characters identified as religious or Caucasian show slight accuracy increases, contrasting with significant drops for the other group members. Across all three portrayal dimensions, incorporating demographic information significantly reduces model performance. The most substantial decline occurs when"}, {"title": "Conclusion", "content": "We have proposed a new framework called LIIPA that uses LLMs to infer implicit character portrayal within narrative text. LIIPA outperforms non-LLM character portrayal estimation in both accuracy and fairness while being robust to longer texts with more characters. However, the identified fairness-accuracy tradeoff underscores the need for cautious application of LLMs when estimating character portrayal. We also introduced ImPortPrompts, a dataset for character portrayal estimation that offers improved diversity and greater cross-topic similarity over existing benchmarks. Future work can apply LIIPA to better understand how implicit portrayal biases manifest in narratives and to improve portrayal visualization tools such as those developed by"}, {"title": "Additional Fairness Plots", "content": "Here, we show further measurements of accuracy stratified across different demographic groups, this time looking at the other two label types: intellect and appearance."}, {"title": "Misc", "content": "Here, we recap the various processes in which we sample from LLMs in our work. Our LLM sampling processes include generating narratives from constraints (Sec 2.2), sampling character-attribute word lists, and sampling IAP labels either from word lists or directly from narratives and characters. This can be formally described below:\n1. Dataset Sampling: $x^{(i)} \\sim p_g(\\cdot|C_i)$\n2. Word list Sampling: $w^{(i)}_j \\sim p_w(x^{(i)}, c_j)$ For a given character $c_j$ in narrative $x^{(i)}$, we sample a word list $w^{(i)}_j$ from LLM $p_w$. This is the output format of LIIPA-SENTENCE and LIIPA-STORY.\n3. Label Sampling: There are two separate sampling processes for IAP label generation:\n\u2022 Word list-based Sampling (LLM Judge): $y^{(i)}_j \\sim p_l(\\cdot|w^{(i)}_j)$ For a given character $c_j$ in narrative $x^{(i)}$, we sample IAP labels $y^{(i)}_j$ from LLM $p_l$, conditioned on the generated word list $w^{(i)}_j$. This is used to \u201cjudge\u201d the word lists generated by LIIPA-SENTENCE and LIIPA-STORY.\n\u2022 Narrative-based Sampling: $y^{(i)}_j \\sim p_m(\\cdot|x^{(i)}, c_j)$ We also sample IAP labels $y^{(i)}_j$ from LLM $p_m$, conditioned on the full narrative text $x^{(i)}$ and a given character $c_j$. This is the output format of LIIPA-DIRECT.\nTo avoid self-preference bias, a phenomenon where LLM evaluators recognize and favor their own generations, we must ensure the LLM model family responsible for label generation ($p_l$ and $p_m$) are distinct from the families used for narrative ($p_g$) and word list generation ($p_w$). We initialize $p_g$ to be GPT, Claude, and $p_w$ to be GPT. For label generation, we initialize $p_l$ and $p_m$ to be Google Gemini. Thus, we ensure that the label-generating LLM evaluates the content objectively, without favoring its own prior outputs which helps maintain the integrity of our evaluations and supports the validity of our findings."}]}