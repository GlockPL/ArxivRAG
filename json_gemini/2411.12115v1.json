{"title": "Distill the Best, Ignore the Rest: Improving Dataset Distillation with Loss-Value-Based Pruning", "authors": ["Brian B. Moser", "Federico Raue", "Tobias C. Nauen", "Stanislav Frolov", "Andreas Dengel"], "abstract": "Dataset distillation has gained significant interest in recent years, yet existing approaches typically distill from the entire dataset, potentially including non-beneficial samples. We introduce a novel \u201cPrune First, Distill After\" framework that systematically prunes datasets via loss-based sampling prior to distillation. By leveraging pruning before classical distillation techniques and generative priors, we create a representative core-set that leads to enhanced generalization for unseen architectures a significant challenge of current distillation methods. More specifically, our proposed framework significantly boosts distilled quality, achieving up to a 5.2 percentage points accuracy increase even with substantial dataset pruning, i.e., removing 80% of the original dataset prior to distillation. Overall, our experimental results highlight the advantages of our easy-sample prioritization and cross-architecture robustness, paving the way for more effective and high-quality dataset distillation.", "sections": [{"title": "1. Introduction", "content": "Large-scale datasets are crucial for training high-quality machine learning models across various applications [10,20]. However, the sheer volume of data brings significant computational and storage challenges, making efficient dataset distillation methods highly desirable [2, 5]. Dataset distillation aims to compress these large datasets into smaller, synthetic subsets while preserving training quality, yet existing techniques often fall short in achieving cross-architecture robustness [17]. Classifiers generally perform best when their architecture matches the one used during distillation, but performance degrades significantly when trained on other architectures [3, 17]. This challenge is compounded by the retention of noisy samples, which dilutes the core representational value of distilled data. Addressing these issues requires not only compact and representative data but also a selective sampling approach to enhance performance consistency across a range of unseen architectures.\nInspired by previous work on dataset pruning for various computer vision tasks [1,4,7,15,16], we explore the interplay between dataset pruning and dataset distillation, proposing a novel approach that systematically prunes samples prior to distillation. As shown in Figure 1, our method enables the creation of compact yet highly representative core-sets, a subset of the original dataset, through targeted pruning that enhance performance and stability across diverse and unseen architectures. To realize this, we introduce a loss-value-based sampling strategy that leverages a pre-trained classifier model to rank data samples by their \u201cclassification difficulty\", helping to capture the key characteristics of each class. This analysis combines two sampling strategies: ascending (starting with simpler samples) and descending (starting with complex samples), allowing us to examine their respective effects on distillation quality. As a result, we"}, {"title": "2. Background", "content": "This section introduces the key concepts of dataset distillation alongside their fusion with generative priors."}, {"title": "2.1. Dataset Distillation", "content": "Let $\\mathcal{T} = (X_{real}, Y_{real})$ represent a real dataset with $X_{real} \\in \\mathbb{R}^{N \\times H \\times W \\times C}$, where $N$ denotes the total number of samples. The objective of dataset distillation is to compress $\\mathcal{T}$ into a smaller synthetic set $\\mathcal{S} = (X_{syn}, Y_{syn})$, with $X_{syn} \\in \\mathbb{R}^{M \\times H \\times W \\times C}$, where $M = C \\cdot IPC$, $C$ represents the number of classes, and $IPC$ the images per class. Our goal is to achieve $M < N$ by optimizing\n$\\mathcal{S}^* = \\arg \\min_{\\mathcal{S}} \\mathcal{L}(\\mathcal{S},\\mathcal{T}),$ (1)\nwhere $\\mathcal{L}$ represents the distillation objective, which is defined by the applied dataset distillation method, e.g., Dataset Condensation (DC) [25], Distribution Matching (DM) [24], and Matching Training Trajectories (MTT) [2]."}, {"title": "Dataset Condensation (DC)", "content": "Dataset Condensation (DC) aligns gradients by minimizing the difference between the gradients on the synthetic and real datasets. This is expressed as\n$\\mathcal{L}_{DC}(\\mathcal{S},\\mathcal{T}) = 1-\\frac{\\langle \\nabla_{\\theta}l_S(\\theta), \\nabla_{\\theta}l_T(\\theta) \\rangle}{\\|\\nabla_{\\theta}l_S(\\theta)\\| \\|\\nabla_{\\theta}l_T(\\theta)\\|}.$ (2)"}, {"title": "Distribution Matching (DM)", "content": "Distribution Matching (DM) enforces similar feature representations for real and synthetic data by aligning the feature distribution across classes:\n$\\mathcal{L}_{DM}(\\mathcal{S},\\mathcal{T}) = \\frac{1}{C}\\sum_{c=1}^{C}\\left\\|\\frac{1}{|T_c|}\\sum_{x \\in T_c}\\psi(x) - \\frac{1}{|S_c|}\\sum_{s \\in S_c}\\psi(s)\\right\\|^2,$ (3)\nwhere $T_c$ and $S_c$ represent real and synthetic samples for each class $c$."}, {"title": "Matching Training Trajectories (MTT)", "content": "Matching Training Trajectories (MTT) minimizes the distance between parameter trajectories of networks trained on real and synthetic data. Using several model instances, MTT saves the training path of model parameters $\\theta^{t+i}_{T}$ at regular intervals, called expert trajectories. For distillation, it initializes a network, $\\theta^{t+n}_{S}$, with $N$ steps, on the synthetic data, tracking its trajectory, and minimizes the distance from the real data trajectory, $\\theta^{t+m}_{T}$ with $M$ steps:\n$\\mathcal{L}_{MTT}(\\mathcal{S},\\mathcal{T}) = \\frac{\\| \\theta^{t+N}_S - \\theta^{t+M}_T \\|^2}{\\| \\theta^t_S - \\theta^{t+M}_T \\|^2}.$ (4)"}, {"title": "2.2. Dataset Distillation with Generative Prior", "content": "In dataset distillation with a generative prior, a pre-trained generative model is used to synthesize latent codes rather than raw pixel values [3]. Incorporating generative priors into dataset distillation offers several advantages, primarily by compressing informative features into a more structured latent space. This transformation enables greater flexibility, as latent codes are easier to manipulate and adapt compared to pixel-level data. Specifically, let $D : \\mathbb{R}^{M \\times h \\times w \\times d} \\rightarrow \\mathbb{R}^{M \\times H \\times W \\times C}$ denote the generative model, where $h \\cdot w \\ll H \\cdot W$. This transforms the distillation objective as follows:\n$\\mathcal{Z}^* = \\arg \\min_{Z} \\mathcal{L}(D(Z), \\mathcal{T}),$ (5)\nwhere $\\mathcal{L}$ is the distillation objective defined by the used algorithm (see subsection 2.1). GLaD [3], one of the initial methods to leverage generative priors, employs a pre-trained StyleGAN-XL [21]. LD3M [17] extends GLaD by replacing the StyleGAN-XL with a modified diffusion model, i.e., Stable Diffusion [20], tailored for dataset distillation."}, {"title": "2.3. Core-set Selection in Distilled Datasets", "content": "Combining distilled datasets with core-set approaches has already been proposed for small-scale datasets, such as CIFAR100 and Tiny ImageNet. He et al. [11] suggested applying core-set selection directly to the distilled dataset using two specific rules. The first rule selects images that are easy"}, {"title": "3. Methodology", "content": "The overarching goal of dataset distillation is to reduce the size of a dataset by learning strong and representative synthetic samples. Yet, finding the most representative and informative features also involves the complementary task of removing noisy and poor information. Thus, we propose"}, {"title": "3.1. Core-Sets", "content": "Let $\\mathcal{T} = (X_{real}, Y_{real})$ represent a dataset of size $N_T$, where $x_i \\in X_{real}$ denotes the $i$-th image sample and $y_i \\in Y_{real}$ its label. A core-set $\\pi_{\\rho} \\subset \\mathcal{T}$ is defined as a subset of size $N_{\\pi} \\approx r \\cdot N_T$, with $r \\in (0, 1)$ determining its relative size.\nThe goal is to construct a core-set that improves the quality of distilled datasets by focusing on key samples, achieving the specified subset size through a sampling strategy that satisfies the condition\n$N_{\\pi} = \\sum_{(x_i,y_i) \\in T} \\mathbb{1}_{\\pi_{\\rho}}(x_i) \\approx r \\cdot N_T,$ (6)\nwhere $\\mathbb{1}_{\\pi_{\\rho}}: \\mathcal{T} \\rightarrow \\{0, 1\\}$ serves as an indicator function, marking membership of elements in the core-set $\\pi_{\\rho}$ within the larger dataset $\\mathcal{T}$. In other words, the core-set $\\pi_{\\rho}$ acts between the original dataset and the synthetic dataset, representing only a fraction of the original dataset to the dataset distillation methods. The concrete realization of the core-set depends on the sampling mechanism that defines the indicator function $\\mathbb{1}_{\\pi_{\\rho}}$."}, {"title": "3.2. Loss-Value-based Sampling", "content": "In image classification, one tries to approximate $y_i \\approx M_{\\theta}(x_i)$, where $M_{\\theta}$ is a classifier with parameters $\\theta$ (i.e.,"}, {"title": "4. Experimental Setup", "content": "We follow Cazenavette et al. [3] and evaluate the cross-architecture performance with generative priors (GLaD and LD3M), for IPC=1 (MTT, DC, DM) and IPC=10 (DC, DM) with image size 128 \u00d7 128 as well as an evaluation with DC and image size 256 \u00d7 256 for IPC=1. In all experiments, we maintain consistent hyperparameters to guarantee a fair comparison. Our code can be found on GitHub\u00b9."}, {"title": "4.1. Datasets", "content": "We assess classifier accuracy on synthetic images from various 10-class subsets of ImageNet-1k [6]. The subsets include ImageNet-Birds, ImageNet-Fruits, and ImageNet-Cats, as defined in prior work [2], as well as two widely used subsets, ImageNette and ImageWoof [12]. Additionally, we use subsets based on ResNet-50 performance on ImageNet [3]: ImageNet-A contains the top 10 classes, followed by ImageNet-B, ImageNet-C, ImageNet-D, and ImageNet-E."}, {"title": "4.2. Evaluation Protocol", "content": "We begin by distilling synthetic datasets using the chosen algorithms (i.e., DC, DM, and MTT), followed by assessing"}, {"title": "4.3. Network Architectures", "content": "Following Cazenavette et al. [3] and previous work in dataset distillation [5, 17-19], we use ConvNet to distill the 128 x 128 and 256 \u00d7 256 datasets, respectively [9]. For evaluating unseen architectures, we employ AlexNet [14], VGG-11 [22], ResNet-18 [10], and a Vision Transformer [8]."}, {"title": "4.4. Generative Priors", "content": "We incorporate two types of generative priors: GLaD [3], which leverages a StyleGAN-XL [21] model, and LD3M [17], which uses a modified Stable Diffusion [20] model adapted specifically for dataset distillation."}, {"title": "4.5. Pruning Details", "content": "We applied a pre-trained ResNet-18 model [10] with cross-entropy loss as the scoring mechanism for our proposed loss-value-based sampling."}, {"title": "5. Results", "content": "This section evaluates the impact of our pruning approach across several ImageNet subsets and distillation methods, where we used $\\pi^{easy}_{\\rho}$ (r = 0.2 for DM and r = 0.6 for DC and MTT). Next, we present our analysis of $\\pi^{easy}_{\\rho}$ and $\\pi^{hard}_{\\rho}$, from which we derive the optimal settings and values for r."}, {"title": "5.1. Distillation Results", "content": "This section demonstrates the impact of our pruning approach on the performance of dataset distillation across multiple ImageNet subsets and architectures. The pruning settings (60% for DC, MTT, and 20% for DM) were derived from the analysis provided in the next subsection 5.2. Overall, the proposed loss-based pruning consistently improves performance across various distillation algorithms.\nIPC=1, 128\u00d7128. As shown in Table 1, applying pruning yields significant accuracy gains across nearly all subsets, with particularly strong improvements for the DC and DM algorithms for IPC=1. Specifically, pruning prior to DM with GLaD improves accuracy by +5.2 percentage points on the ImageNet-A and B subsets and by +3.9 percentage points on ImageNet-C. Similarly, DM with LD3M sees notable boosts, particularly on ImageNet-A (+3.0 points) and ImageNette (+2.8 points). Overall, GLaD demonstrates more pronounced accuracy improvements than LD3M, emphasizing the effectiveness of pruning and generalizability across"}, {"title": "5.2. Pruning Analysis", "content": "Prior to the experiments presented so far, we analyzed the impact of pruning settings on ImageNette, IPC=1 across the three distillation methods with LD3M: DC, DM, and MTT. We evaluated relative dataset sizes in two ranges: fine-grained sizes from 1% to 9%, and broader sizes from 10% to 100% in increments of 10%. For each relative size r \u2208 (0, 100), we apply the same relative size class-wise, maintaining a representative subset that preserves class diversity and minimizes bias.\nThe results of our first investigation are shown in Figure 6 (more detailed Tables in the appendix). It shows the performance of DC, DM, and MTT methods as pruning is applied in steps from 10% to 100% ($\\pi^{easy}_{asy}$). The general trend across all methods shows that higher pruning rates (lower percentages) lead to fluctuation in the performance compared to the training on the full dataset. Moreover, it shows that MTT and DM can perform better than the entire dataset for various"}, {"title": "6. Limitations & Future Work", "content": "One limitation of our approach is determining the optimal pruning factor r before starting the distillation process. This pre-computation step requires a preliminary analysis to identify the ideal amount of data to retain in the core-set, which can add computational overhead. The r value may vary depending on the dataset, architecture, and distillation method used, necessitating fine-tuning for each new scenario.\nFuture work should develop dynamic methods for determining r, such as adaptive pruning strategies that adjust based on the specific characteristics of the dataset and target architecture. Such advancements would streamline the pruning process and make it more applicable to a broader range of datasets and architectures, further improving the generalizability of distilled datasets."}, {"title": "7. Conclusion", "content": "We introduced a novel \"Prune First, Distill After\" framework that combines loss-value-based sampling and pruning to optimize distillation quality, enhancing the performance on unseen architectures. We effectively address critical limitations in existing dataset distillation approaches, such as cross-architecture generalization and data redundancy, by emphasizing the importance of \u201ceasier\u201d samples, i.e., samples with low loss values, for dataset distillation. Experimental results on various ImageNet subsets demonstrate that even substantial pruning (e.g., up to removing 80% of the original dataset) prior to distillation maintains or, most of the time, improves distillation quality, with accuracy gains reaching up to 5.2 percentage points on various subsets. This robust performance across diverse architectures and pruning factors showcase the scalability and generalizability of our framework, marking a significant advancement in the field of dataset distillation."}]}