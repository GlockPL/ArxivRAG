{"title": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following", "authors": ["Yuxiao Yang", "Shenao Zhang", "Zhihan Liu", "Huaxiu Yao", "Zhaoran Wang"], "abstract": "This work focuses on building a task planner for Embodied Instruction Following (EIF) using Large Language Models (LLMs). Previous works typically train a planner to imitate expert trajectories, treating this as a supervised task. While these methods achieve competitive performance, they often lack sufficient robustness. When a suboptimal action is taken, the planner may encounter an out-of-distribution state, which can lead to task failure. In contrast, we frame the task as a Partially Observable Markov Decision Process (POMDP) and aim to develop a robust planner under a few-shot assumption. Thus, we propose a closed-loop planner with an adaptation module and a novel hindsight method, aiming to use as much information as possible to assist the planner. Our experiments on the ALFRED dataset indicate that our planner achieves competitive performance under a few-shot assumption. For the first time, our few-shot agent's performance approaches and even surpasses that of the full-shot supervised agent.", "sections": [{"title": "1 Introduction", "content": "With the development of AI and robotics, many previous works have combined them to handle Embodied Instruction Following (EIF). Among them, the Action Learning From Realistic Environments and Directives (ALFRED) benchmark (Shridhar et al., 2020) is particularly challenging because it requires an agent to learn a long-horizon policy that maps egocentric images and language instructions into a sequence of actions. In each task, the agent will be given a natural instruction (e.g. \"Put a heated mug down on a table\") and an egocentric visual observation at each step. The agent is required to output low-level actions (e.g. MoveAhead, RotateRight, etc.) based on the observation to complete the task. These tasks are usually challenging due to the sparse reward settings. For such a reason, many works have adopted a hierarchical structure to deal with it (Song et al., 2023; Min et al., 2021; Blukis et al., 2021; Kim et al., 2024). The high-level module decomposes the whole task into several sub-goals, the low-level module outputs actions to finish each sub-goal. Previously, sub-goal planners are trained on human-annotated dataset through supervised learning. However, they require large amounts of data and often lack robustness (Min et al., 2021; Blukis et al., 2021; Kim et al., 2024)."}, {"title": "2 Related Work", "content": "With recent advancements in Large Language Models (LLMs), many studies have explored using LLMs as sub-goal planners, utilizing their in-context learning abilities (Song et al., 2023; Shin et al., 2024; Ahn et al., 2022). Although these methods have achieved competitive performance under the few-shot assumption, a critical limitation is that these approaches all study the problem from a supervised learning perspective. They merely attempt to imitate the ground truth trajectories, which results in a lack of robustness within their agents. EIF benchmarks, on the other hand, require long-horizon planning ability. For example, the task \"Put a warmed apple in the fridge\" requires 12-step planning. Assuming that after applying in-context learning, the distribution of the agent's output actions becomes closer to that of the Oracle, with an accuracy of 0.9, the overall accuracy of the entire planning task decreases to 0.912 = 0.28. Traditionally, a large amount of data is required to mitigate such an issue (Blukis et al., 2021; Kim et al., 2024). However, under the few-shot assumption, in-context learning methods rely heavily on the reasoning ability of pretrained LLMs (Brown et al., 2020; Dong et al., 2024). The hallucination problem of LLMs (Zhang et al., 2023) suggests that supervised methods through in-context learning are limited.\nTo address this issue, we approach the ALFRED task (Shridhar et al., 2020) as a Partially Observable Markov Decision Process (POMDP), where the planner makes decisions based on its current state. Each task begins with a natural language description. At each step, the planner receives an egocentric RGB image and returns a high-level sub-goal. The planner can only receive reward signals (Success or Fail) at the end of the task. There are three major challenges in building a robust planner: (1) The sparse reward settings make it difficult for the planner to learn and make accurate decisions. (2) The planner can only receive an egocentric picture and cannot detect the whole state. (3) Under the few-shot assumption, the planner cannot obtain enough information from trajectories.\nFor the first problem, we adopt an actor-critic framework (Liu et al., 2024) which consists of two actors, one critic, and one generator. At each step, the planner receives a new state and performs a tree search with the actors and generator to plan future trajectories, rather than directly outputting a sub-goal. The critic is then used to select the best rollout and return its initial action. Thus, the planner can optimize the output over the long horizon to address the issue of sparse reward. For the second difficulty, we design an adaptation module instantiated by LLMs. Upon receiving an egocentric image, the adaptation module aims to predict the invisible latent PDDL variables of the task, which could help the planner better understand the environment. For the third challenge, we propose a novel hindsight method. It collects suboptimal trajectories from the agent in the training environment and relabels them to complete the task. This approach provides the planner with additional information. During the deployment phase, we prompt one actor with ground truth samples, while the other actor is prompted with hindsight samples. Thus, the relabeled trajectories can guide the planner in adjusting its policy when incorrect actions are proposed and executed."}, {"title": "2.1 Large Language Model (LLM) and In-Context Learning (ICL)", "content": "Large language models (LLMs) have shown incredible reasoning ability (Vaswani et al., 2023; Wei et al., 2022; Touvron et al., 2023; OpenAI et al., 2024) across a wide range of tasks. A crucial way to enhance this reasoning ability is through in-context learning (ICL) (Brown et al., 2020; Dong et al., 2024), which allows LLMs to solve complex tasks with only a few samples. Furthermore, this approach removes the need for fine-tuning, which can be time-consuming and computationally expensive. To utilize the ICL ability better, many studies propose certain frameworks aimed at enhancing the reasoning capabilities of LLMS (Yao et al., 2023; Wei et al., 2023; Yao et al., 2024). Among them, Liu et al. (2024) proposes a novel perspective by bridging RL and LLM, which inspires us to study ICL from an RL aspect. Xie et al. (2022) interprets ICL as Implicit Bayesian Inference, while Dai et al. (2023) believes that ICL is performing implicit Gradient Descent. All of these imply"}, {"title": "2.2 Adaptation Module in POMDP", "content": "In a Partially Observable Markov Decision Process (POMDP), planners are presented with observable states, while the latent states are invisible to the planner. Making decisions with incomplete information is challenging; therefore, a component to map the observable state into the latent space is crucial (Lee et al., 2023). Adaptation modules have been proven effective in legged robots (Kumar et al., 2021; Zhou et al., 2019; Peng et al., 2020). These modules aim to bridge the gap between the simulator and the real world. They are often trained to predict crucial information that a robot can sense in the simulator but not through its sensors in the actual world, such as surface friction or payload of the robot. The base policy then makes decisions based on the observed information and the invisible latent information predicted by adaptation modules. Inspired by this, we propose an adaptation model that maps the visible object list to the latent, invisible Planning Domain Definition Language (PDDL) (Chapman, 1987) of ALFRED (Shridhar et al., 2020).\nPrevious work such as Min et al. (2021), trains a BERT (Devlin et al., 2019) to predict the PDDL arguments and decompose high-level instructions into templated sub-goals. However, our approach differs from these in two aspects: (1) Previous works predict the arguments at the beginning of a task, which is equivalent to predicting the latent variables based on the initial observed state. In contrast, our method predicts the latent arguments at each time before reasoning, allowing predictions to be adjusted through exploration, which makes our planner more robust. (2) We do not apply the templated approach directly. The adaptation module is used to reveal the latent information for the planner and assist the planner in making better decisions. Experiments show that our method achieves competitive performance even without the assistance of the adaptation model, as demonstrated in Table 4."}, {"title": "2.3 Hindsight in LLMs", "content": "Hindsight algorithms (Andrychowicz et al., 2018; Li et al., 2020; Pong et al., 2020) are widely adopted in the reinforcement learning (RL) area. Generally, the hindsight method aims to reveal future information after collecting a trajectory and relabel the trajectory to make it more informative during training process (Furuta et al., 2022; Andrychowicz et al., 2018). Furuta et al. (2022) applies the hindsight method in training a Transformer model and achieves competitive performance on several baselines. However, training a model from scratch usually requires a large amount of data. In contrast, in-context learning, leveraging the reasoning ability of LLMs, allows an agent to complete complex tasks with only a few samples. Dai et al. (2023) has shown that ICL executes an implicit parameter update. As a result, we utilize ICL in our proposed method. Intuitively, we hope hindsight prompts can provide guidance when an out-of-distribution state is encountered. For example, \"Wash a pan and put it away\" requires the agent to wash a Pan and put it on the Dining Table. The trajectory from a planner could be: {(PickupObject, Pan), (PutObject, Sink), (ToggleObjectOn, Faucet), (PickupObject, Pan), (PutObject, CoffeeMachine)}."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Definition in POMDP", "content": "In a POMDP M, consider an action space A, latent state space X, observation space Y, transition probability function p(x'|x, a), emission function o(y|x), reward function r(x, a) and discount factor \u03b3\u2208 [0,1). The trajectory \u03c4is defined as \u03c4 = {xo, Yo, ao, X1,Y1,41,...} and the initial state xo is generated through xo ~ po(\u00b7). The policy \u03c0\u03bf(\u00b7|y) aims to map the observation space into the action space, with e denoting its parameters. The goal of RL is to train a policy such that\n$\\pi_{\\theta} = \\arg \\max_{\\pi} E_{\\tau \\sim P(\\cdot|\\pi)} [R(\\tau)],$ \n(3.1)\nwhere P(\u03c4|\u03c0) = \u03c1\u03bf(\u03a7\u03bf) \u03a0+=0P(Xt+1|Xt, at)O(Yt|xt)\u03c0(at|Yyt), R(T) = \u2211=0tr(xt, at).\n-1\nGiven a parameterized reward function rz(x, a), where z \u2208 Z is a variable indicating the goal for the agent, the conditional policy \u03c0(\u00b7|y, z) aims to accomplish different goals based on its observations. The goal in Equation (3.1) becomes\n$\\$\\pi_{\\theta} \\arg \\max_{\\pi} E_{\\tau \\sim P(\\cdot|\\pi,z),z\\sim p(z)} [R_z(\\tau)],$ \n(3.2)\nwhere R\u2082(\u03c4) = \u2211t=0ytrz(xt, at). Equation (3.2) can be considered as the multi-task RL objective to optimize, which is the core of EIF."}, {"title": "3.2 Information Matching", "content": "Following Furuta et al. (2022), we define the information matching (IM) problem as training a policy \u03c0\u03bf that satisfies\n$\\pi_{\\theta} arg \\min_{\\pi} E_{\\tau \\sim P(\\cdot|\\pi,z),z\\sim p(z)} [KL(I(\\tau), z)],$ \n(3.3)"}, {"title": "4 Hindsight Planner", "content": ""}, {"title": "4.1 Overview", "content": "The Hindsight Planner outputs a sub-goal based on the observed objects and natural language instructions. During the collection phase, suboptimal trajectories are collected, and we apply our"}, {"title": "4.2 Prompt Design", "content": "All components follow a similar design. The prompt begins with an intuitive explanation of the task and a role description of the LLM. A frozen BERT is then used as a kNN retriever, encoding the task description and selecting K examples with the closest Euclidean distance from the sample pool as in-context samples (Song et al., 2023). Intuitively, the planner would make similar suboptimal actions in similar tasks. For instance, if in an in-context sample \"Place two spray bottles into the cabinet,\" the planner fails to open the cabinet when putting the second spray bottle into it. In the current task \"Putting two candles in a cabinet\", the planner would know to avoid a similar mistake."}, {"title": "4.3 Hindsight Method", "content": "In Section 3, we gain a coherent framework to describe previous hindsight methods. However, we find that such methods can lead to the policy a being suboptimal, particularly when the number of samples is insufficient. To illustrate this better, we consider the optimization objective in Equation (3.2). It aims to learn a policy under different values of z where z ~ p(z). During the collection phase, the agent's trajectory is usually suboptimal and random. Assume the distribution of I(\u315c) ~ q. The training objective after relabeling is to train a policy satisfies that\n$\\hat{\\pi} = arg max E_{\\tau \\sim P(\\cdot|\\pi,z),z\\sim q(z)} [R_z(\\tau)],$ \n(4.1)\nDefine the \u03c0* as the oracle. It is easy to see that\n$E_{\\tau \\sim P(\\cdot|\\pi,z),z\\sim p(z)} [R_z(\\tau)] <E_{\\tau \\sim P(\\cdot|\\pi^*,z),z\\sim p(z)} [R_z(\\tau)],$ \n(4.2)\nas the distribution of z is shifted from pto q."}, {"title": "4.4 Adaptation Module", "content": "In a POMDP, the adaptation module is used to predict the latent variables from the observed environment yt (Lee et al., 2023; Kumar et al., 2021) and construct the whole state xt = (Adapter(yt), yt). In practice, we utilize an LLM as the adaptation module and set PDDL arguments as the prediction target for it. The input prompt for the adaptation module begins with an intuitive explanation of ALFRED, followed by several in-context samples. At the end of the prompt is the current task and the object list. At each step, the object list is updated as the agent explores the environment.\nThe output from the adaptation module varies depending on the task description. Inspired by PDDL (Chapman, 1987; Silver et al., 2023) of ALFRED, the adaptation module needs to predict the following arguments at each step: (1) object_target: The specific object to be interacted with during the task. (2) parent_target: The final place for the object in the task. (3) mrecep_target: The container or vessel necessary for the task. (4) toggle_target: The device that needs to be toggled in the task. (5) object_state: Indicates whether the target object needs to be cleaned, heated, or cooled. (6) object_sliced: Determines if the object must be sliced. (7) two_object: Specifies whether the task involves handling and placing two objects. The adaptation module predicts these arguments at each time before reasoning. Then, the arguments are processed into a specific format to assist the task planner to sense the environment better."}, {"title": "4.5 Task Planner", "content": "We adopt an actor-critic planner (Liu et al., 2024). At each time step t, the planner receives xt from the environment and the adaptation module. We initiate two Actors: Actorgt and Actorhind, with different samples from the sample pool D. For each state, we prompt each Actor to generate W actions. The Critic then selects the top B actions. A generator & generates the next state based on each action. In this way, we map Actors and Critic to B future trajectories and select the best future trajectory (xt, a, ...) through Critic. at is then returned as the sub-goal for Low-PL. The left half of Figure 1 shows the reasoning process of the planner."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Setups", "content": "We validate our framework using the ALFRED benchmark (Shridhar et al., 2020). This benchmark assesses the agent's capability to execute a series of actions for long-horizon household tasks based on natural language task descriptions and egocentric vision. The ALFRED dataset consists of 25k annotations, 108 distinct objects, 7 types of tasks, and 120 scenes. The dataset is divided into training, validation, and testing splits. The validation and test splits contain \"seen\" subsets, which are part of the training fold, and \u201cunseen\" subsets, which are distinct from it. The evaluation is based on Success Rate (SR) and Goal Condition (GC). Given the inherent noise in natural language instructions and the complexities of long-horizon task planning, the ALFRED benchmark presents significant challenges for embodied agents in formulating robust and precise plans.\nSimilar to previous work (Song et al., 2023; Shin et al., 2024), we only utilize a few examples from the 21k training set annotations. For each of the 7 task types, we randomly select 20 trajectories as the initial sample pool. At the collection phase, we run our planner on the 140 trajectories and collect sub-optimal trajectories. During collection, the same task is not included as in-context samples.\nWe then give a detailed discussion of the relabeling process. Directly applying the task description from ALFRED may lead to unsatisfactory results, as the task description is often vague. For example, the task \"Put a chilled potato on the small black table\" requires the planner to put the potato on a Side Table. If the task description is applied directly, LLMs might focus incorrectly on the Black Table and return an incorrect action \"PutObject BlackTable\". If the task description is not included in the prompt, it could lead LLMs to imitate the ground truth trajectory. However, planners usually have multiple ways to complete a certain task. For instance, in a task requiring the planner to slice an apple, after slicing the apple, the planner could put the Knife on the DiningTable or CounterTop. To address this issue, we relabel the task based on the latent PDDL arguments. The task description \"Put a chilled potato on the small black table\" becomes \"Pick up one cooled potato and put it on the SideTable\". This approach helps clarify the task for the planner and reduces the ambiguity in instructions.\nFor the kNN retriever, we use a frozen BERT from Wolf et al. (2020). We employ GPT-4 Turbo (OpenAI et al., 2024) as the target LLM and set temperature to 0. For the Adapter, 5 in-context"}, {"title": "5.2 Main Results", "content": "We initially compare our method to other few-shot methods, as shown in Table 1. It is evident that our method achieves a 10.18 and 5.36 higher success rate in \"Test Seen\" and \"Test Unseen\" categories, respectively, compared to the previous state-of-the-art method (LLM-Planner) that uses high-level instructions only. Moreover, even when compared to methods utilizing low-level,"}, {"title": "5.3 Ablation Study", "content": "We conduct ablation studies to under-stand the effectiveness of the components in our framework. First, we ablate theadaptation module Adapter, which re-quires the planner to make decisions basedsolely on the partially observed informa-tion. The results show that this causes adrop of -2.44 and -4.01 in the successrates for the \"Valid Seen\" and \"Valid Unseen\" splits. Then, we remove the hindsight prompts. For afair comparison, the original planner requires both Actorgt and Actorhind to generate one actionper state. We also ablate by prompting Actorgt to output two actions for each state. Table 4 showsthat the success rates drop by -2.08 and -2.68 in the \"Valid Seen\" and \"Valid Unseen\" splits.\nFor a more comprehensive analysis, we report the success rates for each task type in the \"ValidSeen\" split, as shown in Table 3. Additionally, we also present the average sub-goal lengths in Table 5.This analysis reveals that hindsight prompting is especially crucial in relatively long-horizon tasks,such as \"Cool Object\u201d and \u201cHeat Object\u201d. This is likely because, in long-horizon tasks, plannersare more likely to output suboptimal actions, allowing the hindsight actor to correct its mistakes.\nOn the other hand, the adaptation module can assist the planner inbetter sensing the environment, leading to a general improvementacross nearly all areas."}, {"title": "6 Conclusion", "content": "This paper explores an effective few-shot framework for EmbodiedInstruction Following. We approach the task as a POMDP anddesign a closed-loop Hindsight Planner equipped with an adaptationmodule to enhance the agent's environmental sensing capabilities.Compared to previous open-loop, supervised methods, our approachis more robust and performs better. Furthermore, the plannerincorporates a novel hindsight method that enables it to learn fromsuboptimal trajectories. we hope our work inspires future researchin this area."}, {"title": "A More Algorithm", "content": "In Algorithm 2, we present a beam search example of a hindsight planner. During the collection phase, one Actor prompted from the ground truth sample pool is required to output W actions for each state, and Critic is used to retain the best B actions for the next round of planning. When the search depth U is reached, the best rollout is selected and the first action from it is returned. At the deployment phase, two Actors are prompted with hindsight prompts and ground truth samples. Each Actor is required to generate actions.\nAlgorithm 3 outlines the algorithm for the collection phase. To preserve the few-shot assumption, the planner collects suboptimal trajectories from Dgt. During the execution of the current task, this task is specifically excluded from being used as an ICL sample to the planner. We employ a prompt generator & to relabel tasks and mitigate ambiguity in the instructions."}, {"title": "B Prompts", "content": ""}, {"title": "B.1 Prompts for Planner", "content": "Here, we display prompts for various components. The <base_info> defines the role descriptions while the <samples> provide in-context examples for the Actors, theCritic, and theAdapter.\nWe first show the role description for the Actors, the Critic, and the Adapter."}, {"title": "B.2 Prompts for hindsight", "content": "We now present the prompts used to query LLMs in our hindsight method. During the relabeling process for the Actor, we first prompt the LLMs to generate a <Think> for the suboptimal trajectory, and then query them to complete the task based on it. For the relabeling process of the Critic, we directly prompt the LLMs to generate an evaluation for the suboptimal trajectory. We first present the hindsight samples for clarity."}]}