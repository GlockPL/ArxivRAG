{"title": "LLM Augmentations to support Analytical Reasoning over Multiple Documents", "authors": ["Raquib Bin Yousuf", "Nicholas Defelice", "Mandar Sharma", "Shengzhe Xu", "Naren Ramakrishnan"], "abstract": "Building on their demonstrated ability to perform a variety of tasks, we investigate the application of large language models (LLMs) to enhance in-depth analytical reasoning within the context of intelligence analysis. Intelligence analysts typically work with massive dossiers to draw connections between seemingly unrelated entities, and uncover adversaries' plans and motives. We explore if and how LLMs can be helpful to analysts for this task and develop an architecture to augment the capabilities of an LLM with a memory module called dynamic evidence trees (DETs) to develop and track multiple investigation threads. Through extensive experiments on multiple datasets, we highlight how LLMs, as-is, are still inadequate to support intelligence analysts and offer recommendations to improve LLMs for such intricate reasoning applications.", "sections": [{"title": "I. INTRODUCTION", "content": "The impressive generation capabilities of LLMs [1]\u2013[3], along with their successful application in diverse areas [4]\u2013 [7] led us to explore their utility for intelligence analysis (IA). Key IA tasks involve uncovering plots from textual reports such that interventions can be made to prevent unfortunate events. This entails making connections between seemingly unrelated entities and events. This undertaking traditionally requires significant investments of time and effort from human analysts [8], [9]. Essentially, they are carrying out a \"con- necting the dots\" task, where dots are the information bits involving different entities/events in their reports.\nIA can be viewed as comprising the following subtasks: i) Marshaling evidence, ii) Orchestration of the gathered evidence: sensemaking and the construction of defensible and persuasive arguments from evidence, and iii) narrative generation. To be successful in IA, analysts often require an overarching capability of creative, speculative, and imaginative reasoning which helps build hypotheses. Even after making the connection between relevant information \u2018dots', an analyst must decide what this synthesized information portrays for the context at hand. New evidence can support or topple the hypothesis under consideration. After gathering all required evidence, analysts need to produce convincing arguments and reasoning to generate an appropriate alert to their superiors/au- thorities.\nWhile numerous representations have been proposed for reasoning with LLMs [10]\u2013[12] they are typically focused on individual questions or tasks [2], [13], [14]. Our focus here is on reasoning emergent from assimilating hundreds of documents, sometimes beyond context window limits for LLMs. We design a set of experiments to study whether LLMs can solve IA problems and if not, how can they be augmented to support such analysis. Our codebase is publicly available at https://github.com/DiscoveryAnalyticsCenter/speculatores. Our key contributions are:\n1) We conduct the first investigation of the feasibility of us- ing LLMs in intelligence analysis where both evidence- based reasoning and analytical creativity is of utmost importance.\n2) We develop a three-step augmentation to support the use of LLMs for IA: i) Dynamic Evidence Trees (DETs), a memory module to help organize evidences, ii) Data condensation via LLMs, and iii) an LLM-driven search and retrieval process.\n3) Applying our framework on multiple IA datasets, we show that while our augmentations help orchestrate and improves narratives on large datasets, LLMs still lack the analytical creativity to craft convincing arguments.\n4) We outline detailed recommendations for applying LLMs to IA tasks and specifically how they can serve as modules in specific subtasks such as evidence mar- shalling and narrative generation."}, {"title": "II. METHODOLOGY", "content": "We design our experiments with three intelligence analy- sis datasets. These datasets contain field reports of various events conducted by adversaries. Among these, some may be relevant and others are irrelevant. Analysts need to find pertinent information by isolating relevant reports and piecing the information together to speculate the main plot. We pose four research questions (RQ) to understand whether LLMs can be useful for the IA process:\n\u2022 RQ1: Can LLMs solve IA problems on their own?\n\u2022 RQ2: Does augmentation help? If so what kinds of augmentation support the IA task?"}, {"title": "A. Preliminaries", "content": "Each IA dataset is a set of chronological reports R = {r_1,r_2,...,r_n} such that report r_{i-1} originates or was as- signed before r_i. When there are multiple reports available on a single day, we can order them arbitrarily.\nWe also have a set of evidential information nuggets called 'dots' ED = {ed_1,ed_2,...,ed_m}. We can model each report r_i as one evidential dot ed_{mi} or an amalgamation of multiple evidential dots, i.e., r_i = {ed_1,ed_2,...,ed_m}. Thus, we can represent each dataset as a tuple of (R, ED).\nConceptually there can be two types of information dots in IA: i) evidential dots ed_i, which come directly from the intelligence reports; ii) hypothesis dots hd_i, which are pro- duced by synthesizing multiple evidential dots (and/or other hypothesis dots). Thus each hypothesis dot is modeled as hd_i = {hd_1, hd_2, ..., hd_k }, ... {ed_1, ed_2, ..., ed_l}.\nThe goal is to use the tuple (R, ED) to create a set of hypothesis dots, i.e. Hp = {hd_1, hd_2,..., hd_{an}}. In the most basic format, a hypothesis dot ha_i can thought to be built with two evidential dots; hd_i = {ed_q,ed_p}. Thus, the LLM autoregressively generates each token hd_{i,j} of a hypothesis dot hd_i as:\nP(hd_i) = P(hd_{i,1}, hd_{i,2}, ..., hd_{i,n})\n~\n\n\nWe postulate that the autoregressive nature of LLMs will help model a report with evidential dot (or dots) as a whole, instead of modeling reports as an amalgamation of a set of entities. We note that each evidential dot ed_i is comprised of a set of entities. The autoregressive nature of LLMs will keep the rich contexts of evidential dots intact and build up each hypothesis dot (hd_j). We are postulating that LLMs with their auto-regressive and generalization properties will be able to model dots as standalone artifacts and connect them when needed."}, {"title": "B. Initial experiments", "content": "As the most basic step of the evaluation of LLMs for IA, we attempt to use each dataset in its entirety in the context windows, as the limit permits, to generate the narrative. We empirically test with varied system prompts. However, we find that the LLMs tend to only summarize the documents and present superficial information, despite numerous prompt variations to bring out implicit connections and the broader story. Moreover, the context length limits the number of documents that can be fit into the prompt. We randomly sub- sample the number of documents for datasets according to the context limit. We also quantitatively and qualitatively evaluate the responses as shown in the results section IV."}, {"title": "C. LLMs for Intelligence Analysis: Challenges", "content": "Through our initial experiments on LLMs without any augmentations, we identified some shortcomings for complex analytical reasoning tasks. A major limitation for LLMs is the context length and loss of attention with the increasing input size [15]. The two main challenges are i) lack of a proper memory module to keep track of all the evolving investigation threads, ii) limited context length that limits the number of reports that can be processed. To solve the first problem, we propose an augmented architecture with a much needed memory module to the pipeline, named \u201cDynamic Evidence Trees (DETs)\". Memory modules are increasingly being used to augment LLMs in various applications [16]. As such, we devise a memory module in form of trees to help LLMs orchestrate evidence as the reasoning goes along. As an improvement on the second front, we also perform the tests with two different granularities of the reports. We propose condensing the reports into concise information chunks. In the later sections, we will formally define and present the inner workings of these two components."}, {"title": "D. First augmentation: Dynamic Evidence Trees (DETs)", "content": "As an improvement to the basic LLM, we augment it with dynamic evidence trees (DETs). This is the main memory component to save the evidence.\n= \nWe define the main graph structure of DETs as G = (V,E) where V denotes the vertices with v \u2208 V such that each vertex v_i is a structure named DOT (i.e., in- formation dot); and E denotes edges e_{ij} \u2208 E which is the parent-children relationship between the DOT_i and DOT_j. Thus set of vertices V can be defined as {DOT_1, DOT_2, . . . DOT_i}. Each DOT_i can be both eviden- tial and hypothesis dots and we define DOT as a quadruple of (information, DOTchildren, DOTparents, document), such that each DOT_i can have another set dots as children or parents. Evidential dots belong to one report of the dataset and we save that information in the node. DETs also holds a database object with LLM based embedding vectors for retrieval operation. Thus we denote DETs as tuple of (DB, G). During the creation of the DETs, we can think of each DOT as a subgraph with a tree like structure (with one node if it does not have any children or parents). LLM based operations will decide if multiple {DOT_1, DOT_2, . . . DOT_i} can be merged to create a new hypothesis DOT. This new DOT will have all the comprising dots as its children and the children dots will also save the new DOT as their parents. Thus, we are creating new connections among different disjoint subgraphs {DOT_1, DOT_2, . . . DOT_i} in the form of a new parent DOT.\nWe also experiment with a variant of DETs, where each dot is amalgamation of all information of a person in the dataset and the edges are defined as connections between two persons. Thus we can define DETs as G = (V, E) where V denotes the vertices with v \u2208 V such that each vertex v_i is information about a person; and E denotes edges e_{ij} \u2208 E which is the connections between two persons, i.e., DOT_i and DOT_j.\nWith the help of DETs, LLMs try to build up hypotheses and investigation threads. Intelligence analysts go through a process of discovery and combining the dots to build up various hypotheses. Like human analysts might do, DETs helps to keep track of all the information dots and keeps connecting them with new relevant information in a tree like structure. Each hypothesis dot represents an investigation thread that may or may not get added to another related investigation thread. We report results for both regular and person-based DETs in section IV"}, {"title": "E. Second augmentation: Data Condensation", "content": "We utilize the language modeling capabilities of LLMs to digest the set of reports into usable information dots before generating reports. These condensed information dots can be merged with other information dots to create hypotheses. We empirically test various system prompts to break down the report in a zero-shot fashion such that a report r_i turns into an evidential dot ed_i. We use dynamic pre-processing to break it down further, if deemed necessary by the LLM. All the reports in our experiments yielded a single dot each. This also indicates the concise reporting practice of the intelligence community."}, {"title": "F. Third augmentation: LLMs for Retrieval", "content": "To build up a DET, we require search and retrieval ca- pabilities on the existing evidence list, given new evidence. We augment the LLMs with a two-step retrieval pipeline. The first step is powered by an LLM-based embedding vector database and similarity-based search. Following the recent use of LLMs-based embeddings for retrieval [17], [18], we adopt an instructor embedding model [19] for our database. The second step uses an LLM to filter the extracted set of DOTS."}, {"title": "G. The augmentations altogether", "content": "The augmented version of the experiment is designed to process intelligence report sequentially and keep track of the evidence in DETs. This sequential approach helps in three ways; first, to conform to the dynamic nature of how intelli- gence report are available and assigned over period of time; second, to help build up DETs to keep track of the emerging evidence; third, to get around the context limitation of the models. This also follows the natural tendencies of how analyst must make and keep the hypotheses running until enough evidence can be accrued and a reasoning can be established for each hypothesis. New reports and the resulting information can either approve or disprove any existing hypotheses. The DET- based augmented pipeline considers this aspect and inputs the reports iteratively to simulate the temporal continuum of intelligence analysis.\nEach report goes through a pre-processing step based on the dataset provided and a set of information dots are extracted from the report. We initialize a database and keep updating it with new information dots during runtime. During the tree building operation, the system first attempts to search relevant DOTs from the DETs. The extracted DOTScandidates go through a parent level hypothesis DOTs identification and consolidation process. We take the lowest common parent hypothesis nodes and assign the new DOT to that node. This ensures that we are assigning the new DOT to the most relevant branch of the evidence tree. Afterwards, each evidential dots of the resulting new branch are extracted. LLMs will synthesize all the evidential dots to create a short narrative for this particular investigation thread. From the resulting DETs, we isolate the largest chain of events and use that as the main DET for the input. The full generated DETs along with its disjoint nodes also demonstrate how LLMs are being augmented to keep track different documents and how each of these documents are being utilized through the chain of events for building up to the final hypothesis."}, {"title": "III. EVALUATION SETUP", "content": "In this section, we describe the layout of our evaluation procedure. Because we aim to test the efficacy of LLMs as an intelligence analyst, we evaluate the augmented architecture in an ablated fashion. We remove augmentations one by one and capture the improvement from the most basic bare-bone LLM. We also consider document-entity networks and clustering as a more traditional baseline sans any language modeling. For the basic form, we adopt two versions, one with a sub-sampled report set and another with the highest performing clusters on the report set coming from one of the traditional baselines. We use default parameters for the generation, with an enu- merative testing with temperature and word limit to capture the randomness and creativity in reasoning. Our architecture is designed to use any type of LLM, both through API and local storage. It can fall back to a local LLM if it fails to get a results from the API calls. We experimented with five models from four different model families: i) GPT-3.5, and ii) GPT-4 from OpenAI, iii) Llama-2 [11] from Meta AI, iv) Mistral- 7B [20] from MistralAI, and v) Gemma-2 [21] from Gemini platform, Google."}, {"title": "A. Datasets and ground truth", "content": "We utilize three datasets-Sign of the Crescent(Crescent), Atlantic Storm and Manpad-popular in training and analytics competitions [22]. The Crescent and Atlantic Storm datasets have their solutions divided into a few subplots and charts. Crescent has 3 subplots, each divided into 1-3 charts, with a total of 8 charts describing the information dots and the chain of reasoning. Similarly, the Atlantic Storm dataset has 6 subplots and 13 charts. All datasets use irrelevant reports as noise to make the problem more challenging. Statistics of the datasets and nodes created in DETs by the augmented architecture shown are shown in Table I. We differentiate surface level ground truth text with implicit ground truth. An AI model's true capabilities in case of solving IA process can only be evaluated by comparing the implicit ground truth. For Crescent and Atlantic Storm, we isolate the implicit information by manually going through the chart and removing the document level dots."}, {"title": "B. Document-entity network and clustering", "content": "We consider a document-entity network as the basic base- line, as demonstrated in the entity graph based approaches [22]-[24]. These works typically consider the story as an established connection between a set of entities by graph properties. The task of sense-making, finding out the bigger story, and narrative generation are still up to the analysts."}, {"title": "C. Evaluation Metrics", "content": "We report the F1 score for the relevant/irrelevant document classification accuracy in all three datasets. We also compare the ground truth narrative with model narrative by adopting traditional ML metrics, i.e., average ROUGE 1/2/L(ROUGE- Lsum) [30], and METEOR [31]. To capture the quality of the narratives, we also employ GPT-4 [32] ratings. Recent works established LLMs as viable judge to evaluate the quality of open ended text generation [33]\u2013[37] and to address the burdensome work of evaluation with human preferences [12], [38], [39]. We identified three important qualities for the model responses, i.e., relevance, coverage, and thoughtfulness, and test GPT-4 ratings in respect to these qualities."}, {"title": "IV. RESULTS", "content": "Our experiments show that even with the augmented archi- tecture, LLMs are unable to go beyond superficial reasoning. In this section, we expand on each RQ with the respective results."}, {"title": "A. RQ1 and RQ2: Can LLMs solve IA datasets on their own? Does augmentation help? If so what kind of augmentation helps?", "content": "To answer this set of RQs, we compare narratives from different experiments to the ground truth solutions. For each of Crescent and Atlantic Storm, we combined the ground truth of the charts into one report. For Manpad, we followed the rule- based grade to isolate relevant documents, with the entities extracted and desirable ones.\nWe first compare the classification accuracy for relevant documents. LLMs were only marginally better than clustering which suggests that the LLM-driven retrieval process is not upto mark, even though we had a two-step retrieval process. Moreover, we also compare the narrative qualities against ground truth. The results for GPT-3.5 (Table IIIa) show that the basic prompted LLM has unsatisfactory metrics against the ground truth. Because our ground truth is the implicit text, rather than the superficial text, it shows that LLM was unable to pick up and guess implicit stories from the documents. We also noticed a trend of subpar scores in the Atlantic Storm dataset which indicates that LLMs struggle with larger datasets. To answer if augmentations help, we plot the normalized metrics for different methods with multiple temperatures (0.2-1.5) in Fig. 7. DET (regular) and DET (person-based) show improved performance across metrics and datasets. We also observe the largest improvement with the augmentation occurs for Atlantic Storm, which indicates that the augmented version helps in better structuring the large sets of reports. In other datasets however, it only slightly outperforms basic strategy. Moreover, the augmented version's performance degraded once the data condensation feature was turned off, which suggests that, to build a structured architec- ture, summarization and cleaning the data helps tremendously. Traditional evaluation metrics often fail to correctly quantify the open-ended generation for the desirable characteristics [33], [40], so we also used GPT-4 to score the quality of the narratives. Across the qualities (i.e., relevance, coverage and thoughtfulness), we quantify the ratings in single Likert scale score (1-7) (Table IIIb). The trend confirms the narrative quality scores reported in Table IIIa. However, in the next sec- tion, we expand on a potential pitfall of traditional evaluation systems."}, {"title": "B. RQ3: What is the effect of temperature and allowable context size on the skills of LLMs?", "content": "We also investigate if it is possible to increase the creativi- ty/analytical skills by utilizing the randomness and creativity focused prompts for language models. For the basic setting, we tested two types of prompts with one containing system prompts about \u201cbeing creative and imaginative\u201d in reasoning. For the augmented architecture, the quality of the narrative is affected by both temperature and allowable context window limit for each hypothesis dot. Rather than limiting the gener- ation by setting the token length, we asked the model to curb generation within a certain word limit by prompt. We carried out parameter sweep for this setup on Crescent dataset and plot the results for combined ROUGE score and METEOR score (appendix). Experiments show that for basic prompts, there are no trends in increasing temperature, for both type of prompts. For the augmented architecture in GPT models, we find two optimum setups for ROUGE and METEOR, i.e., (100 words and 0.7 temperature; 150 words and 0.5 temperature)."}, {"title": "C. Use case studies", "content": "Here we will qualitatively showcase the inability for the LLMs to look past the surface-level information. The goal of intelligence analysis is not just to summarize the reports, rather to find out the connections, along with back stories about the adversaries. We showcase the narratives from experiments for Crescent dataset in Fig. 8b and a high level sketch of the solution in Fig. 8a. Crescent dataset plot describes the plans for three synchronized attacks by Al Qaeda operatives, coordinated by Muhammad bin Harazi. Harazi sourced the funds and and organized operatives on three fronts, i) a container ship bound for Boston from Amsterdam, ii) Amtrak Train #19 bound for Atlanta, and iii) the New York Stock Exchange. Ziad al Shibh, Sahim Albakri, Abu al Masri, Saeed Khallad, Tawfiq al Adel are the main individuals, cobwebbed by other operatives on each of three fronts. Reviewing the output against ground truth (Fig. 8c), we find that LLMs lack on two fronts: i) failure to paint out an implicit story about the connections, ii) filtering out relevant and irrelevant documents. We also look at one particular subplot of Crescent dataset. The story is about a ship called \"Holland Queen\" and how it might have a potentially dangerous cargo onboard. Different versions of our LLMs only described the document- level entities and failed to make a guess that the ship might be carrying dangerous cargo."}, {"title": "V. RECOMMENDATIONS", "content": "We answer (RQ4: Where can LLMs contribute in the 3- step process outlined in section \u00a71) in the form of recommen- dations from our findings. From the quantitative and qualitative experiments, it is apparent that the generation of LLMs studied here struggle with in-depth analytical reasoning. Across all datasets, we see trends of good summarization, and less on analytical creativity or speculative/imaginative reasoning (not notwithstanding LLMs ability to hallucinate information). We briefly describe these points here:\nSteering speculative reasoning is hard: LLMs are adept at summarizing documents into groups. However, our repeated attempts with multiple types of prompts and parameter sweeps failed to invoke the capability required to properly speculate connections. Complementary to this finding, we also experi- mented with a shorter use case with brief descriptions of four persons as shown in Fig. 9. When asked about the connection between only two persons, LLMs were able to speculate based on the similarity of their names. However, with the addition of two more persons in the mix, LLMs failed to invoke the previous speculation. We also noticed that the position of the target entities mattered. Our finding is consistent with those of recent research, e.g., needle in the haystack [41], lost in the middle [42] studies. Moreover, our findings suggest that, LLMs still struggle on a much smaller scale than previously thought of and this depends on both task complexity (i.e., number of entities in IA) and context length. We plan to address this in future work.\nLarger is not necessarily better: Despite GPT-4 being significantly larger, we find no discernible improvement as shown in Table IIIc. The results were consistent across dif- ferent model families and indicates that the limitation in such reasoning is rather inherent to the LLMs in general. Notably, Gemma-2 scored lower than others, despite being a newer (and supposedly better) model which shows that it is important to establish such real-world and data-centric tasks for improving LLMs in general. As a ray of hope, preliminary tests on OpenAI's o1 model [43] (unavailable during submission) show substantial improvement which can be attributed to the additional chain-of-thought reasoning steps invoked during generation. This reinforces that additional reasoning augmentations can be helpful for IA tasks.\nLLMs are good organizers: Even with the lack of in-depth analytical reasoning, LLMs were able to group related entities and events together, specially with smaller datasets. With large datasets, augmentation is needed to orchestrate some of the required grouping because of the context limitation and loss of attention [15].\nOrchestrating the evidence is a major challenge: For data-centric analytical tasks like IA, it is hard for an LLM to take everything into context in a single prompt. Augmentation is necessary to organize the evidence. We propose a LLM-driven framework for these tasks. LLMs can also help in the search and retrieval process for such frameworks. Frameworks designed to show the immediate reasoning steps can also be helpful for the analysts to develop mental models.\nLLMs can generate good quality reports: Although LLMs throughout our experiments lacked in-depth analyt- ical/imaginative reasoning, they can produce good quality reports with the set of evidences. We suggest the use of LLMs as an interface before and after the data pipeline for such tasks.\nPitfalls of using traditional metrics: We also noticed a significant limitation of traditional metrics, i.e., lexical simi- larity metrics (ROUGE, METEOR) and contextual embedding based metric (BERTScore) [44]. Due to the common themes, vocabulary, phrases, and content overlap across reports, these metrics exhibit unusually high scores, even across different datasets. This is particularly evident for the Atlantic Storm and Crescent datasets; the most notorious results being that of BERTScore, showing almost equal scores across different datasets. We report the findings in Table IV. In light of these abnormalities, we suggest to use larger foundation models for automatic evaluations [45].\nData condensation is helpful: Turning off data condensa- tion had a noticeably negative impact on experimental results with DETs. This is because DETs rely on concise and precise text for efficient search, retrieval, and other operations. We recommend incorporating data condensation in any framework designed for data-centric applications, as it can significantly improve results.\nClassifying relevant/irrelevant documents: One of the challenges for data-centric frameworks such as proposed here is the need for good search and retrieval capabilities; poor performance at this stage percolates downward to lackluster results, a theme well known from retrieval augmented gener- ation (RAG) studies.\nLeveraging external knowledge sources: We find some cases where it would be beneficial to have access to an external knowledge base or search engine. One such case arises in the Crescent dataset, where an information dot about reservations on AMTRAK Train #19 was referred. Moreover, the word \"Crescent\" was also mentioned separately in later reports.\nLLMs can generate good quality reports: Although Without access to external knowledge or the AMTRAK sched-"}]}