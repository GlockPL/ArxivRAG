{"title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation", "authors": ["Dongsheng Zhu", "Weixian Shi", "Zhengliang Shi", "Zhaochun Ren", "Shuaiqiang Wang", "Lingyong Yan", "Dawei Yin"], "abstract": "Although current Large Language Models (LLMs) exhibit impressive capabilities, performing complex real-world tasks still requires tool learning. Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to interact with external environments, but they are limited in perceptual scope and lack adequate task-planning capability. To address these limitations, other studies introduce the first Search-based Decision Tree (DFSDT), which still suffers from the high computational cost. In this paper, we introduce a novel parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama). First, we transform traditional tree-based tool search paths into Directed Acyclic Graph (DAG) structure, generating a high-quality parallel tool invocation dataset. The DTA-Llama is then trained on the dataset to learn to iteratively divide the current task into several parallel tool invocation sub-tasks and aggregate the invocation results to decide the next actions. Furthermore, we introduce an efficient inference framework inspired by the Process/Threads mechanism when applying the DTA-Llama to practical tasks. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), which are pre-trained and fine-tuned on massive amounts of textual data, have demonstrated powerful proficiency in various artificial intelligence tasks, such as conversation (Zheng et al., 2023), logical reasoning (Pan et al., 2023) and coding (Nijkamp et al., 2023). However, more real-world tasks often require the LLMs to interact with the environment to get necessary external information or feedback, such as checking real-time flight status (Guan et al., 2024) and complicate calculation for data analysis (Sun et al., 2024). To this end, tool learning has emerged recently which aims to equip the LLMs with external tools and teach them how to leverage the tools to accomplish real-world tasks.\nPrevious tool learning methods typically work in pipelined and tree-based paradigms. Concretely, early studies usually perform the tool invocation in a pipeline, such as the Chain-of-Thought (CoT) reasoning (Kojima et al., 2022), and the ReAct mechanism (Yao et al., 2022). In these methods, tool learning agents (i.e. the LLMs) interact with the environment through a Thought-Action-Observation framework (as shown in Figure 1(a)). However, these methods usually focus on reflecting and planning based on local observations, rather"}, {"title": "2 Related Work", "content": "Tool Learning The agent tool learning aims to expand LLMs capabilities by teaching LLMs to use external tools. Many early studies (Patil et al., 2023; Tang et al., 2023; Huang et al., 2023) focus on laying the groundwork for datasets yet exhibit limited variety in tool usage. To bridge this gap, Qin et al. (2023) developed a more comprehensive multi-tool benchmark and proposed an advanced tool invocation method using Depth First Search-based Decision Tree (DFSDT). Building on this, Zhuang et al. (2023) employed A* search algorithm for pruning, while Kim et al. (2023) adopted a compiler-based approach to parallelize tool invocation, both of which improved efficiency to some extent. Meanwhile, Du et al. (2024) and Chen et al. (2024) controlled the stability of LLM tool invocation through self-reflection and Direct Preference Optimization (DPO, Rafailov et al., 2024), respectively. Despite these advances, these methods remain rooted in tree-based search paradigms, lacking a broader perspective on task planning. Additionally, recent works have begun exploring tool creation and integration with agents, opening new avenues for research (Qian et al., 2023; Hao et al., 2024; Schick et al., 2024; Hao et al., 2024; Yuan et al., 2023).\nTask-Planning for LLMs Task planning capability is a crucial factor for the success of LLMs in problem-solving. Some methods attempt to decompose tasks into sub-goals and then plan for"}, {"title": "3 Methodology", "content": "In this section, we describe: (1) the shortcoming analysis of previous methods (\u00a7 3.1); (2) how the DTA-Llama be trained based on constructed parallel tool using data (\u00a7 3.2); (3) how to implement an efficient, Process/Threads-based parallel framework during inference (\u00a7 3.3). The last two parts work together to ensure the robustness and parallelism of tool invocation of DTA-Llama."}, {"title": "3.1 Background", "content": "As aforementioned, most recent tool invocation studies are tree-based, which are first developed by ToolLLM (Qin et al., 2023). Comparing to the ReAct (Yao et al., 2022) (or CoT (Wei et al., 2022)) that invokes tools through pipelined tool interaction, the tree-based methods replace the serial tool usage with the Depth First Search-based Decision Tree (DFSDT) algorithm. Consequentially, these methods increase the fault tolerance of LLMs and improves task planning capabilities. However, this comes at the cost of increased time complexity. This is mainly because that DFSDT typically generates longer tool invocation sequences due to its backtracking mechanism, which involves multiple attempts at new nodes. While this improves task completion rates, it sacrifices execution efficiency (see Figure 1 for an example)."}, {"title": "3.2 Divide-then-Aggregate Tool Invocation", "content": "To address the above problems, we propose the Divide-Then-Aggregate (DTA) tool invocation paradigm. This method allows LLMs to decompose the task, generate a set of parallel tool invocations, and aggregate the results after execution. DTA improves task planning and optimizes the reasoning framework for tool invocation, enabling LLMs to invoke tools efficiently in parallel and better tackle complex tasks.\nTransforming the Serial Tool Using data to Parallel To steer the LLMs with capabilities of parallel tool invocation, it is critical to construct the corresponding finetuning datasets. However, in most previous methods, e.g., vanilla CoT/ReAct methods or tree search-based algorithms like DFSDT, LLMs typically rely on invoking one tool at a time, which is not consistent with our setups. To this end, we utilize this type of data to transform it from a serial structure into a parallel structure.\nAs shown in Figure 2, we first collect the serial successful tool innovation path from original tool searching trees. Given the tree-like tool searching trajectories generated by the tree search-based algorithm, it is inevitable that the trajectories contain redundant or erroneous paths. Therefore, we take the successful node series spanning from the root to the successful leaf node as successful path P, and retain the nodes in P; all nodes in other paths are filtered. If a node in the failed paths is also part of the successful path, it is retained.\nNext, we utilize a powerful LLM, to identify whether any tools in P can be executed in parallel. We choose GPT-4-turbo (OpenAI, 2023) to perform this task. If GPT-4 detects parallelizable tools, it establishes their relationships and organizes them into a Directed Acyclic Graph (DAG), represented as G. The feasibility of parallel execution depends on input-output dependencies and logical causal relationships. For further details on transforming to the DAG structure using GPT-4, refer to Appendix A. For some data of P that cannot be parallelized, we do not discard them but instead retain their original structure. Finally, we construct the tool invocation mechanism by performing the level-order traversal on G, allowing tools on the same level to be controllably invoked in parallel and aggregate. This implements the Divide-Then-Aggregate approach during the data construction phase.\nIt is important that we only transform the struc-"}, {"title": "3.3 Process/Thread-based Inference", "content": "We fine-tuned the LLM with the DTA-Tool, resulting in DTA-Llama, which can invoke tools in parallel. To support the modified LLMs, we developed a new inference framework, as illustrated in the Figure 2(b). This framework redefines the Thought-Observation cycle based on CoT/ReAct, executing tool invocation in the form of Process/Threads.\nProcess Originally, Thought could only design an invocation strategy for single tool, limiting its perceptual scope. In contrast, Process enhances the LLMs' ability to divide tasks and plan multiple parallelizable tool invocation strategies. Specifically, during each round, LLMs first evaluate the task's status and progress based on the historical trajectories. Then, LLMs analyze what needs to be done in the current step and decompose the task based on the available tools. Finally, LLMs sequentially generate a series of complete tool names along with their corresponding input parameters, which can be executed in parallel. This multi-tool approach helps broaden the perspective of LLMs, increasing the informational richness of each Thought step. In our framework, Process directly integrates Action into Thought. After careful deliberation, the LLMs provide a formalized tool invocation plan that can be extracted using regular expressions, facilitating subsequent execution by Threads.\nThreads Threads refers to the steps faithfully execute the tool strategies presented in the Process. In previous frameworks, Thought provides only one tool invocation strategy. However, when Thought is capable of proposing parallel tool invocation strategies, the execution tool module must also be adapted accordingly. To address this, we introduced Threads. All the tool invocation strategies provided by the Process are distributed across multiple Threads, which then independently and concurrently execute each strategy. It is worth noting that real-time tool APIs generally allow a certain level of concurrency. Moreover, the tool invocation plans proposed by the Process are typically small in scale. Therefore, even if the parallel plans all target the same API, the concurrency is not high enough to cause an API crash.\nIntermediate State Lock When tools are invoked using Threads, the information processing load on the inference framework increases proportionally. The original observation only needed to record the execution result of one tool. Now, it must systematically link multiple tools and their corresponding results in an orderly manner. Otherwise, a disorganized observation could hinder the LLM's subsequent decision-making. To achieve this, we have specifically implemented a thread-oriented intermediate state lock at the end of each Threads round. The lock is only released once all Threads have completed their execution and returned the results. During the complete invocation process, the intermediate state lock regularly maintains communication between Threads and Process. The execution results of Threads are aggregated and used as part of the input to interact with the LLM, initiating the next round of Process. This cycle repeats until the task is completed."}, {"title": "4 Experimental Setup", "content": "Dataset We use StableToolBench (Guo et al., 2024) for evaluation. All test cases in StableToolBench are actually derived from the test portion of ToolBench (Touvron et al., 2023b). Concretely, ToolBench is divided into six evaluation subsets based on tool categories and scenarios. The tool categories are as follows: Inst. denotes unseen instructions for the same set of tools in the training data, Tool denotes unseen tools within the same (seen) category as those in the training data, and Cat. denotes unseen tools from a different (unseen) category. The scenarios are: II for single-tool instructions, I2 for intra-category multi-tool instructions, and 13 for intra-collection multi-tool instructions. The difficulty level of the task escalates progressively from Il-Inst. to I3-Inst.. Comparing to the original ToolBench, StableToolBench introduces an extra caching system and an API simulator to mitigate the instability issues associated with real-time APIs.\nAs for the DTA-Llama training, we adopt the training portion of ToolBench and transform it into DTA-Tool style using GPT-4-turbo (OpenAI, 2023) through the method described in Sec.3.2. ToolBench provides a corresponding API set for each data point, enabling us to focus on tool learning without having to pay attention to tool retrieval. A detailed overview of DTA-Tool is presented in Table 1, with an instance provided in Appendix C.\nBaselines We use both GPT-series and other open-source LLMs as our baselines. For the GPT-series models, we use OpenAI's GPT-3.5-turbo and GPT-4-turbo (OpenAI, 2023), leveraging their function calling capabilities. While the exact mechanisms remain unclear, OpenAI has enabled parallel tool invocation in these models. Therefore, in addition to ReAct and DFSDT, we include Parallel as baseline paradigms. For open-source models, we use ToolLLaMA (Qin et al., 2023), which is fine-tuned from Llama2-7B on ToolBench, and com-"}, {"title": "5 Experiments", "content": "In this section, we first evaluate the performance of our method in tool learning tasks through extensive experiments in \u00a7 5.1. Next, we analyze its computational costs compared to baselines in \u00a7 5.2, and extend our method to different models to evaluate its generalizability in \u00a7 5.3. We showcase the practical workflow of DTA-Llama through case examples in Appendix E."}, {"title": "5.1 Main Experiments", "content": "SoPR As shown in Table 2, our method demonstrates a significant advantage over all open-source baselines *. While GPT series models, particularly GPT-4, show overwhelming performance advantages when compared to earlier open-source models, our approach not only surpasses GPT-3.5 but also competes with GPT-4. These results suggest that our method has a notable impact on enhancing the tool invocation capabilities of LLMs.\nFurthermore, under the same GPT model conditions, DFSDT and Parallel show no significant performance differences, indicating that their parallel function call strategy does not significantly im- *ReAct\u2020 and DFSDT\u2020 are specially retrained versions of ToolLLaMA, aimed at eliminating data filtering factors. However, the performance of these versions is inferior to that of the original model, suggesting that data filtering is not the primary reason for the improvement of our method."}, {"title": "5.2 Computational Cost", "content": "Token Consumption We count the tokens for both Completion and Prompt. Completion refers to the output tokens generated by LLMs, while Prompt refers to the input tokens provided to the LLMs. Typically, the cost of Completion is higher than that of Prompt. Figure 3 shows the token consumption for all methods. For each subset of StableToolBench, we calculate the average token"}, {"title": "5.3 Generalizability", "content": "To validate whether our method can achieve similar improvement across a wide range of LLMs besides Llama2-7B, we select ToolLLaMA (DFSDT) as the baseline and conduct fine-tuning and testing on Llama2-13B and Llama3-8B. The average experimental results of six benchmark subsets are presented in Figure 4. The Llama2-7B results are derived from the Averages of ToolLLaMA (DFSDT) and DTA-Llama in Table 2. The results show that our method significantly outperforms the baselines across across all scales of LLMs, especially in the SoWR metric for Llama3-8B, where the improvement exceeds 40%. More detailed experimental results and analysis are provided in Appendix F."}, {"title": "6 Conclusion", "content": "In this paper, we introduce DTA-Llama, a novel tool learning approach based on the parallel invocation of tools through the iteratively Divide-Then-Aggregate paradigm. We construct the training data by transforming sequential data into a parallel DAG structure and use this data to train the model. Subsequently, we integrate a Process/Threads-based inference framework to enable LLMs to perform tool invocation in parallel. Extensive experimental results demonstrate that, compared to existing methods, DTA-Llama not only significantly improves performance but also substantially enhances the efficiency of tool learning in LLMs."}, {"title": "Limitations", "content": "This paper aims to advance the research of tool learning in LLMs, particularly in both industry and academia. However, due to limitations in human resources, computational power, and the current research conditions, there are certain constraints, as outlined below:\nFirst, due to resource constraints, we were unable to conduct additional experiments on larger models to further validate the effectiveness of DTA-Llama. With the publication of this paper, we hope that more researchers in the field will attempt to build upon and extend our work.\nSecond, although our method shows improvements over existing tool learning approaches, LLMs still struggle to reliably and consistently address complex real-world problems through tool invocation. We hope to attract more researchers to the study of tool learning, as this area urgently requires more attention and resources."}, {"title": "A Prompt", "content": "Data Transformation Prompt Figure 5 shows the prompts used during the data transformation process. We use this prompt with GPT-4-turbo to transform the serial data into DAG structure. The prompt first evaluates the reasonableness of the conversation, discarding any data that doesn't meet the criteria. It then assesses whether the steps can be processed in parallel, creating a planning path in DAG structure. The evaluations of reasonableness and parallelism follow distinct analytical processes, embodying a form of internalized CoT prompt engineering. Applying these analyses to DAG generation leads to more accurate results.\nSystem Prompt Figure 6 illustrates the system prompt used during training and inference. In ToolBench, each user instruction is paired with a list of tool candidates, comprising both relevant and unrelated tools. The tools includes the names and descriptions of all tools in the candidate set. Additionally, each tool is equipped with a set of APIs designed to handle different types of tasks. The API is represented as a JSON list containing detailed information about the names, descriptions, and parameters of the APIs associated with each tool."}, {"title": "B Data Filtering Rules", "content": "The rules for data filtering are divided into before and after structural transformation. The specific rules are listed in text form in Figure 7. These rules can be easily implemented in code to filter the data."}, {"title": "CDTA-Tool Instance", "content": "Figure 8 illustrates a data entry in DTA-Tool, stored in JSON format. The outer layer consists of two keys: \u201cid\u201d and \u201cconversations\u201d. The \u201cid\u201d represents the user's instruction, while \u201cconversations\" details the task execution process by the LLMs. Within \"conversations\", there are four roles: \u201csystem", "user": "assistant\u201d, and \u201cfunction\u201d. The \"system\" role, which represents the system prompt, is introduced in Appendix A and is omitted here; \"user\" is the same as \"id\" and reflects the user's instruction; \"assistant\u201d represents the LLMs' reasoning process (marked by Thought) and provides a specific tool invocation plan (marked by Function Call); \"function\" contains the result of the tool execution. When tool invocation are parallel, the results are concatenated sequentially.\""}, {"title": "D Training Details", "content": "We train the LLMs using multi-round conversations with the following hyperparameters: for Llama2-7B and Llama3-8B, the learning rate is 5 \u00d7 10-5, warmup ratio is 4 \u00d7 10\u22122, with 4 epochs, a batch size of 64, and a maximum sequence length of 8192. All other settings are default. Training is performed on 8 \u00d7 A100 GPUs, while evaluation is done on one A100 GPU. For Llama2-13B, the hyperparameters are similar, with a learning rate of 5 \u00d7 10-5, warmup ratio of 4 \u00d7 10-2, 5 epochs, a batch size of 64, and a maximum sequence length of 8192. The model is trained on 8 x A100 GPUs, and evaluation is conducted on 4 x A100 GPUs."}, {"title": "E Case Study", "content": "In Figure 9, we present several cases to demonstrate the performance of DTA-Llama on practical tasks. All the tasks and tools in the figure are sourced from ToolBench. The figure presents two cases, each consisting of the user's Question (instruction), the LLMs' Output and the Tool Response. In DTA-Llama, if necessary, the system concurrently executes multiple tool strategies during the LLM Output phase and aggregates the results of all tool invocation during the Tool Response phase."}, {"title": "F Supplementary of the Experiments", "content": "More Details on Token Consumption In Table 5, we present the maximum token consumption for each method across various subsets. As shown in the table, the ReAct method has the lowest maximum token count, with our method following closely behind. The ReAct method employs a simple and straightforward task-planning mechanism, resulting in low resource consumption but the lowest success rate. It frequently struggles with complex tasks. In contrast, our method efficiently leverages additional tokens to tackle more challenging tasks without substantially increasing token usage, demonstrating superior cost-effectiveness.\nDetailed Generalization Experiments Table 6 presents the detailed results of the generalization experiments conducted on Llama2-13B and Llama3-8B. Using ToolLLaMA (DFSDT) as the baseline, our method demonstrates significant improvements over the baseline across all subsets."}]}